{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n\npath = '../input/movie-review-sentiment-analysis-kernels-only/train.tsv'\nfeatures = ['pid','sid','p','s']\nsms = pd.read_csv(path,names=features,sep='\\t',header=0)\nprint(sms.shape)\nprint(sms.head(10))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3119c61c000802b27f68dc8b6269ea711ec673a"},"cell_type":"markdown","source":"**Visualizing Class Imbalance**"},{"metadata":{"trusted":true,"_uuid":"d1556522bd56df8848d3d6c5251e3d3d18bebe42","scrolled":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_bars(auto_prices, cols):\n    for col in cols:\n        fig = plt.figure(figsize=(6,6)) # define plot area\n        ax = fig.gca() # define axis    \n        counts = auto_prices[col].value_counts() # find the counts for each unique category\n        counts.plot.bar(ax = ax, color = 'blue') # Use the plot.bar method on the counts data frame\n        ax.set_title('Number sentiments' + col) # Give the plot a main title\n        ax.set_xlabel(col) # Set text for the x axis\n        ax.set_ylabel('freq')# Set text for y axis\n        plt.show()\n\nplot_cols = ['s']\nplot_bars(sms, plot_cols)  \nprint(sms.s.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09ec91321659b4b0de40b05cc79191222468ffdf"},"cell_type":"code","source":"X=sms.p\nY=sms.s","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a737972a07800a8f33bb838148cf10721035608"},"cell_type":"markdown","source":">** Stemming and Lower casing.**"},{"metadata":{"trusted":true,"_uuid":"495459129ee61ae7f28ade284c50020ed8417f5b"},"cell_type":"code","source":"import nltk\nfrom nltk.stem import PorterStemmer\nps=PorterStemmer()\nl2=[]\nreview=[]\ns2=''\nfor row in X:\n    for words in nltk.word_tokenize(row):\n            #print(words)\n            l2.append(words.lower())\n            l2.append(' ')\n    s2=''.join(l2)\n    review.append(s2)\n    s2=''\n    l2=[]\nX=review\nprint(X[:1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1156a224366c9acd45202ff7188f0067ef4659f"},"cell_type":"markdown","source":"**Train, Test, Validation Split**"},{"metadata":{"trusted":true,"_uuid":"069675f0b322dddf57ad38ce604a61950f5ea138","scrolled":true},"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\nX_train, X_inter, Y_train, Y_inter = train_test_split(X, Y,test_size=0.3,random_state=1)\nX_val, X_test, Y_val, Y_test = train_test_split(X_inter, Y_inter,test_size=0.5,random_state=1)\nprint(len(X_train))\nprint(len(X_val))\nprint(len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b67ab08cd8066fb457cf2ffb5dbd7348071390af"},"cell_type":"markdown","source":"**Using Glove Embeddings**"},{"metadata":{"_uuid":"4e251b9d28f05c67a22d6d9895d111e2297f641a"},"cell_type":"markdown","source":"**Fitting training text on tokenizer for latter use tokenizer indexing.**"},{"metadata":{"trusted":true,"_uuid":"d18f1e7f67f16ec48e1138ca4535b7e461a10dc3"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nmax_sentence=len(max(X,key=len))\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb8f7a8abf83d3ea7d31ebbc5b70f43d7c4d098d"},"cell_type":"markdown","source":"**Creating Dict out of Glove twitter embeddings file.**"},{"metadata":{"trusted":true,"_uuid":"19c15de34f8d667a772602cb6d426141c8f0672c"},"cell_type":"code","source":"from numpy import asarray\nembeddings_index = dict()\nf = open('../input/glove-t/glove.twitter.27B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"944856b77536dc4177d0e0f903594e4d69b7b566"},"cell_type":"markdown","source":"**Creating Embedding Matrix with respect to tokenizer indexing**"},{"metadata":{"trusted":true,"_uuid":"1a0b58a991d799c47f4eb52c580077322cde9f18"},"cell_type":"code","source":"from numpy import zeros\nvocab_size = len(tokenizer.word_index) + 1\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"febd8af71abdce26a09bc810530718928e7b3218"},"cell_type":"markdown","source":"**Padding and Conversion of Text into Sequences**"},{"metadata":{"trusted":true,"_uuid":"4b7a6c3ce08dde1fcfec934342e1657d9d3fd681","scrolled":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nmax_sentence=len(max(X,key=len))\n\n#tokenizer = Tokenizer()\n#tokenizer.fit_on_texts(X_train)\nencoded_docs = tokenizer.texts_to_sequences(X_train)\ntrain_x = pad_sequences(encoded_docs, maxlen=max_sentence, padding='post')\nprint(train_x[0])    \n\nencoded_docs=0\nencoded_docs = tokenizer.texts_to_sequences(X_val)\nval_x = pad_sequences(encoded_docs, maxlen=max_sentence, padding='post')\nprint(val_x[1])\n\nencoded_docs=0\nencoded_docs = tokenizer.texts_to_sequences(X_test)\ntest_x = pad_sequences(encoded_docs, maxlen=max_sentence, padding='post')\nprint(test_x[1])\n\nencoder = LabelEncoder()\nencoder.fit(Y_train)\nencoded_Y_train = encoder.transform(Y_train)\ndummy_y_train = np_utils.to_categorical(encoded_Y_train)\nprint(dummy_y_train[:3])\n\nencoded_Y_val = encoder.transform(Y_val)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y_val = np_utils.to_categorical(encoded_Y_val)\n\n\n\n\nvocab_size = len(tokenizer.word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da92dacebf75a4c53247f2bba61c3b66b4bff0ee"},"cell_type":"markdown","source":"**Model Creation and Training**"},{"metadata":{"trusted":true,"_uuid":"193cef872b88d64fe14c0adc4d48790bf6ceab09"},"cell_type":"code","source":"from keras.layers import Input, Dense, concatenate, Activation\nfrom keras.models import Model\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Embedding,CuDNNGRU,Bidirectional\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.layers import LocallyConnected1D\nfrom keras.layers import Flatten\n\n\ntweet_input = Input(shape=(max_sentence,), dtype='int32')\n\ntweet_encoder = Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sentence, trainable=True, weights=[embedding_matrix])(tweet_input)\nbigram_branch = LocallyConnected1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\nbigram_branch = GlobalMaxPooling1D()(bigram_branch)\ntrigram_branch = LocallyConnected1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\ntrigram_branch = GlobalMaxPooling1D()(trigram_branch)\nfourgram_branch = LocallyConnected1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\nfourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\nmerged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n\nmerged = Dense(256, activation='relu')(merged)\n#merged = Dropout(0.2)(merged)\nmerged = Dense(5, activation='softmax')(merged)\nmodel = Model(inputs=[tweet_input], outputs=[merged])\nprint(model.summary())\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(train_x, dummy_y_train,  validation_data=(val_x, dummy_y_val), epochs=3,batch_size=128,verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6504189546a7a07bdb60d50be9b7390ad5de239","scrolled":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Embedding,CuDNNGRU,Bidirectional\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.layers import LocallyConnected1D\nfrom keras.layers import Flatten\n\n\"\"\"\nmodel=0\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sentence, trainable=True, weights=[embedding_matrix] ))\nmodel.add(LocallyConnected1D(128, 2,strides=1,padding='valid', activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(256, activation='relu'))          \nmodel.add(Dense(5, activation='softmax'))\nprint(model.summary())\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(train_x, dummy_y_train,  validation_data=(val_x, dummy_y_val), epochs=2,batch_size=128,verbose=1)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f5bdfc1108b9136e5843cbc249be9e5ff3d8b4f"},"cell_type":"markdown","source":"**Metrics **"},{"metadata":{"trusted":true,"_uuid":"f1c8edbc74b50cb12a6aa84d261a99962d6a497f","scrolled":false},"cell_type":"code","source":"import sklearn.metrics as sklm\npredictions=model.predict(test_x)\npred=[]\nfor idx,val in enumerate(predictions):\n    pred.append(np.argmax(val))\n\n\nprint(len(Y_test))\nprint(len(pred))\nprint(set(Y_test))\nprint(set(pred))\nmetrics = sklm.precision_recall_fscore_support(Y_test, pred)\n\n\nprint('Accuracy  %0.2f' % sklm.accuracy_score(Y_test, pred))\nprint('           0     1     2     3     4')\nprint('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1]+ '        %6.2f' % metrics[0][2]+ '        %6.2f' % metrics[0][3]+ '        %6.2f' % metrics[0][4])\nprint('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1]+ '        %6.2f' % metrics[1][2]+ '        %6.2f' % metrics[1][3]+ '        %6.2f' % metrics[1][4])\nprint('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1]+ '        %6.2f' % metrics[2][2]+ '        %6.2f' % metrics[2][3]+ '        %6.2f' % metrics[2][4])\n\nY_test=pd.Series(Y_test)\npred=pd.Series(pred)\npd.crosstab(Y_test, pred, rownames=['True'], colnames=['Predicted'], margins=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dd5ae8b9758374074064031cee2519e4304697e"},"cell_type":"markdown","source":"**test.csv Prediction, for the sake of simplicity i have not merged test and train code.**"},{"metadata":{"trusted":true,"_uuid":"1d8327eddb4bb585670bc591d61e52f64df5f7c6"},"cell_type":"code","source":"\n\npath = '../input/movie-review-sentiment-analysis-kernels-only/test.tsv'\nfeatures = ['PhraseId','sid','p']\ntest_frame = pd.read_csv(path,names=features,sep='\\t',header=0)\n\n\n#For test dataset\n\nl2=[]\nreview=[]\ns2=''\nfor row in test_frame['p']:\n    for words in nltk.word_tokenize(row):\n            #print(words)\n            l2.append(words.lower())\n            l2.append(' ')\n    s2=''.join(l2)\n    review.append(s2)\n    s2=''\n    l2=[]\ntest_frame['p_stemmed']=review\n\n\n\n\nencoded_docs=0\nencoded_docs = tokenizer.texts_to_sequences(test_frame['p_stemmed'])\ntemp_test = pad_sequences(encoded_docs, maxlen=max_sentence, padding='post')\n\npredictions=model.predict(temp_test)\npred=[]\nfor idx,val in enumerate(predictions):\n    pred.append(np.argmax(val))\n\ntest_frame['Sentiment']=pred\ntest_frame.drop(['p','sid','p_stemmed'],axis=1,inplace=True)\nprint(test_frame.head(10))\ntest_frame.to_csv('output.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}