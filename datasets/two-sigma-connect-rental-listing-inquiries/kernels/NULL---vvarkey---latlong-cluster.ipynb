{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f560b2c7-1cd6-741d-666b-465ce3b756fc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from math import exp\n",
        "import xgboost as xgb\n",
        "from sklearn.cluster import Birch\n",
        "\n",
        "random.seed(321)\n",
        "np.random.seed(321)\n",
        "\n",
        "X_train = pd.read_json(\"../input/train.json\")\n",
        "X_test = pd.read_json(\"../input/test.json\")\n",
        "\n",
        "interest_level_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "X_train['interest_level'] = X_train['interest_level'].apply(lambda x: interest_level_map[x])\n",
        "X_test['interest_level'] = -1\n",
        "\n",
        "#add features\n",
        "feature_transform = CountVectorizer(stop_words='english', max_features=150)\n",
        "X_train['features'] = X_train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n",
        "X_test['features'] = X_test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n",
        "feature_transform.fit(list(X_train['features']) + list(X_test['features']))\n",
        "\n",
        "train_size = len(X_train)\n",
        "low_count = len(X_train[X_train['interest_level'] == 0])\n",
        "medium_count = len(X_train[X_train['interest_level'] == 1])\n",
        "high_count = len(X_train[X_train['interest_level'] == 2])\n",
        "\n",
        "def find_objects_with_only_one_record(feature_name):\n",
        "    temp = pd.concat([X_train[feature_name].reset_index(), \n",
        "                      X_test[feature_name].reset_index()])\n",
        "    temp = temp.groupby(feature_name, as_index = False).count()\n",
        "    return temp[temp['index'] == 1]\n",
        "\n",
        "managers_with_one_lot = find_objects_with_only_one_record('manager_id')\n",
        "buildings_with_one_lot = find_objects_with_only_one_record('building_id')\n",
        "addresses_with_one_lot = find_objects_with_only_one_record('display_address')\n",
        "\n",
        "lambda_val = None\n",
        "k=5.0\n",
        "f=1.0\n",
        "r_k=0.01 \n",
        "g = 1.0\n",
        "\n",
        "def categorical_average(variable, y, pred_0, feature_name):\n",
        "    def calculate_average(sub1, sub2):\n",
        "        s = pd.DataFrame(data = {\n",
        "                                 variable: sub1.groupby(variable, as_index = False).count()[variable],                              \n",
        "                                 'sumy': sub1.groupby(variable, as_index = False).sum()['y'],\n",
        "                                 'avgY': sub1.groupby(variable, as_index = False).mean()['y'],\n",
        "                                 'cnt': sub1.groupby(variable, as_index = False).count()['y']\n",
        "                                 })\n",
        "                                 \n",
        "        tmp = sub2.merge(s.reset_index(), how='left', left_on=variable, right_on=variable) \n",
        "        del tmp['index']                       \n",
        "        tmp.loc[pd.isnull(tmp['cnt']), 'cnt'] = 0.0\n",
        "        tmp.loc[pd.isnull(tmp['cnt']), 'sumy'] = 0.0\n",
        "\n",
        "        def compute_beta(row):\n",
        "            cnt = row['cnt'] if row['cnt'] < 200 else float('inf')\n",
        "            return 1.0 / (g + exp((cnt - k) / f))\n",
        "            \n",
        "        if lambda_val is not None:\n",
        "            tmp['beta'] = lambda_val\n",
        "        else:\n",
        "            tmp['beta'] = tmp.apply(compute_beta, axis = 1)\n",
        "            \n",
        "        tmp['adj_avg'] = tmp.apply(lambda row: (1.0 - row['beta']) * row['avgY'] + row['beta'] * row['pred_0'],\n",
        "                                   axis = 1)\n",
        "                                   \n",
        "        tmp.loc[pd.isnull(tmp['avgY']), 'avgY'] = tmp.loc[pd.isnull(tmp['avgY']), 'pred_0']\n",
        "        tmp.loc[pd.isnull(tmp['adj_avg']), 'adj_avg'] = tmp.loc[pd.isnull(tmp['adj_avg']), 'pred_0']\n",
        "        tmp['random'] = np.random.uniform(size = len(tmp))\n",
        "        tmp['adj_avg'] = tmp.apply(lambda row: row['adj_avg'] *(1 + (row['random'] - 0.5) * r_k),\n",
        "                                   axis = 1)\n",
        "    \n",
        "        return tmp['adj_avg'].ravel()\n",
        "     \n",
        "    #cv for training set \n",
        "    k_fold = StratifiedKFold(5)\n",
        "    X_train[feature_name] = -999 \n",
        "    for (train_index, cv_index) in k_fold.split(np.zeros(len(X_train)),\n",
        "                                                X_train['interest_level'].ravel()):\n",
        "        sub = pd.DataFrame(data = {variable: X_train[variable],\n",
        "                                   'y': X_train[y],\n",
        "                                   'pred_0': X_train[pred_0]})\n",
        "            \n",
        "        sub1 = sub.iloc[train_index]        \n",
        "        sub2 = sub.iloc[cv_index]\n",
        "        \n",
        "        X_train.loc[cv_index, feature_name] = calculate_average(sub1, sub2)\n",
        "    \n",
        "    #for test set\n",
        "    sub1 = pd.DataFrame(data = {variable: X_train[variable],\n",
        "                                'y': X_train[y],\n",
        "                                'pred_0': X_train[pred_0]})\n",
        "    sub2 = pd.DataFrame(data = {variable: X_test[variable],\n",
        "                                'y': X_test[y],\n",
        "                                'pred_0': X_test[pred_0]})\n",
        "    X_test.loc[:, feature_name] = calculate_average(sub1, sub2)                               \n",
        "\n",
        "def transform_data(X):\n",
        "    #add features    \n",
        "    feat_sparse = feature_transform.transform(X[\"features\"])\n",
        "    vocabulary = feature_transform.vocabulary_\n",
        "    del X['features']\n",
        "    X1 = pd.DataFrame([ pd.Series(feat_sparse[i].toarray().ravel()) for i in np.arange(feat_sparse.shape[0]) ])\n",
        "    X1.columns = list(sorted(vocabulary.keys()))\n",
        "    X = pd.concat([X.reset_index(), X1.reset_index()], axis = 1)\n",
        "    del X['index']\n",
        "    \n",
        "    X[\"num_photos\"] = X[\"photos\"].apply(len)\n",
        "    X['created'] = pd.to_datetime(X[\"created\"])\n",
        "    X[\"created_hour\"] = X[\"created\"].dt.hour\n",
        "\n",
        "    X[\"num_description_words\"] = X[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "    X['price_per_bed'] = X['price'] / X['bedrooms']    \n",
        "    X['price_per_bath'] = X['price'] / X['bathrooms']\n",
        "    X['price_per_room'] = X['price'] / (X['bathrooms'] + X['bedrooms'] )\n",
        "    \n",
        "    X['low'] = 0\n",
        "    X.loc[X['interest_level'] == 0, 'low'] = 1\n",
        "    X['medium'] = 0\n",
        "    X.loc[X['interest_level'] == 1, 'medium'] = 1\n",
        "    X['high'] = 0\n",
        "    X.loc[X['interest_level'] == 2, 'high'] = 1\n",
        "    \n",
        "    X['display_address'] = X['display_address'].apply(lambda x: x.lower().strip())\n",
        "    X['street_address'] = X['street_address'].apply(lambda x: x.lower().strip())\n",
        "    \n",
        "    X['pred0_low'] = low_count * 1.0 / train_size\n",
        "    X['pred0_medium'] = medium_count * 1.0 / train_size\n",
        "    X['pred0_high'] = high_count * 1.0 / train_size\n",
        "    \n",
        "    X.loc[X['manager_id'].isin(managers_with_one_lot['manager_id'].ravel()), \n",
        "          'manager_id'] = \"-1\"\n",
        "    X.loc[X['building_id'].isin(buildings_with_one_lot['building_id'].ravel()), \n",
        "          'building_id'] = \"-1\"\n",
        "    X.loc[X['display_address'].isin(addresses_with_one_lot['display_address'].ravel()), \n",
        "          'display_address'] = \"-1\"\n",
        "          \n",
        "    return X\n",
        "\n",
        "def normalize_high_cordiality_data():\n",
        "    high_cardinality = [\"building_id\", \"manager_id\"]\n",
        "    for c in high_cardinality:\n",
        "        categorical_average(c, \"medium\", \"pred0_medium\", c + \"_mean_medium\")\n",
        "        categorical_average(c, \"high\", \"pred0_high\", c + \"_mean_high\")\n",
        "\n",
        "def transform_categorical_data():\n",
        "    categorical = ['building_id', 'manager_id', \n",
        "                   'display_address', 'street_address']\n",
        "                   \n",
        "    for f in categorical:\n",
        "        encoder = LabelEncoder()\n",
        "        encoder.fit(list(X_train[f]) + list(X_test[f])) \n",
        "        X_train[f] = encoder.transform(X_train[f].ravel())\n",
        "        X_test[f] = encoder.transform(X_test[f].ravel())\n",
        "                  \n",
        "\n",
        "def remove_columns(X):\n",
        "    columns = [\"photos\", \"pred0_high\", \"pred0_low\", \"pred0_medium\",\n",
        "               \"description\", \"low\", \"medium\", \"high\",\n",
        "               \"interest_level\", \"created\"]\n",
        "    for c in columns:\n",
        "        del X[c]\n",
        "\n",
        "def cluster_latlon(n_clusters, data):  \n",
        "    #split the data between \"around NYC\" and \"other locations\" basically our first two clusters \n",
        "    data_c=data[(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n",
        "    data_e=data[~(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n",
        "    #put it in matrix form\n",
        "    coords=data_c.as_matrix(columns=['latitude', \"longitude\"])\n",
        "    \n",
        "    brc = Birch(branching_factor=100, n_clusters=n_clusters, threshold=0.01,compute_labels=True)\n",
        "\n",
        "    brc.fit(coords)\n",
        "    clusters=brc.predict(coords)\n",
        "    data_c[\"cluster_\"+str(n_clusters)]=clusters\n",
        "    data_e[\"cluster_\"+str(n_clusters)]=-1 #assign cluster label -1 for the non NYC listings \n",
        "    data=pd.concat([data_c,data_e])\n",
        "    return brc, data\n",
        "\n",
        "def cluster_latlon2(brc, data):  \n",
        "    #split the data between \"around NYC\" and \"other locations\" basically our first two clusters \n",
        "    data_c=data[(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n",
        "    data_e=data[~(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n",
        "    #put it in matrix form\n",
        "    coords=data_c.as_matrix(columns=['latitude', \"longitude\"])\n",
        "    \n",
        "    clusters=brc.predict(coords)\n",
        "    data_c[\"cluster_\"+str(n_clusters)]=clusters\n",
        "    data_e[\"cluster_\"+str(n_clusters)]=-1 #assign cluster label -1 for the non NYC listings \n",
        "    data=pd.concat([data_c,data_e])\n",
        "    return data\n",
        "\n",
        "print(\"Starting transformations\")        \n",
        "X_train = transform_data(X_train)    \n",
        "X_test = transform_data(X_test) \n",
        "\n",
        "brc, X_train = cluster_latlon(5, X_train)\n",
        "X_test=cluster_latlon2(brc, X_test)\n",
        "\n",
        "y = X_train['interest_level'].ravel()\n",
        "\n",
        "print(\"Normalizing high cordiality data...\")\n",
        "normalize_high_cordiality_data()\n",
        "transform_categorical_data()\n",
        "\n",
        "remove_columns(X_train)\n",
        "remove_columns(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "11553527-4b0d-149f-84a9-d90c9b54efc3"
      },
      "outputs": [],
      "source": [
        "print(\"Start fitting...\")\n",
        "\n",
        "param = {}\n",
        "param['objective'] = 'multi:softprob'\n",
        "param['eta'] = 0.02\n",
        "#param['eta'] = 0.1\n",
        "param['max_depth'] = 6\n",
        "param['silent'] = 1\n",
        "param['num_class'] = 3\n",
        "param['eval_metric'] = \"mlogloss\"\n",
        "param['min_child_weight'] = 1\n",
        "param['subsample'] = 0.7\n",
        "param['colsample_bytree'] = 0.7\n",
        "param['seed'] = 0\n",
        "param['nthread'] = 8\n",
        "num_rounds = 3000\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_train, y, random_state=0)\n",
        "\n",
        "xgtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "xgtest = xgb.DMatrix(X_te, label=y_te)\n",
        "watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
        "clf = xgb.train(param, xgtrain, num_rounds, watchlist, early_stopping_rounds=25,\n",
        "\tverbose_eval=25)\n",
        "\n",
        "print(\"Fitted\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}