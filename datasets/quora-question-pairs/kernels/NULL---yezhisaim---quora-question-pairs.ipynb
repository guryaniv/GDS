{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d4282d8b-d5f0-7108-b564-5f92823f9085"
      },
      "source": [
        "Quora Questions Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f95f9bab-2061-23c2-76eb-3fc8be55fa7c"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "from collections import Counter\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2f934d44-9c48-fd70-f7ca-f74ccfbaf0cc"
      },
      "outputs": [],
      "source": [
        "#Load and Explore data \n",
        "\n",
        "train_set = pd.read_csv(\"../input/train.csv\")\n",
        "test_set = pd.read_csv(\"../input/test.csv\")\n",
        "print(train_set.shape)\n",
        "print(test_set.shape)\n",
        "\n",
        "print(train_set.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "835637d2-be8f-8567-2354-2621089b70ca"
      },
      "outputs": [],
      "source": [
        "#Preprocess Text\n",
        "\n",
        "def preprocess(question):\n",
        "\n",
        "    #Remove non-letters        \n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", question) \n",
        "    \n",
        "    #Convert to lower case\n",
        "    text = letters_only.lower()                          \n",
        "\n",
        "    # Replace punctuation with tokens so we can use them in our model\n",
        "    for c in string.punctuation:\n",
        "         text = text.replace(c,\"\")\n",
        "    \n",
        "    #Convert text to words\n",
        "    text_to_array = text.split()\n",
        "\n",
        "     #Convert the stop words to a set\n",
        "    stops = set(stopwords.words(\"english\"))                  \n",
        "    \n",
        "    #Remove stop words\n",
        "    meaningful_words = [w for w in text_to_array if not w in stops]    \n",
        "    \n",
        "    # Remove all words with  5 or fewer occurences\n",
        "    #word_counts = Counter(words)\n",
        "    #trimmed_words = [word for word in words if word_counts[word] > 5]\n",
        "    \n",
        "    return meaningful_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6777d60-b125-dbc9-cad0-afdfc9aacc17"
      },
      "outputs": [],
      "source": [
        "#Convert questions to words\n",
        "\n",
        "words = []\n",
        "\n",
        "for x in range(0,40):#train_set.shape[0]):\n",
        "    words += preprocess(train_set.iloc[x,3])\n",
        "    words += preprocess(train_set.iloc[x,4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "53170bf4-c7e5-b192-5994-011e2904f4ff"
      },
      "outputs": [],
      "source": [
        "#Create Lookup Tables\n",
        "\n",
        "def create_lookup_tables(words):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    :param words: Input list of words\n",
        "    :return: A tuple of dicts.  The first dict....\n",
        "    \"\"\"\n",
        "    word_counts = Counter(words)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
        "\n",
        "    return vocab_to_int, int_to_vocab\n",
        "\n",
        "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
        "int_words = [vocab_to_int[word] for word in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "24966bf7-d5fa-05dc-1ac1-79a0e2d82241"
      },
      "outputs": [],
      "source": [
        "#Subsampling\n",
        "\n",
        "threshold = 1e-5\n",
        "word_counts = Counter(int_words)\n",
        "total_count = len(int_words)\n",
        "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
        "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
        "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3349c654-d1ad-eaf4-97cb-30a79969664d"
      },
      "outputs": [],
      "source": [
        "# Receives a list of words, an index, and a window size, then returns a list of words in the window around the index\n",
        "\n",
        "def get_target(words, idx, window_size=5):\n",
        "    ''' Get a list of words in a window around an index. '''\n",
        "    \n",
        "    R = np.random.randint(1, window_size+1)\n",
        "    start = idx - R if (idx - R) > 0 else 0\n",
        "    stop = idx + R\n",
        "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
        "    \n",
        "    return list(target_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c2475f5e-af45-64b0-9beb-5d6162cbcb81"
      },
      "outputs": [],
      "source": [
        "#Get batches for the network\n",
        "\n",
        "def get_batches(words, batch_size, window_size=5):\n",
        "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
        "    \n",
        "    n_batches = len(words)//batch_size\n",
        "    \n",
        "    # only full batches\n",
        "    words = words[:n_batches*batch_size]\n",
        "    \n",
        "    for idx in range(0, len(words), batch_size):\n",
        "        x, y = [], []\n",
        "        batch = words[idx:idx+batch_size]\n",
        "        for ii in range(len(batch)):\n",
        "            batch_x = batch[ii]\n",
        "            batch_y = get_target(batch, ii, window_size)\n",
        "            y.extend(batch_y)\n",
        "            x.extend([batch_x]*len(batch_y))\n",
        "        yield x, y"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}