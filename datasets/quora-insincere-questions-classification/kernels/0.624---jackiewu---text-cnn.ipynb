{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#coding:utf-8\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\nfrom tensorflow.contrib.rnn import BasicLSTMCell\nfrom sklearn.utils import shuffle\nfrom keras.preprocessing.sequence import pad_sequences\nimport time\nfrom sklearn.metrics import f1_score\nimport sys\nimport numpy as np\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1b09cb93332de306d2bcb3d33d0fe7aa0a1d025"},"cell_type":"code","source":"class BaseModel(object):\n\tdef __init__(self):\n\t\tself.output_feeds = []\n\t\tprint('init Base model')\n\t\t# pass\n\tdef add_train_op(self, loss,lr_method='adam', lr=0.001,clip=-1):\n\t\t\"\"\"Defines self.train_op that performs an update on a batch\n\t\tArgs:\n\t\t\tlr_method: (string) sgd method, for example \"adam\"\n\t\t\tlr: (tf.placeholder) tf.float32, learning rate\n\t\t\tloss: (tensor) tf.float32 loss to minimize\n\t\t\tclip: (python float) clipping of gradient. If < 0, no clipping\n\t\t\"\"\"\n\t\t_lr_m = lr_method.lower()  # lower to make sure\n\n\t\twith tf.variable_scope(\"train_step\"):\n\t\t\tif _lr_m == 'adam':  # sgd method\n\t\t\t\tprint('adam')\n\t\t\t\toptimizer = tf.train.AdamOptimizer(lr)\n\t\t\telif _lr_m == 'adagrad':\n\t\t\t\toptimizer = tf.train.AdagradOptimizer(lr)\n\t\t\telif _lr_m == 'sgd':\n\t\t\t\toptimizer = tf.train.GradientDescentOptimizer(lr)\n\t\t\telif _lr_m == 'rmsprop':\n\t\t\t\toptimizer = tf.train.RMSPropOptimizer(lr)\n\t\t\telse:\n\t\t\t\traise NotImplementedError(\"Unknown method {}\".format(_lr_m))\n\n\t\t\tif clip > 0:  # gradient clipping if clip is positive\n\t\t\t\tgrads, vs = zip(*optimizer.compute_gradients(loss))\n\t\t\t\tgrads, gnorm = tf.clip_by_global_norm(grads, clip)\n\t\t\t\tself.train_op = optimizer.apply_gradients(zip(grads, vs))\n\t\t\telse:\n\t\t\t\tself.train_op = optimizer.minimize(loss)\n\t\tself.saver = tf.train.Saver(max_to_keep=3, pad_step_number=True)\n\t\tself.params = tf.trainable_variables()\n\t#check params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61848a6039536c3470fd1027a92279b39e0f9ba2"},"cell_type":"code","source":"class TextCNN(BaseModel):\n\tdef __init__(self,config):\n\t\tself.max_len = config[\"max_len\"]\n\t\tself.vocab_size = config[\"vocab_size\"]\n\t\tself.embedding_size = config[\"embedding_size\"]\n\t\tself.n_class = config[\"n_class\"]\n\t\tself.learning_rate = config[\"learning_rate\"]\n\n\t\t# placeholder\n\t\tself.x = tf.placeholder(tf.int32, [None, self.max_len])\n\t\tself.label = tf.placeholder(tf.int32, [None])\n\t\tself.use_pretrain_embedding = False\n\t\tself.filter_sizes = config['filter_sizes']\n\t\tself.num_filters = config['num_filters']\n\t\tself.keep_prob = tf.placeholder(tf.float32)\n\n\tdef build_graph(self):\n\t\tif not self.use_pretrain_embedding:\n\t\t\tembeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n\t\t\t\t\t\t\t\t\t\t trainable=True)\n\t\telse:\n\t\t\tpass\n\t\tbatch_embedded = tf.nn.embedding_lookup(embeddings_var, self.x)\n\t\tembeded_char_expanded = tf.expand_dims(batch_embedded,axis=-1)\n\n\t\tpool_list = list()\n\t\tfor filter_size,filter_num in zip(self.filter_sizes,self.num_filters):\n\t\t\twith tf.variable_scope('cov2d-maxpool%s'% filter_size):\n\t\t\t\tfilter_shape = [filter_size,300,1,filter_num]\n\t\t\t\tW = tf.Variable(tf.truncated_normal(shape=filter_shape,stddev=0.1),name='W')\n\t\t\t\tb = tf.Variable(tf.constant(0.1,shape=[filter_num]),name='b')\n\n\t\t\t\tconv = tf.nn.conv2d(embeded_char_expanded,W,strides=[1,1,1,1],padding='VALID',name='conv')\n\n\t\t\t\th = tf.nn.tanh(tf.nn.bias_add(conv,b),name='tanh')\n\t\t\t\tpooled = tf.nn.max_pool(\n\t\t\t\t\th,\n\t\t\t\t\tksize=[1,self.max_len-filter_size+1,1,1],\n\t\t\t\t\tstrides=[1,1,1,1],\n\t\t\t\t\tpadding='VALID',\n\t\t\t\t\tname='pool'\n\t\t\t\t)\n\t\t\t\tpool_list.append(pooled)\n\t\ttotal_filter_num = sum(self.num_filters)\n\t\tself.h_pool = tf.concat(pool_list,3)\n\t\tself.h_pool_flat = tf.reshape(self.h_pool,[-1,total_filter_num])\n\t\tself.h_drop = tf.nn.dropout(self.h_pool_flat, self.keep_prob)\n\n\t\tlogits = tf.layers.dense(self.h_pool_flat,self.n_class,activation=tf.nn.sigmoid)\n\n\t\tself.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.label))\n\n\t\tself.add_train_op(self.loss)\n\n\t\tself.prediction = tf.argmax(tf.nn.softmax(logits),axis=1)\n\t\tself.scores = tf.nn.softmax(logits)\n\t\tself.global_step = tf.Variable(0, name=\"global_step\", trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"844fb13858d6bbaf4909753d95d9c51943dffd3c"},"cell_type":"code","source":"names = [\"qid\", \"question_text\", \"target\"]\nimport pandas as pd\n\n\ndef load_data(file_name, sample_ratio=1, names=names):\n\t'''load data from .csv file'''\n\tcsv_file = pd.read_csv(file_name, names=names)\n\tshuffle_csv = csv_file.sample(frac=sample_ratio)\n\treturn shuffle_csv[\"question_text\"], shuffle_csv[\"target\"]\n\n\ndef data_preprocessing_v2(all_text,train, max_len, max_words=40000):\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n    tokenizer.fit_on_texts(all_text)\n    train_idx = tokenizer.texts_to_sequences(train)\n    train_padded = pad_sequences(train_idx, maxlen=max_len )\n    # vocab size = len(word_docs) + 2  (<UNK>, <PAD>)\n    return train_padded, max_words + 2\ndef split_dataset(x_test, y_test, dev_ratio):\n    \"\"\"split test dataset to test and dev set with ratio \"\"\"\n    test_size = len(x_test)\n    print(test_size)\n    dev_size = (int)(test_size * dev_ratio)\n    print(dev_size)\n    x_train, x_dev,y_train, y_dev = train_test_split(x_test, y_test, train_size=0.95,\n\t                                                  random_state=233)\n    return x_train, x_dev, y_train, y_dev, dev_size, test_size - dev_size\n\n\ndef fill_feed_dict(data_X, data_Y, batch_size):\n    \"\"\"Generator to yield batches\"\"\"\n    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n    for idx in range(data_X.shape[0] // batch_size):\n        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n        yield x_batch, y_batch\n\ndef make_train_feed_dict(model, batch):\n    \"\"\"make train feed dict for training\"\"\"\n    feed_dict = {model.x: batch[0],\n                 model.label: batch[1],\n                 model.keep_prob: 0.5}\n    return feed_dict\n\n\ndef make_test_feed_dict(model, batch):\n    feed_dict = {model.x: batch[0],\n                 model.label: batch[1],\n                 model.keep_prob: 1.0}\n    return feed_dict\n\n\ndef run_train_step(model, sess, batch):\n    feed_dict = make_train_feed_dict(model, batch)\n    to_return = {\n        'train_op': model.train_op,\n        'loss': model.loss,\n        'global_step': model.global_step,\n    }\n    return sess.run(to_return, feed_dict)\n\n\ndef run_eval_step(model, sess, batch):\n    feed_dict = make_test_feed_dict(model, batch)\n    scores,prediction = sess.run([model.scores,model.prediction], feed_dict)\n    return scores,prediction\n\n\ndef get_attn_weight(model, sess, batch):\n    feed_dict = make_train_feed_dict(model, batch)\n    return sess.run(model.alpha, feed_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e82dbde7122a5ad2448c03a6271a7b3caaea1a8"},"cell_type":"code","source":"import os \ndataset_dir = '../input/'\nprint(os.listdir(dataset_dir))\nEMBEDDING_FILE = dataset_dir+'glove.840B.300d.txt'\nnames = [\"qid\", \"question_text\", \"target\"]\ndata_dir =''\ntrain=pd.read_csv(dataset_dir+'train.csv')\ntest=pd.read_csv(dataset_dir+'test.csv')\ntrain_sam=train.sample(frac=1.0)\nx_test=test['question_text']\nx_train, y_train = train_sam['question_text'],train_sam['target']\nmax_features = 40000\nmax_len = 50\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(x_test.values)+list(x_train.values))\ntrain_idx = tokenizer.texts_to_sequences(x_train)\n\nX_train = pad_sequences(train_idx, maxlen=max_len)\n\ntest_idx = tokenizer.texts_to_sequences(x_test)\nX_test = pad_sequences(test_idx,maxlen=max_len)\n\nprint(X_train.shape)\nprint(X_test.shape)\n\n\t#x_train, vocab_size = data_preprocessing_v2(list(x_test.values)+list(x_train.values),x_train, max_len=50)\nx_train, x_dev, y_train, y_dev, dev_size, train_size = split_dataset(X_train, y_train, 0.05)\n\t#x_train, x_dev,y_train, y_dev = train_test_split(X_train, y_train, train_size=0.95,\n\t#\t                                                          random_state=233)\n\n\n# \tdef get_coefs(word, *arr):\n# \t\treturn word, np.asarray(arr, dtype='float32')\n\n\n# \tembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n# \tembed_size = 300\n# \tword_index = tokenizer.word_index\n# \tnb_words = min(max_features, len(word_index))\n# \tembedding_matrix = np.zeros((nb_words, embed_size))\n# \tfor word, i in word_index.items():\n# \t\tif i >= max_features: continue\n# \t\tembedding_vector = embeddings_index.get(word)\n# \t\tif embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\nprint(\"loading embedding done !\")\nvocab_size = max_features\n\nconfig = {\n    \"max_len\": 50,\n    \"hidden_size\": 64,\n    \"vocab_size\": vocab_size,\n    \"embedding_size\": 128,\n    \"n_class\": 2,\n    \"learning_rate\": 1e-3,\n    \"batch_size\": 32,\n    \"train_epoch\": 3\n}\ntf.reset_default_graph()\n#classifier = ABLSTM(config)\nconfig = {\n    \"max_len\": 50,\n    \"vocab_size\": vocab_size,\n    \"embedding_size\": 300,\n    \"learning_rate\": 1e-3,\n    \"l2_reg_lambda\": 1e-3,\n    \"batch_size\": 256,\n    \"n_class\": 2,\n\n    # random setting, may need fine-tune\n    \"filter_sizes\": [1, 2, 3,5],\n    \"num_filters\": [36, 36,36,36],\n    \"train_epoch\": 6,\n    \"hidden_size\": 256\n}\n# from text_cnn import TextCNN\n# from bilstm import BLSTM\n#classifier = BLSTM(config)\nclassifier = TextCNN(config)\nclassifier.build_graph()\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\ndev_batch = (x_dev, y_dev)\nstart = time.time()\npredictions = []\npred_thresh =0.0 \nfor e in range(config[\"train_epoch\"]):\n    t0 = time.time()\n    print(\"Epoch %d start !\" % (e + 1))\n    for x_batch, y_batch in tqdm(fill_feed_dict(x_train, y_train, config[\"batch_size\"])):\n        return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n        # attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n    t1 = time.time()\n    print(\"Train Epoch time:  %.3f s\" % (t1 - t0))\n    scores,dev_preds = run_eval_step(classifier, sess, dev_batch)\n    best_score = 0.0\n    best_thresh =0.0\n    for thresh in np.arange(0.1,0.501,0.01):\n        score = f1_score(y_dev.values,(scores[:,1]>thresh))\n        if score>best_score: \n            best_score = score\n            best_thresh = thresh\n            pred_thresh = best_thresh\n    f1_scores = f1_score(y_dev.values,(scores[:,1]>best_thresh).astype(np.int))#,dev_preds = run_eval_step(classifier, sess, dev_batch)\n    print(\"validation F1: %.3f \" % f1_scores)\nprint(\"prediting on test_data\")\nprint(pred_thresh)\ntarget = [0 for i in range(test.shape[0])]\ntest['target'] = target\ntest_batch=(X_test,test['target'].values)\nscores,dev_preds = run_eval_step(classifier,sess,test_batch)\npreditions = (scores[:,1]>pred_thresh).astype(np.int)\nsubmission_final = pd.DataFrame.from_dict({'qid':test['qid']})\nsubmission_final['prediction'] =preditions\nsubmission_final.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0b350ba4d15abf88b1a4a457b0c6e591d140fa0"},"cell_type":"code","source":"submission_final.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cc8d0b9a70eca84e9dcd0a798edc8ab7ec322b4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}