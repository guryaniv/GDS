{"cells":[{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3121372b6b1d3614f4b89c3ef8bf33db0ea83704"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c623f7ae4bbe98f6697a3fdfa85729d2d11a7692"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', index_col=0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59798c04824b48f383aa38f26862b5edd272e602"},"cell_type":"markdown","source":"It looks like we mainly have sparse numeric features here."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3fe402c551a7c757c7ee2b814a2625a87127691d"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23ac3febc5cb8d62a970ce2d2df5b86bc4a61e4c"},"cell_type":"markdown","source":"The data has floating point and integer features. We may be able to reduce the in-memory size here.\n\nWe are scored on root-mean-square log error, so we should transform the target by a logarithm so that we can make use of methods based on the RMS error."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"7e8ce566fa67cf728325ae88053c095be9e7ffcb"},"cell_type":"code","source":"train['log_target'] = np.log1p(train['target'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6094347c19475d1fc73a0fa22fdb369d3e9c783"},"cell_type":"markdown","source":"Now, let's see what the raw target looks like."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"85ad7bd8a306eda4dbfdeea705757da65a812166"},"cell_type":"code","source":"plt.hist(train.target,range=(0,4e7),bins=100)\nplt.xlabel('Target')\nplt.ylabel('Number of Samples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fc99733c8519e9a6b0ac2c0f7daf83e7111247a"},"cell_type":"markdown","source":"It covers several orders of magnitude and also has some quantization. Let's start zooming in."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c5db1cbd84bd6a20c14f46cc56ee07ff7d9d050d"},"cell_type":"code","source":"plt.hist(train.target,range=(0,1e7),bins=100)\nplt.xlabel('Target')\nplt.ylabel('Number of Samples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd6c938ca9ec7c143087b8723a6c900f260cf77"},"cell_type":"markdown","source":"The quantization looks somewhat regular but at several different scales."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"078a1fb4362c3303063b85b2c666fc1ee9d90f2a"},"cell_type":"code","source":"plt.hist(train.target,range=(0,2e6),bins=100)\nplt.xlabel('Target')\nplt.ylabel('Number of Samples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"516e4bc00343a436b98956d92ee2488d155c9779"},"cell_type":"markdown","source":"We should remove all features that have no variation."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b44f7d1382fda41c3c539687921015c70bfc9612"},"cell_type":"code","source":"std = train.std().sort_values()\nbad_fields = std[std==0].index\ntrain = train.drop(bad_fields, axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43d7a0f9bf092491a86c5a1abb57d161261883bc"},"cell_type":"markdown","source":"This removed a couple hundred features.\n\nWe can also check that none of the features have minimum values less than zero. We can exploit this to reduce the size of the integer data."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"031fe8795df2eb6a8b066ecfe9819a81cc87d50c"},"cell_type":"code","source":"changed_type = []\nfor col, dtype in train.dtypes.iteritems():\n    if dtype==np.int64:\n        max_val = np.max(train[col])\n        bits = np.log(max_val)/np.log(2)\n        if bits < 8:\n            new_dtype = np.uint8\n        elif bits < 16:\n            new_dtype = np.uint16\n        elif bits < 32:\n            new_dtype = np.uint32\n        else:\n            new_dtype = None\n        if new_dtype:\n            changed_type.append(col)\n            train[col] = train[col].astype(new_dtype)\nprint('Changed types on {} columns'.format(len(changed_type)))\nprint(train.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1226a42bf68ceb09885c79047dff5c97f94ef029"},"cell_type":"markdown","source":"This was a significant reduction in memory.\n\nWe also want to remove features that are too sparse. We can first calculate the sparsity of each feature."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"62caaff2de0398f96b1120903eac578c374cf6b1"},"cell_type":"code","source":"sparsity = {\n    col: (train[col] == 0).mean()\n    for idx, col in enumerate(train)\n}\nsparsity = pd.Series(sparsity)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c8c56bb0e45f175e34d4c9f32806e45a0e786eda"},"cell_type":"code","source":"fig = plt.figure(figsize=[7,12])\nax = fig.add_subplot(211)\nax.hist(sparsity, range=(0,1), bins=100)\nax.set_xlabel('Sparsity of Features')\nax.set_ylabel('Number of Features')\nax = fig.add_subplot(212)\nax.hist(sparsity, range=(0.8,1), bins=100)\nax.set_xlabel('Sparsity of Features')\nax.set_ylabel('Number of Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"310429dd7c90597d3f2063f527c7fb9587e2c68e"},"cell_type":"markdown","source":"Most of the features are very sparse. We'll set a minimum number of non-zero values of 10 and remove all the other features."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"932eb34330027370e208f6e11232c54c72083bfa"},"cell_type":"code","source":"min_non0 = 10\ntoo_sparse = sparsity[(((1-sparsity) * train.shape[0]) < min_non0)].index\ntrain = train.drop(too_sparse, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"92950d4b81870820c8bc6c1f8504029769b4c883"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5c3c68701c998a6861ca7a12dff7e9b4b43483f5"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8251942c8dd3d59596d91bbc73888f6be0a6608"},"cell_type":"markdown","source":"Now that we've prepared some data, we can train a model. Decision trees typically don't care about the scale of the features, so we can plug everything into a decision tree model to get a very simple result.\n\nFor other models such as linear regression, SVMs, neural nets, etc. we will want o figure out how to properly scale the various features.\n\nI haven't done much tuning at all, but I will run a 10-fold cross validation using the built-in LightGBM cv() function."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"7f51b580bfd58660e898779685af99190d0417cf"},"cell_type":"code","source":"import lightgbm as lgb\nfeatures = train.drop(['target','log_target'], axis=1).values\ntargets = train['log_target'].values.reshape([-1])\nfeature_names = list(train.drop(['target','log_target'], axis=1).columns.values)\ntrain_dataset = lgb.Dataset(\n        features,\n        targets,\n        feature_name=feature_names \n)\n\n    \nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'metric': {'rmse'},\n    'learning_rate': 0.01,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8    \n}\n\ncv_output = lgb.cv(\n    params,\n    train_dataset,\n    num_boost_round=500,\n    nfold=10,\n    stratified=False\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a53b2008248cedaaedac33997b3d76a4050144a5"},"cell_type":"markdown","source":"The cross validation will also show us the optimal number of iterations to use."},{"metadata":{"scrolled":false,"trusted":false,"collapsed":true,"_uuid":"834003abc2eadb12bee4bf583bf7109289193ef5"},"cell_type":"code","source":"n_iterations = np.argmin(cv_output['rmse-mean'])\nprint('Optimal # of iterations: {}'.format(n_iterations))\nprint('Score: {:0.5}, Std. Dev.: {:0.5}'.format(\n    cv_output['rmse-mean'][n_iterations],\n    cv_output['rmse-stdv'][n_iterations]\n))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0bef3743ae2466a22ac03e8fedaec13e3b1da2e"},"cell_type":"markdown","source":"Now we can train the model. This is very fast on this dataset, so I won't bother saving the model."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ec64324c2ce1569022389e100ffbf99bffab7f5c"},"cell_type":"code","source":"model = lgb.train(\n    params,\n    train_dataset,\n    num_boost_round=n_iterations\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9066dba7430e898d42d9c1d5ec3302829bb73b00"},"cell_type":"markdown","source":"Now I can prepare the test data."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f09782f409dcffe095484657b1d03bf475ca1e20"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv', index_col=0)\ntest = test.drop(bad_fields, axis=1)\ntest = test.drop(too_sparse, axis=1)\ntest.info()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d101071fa705d367f26cb53e336e45962e90f492"},"cell_type":"markdown","source":"It's interesting that now all the features are floating-point on the test data. This means that there is some difference between how the training and test data was prepared. I won't investigate this here."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"84c7c751ba5e8656d8b7715c438bba49acceb8ce"},"cell_type":"code","source":"preds = model.predict(test.values)\npreds = np.exp(preds) - 1\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4def01ca4a03e50f578f68978eb7e8d64f318c43"},"cell_type":"code","source":"test['target'] = preds\ntest[['target']].to_csv('lightgbm_basic.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"086fc6316fa7e65487763d24d4dcacbbbdbac7df"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}