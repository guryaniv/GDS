{"cells":[{"metadata":{"_uuid":"c11ab2976df118ebc3f685096f89dc79362367d7"},"cell_type":"markdown","source":"# Interpreting toxic comment classfication\nAuthor: Kalaivani Sundararajan\n\nEmail: kalaivani.s@ufl.edu"},{"metadata":{"_uuid":"656e150f1c3fd57d72dd48cd3e63ab037e32436c"},"cell_type":"markdown","source":"## Introduction\nThe [Conversation AI](https://conversationai.github.io/) team hosted the [Toxic comment classification challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) to make online conversations better. One aspect of it involves identifying toxic comments that are rude, disrespectful or obscene. This challenge involves identifying different categories of toxic comments, i.e. toxic, severely toxic, obscene, threat, insult or identity hate. Each comment can be attributed to more than one of these categories. In this notebook, we will try to interpret the aspects that machine learning models use to determine different toxic categories. Hence, we will use classic but interpretable techniques like linear SVM / random forests with TFIDF vector representations. The performance obtained is slightly below the [leading approaches submitted on Kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/leaderboard) but the motivation here is to understand what drives these machine learning models to classify certain comments as toxic."},{"metadata":{"_uuid":"d9994be80f3fb8edef920bea4becce153c6a6f64"},"cell_type":"markdown","source":"## Loading and preprocessing data\nTo start with we load the training and test data. Both the training and test data are preprocessed such that words that are not stop words are lemmatized to account for morphological variations of the same word. For the test data, we only choose samples that were used for evaluation in the Kaggle challenge, i.e. the ones whose labels were not -1. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nimport spacy\n\n# Get list of stopwords\nstop_words = []\nwith open('../input/en-stopwords/stop_words.txt','r') as fp:\n    for line in fp:\n        stop_words.append(line.strip())\n        \n# Preprocess text\nnlp = spacy.load('en')\nlemmatizer = WordNetLemmatizer()\ndef preprocess(df):\n    for index, row in df.iterrows():\n        text = row['comment_text']\n        text = text.encode('ascii',errors='ignore').decode('utf8')\n        doc = nlp(text)\n        text = [wd.lemma_ if wd.text.lower() not in stop_words else wd for wd in doc]\n        text = ' '.join(str(wd) for wd in text)\n        row['comment_text'] = text\n    return df\n    \n# Get training data\ntrain_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntrain_data = train_df[['comment_text']]\ntrain_data = preprocess(train_data)\ntrain_lbl = train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']]\n\n# Get test data\ntest_data = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\ntest_data = preprocess(test_data)\ntest_lbl = pd.read_csv('../input/test-labels/test_labels.csv')\ntest_id = test_lbl[test_lbl.sum(axis=1) >= 0]['id']\ntest_data = test_data[test_data['id'].isin(test_id)]\ntest_lbl = test_lbl[test_lbl['id'].isin(test_id)]\ntest_lbl = test_lbl[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a93d274848aa6c7c26cc3ae8cfb248e99440a71"},"cell_type":"markdown","source":"## Visualizing distribution of toxic categories\nWe get some statistics on the different categories of toxic comments for exploratory purposes. It should also be noticed that this problem consists of multiple labels for each sample, i.e. the same comment can be tagged with different toxic categories simultaneously. Here, we visualize only up to two labels in the confusion matrix."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Plot distribution of data\nnot_toxic = train_lbl[train_lbl.sum(axis=1) == 0].shape[0]\ntoxic = train_lbl.shape[0]-not_toxic\nnot_toxic_perc = not_toxic*100.0/train_lbl.shape[0]\ntoxic_perc = 100.0 - not_toxic_perc\nprint('%d samples (%0.2f%%) are not toxic. %d samples (%0.2f%%) are toxic.' % (not_toxic,not_toxic_perc,toxic,toxic_perc))\n\n# Label distribution of toxic labels\nimport itertools\nimport matplotlib.pyplot as plt\nconf_mat = np.matmul(train_lbl.values.T,train_lbl.values)\n#conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\nclasses = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']\nplt.imshow(conf_mat, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Distribution of toxic labels')\nplt.colorbar()\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=90)\nplt.yticks(tick_marks, classes)\nfor i, j in itertools.product(range(conf_mat.shape[0]), range(conf_mat.shape[1])):\n    plt.text(j, i, format(conf_mat[i, j], 'd'), horizontalalignment=\"center\", color='black')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d5f9eb9102caa62012617c1eee83653d1770554"},"cell_type":"markdown","source":"As we can see, the proportion of non-toxic comments and toxic comments are heavily imbalanced. Even amongst different toxic comment categories, the proportion of 'toxic' category is much higher than the other five categories. Hence, we balance the number of toxic and non-toxic comments in the training data."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f04c68de45fa405e9f6904af91bec395d95c32e2"},"cell_type":"code","source":"# Balance non-toxic and toxic comments in training data\nimport random\nnsamp = min(toxic,not_toxic)\nnot_toxic_data = train_data[train_lbl.sum(axis=1) == 0]\nnot_toxic_data = not_toxic_data[:nsamp]\nnot_toxic_lbl = train_lbl[train_lbl.sum(axis=1) == 0]\nnot_toxic_lbl = not_toxic_lbl[:nsamp]\ntoxic_data = train_data[train_lbl.sum(axis=1) > 0]\ntoxic_data = toxic_data[:nsamp]\ntoxic_lbl = train_lbl[train_lbl.sum(axis=1) > 0]\ntoxic_lbl = toxic_lbl[:nsamp]\nbtrain_data = pd.concat([not_toxic_data,toxic_data])\nbtrain_lbl = pd.concat([not_toxic_lbl,toxic_lbl])\nbtrain = pd.concat([btrain_data,btrain_lbl],axis=1)\nbtrain = btrain.sample(frac=1).reset_index(drop=True)\ntrain_data = btrain[['comment_text']]\ntrain_lbl = btrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75edd54a31bc3e2dc5b1eb3de728bd758550673c"},"cell_type":"markdown","source":"## Visualizing word clouds for toxic categories"},{"metadata":{"_uuid":"f808c92688943edbb5ed4c5d954e67f91abac6b7"},"cell_type":"markdown","source":"We will visualize the frequent words used in each of these toxic comment categories using word clouds. The size of the words is determined by their frequencies."},{"metadata":{"trusted":true,"_uuid":"b6002f3bcc072ec0c0ee589594f4d7caa88efdf6"},"cell_type":"code","source":"# Plot word clouds for each toxic category\nfrom wordcloud import WordCloud\nwordclouds = []; titles = []\nfor categ in classes:\n    comments = train_data[train_lbl[categ] == 1]['comment_text']\n    text = ' '.join(samp for samp in comments)\n    if len(text) == 0:\n        continue\n    wdcloud = WordCloud(max_font_size=80,background_color=\"white\",collocations=False,width=400, height=300).generate(text)\n    wordclouds.append(wdcloud)\n    titles.append(categ)\n\nfig,axes = plt.subplots(2,3,figsize=(18, 9))\nfor ax,wdcloud,categ in zip(axes.flatten(),wordclouds,titles):\n    ax.imshow(wdcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.set_title(categ)\nfig.suptitle('Word clouds for different toxic categories')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f909cd4a8f024b62b1c04b4f4ed9fa1ce7b3abf7"},"cell_type":"markdown","source":"As it can be observed from the wordclouds, some words are repeatedly used in multiple toxic categories. "},{"metadata":{"_uuid":"887afb33bf3d7212bfb6a3b1ca92db4fba2a499a"},"cell_type":"markdown","source":"##  Create TFIDF vector representation\nWe need to create a representation of text samples for classification. We will choose the simpler but interpretable TFIDF vector representation of samples."},{"metadata":{"trusted":true,"_uuid":"2ec51d99596a0a42653742016003b8c9071beaf4"},"cell_type":"code","source":"# Create tfidf representation\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntrain_comments = list(train_data['comment_text'])\ntest_comments = list(test_data['comment_text'])\ntfidf = TfidfVectorizer()\ntrain_vec = tfidf.fit_transform(train_comments)\ntest_vec = tfidf.transform(test_comments)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f96d21fa6cf22a285a268e82e8ab2dbbfdf37a91"},"cell_type":"markdown","source":"## Classification\nAs mentioned earlier, this is a multilabel classification problem. However, to keep the model interpretable for each category, we treat assume that the labels are independent of each other for the same sample. Hence, we perform a one-vs-rest binary classification for each toxic category using Linear SVM. Again, we choose a linear model for interpretability. Record performance metrics on the test data and also the feature weights for different words used in the six toxic categories."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"21b93b66f4557e4971505c1f7f77b1fb8fcc9adf"},"cell_type":"code","source":"# Multilabel classification - treats labels as independent\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import accuracy_score, hamming_loss\nfrom skmultilearn.problem_transform import BinaryRelevance, LabelPowerset\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = CalibratedClassifierCV(LinearSVC(),cv=5)\ntest_prob = None\nclf_feat_wt = None\nfor categ in classes:\n    clf.fit(train_vec,train_lbl[categ])\n    y_prob = clf.predict_proba(test_vec)\n    y_pred = y_prob.argmax(axis=1)\n    y_true = test_lbl[categ]\n    score = accuracy_score(y_true,y_pred)*100.0\n    print('Accuracy for %s = %0.2f' % (categ,score))\n    if test_prob is None:\n        test_prob = y_prob[:,1].reshape(-1,1)\n    else:\n        test_prob = np.hstack((test_prob,y_prob[:,1].reshape(-1,1)))\n    # Compute feature weights\n    feat_wt = None\n    for i in range(len(clf.calibrated_classifiers_)):\n        if feat_wt is None:\n            feat_wt = clf.calibrated_classifiers_[i].base_estimator.coef_\n        else:\n            feat_wt += clf.calibrated_classifiers_[i].base_estimator.coef_\n    feat_wt = feat_wt/float(len(clf.calibrated_classifiers_))\n    if clf_feat_wt is None:\n        clf_feat_wt = feat_wt\n    else:\n        clf_feat_wt = np.vstack((clf_feat_wt,feat_wt))\ntest_prob_bck = test_prob.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"035a2df08dc9d5ae8d5d2cfb6955e8dd7913ea01"},"cell_type":"markdown","source":"## Compute multilabel classification accuracy\nThough we treat the labels independently, we compute multilabel classification accuracy using HammingLoss. The accuracy for toxic and non-toxic comments are also computed."},{"metadata":{"trusted":true,"_uuid":"599d9b526f099bcac8604ee54f65624edb718cb5","scrolled":false},"cell_type":"code","source":"# Compute multilabel classification loss\nth = 0.5\ntest_prob = test_prob_bck.copy()\ntest_prob[test_prob < th] = 0\ntest_prob[test_prob >= th] = 1\naccuracy = (1.0 - hamming_loss(test_prob,test_lbl.values)) * 100.0\nprint('Multilabel accuracy for all comments = %0.2f' % accuracy)\ntoxic_lbl = test_lbl[test_lbl.sum(axis=1)>0]\ntoxic_prob = test_prob[test_lbl.sum(axis=1)>0]\naccuracy = (1.0 - hamming_loss(toxic_prob,toxic_lbl.values)) * 100.0\n#print(toxic_lbl.sum(axis=1),toxic_prob.sum(axis=1))\nprint('Multilabel accuracy for toxic comments = %0.2f' % accuracy)\nnontoxic_lbl = test_lbl[test_lbl.sum(axis=1)==0]\nnontoxic_prob = test_prob[test_lbl.sum(axis=1)==0]\naccuracy = (1.0 - hamming_loss(nontoxic_prob,nontoxic_lbl.values)) * 100.0\nprint('Multilabel accuracy for non-toxic comments = %0.2f' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2638e3ea43f5b6598d93cabfb337d22d9606446a"},"cell_type":"markdown","source":"## Highlight important features for each category\nUsing the feature importances computed by each classifier, visualize a word cloud for each toxic category where the size of the word is based on its importance."},{"metadata":{"trusted":true,"_uuid":"729e2bf31adc0623b1b7be650d796423ca45cebf","scrolled":false},"cell_type":"code","source":"# Postprocess feature weights to retain only important features\nclf_feat_wt = abs(clf_feat_wt)\nfor row in range(clf_feat_wt.shape[0]):\n    x = clf_feat_wt[row]\n    mean = x.mean()\n    std = x.std()\n    x[(x>(mean-3*std)) & (x<(mean+3*std))] = 0.0\n\n# Generate word clouds based on feature importance\nwordclouds = []; titles = []\nfor idx,categ in enumerate(classes):\n    wc_dict = {}\n    for key in tfidf.vocabulary_:\n        wc_dict[key] = clf_feat_wt[idx,tfidf.vocabulary_[key]]\n    wdcloud = WordCloud(max_font_size=80,background_color=\"white\",collocations=False,width=400, height=300).generate_from_frequencies(wc_dict)\n    wordclouds.append(wdcloud)\n    titles.append(categ)\n\nfig,axes = plt.subplots(2,3,figsize=(18, 9))\nfor ax,wdcloud,categ in zip(axes.flatten(),wordclouds,titles):\n    ax.imshow(wdcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.set_title(categ)\nfig.suptitle('Salient words for different toxic categories')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c303a86bb6234a72a204bc3bd3cd1901273bfbb9"},"cell_type":"markdown","source":"## Analyze cases where toxic comments were classified and missed\nVisualize samples where the classifications were correct and where the classification failed.\n"},{"metadata":{"trusted":true,"_uuid":"0f464d4339caa4f876fd921e2d59e2ca93d40393"},"cell_type":"code","source":"# Analyze only toxic comments\ntoxic_data = test_data[test_lbl.sum(axis=1)==6].reset_index(drop=True)\ntoxic_lbl = test_lbl[test_lbl.sum(axis=1)==6].reset_index(drop=True)\ntoxic_prob = test_prob[test_lbl.sum(axis=1)==6]\naccuracy = []\nfor i in range(toxic_lbl.shape[0]):\n    accuracy.append((1.0 - hamming_loss(toxic_prob[i],toxic_lbl.values[i])) * 100.0)\naccuracy = np.array(accuracy)\naccuracy_df = pd.DataFrame(accuracy,columns=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41050c8a81594fad69e9e2936079ecb8a75d60cb"},"cell_type":"code","source":"# Highlight text based on toxic categories\nfrom termcolor import colored\ndef highlight_text(df):\n    color = ['blue','red','green','magenta','cyan','yellow','white']\n    text = colored('toxic', 'blue', attrs=['reverse','bold'])\n    text = text + ' ' + colored('severe_toxic', 'red', attrs=['reverse','bold'])\n    text = text + ' ' + colored('obscene', 'green', attrs=['reverse','bold'])\n    text = text + ' ' + colored('threat', 'magenta', attrs=['reverse','bold'])\n    text = text + ' ' + colored('insult', 'cyan', attrs=['reverse','bold'])\n    text = text + ' ' + colored('identity_hate', 'yellow', attrs=['reverse','bold']) \n    text = text + ' ' + colored('multiple', 'white', attrs=['reverse','bold']) + '\\n'\n    print(text)\n\n    for index,row in df.iterrows():\n        print('--- Sample ' + str(index) + ' ---')\n        text = nltk.word_tokenize(row['comment_text'])\n        text = [lemmatizer.lemmatize(wd) if wd.lower() not in stop_words else wd for wd in text]\n        tags = []\n        for wd in text:\n            if wd.lower() in tfidf.vocabulary_ and wd.lower() not in stop_words:\n                idx = tfidf.vocabulary_[wd.lower()]\n                tags.append((wd,clf_feat_wt[0,idx],clf_feat_wt[1,idx],clf_feat_wt[2,idx],clf_feat_wt[3,idx],clf_feat_wt[4,idx],clf_feat_wt[5,idx]))\n            else:\n                tags.append((wd,0,0,0,0,0,0))\n        mod_text = ''\n        for tup in tags:\n            if sum(tup[1:]) > 0:\n                scores = np.array(tup[1:])\n                th = scores.mean() + scores.std()\n                scores[scores < th] = 0\n                if (scores == 0).sum() == 5: # multiple tags\n                    mod_text = mod_text + ' ' + colored(tup[0],'white',attrs=['reverse','bold'])\n                else:\n                    idx = np.argmax(scores)\n                    mod_text = mod_text + ' ' + colored(tup[0],color[idx],attrs=['reverse','bold'])\n            else:\n                mod_text = mod_text + ' ' + tup[0]\n        print(mod_text+'\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c3578d6cea4c911a9c9817102fbf084219e4b0"},"cell_type":"code","source":"# Success cases     \nprint('**** CASES WHERE TOXIC COMMENTS WERE CORRECTLY IDENTIFIED ****')\nsuccess_data = toxic_data[accuracy_df['accuracy']>75.0].reset_index(drop=True)\nhighlight_text(success_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"978095b1dd0dee8b964b39e45983c6249a2ef39f"},"cell_type":"markdown","source":"From the above samples, we know that they contain words that were considered important by the classifiers corresponding to each toxic category. \n"},{"metadata":{"trusted":true,"_uuid":"a2608200e860d3e793aee9fa5c45e1906cfcadf0"},"cell_type":"code","source":"# Failure cases\nprint('**** CASES WHERE TOXIC COMMENTS WERE NOT CORRECTLY IDENTIFIED ****')\nfailed_data = toxic_data[accuracy_df['accuracy']<50.0].reset_index(drop=True)\nhighlight_text(failed_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcd9f09a4629738a00960ad140cb6b0e917a42f2"},"cell_type":"markdown","source":"For scenarios where the classification failed, we know that the samples did not contain words that were considered important by the classifier for each category."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}