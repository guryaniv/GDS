{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b8e84890-da08-0a8d-7b62-e824d3f40682"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "X_train = pd.read_json(\"../input/train.json\")\n",
        "X_test = pd.read_json(\"../input/test.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8df4d4fe-429e-ad82-c22c-9fa5e4c6967b"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ed3fd538-7425-0fec-6a12-c840c50ab6c8"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "83231c83-a367-d26c-e2df-f0d092502022"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train[\"longitude\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0727119-a154-0414-5523-6a792485af38"
      },
      "outputs": [],
      "source": [
        "X_train[\"latitude\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f5950e7c-ffb3-9afe-4013-e458d2de1c67"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf5ef3b4-319e-3986-0cd0-1f43f3b30793"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8df9aab9-7cbb-9e1b-05f5-78f90f09cac4"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f8fed030-d022-5817-d4c5-f454f3c07ff9"
      },
      "outputs": [],
      "source": [
        "sample=pd.read_csv(\"../input/sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5096028d-42f1-dde3-4c65-d8846bd872d6"
      },
      "outputs": [],
      "source": [
        "sample.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1869970-924f-a22e-5799-cdb3dd08f850"
      },
      "outputs": [],
      "source": [
        "print(check_output([\"ls\", \"../input/images_sample/\"]).decode(\"utf8\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c25135a-5a11-6109-99c5-b3adb16dae3b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import random\n",
        "from math import exp\n",
        "import xgboost as xgb\n",
        "\n",
        "random.seed(321)\n",
        "np.random.seed(321)\n",
        "\n",
        "X_train = pd.read_json(\"../input/train.json\")\n",
        "X_test = pd.read_json(\"../input/test.json\")\n",
        "\n",
        "interest_level_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "X_train['interest_level'] = X_train['interest_level'].apply(lambda x: interest_level_map[x])\n",
        "X_test['interest_level'] = -1\n",
        "\n",
        "#add features\n",
        "feature_transform = CountVectorizer(stop_words='english', max_features=150)\n",
        "X_train['features'] = X_train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n",
        "X_test['features'] = X_test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n",
        "feature_transform.fit(list(X_train['features']) + list(X_test['features']))\n",
        "\n",
        "train_size = len(X_train)\n",
        "low_count = len(X_train[X_train['interest_level'] == 0])\n",
        "medium_count = len(X_train[X_train['interest_level'] == 1])\n",
        "high_count = len(X_train[X_train['interest_level'] == 2])\n",
        "\n",
        "def find_objects_with_only_one_record(feature_name):\n",
        "    temp = pd.concat([X_train[feature_name].reset_index(), \n",
        "                      X_test[feature_name].reset_index()])\n",
        "    temp = temp.groupby(feature_name, as_index = False).count()\n",
        "    return temp[temp['index'] == 1]\n",
        "\n",
        "managers_with_one_lot = find_objects_with_only_one_record('manager_id')\n",
        "buildings_with_one_lot = find_objects_with_only_one_record('building_id')\n",
        "addresses_with_one_lot = find_objects_with_only_one_record('display_address')\n",
        "\n",
        "lambda_val = None\n",
        "k=5.0\n",
        "f=1.0\n",
        "r_k=0.01 \n",
        "g = 1.0\n",
        "\n",
        "def categorical_average(variable, y, pred_0, feature_name):\n",
        "    def calculate_average(sub1, sub2):\n",
        "        s = pd.DataFrame(data = {\n",
        "                                 variable: sub1.groupby(variable, as_index = False).count()[variable],                              \n",
        "                                 'sumy': sub1.groupby(variable, as_index = False).sum()['y'],\n",
        "                                 'avgY': sub1.groupby(variable, as_index = False).mean()['y'],\n",
        "                                 'cnt': sub1.groupby(variable, as_index = False).count()['y']\n",
        "                                 })\n",
        "                                 \n",
        "        tmp = sub2.merge(s.reset_index(), how='left', left_on=variable, right_on=variable) \n",
        "        del tmp['index']                       \n",
        "        tmp.loc[pd.isnull(tmp['cnt']), 'cnt'] = 0.0\n",
        "        tmp.loc[pd.isnull(tmp['cnt']), 'sumy'] = 0.0\n",
        "\n",
        "        def compute_beta(row):\n",
        "            cnt = row['cnt'] if row['cnt'] < 200 else float('inf')\n",
        "            return 1.0 / (g + exp((cnt - k) / f))\n",
        "            \n",
        "        if lambda_val is not None:\n",
        "            tmp['beta'] = lambda_val\n",
        "        else:\n",
        "            tmp['beta'] = tmp.apply(compute_beta, axis = 1)\n",
        "            \n",
        "        tmp['adj_avg'] = tmp.apply(lambda row: (1.0 - row['beta']) * row['avgY'] + row['beta'] * row['pred_0'],\n",
        "                                   axis = 1)\n",
        "                                   \n",
        "        tmp.loc[pd.isnull(tmp['avgY']), 'avgY'] = tmp.loc[pd.isnull(tmp['avgY']), 'pred_0']\n",
        "        tmp.loc[pd.isnull(tmp['adj_avg']), 'adj_avg'] = tmp.loc[pd.isnull(tmp['adj_avg']), 'pred_0']\n",
        "        tmp['random'] = np.random.uniform(size = len(tmp))\n",
        "        tmp['adj_avg'] = tmp.apply(lambda row: row['adj_avg'] *(1 + (row['random'] - 0.5) * r_k),\n",
        "                                   axis = 1)\n",
        "    \n",
        "        return tmp['adj_avg'].ravel()\n",
        "     \n",
        "    #cv for training set \n",
        "    k_fold = StratifiedKFold(5)\n",
        "    X_train[feature_name] = -999 \n",
        "    for (train_index, cv_index) in k_fold.split(np.zeros(len(X_train)),\n",
        "                                                X_train['interest_level'].ravel()):\n",
        "        sub = pd.DataFrame(data = {variable: X_train[variable],\n",
        "                                   'y': X_train[y],\n",
        "                                   'pred_0': X_train[pred_0]})\n",
        "            \n",
        "        sub1 = sub.iloc[train_index]        \n",
        "        sub2 = sub.iloc[cv_index]\n",
        "        \n",
        "        X_train.loc[cv_index, feature_name] = calculate_average(sub1, sub2)\n",
        "    \n",
        "    #for test set\n",
        "    sub1 = pd.DataFrame(data = {variable: X_train[variable],\n",
        "                                'y': X_train[y],\n",
        "                                'pred_0': X_train[pred_0]})\n",
        "    sub2 = pd.DataFrame(data = {variable: X_test[variable],\n",
        "                                'y': X_test[y],\n",
        "                                'pred_0': X_test[pred_0]})\n",
        "    X_test.loc[:, feature_name] = calculate_average(sub1, sub2)                               \n",
        "\n",
        "def transform_data(X):\n",
        "    #add features    \n",
        "    feat_sparse = feature_transform.transform(X[\"features\"])\n",
        "    vocabulary = feature_transform.vocabulary_\n",
        "    del X['features']\n",
        "    X1 = pd.DataFrame([ pd.Series(feat_sparse[i].toarray().ravel()) for i in np.arange(feat_sparse.shape[0]) ])\n",
        "    X1.columns = list(sorted(vocabulary.keys()))\n",
        "    X = pd.concat([X.reset_index(), X1.reset_index()], axis = 1)\n",
        "    del X['index']\n",
        "    \n",
        "    X[\"num_photos\"] = X[\"photos\"].apply(len)\n",
        "    X['created'] = pd.to_datetime(X[\"created\"])\n",
        "    X[\"num_description_words\"] = X[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "    X['price_per_bed'] = X['price'] / X['bedrooms']    \n",
        "    X['price_per_bath'] = X['price'] / X['bathrooms']\n",
        "    X['price_per_room'] = X['price'] / (X['bathrooms'] + X['bedrooms'] )\n",
        "    \n",
        "    X['low'] = 0\n",
        "    X.loc[X['interest_level'] == 0, 'low'] = 1\n",
        "    X['medium'] = 0\n",
        "    X.loc[X['interest_level'] == 1, 'medium'] = 1\n",
        "    X['high'] = 0\n",
        "    X.loc[X['interest_level'] == 2, 'high'] = 1\n",
        "    \n",
        "    X['display_address'] = X['display_address'].apply(lambda x: x.lower().strip())\n",
        "    X['street_address'] = X['street_address'].apply(lambda x: x.lower().strip())\n",
        "    \n",
        "    X['pred0_low'] = low_count * 1.0 / train_size\n",
        "    X['pred0_medium'] = medium_count * 1.0 / train_size\n",
        "    X['pred0_high'] = high_count * 1.0 / train_size\n",
        "    \n",
        "    X.loc[X['manager_id'].isin(managers_with_one_lot['manager_id'].ravel()), \n",
        "          'manager_id'] = \"-1\"\n",
        "    X.loc[X['building_id'].isin(buildings_with_one_lot['building_id'].ravel()), \n",
        "          'building_id'] = \"-1\"\n",
        "    X.loc[X['display_address'].isin(addresses_with_one_lot['display_address'].ravel()), \n",
        "          'display_address'] = \"-1\"\n",
        "          \n",
        "    return X\n",
        "\n",
        "def normalize_high_cordiality_data():\n",
        "    high_cardinality = [\"building_id\", \"manager_id\"]\n",
        "    for c in high_cardinality:\n",
        "        categorical_average(c, \"medium\", \"pred0_medium\", c + \"_mean_medium\")\n",
        "        categorical_average(c, \"high\", \"pred0_high\", c + \"_mean_high\")\n",
        "\n",
        "def transform_categorical_data():\n",
        "    categorical = ['building_id', 'manager_id', \n",
        "                   'display_address', 'street_address']\n",
        "                   \n",
        "    for f in categorical:\n",
        "        encoder = LabelEncoder()\n",
        "        encoder.fit(list(X_train[f]) + list(X_test[f])) \n",
        "        X_train[f] = encoder.transform(X_train[f].ravel())\n",
        "        X_test[f] = encoder.transform(X_test[f].ravel())\n",
        "                  \n",
        "\n",
        "def remove_columns(X):\n",
        "    columns = [\"photos\", \"pred0_high\", \"pred0_low\", \"pred0_medium\",\n",
        "               \"description\", \"low\", \"medium\", \"high\",\n",
        "               \"interest_level\", \"created\"]\n",
        "    for c in columns:\n",
        "        del X[c]\n",
        "\n",
        "print(\"Starting transformations\")        \n",
        "X_train = transform_data(X_train)    \n",
        "X_test = transform_data(X_test) \n",
        "y = X_train['interest_level'].ravel()\n",
        "\n",
        "print(\"Normalizing high cordiality data...\")\n",
        "normalize_high_cordiality_data()\n",
        "transform_categorical_data()\n",
        "\n",
        "remove_columns(X_train)\n",
        "remove_columns(X_test)\n",
        "\n",
        "print(\"Start fitting...\")\n",
        "\n",
        "param = {}\n",
        "param['objective'] = 'multi:softprob'\n",
        "param['eta'] = 0.02\n",
        "param['max_depth'] = 4\n",
        "param['silent'] = 1\n",
        "param['num_class'] = 3\n",
        "param['eval_metric'] = \"mlogloss\"\n",
        "param['min_child_weight'] = 1\n",
        "param['subsample'] = 0.7\n",
        "param['colsample_bytree'] = 0.7\n",
        "param['seed'] = 321\n",
        "param['nthread'] = 8\n",
        "num_rounds = 2000\n",
        "\n",
        "xgtrain = xgb.DMatrix(X_train, label=y)\n",
        "clf = xgb.train(param, xgtrain, num_rounds)\n",
        "\n",
        "print(\"Fitted\")\n",
        "\n",
        "def prepare_submission(model):\n",
        "    xgtest = xgb.DMatrix(X_test)\n",
        "    preds = model.predict(xgtest)    \n",
        "    sub = pd.DataFrame(data = {'listing_id': X_test['listing_id'].ravel()})\n",
        "    sub['low'] = preds[:, 0]\n",
        "    sub['medium'] = preds[:, 1]\n",
        "    sub['high'] = preds[:, 2]\n",
        "    sub.to_csv(\"submission.csv\", index = False, header = True)\n",
        "\n",
        "prepare_submission(clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a72b86f3-1ecb-29d8-78e1-492c44bf55d2"
      },
      "outputs": [],
      "source": [
        "def prepare_submission(model):\n",
        "    xgtest = xgb.DMatrix(X_test)\n",
        "    preds = model.predict(xgtest)    \n",
        "    sub = pd.DataFrame(data = {'listing_id': X_test['listing_id'].ravel()})\n",
        "    sub['low'] = preds[:, 0]\n",
        "    sub['medium'] = preds[:, 1]\n",
        "    sub['high'] = preds[:, 2]\n",
        "    sub.to_csv(\"submission.csv\", index = False, header = True)\n",
        "\n",
        "prepare_submission(clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd21cd1a-3274-e91e-04d6-0371e7afb427"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess as sub\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f for f in listdir('../input/images_sample/6811957/') if isfile(join('../input/images_sample/6811957/', f))]\n",
        "print (onlyfiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f7c29c9f-dd95-4d19-280f-92bcbb963241"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "img=[]\n",
        "for i in range (0,5):\n",
        "    img.append(mpimg.imread('../input/images_sample/6811957/'+onlyfiles[i]))\n",
        "    plt.imshow(img[i])\n",
        "    fig = plt.figure()\n",
        "    a=fig.add_subplot()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d1ebcedc-a90c-3c47-5542-0440b1edae8c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "X_train.dropna(subset = ['interest_level'])\n",
        "X_train.shape\n",
        "print (X_train['interest_level'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1a90927-22e5-3524-2734-0b55919228e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "grouped = X_train.groupby(['interest_level'])\n",
        "print (grouped.size())\n",
        "#probability\n",
        "print (grouped.size()/len(X_train))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ccf981bb-3123-86e9-a4bc-1a2964478423"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train[\"num_photos\"] = X_train[\"photos\"].apply(len)\n",
        "X_train[\"num_features\"] = X_train[\"features\"].apply(len)\n",
        "X_train[\"num_description_words\"] = X_train[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "X_train[\"created\"] = pd.to_datetime(X_train[\"created\"])\n",
        "X_train[\"created_year\"] = X_train[\"created\"].dt.year\n",
        "X_train[\"created_month\"] = X_train[\"created\"].dt.month\n",
        "X_train[\"created_day\"] = X_train[\"created\"].dt.day\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "50cb373d-52ce-1f1e-891c-a7267fe06c6a"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_test[\"num_photos\"] = X_test[\"photos\"].apply(len)\n",
        "X_test[\"num_features\"] = X_test[\"features\"].apply(len)\n",
        "X_test[\"num_description_words\"] = X_test[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "X_test[\"created\"] = pd.to_datetime(X_test[\"created\"])\n",
        "X_test[\"created_year\"] = X_test[\"created\"].dt.year\n",
        "X_test[\"created_month\"] = X_test[\"created\"].dt.month\n",
        "X_test[\"created_day\"] = X_test[\"created\"].dt.day\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d4da872d-3caf-e565-df5f-0662fa21b93c"
      },
      "outputs": [],
      "source": [
        "num_feats = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\n",
        "             \"num_photos\", \"num_features\", \"num_description_words\",\n",
        "             \"created_year\", \"created_month\", \"created_day\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d83c1d92-c9d5-d60e-e5ab-a5cd8ed206db"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "longitude=X_train[\"longitude\"]\n",
        "latitude=X_train[\"latitude\"]\n",
        "listing_id=X_train[\"listing_id\"]\n",
        "plt.scatter(longitude, latitude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b32367a5-918a-b26f-1f10-acdcb2edb7af"
      },
      "outputs": [],
      "source": [
        "plt.scatter(listing_id, latitude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d65886ac-dd54-41bf-546d-582268fc530d"
      },
      "outputs": [],
      "source": [
        "plt.scatter(listing_id, longitude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "88382285-644c-a79a-b6dd-1ed79c1af9c0"
      },
      "outputs": [],
      "source": [
        "X_train3=X_train[X_train[\"latitude\"]==0]\n",
        "X_train3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7f33be87-251e-3fc4-5d78-b6c3449f1795"
      },
      "outputs": [],
      "source": [
        "X_train2=X_train[X_train[\"longitude\"]!=0]\n",
        "X_train2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c19babc2-0450-2cc5-83b2-a7b9e4661d1e"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_train2[\"longitude\"], X_train2[\"latitude\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0b8693c-9aea-fe9e-5780-6a40efea2f55"
      },
      "outputs": [],
      "source": [
        "X_train4=X_train2[X_train2[\"longitude\"]>-110]\n",
        "plt.scatter(X_train4[\"longitude\"], X_train4[\"latitude\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f9ee04e-013e-a832-97c4-9c4c7e6fb157"
      },
      "outputs": [],
      "source": [
        "X_train5=X_train2[X_train2[\"longitude\"]<-110]\n",
        "X_train5[\"interest_level\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d2c3d987-e8ae-ed91-d709-7f8551fadade"
      },
      "outputs": [],
      "source": [
        "X_train5.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "21b8cc06-bd4a-f61e-ccd9-130d1f7c98ae"
      },
      "outputs": [],
      "source": [
        "#Preprocessing\n",
        "from wordcloud import WordCloud\n",
        "text = ''\n",
        "text_dispadd = ''\n",
        "text_street = ''\n",
        "text_desc =''\n",
        "for ind, row in train.iterrows():\n",
        "    for feature in row['features']:\n",
        "        text = \" \".join([text, \"_\".join(feature.strip().split(\" \"))])\n",
        "    text_dispadd = \" \".join([text_dispadd,\"_\".join(row['display_address'].strip().split(\" \"))])\n",
        "    text_street = \" \".join([text_street, row['street_address']])\n",
        "    text_desc=\" \".join([text_desc, row['description']])\n",
        "text = text.strip()\n",
        "text_dispadd = text_dispadd.strip()\n",
        "text_street = text_street.strip()\n",
        "text_desc = text_desc.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b63ff4b0-8f6a-4707-0290-cdb52e735a28"
      },
      "outputs": [],
      "source": [
        "list(X_train.columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "799162bb-950f-9a1b-a831-ff0c110d3263"
      },
      "outputs": [],
      "source": [
        "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
        "X_train[\"label\"] = X_train[\"interest_level\"].apply(lambda x: target_num_map[x])\n",
        "X_train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9e64ada5-9d46-deeb-7b39-cb9ced66de95"
      },
      "outputs": [],
      "source": [
        "\n",
        "y = X_train[\"label\"]\n",
        "\n",
        "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aecc35cb-1284-2601-bca7-dc3180cc6999"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL_COLUMNS = [\"building_id\",\"description\", \"display_address\", \"street_address\"]\n",
        "#CATEGORICAL_COLUMNS = [\"building_id\",\"description\", \"display_address\", \"features\", \"street_address\"]\n",
        "CONTINUOUS_COLUMNS = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\", \"num_photos\", \"listing_id\", \n",
        "                      \"num_features\", \"num_description_words\", \"created_year\", \"created_month\", \"created_day\"]\n",
        "LABEL_COLUMN =[\"label\"]\n",
        "import tensorflow as tf\n",
        "continuous_cols = {k: tf.constant(X_train[k].values) for k in CONTINUOUS_COLUMNS}\n",
        "continuous_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "00302463-b2d3-3ee0-fcde-82e2097c6d77"
      },
      "outputs": [],
      "source": [
        "X_train[\"label\"].size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1c9a1b8-6ef8-6e55-8859-1805af086b17"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "categorical_cols = {k: tf.SparseTensor(\n",
        "    indices=[[i, 0] for i in range(X_train[k].size)], \n",
        "    values=X_train[k].values, \n",
        "    dense_shape = [X_train[k].size , 1]) \n",
        "                    for k in CATEGORICAL_COLUMNS}\n",
        "categorical_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4bdf350a-62d1-a6e4-d331-a9e5b6771550"
      },
      "outputs": [],
      "source": [
        "merged = dict(continuous_cols.items() )\n",
        "merged.update(categorical_cols.items())\n",
        "merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b3f3e753-542d-11d4-3c38-42b2b3219d1e"
      },
      "outputs": [],
      "source": [
        "merged2=continuous_cols\n",
        "merged2.update(categorical_cols)\n",
        "merged2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "84be7377-31da-7829-03ea-a6f4e61759af"
      },
      "outputs": [],
      "source": [
        "y = X_train[\"label\"]\n",
        "\n",
        "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5515837b-0ff8-2db5-e91a-fd4d5f36a9b8"
      },
      "outputs": [],
      "source": [
        "def input_fn(df):\n",
        "    #Creates a dictionary mapping from each continuous feature column name (k) to\n",
        "    #the values of that column stored in a constant Tensor.\n",
        "    continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}      \n",
        "    #Creates a dictionary mapping from each categorical feature column name (k)\n",
        "    #to the values of that column stored in a tf.SparseTensor.    \n",
        "    categorical_cols = {k: tf.SparseTensor(indices=[[i, 0] for i in range(df[k].size)], values=df[k].values, dense_shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}\n",
        "    # Merges the two dictionaries into one.\n",
        "    feature_cols=continuous_cols\n",
        "    feature_cols.update(categorical_cols)\n",
        "    # Converts the label column into a constant Tensor.\n",
        "    label = tf.constant(df[LABEL_COLUMN].values)\n",
        "    #Returns the feature columns and the label.\n",
        "    return feature_cols, label\n",
        "\n",
        "def train_input_fn():\n",
        "    return input_fn(X_train2)\n",
        "\n",
        "def eval_input_fn():\n",
        "    return input_fn(X_val)\n",
        "\n",
        "def test_input_fn():\n",
        "    return input_fn(X_test)\n",
        "\n",
        "train_input_fn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c60ae6a0-b810-16fa-0de2-de3a42ba5162"
      },
      "outputs": [],
      "source": [
        "eval_input_fn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fe76c057-a338-8b71-ed8c-3bbaae2fb98b"
      },
      "outputs": [],
      "source": [
        "\n",
        "building_id = tf.contrib.layers.sparse_column_with_hash_bucket(\"building_id\", hash_bucket_size=1000)\n",
        "description = tf.contrib.layers.sparse_column_with_hash_bucket(\"description\", hash_bucket_size=1000)\n",
        "display_address = tf.contrib.layers.sparse_column_with_hash_bucket(\"display_address\", hash_bucket_size=1000)\n",
        "street_address = tf.contrib.layers.sparse_column_with_hash_bucket(\"street_address\", hash_bucket_size=1000)\n",
        "\n",
        "bathrooms = tf.contrib.layers.real_valued_column(\"bathrooms\")\n",
        "bedrooms = tf.contrib.layers.real_valued_column(\"bedrooms\")\n",
        "latitude = tf.contrib.layers.real_valued_column(\"latitude\")\n",
        "longitude = tf.contrib.layers.real_valued_column(\"longitude\")\n",
        "price = tf.contrib.layers.real_valued_column(\"price\")\n",
        "num_photos = tf.contrib.layers.real_valued_column(\"num_photos\")\n",
        "listing_id = tf.contrib.layers.real_valued_column(\"listing_id\")\n",
        "num_features = tf.contrib.layers.real_valued_column(\"num_features\")\n",
        "num_description_words = tf.contrib.layers.real_valued_column(\"num_description_words\")\n",
        "created_year = tf.contrib.layers.real_valued_column(\"created_year\")\n",
        "created_month = tf.contrib.layers.real_valued_column(\"created_month\")\n",
        "created_day = tf.contrib.layers.real_valued_column(\"created_day\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "10539b2b-c9e7-20db-7a1c-c66a4d7e47ba"
      },
      "outputs": [],
      "source": [
        "daddress_x_saddress = tf.contrib.layers.crossed_column([display_address, street_address], hash_bucket_size=int(1e4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d9e4e6e9-9ed1-f6d4-0d2f-2a9fba427242"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "model_dir = tempfile.mkdtemp()\n",
        "m = tf.contrib.learn.LinearClassifier(feature_columns=[\n",
        "  description, street_address, bathrooms, bedrooms, latitude, longitude, price,\n",
        "  num_photos, num_features, num_description_words, created_year, created_month, created_day],\n",
        "  model_dir=model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4042a897-ed31-1a85-a05e-cb1e329f75d7"
      },
      "outputs": [],
      "source": [
        "m.fit(input_fn=train_input_fn, steps=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da867974-7078-6242-7f65-a3d45db669b8"
      },
      "outputs": [],
      "source": [
        "results = m.evaluate(input_fn=eval_input_fn, steps=1)\n",
        "for key in sorted(results):\n",
        "    print (\"%s: %s\" % (key, results[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b38bd0bb-f56e-46bd-37df-2b5fa312b7d0"
      },
      "outputs": [],
      "source": [
        "results2 = m.evaluate(input_fn=train_input_fn, steps=1)\n",
        "for key in sorted(results2):\n",
        "    print (\"%s: %s\" % (key, results2[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8029a3e4-cb9b-2cfe-2c79-82906748192b"
      },
      "outputs": [],
      "source": [
        "def input_fn2(df):\n",
        "    #Creates a dictionary mapping from each continuous feature column name (k) to\n",
        "    #the values of that column stored in a constant Tensor.\n",
        "    continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}      \n",
        "    #Creates a dictionary mapping from each categorical feature column name (k)\n",
        "    #to the values of that column stored in a tf.SparseTensor.    \n",
        "    categorical_cols = {k: tf.SparseTensor(indices=[[i, 0] for i in range(df[k].size)], values=df[k].values, dense_shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}\n",
        "    # Merges the two dictionaries into one.\n",
        "    feature_cols=continuous_cols\n",
        "    feature_cols.update(categorical_cols)\n",
        "    # Converts the label column into a constant Tensor.\n",
        "    #label = tf.constant(df[LABEL_COLUMN].values)\n",
        "    #Returns the feature columns and the label.\n",
        "    return feature_cols\n",
        "\n",
        "def test_input_fn():\n",
        "    return input_fn2(X_test)\n",
        "\n",
        "test_input_fn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1989b590-ae07-d13e-7cff-1d9f67442a57"
      },
      "outputs": [],
      "source": [
        "print (m.predict(input_fn=lambda: test_input_fn()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4ae7b979-a627-8901-4200-606e25cb66cf"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}