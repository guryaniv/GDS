{"nbformat_minor": 1, "cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "ad51e33f288d508a297949ffa125f11b0576284f", "_cell_guid": "85423104-78f3-4b37-86ee-02c548dba6ec"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import pandas as pd\n", "import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import plotly.offline as py\n", "\n", "from collections import Counter\n", "from sklearn import metrics\n", "from sklearn import preprocessing\n", "from sklearn.metrics import classification_report\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import accuracy_score\n", "from sklearn import linear_model\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.decomposition import PCA\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.model_selection import cross_val_score\n", "from lightgbm import LGBMClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "\n", "# Read the train data set\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "print(train.head())\n", "\n", "# Check for null value\n", "train.isnull().any().any()\n", "print(train.isnull().any().any())\n", "\n", "Counter(train.dtypes.values)\n", "print(Counter(train.dtypes.values))\n", "\n", "# Check for missing vales by its features\n", "train2= (train.isnull().sum() / len(train)) * 100\n", "misval = train2.drop(train2[train2 == 0].index).sort_values(ascending=False)[:30]\n", "missing = pd.DataFrame({'Missing %' :misval})\n", "missing.head(10)\n", "print(missing.head(10))\n", "\n", "# Phishing out potentential values probably having -1\n", "train_copy = train.replace(-1, np.NaN)\n", "train_copy= (train_copy.isnull().sum() / len(train_copy)) * 100\n", "train_copy = train_copy.drop(train_copy[train_copy == 0].index).sort_values(ascending=False)[:30]\n", "missing = pd.DataFrame({'Missing %' :train_copy})\n", "missing.head(10)\n", "print(missing.head(10))\n", "\n", "# group to either intger or floating data type\n", "train_float = train.select_dtypes(include=['float64'])\n", "train_int = train.select_dtypes(include=['int64'])\n", "print(train_float)\n", "print(train_int)\n", "\n", "# group by feature types\n", "bin_col = [col for col in train.columns if '_bin' in col] #binary\n", "cat_col = [col for col in train.columns if '_cat' in col] #categorical\n", "# group by numerical features\n", "num_col = [x for x in train.columns if x[-3:] not in ['bin', 'cat']]\n", "# group by individual, car, region and calculated fields\n", "ind_col = [col for col in train.columns if '_ind_' in col] #individual\n", "car_col = [col for col in train.columns if '_car_' in col] #car\n", "reg_col = [col for col in train.columns if '_reg_' in col] #region\n", "calc_col = [col for col in train.columns if '_calc_' in col] #calculation\n", "\n", "zero_list = []\n", "one_list = []\n", "for col in bin_col:\n", "    zero_list.append((train[col] == 0).sum())\n", "    one_list.append((train[col] == 1).sum())\n", "print(zero_list)\n", "print(one_list)\n", "\n", "# Corralation matrix\n", "cor_matrix = train[num_col].corr().round(2)\n", "print(cor_matrix)\n", "\n", "# Correlation of float values\n", "colormap = plt.cm.magma\n", "plt.figure(figsize=(16,12))\n", "plt.title('Correlation of float features', y=1.05, size=15)\n", "sns.heatmap(train_float.corr(),linewidths=0.1,vmax=1.0, square=True,\n", "            cmap=colormap, linecolor='white', annot=True)\n", "plt.show()\n", "\n", "tot_cat_col = list(train.select_dtypes(include=['category']).columns)\n", "\n", "other_cat_col = [c for c in tot_cat_col if c not in cat_col+ bin_col]\n", "other_cat_col\n", "\n", "# Using PCA\n", "X = train.drop(['id', 'target'], axis=1).values\n", "y = train['target'].values.astype(np.int8)\n", "\n", "# Standardize feature\n", "X_scaled = preprocessing.scale(X)\n", "print(X)\n", "\n", "target_names = np.unique(y)\n", "print('\\nThere are %d unique target valuess in this dataset:' % (len(target_names)), target_names)\n", "n_comp = 10\n", "\n", "# PCA\n", "print('\\nRunning PCA ...')\n", "pca = PCA(n_components=n_comp, svd_solver='full', random_state=1001)\n", "X_pca = pca.fit_transform(X)\n", "print('Explained variance: %.4f' % pca.explained_variance_ratio_.sum())\n", "\n", "print('Individual variance contributions:')\n", "for j in range(n_comp):\n", "    print(pca.explained_variance_ratio_[j])\n", "\n", "# Plotting the scatter plot of the training data on the 1st and 2nd PC\n", "colors = ['orange', 'blue']\n", "plt.figure(1, figsize=(11, 11))\n", "\n", "for color, i, target_name in zip(colors, [0, 1], target_names):\n", "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, s=1,\n", "                alpha=.7, label=target_name, marker='.')\n", "plt.legend(loc='best', shadow=False, scatterpoints=3)\n", "plt.title(\n", "        \"Training data projected on the 1st \"\n", "        \"and 2nd principal components\")\n", "plt.xlabel(\"Principal axis 1 - Explains %.1f %% of the variance\" % (\n", "        pca.explained_variance_ratio_[0] * 100.0))\n", "plt.ylabel(\"Principal axis 2 - Explains %.1f %% of the variance\" % (\n", "        pca.explained_variance_ratio_[1] * 100.0))\n", "plt.show()\n", "\n", "# Validation of the train data\n", "num_folds = 8\n", "seed = 8\n", "scoring = 'Accuracy'\n", "\n", "X = X_pca\n", "Y = np.array(train['target'])\n", "\n", "validation_size = 0.25\n", "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=\n", "                                                                validation_size, random_state=seed)\n", "# generate results with linear algorithms\n", "models = [('LR', LogisticRegression()),\n", "          ('NB', GaussianNB())]\n", "results =[]\n", "names = []\n", "for name, model in models:\n", "    print(\"Training model %s\" % (name))\n", "    model.fit(X_train, Y_train)\n", "    result = model.score(X_validation, Y_validation)\n", "    info = \"Classifier score %s: %f\" %(name, result)\n", "    print(info)\n", "print(\"Done\")\n", "\n", "# Trying a boosting method using the python package Light Gradient Boosting Method(LGBM)\n", "id_test = test['id'].values\n", "target_train = train['target'].values\n", "\n", "train = train.drop(['target','id'], axis = 1)\n", "test = test.drop(['id'], axis = 1)\n", "\n", "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n", "train = train.drop(col_to_drop, axis=1)\n", "test = test.drop(col_to_drop, axis=1)\n", "\n", "train = train.replace(-1, np.nan)\n", "test = test.replace(-1, np.nan)\n", "\n", "cat_features = [a for a in train.columns if a.endswith('cat')]\n", "\n", "for column in cat_features:\n", "    temp = pd.get_dummies(pd.Series(train[column]))\n", "    train = pd.concat([train, temp], axis=1)\n", "    train = train.drop([column], axis=1)\n", "\n", "for column in cat_features:\n", "    temp = pd.get_dummies(pd.Series(test[column]))\n", "    test = pd.concat([test, temp], axis=1)\n", "    test = test.drop([column], axis=1)\n", "\n", "print(train.values.shape, test.values.shape)\n", "\n", "class Ensemble(object):\n", "    def __init__(self, n_splits, stacker, base_models):\n", "        self.n_splits = n_splits\n", "        self.stacker = stacker\n", "        self.base_models = base_models\n", "\n", "    def fit_predict(self, X, y, T):\n", "        X = np.array(X)\n", "        y = np.array(y)\n", "        T = np.array(T)\n", "\n", "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n", "\n", "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n", "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n", "        for i, clf in enumerate(self.base_models):\n", "\n", "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n", "\n", "            for j, (train_idx, test_idx) in enumerate(folds):\n", "                X_train = X[train_idx]\n", "                y_train = y[train_idx]\n", "                X_holdout = X[test_idx]\n", "\n", "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n", "                clf.fit(X_train, y_train)\n", "                y_pred = clf.predict_proba(X_holdout)[:,1]\n", "\n", "                S_train[test_idx, i] = y_pred\n", "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n", "            S_test[:, i] = S_test_i.mean(axis=1)\n", "\n", "        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n", "        print(\"Stacker score: %.5f\" % (results.mean()))\n", "        self.stacker.fit(S_train, y)\n", "        res = self.stacker.predict_proba(S_test)[:, 1]\n", "        return res\n", "\n", "# LightGBM params\n", "lgb_params = {}\n", "lgb_params['learning_rate'] = 0.02\n", "lgb_params['n_estimators'] = 650\n", "lgb_params['max_bin'] = 10\n", "lgb_params['subsample'] = 0.8\n", "lgb_params['subsample_freq'] = 10\n", "lgb_params['colsample_bytree'] = 0.8\n", "lgb_params['min_child_samples'] = 500\n", "lgb_params['seed'] = 99\n", "\n", "lgb_params2 = {}\n", "lgb_params2['n_estimators'] = 1090\n", "lgb_params2['learning_rate'] = 0.02\n", "lgb_params2['colsample_bytree'] = 0.3\n", "lgb_params2['subsample'] = 0.7\n", "lgb_params2['subsample_freq'] = 2\n", "lgb_params2['num_leaves'] = 16\n", "lgb_params2['seed'] = 99\n", "\n", "lgb_params3 = {}\n", "lgb_params3['n_estimators'] = 1100\n", "lgb_params3['max_depth'] = 4\n", "lgb_params3['learning_rate'] = 0.02\n", "lgb_params3['seed'] = 99\n", "\n", "lgb_model = LGBMClassifier(**lgb_params)\n", "lgb_model2 = LGBMClassifier(**lgb_params2)\n", "lgb_model3 = LGBMClassifier(**lgb_params3)\n", "\n", "log_model = LogisticRegression()\n", "\n", "stack = Ensemble(n_splits=3,\n", "                 stacker=log_model,\n", "                 base_models=(lgb_model, lgb_model2, lgb_model3))\n", "\n", "y_pred = stack.fit_predict(train, target_train, test)\n", "\n", "sub = pd.DataFrame()\n", "sub['id'] = id_test\n", "sub['target'] = y_pred\n", "sub.to_csv('porto_output.csv', index=False)\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": []}], "metadata": {"language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4}