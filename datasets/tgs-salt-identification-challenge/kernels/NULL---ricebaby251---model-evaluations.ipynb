{"cells":[{"metadata":{"_uuid":"d1a412a180daf616b1392dcc64b6360b99c7f1bc"},"cell_type":"markdown","source":"# Purpose\n\n* The purpose of this notebook is to have a way to provide effective and systematic judgement on your models.\n* It calculates the jaccard distance and displays it in a table:\n    * This is based on the coverage class per mask which ranges from 0 to 10 inclusively.\n* It also displays the visuals of the image, mask and predicted mask (after thresholding):\n    * You can change the number of pictures per class.\n\nAdd your model and anything else to the notebook and run it. Where to add your model is highlighted in red."},{"metadata":{"_uuid":"fe90a09fcd7afa09d6b07294b9f4a436835f915e"},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n\nfrom functools import partial\nimport keras.backend as K\n\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aca8efe5abd17dd24d5486d2e138bf6b35d6de7","collapsed":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9014a2275315bad6929f1206f987546794b170f1"},"cell_type":"markdown","source":"## Parameters"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# train_path = \"../input/train.csv\"\n# depths_path = \"../input/depths.csv\"\n\n# images_path = \"../input/train/images/{}.png\"\n# masks_path = \"../input/train/masks/{}.png\"\n# test_path = \"../input/test/images/{}.png\"\n\nprint(os.listdir(\"../input\"))\n\ntrain_path = \"../input/tgs-salt-identification-challenge/train.csv\"\ndepths_path = \"../input/tgs-salt-identification-challenge/depths.csv\"\n\nimages_path = \"../input/tgs-salt-identification-challenge/train/images/{}.png\"\nmasks_path = \"../input/tgs-salt-identification-challenge/train/masks/{}.png\"\ntest_path = \"../input/tgs-salt-identification-challenge/test/images/{}.png\"\n\nimage_1 = \"0b73b427d1\"\nimage_2 = \"0c02f95a08\"\n\nimg_size_target = 128\nimg_size_original = 101\nimg_channels = 1\n\ntest_size = 0.2\ntrain_size = 4000\nrandom_state = 1337","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e86b21d03d87a195252ce5c9e24142ddc225f81"},"cell_type":"markdown","source":"## Helper Functions"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"23735f9af2ffc803829ddb937fd1e7a1717a0464"},"cell_type":"code","source":"def _resize(img, target_size):\n    img_size = img.size\n    if img_size == target_size:\n        return img\n    return resize(img, (target_size, target_size), mode='constant', preserve_range=True)\n\ndef upsample(img, target_size=img_size_target):\n    return _resize(img, target_size)\n    \ndef downsample(img, target_size=img_size_original):\n    return _resize(img, target_size)\n\ndef cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59b45389eae835faa6e67d2c0e54fb294d2f75b9"},"cell_type":"markdown","source":"## Data"},{"metadata":{"trusted":true,"_uuid":"417b455457bc9bf35585c0e0ccd44a2823894195","collapsed":true},"cell_type":"code","source":"train_df = pd.read_csv(train_path, index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(depths_path, index_col=\"id\")\n\ntrain_df = train_df.join(depths_df)\n\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\n\ntrain_df[\"images\"] = [\n    np.array(load_img(images_path.format(idx), grayscale=True)) / 255 \n    for idx in tqdm_notebook(train_df.index)\n]\n\ntrain_df[\"masks\"] = [\n    np.array(load_img(masks_path.format(idx), grayscale=True)) / 255 \n    for idx in tqdm_notebook(train_df.index)\n]\n\ntrain_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_original, 2)        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n\nx_test = np.array([\n    upsample(np.array(load_img(test_path.format(idx), grayscale=True))) / 255 \n    for idx in tqdm_notebook(test_df.index)\n]).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e721020908801fddf3c6e6c07e463a3fc588630"},"cell_type":"markdown","source":"## Split the Data"},{"metadata":{"trusted":true,"_uuid":"5938073b1197d3afa6657f653200d6976fde0ced","collapsed":true},"cell_type":"code","source":"id_train = train_df.index.values\nx_train = np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\ny_train = np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\ncoverage = train_df.coverage.values\ncoverage_class = train_df.coverage_class.values\ndepths = train_df.z.values\n\nID_train, ID_valid, X_train, X_valid, Y_train, Y_valid, C_train, C_valid, CC_train, CC_valid, Z_train, Z_valid = train_test_split(\n    id_train,\n    x_train,\n    y_train,\n    coverage,\n    coverage_class,\n    depths,\n    test_size=test_size,\n    stratify=train_df.coverage_class,\n    random_state=random_state\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85ee2dc4e6d22c7ae7a42df45b8658b48660220b"},"cell_type":"markdown","source":"## Load Model"},{"metadata":{"_uuid":"bc2ef7d9065333462c0de37b07d6cc2fce837f9d"},"cell_type":"markdown","source":"<span style=\"color:red\">Add your model and loss function.</span>"},{"metadata":{"_uuid":"a73f1141886a49aca143b0dee17c17950233372f"},"cell_type":"markdown","source":"### First Model"},{"metadata":{"trusted":true,"_uuid":"1093abbebc8cad3e12a648165258bceb238d4b0f","collapsed":true},"cell_type":"code","source":"model_path = \"../input/u-net-dropout-augmentation-stratification/keras.model\"\n\nmodel = load_model(model_path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6121d4a7349c9c3afa43981041ddd0b5c142f540"},"cell_type":"markdown","source":"### Second Model"},{"metadata":{"trusted":true,"_uuid":"20efcf04dc7abdda0b67e01512dc33da51ed86af","collapsed":true},"cell_type":"code","source":"def dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return K.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\nmodel_path = \"../input/u-net-bn-aug-strat-dice/keras.model\"\n\nclf = partial(bce_dice_loss)\nclf.__name__ = \"bce_dice_loss\"\n\nmodel = load_model(model_path, custom_objects={'bce_dice_loss': clf})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14e080da1f7ea09623b007856ce987f27669055f"},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true,"_uuid":"f59812c07e63d1e896768ce907ac8b0385d0d4c8","collapsed":true},"cell_type":"code","source":"P_valid = model.predict(X_valid).reshape(-1, img_size_target, img_size_target)\nP_valid = np.array([downsample(x) for x in P_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ID_valid])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fde858d04f131ff77bbc18dbe6beb11c32448976"},"cell_type":"markdown","source":"## Scoring"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fbf2d1bcfec441e73edfebfb93a44ca894586d7e"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4feb358615617a067c6446e908f4a31719ec0f78","collapsed":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(P_valid > threshold)) for threshold in tqdm_notebook(thresholds)])\n\nthreshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]\n\nPA_valid = np.array([\n    np.round(pred > threshold_best) \n    for pred in tqdm_notebook(P_valid)\n], dtype=np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"722dbc1e79fbb3e6159b28daaae05eaccbd0b147","collapsed":true},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddb16106f2b9c843217117b5d9aa87a5db11e9c8"},"cell_type":"markdown","source":"## Evaulation"},{"metadata":{"_uuid":"2ed3a00b3e2b5c1d969635c5d31f1333e733a4eb"},"cell_type":"markdown","source":"### Validation DF"},{"metadata":{"trusted":true,"_uuid":"e032bf7534093981c4c3fac097ae73dcf10adec4","collapsed":true},"cell_type":"code","source":"images_valid = [downsample(X_valid.reshape(-1,128,128)[i]) for i in range(len(X_valid))]\nmasks_valid = [downsample(Y_valid.reshape(-1,128,128)[i]) for i in range(len(Y_valid))]\n\npreds_valid = [P_valid[i] for i in range(len(P_valid))]\npreds_adjusted_valid = [PA_valid[i] for i in range(len(PA_valid))]\n\ncoverage_valid = C_valid\ncoverage_class_valid = CC_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05c84f0a83a247e75e7104ca05a63565fff4a785","collapsed":true},"cell_type":"code","source":"valid_df = pd.DataFrame({\n    \"images\" : images_valid,\n    \"masks\" : masks_valid,\n    \"depths\" : Z_valid,\n    \"coverages\" : coverage_valid,\n    \"coverage_classes\" : coverage_class_valid,\n    \"predictions\": preds_valid,\n    \"predictions_with_threshold\": preds_adjusted_valid, \n}, index=ID_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cab92d780307b04df55ee96df09ec8aaa068e8a1"},"cell_type":"markdown","source":"## Confusion Matrix"},{"metadata":{"trusted":true,"_uuid":"0beb0ef68f1ba1c526739ee9169e8b97c7e95bea","collapsed":true},"cell_type":"code","source":"class_count = valid_df.groupby(\"coverage_classes\").agg(\"count\")[\"coverages\"]\n\ndef jaccard_index(row):\n    a_n_b = np.sum(row['predictions_with_threshold'] == row['masks'])\n    index = float(a_n_b) / float((img_size_original ** 2) * 2 - a_n_b)\n    return index\n\nvalid_df[\"jaccard_index\"] = valid_df.apply(jaccard_index, axis=1)\n\njaccard_index_df = valid_df.groupby(\"coverage_classes\").agg([\"mean\", \"count\"])[\"jaccard_index\"]\njaccard_index_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e98ec30086e617d0275ccaa35defba95b16bee4a"},"cell_type":"markdown","source":"## Visuals (Threshold Adjusted Predictions)"},{"metadata":{"_uuid":"9101174064c7948da9ae0cb8fa55e318eb7cb896"},"cell_type":"markdown","source":"<span style=\"color:red\">Change these params to adjust number of images per classes to display.</span>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7e9fa8f276f55130478e362673bd260336d796ac"},"cell_type":"code","source":"number_of_images_per_class = 12\nnumber_of_classes = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"513f5042ea0892ad7e9460798ed9aeb3e3efbe43","collapsed":true},"cell_type":"code","source":"df = valid_df.reset_index().set_index(\"coverage_classes\").join(jaccard_index_df).reset_index().set_index(\"index\")\n\nnumber_of_images_per_data = 3\nnumber_of_images = number_of_images_per_class * number_of_classes * number_of_images_per_data\ngrid_width = 12\ngrid_height = number_of_images // grid_width\n\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width * 2, grid_height * 2))\nimg_count = 0\n        \nfor coverage_class in jaccard_index_df.sort_values(\"mean\").index[:number_of_classes]:\n    temp = df[df[\"coverage_classes\"] == coverage_class].head(number_of_images_per_class)\n    for i in range(number_of_images_per_class):\n        data = temp.iloc[i]\n        \n        img = data.images\n        mask = data.masks\n        pred = data.predictions_with_threshold\n        depth = data.depths\n        coverage = round(data.coverages, 2)\n        \n        # image\n        ax = axs[img_count // grid_width, img_count % grid_width]\n        img_count += 1\n        ax.imshow(img, cmap=\"Greys\")\n        # mask\n        ax = axs[img_count // grid_width, img_count % grid_width]\n        img_count += 1\n        ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n        # prediction\n        ax = axs[img_count // grid_width, img_count % grid_width]\n        img_count += 1\n        ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n        \n        # details\n        ax.text(1, img_size_original - 1, depth, color=\"black\")\n        ax.text(img_size_original - 1, 1, coverage, color=\"black\", ha=\"right\", va=\"top\")\n        ax.text(1, 1,                     coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}