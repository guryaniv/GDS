{"cells": [{"source": "Updates:\nThe stacking model:\nhttps://www.kaggle.com/schoolpal/nn-stacking-magic-no-magic-30409-private-31063\n\nhttps://www.kaggle.com/schoolpal/lgbm-lb-0-3093-0-3094/ Another kernel for the LGBM model, they were used together with in this one and one more DNN model in the stacking\n\n---------------------------------------------\n\nLB performance not impressive (0.3113 and 0.315), but both models are used for my later stacking. A simple linear combination with weights 0.8,0.2, or 0.75,0.25 can give you a LB score at 0.3109. I can't remember the numbers very clearly, sorry.\n\nI want to say thanks to Reynaldo, almost all my work in the early stage were based on his script. Again, the magic number (0.969) is taken from Andy's script (proposed by Louis?). The improvement I made to the original Reynaldo script (without magic number) gives a LB score like 0.3133, only slightly better than the original script (LB 0.3134?).\n\nSometimes a bad LB score are simply caused by a higher mean, but the model itself can be very useful for later stacking. For example, The log model actually predicted a much higher mean, possibly due to the removal of bad prices. ", "cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "40da508a-4299-4e1b-93da-d93e84349763", "collapsed": false, "_uuid": "b7e76fdfadfa6737727f74f5a0254d17d4c78b91", "_execution_state": "idle"}}, {"source": "import numpy as np\nimport pandas as pd\nfrom sklearn import model_selection, preprocessing\nimport xgboost as xgb\ndef process_log():\n    train = pd.read_csv('../input/train.csv',parse_dates=['timestamp'])\n    test = pd.read_csv('../input/test.csv',parse_dates=['timestamp'])\n    train=train[(train.price_doc>1e6) & (train.price_doc!=2e6)  & (train.price_doc!=3e6)  ]\n    train['price_doc']*=0.969\n    train=train.reset_index(drop=True)\n    id_test = test.id\n\n    times=pd.concat([train.timestamp,test.timestamp])\n    y_train = train[\"price_doc\"]\n    \n    num_train=len(train)\n    times=pd.concat([train.timestamp,test.timestamp])\n    x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n    x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n    df_all=pd.concat([x_train,x_test])\n    df_cat=None\n    for c in df_all.columns:\n        if df_all[c].dtype == 'object':\n            if c=='sub_area':\n                oh=pd.get_dummies(df_all[c],prefix=c)\n                if df_cat is None:\n                    df_cat=oh\n                else:\n                    df_cat=pd.concat([df_cat,oh],axis=1)\n                df_all.drop([c],inplace=True,axis=1)\n            else:\n                lbl = preprocessing.LabelEncoder()\n                lbl.fit(list(df_all[c].values))\n                df_all[c] = lbl.transform(list(df_all[c].values))\n\n    if df_cat is not None:\n        df_all = pd.concat([df_all, df_cat], axis=1)\n\n    x_train=df_all[:len(x_train)]\n    x_test=df_all[len(x_train):]\n\n    xgb_params = {\n        'eta': 0.05,\n        'max_depth': 5,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'silent': 1,\n    }\n\n    x_train=df_all[:len(x_train)]\n    x_test=df_all[len(x_train):]\n\n\n    num_boost_rounds=345\n    dtrain = xgb.DMatrix(x_train, np.log(y_train))\n    dtest = xgb.DMatrix(x_test)\n    model = xgb.train(dict(xgb_params, max_depth=5,silent=1), dtrain,num_boost_round= num_boost_rounds)\n    y_predict_log=np.exp(model.predict(dtest))\n    y_predict=y_predict_log\n    return id_test,y_predict\ndef process():\n    train = pd.read_csv('../input/train.csv',parse_dates=['timestamp'])\n    train['price_doc']*=0.969\n    test = pd.read_csv('../input/test.csv',parse_dates=['timestamp'])\n    id_test = test.id\n\n    times=pd.concat([train.timestamp,test.timestamp])\n    y_train = train[\"price_doc\"]\n    num_train=len(train)\n\n    x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n    x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n    df_all=pd.concat([x_train,x_test])\n\n    for c in df_all.columns:\n        if df_all[c].dtype == 'object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(df_all[c].values))\n            df_all[c] = lbl.transform(list(df_all[c].values))\n    x_train=df_all[:len(x_train)]\n    x_test=df_all[len(x_train):]\n\n    \n    xgb_params = {\n        'eta': 0.05,\n        'max_depth': 5,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'silent': 1,\n    }\n\n    dtrain = xgb.DMatrix(x_train, y_train)\n    dtest = xgb.DMatrix(x_test)\n\n    num_boost_rounds=345\n    model = xgb.train(dict(xgb_params, silent=1), dtrain,num_boost_round= num_boost_rounds)\n    y_predict = model.predict(dtest)\n  \n        \n    return id_test,y_predict\nif __name__=='__main__':\n    id_test,y_predict=process()\n    id_test,y_predict_log=process_log()\n    print('Mean:',y_predict.mean(), 'LB 0.3113')\n    print ('LOG Mean:',y_predict_log.mean(),'LB 0.314-0.315')\n    output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n    output.to_csv('xgb.csv', index=False)\n    output = pd.DataFrame({'id': id_test, 'price_doc': y_predict_log})\n    output.to_csv('xgb_log.csv', index=False)\n", "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"trusted": false, "_cell_guid": "0c2f603c-6e25-4ac9-8985-68b0e5805652", "_uuid": "48b8ca1403ab64a142b6a0293575566f84adb2b5"}}], "metadata": {"language_info": {"version": "3.6.1", "name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat_minor": 0, "nbformat": 4}