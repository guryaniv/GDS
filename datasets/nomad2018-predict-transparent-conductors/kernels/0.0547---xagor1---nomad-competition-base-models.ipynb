{"cells":[{"metadata":{"_cell_guid":"e7c9891f-389c-448a-9d98-8bccc3abccd0","_uuid":"fd96955ca1bdfc9ae645c26fdc4c0efaa55f0ba6"},"cell_type":"markdown","source":"# Introduction\n\nLet's give the NOMAD contest a go! I should have some domain knowledge for this problem, since I come from a materials science / theoretical physics background.\n\nThe aim is to predict the band gap and formation energy per atom for a set of materials, with the idea being to predict transparent semiconductors. Predictions are made based on structural information, such as the space group and lattice parameters, along with composition. Potentially you can also extract more detailed structural information from their .xyz files, but I'll leave that for later.\n\nI found some code from Tony Y to extract the xyz files into here, so will make use of that later.\n(https://www.kaggle.com/tonyyy/how-to-get-atomic-coordinates)"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nimport glob\nimport io\nimport math\nimport matplotlib\n\n\nfrom sklearn.cross_validation import train_test_split\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import metrics\nimport seaborn as sns\nprint(os.listdir(\"../input\"))\nfrom sklearn import tree\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.\n","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"35ee46aa-6b38-4e6e-8738-cf6d0576696c","_uuid":"6530782edc7a5c5b2c590a9495b714c314893bca"},"cell_type":"markdown","source":"# Checking the Data\n\nBefore running any models, I just want to quickly check over the data, to see what it looks like, and what features we have.\n\nAfter this, I started to separate some of the columns out for other uses, like the target variables, and the ID numbers.\n\nBefore doing any transformations on the data, I combined the train and test sets, to make sure the same transformations were applied to both.\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"path = '../input/'\ntrain_df = pd.read_csv(path+\"/train.csv\")\ntest_df = pd.read_csv(path+\"/test.csv\")","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"684af51f-dd01-4d4b-9c5c-ff4087d39681","_kg_hide-input":true,"_uuid":"62f08acd3a2888e6947d4cbf695d857a7f05f06d","collapsed":true,"trusted":true},"cell_type":"code","source":"#What size data are we dealing with?\nprint(\"Training data shape\",\"\\n\")\nprint(train_df.shape,\"\\n\")\nprint(\"Testing data shape\",\"\\n\")\nprint(test_df.shape,\"\\n\")\n\n#What columns / features do we have?\nprint(\"Training columns\",\"\\n\")\nprint(train_df.columns,\"\\n\")\nprint(\"Testing columns\",\"\\n\")\nprint(test_df.columns,\"\\n\")\n\n#What type of data do we have?\n#print(\"Train data types\",\"\\n\")\n#print(train_df.dtypes,\"\\n\")\n#print(\"Test data types\",\"\\n\")\n#print(test_df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cb12afbe-8b1e-4ef2-937a-091ed61f53b8","_kg_hide-input":true,"_uuid":"605e4ee628d43c7fea10047cf7c17d8c71077017","collapsed":true,"trusted":true},"cell_type":"code","source":"#Pull out Targets for later\nTargets_df=pd.DataFrame()\nTargets_df[\"bandgap_energy_ev\"]=train_df[\"bandgap_energy_ev\"].copy()\nTargets_df[\"formation_energy_ev_natom\"]=train_df[\"formation_energy_ev_natom\"].copy()\ntrain_df=train_df.drop([\"formation_energy_ev_natom\",\"bandgap_energy_ev\"],axis=1)\n\n#Pull out IDs for later, since don't need in the models\ntrain_id_df=pd.DataFrame()\ntrain_id_df[\"id\"]=train_df[\"id\"].copy()\ntrain_df=train_df.drop([\"id\"],axis=1)\ntest_id_df=pd.DataFrame()\ntest_id_df[\"id\"]=test_df[\"id\"].copy()\ntest_df=test_df.drop([\"id\"],axis=1)\n\n#Combine both sets of data, so it's possible to do the same transformations on both\ncombined_df = pd.concat([train_df, test_df], ignore_index=True)\n#Count number of NAs\nprint(\"Total number of null values in the df\",\"\\n\")\nprint(combined_df.isna().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"557bf22f-3832-4b81-ae19-d0c7f520f25a","_uuid":"ad6db9f87e7ec5ed4f974ba064278e55648d058a"},"cell_type":"markdown","source":"The space group is categorical data, so I will one-hot encode that.\n\nModels like linear regression work better with regularized data, so I applied some transformations to the data, such as linear or log scaling, based on the skew of the data.\n\nOther models, like XG boost work fine with just the original features, since they apply their own regularization."},{"metadata":{"_cell_guid":"0bfde1d6-2ccc-49ed-8963-fc02db4c3c7d","_kg_hide-input":true,"_uuid":"bb7b04ee8fc48ec86bf407f449fba552e49cdbbb","collapsed":true,"trusted":true},"cell_type":"code","source":"numerical_df=pd.DataFrame.copy(combined_df[['number_of_total_atoms', 'percent_atom_al',\n       'percent_atom_ga', 'percent_atom_in', 'lattice_vector_1_ang',\n       'lattice_vector_2_ang', 'lattice_vector_3_ang',\n       'lattice_angle_alpha_degree', 'lattice_angle_beta_degree',\n       'lattice_angle_gamma_degree']])\n\none_hot_df=pd.DataFrame.copy(combined_df[[\"spacegroup\"]])\n\none_hot_df=pd.get_dummies(one_hot_df,prefix=[\"spacegroup\"],\n                       columns=[\"spacegroup\"])\n\nfeatures_df=pd.concat([numerical_df,one_hot_df],axis=1)\n\n#Manual unskewing / normalizing / standardizing\n#Needed for linear methods etc, but don't need to worry about with XGB.\n\n#print(\"Original skew\",\"\\n\")\n#print(numerical_df.skew())\n#Get names of features which I'll class as skewed and unskewed(at least wrt right skewed)\nskewed_feats= numerical_df.skew()\nskewed_feats = skewed_feats[skewed_feats > 0.1]\nskewed_feats = skewed_feats.index\n\nunskewed_feats= numerical_df.skew()\nunskewed_feats = unskewed_feats[unskewed_feats < 0.1]\nunskewed_feats = unskewed_feats.index\n\n#The X is an arbitrary cut-off & can be fine-tuned to get the best result\n\n#Linearize the unskewed features & log transform the skewed features.\ntransform_df=pd.DataFrame()\ntransform_df[unskewed_feats]=(numerical_df[unskewed_feats]\n                               - numerical_df[unskewed_feats].mean()) / (numerical_df[unskewed_feats].max() - numerical_df[unskewed_feats].min())\ntransform_df[skewed_feats] = np.log1p(numerical_df[skewed_feats])\n\n#Check this worked.\n#print(\"Transformed skew\",\"\\n\")\n#print(transform_df.skew())\n#_ = transform_df.hist(bins=20, figsize=(18, 18), xlabelsize=10)\n\nfeatures_transform_df=pd.concat([transform_df,one_hot_df],axis=1)\nfeatures_df=pd.concat([numerical_df,one_hot_df],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76183ca8-5b59-4548-a383-1ca135ea74f8","_kg_hide-input":true,"_uuid":"6fd54c646a6e6efc4b3d590ea9655273a9366cee","collapsed":true,"trusted":true},"cell_type":"code","source":"#Split back\ntraining_examples=features_df.iloc[0:2400].copy()\ntest_examples=features_df.iloc[2400:3000].copy()\ntraining_examples_transform=features_transform_df.iloc[0:2400].copy()\ntest_examples_transform=features_transform_df.iloc[2400:3000].copy()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db6f48e2-6e1b-4d94-86c0-a6af37131cee","_uuid":"10bb67d540384dd3f12e3a070ff5b3c2e01734bd"},"cell_type":"markdown","source":"# Simple Models\n\nBefore dealing with feature engineering, I wanted to see how good a model I could build with just the data as presented. I found that both Ridge and Lasso did not have any added benefit over the basic Linear Regression, with both favouring alphas of zero.\n\nI found that I got the best results from linear regression when I applied a log transform to all feaures with a skew of above 0.1."},{"metadata":{"_cell_guid":"f01ba81d-6385-4a56-a429-2bf116f01ac2","_kg_hide-input":true,"_uuid":"520732294356da03c7db3091bb118d0535d88b3e","collapsed":true,"trusted":true},"cell_type":"code","source":"#rmsle_cv measure\ndef rmsle_cv(model):\n    rmsle= np.sqrt(-cross_val_score(model, training_examples_transform, training_targets, scoring=\"neg_mean_squared_log_error\", cv = 5))\n    return(rmsle)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9bb6bda0-9fdc-41b8-9b2a-9c6e7eb5d4f3","_kg_hide-input":true,"_uuid":"fb16bd44d96f36acb6a4228ecd278ed2e31e47c4","collapsed":true,"trusted":true},"cell_type":"code","source":"#Linear regression model\ntraining_targets=Targets_df[\"bandgap_energy_ev\"].copy()\nmodel_linear=LinearRegression().fit(training_examples_transform, training_targets)\nlinear_BG_pred = model_linear.predict(test_examples_transform)\nBG_rmsle=rmsle_cv(model_linear).mean()\nprint(\"Band gap RMSLE:\")\nprint(BG_rmsle,\"\\n\")\n#Get 0.1217\n\n#Linear regression model\ntraining_targets=Targets_df[\"formation_energy_ev_natom\"].copy()\nmodel_linear=LinearRegression().fit(training_examples_transform, training_targets)\nlinear_EF_pred = model_linear.predict(test_examples_transform)\nEF_rmsle=rmsle_cv(model_linear).mean()\nprint(\"Formation Energy RMSLE:\")\nprint(EF_rmsle,\"\\n\")\n# Get 0.0491 with the >0.1 skew transform\n\nprint(\"Expected combined RMSLE\")\ncombined_rmsle=(EF_rmsle+BG_rmsle)/2\nprint(combined_rmsle)\n#Expect 0.0854 for just linear regression\n\nPredictions_df=pd.DataFrame()\nPredictions_df[\"id\"]=test_id_df[\"id\"].copy()\nPredictions_df[\"formation_energy_ev_natom\"]=linear_EF_pred\nPredictions_df[\"bandgap_energy_ev\"]=linear_BG_pred\nPredictions_df.to_csv(\"Linear_Nomad.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a195b112-4837-4d98-bb2a-e39c0fff5bf2","_uuid":"daa079cc57a066126f94a75c8f6e7df1465614b8"},"cell_type":"markdown","source":"For the simple linear regression model, I get an RMSLE on the private leaderboard of 0.0884, which is only  0.0030 different from the expected value. If I pool the public and private leaderboard scores together, and weight for the data split, then it becomes 0.0860, which is basically the same as the expected value."},{"metadata":{"_cell_guid":"0e18235a-0bc5-4246-8b9a-5fd0de8b5193","_uuid":"0699b005940f0116b828ba5c8906bcb7f7cd0a64"},"cell_type":"markdown","source":"After establishing the linear model, as a baseline, I next turned to the old favourite, XG Boost. It doesn't have a built in RMSLE metric, so if I want to tune my model, I need my own version.\n\nThis is easy enough, since you can just log transform the targets, and get the cross-validated RMSE for that. You just need to make sure to transform back the predictions, otherwise they'll be completely wrong.\n\nFor both sets of predictions, I tuned the XGB parameters to try and get the best test accuracy, using the inbuilt cross-validation function. This lead to minor improvements, for example reducing the RMSLE for the band gap from ~0.090 to ~0.086."},{"metadata":{"_cell_guid":"1f1cec2f-21f6-4bdd-b5bd-4db205adeebd","_kg_hide-input":true,"_uuid":"9cdabaf375ac0dc7003011cdcfd8e0e64fb4ca59","collapsed":true,"trusted":true},"cell_type":"code","source":"#XGB model for BG\ntraining_targets=np.log1p(Targets_df[\"bandgap_energy_ev\"].copy())\ndtrain = xgb.DMatrix(training_examples, label = training_targets)\ndtest = xgb.DMatrix(test_examples)\n#tune wrt params\nparams = {\"max_depth\":2,\n          \"eta\":0.1,\n         'gamma':0,  \n         'subsample':0.8,\n         'colsample_bytree':1,\n         'min_child_weight':10,\n         'reg_alpha':7e-3,\n         'reg_lambda':1,\n        }\n#for i in range(1,11):\n#    params[\"max_depth\"]=i\n#    for j in range(1,11):\n#        params[\"min_child_weight\"]=j\n#        model_xgb = xgb.cv(params, dtrain,  num_boost_round=500,early_stopping_rounds=100)\n#        model_xgb.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n#        print(\"max_depth:\")\n#        print(params[\"max_depth\"])\n#        print(\"min_child_weight:\")\n#        print(params[\"min_child_weight\"])\n#        last=len(model_xgb.loc[:])-1\n#        print(model_xgb.loc[last:,[\"test-rmse-mean\"]])\n#for i in np.arange(0,1.1,0.1):\n#    params[\"gamma\"]=i\n#    model_xgb = xgb.cv(params, dtrain,  num_boost_round=500,early_stopping_rounds=100)\n#    model_xgb.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n#    print(\"gamma:\")\n#    print(params[\"gamma\"])\n#    last=len(model_xgb.loc[:])-1\n#   print(model_xgb.loc[last:,[\"test-rmse-mean\"]])\n#for i in np.arange(0.1,1.1,0.1):\n#    params[\"subsample\"]=i\n#    for j in np.arange(0.1,1.1,0.1):\n#        params[\"colsample_bytree\"]=j\n#        model_xgb = xgb.cv(params, dtrain,  num_boost_round=500,early_stopping_rounds=100)\n#        model_xgb.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n#        print(\"subsample:\")\n#        print(params[\"subsample\"])\n#        print(\"colsample_bytree:\")\n#        print(params[\"colsample_bytree\"])\n#        last=len(model_xgb.loc[:])-1\n#        print(model_xgb.loc[last:,[\"test-rmse-mean\"]])\n\n#for i in (1,0.9,0.8,1.1,1.2,1.5):\n#    params[\"reg_lambda\"]=i\n#    model_xgb = xgb.cv(params, dtrain,  num_boost_round=1000,early_stopping_rounds=100)\n#    print(\"reg_lambda:\")\n#    print(params[\"reg_lambda\"])\n#    last=len(model_xgb.loc[:])-1\n#    print(model_xgb.loc[last:,[\"test-rmse-mean\"]])\n    \n#for i in (0.1,0.09,0.08,0.07,0.06,0.05,0.04,0.03,0.02,0.01):\n#    params[\"eta\"]=i\n#    model_xgb = xgb.cv(params, dtrain,  num_boost_round=1000,early_stopping_rounds=100)\n#   print(\"eta:\")\n#    print(params[\"eta\"])\n#    last=len(model_xgb.loc[:])-1\n#    print(model_xgb.loc[last:,[\"test-rmse-mean\"]])\nmodel_xgb = xgb.cv(params, dtrain,  num_boost_round=500,early_stopping_rounds=100)\nmodel_xgb.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\nlast=len(model_xgb.loc[:])-1\nprint(model_xgb.loc[last:,[\"test-rmse-mean\"]])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f5f7c9c1-84b4-4aa1-8e89-08ebe8bd6374","_kg_hide-input":true,"_uuid":"ea53bc0836d9319dc0e553417b5ca2d0d3a97800","collapsed":true,"trusted":true},"cell_type":"code","source":"#Fit a model with optimized parameters\nmodel_xgb = xgb.XGBRegressor(n_estimators=last,max_depth=2, learning_rate=0.1,\n                             gamma=0,subsample=0.8,colsample_bytree=1,min_child_weight=10,\n                            reg_alpha=7e-3,reg_lambda=1)\nmodel_xgb.fit(training_examples, training_targets)\nxgb.plot_importance(model_xgb)\n\n#Predictions\nxgb_BG_preds = np.expm1(model_xgb.predict(test_examples))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"558dc6c6-a8d0-456a-9dc6-6e149d6bf9b6","_kg_hide-input":true,"_uuid":"4bf96f23873150f03d937ff201ba7cb1fe927291","collapsed":true,"trusted":true},"cell_type":"code","source":"#XGB model for EF\ntraining_targets=np.log1p(Targets_df[\"formation_energy_ev_natom\"].copy())\ndtrain = xgb.DMatrix(training_examples, label = training_targets)\ndtest = xgb.DMatrix(test_examples)\n#tune wrt params\nparams = {\"max_depth\":4,\n          \"eta\":0.08,\n          'gamma':0,  \n         'subsample':1,\n          'colsample_bytree':0.4,\n          'min_child_weight':3,\n          'reg_alpha':0,\n          'reg_lambda':0,\n         }\n#for i in range(1,11):\n#    params[\"max_depth\"]=i\n#    for j in range(1,11):\n#        params[\"min_child_weight\"]=j\n#        model_xgb = xgb.cv(params, dtrain,  num_boost_round=500,early_stopping_rounds=100)\n#        model_xgb.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n#        print(\"max_depth:\")\n#        print(params[\"max_depth\"])\n#        print(\"min_child_weight:\")\n#        print(params[\"min_child_weight\"])\n#        last=len(model_xgb.loc[:])-1\n#        print(model_xgb.loc[last:,[\"test-rmse-mean\"]])\n#for i in np.arange(0,1.1,0.1):\n#    params[\"gamma\"]=i\n#    model_xgb = xgb.cv(params, dtrain,  num_boost_round=500,early_stopping_rounds=100)\n#    model_xgb.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n#    print(\"gamma:\")\n#    print(params[\"gamma\"])\n#    last=len(model_xgb.loc[:])-1\n#    print(model_xgb.loc[last:,[\"test-rmse-mean\"]])\n#for i in np.arange(0.1,1.1,0.1):\n#    params[\"subsample\"]=i\n#    for j in np.arange(0.1,1.1,0.1):\n#        params[\"colsample_bytree\"]=j\n#        model_xgb = xgb.cv(params, dtrain,  num_boost_round=500,early_stopping_rounds=100)\n#        model_xgb.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n#        print(\"subsample:\")\n#        print(params[\"subsample\"])\n#        print(\"colsample_bytree:\")\n#        print(params[\"colsample_bytree\"])\n#        last=len(model_xgb.loc[:])-1\n#        print(model_xgb.loc[last:,[\"test-rmse-mean\"]])\n#for i in (0,1,1e-4,1e-3,1e-2,0.1,1.5):\n#    params[\"reg_lambda\"]=i\n#    model_xgb = xgb.cv(params, dtrain,  num_boost_round=1000,early_stopping_rounds=100)\n#    print(\"reg_lambda:\")\n#    print(params[\"reg_lambda\"])\n#    last=len(model_xgb.loc[:])-1\n#    print(model_xgb.loc[last:,[\"test-rmse-mean\"]])\nmodel_xgb = xgb.cv(params, dtrain,  num_boost_round=500,early_stopping_rounds=100)\nmodel_xgb.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\nlast=len(model_xgb.loc[:])-1\nprint(model_xgb.loc[last:,[\"test-rmse-mean\"]])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"14dedabc-3287-4c09-80c1-8c7dc285bc7b","_kg_hide-input":true,"_uuid":"c02ff0ce35725a5824af138b1d664dc7f4759350","collapsed":true,"trusted":true},"cell_type":"code","source":"#Fit a model with optimized parameters\nmodel_xgb = xgb.XGBRegressor(n_estimators=last,max_depth=4, learning_rate=0.08,\n                            gamma= 0, colsample=1,colsample_bytree=0.4,min_child_weight=3,\n                            reg_lambda=0)\nmodel_xgb.fit(training_examples, training_targets)\nxgb.plot_importance(model_xgb)\n\n#Predictions\nxgb_EF_preds = np.expm1(model_xgb.predict(test_examples))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2c4c18d-5fd6-416d-914b-d689d77c71f0","_uuid":"ae6f6ca6953f84e8e203cff02313fcc0c51dd67b"},"cell_type":"markdown","source":"Combined, for the XGB predictions, I would expect RMSLE of ~ 0.0594, so assume it won't be that good!\n\nIn reality, I get 0.0693, which is a new best score, but also 0.01 different from this result. This suggests I'm over-fitting to the training set?"},{"metadata":{"_cell_guid":"f8dfed84-14a6-400d-b30a-f0b1773f14a9","_kg_hide-input":true,"_uuid":"ed7d61c6d7b84ace5d0ad3881fe25f0621acf430","collapsed":true,"trusted":true},"cell_type":"code","source":"Predictions_df=pd.DataFrame()\nPredictions_df[\"id\"]=test_id_df[\"id\"].copy()\nPredictions_df[\"formation_energy_ev_natom\"]=xgb_EF_preds\nPredictions_df[\"bandgap_energy_ev\"]=xgb_BG_preds\nPredictions_df.to_csv(\"XGB_Nomad.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3238c5b2-84e4-481b-a9c4-dad14f1f7f08","_uuid":"b400a01855ec71c2ec5d91f95b9104668bc9254c"},"cell_type":"markdown","source":"Can I combine the linear model with XGB? Stacking leads to a very minor improvement. If I use a 0.9/0.1 XGB/Linear split for both sets of predictions, I can improve the RMSLE to 0.0691, which is barely anything.\n\nImproving any further will require feature engineering of some kind.\n\nAddendum: I did some very light further tuning with reg_alpha and lambda, which after stacking got my final model to 0.0690."},{"metadata":{"_cell_guid":"7209f8da-5797-45e3-b820-bbe052050814","_kg_hide-input":true,"_uuid":"47bbb9c6e85e366010e6df737da4fc198ec02042","collapsed":true,"trusted":true},"cell_type":"code","source":"Predictions_df=pd.DataFrame()\nPredictions_df[\"id\"]=test_id_df[\"id\"].copy()\nstacked_EF_preds=0.9*xgb_EF_preds+0.1*linear_EF_pred\nPredictions_df[\"formation_energy_ev_natom\"]=stacked_EF_preds\nstacked_BG_preds=0.9*xgb_BG_preds+0.1*linear_BG_pred\nPredictions_df[\"bandgap_energy_ev\"]=stacked_BG_preds\nPredictions_df.to_csv(\"Stacked_Nomad.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}