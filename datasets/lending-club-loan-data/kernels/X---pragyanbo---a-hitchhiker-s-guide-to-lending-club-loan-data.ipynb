{"cells":[{"metadata":{"_uuid":"d46ed1c3995a503671cce9c5a2b0be7c9afafa0e"},"cell_type":"markdown","source":"# A Hitchhiker's Guide to Lending Club Loan Data"},{"metadata":{"_uuid":"eea3a24dae54a8a7c45b28e7fd086302bec9a5ca"},"cell_type":"markdown","source":"In this kernel I will be going over the Lending Club Loan Data where the data is imbalanced, big and has multiple features with different data types. For the purpose of modelling, I will be taking all default loans as the target variable and will be trying to predict if a loan will default or not.\n\n---"},{"metadata":{"_uuid":"efc14f0d4f3eb7aebe0e428a12778818136d7aa1"},"cell_type":"markdown","source":"\n# Importing the data"},{"metadata":{"_uuid":"9a48cff4fdda573f7c44956cbfca264d46cc24c8"},"cell_type":"markdown","source":"First, importing necessary libraries,"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"#Imports \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39f5d0e2ca907cca87765e7ed87c2c9669e1ad42","collapsed":true},"cell_type":"code","source":"start_df = pd.read_csv('../input/loan.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f422dd4f752638e4d98c37cd594532897be54f64"},"cell_type":"markdown","source":"Working on a copy of the dataframe so that I do not have to re-read the entire dataset again in order to save memory."},{"metadata":{"trusted":true,"_uuid":"ba2b9730ea92804dccf425b78473c34256c74cc1","scrolled":false},"cell_type":"code","source":"df = start_df.copy(deep=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96e014278797465e932a6073b54813037f9349e0"},"cell_type":"markdown","source":"Checking the dimensions,"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"cf4007a1a46e22421ede9b7b28c206cca81aac66"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"646fc2a5e52e52988adcb3e7200bc7c54b8722e0"},"cell_type":"markdown","source":"Printing out the column names,"},{"metadata":{"trusted":true,"_uuid":"7074c8899843dd187eccde3cdfa093330a3f90fe"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bffa6c7b34b2d574f82d52cca6d5e0a5e692369"},"cell_type":"markdown","source":"So, we've got a fair amount of columns that we need to understand. Knowing what the columns mean can help us a lot for feature engineering later on.\n\n---"},{"metadata":{"_uuid":"04e82722ac0b7ffed24f3a1e0c175623e05852ba"},"cell_type":"markdown","source":"# Understanding the data "},{"metadata":{"_uuid":"475d01087d4d657b6d5c6dd0e32939f4e6d41b33"},"cell_type":"markdown","source":"First, let's check the description of the various column fields in the dataset."},{"metadata":{"trusted":true,"_uuid":"70dda9027a21e7233c14fd2544eea26901a7019b","scrolled":false},"cell_type":"code","source":"df_description = pd.read_excel('../input/LCDataDictionary.xlsx').dropna()\ndf_description.style.set_properties(subset=['Description'], **{'width': '1000px'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62e7fa179d670f829abbf769bb0fe2d0e7595b11"},"cell_type":"markdown","source":"> Looking at the columns description, a good thing we could do is find columns that carry importance and at the same time find columns that are redundant for their lack of information.\n\n\nLet us also see the number and percentage of missing values,"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"59d877770d94c6be172ae3f0ddeb7f87b3eba3f6"},"cell_type":"code","source":"def null_values(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8edcf18e99512d419e17cb3d7b3a6e457549681","scrolled":true},"cell_type":"code","source":"# Missing values statistics\nmiss_values = null_values(df)\nmiss_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0b94824120f605205d48cdf792bccdb7db6d0bf"},"cell_type":"markdown","source":"> The percentage of missing data in many columns are far more than we can work with. So, we'll have to remove columns having a certain percentage of data less than the total data later on."},{"metadata":{"_uuid":"c0f8ae6233547d536407e8b237ad55f0d69e8d6d"},"cell_type":"markdown","source":"Another thing we would want to examine is that how many loans have a default loan status in comparison to other loans. A common thing to predict in datasets like these are if a new loan will get default or not. I'll be keeping loans with default status as my target variable."},{"metadata":{"trusted":true,"_uuid":"741796ff1e353584514b6dfd847e8007895e5544"},"cell_type":"code","source":"target_list = [1 if i=='Default' else 0 for i in df['loan_status']]\n\ndf['TARGET'] = target_list\ndf['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0772af7a7daa23fdedaa005f3ab23f3afdd3d51c"},"cell_type":"markdown","source":"> This clearly is a case of an imbalanced class problem where the value of class is far less than the other. There are cost function based approaches and sampling based approaches for handling this kind of problem which we will use later so that our model doesn't exhibit high bias while trying to predict if a loan will default or not."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d1865a5563085e9a024a2d49b1ad1b6809eab40"},"cell_type":"code","source":"df.drop('loan_status',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b773eab9957b8ad5b5506abd5bca03812114e982"},"cell_type":"markdown","source":"Then, seeing the distribution of data types we are working with,"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"74e310a4008a6c570c34187e16d15de47feb17ba"},"cell_type":"code","source":"# Number of each type of column\ndf.dtypes.value_counts().sort_values().plot(kind='barh')\nplt.title('Number of columns distributed by Data Types',fontsize=20)\nplt.xlabel('Number of columns',fontsize=15)\nplt.ylabel('Data type',fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f45c6bac306ce3471363306445adf154d1082d8"},"cell_type":"markdown","source":"> So we have quite a number of columns having objects data type which are going to pose a problem while modelling. "},{"metadata":{"_uuid":"9c6d198a6d57749eb23c547824a2c1de7a97e81b"},"cell_type":"markdown","source":"Let us see how many categorical data do the columns having 'object' data types contain:"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a33b79a2e7a06826f6400f72cf4e39f1822d29a3"},"cell_type":"code","source":"df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28ac056c4ae0f5632172a84ff8eb5c54fadac98f"},"cell_type":"markdown","source":">  We would want to label encode the columns having only 2 categorical data and one-hot encode columns with more than 2 categorical data. Also, columns like emp_title, url, desc, etc. should be dropped because there aren't any large number of unique data for any of the categories they contain. Also, Principal Component Analysis can be carried out for the one-hot encoded columns to bring the feature dimensions down."},{"metadata":{"_uuid":"59bfd74eeb2c29184f5d9a28794f0163f0dc391f"},"cell_type":"markdown","source":"## Anomaly Detection"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dc9831550615fc78a1bf722b626a7d1a167b00ce"},"cell_type":"markdown","source":"Let us check for any anomalies on the data we might have. Possible data anamolies are often found in columns dealing with time like years of employment. Let's quickly go through them."},{"metadata":{"trusted":true,"_uuid":"09b08890e74d780077209481c273c606172cc4b0"},"cell_type":"code","source":"df['emp_length'].head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f963b2964302008c7b5df6848d7ace48efbaa93"},"cell_type":"markdown","source":"I'll be filling the null values with 0 assuming that the borrower hasn't worked many years for his data to be recorded. Also, I'll be using regex to extract the number of years from all of the data."},{"metadata":{"trusted":true,"_uuid":"f4a9b10c31c32be99a995fbf3181d82c6a354e02","scrolled":false},"cell_type":"code","source":"df['emp_length'].fillna(value=0,inplace=True)\n\ndf['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\n\ndf['emp_length'].value_counts().sort_values().plot(kind='barh',figsize=(18,8))\nplt.title('Number of loans distributed by Employment Years',fontsize=20)\nplt.xlabel('Number of loans',fontsize=15)\nplt.ylabel('Years worked',fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4ecb0ce275fadb9b5a02a099ddeaf1b85e7d87f8"},"cell_type":"markdown","source":"> The column looks fine. Also, it can be seen that people who have worked for 10 or more years are more likely to take a loan."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a95f04bf915cdf70755b01604ff8ea4d3b8a6854"},"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nsns.violinplot(x=\"TARGET\",y=\"loan_amnt\",data=df, hue=\"pymnt_plan\", split=True)\nplt.title(\"Payment plan - Loan Amount\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Loan Amount\", fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30ac8bb2729535107cb71e5ab0bfb5ffaf464bf3"},"cell_type":"markdown","source":"> Naturally, the defaulted loans had no payment plan"},{"metadata":{"_uuid":"cb45ef7f1ba224f33a7e32247e46e557a479d7bc"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"_uuid":"fb99f8458cbcf8295fcd9685210cb02c90ae2f26"},"cell_type":"markdown","source":"Let me remove all the columns with more than 70% missing data as they won't be helping for modelling and exploration."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"04a7e01890b1945b54fbda359d6fb9c0c770ee9d"},"cell_type":"code","source":"temp = [i for i in df.count()<887379 *0.30]\ndf.drop(df.columns[temp],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e759d609b2b077be7e68822dec2f0da4c4d4afc"},"cell_type":"code","source":"corr = df.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', corr.tail(10))\nprint('\\nMost Negative Correlations:\\n', corr.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd855fe5761dd28d164edf467bcc11811e6c0672"},"cell_type":"markdown","source":"> Besides from the perfect correlation of TARGET column with itself, columns like int_rate which is interest rate, out_prncp_inv which is remaining outstanding principal, etc. have high positive correlation with the TARGET column and these are quite true as higher the interest rate, higher it is harder for a borrower to pay back a loan. However, columns like out_prncp_inv, out_prncp, total_rec_int, total_rec_late_fee, inq_last_6mths and revol_util are bound to be higher when a borrower doesn't pay back a loan and thus doesn't carry much significance. So, the column of interest after int_rate could be the dti which is the Debt to Income ratio which understandably will affect if a borrower can pay back a loan or not.\n\n> Also, columns like recoveries, total_rev_hi_lim, etc. have negative correlation with the TARGET column as a borrower who has paid back money is more likely to repay the loan."},{"metadata":{"_uuid":"710501350bb2bb211ef724a76602a7e73c9c0e2f"},"cell_type":"markdown","source":"Examining further on debt to income ratio and interest rate,"},{"metadata":{"trusted":true,"_uuid":"54fc1e848175f740002e83b97f6857ac7cb51d61","scrolled":true},"cell_type":"code","source":"df.corr()['dti'].sort_values().tail(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8af90003a76117a56061b93e4b980dc3c5bed48a"},"cell_type":"markdown","source":"> It can be seen that the interest rate is also highly positively correlated with the debt to income ratio."},{"metadata":{"_uuid":"176a35c11f7fde5ab3b1ab50fc7f044f05f6cf43"},"cell_type":"markdown","source":"Let us do make some Kernel Density Estimation Plots to see how the interest rate and debt to income ratio are distributed for the two classes in the TARGET column."},{"metadata":{"trusted":true,"_uuid":"10adefebe3c1a972417b8fe7895acd0b46e907b5"},"cell_type":"code","source":"fig = plt.figure(figsize=(22,6))\nsns.kdeplot(df.loc[df['TARGET'] == 1, 'int_rate'], label = 'target = 1')\nsns.kdeplot(df.loc[df['TARGET'] == 0, 'int_rate'], label = 'target = 0');\nplt.xlabel('Interest Rate (%)',fontsize=15)\nplt.ylabel('Density',fontsize=15)\nplt.title('Distribution of Interest Rate',fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fe59a13472e3a49f373edbb911ad572994112820"},"cell_type":"markdown","source":"> The density of interest rates follow kind of a Gaussian distribution with more density on interest rates between 12%-18%."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"125d315a1703f2c9f0521af8b385c8a6141a6f28"},"cell_type":"markdown","source":"While we are looking at distributions, some other distributions that would be interesting to examine are,"},{"metadata":{"_uuid":"7baaa7b653dd5812d90b0eeafc43991699e0a6b9"},"cell_type":"markdown","source":"** Violin-plot of TARGET classes with distribution of loan amount differentiated by the terms. **"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5394b68d274bba20bc5133c8b76bdbc486b59eaf"},"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nsns.violinplot(x=\"TARGET\",y=\"loan_amnt\",data=df, hue=\"term\", split=True,color='pink')\nplt.title(\"Term - Loan Amount\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Loan Amount\", fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98395736864cc1b22b70a47c5208150efd8aff2e"},"cell_type":"markdown","source":"> Most of the Loans of higher terms have high amount and vice versa for the TARGET classes."},{"metadata":{"_uuid":"bec4425767f560b963115817d35d8c176507ef17"},"cell_type":"markdown","source":"** Violin-plot of TARGET classes with distribution of loan amount differentiated by the application type. **"},{"metadata":{"trusted":true,"_uuid":"c215b150da13b8bec5e08344964a67eab8956cae"},"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nsns.violinplot(x=\"TARGET\",y=\"loan_amnt\",data=df, hue=\"application_type\", split=True,color='green')\nplt.title(\"Application Type - Loan Amount\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Loan Amount\", fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ee3eb657b83becdbf25460002708c5050eb17b82"},"cell_type":"markdown","source":"So all the loans that have been defaulted are from individuals rather than from two or more people. "},{"metadata":{"trusted":true,"_uuid":"88369af63a5297618590dec62e3e26c83e87569f"},"cell_type":"code","source":"df['application_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8875278354d03e25b8339b594e80ce7d49bc156d"},"cell_type":"markdown","source":"> Seeing the number of joint applicants in comparison to the the total applicants, it **isn't** significant enough to conclude that the loan taken by all Joint applicants are paid back. "},{"metadata":{"_uuid":"a14d6a84e8b47c278b75ba3f8f8a1eac2abffe71"},"cell_type":"markdown","source":"** Violin-plot of TARGET classes with distribution of interest rate differentiated by the loan grades. **"},{"metadata":{"trusted":true,"_uuid":"1147a2915ff2c061410d905beb3768e12718c39d"},"cell_type":"code","source":"fig = plt.figure(figsize=(18,8))\nsns.violinplot(x=\"TARGET\",y=\"int_rate\",data=df, hue=\"grade\")\nplt.title(\"Grade - Interest Rate\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Interest Rate\", fontsize=15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5107e7922245c561c5e8523265e769800be95750"},"cell_type":"markdown","source":"> Both target classes have similar kind of interest rates by grades."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"28105fdcb490a4f69ab29ad035d56ce129748cd9"},"cell_type":"markdown","source":"Let us also check the correlation of annual income with loan amount taken. "},{"metadata":{"trusted":true,"_uuid":"2961954d34a2e81798521b06ef98bf10c43903ae"},"cell_type":"code","source":"df.corr()['annual_inc'].sort_values().tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6cbbc8a6785a9873377f839ea0419f5fc12165fe"},"cell_type":"markdown","source":"> The annual income of the applicant has high positive correlation with the amount of loan they have taken."},{"metadata":{"_uuid":"288a6b87a1b60e3241a60c23806c271a76ce8571"},"cell_type":"markdown","source":"** From where do most of the loans tend to be defaulted? **"},{"metadata":{"trusted":true,"_uuid":"95de9ba6c0df3129208da815f3e2e4bab918ab64"},"cell_type":"code","source":"fig = plt.figure(figsize=(18,10))\ndf[df['TARGET']==1].groupby('addr_state')['TARGET'].count().sort_values().plot(kind='barh')\nplt.ylabel('State',fontsize=15)\nplt.xlabel('Number of loans',fontsize=15)\nplt.title('Number of defaulted loans per state',fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"166fc5f080061ddec1371f90b8780c91420a7b6c"},"cell_type":"code","source":"fig = plt.figure(figsize=(18,10))\ndf[df['TARGET']==0].groupby('addr_state')['TARGET'].count().sort_values().plot(kind='barh')\nplt.ylabel('State')\nplt.xlabel('Number of loans')\nplt.title('Number of not-defaulted loans per state');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"00d9ea1b27f4be92b906a0c59fbebd2eba6dcb09"},"cell_type":"markdown","source":"> It can be seen that there are more number of loans taken amount from the same states where there are more number of defaulted risk. This is why the state cannot be taken as a major feature for knowing if a loan will be defaulted or not.\n"},{"metadata":{"_uuid":"6c0ec2d78f5b78161020c00ccee485e2a8df066c"},"cell_type":"markdown","source":"Let's see if we have any members taking multiple loans."},{"metadata":{"trusted":true,"_uuid":"eb5f497c6407705e61db380bbbeb47d3f3e03957","scrolled":false},"cell_type":"code","source":"df['member_id'].value_counts().head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"406f05daf4c4cfab1d5929f4fe6f094a9ca7ad18"},"cell_type":"markdown","source":"> Suprisingly there is not a single member taking loan more than once. So, member id column can also be dropped along with the id column."},{"metadata":{"_uuid":"2cdf543a65c0c6756f3ee2fd515cd195c3bf7ba8"},"cell_type":"markdown","source":"# Cleaning the data"},{"metadata":{"_uuid":"cf9a236087dbade5819ca427c6a109a67151c98e"},"cell_type":"markdown","source":"\nAs we had observe, some columns like annual_inc, int_rate, etc. may be much useful for building our model but on the other hand, some columns like id, member_id, etc. will not be helping. \n\nAlso, columns like 'title' and 'emp_title' are text which cannot be one-hot encoded / label encoded as they have arbitrary categorical text and very less unique data for each of their categories."},{"metadata":{"trusted":true,"_uuid":"eaa689271ee0010565234a4fd60b4c59632868e0"},"cell_type":"code","source":"df['emp_title'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83ee13454aee7d36ccea798653ddb9793fe11eb7","collapsed":true},"cell_type":"code","source":"df.drop(['id','member_id','emp_title','title','zip_code','url'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc777666c5e8933a3a17645f47b1d4a3d8aad5fb"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3608347e80ad83fb2a18e1b328b207060cfa045c"},"cell_type":"markdown","source":"So, now we have 48 columns remaining. Let's print them out to get a quick look of what we are dealing with,"},{"metadata":{"trusted":true,"_uuid":"0127a75a3f8717a8f9f4d7b1010d7b884b67664c"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36c016d07ae421efddee9b92970121b2d67c7280"},"cell_type":"markdown","source":"The memory usage is 325+ MB. Some of these columns still look like they could need some work i.e. more cleaning! \n\nI will be fixing the data types and then handling the missing data."},{"metadata":{"_uuid":"f143cc9ddf7fb6672a5fd00b29aff6c9698c9cc9"},"cell_type":"markdown","source":"First, I'll be converting the date object columns into integer number of years or months just because I do not want to blow up the number of feature columns by performing one-hot encoding on them. For filling the null values I have taken the dates with the highest number of counts."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8339d2c1d4816c741c4537f7f6beb713c66e3b75"},"cell_type":"code","source":"df['issue_d']= pd.to_datetime(df['issue_d']).apply(lambda x: int(x.strftime('%Y')))\ndf['last_pymnt_d']= pd.to_datetime(df['last_pymnt_d'].fillna('2016-01-01')).apply(lambda x: int(x.strftime('%m')))\ndf['last_credit_pull_d']= pd.to_datetime(df['last_credit_pull_d'].fillna(\"2016-01-01\")).apply(lambda x: int(x.strftime('%m')))\ndf['earliest_cr_line']= pd.to_datetime(df['earliest_cr_line'].fillna('2001-08-01')).apply(lambda x: int(x.strftime('%m')))\ndf['next_pymnt_d'] = pd.to_datetime(df['next_pymnt_d'].fillna(value = '2016-02-01')).apply(lambda x:int(x.strftime(\"%Y\")))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8269a0a934cd32dadc909b1369a777098e66d26a"},"cell_type":"markdown","source":"\nLet's see how we can handle our categorical data. Two methods we can use are Label Encoding and One Hot Encoding.\n\nThe problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. So, If we only have two unique values for a categorical variable (such as Yes/No), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the better option.\n\nHowever, due to the large number of columns originated after One-Hot Encoding, we may have to conduct Principle Component Analysis (PCA) for dimensionality reduction."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"952730738c174e274ade3975e5571c1e746a761c"},"cell_type":"code","source":"from sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4607079c9ece6ce57762226e30446367fff30dcc"},"cell_type":"code","source":"count = 0\n\nfor col in df:\n    if df[col].dtype == 'object':\n        if len(list(df[col].unique())) <= 2:     \n            le = preprocessing.LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n            count += 1\n            print (col)\n            \nprint('%d columns were label encoded.' % count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffe8aa51d3aa5c1f619f55642d56c7975076315e"},"cell_type":"markdown","source":"And one-hot encoding the rest categorical columns,"},{"metadata":{"trusted":true,"_uuid":"14780d96592d3516cbc2a16faa156fd67f37178b","scrolled":true},"cell_type":"code","source":"df = pd.get_dummies(df)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba5f8e0d3a56553a7429e7cac0bbc0c79806e7b4"},"cell_type":"markdown","source":"For the 'mths_since_last_delinq' column, I'll be filling in the missing value with the median of the columns as the data in the column is continuous."},{"metadata":{"trusted":true,"_uuid":"b0fa6ea4726bf478e2a3a095efaa5212b2287bfd","collapsed":true},"cell_type":"code","source":"df['mths_since_last_delinq'] = df['mths_since_last_delinq'].fillna(df['mths_since_last_delinq'].median())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e31f8eebf31200840fed444f8411f1dd001976a5"},"cell_type":"markdown","source":"However for columns like 'total_rev_hi_lim','tot_col_ammnt',etc. , I won't be filling in the missing data because they will certainly be of high feature importance due to their description. If they do not seem to be of high importance we can always re-iterate and fill the missing values later. \n\nSo, dropping all remaining null values,"},{"metadata":{"trusted":true,"_uuid":"deeb718addb1fc6f0a6fc06fb955b69eb3169bf2","collapsed":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6643d96fa107c22436b046a85465d6bd1f88f793"},"cell_type":"markdown","source":"And checking the count,"},{"metadata":{"trusted":true,"_uuid":"4cea8e1190b426ce4931b451b356c7ff8c351014"},"cell_type":"code","source":"df.count().sort_values().head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f033f97624802b521b8892826793f00429947b82"},"cell_type":"code","source":"df['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfb834d8f791d5e08822d1511bd69abee6878866"},"cell_type":"markdown","source":"We are now left with a reasonable amount of data for modelling.\n\n---"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0d2f59985dbca1746a10b44bdd5dd62660c35d16"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"_uuid":"82cad6592663ae114303aead155f1ccf167ce32b"},"cell_type":"markdown","source":"Now, for modeling I will be using two ensemble methods and comparing them.\n\ni) Bootstrap Aggregrating or Bagging\n\nii) Boosting"},{"metadata":{"_uuid":"728db0b5e5f15eda02ab3a2bd11f12e4b135aab1"},"cell_type":"markdown","source":"# 1) Bagging - Random Forest\n\n* Ensemble of Decision Trees\n\n* Training via the bagging method (Repeated sampling with replacement)\n  * Bagging: Sample from samples\n  * RF: Sample from predictors. $m=sqrt(p)$ for classification and $m=p/3$ for regression problems.\n\n* Utilise uncorrelated trees"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b2a40b5f4d8603d185c993de581ab60f29ebffb0"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"642af35915e508aaef47c6e5f2feba1c323eb099"},"cell_type":"markdown","source":"Creating a classification report function,"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"da4cf4ea3fa80740321b1344a311d560a78f2ea3"},"cell_type":"code","source":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        print(\"Train Result:\\n\")\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n    elif train==False:\n        print(\"Test Result:\\n\")        \n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))    \n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30fe6c04f194a31d517a7656af06241343d4e4a6"},"cell_type":"markdown","source":"Conducting train test split."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bfde34d1796e2f5fa1910fde18ad831b930599bc"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6e018cdff71158d658145afc397cf5a4855ec84","collapsed":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.drop('TARGET',axis=1),df['TARGET'],test_size=0.15,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01cd9f36048e47a45882efd244cf6de705b7e015"},"cell_type":"markdown","source":"Freeing up the memory."},{"metadata":{"trusted":true,"_uuid":"b1d069f61ba00d9eab3e776993ab703b87bfb8af","collapsed":true},"cell_type":"code","source":"del start_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34ec277688a7e781775dcf4884552597337b24d0"},"cell_type":"markdown","source":"Standardizing features by removing the mean and scaling to unit variance"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6210def8ab1c05905131dbbefb94d9010b459ed9"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d7af86ed67ed7a016c5655ac132378f327adc52","collapsed":true},"cell_type":"code","source":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dc343c7d9a2c4cfa655c5be62f879df139e3628"},"cell_type":"markdown","source":"Oversampling only the training set using Synthetic Minority Oversampling Technique ([SMOTE](https://jair.org/index.php/jair/article/view/10302))"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1d20e1559b50c1837b2ec2be7c5edf1b62b829df"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af1c9017ed7ee69d3c1d44ac4617441d607c05c2","scrolled":false,"collapsed":true},"cell_type":"code","source":"sm = SMOTE(random_state=12, ratio = 1.0)\nx_train_r, y_train_r = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5948fc8f16e5ddcb2342faad61bea265b895b5f4"},"cell_type":"markdown","source":"Now, I'll be trying out different models to get the best prediction score."},{"metadata":{"_uuid":"8623e31c14641298d9d914d7d8846d989a95ff43"},"cell_type":"markdown","source":"**Creating a baseline for accuracy and recall using Logistic regression, **"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2b0a73a4f3a6c4f1c0beb1afad84dd4e7e7a4d46","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(C = 0.0001,random_state=21)\n\nlog_reg.fit(x_train_r, y_train_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"167bf11153597d53c93829f01764fa195d85103d","collapsed":true},"cell_type":"code","source":"print_score(log_reg, x_train_r, y_train_r, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d532ff91b9386eb43a3adaa03539c3a03ca6ea9e"},"cell_type":"markdown","source":"The accuracy came out to be satisfactory for the baseline along with the recall score. However, precision seems to be very off."},{"metadata":{"_uuid":"58f5b1e9477f757d173bf3c89d6b009652a432da"},"cell_type":"markdown","source":"For our case, overfitting will be a huge concern. So, I'm using Random Forest as it is known to decrease overfitting by selecting features at random. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"82d54fd81da66b23e320293fc87254e96c4c8ecb"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdf0316bc64dd6539b2a78220c6c64ef0cf3a46f","collapsed":true},"cell_type":"code","source":"clf_rf = RandomForestClassifier(n_estimators=40, random_state=21)\nclf_rf.fit(x_train_r, y_train_r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d0849504f79e309e919a374a472fb5d7d991496","collapsed":true},"cell_type":"code","source":"print_score(clf_rf, x_train_r, y_train_r, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2ee1c2854ad5782ad84f112c312e930c584ed25"},"cell_type":"markdown","source":"We have high precision but a low recall for our validation set. Using this model is not a good idea as most of our default loans will be falsely classified."},{"metadata":{"_uuid":"91845ea2928981a642bef479cb96c3c1118f9b46"},"cell_type":"markdown","source":"## 2) Boosting:\n\n* Train weak classifiers \n* Add them to a final strong classifier by weighting. Weighting by accuracy (typically)\n* Once added, the data are reweighted\n  * Misclassified samples gain weight \n  * Algo is forced to learn more from misclassified samples    "},{"metadata":{"_uuid":"dcd4818bb3b9c0e08203f9b59f63e8cb2f395a59"},"cell_type":"markdown","source":"For boosting I will be using the [LightGBM](https://www.youtube.com/watch?v=5CWwwtEM2TA) classifier (evalulation metric as AUC) along with [Kfold cross validation](https://www.youtube.com/watch?v=TIgfjmp-4BA)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"221b1dc522f5be202fa27994381f298a40db669b"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"810cc9cd7c64bc2566ee9bdd908f4fceff7ecafe"},"cell_type":"markdown","source":"Function to use LightGBM with Kfold cross validation,"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c55f80a821b9369c88390ea5dafa50ab853a0fd2"},"cell_type":"code","source":"def kfold_lightgbm(train_df, num_folds, stratified = False):\n    print(\"Starting LightGBM. Train shape: {}\".format(train_df.shape))\n    \n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=47)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n\n    oof_preds = np.zeros(train_df.shape[0])\n\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['TARGET']]\n    \n    # Splitting the training set into folds for Cross Validation\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=32,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.04,\n            reg_lambda=0.073,\n            min_split_gain=0.0222415,\n            min_child_weight=40,\n            silent=-1,\n            verbose=-1,\n            )\n\n        # Fitting the model and evaluating by AUC\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric= 'auc', verbose= 1000, early_stopping_rounds= 200)\n        print_score(clf, train_x, train_y, valid_x, valid_y, train=False)\n        # Dataframe holding the different features and their importance\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n        # Freeing up memory\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    display_importances(feature_importance_df)\n    return feature_importance_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c5f63c320fbcd75200d55c055785b3a35d57931"},"cell_type":"markdown","source":"Function for displaying the importance of the features,"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"edd0e7802d06196b7ca7ee24f298b2366793573a"},"cell_type":"code","source":"def display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(15, 12))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf1a03409aa1023066d4dbbcfdb9b5bd38f1a51e"},"cell_type":"code","source":"feat_importance = kfold_lightgbm(df, num_folds= 3, stratified= False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baa1b31d1b2a24e84e4a47c60416ae8f3a64638c"},"cell_type":"markdown","source":"As we can see, LightGBM did a great job for getting high precision as well as a high recall. Hence, this model is the best in terms of the 3 models that we evaluated."},{"metadata":{"_uuid":"ec40193cd5db060c89aacf2ab6c839550beb432b"},"cell_type":"markdown","source":"For further enhancements to the model, feature engineering could be done. Also a broader term like 'good loan' and 'bad loan' could have been used by encompassing different loan statuses together to get a more balanced counts of classes rather than default/non-default."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}