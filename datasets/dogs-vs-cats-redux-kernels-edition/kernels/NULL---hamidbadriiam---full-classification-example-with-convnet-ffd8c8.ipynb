{"cells":[{"metadata":{"_cell_guid":"be4faf86-56fa-bd74-403d-b86fb9e98c5c","_uuid":"b1fec00303910d5c75931800a6976bae1e894a15"},"cell_type":"markdown","source":"## Full run through of raw images to classification with Convolutional Neural Network ##\n\nIn this tutorial, we're going to be running through taking raw images that have been labeled for us already, and then feeding them through a convolutional neural network for classification. \n\nThe images are either of dog(s) or cat(s). \n\nOnce you have downloaded and extracted the data from https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data, you're ready to begin."},{"metadata":{"_cell_guid":"1e6df50d-d318-7398-304e-b0ae37f60e17","_uuid":"56bf488f1ef6fa610ac2eae6d2f664e01fa49ad2"},"cell_type":"markdown","source":"## Part 1 - Preprocessing ##\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gT4F3HGYXf4\" frameborder=\"0\" allowfullscreen></iframe>\n\nWe've got the data, but we can't exactly just stuff raw images right through our convolutional neural network. First, we need all of the images to be the same size, and then we also will probably want to just grayscale them. Also, the labels of \"cat\" and \"dog\" are not useful, we want them to be one-hot arrays. \n\nInterestingly, we may be approaching a time when our data might not need to be all the same size. Looking into TensorFlow's research blog: https://research.googleblog.com/2017/02/announcing-tensorflow-fold-deep.html\n\n\"TensorFlow Fold makes it easy to implement deep-learning models that operate over data of varying size and structure.\"\n\nFascinating...but, for now, we'll do it the old fashioned way.\n\n**Package Requirements**\nnumpy (pip install numpy)\ntqdm (pip install tqdm)\n\nI will be using the GPU version of TensorFlow along with tflearn. \n\nTo install the CPU version of TensorFlow, just do pip install tensorflow\nTo install the GPU version of TensorFlow, you need to get alllll the dependencies and such.\n\n**TensorFlow Installation tutorials:**\n\nhttps://pythonprogramming.net/how-to-cuda-gpu-tensorflow-deep-learning-tutorial/\n\nTensorFlow on Windows: https://www.youtube.com/watch?v=r7-WPbx8VuY\n\n**Using TensorFlow and concept tutorials:**\n\nIntroduction to deep learning with neural networks: https://pythonprogramming.net/neural-networks-machine-learning-tutorial\n\nIntroduction to TensorFlow: https://pythonprogramming.net/tensorflow-introduction-machine-learning-tutorial/\n\nIntro to Convolutional Neural Networks: https://pythonprogramming.net/convolutional-neural-network-cnn-machine-learning-tutorial/\n\nConvolutional Neural Network in TensorFlow tutorial: https://pythonprogramming.net/cnn-tensorflow-convolutional-nerual-network-machine-learning-tutorial/\n\nFinally, I will be making use of https://pythonprogramming.net/tflearn-machine-learning-tutorial/. Once you have TensorFlow installed, do pip install tflearn.\n\nFirst, we'll get our imports and constants for preprocessing:\n"},{"metadata":{"_cell_guid":"d5afc7e8-ef5a-c672-4f51-c469ed97be86","_uuid":"21d1fab98708a56319e65aca984f726b65188fee","trusted":false},"cell_type":"code","source":"import cv2                 # working with, mainly resizing, images\nimport numpy as np         # dealing with arrays\nimport os                  # dealing with directories\nfrom random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\nfrom tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel Bühler for this suggestion\n\nTRAIN_DIR = '../input/train'\nTEST_DIR = '../input/test'\nIMG_SIZE = 50\nLR = 1e-3\n\nMODEL_NAME = 'dogsvscats-{}-{}.model'.format(LR, '2conv-basic') # just so we remember which saved model is which, sizes must match","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"da647f1a-7011-1555-6013-3e0e0126b246","_uuid":"291c42db1696376566ffbad0b57c343698f67bde"},"cell_type":"markdown","source":"Now, our first order of business is to convert the images and labels to array information that we can pass through our network. To do this, we'll need a helper function to convert the image name to an array. \n\nOur images are labeled like \"cat.1\" or \"dog.3\" and so on, so we can just split out the dog/cat, and then convert to an array like so:"},{"metadata":{"_cell_guid":"d1c5fbd5-94a3-1c60-427d-883601dbd461","_uuid":"e511717c31dead00450c4863e6f47efbae5fac8a","trusted":false},"cell_type":"code","source":"def label_img(img):\n    word_label = img.split('.')[-3]\n    # conversion to one-hot array [cat,dog]\n    #                            [much cat, no dog]\n    if word_label == 'cat': return [1,0]\n    #                             [no cat, very doggo]\n    elif word_label == 'dog': return [0,1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e1eafd3-99b2-7c7a-0164-31b3efa1e39a","_uuid":"1ac9478543a43e51c516a56a04efc6e955ff4509"},"cell_type":"markdown","source":"Now, we can build another function to fully process the training images and their labels into arrays:"},{"metadata":{"_cell_guid":"246e8504-fc0a-00ac-34d2-ea6b13af1732","_uuid":"6222d049f99e81d5a9177c895a6138bdba8f51f9","trusted":false},"cell_type":"code","source":"def create_train_data():\n    training_data = []\n    for img in tqdm(os.listdir(TRAIN_DIR)):\n        label = label_img(img)\n        path = os.path.join(TRAIN_DIR,img)\n        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        training_data.append([np.array(img),np.array(label)])\n    shuffle(training_data)\n    np.save('train_data.npy', training_data)\n    return training_data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4935379-6d26-818a-b4a2-d92b3a3ee2a3","_uuid":"f59c8bee9c1dc144df5fcb6cefed3b644105dc3e"},"cell_type":"markdown","source":"The tqdm module was introduced to me by one of my viewers, it's a really nice, pretty, way to measure where you are in a process, rather than printing things out at intervals...etc, it gives a progress bar. Super neat. \n\nAnyway, the above function converts the data for us into array data of the image and its label. \n\nWhen we've gone through all of the images, we shuffle them, then save. Shuffle modifies a variable in place, so there's no need to re-define it here. \n\nWith this function, we will both save, and return the array data. This way, if we just change the neural network's structure, and not something with the images, like image size..etc..then we can just load the array file and save some processing time. While we're here, we might as well also make a function to process the testing data. This is the *actual* competition test data, NOT the data that we'll use to check the accuracy of our algorithm as we test. This data has no label. "},{"metadata":{"_cell_guid":"277478aa-2ffa-eb36-f0ec-2fe04743124a","_uuid":"3246ad44162eadc02ad6cff2062a9881606f58f8","trusted":false},"cell_type":"code","source":"def process_test_data():\n    testing_data = []\n    for img in tqdm(os.listdir(TEST_DIR)):\n        path = os.path.join(TEST_DIR,img)\n        img_num = img.split('.')[0]\n        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        testing_data.append([np.array(img), img_num])\n        \n    shuffle(testing_data)\n    np.save('test_data.npy', testing_data)\n    return testing_data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e3beebd-676c-3884-75df-77d42b255ad0","_uuid":"8cfcad3ceafc7357ed5c9288cc07568b45d646f0"},"cell_type":"markdown","source":"Now, we can run the training:"},{"metadata":{"_cell_guid":"62b30187-0d69-13e9-5829-4b05962e5e79","_uuid":"4857e8140a2d3d081ff146f747eb52bb57936054","trusted":false},"cell_type":"code","source":"train_data = create_train_data()\n# If you have already created the dataset:\n#train_data = np.load('train_data.npy')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53cacc97-6c0e-e5ca-dc63-5eae37ca03d5","_uuid":"89ccabdd442662232689254a4a27e45ae84d5f6a"},"cell_type":"markdown","source":"## Convolutional Neural Network ##\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ge65ukmJTzQ\" frameborder=\"0\" allowfullscreen></iframe>\n\nNext, we're ready to define our neural network:"},{"metadata":{"_cell_guid":"6dfcdcae-fc1b-06de-875b-3b853f95ca55","_uuid":"d94b603e3748b2a15f41cd402bfbd4da34fc6c28","trusted":false},"cell_type":"code","source":"import tflearn\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.estimator import regression\n\nconvnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = fully_connected(convnet, 1024, activation='relu')\nconvnet = dropout(convnet, 0.8)\n\nconvnet = fully_connected(convnet, 2, activation='softmax')\nconvnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n\nmodel = tflearn.DNN(convnet, tensorboard_dir='log')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aed07464-bd4a-1321-c772-2a3cb252a7e4","_uuid":"8efe3ab0e41954a506b9edd25ce99384dcba4d3c"},"cell_type":"markdown","source":"What we have here is a nice, 2 layered convolutional neural network, with a fully connected layer, and then the output layer. It's been debated whether or not a fully connected layer is of any use. I'll leave it in anyway. \n\nThis exact convnet was good enough for recognizing hand 28x28 written digits. Let's see how it does with cats and dogs at 50x50 resolution. \n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ViO56ASqeks\" frameborder=\"0\" allowfullscreen></iframe>"},{"metadata":{"_cell_guid":"e418f3b2-fb65-ab5a-3862-2ca2ab438bf4","_uuid":"7a0e03f366ff04afdaee2cbe24bd0e541aa43934"},"cell_type":"markdown","source":"Now, it wont always be the case that you're training the network fresh every time. Maybe first you just want to see how 3 epochs trains, but then, after 3, maybe you're done, or maybe you want to see about 5 epochs. We want to be saving our model after every session, and reloading it if we have a saved version, so I will add this:"},{"metadata":{"_cell_guid":"c6949aec-7b7a-297e-43d0-9a868d6c9b60","_uuid":"6c8972c8fa86f452504f3ec0899a50703017d4c0","trusted":false},"cell_type":"code","source":"if os.path.exists('{}.meta'.format(MODEL_NAME)):\n    model.load(MODEL_NAME)\n    print('model loaded!')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42c74821-c028-3e04-f04f-398f0531ef86","_uuid":"72b5e54f9f9ae2b0b7805e7f6fe66df7982d77b1"},"cell_type":"markdown","source":"Now, let's split out training and testing data:"},{"metadata":{"_cell_guid":"2770fddb-4083-8a0a-7743-9753620cb681","_uuid":"9783c2e656725d13523de4b6ddfdeb84eb2f9268","trusted":false},"cell_type":"code","source":"train = train_data[:-500]\ntest = train_data[-500:]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5fab0abb-7783-cfb9-75c4-6d2ad8e2dcf2","_uuid":"bdbaf00c69516d8fd78a73f0f5195abcdb1ce056"},"cell_type":"markdown","source":"Now, the training data and testing data are both labeled datasets. The training data is what we'll fit the neural network with, and the test data is what we're going to use to validate the results. The test data will be \"out of sample,\" meaning the testing data will only be used to test the accuracy of the network, not to train it. \n\nWe also have \"test\" images that we downloaded. THOSE images are not labeled at all, and those are what we'll submit to Kaggle for the competition.\n\nNext, we're going to create our data arrays. For some reason, typical numpy logic like:\n\narray[:,0] and array[:,1] did NOT work for me here. Not sure what I'm doing wrong, so I do this instead to separate my features and labels:"},{"metadata":{"_cell_guid":"bd7c2022-a8e5-6a7f-e8e8-244d97b1c141","_uuid":"2f6e663efec4f0f59933062be4c6b8ce8fccbf55","trusted":false},"cell_type":"code","source":"X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\nY = [i[1] for i in train]\n\ntest_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\ntest_y = [i[1] for i in test]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e7e1dd3a-2681-9838-cd6b-fc565094f2d9","_uuid":"33fac2363796d90b27b2066386cacc126bf7dbd7"},"cell_type":"markdown","source":"Now we fit for 2 epochs:"},{"metadata":{"_cell_guid":"7d5325d3-2a96-53fd-952f-d353660abde8","_uuid":"2223f22528c7634b69c55d8245a918555c32cd10","trusted":false},"cell_type":"code","source":"model.fit({'input': X}, {'targets': Y}, n_epoch=2, validation_set=({'input': test_x}, {'targets': test_y}), \n    snapshot_step=50000, show_metric=True, run_id=MODEL_NAME)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b92bc0c4-c153-69c2-0b17-b810906d0d7b","_uuid":"0ca1c42866393eede03352ca672ada5e4ce45e6d"},"cell_type":"markdown","source":"Hmm... it doesn't look like we've gotten anywhere at all. "},{"metadata":{"_cell_guid":"4ebe7b41-88d9-f5a5-f3b1-336bf0469fa6","_uuid":"0ac8d428640c1ab45752b96efef8b2a32c7ad5bb"},"cell_type":"markdown","source":"We could keep trying, but, if you haven't made accuracy progress in the first 3 epochs, you're probably not going to at all, unless it's due to overfitment...at least in my experience. \n\nSo... now what?"},{"metadata":{"_cell_guid":"64495318-5893-0917-7e6e-f214384ce6e7","_uuid":"e7b1ebc213a65bc10f66a8bc61235ebeeab4450d"},"cell_type":"markdown","source":"## Size Matters ##\n\nWe're gonna need a bigger network\n\nFirst, we need to reset the graph instance, since we're doing this in a continuous environment:"},{"metadata":{"_cell_guid":"5485d475-bc57-ba04-939b-dcb5eeb21fbc","_uuid":"03c94c4a14189f3b7a3f5fd1a4e228c3354a1b47","trusted":false},"cell_type":"code","source":"import tensorflow as tf\ntf.reset_default_graph()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"13516279-c02c-9139-72ff-ba3e489c3c08","_uuid":"4fe058dcb7fd56513a2483d221af2f75c6d399cf","trusted":false},"cell_type":"code","source":"convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 32, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = conv_2d(convnet, 64, 5, activation='relu')\nconvnet = max_pool_2d(convnet, 5)\n\nconvnet = fully_connected(convnet, 1024, activation='relu')\nconvnet = dropout(convnet, 0.8)\n\nconvnet = fully_connected(convnet, 2, activation='softmax')\nconvnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n\nmodel = tflearn.DNN(convnet, tensorboard_dir='log')\n\n\n\nif os.path.exists('{}.meta'.format(MODEL_NAME)):\n    model.load(MODEL_NAME)\n    print('model loaded!')\n\ntrain = train_data[:-500]\ntest = train_data[-500:]\n\nX = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\nY = [i[1] for i in train]\n\ntest_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\ntest_y = [i[1] for i in test]\n\nmodel.fit({'input': X}, {'targets': Y}, n_epoch=4, validation_set=({'input': test_x}, {'targets': test_y}), \n    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"993c22d5-29dd-e022-44ea-0b114acb2505","_uuid":"88c6f44d97eb04f7023f9857a8f7f79e0ca09a9a"},"cell_type":"markdown","source":"WELL WELL WELL... Looks like we've got a winner. With neural networks, size matters a ton. We went from having apparently un-trainable data to having obviously trainable data, and this was only 3 epochs. \n\nIf you are happy with the model, go ahead and save it:"},{"metadata":{"_cell_guid":"ccd6d567-ab2a-15ce-29e2-62074df9c0c6","_uuid":"a673f0158ad4a188e58465c68a8fcd2278b2e1bc","trusted":false},"cell_type":"code","source":"model.save(MODEL_NAME)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dd467f72-9f39-9d99-c0b9-db8fa020528e","_uuid":"67ead891f4908bb922db663c416fe7602b058b8f"},"cell_type":"markdown","source":"Now we can reload the model, and continue training (we don't NEED to reload the model here since this is continuous and the model is still in memory, but if you were running this as a program you would)"},{"metadata":{"_cell_guid":"4c591c48-2985-f138-5315-7054e99b147c","_uuid":"f1008f93ab20ddadd67dab4ab0c0b37aea42bcc9"},"cell_type":"markdown","source":"## You can be too big##\n\nBigger is not always better, there does get to be a limit, at least from my experience. A bigger network figures things out better, and quicker, but tends to also overfit the training data. You can use dropout (sets randomly a certain % of nodes to not take part in the network for more robusts networks) to rectify this slightly, but there does seem to be a limit. \n\nOkay, now what? Let's see how we've done!"},{"metadata":{"_cell_guid":"0b7137d8-6ba2-d36a-e151-923cb54cb8b8","_uuid":"17b9ec622873bf0cc1bb6c5df5bf8de6e15a8950"},"cell_type":"markdown","source":"## Visually inspecting our network against unlabeled data ##\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/27FPv1VHSsQ\" frameborder=\"0\" allowfullscreen></iframe>"},{"metadata":{"_cell_guid":"32f6bded-827a-f928-b567-cb2c1180ec7d","_uuid":"7fc9538e57e78779194534c8a6f5df853cc394c9","trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# if you need to create the data:\ntest_data = process_test_data()\n# if you already have some saved:\n#test_data = np.load('test_data.npy')\n\nfig=plt.figure()\n\nfor num,data in enumerate(test_data[:12]):\n    # cat: [1,0]\n    # dog: [0,1]\n    \n    img_num = data[1]\n    img_data = data[0]\n    \n    y = fig.add_subplot(3,4,num+1)\n    orig = img_data\n    data = img_data.reshape(IMG_SIZE,IMG_SIZE,1)\n    #model_out = model.predict([data])[0]\n    model_out = model.predict([data])[0]\n    \n    if np.argmax(model_out) == 1: str_label='Dog'\n    else: str_label='Cat'\n        \n    y.imshow(orig,cmap='gray')\n    plt.title(str_label)\n    y.axes.get_xaxis().set_visible(False)\n    y.axes.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"13e156c7-277d-abe1-fb52-7f2dd228245d","_uuid":"aa7adefc2a9641c004e6bff7552bb99bffc961e9"},"cell_type":"markdown","source":"Alright, so we made a couple mistakes, but not too bad actually! \n\nIf you're happy with it, let's compete!"},{"metadata":{"_cell_guid":"52d2f5e2-c47e-460d-5b92-dbe0cfd42864","_uuid":"e4221c9e8c0b9f4d3d0e77723fd3bb3ec243d69d","trusted":false},"cell_type":"code","source":"with open('submission_file.csv','w') as f:\n    f.write('id,label\\n')\n            \nwith open('submission_file.csv','a') as f:\n    for data in tqdm(test_data):\n        img_num = data[1]\n        img_data = data[0]\n        orig = img_data\n        data = img_data.reshape(IMG_SIZE,IMG_SIZE,1)\n        model_out = model.predict([data])[0]\n        f.write('{},{}\\n'.format(img_num,model_out[1]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3a087c2c-b01e-0c0a-04a6-38796c2e5882","_uuid":"34a4bebea1f7e16dc139721b4ceeecdfe67f3777"},"cell_type":"markdown","source":"Heading to Kaggle > Competitions > Dogs vs. Cats Redux: Kernels Edition... Let's submit!\n\nThis got me ~700th place with a logloss of 0.55508 when trained out to 10 epochs"},{"metadata":{"_cell_guid":"30df960f-ba10-e18b-358a-3d5a2e994d83","_uuid":"3d8499e87c76f6e7e843720539fbb00485db9b40","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}