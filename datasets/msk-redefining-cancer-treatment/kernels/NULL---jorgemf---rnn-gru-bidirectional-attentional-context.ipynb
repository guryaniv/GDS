{"metadata": {"hide_input": false, "language_info": {"name": "python", "version": "3.6.2", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "cells": [{"metadata": {}, "source": ["# RNN + GRU + bidirectional + Attentional context\n", "\n", "This kernes uses a recurrent neural network in keras that uses GRU cells with a bidirectional layer and an attention context layer. The model uses the begining of the text and the end of the text and them join both outputs along with an one hot encoded layer for the gene and another for the variation. The variation has been encoded using the first and the last letter.\n", "\n", "This kernel is based in a kernel by [ReiiNakanoBasic](https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm)."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "ef08df1c-3c94-4f63-bbc7-6cc93f46fb52", "_execution_state": "idle", "_uuid": "7b62b6d64cc54d675b18c29ee12eed7cd45a3154"}, "source": ["%matplotlib inline\n", "import pandas as pd\n", "import numpy as np\n", "\n", "from sklearn.model_selection import cross_val_predict\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import log_loss, accuracy_score\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.svm import SVC\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.preprocessing import LabelEncoder\n", "\n", "import gensim\n", "\n", "import scikitplot.plotters as skplt\n", "\n", "import nltk\n", "\n", "import os"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "0b39f26e-9c4c-46db-8fe5-86967734c0e5", "_uuid": "58b74d086e6f6f067f337470219be9fd43211c08"}, "source": ["## Load data"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"scrolled": true, "_cell_guid": "011dde04-0160-41cd-9149-5aeac26295fb", "_execution_state": "idle", "_uuid": "1b07b556871714b93d70806d58b5225be507e716"}, "source": ["df_train_txt = pd.read_csv('../input/training_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "df_train_var = pd.read_csv('../input/training_variants')\n", "df_val_txt = pd.read_csv('../input/test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "df_val_var = pd.read_csv('../input/test_variants')\n", "\n", "df_test_txt = pd.read_csv('../input/stage2_test_text.csv', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "df_test_var = pd.read_csv('../input/stage2_test_variants.csv')"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {}, "source": ["df_val_txt = pd.read_csv('../input/test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "df_val_var = pd.read_csv('../input/test_variants')\n", "df_val_labels = pd.read_csv('../input/stage1_solution_filtered.csv')\n", "df_val_labels['Class'] = pd.to_numeric(df_val_labels.drop('ID', axis=1).idxmax(axis=1).str[5:])\n", "df_val_labels = df_val_labels[['ID', 'Class']]\n", "df_val_txt = pd.merge(df_val_txt, df_val_labels, how='left', on='ID')"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {}, "source": ["df_train = pd.merge(df_train_var, df_train_txt, how='left', on='ID')\n", "df_train.head()\n", "df_test = pd.merge(df_test_var, df_test_txt, how='left', on='ID')\n", "df_test.head()\n", "df_val = pd.merge(df_val_var, df_val_txt, how='left', on='ID')\n", "df_val = df_val[df_val_txt['Class'].notnull()]\n", "df_val.head()"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "f6abf33d-337d-4be8-8d50-34d2292a4c71", "_uuid": "525e0ac244f14d7dbc0edfe5783c1439a3a10d3d"}, "source": ["## Word2Vec model"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "609177df-1799-4f5e-b3e7-77c820561a7c", "_execution_state": "busy", "_uuid": "d631c05ece2e126a82481fa5a262d12ec7577e38"}, "source": ["class MySentences(object):\n", "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n", "    \n", "    Takes a list of numpy arrays containing documents.\n", "    \n", "    Args:\n", "        arrays: List of arrays, where each element in the array contains a document.\n", "    \"\"\"\n", "    def __init__(self, *arrays):\n", "        self.arrays = arrays\n", " \n", "    def __iter__(self):\n", "        for array in self.arrays:\n", "            for document in array:\n", "                for sent in nltk.sent_tokenize(document):\n", "                    yield nltk.word_tokenize(sent)\n", "\n", "def get_word2vec(sentences, location):\n", "    \"\"\"Returns trained word2vec\n", "    \n", "    Args:\n", "        sentences: iterator for sentences\n", "        \n", "        location (str): Path to save/load word2vec\n", "    \"\"\"\n", "    if os.path.exists(location):\n", "        print('Found {}'.format(location))\n", "        model = gensim.models.Word2Vec.load(location)\n", "        return model\n", "    \n", "    print('{} not found. training model'.format(location))\n", "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=8)\n", "    print('Model done training. Saving to disk')\n", "    model.save(location)\n", "    return model"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "e62cf926-34d9-461a-bf92-21e38e9ff2d3", "_execution_state": "busy", "_uuid": "80d89a6184209c1630976aa5bdb9a93b1290b363"}, "source": ["w2vec = get_word2vec(\n", "    MySentences(\n", "        df_train['Text'].values, \n", "        df_val['Text'].values\n", "    ),\n", "    'w2vmodel'\n", ")"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "591cc8cb-49c2-41d0-be0a-ff8ee7d4f230", "_uuid": "b41f276a240b388d56fb5187a5b99ac0d3b76d59"}, "source": ["### Tokenizer\n", "We'll define a transformer (with sklearn interface) to convert a document into its corresponding vector"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "8be99804-9357-4522-a80b-aa328f5ef973", "_execution_state": "busy", "_uuid": "f188944da320461ad2e8f76a7a991454e2c0763c"}, "source": ["class MyTokenizer:\n", "    def __init__(self):\n", "        pass\n", "    \n", "    def fit(self, X, y=None):\n", "        return self\n", "    \n", "    def transform(self, X):\n", "        transformed_X = []\n", "        for document in X:\n", "            tokenized_doc = []\n", "            for sent in nltk.sent_tokenize(document):\n", "                tokenized_doc += nltk.word_tokenize(sent)\n", "            transformed_X.append(np.array(tokenized_doc))\n", "        return np.array(transformed_X)\n", "    \n", "    def fit_transform(self, X, y=None):\n", "        return self.transform(X)\n", "\n", "class MeanEmbeddingVectorizer(object):\n", "    def __init__(self, word2vec):\n", "        self.word2vec = word2vec\n", "        # if a text is empty we should return a vector of zeros\n", "        # with the same dimensionality as all the other vectors\n", "        self.dim = len(word2vec.wv.syn0[0])\n", "\n", "    def fit(self, X, y=None):\n", "        return self\n", "\n", "    def transform(self, X):\n", "        X = MyTokenizer().fit_transform(X)\n", "        \n", "        return np.array([\n", "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n", "                    or [np.zeros(self.dim)], axis=0)\n", "            for words in X\n", "        ])\n", "    \n", "    def fit_transform(self, X, y=None):\n", "        return self.transform(X)\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "7db18b6e-d23f-412a-9f16-429082572649", "_uuid": "934ddee5cd73ca17ee2bd73f647290697aca997c"}, "source": ["## RNN in Keras\n", "We use a vocabulary of 10000 most used words and a sequence length of 3000 words (3000 for the beggining and 3000 for the ending)\n", "\n", "This takes about few hours to run on GPU"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "d1b0a192-ea18-419e-9ba4-bb52f12b9f78", "_execution_state": "busy", "_uuid": "8d4ee34e1e95faa8be9d93ecced116c324b61465"}, "source": ["from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "# Use the Keras tokenizer\n", "VOCABULARY_SIZE = 10000\n", "SEQUENCE_LENGTH = 3000\n", "tokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\n", "tokenizer.fit_on_texts(df_train['Text'].values)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "0610df13-711b-4d28-810a-2f7361cea388", "_execution_state": "busy", "_uuid": "2fba830b1fd73426d1b3b2f22bac503d91d0a923"}, "source": ["# Train set\n", "train_set = df_train.sample(frac=1) # shuffle data first\n", "train_set_input = tokenizer.texts_to_sequences(train_set['Text'].values)\n", "train_set_input_reverse = [list(reversed(x)) for x in train_set_input]\n", "train_set_input_begin = pad_sequences(train_set_input, maxlen=SEQUENCE_LENGTH)\n", "train_set_input_end = pad_sequences(train_set_input_reverse, maxlen=SEQUENCE_LENGTH)\n", "train_set_output = pd.get_dummies(train_set['Class']).values\n", "print(train_set_input_begin.shape, train_set_input_end.shape, train_set_output.shape)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "0610df13-711b-4d28-810a-2f7361cea388", "_execution_state": "busy", "_uuid": "2fba830b1fd73426d1b3b2f22bac503d91d0a923"}, "source": ["# Validation set\n", "val_set_input = tokenizer.texts_to_sequences(df_val['Text'].values)\n", "val_set_input_reverse = [list(reversed(x)) for x in val_set_input]\n", "val_set_input_begin = pad_sequences(val_set_input, maxlen=SEQUENCE_LENGTH)\n", "val_set_input_end = pad_sequences(val_set_input_reverse, maxlen=SEQUENCE_LENGTH)\n", "val_set_output = pd.get_dummies(df_val['Class']).values\n", "print(val_set_input_begin.shape, val_set_input_end.shape, val_set_output.shape)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "0610df13-711b-4d28-810a-2f7361cea388", "_execution_state": "busy", "_uuid": "2fba830b1fd73426d1b3b2f22bac503d91d0a923"}, "source": ["# Test set\n", "test_set_input = tokenizer.texts_to_sequences(df_test['Text'].values)\n", "test_set_input_reverse = [list(reversed(x)) for x in test_set_input]\n", "test_set_input_begin = pad_sequences(test_set_input, maxlen=SEQUENCE_LENGTH)\n", "test_set_input_end = pad_sequences(test_set_input_reverse, maxlen=SEQUENCE_LENGTH)\n", "print(test_set_input_begin.shape, test_set_input_end.shape)"], "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["#### Add genes and variations as one hot encoding\n", "We only transform the variations to use the first and last letter, otherwise it will be almos one variation per exmple and it will be useless."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "0610df13-711b-4d28-810a-2f7361cea388", "_execution_state": "busy", "_uuid": "2fba830b1fd73426d1b3b2f22bac503d91d0a923"}, "source": ["# Add gene and variation to predictor\n", "gene_le = LabelEncoder()\n", "all_genes = np.concatenate([df_train['Gene'], df_val['Gene'], df_test['Gene']])\n", "all_variations = np.concatenate([df_train['Variation'], df_val['Variation'], df_test['Variation']])\n", "all_variations = np.asarray([v[0]+v[-1] for v in all_variations])\n", "print (\"Unique genes: \", len(np.unique(all_genes)))\n", "print (\"Unique variations:\", len(np.unique(all_variations)))\n", "\n", "# gene_encoded = gene_le.fit_transform(all_genes.ravel()).reshape(-1, 1)\n", "# gene_encoded = gene_encoded / np.max(gene_encoded.ravel())\n", "# variation_le = LabelEncoder()\n", "# variation_encoded = variation_le.fit_transform(all_variations).reshape(-1, 1)\n", "# variation_encoded = variation_encoded / np.max(variation_encoded)\n", "\n", "gene_encoded = pd.get_dummies(all_genes).values\n", "variation_encoded = pd.get_dummies(all_variations).values\n", "\n", "len_train_set = len(train_set_input)\n", "len_val_set = len(val_set_input)\n", "len_test_set = len(test_set_input)\n", "train_set_input_gene = gene_encoded[:len_train_set]\n", "train_set_input_variation = variation_encoded[:len_train_set]\n", "val_set_input_gene = gene_encoded[len_train_set:-len_test_set]\n", "val_set_input_variation = variation_encoded[len_train_set:-len_test_set]\n", "test_set_input_gene = gene_encoded[-len_test_set:]\n", "test_set_input_variation = variation_encoded[-len_test_set:]\n", "\n", "print (len_train_set, len(train_set_input_gene))\n", "print (len_val_set, len(val_set_input_gene))\n", "print (len_test_set, len(test_set_input_gene))"], "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["## Attention layer\n", "\n", "from: https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": ["from keras import backend as K\n", "from keras.engine.topology import Layer\n", "from keras import initializers, regularizers, constraints\n", "import numpy as np\n", "\n", "def dot_product(x, kernel):\n", "    \"\"\"\n", "    Wrapper for dot product operation, in order to be compatible with both\n", "    Theano and Tensorflow\n", "    Args:\n", "        x (): input\n", "        kernel (): weights\n", "    Returns:\n", "    \"\"\"\n", "    if K.backend() == 'tensorflow':\n", "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n", "    else:\n", "        return K.dot(x, kernel)\n", "    \n", "\n", "class AttentionWithContext(Layer):\n", "    \"\"\"\n", "    Attention operation, with a context/query vector, for temporal data.\n", "    Supports Masking.\n", "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n", "    \"Hierarchical Attention Networks for Document Classification\"\n", "    by using a context vector to assist the attention\n", "    # Input shape\n", "        3D tensor with shape: `(samples, steps, features)`.\n", "    # Output shape\n", "        2D tensor with shape: `(samples, features)`.\n", "    How to use:\n", "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n", "    The dimensions are inferred based on the output shape of the RNN.\n", "    Note: The layer has been tested with Keras 2.0.6\n", "    Example:\n", "        model.add(LSTM(64, return_sequences=True))\n", "        model.add(AttentionWithContext())\n", "        # next add a Dense layer (for classification/regression) or whatever...\n", "    \"\"\"\n", "\n", "    def __init__(self,\n", "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n", "                 W_constraint=None, u_constraint=None, b_constraint=None,\n", "                 bias=True, **kwargs):\n", "\n", "        self.supports_masking = True\n", "        self.init = initializers.get('glorot_uniform')\n", "\n", "        self.W_regularizer = regularizers.get(W_regularizer)\n", "        self.u_regularizer = regularizers.get(u_regularizer)\n", "        self.b_regularizer = regularizers.get(b_regularizer)\n", "\n", "        self.W_constraint = constraints.get(W_constraint)\n", "        self.u_constraint = constraints.get(u_constraint)\n", "        self.b_constraint = constraints.get(b_constraint)\n", "\n", "        self.bias = bias\n", "        super(AttentionWithContext, self).__init__(**kwargs)\n", "\n", "    def build(self, input_shape):\n", "        assert len(input_shape) == 3\n", "\n", "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n", "                                 initializer=self.init,\n", "                                 name='{}_W'.format(self.name),\n", "                                 regularizer=self.W_regularizer,\n", "                                 constraint=self.W_constraint)\n", "        if self.bias:\n", "            self.b = self.add_weight((input_shape[-1],),\n", "                                     initializer='zero',\n", "                                     name='{}_b'.format(self.name),\n", "                                     regularizer=self.b_regularizer,\n", "                                     constraint=self.b_constraint)\n", "\n", "        self.u = self.add_weight((input_shape[-1],),\n", "                                 initializer=self.init,\n", "                                 name='{}_u'.format(self.name),\n", "                                 regularizer=self.u_regularizer,\n", "                                 constraint=self.u_constraint)\n", "\n", "        super(AttentionWithContext, self).build(input_shape)\n", "\n", "    def compute_mask(self, input, input_mask=None):\n", "        # do not pass the mask to the next layers\n", "        return None\n", "\n", "    def call(self, x, mask=None):\n", "        uit = dot_product(x, self.W)\n", "\n", "        if self.bias:\n", "            uit += self.b\n", "\n", "        uit = K.tanh(uit)\n", "        ait = dot_product(uit, self.u)\n", "\n", "        a = K.exp(ait)\n", "\n", "        # apply mask after the exp. will be re-normalized next\n", "        if mask is not None:\n", "            # Cast the mask to floatX to avoid float64 upcasting in theano\n", "            a *= K.cast(mask, K.floatx())\n", "\n", "        # in some cases especially in the early stages of training the sum may be almost zero\n", "        # and this results in NaN's. A workaround is to add a very small positive number \u03b5 to the sum.\n", "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n", "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n", "\n", "        a = K.expand_dims(a)\n", "        weighted_input = x * a\n", "        return K.sum(weighted_input, axis=1)\n", "\n", "    def compute_output_shape(self, input_shape):\n", "        return input_shape[0], input_shape[-1]"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "af64e666-69b4-4fe7-a8ce-c098e640cbf9", "_execution_state": "busy", "_uuid": "6aff6b56a17d5bc0650e1bb41e60fee7be6f52ac"}, "source": ["from keras.models import Sequential, Model\n", "from keras.layers import Dense, Embedding, LSTM, GRU, Bidirectional, Merge, Input, concatenate\n", "from keras.layers.merge import Concatenate\n", "from keras.utils.np_utils import to_categorical\n", "from keras.callbacks import ModelCheckpoint\n", "from keras.models import load_model\n", "from keras.optimizers import Adam\n", "# Build out our simple LSTM\n", "embed_dim = 128\n", "lstm_out = 196\n", "\n", "# Model saving callback\n", "ckpt_callback = ModelCheckpoint('keras_model', \n", "                                 monitor='val_loss', \n", "                                 verbose=1, \n", "                                 save_best_only=True, \n", "                                 mode='auto')\n", "\n", "\n", "input_sequence_begin = Input(shape=(train_set_input_begin.shape[1],))\n", "input_sequence_end = Input(shape=(train_set_input_end.shape[1],))\n", "input_gene = Input(shape=(train_set_input_gene.shape[1],))\n", "input_variant = Input(shape=(train_set_input_variation.shape[1],))\n", "\n", "merged = concatenate([input_gene, input_variant])\n", "dense = Dense(32, activation='sigmoid')(merged)\n", "\n", "embeds_begin = Embedding(VOCABULARY_SIZE, embed_dim, input_length = SEQUENCE_LENGTH)(input_sequence_begin)\n", "embeds_out_begin = Bidirectional(GRU(lstm_out, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))(embeds_begin)\n", "attention_begin = AttentionWithContext()(embeds_out_begin)\n", "\n", "embeds_end = Embedding(VOCABULARY_SIZE, embed_dim, input_length = SEQUENCE_LENGTH)(input_sequence_end)\n", "embeds_out_end = Bidirectional(GRU(lstm_out, recurrent_dropout=0.2, dropout=0.2, return_sequences=True))(embeds_end)\n", "attention_end = AttentionWithContext()(embeds_out_end)\n", "\n", "merged2 = concatenate([attention_begin, attention_end, dense])\n", "dense2 = Dense(9,activation='softmax')(merged2)\n", "\n", "model = Model(inputs=[input_sequence_begin, input_sequence_end, input_gene, input_variant], outputs=dense2)\n", "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n", "print(model.summary())"], "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["### Training"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"scrolled": true, "_cell_guid": "d7eac40d-a8d6-47a7-9542-1a2b06f76ca8", "_execution_state": "busy", "_uuid": "a0da0f4a43684380e3dd1b6f53d03cc45384fbf9"}, "source": ["model.fit([train_set_input_begin, train_set_input_end, train_set_input_gene, train_set_input_variation], train_set_output, \n", "          epochs=6, batch_size=16, \n", "          validation_data=([val_set_input_begin,val_set_input_end,val_set_input_gene,val_set_input_variation], val_set_output), \n", "          callbacks=[ckpt_callback])"], "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["### Validation"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "72101f81-dd5a-4b27-b985-e4fa03a7d7fd", "_execution_state": "busy", "_uuid": "142200cc3855cd276bb3a9abf3526a61988fafee"}, "source": ["model = load_model('keras_model', custom_objects={'AttentionWithContext': AttentionWithContext})"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "da8194c1-bce5-4d3d-93a7-fbb6b832113e", "_execution_state": "busy", "_uuid": "fffe9f6d9907deabbf827f183c4cf33af917b131"}, "source": ["probas = model.predict([val_set_input_begin, val_set_input_end, val_set_input_gene, val_set_input_variation])\n", "pred_indices = np.argmax(probas, axis=1)\n", "classes = np.array(range(1, 10))\n", "preds = classes[pred_indices]\n", "print('Log loss: {}'.format(log_loss(classes[np.argmax(val_set_output, axis=1)], probas)))\n", "print('Accuracy: {}'.format(accuracy_score(classes[np.argmax(val_set_output, axis=1)], preds)))\n", "skplt.plot_confusion_matrix(classes[np.argmax(val_set_output, axis=1)], preds)\n"], "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["## Train with validation set\n", "\n", "For the final submission we can add the validation set to the training set and run the network for 4 epochs. After 4 epcohs it starts overfitting in the validation set. We only do this to add more training samples and try to get better results this way"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["model.fit([\n", "    np.concatenate([train_set_input_begin, val_set_input_begin]), \n", "    np.concatenate([train_set_input_end,val_set_input_end]), \n", "    np.concatenate([train_set_input_gene, val_set_input_gene]), \n", "    np.concatenate([train_set_input_variation, val_set_input_variation])\n", "],  np.concatenate([train_set_output,val_set_output]), \n", "    epochs=4, batch_size=16, callbacks=[ckpt_callback])"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "e70c3e0c-3722-4ed0-943f-3909c2078f38", "_uuid": "2eeb0f102a078b433d0c30b9d4e5fedebc58a236"}, "source": ["## Submission"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "4d608be7-741c-462c-8a2a-39b81612000d", "_execution_state": "busy", "_uuid": "7b454ebc7996841cffcaba3a165d8b28b5c76466"}, "source": ["probas = model.predict([test_set_input_begin, test_set_input_end, test_set_input_gene, test_set_input_variation])"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "dbcb71f9-1854-44c3-b9cf-d060494887d8", "_execution_state": "busy", "_uuid": "81165d6e9870765c90fbb4624743ea49f9f6d1d8"}, "source": ["submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\n", "submission_df['ID'] = df_test['ID']\n", "submission_df.head()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "9ce11af2-21b8-47e0-a83b-55ee14c1c53c", "_execution_state": "busy", "_uuid": "7264bb21f9c0028b6be3e637c8414d543266d113"}, "source": ["submission_df.to_csv('submission.csv', index=False)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "d1aa03cb-3faf-4dfe-97a7-97dbfff856ec", "_uuid": "1ac56c47c3e55b3a6fde88332fe1e045e59fef9b"}, "source": ["# Public LB Score: 0.93662\n", "\n", "The private leaderboard shows and score of 2.8. Everybody get much worse results in the private leader board, there has been a long discussion in the forums."], "cell_type": "markdown"}], "nbformat_minor": 1, "nbformat": 4}