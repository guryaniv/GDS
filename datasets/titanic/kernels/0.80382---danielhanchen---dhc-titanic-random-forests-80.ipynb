{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "56512c46-c8ff-38c0-1d64-1450be8cb83b"
      },
      "source": [
        "# Hello! #\n",
        "\n",
        "I am Daniel Han-Chen (DHC), a 2nd year student at UNSW.\n",
        "----------\n",
        "The aims of this task include:\n",
        "\n",
        " 1. Predict as accurately as possible whether the passengers had died (0) or survived (1).\n",
        " 2. Clean and prepare the data appropriately.\n",
        " 3. Introduce new features (feature engineering) to lift the accuracy of the model.\n",
        " 4. Utilise dimensionality reduction or feature selection to reduce the model complexity, but maintain its accuracy.\n",
        "![Titanic Photo][1]\n",
        "\n",
        "\n",
        "  [1]: https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/325px-RMS_Titanic_3.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dfa6469f-c39e-0253-b8f9-594800578383"
      },
      "source": [
        "#We first need to import the libraries into Python that we need to use.\n",
        "\n",
        " 1. Matplotlib is a graphing interface for nice plots.\n",
        " 2. Sklearn is the predictive model set we are using.\n",
        " 3. Numpy and Pandas are both dataframe and array modifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f13530d-3cf9-407b-913d-c3e7201d0e12"
      },
      "outputs": [],
      "source": [
        "% matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ffca2248-2232-fefe-5838-be2424ffe1c0"
      },
      "source": [
        "### Random Forests are a type of Decision Tree based modelling, where you go down each brach until you reach the end point (which is the prediction)\n",
        "\n",
        "![Random Forest Example][1]\n",
        "\n",
        "\n",
        "  [1]: http://s7.postimg.org/ik9dd7ny3/Random_Forest_Concepts_Page_3.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "110bf3d8-b17f-2794-71d2-ed8898ab72fe"
      },
      "source": [
        "###We first need to read in the two datasets (training and testing).\n",
        "\n",
        "We first remove the column \"Survived\" from x, and use it later for fitting our model.\n",
        "\n",
        "We then merge the two sets together, as we need to apply cleaning and feature engineering on both (or else they become out of sync)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d9f22c4-f54d-16aa-a14c-00ca9e503564"
      },
      "outputs": [],
      "source": [
        "x = pd.read_csv(\"../input/train.csv\")\n",
        "x_2 = pd.read_csv(\"../input/train.csv\")\n",
        "y = pd.read_csv(\"../input/test.csv\")\n",
        "toPredict = x.pop('Survived')\n",
        "data = pd.concat([x,y])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "df016bab-6ae5-75d0-3112-69ecf47cc5d8"
      },
      "source": [
        "### Now, let's see what's in our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "604dd3e4-f3b1-ca0d-2270-2a816bb3ce97"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "73d61f27-0bc6-cb76-b064-e664ede2aae2"
      },
      "outputs": [],
      "source": [
        "data.describe(include=['O'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bfe442cd-1ee4-6b28-71c0-af0d38307d29"
      },
      "source": [
        "##Clearly, we can see the columns Age, Fare, Cabin and Embarked have missing values.\n",
        "\n",
        "###So, we first clean the age column.\n",
        "We find the median age (after we filter out the same characterisitc data subset (ie same Pclass and Sex)), and apply it to the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "164454a6-655d-6224-1cf6-8f19a9115dfd"
      },
      "outputs": [],
      "source": [
        "newage = data[['Age','Pclass','Sex']].dropna()\n",
        "\n",
        "## Get the subset data\n",
        "print('Pclass 1 F = '+str(    np.median(((newage.query('Pclass == 1 and Sex == \"female\"')))['Age'])))\n",
        "print('Pclass 2 F = '+str(    np.median(((newage.query('Pclass == 2 and Sex == \"female\"')))['Age'])))\n",
        "print('Pclass 3 F = '+str(    np.median(((newage.query('Pclass == 3 and Sex == \"female\"')))['Age'])))\n",
        "print('Pclass 1 M = '+str(    np.median(((newage.query('Pclass == 1 and Sex == \"male\"')))['Age'])))\n",
        "print('Pclass 2 M = '+str(    np.median(((newage.query('Pclass == 2 and Sex == \"male\"')))['Age'])))\n",
        "print('Pclass 3 M = '+str(    np.median(((newage.query('Pclass == 3 and Sex == \"male\"')))['Age'])))\n",
        "\n",
        "## Get the subset data and fillna with median\n",
        "data1 = data.query('Pclass == 1 and Sex == \"female\"');     data1['Age'] = data1['Age'].fillna(36)\n",
        "data2 = data.query('Pclass == 2 and Sex == \"female\"');     data2['Age'] = data2['Age'].fillna(28)\n",
        "data3 = data.query('Pclass == 3 and Sex == \"female\"');     data3['Age'] = data3['Age'].fillna(22)\n",
        "data4 = data.query('Pclass == 1 and Sex == \"male\"');       data4['Age'] = data4['Age'].fillna(42)\n",
        "data5 = data.query('Pclass == 2 and Sex == \"male\"');       data5['Age'] = data5['Age'].fillna(29.5)\n",
        "data6 = data.query('Pclass == 3 and Sex == \"male\"');       data6['Age'] = data6['Age'].fillna(25)\n",
        "\n",
        "## Merge all subsetted datasets and sort by PassengerID\n",
        "data = pd.concat([data1,data2,data3,data4,data5,data6])\n",
        "\n",
        "data = data.sort('PassengerId')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ac6d9d29-1d37-f38d-18bf-2392ed680c88"
      },
      "source": [
        "### Now let us do some exploratory data analysis after we have done cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3ea191cd-f415-c10a-1d32-785ec12af681"
      },
      "outputs": [],
      "source": [
        "x_2.groupby('Survived').agg({'Fare':\n",
        "                                  {'Median':'median',\n",
        "                                  'Mean':'mean', },\n",
        "                           \n",
        "                          'Sex':  {'Male':lambda x: (x == 'male').sum(),\n",
        "                                  'Female':lambda x: (x == 'female').sum(), },\n",
        "                             \n",
        "                           'Pclass':  {'1':lambda x: (x == 1).sum(),\n",
        "                                  '2':lambda x: (x == 2).sum(),\n",
        "                                   '3':lambda x: (x == 3).sum(),},\n",
        "                             \n",
        "                        'SibSp':  {'Mean':'mean',},\n",
        "                           'Parch':  {'Mean':'mean',},\n",
        "                             \n",
        "                           'Embarked':  {'S':lambda x: (x == 'S').sum(),\n",
        "                                  'C':lambda x: (x == 'C').sum(),\n",
        "                                   'Q':lambda x: (x == 'Q').sum()},\n",
        "                             \n",
        "                           'Age':  {'Median':'median',\n",
        "                                  'Mean':'mean', },\n",
        "                          }\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9a3e530f-33af-98a0-9370-8888a4b71f75"
      },
      "source": [
        "### Now, let's see some cool graphs about Age, Sex and Pclass survival rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8fd7a9b5-62d9-8640-49a6-414f2e9ae6e3"
      },
      "outputs": [],
      "source": [
        "## Survival by sex\n",
        "survived_sex = x_2[x_2['Survived']==1]['Sex'].value_counts()\n",
        "dead_sex = x_2[x_2['Survived']==0]['Sex'].value_counts()\n",
        "\n",
        "df = pd.DataFrame([survived_sex,dead_sex])\n",
        "\n",
        "df.index = ['Survived','Dead']\n",
        "plot_sex = df.plot(kind='bar',stacked=True, figsize=(2,4)\n",
        "       , color = ['g','r'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "318e9919-9ee6-04f9-d639-0a4974b8859b"
      },
      "outputs": [],
      "source": [
        "## Survival by Age\n",
        "\n",
        "plot_age = plt.hist([x_2[x_2['Survived']==1]['Age'].fillna(-10),\n",
        "          x_2[x_2['Survived']==0]['Age'].fillna(-10)], \n",
        "         stacked=True, color = ['g','r'],\n",
        "         bins = 30,label = ['Survived','Dead'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6c640677-009a-8629-ad88-9d55d726b7b9"
      },
      "outputs": [],
      "source": [
        "## Survival by Fare\n",
        "\n",
        "plot_fare = plt.hist([x_2[x_2['Survived']==1]['Fare'].fillna(-10),\n",
        "          x_2[x_2['Survived']==0]['Fare'].fillna(-10)], \n",
        "         stacked=True, color = ['g','r'],\n",
        "         bins = 30,label = ['Survived','Dead'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1a9df102-e5e1-01af-83d2-4b6d9614325c"
      },
      "source": [
        "### We can now infer the following: \n",
        "\n",
        "<ol>\n",
        "  <li>**Pclass:**          The lower your class (3), the more likely you'll die</li>\n",
        "  \n",
        "  <li>**Fare:**The lower your fare, the more likely you'll die</li>\n",
        "  \n",
        "  <li>**Embarked**: Most of them are S, and most of them died</li>\n",
        "  \n",
        "  <li>**Age**: The higher your age, the more likely you'll die</li>\n",
        "  \n",
        "  <li>**SibSp** (Siblings, Spouse): The more you have the more likely you'll die</li>\n",
        "  \n",
        "  <li>**Sex**: If you are a male, you have the more likely you'll die</li>\n",
        "  \n",
        "  <li>**Parch** (Parents, Children): The less you have the more likely you'll die</li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3faa3525-5d5c-d530-1877-6ebdc4d27768"
      },
      "source": [
        "<h3>After we know the main features of the data, we know have to clean more data or include new features. </h3>\n",
        "\n",
        "\n",
        "----------\n",
        "<h1> TIME FOR FEATURE ENGINEERING AND MORE CLEANING!!! </h1>\n",
        "![Feature Engineering][1]\n",
        "\n",
        "\n",
        "  [1]: http://3.bp.blogspot.com/-v7RCgnSNLfA/U5stfq5zuJI/AAAAAAAADt4/9_87WaSa620/s1600/features-in-ML.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "457402d6-72f0-7569-3abe-9822e9e292dd"
      },
      "source": [
        "<h3>Now, we find which person has a missing fare value </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b29ee52c-9016-5651-d61b-77e03fae71ae"
      },
      "outputs": [],
      "source": [
        "u = y[y.Fare.isnull()]\n",
        "print(u)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1aac6c90-4e2a-2275-0540-50543f636a6a"
      },
      "source": [
        "<h3> Now, we subset the data to find the best median fare for that missing. </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0517d6ce-4596-b2a1-5963-49890995e2f7"
      },
      "outputs": [],
      "source": [
        "print(np.median((x.query('Pclass == 3 and Sex == \"male\" and Age >= 50' ))['Fare']))\n",
        "data['Fare'] = data['Fare'].fillna(7.75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fe44bdcd-6f3b-2d3a-d6ac-3989e7d9f5fa"
      },
      "source": [
        "<h3>Then, we fill the missing Cabin values with \"MISSING\", and we also filled Embarked with \"S\" (highest frequency) </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5fc36617-f15c-6273-a497-afba251809dc"
      },
      "outputs": [],
      "source": [
        "data['Cabin']=data['Cabin'].fillna('Missing')\n",
        "data['Embarked']=data['Embarked'].fillna('S')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2d7cc710-094f-c1d2-fb99-be22b8292e0c"
      },
      "source": [
        "<h3> We note how both Embarked and Sex columns are categorical in nature. We then set them to be categorical in nature, and convert them to category numbers (male = 0, female = 1 etc.)</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "983e2de5-b1f0-efef-8e99-41a178a22e9a"
      },
      "outputs": [],
      "source": [
        "data['Embarked'].value_counts()\n",
        "\n",
        "## Set both columns to type category\n",
        "data['Embarked'] = data['Embarked'].astype('category')\n",
        "data['Sex'] = data['Sex'].astype('category')\n",
        "\n",
        "## Use cat.codes method to convert categories to numerical labels\n",
        "columns = data.select_dtypes(['category']).columns\n",
        "data[columns] = data[columns].apply(lambda fx: fx.cat.codes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9c2716f7-06cc-4473-8fc7-81ad820b3401"
      },
      "source": [
        "### Now, when we inspect the column Cabin, we can see it is filled with Cabin Letters and Numbers. We will attempt to split this data into columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9337b550-ddb9-face-87cd-a543f279da2c"
      },
      "outputs": [],
      "source": [
        "## Intialise two lists to input Cabin Letters (Code) and Numbers (Num)\n",
        "cabinCode = []\n",
        "cabinNum = []\n",
        "\n",
        "## Loop inside Cabin names and get unique values\n",
        "for row in data['Cabin'].str.split('([A-z]+)(\\d+)'):\n",
        "    if len(row)==1:\n",
        "        cabinCode.append('Missing')\n",
        "        cabinNum.append('0')\n",
        "    else:\n",
        "        cabinCode.append(row[1])\n",
        "        cabinNum.append(row[2])\n",
        "        \n",
        "cabinCode = pd.Series(cabinCode).unique()\n",
        "cabinNum = pd.Series(cabinNum).unique()\n",
        "\n",
        "## Loop inside Cabin Codes and apply a cross referencing categorical conversion method\n",
        "for unique in cabinCode:\n",
        "    X = pd.DataFrame((data['Cabin'].str.find(unique)))\n",
        "    X.columns=[unique+'place']\n",
        "    data = pd.concat([data,X],1)\n",
        "\n",
        "## Loop inside Cabin Nums and apply a cross referencing categorical conversion method\n",
        "for unique in cabinNum:\n",
        "    X = pd.DataFrame((data['Cabin'].str.find(unique)))\n",
        "    X.columns=[unique+'num']\n",
        "    data = pd.concat([data,X],1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d3ce4540-e4fb-15af-cf81-252b9c7ca106"
      },
      "source": [
        "### Now, when we check the Names of the passengers, we see they have honorifics (Mr, Ms). We will attempt to separate this data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c51f1b4-ca2c-08f3-630b-4c1d779fcb36"
      },
      "outputs": [],
      "source": [
        "## Intialise the honorific list\n",
        "honorific_list = []\n",
        "\n",
        "## Loop inside name and get the unique honorifics\n",
        "for name in (data['Name'].str.split(',')):\n",
        "    honorific = (name[1].split('.'))[0]\n",
        "    honorific_list.append(honorific)\n",
        "    \n",
        "honorUnique = pd.Series(honorific_list).unique()\n",
        "\n",
        "## Loop inside again, and apply a cross referencing categorical conversion method\n",
        "for unique in honorUnique:\n",
        "    X = pd.DataFrame((data['Name'].str.find(unique)))\n",
        "    X.columns=[unique+'honor']\n",
        "    data = pd.concat([data,X],1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79be4353-24f0-0f6a-0258-e29f4430aeb6"
      },
      "source": [
        "### Now, when we investigate Tickets, we can change the data into 5 datasets:\n",
        "\n",
        "1. Ticket Length\n",
        "2. How many Dots\n",
        "3. How many Dashes\n",
        "4. Strings of Ticket\n",
        "5. Numbers of Ticket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9b993d1-6303-3182-621d-eb3ebbeb97d7"
      },
      "outputs": [],
      "source": [
        "## Intiliase our 5 lists:\n",
        "\n",
        "ticklength = []; dot_list=[]; dash_list=[]; string_list=[]; num_list=[]\n",
        "\n",
        "## Get ticket length\n",
        "for name in (data['Ticket']):\n",
        "    ticklength.append(len(name))\n",
        "data['tick_length']=ticklength\n",
        "\n",
        "## Split tickets into dots, dashes, strings and numbers\n",
        "ticket_split = data['Ticket'].str.split(' ')\n",
        "for ticket_one in ticket_split:\n",
        "    if len(ticket_one)==1:\n",
        "        if ticket_one==['LINE']:\n",
        "            string_length=-1\n",
        "            num_length=-1\n",
        "            dashes=-2\n",
        "            dots=-2\n",
        "        else:\n",
        "            string_length=0\n",
        "            num_length=(len(str(ticket_one[0])))\n",
        "            dashes=-2\n",
        "            dots=-2\n",
        "    else:\n",
        "        string_length=(len(ticket_one[0]))\n",
        "        num_length=(len(str(ticket_one[1])))\n",
        "        dashes=(ticket_one[0].find('/'))\n",
        "        dots=(ticket_one[0].find('.'))\n",
        "    dot_list.append(dots)\n",
        "    dash_list.append(dashes)\n",
        "    string_list.append(string_length)\n",
        "    num_list.append(num_length)\n",
        "\n",
        "## Combine the datasets onto the main data:\n",
        "add_on = pd.concat([pd.DataFrame(dot_list),pd.DataFrame(dash_list),pd.DataFrame(string_list),\n",
        "           pd.DataFrame(num_list)],1)\n",
        "add_on.columns=['dot_list','dash_list','string_list','num_list']\n",
        "t = data.pop('Ticket')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1530c3f2-92f2-7e48-3cbf-6c9ddeade46e"
      },
      "source": [
        "### Now, we reset our data by sorting by PassengerId and add the Ticket data onto it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e9acd41-a09a-9c8d-29db-f426a5e1404e"
      },
      "outputs": [],
      "source": [
        "data.reset_index(inplace=True)\n",
        "data=pd.concat([data,add_on],1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3853fad2-4735-e822-0e58-7fd98725779a"
      },
      "source": [
        "### Now, we can add 3 more features:\n",
        "\n",
        "1. Family Size\n",
        "2. Fare per family member\n",
        "3. Length of ticket data once aggregated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb3d0fe5-057a-ab09-204f-aee5f197db19"
      },
      "outputs": [],
      "source": [
        "data['Family_size']=data['SibSp']+data['Parch']\n",
        "\n",
        "data['Fare_per']=data['Fare']/(data['Family_size']+1)\n",
        "\n",
        "data['Length']=data['dot_list']+data['dash_list']+data['string_list']+data['num_list']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "60d0fcb3-b033-7acd-b0d0-4dba496272d0"
      },
      "source": [
        "### We then delete columns \"Index\" and \"Level 0\" for cleaning and other columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f0b43fb-64e8-e48e-fac7-a62df923ba6e"
      },
      "outputs": [],
      "source": [
        "i = data.pop('index')\n",
        "c = data.pop('Cabin')\n",
        "n = data.pop('Name')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "af6e4480-476e-6729-71ee-1103ad9cd304"
      },
      "source": [
        "# AND WE ARE DONE FOR FEATURE ENGINEERING!\n",
        "\n",
        "----------\n",
        "### Now, we need to test the training set, and build our model.\n",
        "![Model building][1]\n",
        "\n",
        "\n",
        "  [1]: https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAk3AAAAJDBiZGU1OWZiLWIzZWMtNDU0ZC04NTEwLWY5OWM2ZDc5NGYwMA.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "710267e5-4f6a-f2b0-0c67-73c89c5ce653"
      },
      "source": [
        "### We first split the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d8472a7-7927-6409-fffa-cb5cf7571004"
      },
      "outputs": [],
      "source": [
        "x_train = data[0:891]\n",
        "\n",
        "y_test = data[891:]\n",
        "\n",
        "## Place into arrays for better processing\n",
        "x_train = np.array(x_train)\n",
        "toPredict = np.array(toPredict)\n",
        "testing = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6534884e-c63a-240e-3cf2-9bcc25b71cff"
      },
      "source": [
        "### We can now build our Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cb074829-1f14-725f-93f5-08a6ccf18743"
      },
      "outputs": [],
      "source": [
        "## Build the model:\n",
        "model = rfc(n_estimators=1000,min_samples_split=10, min_samples_leaf=1, \n",
        "                max_features='auto', max_leaf_nodes=None, \n",
        "                oob_score=True, n_jobs=1, random_state=1)\n",
        "model.fit(x_train,toPredict)\n",
        "\n",
        "## Print scores:\n",
        "print(model.score(x_train,toPredict))\n",
        "\n",
        "## Plot the feature importances:\n",
        "plt.plot(model.feature_importances_)\n",
        "\n",
        "## Print out the Out Of Bag Accuracy:\n",
        "print(\"%.4f\" % model.oob_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d5dee70d-c925-baf6-4213-8d510374a7cd"
      },
      "source": [
        "### Noticing how the training time was slow, we can see how the Random Forest automatically removed the unnecessary features (by making them to 0).\n",
        "\n",
        "\n",
        "----------\n",
        "### So no need for us to do dimensionality reduction!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0dac4057-54e9-d00c-7c53-1b86e7636a9a"
      },
      "source": [
        "### Now, to optimise our model to increase the OOB score or Out Of Bag Score, we will iterate through these 3 parameters: n_estimators, sample splits and leaves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6c0c9988-5f6d-08bf-30fb-e5ac74b8d453"
      },
      "source": [
        "***Let us first iterate the n_estimators:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1f0276b-a7f1-895d-0c25-7cac1f8ac59d"
      },
      "outputs": [],
      "source": [
        "## Initialise vectors for iteration:\n",
        "\n",
        "n_est = [500, 600, 700, 800, 900, 1000, 1100, 1200, 1500, 2000, 2500, 3000]\n",
        "\n",
        "for n_e in n_est:\n",
        "    model = rfc(n_estimators=n_e,min_samples_split=10, min_samples_leaf=1, \n",
        "                max_features='auto', max_leaf_nodes=None, \n",
        "                oob_score=True, n_jobs=1, random_state=1)\n",
        "    model.fit(x_train,toPredict)\n",
        "    print(str(n_e)+' : '+str(model.oob_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "25f94258-d5ea-588a-913d-fcde5ba19ea0"
      },
      "source": [
        "### Fascinatingly enough, we see the OOB score is greatest at around 2000 iterations.\n",
        "\n",
        "We will now attempt to iterate around 2000 to see if it improves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "86f8c49e-9663-5100-3e37-07fde9b5728b"
      },
      "outputs": [],
      "source": [
        "## Initialise vectors for iteration:\n",
        "\n",
        "n_est = [1600, 1700, 1800, 1900, 2100, 2200, 2300, 2400]\n",
        "\n",
        "for n_e in n_est:\n",
        "    model = rfc(n_estimators=n_e,min_samples_split=10, min_samples_leaf=1, \n",
        "                max_features='auto', max_leaf_nodes=None, \n",
        "                oob_score=True, n_jobs=1, random_state=1)\n",
        "    model.fit(x_train,toPredict)\n",
        "    print(str(n_e)+' : '+str(model.oob_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7d1080ec-75a8-dd76-9d36-93d5fcfe0d98"
      },
      "source": [
        "### Great! Around 2200 it increases! Let's do it once more time around 2200:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7c8ed0d8-b566-db9b-0b1b-359a9cd1a604"
      },
      "outputs": [],
      "source": [
        "## Initialise vectors for iteration:\n",
        "\n",
        "n_est = [2110, 2120, 2130, 2140, 2150, 2160, 2170, 2180, 2190, 2210, 2220, 2230, 2240, 2250, 2260]\n",
        "\n",
        "for n_e in n_est:\n",
        "    model = rfc(n_estimators=n_e,min_samples_split=10, min_samples_leaf=1, \n",
        "                max_features='auto', max_leaf_nodes=None, \n",
        "                oob_score=True, n_jobs=1, random_state=1)\n",
        "    model.fit(x_train,toPredict)\n",
        "    print(str(n_e)+' : '+str(model.oob_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "80832277-fb55-277a-1270-90ad146b1f90"
      },
      "source": [
        "### We see no change. We will leave it at 2200."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3a9b452f-425b-e393-f34e-e68ba8bb4c40"
      },
      "source": [
        "### Let's iterate through Sample Splits now, by using the same method (LOL i think this is a from of Gradient Descent, though using guessing)\n",
        "\n",
        "![Newton's Method][1]\n",
        "\n",
        "\n",
        "  [1]: https://i.stack.imgur.com/ac9O7.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cf96a3e3-37cf-50d5-4fe9-fa0da1988128"
      },
      "outputs": [],
      "source": [
        "for n_e in range(2,20):\n",
        "    model = rfc(n_estimators=2200,min_samples_split=n_e, min_samples_leaf=1, \n",
        "                max_features='auto', max_leaf_nodes=None, \n",
        "                oob_score=True, n_jobs=1, random_state=1)\n",
        "    model.fit(x_train,toPredict)\n",
        "    print(str(n_e)+' : '+str(model.oob_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8be9db58-bc33-33d4-893b-91309872e1e9"
      },
      "source": [
        "### We can see it stops increasing at 10, so I leave it there at 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1c1850b4-b02d-5d7c-28e7-2ce8cc1edff5"
      },
      "source": [
        "### Finally, we need to check leaf splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36d210b5-6911-2127-093d-2c93a696f741"
      },
      "outputs": [],
      "source": [
        "for n_e in range(1,20):\n",
        "    model = rfc(n_estimators=2200,min_samples_split=10, min_samples_leaf=n_e, \n",
        "                max_features='auto', max_leaf_nodes=None, \n",
        "                oob_score=True, n_jobs=1, random_state=1)\n",
        "    model.fit(x_train,toPredict)\n",
        "    print(str(n_e)+' : '+str(model.oob_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7ae4be8e-529e-cbfa-4309-669a09f0d6c6"
      },
      "source": [
        "### Once again, no change. We'll leave it at 1 split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9465071f-6b24-6274-fa50-0ceb7ce441d6"
      },
      "source": [
        "### YAY! Now let's predict our testing data!\n",
        "\n",
        "\n",
        "----------\n",
        "![Test][1]\n",
        "\n",
        "\n",
        "  [1]: https://lh4.ggpht.com/wKrDLLmmxjfRG2-E-k5L5BUuHWpCOe4lWRF7oVs1Gzdn5e5yvr8fj-ORTlBF43U47yI=w300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "52f36409-e25b-8077-83c4-e11af7285c4f"
      },
      "source": [
        "### We use the predict function and then publish the test results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a78e5553-7cda-37e0-e3a9-7d6dd3713fea"
      },
      "outputs": [],
      "source": [
        "## Make final model\n",
        "model = rfc(n_estimators=2200,min_samples_split=10, min_samples_leaf=1, \n",
        "                max_features='auto', max_leaf_nodes=None, \n",
        "                oob_score=True, n_jobs=1, random_state=1)\n",
        "model.fit(x_train,toPredict)\n",
        "    \n",
        "## Predict!\n",
        "h = model.predict(testing)\n",
        "Results = pd.concat([y['PassengerId'],pd.DataFrame(h)],1)\n",
        "\n",
        "Results.columns=['PassengerId','Survived']\n",
        "Results.set_index('PassengerId')\n",
        "\n",
        "Results.to_csv('new.csv',sep=',',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b255490-d4de-bee0-b368-6a1f2c89e7a3"
      },
      "source": [
        "# And we're done!\n",
        "\n",
        "### Note I got an accuracy of 80.383%. Not bad for my first data set I guess. I did 43 revisions LOL. Testing different models and increasing or deleting features.\n",
        "\n",
        "----------\n",
        "\n",
        "#Conclusion\n",
        "Feature engineering is very important if you want to boost your scores.\n",
        "Data cleaning is a very important step, as real world data is also messy.\n",
        "Random Forests are very useful, especially after you optimise them.\n",
        "Breaking down the problem is very important.\n",
        "If you fail, keep trying. You'll eventually get it :)\n",
        "\n",
        "### I am also writing a Kaggle Keras Model for the Titanic Data.\n",
        "\n",
        "## Anyways, thanks a lot!\n",
        "![Thanks][1]\n",
        "\n",
        "\n",
        "  [1]: http://www.clipartkid.com/images/91/thanks-clipart-give-thanks-thanks-for-watching-thanksgiving-thanks-kKtxd9-clipart.jpg"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}