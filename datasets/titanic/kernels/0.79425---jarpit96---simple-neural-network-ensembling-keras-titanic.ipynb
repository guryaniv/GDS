{"cells":[{"metadata":{"_uuid":"0e7ec8552ee482330d3480b34f1fec133d41820f"},"cell_type":"markdown","source":"# A simple Neural Network in Keras with Basic Ensembling on Titanic Problem."},{"metadata":{"_uuid":"2160df4e75942ed535745a6ffde671c11ac20c8d"},"cell_type":"markdown","source":"This is a a very simple neural network in Keras and uses a simple ensemble technique i.e. voting. This is my first problem on Kaggle, so any suggestions are appreciated.\n## 1. Import Packages\nWe begin by import relevant packages. The packages we will be using are:\n+ Pandas: For easy dataset handling\n+ Numpy: For efficient manipulation of matrices and arrays\n+ Keras: For building the neural network model\n+ scikit-learn: For cross validation"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"#Import Packages\nimport pandas as pd\nimport numpy as np\n\n#Keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\n\n#Sklearn\nfrom sklearn.model_selection import StratifiedKFold","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"4cb22c39bdd3fb347d835ef8904ae45cff633935"},"cell_type":"markdown","source":"## 2. Data Loading and Preprocessing\nWe load the data from csv files saved in input folders using read_csv function of pandas to get train dataframe and test dataframe."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"65d1683eb69ac95dec15d9bd577bbea5fbdeb238"},"cell_type":"code","source":"#Load Data\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"c8d8b058c54ac4a61829afb3b53c61d81e692224"},"cell_type":"markdown","source":"Display the column names from the train dataframe. The dataset consists of following features, where Survived is the target class and other columns are input features. "},{"metadata":{"trusted":true,"_uuid":"4e72dfeb06135fdb34bff5d77080e35bf55c70cd","collapsed":true},"cell_type":"code","source":"print(train_df.columns.values)","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"efffb9a0b7cc17548920cc805ffbba6358dca10c"},"cell_type":"markdown","source":"Display the first few records of train dataframe to analyze manually."},{"metadata":{"trusted":true,"_uuid":"125f91999624181c888be8fdc75b94d7046c2a4b","collapsed":true},"cell_type":"code","source":"train_df.head()","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"7613f58edff8e7d5f1de9e90dabf81a465985771"},"cell_type":"markdown","source":"Perform the same steps for test dataframe and analyze."},{"metadata":{"trusted":true,"_uuid":"52b89e5047e9d6ab1a07fb617ac04ed936067f26","collapsed":true},"cell_type":"code","source":"print(test_df.columns.values)\ntest_df.head()","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"b59d8e3884f8d45a72ff3c961558b57e868f0760"},"cell_type":"markdown","source":"We drop Name, Ticket, Cabin features from train and test dataframes as it will be easier to work with numerical and categorical data. Also, drop PassengerId from train dataframe as it does not provide any information for the model. We do not drop PassengerId from test dataframe as it will be required to make the submission file further.\nWe display the shape of train_df and test_df before and after dropping the columns."},{"metadata":{"trusted":true,"_uuid":"ee15dcb21677bcddf610b730dec771bad611ce6b","collapsed":true},"cell_type":"code","source":"#Dropping columns\nprint(\"Before: \", train_df.shape, test_df.shape)\ntrain_df = train_df.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis = 1)\ntest_df = test_df.drop(['Name', 'Ticket', 'Cabin'], axis = 1)\nprint(\"After: \", train_df.shape, test_df.shape)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"c8d2246c8abad5be4c68d1df2807fa14ee0697df"},"cell_type":"markdown","source":"Create a function to display the number of empty or null cells for each column. This helps us to either fill in the missing values or drop the records with missing values. The function loops over the columns and prints the number of null cells in that column."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d9b44255b1ef462d0e7e0fd3b60c04dc9f1980d4"},"cell_type":"code","source":"def print_empty_cells(dataset):\n    print(\"Empty Cells->\")\n    cols = dataset.columns.values\n    for col in cols:\n        print(col, dataset[col].isnull().sum())","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"ecd6f8b35c03f0a733844d590e4026d2ee2b2d68"},"cell_type":"markdown","source":"Display the empty cells for train_df. "},{"metadata":{"trusted":true,"_uuid":"3426bbe511c3169754b6c276021d9655bd733d06","collapsed":true},"cell_type":"code","source":"#print empty/null number of cells in each column\nprint_empty_cells(dataset = train_df)","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"c36434ab2f334dfa36f775a852963f65c40dd1ab"},"cell_type":"markdown","source":"Age and Embarked columns contain null values, we have to replace these missing values.\nFor Age column, we can use the median age value and for Embarked column we pick the most frequent value i.e. 'S'."},{"metadata":{"trusted":true,"_uuid":"2acb0f70e6f3157e897d4b2114e303c14b1a695f","collapsed":true},"cell_type":"code","source":"#Fill Null or Empty values with default values\nfreq_port = train_df.Embarked.dropna().mode()[0]\nprint(freq_port)\ntrain_df['Age'] = train_df['Age'].fillna(train_df['Age'].dropna().median()) # Median Age\ntrain_df['Embarked'] = train_df['Embarked'].fillna(freq_port) #Most Frequent Port S","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"e7c2692889eeb8b0ccd7f33c8986ea6b2d37f5da"},"cell_type":"markdown","source":"Repeat the same process for test_df."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"eed45fd03e20d3d4ac5677af3783efabea6e84b3","collapsed":true},"cell_type":"code","source":"#print empty/null number of cells in each column\nprint_empty_cells(dataset = test_df)","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"efc191198797aae1e1727f708d10696cba8e8f93"},"cell_type":"markdown","source":"Age and Fare columns contain null values, we have to replace these missing values or drop records with missing values.\nFor Age column, we can use the median age value and for Fare column we drop the record."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"044b38a1fdd11c432c7c76aeadc346a3bc5bf67f"},"cell_type":"code","source":"#Fill Null or Empty values with default values\ntest_df['Age'] = test_df['Age'].fillna(test_df['Age'].dropna().median()) # Median Age\ntest_df['Fare'] = test_df['Fare'].dropna() ","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"9803b1ea762d08d292d0ad3b9a82f9d5c973343d"},"cell_type":"markdown","source":"Now, we convert the categorical columns Sex and Embarked from string values to numerical values. We create two maps and change the values from string to numbers."},{"metadata":{"trusted":true,"_uuid":"c4bed5ee242cbdec1a5dfb22cc6d797e671135f2","collapsed":true},"cell_type":"code","source":"#Maps for Sex and Embarked\nsex_mapping = {'male' : 0, 'female' : 1}\nembarked_mapping = {'S' : 0, 'Q' : 1, 'C' : 2}\n\n#Categorical to Numerical Sex and Embarked, Fill NA with Most Frequent Female and S\ntrain_df['Sex'] = train_df['Sex'].map(sex_mapping)\ntrain_df['Embarked'] = train_df['Embarked'].map(embarked_mapping)\ntest_df['Sex'] = test_df['Sex'].map(sex_mapping)\ntest_df['Embarked'] = test_df['Embarked'].map(embarked_mapping)\n\nprint(train_df.head())","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"acca45eab78b636375a0c509ec904d133986e1d0"},"cell_type":"markdown","source":"Extraxt X_train, Y_Train and X_Test sets from the dataframes."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c571d521d47863e886715cdcec66a84363e2a819"},"cell_type":"code","source":"#Extracting Test and Train Sets for NN, Dataframe to Numpy\nX_train = train_df.drop(['Survived'], axis=1).values\nY_train = train_df['Survived'].values\nX_test = test_df.drop(['PassengerId'], axis = 1).values","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"c5afd9f518cda3001aa1baf148f0c317f5fadb83"},"cell_type":"markdown","source":"## 3. Model \nDefine the model hyper-parametrs or config parameters, like number of epochs and batch size."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d32444c007b662a304ee2f7fab84b8c5deda7bff"},"cell_type":"code","source":"#Config Parameters\nnum_epochs = 200\nnum_cv_epochs = num_epochs\ntrain_batch_size = 32\ntest_batch_size = 32\nfolds = 10","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"8c4270ae817efbabc27978ef56f2d7487f1125cc"},"cell_type":"markdown","source":"Now, we create a simple feed forward dense model in Keras. It consists of a single hidden layer with 20 neurons and relu activation function. The output unit is a single sigmoidal neuron as it is a binary classification problem. Batch Normalization is used to normalize the inputs.\nBinary cross entropy loss and Adam optimizer is used."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8b61a1f91267d6b9f3c7a3177ea50321804cb48"},"cell_type":"code","source":"def get_model():\n    m = Sequential()\n    m.add(BatchNormalization(input_shape=(7,)))\n    m.add(Dense(20, activation='relu'))\n    m.add(Dense(1, activation='sigmoid'))\n    m.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n    return m","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"76e89ca6953da85c78fd738677a910b2d7fd01d6"},"cell_type":"markdown","source":"We get the model from the get_model function and fit on the complete training data set."},{"metadata":{"trusted":true,"_uuid":"301b23498d80cd131f1a4f355c885d490de9c442","collapsed":true},"cell_type":"code","source":"#Make Keras Model and Fit on Training Data\nmodel = get_model()\nmodel.fit(X_train, Y_train, epochs=num_epochs, batch_size=train_batch_size, verbose=1)","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"cf65771275fd132ebd4a6638e640064f666f0c2b"},"cell_type":"markdown","source":"Now, we create a cross validation function for two major purposes:\n1. Find model accuracy on the training data without risking overfitting.\n2. Use cross validation models for ensembling.\n\n#### 1. Cross Validation to Determine Model Accuracy\nIf we evaluate the trained model on complete train dataset then we are learning and evaluating on the same data i.e. data which is already seen the model is used to evaluate it's performance. This can lead to overfitting as it incentivises the model to perform better on the training set by compromising it's generalizing abilility. \n**K-Fold Cross Validation** divides the dataset into k folds and learns a model on k-1 folds and evaluates on the remaining k<sup>th</sup> fold. This process is repeated for every fold and accuracy is averaged.\n\n#### 2. Cross Validation Models for Ensembling\nThe models trained in every iteration are used to make predictions on the test dataset. The output generated by prediction is the probabilities for every record to have value survived=1. These probabilities are summed over all the models in y_pred. y_pred is returned after function completion."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ec9c34fcd07c63fb9da3233e6e0c8c5912f1c10b"},"cell_type":"code","source":"def cross_validation(X_train, Y_train, X_test, num_cv_epochs, k = 5):\n    print(\"------------Cross Validation--------\")\n    k = max(k, 2) #Minimum 2 folds\n    kfold = StratifiedKFold(n_splits=k, shuffle=True)\n    cvscores = [] #metric scores/accuracy for each iteration\n    y_pred = np.zeros((X_test.shape[0], 1)) #sum of predicted probabilities by models trained on different k-1 folds \n    for train, test in kfold.split(X_train, Y_train): #for every iteration \n        model_acc = get_model() #get a new keras model\n        model_acc.fit(X_train[train], Y_train[train], epochs=num_cv_epochs, batch_size=train_batch_size, verbose=0) #fit/train on k-1 folds\n        scores = model_acc.evaluate(X_train[test], Y_train[test], verbose=0) #evaluate on kth fold\n        y_pred += model_acc.predict(X_test) #predict on test dataset for soft voting/ensembling\n        print(\"%s: %.2f%%\" % (model_acc.metrics_names[1], scores[1]*100))\n        cvscores.append(scores[1] * 100)\n    print(\"%.2f%%\" % (np.mean(cvscores)))\n    return y_pred","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"9ebbd57072730c9748d5407204b2d4d9dbcc4427"},"cell_type":"markdown","source":"Cross Validation is called to generate the Y_pred predicted probabilities."},{"metadata":{"trusted":true,"_uuid":"99ecd7e0f0f195b30f71ebd7e77201b8eebf01e5","collapsed":true},"cell_type":"code","source":"#Evaluate Model On Train Data To Find Model Accuracy\n#KFold Cross Validation\nY_pred = cross_validation(X_train=X_train, Y_train=Y_train, X_test=X_test, num_cv_epochs=num_cv_epochs, k = folds)","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"f97ecac9008e7235a0a91742ca2600539fc7a42d"},"cell_type":"markdown","source":"The probabilities are predicted over the model trained using the complete training dataset. These probabilities are added with previous probabilities from croos validation function and averaged. The probability values are then converted to integer values (0 or 1) using 0.5 as the threshold probability."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2439a8fa69c98d148c1609148b6b73738c151c7c"},"cell_type":"code","source":"#Evaluate Test Data to Get Prediction\nY_pred += model.predict(X_test, batch_size=32)\nY_pred = Y_pred.reshape((Y_pred.shape[0],)) #reshape (418,1) to (418,)\nY_pred = Y_pred / (folds+1) #Average predicted probabilty\nY_pred = [int(p > 0.5) for p in Y_pred] #Converting class probabilities to Binary value","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"36b1072a8105972d6d04026431a252d65ffa4096"},"cell_type":"markdown","source":"The submission file is created using Y_pred and PassengerId from test_df and wriiten to file."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2009dc086415d68adf7b37e71ed8cb676674bdca"},"cell_type":"code","source":"#Make submission Dataframe and Save to file\nsubmission = pd.DataFrame({ \"PassengerId\": test_df[\"PassengerId\"], \"Survived\": Y_pred })\nsubmission.to_csv('submission.csv', index=False)","execution_count":59,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"53fbef8d911ab440e7705432519df41c59057c26"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}