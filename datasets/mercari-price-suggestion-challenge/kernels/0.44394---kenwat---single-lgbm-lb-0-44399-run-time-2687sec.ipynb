{"nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.4"}}, "cells": [{"cell_type": "markdown", "source": [" This kernel is only a  simple lgbm model.  If you find it is useful, please give me an upvote.   The running finished within 1 hour.  I am sure the parameters can be further tuned to get better result."], "metadata": {"_uuid": "882a3cdde35c17715b89112a5e6f6dadea63ab1f", "_cell_guid": "e8b8fa48-afd2-484e-844b-b45093f1b06d"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_uuid": "5e492c083ffbd2d163864e49cbe3d4492ced5196", "_cell_guid": "5b2aa633-66f2-4a44-8cb8-5afe39f73cf2", "collapsed": true}, "source": ["import lightgbm as lgbm\n", "from sklearn.metrics import mean_squared_error\n", "from scipy import sparse as ssp\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.model_selection import KFold\n", "from sklearn.pipeline import FeatureUnion\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.metrics import mean_squared_log_error\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.preprocessing import OneHotEncoder\n", "import time\n", "import re\n", "import collections\n", "import gc\n", "\n", "t00 = time.time()\n", "#  stop-word, can add any wording I want to replace\n", "stopwords=set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n", "              'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n", "              'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n", "              'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', \n", "              'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', \n", "              'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n", "              'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n", "              'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n", "              'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n", "              'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few',\n", "              'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n", "              'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', \n", "              've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', \n", "              'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn',\n", "               '&','brand new','new','\\[rm\\]','free ship.*?',\n", "               'rm','price firm','no description yet'               \n", "              ])\n", "\n", "pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n", "train = pd.read_csv('../input/train.tsv', sep=\"\\t\",encoding='utf-8',\n", "                    converters={'item_description':lambda x:  pattern.sub('',x.lower()),\n", "                               'name':lambda x:  pattern.sub('',x.lower())}\n", "                   )\n", "test = pd.read_csv('../input/test.tsv', sep=\"\\t\",encoding='utf-8',\n", "                    converters={'item_description':lambda x:  pattern.sub('',x.lower()),\n", "                               'name':lambda x:  pattern.sub('',x.lower())}\n", "                    )\n", "train_label = np.log1p(train['price'])\n", "train_texts = train['name'].tolist()\n", "test_texts = test['name'].tolist()\n", "print('load tsv completed')\n", "\n", "# \n", "#  replace missing word\n", "# \n", "train['category_name'].fillna('other', inplace=True)\n", "test['category_name'].fillna('other', inplace=True)\n", "\n", "train['brand_name'].fillna('missing', inplace=True)\n", "test['brand_name'].fillna('missing', inplace=True)\n", "\n", "test['item_description'].fillna('none', inplace=True)\n", "train['item_description'].fillna('none', inplace=True)\n", "\n", "test['nm_word_len']=list(map(lambda x: len(x.split()), test_texts))\n", "train['nm_word_len']=list(map(lambda x: len(x.split()),train_texts))\n", "test['desc_word_len']=list(map(lambda x: len(x.split()), test['item_description'].tolist()))\n", "train['desc_word_len']=list(map(lambda x: len(x.split()), train['item_description'].tolist()))\n", "test['nm_len']=list(map(lambda x: len(x),test_texts))\n", "train['nm_len']=list(map(lambda x: len(x),train_texts))\n", "test['desc_len']=list(map(lambda x: len(x), test['item_description'].tolist()))\n", "train['desc_len']=list(map(lambda x: len(x), train['item_description'].tolist()))\n", "nrow_train = train.shape[0]\n", "test_id=test['test_id']\n", "\n", "\n", "def split_cat(text):\n", "    try:\n", "        cat_nm=text.split(\"/\")\n", "        if len(cat_nm)>=3:\n", "            return cat_nm[0],cat_nm[1],cat_nm[2]\n", "        if len(cat_nm)==2:\n", "            return cat_Nm[0],cat_nm[1],'missing'\n", "        if len(cat_nm)==1:\n", "            return cat_nm[0],'missing','missing'\n", "    except: return (\"missing\", \"missing\", \"missing\")\n", "train['subcat_0'], train['subcat_1'], train['subcat_2'] = \\\n", "zip(*train['category_name'].apply(lambda x: split_cat(x)))\n", "test['subcat_0'], test['subcat_1'], test['subcat_2'] = \\\n", "zip(*test['category_name'].apply(lambda x: split_cat(x)))\n", "                                 \n", "NAME_MIN_DF=35\n", "count = CountVectorizer(min_df=NAME_MIN_DF)\n", "X_name_mix = count.fit_transform(train['name'].append(test['name']))\n", "X_name=X_name_mix[:nrow_train]\n", "X_t_name = X_name_mix[nrow_train:]\n", "\n", "\n", "MAX_FEATURES_ITEM_DESCRIPTION=25000\n", "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n", "                         ngram_range=(1,3))\n", "X_description_mix = tv.fit_transform(train['item_description'].append(test['item_description']))\n", "X_description=X_description_mix[:nrow_train]\n", "X_t_description = X_description_mix[nrow_train:]\n", "\n", "\n", "print('make categorical features')\n", "cat_features=['subcat_2','subcat_1','subcat_0','brand_name','category_name','item_condition_id','shipping']\n", "for c in cat_features:\n", "    newlist=train[c].append(test[c])\n", "    le = LabelEncoder()\n", "    le.fit(newlist)\n", "    train[c] = le.transform(train[c])\n", "    test[c] = le.transform(test[c])\n", "enc = OneHotEncoder()\n", "enc.fit(train[cat_features].append(test[cat_features]))\n", "X_cat = enc.transform(train[cat_features])\n", "X_t_cat = enc.transform(test[cat_features])\n", "    \n", "train_feature=['desc_word_len','nm_word_len','desc_len','nm_len']\n", "train_list = [train[train_feature].values,X_description,X_name,X_cat]\n", "test_list = [test[train_feature].values,X_t_description,X_t_name,X_t_cat]\n", "X = ssp.hstack(train_list).tocsr()\n", "X_test = ssp.hstack(test_list).tocsr()\n", "\n", "print (' finish feature for training')\n", "\n", "NFOLDS = 4\n", "kfold =KFold(n_splits=NFOLDS, shuffle=True, random_state=128)\n", "\n", "\n", "learning_rate = 0.8\n", "num_leaves =128\n", "min_data_in_leaf = 1000\n", "feature_fraction = 0.5\n", "bagging_freq=1000\n", "num_boost_round = 1000\n", "params = {\"objective\": \"regression\",\n", "          \"boosting_type\": \"gbdt\",\n", "          \"learning_rate\": learning_rate,\n", "          \"num_leaves\": num_leaves,\n", "          \"feature_fraction\": feature_fraction, \n", "          \"bagging_freq\": bagging_freq,\n", "          \"verbosity\": 0,\n", "          \"metric\": \"l2_root\",\n", "          \"nthread\": 4,\n", "          \"subsample\": 0.9\n", "          }\n", "cv_pred = np.zeros(len(test_id))\n", "kf = kfold.split(X)\n", "for i, (train_fold, test_fold) in enumerate(kf):\n", "    train_t0 = time.time()\n", "    X_train, X_validate, label_train, label_validate = \\\n", "            X[train_fold, :], X[test_fold, :], train_label[train_fold], train_label[test_fold]\n", "    dtrain = lgbm.Dataset(X_train, label_train)\n", "    dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n", "    bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, verbose_eval=100,early_stopping_rounds=100)\n", "    cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n", "    print ('training & predict time',time.time()-train_t0)\n", "    gc.collect()\n", "\n", "cv_pred /= NFOLDS\n", "cv_pred = np.expm1(cv_pred)\n", "submission = test[[\"test_id\"]]\n", "submission[\"price\"] = cv_pred\n", "submission.to_csv(\"./myNNsubmission.csv\", index=False)\n", "print ('overall time',time.time()-t00)\n"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_uuid": "63f8c7b84c10bc9f71a4f60c41a6ebae3fa4014d", "_cell_guid": "3cfa3351-a801-4743-a2fc-8a6349bcd51f", "collapsed": true}, "source": []}]}