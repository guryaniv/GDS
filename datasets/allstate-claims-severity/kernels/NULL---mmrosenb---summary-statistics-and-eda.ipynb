{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b4a2d0c7-6c99-b862-2df2-3415ab793f5c"
      },
      "source": [
        "# Allstate Severe Insurance Claim Competition: EDA\n",
        "\n",
        "_Description: contains my summary statistics for the [Allstate Severe Insurance Claim Competition](https://www.kaggle.com/c/allstate-claims-severity)._\n",
        "\n",
        "_Last Updated: 10/20/2016 11:24 AM._\n",
        "\n",
        "_By: [Michael Rosenberg](https://www.kaggle.com/mmrosenb)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "663aa814-4928-e331-0cf8-c5e757dcf580"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML, Markdown, display #display settings\n",
        "import warnings #for filtering warnings\n",
        "\n",
        "#constants\n",
        "%matplotlib inline\n",
        "sns.set_style(\"dark\")\n",
        "#to ignore warnings in output\n",
        "warnings.filterwarnings('ignore')\n",
        "#global information settings\n",
        "sigLev = 2 #three significant digits\n",
        "percentMul = 100 #for percentage multiplication\n",
        "figWidth = figHeight = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e9492cfa-4015-27f8-4041-c12ec41d5b57"
      },
      "outputs": [],
      "source": [
        "#load in dataset\n",
        "trainFrame = pd.read_csv(\"../input/train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8e896ed4-1d5f-338f-3145-8a74e7f01f69"
      },
      "source": [
        "We see that there are 188318 observations in our dataset and 130 features for our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5ac24a4d-75a8-cae9-187b-6273791bdae9"
      },
      "source": [
        "## Univariate Analysis\n",
        "\n",
        "### Target Variable Analysis\n",
        "\n",
        "Let us start by studying some aspects of the target variable, `loss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2b2eb990-c964-a8e2-6de4-2e83864e6f19"
      },
      "outputs": [],
      "source": [
        "lossFigure = plt.figure(figsize = (figWidth,figHeight))\n",
        "lossHistogram = plt.hist(trainFrame[\"loss\"])\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7e010931-7bbd-251e-b44b-2311f70c16e1"
      },
      "source": [
        "_Figure 1: Distribution of our target variable `loss` across our training set._\n",
        "\n",
        "It is apparent that our target variable is extremely right-skewed. It may be preferable for us to target a more favorably-distributed variable. Let us study $\\log(loss).$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c72e071b-c6c2-2ec4-f73c-7983a6cdf065"
      },
      "outputs": [],
      "source": [
        "trainFrame[\"logLoss\"] = np.log(trainFrame[\"loss\"])\n",
        "logLossFigure = plt.figure(figsize = (figWidth,figHeight))\n",
        "logLossHistogram = plt.hist(trainFrame[\"logLoss\"])\n",
        "plt.xlabel(\"$\\log(Loss)$\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of $\\log(Loss)$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2d76a641-4d95-371e-7691-41db4767ec08"
      },
      "source": [
        "_Figure 2: Distribution of the log of our `loss` variable._\n",
        "\n",
        "Interestingly, we see an outcome with $\\log(Loss)$ of $-0.4$, or around $e^{-0.4} = .67$ in loss claim. This seems to be an unusual outlier and is likely not a strong representation of the distribution overall. We see that most of our observations are centered around a loss claim of $e^7 \\approx 1100.$ If this is scaled in dollars, this actually a relatively moderate loss claim. That being said, since the competition does not specify the loss measurement, it is difficult to confirm this suggestion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2df7625a-c8a3-b441-9051-028b1534e94d"
      },
      "source": [
        "### Categorical Variable Summary Statistics\n",
        "\n",
        "Let us get a sense of some of the distributions of our categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f6c412b-760a-47b8-4692-28edc5332065"
      },
      "outputs": [],
      "source": [
        "#check number of categorical variables\n",
        "categoricalColumns = [col for col in trainFrame.columns if \"cat\" in col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "590eb953-5950-5560-7692-659bbfeea2b0"
      },
      "source": [
        "We see that there are 116 categorical variables in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "395163f7-d50b-f0cd-fc5d-a50badd387ba"
      },
      "outputs": [],
      "source": [
        "#get num unique for each\n",
        "categoricalTrainFrame = trainFrame[categoricalColumns]\n",
        "uniqueVec = categoricalTrainFrame.apply(lambda x : x.nunique(),axis = 0)\n",
        "nuniqueMode = uniqueVec.mode()\n",
        "#then plot the distribution of number of unique levels\n",
        "categoricalUniqueFigure = plt.figure(figsize = (figWidth,figHeight))\n",
        "categoricalNUniqueHist = plt.hist(uniqueVec)\n",
        "plt.xlabel(\"Number of Unique Levels\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Number of Unique Levels\\nGiven Categorical Variable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "020111e5-99b5-8227-a07f-ebc3d0f3c71b"
      },
      "source": [
        "_Figure 3: Distribution of the number of unique levels for a categorical variable._\n",
        "\n",
        "We see an extreme right skew in the distribution of the number of unique levels. The mode number of unique levels is 2, while we hae some observations that range near 300 levels. This suggests that there may be some usability issues related to some variables with high amounts of levels. That being said, there are relatively few of them, which suggests that we will probably be in the clear on these level issues for the most part.\n",
        "\n",
        "Let us check to see if there are any very low variance categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69eb62dd-feb1-5b58-0bf3-aa2a8730fc66"
      },
      "outputs": [],
      "source": [
        "#make mapper functions\n",
        "def integerizeCol(catColumn):\n",
        "    #helper for integerizing a categorical column\n",
        "    levels = catColumn.unique()\n",
        "    counter = 0\n",
        "    mapDict = {} #will add to this\n",
        "    for lev in levels:\n",
        "        mapDict[lev] = counter\n",
        "        counter += 1\n",
        "    #then integerize the column\n",
        "    intCol = catColumn.map(mapDict)\n",
        "    return intCol\n",
        "\n",
        "def propMode(catColumn):\n",
        "    #helper that finds the proportion of a given column is the mode\n",
        "    #integerize it\n",
        "    intCatColumn = integerizeCol(catColumn)\n",
        "    #then get the mode\n",
        "    modeOfCategory = int(intCatColumn.mode())\n",
        "    #then get proportion\n",
        "    numMode = intCatColumn[intCatColumn == modeOfCategory].shape[0]\n",
        "    propMode = float(numMode) / intCatColumn.shape[0]\n",
        "    return propMode\n",
        "#then apply over categorical columns\n",
        "propModeVec = categoricalTrainFrame.apply(propMode,axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e3d65be4-31f5-99ed-31d8-b8a4da5f45da"
      },
      "outputs": [],
      "source": [
        "#then plot\n",
        "givenFigure = plt.figure(figsize = (figWidth,figHeight))\n",
        "plt.hist(propModeVec)\n",
        "plt.xlabel(\"Proportion Mode\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of\\nProportion Mode Across Categorical Variables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d0ebd7b9-086b-89e8-af9b-cc101e70c05c"
      },
      "source": [
        "_Figure 4: Distribution of mode proportions across categorical variables._\n",
        "\n",
        "We see that many of our categorical variables are low in variance, as around $60$ categorical variables have their modes taking up over $90\\%$ of their distribution. This may suggest that we will have to consider removing many low variance features from consideration. For initial interpretation, we will not consider any variables with fewer than $100$ observations not in the mode. In the future, we may consider extending this to more low-variance features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "97fe68f9-cca3-4f39-e720-bd6b8f78378b"
      },
      "outputs": [],
      "source": [
        "categoricalCutoff = 100 #won't consider categorical variables with fewer than\n",
        "#this amount of features\n",
        "propModeBelowCutoff = (\n",
        "    propModeVec[(1 - propModeVec) * trainFrame.shape[0] < categoricalCutoff])\n",
        "catToRemove = list(propModeBelowCutoff.index)\n",
        "categoricalColumns = [catCol for catCol in categoricalColumns if\n",
        "                        catCol not in catToRemove]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ad81a2f4-87cf-3c58-9098-e0e06dcf21c5"
      },
      "source": [
        "### Continuous Variable Summary Statistics\n",
        "\n",
        "We will now study some aspects of our continuous variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1d390d41-029d-5bc3-5dd5-70f237c7cc9d"
      },
      "outputs": [],
      "source": [
        "#get continuous variables\n",
        "continuousColumns = [col for col in trainFrame.columns if \"cont\" in col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d725c44f-5573-24ac-a44c-47d372b7f3a9"
      },
      "source": [
        "We have 14 continuous variables in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea900886-0504-76a8-f51c-3b9aa0607fd8"
      },
      "outputs": [],
      "source": [
        "#get standard deviations of continuous columns\n",
        "continuousTrainFrame = trainFrame[continuousColumns]\n",
        "continuousVarVec = continuousTrainFrame.apply(lambda x: x.std(),axis = 0)\n",
        "display(continuousVarVec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12137310-ead7-21d5-c53b-273c93aad3a1"
      },
      "source": [
        "_Table 1: Our continuous variables by their sample standard errors._\n",
        "\n",
        "Given that our continuous variables have been normalized to between $0$ and $1$, these standard errors for our continuous variables are very reasonable. Thus, I am doubtful that we will have to worry about low-variance features when dealing with continuous variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "de7b6805-87d3-eb87-ae35-769a8396e7d9"
      },
      "source": [
        "### Check for Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7df4d7d-8706-45a5-14da-927bad790439"
      },
      "outputs": [],
      "source": [
        "def numNull(colVec):\n",
        "    #helper that gets the number of null values in a given columns\n",
        "    nullValColVec = colVec[colVec.isnull()]\n",
        "    return nullValColVec.shape[0]\n",
        "numMissingVec = trainFrame.apply(numNull,axis = 0)\n",
        "anyMissing = (numMissingVec.sum() > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3e072237-20c4-e47a-876d-b0c5c9ebcc4a"
      },
      "source": [
        "To our benefit, there are no missing values in this table. The wonders of cleaned data!\n",
        "\n",
        "## Bivariate Analysis\n",
        "\n",
        "Let us consider the variables that have non-trivial correlation with our `logLoss` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "306c272b-ce0b-a380-aa4e-f4d563a82eb5"
      },
      "outputs": [],
      "source": [
        "#reencode our categorical variables\n",
        "intTrainFrame = trainFrame\n",
        "intTrainFrame = intTrainFrame.loc[:,\n",
        "                                  ~trainFrame.columns.isin(categoricalColumns)]\n",
        "for catCol in categoricalColumns:\n",
        "    intTrainFrame[\"int_\" + catCol] = integerizeCol(trainFrame[catCol])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b53aeb1e-c12a-e6a2-e23e-69c41933f7d0"
      },
      "outputs": [],
      "source": [
        "#check for correlations\n",
        "trainCorFrame = intTrainFrame.corr()\n",
        "#clear up irrelevant variable\n",
        "trainCorFrame = trainCorFrame.drop([\"id\",\"loss\"],axis=0)\n",
        "trainCorFrame = trainCorFrame.drop([\"id\",\"loss\"],axis=1)\n",
        "#then get relevant vector\n",
        "logLossCorVec = trainCorFrame.loc[\"logLoss\",:]\n",
        "logLossCorVec = logLossCorVec.drop(\"logLoss\",axis = 0)\n",
        "logLossAbsCorVec = abs(logLossCorVec)\n",
        "#then plot it\n",
        "givenFigure = plt.figure(figsize = (figWidth,figHeight))\n",
        "plt.hist(logLossAbsCorVec)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Absolute Correlation with $\\log(Loss)$\")\n",
        "plt.title(\"Distribution of\\nAbsolute Correlation (Pearson) with $\\log(Loss)$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a8c209b3-8a3f-9b38-a461-1a82c2aa1672"
      },
      "source": [
        "_Figure 5: Distribution of the absolute correlation between $\\log(Loss)$ and our variables._\n",
        "\n",
        "As expected, we see a severely right-skewed distribution, with many variables having little to no correlation with $|\\log(Loss)|$ and a long tail of variables have rather high absolute correlations with $\\log(Loss).$  I generally prefer to look at variables with absolute correlations with the target at above $.1$, but given the large number of variables that have close to $0$ absolute correlation with our target variable, I would think that reducing features solely based on low absolute correlation may be a poor decision. Hence, for model selection purposes, I will likely attempt to add these covariates back in for out-of-sample prediction purposes.\n",
        "\n",
        "Let us study our top $5$ features in terms of absolute correlation with $\\log(Loss)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "835d7d9d-43f9-5ba5-5cef-d9dbfa28514b"
      },
      "outputs": [],
      "source": [
        "#order absolute correlation vector\n",
        "logLossAbsCorVec = logLossAbsCorVec.sort_values(ascending = False)\n",
        "#get top 5 features\n",
        "numFeaturesConsidered = 5\n",
        "topFiveFeatures = list(logLossAbsCorVec.index)[0:numFeaturesConsidered]\n",
        "display(topFiveFeatures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5a70a22f-96f8-0d68-1fdc-c0427f353406"
      },
      "source": [
        "_Table 2: The $5$ features that have the highest absolute correlation with $\\log(Loss)$._\n",
        "\n",
        "Intersetingly, our $5$ features with highest absolute correlation with $\\log(Loss)$ happen to be categorical variables. This may simply be a mathematical anomaly, since the continuous variables are limited to the $[0,1]$ scale, this is perhaps a mathematical anomaly of the variable domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "27947518-8cbe-514f-1064-b5389880dd3a"
      },
      "outputs": [],
      "source": [
        "#clean up feature names for full train frame\n",
        "def plotCategory(catName):\n",
        "    #helper for plotting our categories\n",
        "    figure = plt.figure(figsize = (figWidth,figHeight))\n",
        "    newBoxplot = sns.boxplot(x = catName,y = \"logLoss\",data = trainFrame)\n",
        "    #then set labels\n",
        "    sns.plt.xlabel(catName)\n",
        "    sns.plt.ylabel(\"$\\log(Loss)$\")\n",
        "    sns.plt.title(\"$\\log(Loss)$ on \" + catName)\n",
        "topFiveFilteredFeatures = [feat[len(\"int_\"):] for feat in topFiveFeatures]\n",
        "for category in topFiveFilteredFeatures:\n",
        "    plotCategory(category)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5a820447-6ed4-4621-79e1-e02c5e1c704f"
      },
      "source": [
        "_Figure 6: Boxplots of $\\log(Loss)$ given particular levels of the top five categories for absolute correlation with $\\log(Loss)$._\n",
        "\n",
        "We see that the effects we are considering make meaningful differences on on the inter-quartile range of $\\log(Loss)$, but the general five number summary range stays pretty close across all levels of a given category. This suggests to me that the univariate predictive effects will likely be not that strong, and so we will likely need to spend some time considering the interaction effects available across our categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "06b833ef-0a67-743a-bcfa-0e3c84404a1c"
      },
      "source": [
        "TODO:\n",
        "\n",
        "* Finish Bivariate Analysis\n",
        "\n",
        "* Work on studying potential interaction effects among features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "40361bff-a7dc-49c9-05e1-18762168e383"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}