{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df\n\nimport matplotlib.pyplot as plt\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=True,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"9b44679092676ad6efc62ec551026480dacc2af3"},"cell_type":"code","source":"train = import_data(\"../input/train.csv\")\ntest = import_data(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73c1873b845e31f4d6e32f06655a0ccd7c72ccd7"},"cell_type":"code","source":"test_id = test['id']\ntest.drop(['id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18d19999d24d1a3d835fdc8f26e56eb4da3e68f0"},"cell_type":"markdown","source":"# LGB"},{"metadata":{"trusted":true,"_uuid":"045ac44ee4517ba793a2494dd333d45b95010997"},"cell_type":"code","source":"import lightgbm as lgb\ndic = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\ndic1 = {'CA':0,'DA':1,'SS':3,'LOFT':4}\ntrain[\"event\"] = train[\"event\"].apply(lambda x: dic[x])\ntrain[\"event\"] = train[\"event\"].astype('int8')\ntrain['experiment'] = train['experiment'].apply(lambda x: dic1[x])\ntest['experiment'] = test['experiment'].apply(lambda x: dic1[x])\n\ntrain['experiment'] = train['experiment'].astype('int8')\ntest['experiment'] = test['experiment'].astype('int8')\n\ny = train['event']\ntrain.drop(['event'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4376d785f63904185ce35dd4b42acf0743ffcae"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.4, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dac024ffd193fb768ef785d8df05ac576b521ad"},"cell_type":"markdown","source":"# Imbalance learning\n\n* What is the Class Imbalance Problem?\nIt is the problem in machine learning where the total number of a class of data (positive) is far less than the total number of another class of data (negative). This problem is extremely common in practice and can be observed in various disciplines including fraud detection, anomaly detection, medical diagnosis, oil spillage detection, facial recognition, etc.\n\n* Why is it a problem?\nIf there is a dataset consisting of 10000 genuine and 10 fraudulent transactions, the classifier will tend to classify fraudulent transactions as genuine transactions. The reason can be easily explained by the numbers. Suppose the machine learning algorithm has two possibly outputs as follows:\n    \n        * Model 1 classified 7 out of 10 fraudulent transactions as genuine transactions and 10 out of 10000 genuine transactions as fraudulent transactions.\n        * Model 2 classified 2 out of 10 fraudulent transactions as genuine transactions and 100 out of 10000 genuine transactions as fraudulent transactions.\n \n \nIf the classifier’s performance is determined by the number of mistakes, then clearly Model 1 is better as it makes only a total of 17 mistakes while Model 2 made 102 mistakes. However, as we want to minimize the number of fraudulent transactions happening, we should pick Model 2 instead which only made 2 mistakes classifying the fraudulent transactions. Of course, this could come at the expense of more genuine transactions being classified as fraudulent transactions, but will be a cost we can bear for now. Anyhow, a general machine learning algorithm will just pick Model 1 than Model 2, which is a problem. In practice, this means we will let a lot of fraudulent transactions go through although we could have stopped them by using Model 2. This translates to unhappy customers and money lost for the company."},{"metadata":{"_uuid":"69f0101169b88089183fd5d6372aa9288f47ac09"},"cell_type":"markdown","source":"# imbalanced-learn package\nimbalanced-learn is a python package offering a number of re-sampling techniques commonly used in datasets showing strong between-class imbalance. It is compatible with scikit-learn and is part of scikit-learn-contrib projects."},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"af3bdceee0cb509c53c70bf894df85c820902b13"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nX_train, y_train = SMOTE().fit_resample(X_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94576568cf66b0bcffb29ea947b8a07b237e4fb0"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import VotingClassifier\nimport lightgbm as lgb\n\n\nclf1 = lgb.LGBMClassifier(\n        n_estimators=100,\n        learning_rate=0.04)\nclf2 = lgb.LGBMClassifier(\n        n_estimators=300,\n        learning_rate=0.01)\nclf3 = lgb.LGBMClassifier(\n        n_estimators=500,\n        learning_rate=0.02)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea61b82edc718a879b6c0a78fd357b4a015c4b6e"},"cell_type":"code","source":"clf1.fit(X_train, y_train)\npred1 = clf1.predict(X_test)\nprint(\"lgbm1: \", accuracy_score(pred1, y_test))\n\npred = clf1.predict_proba(test)\nsub = pd.DataFrame(pred,columns=['A', 'B', 'C', 'D'])\nsub['id'] = test_id\ncols = sub.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nsub = sub[cols]\nsub.to_csv(\"sub_lgb1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94008d25a9d534360699e4cb59f139ac2e43418e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}