{"cells":[{"metadata":{"_cell_guid":"7a0d2d61-5837-4702-b352-d6ba8035aa58","_uuid":"be5c2ca7adc73c81f55877dca7b3c9c4fbd6e16d"},"cell_type":"markdown","source":"# Introduction\n"},{"metadata":{"_cell_guid":"a30d8eb5-9498-4805-9464-6b73b34eab73","_uuid":"0a41ac02abe8f53aa2443f1ec38c5ae3492d5384"},"cell_type":"markdown","source":"This notebook simply gives the set of features you can use to get a good score on the Leaderbord."},{"metadata":{"_cell_guid":"8530045b-6de1-46a9-ab64-60cfd56ef3b6","_uuid":"3b451393afcbaf2f519c8e6732c10804c1936469","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport os\nfrom nltk.corpus import stopwords\nimport string\nimport re\nfrom sklearn import svm\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk import word_tokenize, pos_tag, ne_chunk, tree2conlltags","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a81874f3-6977-46a9-b205-b9114c601241","_uuid":"33a188c241e8b10fff820d1a289738e728d79378","trusted":false},"cell_type":"code","source":"# Load the data\ndf_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")\ndf_sample = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"023c1f68-59a6-4621-bf63-e4f232dffd1d","_uuid":"a8488038974bb362e5a91fae8d3d17392965cb30"},"cell_type":"markdown","source":"## Features which have been engineered"},{"metadata":{"_cell_guid":"3ed49487-b6fb-4257-a981-c2c15da0b247","_uuid":"0fa5bc7c9f11c9dd8a06901217be08b3b696e045"},"cell_type":"markdown","source":"NLTK has very basic sentiment analyzer"},{"metadata":{"collapsed":true,"_cell_guid":"45a08703-b138-4c06-b16a-33d6d02616fc","_uuid":"3ea0fd9ff0a80496bbcc858eb90008045b786823","trusted":false},"cell_type":"code","source":"sia = SentimentIntensityAnalyzer()\ndef sentiment_nltk(text):\n    res = sia.polarity_scores(text)\n    return res['compound']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"af760eae-9172-4026-8738-354af125a606","_uuid":"4dec4eef9f9240538cebd74516ef285158bdfd8a"},"cell_type":"markdown","source":"Some authors really loves commas, moreover commas are very common to appear in poems!"},{"metadata":{"collapsed":true,"_cell_guid":"fcee0802-8a3f-4fe1-822d-d7c4dafcdb53","_uuid":"7db82f5d2751ca2846216483a7e4e1dedba39f9f","trusted":false},"cell_type":"code","source":"def chars_between_commas(text):\n    return np.mean([len(chunk) for chunk in text.split(\",\")])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3add46fd-69e8-47d2-a599-e4ec2be7c966","_uuid":"a8da85ed091448a41380a1000b422033baa59aaa"},"cell_type":"markdown","source":"Looks like that some texts have non ASCII chars, well, those chars not unknown but yet not recognized"},{"metadata":{"collapsed":true,"_cell_guid":"226e60f1-117e-4a8f-b38a-8eb230f289f7","_uuid":"0b370533d3cde72833fb0cffacfb987187aea1ba","trusted":false},"cell_type":"code","source":"def count_unknown_symbols(text):\n    symbols_known = string.ascii_letters + string.digits + string.punctuation\n    return sum([not x in symbols_known for x in text])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"715d0ce5-cf7f-4b7d-ab87-a808702b4fb6","_uuid":"88363b67aab696749876f55094121fddf535563b"},"cell_type":"markdown","source":"**Named Entity Recognition** task is quite non-trivial, and there are a lot of dedicated studies re;ated to the field, but NLTK offers the tool to deal with with problem. I would not say that it is state of the art, but it enought for fast prototyping and educational purposes."},{"metadata":{"collapsed":true,"_cell_guid":"7959af0b-4595-4060-8de0-441f4cf14469","_uuid":"4b5de8ad2f1fd6f3b14d9a261106acfe6408448e","trusted":false},"cell_type":"code","source":"def get_persons(text):\n    # Some names have family and given names, but both belong to the same person\n    # Bind them!\n    def bind_names(tagged_words):\n        names = list()\n        name = list()\n        # Bind several consequtive words with 'PERSON' tag\n        for i, w in enumerate(tagged_words):\n            if i == 0:\n                continue\n            if \"PERSON\" in w[2]:\n                name.append(w[0])\n            else:\n                if len(name) != 0:\n                    names.append(\" \".join(name))\n                name = list()\n        return names\n        \n    res_ne_tree = ne_chunk(pos_tag(word_tokenize(text)))\n    res_ne = tree2conlltags(res_ne_tree)\n    res_ne_list = [list(x) for x in res_ne]\n    return bind_names(res_ne_list)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6fae26fe-790f-4626-84b7-96d122ec103e","_uuid":"fd2586cc1beb75396bfd45c9356e20e1c422ab75"},"cell_type":"markdown","source":"Intersection of tokens clouds (words or n-grams) related to the authors"},{"metadata":{"collapsed":true,"_cell_guid":"b612017f-e113-4a7b-9c93-f0d9585b7566","_uuid":"c6ffad3c8e077f99c640bbf2329145dcd5fc46c9","trusted":false},"cell_type":"code","source":"class WordCloudIntersection():\n    \n    def __init__(self, stopwords=list(), punctuation=list(), stemmer=None, ngram=1):\n        self.stopwords = stopwords\n        self.punctuation = punctuation\n        self.remove = self.stopwords + self.punctuation\n        self.clouds = dict()\n        self.texts = dict()\n        self.stemmer = stemmer\n        self.ngram = ngram\n    \n    def find_ngrams(self, input_list, n):\n        return [\" \".join(list(i)) for i in zip(*[input_list[i:] for i in range(n)])]\n    \n    # It would be much  more correct to call this function 'get_tokens'\n    # it extracts not only words, but n-grams as well\n    def get_words(self, text):\n        words = nltk.tokenize.word_tokenize(text)\n        words = [w for w in words if not w in self.remove]\n        if not self.stemmer is None:\n            words = [self.stemmer.stem(w) for w in words]\n        \n        if self.ngram > 1:\n            words = self.find_ngrams(words, self.ngram)\n        return words\n    \n    # Jaccard distance again\n    def relative_intersection(self, x, y):\n        try:\n            return len(x & y)/len(x | y)\n        except:\n            return 0.0\n    \n    def fit(self, x, categories, data_train, data_test=None):\n        cat_names = np.unique(data_train[categories])\n        \n        text_train = \" \".join(list(data_train[x]))\n        text_test = \"\"\n        if not data_test is None:\n            text_test = \" \".join(list(data_test[x]))\n        \n        # Tokens presenting in both train and test data\n        words_unique = self.get_words((text_train + text_test).lower())\n        \n        for cat in cat_names:\n            self.texts[cat] = (\" \".join(list(data_train[x][data_train[categories] == cat]))).lower()\n            words = self.get_words(self.texts[cat])\n            self.clouds[cat] = pd.value_counts(words)\n        \n        # use only tokens presented in both train and test data, \n        # feature will force your model to overfit to the train data otherwise    \n        for cat in cat_names:\n            self.clouds[cat] = self.clouds[cat][list(set(self.clouds[cat].index) & set(words_unique))]\n        \n        # Keep only author-specific tokens\n        for cat in cat_names:\n            key_leftover = list(set(cat_names) - set([cat]))\n            bigrams_other = set(self.clouds[key_leftover[0]].index) | set(self.clouds[key_leftover[1]].index)\n            self.clouds[cat] = self.clouds[cat][list(set(self.clouds[cat].index) - bigrams_other)]\n        \n    def transform(self, x, data):\n        intersection = dict()\n        prefix = '_intersect_'\n        if self.ngram > 1:\n            prefix = '%s-gram%s' % (self.ngram, prefix)\n        else:\n            prefix = 'word' + prefix\n        for key in self.clouds.keys():\n            category_words_set = set(self.clouds[key].index)\n            intersection[prefix+key] = list()\n            for text in data[x]:\n                unique_words = set(self.get_words(text.lower()))\n                fraction = self.relative_intersection(unique_words, category_words_set)\n                intersection[prefix+key].append(fraction)\n        return pd.DataFrame(intersection)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1f867116-bc1d-4136-8bb8-ffd86a4c58d4","_uuid":"5d7b5600fd88090056b3e0652ea17b8eaa662e33","trusted":false},"cell_type":"code","source":"\n# Split text into words\ndef get_words(text):\n    words = nltk.tokenize.word_tokenize(text)\n    return [word for word in words if not word in string.punctuation]\n\n# string.punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\ndef count_punctuation(text):\n    return sum([x in string.punctuation for x in text])\n\ndef count_capitalized_words(text):\n    return sum([word.istitle() for word in get_words(text)])\n\ndef count_uppercase_words(text):\n    return sum([word.isupper() for word in get_words(text)])\n    \ndef count_tokens(text, tokens):\n    return sum([w in tokens for w in get_words(text)])\n\ndef first_word_len(text):\n    return len(get_words(text)[0])\n\ndef last_word_len(text):\n    return len(get_words(text)[-1])\n\ndef symbol_id(x):\n    symbols = [x for x in string.ascii_letters + string.digits + string.punctuation]\n    return np.where(np.array(symbols) == x)[0][0]\n\n# It is not a feature! It is just Jaccard distance\ndef relative_len(set_x, set_y):\n    return len(set_x & set_y)/len(set_x | set_y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1064d077-f86d-4ac8-b74b-6f7fb4c408a6","_uuid":"7d079717b22cd8f17070b3229adbc58277bc2a05"},"cell_type":"markdown","source":"## Generate features"},{"metadata":{"_cell_guid":"25557424-cfd4-4c8b-a205-a8ca3c7759ed","_uuid":"8fd4f20050168726a499ecbe6cab5b148e8fec7e"},"cell_type":"markdown","source":"### Bigram clouds"},{"metadata":{"collapsed":true,"_cell_guid":"e66b744f-6a1f-4eee-9e97-70a73067270f","_uuid":"dc93100ee0b02c79bb4709c2ac4f65672ab05b83","trusted":false},"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\nbigci = WordCloudIntersection(stopwords=stopwords, \n                            punctuation=list(string.punctuation),\n                            stemmer=nltk.stem.SnowballStemmer('english'), ngram=2)\nbigci.fit(x='text', categories='author', data_train=df_train, data_test=df_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"956d250c-3b2c-4486-80c0-547c7c5c4723","_uuid":"da959a4c52432cee0de9aa93814f1d2e8c887aa7","trusted":false},"cell_type":"code","source":"df_train_intersections = bigci.transform(x='text', data=df_train)\ndf_test_intersections = bigci.transform(x='text', data=df_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e47cdef3-4c68-43ea-be4d-6f8e3d8f91ee","_uuid":"295f776d4b458201647749cc23bcc35c09c2cb5f","trusted":false},"cell_type":"code","source":"df_train = pd.concat([df_train, df_train_intersections], axis=1)\ndf_test = pd.concat([df_test, df_test_intersections], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6df6b7d9-8baa-4dbf-84c7-8c3de971ba99","_uuid":"3d14dfc34b11fd8052ad82f56bba2623453aede7"},"cell_type":"markdown","source":"Theoretically this feature should work very well for EAP"},{"metadata":{"collapsed":true,"_cell_guid":"9a971947-0cc1-4462-be52-ef788183a57d","_uuid":"afddd69fb424c8a0c142d795a8d866ddaa39b096","trusted":false},"cell_type":"code","source":"_, axes = plt.subplots(1, 3, figsize=(16,6))\nsns.violinplot(x='author', y='2-gram_intersect_EAP', data=df_train, ax=axes[0])\nsns.violinplot(x='author', y='2-gram_intersect_HPL', data=df_train, ax=axes[1])\nsns.violinplot(x='author', y='2-gram_intersect_MWS', data=df_train, ax=axes[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c65e58c-ef24-4450-8a04-ea2e6277dce5","_uuid":"f16397348be4147ebc6e976b5c075fb10f5efd79"},"cell_type":"markdown","source":"### Prepare data for names of persons intersection"},{"metadata":{"collapsed":true,"_cell_guid":"67894f19-708c-402d-b7c6-6d15c711c89e","_uuid":"52e0b63d3a30c6c7c04522f45a231d7212a35208","trusted":false},"cell_type":"code","source":"text_EAP = \" \".join(list(df_train['text'][df_train['author'] == \"EAP\"]))\ntext_HPL = \" \".join(list(df_train['text'][df_train['author'] == \"HPL\"]))\ntext_MWS = \" \".join(list(df_train['text'][df_train['author'] == \"MWS\"]))\npersons_EAP = set(get_persons(text_EAP))\npersons_HPL = set(get_persons(text_HPL))\npersons_MWS = set(get_persons(text_MWS))\n# Keep only names related to the authors without any intersections with others\npersons_EAP = persons_EAP - persons_HPL - persons_MWS\npersons_HPL = persons_HPL - persons_EAP - persons_MWS\npersons_MWS = persons_MWS - persons_EAP - persons_HPL","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1de1e274-5aac-4f1e-abab-848ae1b8b77a","_uuid":"9215542f6236d0a904dd195dffa79398b0d9237f","trusted":false},"cell_type":"code","source":"for df, name in zip([df_train, df_test], [\"train\", \"test\"]):\n    df['persons_EAP_frac'] = df['text'].apply(lambda x: relative_len(persons_EAP, set(get_persons(x))))\n    df['persons_HPL_frac'] = df['text'].apply(lambda x: relative_len(persons_HPL, set(get_persons(x))))\n    df['persons_MWS_frac'] = df['text'].apply(lambda x: relative_len(persons_MWS, set(get_persons(x))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8d9f67d7-8d46-4d87-a047-508a4feb2d19","_uuid":"00ab5e5890208df06ba66cee89056fa87b85fe55","trusted":false},"cell_type":"code","source":"_, axes = plt.subplots(1, 3, figsize=(16,6))\nsns.violinplot(x='author', y='persons_EAP_frac', data=df_train, ax=axes[0])\nsns.violinplot(x='author', y='persons_HPL_frac', data=df_train, ax=axes[1])\nsns.violinplot(x='author', y='persons_MWS_frac', data=df_train, ax=axes[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ebba126c-2d44-48d7-9c63-62f2fc0903b7","_uuid":"3d0506b252abe3db1959012faea8b6e1244910e3"},"cell_type":"markdown","source":"### Remaining features"},{"metadata":{"_cell_guid":"d710b6cb-ba11-4f7d-8af3-0fbc7ca3bcdc","_uuid":"4e09ce6c7b7e2b4bb54cacbae1bb33d75bf73f4f","trusted":false,"collapsed":true},"cell_type":"code","source":"for df, name in zip([df_train, df_test], [\"train\", \"test\"]):\n    print(\"Generating features for %s...\" % name)\n    words_count = df['text'].apply(lambda x: len(get_words(x)))\n    chars_count = df['text'].apply(lambda x: len(x))\n    \n    print(\"\\tFeatures related to words\")\n    df['capitalized_words_frac'] = df['text'].apply(lambda x: count_capitalized_words(x))/words_count\n    df['uppercase_words_frac'] = df['text'].apply(lambda x: count_uppercase_words(x))/words_count\n    df['single_frac'] = df['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))/words_count\n    df['plural_frac'] = df['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))/words_count\n    \n    print(\"\\tFeatures related to chars\")\n    df['unknown_symb_frac'] = df['text'].apply(lambda x: count_unknown_symbols(x))/chars_count\n    df['chars_between_commas_relative'] = df['text'].apply(chars_between_commas)/chars_count   \n    \n    df['first_word_len_relative'] = df['text'].apply(lambda x: first_word_len(x))/chars_count\n    df['last_word_len_relative'] = df['text'].apply(lambda x: last_word_len(x))/chars_count\n        \n    df['sentiment'] = df['text'].apply(sentiment_nltk)\n    df['first_symbol_id'] = df['text'].apply(lambda x: symbol_id(x[0]))\n    df['last_symbol_id'] = df['text'].apply(lambda x: symbol_id(x[-1]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6af26449-88ba-433b-8a91-991d1e09e68c","_uuid":"d229b7ca84d0cfe6aecd8b1be87d2b1fda1e4de5","trusted":false,"collapsed":true},"cell_type":"code","source":"features = ['capitalized_words_frac', 'uppercase_words_frac', \n            'single_frac', 'plural_frac', 'unknown_symb_frac', \n            'chars_between_commas_relative', 'first_word_len_relative', \n            'last_word_len_relative', 'sentiment', 'first_symbol_id', 'last_symbol_id']\n_, axes = plt.subplots(4, 3, figsize=(16,16))\nfor i, feature in enumerate(features):\n    sns.violinplot(x='author', y=feature, data=df_train, ax=axes[int(i/3),i%3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a2a88fa8-16c1-4533-896e-7db8ee16b317","_uuid":"6922ac639d8e1d23dba14f979ea3baf82994099e"},"cell_type":"markdown","source":"Feel free to use these features if you like"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}