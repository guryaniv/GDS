{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"<font color=blue>\nUpdates 2.0: I made a huge mistake in my prior version of kernel by dropping the entire set of BINARY VARIABLES. I have just realized that these are truly likely One-Hot Encoded variables and contain much inferential information; in the end, I did a univariate (pair-wise) linear regression on X and y, ending up getting a dozen statistically significant variables. \n\n\nUpdates 1.0: Mutual Information Regression is added to supplement ANOVA in Categorical feature selection, results reveal robust evidence that these three variables, '0f49e0f05', '7bf58da23', 'c16a7d537', are the only ones significant to be considered within the category. \n</font>\n## In a nutshell:\n\nThis notebook aims to extract a small number of features from the vast pool of unlabelled variables. \n\nIt first subsets features into univariate, binary, categorical and continuous. Then it goes on to drop both the univariate and binary (501 in total). For the categorical and continuous, the following are kept.\n> - Categorical: ['0f49e0f05', '7bf58da23', 'c16a7d537']\n> - Continuous: ['fb0f5dbfe', 'eeb9cd3aa', '58e2e02e6', '9fd594eec', '241f0f867', 'b43a7cfd5', 'd6bb78916', '66ace2992', '402b0d650', '58232a6fb', '20aa07010', 'f190486d6', '6eef030c1', '15ace8c9f']\n\n\n\n\nEnjoy!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1499f784069e4e378ec1caad64204443af73c6e5"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nimport plotly.offline as pyo\nimport plotly.graph_objs as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9e3c078470bec297102368b3caedbd598042cab7"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv', index_col=0)\ntest_df = pd.read_csv('../input/test.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d4d46683a51ad2cc0e86fca96aec234bf6a5e06e"},"cell_type":"code","source":"target_df = train_df['target']\ntarget_df_log = np.log(target_df)\ntrain_df.drop('target', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37a071afc4b57c77ec2d299be489e369c81246d5"},"cell_type":"markdown","source":"## Dimension and Data Type\n1. Target Distribution\n2. Check Missing Values\n3. Check Data Types\n4. Reconcile Data Types btw Train and Test"},{"metadata":{"trusted":true,"_uuid":"b27221d10a3fb1b2e8a39d5f0ab8348c6469dd35","collapsed":true},"cell_type":"code","source":"variable_dict = {'Train Set': train_df, 'Test Set': test_df, 'Target': target_df}\nfor i in ['Train Set', 'Test Set', 'Target']:\n    print('The Dimension (row x col) of %s is: ' %i, variable_dict[i].shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cab063e7e5d1fc5615ae39508bb334c65a2ee20b"},"cell_type":"markdown","source":"#### Distribution of Target Variable\n> Descriptive: \n  > - Target is nowhere near bar bell shaped and has very extreme fat tails\n  \n  > - The most common values are mutliples of million, e.g. 2M, 10M, etc.\n  \n  > - Large extreme values make predictions harder\n\n\n***\n- ** Based on the magnitude of the value, combined with the business nature of Santander, the transaction amount is likely to be determined more heavily by qualitative characteristics.**"},{"metadata":{"trusted":true,"_uuid":"05f8e424b92fc234d7e8700dd236118ec5b7b7ef","collapsed":true},"cell_type":"code","source":"target_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"203581792f99292e5d6c6262745c2b51b2cc7068","collapsed":true},"cell_type":"code","source":"# Sort most common values\nfrom collections import Counter\ncnt = Counter(target_df)\ncnt.most_common(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"882681aeba8dbbf9b3997740d3dd5d1f02c9b7c2","collapsed":true},"cell_type":"code","source":"pyo.init_notebook_mode()\ndata = [go.Histogram(x = target_df)]\npyo.iplot(figure_or_data= data, filename='Histogram_Target')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14ea3f37cc719a4dc49ba949da69ee6b798448bd"},"cell_type":"markdown","source":"#### Check Missing Value\n- Luckily we have no missing values between train and test"},{"metadata":{"trusted":true,"_uuid":"b1265c8194b8c00cf207db2cc90f2f9302d3d526","collapsed":true},"cell_type":"code","source":"print('Sum of NAs in Train = ', train_df.isnull().sum(axis = 0).unique())\nprint('Sum of NAs in Test = ',  test_df.isnull().sum(axis = 0).unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7060f8114d6930ee68f9e12e0f4293ce2b754fc6","collapsed":true},"cell_type":"code","source":"## Train set contains a portion of int64, most of which are categorical values, i.e.[0, some_other_value].\ntrain_df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6316c9bf9a56e6ec5d8ed3a01363817e4b4e7d1","collapsed":true},"cell_type":"code","source":"## Test set only contains float64 dtype\ntest_df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a115de30342e21f991f608a38b139af4cede00b1","collapsed":true},"cell_type":"code","source":"# So we convert all Train variables to float64\ntrain_df = train_df.astype(dtype = 'float64',copy = True)\ntrain_df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e3e22dd7ea7d59a83c6b2fada15ada34fca118f"},"cell_type":"markdown","source":"## Feature Importance: Categorizing variable types before selection\n > ### Subset features into 'Constant, Binary, Categorical, and Continuous\n - Constant: only has value zero\n - Binary: embodies no meaningful binary information\n - Categorical: univariate ANOVA test to identify features\n - Continuous: regularized regression, random forest, gradient boosting"},{"metadata":{"_uuid":"16f4ada9a5a3dd3ef12ab9cee277582f715b77a5"},"cell_type":"markdown","source":"<font color=blue>_ This function filters features with n or less unique values and hence serves to subset feature types _</font>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e916a42fa155336bbfe51b956a735e93820b4dc4"},"cell_type":"code","source":"def categorical_filter(df, low_exclusive = 2, high_inclusive = 15):\n    \"\"\"function returns features (col_names) that have unique values\n    less than or equal to n_categories\n    \n    \"\"\" \n    list_of_features = []\n    for i in df.columns:\n        if low_exclusive == high_inclusive:\n            if df[i].nunique() <= low_exclusive :\n                list_of_features.append(i)\n        else:\n            if df[i].nunique() <= high_inclusive and df[i].nunique() > low_exclusive :\n                list_of_features.append(i)\n    return list_of_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c64c88ec429cba2c283b5e855647039505c17950","collapsed":true},"cell_type":"code","source":"category_1_cols = categorical_filter(train_df, 1, 1 )\nprint('# of Constant Variables = ',len(category_1_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d47ba2aff5882da470f6bb69314bfd1937d765be","collapsed":true},"cell_type":"code","source":"category_2_cols = categorical_filter(train_df, 1, 2)\nprint('# of Binary Varialbes = ',len(category_2_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"142a555c519c8731a869533d96b973bd6b881e82","collapsed":true},"cell_type":"code","source":"category_15_cols = categorical_filter(train_df, 2, 15)\nprint('# of Variables less than or equal to 15 categories = ',len(category_15_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"109119846143f5db0ccd9f21c8bff647eea4a655","collapsed":true},"cell_type":"code","source":"remainder_cols = categorical_filter(train_df, 15, len(train_df))\nprint('# of Continuous Variables (with more than 15 categories) = ',len(remainder_cols))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71b6a353b27c5056df2b4b6aa209108e5e69cb83"},"cell_type":"markdown","source":"### Constant Variable: drop it!\n***\n- ** there are 256 features, all of which possess single value 0.0 **\n\n- ** As they do not contain information as to the change in Target, I'd have them truncated **\n\n\n*Notice that if you randomly choose a category_1_cols feature from the Test Set, you end up getting a majority of 0.0 (in fact, 99.9% are zeros). So it can be fairly deducted that the non-zero observations are just noise"},{"metadata":{"trusted":true,"_uuid":"ea33e9c529d2f0a640b348f041e5046a5e60f845","collapsed":true},"cell_type":"code","source":"# check if all constants are equal to 0\n((train_df[category_1_cols] == 0.0).all()).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"826720095dbda379ee90eaaf9fc3fd85a244792a","collapsed":true},"cell_type":"code","source":"# See how these features cause noise in the Test Set\ntest_df[category_1_cols[np.random.randint(0,len(category_1_cols))]].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"354279bb3801c3d9ac662662a1239981abce4d6f"},"cell_type":"markdown","source":"### Binary Variable: def one-hot-encoded!\n***\n- 245 of these.\n\n- These features all contain only one non-zero value, indicating that they are extremely likely to be one-hot encoded varialbes."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"daed23ca6f7cd73eef99f2b8e1ed8c5822f4a586"},"cell_type":"code","source":"n_rows = len(train_df)\n\ntrain_df_binary = train_df[category_2_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01d88e31fb1928b60610c2c7a951276f871d4b3c"},"cell_type":"code","source":"count_nonzero = pd.DataFrame(data = np.zeros((2,len(category_2_cols))), index=['zero', 'nonzero'], columns=category_2_cols)\nfor i in train_df_binary.columns:\n    n_zero = train_df_binary[i].value_counts()[0]\n    n_nonzero = n_rows - n_zero\n    count_nonzero[i].iloc[0] = n_zero\n    count_nonzero[i].iloc[1] = n_nonzero\ncount_nonzero","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d47d3a1ef40d02a134fd6c67faf2ec5826d675f"},"cell_type":"code","source":"from sklearn.feature_selection import f_regression\n\nf, p_val = f_regression(train_df_binary,target_df_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4149cb5389c932721d6d435997fd7deda9c9dab"},"cell_type":"code","source":"f_reg_df = pd.DataFrame(np.array([f, p_val]).T, index = train_df_binary.columns, columns = ['f-statistic', 'p-value'])\nbinary_stored_features = f_reg_df[f_reg_df['p-value'] < 0.05].sort_values(by = 'f-statistic', ascending = False)\nbinary_stored_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f8f0fbc80b397f1819cdf7939b0a7d54ec42033"},"cell_type":"code","source":"selected_features_binary = np.array(binary_stored_features.index)\n\nprint('Features selected among binary variables: \\n', selected_features_binary)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4d5eba876ff9e5deb2015503cf2745bd21b69bb"},"cell_type":"markdown","source":"### Categorical Variable: \n\n> - As prescribed above, I limit the number of unique values of categorical variables to be (2,15]\n> - 15 is an arbitrary cutoff. Later on, a lesser or greater number may make more sense later on\n\n- ANOVA is used, as a parametric method, to identify if there's 1) linear correlation; 2) difference in variance in each feature's value spectrum\n- Mutual Information is a non-parametric, entropy based method. It identifies **DEPENDENCY** (both linear and non-linear) between Y and each X. \n- Tree-based method. Random Forest is added here to complement the previous two methods to spot overlaps. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"216164da29adca9a90278ddfa3fd4f93b5ed4ff1"},"cell_type":"code","source":"from scipy.stats import f_oneway\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"83c8c689eb7e5698f66f4feddce444b2f3fd2d9f"},"cell_type":"code","source":"train_df_categorical = train_df[category_15_cols]\n\n# Label encode categories\nle=LabelEncoder()\n\n# create a dataframe to store label encoded values\ntrain_df_categorical_le = train_df_categorical.copy()\n\nfor i in train_df_categorical.columns:\n    le.fit(train_df_categorical[i])\n    train_df_categorical_le[i] = le.transform(train_df_categorical[i]).copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9d43ad0bce5c3fba8fda9be9c6f12ac72b8f107"},"cell_type":"markdown","source":"#### ANOVA Test\n- Drawback: ANOVA relies on the assumption that distributions of Y will be different across varying values of X. The greater the variance of Y is relative to each categorical value of X, the higher the f-statistic is.\n- It only captures linear correlation, therefore if non-linear relationship exists, we need other tests to complement ANOVA "},{"metadata":{"_uuid":"4405aa57d97b8ef990b118f4aa7df71a72355992"},"cell_type":"markdown","source":"<font color=blue>_ function conducts univariate ANOVA test between each categorical feature and target _</font>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5f1351bdc44db77bd3e8d3f45059cb0bc701158e"},"cell_type":"code","source":"def one_way_anova(categorical_data, target_data):\n    # create an empty dataframe to store f-statistic and p-value\n    stats_df = pd.DataFrame(np.zeros((len(categorical_data.columns), 2)), index = categorical_data.columns, columns = ['f-statistic', 'p-value'])\n    \n    # merge independent dataframe with target \n    merged_df = categorical_data.merge(pd.DataFrame(target_data, columns = ['target']), left_index=True, right_index=True)\n    for i in categorical_data.columns:\n        unique_values = categorical_data[i].unique()\n        tuple_list = []\n        for value in unique_values:\n            store_values = merged_df['target'].loc[merged_df[i]==value].values\n            tuple_list.append(store_values)\n         \n        # get stats from f_oneway test\n        statistic, pvalue = f_oneway(*tuple_list)\n        stats_df.loc[i, 'f-statistic'] = statistic\n        stats_df.loc[i, 'p-value'] = pvalue\n        \n    return stats_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98dde2301ad7e5d693997a6cdf9b95d3a30184f3","collapsed":true},"cell_type":"code","source":"f_test_df = one_way_anova(train_df_categorical_le, target_df_log)\nf_top10_features = f_test_df[f_test_df['p-value'] < 0.05].sort_values(by = 'f-statistic', ascending = False).head(10)\nf_top10_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a43b2fb5d41b3c852eaf5be9cd67234e4a90741","collapsed":true},"cell_type":"code","source":"sns.heatmap(data = f_top10_features, annot=True )\nplt.title('Top 10 Categorical Features -  Correlation w/ Target and log(Target)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79de05ca21c26254ce5ede01a1fb86b7c236d705"},"cell_type":"markdown","source":"#### Mutual Information (MI)\n- MI measures the dependence of X to Y. If X and Y are independent, their mutual information is 0; if fully dependent, mutual information is 1. \n- Unlike ANOVA or f-test, MI does capture non-linear relationship. "},{"metadata":{"trusted":true,"_uuid":"1cb13c3d41bc5eb564c83a107aa513ab219d6852","collapsed":true},"cell_type":"code","source":"mi = mutual_info_regression(train_df_categorical_le, target_df_log, discrete_features = True, \n                             n_neighbors=5, copy=True, random_state=None)\nmi_df = pd.DataFrame(mi, index = train_df_categorical.columns, columns = ['mutual_information'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cf639510a7c1dfa460fddc6f4bb5bc37d0d223a","collapsed":true},"cell_type":"code","source":"mi_top10_features = mi_df.sort_values(by = 'mutual_information', ascending=False).head(10)\nsns.heatmap(data = mi_top10_features, annot=True )\nplt.title('Top 10 Categorical Features - Mutual Information Regression - Discrete Features')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3d54eb7cbd33cc75bdd5c59fe412252fe6e66a4"},"cell_type":"markdown","source":"#### Tree-Based Model"},{"metadata":{"trusted":true,"_uuid":"62082c19024b7ecb48f4894c9be9786679ca2a56","collapsed":true},"cell_type":"code","source":"rf_cat = RandomForestRegressor(n_estimators=100, criterion='mse', max_features='sqrt')\nrf_cat.fit(train_df_categorical_le, target_df_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe8d6048f711b65e480486220024072c50896b06","collapsed":true},"cell_type":"code","source":"# Store the top 10 most important features based off rf regressor\nrf_cat_feature_importance_df = pd.DataFrame(rf_cat.feature_importances_, train_df_categorical.columns, columns=['Importance_Value'])\nrf_cat_top10_features = rf_cat_feature_importance_df.sort_values(by = ['Importance_Value'], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21f815a1efb026c740a49f305a5fabca7d85923c","collapsed":true},"cell_type":"code","source":"sns.heatmap(data = rf_cat_top10_features, annot=True )\nplt.title('Top 10 Categorical Features - Random Forest - Feature Importance Value')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c01cc72deaf905d28ac6a93020316cfea1f76ef"},"cell_type":"markdown","source":"#### Summary:\n- all 3 methods point to this subset of features: {'0f49e0f05', '7bf58da23', 'c16a7d537'}"},{"metadata":{"trusted":true,"_uuid":"cd4444180e65c14403adcc4bee6e9372d080f9dd","collapsed":true},"cell_type":"code","source":"# Subset of intersection of both f-test and mi-test \n\nset(f_top10_features.index).intersection(mi_top10_features.index).intersection(rf_cat_top10_features.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8776c927b5b34c20db31491f4b4b33192a10f8ec"},"cell_type":"code","source":"selected_features_categorical = ['0f49e0f05', '7bf58da23', 'c16a7d537']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d4a18aa658675a5b335f1c4954021fedcb640c1","collapsed":true},"cell_type":"code","source":"# scatter plot btw target and shortlisted features\nindex_feature = selected_features_categorical\nplt.subplots(3,1,figsize=(5,14))\nfor i in range(1, 4):\n    col = index_feature[i-1]\n    plt.subplot(3, 1, i)\n    sns.regplot(x=train_df_categorical[col], y = target_df, fit_reg=False)\n    plt.xscale('log')\n    plt.yscale('log')\n    plt.tight_layout()\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89150b1f4af80539be5fff9cbfdcdefcf60e56bc"},"cell_type":"markdown","source":"### Continuous Variable: \n\n- ** L1 Regularization: Lasso **\n- ** Random Forest **\n- ** Gradient Tree Boosting **"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e76a659b2b0ed9d3e556d467bb3e571807111633"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"24ef157b8ee25fac4cee501b5292ba679d6eef32"},"cell_type":"code","source":"train_df_continuous = train_df[remainder_cols]\n\n# Standardize X variables\nscaler = StandardScaler()\nX_train_df_continuous = scaler.fit_transform(train_df_continuous)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0626ba8ee21eff69d4da2081a44f9f0a63fe8e6b"},"cell_type":"markdown","source":"#### Lasso Regression\n- L1 norm tends to produce sparse solutions by forcing weak features to have coefficient values of zero, which is neat for reducting the dimensionality of data\n- To prevent overfitting, we choose a range of alpha to explore the optimal penalty value\n- As alpha increases, the model complexity reduces, namely the degree of overfitting decreases.\n- Overall, only four features stand out to own non-zero coefficients"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3ced7878a9dfe717dd76d75cf39ec3f9060e86ad"},"cell_type":"code","source":"alpha = [0.2, 0.25, 0.275, 0.3, 0.325, 0.35]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f05dc5d74591eb05a258483f07f49afcb27fc9ad"},"cell_type":"code","source":"lasso_feature_coef_df = pd.DataFrame(np.zeros((len(remainder_cols), len(alpha))), index=remainder_cols, columns=alpha)\nfor a in alpha: \n    lasso = Lasso(alpha=a)\n    lasso.fit(X_train_df_continuous, target_df_log)\n    \n    lasso_feature_coef_df[a] = lasso.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b2aa80663f34d591703528534c10ec7a5cd6a27","collapsed":true},"cell_type":"code","source":"lasso_top10_features = lasso_feature_coef_df.reindex(index=lasso_feature_coef_df[0.30].abs()\\\n.sort_values(ascending = False).index).head(10)\n\nlasso_top10_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adfe639228770cc24432a9601b7d54f50244d296","collapsed":true},"cell_type":"code","source":"selected_features_lasso = lasso_top10_features.index[:4].tolist()\nprint('Features of significant coefficient include: \\n', selected_features_lasso)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f41616968af6d09f8e7fb15bafefd540eb06391"},"cell_type":"markdown","source":"#### Random Forest Regressor"},{"metadata":{"trusted":true,"_uuid":"03ae345664f2780d6bfacc31808bd02d2100a55c","collapsed":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=200, criterion='mse', max_features='sqrt')\nrf.fit(X_train_df_continuous, target_df_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"047ff8ef08790dfd1830b117df9b557f9e553b92"},"cell_type":"code","source":"# Store the top 30 most important features based off rf regressor\nrf_feature_importance_df = pd.DataFrame(rf.feature_importances_, index=remainder_cols, columns=['Importance_Value'])\nrf_top30_features = rf_feature_importance_df.sort_values(by = ['Importance_Value'], ascending=False).head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"855fdf7d76bfe79b8f2024d464afe08df8bc8668","collapsed":true},"cell_type":"code","source":"ax0 = sns.barplot(x = rf_top30_features.index, y = 'Importance_Value', data=rf_top30_features)\nax0.set_xticklabels(ax0.get_xticklabels(), fontsize = 12, rotation=40, ha=\"right\")\nplt.title('Top 30 Features - Random Forest Regression')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06eb58e905ff4961d057a1dcd6cfa9f6191810f7","collapsed":true},"cell_type":"code","source":"selected_features_rf = rf_top30_features[rf_top30_features.Importance_Value >= 0.005].index.tolist()\n\nprint('Features of high importance value include: \\n', selected_features_rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"552378cf412257d1aa655c4c465a343e040e47f6"},"cell_type":"markdown","source":"#### Gradient Tree Boosting Regression"},{"metadata":{"trusted":true,"_uuid":"84ec936383d1f896f01db25d2ea4e742be084789","collapsed":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(loss='ls', n_estimators=200, learning_rate=0.1, \n                                max_depth=8, max_features = 'sqrt',  \n                                min_samples_split = 500, random_state=0)\ngbr.fit(X_train_df_continuous, target_df_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"94e811edf6481f6eef7108981414e54f928c9768"},"cell_type":"code","source":"gbr_feature_importance_df = pd.DataFrame(gbr.feature_importances_, index=remainder_cols, columns=['Importance_Value'])\ngbr_top30_features = gbr_feature_importance_df.sort_values(by = ['Importance_Value'], ascending=False).head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e13667b5ef52c88fc34a94f30e55c58af48f3596","collapsed":true},"cell_type":"code","source":"ax1 = sns.barplot(x = gbr_top30_features.index, y = 'Importance_Value', data=gbr_top30_features)\nax1.set_xticklabels(ax1.get_xticklabels(), fontsize = 12, rotation=40, ha=\"right\")\nplt.title('Top 30 Features - Gradient Tree Boosting Regression')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9c0e86619189a90f6e200019eaefab89b93c439","collapsed":true},"cell_type":"code","source":"selected_features_gbr = gbr_top30_features[gbr_top30_features.Importance_Value >= 0.005].index.tolist()\n\nprint('Features of high importance value include: \\n', selected_features_gbr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96f84fddaece0352bfa224d3ae7ba217ad28fad0","collapsed":true},"cell_type":"code","source":"#### Let's merge all features selected from the Continuous section\n\nselected_features_continuous = set(selected_features_rf+selected_features_lasso+selected_features_gbr)\n\nprint('Selected features among continuous variables include: \\n', selected_features_continuous)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4f7e01d8f7ac201a4d19ba38cfefe4dacd860f3"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"db0702e99d46838d575f311ea2d7a2d957da121a"},"cell_type":"markdown","source":"* * * Work in Process... Stay tuned"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"76628f1666ea8bcfd700d3b4bbc6b9aa244c560a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}