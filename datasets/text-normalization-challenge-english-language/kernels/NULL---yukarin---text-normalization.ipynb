{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "nbconvert_exporter": "python", "version": "3.6.3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {"collapsed": true}, "source": [], "cell_type": "code", "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "4e4d8917-87bd-4c7d-b530-8c5e8354a13d", "_kg_hide-input": true, "_uuid": "29c9d02c4c9e86617f09c8840263f82ed3ac6bc3"}, "source": ["import numpy as np\n", "import os\n", "import pickle\n", "import gc #garabag collection\n", "import xgboost as xgb\n", "import re\n", "import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "\n", "#max_num_features = 10\n", "#pad_size = 1\n", "#boundary_letter = -1\n", "#space_letter = 0\n", "max_data_size = 960000\n", "max_num_features = 10\n", "\n", "def ascii(x, max_num_features = 10, space_letter = 0):\n", "    try:\n", "        t = map(ord, x[0])\n", "    except:\n", "        return max_num_features*[space_letter] + [-1]\n", "    l = min(len(t), max_num_features)\n", "    return t[:l] + (max_num_features-l)*[space_letter] + [x[-1]]\n", "\n", "def context_window(data, pad_pre = 1, pad_pos = 1, boundary_letter = -1):\n", "    #pad_before: num of words before\n", "    new_data = []\n", "    pad = max_num_features*[0] + [-1]\n", "    emp = [boundary_letter]+(max_num_features)*[0]\n", "    data = [pad for i in range(pad_pre)] + data + [pad for i in range(pad_pos)]\n", "    for i in range(pad_pre, len(data) - pad_pos):\n", "        l = data[i][-1]\n", "        t, tmp = [], []\n", "        for j in range(i-pad_pre, i+pad_pos+1):\n", "            if data[j][boundary_letter] == l:\n", "                tmp += [j]\n", "                t += [boundary_letter] + data[j][:-1]\n", "        new_data.append(emp*(tmp[0]-i+pad_pre) + t + emp*(i+pad_pos-tmp[-1])+[boundary_letter])\n", "    return new_data\n", "\n", "out_path = r'.'\n", "df = pd.read_csv(r'en_train.csv')\n", "gc.collect()\n", "#sentence_id + ascii\n", "x_data = map(ascii, [[df['before'][i],df['sentence_id'][i]] for i in range(max_data_size)]) \n", "gc.collect()\n", "x_data = x_data[:max_data_size]\n", "x_data = np.array(context_window(x_data))\n", "\n", "y_data =  pd.factorize(df['class'])\n", "labels = y_data[1]\n", "y_data = np.array(y_data[0][:max_data_size])"], "cell_type": "code", "execution_count": null, "outputs": []}]}