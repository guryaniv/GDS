{"cells":[{"metadata":{"_uuid":"028b44b4c521d6d685d542f4dcd8c6ead4d52a29"},"cell_type":"markdown","source":"# Coreference resolution with fast.ai\n\nIn this notebook, we will explore ULMFiT approach to solve this task. With proper fine-tuning, you can get decent results in a matter of 20 minutes. Some 15 epochs of fine-tuning will get you up to 20-ish place.\n\nChanges in this version:\n1. More civilized approach to validation.\n2. The model uses the representation of the last token of the entity instead of the first token. With a unidirectional encoder, this might be the right thing to do.\n\nI will be grateful for any suggestions, especially about converting two logits/probabilities into the three classes without the need for an additional layer.\n\n## Collect the data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from fastai.text import *\nfrom tqdm import tqdm_notebook as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ec42552a08892a35e7787748ea516b7e58b9edb"},"cell_type":"code","source":"!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data_path = Path(\".\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"790674e9f572c4d6a0a19081789b7b0b8ae1f7d6"},"cell_type":"code","source":"train = pd.read_csv(data_path/\"gap-development.tsv\", sep=\"\\t\")\nval = pd.read_csv(data_path/\"gap-validation.tsv\", sep=\"\\t\")\ntest = pd.read_csv(data_path/\"gap-test.tsv\", sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd8fc631370646784dfa9757a98b682b2d9adba1"},"cell_type":"code","source":"print(len(train), len(val), len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f351e845ab84a4f29a84e4f65929b1975bbd6792"},"cell_type":"code","source":"train[\"is_valid\"] = True\ntest[\"is_valid\"] = False\nval[\"is_valid\"] = True\n\ndf_pretrain = pd.concat([train, test, val])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eaa64857d482fbed4ea996f3a8520232996b5f83"},"cell_type":"markdown","source":"Finetune the language model:"},{"metadata":{"trusted":true,"_uuid":"c6f54e378d0cfec8dab4935f7430e7c14c0ca8ac"},"cell_type":"code","source":"db = (TextList.from_df(df_pretrain, data_path/\"db\", cols=\"Text\").split_from_df(col=\"is_valid\").label_for_lm().databunch())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5d9ab1b82a244555c46f280761c9120298caec0"},"cell_type":"code","source":"vocab = db.vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b2434521b7e3287c8c6dad959787ed6f545291f"},"cell_type":"code","source":"lm = language_model_learner(db, AWD_LSTM, drop_mult=0.5, pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"030bac492091e483046405266580ca15718ad1a2"},"cell_type":"markdown","source":"As the language model is already trained on Wikipedia, which is also the source of the excerpts, we can proceed to unfreezing right away:"},{"metadata":{"trusted":true,"_uuid":"0e4111993ec671d09463c1531037746dd0e22c14"},"cell_type":"code","source":"lm.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"745f53ab2a6edbd3b565d6c6a3ace9c634d053a9"},"cell_type":"code","source":"lm.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fae9307da8420efd51ec7480fe66ffd6fc0961f"},"cell_type":"code","source":"lm.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcfedad52f103c9b0d60ce48a2216bd634f689cd"},"cell_type":"code","source":"lm.fit_one_cycle(3, 1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6329d4fce060e4df1f7e9af68ad78842b7087fb8"},"cell_type":"code","source":"lm.fit_one_cycle(3, 1e-3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aff2f217a1b6916001afca23ff6d47ad7eeaf06a"},"cell_type":"markdown","source":"## Preprocess the dataset for classification"},{"metadata":{"trusted":true,"_uuid":"a227caf12041d58b50609836d587e03ef8037ce5"},"cell_type":"code","source":"spacy_tok = SpacyTokenizer(\"en\")\ntokenizer = Tokenizer(spacy_tok)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5f5eaf2e467ae9889a7c319286f5e7a2d136bcd"},"cell_type":"code","source":"df_pretrain.Text.apply(lambda x: len(tokenizer.process_text(x, spacy_tok))).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9f84eca4aff61bc042b61d61c79b8a31227f2f1"},"cell_type":"markdown","source":"Note that for simplicity we only use the first token of the entity, this is a point that can be improved."},{"metadata":{"trusted":true,"_uuid":"863a7c384d8e040ee657cccdc5b76bbcc52f6f40"},"cell_type":"code","source":"import spacy\nnlp = spacy.blank(\"en\")\n\ndef get_token_num_by_offset(s, offset):\n  s_pre = s[:offset]\n  return len(spacy_tok.tokenizer(s_pre))\n\n# note that 'xxunk' is not special in this sense\nspecial_tokens = ['xxbos','xxfld','xxpad', 'xxmaj','xxup','xxrep','xxwrep']\n\n\ndef adjust_token_num(processed, token_num):\n  \"\"\"\n  As fastai tokenizer introduces additional tokens, we need to adjust for them.\n  \"\"\"\n  counter = -1\n  do_unrep = None\n  for i, token in enumerate(processed):\n    if token not in special_tokens:\n      counter += 1\n    if do_unrep:\n      do_unrep = False\n      if processed[i+1] != \".\":\n        token_num -= (int(token) - 2) # one to account for the num itself\n      else:  # spacy doesn't split full stops\n        token_num += 1\n    if token == \"xxrep\":\n      do_unrep = True\n    if counter == token_num:\n      return i\n  else:\n    counter = -1\n    for i, t in enumerate(processed):\n      if t not in special_tokens:\n        counter += 1\n      print(i, counter, t)\n    raise Exception(f\"{token_num} is out of bounds ({processed})\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"473cd798c91c157957a5c37b2f5c8b5322a0e0ed"},"cell_type":"code","source":"def dataframe_to_tensors(df, max_len=512):\n  # offsets are: pron_tok_offset, a_tok_offset, a_tok_right_offset, b_tok_offset, b_tok_right_offset\n  offsets = list()\n  labels = np.zeros((len(df),), dtype=np.int64)\n  processed = list()\n  for i, row in tqdm(df.iterrows()):\n    try:\n      text = row[\"Text\"]\n      a_offset = row[\"A-offset\"]\n      a_len = len(nlp(row[\"A\"]))\n      \n      b_offset = row[\"B-offset\"]\n      b_len = len(nlp(row[\"B\"]))\n\n      pron_offset = row[\"Pronoun-offset\"]\n      is_a = row[\"A-coref\"]\n      is_b = row[\"B-coref\"]\n      a_tok_offset = get_token_num_by_offset(text, a_offset)\n      b_tok_offset = get_token_num_by_offset(text, b_offset)\n      a_right_offset = a_tok_offset + a_len - 1\n      b_right_offset = b_tok_offset + b_len - 1\n      pron_tok_offset = get_token_num_by_offset(text, pron_offset)\n      tokenized = tokenizer.process_text(text, spacy_tok)[:max_len]\n      tokenized = [\"xxpad\"] * (max_len - len(tokenized)) + tokenized  # add padding\n      a_tok_offset = adjust_token_num(tokenized, a_tok_offset)\n      a_tok_right_offset = adjust_token_num(tokenized, a_right_offset)\n      b_tok_offset = adjust_token_num(tokenized, b_tok_offset)\n      b_tok_right_offset = adjust_token_num(tokenized, b_right_offset)\n      pron_tok_offset = adjust_token_num(tokenized, pron_tok_offset)\n      numericalized = vocab.numericalize(tokenized)\n      processed.append(torch.tensor(numericalized, dtype=torch.long))\n      offsets.append([pron_tok_offset, a_tok_offset, a_tok_right_offset, b_tok_offset, b_tok_right_offset])\n      if is_a:\n        labels[i] = 0\n      elif is_b:\n        labels[i] = 1\n      else:\n        labels[i] = 2\n    except Exception as e:\n      print(i)\n      raise\n  processed = torch.stack(processed)\n  offsets = torch.tensor(offsets, dtype=torch.long)\n  labels = torch.from_numpy(labels)\n  return processed, offsets, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45594da37e683be4ed7b3b393b18d26d841bc5ce","scrolled":true},"cell_type":"code","source":"train_ds = TensorDataset(*dataframe_to_tensors(test))\nvalid_ds = TensorDataset(*dataframe_to_tensors(val))\ntest_ds = TensorDataset(*dataframe_to_tensors(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f22f6b99aecc792e4b97ec8f24749ce60ce96e8e"},"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False)\ntest_dl = DataLoader(test_ds, batch_size=32, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a95fefd9bfe6357249382a25d58860be14efdf12"},"cell_type":"markdown","source":"## Classifier architecture \n\nUnfortunately, the magic of fast.ai stops here: we need to create a custom classifier on top. What we do here is:\n1. Extract hidden states corresponding to entities and the pronoun.\n2. For each pair (pronoun, entity) we run it through a hidden layer to retrieve a 25-dimensional vector that describes their similarity.\n3. Concat the vectors.\n4. Use another layer to turn these into probabilities."},{"metadata":{"trusted":true,"_uuid":"6711e79b78313bc49a3ca3162c39595749898aeb"},"cell_type":"code","source":"lm.freeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe723133069de9b5a074328094564b5267e4ddbd"},"cell_type":"code","source":"encoder_hidden_sz = 400\n\ndevice = torch.device(\"cuda\")\n\nclass CorefResolver(nn.Module):\n  def __init__(self, encoder, dropout_p=0.3):\n    super(CorefResolver, self).__init__()\n    self.encoder = encoder\n    self.dropout = nn.Dropout(dropout_p)\n    self.hidden2hidden = nn.Linear(encoder_hidden_sz * 2 + 1, 25)\n    self.hidden2logits = nn.Linear(50, 3)\n    self.relu = nn.ReLU()\n    self.activation = nn.LogSoftmax(dim=1)\n    self.loss = nn.NLLLoss()\n    \n  def forward(self, seqs, offsets, labels=None):\n    encoded = self.dropout(self.encoder(seqs)[0][2])\n    a_q = list()\n    b_q = list()\n    for enc, offs in zip(encoded, offsets):\n      # extract the hidden states that correspond to A, B and the pronoun, and make pairs of those \n      a_repr = enc[offs[2]]\n      b_repr = enc[offs[4]]\n      a_q.append(torch.cat([enc[offs[0]], a_repr, torch.dot(enc[offs[0]], a_repr).unsqueeze(0)]))\n      b_q.append(torch.cat([enc[offs[0]], b_repr, torch.dot(enc[offs[0]], b_repr).unsqueeze(0)]))\n    a_q = torch.stack(a_q)\n    b_q = torch.stack(b_q)\n    # apply the same \"detector\" layer to both batches of pairs\n    is_a = self.relu(self.dropout(self.hidden2hidden(a_q)))\n    is_b = self.relu(self.dropout(self.hidden2hidden(b_q)))\n    # concatenate outputs of the \"detector\" layer to get the final probability distribution\n    is_a_b = torch.cat([is_a, is_b], dim=1)\n    is_logits = self.hidden2logits(self.dropout(self.relu(is_a_b)))\n\n    activation = self.activation(is_logits)\n    if labels is not None:\n      return activation, self.loss(activation, labels)\n    else:\n      return activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b7042607f3de0ecb4ccf8a79672be8a72c3b154"},"cell_type":"code","source":"enc = lm.model[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"642af6e86490d426ee5b156d173d1f2a604ce090"},"cell_type":"code","source":"resolver = CorefResolver(enc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d4211f4d9482434006c1fe886192197f2033445"},"cell_type":"code","source":"resolver.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"982f65b2377e3d104d3144cb7a7e2cb0b63f3f28"},"cell_type":"code","source":"for param in resolver.encoder.parameters():\n  param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380024dd598e9e4f759cfa84ee4215232cfa5d40"},"cell_type":"code","source":"lr = 0.001\n\nloss_fn = nn.NLLLoss()\noptimizer = torch.optim.Adam(resolver.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"974af0cbdb7026476bfcc6467924ea3af6166158"},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"754a613a4a251c6add0ce425a1497cd2997bc4d9"},"cell_type":"markdown","source":"## Define the training loop "},{"metadata":{"trusted":true,"_uuid":"7b680e7f3c3ec3695ec95320777585017adc5352"},"cell_type":"code","source":"def train_epoch(model, optimizer, train_dl, report_every=10):\n  model.train()\n  step = 0\n  total_loss = 0\n  \n  for texts, offsets, labels in train_dl:\n    texts, offsets, labels = texts.to(device), offsets.to(device), labels.to(device)\n    step += 1\n    optimizer.zero_grad()\n    _, loss = model(texts, offsets, labels)\n    total_loss += loss.item()\n    \n    loss.backward()\n    optimizer.step()\n    \n    if step % report_every == 0:\n      print(f\"Step {step}, loss: {total_loss/report_every}\")\n      total_loss = 0\n      \ndef evaluate(model, optimizer, valid_dl, probas=False):\n  probas = list()\n  model.eval()\n  predictions = list()\n  total_loss = 0\n  all_labels = list()\n  with torch.no_grad():\n    for texts, offsets, labels in valid_dl:\n      texts, offsets, labels = texts.cuda(), offsets.cuda(), labels.cuda()\n      preds, loss = model(texts, offsets, labels)\n      total_loss += loss.item()\n      probas.append(preds.cpu().detach().numpy())\n      predictions.extend([i.item() for i in preds.max(1)[1]])\n    \n    \n  print(f\"Validation loss: {total_loss/len(valid_dl)}\")\n  print()\n  print(classification_report(valid_dl.dataset.tensors[2].numpy(), predictions))\n  if probas:\n    return total_loss, np.vstack(probas)\n  return total_loss, predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6b2952ec137595b9545ed220f87e2172ce21839"},"cell_type":"markdown","source":"## Train "},{"metadata":{"trusted":true,"_uuid":"2a97960f3c98af31acf7500f226103ef930595f8"},"cell_type":"code","source":"total_epoch = 0\nbest_loss = 1e6\n\nfor i in range(3):\n  print(\"Epoch\", i + 1)\n  total_epoch += 1\n  train_epoch(resolver, optimizer, train_dl) \n  loss, labels = evaluate(resolver, optimizer, valid_dl)\n  if loss < best_loss:\n    best_loss = loss\n    print(f\"Loss improved, saving {total_epoch}\")\n    torch.save(resolver.state_dict(), data_path/\"model_best.pt\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f990cf42a93c5ada5c8ce8743e0eb0254b79f03"},"cell_type":"markdown","source":"Unfreeze the encoder and do fine-tuning. We do the finetuning until the model starts to recognize class `2`."},{"metadata":{"trusted":true,"_uuid":"df37aa35a62c3117b82f1c507b10ded67deaba9e"},"cell_type":"code","source":"for param in resolver.encoder.parameters():\n  param.requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc0f1871538a13a24baac31a0d56df182e391c2b"},"cell_type":"code","source":"lr = 3e-4\noptimizer = torch.optim.Adam(resolver.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"320c0a45dd83cdf426f2e4d25663ed94aa83ce16","scrolled":false},"cell_type":"code","source":"for i in range(6):\n  print(\"Epoch\", i + 1)\n  total_epoch += 1\n  train_epoch(resolver, optimizer, train_dl)\n  loss, labels = evaluate(resolver, optimizer, valid_dl)\n  if loss < best_loss:\n    best_loss = loss\n    print(f\"Loss improved, saving {total_epoch}\")\n    torch.save(resolver.state_dict(), data_path/\"model_best.pt\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4932cc048bdd80fb3ed8a30a40b18a90d93d72a"},"cell_type":"markdown","source":"## Fin: get the predictions and submit!"},{"metadata":{"trusted":true,"_uuid":"3a64168aacfb251830ea35d8f29451eb9aaedb49"},"cell_type":"code","source":"resolver.load_state_dict(torch.load(data_path/\"model_best.pt\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf35cc404ad90fefb7713450b87acadd3e0110a5"},"cell_type":"code","source":"loss, res = evaluate(resolver, optimizer, test_dl, True)\nres_s = np.exp(res)  # don't forget that we have log-softmax outputs:\nsubmission = pd.DataFrame(res_s, index=train[\"ID\"], columns=[\"A\", \"B\", \"NEITHER\"])\nsubmission.to_csv(\"submission.csv\", index=\"id\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}