{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f27711ab-64f1-d179-d9b3-bbfd17496994"
      },
      "source": [
        "This is my first script, I'm analyzing the biology corpus to extract tags using tfidf score, I increased the value for words that appear in the title, and I'm evaluating the output with the corpus tags, i got a low score: 0.073, I'm going to apply advanced techniques in order to increase the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f086258f-b3f0-02f4-ce21-bd0ff1c048a2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from subprocess import check_output\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "275b65ba-7202-8e6b-0425-f1d98dbb7331"
      },
      "outputs": [],
      "source": [
        "biology = pd.read_csv(\"../input/biology.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d4764359-7aeb-dfb8-8f6f-93411191c9b3"
      },
      "outputs": [],
      "source": [
        "biology.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "de3aeab9-c684-e411-6729-dee3b2bd17f9"
      },
      "outputs": [],
      "source": [
        "swords1 = stopwords.words('english')\n",
        "\n",
        "punctuations = string.punctuation\n",
        "\n",
        "def data_clean(data):\n",
        "    print('Cleaning data')\n",
        "    data = data.apply(lambda x: x.lower())\n",
        "    data = data.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
        "    data = data.apply(lambda x: re.sub(r'^\\W+|\\W+$',' ',x))\n",
        "    data = data.apply(lambda i: ''.join(i.strip(punctuations))  )\n",
        "    #print('tokenize')\n",
        "    data = data.apply(lambda x: word_tokenize(x))\n",
        "\n",
        "    #Select only the nouns\n",
        "    is_noun = lambda pos: pos[:2] == 'NN' \n",
        "    for i in range(len(data)):\n",
        "        data[i] = [word for (word, pos) in nltk.pos_tag(data[i]) if is_noun(pos)]\n",
        "    \n",
        "    #print('Remove stopwords')\n",
        "    data = data.apply(lambda x: [i for i in x if i not in swords1 if len(i)>2])\n",
        "    #print('minor clean some wors')\n",
        "    data = data.apply(lambda x: [i.split('/') for i in x] )\n",
        "    data = data.apply(lambda x: [i for y in x for i in y])\n",
        "    #print('Lemmatizing')\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    data = data.apply(lambda x: [wordnet_lemmatizer.lemmatize(i) for i in x])\n",
        "    data = data.apply(lambda x: [i for i in x if len(i)>2])\n",
        "    return(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "962ff0f7-a029-2934-2d5a-dc22c5a416bc"
      },
      "outputs": [],
      "source": [
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f8c47b4-ac55-0ff8-2766-cd30e3df7135"
      },
      "outputs": [],
      "source": [
        "def get_frequency(content, title):\n",
        "    \n",
        "    frequency = []\n",
        "    inverse_frequency = {}\n",
        "    for i in range(len(content)):\n",
        "        word_count = {}\n",
        "        important = {}\n",
        "        for word in title[i]:\n",
        "            if word in word_count:\n",
        "                word_count[word] = word_count[word] + 100\n",
        "            else:\n",
        "                word_count[word] = 10\n",
        "                important[word] = True\n",
        "        for word in content[i]:\n",
        "            if word in word_count:\n",
        "                if word in important:\n",
        "                    word_count[word] = word_count[word] + 50\n",
        "                else:\n",
        "                    word_count[word] = word_count[word] + 1\n",
        "            else:\n",
        "                word_count[word] = 1\n",
        "                \n",
        "        for word in word_count:\n",
        "            if word in inverse_frequency:\n",
        "                inverse_frequency[word] = inverse_frequency[word] + 1\n",
        "            else:\n",
        "                inverse_frequency[word] = 1            \n",
        "        frequency.append(word_count)\n",
        "    return (frequency, inverse_frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "472ded81-fb73-40d3-3aa8-b3ecb35f349c"
      },
      "outputs": [],
      "source": [
        "content = data_clean(biology.content)\n",
        "title = data_clean(biology.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eb39af5c-23d9-5d05-5968-f0537ce20d13"
      },
      "outputs": [],
      "source": [
        "frequency, inverse_frequency = get_frequency(content, title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "be1fb60e-9495-b191-042e-61cffed1e964"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "frequency_words = {}\n",
        "for document in frequency:\n",
        "    for word in document:\n",
        "        if word in frequency_words:\n",
        "            frequency_words[word] = frequency_words[word] + document[word]\n",
        "        else:\n",
        "            frequency_words[word] = document[word]            \n",
        "frequency_words = sorted(frequency_words.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fe3bd07d-6432-6f24-0729-5097da930db3"
      },
      "outputs": [],
      "source": [
        "print('number of words:',len(frequency_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3704022f-8b8d-9234-4b1a-1e14d55b994a"
      },
      "outputs": [],
      "source": [
        "plt.plot(frequency_words)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "50dcb082-547b-53f8-6ecf-bc06f84d62f7"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.log(frequency_words))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c142067e-5461-5061-f060-7c56ab632011"
      },
      "outputs": [],
      "source": [
        "tfidf = frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "caa264bc-d6c6-6569-bafe-1c92cdfbfffd"
      },
      "outputs": [],
      "source": [
        "tfidf_distribution = []\n",
        "for document in tfidf:\n",
        "    if document == {}:\n",
        "        continue\n",
        "    max_frequency = sorted(document.items(), key=operator.itemgetter(1), reverse=True)[0][1]\n",
        "    for word in document:\n",
        "        document[word] = document[word]/(max_frequency + 0.0)*np.log(len(tfidf)/(inverse_frequency[word]+0.))\n",
        "        tfidf_distribution.append(document[word])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9e35f2fd-3bf0-1f85-e4cb-358fe4c4984d"
      },
      "outputs": [],
      "source": [
        "index = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fd67635d-d140-2fc5-6286-e461db056fe5"
      },
      "outputs": [],
      "source": [
        "sorted(tfidf[index].items(), key=operator.itemgetter(1), reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5ddec35d-243a-aa61-3817-4d294e8c7d37"
      },
      "outputs": [],
      "source": [
        "print(biology.title[index])\n",
        "print(biology.content[index])\n",
        "print(biology.tags[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a65b0d87-bffa-8275-7607-b2b5aa60a1a4"
      },
      "outputs": [],
      "source": [
        "tfidf_distribution = sorted(tfidf_distribution)\n",
        "print(len(tfidf_distribution))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2b2dddf2-0512-2c46-0349-fa987df12300"
      },
      "outputs": [],
      "source": [
        "plt.plot(tfidf_distribution)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "096ac1c3-1081-699b-dd32-e40272bc8111"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.log(tfidf_distribution))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cae9e292-3d51-dbd0-a91a-be243a7cf8eb"
      },
      "outputs": [],
      "source": [
        "def getF1(prediction,tags):\n",
        "    if len(prediction) == 0 or len(tags) == 0:\n",
        "        return 0.0\n",
        "    tags = set(tags.split())\n",
        "    corrects = 0\n",
        "    for p in prediction:\n",
        "        if p in tags:\n",
        "            corrects = corrects + 1\n",
        "    \n",
        "    precision = corrects / (len(prediction) + 0.)\n",
        "    recall = corrects / (len(tags) + 0.)\n",
        "    if precision == 0 or recall == 0:\n",
        "        return 0.0     \n",
        "    return 2*precision*recall/(precision + recall)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3a743d5c-160e-f600-7677-8065400a9e29"
      },
      "outputs": [],
      "source": [
        "top = 3\n",
        "corpusf1 = []\n",
        "for i in range(len(tfidf)):\n",
        "    prediction = sorted(tfidf[i], key=tfidf[i].get, reverse=True)[0:top]\n",
        "    corpusf1.append(getF1(prediction, biology.tags[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5b9406b2-8356-8b98-f268-030c1556b135"
      },
      "outputs": [],
      "source": [
        "print(np.average(corpusf1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5c6aaa63-93c9-c472-9650-f09184f8c569"
      },
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}