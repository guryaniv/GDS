{"cells":[{"metadata":{"_uuid":"a3bab25cea240645f654a47a8eb442e7563b3a97"},"cell_type":"markdown","source":"# **Don't Overfit II: The Overfittening**\n**Choosing a model - Kernel**  \n https://www.kaggle.com/c/dont-overfit-ii  \nby *Leopoldo Sprandel - leopoldosprandel@yahoo.com.br*"},{"metadata":{"_uuid":"6fe313440e80cc9a5a7d55097fba907ada266d30"},"cell_type":"markdown","source":"## Don't Overfit II: The Overfittening  \nPlayground Prediction Competition - www.kaggle.com  \nPrediction made by *Leopoldo Sprandel - leopoldosprandel@yahoo.com.br*  \n    \nTarget  \nPredicting the binary target associated with each row, without overfitting to the minimal set of training examples provided.  \n\nFiles  \ntrain.csv - the training set. 250 rows. test.csv - the test set. 19,750 rows. sample_submission.csv - a sample submission file in the correct format  \n\nColumns  \nid- sample id target- a binary target of mysterious origin. 0-299- continuous variables.  \n\n## Summay\n1 - Data Retrieve  \n2 - Data Preparation  \n> 2.1 - Data Processing and Wrangling  --> already done by kaggle  \n2.2 - Feature Exploration and Engineering  \n2.3 - Feature Scaling and Selection  \n\n3 - Modeling  \n> Machine Learning algorithms  \n\n4 - Model Evaluation and Tunning  \n>Parameter optimisation  \n> AUC ROC visualization  \n\n5 - Submission  \n\n"},{"metadata":{"_uuid":"51de313d3cc84624706cb1ec3d433d71ff84d727"},"cell_type":"markdown","source":"## 1- Data Retrieve  \nSetup"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pylab as pl\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"### Load data and store in dataframes (submission, test_df, train_df)"},{"metadata":{"_uuid":"9b94954e69efaa89cd53a0450267ff6a37315f5a","trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\ntest_df = pd.read_csv('../input/test.csv')\ntrain_df = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2597e7a8ca04dfe9f3e616766554123196cbe60","trusted":true},"cell_type":"code","source":"print(train_df.shape)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82cf245ed6e2af42d2bcab857c5c36cb309c28fc","trusted":true},"cell_type":"code","source":"print(test_df.shape)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a016e5724e0da9cca3f13bc5338257b24fe595fc"},"cell_type":"markdown","source":"## 2- Data Preparation  \n\nImport visualization packages \"Matplotlib\" and \"Seaborn\". Using \"%matplotlib inline\" to plot in a Jupyter notebook:  \n"},{"metadata":{"_uuid":"ac13cb10d85760136a3894ca7f64fd451623b415","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a41ab96e6dc6645c49efc2f0d7b61da2758e1458"},"cell_type":"markdown","source":"Check the type of variables"},{"metadata":{"_uuid":"7b1fceeb6a77949003f6ba7c59f15746f3183024","trusted":true},"cell_type":"code","source":"train_df.dtypes.to_frame().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b1f9827e8d4c56f4ae86d8d1a0c2bc0e4dae274"},"cell_type":"code","source":"test_df.dtypes.to_frame().head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61cf82632a45dd92a5191ec2c13a32c63dcc1b21"},"cell_type":"markdown","source":"## 2.2 Feature Exploration and Engineering"},{"metadata":{"_uuid":"021a37c3f3dd02623db8b13b675966bbc056530e"},"cell_type":"markdown","source":"We need to get some sense of how balanced our dataset is:"},{"metadata":{"_uuid":"0550e96e086b354b5c52ad941a5ad0581fd63f77","trusted":true},"cell_type":"code","source":"# Some basic stats on the target variable\nprint ('# target = 1: {}'.format(len(train_df[train_df['target'] == 1])))\nprint ('# target = 0: {}'.format(len(train_df[train_df['target'] == 0])))  \nprint ('% target = 1: {}%'.format(round(float(len(train_df[train_df['target'] == 1])) / len(train_df) * 100))) \ntax=round(float(len(train_df[train_df['target'] == 1])) / len(train_df) * 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"144c0d48add4728796c71e897d17784057c40065"},"cell_type":"markdown","source":"Knowing that 64% of the targets are 1, that will helps us not be excited by machine learning diagnostics that aren't much better than 64%. If I predicted that all targets are 1, I would be 64% accurate!"},{"metadata":{"_uuid":"16d8c85cd621d88910a208d7af6346182b81817f"},"cell_type":"markdown","source":"Checking the correlation between all variables.\nWe can calculate the correlation between variables of type \"int64\" or \"float64\" using the method \"corr\":  \nWe observe there is no high correlation between the target and any feature. Therefore it is not a reliable variable to work only with simple correlations.  "},{"metadata":{"_uuid":"71648aedf900c4bb3022c7cc1b6f7cf6b832802c","trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(20, 20), dpi=80, facecolor='w', edgecolor='k')\nplt.pcolor(train_df.corr(), cmap='Accent')\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"512baff3544884e913560528c44816d20dec8551","trusted":true},"cell_type":"code","source":"train_df_correlations = train_df.corr()\ntrain_df_correlations = train_df_correlations.values.flatten()\ntrain_df_correlations = train_df_correlations[train_df_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_df_correlations, color=\"Red\", label=\"train\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb815f9ea331cc06aef84513ab4210c72101ed3d"},"cell_type":"markdown","source":"No significant correlation between variables!"},{"metadata":{"_uuid":"c29d944b4a99ab0c4327e879c02ef113a5d4f7f2"},"cell_type":"markdown","source":"let's look at the distribution of all variables:"},{"metadata":{"_uuid":"7edf2d7b644bb2d43a260059c179ec8ccae023e3","trusted":true},"cell_type":"code","source":"def plot_feature_distribution(df0, df1, df2, label0, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(24,18))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df0[feature], bw=0.5,label=label0)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46b313a83ef878a8311e65690297bf3a61946b36","trusted":true},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nt2 = test_df\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0, t1, t2, '0', '1', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"429d5fd4271ad87dda6ab16b246facca691790b9","trusted":true},"cell_type":"code","source":"features = train_df.columns.values[102:202]\nplot_feature_distribution(t0, t1, t2, '0', '1', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaaf2a8a0039b5147403ff1bc663098cf59bada0"},"cell_type":"code","source":"features = train_df.columns.values[202:302]\nplot_feature_distribution(t0, t1, t2, '0', '1', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"724c6bafc3e4e92fc8758a1fac4c06dbf2b89568"},"cell_type":"markdown","source":"**Conclusion of Data Exploration**  \nAll the features have a normal distribution shape.The distribution wen we look the target can't be modeled alone, we can observe a lot o overlap in all features.   Some interesting feature should be looked in more detail, like the feature 33 (some observed features: 9,33,65,63,76.  \nIn the 33 feature, we can observe the test dataframe are more like the train dataframe when the feature are equal to one.  \nSome features has almost the same distribution, like 102,103,112 and 116 and others.  \nWhat is bether???  \n"},{"metadata":{"_uuid":"bdfa07e885040ceae22a6f599585662b151b8464"},"cell_type":"markdown","source":"**Feature Transformation**  \nFirst we can make a Log Transform.\nLog transforms are useful when applied to skewed distributions as they tend to expand the values which fall in the range of lower magnitudes and tend to compress or reduce the values which fall in the range of higher magnitudes. Their main significance is that they help in stabilizing variance, adhering closely to the normal distribution and making the data independent of the mean based on its distribution.\nSource:https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b"},{"metadata":{"trusted":true,"_uuid":"bb749c8809cdc0fffa6b1b1601b610f9cb532938"},"cell_type":"code","source":"d=pd.Series(range(0,300))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b0402d29b1cd75c35c605dea10714cfc0502ebc"},"cell_type":"code","source":"for df in [test_df, train_df]: \n    d=pd.Series(d)\n    d=(d + 1).tolist()\n    for x in d:\n        df[df.columns[x]]=(df[df.columns[x]]-df[df.columns[x]].min())/(df[df.columns[x]].max()-df[df.columns[x]].min())\n        df[df.columns[x]]=np.log((1+df[df.columns[x]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e698457b2f1c2b9317fbd907f0c5e72d94bceb2"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nt2 = test_df\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0, t1, t2, '0', '1', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1efed710a7fd0167eeb67f723a2e6061e304c4b1"},"cell_type":"code","source":"features = train_df.columns.values[102:202]\nplot_feature_distribution(t0, t1, t2, '0', '1', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4f68c24401390ddf09ebd517d93255ae44dd87c"},"cell_type":"code","source":"features = train_df.columns.values[202:302]\nplot_feature_distribution(t0, t1, t2, '0', '1', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c6cf34a653cf9be75ee4dbcae1a87aee5ed75b3"},"cell_type":"markdown","source":"Looking at the distribution shape off all features we can  select some that have differente central point to see how the are presented in a pair plot and see if we can find some groups.\nVisuali I select the following features to pair plot: 4,13,16,24,33,65,73,80,91,183,189,194,199,217,276,795,298"},{"metadata":{"trusted":true,"_uuid":"986b7b1e2236c6ab25fb38509a84d9a9c3a65943"},"cell_type":"code","source":"#sample=['target','4','13','16','24','33','65','73','80','91','183','189','194','199','217','276','295','298']\nsample= ['target','16', '33', '80', '91', '217', '295']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bf658baa1964b3b9d17ffa20f4fd90908f548fd"},"cell_type":"code","source":"sns.set(style=\"ticks\", color_codes=True)\npair_sample=train_df[sample]\n# Pairwise plots\npplot = sns.pairplot(pair_sample, hue=\"target\", height=3, kind ='scatter', diag_kind='kde', plot_kws=dict(s=20, linewidth=0) ) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58c82b23ebf2d01f6dfbdd9ba497cb4f13674771"},"cell_type":"markdown","source":"1. Making a zoom on some interesting pairs:  (33x217, 16x33,  33x80,  33x91,  33x295)"},{"metadata":{"_uuid":"6c02120f456216fbbaad12629ab9ce19e9fef9ef","trusted":true},"cell_type":"code","source":"#Ex.: between the variables\nsns.lmplot( x='33', y='217', data=train_df, fit_reg=False, hue='target', legend=True)\nsns.lmplot( x='33', y='16', data=train_df, fit_reg=False, hue='target', legend=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02bb08a03f43100ca4541ce8457a0dd8b162ca34"},"cell_type":"markdown","source":"We can observe a concentration of blue points (target=0) in a certain region in this plot. That can help us to be more acetive in engineering feature."},{"metadata":{"trusted":true,"_uuid":"941a888dc3ab303d687f632659c2674339fec47a","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#feat_engin=[4,13,16,24,33,65,73,80,91,183,189,194,199,217,276,295,298]\n#feat_engin=[33,217,295,298]\n#feat_engin=[2,4,6,11,14,17,21,29,33,45,46,61,70,71,74,76,80,131,132,135,141,177,205,231,246,293]\n#feat_engin2=[1,13,16,25,26,42,48,65,66,83,111,116,117,138,147,176,195,228,237,266]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f50b60bec4cb8c570c1e60c323cf9837b72a5927","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#for df in [test_df, train_df]:\n#      \n#    d=pd.Series(feat_engin)\n#    d=(d + 1).tolist()\n#           \n#    df['300'] = df[df.columns[d]].sum(axis=1)  \n#    df['301'] = df[df.columns[d]].min(axis=1) \n#    df['302'] = df[df.columns[d]].max(axis=1) \n#    df['303'] = df[df.columns[d]].mean(axis=1) \n#    df['304'] = df[df.columns[d]].var(axis=1)\n#    df['305'] = df[df.columns[d]].sum(axis=1)+df[df.columns[d]].median(axis=1)\n#    df['306'] = df[df.columns[d]].std(axis=1) \n#    df['307'] = df[df.columns[d]].mean(axis=1)\n#    df['308'] = df[df.columns[d]].median(axis=1)\n#    \n#    d2=pd.Series(feat_engin2)\n#    d2=(d2 + 1).tolist()\n           \n#    df['309'] = df[df.columns[d2]].sum(axis=1)  \n#    df['310'] = df[df.columns[d2]].min(axis=1) \n#    df['311'] = df[df.columns[d2]].max(axis=1) \n#    df['312'] = df[df.columns[d2]].mean(axis=1) \n#    df['313'] = df[df.columns[d2]].var(axis=1)\n#    df['314'] = df[df.columns[d2]].sum(axis=1)+df[df.columns[d2]].median(axis=1)\n#    df['315'] = df[df.columns[d2]].std(axis=1) \n#    df['316'] = df[df.columns[d2]].mean(axis=1)\n#    df['317'] = df[df.columns[d2]].median(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"101094df926fdd057a451ac8b129c9b81debdc60","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#train_df[train_df.columns[302:]].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaa9d0d17afd9788e8b1e22dde1efcde134c5540","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#test_df[test_df.columns[301:]].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"143d549d48e25a093d7ca5f30678bf3bd20fc64f","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#def plot_new_feature_distribution(df1, df2, label1, label2, features):\n#    i = 0\n#    sns.set_style('whitegrid')\n#    plt.figure()\n#    fig, ax = plt.subplots(3,6,figsize=(24,9))\n#\n#    for feature in features:\n#        i += 1\n#        plt.subplot(3,6,i)\n#        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n#        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n#        plt.xlabel(feature, fontsize=9)\n#        locs, labels = plt.xticks()\n#        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n#        plt.tick_params(axis='y', which='major', labelsize=6)\n#    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb30dc3420c4516ce4168ba344f42a953d64aeb2","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#t0 = train_df.loc[train_df['target'] == 0]\n#t1 = train_df.loc[train_df['target'] == 1]\n#features = train_df.columns.values[302:]\n#plot_new_feature_distribution(t0, t1, '0', '1', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7ef4515e240c4f119a054e78ab320c3b55b2375"},"cell_type":"markdown","source":"Let's take a look on the points in a sctter plot between the secondaryes features"},{"metadata":{"trusted":true,"_uuid":"2ef3a270d78a1852e696ddac1d19291bcd24f377","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#sns.lmplot( x='305', y='314', data=train_df, fit_reg=False, hue='target', legend=True)\n#sns.lmplot( x='305', y='309', data=train_df, fit_reg=False, hue='target', legend=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19e31cfb101384eaed1c4bf134a2fbd63a76cae5"},"cell_type":"markdown","source":"## 2.3 Feature Scaling and Selection  "},{"metadata":{"_uuid":"ccec3228934f1113ced747119c56273d49556ff3","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#z_selection=pd.DataFrame()\n#d=pd.Series()\n#z_box=train_df.groupby(['target']).describe()\n#z_box.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34b9716a69b8b9a8671d2298f3bdbfc303ba0c48","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#i=0\n#minim=1\n#for x in train_df.columns.drop(['id','target']):\n#    z_selection[x]=[abs((z_box[x]['25%'].iloc[(0)]-z_box[x]['25%'].iloc[(1)])/(z_box[x]['25%'].iloc[(0)]-z_box[x]['75%'].iloc[(1)]))+\n#                   abs((z_box[x]['75%'].iloc[(0)]-z_box[x]['75%'].iloc[(1)])/(z_box[x]['75%'].iloc[(0)]-z_box[x]['25%'].iloc[(1)]))]\n#d=np.where(z_selection>0.5)[1]\n#d=feat_engin\nd=sample[1:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae08556d9bfdc58859d580c76149aeee46444c45"},"cell_type":"markdown","source":"Creation a dataframe to store all results and compare each model.  \nCreation of  the dataframes for test and training based on the train dataframe."},{"metadata":{"trusted":true,"_uuid":"714711dddcfc74a3d841f5f0f95264c5a3e19ca0"},"cell_type":"code","source":"result=pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"451043131b7ccfcae8fa0180df20de4bcf5958d5","trusted":true},"cell_type":"code","source":"y = train_df['target'] \nX = train_df.drop(['target','id'], axis=1) # Data set to create and tunning the model\nX_ol=X[d] # Data set with the selected features  - to be studied if is an improvement.\nprint ('Train set:', X.shape,  y.shape, X_ol.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82112719c7a793d2e50d00dc0bbfc4c3ce8b0ace","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=6)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\n\nX_train_ol=X_train[d]\nX_test_ol=X_test[d]\n\nprint ('Train set select:', X_train_ol.shape,  y_train.shape)\nprint ('Test set select:', X_test_ol.shape,  y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b4ea8985d201f913e144695ceaa602ba9f6d346"},"cell_type":"code","source":"print ('% target = 1 in train set: {}%'.format(round(float(len(y_train[y_train==1])) / len(y_train) * 100))) \nprint ('% target = 1 in test set: {}%'.format(round(float(len(y_test[y_test==1])) / len(y_test) * 100)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8dadd6cddd0dc135461b98729676bb016ef5f2d"},"cell_type":"markdown","source":"## 3 - Modeling"},{"metadata":{"_uuid":"9060066c76d6d769a0629ac5aa62d56a935e0ead"},"cell_type":"markdown","source":"### **Multi linear regression**"},{"metadata":{"_uuid":"d5478294770b3b045ce018c5513ce6a4422abaf8","trusted":true},"cell_type":"code","source":"from sklearn import linear_model\n#Model with all 300 features\nregr_all = linear_model.LinearRegression(fit_intercept=True, normalize=True)\nregr_all.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7022b1bf63ef9fb4807775dfb4f92afd8be4e44","trusted":true},"cell_type":"code","source":"y_hat= regr_all.predict(X_test)\ny_hat=y_hat*0.5/(np.sort(y_hat)[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['regr_all']=y_hat\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat>=0.5])) / len(y_hat) * 100))) \n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % regr_all.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat)) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b064969940573db91437f730fe77c789e6960f8","trusted":true},"cell_type":"code","source":"#Model with the selected features\nregr_ol = linear_model.LinearRegression(fit_intercept=True, normalize=True)\nregr_ol.fit(X_train_ol,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"701f21f0990304de65a8287304105f0418893c5e","trusted":true},"cell_type":"code","source":"regr_ol.predict(X_test_ol)\n#y_hat=(y_hat-y_hat.min())/(y_hat.max()-y_hat.min()) # turn the result to be between 0 and 1\ny_hat=y_hat*0.5/(np.sort(y_hat)[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['regr_ol']=y_hat\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % regr_ol.score(X_test_ol, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat)) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc90b2a110a9da6fdb1c973cdafdedb14c51d662","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n#Selecting variables via RFE\ni=0\nscore=pd.DataFrame()\nfor nfeatures in range(5,301,3):\n    selector = RFE(regr_all, nfeatures, step=1,verbose=0)\n    selector.fit(X_train,y_train)\n    y_hat= selector.predict(X_test)\n    score[i]=[nfeatures,roc_auc_score(y_test, y_hat)]\n    #print(score, nfeatures)\n    i=i+1\n    \nscore=score.transpose()\nscore.columns=['NumberOfFeatures','AUC ROC']\nplt.plot(score['NumberOfFeatures'],score['AUC ROC'])\nplt.xlabel('Number of Features')\nplt.ylabel('AUC ROC')\nplt.title('AUC ROC x Number of features')\nplt.grid(True)\nplt.show() # best is between 90 and 100 or 201 and 210\n\n#selector.get_support(indices=True) # selected features via RFE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b72d46d7af65b4ea8af51e2c75f3f35e499a1358"},"cell_type":"code","source":"best=score[score['AUC ROC']>score['AUC ROC'].max()*0.95]\nbest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd76d7189709e07cc8d641f0f9a1949d619fe521"},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nregr_sel = RFE(regr_all, 4, step=1,verbose=0)\nregr_sel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c507c61106097788f0f4c81e68e8d61de5cf139","trusted":true},"cell_type":"code","source":"y_hat= regr_sel.predict(X_test)\ny_hat=y_hat*0.5/(np.sort(y_hat)[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['regre_sel']=y_hat\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % regr_sel.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat)) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"befd3b70f4a0508dae2092880cfd89b18c77132e"},"cell_type":"markdown","source":"### **Decision tree**"},{"metadata":{"trusted":true,"_uuid":"b591ed8ac87989e0a032e6d881015747816c939d"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nparam_grid = { 'criterion': ['gini','entropy'],\n              'splitter': ['best','random'],\n              'max_depth': [2,4,10,20,None],\n              'max_features' : ['auto', 'sqrt', 'log2']}\nTree = DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b03e93ac331846d53db6b500d33b5a164efc75ed"},"cell_type":"code","source":"grid = GridSearchCV(estimator=Tree, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1,cv=2)\ngrid.fit(X_train,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3312d5a2de2f8396a47cf61228060f9ff89d7fd0"},"cell_type":"code","source":"Tree = DecisionTreeClassifier(**best_param)\nTree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8f3168974be7969c145f6142b39841b6b977b04","trusted":true},"cell_type":"code","source":"y_hat= Tree.predict(X_test)\nresult['Tree']=y_hat\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % Tree.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat)) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3deb3492b042ef890f1e62890baf721ea997c422"},"cell_type":"code","source":"grid = GridSearchCV(estimator=Tree, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1,cv=2)\ngrid.fit(X_train_ol,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39688e21ec8efcdd4c037c010f341489e10000f7","trusted":true},"cell_type":"code","source":"Tree_ol = DecisionTreeClassifier(**best_param)\nTree_ol.fit(X_train_ol,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bffa7555d30b6c7e42dfd5957f23f034964e69e","trusted":true},"cell_type":"code","source":"y_hat= Tree_ol.predict(X_test_ol)\nresult['Tree_ol']=y_hat\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % Tree_ol.score(X_test_ol, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat)) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ff9b2a7a5eb5b76df7049d2bbb97f1c4ef3870f"},"cell_type":"code","source":"#Selecting variables via RFE\ni=0\nscore=pd.DataFrame()\nfor nfeatures in range(5,301,5):\n    selector = RFE(Tree, nfeatures, step=1,verbose=0)\n    selector.fit(X_train,y_train)\n    y_hat= selector.predict(X_test)\n    score[i]=[nfeatures,roc_auc_score(y_test, y_hat)]\n    #print(score, nfeatures)\n    i=i+1\n    \nscore=score.transpose()\nscore.columns=['NumberOfFeatures','AUC ROC']\nplt.plot(score['NumberOfFeatures'],score['AUC ROC'])\nplt.xlabel('Number of Features')\nplt.ylabel('AUC ROC')\nplt.title('AUC ROC x Number of features')\nplt.grid(True)\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8620bac161d4208945450be6675a9bcc0ab3752b"},"cell_type":"code","source":"Tree_selec = RFE(Tree, 94, step=1,verbose=0)\nTree_selec.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e244cf2c0348df549b146dd610b11ccfe18c742"},"cell_type":"code","source":"y_hat= Tree_selec.predict(X_test)\nresult['Tree_selec']=y_hat\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % Tree_selec.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat)) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbbd534bf455d92ab1037202bc422ae8b1ac7847"},"cell_type":"markdown","source":"### **Logistic Regression**\n"},{"metadata":{"_uuid":"a53bba301d67ca2118d88e7f4d33f613c9966688","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nparam_grid = { 'C': [0.001,0.1,10.0,100.0],\n              'fit_intercept': [True,False],\n              'class_weight': ['balanced', None],\n              'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\nLR = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d76a8ccbae960abdbaaf3d664beebb481a5db925","trusted":true},"cell_type":"code","source":"grid = GridSearchCV(estimator=LR, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1,cv=2)\ngrid.fit(X_train,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4621564b5ec6c9364b467e3ca5c67b722177c023","trusted":true},"cell_type":"code","source":"LR = LogisticRegression(**best_param)\nLR.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9454fb33719a70c41d1c809b5de4e501eb19223","trusted":true},"cell_type":"code","source":"y_hat= LR.predict_proba(X_test)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['LR']=y_hat[:,1]\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat[:,1] - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % LR.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat[:,1])) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa6f082c5338f2d42aec83920ee0e0a379176fb1"},"cell_type":"code","source":"grid = GridSearchCV(estimator=LR, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1,cv=2)\ngrid.fit(X_train_ol,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bdca08ce2e48bacfe9f37cf11b7e9ab98582e91"},"cell_type":"code","source":"LR_ol = LogisticRegression(**best_param)\nLR_ol.fit(X_train_ol,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9e54f69bd05d41e0c460104a7ed8a056e749ad3"},"cell_type":"code","source":"y_hat= LR_ol.predict_proba(X_test_ol)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['LR_ol']=y_hat[:,1]\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat[:,1] - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % LR_ol.score(X_test_ol, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat[:,1])) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea999e2b0e7787668b033c8490984019cd8d319a"},"cell_type":"code","source":"#Selecting variables via RFE\ni=0\nscore=pd.DataFrame()\nfor nfeatures in range(5,301,3):\n    selector = RFE(LR, nfeatures, step=1,verbose=0)\n    selector.fit(X_train,y_train)\n    y_hat= selector.predict(X_test)\n    score[i]=[nfeatures,roc_auc_score(y_test, y_hat)]\n    #print(score, nfeatures)\n    i=i+1\n    \nscore=score.transpose()\nscore.columns=['NumberOfFeatures','AUC ROC']\nplt.plot(score['NumberOfFeatures'],score['AUC ROC'])\nplt.xlabel('Number of Features')\nplt.ylabel('AUC ROC')\nplt.title('AUC ROC x Number of features')\nplt.grid(True)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab929d6d809373269dda51d9a7d9ccb03f4c8c60"},"cell_type":"code","source":"#take the best results\nbest=score[score['AUC ROC']>score['AUC ROC'].max()*0.95]\nbest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1f4c75dfaf107000c252735a349ef9e52a1aecf"},"cell_type":"code","source":"LR_selec = RFE(LR, 14, step=1,verbose=0)\nLR_selec.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77b10b0d7a8c44ac3d330ae3f2782cad131aed78"},"cell_type":"code","source":"y_hat= LR_selec.predict_proba(X_test)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['LR_select']=y_hat[:,1]\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat[:,1] - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % LR_selec.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat[:,1])) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb75660200b8c060ee79a66dc393b0c35a3daca8"},"cell_type":"markdown","source":"### **SVM** "},{"metadata":{"_uuid":"58bbe1639434f97c1e7831941c01481050515cef"},"cell_type":"markdown","source":"### **Multi-layer Perceptron** "},{"metadata":{"_uuid":"14dd689dd97a4573302f5e4fff1e4fba1a1d513d","trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nparam_grid = { 'activation': ['identity', 'logistic', 'tanh', 'relu'],\n              'solver': ['lbfgs', 'sgd', 'adam'],\n              'learning_rate': ['constant', 'invscaling', 'adaptive']}\nMLPerseptron = MLPClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0aaf7a42af88485c792d41f60dfccbaf267472a"},"cell_type":"code","source":"grid = GridSearchCV(estimator=MLPerseptron, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1,cv=2)\ngrid.fit(X_train,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42ae19fb799ca92efc7b3d884cdfd4d3f00846ca"},"cell_type":"code","source":"MLPerseptron = MLPClassifier(**best_param)\nMLPerseptron.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fdddde1c95b35fbe9070e2a0025ecb638a274eb","trusted":true},"cell_type":"code","source":"y_hat= MLPerseptron.predict_proba(X_test)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['MLPerseptron']=y_hat[:,1]\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat[:,1] - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % MLPerseptron.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat[:,1])) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c7709a3dd02dc8f5f19554bd59a70469d6ab331"},"cell_type":"code","source":"grid = GridSearchCV(estimator=MLPerseptron, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1,cv=2)\ngrid.fit(X_train_ol,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10391dd641d312ff109714b22b5b289d3ff06df5","trusted":true},"cell_type":"code","source":"MLPerseptron_ol = MLPClassifier(**best_param)\nMLPerseptron_ol.fit(X_train_ol,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"409b82b4b10990a4d7e929d204ed53b18e03e6c5","trusted":true},"cell_type":"code","source":"y_hat= MLPerseptron_ol.predict_proba(X_test_ol)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['MLPerseptron_ol']=y_hat[:,1]\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat[:,1] - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % MLPerseptron_ol.score(X_test_ol, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat[:,1])) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b33bd3a78696ce82c950a6930f0cee8e2980ea9"},"cell_type":"markdown","source":"### **Gaussian Naive Bayes (GaussianNB)**"},{"metadata":{"_uuid":"34b915440ad6d166c5d58b215728c4a92d8d89ce","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nparam_grid = { 'priors': [None],\n              'var_smoothing': [1e-3,1e-6,1e-9,1e-12]}\ng_NB = GaussianNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d991217c6f33c104ff52e0a05a91229405ebe480"},"cell_type":"code","source":"grid = GridSearchCV(estimator=g_NB, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1,cv=2)\ngrid.fit(X_train,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1ab77cda721cb148e51954ab91aebba6f8e9a43","trusted":true},"cell_type":"code","source":"g_NB = GaussianNB(**best_param)\ng_NB.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d55190306517ff2ebb6faa73d2e28e614eedd80a","trusted":true},"cell_type":"code","source":"y_hat= g_NB.predict_proba(X_test)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['g_NB']=y_hat[:,1]\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat[:,1] - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % g_NB.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat[:,1])) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5abb12ad537ec9513cf18b8af468fe99da9e00ab"},"cell_type":"code","source":"grid.fit(X_train_ol,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"561fc03abdd2a9aaa40952034bf02af33aeccc9f"},"cell_type":"code","source":"grid.fit(X_train_ol,y_train)\nbest_param=grid.best_params_\ng_NB_ol = GaussianNB(**best_param)\ng_NB_ol.fit(X_train_ol, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"163f7de1635e9d4acf8558bc454fb6bb21a82516"},"cell_type":"code","source":"y_hat= g_NB.predict_proba(X_test)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['g_NB_ol']=y_hat[:,1]\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat[:,1] - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % g_NB.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat[:,1])) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30834943ef34fd6b4d688d6d91f58887e2ef9abb"},"cell_type":"markdown","source":"## **KNN model**"},{"metadata":{"trusted":true,"_uuid":"1f46dc2e1619866d019c277f3702a17052d056fb"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nparam_grid = { 'n_neighbors': list(range(4,100)),\n              'weights': ['uniform','distance'],\n              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n              'p' : [1,2]}\nknn = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4256b497a8c2f8dc4f004ba185c0be40d0d6d2c4"},"cell_type":"code","source":"grid = GridSearchCV(estimator=knn, param_grid=param_grid, scoring=  'roc_auc', verbose=1, n_jobs=-1)\ngrid.fit(X_train,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"881fa17f7901c7d211618c02e668c3cefd3276cc"},"cell_type":"code","source":"knn = KNeighborsClassifier(**best_param)\nknn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"950529d20b86903b4e2add1da9b3b859dc45f0bf"},"cell_type":"code","source":"y_hat= knn.predict_proba(X_test)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['knn']=y_hat[:,1]\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat[:,1] - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % knn.score(X_test, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat[:,1])) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91cb8d0daf03d0a3f4601975d1bb1e333ce2035c"},"cell_type":"code","source":"grid.fit(X_train_ol,y_train)\nprint(\"Best Score= \" + str(grid.best_score_))\nprint (\"Best Parameters= \"+str(grid.best_params_))\nbest_param=grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89a6d2213ff90a3d03f31efec41a1605905f0e7e"},"cell_type":"code","source":"grid.fit(X_train_ol,y_train)\nbest_param=grid.best_params_\nknn_ol = KNeighborsClassifier(**best_param)\nknn_ol.fit(X_train_ol, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"450f613ac4256c2cce2042c5f1fa3b94693bfbae"},"cell_type":"code","source":"y_hat= knn_ol.predict_proba(X_test_ol)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[100-tax]) # --> ajust the array to have 64% of target > 0.5\nresult['knn_ol']=y_hat[:,1]\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))\n\nprint(\"Residual sum of squares: %.2f\" % np.mean((y_hat[:,1] - y_test) ** 2)) # 0 is best score\nprint('Variance score: %.2f' % knn_ol.score(X_test_ol, y_test)) # 1 is perfect prediction\nprint('AUC ROC: %.2f' % roc_auc_score(y_test, y_hat[:,1])) #1 is best","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de7c19c13a958af3238b6d603037b6264d8a182c"},"cell_type":"markdown","source":"## **4 - Model Evaluation and Tunning  **"},{"metadata":{"_uuid":"772c8659bb44110569bfa684abcae83823398976"},"cell_type":"markdown","source":"Choosing the best prediction"},{"metadata":{"_uuid":"13049cda2c169a75dd38399969be710e8270e210","trusted":true},"cell_type":"code","source":"temp=pd.DataFrame()\ntemp['mean']=result.mean(axis=1) \nresult2=pd.concat([result, temp], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0587d26b3cd88f0cfd966c6c3befeae98e58be9"},"cell_type":"markdown","source":"Check if the models can be better if combined"},{"metadata":{"_uuid":"75eed244de86e8eddfa969b7281905e7d111e15d","trusted":true},"cell_type":"code","source":"FinalResult=pd.DataFrame()\nfor results in result2.columns:\n    y_hat=result2[results]\n    AUC_ROC=roc_auc_score(y_test, y_hat)\n    FinalResult[results]=[AUC_ROC]\nFinalResult.transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ead88aaffadeac4036da0bccf5c3549f41ee785b"},"cell_type":"markdown","source":"### **AUC ROC Visualization**\n"},{"metadata":{"_uuid":"2551f0ccd28fec76237f86012885c16155d38203","trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4666d9f883b3c3f95201ddfc5959aed12a4f9d6","trusted":true},"cell_type":"code","source":"def plot_AUCROC(y_test, y_hat, labels):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4,5,figsize=(20,12))\n\n    for Label in labels:\n        # calculate the fpr and tpr for all thresholds of the classification\n        fpr, tpr, threshold = metrics.roc_curve(y_test, y_hat[Label])\n        roc_auc = metrics.auc(fpr, tpr)\n\n        # method I: plt\n        i += 1\n        plt.subplot(4,5,i)    \n        plt.title(Label)\n        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n        plt.legend(loc = 'lower right')\n        plt.plot([0, 1], [0, 1],'r--')\n        plt.xlim([0, 1])\n        plt.ylim([0, 1])\n        plt.ylabel('True Positive Rate')\n        plt.xlabel('False Positive Rate')\n        plt.tick_params(axis='x', which='major', labelsize=1, pad=-10)\n        plt.tick_params(axis='y', which='major', labelsize=1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e6c430a4a45f6aa8b1eeb69d3b2660e603b2a66","trusted":true},"cell_type":"code","source":"labels = result2.columns\nplot_AUCROC(y_test,result2, labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91bf1922b0c9e85c05afb38402fb715886286163"},"cell_type":"markdown","source":"## Make de prediction based on the regr_ol model"},{"metadata":{"_uuid":"bb61b3aebd901ccdc3e7b82edbfb482e8fa95102","trusted":true},"cell_type":"code","source":"X_Test = test_df.drop(columns=[\"id\"])\nX_Test_ol=X_Test[d]\n#LR.fit(X,y)\nLR_ol.fit(X_ol,y)\n#regr_ol.fit(X_ol, y)  \n#g_NB.fit(X, y)\n#selector.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a9a23e75d12d1c07a174c3017111e8cdfac699a","trusted":true},"cell_type":"code","source":"#y_hat=g_NB.predict_proba(X_Test)\n#y_hat=regr_ol.predict(X_Test_ol)\n#y_hat=(y_hat-y_hat.min())/(y_hat.max()-y_hat.min())\n#y_hat= neigh_grid.predict_proba(X_Test_ol)\n#y_hat = gbm.predict(X_Test, num_iteration=gbm.best_iteration)\n#y_hat= selector.predict(X_Test)\ny_hat= knn_ol.predict_proba(X_Test_ol)\ny_hat=y_hat*0.5/(np.sort(y_hat[:,1])[round((y_hat[:,1].size)*(100-tax)/100)]) # --> ajust the array to have 64% of target > 0.5\n\n#submission['target']=y_hat\nsubmission['target']=y_hat[:,1]\n\nprint ('% target = 1: {}% (in the train sample 64%)'.format(round(float(len(y_hat[y_hat[:,1]>=0.5])) / len(y_hat) * 100)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"601a1d30fc89041f47789aec769bac9b9c500aa8","trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de1dc7977e8b1b7eecee47526f001366dcbd4a53","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}