{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "15f13016-7583-1d9a-dbb2-c3e2c27faa4a"
      },
      "source": [
        "## Sklearn Basic Neural Network (0.583 LB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "84898e14-6057-047a-d85d-e82b52c44a86"
      },
      "source": [
        "In the following network I build a NN based on the Sklearn interface. This is a brief summary of the steps I follow to build it:\n",
        "\n",
        "- Load data\n",
        "- Generate location features\n",
        "- Feature engineering ('basic_preprocess' function)\n",
        "- Normalize features\n",
        "- GridSearch on NN parametes to find the optimum\n",
        "- Generate predictions on the test dataset\n",
        "\n",
        "The generated predictions got a 0.584 log-loss (LB).\n",
        "\n",
        "Some of the feature engineering here is based on two previous notebooks:\n",
        "\n",
        "- [Unsupervised and supervised neighborhood encoding](https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/unsupervised-and-supervised-neighborhood-encoding)\n",
        "- [Price/Bedrooms/Bathrooms](https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths)\n",
        "\n",
        "\n",
        "*Any feedback or comment will be appreciated! Upvote if you found it interesting/useful :)\n",
        "Thanks!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e58ad318-e1a9-2a8a-f831-73579c3acc28"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time as time\n",
        "from sklearn.preprocessing import StandardScaler, Imputer, LabelBinarizer\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.grid_search import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def get_skf_indexes(df, target, kfold=4):\n",
        "    X = df.values\n",
        "    y = df[target].values\n",
        "    skf = StratifiedKFold(n_splits=4);\n",
        "    skf.get_n_splits(X, y);\n",
        "    indexes = [[],[]]\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        indexes[0].append(train_index)\n",
        "        indexes[1].append(test_index)\n",
        "    return indexes\n",
        "\n",
        "\n",
        "def output_results(clf, x_test, listing, fname):\n",
        "    preds = clf.predict_proba(x_test)\n",
        "    preds = pd.DataFrame(preds)\n",
        "    cols = ['low', 'medium', 'high']\n",
        "    preds.columns = cols\n",
        "    preds['listing_id'] = listing\n",
        "    preds.to_csv(fname, index=None)\n",
        "    print(preds[cols].mean().values)\n",
        "\n",
        "\n",
        "def basic_preprocess(df_train, df_test, n_min=50, precision=3):\n",
        "    \n",
        "    # Interest: Numerical encoding of interest level\n",
        "    df_train['y'] = 0.0\n",
        "    df_train.loc[df_train.interest_level=='medium', 'y'] = 1.0\n",
        "    df_train.loc[df_train.interest_level=='high', 'y'] = 2.0\n",
        "    \n",
        "    # Location features: Latitude, longitude\n",
        "    df_train['num_latitude'] = df_train.latitude.values\n",
        "    df_test['num_latitude'] = df_test.latitude.values\n",
        "    df_train['num_longitude'] = df_train.longitude.values\n",
        "    df_test['num_longitude'] = df_test.longitude.values\n",
        "    x = np.sqrt(((df_train.latitude - df_train.latitude.median())**2) + (df_train.longitude - df_train.longitude.median())**2)\n",
        "    df_train['num_dist_from_center'] = x.values\n",
        "    x = np.sqrt(((df_test.latitude - df_train.latitude.median())**2) + (df_test.longitude - df_train.longitude.median())**2)\n",
        "    df_test['num_dist_from_center'] = x.values\n",
        "    df_train['pos'] = df_train.longitude.round(precision).astype(str) + '_' + df_train.latitude.round(precision).astype(str)\n",
        "    df_test['pos'] = df_test.longitude.round(precision).astype(str) + '_' + df_test.latitude.round(precision).astype(str)\n",
        "    \n",
        "    # Degree of \"outlierness\"\n",
        "    OutlierAggregated = (df_train.bedrooms > 4).astype(float)\n",
        "    OutlierAggregated2 = (df_test.bedrooms > 4).astype(float)\n",
        "    OutlierAggregated += (df_train.bathrooms > 3).astype(float)\n",
        "    OutlierAggregated2 += (df_test.bathrooms > 3).astype(float)\n",
        "    OutlierAggregated += (df_train.bathrooms < 1).astype(float)\n",
        "    OutlierAggregated2 += (df_test.bathrooms < 1).astype(float)\n",
        "    x = np.abs((df_train.price - df_train.price.median())/df_train.price.std()) > 0.30\n",
        "    OutlierAggregated += x.astype(float)\n",
        "    x2 = np.abs((df_test.price - df_train.price.median())/df_train.price.std()) > 0.30\n",
        "    OutlierAggregated2 += x2.astype(float)\n",
        "    x = np.log1p(df_train.price/(df_train.bedrooms.clip(1,3) + df_train.bathrooms.clip(1,2))) > 8.2\n",
        "    OutlierAggregated += x.astype(float)\n",
        "    x2 = np.log1p(df_test.price/(df_test.bedrooms.clip(1,3) + df_test.bathrooms.clip(1,2))) > 8.2\n",
        "    OutlierAggregated2 += x2.astype(float)\n",
        "    x = np.sqrt(((df_train.latitude - df_train.latitude.median())**2) + (df_train.longitude - df_train.longitude.median())**2) > 0.30\n",
        "    OutlierAggregated += x.astype(float)\n",
        "    x2 = np.sqrt(((df_test.latitude - df_train.latitude.median())**2) + (df_test.longitude - df_train.longitude.median())**2) > 0.30\n",
        "    OutlierAggregated2 += x2.astype(float)\n",
        "    df_train['num_OutlierAggregated'] = OutlierAggregated.values\n",
        "    df_test['num_OutlierAggregated'] = OutlierAggregated2.values\n",
        "    \n",
        "    # Average interest in unique locations at given precision\n",
        "    x = df_train.groupby('pos')['y'].aggregate(['count', 'mean'])\n",
        "    d = x.loc[x['count'] >= n_min, 'mean'].to_dict()\n",
        "    impute = df_train.y.mean()\n",
        "    df_train['num_pos'] = df_train.pos.apply(lambda x: d.get(x, impute))\n",
        "    df_test['num_pos'] = df_test.pos.apply(lambda x: d.get(x, impute))\n",
        "    \n",
        "    # Density in unique locations at given precision\n",
        "    vals = df_train['pos'].value_counts()\n",
        "    dvals = vals.to_dict()\n",
        "    df_train['num_pos_density'] = df_train['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
        "    df_test['num_pos_density'] = df_test['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
        "\n",
        "    # Building null\n",
        "    df_train['num_building_null'] = (df_train.building_id=='0').astype(float)\n",
        "    df_test['num_building_null'] = (df_test.building_id=='0').astype(float)\n",
        "    \n",
        "    # Building supervised\n",
        "    x = df_train.groupby('building_id')['y'].aggregate(['count', 'mean'])\n",
        "    d = x.loc[x['count'] >= n_min, 'mean'].to_dict()\n",
        "    impute = df_train.y.mean()\n",
        "    df_train['num_building_id'] = df_train.building_id.apply(lambda x: d.get(x, impute))\n",
        "    df_test['num_building_id'] = df_test.building_id.apply(lambda x: d.get(x, impute))\n",
        "    \n",
        "    # Building frequency\n",
        "    d = np.log1p(df_train.building_id.value_counts()).to_dict()\n",
        "    impute = np.min(np.array(list(d.values())))\n",
        "    df_train['num_fbuilding'] = df_train.building_id.apply(lambda x: d.get(x, impute))\n",
        "    df_test['num_fbuilding'] = df_test.building_id.apply(lambda x: d.get(x, impute))\n",
        "    \n",
        "    # Manager supervised\n",
        "    x = df_train.groupby('manager_id')['y'].aggregate(['count', 'mean'])\n",
        "    d = x.loc[x['count'] >= n_min, 'mean'].to_dict()\n",
        "    impute = df_train.y.mean()\n",
        "    df_train['num_manager'] = df_train.manager_id.apply(lambda x: d.get(x, impute))\n",
        "    df_test['num_manager'] = df_test.manager_id.apply(lambda x: d.get(x, impute))\n",
        "\n",
        "    # Manager frequency\n",
        "    d = np.log1p(df_train.manager_id.value_counts()).to_dict()\n",
        "    impute = np.min(np.array(list(d.values())))\n",
        "    df_train['num_fmanager'] = df_train.manager_id.apply(lambda x: d.get(x, impute))\n",
        "    df_test['num_fmanager'] = df_test.manager_id.apply(lambda x: d.get(x, impute))\n",
        "    \n",
        "    # Creation time features\n",
        "    df_train['created'] = pd.to_datetime(df_train.created)\n",
        "    df_train['num_created_weekday'] = df_train.created.dt.dayofweek.astype(float)\n",
        "    df_train['num_created_weekofyear'] = df_train.created.dt.weekofyear\n",
        "    df_test['created'] = pd.to_datetime(df_test.created)\n",
        "    df_test['num_created_weekday'] = df_test.created.dt.dayofweek\n",
        "    df_test['num_created_weekofyear'] = df_test.created.dt.weekofyear\n",
        "    \n",
        "    # Bedrooms/Bathrooms/Price\n",
        "    df_train['num_bathrooms'] = df_train.bathrooms.clip_upper(4)\n",
        "    df_test['num_bathrooms'] = df_test.bathrooms.clip_upper(4)\n",
        "    df_train['num_bedrooms'] = df_train.bedrooms.clip_upper(5)\n",
        "    df_test['num_bedrooms'] = df_test.bedrooms.clip_upper(5)\n",
        "    df_train['num_price'] = df_train.price.clip_upper(10000)\n",
        "    df_test['num_price'] = df_test.price.clip_upper(10000)\n",
        "    bins = df_train.price.quantile(np.arange(0.05, 1, 0.05))\n",
        "    df_train['num_price_q'] = np.digitize(df_train.price, bins)\n",
        "    df_test['num_price_q'] = np.digitize(df_test.price, bins)\n",
        "    \n",
        "    # Composite features based on: \n",
        "    # https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths\n",
        "    df_train['num_priceXroom'] = (df_train.price / (1 + df_train.bedrooms.clip(1, 4) + 0.5*df_train.bathrooms.clip(0, 2))).values\n",
        "    df_test['num_priceXroom'] = (df_test.price / (1 + df_test.bedrooms.clip(1, 4) + 0.5*df_test.bathrooms.clip(0, 2))).values\n",
        "    df_train['num_even_bathrooms'] = ((np.round(df_train.bathrooms) - df_train.bathrooms)==0).astype(float)\n",
        "    df_test['num_even_bathrooms'] = ((np.round(df_test.bathrooms) - df_test.bathrooms)==0).astype(float)\n",
        "    \n",
        "    # Other features\n",
        "    df_train['num_features'] = df_train.features.apply(lambda x: len(x))\n",
        "    df_test['num_features'] = df_test.features.apply(lambda x: len(x))\n",
        "    df_train['num_photos'] = df_train.photos.apply(lambda x: len(x))\n",
        "    df_test['num_photos'] = df_test.photos.apply(lambda x: len(x))\n",
        "    df_train['num_desc_length'] = df_train.description.str.split(' ').str.len()\n",
        "    df_test['num_desc_length'] = df_test.description.str.split(' ').str.len()\n",
        "    df_train['num_desc_length_null'] = (df_train.description.str.len()==0).astype(float)\n",
        "    df_test['num_desc_length_null'] = (df_test.description.str.len()==0).astype(float)\n",
        "    \n",
        "    # Features/Description Features\n",
        "    bows = {'nofee': ['no fee', 'no-fee', 'no  fee', 'nofee', 'no_fee'],\n",
        "            'lowfee': ['reduced_fee', 'low_fee','reduced fee', 'low fee'],\n",
        "            'furnished': ['furnished'],\n",
        "            'parquet': ['parquet', 'hardwood'],\n",
        "            'concierge': ['concierge', 'doorman', 'housekeep','in_super'],\n",
        "            'prewar': ['prewar', 'pre_war', 'pre war', 'pre-war'],\n",
        "            'laundry': ['laundry', 'lndry'],\n",
        "            'health': ['health', 'gym', 'fitness', 'training'],\n",
        "            'transport': ['train', 'subway', 'transport'],\n",
        "            'parking': ['parking'],\n",
        "            'utilities': ['utilities', 'heat water', 'water included']\n",
        "          }\n",
        "    for fname, bow in bows.items():\n",
        "        x1 = df_train.description.str.lower().apply(lambda x: np.sum([1 for i in bow if i in x]))\n",
        "        x2 = df_train.features.apply(lambda x: np.sum([1 for i in bow if i in ' '.join(x).lower()]))\n",
        "        df_train['num_'+fname] = ((x1 + x2) > 0).astype(float).values\n",
        "        x1 = df_test.description.str.lower().apply(lambda x: np.sum([1 for i in bow if i in x]))\n",
        "        x2 = df_test.features.apply(lambda x: np.sum([1 for i in bow if i in ' '.join(x).lower()]))\n",
        "        df_test['num_'+fname] = ((x1 + x2) > 0).astype(float).values\n",
        "\n",
        "    return df_train, df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "36a523f5-3f68-6188-d405-b87c3a999a35"
      },
      "source": [
        "### A. Load and preprocess datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1f3ca83f-9625-60a5-af33-51bcd7852d9d"
      },
      "source": [
        "Load data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d66aff1f-8386-0d09-0ce4-6af1243c71bb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json('../input/train.json')\n",
        "df_test = pd.read_json('../input/test.json')\n",
        "df['created'] = pd.to_datetime(df.created)\n",
        "df_test['created'] = pd.to_datetime(df_test.created)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "444fac90-e6de-dd05-1999-9aee2175b973"
      },
      "source": [
        "Location encoding based on:\n",
        "\n",
        "- [Unsupervised and supervised neighborhood encoding](https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/unsupervised-and-supervised-neighborhood-encoding)\n",
        "- [Price/Bedrooms/Bathrooms](https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "389404b6-0ad7-16dc-6a10-46c3982df469"
      },
      "outputs": [],
      "source": [
        "dftemp = df.copy()\n",
        "for i in ['latitude', 'longitude']:\n",
        "    while(1):\n",
        "        x = dftemp[i].median()\n",
        "        ix = abs(dftemp[i] - x) > 3*dftemp[i].std()\n",
        "        if ix.sum()==0:\n",
        "            break\n",
        "        dftemp.loc[ix, i] = np.nan\n",
        "dftemp = dftemp.loc[dftemp[['latitude', 'longitude']].isnull().sum(1) == 0, :]\n",
        "\n",
        "dfm = DataFrameMapper([(['latitude'], [StandardScaler()]), (['longitude'], [StandardScaler()])])\n",
        "\n",
        "for i in [5, 10, 20, 40]:\n",
        "    pipe_location = make_pipeline(dfm, KMeans(n_clusters=i, random_state=1))\n",
        "    pipe_location.fit(dftemp);\n",
        "    df['location_'+str(i)] = pipe_location.predict(df).astype(str)\n",
        "    df_test['location_'+str(i)] = pipe_location.predict(df_test).astype(str)\n",
        "for i in df.location_10.unique():\n",
        "    df['num_location_10_'+str(i)] = (df.location_10==i).astype(float)\n",
        "    df_test['num_location_10_'+str(i)] = (df_test.location_10==i).astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "82cbe6d5-e8f0-db38-4907-7bb6666be015"
      },
      "source": [
        "### B. Keep only relevant numerical features and normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4998fc01-ef4a-8e52-b646-385891a10980"
      },
      "outputs": [],
      "source": [
        "# Get relevant features\n",
        "df, df_test = basic_preprocess(df, df_test, n_min=15, precision=3)\n",
        "feats = [i for i in df.columns.values if i.startswith('num_')]\n",
        "x_train = df[feats].values\n",
        "x_test = df_test[feats].values\n",
        "print(x_train.shape, x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8fdfe31e-3b3b-50aa-dc16-3a15091b2541"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "for i in range(x_train.shape[1]):\n",
        "    x_test[:, i] = (x_test[:, i] - np.mean(x_train[:, i]))/np.std(x_train[:, i])\n",
        "    x_train[:, i] = (x_train[:, i] - np.mean(x_train[:, i]))/np.std(x_train[:, i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "91365173-5d50-10e3-aed6-9df2bb5668b5"
      },
      "source": [
        "### C. Build and evaluate Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c3ef811-298b-a919-d15a-267da17d6de2"
      },
      "outputs": [],
      "source": [
        "# Classifier\n",
        "clf_nn = MLPClassifier(solver='lbfgs', random_state=1)\n",
        "params = {\n",
        "    'alpha': [1e-6], # 1e-5, 1e-4...\n",
        "    'activation': ['tanh'], # 'relu', 'sigmoid'....\n",
        "    'hidden_layer_sizes': [(10, 30, 5)]#, (30, 30, 5), (20, 20, 20), (30, 30, 5)]\n",
        "}\n",
        "gs_nn = GridSearchCV(clf_nn, param_grid=params, scoring='neg_log_loss', n_jobs=2, cv=2, verbose=2, refit=True) # cv=5\n",
        "start = time.time()\n",
        "gs_nn.fit(x_train, df.y.values)\n",
        "print('- Time: %.2f minutes' % ((time.time() - start)/60))\n",
        "print('- Best score: %.4f' % gs_nn.best_score_)\n",
        "print('- Best params: %s' % gs_nn.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "11bab4cf-f8c6-9c72-4f93-aa7e047cebd1"
      },
      "outputs": [],
      "source": [
        "output_results(gs_nn, x_test, df_test.listing_id.values, 'basic_nn.csv') # 0.58372 LB"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}