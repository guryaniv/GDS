{"cells":[{"metadata":{"_uuid":"63d5b667c3046cc2bb3f30a1d7523a53dc3e0517"},"cell_type":"markdown","source":"# Introduction: Deep Learning with Embedding Layers\n\n\nThis notebook is intended for those who want an introduction into Embedding Layers with Keras. I choosed not to focus on describing the preprocessing nor the different methods, to merge all the table, but rather to focus more specificaly on how to get started in Embedding.\n\nEmbedding is a technique used to encode categorical features like One-Hot encoding or target encoding, it is a bit more difficult to implement but keras allow us to create a model pretty easily.\n\nEmbeddings help to generalize better when the data is sparse and statistics is unknown. Thus, it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit.\n\nWhy should we use Entity Embedding instead of One-Hot Encoding ? There are mutiple reasons for that :\n\n*  One-Hot encoded vectors are high-dimensional and sparse. In this dataset we have a feature that represent an organization type (denoted: ORGANIZATION_TYPE) of 58 distinct value . This means that, when using one-hot encoding, this feature will be represented by a vector containing 58 integers. And 57 of these integers are zeros. In a big dataset or in NLP ( Natural Language Processing) when you have more than 2000 outcomes for a feature, this approach is not computationally efficient.\n\n\n* The vectors of each embedding get updated while training the neural network. This allows us to visualize relationships between words or more generally speaking categories, but also between everything that can be turned into a vector through an embedding layer. Please look at the image below  that show how similarities between categories can be found in a multi-dimensional space.\n\n![](https://cdn-images-1.medium.com/max/1000/1*sXNXYfAqfLUeiDXPCo130w.png)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Merge, merge, Reshape, Dropout, Input, Flatten, Concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import EarlyStopping\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":208,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def display_roc_curve(y_, oof_preds_, folds_idx_):\n    # Plot ROC curves\n    plt.figure(figsize=(6,6))\n    scores = [] \n    for n_fold, (_, val_idx) in enumerate(folds_idx_):  \n        # Plot the roc curve\n        fpr, tpr, thresholds = roc_curve(y_.iloc[val_idx], oof_preds_[val_idx])\n        score = roc_auc_score(y_.iloc[val_idx], oof_preds_[val_idx])\n        scores.append(score)\n        plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (n_fold + 1, score))\n    \n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Luck', alpha=.8)\n    fpr, tpr, thresholds = roc_curve(y_, oof_preds_)\n    score = roc_auc_score(y_, oof_preds_)\n    plt.plot(fpr, tpr, color='b',\n             label='Avg ROC (AUC = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n             lw=2, alpha=.8)\n    \n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Embedding Neural Network ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.tight_layout()\n    plt.show()\n    \ndef display_precision_recall(y_, oof_preds_, folds_idx_):\n    # Plot ROC curves\n    plt.figure(figsize=(6,6))\n    \n    scores = [] \n    for n_fold, (_, val_idx) in enumerate(folds_idx_):  \n        # Plot the roc curve\n        fpr, tpr, thresholds = roc_curve(y_.iloc[val_idx], oof_preds_[val_idx])\n        score = average_precision_score(y_.iloc[val_idx], oof_preds_[val_idx])\n        scores.append(score)\n        plt.plot(fpr, tpr, lw=1, alpha=0.3, label='AP fold %d (AUC = %0.4f)' % (n_fold + 1, score))\n    \n    precision, recall, thresholds = precision_recall_curve(y_, oof_preds_)\n    score = average_precision_score(y_, oof_preds_)\n    plt.plot(precision, recall, color='b',\n             label='Avg ROC (AUC = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n             lw=2, alpha=.8)\n    \n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Embedding Neural Network Recall / Precision')\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    \n    plt.show()","execution_count":209,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ad73c91694dd815f732dae39e1db753849df26ba"},"cell_type":"code","source":"def preprocessing(input_dir, debug=False):\n    # No target encoding\n    num_rows = 10000 if debug else None\n    \n    print('Preprocessing started.')\n    print('Bureau_Balance')\n    buro_bal = pd.read_csv(input_dir + 'bureau_balance.csv', nrows=num_rows)\n    \n    buro_counts = buro_bal[['SK_ID_BUREAU', 'MONTHS_BALANCE']].groupby('SK_ID_BUREAU').count()\n    buro_bal['buro_count'] = buro_bal['SK_ID_BUREAU'].map(buro_counts['MONTHS_BALANCE'])\n    \n    avg_buro_bal = buro_bal.groupby('SK_ID_BUREAU').mean()\n    \n    avg_buro_bal.columns = ['avg_buro_' + f_ for f_ in avg_buro_bal.columns]\n    del buro_bal\n    gc.collect()\n    \n    print('Bureau')\n    buro_full = pd.read_csv(input_dir + 'bureau.csv', nrows=num_rows)\n\n    gc.collect()\n    \n    buro_full = buro_full.merge(right=avg_buro_bal.reset_index(), how='left', on='SK_ID_BUREAU', suffixes=('', '_bur_bal'))\n    \n    nb_bureau_per_curr = buro_full[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\n    buro_full['SK_ID_BUREAU'] = buro_full['SK_ID_CURR'].map(nb_bureau_per_curr['SK_ID_BUREAU'])\n    \n    avg_buro = buro_full.groupby('SK_ID_CURR').mean()\n    \n    del buro_full\n    gc.collect()\n    \n    print('Previous_Application')\n    prev = pd.read_csv(input_dir + 'previous_application.csv', nrows=num_rows)\n    \n    prev_cat_features = [\n        f_ for f_ in prev.columns if prev[f_].dtype == 'object'\n    ]\n    \n    \n    nb_prev_per_curr = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n    prev['SK_ID_PREV'] = prev['SK_ID_CURR'].map(nb_prev_per_curr['SK_ID_PREV'])\n    \n    avg_prev = prev.groupby('SK_ID_CURR').mean()\n    del prev\n    gc.collect()\n    \n    print('POS_CASH_Balance')\n    pos = pd.read_csv(input_dir + 'POS_CASH_balance.csv', nrows=num_rows)\n    \n    \n    nb_prevs = pos[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n    pos['SK_ID_PREV'] = pos['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n    \n    avg_pos = pos.groupby('SK_ID_CURR').mean()\n    \n    del pos, nb_prevs\n    gc.collect()\n    \n    print('Credit_Card_Balance')\n    cc_bal = pd.read_csv(input_dir + 'credit_card_balance.csv', nrows=num_rows)\n\n    nb_prevs = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n    cc_bal['SK_ID_PREV'] = cc_bal['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n    \n    avg_cc_bal = cc_bal.groupby('SK_ID_CURR').mean()\n    avg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\n    \n    del cc_bal, nb_prevs\n    gc.collect()\n    \n    print('Installments_Payments')\n    inst = pd.read_csv(input_dir + 'installments_payments.csv', nrows=num_rows)\n    nb_prevs = inst[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n    inst['SK_ID_PREV'] = inst['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n    \n    avg_inst = inst.groupby('SK_ID_CURR').mean()\n    avg_inst.columns = ['inst_' + f_ for f_ in avg_inst.columns]\n    \n    print('Train/Test')\n    data = pd.read_csv(input_dir + 'application_train.csv', nrows=num_rows)\n    test = pd.read_csv(input_dir + 'application_test.csv', nrows=num_rows)\n    print('Shapes : ', data.shape, test.shape)\n        \n    data = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n    \n    data = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n    \n    data = data.merge(right=avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right=avg_pos.reset_index(), how='left', on='SK_ID_CURR')\n    \n    data = data.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n    \n    data = data.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n    test = test.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n    \n    del avg_buro, avg_prev\n    gc.collect()\n    \n    print('Preprocessing done.')\n\n    return data, test","execution_count":210,"outputs":[]},{"metadata":{"_uuid":"38534789d4b292c4dd861e08e83e0f6f8bb398d8"},"cell_type":"markdown","source":"# Prepare the data"},{"metadata":{"trusted":true,"_uuid":"896323d914471d700c6d44e60fa7995baf15cbfa","collapsed":true},"cell_type":"code","source":"train, test = preprocessing('../input/', debug=False)","execution_count":211,"outputs":[]},{"metadata":{"_uuid":"a2b0c4aeea24f1a91f637066164c4012a991d502"},"cell_type":"markdown","source":"There is **307511** lines in the train file and **48744** lines in the test files. We have 121 differents features ( I'm deliberating excluding **SK_ID_CURR** which act as an ID and the **TARGET** variable)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"128bcebaae343f5d656ee414f0a78f30f152c924"},"cell_type":"code","source":"# Drop the target and the ID\nX_train, y_train = train.iloc[:,2:], train.TARGET\nX_test = test.iloc[:,1:]","execution_count":212,"outputs":[]},{"metadata":{"_uuid":"d10397a374a67e08f069ddb9f36236ec4ca28cbc"},"cell_type":"markdown","source":"# Variable Type"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"befa6fb4c70997184931155663d81ec6a5ec901a"},"cell_type":"code","source":"col_vals_dict = {c: list(X_train[c].unique()) for c in X_train.columns if X_train[c].dtype == object}","execution_count":213,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f8810989afeb8aab9f5f8e03729de27c7367fbd","collapsed":true},"cell_type":"code","source":"nb_numeric   = len(X_train.columns) - len(col_vals_dict)\nnb_categoric = len(col_vals_dict)\nprint('Number of Numerical features:', nb_numeric)\nprint('Number of Categorical features:', nb_categoric)","execution_count":214,"outputs":[]},{"metadata":{"_uuid":"18639e3db5a4d0648a1ec2c24c0b1a70c6cd94b9"},"cell_type":"markdown","source":"# Label encode the categorical features"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d9825be52abe7dbf07e72ac3d01da4032490bf9"},"cell_type":"code","source":"# Store the labels of each features\ncol_vals_dict = {c: list(X_train[c].unique()) for c in X_train.columns if X_train[c].dtype == object}","execution_count":215,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"305c7e8319d5d00a0e9ce3954351f3e7be040dbd"},"cell_type":"code","source":"# Generator to parse the cat\ngenerator = (c for c in X_train.columns if X_train[c].dtype == object)\n\n# Label Encoder\nfor c in generator:\n    lbl = LabelEncoder()\n    lbl.fit(list(X_train[c].values) + list(X_test[c].values))\n    X_train[c] = lbl.transform(list(X_train[c].values))\n    X_test[c] = lbl.transform(list(X_test[c].values))","execution_count":216,"outputs":[]},{"metadata":{"_uuid":"6a5a41b7899e6767f31ce77af14be7afe307323c"},"cell_type":"markdown","source":"# Create the network"},{"metadata":{"_uuid":"3094546e72beb9f09c11c357ff9f40152a1f5aea"},"cell_type":"markdown","source":"In order to create our embedding model we need to have a look at the spatiality of the cat features. We choose here to use Embedding only on cat features that present more than 2 outcomes otherwise it is count as a numeric value (0 or 1)."},{"metadata":{"trusted":true,"_uuid":"41c5e56c1d82eea39aa639b1c3cc14d519462bb7","collapsed":true},"cell_type":"code","source":"embed_cols = []\nlen_embed_cols = []\nfor c in col_vals_dict:\n    if len(col_vals_dict[c])>2:\n        embed_cols.append(c)\n        len_embed_cols.append(len(col_vals_dict[c]))\n        print(c + ': %d values' % len(col_vals_dict[c])) #look at value counts to know the embedding dimensions\n        \nprint('\\n Number of embed features :', len(embed_cols))","execution_count":217,"outputs":[]},{"metadata":{"_uuid":"26ba736bebee43b04b75856030dc685986d14349"},"cell_type":"markdown","source":"We are including 13 features **out of 16 categorical features** into our Embedding.\n\nWe can see that our features have a reatively small number of outcomes except for **OCCUPATION_TYPE** and **ORGANIZATION_TYPE** which will be represented in a high dimensional spaces in our Embedding.\n\nThe first layer of our network is the embedding layer with the size of 3 \"CODE_GENDER\". The embedding-size defines the dimensionality in which we map the categorical variables (in a 3D spaces for instance). One good rule of thumb to use for the output is : \n\n**embedding size = min(50, number of categories/2)**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8c11ad89dcd9df38633cf40e058aeba03dc70aa"},"cell_type":"code","source":"def build_embedding_network(len_embed_cols):\n    \n    model_out = []\n    model_in  = []\n    \n    for dim in len_embed_cols:\n        input_dim = Input(shape=(1,), dtype='int32')\n        embed_dim = Embedding(dim, dim//2, input_length=1)(input_dim)\n        embed_dim = Dropout(0.25)(embed_dim)\n        embed_dim = Reshape((dim//2,))(embed_dim)\n        model_out.append(embed_dim)\n        model_in.append(input_dim)\n    \n    input_num = Input(shape=(176,), dtype='float32')\n    outputs = Concatenate(axis=1)([*model_out, input_num])\n    \n    outputs = (Dense(128))(outputs) \n    outputs = (Activation('relu'))(outputs)\n    outputs = (Dropout(.35))(outputs)\n    outputs = (Dense(64))(outputs)\n    outputs = (Activation('relu'))(outputs)\n    outputs = (Dropout(.15))(outputs)\n    outputs = (Dense(32))(outputs) \n    outputs = (Activation('relu'))(outputs)\n    outputs = (Dropout(.15))(outputs)\n    outputs = (Dense(1))(outputs)\n    outputs = (Activation('sigmoid'))(outputs)\n    \n    model = Model([*model_in, input_num], outputs)\n\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return model","execution_count":218,"outputs":[]},{"metadata":{"_uuid":"d1b3f27b8024cb18551ebd01c6623bc12032443d"},"cell_type":"markdown","source":"In order for keras to know which features are going to be included into the Embedding layers we need to create a list containing for each feature the corresponding numpy array (**13** in total for us). The last element of the list will be our numerical features (**173**) and the categorical features that we decided not to include in the Embedding (**3**) for a total of **176** distinct features.\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"94257dbc6efbc5629498a0738bc480854f1df854"},"cell_type":"code","source":"def preproc(X_train, X_val, X_test):\n\n    input_list_train = []\n    input_list_val = []\n    input_list_test = []\n    \n    #the cols to be embedded: rescaling to range [0, # values)\n    for c in embed_cols:\n        raw_vals = np.unique(X_train[c])\n        val_map = {}\n        for i in range(len(raw_vals)):\n            val_map[raw_vals[i]] = i       \n        input_list_train.append(X_train[c].map(val_map).values)\n        input_list_val.append(X_val[c].map(val_map).fillna(0).values)\n        input_list_test.append(X_test[c].map(val_map).fillna(0).values)\n        \n    #the rest of the columns\n    other_cols = [c for c in X_train.columns if (not c in embed_cols)]\n    input_list_train.append(X_train[other_cols].values)\n    input_list_val.append(X_val[other_cols].values)\n    input_list_test.append(X_test[other_cols].values)\n    \n    return input_list_train, input_list_val, input_list_test","execution_count":219,"outputs":[]},{"metadata":{"_uuid":"7500ddea23c2636a50f32a2d910bb2fff3fe16df"},"cell_type":"markdown","source":"Let us go more specifically into this function : "},{"metadata":{"trusted":true,"_uuid":"7da2cf8a5c332e1f279674030095fb0f707a96c8","collapsed":true},"cell_type":"code","source":"proc_X_train_f, proc_X_val_f, proc_X_test_f = preproc(X_train, X_train, X_test)\nprint('Length of the list:', len(proc_X_train_f))","execution_count":220,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5b80f1aa1f8863822a49e9073a2a63f045900ed","collapsed":true},"cell_type":"code","source":"proc_X_train_f","execution_count":221,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bab380a6909716a8d357ba645413a6be25eec71","collapsed":true},"cell_type":"code","source":"print(proc_X_train_f[12].shape)","execution_count":222,"outputs":[]},{"metadata":{"_uuid":"ab3ddb15f9326ada81b183a6f9ceb6a990315a2b"},"cell_type":"markdown","source":"This list will be passed into the network. It is composed of 14 numpy arrays containing our categorical features that are going throught the Embedding Layers (**13 layers**). The last element of the list is a numpy array composed of the **173 numerics features added to the 3 categorical features that have at most 2 distinct outcomes**. "},{"metadata":{"trusted":true,"_uuid":"8ec6c70d8f49e26943fd7c0aa605ebc5ac781e10","collapsed":true},"cell_type":"code","source":"del proc_X_train_f, proc_X_val_f, proc_X_test_f\ngc.collect()","execution_count":223,"outputs":[]},{"metadata":{"_uuid":"920d717b3463745139d91446f2d1301a338a05f5"},"cell_type":"markdown","source":"# Prepare the data\n\nIn neural networks, it is a best practice to scale input data before use. Data scaling\nmakes the training of the network faster, memory efficient and yield accurate\nforecast results. Neural networks only work with data usually between a specified range (1 to 1 or 0 to 1), it makes it necessary then that data is scaled down and normalized. \n\nScaling can be as simple as taking the ratios (reciprocal normalization), computing the differences\n(range normalization) or multiplicative normalization.\nNormalization ensures that data is roughly uniformly distributed between the network inputs\nand the outputs."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4d22bacb8ecd1751012e7930ccf4fc5c519d03b8"},"cell_type":"code","source":"# Select the numeric features\nnum_cols = [x for x in X_train.columns if x not in embed_cols]\n\n\n# Impute missing values in order to scale\nX_train[num_cols] = X_train[num_cols].fillna(value = 0)\nX_test[num_cols] = X_test[num_cols].fillna(value = 0)\n\n# Fit the scaler only on train data\nscaler = MinMaxScaler().fit(X_train[num_cols])\nX_train.loc[:,num_cols] = scaler.transform(X_train[num_cols])\nX_test.loc[:,num_cols] = scaler.transform(X_test[num_cols])","execution_count":224,"outputs":[]},{"metadata":{"_uuid":"616bb7c968857a69128897e417e61eb7b055216e"},"cell_type":"markdown","source":"# Train the network"},{"metadata":{"trusted":true,"_uuid":"d5700bbc99661e1a6b6f87d7d9aa05ffd80e2446","collapsed":true},"cell_type":"code","source":"K = 5\nruns_per_fold = 1\nn_epochs = 250\npatience = 10\n\ncv_aucs   = []\nfull_val_preds = np.zeros(np.shape(X_train)[0])\ny_preds = np.zeros((np.shape(X_test)[0],K))\n\nkfold = StratifiedKFold(n_splits = K,  \n                            shuffle = True, random_state=1)\n\nfor i, (f_ind, outf_ind) in enumerate(kfold.split(X_train, y_train)):\n\n    X_train_f, X_val_f = X_train.loc[f_ind].copy(), X_train.loc[outf_ind].copy()\n    y_train_f, y_val_f = y_train[f_ind], y_train[outf_ind]\n    \n    X_test_f = X_test.copy()\n    \n    \n    # Shuffle data\n    idx = np.arange(len(X_train_f))\n    np.random.shuffle(idx)\n    X_train_f = X_train_f.iloc[idx]\n    y_train_f = y_train_f.iloc[idx]\n    \n    #preprocessing\n    proc_X_train_f, proc_X_val_f, proc_X_test_f = preproc(X_train_f, X_val_f, X_test_f)\n    \n    #track oof prediction for cv scores\n    val_preds = 0\n    \n    for j in range(runs_per_fold):\n    \n        NN = build_embedding_network(len_embed_cols)\n\n        # Set callback functions to early stop training and save the best model so far\n        callbacks = [EarlyStopping(monitor='val_loss', patience=patience)]\n\n        NN.fit(proc_X_train_f, y_train_f.values, epochs=n_epochs, batch_size=4096, verbose=1,callbacks=callbacks,validation_data=(proc_X_val_f, y_val_f))\n        \n        val_preds += NN.predict(proc_X_val_f)[:,0] / runs_per_fold\n        y_preds[:,i] += NN.predict(proc_X_test_f)[:,0] / runs_per_fold\n        \n    full_val_preds[outf_ind] += val_preds\n        \n    cv_auc  = roc_auc_score(y_val_f.values, val_preds)\n    cv_aucs.append(cv_auc)\n    print ('\\nFold %i prediction cv AUC: %.5f\\n' %(i,cv_auc))\n    \nprint('Mean out of fold AUC: %.5f' % np.mean(cv_auc))\nprint('Full validation AUC: %.5f' % roc_auc_score(y_train.values, full_val_preds))","execution_count":225,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c528d98b9cdfea9738005134185da1be3b10f53e","collapsed":true},"cell_type":"code","source":"folds_idx = [(trn_idx, val_idx) for trn_idx, val_idx in kfold.split(X_train, y_train)]\ndisplay_roc_curve(y_=y_train, oof_preds_=full_val_preds, folds_idx_=folds_idx)","execution_count":226,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ce3f7bbec17fcb2a5a3f77e11d92ede2b0ed76e","collapsed":true},"cell_type":"code","source":"display_precision_recall(y_=y_train, oof_preds_=full_val_preds, folds_idx_=folds_idx)","execution_count":227,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9d7ea775d553ef13ba3049075ce181b97fea8950"},"cell_type":"code","source":"test['TARGET'] = np.mean(y_preds, axis=1)\ntest = test[['SK_ID_CURR', 'TARGET']]\nout_df = pd.DataFrame({'SK_ID_CURR': test['SK_ID_CURR'], 'TARGET': test['TARGET']})\nout_df.to_csv('nn_embedding_submission.csv', index=False)","execution_count":228,"outputs":[]},{"metadata":{"_uuid":"be1b6dfa3b3dd1e7487f63cec322085bcc5c8a7d"},"cell_type":"markdown","source":"We can see that the model is performing well with an **AVG AUC of 0.75 on CV5 out-of-fold ** and an **AUC of 0.748 on LB**. \n\nHowever I'm having difficulties to perform as well as Boosted Trees like XGBoost, Lgbm and Catboost. If anyone have any hint on how to improve this kernel, please let me know."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}