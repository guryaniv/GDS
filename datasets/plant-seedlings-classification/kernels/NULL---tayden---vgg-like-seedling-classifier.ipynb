{"nbformat": 4, "cells": [{"execution_count": null, "source": ["import tensorflow as tf\n", "import numpy as np\n", "import matplotlib.pylab as plt\n", "import keras\n", "import os\n", "from PIL import Image, ImageOps\n", "\n", "%matplotlib inline"], "outputs": [], "metadata": {}, "cell_type": "code"}, {"execution_count": null, "source": ["SPECIES = [\n", "    \"Black-grass\",\n", "    \"Charlock\",\n", "    \"Cleavers\",\n", "    \"Common Chickweed\",\n", "    \"Common wheat\",\n", "    \"Fat Hen\",\n", "    \"Loose Silky-bent\",\n", "    \"Maize\",\n", "    \"Scentless Mayweed\",\n", "    \"Shepherds Purse\",\n", "    \"Small-flowered Cranesbill\",\n", "    \"Sugar beet\"\n", "]\n", "TRAIN_PATH = './train'\n", "TEST_PATH = './test'\n", "RANDOM_SEED=42\n", "IMG_DIMS = (100, 100)\n", "IMG_BANDS = 3"], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "source": ["# Load the data\n", "train_X = []\n", "train_y = []\n", "for sp_id, sp in enumerate(SPECIES):\n", "    for img in os.listdir(os.path.join(TRAIN_PATH, sp)):\n", "        path = os.path.join(TRAIN_PATH, sp, img)\n", "        img = Image.open(path)\n", "        train_X.append(ImageOps.fit(img, IMG_DIMS, Image.ANTIALIAS).convert('RGB'))\n", "        train_y.append(sp_id)\n", "        \n", "test_X = []\n", "for img in os.listdir(TEST_PATH):\n", "    path = os.path.join(TEST_PATH, img)\n", "    img = Image.open(path)\n", "    test_X.append(ImageOps.fit(img, IMG_DIMS, Image.ANTIALIAS).convert('RGB'))"], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "source": ["from sklearn.model_selection import train_test_split\n", "\n", "# Create the validation set\n", "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=RANDOM_SEED)"], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "source": ["# Convert all images to ndarray\n", "train_X = np.array([np.array(im) for im in train_X])\n", "train_X = train_X.reshape(train_X.shape[0], IMG_DIMS[0], IMG_DIMS[1], IMG_BANDS) / 255\n", "\n", "val_X = np.array([np.array(im) for im in val_X])\n", "val_X = val_X.reshape(val_X.shape[0], IMG_DIMS[0], IMG_DIMS[1], IMG_BANDS) / 255\n", "\n", "test_X = np.array([np.array(im) for im in test_X])\n", "test_X = test_X.reshape(test_X.shape[0], IMG_DIMS[0], IMG_DIMS[1], IMG_BANDS) / 255\n", "\n", "# Convert to one-hot labels\n", "train_y = np.asarray(keras.utils.to_categorical(train_y, num_classes=len(SPECIES)))\n", "val_y = np.asarray(keras.utils.to_categorical(val_y, num_classes=len(SPECIES)))"], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "source": ["from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Flatten\n", "from keras.layers import Conv2D, MaxPooling2D\n", "from keras.optimizers import Adam\n", "from keras.callbacks import ModelCheckpoint\n", "\n", "model = Sequential()\n", "\n", "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(IMG_DIMS[0], IMG_DIMS[1], IMG_BANDS)))\n", "model.add(MaxPooling2D(pool_size=(2, 2)))\n", "\n", "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n", "model.add(MaxPooling2D(pool_size=(2, 2)))\n", "\n", "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n", "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n", "model.add(MaxPooling2D(pool_size=(2, 2)))\n", "\n", "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n", "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n", "model.add(MaxPooling2D(pool_size=(2, 2)))\n", "\n", "model.add(Flatten())\n", "model.add(Dense(512, activation='relu'))\n", "model.add(Dropout(0.3))\n", "model.add(Dense(512, activation='relu'))\n", "model.add(Dropout(0.3))\n", "model.add(Dense(len(SPECIES), activation='softmax'))\n", "\n", "adam = Adam()\n", "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n", "\n", "checkpoint = ModelCheckpoint('./model.hd5', monitor='val_acc', save_best_only=True, mode='auto', period=1)\n", "history = model.fit(train_X, train_y, batch_size=32, epochs=100, \n", "                    validation_data=(val_X, val_y),\n", "                    callbacks=[checkpoint])"], "outputs": [], "metadata": {"scrolled": true}, "cell_type": "code"}, {"execution_count": null, "source": ["from keras.models import load_model\n", "\n", "model = load_model('./model.hd5')\n", "\n", "score = model.evaluate(val_X, val_y, batch_size=128)\n", "print(\"Validation score: %f\" % score[1])"], "outputs": [], "metadata": {}, "cell_type": "code"}, {"execution_count": null, "source": ["# Generate a submission\n", "predictions = model.predict(test_X, batch_size=128, verbose=0)\n", "predictions = np.argmax(predictions, axis=1)"], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "source": ["import csv\n", "\n", "output = [[path, SPECIES[predictions[i]]] for i, path in enumerate(os.listdir(TEST_PATH))]\n", "\n", "with open('submission.csv', 'w') as csvfile:\n", "    writer = csv.writer(csvfile, delimiter=',')\n", "    writer.writerow(['file', 'species'])\n", "    for r in output:\n", "        writer.writerow(r)    "], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "source": [], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}], "nbformat_minor": 1, "metadata": {"language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "nbconvert_exporter": "python", "version": "3.5.2", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}}