{"cells":[{"metadata":{"_uuid":"1da8f76df50359b40086856597fefea47e4c8e24"},"cell_type":"markdown","source":"# [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge)"},{"metadata":{"_uuid":"f92a37555b46b9acfa4733b88649e78a3aa79d2f"},"cell_type":"markdown","source":"## Table of contents: <a name='TOC'></a>\n[Changelog](#A)  \n[Sources](#B)  \n[About](#C)  \n[U-Net model](#D)  \n[Setup notebook](#E)  \n[Data Exploration](#F)  \n[Visualize images and masks (overlayed)](#G)  \n[Train validation split](#H)  \n[Custom metrics](#I)  \n[Model architecture](#J)  \n[Build model](#K)  \n[Train model](#L)  \n[Learning curves](#M)  \n[Check performance on validation set](#N)  \n[Scoring](#O)  \n[Predict test set](#P)  \n[Submission](#Q)  "},{"metadata":{"_uuid":"58dd135446f3299c81729910084390b2066f6ba0"},"cell_type":"markdown","source":"## Changelog <a name='A'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\n15/aug/18:\n- added TOC and anchors\n- removed dataframe features - as duplicates of np.array\n- removed dice_loss function\n- removed 'depth' input at middle of model\n\n14/aug/18:\n - standardize depth and input earlier in model\n - added image cumsum as second layer\n - removed data generator for augemnting and replaced by np.fliplr()\n - added DEV mode, to speed up commit and avoid dying kernel (memory error)\n\n13/aug/18:\n - changed coverage_class - typo\n - to prevent kernel from dying(and unable to commit), added code to delete unused lists, features  \n - changed scoring - best iou prediction threshold\n \n 12/aug/18:  \n - Added tuning options:\n   - SGD optimizer\n   - kullback_leibler_divergence loss function (doesnot perform well)\n   - gradient clipping, to control beginning, plateaus and shoulders\n - changed RLenc function\n - changed dice_coef_loss\n - changed coverage_class\n\n11/aug/18: \n- Start UNet with 16 channels (iso 8)\n- Cleaning code and comments\n\n10/aug/18: \n- Changed hyperparameters\n- Added depth to middle of model\n\n"},{"metadata":{"_uuid":"9ea679105fd85402fcf0af151c011a9f11ebcfab"},"cell_type":"markdown","source":"## Sources that earn credits <a name='B'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\nI got initial inspiration and insights for this notebook from:  \n\n[Alexander Liao](https://www.kaggle.com/alexanderliao/u-net-bn-aug-strat-dice/notebook)   \n[Jesper Dramsch](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics)   \n[Bruno G, do Amaral](https://www.kaggle.com/bguberfain/unet-with-depth)   \n[Bartek](https://www.kaggle.com/melgor/u-net-batchnorm-augmentation-stratificat-b0026c)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"## About<a name='C'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\nSeismic data is a neat thing. You can imagine it like an ultra-sound of the subsurface. However, in an ultra-sound, we use much smaller wavelengths to image our body. Seismic data usually has wavelengths around 1m to 100m. That has some physical implications, but for now, we don't have to deal with that. It's just something to keep in mind while thinking about resolution. \n\nImaging salt has been a huge topic in the seismic industry, basically since they imaged salt the first time. The Society of Exploration geophysicist alone has over 10,000 publications with the [keyword salt](https://library.seg.org/action/doSearch?AllField=salt). Salt bodies are important for the hydrocarbon industry, as they usually form nice oil traps. So there's a clear motivation to delineate salt bodies in the subsurface. If you would like to do a deep dive, you can see [this publication](https://www.iongeo.com/content/documents/Resource%20Center/Articles/INT_Imaging_Salt_tutorial_141101.pdf)\n\nSeismic data interpreters are used to interpreting on 2D or 3D images that have been heavily processed. The standard work of [seismic data analysis](https://wiki.seg.org/wiki/Seismic_Data_Analysis) is open access.\nYou'll find sections on Salt in there as well (https://wiki.seg.org/wiki/Salt-flank_reflections and https://wiki.seg.org/wiki/Salt_flanks). The seismic itself is pretty \"old\" in the publication, and you're dealing with data that is less noisy here, which is nice.\n\n[![Seismic Data with salt CC-BY-SA Yilmaz](https://wiki.seg.org/images/1/14/Ch05_fig0-1.png)](https://wiki.seg.org/wiki/Salt-flank_reflections#/media/File:Ch05_fig0-1.png)\nCaption: Figure 5.0-1  Conflicting dips associated with salt flanks: (a) CMP stack without dip-moveout correction; (b) time migration of the stack in (a); (c) the stack with dip-moveout correction; (d) time migration of the stack in (c). CC-BY-SA Yilmaz.\n\nInterpretation on seismic images has long used texture attributes, to identify better and highlight areas of interest. These can be seen like feature maps on the texture of the seismic. For salt, you will notice that the texture in the salt masks is rather chaotic, where the surrounding seismic is more \"striped\". You can think of Earth as layered. Sand gets deposited on top of existing sand. In comes salt, which is behaving very much, unlike other rocks. There is an entire research branch dedicated to salt tectonics, that is the movement of salt in the subsurface. To give you the gist, these salt diapirs form from salt layers somewhere else that were under much pressure. These started to flow (behave ductile) and find a way into other layers above. I have written a bit about salt on [my blog](http://the-geophysicist.com/the-showroom-data-for-my-thesis).\n\nOne common seismic attribute is called \"chaos\" or \"seismic disorder\". So if you talk to cynic geophysicists, you'll hear \"that deep learning better outperform the Chaos attribute\". A good starting point is [this publication](http://www.chopraseismic.com/wp-content/uploads/2016/08/Chopra_Marfurt_TLE_Aug2016-LowRes.pdf).\n\nRecently, geoscience has started to adopt deep learning, and it has seen a clear boom, particularly in imaging salt. Code for automatic seismic interpretation can be found here: \n\n+ https://github.com/waldeland/CNN-for-ASI\n+ https://github.com/bolgebrygg/MalenoV\n+ https://github.com/crild/facies_net\n\nYou will notice that these solutions load a specific SEG-Y file, which luckily we don't have to bother with. TGS provided some nice PNG files instead. However, you can glean some information from them how to approach seismic data. If you find you need some geophysical helpers, you can [import Bruges](https://github.com/agile-geoscience/bruges)\n"},{"metadata":{"_uuid":"2e06f2634d4797bd0a37c33fcc1dde71fa5df3e7"},"cell_type":"markdown","source":"## U-Net model<a name='D'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\nThe seismic images resemble X-rays and Ultrasound scans. The U-Net model is a CNN used for Biomedical Image Segmentation.   \n\nPlease read the paper: <a href=\"https://arxiv.org/pdf/1505.04597.pdf\"> U-Net: Convolutional Networks for Biomedical Image Segmentation</a>) \n\n<a href=\"https://github.com/jocicmarko/ultrasound-nerve-segmentation\">Another interesting read</a> from the Kaggle Ultrasound Nerve Segmentation competition.</p>\n\n<p><img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"\"></p>"},{"metadata":{"_uuid":"50e97c3bc54a8d6426e627e1fd80ed5453640297"},"cell_type":"markdown","source":"## Setup notebook<a name='E'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>"},{"metadata":{"_uuid":"d40de30474788a5cfd2a6634173bdd69ca00ce71"},"cell_type":"markdown","source":"### Notebook mode"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9afadfe2cccbae6a2e16e1fb960f2321d282be74"},"cell_type":"code","source":"# Set Commit/Development mode\nCOMMIT = True\nDEV = not COMMIT\nDBG = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd830b0bbe87f8178ea073bc8ecf3fd12c57bb44"},"cell_type":"markdown","source":"### Profiling helpers"},{"metadata":{"trusted":true,"_uuid":"3f34558620dc6297f6e9590a082aeb5f95e5a6d4","collapsed":true},"cell_type":"code","source":"def mem_used():\n    \"\"\"Memory used\"\"\"\n    import resource\n    return round(resource.getrusage(resource.RUSAGE_SELF)[2] * 10/1028 / 10, 1)\n\ndef mem_fun(fun, **kwargs):\n    \"\"\"\"\"\"\n    mem_start = mem_used()\n    _ = fun(**kwargs)\n    print(f'memory used by function: {(mem_used() - mem_start):.1f}mb')\n\nnb_mem = mem_used()\nf'Initial memory used by this notebook: {nb_mem}mb'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"00833d394e3069216af171fd979c814e7e1e430d","trusted":true,"scrolled":false,"collapsed":true},"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport random\nfrom memory_profiler import profile\n\nimport pandas as pd\nimport numpy as np\nfrom itertools import chain\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n%matplotlib inline\n\nfrom tqdm import tqdm_notebook\n\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout, BatchNormalization, UpSampling2D\nfrom keras.layers.core import Lambda, RepeatVector, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.optimizers import Adam, SGD\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd4ff5631c7a2da1195d9a3acd56b84f794ff015"},"cell_type":"markdown","source":"> ### Set file locations\nFor convenience when working from different locations."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"0e26e21ff39e8b2afc0003fec4e4f5269f61aa4c","collapsed":true,"trusted":true},"cell_type":"code","source":"path = '../input'\npath_train = f'{path}/train'\npath_test = f'{path}/test'\nimgs_train = f'{path}/train/images'\nmasks_train = f'{path}/train/masks'\nimgs_test = f'{path}/test/images'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a911664c3a24af153504d8d82491bc3b903fb04"},"cell_type":"markdown","source":"### Set/initiate image constants"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"67f625a8c96490b72fe6b30fa8383957890934c5"},"cell_type":"code","source":"IMG_SIZE = 101   # original/raw image size\nTGT_SIZE = 128   # model/input image size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37120ca59ac0ae444b89509fe5b9befa8906cb27","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Check memory allocation needed for train/test set\ndef test_alloc_size(n, m, c):\n    \"\"\"\"\"\"\n    imgs_test_alloc = list(range(n))\n    for i in range(n):\n        imgs_test_alloc[i] = np.ones((m, TGT_SIZE, TGT_SIZE, c)) * random.randint(0,100)\n    return imgs_test_alloc\n    \nif DBG:\n    mem_fun(test_alloc_size, n=1, m=18000, c=2)\n    print(f'notebook memory used: {mem_used()}mb')\n    ## memory used by function: 4482.3mb for (1800, 128, 1288, 2)\n    ## notebook memory used: 4805.7mb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dcda0baa980348f9bca3912f52a685fdcea6216","collapsed":true},"cell_type":"code","source":"print(f'notebook memory used: {mem_used()}mb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21ee3e81182b5e6ea089c68ac5daf245602524fb","collapsed":true},"cell_type":"code","source":"# Helper functions\ndef upsample(img, img_size_target=TGT_SIZE):\n    \"\"\"Resize image to target shape(model_input) or back to original shape\"\"\"\n    if img.shape[0] == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n\n\ndef imgs_2_array(path, img_names, ftype='.png', size=TGT_SIZE, flip=True):\n    \"\"\"Load images and transform to array with image and cumsum layer\"\"\"\n    imgs = np.zeros((len(img_names) * (1 + flip), TGT_SIZE, TGT_SIZE, 1))\n    imgs[:len(img_names), ..., :1] = np.array([upsample(np.array(load_img(f'{path}/{name}{ftype}', grayscale=True))) / 255\n                      for name in tqdm_notebook(img_names)]).reshape(-1, TGT_SIZE, TGT_SIZE, 1)\n    if flip:\n        print('...extend set with flipped images')\n        imgs[len(img_names):, ..., :1] = np.array([np.fliplr(img) \n                                for img in imgs[:len(img_names), ..., :1]]).reshape(-1, TGT_SIZE, TGT_SIZE, 1)\n    return imgs\n        \n    \ndef csum(img, weight=.5, border=5):\n    \"\"\"Create image cumsum from image\n    Sort of image bleeding downwards\"\"\"\n    center_mean = img[border:-border, border:-border].mean()\n    csum = (np.float32(img)-center_mean).cumsum(axis=0)         \n    csum -= csum[border:-border, border:-border].mean()\n    csum /= max(1e-3, csum[border:-border, border:-border].std())\n    return csum * weight\n\ndef clip_norm(img, weight=1.96):\n    \"\"\"Normalized and clipped image for second image layer\"\"\"\n    img = np.clip(img, -weight*img.std(), weight*img.std())\n    return (img - img.mean()) / img.std()\n\ndef imgs_2_fn(path, img_names, ftype='.png', size=TGT_SIZE, flip=True, weight=1, fn=clip_norm):\n    \"\"\"Load images and transform to array with image and cumsum layer\"\"\"\n    imgs = np.zeros((len(img_names) * (1 + flip), TGT_SIZE, TGT_SIZE, 2))\n    imgs[:len(img_names), ..., :1] = np.array([upsample(np.array(load_img(f'{path}/{name}{ftype}', grayscale=True))) / 255\n                      for name in tqdm_notebook(img_names)]).reshape(-1, TGT_SIZE, TGT_SIZE, 1)\n    if flip:\n        print('...extend set with flipped images')\n        imgs[len(img_names):, ..., :1] = np.array([np.fliplr(img) \n                                for img in imgs[:len(img_names), ..., :1]]).reshape(-1, TGT_SIZE, TGT_SIZE, 1)\n    imgs[..., 1] = [fn(img, weight) for img in tqdm_notebook(imgs[..., 0])]\n    return imgs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89455be399a79910334eb76beafc40bcdab08f83"},"cell_type":"markdown","source":"## Data Exploration<a name='F'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\nLoad and build train and test set.\n\n - contemplating to use depth as a layer, this explains the stacking in `images_d`.\n - checking depth\n - checking coverage\n - checking depth - coverage relationship\n - visualize seismic images with masks"},{"metadata":{"_uuid":"18fa7ffa1d2e0dd678463bbdb551cdc784ddf11e"},"cell_type":"markdown","source":"### Load csv files\n\nThese files contain image filenames(without .png) and seismic depths of the images.  \n - train.csv - filenames of train set  \n - depths.csv - depths of all images test & train set"},{"metadata":{"trusted":true,"_uuid":"dfb59cb69dfc50476771033dc75cb783e4468fb1","collapsed":true},"cell_type":"code","source":"train_df_ = pd.read_csv(f'{path}/train.csv', index_col=\"id\", usecols=[0])\ndepths_df_ = pd.read_csv(f'{path}/depths.csv', index_col=\"id\") # train and test\ntrain_df_ = train_df_.join(depths_df_)\ntest_df = depths_df_[~depths_df_.index.isin(train_df_.index.values)]\n\n# Indices\ntrain_indices = train_df_.index.values\ntest_indices = test_df.index.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f97420fa2494e1b48f69461a984dfd763ec4dd2a","collapsed":true},"cell_type":"code","source":"# Free up some RAM\ndel depths_df_\nprint(f'notebook memory used: {mem_used()}mb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3f2f85edf1bb13033472e7de80f281ab69c23657"},"cell_type":"code","source":"# Flip(augment) train images -> first duplicate train_df: images & depth\ntrain_df = pd.concat([train_df_, train_df_])\ntrain_df.index = np.concatenate([train_indices, train_indices+'_'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed6020879643901c58cb5db00a8fc5aaed9ec0c5","collapsed":true},"cell_type":"code","source":"if DEV:\n    print(train_df.index[:5], train_df.index[4000:4005])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c23688b27a1e890983c6207e8b4057e2cbe039d"},"cell_type":"markdown","source":"### Load image and mask vectors"},{"metadata":{"trusted":true,"_uuid":"f7b88e4a2dfac32a8e8785775bc21ff8cf32983f","scrolled":true,"collapsed":true},"cell_type":"code","source":"print('Loading train set images...')\n# Use without second layer\nX_imgs = imgs_2_array(imgs_train, train_indices, '.png', TGT_SIZE)\n\n# Use with second layer\n# TODO: Train & test results are less with this second layer\n# X_imgs = imgs_2_fn(imgs_train, train_indices, '.png', TGT_SIZE, weight=0)\n\nprint('Loading train set masks...')\nX_masks = imgs_2_array(masks_train, train_indices, '.png', TGT_SIZE)\n\nprint('Computing salt mask coverage...')\nX_coverages = np.array([np.sum(mask) / (mask.shape[0]*mask.shape[1]) for mask in X_masks])\nX_cov_class = (X_coverages - .01) * 100//10 + 1\n\n# Normalize depth\nprint('Computing normalized seismic dept...')\ndepth = train_df[\"z\"]\nmean_depth, std_depth, max_depth = depth.mean(), depth.std(), depth.max()\nX_norm_depth = (depth - mean_depth) / std_depth\n\nprint(f'Loading ready.\\nnotebook memory used: {mem_used()}mb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2b371081f248c805819bfd3d0fd64f0835fe9a6","collapsed":true},"cell_type":"code","source":"# Sanity check classes are correct\nif DEV:\n    X_coverages[:10], X_cov_class[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bfe2cd180955081a3b890062488d8fae3dfff09","collapsed":true},"cell_type":"code","source":"# Sanity check flip image have same depth\nif DEV:\n    X_norm_depth[:5].values == X_norm_depth[4000:4005].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0c6b51167d6046e0af27784e3576985d28d9213"},"cell_type":"markdown","source":"### Depth\n\n - checking the distribution\n \n*As per below: depth is 'normal' distributed and train and test set have same distribution.*"},{"metadata":{"trusted":true,"_uuid":"63688c8b8fdfffbfadac83c00c28d8b22e19c2ca","collapsed":true},"cell_type":"code","source":"if DEV:\n    _ = sns.distplot(train_df.z, label=\"Train\")\n    _ = sns.distplot(test_df.z, label=\"Test\")\n    _ = plt.legend()\n    _ = plt.title(\"Depth distribution\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4436a692fb089faa9bdaf88297daf30d7f3d97ba"},"cell_type":"markdown","source":"### Mask coverage\n\n*As per below: there are 8-10 times more seismic images with 0-10% salt areas. Stratification in train-validation split must prevent overfitting.*"},{"metadata":{"trusted":true,"_uuid":"be6ffd3f87d68d6137f66754dd2ade63c67bee96","collapsed":true},"cell_type":"code","source":"# Helper functions for printing masks\ndef coverage(mask):\n    \"\"\"Compute salt mask coverage\"\"\"\n    return np.sum(mask) / (mask.shape[0]*mask.shape[1])\n\n\ndef norm_coverage(masks):\n    \"\"\"Compute salt mask coverage\"\"\"\n    coverages = np.array([coverage(mask) for mask in masks])\n    mean_cov, std_cov, max_cov = coverages.mean(), coverages.std(), coverages.max()\n    return (coverages - mean_cov) / std_cov\n\n\ndef coverage_class(mask):\n    \"\"\"Compute salt mask coverage class\"\"\"\n    if coverage(mask) == 0:\n        return 0\n    return (coverage(mask) * 100 //10).astype(np.int8) +1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31d32c79955a45eb347685c2fc4739f25a3de8e2","collapsed":true},"cell_type":"code","source":"if DEV:\n    _ = sns.distplot(X_cov_class, label=\"Train\", kde=False)\n    _ = plt.legend()\n    _ = plt.title(\"Coverage distribution\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a786c5258bdf9d39411f42440c60da151435ad88"},"cell_type":"markdown","source":"### Depth vs. coverage\n\n - checking the correlation between depth and coverage\n \n Stretching and offsetting the normalized salt coverage for visual comparison reason only\n \n*As per below: no pattern or correlation visible, depth and coverage are unrelated. Depth might still be a factor in the prediction, e.g. the structure/grain might relate to depth.*"},{"metadata":{"trusted":true,"_uuid":"cc7a482702bdb4fd8ed71b88d8d410ee9eb33ebc","collapsed":true},"cell_type":"code","source":"if DEV:\n    salt_cover_norm = (X_coverages - np.mean(X_coverages)) / np.std(X_coverages)\n\n    plt.figure(figsize=(20,10))\n    plt.scatter(range(len(X_norm_depth)), X_norm_depth, alpha=.5, label='Normalized Seismic Depth')\n    plt.scatter(range(len(salt_cover_norm)), salt_cover_norm, color='r', alpha=.5, label='Normalized Salt Coverage')\n    plt.title('Normalized Depth vs. Salt coverage as % of image size', fontsize=20)\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, fontsize=16);\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2901593940a82b243f3b12e76e14dd73f37d943b"},"cell_type":"markdown","source":"## Visualize images and masks (overlayed)<a name='G'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\nHelper function for using throughout notebook"},{"metadata":{"trusted":true,"_uuid":"8d12014d3992c3c77ca078a7e7f03a92f019e595","collapsed":true},"cell_type":"code","source":"def plot_imgs_masks(imgs, masks, **kwargs):\n    \"\"\"Visualize seismic images with their salt area mask(green) and optionally salt area prediction(pink). \n    The prediction mask can be either in probability-mask or binary-mask form(based on threshold)\n    \"\"\"\n    depth = kwargs.get('depth', None)\n    preds_valid = kwargs.get('preds_valid', None)\n    thres = kwargs.get('thres', None)\n    grid_width = kwargs.get('grid_width', 10)\n    zoom = kwargs.get('zoom', 1.5)\n    \n    grid_height = 1 + (len(imgs)-1) // grid_width\n    fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*zoom, grid_height*zoom))\n    axes = axs.ravel()\n    \n    for i, (img, mask) in enumerate(zip(imgs, masks)):\n        \n        ax = axes[i] #//grid_width, i%grid_width]\n        _ = ax.imshow(img[..., 0], cmap=\"Greys\")\n#         _ = ax.imshow(img[..., 1], alpha=0.15, cmap=\"seismic\") # TODO\n        _ = ax.imshow(mask[..., 0], alpha=0.3, cmap=\"Greens\")\n        \n        if preds_valid is not None:\n            pred = preds_valid[i]\n            pred = pred[..., 0]\n            if thres is not None:\n                pred = np.array(np.round(pred > thres), dtype=np.float32)\n                iou = f'IoU: {_iou(mask, pred).round(3)}'\n                _ = ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n                _ = ax.text(2, img.shape[0]-2, iou, color=\"k\")\n            else:\n                _ = ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n            \n        if depth is not None:\n            _ = ax.text(2, img.shape[0]-2, f'depth: {depth[i]}', color=\"k\")\n            \n        _ = ax.text(2, 2, f'{coverage(mask).round(3)}({coverage_class(mask)})', color=\"k\", ha=\"left\", va=\"top\")\n        _ = ax.set_yticklabels([])\n        _ = ax.set_xticklabels([])\n        _ = plt.axis('off')\n    plt.suptitle(\"Green: Salt area mask \\nTop-left: coverage class, top-right: salt coverage, bottom-left: depth\", y=1+.5/grid_height, fontsize=20)\n    plt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59a8b6c3a76f66bd5d230548b8c1148bd44e407b","scrolled":false,"collapsed":true},"cell_type":"code","source":"if DEV:\n    N = 30\n    plot_imgs_masks(X_imgs[:N], X_masks[:N], depth=train_df.iloc[:N].z)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"915611b2c849a26600997fea1f3df8f7c2e2e441"},"cell_type":"markdown","source":"## Train validation split<a name='H'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\nValidation set will be used for saving model checkpoints and early stopping. \nTradeoff:\n - larger validation set for more validation accuracy and saving better generalizing model, which means less training samples and thus less generalizing model.\n\nUsing images with depth layer."},{"metadata":{"trusted":true,"_uuid":"64226bdf35be4968357ed4d16c447306d6defe0f","collapsed":true},"cell_type":"code","source":"VAL_SIZE = 0.20\n\nprint(f'notebook memory used before split: {mem_used()}mb')\n\nX_train, X_valid, Y_train, Y_valid, depth_train, depth_valid = train_test_split(\n    X_imgs,\n    X_masks,\n    np.array(X_norm_depth).reshape(-1, 1),\n    test_size=VAL_SIZE, \n    stratify=X_cov_class, \n    random_state=1)\n\ngc.collect()\ndel X_imgs, X_masks, X_cov_class\ngc.collect()\nprint(f'notebook memory used after split: {mem_used()}mb')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fe1fc2a254b29de945f08baa7f48ceef7089f5b"},"cell_type":"markdown","source":"#### Sanity check "},{"metadata":{"trusted":true,"_uuid":"1b39484034391c24fdb58e8f5eedb42dd5444d95","scrolled":true,"collapsed":true},"cell_type":"code","source":"if DEV:\n    print(X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape, depth_train.shape, depth_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06e052b5433560d5e98d91b65fa253feca6e4f4b","collapsed":true},"cell_type":"code","source":"if DEV:\n    N = 20\n    plot_imgs_masks(X_valid[:N], Y_valid[:N])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d66a11a8d8d48e16640307185062f5494c1f5b6"},"cell_type":"markdown","source":"## Custom metrics<a name='I'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\nCustom metrics can be passed at the compilation step. The function would need to take (y_true, y_pred) as arguments and return a single tensor value. A metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model.\n\n"},{"metadata":{"_uuid":"b4716a2112dfb71c75e60bff90cb17836f78bf66","collapsed":true,"trusted":true},"cell_type":"code","source":"def mean_iou(Y_true, Y_pred, score_thres=0.5):\n    \"\"\"Compute mean(IoU) metric\n    IoU = intersection / union\n    \n    For each (mask)threshold in provided range:\n     - convert probability mask to boolean mask based on given threshold\n     - score the mask 1 if(IoU > score_threshold(0.5))\n    Take the mean of the scoress\n\n    https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou\n    \"\"\"\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        Y_pred_bool = tf.to_int32(Y_pred > t) # boolean mask by threshold\n        score, update_op = tf.metrics.mean_iou(Y_true, Y_pred_bool, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([update_op]):\n            score = tf.identity(score) #!! use identity to transform score to tensor\n        prec.append(score) \n        \n    return K.mean(K.stack(prec), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e7c423ccf5145d6ac991dad85262540735e4dfe"},"cell_type":"markdown","source":"## Model architecture<a name='J'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\nBelow functions create the UNet model in a functional and recursive way."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f76510b3f7bc6b0e06a8648b6c8f5e8214a2051f"},"cell_type":"code","source":"def conv_block(m, ch_dim, acti, bn, res, do=0):\n    \"\"\"CNN block\"\"\"\n    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(m)\n    n = BatchNormalization()(n) if bn else n\n    n = Dropout(do)(n) if do else n\n    n = Conv2D(ch_dim, 3, activation=acti, padding='same')(n)\n    n = BatchNormalization()(n) if bn else n\n    return Concatenate()([m, n]) if res else n\n\ndef input_feature(f, n, n_features=1):\n    \"\"\"Input block\"\"\"\n    features = 1\n    xx = K.int_shape(n)[1]\n    f_repeat = RepeatVector(xx*xx)(f)\n    f_conv = Reshape((xx, xx, n_features))(f_repeat)\n    n = Concatenate(axis=-1, name=f'feat_{2}')([n, f_conv])\n    n = BatchNormalization()(n)            \n    return n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6ed9cc06a8d8e056c84b43cd7b2aeb92edf59e0d"},"cell_type":"code","source":"def level_block(m, ch_dim, depth, inc_rate, acti, do, bn, mp, up, res, inp_feat):\n    \"\"\"Recursive CNN builder\"\"\"\n    if depth > 0:\n        n = conv_block(m, ch_dim, acti, bn, res) # no drop-out\n        m = MaxPooling2D()(n) if mp else Conv2D(ch_dim, 3, strides=2, padding='same')(n)\n        if (inp_feat is not None) and (depth==2):\n            m = Concatenate()([m, input_feature(inp_feat, m)])\n        m = level_block(m, int(inc_rate*ch_dim), depth-1, inc_rate, acti, do, bn, mp, up, res, inp_feat)\n        \n        # Unwind recursive stack calls - creating the upscaling part of the model\n        if up:\n            # Repeat the rows and columns of the data by 2 and 2 respectively\n            m = UpSampling2D()(m)\n            m = Conv2D(ch_dim, 2, activation=acti, padding='same')(m)\n        else:\n            # Transposed convolutions are going in the opposite direction of a normal convolution\n            m = Conv2DTranspose(ch_dim, 3, strides=2, activation=acti, padding='same')(m)\n        n = Concatenate()([n, m])\n        m = conv_block(n, ch_dim, acti, bn, res)\n    else:\n        # Depth == 0 - deepest conv_block\n        m = conv_block(m, ch_dim, acti, bn, res, do)\n    return m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"42259555ee8e6aff5d5b65426ea9cb6b85db4cc2"},"cell_type":"code","source":"def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n        dropout=0.5, batchnorm=False, maxpool=True, upconv=False, residual=False):\n    \"\"\"Returns model\"\"\"\n    inputs = Input(shape=img_shape, name='img')\n    inp_feat = Input(shape=(1,), name='feat') # or None\n    outputs = level_block(inputs, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual, inp_feat)\n    outputs = Conv2D(out_ch, 1, activation='sigmoid')(outputs)\n    return Model(inputs=[inputs, inp_feat], outputs=outputs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"729aa2fed66650f410df4f075e3467a2fd90c3f2"},"cell_type":"markdown","source":"\n> ## Build model<a name='K'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>"},{"metadata":{"trusted":true,"_uuid":"fedf33b412f579386f585e57c9e05b18ff011563","scrolled":true,"collapsed":true},"cell_type":"code","source":"IMG_CH = 1     # layers of image\nCONV_CH = 16   # number of channels to start/end UNet with\nDEPTH = 5      # number of CONV blocks to max model depth\nD_OUT = 0.1\nBN = True\nUP_CONV = False\nRES = True\n\nmodel = UNet((TGT_SIZE, TGT_SIZE, IMG_CH), \n             start_ch=CONV_CH, \n             depth=DEPTH, \n             dropout=D_OUT,\n             batchnorm=BN, \n             upconv=UP_CONV,\n             residual=RES)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85fae44caafe01269e7e9d389513708f8ba0784c"},"cell_type":"markdown","source":"### Compile and visualize model"},{"metadata":{"trusted":true,"_uuid":"df1fac2f7fd5169a9ac3727e5c2f8fc49fb57589","collapsed":true},"cell_type":"code","source":"LR = 10e-3\n# Define optimizer\n# Clip gradients to norm 1., \noptimizer = [Adam(lr=LR, beta_1=0.9, beta_2=0.9999, decay=LR/100, clipvalue=.5),\n             SGD(lr=LR, decay=LR/100, momentum=0.9, nesterov=True, clipnorm=1.)]\n\n# Define loss\nloss = [\"binary_crossentropy\", \"kullback_leibler_divergence\"]\n\n# Compile model\nmodel.compile(loss=loss[0], optimizer=optimizer[0], metrics=[\"accuracy\", mean_iou])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e3ee4474eba6c62206b30320c8b72f05c89c8ae","scrolled":false,"collapsed":true},"cell_type":"code","source":"model_name = f'TGS_salt_UNet_{IMG_CH}_{CONV_CH}_{DEPTH}_{D_OUT>0}_{BN}_{UP_CONV}_{RES}.h5'\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af54d71b8bc838b6d91941a0de5b7b09a242dc21"},"cell_type":"markdown","source":"## Train model<a name='L'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>"},{"metadata":{"trusted":true,"_uuid":"fe656492f6854fb81220cca1a62b6d97ff48ebee","collapsed":true},"cell_type":"code","source":"BATCH_SIZE = 32 # larger will have more stable learning, but needs more GPU\nEPOCHS = 50\nif DEV:\n    EPOCHS = int(input('Number of training epochs?:'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"362f009a29b4001b29f421e8c68f3d8fa7977a39","scrolled":false,"collapsed":true},"cell_type":"code","source":"callbacks = [\n    EarlyStopping(patience=11, verbose=1),\n    ReduceLROnPlateau(patience=3, min_lr=0.00001, verbose=1),\n    ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True, verbose=1)]\n\nX_train_dict = {'img': X_train, \n              'feat': depth_train}\n\nX_val_dict = {'img': X_valid, \n              'feat': depth_valid}\n\nhistory = model.fit(X_train_dict, \n                    Y_train, \n                    validation_data=(X_val_dict, Y_valid),\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abdd6c6f509a14df9b75d22ad6270a715d0769b9","scrolled":true,"collapsed":true},"cell_type":"code","source":"if DEV:\n    print(history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a41b8ac40a8a655c43a5f26dac5da7f155fdb87"},"cell_type":"markdown","source":"> ## Learning curves<a name='M'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\n1. Check the curves for errors  \n2. Check the curves for roughness and convergence to tune the hyperparameters:  \n - learning rate and decay\n - batch size\n - model architecture\n - epochs and earlystopping"},{"metadata":{"trusted":true,"_uuid":"306a108946269dbe7b331b770835019ee8d8f06f","collapsed":true},"cell_type":"code","source":"if DEV:\n    fig, (ax_loss, ax_acc, ax_iou) = plt.subplots(1, 3, figsize=(15,5))\n\n    _ = ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    _ = ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    _ = ax_loss.legend()\n    _ = ax_loss.set_title('Loss')\n    \n    _ = ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n    _ = ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\n    _ = ax_acc.legend()\n    _ = ax_acc.set_title('Accuracy')\n    \n    _ = ax_iou.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train IoU\")\n    _ = ax_iou.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Validation IoU\")\n    _ = ax_iou.legend()\n    _ = ax_iou.set_title('IoU')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ab8516fb8ab135872dd4f4b895b5d76206df1fa"},"cell_type":"markdown","source":"## Check performance on validation set <a name='N'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>"},{"metadata":{"_uuid":"2316034edcb7227673fd9b69264ca9c0d0e87f14","scrolled":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Load best model\nmodel = load_model(model_name, custom_objects={'mean_iou': mean_iou})\nprint('model loading done.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9096bc0ac6ea0713b80ce76825914792f692bdb6","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Evaluate best model on validation set\nif DEV:\n    model.evaluate(X_val_dict, Y_valid, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19ea7a03ea9e2711cc16cda20a46558e9f378cf7","scrolled":false,"collapsed":true},"cell_type":"code","source":"# Predict on validation set\npreds_valid = model.predict(X_val_dict, verbose=1).reshape(-1, TGT_SIZE, TGT_SIZE)\npreds_valid = preds_valid.reshape(-1, TGT_SIZE, TGT_SIZE, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac1ae5e286f50694f7d328329d6e60c42d9dc316","collapsed":true},"cell_type":"code","source":"if DEV:\n    print(preds_valid.shape, Y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12fb0e51f826bacffa43e1c5cd643b32238b2cfa"},"cell_type":"markdown","source":"### Visualize probability('float') masks\n\n- Green are false positives (FP)\n- Pink are false negatives (FN)\n- Brown are true positives (TP)\n- Grey are true negatives (TN)"},{"metadata":{"trusted":true,"_uuid":"75c2b3acb89324ae97bd5985e4ba93b46c53bf03","collapsed":true},"cell_type":"code","source":"if DEV:\n    N = 40\n    plot_imgs_masks(X_valid[:N], Y_valid[:N], preds_valid=preds_valid[:N])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5509ee0942a00bdc89c2f45fbbf0d943e7c9d4ef"},"cell_type":"markdown","source":"## Scoring<a name='O'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>\n\nScore the model and do a threshold optimization by the best IoU"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c7aca6f373f75e59c37adf4c7f1aea238cb6a48d"},"cell_type":"code","source":"def _iou(Y_true, Y_pred):\n    \"\"\"IoU\"\"\"\n    Y_true_f, Y_pred_f = Y_true.ravel(), Y_pred.ravel()\n    intersection = np.sum(Y_true_f * Y_pred_f)\n    union = np.sum((Y_true_f + Y_pred_f) > 0)\n    return intersection/ max(1e-9, union)\n\ndef miou(Y_trues, Y_preds):\n    \"\"\"Mean intersection over union\"\"\"\n    return np.mean([_iou(Y_trues[i], Y_preds[i]) for i in range(Y_trues.shape[0])])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac7c2d9b74ef49e1ea0f520fc1841a3205182d85"},"cell_type":"markdown","source":"### Best threshold for best IoU score (submission)\n\nChecking which threshold delivers the best IoU score, so this threshold will be used for Test set prediction."},{"metadata":{"trusted":true,"_uuid":"25f3792c80e5684be9804bf3cb22999600e5f834","collapsed":true},"cell_type":"code","source":"thresholds = np.linspace(0.1, 0.9, 80)\nious = np.array([miou(Y_valid, np.int32(preds_valid > threshold)) \n                 for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9bfeb44dfc6e9c0389306c55bd4553e783e928f6"},"cell_type":"code","source":"threshold_best_index = np.argmax(ious)\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5eef925cd24df9f79cb529affb61d3755724136d","scrolled":true,"collapsed":true},"cell_type":"code","source":"if DEV:\n    _ = plt.plot(thresholds, ious)\n    _ = plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n    _ = plt.xlabel(\"Threshold\")\n    _ = plt.ylabel(\"IoU\")\n    _ = plt.title(\"Threshold: {} delivers best mean-IoU: {} \".format(threshold_best.round(3), iou_best.round(3)))\n    _ = plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2541f22c632819f1d92500102fb048cbf391758"},"cell_type":"markdown","source":"### Visualize binary masks based on best threshold\n\n- Green are false positives (FP)\n- Pink are false negatives (FN)\n- Brown are true positives (TP)\n- Grey are true negatives (TN)\n"},{"metadata":{"trusted":true,"_uuid":"f8f6f8cc498664acae8a66fb55fb3a5596088209","scrolled":true,"collapsed":true},"cell_type":"code","source":"if DEV:\n    N = 60\n    plot_imgs_masks(X_valid[:N], Y_valid[:N], preds_valid=preds_valid[:N], thres=threshold_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5a06dcd4bc07e93e9b9a4e6e19b4ce471e9aa9d","collapsed":true},"cell_type":"code","source":"# Free up memory: need ~7gb for test set\ngc.collect()\nprint(f'notebook memory used: {mem_used()}mb')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17bcea2636f72e75b0a8db5927f09ab7721ec770"},"cell_type":"markdown","source":"## Predict test set<a name='P'></a>\n<div align=\"right\">[>>TOC](#TOC)</div>"},{"metadata":{"_uuid":"68172d012b57e5464cf3a58583a152944c5f435e"},"cell_type":"markdown","source":"### Sanity check if all indices and images match"},{"metadata":{"trusted":true,"_uuid":"7392c68c1c9174b494c178c4f87bb228eed5a714","collapsed":true},"cell_type":"code","source":"if DEV:\n    test_ids = next(os.walk(path_test+\"/images\"))[2]\n    assert len(set(test_ids) ^ set(test_df.index+'.png')) == 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec7445c121de29f95c88e2dee7dff3edfaff4b67"},"cell_type":"markdown","source":"> ### Convert test set images and depths to layers\n\n - Upsample images to arrays\n - Reshape for modeling\n - Create depth layer"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ef92625313ce68b23b44d0fb06ca5fa3c05a9fa4","collapsed":true},"cell_type":"code","source":"# Load test set (keep >5gb free RAM!)\nX_test = imgs_2_array(imgs_test, test_indices, '.png', TGT_SIZE, flip=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dd7aacdf25851d2c5ae5ed631c5bc00b2e0b65f","collapsed":true},"cell_type":"code","source":"gc.collect()\nprint(f'notebook memory used: {mem_used()}mb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a99314ba2175f795dcf1cbccad8c60d281669556","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Normalize depth - !use train mean and std\ndepth_test = (test_df[\"z\"] - mean_depth) / std_depth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d51fdb0694e0a82b7bbcda773d0e911982ac32e","collapsed":true},"cell_type":"code","source":"if DEV:\n    print(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfa4e2621a1f6b823cb7bba28213323b803233bd","collapsed":true},"cell_type":"code","source":"X_test_dict = {'img': X_test, \n              'feat': depth_test} \n\npreds_test = model.predict(X_test_dict) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7da2f8a9993dd5fdc6ba72eda8c34793f118b198"},"cell_type":"markdown","source":"## Submission  <a name='Q'></a> \n<div align=\"right\">[>>TOC](#TOC)</div>\nSubmission is in csv form:\n - `id`: index (equals filename)\n - `rle_mask`: run-length format (down-then-right): `masked_pixel_start` `<space>` `length_of_masked_pixels` ..."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c5f2c21b9160d43832a1916a05a6b97c72b5683"},"cell_type":"code","source":"def RLenc(img, order='F'):\n    \"\"\"Convert binary mask image to run-length array or string.\n    \n    Args:\n    img: image in shape [n, m]\n    order: is down-then-right, i.e. Fortran(F)\n    string: return in string or array\n\n    Return:\n    run-length as a string: <start[1s] length[1s] ... ...>\n    \"\"\"\n    bytez = img.reshape(img.shape[0] * img.shape[1], order=order)\n    bytez = np.concatenate([[0], bytez, [0]])\n    runs = np.where(bytez[1:] != bytez[:-1])[0] + 1 # pos start at 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n# Use for sanity check the encode function\ndef RLdec(rl_string, shape=(101, 101), order='F'):\n    \"\"\"Convert run-length string to binary mask image.\n    \n    Args:\n    rl_string: \n    shape: target shape of array\n    order: decode order is down-then-right, i.e. Fortran(F)\n\n    Return:\n    binary mask image as array\n    \"\"\"\n    s = rl_string.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape, order=order)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61ed2a4ec975b3ba3a3f17cd3ae5a91176f23e77","collapsed":true},"cell_type":"code","source":"pred_dict = {idx: RLenc(np.round(upsample(preds_test[i], IMG_SIZE) > threshold_best)) \n             for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51bf4c9d650248e813e3e0bc6e64c18209027be0","collapsed":true},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict, orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')\nsub.head()\nprint('submission saved!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc9c854d36491afa6de5d98d8e6826055fa18f98"},"cell_type":"markdown","source":"### Thanks for upvoting and sharing your thoughts!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}