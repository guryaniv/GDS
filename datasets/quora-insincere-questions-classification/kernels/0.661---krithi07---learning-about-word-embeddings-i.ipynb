{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time \nimport math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNGRU, Bidirectional, Embedding, Activation, Dropout\nfrom keras.layers import GlobalMaxPool1D, Conv1D\nfrom keras import regularizers, initializers, optimizers, layers, constraints\nfrom keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape -\", train_df.shape)\nprint(\"Test shape -\", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcefabe92de40cbf683a1e22a66d5117b5e2e793"},"cell_type":"code","source":"train_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9a71a37d87f8119c163e2b3251659bdbbbb7305"},"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size = 0.1, random_state = 1993)\n\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b5bdd8915b033ce828f03476575a1cd531b0411"},"cell_type":"code","source":"print(train_X.shape)\nprint(val_X.shape)\nprint(train_X[1:3])\n\nprint(\"Inapt Questions are\", train_df[\"target\"].sum())\nprint(\"Total Questions are\", train_df[\"target\"].count())\nprint(\"Fraction of Inapt Questions\", round(train_df[\"target\"].sum()/train_df[\"target\"].count(),3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbdc7ef4c48994452b7cf68eaae47b5ade16dd4e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bf16222f7af96dfdeb20f78131236ee2bef2bc4"},"cell_type":"code","source":"vocab_size = 50000 #Number of words used to create embedding\nmax_features = 300 #Number of features in a single embedding vector\nmax_length = 100 #Max length of each question to consider \n\nt = Tokenizer(num_words=vocab_size) #Initialize a Tokenizer\nt.fit_on_texts(list(train_X)) #Learn the Tokens #Converting Train from an Arrary to a List\n\n#print(t.word_counts) \nprint(t.document_count)\n#print(t.word_index)\n#print(t.word_docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a01cff052ee8f54a521a767cb7f45ff8904d0bd"},"cell_type":"code","source":"train_X = t.texts_to_sequences(train_X) #Using trained tokenizer to encode train, test and val.\nval_X = t.texts_to_sequences(val_X)\ntest_X = t.texts_to_sequences(test_X)\n\nprint(train_X[1:3]) #The words are now replaced by the corresponding encodings from the tokenizer\nprint(test_X[1:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4341a9d7fe2d30d3fa1cb7e736781ef914c86ef9"},"cell_type":"code","source":"# For each record, this creates a vector of varying size. Let's make every record of same size, 100\n# Sequence Padding\n\ntrain_X = pad_sequences(train_X, maxlen=max_length)\nval_X = pad_sequences(val_X, maxlen=max_length)\ntest_X = pad_sequences(test_X, maxlen=max_length)\n\n# Now we have the input table created, let's create the output table\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values\n\nprint(train_X[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aa6c8e01be93ce957c698c935c5c47d8b444b99"},"cell_type":"code","source":"!ls ../input/embeddings/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d99e108f62fe870fe417f547439a87bfc168cdc"},"cell_type":"code","source":"# Code to get the embeddings from \nEMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = t.word_index\nnb_words = min(vocab_size, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, max_features))\nfor word, i in word_index.items():\n    if i >= vocab_size: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12ae94383359e371c5d5d655cf9f7dd9c2b77bea"},"cell_type":"code","source":"inp = Input(shape=(max_length,))\nx = Embedding(vocab_size,max_features, weights=[embedding_matrix], trainable=False)(inp) \nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = Bidirectional(CuDNNGRU(32, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(32,activation='relu')(x)\nx = Dropout(0.1)(x)\nx = Dense(16,activation='relu')(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation='sigmoid')(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d45c7b4a3085a14deea5f412245ae6a75f5fd265"},"cell_type":"code","source":"## Train the model \nmodel.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c2b53b7e90407547d5e23c050c86b87896221ac"},"cell_type":"code","source":"Glove_Result = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh,2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh,metrics.f1_score\n                                                    (val_y, (Glove_Result>thresh).astype(int))))\n\n# best Threshold coming out to be 0.43","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"084a4185bc1144195ca6bd5f683a99204dedc8db"},"cell_type":"code","source":"pred_glove_test_y = model.predict([test_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2c0d171525aeab7f10dca79041067f7dca8a20e"},"cell_type":"code","source":"pred_test_y = (pred_glove_test_y>0.38).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65ea68178810f0a1bd6714ded0e6b114aac6c1c9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}