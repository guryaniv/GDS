{"cells":[{"metadata":{"_uuid":"4f56858a836e7fe80c0022cc4c8fedbe151e0ef9"},"cell_type":"markdown","source":"# Credit Card Fraud Detection - Forward Chaining\n\nThe Credit Card Fraud Dataset is a time-series data set. Accordingly, methods like K-Fold Cross Validation cannot be used because they end up destroying the temporal nature of the data set. Instead, the correct approach is to use what is known as [Forward Chaining](https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection) in order to test the performance of the model while maintaining the temporal structure.\n\n### Agenda:\n1. Data Preprocessing\n2. Feature Engineering\n3. Model Fitting\n4. Model Evaluation via Forward Chaining"},{"metadata":{"_uuid":"581018009de246b56beed8dc63746742679c9d67","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"617f204fe36714eb0fd4e20b517646f35403783f","trusted":false},"cell_type":"markdown","source":"## Load Data Set"},{"metadata":{"trusted":true,"_uuid":"74a8e13c68d6cf284207cc9f64b5406bfed23b6a"},"cell_type":"code","source":"data = pd.read_csv('../input/creditcard.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba4eba927941993dd86f486f651626a685c7f535"},"cell_type":"markdown","source":"## Check various columns"},{"metadata":{"trusted":true,"_uuid":"a57c3cdcbd436019798f144222b2609bff4d7bcf"},"cell_type":"code","source":"# Check Time Column\ndata[['Time']].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b671325d5a40612ede2fd774c5b5a1670324e93"},"cell_type":"markdown","source":"Time column seems clean. Minimum value is 0 and maximum value is 172792"},{"metadata":{"trusted":true,"_uuid":"332efee867c7cab06d9c17049aa1d1c2e127121a"},"cell_type":"code","source":"# How are the class labels spread\npd.value_counts(data['Class'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee33acc378d5050d051c537e42e2335b6f03e490"},"cell_type":"markdown","source":"## SMOTE + Undersampling\nAs we knew already, the classes are highly imbalanced!\n\nWe will need to resolve this. There are various approaches, but here I will be using SMOTE + Undersampling of majority class.\n\nSMOTE stands for Synthetic Minority Oversampling Technique. SMOTE seeks to create \"synthetic minority data points\" (that is, it creates artificial data points belonging to the minority class) based on randomized interpolation between a minority data point and 'n' randomly chosen other nearby minority data points.\n\n[Technical Paper on SMOTE](https://arxiv.org/abs/1106.1813)\n\nHere is a helpful visual explanation of how SMOTE works:\n\n![Helpful pictorial representation of how SMOTE works](https://www.researchgate.net/publication/282830682/figure/fig1/AS:324534361182217@1454386429561/Schematic-diagram-of-SMOTE-algorithm.png)\n\n## Feature Engineering\nSince the data provided is time series data, we can create lagged features. to give our classification model reference values into previous times. The important idea is to do this **before** doing the SMOTE + Undersampling, as the sampling will mess up the time series structure.\n\nAnother idea is to not rely on just 1 Time feature and instead break up the Time feature (currently in seconds) into 3 new features: Hours, Minutes, Seconds"},{"metadata":{"trusted":true,"_uuid":"9ae950ea3a0fa3d7972d8245c0b2674235e70623"},"cell_type":"code","source":"# Convert Time-rows into columns (window method) T, T-1, T-2 (3 at a time)\ndef lagged_features(df_long, lag_features, window=2, lag_prefix='lag', lag_prefix_sep='_'):\n    \"\"\"\n    Function calculates lagged features (only for columns mentioned in lag_features)\n    based on time_feature column. The highest value of time_feature is retained as a row\n    and the lower values of time_feature are added as lagged_features\n    :param df_long: Data frame (longitudinal) to create lagged features on\n    :param lag_features: A list of columns to be lagged\n    :param window: How many lags to perform (0 means no lagged feature will be produced)\n    :param lag_prefix: Prefix to name lagged columns.\n    :param lag_prefix_sep: Separator to use while naming lagged columns\n    :return: Data Frame with lagged features appended as columns\n    \"\"\"\n    if not isinstance(lag_features, list):\n        # So that while sub-setting DataFrame, we don't get a Series\n        lag_features = [lag_features]\n\n    if window <= 0:\n        return df_long\n\n    df_working = df_long[lag_features].copy()\n    df_result = df_long.copy()\n    for i in range(1, window+1):\n        df_temp = df_working.shift(i)\n        df_temp.columns = [lag_prefix + lag_prefix_sep + str(i) + lag_prefix_sep + x\n                           for x in df_temp.columns]\n        df_result = pd.concat([df_result.reset_index(drop=True),\n                               df_temp.reset_index(drop=True)],\n                               axis=1)\n\n    return df_result\n\n\n# Augment data set with lagged features\nlag_features = [col for col in data.columns if col not in ['Time', 'Amount', 'Class']]\ndata = lagged_features(data, lag_features)\n\nprint(data.shape)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"455b9bde83bbefd0df47ff669e273933db85de25"},"cell_type":"code","source":"# Remove missing values that are introduced due to lagging\ndata = data.dropna()\n\n# Restructure data set to keep 'Class' at end\ncol_order = [col for col in data.columns if col != 'Class'] + ['Class']\ndata = data[col_order]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2f8d184a672051d41ccc803c2d7cf5efe7129fe"},"cell_type":"code","source":"# SMOTE + Undersampling\n# Split data into X and y\nX, y = data[[col for col in data.columns if col != 'Class']], data[['Class']].astype(str)\nprint(X.head())\nprint(y.head())\n\n# Undersampling of majority class\nrus = RandomUnderSampler(sampling_strategy={'0': 8124, '1': 492}, \n                         replacement=True, \n                         random_state=123)\nX_under, y_under = rus.fit_resample(X, y)\nprint(X_under.shape)\nprint(y_under.shape)\n\ndata_under = pd.DataFrame(np.hstack((X_under, y_under)))\ndata_under.columns = data.columns\nprint(data_under.shape)\nprint(data_under.head())\n\n# Clean up temp variables\ndel X, y, X_under, y_under\n\nX, y = data_under[[col for col in data_under.columns if col != 'Class']], data_under[['Class']]\nprint(y.head())\n\n# SMOTE sampling of minority class\nsmote = SMOTE(sampling_strategy={'0': 8124, '1': 3444},\n              k_neighbors=7,\n              random_state=213)\nX_smote, y_smote = smote.fit_resample(X, y)\nprint(X_smote.shape)\nprint(y_smote.shape)  # For some odd reason, the shape of y_smote is (nrow, ) instead of (nrow, 1). So we need to fix that.\ny_smote = np.reshape(y_smote, (y_smote.shape[0], 1))\n\ndata_smote = pd.DataFrame(np.hstack((X_smote, y_smote)))\ndata_smote.columns = data.columns\nprint(data_smote.shape)\nprint(data_under.head())\n\n# Look at class counts now\nprint(pd.value_counts(data_smote['Class']))\nprint(data_smote.columns)\n\ndata_sorted = data_smote.sort_values(['Time'], axis=0, ascending=[1]).reset_index(drop=True)\n\nprint('Final dataset glimpse...')\nprint(data_sorted.head())\n\n# Clean up variables and objects not needed\ndel rus, smote, data_under, X, y, X_smote, y_smote, data_smote","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be1cd29348e3492fa93ccce4b459ef8568234359"},"cell_type":"code","source":"# Augment data set with hours, minutes, seconds break up of 'Time'\ndata_time = data_sorted.Time.apply(lambda x: pd.Series({'Hours': np.floor(x/(60*60)), \n                                                        'Minutes': np.floor((x - np.floor(x/(60*60)) * (60*60))/60), \n                                                        'Seconds': x - np.floor(x/(60*60)) * (60*60) - np.floor((x - np.floor(x/(60*60)) * (60*60))/60) * 60\n                                                       }))\ndata_aug = pd.concat([data_sorted.reset_index(drop=True), data_time.reset_index(drop=True)], axis=1)\n\n# Reorder columns to keep time columns close together\ncol_order = ['Time', 'Hours', 'Minutes', 'Seconds'] + [col for col in data_aug.columns if col not in ['Time', 'Hours', 'Minutes', 'Seconds']]\ndata_aug = data_aug[col_order]\n\nprint(data_aug.shape)\ndata_aug.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ac729a5951bf5693afa25bdde0c3d894aa238df"},"cell_type":"markdown","source":"## Data Pre-Processing\n\nTo pre-process the data, we will only need to Standardize the values in the Amount column. This is because the other features are already standardized via transformation through PCA.\n"},{"metadata":{"trusted":true,"_uuid":"1c25328b9abf497ef861e7446f39910b85e9851a"},"cell_type":"code","source":"# Standardize Amount column\ndata_aug['Amount'] = StandardScaler().fit_transform(data_sorted['Amount'].values.reshape(-1, 1))\ndata_aug.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4e43a12b6695fc83b04614243abf91a704ea9ea"},"cell_type":"markdown","source":"## Time Series Split + Logistic Regression\n\nAs mentioned earlier, we cannot use Cross Validation with Time Series Data. Instead, we will use Forward Chaining to ensure that our train and test sets are always using information available either in that moment, or from the past (never from the future).\n\nWe will use sklearn's ModelSelection sub-module for this. For each split, we will fit a logistic regression model on the split's train data and then test on the split's test data."},{"metadata":{"trusted":true,"_uuid":"7b8fcc3c792df4b336953461ceadab4c957f6697"},"cell_type":"code","source":"# Define object to handle time series splits\ntscv = TimeSeriesSplit(n_splits=5)\n\n# Loop over time series splits, fit model, and test on test data\ndependent_cols = [col for col in data_aug.columns if col not in ['Class', 'Time']]  # Time information has been captured in Hours, Minutes, Seconds\nindependent_col = ['Class']\nfor train_index, test_index in tscv.split(data_aug):\n    print('------------------------------------')\n    X_train, X_test = data_aug.iloc[train_index][dependent_cols], data_aug.iloc[test_index][dependent_cols]\n    y_train, y_test = data_aug.iloc[train_index][independent_col], data_aug.iloc[test_index][independent_col]\n    \n    # Fit logistic regression model to train data and test on test data\n    lr_mod = LogisticRegression(C = 0.01, penalty='l2')  # The value of C should be determined by nested validation\n    lr_mod.fit(X_train, y_train)\n    \n    y_pred_proba = lr_mod.predict_proba(X_test)\n    y_pred = lr_mod.predict(X_test)\n    \n    # Print Confusion Matrix and ROC AUC score\n    print('Confusion Matrix:')\n    print(confusion_matrix(y_test, y_pred))\n    \n    print('ROC AUC score:')\n    print(roc_auc_score(y_test.Class.astype(int), y_pred_proba[:, 1]))\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"935ac1e9ffb1ed7f280ef7942bc6b7e2227963f3"},"cell_type":"markdown","source":"## Conclusion\n\nThe primary purpose of this notebook was to show case how time series data set should be treated differently than cross-sectional data sets. The main take away for the reader should be with respect to feature engineering (some feature engineering should be done before sampling, some can be done after sampling), as well as how to use forward chaining in order to assess model performance.\n\nNevertheless, I did gloss over some approaches which should not be done in practise. Here are some ways that our approach could be even more robust:\n1. **Nested Cross Validation to choose hyper-parameters (like SMOTE sampling and Undersampling parameters):** In our approach, we took a rough approach to oversample the minority class and undersample the majority class. In practice, we could choose these values by nested cross validation. In nested cross validation, we would create n_outer number of parallel datasets where each dataset was sampled somewhat differently. For instance, with n_outer = 5, we could have 5 parallel approaches where in the first approach we use SMOTE with k_neighbors = 3, in the 2nd approach we have k_neighbors = 5 etc.\n2. **Standardization of columns should be done only for Train Data**: In this notebook, we used StandardScaler at one go, after which the data set is split into X_train and X_test. However, this leaks information about X_train to X_test. The correct approach should be to first split into X_train, X_test, then use StandardScaler on X_train, and then use the mean and sd values so obtained to transform X_test."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}