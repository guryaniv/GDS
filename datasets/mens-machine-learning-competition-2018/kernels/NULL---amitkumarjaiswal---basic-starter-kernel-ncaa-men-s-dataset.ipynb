{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3ad3ea2d4ad42f1ece07ff4157f7484249949be8"},"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.optimizers import SGD, Adam\nfrom keras.utils import np_utils\nimport os\nnp.random.seed(1671)\n\nfrom keras.models import load_model\nfrom keras.constraints import maxnorm\nfrom keras.regularizers import l1\nfrom keras.regularizers import l2\nfrom keras.regularizers import L1L2\nfrom keras.layers.normalization import BatchNormalization\nimport matplotlib.pyplot as plt\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import TensorBoard\ntb_monitor = TensorBoard(log_dir='./logs', histogram_freq=5, batch_size=10,\n                         write_graph=True, write_grads=True, \n                         write_images=False, embeddings_freq=0, \n                         embeddings_layer_names=None, embeddings_metadata=None)\nearly_stopping_monitor = EarlyStopping(monitor='val_acc', patience=5)\n\ndef ncaaDNN(Train_Predictors,Train_class,NUM_PREDICTORS, NB_CLASSES, YEAR):\n\n    #training hyper-parameters\n    NB_EPOCH = 200\n    BATCH_SIZE = 10\n    N_HIDDEN = 415\n    VERBOSE = 1 #display results during training\n    #OPTIMIZER = SGD() # choose optimizer\n    OPTIMIZER = Adam() # choose optimizer\n    VALIDATION_SPLIT = 0.1 #80% training and 20%validation\n    METRICS =['accuracy']\n    LOSS = 'binary_crossentropy'\n    DROP_OUT = 0.3\n\n    model = Sequential()\n    \n    #add hidden layer with NUM_PREDICTORS\n    model.add(Dense(units=N_HIDDEN, input_shape=(NUM_PREDICTORS,), kernel_initializer='uniform',\n                    kernel_regularizer='ones', activity_regularizer=l2(0.01))) \n                    #kernel_regularizer=l2(0.01), activity_regularizer=l2(0.01))) #Ridge\n                    #W_regularizer=l1(0.01), activity_regularizer=l1(0.01))) #Lasso\n    model.add(Activation('relu'))\n    model.add(Dropout(DROP_OUT))\n    \n    #add layer\n    model.add(Dense(units=N_HIDDEN, input_shape=(33,), kernel_initializer='uniform',\n                    kernel_regularizer='ones', activity_regularizer=l2(0.01))) \n                    #kernel_regularizer=l2(0.01), activity_regularizer=l2(0.01))) #Ridge\n                    #W_regularizer=l1(0.01), activity_regularizer=l1(0.01))) #Lasso\n    model.add(Activation('relu'))\n    model.add(Dropout(DROP_OUT))\n    \n    #add layer\n    model.add(Dense(units=N_HIDDEN, input_shape=(33,), kernel_initializer='uniform',\n                    kernel_regularizer='ones', activity_regularizer=l2(0.01))) \n                    #kernel_regularizer=l2(0.01), activity_regularizer=l2(0.01))) #Ridge\n                    #W_regularizer=l1(0.01), activity_regularizer=l1(0.01))) #Lasso\n    model.add(Activation('relu'))\n    model.add(Dropout(DROP_OUT))\n\n    #add layer\n    model.add(Dense(units=N_HIDDEN, input_shape=(33,), kernel_initializer='uniform',\n                    kernel_regularizer='ones', activity_regularizer=l2(0.01))) \n                    #kernel_regularizer=l2(0.01), activity_regularizer=l2(0.01))) #Ridge\n                    #W_regularizer=l1(0.01), activity_regularizer=l1(0.01))) #Lasso\n    model.add(Activation('relu'))\n    model.add(Dropout(DROP_OUT))\n\n    #add output layer with NB_CLASSES \n    model.add(Dense(NB_CLASSES, kernel_regularizer='ones'))    \n    model.add(Activation('sigmoid')) #set activation function for the output layer\n\n    model.compile(loss=LOSS, optimizer = OPTIMIZER, metrics =METRICS)\n    \n    filepath=\"./ckpts/model_dropout_\"+YEAR+str(DROP_OUT)+\"_{epoch:02d}_{val_acc:.2f}.ckpt\"\n    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    #callbacks_list = [checkpoint]    \n    \n    Tuning = model.fit(Train_Predictors,Train_class,batch_size=BATCH_SIZE, epochs = NB_EPOCH, verbose = VERBOSE,\n                       #validation_split = VALIDATION_SPLIT,callbacks=[early_stopping_monitor,checkpoint])\n                       validation_split = VALIDATION_SPLIT, callbacks = [checkpoint])\n    print(model.summary())\n    return model,Tuning\n\n#perform prediction on the test data\ndef deepPredict(model,Test_Predictors,Test_class,NUM_PREDICTORS, NB_CLASSES):\n    score = model.evaluate(Test_Predictors,Test_class)\n    print(\"Test score: \", score[0] )\n    print(\"Test accuracy: \", score[1])\n\n    import matplotlib.pyplot as plt\n#plot error during training with number of epochs\ndef plotTrainingLoss(Tuning,Title):\n    plt.figure(200)\n    plt.plot(Tuning.history['loss'])\n    plt.plot(Tuning.history['val_loss'])\n    plt.title(Title)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'vali'], loc='upper left')\n    plt.show()\n\n#plot accuracy during training with number of epochs\ndef plotTrainingAcc(Tuning, Title):\n    plt.figure(100)\n    plt.plot(Tuning.history['acc'])\n    plt.plot(Tuning.history['val_acc'])\n    plt.title(Title)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'vali'], loc='upper left')\n    plt.show()\n\n#return history of accuracy and loss w.r.t to epoch as a numpy array\ndef SaveHistory(Tuning,outfile):\n    #keys = Tunning.history.keys()\n    Hist = np.empty(shape=(len(Tuning.history['val_loss']),4))\n    Hist[:,0] = Tuning.history['val_loss']\n    Hist[:,1] = Tuning.history['val_acc']\n    Hist[:,2] = Tuning.history['loss']\n    Hist[:,3] = Tuning.history['acc']\n    np.savetxt(outfile, Hist, fmt='%.8f',delimiter=\",\",header=\"val_loss,val_acc,train_loss,train_acc\",comments=\"\")\n    return Hist","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}