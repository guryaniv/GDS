{"cells":[{"metadata":{"_uuid":"ca4ec79fb9e9abb7babd084196558187a91bc29e"},"cell_type":"markdown","source":"**Pleae look at the version 4 for the correct kernal**\n* Hi, here we will Develop a keras way model from scratch.\n1. First we will use the normal Keras Sequential way, then will move on to the, FCs, CNNs."},{"metadata":{"trusted":true,"_uuid":"603d22e6cd444ce18917ce952c056dbda599919d"},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\")) # looking at the dicrectory","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"547c40819f19dd3ea24005cded3c9a2e3c095ffa"},"cell_type":"markdown","source":"We then load all the libraries for the model."},{"metadata":{"trusted":true,"_uuid":"f5cbe3a965032031fc577f86613cb215ef08fd6e"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# fix random seed for reproducibility\nnp.random.seed(43)\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,Activation\nfrom keras.optimizers import Adam ,RMSprop\nfrom sklearn.model_selection import train_test_split\nfrom keras import  backend as K\nfrom keras.preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"518b39914a3906199b6a4323ed176e861a0a5970"},"cell_type":"code","source":"train=pd.read_csv(\"../input/train.csv\")\ntrain.head() # looking at the trainning dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41a242d769c33dcc392912b94963c1db87810a36"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb2114cee641fcfb5149df71b3e1b29a013d6631"},"cell_type":"code","source":"test=pd.read_csv(\"../input/test.csv\")\ntest.head() # looking at the trainning dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"847c71a2999ab4206011fea0797c6b1f4579cff5"},"cell_type":"markdown","source":"**DATA PREPARATION**"},{"metadata":{"_cell_guid":"fec41147-092c-4877-bb52-7308db2ad4ec","_uuid":"a839f3e8bd85ce85ec79003d95df7a3dde482623"},"cell_type":"markdown","source":"By looking at the trainning set, we should separate the label column which is the value set (target values) of our trainning dataset\n\nIn the given data\n1. Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. \n2. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. \n3. This pixel-value is an integer between 0 and 255, inclusive.\n\n* Now we sperate the labels from the tranning set \n* Hot encoding for the labels to represent as vector.\n* We Normalize the data to centre the data around zero mean and unit variance."},{"metadata":{"_cell_guid":"729f1905-4eb0-4378-ba7a-763710fc6369","_uuid":"63d5848b029396e156ce77502782b8be2aebf989","trusted":true},"cell_type":"code","source":"\"\"\"It is important preprocessing step. It is used to centre the data around zero mean and unit variance.\"\"\"\nimg_rows, img_cols = 28, 28 # height and width pf pixels\nnum_classes = 10 # total number of labels\nimport keras\ndef data_prep(raw):\n    train_y = keras.utils.np_utils.to_categorical(raw.label, num_classes) # hot encoding for the labels \n    num_images = raw.shape[0]\n    x_as_array = raw.values[:,1:]\n    x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)\n    train_x = (x_shaped_array.astype('float32') / 255.) #normalizing the data \n    return train_x, train_y\ntrain_x, train_y = data_prep(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72dbe11c16d25afea631852fb6899ffee5be0a97"},"cell_type":"markdown","source":" **Building Models**"},{"metadata":{"trusted":true,"_uuid":"edeee7869ff430091a7001850380c3e7639747e6"},"cell_type":"code","source":"\nfrom keras.utils.np_utils import to_categorical\nY_train = train[\"label\"]\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \nX_train = X_train / 255.0\ntest = test / 255.0\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\nY_train = to_categorical(Y_train, num_classes = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e983726940c284b3fa43e2cacc221d8483fedef"},"cell_type":"code","source":"# Set the random seed\nrandom_seed = 2\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cf9df4da1c59312e918e3b129a60d4ea44ff354"},"cell_type":"markdown","source":"**Lets start with simple Linear model**\n1.  In building a keras layer we need to define the input dimensions of our data in the 1st layer\n1. Now add Desne layer which is fully connected layer with N(desired) number of neurons\n2. In the last Dense layer we need to define the output dimensions/classes of the model."},{"metadata":{"trusted":true,"_uuid":"1494d7bc33b24ed4fba2697ac7402a6bcf9eef9e"},"cell_type":"code","source":"\nmodel=Sequential()\nmodel.add(Dense(64,input_shape=(28,28,1)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten()) #converts into a vector \nmodel.add(Dense(10, activation=\"softmax\"))\nmodel.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82ac9732fcf3622628894dbacdbf13806995733d"},"cell_type":"code","source":"model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.fit(train_x, train_y,\n          batch_size=128,\n          epochs=2,\n          validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2721f8404ff684521a081c3029db3a8368af1a08"},"cell_type":"markdown","source":"Wow we got an accuracy of 91% for simple dense layer. But its a kind of low. lets move on to the Convolutional Neural Network and see how it could help us."},{"metadata":{"trusted":true,"_uuid":"dc4ba6611a88647501c1ea1ab1a856412078aea7"},"cell_type":"markdown","source":"Now we Split the tranning data randomly with seed=42"},{"metadata":{"trusted":true,"_uuid":"9d33de87562672bb886aca6eeaef54c50ab0e632"},"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, test_size = 0.1, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f69234fef83ce4ad4d6b025e1a3ee53110091d98"},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72f17205f4d85dc3f06e3e6aa66c6359c46c456f"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7515a1b052343e42981a0369dc9b8a3dde51e32"},"cell_type":"code","source":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1b260442c6bd41b2034bc5c5eeb4cd225e68c10"},"cell_type":"code","source":"model.compile(optimizer=optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"631fbe707da3288f5f1f87512ae76ac01b64ddbc"},"cell_type":"markdown","source":"**Data Agumentation**\nHere we change the image rotation, width shift, height, we zoom a little bit, by doing all this we can increase over data almost double to the present and make our model more roboust.\nSome popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more."},{"metadata":{"trusted":true,"_uuid":"9746264d422fdaed7317f6fcdc7c5d4a1c4c7379"},"cell_type":"code","source":"from keras.preprocessing import image\ngen = image.ImageDataGenerator()\n#datagen=ImageDataGenerator(rotation_range=10, width_shift_range=0.1, shear_range=0.1,\n #                              height_shift_range=0.1, zoom_range=0.1)\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)\n#batches = datagen.flow(X_train, Y_train, batch_size=64)\n#val_batches = datagen.flow(X_val, Y_val, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6d3b9ff111b5eba371fe1375e6eb5f9cc92d3fd"},"cell_type":"code","source":"from keras.callbacks import ReduceLROnPlateau\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8578718004c970dd6d96e2b2c3bedab3b61588ca","scrolled":true},"cell_type":"code","source":"batch_size = 86\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = 30, validation_data = (X_val,Y_val),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size\n                              , callbacks=[learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c9cb63aac1ad7c56986b59b0fb202afcc80b761"},"cell_type":"code","source":"model.evaluate(X_val, Y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1df29cf902bdc5ad8395e6df54057fe69a413b25"},"cell_type":"code","source":"test = test.values.reshape(-1,28,28,1)\ntrain_x = (test.astype('float32') / 255.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b3693b1a02f60027920d10bbe23c4a71eda18bf"},"cell_type":"code","source":"\n# predict results\nresults = model.predict(test)\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88570b3af163ae77640d4a867c01b8c95836d7d4"},"cell_type":"code","source":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c1679e93bdf0bfab03dec0d48fc2a92c46c7c74"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}