{"cells":[{"metadata":{"trusted":true,"_uuid":"d513623490c7de88a3b09f77636215f951e95157"},"cell_type":"code","source":"print(\"hello moto\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e78c6e5ae4963b5fdbf20f12cf966dff85917aa4"},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport numpy as np\nimport math \n\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c793d616a4b3d9f90948062d8a7b6ef34af8180"},"cell_type":"markdown","source":"Declaration of functions"},{"metadata":{"trusted":true,"_uuid":"17096b42710d9d83763f2577c489f107185bdb44"},"cell_type":"code","source":"def data_to_np(input_file):\n    \n    #we need to define the nrows because the output files generated with Data_Refine include cuts that modify the shape\n    #of the output files (maybe we can fix this later)\n    \n    df = pd.read_csv(input_file, sep=',', nrows = 900000)\n    \n    header_names = ['pickup_longitude','pickup_latitude','dropoff_longitude',\n                    'dropoff_latitude','passenger_count','distance']\n    \n    #getting only the columns that we are going to use for the Keras NN\n    \n    df_train = df[header_names]\n    np_df_train = df_train.values\n    \n    df_label = df['fare_amount']\n    np_df_label = df_label.values\n    \n    return np_df_train, np_df_label    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"596ef01c603404ffec999359b6fe061d2ce9c718"},"cell_type":"code","source":"def global_mean_per_column(mynp_train_list):\n    \n    sum_mean = 0\n    \n    for con in range(len(mynp_train_list)):\n        sum_mean = sum_mean + np.mean(mynp_train_list[con], axis=0) \n    \n    #mean_0 = np.mean(mynp_train_0, axis=0) \n    #mean_1 = np.mean(mynp_train_1, axis=0)\n    \n    mean = sum_mean/len(mynp_train_list)\n        \n    return mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"180b06bf15e1ce2d9854878ceb0055d59dd002d6"},"cell_type":"code","source":"def global_std_per_column(mynp_train_list, global_mean):\n    \n    sum_mean_x2 = 0\n    \n    for con in range(len(mynp_train_list)):\n        sum_mean_x2 += np.mean((mynp_train_list[con] - global_mean)**2, axis=0)\n    \n    #mean_x2_0 = np.mean((mynp_train_0 - global_mean)**2, axis=0)\n    #mean_x2_1 = np.mean((mynp_train_1 - global_mean)**2, axis=0)\n    \n    std = np.sqrt(sum_mean_x2/len(mynp_train_list))\n    \n    return std\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4eaa5acd574443c81736142ed9be19f59b3f9915"},"cell_type":"code","source":"def norm_mynp_train(mynp_train, mean, std):\n    \n    mynp_train_norm = (mynp_train - mean)/std\n       \n    return mynp_train_norm    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23291b2bd40af3f67648e385c6dd9915832c052b"},"cell_type":"markdown","source":"Reading training csv files 1 by 1. We create np arrays with fixed number of lines (90K for the first outputs of refined data, and later using 900K when we are sure this is working)"},{"metadata":{"trusted":true,"_uuid":"76b61a3952349904d70ef215ee24e17bc9c47fc4"},"cell_type":"code","source":"mynp_train_0, mynp_label_0 = data_to_np('../input/my-taxi-fare-data/train_r0.csv')\nmynp_train_1, mynp_label_1 = data_to_np('../input/my-taxi-fare-data/train_r1.csv')\nmynp_train_2, mynp_label_2 = data_to_np('../input/my-taxi-fare-data/train_r2.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efb2e83c312f13ce24afaaebd6bc02c14dcf4107"},"cell_type":"markdown","source":"Reshuffling the np.arrays before normalization, as in our working example running on just one file of 100K lines"},{"metadata":{"trusted":true,"_uuid":"9da482279351f4cc32bad7cb8884fe888888f61f"},"cell_type":"code","source":"order = np.argsort(np.random.random(mynp_label_0.shape))\n\nmynp_train_0 = mynp_train_0[order]\nmynp_label_0 = mynp_label_0[order]\n\norder = np.argsort(np.random.random(mynp_label_1.shape))\n\nmynp_train_1 = mynp_train_1[order]\nmynp_label_1 = mynp_label_1[order]\n\norder = np.argsort(np.random.random(mynp_label_2.shape))\n\nmynp_train_2 = mynp_train_2[order]\nmynp_label_2 = mynp_label_2[order]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e908bf56d097b969210fcaa2096765b844e4c365"},"cell_type":"markdown","source":"DEFINITION of the mega array of mynp arrays, still befor normalizing"},{"metadata":{"trusted":true,"_uuid":"0467d89f06caa4ce20a0c567af6a46588254bdd2"},"cell_type":"code","source":"mynp_train_list = []\nmynp_train_list.append(mynp_train_0)\nmynp_train_list.append(mynp_train_1)\nmynp_train_list.append(mynp_train_2)\n\nmynp_label_list = []\nmynp_label_list.append(mynp_label_0)\nmynp_label_list.append(mynp_label_1)\nmynp_label_list.append(mynp_label_2)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34e901c70d5b85ca6646f8be14e919757e8194e6"},"cell_type":"markdown","source":"Computing the global mean and std deviation considering all the datases"},{"metadata":{"trusted":true,"_uuid":"9ec7b04f2a4da9e6acf62b271c773fc63cb561e3"},"cell_type":"code","source":"global_mean = global_mean_per_column(mynp_train_list)\nglobal_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76138f4a0d0b9d0e97002a6388403d52a967e2fa"},"cell_type":"code","source":"global_std = global_std_per_column(mynp_train_list,global_mean)\nglobal_std\n\n#notice the small value of the std deviation of lat and lon variables","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2a94cf9cb9fc3d8ba3602b19d5746223fe72ce9"},"cell_type":"markdown","source":"Definitionof the model and running over the normalized files"},{"metadata":{"trusted":true,"_uuid":"53d6982f634fe39ff9c1da99512e786136ba1754"},"cell_type":"code","source":"#def build_model(shape_1_of_np_array):\ndef build_model(shape_of_np_array):\n    model = keras.Sequential([\n            keras.layers.Dense(64, activation=tf.nn.relu, \n                               input_shape=(shape_of_np_array,)),\n            keras.layers.Dense(64, activation=tf.nn.relu),\n            keras.layers.Dense(64, activation=tf.nn.relu),\n            keras.layers.Dense(1)])\n\n    optimizer = tf.train.RMSPropOptimizer(0.001)\n\n    #Definition of the function to minimize (loss function)\n    #model.compile(loss='mse',optimizer=optimizer,metrics=[\"accuracy\"])\n    model.compile(loss='mse',optimizer=optimizer,metrics=[\"mae\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48f18615f609a2794db18df07371dbd8b7a3ccdc"},"cell_type":"code","source":"# Display training progress by printing a single dot for each completed epoch.\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self,epoch,logs):\n    if epoch % 5 == 0: print('epoch',epoch)\n    print('.'),\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f1e479480df288d06d32ff246f325ab7098b88f"},"cell_type":"markdown","source":"Definition of the model using the shape of the first training array\n\nThe shape[1] value is the number of columns to be used for training and prediction. In our case is postion and distance (not yet pickup_datetime). \nHere we are assuminf that the shapes of all the training subdataset and also the test one. This data engeneering must be done outside in order to \nbe safe here, becasue we are reading specific columns in the head functions above that make sure that the np arrays are well behaved, but if the \nincoming data is sick then we can run intro troubles because I have not added code for contro"},{"metadata":{"trusted":true,"_uuid":"728a1059f66033e83e82c8df1ad9cd97dfbb510b"},"cell_type":"code","source":"df_test = pd.read_csv('../input/my-taxi-fare-data/test_r0.csv', sep=',')\ndf_test = df_test[['key','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','distance']]\ndf_test_fn = df_test[['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','distance']]\n\nmynp_test = df_test_fn.values\nmean_test = np.mean(mynp_test,axis=0)\nstd_test = mynp_test.std(axis=0)\nmynp_test = (mynp_test - mean_test)/std_test\n\nmynp_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab2fb9fa339995104e7ae0e11b56ce2173c53306"},"cell_type":"markdown","source":"Attention with the following function!! this is not working yet. \n\nThe following routine should be used by fit_generator, using something like\n\nmodel.fit_generator(data_gen(1), epochs=EPOCHS, steps_per_epoch = 3, verbose=0, callbacks=[early_stop, PrintDot()])\n\nbut I repeat, this is not wroking yet!! the chi2 is around 9, which is well overpassed by the simple training of one of the train files and using the std model.fit(). \nAlso, the training of the merged data file with 5 epochs do not improve the chi2 but still is around 3.8 (our best was 3.7) \n\nAt least is running without errors"},{"metadata":{"trusted":true,"_uuid":"5c626761663515b03d591a8d6557cb90478b2db3"},"cell_type":"markdown","source":"def data_gen(batch_size):\n    while True:\n        \n        for con_np in range(3):\n            #we first normalize each training np.array\n            mynp_train_norm = norm_mynp_train(mynp_train_list[con_np], global_mean, global_std)\n                \n            mynp_label = mynp_label_list[con_np]    \n                \n            yield (mynp_train_norm, mynp_label)\n                "},{"metadata":{"_uuid":"60bd44c415f792476c0cfbb0f7622f35c53c3a2d"},"cell_type":"markdown","source":"Let us try to merge the np.arrays and try to run over the full dataset composed by the components of mynp_train_list and mynp_label_list. \n\nThis should be run with \n\nmodel.fit(mynp_train_concat, mynp_label_concat, epochs=EPOCHS, validation_split=0.2, verbose=0, callbacks=[early_stop, PrintDot()])"},{"metadata":{"trusted":true,"_uuid":"1c2aae4289a4d3ca4fbcda3e09c17b09556fa6cb"},"cell_type":"code","source":"#normaalization\nmynp_train_norm_0 = norm_mynp_train(mynp_train_list[0], global_mean, global_std)\nmynp_train_norm_1 = norm_mynp_train(mynp_train_list[1], global_mean, global_std)\nmynp_train_norm_2 = norm_mynp_train(mynp_train_list[2], global_mean, global_std)\n\n#concatanation\nmynp_train_concat = np.concatenate((mynp_train_norm_0,mynp_train_norm_1,mynp_train_norm_2),axis=0)\nmynp_label_concat = np.concatenate((mynp_label_0,mynp_label_1,mynp_label_2),axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1962bb74166241a64284f937654c6e9533a2736"},"cell_type":"markdown","source":"Definition of model and fit call"},{"metadata":{"trusted":true,"_uuid":"9a8803bd87c1e9ba7fbe6de4d332eaa9735135ff"},"cell_type":"code","source":"early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bc6f5bf724fd8d7fc97385a949e814137d42cd6"},"cell_type":"markdown","source":"model = build_model(mynp_train_0.shape[1]) \n\n#for con in range(len(mynp_train_super_array)):\n\n#for con in range(3):\n\nEPOCHS = 10\nmodel.fit(mynp_train_concat, mynp_label_concat, epochs=EPOCHS, validation_split=0.2, verbose=0, callbacks=[early_stop, PrintDot()])"},{"metadata":{"trusted":true,"_uuid":"3ef9cf51fcb0eeaec5d66e50d7d3c496bb0fcda9"},"cell_type":"markdown","source":"test_predictions = model.predict(mynp_test).flatten()\ntest_predictions"},{"metadata":{"trusted":true,"_uuid":"9e48251912c1d70ec438fa8711d07322c4010572"},"cell_type":"code","source":"mynp_train_norm_0.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"695020d7d7c4e94740364d8eaeab7ca60ee59043"},"cell_type":"code","source":"#reading a sub-array from the big one for the grid study\n\ngrid_train = mynp_train_concat[:100]\ngrid_label = mynp_label_concat[:100]\n\nprint(grid_train.shape, grid_label.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07630d9ab76ceaff6342a1e864eeacae9e0dfe20"},"cell_type":"markdown","source":"Note that n_jobs is a parameter which is meant to be used when the grid search is run in parallel. \nIn particular n_jobs=-1 implies parallelism, which is suggested by the examples of \nhttps://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\nhowever the code het hang even for one element in param_grid, at least in the notebook version\n\nChanging n_jobs=1 works better"},{"metadata":{"trusted":true,"_uuid":"e4ba86c37a97d68834dfbb48c4bf8d5eb033c298"},"cell_type":"code","source":"model_for_grid = KerasClassifier(build_fn=build_model, shape_of_np_array = mynp_train_0.shape[1], validation_split=0.2, verbose = 0) \nepochs = [3,5]\nbatches = [32,64]\nparam_grid = dict(epochs=epochs,batch_size=batches) \ngrid = GridSearchCV(estimator=model_for_grid, param_grid=param_grid, n_jobs=1, scoring=\"neg_mean_absolute_error\") \ngrid_result = grid.fit(grid_train, grid_label,callbacks=[early_stop, PrintDot()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"236a8383643c663066749b2c600a01a77e6075ff"},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08846f3ded4a3b3492ba399f1205d5dd689f5344"},"cell_type":"code","source":"model_after_gridSearch = build_model(mynp_train_0.shape[1]) \nEPOCHS = grid_result.best_params_['epochs']\nBATCH_SIZE = grid_result.best_params_['batch_size']\n\nprint(EPOCHS,BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0066467e0bf4d7d6656875eaf77cb6f68d37fdcd"},"cell_type":"code","source":"model_after_gridSearch.fit(mynp_train_concat, mynp_label_concat, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n#model_after_gridSearch.fit(mynp_train_concat, mynp_label_concat, epochs=EPOCHS, validation_split=0.2, verbose=0, callbacks=[early_stop, PrintDot()])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9c08f9ca88c47293b3f7c525b121c6ca01427a8"},"cell_type":"markdown","source":"From here is independent of the fit procedure, as long as it produces a model that can predict something"},{"metadata":{"trusted":true,"_uuid":"fcf39b8d2788402dd19d3208d8f5eecf592c7bd7"},"cell_type":"code","source":"#test_predictions = test_predictions_total/len(mynp_train_super_array)\n#test_predictions = test_predictions_total\ntest_predictions = model_after_gridSearch.predict(mynp_test).flatten()\ntest_predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6253fc1d2e93fba95ca07fb3c820e00b5e00023d"},"cell_type":"markdown","source":"Predictions after using the model.fit repeated times over several np normalized entries"},{"metadata":{"_uuid":"9bbe91aef3fea009e8be893cd2906134ad2b7381"},"cell_type":"markdown","source":"Reading the test file and evaluating the super model.fit iterated over all the input files"},{"metadata":{"trusted":true,"_uuid":"60ee8cf1960f43b5882af0481bdc965cc043c75e"},"cell_type":"code","source":"test_key_array = df_test['key'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f6295db5ee5908c7b6e27f904296b32d4ff3513"},"cell_type":"code","source":"test_key_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12c1b510c5a570f753859f6c937504921e49fb75"},"cell_type":"code","source":"df_output = pd.DataFrame({'key': test_key_array,'fare_amount': test_predictions})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7284418cb642ed4ca9b44d9cc2b8c6764f504d82"},"cell_type":"code","source":"df_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7918661a9fa216e0915dfd869a96a07ad546748f"},"cell_type":"code","source":"df_output.to_csv('submission_file.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c92fdbb4a3709ca94e918263c6a3dd410b64371c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}