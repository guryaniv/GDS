{"cells":[{"metadata":{"_uuid":"d2d71d481691b93edf8f3fa576bbe304fb3270d2"},"cell_type":"markdown","source":"This is one of the first few Kaggle competitions I've participated in so it's quite special to me as it helped kick off my data science journey. I've taken inspiration from the following awesome notebook(s):-  \n\n- [Stacked Regressions to predict House Prices](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard/notebook) by **Serigne**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# First, let's import the necessary stuff and load our training and test data\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy\nsns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fa1536896ad086f206e2d9d523c3cb07107c6e4"},"cell_type":"code","source":"# suppressing all warnings for readability (I wouldn't do this unless I'm really really sure...)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee2889976cdb01ab39ef1873b1787fb30722d281"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e248814d55dc7c8471e52e49afbda7602aad526"},"cell_type":"code","source":"# firstly, we do not need the ID column in both the train and test datasets so we'll go ahead and drop them\ntrain.drop('Id', axis = 1, inplace = True)\ntest_ids = test['Id']\ntest.drop('Id', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0193afce3deedd68f3cb99e408326755e8218a02"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true,"_uuid":"843eeab10baa024d4c0a382d26239a2ab6ba5bcf"},"cell_type":"code","source":"# after studying the dataset, it is clear that there are quite a few outliers in several of the predictors\n# but given that we only have 1460 rows to play with, it doesn't make sense to remove all of them\n# So, for now, I am only removing the outliers from the 'ground living area' predictor\n\nfig, axes = plt.subplots(nrows = 1, ncols = 2)\nsns.scatterplot(x = train.GrLivArea, y = train.SalePrice, data = train, ax = axes[0])\naxes[0].set_xlim(0, 4000)\naxes[0].set_ylim(0, 650000)\naxes[0].set_title(\"Before dropping outliers\")\ntrain = train[~(np.abs(train.GrLivArea - train.GrLivArea.mean()) > (3.5 * train.GrLivArea.std()))]\nprint(f\"Updated shape of the dataset: {train.shape}\")\nsns.scatterplot(x = train.GrLivArea, y = train.SalePrice, data = train, ax = axes[1], color = \"brown\")\naxes[1].set_xlim(0, 4000)\naxes[1].set_ylim(0, 650000)\naxes[1].set_title(\"After dropping outliers\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0dc1ad93f65338b0cf9a8e9a4c93c4ca8e34e96"},"cell_type":"markdown","source":"## Now, we'll analyze the response variable"},{"metadata":{"trusted":true,"_uuid":"b1f039fc3c133674f61e29ce4ffec7e48beb2028"},"cell_type":"code","source":"# let's study the distribution of the response\nsns.distplot(train.SalePrice)\n\n# qq-plot\nfig = plt.figure()\nres = scipy.stats.probplot(train['SalePrice'], plot = plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d7b1598cb6d39f08d16f276623f727fed8a0ae0"},"cell_type":"code","source":"# we see that the response data is skewed\n# it would be preferable to have normality in our data, so let's do that first\n# for this purpose, a natural log of the data seems to be doing the trick\n\ntrain['SalePrice'] = np.log(train['SalePrice'])\nfig = plt.figure()\nres = scipy.stats.probplot(train['SalePrice'], plot = plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f10bb638dc89cce4adf08f31b627f84736959d0"},"cell_type":"markdown","source":"## Time for feature engineering"},{"metadata":{"trusted":true,"_uuid":"6bdf65db9d6ff2c4e06e4d99fdb63c9611561475"},"cell_type":"code","source":"# to aid is better in this process, we would be combining the train and test data\n# this would prove beneficial in some cases. For instance, when we are imputing missing data\n\nntrain = len(train)\ny = train.SalePrice\ncombined = pd.concat([train, test], ignore_index = True)\ncombined.drop(columns='SalePrice', inplace=True)\nprint(f\"Shape of combined data: {combined.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4af207d653638a41aae06848ed474bc7448e957"},"cell_type":"code","source":"# let's analyze the missing data now\n(combined.isna().sum().sort_values(ascending = False) / len(combined)) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e012cfebdb21b7d462b51628ed5f7c80104be43c"},"cell_type":"code","source":"# we see that PoolQC, MiscFeature, Alley, Fence, FireplaceQu have a substantial number of missing values\n# it doesn't make sense to try to impute/fill them as it'd be an inaccurate representation anyways\n# so let's go ahead and drop them\n\ncombined.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], inplace = True, axis = 1)\nprint(f\"Update shape of the combined dataset: {combined.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34631d2f63a36967ede801592d9c02d5dda83bca"},"cell_type":"code","source":"# for lot frontage, since we can roughly expect streets in a neighbourhood to have similar lot areas\n# we can impute the missing values based on this grouping\n\ncombined.LotFrontage = combined.groupby('Neighborhood').LotFrontage.apply(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3306ed54f676fd3e706d2b3c1078bf44810fbb43"},"cell_type":"code","source":"# the following will be replaced with \"None\"\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'MSSubClass', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    combined[col] = combined[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee6d1184c8e20a8395bbb2d04c063f57ae4a072d"},"cell_type":"code","source":"# the following will be replaced with a zero\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    combined[col] = combined[col].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef2b652522a73685e1cda0e3aa70af0db6490a7d"},"cell_type":"code","source":"# the following categorical predictors will be replaced by the mode (most frequently occuring class)\ncombined['MSZoning'] = combined['MSZoning'].fillna(combined['MSZoning'].mode()[0])\ncombined['Electrical'] = combined['Electrical'].fillna(combined['Electrical'].mode()[0])\ncombined['KitchenQual'] = combined['KitchenQual'].fillna(combined['KitchenQual'].mode()[0])\ncombined['Exterior1st'] = combined['Exterior1st'].fillna(combined['Exterior1st'].mode()[0])\ncombined['Exterior2nd'] = combined['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0])\ncombined['SaleType'] = combined['SaleType'].fillna(combined['SaleType'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2a2435b94c5d6a5f317b59854049be673fbab31"},"cell_type":"code","source":"# for function, the data description says NA means 'typical'\ncombined[\"Functional\"] = combined[\"Functional\"].fillna(\"Typical\")\n\n# and finally, for Utilities, we might as well get rid of it as almost all of the entries have the same value for it\n# so it won't really aid us in the modeling process\ncombined.drop('Utilities', axis = 1, inplace = True)\nprint(f\"The final dimensions of the combined data {combined.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96581ce97ec109afbacd94fbefe78decefdecb04"},"cell_type":"markdown","source":"## Aaanddd, that's a goodbye to missing values!\n\n### Now, let's do further feature engineering"},{"metadata":{"trusted":true,"_uuid":"e9456d8a62a31bbcbe30b8f31c090b356ee42cff"},"cell_type":"code","source":"# let's now transform some of the numerical variables that are really categorical\ncombined['MSSubClass'] = combined['MSSubClass'].apply(str)\ncombined['OverallCond'] = combined['OverallCond'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18715f29c3e7426a25bde51a5fea0bc1cd05de4b"},"cell_type":"code","source":"# given how important the total sq. footage is in determining a house price, we are creating a TotalSF predictor\ncombined['TotalSF'] = combined['TotalBsmtSF'] + combined['1stFlrSF'] + combined['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ce3424f7a3040355b53183f4d8ad958d4d8b362"},"cell_type":"code","source":"# let's check for skewness in our numerical data\nnumeric_feats = combined.dtypes[combined.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = combined[numeric_feats].apply(lambda x: x.skew()).sort_values(ascending = False)\nskewness = pd.DataFrame({'Skew': skewed_feats})\nskewness","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9325f7ceb764acf46aea3e70423c9de874f63c0b"},"cell_type":"code","source":"# with the above info, let's perform a box-cox transformation to bring data as close as possible to a gaussian distribution\nfrom scipy.special import boxcox1p\n\nskewness = skewness[abs(skewness) > 0.75]\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    combined[feat] = boxcox1p(combined[feat], lam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a7f009f6804574d022d541a95b2ab69f3d68582"},"cell_type":"code","source":"# finally, let's assign dummy variables to our categorical data (algorithms like linear regression need this)\ncombined = pd.get_dummies(combined)\ncombined.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9584f036a710b6f79e15a0456f4b81bb4e7766b3"},"cell_type":"code","source":"# let's split our datasets back to training and testing\ntrain = combined[:ntrain]\ntest = combined[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d13f9ef97c3924314dbe21d6c8a8a0eb4d3728c"},"cell_type":"markdown","source":"## Now, we can finally start modeling"},{"metadata":{"trusted":true,"_uuid":"f5a91f06a3e669c2edcf7b1c518435d5dfde935e"},"cell_type":"code","source":"# so we have the predictors in 'train' and the response in 'y'\n# since we've already taken log of the response, it would suffice if we just calculated the RMSE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\ny.reset_index(drop = True, inplace = True)\n\ndef cv_rmsle(model):\n    cv_score_array = np.sqrt(-cross_val_score(model, train, y, cv = 5, scoring = \"neg_mean_squared_error\"))\n    return cv_score_array.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0314f6d937a837b39a4069b71f5ed45aaaac54b"},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true,"_uuid":"47ef8547503c3236fda7f15e73fd7205a7f58191","scrolled":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlm_rmsle = cv_rmsle(LinearRegression())\nprint(f\"RMSLE for Linear Regression: [{lm_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c34fc6310adddda22ac38d69d228c31ab040697e"},"cell_type":"markdown","source":"### LASSO"},{"metadata":{"trusted":true,"_uuid":"b1335090f4724c7a450fbfa4a6ec602034059a33"},"cell_type":"code","source":"# for LASSO, we first need to determine the value of alpha\n# we will use LassoCV for that purpose\n\nfrom sklearn.linear_model import Lasso, LassoCV\n\nlassocv_model = LassoCV(cv = 5, random_state = 1)\nlassocv_model.fit(train, y)\nbest_alpha = lassocv_model.alpha_\n\nlasso_model = make_pipeline(RobustScaler(), Lasso(alpha = best_alpha, random_state = 1))\nlasso_rmsle = cv_rmsle(lasso_model)\nprint(f\"RMSLE for LASSO (L1 Regularization): [{lasso_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9089c1f29aedf118e82e0b19be011edb05b44645"},"cell_type":"markdown","source":"### Ridge Regression"},{"metadata":{"trusted":true,"_uuid":"b611b477b7fec18f1b970c0fad0222b75c63869e"},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV, Ridge\n\nridgecv_model = make_pipeline(RobustScaler(), RidgeCV(alphas = np.logspace(-10, 10, 100)))\nridge_rmsle = cv_rmsle(ridgecv_model)\nprint(f\"RMSLE for Ridge Regression (L2 Regularization): [{ridge_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a99629649434e16c764c359c18eb1fbc02b4670"},"cell_type":"markdown","source":"### Elastic Net"},{"metadata":{"trusted":true,"_uuid":"69f9ea75167733526a11109fae616acd0af07e09"},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, ElasticNetCV\n\nenetcv_model = ElasticNetCV(l1_ratio = np.arange(0.1, 1, 0.1), cv = 5, random_state = 1)\nenetcv_model.fit(train, y)\nbest_l1_ratio = enetcv_model.l1_ratio_\nbest_alpha = enetcv_model.alpha_\n\nenet_model = make_pipeline(RobustScaler(), ElasticNet(alpha = best_alpha, l1_ratio = best_l1_ratio, random_state = 1))\nenet_rmsle = cv_rmsle(enet_model)\nprint(f\"RMSLE for Elastic Net (L1 and L2 Regularization): [{enet_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcf97579c6d6f8b5ba8e9f5a9457d6c66fea7130"},"cell_type":"markdown","source":"### Moving on to ensemble methods..\n\n### Gradient Boosting"},{"metadata":{"trusted":true,"_uuid":"0b87a5903f0cfc3a2a8497017ccfe6a447dcac74"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\n# after much trial and error, I arrived at the hyperparameters used in gradient boosting\ngboost_model = GradientBoostingRegressor(loss = 'huber', learning_rate = 0.1, n_estimators = 3000, max_depth = 1, random_state = 1)\ngboost_rmsle = cv_rmsle(gboost_model)\nprint(f\"RMSLE for Gradient Boosting: [{gboost_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"294e709ee4ec565d9159a44d3721f06d911bd576"},"cell_type":"markdown","source":"### Adaptive Boosting (with LASSO as the estimator)"},{"metadata":{"trusted":true,"_uuid":"87f6bf8c11ee773a8a97db6402e99043e27e077f"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\n# after much trial and error, I arrived at the hyperparameters used in adaptive boosting\nadaboost_lasso_model = AdaBoostRegressor(lasso_model, n_estimators = 50, learning_rate = 0.001, random_state = 1)\nadaboost_lasso_rmsle = cv_rmsle(adaboost_lasso_model)\nprint(f\"RMSLE for ADA Boosting (with LASSO): [{adaboost_lasso_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a8ace0498bdd613bf6bbb8c57f3eb036f68f522"},"cell_type":"markdown","source":"### Adaptive Boosting (with Elastic Net as the estimator)"},{"metadata":{"trusted":true,"_uuid":"198257cc2abbb36ab7a8b28b234eea02ff46150a"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\n# after much trial and error, I arrived at the hyperparameters used in adaptive boosting\nadaboost_enet_model = AdaBoostRegressor(enet_model, n_estimators = 50, learning_rate = 0.001, random_state = 1)\nadaboost_enet_rmsle = cv_rmsle(adaboost_enet_model)\nprint(f\"RMSLE for ADA Boosting (with Elastic Net): [{adaboost_enet_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eb62c8a3577dc557d4d60a34bf6524c569cfaf5"},"cell_type":"markdown","source":"### Random Forests"},{"metadata":{"trusted":true,"_uuid":"6544678e313e6dd551e3f810d8a27a7f81399c3b"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrforest_rmsle = cv_rmsle(RandomForestRegressor(random_state = 1))\nprint(f\"RMSLE for Random Forests: [{rforest_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3b28756c954c1d58f62bf1a899cd9fadf6e1733"},"cell_type":"markdown","source":"### Extremely randomized trees"},{"metadata":{"trusted":true,"_uuid":"2ffc6d6dcb724283a85b2a08b0068dbc5e0e8cd4"},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\n\netrees_rmsle = cv_rmsle(ExtraTreesRegressor(random_state = 1))\nprint(f\"RMSLE for extremely randomized trees: [{etrees_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"171bfbbc3635e2318dbc1b0d0c648c6316652909"},"cell_type":"markdown","source":"### PCA\n\n### Let's see if reducing dimensionality has a positive effect on the predictions"},{"metadata":{"trusted":true,"_uuid":"7e4b9746dc762732b220605ceea59d3323e15e63"},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\npca = make_pipeline(RobustScaler(), PCA(n_components = 3, random_state = 1))\npca.fit(train.transpose())\nprint(f\"Proportion of variance explained by the components: {pca.steps[1][1].explained_variance_ratio_}\")\n\n# we are using 3 components in this case\np_comps = pca.steps[1][1].components_.transpose()\n\npca_lm_rmsle = np.sqrt(-cross_val_score(LinearRegression(), p_comps, y, cv = 5, scoring = \"neg_mean_squared_error\")).mean()\nprint(f\"RMSLE for Linear Regression after PCA reduction: [{pca_lm_rmsle}]\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fff628df4225f836c6a95e2b78e80a94d501125"},"cell_type":"markdown","source":"So PCA reduction didn't really help our case. So we are now done with all our models so it's time to have some fun.\n\nLet's try combining some of these models and see if we get better results"},{"metadata":{"trusted":true,"_uuid":"cdbcf3255f5b98ca833b2e1f2526d73098a520f7"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\n\n# we also need to define a function that returns cross-validated predictions for the data\ndef cv_pred(model):\n    return cross_val_predict(model, train, y, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb1f373d6ed4cd4186b935767b3a88b24d9b3b00"},"cell_type":"markdown","source":"We will try out various combinations of our top 3 models which are:-\n1. Adaptive boosting with LASSO (`adaboost_lasso_model`)\n2. Adaptive boosting with Elastic Net (`adaboost_enet_model`)\n3. LASSO (`lasso_model`)"},{"metadata":{"trusted":true,"_uuid":"fe38ada867dae5afbda3906eed1c4feab4b159c7"},"cell_type":"code","source":"adaboost_enet_pred = cv_pred(adaboost_enet_model)\nadaboost_lasso_pred = cv_pred(adaboost_lasso_model)\nlasso_pred = cv_pred(lasso_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"887b3a277d9b50d4bc2187b1a9dd5bffa1d3c2a9"},"cell_type":"code","source":"# now let's take the exponential of the responses to bring them back to their original scale\nadaboost_enet_pred = np.exp(adaboost_enet_pred)\nadaboost_lasso_pred = np.exp(adaboost_lasso_pred)\nlasso_pred = np.exp(lasso_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a078f2dae6011df26ec3e0d89c8f87baa70e2611"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\ny_actual = np.exp(y)\n\nrmsle_summary = pd.DataFrame(columns = ['ADA_ENet', 'ADA_Lasso', 'Lasso', 'RMSLE'])\nrmsle_summary = rmsle_summary.append({'ADA_ENet':1, 'ADA_Lasso':0, 'Lasso':0, 'RMSLE':adaboost_enet_rmsle}, ignore_index = True)\nrmsle_summary = rmsle_summary.append({'ADA_ENet':0, 'ADA_Lasso':1, 'Lasso':0, 'RMSLE':adaboost_lasso_rmsle}, ignore_index = True)\nrmsle_summary = rmsle_summary.append({'ADA_ENet':0, 'ADA_Lasso':0, 'Lasso':1, 'RMSLE':lasso_rmsle}, ignore_index = True)\n\nfor i in np.arange(0.1, 0.9, 0.1).tolist():\n    for j in np.arange(0.1, 1 - i, 0.1).tolist():\n            final_preds = round(i, 1)*adaboost_enet_pred + round(j, 1)*adaboost_lasso_pred + round(1 - (i + j), 1)*lasso_pred\n            rmsle = np.sqrt(mean_squared_log_error(y_actual, final_preds))\n            rmsle_summary = rmsle_summary.append({'ADA_ENet':round(i, 1), 'ADA_Lasso':round(j, 1), 'Lasso':round(1 - (i + j), 1), 'RMSLE':rmsle}, ignore_index = True)\n            \nprint(rmsle_summary)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09eb9643e3d7e4db107d4f95d03b623586415a6d"},"cell_type":"markdown","source":"Ahh... all that effort and we still see that ADA boosting with LASSO gives us the best results!\n\n### Submission time!"},{"metadata":{"trusted":true,"_uuid":"ab1e42b4434b78762c85fb3ad13a8c347a4f5f58"},"cell_type":"code","source":"adaboost_lasso_model.fit(train, y_actual)\nsubmission_preds = adaboost_lasso_model.predict(test)\n\nresults = pd.DataFrame({'Id':test_ids, 'SalePrice':submission_preds})\nresults.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}