{"cells": [{"outputs": [], "metadata": {"collapsed": false, "_uuid": "e013f48301e74efdcb1b76840cc76080baf65983", "_execution_state": "idle"}, "cell_type": "markdown", "source": "", "execution_count": null}, {"outputs": [], "metadata": {"collapsed": true, "_execution_state": "idle", "_uuid": "42c8c8e2b10b70450a4a4ca7997ed3b3fef03452", "trusted": false, "_cell_guid": "b50aedfe-d0ca-44bd-919c-4558c4d44dd7"}, "cell_type": "code", "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))", "execution_count": 2}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "9c92927cd99d0536849fe9167ddd846fff44c50b", "_execution_state": "idle"}, "cell_type": "markdown", "source": "Let's explore this data.", "execution_count": null}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "2ebad9297c3c653889c67bed3264924c7813f47c", "_execution_state": "idle"}, "cell_type": "code", "source": "print('Reading data...')\nkey_1 = pd.read_csv('../input/key_1.csv')\ntrain_1 = pd.read_csv('../input/train_1.csv')\nss_1 = pd.read_csv('../input/sample_submission_1.csv')\n\nprint ('Data has been read')", "execution_count": 3}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "041632e409051ce4cb21bf6732976c5da79994a4", "_execution_state": "idle"}, "cell_type": "markdown", "source": "ljust is a Python method to justify the printed output\nwe are using the Pandas shape method, which gives you rows by columns", "execution_count": null}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "9b2b2381d13d4d0f83fa49f601b3c7a008b6bd2f", "_execution_state": "idle"}, "cell_type": "code", "source": "\nprint(\"Train\".ljust(15), train_1.shape)\nprint(\"Key\".ljust(15), key_1.shape)\n\n# print (\"Train head\".ljust(15), train_1.head())\n# print (\"Key head\".ljust(15), key_1.head())", "execution_count": null}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "1c4aafe654df0670ffe0f046cc7cb4a0f3d9ebb5", "_execution_state": "idle"}, "cell_type": "markdown", "source": "The train file's first column is Page, and the rest of the columns have date names.\n\nThe dataset doesn't distinguish between no data and no visits.\nSo we have to choose between 0 and median, essentially, before going on to make our predictions.\n\nThe dataset is just Wikipedia articles.", "execution_count": null}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "b87cabc2cdf09942c5ec7834b72d23362e3d3e1b", "_execution_state": "idle"}, "cell_type": "markdown", "source": "The output shows that the key file has just two columns: Page, and Id. This is what we use to make our predictions", "execution_count": null}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "58509475922f4206be7358cc9e3bb63cd3b881aa", "_execution_state": "idle"}, "cell_type": "code", "source": "# Each article name has the following format: 'name_project_access_agent' \n# Take the Page column, split it, throw it in a DataFrame\n# Leave off the name with [-3:]\nprint (\"Exploring page names\")\npage_details = pd.DataFrame([i.split(\"_\")[-3:] for i in train_1[\"Page\"]])\n# Rename the columns \npage_details.columns = [\"project\", \"access\", \"agent\"]\nprint(page_details.describe())\n\n# Filter to unique values and take a look\nprint(\"The unique values in a list: \")\nproject_columns = page_details['project'].unique()\naccess_columns = page_details['access'].unique()\nagents_columns = page_details['agent'].unique()\nprint(list(project_columns))\nprint(list(access_columns))\nprint(list(agents_columns))", "execution_count": 4}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "d78ddba0cdc49f26e8ae602ef6347cb23f77850f", "_execution_state": "idle"}, "cell_type": "markdown", "source": " The commented-out script below uses the NumPy nanmedian() method, which computes the median but ignores NaNs.\n\nWhy does the nanmedian() method take all rows, but only the last 56 columns?\nThere are 551 columns.\n\naxis=1 means that we are computing the median along each row. We round because fractional visits don't make sense.", "execution_count": null}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "8bd12267216f06f65d745b2bc915cd06e539e25a", "_execution_state": "idle"}, "cell_type": "code", "source": "#np.nan_to_num(np.round(np.nanmedian(train_1.drop('Page', axis=1).values[:, -56:], axis=1)))", "execution_count": null}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "cd876bfd791d45ef2d3b7ed97140d7881037f8c9", "_execution_state": "idle"}, "cell_type": "markdown", "source": "At this point we are set up to make our predictions. We can use ", "execution_count": null}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "193094760b2935a94e0ec7e52be19ffe8ddfafd8", "_execution_state": "idle"}, "cell_type": "code", "source": "# This scores 64.8 according to the Simple Model\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values, axis=1))\n\n# This scores one point worse than -56:, with a score of 46.7 instead of 45.7\n# With -80, we get 46.3.\nvisits = np.nan_to_num(np.round(np.nanmedian(train_1.drop('Page', axis=1).values[:, -100:], axis=1)))", "execution_count": 5}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "c505e13e457ae7348e1d157316b092471533cebc", "_execution_state": "idle"}, "cell_type": "code", "source": "print ('Now we build our submission')\n# for some reason we drop the last 11 letters of the pages\nids = key_1.Id.values\npages = key_1.Page.values\nd_pages = {}\nfor id, page in zip(ids, pages):\n    d_pages[id] = page[:-11]\n\n# Now we put our predicted values into our new dictionary\nd_visits = {}\nfor page, visits_number in zip(pages, visits):\n    d_visits[page] = visits_number\n\nprint('Modifying sample submission...')\n# Take the values of the Id and the Visits columns\nss_ids = ss_1.Id.values\nss_visits = ss_1.Visits.values\n\n# enumerate is a Python method that loops over the index and item\nfor i, ss_id in enumerate(ss_ids):\n    ss_visits[i] = d_visits[d_pages[ss_id]]\n\nprint('Saving submission...')\n# Put in a DataFrame again to use the to_csv method\nsubm = pd.DataFrame({'Id': ss_ids, 'Visits': ss_visits})\nsubm.to_csv('submission.csv', index=False)", "execution_count": 6}], "nbformat": 4, "metadata": {"language_info": {"version": "3.6.1", "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1}