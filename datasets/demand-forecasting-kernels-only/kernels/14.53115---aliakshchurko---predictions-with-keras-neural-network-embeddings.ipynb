{"cells":[{"metadata":{"trusted":true,"_uuid":"539bd2e110482e40c96b7c5b76c90d461b867fa4","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport datetime\n\nfrom keras.layers import Input, Dense, Activation, Reshape\nfrom keras.layers.merge import concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn import preprocessing\n\nimport re\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.layers import BatchNormalization,Dropout\nfrom sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler\nfrom keras.callbacks import ModelCheckpoint\n\nfrom fastai.imports import *\nfrom fastai.column_data import *\nfrom fastai.structured import *\n\nfrom keras.callbacks import ReduceLROnPlateau\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\nimport statsmodels.api as sm\n\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"974fe7dab1c2e3b3048b6e48984b8d3f1b424797","collapsed":true},"cell_type":"code","source":"PATH='../input/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e142112da3fc7944d35d001172c061e358737ed6"},"cell_type":"markdown","source":"### Time trends visualization"},{"metadata":{"trusted":true,"_uuid":"b4bcd10f028343dd33c555e668c59b175e881de2","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e64b41df1abc9d3545a56f1dff24b91ccf99a630","collapsed":true},"cell_type":"code","source":"# from pylab import rcParams\n# rcParams['figure.figsize'] = 11, 9\n\n# decomposition = sm.tsa.seasonal_decompose(y_time, model='additive')\n# fig = decomposition.plot()\n# plt.show()\n#\"Is_quarter_end\", \"Is_quarter_start\"\n#\"Is_month_end\", \"Is_month_start\", \"Is_year_end\", \"Is_year_start\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdfff226a88fcc9d292e98f59e711b7467a74446"},"cell_type":"markdown","source":"Looks like current time series have clear seasonality pattern, as well as an overall increasing trend\n\n### Date preprocessing \n- Now we process train data in order to get represent date components which we will use ac categorical feature in our model\n- We also perfom normalization of non-categorical features\n- For categorical features we will define embedding space and will use it in our model"},{"metadata":{"trusted":true,"_uuid":"4785f0c7b78901d89569421fca6aed1cf6c9d316","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc32ee4313943da222e84ed1e4a5935a58f67b5a"},"cell_type":"code","source":"cat_feature = [\"store\", \"item\"]\ncon_feature = [\"Year\", \"Month\", \"Week\", \"Day\", \"Dayofweek\", \"Dayofyear\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"800ac856adfa8cc85e631b333ab8b1cfec8a25cb"},"cell_type":"code","source":"def prepare_df(data_df, isTrain=True, shuffle=True):\n    add_datepart(data_df, \"date\")\n    if shuffle:\n        data_df = data_df.sample(frac=1)\n\n    for cat_f in cat_feature:\n        data_df[cat_f] = data_df[cat_f].astype(\"category\").cat.as_ordered()\n\n    mapper = DataFrameMapper([\n         (con_feature, StandardScaler())\n    ])\n    data_df[con_feature] = mapper.fit_transform(data_df)\n\n    label_encoders = []\n    for f_name in cat_feature:\n        le = LabelEncoder()\n        le.fit(data_df[f_name])\n        label_encoders.append(le)\n        data_df[f_name] = le.transform(data_df[f_name])\n\n    sales_scaler = None\n    if isTrain:\n        sales_scaler = StandardScaler()\n        sales_values = data_df.sales.values.reshape(-1,1)\n        scaled = sales_scaler.fit_transform(sales_values)\n        data_df.sales = scaled\n    \n    return data_df, sales_scaler, label_encoders\n\ntrain_df = pd.read_csv(f'{PATH}train.csv',parse_dates=['date'])\n\ntrain_df, scaler, label_encoders = prepare_df(train_df)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7809b3d56a256522b4f24989e6051c0c82e7d6f","collapsed":true},"cell_type":"code","source":"# time_df = train_df.copy()\n# time_df[\"date\"] = time_df[\"date\"].values.astype('datetime64')\n# time_idx = time_df.set_index(\"date\")\n\n# y_time = time_idx['sales'].resample('MS').mean()\n\n# y_time.plot(figsize=(15, 6))\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1109211e7bc5ea68bbb46a27bc47e11e6cef33ae"},"cell_type":"markdown","source":"### Feature preparation\n\n- Label Encoding for categorical features\n- Standartization for continious features (only Elapsed in our case)\n"},{"metadata":{"trusted":true,"_uuid":"f98980eaba4980284fd5252d5a78a316a18d0e31","collapsed":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2be669e14ced1259709ab16a72b449f49809b4fe","collapsed":true},"cell_type":"code","source":"def data_for_model(data_df):\n    x_fit = []\n\n    for cat in cat_feature:\n        x_fit.append(data_df[cat].values)\n\n    for con in con_feature:\n        x_fit.append(data_df[con].values)\n        \n    return x_fit\n\ntrain_validation_ratio = 0.9\ntrain_size = int(train_validation_ratio * train_df.shape[0])\n\nx_train_df = train_df[:train_size]\nx_val_df = train_df[train_size:]\ny_train, y_val = train_df.sales[:train_size].values, train_df.sales[train_size:].values\n\nx_fit_train = data_for_model(x_train_df)\nx_fit_val = data_for_model(x_val_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3402221f45e916f9b251ca4242fb9090176bb6be"},"cell_type":"markdown","source":"#### Now we need define embedding space for our categorical features.\nGood rule of thumb for embedding space is: min(50, num_categories)) // 2 )"},{"metadata":{"trusted":true,"_uuid":"9a1f594bab7a8f9197ba8cd99f6dbcef7a66b598"},"cell_type":"code","source":"emb_space = [(len(le.classes_), min(25, len(le.classes_)) // 2 ) for idx, le in enumerate(label_encoders)]\nemb_space","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85572c0a3632e94dc7da1ddfe5ab4efe37a4c9db"},"cell_type":"markdown","source":"#### Building keras model\n- Each categorical feature goes thru embedding matrix\n- Each continues feature (only Elapsed) goes thru simple Dense layer for relu activataion\n- We add several dense layer and make singe linear output"},{"metadata":{"trusted":true,"_uuid":"f9b2685f420cf9bb88976867ed24900402a528fa","collapsed":true},"cell_type":"code","source":"model_inputs = []\nmodel_embeddings = []\n    \nfor input_dim, output_dim in emb_space:\n    i = Input(shape=(1,))\n    emb = Embedding(input_dim=input_dim, output_dim=output_dim)(i)\n    \n    model_inputs.append(i)\n    model_embeddings.append(emb)\n    \n    \ncon_outputs = []\nfor con in con_feature:\n    elaps_input = Input(shape=(1,))\n    elaps_output = Dense(10)(elaps_input) \n    #elaps_output = BatchNormalization()(elaps_output)\n    elaps_output = Activation(\"relu\")(elaps_output)\n    \n    elaps_output = Reshape(target_shape=(1,10))(elaps_output)\n\n    model_inputs.append(elaps_input)\n    con_outputs.append(elaps_output)\n\nmerge_embeddings = concatenate(model_embeddings, axis=-1)\nif len(con_outputs) > 1:\n    merge_con_output = concatenate(con_outputs)\nelse:\n    merge_con_output = con_outputs[0]\n\nmerge_embedding_cont = concatenate([merge_embeddings, merge_con_output])\nmerge_embedding_cont\n\noutput_tensor = Dense(1000, name=\"dense1024\")(merge_embedding_cont)\noutput_tensor = BatchNormalization()(output_tensor)\noutput_tensor = Activation('relu')(output_tensor)\n#output_tensor = Dropout(0.5)(output_tensor)\n\noutput_tensor = Dense(500, name=\"dense512\")(output_tensor)\noutput_tensor = BatchNormalization()(output_tensor)\noutput_tensor = Activation(\"relu\")(output_tensor)\n#output_tensor = Dropout(0.5)(output_tensor)\n\noutput_tensor = Dense(1, activation='linear', name=\"output\")(output_tensor)\n\noptimizer = Adam(lr=10e-3)\n\nnn_model = Model(inputs=model_inputs, outputs=output_tensor)\nnn_model.compile(loss=\"mean_absolute_error\", optimizer=optimizer, metrics=['mape'])\n\n\nreduceLr=ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, verbose=1)\ncheckpoint = ModelCheckpoint(\"nn_model.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')#val_mean_absolute_percentage_error\ncallbacks_list = [checkpoint, reduceLr]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d06aeb44425c5d9f0e4d17f1f36b872dc39506d"},"cell_type":"code","source":"history = nn_model.fit(x=x_fit_train, y=y_train.reshape(-1,1,1),\n                       validation_data=(x_fit_val, y_val.reshape(-1,1,1)),\n                       batch_size=1024, epochs=20, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4649ec7c0000d2bd0623cab323d80e3d7d4fdcc"},"cell_type":"markdown","source":"#### Get model Predictions\n\n- Prepare test data for model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"36f4ff2bf4d0d1e72ca3a2ababc592cca0c08174"},"cell_type":"code","source":"from keras.models import load_model\ntt_model = load_model('nn_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cdc7389e27fd233baf2219f549a8e69a99da0de","collapsed":true},"cell_type":"code","source":"test_df = pd.read_csv(f'{PATH}test.csv',parse_dates=['date']).drop(\"id\", axis=1)\ntest_df, _, _ = prepare_df(test_df, isTrain=False, shuffle=False)\nx_fit_test = data_for_model(test_df)\n\nscaled_preds = tt_model.predict(x=x_fit_test)\n\nscaled_predictions = tt_model.predict(x=x_fit_test)\ny_predictions = scaler.inverse_transform(scaled_preds)\ny_predictions = y_predictions.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bd6d5036408643c65599341e37f0029bffe8e69"},"cell_type":"markdown","source":"### Prepare submission file"},{"metadata":{"trusted":true,"_uuid":"0bd1e008c6abc2e221a4875486a40ca151611574","collapsed":true},"cell_type":"code","source":"submission_df = pd.DataFrame()\nsubmission_df[\"id\"] = pd.read_csv(f'{PATH}test.csv',parse_dates=['date'])[\"id\"]\nsubmission_df[\"sales\"] = y_predictions\nsubmission_df.to_csv('submission.csv',index=False)\n\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68d99b40809066c1c361e7b17c50cf029ed2f892","collapsed":true},"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink('submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb456d195c33f462ecddf3e6d5c376c86af31527","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94f7dedd8cf7e2ef340463ea3b05ce17d2379278"},"cell_type":"markdown","source":"#### Training history visualization"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c40df5bdfe7c52d8e56fe9e3716139806d4fe8ae"},"cell_type":"code","source":"# figure = plt.figure(figsize=(12, 10))\n# grid = plt.GridSpec(12, 12, wspace=4.5, hspace=0.1)\n\n# loss_plot = figure.add_subplot(grid[:5, :6])\n# mse_plot = figure.add_subplot(grid[:5, 6:])\n\n# loss_plot.plot(history.history['loss'])\n# loss_plot.plot(history.history['val_loss'])\n# loss_plot.set_xlabel('epoch')\n# loss_plot.set_ylabel('loss')\n# loss_plot.legend(['Train Loss', 'Validation Loss'], loc='best')\n\n# mse_plot.plot(history.history['mean_absolute_percentage_error'])\n# mse_plot.plot(history.history['val_mean_absolute_percentage_error'])\n# mse_plot.set_xlabel('epoch')\n# mse_plot.set_ylabel('mse')\n# mse_plot.legend(['Train mse', 'Validation mse'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}