{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"collapsed": false, "_cell_guid": "4306c3e3-093a-4316-a144-e07ee32079e9", "_execution_state": "idle", "_uuid": "822c18801d798c6491fe2017da165deb7456e06d"}, "source": "Hello guys we are a team of 3 girls learning to do machine learning. We have put this notebook public for insight on how to improve our models. Please know that the models we used are taken from other kaggle models in this thread. Thus, some parts are strongly inspired from these other models. We are now trying to twerk the code to make it our own and learn how we can improve these models. Big thanks to :\n\n[Data preprocessing && resampling][1]\n\n[Over+Under+SMOTE][2]\n\n[Model train + why not ROC][3]\n\n  [1]: https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now\n  [2]: https://www.kaggle.com/gargmanish/how-to-handle-imbalance-data-study-in-detail\n  [3]: https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "66d8dc05-d7da-4cf9-8021-5a4a3505da33", "_execution_state": "idle", "_uuid": "dd7851674c1134ec916e56a1eb302ad297c9f84e", "trusted": false}, "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import SVC # SVM\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import f1_score,confusion_matrix,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.model_selection import GridSearchCV # for tunnig hyper parameter it will use all combination of given parameters\nfrom sklearn.model_selection import RandomizedSearchCV # same for tunning hyper parameter but will use random combinations of parameters\nfrom sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n\n%matplotlib inline", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "d9b7d5fe-8192-4c5a-8b24-da0147140b74", "_execution_state": "idle", "_uuid": "de2bb47c7032d4c409acca9e1f45f26a5305e4f3", "trusted": false}, "source": "data = pd.read_csv(\"../input/creditcard.csv\",header = 0)\ndata.head()", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "42130d29-9f0d-4248-bdb9-1514fcf2d151", "_execution_state": "idle", "_uuid": "ffd654eccd998f4384cb56dce34ebbf27aacbb17", "trusted": false}, "source": "## Pre-processing the data\n## Normalizing the amount column\n\nfrom sklearn.preprocessing import StandardScaler\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\ndata = data.drop(['Time','Amount'],axis=1)\ndata.head()", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "d2405d3e-977a-4539-be51-aae0e4189a1d", "_execution_state": "idle", "_uuid": "24d39b64a751ee237c7c0ce371906503e1b8c020", "trusted": false}, "source": "## Since the data is largely imbalanced we need to resample the data such that the proportion/ratio between fraudulent and normal transactions are relativeley similar.\n\nx = data.loc[:, data.columns != 'Class']\ny = data.loc[:, data.columns == 'Class']", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "1b49cbe6-8f76-48ca-8789-1ed231fcf18c", "_execution_state": "idle", "_uuid": "8dd0e872c2e26996cccc20d4f3ab101335c37321", "trusted": false}, "source": "#UNDERSAMPLING\n# Number of fraudelent transaction in the existing data\nnumberOffraudulentTransaction = len(data[data.Class == 1])\nfraudIndices = np.array(data[data.Class == 1].index)\n\n# Picking the indices of the normal classes\nnormalIndices = data[data.Class == 0].index\n\n# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\nrandom_normal_indices = np.random.choice(normalIndices, numberOffraudulentTransaction, replace = False)\nrandom_normal_indices = np.array(random_normal_indices)\n", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "9289fae1-997b-4927-88c2-c3ea1db98f08", "_execution_state": "idle", "_uuid": "9d616ab4b30552e9ec4f8531005f66210c9e715e", "trusted": false}, "source": "#UNDERSAMPLING\n# Appending the 2 indices\nunder_sample_indices = np.concatenate([fraudIndices,random_normal_indices])\n\n# Under sample dataset\nunder_sample_data = data.iloc[under_sample_indices,:]\n\nx_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\ny_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']\n\n# Showing ratio\nprint(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\nprint(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\nprint(\"Total number of transactions in resampled data: \", len(under_sample_data))", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "2d29ce07-2b4b-4e9c-b7ba-0515db68b4c9", "_execution_state": "idle", "_uuid": "4bfbc48fa8e1291fe68252dff3d8351c4e42071b", "trusted": false}, "source": "#OVERSAMPLING\n## Splitting the data into Training,Validation and Test Set##\n## Test Set needs to be unused till the mere end##\nX_train, X_test, Y_train, Y_test = train_test_split(data,y, test_size=0.25, random_state=42)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size=0.25, random_state=42)\n# #Figuring out the ratio of normal transction and fraudelent transaction from training data# #\n\n\nnormal_tdata = X_train[X_train[\"Class\"]==0]\nprint(\"train data: length of normal data\",len(normal_tdata))\nfraud_tdata = X_train[X_train[\"Class\"]==1]\nprint(\"train data: length of fraud data\",len(fraud_tdata))\n## dataset for validation set ##\nnormal_vdata = X_val[X_val[\"Class\"]==0]\nprint(\"For Validation Set :length of normal data\",len(normal_vdata))\nfraud_vdata = X_val[X_val[\"Class\"]==1]\nprint(\"For Validation Set :length of fraud data\",len(fraud_vdata))\n\n#SMOTE\n#Since the data is highly imbalanced we use the sklearn package to balance out the data by introducing more fraudulent data ##\n#basically oversampling of data \nsm = SMOTE(random_state=12, ratio = 'auto', k_neighbors=5)\n#Possible ratios : minority, majority, not minority, all, auto\nx_train_res, y_train_res = sm.fit_sample(X_train, Y_train.values.ravel())\n\na = x_train_res[:,28]\nb= np.count_nonzero(a == 1)\nc= np.count_nonzero(a == 0)\nprint(\"length of oversampled data is \",len(x_train_res))\nprint(\"Number of normal transcation in oversampled data\",b)\nprint(\"No.of fraud transcation\",c)\nprint(\"Proportion of Normal data in oversampled data is \",c/len(x_train_res))\nprint(\"Proportion of fraud data in oversampled data is \",b/len(x_train_res))", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "65de5e15-4dcf-4da7-92ac-c7958dfc48b3", "_execution_state": "idle", "_uuid": "d1f2b23bcc564c63c3ac21ab42444e7da2545f67", "trusted": false}, "source": "print (\"UNDERSAMPLING\")\ndf = under_sample_data\n#train, validate, test = np.split(df.sample(frac=1), [int(.5*len(df)), int(.75*len(df))])\nx, x_test, y, y_test = train_test_split(x_undersample,y_undersample,test_size=0.25,train_size=0.75)\nx_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.33,train_size =0.66)\n\nscaler = StandardScaler()\n\n#Get mean+average and standardize to Z\nx_train = scaler.fit_transform (x_train)\n\n#Apply same transformation to hidden data\nx_cv = scaler.transform (x_cv)\nx_test = scaler.transform (x_test)\n\n# cross-validate needs to be here (after the splitting for proper X-V)", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "6d6a5d38-7a63-4112-a5f8-bfc8d9467bd7", "_execution_state": "idle", "_uuid": "4676135f2593511796b62e34333ef531c5f53771", "trusted": false}, "source": "#UNDERSAMPLING\n# My logic is regressing, guys!\n\nlogi = LogisticRegression(class_weight='balanced')\nmdl = logi.fit(x_train, y_train.values.ravel())\npredictions = logi.predict(x_test)\nprint(\"ACCURACY : \",accuracy_score(y_test, predictions))\nprint (\"CMATRIX : \")\nprint(confusion_matrix(y_test, predictions))\nprint (classification_report(y_test, predictions))\nprint(\"LOGICREGRESS\")\n\n'''\n# Neural network, captain!\nlr = LogisticRegression(C = 1, penalty = 'l1')\nlr.fit(x_train, y_train.values.ravel())\ny_pred_test_nn = lr.predict(x_test)\nprint(\"NEURALNETWORK\")\nprint(accuracy_score(y_test, predictions))\nprint (confusion_matrix(y_test, predictions))\nprint (classification_report(y_test, predictions))\n'''\n\n# I will not put the receiver operating characteristic, no sir!\n'''\n# Support vector machine, boss!\nprint(\"SVM\")\n\n#Other models doing 75%\nsvc = SVC(C=1, kernel='linear')\nsvc.fit (x_train,y_train.values.ravel())\nypredsvc = svc.predict (x_test)\nprint(confusion_matrix(y_test, ypredsvc))\nprint (classification_report(y_test, predictions))\nprint(f1_score(y_test, ypredsvc))\n'''\n\n# Random Forest stories, mate!\n\nclassif = RandomForestClassifier(n_estimators=100, n_jobs=2, min_samples_split=2, random_state=0)\n#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n#scores = cross_val_score(clf, X, y)\n#scores.mean()    \nclassif.fit(x_train, y_train.values.ravel())\ny_pred_test_rf = classif.predict(x_test)\nprint(\"RANDOMFOREST\")\nprint(confusion_matrix(y_test, y_pred_test_rf))\nprint(f1_score(y_test, y_pred_test_rf))\nprint(classification_report(y_test, y_pred_test_rf))\n\n\n# Decision Tree, baby!\nclassif2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\nclassif2.fit(x_train, y_train.values.ravel())\ny_pred_test_clf2 = classif2.predict(x_test)\n#scores = cross_val_score(clf, x_train, y_train)\n#scores.mean()\nprint(\"DECISIONTREE1\")\nprint(confusion_matrix(y_test, y_pred_test_clf2))\nprint(f1_score(y_test, y_pred_test_clf2))\nprint(classification_report(y_test, y_pred_test_clf2))\n\nclassif2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=15)\nclassif2.fit(x_train, y_train.values.ravel())\ny_pred_test_clf2 = classif2.predict(x_test)\n#scores = cross_val_score(clf, x_train, y_train)\n#scores.mean()\nprint(\"DECISIONTREE2\")\nprint(confusion_matrix(y_test, y_pred_test_clf2))\nprint(f1_score(y_test, y_pred_test_clf2))\nprint(classification_report(y_test, y_pred_test_clf2))\n\n\n\n# Extra Trees 4 social good, peepz!\nclassif3 = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n#scores = cross_val_score(clf, X, y)\n#scores.mean() > 0.999\nclassif3.fit(x_train, y_train.values.ravel())\ny_pred_test_clf2 = classif3.predict(x_test)\n#scores = cross_val_score(clf, x_train, y_train)\n#scores.mean()\nprint(\"DECISIONTREE3\")\nprint(confusion_matrix(y_test, y_pred_test_clf2))\nprint(f1_score(y_test, y_pred_test_clf2))\nprint(classification_report(y_test, y_pred_test_clf2))\n", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "2f12ffbe-947e-43d9-b4e8-c2a20e4f5d63", "_execution_state": "idle", "_uuid": "55244e4b10b594c7bfeb8a381e25cdd7bde9b527", "trusted": false}, "source": "\n\nprint (\"OVERSAMPLING\")\n\n#substract the min of the column divide by the max  for the whole column\n#also apply on test+validation\n#try without normalizing at all, nor stabilize\n\nlogi = LogisticRegression(class_weight='balanced')\nmdl = logi.fit(X_train, Y_train.values.ravel())\npredictions2 = logi.predict(X_test)\nprint(\"ACCURACY: \",accuracy_score(Y_test, predictions2))\nprint (\"CMATRIX: \")\nprint (confusion_matrix(Y_test, predictions2))\nprint (classification_report(Y_test, predictions2))\nprint(\"LOGICREGRESS2\")\n\n\nsvc = SVC(C=1, kernel='linear')\nsvc.fit (X_train,Y_train.values.ravel())\nypredsvc = svc.predict (X_test)\nscores = cross_val_score(svc, x_train_res, y_train_res)\nprint (\"MYSCORE\")\nprint (scores)\nprint(confusion_matrix(Y_test, ypredsvc))\nprint(f1_score(Y_test, ypredsvc))\nprint(\"SVM\")\n\n# Random Forest stories, mate!\n\nclassif = RandomForestClassifier(n_estimators=100, n_jobs=2, min_samples_split=2, random_state=0)\n#estimator = nb of free in forest, nbjobs = parallel calcul using cpu\n#scores = cross_val_score(clf, X, y)\n#scores.mean()    \nclassif.fit(X_train, Y_train.values.ravel())\nY_pred_test_rf = classif.predict(X_test)\nprint(\"RANDOMFOREST\")\nprint(confusion_matrix(Y_test, Y_pred_test_rf))\nprint(f1_score(Y_test, Y_pred_test_rf))\n\n\n# Decision Tree, baby!\nclassif2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\nclassif2.fit(X_train, Y_train.values.ravel())\nY_pred_test_clf2 = classif2.predict(X_test)\n#scores = cross_val_score(clf, x_train, y_train)\n#scores.mean()\nprint(\"DECISIONTREE\")\nprint(confusion_matrix(Y_test, Y_pred_test_clf2))\nprint(f1_score(Y_test, Y_pred_test_clf2))\n\n# Neural network, captain!\nlr = LogisticRegression(C = 5, penalty = 'l1')\nlr.fit(X_train, Y_train.values.ravel())\nY_pred_test_nn = lr.predict(X_test)\nprint(\"NEURALNETWORK1\")\nprint(confusion_matrix(Y_test, Y_pred_test_nn))\nprint(f1_score(Y_test, Y_pred_test_nn))\n\n\n# Extra Trees 4 social good, peepz!\nclassif3 = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n#scores = cross_val_score(clf, X, y)\n#scores.mean() > 0.999\n'''The main parameters to adjust when using these methods is n_estimators and max_features. \nThe larger n_estimators the better, but also the longer it  will take to compute. \nmax_feat is the size of the random subsets of features to consider when splitting a node. \nlower = greater the reduction of variance, but also the greater the increase in bias. \nEmpirical good default values are max_features=n_features for regression problems, \nand max_features=sqrt(n_features) for classification tasks (where n_features is the number of features \nin the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=1 \n(i.e., when fully developing the trees). The best parameter values should always be cross-validated. \nIn addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) \nwhile the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using \nbootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. \nThis can be enabled by setting oob_score=True.'''", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "6246dba3-8506-4249-bebc-d03aa844f8c7", "_execution_state": "idle", "_uuid": "096fa0ef8326130a8a71c0d433cc19eb3c922a28", "trusted": false}, "source": "tn, fp, fn,tp = confusion_matrix(predictions,y_test).ravel() \nSensitivity=tp/float((tp+fn))#Sensitivity \nprint (\"SENS\",Sensitivity)\n\nSpecificity=tn/float((tn+fp))#Specificity \nprint (\"SPEC\",Specificity)\n\nAccuracy= accuracy_score(predictions,y_test, normalize=True, sample_weight=None)\nprint (\"ACC\",Accuracy)\n\ntn, fp, fn,tp = confusion_matrix(predictions2,Y_test).ravel() \nSensitivity=tp/float((tp+fn))#Sensitivity \nprint (\"SENS\",Sensitivity)\n\nSpecificity=tn/float((tn+fp))#Specificity \nprint (\"SPEC\",Specificity)\n\nAccuracy= accuracy_score(predictions2,Y_test, normalize=True, sample_weight=None)\nprint (\"ACC\",Accuracy)", "execution_count": null, "cell_type": "code", "outputs": []}]}