{"cells":[{"metadata":{"_uuid":"ca1767a8fc3a9e168aa1cd414fad8460782fca91"},"cell_type":"markdown","source":"Reference: https://www.kaggle.com/gpreda/lanl-earthquake-eda-and-prediction"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"import gc\nimport os\nimport time\nimport logging\nimport datetime\nimport warnings\nimport numpy as np \nimport pandas as pd\nimport xgboost as xgb\nimport seaborn as sns\nfrom tqdm import tqdm\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.signal import hann\nimport matplotlib.pyplot as plt\nfrom scipy.signal import hilbert\nfrom scipy.signal import convolve\nfrom sklearn.svm import NuSVR, SVR\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89a386fb5528cc26aa4f7b86a47174e0aa64de59"},"cell_type":"markdown","source":"**Loading 25% of training data**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"percent_data = 25\ntotal_data_points = 629145480\ntrain = pd.read_csv('../input/train.csv', nrows=total_data_points*percent_data/100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9eca906143e4d4cd59f61613c5ea3b3270735a3c"},"cell_type":"code","source":"test = pd.read_csv('../input/test/seg_004cd2.csv')\nprint('Size of test data: ', len(test))\nprint(test.describe())\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"943e70b51459e2208fca455f1851dfa1a73316b2"},"cell_type":"code","source":"test_files = os.listdir(\"../input/test\")\nfig, ax = plt.subplots(4,1, figsize=(20,25))\n\nfor n in tqdm(range(4)):\n    seg = pd.read_csv(\"../input/test/\" + test_files[n])\n    ax[n].plot(seg.acoustic_data.values, c=\"r\")\n    ax[n].set_ylabel(\"Signal\")\n    ax[n].set_ylim([-300, 300])\n    ax[n].set_title(\"Test - {}\".format(test_files[n]));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22660b7d321a543ac8586151b6d60deec5f249cf"},"cell_type":"code","source":"print(train.dtypes)\npd.options.display.precision = 10\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f64a0b8ea70a0a2eb60d55753509718f753437b"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"335a18f24aed28a8ef7c2cbceebe298cd6f33947"},"cell_type":"code","source":"train_ad_sample_df = train['acoustic_data'][::50]\ntrain_ttf_sample_df = train['time_to_failure'][::50]\n\nfig, ax1 = plt.subplots(figsize=(12,8))\nplt.plot(train_ad_sample_df, color='r')\nplt.legend(['acoustic_data'], loc=[0.01, 0.95])\nax2 = ax1.twinx()\nplt.plot(train_ttf_sample_df, color='b')\nplt.legend(['time_to_failure'], loc=[0.01, 0.9])\nplt.grid(True)\n\ndel train_ad_sample_df\ndel train_ttf_sample_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b954c34a3aa019c245a80b11f2706b2becf816f7"},"cell_type":"code","source":"rows = 150000\nsegments = int(np.floor(train.shape[0] / rows))\nprint(\"Number of segments: \", segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d241c5e01b54480bd0b49f1457ab23106f2bc61"},"cell_type":"code","source":"train_X = pd.DataFrame(index=range(segments), dtype=np.float64)\ntrain_y = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa72a61d3dec904464268885106736fb19906078"},"cell_type":"code","source":"def create_features(seg_id, seg, X):\n    \n    xc = pd.Series(seg['acoustic_data'].values)\n    zc = np.fft.fft(xc)\n    \n    X.loc[seg_id, 'mean'] = xc.mean()\n    X.loc[seg_id, 'std'] = xc.std()\n    X.loc[seg_id, 'max'] = xc.max()\n    X.loc[seg_id, 'min'] = xc.min()\n    \n    #FFT transform values\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    \n    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n    X.loc[seg_id, 'Rstd'] = realFFT.std()\n    X.loc[seg_id, 'Rmax'] = realFFT.max()\n    X.loc[seg_id, 'Rmin'] = realFFT.min()\n    \n    X.loc[seg_id, 'Imag_mean'] = imagFFT.mean()\n    X.loc[seg_id, 'Imag_std'] = imagFFT.std()\n    X.loc[seg_id, 'Imag_max'] = imagFFT.max()\n    X.loc[seg_id, 'Imag_min'] = imagFFT.min()\n    \n    X.loc[seg_id, 'Rmean_last_5000'] = realFFT[-5000:].mean()\n    X.loc[seg_id, 'Rstd__last_5000'] = realFFT[-5000:].std()\n    X.loc[seg_id, 'Rmax_last_5000'] = realFFT[-5000:].max()\n    X.loc[seg_id, 'Rmin_last_5000'] = realFFT[-5000:].min()\n    X.loc[seg_id, 'Rmean_first_5000'] = realFFT[:5000].mean()\n    X.loc[seg_id, 'Rstd__first_5000'] = realFFT[:5000].std()\n    X.loc[seg_id, 'Rmax_first_5000'] = realFFT[:5000].max()\n    X.loc[seg_id, 'Rmin_first_5000'] = realFFT[:5000].min()\n    \n    X.loc[seg_id, 'Rmean_last_15000'] = realFFT[-15000:].mean()\n    X.loc[seg_id, 'Rstd_last_15000'] = realFFT[-15000:].std()\n    X.loc[seg_id, 'Rmax_last_15000'] = realFFT[-15000:].max()\n    X.loc[seg_id, 'Rmin_last_15000'] = realFFT[-15000:].min()\n    X.loc[seg_id, 'Rmean_first_15000'] = realFFT[:15000].mean()\n    X.loc[seg_id, 'Rstd_first_15000'] = realFFT[:15000].std()\n    X.loc[seg_id, 'Rmax_first_15000'] = realFFT[:15000].max()\n    X.loc[seg_id, 'Rmin_first_15000'] = realFFT[:15000].min()\n    \n    X.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(xc))\n    X.loc[seg_id, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(xc) / xc[:-1]))[0])\n    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n    X.loc[seg_id, 'abs_min'] = np.abs(xc).min()\n    \n    X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n    X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n    \n    X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n    X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n    \n    X.loc[seg_id, 'avg_first_50000'] = xc[:50000].mean()\n    X.loc[seg_id, 'avg_last_50000'] = xc[-50000:].mean()\n    \n    X.loc[seg_id, 'avg_first_10000'] = xc[:10000].mean()\n    X.loc[seg_id, 'avg_last_10000'] = xc[-10000:].mean()\n    \n    X.loc[seg_id, 'min_first_50000'] = xc[:50000].min()\n    X.loc[seg_id, 'min_last_50000'] = xc[-50000:].min()\n    \n    X.loc[seg_id, 'min_first_10000'] = xc[:10000].min()\n    X.loc[seg_id, 'min_last_10000'] = xc[-10000:].min()\n    \n    X.loc[seg_id, 'max_first_50000'] = xc[:50000].max()\n    X.loc[seg_id, 'max_last_50000'] = xc[-50000:].max()\n    \n    X.loc[seg_id, 'max_first_10000'] = xc[:10000].max()\n    X.loc[seg_id, 'max_last_10000'] = xc[-10000:].max()\n    \n    X.loc[seg_id, 'max_to_min'] = xc.max() / np.abs(xc.min())\n    X.loc[seg_id, 'max_to_min_diff'] = xc.max() - np.abs(xc.min())\n    X.loc[seg_id, 'count_big'] = len(xc[np.abs(xc) > 500])\n    X.loc[seg_id, 'sum'] = xc.sum()\n    \n    X.loc[seg_id, 'mean_change_rate_first_50000'] = np.mean(np.nonzero((np.diff(xc[:50000]) / xc[:50000][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_last_50000'] = np.mean(np.nonzero((np.diff(xc[-50000:]) / xc[-50000:][:-1]))[0])\n    \n    X.loc[seg_id, 'mean_change_rate_first_10000'] = np.mean(np.nonzero((np.diff(xc[:10000]) / xc[:10000][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_last_10000'] = np.mean(np.nonzero((np.diff(xc[-10000:]) / xc[-10000:][:-1]))[0])\n    \n    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n    X.loc[seg_id, 'q99'] = np.quantile(xc, 0.99)\n    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n    X.loc[seg_id, 'q01'] = np.quantile(xc, 0.01)\n    \n    X.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(xc), 0.99)\n    X.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(xc), 0.95)\n    X.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(xc), 0.05)\n    X.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(xc), 0.01)\n    \n    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n    X.loc[seg_id, 'abs_std'] = np.abs(xc).std()\n    \n    X.loc[seg_id, 'mad'] = xc.mad()\n    X.loc[seg_id, 'kurt'] = xc.kurtosis()\n    X.loc[seg_id, 'skew'] = xc.skew()\n    X.loc[seg_id, 'med'] = xc.median()\n    \n    X.loc[seg_id, 'Hilbert_mean'] = np.abs(hilbert(xc)).mean()\n    X.loc[seg_id, 'Hann_window_mean'] = (convolve(xc, hann(150), mode='same') / sum(hann(150))).mean()\n    X.loc[seg_id, 'Moving_average_700_mean'] = xc.rolling(window=700).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_1500_mean'] = xc.rolling(window=1500).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_3000_mean'] = xc.rolling(window=3000).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_6000_mean'] = xc.rolling(window=6000).mean().mean(skipna=True)\n    \n    ewma = pd.Series.ewm\n    X.loc[seg_id, 'exp_Moving_average_300_mean'] = ewma(xc, span=300).mean().mean(skipna=True)\n    X.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(xc, span=3000).mean().mean(skipna=True)\n    X.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(xc, span=6000).mean().mean(skipna=True)\n    \n    no_of_std = 2\n    X.loc[seg_id, 'MA_700MA_std_mean'] = xc.rolling(window=700).std().mean()\n    X.loc[seg_id,'MA_700MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X.loc[seg_id,'MA_700MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X.loc[seg_id, 'MA_400MA_std_mean'] = xc.rolling(window=400).std().mean()\n    X.loc[seg_id,'MA_400MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X.loc[seg_id,'MA_400MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X.loc[seg_id, 'MA_1000MA_std_mean'] = xc.rolling(window=1000).std().mean()\n    \n    X.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n    X.loc[seg_id, 'q999'] = np.quantile(xc,0.999)\n    X.loc[seg_id, 'q001'] = np.quantile(xc,0.001)\n    X.loc[seg_id, 'ave10'] = stats.trim_mean(xc, 0.1)\n    \n    for windows in [10, 100, 1000]:\n        x_roll_std = xc.rolling(windows).std().dropna().values\n        x_roll_mean = xc.rolling(windows).mean().dropna().values\n        \n        X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n        X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n        X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n        X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n        X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n        X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n        X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n        X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n        X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n        \n        X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n        X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n        X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n        X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n        X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n        X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n        X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n        X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n        X.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n        X.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d25895d1472c21bd791386ce78e1595ec64278a"},"cell_type":"code","source":"for seg_id in tqdm(range(segments)):\n    seg = train.iloc[seg_id*rows:seg_id*rows+rows]\n    create_features(seg_id, seg, train_X)\n    train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d3448a08e9fc17469ad9e9d6416385f4c69fac1"},"cell_type":"code","source":"print(\"New training shape: \", train_X.shape)\ntrain_X.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19e77628948d656571b68ae842439c989040fb82"},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(train_X)\nscaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)\nscaled_train_X.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5d685b213a0bf8cc78419acb54fe62c3a42a3e7"},"cell_type":"code","source":"#scaler = Normalizer()\n#scaler.fit(train_X)\n#scaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)\n#scaled_train_X.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3bedb56db2aaaeb10e6f28f6aa6257f28b62511"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\ntest_X = pd.DataFrame(columns=train_X.columns, dtype=np.float64, index=submission.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a66e20e835e7a9ebf9102631464b425a850e002d"},"cell_type":"code","source":"for seg_id in tqdm(test_X.index):\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    create_features(seg_id, seg, test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a62d29e7655398caa54993e8afe9ed8a83892366"},"cell_type":"code","source":"scaled_test_X = pd.DataFrame(scaler.transform(test_X), columns=test_X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64cfc5d50f8748bc184269f030783a04a3bfebb9"},"cell_type":"markdown","source":"SVR"},{"metadata":{"trusted":true,"_uuid":"9c755135a1adecf302bca4c127370d70f39ea441"},"cell_type":"code","source":"Cs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\ngammas = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ngrid_search = GridSearchCV(SVR(kernel='rbf', tol=0.01), param_grid, cv=5)\ngrid_search.fit(scaled_train_X, train_y)\nprint('Best CV Score:', grid_search.best_score_)\nprint('Best parameters: ', grid_search.best_params_)\nprint('Best estimator: ', grid_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72ecdc60db706626869778076f0c3add9fa58216"},"cell_type":"code","source":"predictions = grid_search.predict(scaled_test_X)\nprint(len(predictions))\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfa423519b2dd963a1544b9e31408a7a6310342a"},"cell_type":"markdown","source":"KernelRidge"},{"metadata":{"trusted":true,"_uuid":"8f09b768c1049e8bb5c8fa732d6b96326c797fbb"},"cell_type":"code","source":"param_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3],\n              \"gamma\": np.logspace(-2, 2, 5)}\ngrid_search = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), param_grid, cv=5)\ngrid_search.fit(scaled_train_X, train_y)\nprint('Best CV Score:', grid_search.best_score_)\nprint('Best parameters: ', grid_search.best_params_)\nprint('Best estimator: ', grid_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60333e2ff59fd226018c9ec8797cfbabaa6a20a2"},"cell_type":"code","source":"predictions = grid_search.predict(scaled_test_X)\nprint(len(predictions))\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa48fa43f68344fabd06b4c6a7e62c6c36be2642"},"cell_type":"markdown","source":"CatBoost Algorithm"},{"metadata":{"trusted":true,"_uuid":"86ef4d8ac580151db4c1edab4eea4c9386fe88f5"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(scaled_train_X, train_y, train_size=0.75, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ac1409f6e956f05c149470399a158c2119f8336"},"cell_type":"code","source":"model = CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\nmodel.fit(scaled_train_X, train_y, cat_features=None, eval_set=(X_validation, y_validation), plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ad85c33ee123772d45b4e6cb9186cd596fa0071"},"cell_type":"code","source":"predictions = grid_search.predict(scaled_test_X)\nprint(len(predictions))\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aec73cd7f39d268fa348430265f59b8052f2a134"},"cell_type":"markdown","source":"LightGBM"},{"metadata":{"trusted":true,"_uuid":"fc3251e0b6858c0c2a84af93fa36714c50f377a8"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(scaled_train_X, train_y, train_size=0.75, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76427c6bc2aa5ff49b203c6e677d34d43e7dd726"},"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_validation, y_validation)\n\nparams = {'boosting_type': 'gbdt',\n         'objective': 'regression',\n         'metric': {'l2', 'l1'},\n         'num_leaves': 31,\n         'learning_rate': 0.05,\n         'feature_fraction': 0.9,\n         'bagging_fraction': 0.8,\n         'bagging_freq': 5,\n         'verbose': 0}\ngbm = lgb.train(params, lgb_train, num_boost_round=100, valid_sets=lgb_eval, early_stopping_rounds=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"536fa76098369ac4eea8d6e8da9651a0916a3f0a"},"cell_type":"code","source":"predictions = gbm.predict(scaled_test_X, num_iteration=gbm.best_iteration)\nprint(len(predictions))\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"208359f23fccf5587a57ff949665cdea6f49f4d2"},"cell_type":"code","source":"submission.time_to_failure = predictions\nsubmission.to_csv('submission.csv', index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8832287e3c65db9207aee1c056f7f812aed238ee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}