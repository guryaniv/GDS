{"cells":[{"metadata":{"_uuid":"8667812f02f3089799b9ba8ad8086aa1dd86417e"},"cell_type":"markdown","source":"# Preliminaries\nI know, I know... I come up with some pretty terrible names for my kernels.\n\nSo, as you can imagine, I will be using the [GloVe](https://nlp.stanford.edu/projects/glove/) word embedding. This is just one of many different word embeddings typically used. Other popular choices include Word2Vec and [fastText](https://fasttext.cc/).\n\nSo, I will be passing everything through a convolutional neural network first. CNNs serve an extremely useful purpose; they compress everything, putting far less strain when our data passes through our next layers in our neural network. Despite becoming familiar with CNNs through image recognition, they are extremely valuable for natural language processing as well. See for instance [Yoon Kim's paper](https://arxiv.org/pdf/1408.5882.pdf) on the subject.\n\nNext, we will be passing through a GRU unit. These are a variation on the recurrent neural network (RNN) type architecture. These are pretty standard players at this point in text classification. Some fantastic resources to understanding this would be [Ian Goodfellow et al's 'Deep Learning'](https://www.deeplearningbook.org/), Alex Grave's 'Supervised Sequence Labelling with Recurrent Neural Networks' ([Goodread](https://www.goodreads.com/book/show/14642424-supervised-sequence-labelling-with-recurrent-neural-networks?ac=1&from_search=true)), or [this paper](https://arxiv.org/pdf/1308.0850.pdf) by Alex Grave.\n\nLastly, we are passing everything through a densely connected neural network layer. These provide a nice catch-all to classify the information gathered out from our RNN.\n\nA shoutout to several other kernels that were of some help making this kernel.\n* [A look at different embeddings](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings)\n* [LSTM is all you need! Well, maybe embeddings also.](https://www.kaggle.com/mihaskalic/lstm-is-all-you-need-well-maybe-embeddings-also)"},{"metadata":{"trusted":true,"_uuid":"91373d3bb67bec77dcd437bd950aab6b405929b9"},"cell_type":"code","source":"# Scipy Stacks\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split # To split our training set into training/validation sets.\nfrom sklearn import metrics\n\n# We are performing a sequential neural network.\nfrom keras.models import Sequential\n\n# Used to process text data.\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# What layers will be involved with our neural network.\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Activation, CuDNNGRU, CuDNNLSTM, Conv1D, MaxPooling1D, Bidirectional, GlobalMaxPool1D","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"505a8097e7744ee48c52477f1bccefe1d29b489e"},"cell_type":"markdown","source":"# Loading Data into Memory\nSo we need to load in our training data first. Let's start by importing important libraries."},{"metadata":{"trusted":true,"_uuid":"90563d537f0ccd031d5ccbb1af5baedeaa66fa80"},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffa5fa29d4ba4c3d196dc6c334a5a30a13ebeb8e"},"cell_type":"code","source":"df_train, df_val = train_test_split(df, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f3d6e8591a5336e2c73f619ee11fc76b48a18d2"},"cell_type":"code","source":"# some config values \nembed_size = 500 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 200 # max number of words in a question to use\n\n# fill up the missing values\nx_train = df_train[\"question_text\"].fillna(\"_na_\").values\nx_val = df_val[\"question_text\"].fillna(\"_na_\").values\n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(x_train))\nx_train = tokenizer.texts_to_sequences(x_train)\nx_val = tokenizer.texts_to_sequences(x_val)\n\n# Pad the sentences \nx_train = pad_sequences(x_train, maxlen=maxlen)\nx_val = pad_sequences(x_val, maxlen=maxlen)\n\n# Get the target values\ny_train = df_train['target'].values\ny_val = df_val['target'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0edf63215e7239968dc0fe6de0334537cfef289"},"cell_type":"markdown","source":"# GloVe Embedding\nNext, we need to load the GloVe embedding into memory. The file containing all the weights is already provided. This may take a while to load into memory, the entire file is barely above 2.0GB."},{"metadata":{"trusted":true,"_uuid":"31f343aaf9978804491388332951dade4e827e87"},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: \n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e46da5de21dc3d1a59cfeeb5aafc917639196f3"},"cell_type":"markdown","source":"# Constructing and Training our Model"},{"metadata":{"trusted":true,"_uuid":"f59761ab895aa99b8412af798c420b6fb66bf131"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, \n                    embed_size, \n                    weights=[embedding_matrix]))\nmodel.add(Bidirectional(CuDNNGRU(64, return_sequences=True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27a5b3ed876f4a81bbc651c6117da682e2b24932"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40bb91c09a840af0c9bbcb99f1cb7baa53a9b38d"},"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=512, epochs=2, validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a913b0837dcac7ab80c850b953ead1d1f5313516"},"cell_type":"markdown","source":"Now, usually I find plotting both training and validation loss and accuracy is very useful to get a high level view of training progress, as well as whatevr overfitting may be going on. In this case, I found overfitting really started to take hold after two epochs. This graph only becomes useful after around 5+ epochs."},{"metadata":{"trusted":true,"_uuid":"4813a47f25e36980cd3774d7830ddba3eab4fcb4"},"cell_type":"code","source":"'''\nimport matplotlib.pyplot as plt\n\ndef plot_scores(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(loss) + 1)\n\n    plt.figure(figsize=(20, 10))\n\n    plt.subplot(121)\n    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n    plt.title(\"Training and validation accuracy\")\n    plt.legend()\n\n    plt.subplot(122)\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title(\"Training and validation loss\")\n    plt.legend()\n    plt.show()\n    \nplot_scores(history)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d41d701bb6115747b6b175e2f5d0486cec3567ac"},"cell_type":"markdown","source":"# Testing"},{"metadata":{"trusted":true,"_uuid":"52f3d816b33e2d99b96a7f679355ac21718e9072"},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")\nx_test = df_test[\"question_text\"].fillna(\"_na_\").values\n\nx_test = tokenizer.texts_to_sequences(x_test)\n\nx_test = pad_sequences(x_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69b993f20db2aee727de5f45034cabf32c7ba569"},"cell_type":"code","source":"y_test = model.predict([x_test], batch_size=1024, verbose=1)\ny_test = (y_test > 0.5).astype(int)\ndf_test = pd.DataFrame({\"qid\": df_test[\"qid\"].values})\ndf_test['prediction'] = y_test\ndf_test.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}