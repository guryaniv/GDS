{"nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "version": "3.6.4", "mimetype": "text/x-python"}}, "cells": [{"source": ["In this notebook, I examine the provided data for Kaggle's Humpback Whale ID challenge. I also look at data augmentations in an attempt to inflate the size of the training dataset."], "metadata": {"_cell_guid": "68e3007d-74fa-4eca-855d-49c810e20009", "_uuid": "ca2c0018a6c9f16fadca8d1c131821a73978f318"}, "cell_type": "markdown"}, {"source": ["import math\n", "from collections import Counter\n", "\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import numpy as np\n", "from PIL import Image\n", "\n", "from tqdm import tqdm\n", "\n", "%matplotlib inline"], "outputs": [], "metadata": {"_cell_guid": "7814d047-2889-4011-8979-bd806aa7c43a", "_uuid": "2280846462c23119ef8dd2d4e3d8b285c28c6bfc", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["INPUT_DIR = '../input'"], "outputs": [], "metadata": {"_cell_guid": "ebf109a9-6afe-49e3-8f92-dbb77a76f04c", "_uuid": "8ed38085d2a61a9df3b7f72346cbfc89bcd19a1a", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["def plot_images_for_filenames(filenames, labels, rows=4):\n", "    imgs = [plt.imread(f'{INPUT_DIR}/train/{filename}') for filename in filenames]\n", "    \n", "    return plot_images(imgs, labels, rows)\n", "    \n", "        \n", "def plot_images(imgs, labels, rows=4):\n", "    # Set figure to 13 inches x 8 inches\n", "    figure = plt.figure(figsize=(13, 8))\n", "\n", "    cols = len(imgs) // rows + 1\n", "\n", "    for i in range(len(imgs)):\n", "        subplot = figure.add_subplot(rows, cols, i + 1)\n", "        subplot.axis('Off')\n", "        if labels:\n", "            subplot.set_title(labels[i], fontsize=16)\n", "        plt.imshow(imgs[i], cmap='gray')"], "outputs": [], "metadata": {"_cell_guid": "173fc1a5-0518-420d-bc51-e87685767274", "_uuid": "07fd08f0718cb78959a5c43b3560a818c5d3f490", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["np.random.seed(42)"], "outputs": [], "metadata": {"_cell_guid": "4e68b8f4-2326-4941-b2c5-672d4cb7ec72", "_uuid": "065c3d9fca35b6bd1ca6f2363faf807cf4d9be33", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["## Exploring the dataset"], "metadata": {"_cell_guid": "a500ad3e-5845-4f42-8a90-20a9a78d2119", "_uuid": "e4997792aa81648de39276449fb0047fa730b7cb"}, "cell_type": "markdown"}, {"source": ["train_df = pd.read_csv('../input/train.csv')\n", "train_df.head()"], "outputs": [], "metadata": {"_cell_guid": "aebeb9b5-205b-4d74-8e48-326767b9cbed", "_uuid": "d16ff05d95bcb532e90609d249fedd5e998001b2"}, "execution_count": null, "cell_type": "code"}, {"source": ["Let's plot a couple of images at random."], "metadata": {"_cell_guid": "a78faf22-9a34-4ff2-a86b-5b8aa42a71dd", "_uuid": "03ba5502241770a53367e16df870e2278232aa5b"}, "cell_type": "markdown"}, {"source": ["rand_rows = train_df.sample(frac=1.)[:20]\n", "imgs = list(rand_rows['Image'])\n", "labels = list(rand_rows['Id'])\n", "\n", "plot_images_for_filenames(imgs, labels)"], "outputs": [], "metadata": {"_cell_guid": "bc189b65-5aa9-4d1a-a8cd-9b314027abc0", "_uuid": "eb84095fd97943cb98cb83d468d70636f1e3596e"}, "execution_count": null, "cell_type": "code"}, {"source": ["The competition states that it's hard because: \"there are only a few examples for each of 3,000+ whale ids\", so let's take a look at the breakdown of number of image per category."], "metadata": {"_cell_guid": "fdf8c7e3-8f9d-4399-a187-c72add92b8c1", "_uuid": "c4a48b849c090d32da291b54028319973331d3bd"}, "cell_type": "markdown"}, {"source": ["num_categories = len(train_df['Id'].unique())\n", "     \n", "print(f'Number of categories: {num_categories}')"], "outputs": [], "metadata": {"_cell_guid": "0de4233b-919e-472d-b203-661030310276", "_uuid": "b1bc00295ffb2e4601c950aef9add3bb23d8029d"}, "execution_count": null, "cell_type": "code"}, {"source": ["There appear to be too many categories to graph count by category, so let's instead graph the number of categories by the number of images in the category."], "metadata": {"_cell_guid": "02c2a524-5c28-4023-a0aa-8df9482a7a46", "_uuid": "cffd2dd1e02a992d6c544fa724008bdeca785b0e"}, "cell_type": "markdown"}, {"source": ["size_buckets = Counter(train_df['Id'].value_counts().values)"], "outputs": [], "metadata": {"_cell_guid": "86a8d12b-1e7f-4bae-abf4-302061efdf9a", "_uuid": "083753550c9db61d481e750e85faf744289da6a7", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["plt.figure(figsize=(10, 6))\n", "\n", "plt.bar(range(len(size_buckets)), list(size_buckets.values())[::-1], align='center')\n", "plt.xticks(range(len(size_buckets)), list(size_buckets.keys())[::-1])\n", "plt.title(\"Num of categories by images in the training set\")\n", "\n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "36634bb3-ee2f-44ca-b9ee-bed3ce6a3c72", "_uuid": "041dd6d3e2398133b7639d2e84b89ce6ec077690"}, "execution_count": null, "cell_type": "code"}, {"source": ["As we can see, the vast majority of classes only have a single image in them. This is going to make predictions very difficult for most conventional image classification models."], "metadata": {"_cell_guid": "a06bd89f-f99f-4640-b7ac-35324bf470dc", "_uuid": "7ef86491f073f35d740e96988d87238097329bbe"}, "cell_type": "markdown"}, {"source": ["train_df['Id'].value_counts().head(3)"], "outputs": [], "metadata": {"_cell_guid": "bbdd85b6-0c80-434b-bf1f-b46c835b5daa", "_uuid": "9887c3f282a81dc26f5c2091e7f293a45c923f61"}, "execution_count": null, "cell_type": "code"}, {"source": ["total = len(train_df['Id'])\n", "print(f'Total images in training set {total}')"], "outputs": [], "metadata": {"_cell_guid": "4b823f46-390f-482b-97cc-6c3cf21a470a", "_uuid": "79867633da0e28dcc3083cfdd96e4165e70387d6"}, "execution_count": null, "cell_type": "code"}, {"source": ["New whale is the biggest category with 810, followed by `w_1287fbc`. New whale, I believe, is any whale that isn't in scientist's database. Since we can pick 5 potential labels per id, it's probably going to make sense to always include new_whale in our prediction set, since there's always an 8.2% change that's the right one. Let's take a look at one of the classes, to get a sense what flute looks like from the same whale."], "metadata": {"_cell_guid": "f2bf7d22-d501-4c2d-b703-d1548e058751", "_uuid": "33595e5c0cab6486821b17b5e3467c1c6e0b1136"}, "cell_type": "markdown"}, {"source": ["w_1287fbc = train_df[train_df['Id'] == 'w_1287fbc']\n", "plot_images_for_filenames(list(w_1287fbc['Image']), None, rows=9)"], "outputs": [], "metadata": {"_cell_guid": "48750e6f-9318-4e3c-86f6-172847c65df6", "_uuid": "537e3c1402758539298b2b940cb770a99e1b3773"}, "execution_count": null, "cell_type": "code"}, {"source": ["w_98baff9 = train_df[train_df['Id'] == 'w_98baff9']\n", "plot_images_for_filenames(list(w_98baff9['Image']), None, rows=9)"], "outputs": [], "metadata": {"_cell_guid": "ed338ffd-25a6-4ab1-b8fb-748bb13f11c3", "_uuid": "be40e8b6223994263f584e8707f76aeecb7f9cbf"}, "execution_count": null, "cell_type": "code"}, {"source": ["It's very difficult to build a validation set when most classes only have 1 image, so my thinking is to perform some aggressive data augmentation on the classes with < 10 images before creating a train/validation split. Let's take a look at a few examples of whales with only one example."], "metadata": {"_cell_guid": "39f82e4b-ee88-4b14-b5d5-0ed7c32c5fe3", "_uuid": "1083103782ff0b9f484887b121c56f0cfe5642cf"}, "cell_type": "markdown"}, {"source": ["one_image_ids = train_df['Id'].value_counts().tail(8).keys()\n", "one_image_filenames = []\n", "labels = []\n", "for i in one_image_ids:\n", "    one_image_filenames.extend(list(train_df[train_df['Id'] == i]['Image']))\n", "    labels.append(i)\n", "    \n", "plot_images_for_filenames(one_image_filenames, labels, rows=3)"], "outputs": [], "metadata": {"_cell_guid": "fbf4007f-dcdb-4f14-8668-17fca227bd49", "_uuid": "4f2a784bfc07b4245540d3dd1454e106595012fd"}, "execution_count": null, "cell_type": "code"}, {"source": ["From these small sample sizes, it seems like > 50% of images are black and white, suggesting that a good initial augementation might be to just convert colour images to greyscale and add to the training set. Let's confirm that by looking at a sample of the images."], "metadata": {"_cell_guid": "c0579d91-ec02-4f99-a0bd-8041d9752ad7", "_uuid": "b893356f7e397e38970fe4bdf2bcea18f8559690"}, "cell_type": "markdown"}, {"source": ["def is_grey_scale(img_path):\n", "    \"\"\"Thanks to https://stackoverflow.com/questions/23660929/how-to-check-whether-a-jpeg-image-is-color-or-gray-scale-using-only-python-stdli\"\"\"\n", "    im = Image.open(img_path).convert('RGB')\n", "    w,h = im.size\n", "    for i in range(w):\n", "        for j in range(h):\n", "            r,g,b = im.getpixel((i,j))\n", "            if r != g != b: return False\n", "    return True"], "outputs": [], "metadata": {"_cell_guid": "7c9c7f0c-c3d1-4b1f-8e82-ba39f29b5a42", "_uuid": "fce8a17ebaf701f3754bbe569e60ded956d9d6f1", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["is_grey = [is_grey_scale(f'{INPUT_DIR}/train/{i}') for i in train_df['Image'].sample(frac=0.1)]\n", "grey_perc = round(sum([i for i in is_grey]) / len([i for i in is_grey]) * 100, 2)\n", "print(f\"% of grey images: {grey_perc}\")"], "outputs": [], "metadata": {"_cell_guid": "2e21b5d4-d359-44c6-adff-2f6d2f2ef3df", "_uuid": "f58e03b107e675bfb710656d3025deee1985fb05"}, "execution_count": null, "cell_type": "code"}, {"source": ["It might also be worth capturing the size of the images so we can get a sense of what we're dealing with."], "metadata": {"_cell_guid": "4af57953-8c0d-4a12-97e0-0b7962110499", "_uuid": "1dbd886cada07fd6a29cb04f04289e7b9077f20b"}, "cell_type": "markdown"}, {"source": ["img_sizes = Counter([Image.open(f'{INPUT_DIR}/train/{i}').size for i in train_df['Image']])\n", "\n", "size, freq = zip(*Counter({i: v for i, v in img_sizes.items() if v > 1}).most_common(20))\n", "\n", "plt.figure(figsize=(10, 6))\n", "\n", "plt.bar(range(len(freq)), list(freq), align='center')\n", "plt.xticks(range(len(size)), list(size), rotation=70)\n", "plt.title(\"Image size frequencies (where freq > 1)\")\n", "\n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "30bf3795-ae23-49e5-94c2-8e1489b47eb6", "_uuid": "f81f0dbe8bbe9e16a56dd9dc96235e726beaf5da"}, "execution_count": null, "cell_type": "code"}, {"source": ["## Data Augmentation"], "metadata": {"_cell_guid": "0011e408-e67c-4865-9a0b-fa5f8c9fc107", "_uuid": "0eacf56cebac3cce1d69e62f7fd4d937946a5322"}, "cell_type": "markdown"}, {"source": ["from keras.preprocessing.image import (\n", "    random_rotation, random_shift, random_shear, random_zoom,\n", "    random_channel_shift, transform_matrix_offset_center, img_to_array)"], "outputs": [], "metadata": {"_cell_guid": "d99284b0-7b8e-40da-a927-03221100d80e", "_uuid": "37155e819bd4e4880d012099298b1dce909fb049"}, "execution_count": null, "cell_type": "code"}, {"source": ["img = Image.open(f'{INPUT_DIR}/train/ff38054f.jpg')"], "outputs": [], "metadata": {"_cell_guid": "7fcf6d52-184e-4267-91fe-d03fd4c186dd", "_uuid": "bbc9c306f4683b8aac774a4d7e6a8054516b63f5", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["img_arr = img_to_array(img)"], "outputs": [], "metadata": {"_cell_guid": "18ad4701-f6c9-464a-9b5b-12a5bdce74e2", "_uuid": "82a3fd60b2406c136bb900e4cbae7aba2845123b", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["plt.imshow(img)"], "outputs": [], "metadata": {"_cell_guid": "23a375a2-a828-4c40-9de6-3271596f49a2", "_uuid": "b32fe58ca545eb261d68ee7551216dfad304b844"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Random rotation"], "metadata": {"_cell_guid": "46527baf-bf7a-4711-988d-51a9466ebe84", "_uuid": "cb4644fffdcd499661bcfffad13a065003b86ce2"}, "cell_type": "markdown"}, {"source": ["imgs = [\n", "    random_rotation(img_arr, 30, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest') * 255\n", "    for _ in range(5)]\n", "plot_images(imgs, None, rows=1)"], "outputs": [], "metadata": {"_cell_guid": "fff831e2-c6cc-42bb-8491-42f10212d02f", "_uuid": "84f4cfbe5cfa3a91b966b26c4a8e34254a831851"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Random shift"], "metadata": {"_cell_guid": "c44ea228-8515-49e3-9e95-85aa49c06889", "_uuid": "edc31413fb6eb1c7c516d2213f1810c29af0a454"}, "cell_type": "markdown"}, {"source": ["imgs = [\n", "    random_shift(img_arr, wrg=0.1, hrg=0.3, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest') * 255\n", "    for _ in range(5)]\n", "plot_images(imgs, None, rows=1)"], "outputs": [], "metadata": {"_cell_guid": "64fe5ad9-a875-4ceb-bf09-44a735afbc5c", "_uuid": "e3a05219b500ab559d39889f41620063e233868c"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Random shear"], "metadata": {"_cell_guid": "9cda0bc7-9db8-4362-898b-3d4f5a96fe71", "_uuid": "e73cfed7901c96b7edb5bb7210a312bf5f91402f"}, "cell_type": "markdown"}, {"source": ["imgs = [\n", "    random_shear(img_arr, intensity=0.4, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest') * 255\n", "    for _ in range(5)]\n", "plot_images(imgs, None, rows=1)"], "outputs": [], "metadata": {"_cell_guid": "38dcf7db-1759-4f57-8d12-0dff459a69e4", "_uuid": "480a6983beca719cd8b86ec152d08df2928d8036"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Random zoom"], "metadata": {"_cell_guid": "5c211075-0a69-407a-badc-e5ae36e31542", "_uuid": "9dd3f4f1da2d61488e2d2010f634ebf0836984f9"}, "cell_type": "markdown"}, {"source": ["imgs = [\n", "    random_zoom(img_arr, zoom_range=(1.5, 0.7), row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest') * 255\n", "    for _ in range(5)]\n", "plot_images(imgs, None, rows=1)"], "outputs": [], "metadata": {"_cell_guid": "d01ceafa-9390-4112-bae1-290a3faa3531", "_uuid": "9b97aad967b67a245a210d4a40bff0fd1d369682"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Grey scale\n", "\n", "We want to ensure that all colour images also have a grey scale version."], "metadata": {"_cell_guid": "211543b3-2821-48b1-b0e4-4b4d65ee5969", "_uuid": "dcfdc3f6a473f8ff2a885ddedbe9de0a1b8e8c31"}, "cell_type": "markdown"}, {"source": ["import random\n", "\n", "def random_greyscale(img, p):\n", "    if random.random() < p:\n", "        return np.dot(img[...,:3], [0.299, 0.587, 0.114])\n", "    \n", "    return img\n", "\n", "imgs = [\n", "    random_greyscale(img_arr, 0.5) * 255\n", "    for _ in range(5)]\n", "\n", "plot_images(imgs, None, rows=1)"], "outputs": [], "metadata": {"_cell_guid": "92867723-df40-4bb1-97c5-b2e8bcbe537a", "_uuid": "df6228eb8000546adf514fc5f6037fc576343fcd"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Flips\n", "\n", "Usually for side-on image sets like this we'd include a veritical flip, however, in this case the veritical alignment is requirement to differentiate between flutes, so I'll leave it out."], "metadata": {"_cell_guid": "ec36be6c-f2a5-4851-bdad-1453fae0ad0c", "_uuid": "9148cdfdfe9253e94ddbd87d959e6e1f749c9bd7"}, "cell_type": "markdown"}, {"source": ["### All together\n", "\n", "Going to create an augmentation pipeline which will combine all the augs for a single predictions."], "metadata": {}, "cell_type": "markdown"}, {"source": ["def augmentation_pipeline(img_arr):\n", "    img_arr = random_rotation(img_arr, 18, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n", "    img_arr = random_shear(img_arr, intensity=0.4, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n", "    img_arr = random_zoom(img_arr, zoom_range=(0.9, 2.0), row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n", "    img_arr = random_greyscale(img_arr, 0.4)\n", "\n", "    return img_arr"], "outputs": [], "metadata": {"_cell_guid": "c51a5267-c05b-45c6-8139-49eeda430ce0", "_uuid": "8dcfa2787fe7fc87d14e43aa31085eabf01158ee", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["imgs = [augmentation_pipeline(img_arr) * 255 for _ in range(5)]\n", "plot_images(imgs, None, rows=1)"], "outputs": [], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"source": [], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}]}