{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"version": "3.6.3", "file_extension": ".py", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python", "nbconvert_exporter": "python"}}, "nbformat": 4, "cells": [{"metadata": {"collapsed": true, "_cell_guid": "6d237f6e-f604-4eb2-a259-f94e1039ef84", "_uuid": "a90cba521db119338e6c0daca1e260d1c4aeb28a"}, "cell_type": "code", "execution_count": null, "source": ["# Import the libraries\n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import seaborn as sns\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n"], "outputs": []}, {"metadata": {"_cell_guid": "45315b9a-1f92-4b40-a5a4-3fd11e533937", "_uuid": "119535216330d30086167de4cccf21929699362d"}, "cell_type": "markdown", "source": ["# 1. Simple EDA"]}, {"metadata": {"collapsed": true, "_cell_guid": "7fc6da04-d57e-42c0-b11d-7b0f0fcc6a38", "_uuid": "8c74b89ff09bc8fccbdfe31745dfa80140dc764d"}, "cell_type": "code", "execution_count": null, "source": ["# Import the dataset\n", "data = pd.read_csv(\"../input/creditcard.csv\")\n", "data.head()"], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "04ef39bf-c5e2-4512-a192-4880a3d30fca", "_uuid": "97f82e8b43439ba2fb4f7c91b970013624acaeab"}, "cell_type": "code", "execution_count": null, "source": ["# PCA yields the directions (principal components) that maximize the variance of the data\n", "# V1-V28 are from PCA processing, should be uncorrelated\n", "\n", "# Q1: how to get the PCA analysis? feature selection ??  \n", "\n", "#Plotting a heatmap to visualize the correlation between the variables\n", "sns.heatmap(data.corr())"], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "40bc3296-5135-424e-ac77-c744687d3d0b", "_uuid": "8e6f0ecb92491ab205a0a561c580f16b61c77141"}, "cell_type": "code", "execution_count": null, "source": ["# As mentioned in the project, the data is imblaslanced, we can check the class distributions\n", "sns.countplot(\"Class\", data=data)"], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "caabd9f8-cd41-4f0b-8076-4bfa5e66c5b5", "_uuid": "eb87e535dd91f7271dc1c2325596d0e412e83d56"}, "cell_type": "code", "execution_count": null, "source": ["# for all feature, only amount is not scaled, so we can take a look at the distribution\n", "# maybe the amount distribution is quite different between fraud vs non-fraud transaction\n", "fraud_transacation = data[data[\"Class\"]==1]\n", "non_fraud_transacation= data[data[\"Class\"]==0]\n", "plt.figure(figsize=(10,6))\n", "plt.subplot(121)\n", "fraud_transacation.Amount.plot.hist(title=\"Fraud Transacation\")\n", "plt.subplot(122)\n", "non_fraud_transacation.Amount.plot.hist(title=\"Non-Fraud Transaction\")\n", "# after the plot, we can see there is no clear difference between the two classes"], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "0affdcf9-8ae5-47e1-870a-cb82934d9b4e", "_uuid": "e68212685bc246158c1c39f0fecad29249e4d380"}, "cell_type": "code", "execution_count": null, "source": ["# above plots show that most transactions are below 2.5k amount, so we can focus on the region\n", "fraud_transacation = data[data[\"Class\"]==1]\n", "non_fraud_transacation= data[data[\"Class\"]==0]\n", "plt.figure(figsize=(10,6))\n", "plt.subplot(121)\n", "fraud_transacation[fraud_transacation[\"Amount\"] <= 2500].Amount.plot.hist(title=\"Fraud Transacation\")\n", "plt.subplot(122)\n", "non_fraud_transacation[non_fraud_transacation[\"Amount\"] <= 2500].Amount.plot.hist(title=\"Non-Fraud Transaction\")"], "outputs": []}, {"metadata": {"_cell_guid": "e0d1598a-1b0c-4956-86d0-ae248b967d50", "_uuid": "c405187062818da65929ab941b80920a986c79a7"}, "cell_type": "markdown", "source": ["# 2. Resample the data\n", "* There are several methods to resample imbalanced data: under-sampling, over-sampling, here we will use under-sampling \n", "* If there are N samples of majority class (non-fraud), and n2 samples of minority class (fraud), we will randomly select  n1 (n1=n2) from N majority class, so create 50%-50% dataset\n", "* If we don't do the resample, most prediction will be as majority class"]}, {"metadata": {"_cell_guid": "1d864b97-d9b0-4f54-b3d5-4be08864ed61", "_uuid": "07939bacec65ec620c31a0524bae7c7edec692e2"}, "cell_type": "markdown", "source": ["## 2.1 Feature scaling\n", "* Before we process the resampling, we need to normalize the \"amount\" data"]}, {"metadata": {"collapsed": true, "_cell_guid": "92cf1471-e421-48e7-b1b9-1ecb11aa4925", "_uuid": "eaa9226d28afc3449f0123d1ce7747adfbd90607"}, "cell_type": "code", "execution_count": null, "source": ["from sklearn.preprocessing import StandardScaler\n", "\n", "data[\"Normalized Amount\"] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n", "data.drop([\"Time\",\"Amount\"],axis=1,inplace=True)\n", "data.head()"], "outputs": []}, {"metadata": {"_cell_guid": "a008aa97-970f-4399-bf89-dafbc6357fc7", "_uuid": "1fe3362b18d18befb510eec3ba43010886168ef4"}, "cell_type": "markdown", "source": ["## 2.2 Get the train and test data-set, with and without sampling"]}, {"metadata": {"_cell_guid": "40d38852-89dd-47c2-91be-36af357fcb94", "_uuid": "7033dc837074602343ede68e70323d2501306769"}, "cell_type": "markdown", "source": ["### 2.2.1 Train - Test data split without resampling"]}, {"metadata": {"collapsed": true, "_cell_guid": "3eb3ab04-b779-4335-b267-d5322eab1e8c", "_uuid": "ca3fff1e41c2bf7b5f71c68fed398a71ba1e2645"}, "cell_type": "code", "execution_count": null, "source": ["X = data.iloc[:, data.columns != 'Class'].values\n", "y = data.iloc[:, data.columns == 'Class'].values"], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "c3ac4f51-6de0-4f29-8fcb-33d47ba1a841", "_uuid": "127b817b7ba93420b8f0506752ddc1c460a1dd2a"}, "cell_type": "code", "execution_count": null, "source": ["# Splitting the dataset into the Training set and Test set\n", "from sklearn.model_selection import train_test_split\n", "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n", "\n", "print(\"Original number transactions train dataset: \", len(X_train))\n", "print(\"Original number transactions test dataset: \", len(X_test))\n", "print(\"Total number of transactions: \", len(X_train)+len(X_test))"], "outputs": []}, {"metadata": {"_cell_guid": "3e38e7cb-d3cb-45bd-b769-d38bfe71d63b", "_uuid": "a798e71686ad5f96db92f8212a619389c44632ec"}, "cell_type": "markdown", "source": ["### 2.2.1 Train - Test data split with resampling"]}, {"metadata": {"collapsed": true, "_cell_guid": "5e74379d-3503-4614-a58d-8a86257651aa", "_uuid": "50675debd49205e137d3bab98a78b2d4e111bcba"}, "cell_type": "code", "execution_count": null, "source": ["# Number of data points in the minority class\n", "number_records_fraud = len(data[data.Class == 1])\n", "fraud_indices = np.array(data[data.Class == 1].index)\n", "\n", "# Picking the indices of the normal classes\n", "normal_indices = data[data.Class == 0].index\n", "\n", "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n", "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n", "random_normal_indices = np.array(random_normal_indices)\n", "\n", "# Appending the 2 indices\n", "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n", "\n", "# Under sample dataset\n", "under_sample_data = data.iloc[under_sample_indices,:]\n", "\n", "X_undersample = under_sample_data.iloc[:, under_sample_data.columns != 'Class']\n", "y_undersample = under_sample_data.iloc[:, under_sample_data.columns == 'Class']\n", "\n", "# Showing ratio\n", "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n", "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n", "print(\"Total number of transactions in resampled data: \", len(under_sample_data))\n", "\n", "# Undersampled dataset\n", "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n", "                                                                                                   ,y_undersample\n", "                                                                                                   ,test_size = 0.3\n", "                                                                                                   ,random_state = 0)\n", "print(\"\")\n", "print(\"Number transactions train dataset: \", len(X_train_undersample))\n", "print(\"Number transactions test dataset: \", len(X_test_undersample))\n", "print(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))"], "outputs": []}, {"metadata": {"_cell_guid": "9ca6b5ac-9257-4ca0-811f-3d1198f2d3aa", "_uuid": "e73d58de50e50b90a2d75342123ca754afab3e42"}, "cell_type": "markdown", "source": ["# 3. Logistic Regression"]}, {"metadata": {"_cell_guid": "e79ab625-fd0e-4792-8b7f-d1a18aa54b75", "_uuid": "a3da1f122e7104b7cb08d698a55a09038838d5da"}, "cell_type": "markdown", "source": ["## 3.1 Choose the right parameter\n", "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n", " ### 3.1.1 Three different metrics: Accuracy, Precision, and Recall (in a confusion matrix)\n", " * Accuracy = (TP+TN)/total \n", " * Precision = TP/(TP+FP)\n", " * Recall = TP/(TP+FN)  : most interesting since it trys to capture the most fraudulent transactions\n", " * notation as True_Positive (TP) and so on\n"]}, {"metadata": {"_cell_guid": "a2513733-d14b-43df-a9eb-aa7afb81ec94", "_uuid": "68e8e4377fa42db3131b5e95e16ac3900c414ecd"}, "cell_type": "markdown", "source": ["### 3.1.2 Use KFold method to get the best C (Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization)\n", "* Need to understand this C and usage in SVM??"]}, {"metadata": {"collapsed": true, "_cell_guid": "ffb482ba-ff0a-4b92-8a3e-6e38688a57f7", "_uuid": "d09ea81154ccc64743ae3d3cdf6cc0ef51c6cc42"}, "cell_type": "code", "execution_count": null, "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn.cross_validation import KFold, cross_val_score\n", "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report "], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "b70418f6-baee-464e-8b81-9ff48ce19183", "_uuid": "7863fce06254d46803045ee92ea1752f02e17227"}, "cell_type": "code", "execution_count": null, "source": ["def printing_Kfold_scores(x_train_data,y_train_data):\n", "    fold = KFold(len(y_train_data),5,shuffle=False) \n", "\n", "    # Different C parameters\n", "    c_param_range = [0.01,0.1,1,10,100]\n", "\n", "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n", "    results_table['C_parameter'] = c_param_range\n", "\n", "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n", "    j = 0\n", "    for c_param in c_param_range:\n", "        print('-------------------------------------------')\n", "        print('C parameter: ', c_param)\n", "        print('-------------------------------------------')\n", "        print('')\n", "\n", "        recall_accs = []\n", "        for iteration, indices in enumerate(fold,start=1):\n", "\n", "            # Call the logistic regression model with a certain C parameter\n", "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n", "\n", "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n", "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n", "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n", "\n", "            # Predict values using the test indices in the training data\n", "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n", "\n", "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n", "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n", "            recall_accs.append(recall_acc)\n", "            print('Iteration ', iteration,': recall score = ', recall_acc)\n", "\n", "        # The mean value of those recall scores is the metric we want to save and get hold of.\n", "        results_table.ix[j,'Mean recall score'] = np.mean(recall_accs)\n", "        j += 1\n", "        print('')\n", "        print('Mean recall score ', np.mean(recall_accs))\n", "        print('')\n", "\n", "    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n", "    \n", "    # Finally, we can check which C parameter is the best amongst the chosen.\n", "    print('*********************************************************************************')\n", "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n", "    print('*********************************************************************************')\n", "    \n", "    return best_c"], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "21c9c558-aa24-4d76-8438-e057a56551b7", "_uuid": "eda517171da00476bc2256e7cec6d37914512c52"}, "cell_type": "code", "execution_count": null, "source": ["best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample)"], "outputs": []}, {"metadata": {"_cell_guid": "01b60a75-6871-4ace-ad6a-fce19f7c7402", "_uuid": "c9855f5f6b94692905bbfadd51ab2cb5e6c51604"}, "cell_type": "markdown", "source": ["### 3.1.3 After we get the best C papameter, we can use it to build the model, with under-sampled data\n", "* Question: why we need C, and why we use the penalty = 'l1' below??"]}, {"metadata": {"collapsed": true, "_cell_guid": "e03f33f8-40cb-4432-b01f-9e39e98d69cd", "_uuid": "d09adc628b4027d19a0df4b5931dd31ec6ada057"}, "cell_type": "code", "execution_count": null, "source": ["# Use this C_parameter to build the final model with the sampled training dataset and predict the classes in the test\n", "# dataset\n", "lr = LogisticRegression(C = best_c, penalty = 'l1') # l2 is about 90% recall\n", "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n", "y_pred_undersample = lr.predict(X_test_undersample.values)\n", "\n", "# Compute and plot confusion matrix\n", "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n", "print(\"the recall for this model is :\",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n", "fig= plt.figure(figsize=(6,3))# to plot the graph\n", "print(\"TP\",cnf_matrix[1,1,]) # no of fraud transaction which are predicted fraud\n", "print(\"TN\",cnf_matrix[0,0]) # no. of normal transaction which are predited normal\n", "print(\"FP\",cnf_matrix[0,1]) # no of normal transaction which are predicted fraud\n", "print(\"FN\",cnf_matrix[1,0]) # no of fraud Transaction which are predicted normal\n", "sns.heatmap(cnf_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\n", "plt.title(\"Confusion_matrix\")\n", "plt.xlabel(\"Predicted_class\")\n", "plt.ylabel(\"Real class\")\n", "plt.show()"], "outputs": []}, {"metadata": {"_cell_guid": "9db2f3ec-55e4-470a-a165-423689a58964", "_uuid": "101b125d0b83363309f42bf61a3491ed83747d97"}, "cell_type": "markdown", "source": ["### 3.1.4 Above we just tested the prediction on a small portion of the dataset (the sampled data set). Now we can also use this C_parameter to build the model and predict the classes in the whole data set\n"]}, {"metadata": {"collapsed": true, "_cell_guid": "529141c5-2b94-4103-b015-150111c65c89", "_uuid": "4a4ddf1032414cda3923754d1d743b1dd0dda80c"}, "cell_type": "code", "execution_count": null, "source": ["# Use this C_parameter to build the model with the sampling dataset and predict the classes in the whole test dataset\n", "lr = LogisticRegression(C = best_c, penalty = 'l1')\n", "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n", "y_pred = lr.predict(X_test)\n", "\n", "# Compute and plot confusion matrix\n", "cnf_matrix = confusion_matrix(y_test,y_pred)\n", "\n", "print(\"the recall for this model is :\",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n", "fig= plt.figure(figsize=(6,3))# to plot the graph\n", "print(\"TP\",cnf_matrix[1,1,]) # no of fraud transaction which are predicted fraud\n", "print(\"TN\",cnf_matrix[0,0]) # no. of normal transaction which are predited normal\n", "print(\"FP\",cnf_matrix[0,1]) # no of normal transaction which are predicted fraud\n", "print(\"FN\",cnf_matrix[1,0]) # no of fraud Transaction which are predicted normal\n", "sns.heatmap(cnf_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\n", "plt.title(\"Confusion_matrix\")\n", "plt.xlabel(\"Predicted_class\")\n", "plt.ylabel(\"Real class\")\n", "plt.show()"], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "91a861f5-324c-40fa-8bf7-4362e2c4911d", "_uuid": "cfcbd1313d0d27911fade45a683a4ed797a4a12e"}, "cell_type": "code", "execution_count": null, "source": [], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "0e27dc08-25cc-40a2-bcd7-dc80e1c1a482", "_uuid": "1bc14e9e225d43e8fc68f7821ec1f012d16726fc"}, "cell_type": "code", "execution_count": null, "source": [], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "39fa2eef-c09d-43ab-af3b-72454227ab92", "_uuid": "030a8d2b40b1eb52b32cdeba4e6d421bb437e870"}, "cell_type": "code", "execution_count": null, "source": [], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "19a3f5f0-511d-48b5-b8b7-ed5af21a0203", "_uuid": "d825bbe3ae7530e655f92e5710726f406b451e1d"}, "cell_type": "code", "execution_count": null, "source": [], "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "70b354b5-f4d4-4173-8d45-383442b8a14f", "_uuid": "336e6852d27d317271d03d8ed4c544eadb8af281"}, "cell_type": "markdown", "source": ["### reference: \n", "#### https://www.kaggle.com/gargmanish/how-to-handle-imbalance-data-study-in-detail\n", "#### https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now\n"]}, {"metadata": {"collapsed": true, "_cell_guid": "5650b22d-a6e7-44cd-9a20-acb1444e67d7", "_uuid": "1758e0dd7561217df12c9cb085f3c91d1fb3bd3f"}, "cell_type": "code", "execution_count": null, "source": [], "outputs": []}], "nbformat_minor": 1}