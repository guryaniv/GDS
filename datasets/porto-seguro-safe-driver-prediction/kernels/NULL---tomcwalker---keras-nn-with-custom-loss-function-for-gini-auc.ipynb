{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "version": "3.6.3", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "cells": [{"metadata": {"_cell_guid": "c73d9e8c-028c-443d-8d3b-c4cc2b92257a", "_uuid": "f2517a98c4b0b920265fca01b8f24556b2122d70"}, "cell_type": "markdown", "source": ["# Overview\n", "\n", "\n", "## The problem\n", "\n", "The Porto Seguro data set is heavily unbalanced, with positive examples around 3% of the total data set. This makes it hard for conventional neural network loss functions to work with. We can use class weights to correct this, but then we will tend to overfit to the positive examples. That might be ok if we, say, had only 3% pictures of dogs in a sample of pictures of cats and dogs, but the problem is that a driver doesn't *always* claim in the same way that a dog is *always* a dog; the dataset is inherently noisy and probablistic by it's nature. This is where a ranking based metric like Gini or AUC can be useful.\n", "\n", "However, this is a problem for neural nets, as AUC (and hence Gini which is (2AUC-1) is not differentiable. Using conventional gradient descent metrics such as binary cross entropy will lead us to just classify everything as in class zero, which will give around 97% accuracy, with a good chance of the relative predictions within a class - on which AUC depends - being very noisy as we haven't trained our network to optimise them.\n", "\n", "## The solution\n", "\n", "This notebook demonstrates a custom loss function for neural nets, that provides a differentiable approximation to AUC. AUC, in turn, has a linear relationship with Gini, hence this is very useful when we want to train a network to maximise AUC.\n", "\n", "We set up 2 identical NNs and run them for a few epochs, to show how this approach improves convergence on AUC compared to binary crossentropy.\n", "\n", "I've used this to get a network that has a local CV AUC around 0.642, which corresponds to Gini of 0.284. The performance on the LB test set is considerably worse (around 0.270) in the one case I tested - mainly I've used them as inputs to blends.\n", "\n", "This is hacked together from various bits of my local code, and hasn't been thoroughly tested, so let please me know of any bugs etc.\n", "\n", "I would have coded as a script, but I need to use the Theano backend as the AUC function uses Theano specific code. If anyone knows how to make Kaggle Kernels use the Theano backend for script, let me know, and I'll post it as a script.\n", "\n", "This is my pretty much my first kernel so feedback welcome.\n", "\n", "**Imports and constants**\n", "\n", "First things first...note that we need the Theano backend. Converting to Tensorflow is on my to-do list."]}, {"outputs": [], "metadata": {"_cell_guid": "4d2257b7-b397-4709-8483-34f32e2ba805", "collapsed": true, "_uuid": "08f2a9a1f9347b4a3694e00d066919eb8775cd87"}, "cell_type": "code", "execution_count": null, "source": ["import numpy as np\n", "import pandas as pd\n", "\n", "%env KERAS_BACKEND=theano\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dropout\n", "from keras.layers.normalization import BatchNormalization\n", "from keras import regularizers\n", "from keras.layers import Dense\n", "from keras.optimizers import Adam\n", "from keras.utils import custom_object_scope\n", "from keras import callbacks\n", "\n", "from sklearn.metrics import roc_auc_score\n", "from sklearn import preprocessing\n", "\n", "import theano\n", "\n", "# train and test data path\n", "DATA_TRAIN_PATH = '../input/train.csv'\n", "DATA_TEST_PATH = '../input/test.csv'\n", "\n", "featuresToDrop = [\n", "    'ps_calc_10',\n", "    'ps_calc_01',\n", "    'ps_calc_02',\n", "    'ps_calc_03',\n", "    'ps_calc_13',\n", "    'ps_calc_08',\n", "    'ps_calc_07',\n", "    'ps_calc_12',\n", "    'ps_calc_04',\n", "    'ps_calc_17_bin',\n", "    'ps_car_10_cat',\n", "    'ps_car_11_cat',\n", "    'ps_calc_14',\n", "    'ps_calc_11',\n", "    'ps_calc_06',\n", "    'ps_calc_16_bin',\n", "    'ps_calc_19_bin',\n", "    'ps_calc_20_bin',\n", "    'ps_calc_15_bin',\n", "    'ps_ind_11_bin',\n", "    'ps_ind_10_bin'\n", "]\n", "\n"]}, {"metadata": {"_cell_guid": "77f3ad9c-8fbf-454c-acce-15079d124fcb", "_uuid": "19902b39fbc3b64bb654e62de44ee7dc0c449672"}, "cell_type": "markdown", "source": ["**The secret sauce**\n", "\n", "This is where the magic happens - the soft_AUC function. This \n", "- Takes the predictions\n", "- Splits them into groups according to whether the true values are one/zero\n", "- Takes each pair of predictions from the one/zero groups, and subtracts the zeroes from the ones.\n", "- Takes the mean of the sigmoid of the result\n", "\n", "If AUC is perfect, an (actual) one in the CV data will always have a higher pred than a zero in the CV data. Each time the prediction is wrong, and a one has a lower pred than a zero, the output loss is increased. Hence this is an suitable loss function to substitute for genuine AUC in that it decreases as AUC decreases and vice versa.\n", "\n", "Like AUC, we only care about relative ordering of predictions between the classes. We don't care about the absolute values of the predictions. This means that your final output values from your NN will also only care about ordering, and hence you use them to blend you will need to use ranking or similar to blend.\n", "\n", "It's important to note that you need a large enough batch size, as if you have no examples of one class you'll get no data, and ideally you want several positive cases in each batch. I used a batch size of 4096, which with a 3% approx positive class rate, gives around 100 positives per batch - enough to give a useful result but not so many as to make calculations take forever. I did try calculating on the whole training batch, but convergence was not as good.\n", "\n", "Some useful references:\n", " * http://www.ipipan.waw.pl/~sj/pdf/PKDD07web.pdf\n", " * https://github.com/Lasagne/Lasagne/issues/767 - my code based heavily on this code.\n", " * http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.3727&rep=rep1&type=pdf"]}, {"outputs": [], "metadata": {"_cell_guid": "97449bba-3635-4c35-822b-3c60e5bbec8a", "collapsed": true, "_uuid": "541421e60545bf5cda3c61cabc3b61a531b26a98"}, "cell_type": "code", "execution_count": null, "source": ["\n", "# An analogue to AUC which takes the differences between each pair of true/false predictions\n", "# and takes the average sigmoid of the differences to get a differentiable loss function.\n", "# Based on code and ideas from https://github.com/Lasagne/Lasagne/issues/767\n", "def soft_AUC_theano(y_true, y_pred):\n", "    # Extract 1s\n", "    pos_pred_vr = y_pred[y_true.nonzero()]\n", "    # Extract zeroes\n", "    neg_pred_vr = y_pred[theano.tensor.eq(y_true, 0).nonzero()]\n", "    # Broadcast the subtraction to give a matrix of differences  between pairs of observations.\n", "    pred_diffs_vr = pos_pred_vr.dimshuffle(0, 'x') - neg_pred_vr.dimshuffle('x', 0)\n", "    # Get signmoid of each pair.\n", "    stats = theano.tensor.nnet.sigmoid(pred_diffs_vr * 2)\n", "    # Take average and reverse sign\n", "    return 1-theano.tensor.mean(stats) # as we want to minimise, and get this to zero\n", "\n"]}, {"metadata": {"_cell_guid": "cea17e84-57b3-4f12-b214-934e8339cd24", "_uuid": "f78c0afa84626757da63146ddd3c9e6efa356955"}, "cell_type": "markdown", "source": ["**Callback**\n", "\n", "Now, we define a callback to print out our SKLearn AUC, and add this to the logs so we can use it for early stopping. See https://keras.io/callbacks/\n", "\n", "In my own version I also use this to save down best scores in csv files where I store metadata for each network I run. This makes it much easier to look at trends of hyperparameter performance with overnight runs."]}, {"outputs": [], "metadata": {"_cell_guid": "55522cf5-abf3-43c1-b394-b7aaa87aaa05", "collapsed": true, "_uuid": "fb27cbdeb24fcc9f875a77a2b04a278f797b7be9"}, "cell_type": "code", "execution_count": null, "source": ["\n", "# This callback records the SKLearn calculated AUC each round, for use by early stopping\n", "# It also has slots where you can save down metadata or the model at useful points -\n", "# for Kaggle kernel purposes I've commented these out\n", "class AUC_SKlearn_callback(callbacks.Callback):\n", "    def __init__(self, X_train, y_train, useCv = True):\n", "        super(AUC_SKlearn_callback, self).__init__()\n", "        self.bestAucCv = 0\n", "        self.bestAucTrain = 0\n", "        self.cvLosses = []\n", "        self.bestCvLoss = 1,\n", "        self.X_train = X_train\n", "        self.y_train = y_train\n", "        self.useCv = useCv\n", "\n", "    def on_train_begin(self, logs={}):\n", "        return\n", "\n", "    def on_train_end(self, logs={}):\n", "        return\n", "\n", "    def on_epoch_begin(self, epoch, logs={}):\n", "        return\n", "\n", "    def on_epoch_end(self, epoch, logs={}):\n", "        train_pred = self.model.predict(np.array(self.X_train))\n", "        aucTrain = roc_auc_score(self.y_train, train_pred)\n", "        print(\"SKLearn Train AUC score: \" + str(aucTrain))\n", "\n", "        if (self.bestAucTrain < aucTrain):\n", "            self.bestAucTrain = aucTrain\n", "            print (\"Best SKlearn AUC training score so far\")\n", "            #**TODO: Add your own logging/saving/record keeping code here\n", "\n", "        if (self.useCv) :\n", "            cv_pred = self.model.predict(self.validation_data[0])\n", "            aucCv = roc_auc_score(self.validation_data[1], cv_pred)\n", "            print (\"SKLearn CV AUC score: \" +  str(aucCv))\n", "\n", "            if (self.bestAucCv < aucCv) :\n", "                # Great! New best *actual* CV AUC found (as opposed to the proxy AUC surface we are descending)\n", "                print(\"Best SKLearn genuine AUC so far so saving model\")\n", "                self.bestAucCv = aucCv\n", "\n", "                # **TODO: Add your own logging/model saving/record keeping code here.\n", "                self.model.save(\"best_auc_model.h5\", overwrite=True)\n", "\n", "            vl = logs.get('val_loss')\n", "            if (self.bestCvLoss < vl) :\n", "                print(\"Best val loss on SoftAUC so far\")\n", "                #**TODO -  Add your own logging/saving/record keeping code here.\n", "        return\n", "\n", "    def on_batch_begin(self, batch, logs={}):\n", "        return\n", "\n", "    def on_batch_end(self, batch, logs={}):\n", "        # logs include loss, and optionally acc( if accuracy monitoring is enabled).\n", "        return\n"]}, {"metadata": {"_cell_guid": "389cb28a-bd78-4d41-a65d-3da14634c978", "_uuid": "ff96fbea52d3b482f456c264b99f7c760ac6179c"}, "cell_type": "markdown", "source": ["**Model creation and training functions**\n", "\n", "Standard model creation and training code, note that we reference the custom loss function and the callback here."]}, {"outputs": [], "metadata": {"_cell_guid": "33900d41-17fc-4b28-b579-fd1234abcb12", "collapsed": true, "_uuid": "754b0a54e8c71d95b93342b2832692d29dd960a2"}, "cell_type": "code", "execution_count": null, "source": ["# Create the model.\n", "def create_model_AUC(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout):\n", "    return create_model(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout, \"AUC\")\n", "\n", "def create_model_bce(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout):\n", "    return create_model(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout, \"crossentropy\")\n", "\n", "\n", "def create_model(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout, mode=\"AUC\") :\n", "    print(\"Creating model with input dim \", input_dim)\n", "    # likely to need tuning!\n", "    reg = regularizers.l2(l2reg)\n", "\n", "    model = Sequential()\n", "\n", "    model.add(Dense(units=first_layer_size, kernel_initializer='lecun_normal', kernel_regularizer=reg, activation='relu', input_dim=input_dim))\n", "    model.add(BatchNormalization())\n", "    model.add(Dropout(dropout))\n", "\n", "    model.add(Dense(units=second_layer_size, kernel_initializer='lecun_normal', activation='relu', kernel_regularizer=reg))\n", "    model.add(BatchNormalization(axis=1))\n", "    model.add(Dropout(dropout))\n", "\n", "    model.add(Dense(units=third_layer_size, kernel_initializer='lecun_normal', activation='relu', kernel_regularizer=reg))\n", "    model.add(BatchNormalization())\n", "    model.add(Dropout(dropout))\n", "\n", "    model.add(Dense(1, kernel_initializer='lecun_normal', activation='sigmoid'))\n", "\n", "    # classifier.compile(loss='mean_absolute_error', optimizer='rmsprop', metrics=['mae', 'accuracy'])\n", "    opt = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n", "    if (mode == \"AUC\"):\n", "        model.compile(loss=soft_AUC_theano, metrics=[soft_AUC_theano], optimizer=opt)  # not sure whether to use metrics here?\n", "    else:\n", "        model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)  # not sure whether to use metrics here?\n", "    return model\n", "\n", "\n", "def train_model( X_train, y_train, model, valSplit=0.15, epochs = 5, batch_size = 4096):\n", "\n", "    callbacksList = [AUC_SKlearn_callback(X_train, y_train, useCv = (valSplit > 0))]\n", "    if (valSplit > 0) :\n", "        early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5,\n", "                                                       verbose=0, mode='min')\n", "        callbacksList.append( early_stopping )\n", "    return model.fit(x=np.array(X_train), y=np.array(y_train),\n", "                        callbacks=callbacksList, validation_split=valSplit,\n", "                        verbose=2, batch_size=batch_size, epochs=epochs)\n", "\n"]}, {"metadata": {"_cell_guid": "a7853a08-9376-46bd-ace5-fd0e51d8209f", "_uuid": "6bb0d11018ea9ee0298af4224eb4b005d09c2183"}, "cell_type": "markdown", "source": ["**Data preparation**\n", "\n", "We remove some of the noisier features, there was an excellent Kernel which I will try to find and cite that the list of columns to remove was taken from. This makes a big difference to effectiveness."]}, {"outputs": [], "metadata": {"_cell_guid": "8cbbceb4-f70e-4f1f-ac10-41bc004e7703", "collapsed": true, "_uuid": "1584194e1cd69f9cc105ffc568f187a810dc4fee"}, "cell_type": "code", "execution_count": null, "source": ["\n", "\n", "def scale_features(df_for_range, df_to_scale, columnsToScale) :\n", "    # Scale columnsToScale in df_to_scale\n", "    columnsOut = list(map( (lambda x: x + \"_scaled\"), columnsToScale))\n", "    for c, co in zip(columnsToScale, columnsOut) :\n", "        scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n", "        print(\"scaling \", c ,\" to \",co)\n", "        vals = df_for_range[c].values.reshape(-1, 1)\n", "        scaler.fit(vals )\n", "        df_to_scale[co]=scaler.transform(df_to_scale[c].values.reshape(-1,1))\n", "\n", "    df_to_scale.drop (columnsToScale, axis=1, inplace = True)\n", "\n", "    return df_to_scale\n", "\n", "\n", "def one_hot (df, cols):\n", "    # One hot cols requested, drop original cols, return df\n", "    df = pd.concat([df, pd.get_dummies(df[cols], columns=cols)], axis=1)\n", "    df.drop(cols, axis=1, inplace = True)\n", "    return df\n", "\n", "def get_data() :\n", "    X_train = pd.read_csv(DATA_TRAIN_PATH, index_col = \"id\")\n", "    X_test = pd.read_csv(DATA_TEST_PATH, index_col = \"id\")\n", "\n", "    y_train = pd.DataFrame(index = X_train.index)\n", "    y_train['target'] = X_train.loc[:,'target']\n", "    X_train.drop ('target', axis=1, inplace = True)\n", "    X_train.drop (featuresToDrop, axis=1, inplace = True)\n", "    X_test.drop (featuresToDrop,axis=1, inplace = True)\n", "\n", "    # car_11 is really a cat col\n", "    X_train.rename(columns={'ps_car_11': 'ps_car_11a_cat'}, inplace=True)\n", "    X_test.rename(columns={'ps_car_11': 'ps_car_11a_cat'}, inplace=True)\n", "\n", "    cat_cols = [elem for elem in list(X_train.columns) if \"cat\" in elem]\n", "    bin_cols = [elem for elem in list(X_train.columns) if \"bin\" in elem]\n", "    other_cols = [elem for elem in list(X_train.columns) if elem not in bin_cols and elem not in cat_cols]\n", "\n", "    # Scale numeric features in region of -1,1 using training set as the scaling range\n", "    X_test = scale_features(X_train, X_test, columnsToScale=other_cols)\n", "    X_train = scale_features(X_train, X_train, columnsToScale=other_cols)\n", "\n", "    X_train = one_hot(X_train, cat_cols)\n", "    X_test = one_hot(X_test, cat_cols)\n", "\n", "\n", "    return X_train, X_test, y_train\n"]}, {"metadata": {"_cell_guid": "bb57a2df-e1ce-4a7b-897a-3d80d7be16b2", "_uuid": "b2bc8931a2ab197b71be5bffa698820811f6c0a0"}, "cell_type": "markdown", "source": ["**Put it all together**\n", "\n", "We set this up to run 2 comparable networks over a few epochs, to demonstrate that convergence is superior. I only run for 5 epochs to give a flavour of the comparison and avoid timing out the kernel.\n", "\n", "I found each epoch took around 3-5 minutes on my GT 1070 based windows system, and decent results were obtained after about 30 epochs, so around 2 hours training time. I've not given away the best hyperparameters I've found; I'll leave that as an exercise for the reader.\n", "\n", "The submission generation code is untested, and you would want to run the network for longer and tune the hyperparameters a bit anyway before using this.\n"]}, {"outputs": [], "metadata": {"_cell_guid": "1f4d94c2-75c9-468b-9388-edc11f0a2585", "collapsed": true, "_uuid": "6961420279b611e5b12098afd8c31b17130d61f6"}, "cell_type": "code", "execution_count": null, "source": ["\n", "\n", "def makeOutputFile(pred_fun, test, subsFile) :\n", "    df_out = pd.DataFrame(index=test.index)\n", "    y_pred = pred_fun( test )\n", "    df_out['target'] = y_pred\n", "    df_out.to_csv(subsFile, index_label=\"id\")\n", "\n", "def main() :\n", "    X_train, X_test, y_train = get_data()\n", "    model = create_model( input_dim=X_train.shape[1],\n", "                          first_layer_size=300,\n", "                          second_layer_size=200,\n", "                          third_layer_size=200,\n", "                          lr=0.0001,\n", "                          l2reg = 0.1,\n", "                          dropout = 0.2,\n", "                          mode=\"AUC\")\n", "\n", "    train_model(X_train, y_train, model)\n", "\n", "    with custom_object_scope({'soft_AUC_theano': soft_AUC_theano}):\n", "        pred_fun = lambda x: model.predict(np.array(x))\n", "        makeOutputFile(pred_fun, X_test, \"auc.csv\")\n", "\n", "    model = create_model_bce( input_dim=X_train.shape[1],\n", "                          first_layer_size=300,\n", "                          second_layer_size=200,\n", "                          third_layer_size=200,\n", "                          lr=0.0001,\n", "                          l2reg = 0.1,\n", "                          dropout = 0.2)\n", "\n", "    train_model(X_train, y_train, model)\n", "\n", "    pred_fun = lambda x: model.predict(np.array(x))\n", "    makeOutputFile(pred_fun, X_test, \"no_auc.csv\")\n", "\n", "main()"]}, {"metadata": {"_cell_guid": "0dde9387-ba46-4b20-b8a0-2620935d05d3", "_uuid": "406aa1a4d99ef0fd6b75ab3903910b63e1c85931"}, "cell_type": "markdown", "source": ["**Conclusions**\n", "\n", "We can see that the AUC (and hence Gini) rises much quicker using the custom loss function. Please upvote if you like this kernal or find it useful."]}], "nbformat_minor": 1, "nbformat": 4}