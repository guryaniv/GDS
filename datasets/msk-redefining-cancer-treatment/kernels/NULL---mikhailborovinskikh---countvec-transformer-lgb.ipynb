{"nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1", "name": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "cells": [{"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_cell_guid": "846dc562-1fa6-4698-9971-0443d1034c73", "collapsed": true, "_uuid": "d092eb210d5f6046c63dedf5d0fde3b3febb085a"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["%matplotlib inline\n", "import pandas as pd\n", "import numpy as np\n", "\n", "import lightgbm as lgb\n", "import xgboost as xgb\n", "from sklearn.model_selection import cross_val_predict\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import log_loss, accuracy_score\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.svm import SVC\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "import nltk\n", "import re\n", "from nltk.corpus import stopwords\n", "import os"], "metadata": {"_cell_guid": "79bad09b-bc5c-48fd-8bc0-fbccd642f71b", "collapsed": true, "_uuid": "e64bbd620ec07808c26b8ee8b2114442b90e3a7e"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["df_train_txt = pd.read_csv('../input/training_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "df_train_var = pd.read_csv('../input/training_variants')\n", "df_test_txt = pd.read_csv('../input/test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "df_test_var = pd.read_csv('../input/test_variants')\n", "training_merge_df = df_train_var.merge(df_train_txt,left_on=\"ID\",right_on=\"ID\")\n", "testing_merge_df = df_test_var.merge(df_test_txt,left_on=\"ID\",right_on=\"ID\")"], "metadata": {"_cell_guid": "59f6bcb2-78eb-4ae4-bdce-18bfb53ce438", "collapsed": true, "_uuid": "5b149a7aa3e5a009329867304225c69c54011523"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["training_merge_df.head()"], "metadata": {"_cell_guid": "ab564e11-a628-4e59-a3ee-a31717a20924", "collapsed": true, "_uuid": "4413acbb44867442761ea25b62bf2de660d1a37e"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["def textClean(text):\n", "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n", "    text = text.lower().split()\n", "    stops = {'so', 'his', 't', 'y', 'ours', 'herself', \n", "             'your', 'all', 'some', 'they', 'i', 'of', 'didn', \n", "             'them', 'when', 'will', 'that', 'its', 'because', \n", "             'while', 'those', 'my', 'don', 'again', 'her', 'if',\n", "             'further', 'now', 'does', 'against', 'won', 'same', \n", "             'a', 'during', 'who', 'here', 'have', 'in', 'being', \n", "             'it', 'other', 'once', 'itself', 'hers', 'after', 're',\n", "             'just', 'their', 'himself', 'theirs', 'whom', 'then', 'd', \n", "             'out', 'm', 'mustn', 'where', 'below', 'about', 'isn',\n", "             'shouldn', 'wouldn', 'these', 'me', 'to', 'doesn', 'into',\n", "             'the', 'until', 'she', 'am', 'under', 'how', 'yourself',\n", "             'couldn', 'ma', 'up', 'than', 'from', 'themselves', 'yourselves',\n", "             'off', 'above', 'yours', 'having', 'mightn', 'needn', 'on', \n", "             'too', 'there', 'an', 'and', 'down', 'ourselves', 'each',\n", "             'hadn', 'ain', 'such', 've', 'did', 'be', 'or', 'aren', 'he', \n", "             'should', 'for', 'both', 'doing', 'this', 'through', 'do', 'had',\n", "             'own', 'but', 'were', 'over', 'not', 'are', 'few', 'by', \n", "             'been', 'most', 'no', 'as', 'was', 'what', 's', 'is', 'you', \n", "             'shan', 'between', 'wasn', 'has', 'more', 'him', 'nor',\n", "             'can', 'why', 'any', 'at', 'myself', 'very', 'with', 'we', \n", "             'which', 'hasn', 'weren', 'haven', 'our', 'll', 'only',\n", "             'o', 'before'}\n", "    text = [w for w in text if not w in stops]    \n", "    text = \" \".join(text)\n", "    text = text.replace(\".\",\" \").replace(\",\",\" \")\n", "    return(text)"], "metadata": {"_cell_guid": "4f44f04b-0fbd-444c-aa59-3f28b2048be8", "collapsed": true, "_uuid": "006949bd9fc79b1a95b2fce31a0d3ac4bda3997c"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["trainText = []\n", "for it in training_merge_df['Text']:\n", "    newT = textClean(it)\n", "    trainText.append(newT)\n", "testText = []\n", "for it in testing_merge_df['Text']:\n", "    newT = textClean(it)\n", "    testText.append(newT)"], "metadata": {"_cell_guid": "c097d23b-8540-4e92-ad9b-260bb862f708", "collapsed": true, "_uuid": "551de24b3b634ecd589090e52f3165dc08119d0b"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["from nltk.stem.lancaster import LancasterStemmer\n", "st = LancasterStemmer()\n", "for i in range(len(trainText)):\n", "    trainText[i] = st.stem(trainText[i])\n", "for i in range(len(testText)):\n", "    testText[i] = st.stem(testText[i])"], "metadata": {"collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["%%time\n", "#I used CuntVectorizer before, best result is 0.77.\n", "#Now I use TfIdfVectorizer, best result iz 0.67 with ngram (1,2).\n", "#I think that ngram (1,3) may be better *)\n", "#count_vectorizer = CountVectorizer(min_df=5, ngram_range=(1,2), max_df=0.65,\n", "                       #tokenizer=nltk.word_tokenize,\n", "                       #strip_accents='unicode',\n", "                       #lowercase =True, analyzer='word', token_pattern=r'\\w+',\n", "                       #stop_words = 'english')\n", "count_vectorizer = TfidfVectorizer(ngram_range=(1,1), max_df=0.65,\n", "                        tokenizer=nltk.word_tokenize,\n", "                        strip_accents='unicode',\n", "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n", "                        use_idf=True, smooth_idf=True, sublinear_tf=False, \n", "                        stop_words = 'english')\n", "bag_of_words = count_vectorizer.fit_transform(trainText)\n", "print(bag_of_words.shape)\n", "X_test = count_vectorizer.transform(testText)\n", "print(X_test.shape)"], "metadata": {"_cell_guid": "c4ae40fc-6835-4a7b-b254-48e0cbfe3e20", "collapsed": true, "_uuid": "91a1e63a4b5abaf81a09509ce6aed5e48c601a1d"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["%%time\n", "transformer = TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=False)\n", "transformer_bag_of_words = transformer.fit_transform(bag_of_words)\n", "X_test_transformer = transformer.transform(X_test)\n", "print (transformer_bag_of_words.shape)\n", "print (X_test_transformer.shape)"], "metadata": {"_cell_guid": "6b36a68f-25de-48cb-a954-9f44f5cfb73f", "collapsed": true, "_uuid": "e3b8878359fb4ad170e85581b014275c9bbbe35a"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["gene_le = LabelEncoder()\n", "gene_encoded = gene_le.fit_transform( np.hstack((training_merge_df['Gene'].values.ravel(),testing_merge_df['Gene'].values.ravel()))).reshape(-1, 1)\n", "gene_encoded = gene_encoded / float(np.max(gene_encoded))\n", "\n", "\n", "variation_le = LabelEncoder()\n", "variation_encoded = variation_le.fit_transform( np.hstack((training_merge_df['Variation'].values.ravel(),testing_merge_df['Variation'].values.ravel()))).reshape(-1, 1)\n", "variation_encoded = variation_encoded / float(np.max(variation_encoded))"], "metadata": {"_cell_guid": "280cb27a-38fb-4d1b-967a-69cfbcbfa1f0", "collapsed": true, "_uuid": "fba63e1767a3ed4afcd86033e47d4cb4fa92add3"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["from scipy.sparse import hstack"], "metadata": {"_cell_guid": "3a8f8844-0b82-430c-a929-68cc314eb12c", "collapsed": true, "_uuid": "b7c646e1313611006ddc8eb49b179e01ccf92d4f"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["%%time\n", "#This for (1,1) ngram lambda l2 and num leaves 50 (95), \n", "#num iterations 1000 (500), learning rate 0.01(0.05), max_depth 7 (5)\n", "params = {'task': 'train',\n", "    'boosting_type': 'gbdt',\n", "    'objective': 'multiclass',\n", "    'num_class': 9,\n", "    'metric': {'multi_logloss'},\n", "    'learning_rate': 0.01, \n", "    'max_depth': 10,\n", "    'num_iterations': 1500, \n", "    'num_leaves': 55, \n", "    'min_data_in_leaf': 66, \n", "    'lambda_l2': 1.0,\n", "    'feature_fraction': 0.8, \n", "    'bagging_fraction': 0.8, \n", "    'bagging_freq': 5}\n", "\n", "x1, x2, y1, y2 = train_test_split(hstack((gene_encoded[:training_merge_df.shape[0]], variation_encoded[:training_merge_df.shape[0]], transformer_bag_of_words)), training_merge_df['Class'].values.ravel()-1, test_size=0.1, random_state=1)\n", "d_train = lgb.Dataset(x1, label=y1)\n", "d_val = lgb.Dataset(x2, label=y2)\n", "\n", "model = lgb.train(params, train_set=d_train, num_boost_round=280,\n", "               valid_sets=[d_val], valid_names=['dval'], verbose_eval=20,\n", "               early_stopping_rounds=20)"], "metadata": {"_cell_guid": "8d686e0d-4d0d-46b5-bad8-72ff311f9135", "collapsed": true, "_uuid": "a3c5a50e1a94ee063dce2c11db65449d4c90862e"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["%%time\n", "results = model.predict(hstack((gene_encoded[training_merge_df.shape[0]:], variation_encoded[training_merge_df.shape[0]:], X_test_transformer)))"], "metadata": {"_cell_guid": "bfc937fa-9fb5-46e0-bfc4-d91715349178", "collapsed": true, "_uuid": "c8ae01cab12e14c5c2fb559202948c7d508627f2"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["results_df = pd.read_csv(\"../input/submissionFile\")\n", "for i in range(1,10):\n", "    results_df['class'+str(i)] = results.transpose()[i-1]\n", "results_df.to_csv('output_tf_one_hot11',sep=',',header=True,index=None)\n", "results_df.head()"], "metadata": {"_cell_guid": "18053649-d979-48b5-bda0-373838bfc160", "collapsed": true, "_uuid": "901c8dff2d9e7d3e56cc5e757c58ab5109179a8e"}}]}