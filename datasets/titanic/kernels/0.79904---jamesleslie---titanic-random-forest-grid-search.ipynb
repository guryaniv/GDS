{"cells":[{"metadata":{"_uuid":"9b3e43f6f3787243de855b9416a0b5697be6d116"},"cell_type":"markdown","source":"# Titanic challenge part 2\nIn this kernel, we will be covering all of the steps required to train, tune and assess a random forest model. \n\n[**Part 1**](https://www.kaggle.com/jamesleslie/titanic-eda-wrangling-imputation/notebook) of this series dealt with the pre-processing and manipulation of the data. This notebook will make use of the data sets that were created in the first part.   \n\nWe will do each of the following:\n  - train and test default RF model\n  - introduce cross-validation for model training\n  - use grid search to optimize hyperparameters\n  - submit our predictions for the test set\n  \n[**Part 3**](https://www.kaggle.com/jamesleslie/titanic-neural-network-for-beginners/notebook) of this challenge involves fitting and tuning a **neural network** to make predictions."},{"metadata":{"_uuid":"cc4b58927770beab136969e526dbbd69a9cc21c8","_cell_guid":"969e5a36-8f64-4129-ba03-7fd19b314ca9"},"cell_type":"markdown","source":"# Table of Contents:\n\n- **1. [Load packages and data](#loading)**\n- **2. [Pre-processing](#pre-processing)**\n- **3. [Random Forest](#random-forest)**\n  - **3.1. [Train/test split](#train-test)**\n  - **3.2. [Cross-validation](#cv)**\n  - **3.3. [Grid search](#grid-search)**\n- **4. [Submit predictions](#submission)**"},{"metadata":{"_uuid":"de8369a5716eb80519979ef773ecbb135f66e4b9","_cell_guid":"ec3a4ceb-2397-45b3-aafe-a8ffde879888"},"cell_type":"markdown","source":"<a id=\"loading\"></a>\n# 1. Load packages and data\nWe will be using the train and test sets that we created in [part 1](https://www.kaggle.com/jamesleslie/titanic-eda-wrangling-imputation/notebook) of this series.   \n\nYou can find the dataset [here](https://www.kaggle.com/jamesleslie/titanic-cleaned-data)."},{"metadata":{"_kg_hide-output":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import rcParams\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n%matplotlib inline\nrcParams['figure.figsize'] = 10,8\nsns.set(style='whitegrid', palette='muted',\n        rc={'figure.figsize': (15,10)})\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a36589b28132214f8f50534c4eb61a5a66e52bfe"},"cell_type":"code","source":"print(os.listdir(\"../input/titanic-cleaned-data\"))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load data as Pandas dataframe\ntrain = pd.read_csv('../input/titanic-cleaned-data/train_clean.csv', )\ntest = pd.read_csv('../input/titanic-cleaned-data/test_clean.csv')\ndf = pd.concat([train, test], axis=0, sort=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bafdb419c8b7f571dbbb9c0b63fd3c52c66c3c1b"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8f24917231bd2484e4c2a4896f4fbbdcc3e89ab"},"cell_type":"code","source":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)\n\n        \ndisplay_all(df.describe(include='all').T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca279aca8e6064618de4b7badb8d94366628cf2b"},"cell_type":"code","source":"df['Survived'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"732fc7427f32d790561d03b4a6d870d2a7c67013","_cell_guid":"b1fc54e6-9655-4a05-b147-e2dfe206c7d0"},"cell_type":"markdown","source":"<a id=\"pre-processing\"></a>\n# 2. Encode categorical variables\nWe need to convert all categorical variables into numeric format. The categorical variables we will be keeping are `Embarked`, `Sex` and `Title`.   \n\nThe `Sex` variable can be encoded into single 1-or-0 column, but the other variables will need to be [one-hot encoded](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). Regular label encoding assigns some category labels higher numerical values. This implies some sort of scale (Embarked = 1 is not **more** than Embarked = 0 - it's just _different_). One Hot Encoding avoids this problem.   \n\nWe will assume that there is some ordinality in the `Pclass` variable, so we will leave that as a single column."},{"metadata":{"trusted":true,"_uuid":"7d6d38029e57b2b7eecd8978b4f3b9ab2bbf79d9"},"cell_type":"code","source":"sns.countplot(x='Pclass', data=df, palette='hls', hue='Survived')\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b5e6978df59b98a614297659371823a4a5dbbcc"},"cell_type":"code","source":"sns.countplot(x='Sex', data=df, palette='hls', hue='Survived')\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74e4e4d6a937c94fda45dcd135c8f2936707c8d6"},"cell_type":"code","source":"sns.countplot(x='Embarked', data=df, palette='hls', hue='Survived')\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4350c9dbdfedb4a92d266b30daa3e0e35f3f322"},"cell_type":"code","source":"# convert to category dtype\ndf['Sex'] = df['Sex'].astype('category')\n# convert to category codes\ndf['Sex'] = df['Sex'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"911939a11430319dbda5a2c533c6b8c077a46766","_cell_guid":"ddf8a643-1c1d-4036-818c-0d230a05310d","trusted":true},"cell_type":"code","source":"# subset all categorical variables which need to be encoded\ncategorical = ['Embarked', 'Title']\n\nfor var in categorical:\n    df = pd.concat([df, \n                    pd.get_dummies(df[var], prefix=var)], axis=1)\n    del df[var]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecfe8fb0453e91f3a8cf80c45ed8ad4a406e0e0c"},"cell_type":"code","source":"# drop the variables we won't be using\ndf.drop(['Cabin', 'Name', 'Ticket', 'PassengerId'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"416083bad67fefd62cfee27da0f60bdb57a9540b"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbead52ea8dd6e9cba5da969375c7829a34b87d4","_cell_guid":"bc50ae8f-a1d7-48b6-9771-e50b4e8e169b"},"cell_type":"markdown","source":"<a id=\"random-forest\"></a>\n# 3. Random Forest\nNow, all that is left is to feed our data that has been cleaned, encoded and scaled to a random forest.    \n<a id=\"train-test\"></a>\n## 3.1. Train/test split\nBut first, we need to separate *data_df* back into *train* and *test* sets."},{"metadata":{"trusted":true,"_uuid":"c3fffdde68114df12c2a1871d8501cae18a2e2ce"},"cell_type":"code","source":"train = df[pd.notnull(df['Survived'])]\nX_test = df[pd.isnull(df['Survived'])].drop(['Survived'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26ac216b3215e4b030177916fd5ea79f9ed63bfc"},"cell_type":"markdown","source":"### Validation set\nSince we can't use our test set to assess our model (it doesn't have any labels), we will create a separte 'validation set'. We will use this set to test how our model generalises to unseen data."},{"metadata":{"trusted":true,"_uuid":"94f5bbc7429ec311e8a6eced5d7ed651ef2cba8f"},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(\n    train.drop(['Survived'], axis=1),\n    train['Survived'],\n    test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4282fce18e666cd315d47c7aca63e4d03c224146"},"cell_type":"code","source":"for i in [X_train, X_val, X_test]:\n    print(i.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75f56351057f33dcee93e207c4b3c461fcb65fae","_cell_guid":"edcc8edb-a50b-4ec3-9958-06c081fbcd68"},"cell_type":"markdown","source":"### Create Random Forest model\nWe will first make a random forest model, using all of the default parameters.   \n> Note: set the `random_state` to 42 for reproducibility"},{"metadata":{"trusted":true,"_uuid":"92ff8b5cdbc88dae948d8df9a2219615e674bb09"},"cell_type":"code","source":"rf = RandomForestClassifier(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d96485d799ad3e3850dfc5656680dec1b62ae95","_cell_guid":"34a52736-364d-4f77-87c1-b432a8cc6834"},"cell_type":"markdown","source":"### Train model\nNow, let's train the model on our training set."},{"metadata":{"_uuid":"aa8ceed05ab371e22807e332024f822695916912","_cell_guid":"af48be46-7e2a-4816-95e7-b54489e19c1c","trusted":true},"cell_type":"code","source":"rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71217231ff4f11b9eea2ae0f46470b672d820c7b"},"cell_type":"markdown","source":"### Test model"},{"metadata":{"trusted":true,"_uuid":"0f868bd1d23a3ed93d268f8517b1ef09fc285fa5","scrolled":false},"cell_type":"code","source":"accuracy_score(y_val, rf.predict(X_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"602d97ad434eb9e87faa76cfaf49de88c08df7e5"},"cell_type":"markdown","source":"<a id=\"cv\"></a>\n## 3.2. Cross-validation\nKeeping a separate validation set means that we have less data on which to train our model. Cross-validation allows us to train our model on _all_ of the data, while still assessing its performance on unseen data.\n\nK-folds cross validation is the process of creating *k* different train/validate splits in the data and training the model *k* times.\n\n![CV](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)   \n\nIn the image above, k=4. This means that the model will be trained 4 times, each time using 1/4 of the data for validation. In this way, each of the four 'folds' takes one turn sitting out from training and is used as the validation set.   \n\nLet's combine our train and validation sets back into one training set, and then use cross-validation to assess our model:"},{"metadata":{"trusted":true,"_uuid":"ca04b98bc96895fdbef13f2ce225e26fb0f374f3"},"cell_type":"code","source":"X_train = pd.concat([X_train, X_val])\ny_train = pd.concat([y_train, y_val])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ee3cc7c3270c30eb4e80fabe2623e504c3bc26c7"},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0ba4b6d6382ea13815008d87736f2e1880d1d0d"},"cell_type":"markdown","source":"Now we have all of training data again. Let's fit a model to it, and assess its accuracy using 5-fold cross-validation:"},{"metadata":{"trusted":true,"_uuid":"f99d7b624572cf6826493d4c387304ca2a374e5d"},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=10, random_state=42)\ncross_val_score(rf, X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b24804581b6e9c16d8dd4c212fa57f9e171a981e"},"cell_type":"code","source":"cross_val_score(rf, X_train, y_train, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62df755aaa3130891edc76c01e53e6fd911bb55f"},"cell_type":"markdown","source":"Here, our CV score is slightly lower than our previous single validation score. Taking a look at the scores for each of the folds, the score does seem to vary slightly.   \n\nCross-validation has the added advantage of being a more robust measure of model accuracy than single validation.   \n> Note: the method we used initially is actually just 1-fold cross-validation"},{"metadata":{"_uuid":"06d18e0e5e48f5cf1781f6f91a99ad21c6af11f4"},"cell_type":"markdown","source":"<a id=\"grid-search\"></a>\n## 3.3. Hyperparameter tuning\nOur first model didn't do too badly! It scored over 80% on the CV score. However, we didn't put any thought into our choice of hyperparameters, we simply went with the defaults.   \n\nTake a look at the various parameters by using the `help()` function:"},{"metadata":{"_kg_hide-output":false,"trusted":true,"scrolled":false,"_uuid":"8e6c2fb1f54d4868c37f2d25b80f3c6fb2d00d50"},"cell_type":"code","source":"# help(RandomForestClassifier)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b220d582e6c2210a166807855d273efb8fb73ea"},"cell_type":"markdown","source":"It is hard to know the best values for each of these hyperparameters without first _trying_ them out. If we wanted to know the best value for the `n_estimators` parameter, we could fit a few models, each with a different value, and see which one tests the best.   \n\n**Grid search** allows us to do this for multiple parameters simultaneously. We will select a few different parameters that we want to tune, and for each one we will provide a few different values to try out. Then grid search will fit models to every possible combination of these parameter values and use **cross-validation** to assess the performance in each case.   \n\nFurthermore, since we are using CV, we don't need to keep a separate validation set."},{"metadata":{"_uuid":"eabfa8a3dbb1dd061e642e8263bcf98feae6e65e"},"cell_type":"markdown","source":"### 3.2.1. Number of estimators and max depth\nWe will start by tuning the `n_estimators` (number of trees in the forest) and the `max_depth` (how deep each tree grows) parameters.   \n\nThe first step that we need to do is to define the grid of parameters over which to search:"},{"metadata":{"trusted":true,"_uuid":"aee8c8a76611d4e075179b2191b4da5324c472bd"},"cell_type":"code","source":"# create the grid\nn_estimators = [10, 100, 1000, 2000]\nmax_depth = [None, 5, 10, 20]\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2767ce9f3dae7a78fabf83fb0e57e07f50e7c1b4"},"cell_type":"markdown","source":"We have set out a total of $4 \\times 4 = 16$ models over which to search. Grid search uses cross-validation on each of the models, so if we use 3-folds cross-validation, that will leave us with 48 different fits to try out. (You can see how the number of fits can grow pretty quickly as we increase the number of parameters!)   \n\nThe good news is that SkLearn's grid search allows us to run the job in parallel. Including the `n_jobs=-1` argument below let's grid search run on all of the available cores on the host machine."},{"metadata":{"trusted":true,"_uuid":"aee8c8a76611d4e075179b2191b4da5324c472bd","scrolled":false},"cell_type":"code","source":"# create the default model\nrf = RandomForestClassifier(random_state=42)\n\n# search the grid\ngrid = GridSearchCV(estimator=rf, \n                    param_grid=param_grid,\n                    cv=3,\n                    verbose=2,\n                    n_jobs=-1)\n\ngrid_result = grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a27f7fae51580b6c21e63304b36f15550fdaa7d"},"cell_type":"markdown","source":"Now let's take a look at the results of the grid search.   \n\nWe can get the best performing model directly from `grid_result`:"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"fd49612793fb6dc3b7d1349609730d8d7252c24c"},"cell_type":"code","source":"grid_result.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b838096425ad229c47184e1bdf38cfaf97c77041"},"cell_type":"markdown","source":"Or just the best parameters:"},{"metadata":{"trusted":true,"_uuid":"d53d9749a0bf5512b07d74b52f3af42ef4664efe"},"cell_type":"code","source":"grid_result.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7b709a2647359ad2489cf91bfe9774c68c069fc"},"cell_type":"markdown","source":"Or the best score:"},{"metadata":{"trusted":true,"_uuid":"414c18d0f81e23676d44fd003711af3714960151"},"cell_type":"code","source":"grid_result.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"660e0f9fa4b44a90d957f9961ebd5f5370e3309b"},"cell_type":"markdown","source":"But let's take a look at all of the models so we can make a more informed decision"},{"metadata":{"trusted":true,"_uuid":"aee8c8a76611d4e075179b2191b4da5324c472bd","scrolled":true},"cell_type":"code","source":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"186635a5b21616833ab080a6d311d20345845acc"},"cell_type":"markdown","source":"### 3.2.2. Leaf size\nThe `min_samples_leaf` argument controls the size of the leaves in the trees.   \n\nWe will set out the grid in a similar manner as before, only this time we will use the `max_depth` and `n_estimators` parameters that we found above."},{"metadata":{"trusted":true,"_uuid":"04e78698779d2862174346da739ff9bc262f13fa"},"cell_type":"code","source":"# create the grid\nleaf_samples = [1, 2, 3, 4, 5, 6]\nparam_grid = dict(min_samples_leaf=leaf_samples)\n\n# create the model with new max_depth and n_estimators\nrf = grid_result.best_estimator_\n\n# search the grid\ngrid = GridSearchCV(estimator=rf, \n                    param_grid=param_grid,\n                    cv=3,\n                    verbose=2,\n                    n_jobs=-1)\n\ngrid_result = grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3a7ab234020ad2a55643950392c17c4202a65dc6"},"cell_type":"code","source":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f55ba2987ca48df93dcff646787394999421556"},"cell_type":"markdown","source":"### 3.2.3. To bag or not to bag\nBootstrap aggregating (or bagging) is a special case of the random forest where we bootstrap (sample with replacement) from the n training obersvations to create a new training set of size n for each tree. Furthermore, each tree considers all variables when making each split.   \n\nWe can use grid search to determine if bootstrapping will be an appropriate method to use."},{"metadata":{"trusted":true,"_uuid":"730a03c80e66e543693c39eaa9b906185ee2689c"},"cell_type":"code","source":"# create the grid\nmax_features = [5, 8, 10, 12, None]\nbootstrap = [True, False]\nparam_grid = dict(max_features=max_features, bootstrap=bootstrap)\n\n# create the model with new leaf size\nrf = grid_result.best_estimator_\n\n# search the grid\ngrid = GridSearchCV(estimator=rf, \n                    param_grid=param_grid,\n                    cv=3,\n                    verbose=2,\n                    n_jobs=-1)\n\ngrid_result = grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12d05efb91691a1f7468834e84c3436bfce63071"},"cell_type":"code","source":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae291559b10db2f627a87f62a553d15da43ada31","_cell_guid":"5ccf334a-8a72-45e5-b565-cc516639e087"},"cell_type":"markdown","source":"<a id=\"submission\"></a>\n## 4. Make Predictions on Test Set\nFinally, we can attempt to predict which passengers in the test set survived."},{"metadata":{"trusted":true,"_uuid":"4398cc0ae4581a3722f2e10d3dac944baf8c8d47"},"cell_type":"code","source":"rf = grid_result.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd64287cf95c931d94e8c2263ce64e33e580c50f"},"cell_type":"code","source":"# test our CV score\ncross_val_score(rf, X_train, y_train, cv=5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce685f9df90375c19422e45f82417a1eaa6a01d9"},"cell_type":"code","source":"test['Survived'] = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c644df967bacf87fb867b194a626b8684dffc90f"},"cell_type":"code","source":"solution = test[['PassengerId', 'Survived']]\nsolution['Survived'] = solution['Survived'].apply(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40f53edf0a58f57b9dc450070beba6efef051dda"},"cell_type":"code","source":"solution.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a9b9f4189c03b253c9140c4a93c9f453508a6ba","_cell_guid":"0f646038-2eb3-4a77-94f5-a3abc6c3be1f"},"cell_type":"markdown","source":"## Output Final Predictions"},{"metadata":{"_uuid":"7dc52ff626a2620c8607c25d85bd8952f049690b","_cell_guid":"04e4be98-5955-43ab-a355-be65580a1162","trusted":true},"cell_type":"code","source":"solution.to_csv(\"Random_Forest_Solution.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}