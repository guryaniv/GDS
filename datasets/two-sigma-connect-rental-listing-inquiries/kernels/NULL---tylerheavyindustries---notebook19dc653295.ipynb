{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4a05bdab-000c-8dda-fad1-8d38353a65d9"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e895db36-5d8c-da4a-72eb-4996c206d504"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2aec64d6-8e1c-53b6-5b53-2c769af8e9a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cd274cd8-9b9d-55e7-6868-522212e00c1b"
      },
      "source": [
        "# read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "195645e9-f275-0f6b-7bda-a15469a12670"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(open(\"../input/train.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b276aa60-6294-9a7a-2964-692cd924ba03"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58b09b32-445d-572d-9550-235defff1d62"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "48237d07-fc8f-fb51-81a9-28e5052c0fd5"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ce4d7dc-e9f0-25c5-77cd-4274e477ef95"
      },
      "source": [
        "## Classifying Building ID\n",
        "\n",
        "The text information is very worthwhile but not the easiest to address. Ideally, we'd parse the information in the displayed address, building description, and the building ID (which is stored as a string). We'll start with the easiest one first (building ID) because it's just one element. After we incorporate that, we can burn the other bridges as we come to them. \n",
        "\n",
        "Building ID could be interesting to analyze, but it's not in the right format for Sklearn so let's try building ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f770d6b-716d-36fd-608a-326ad7f422b0"
      },
      "outputs": [],
      "source": [
        "#first lets get a sense of the most popular building IDs, how many are there? \n",
        "#this uses two methods, seperated onto different lines for clarity \n",
        "print(\n",
        "    df['building_id']\n",
        "          .value_counts()\n",
        "          .nlargest(50)\n",
        "     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2c63447c-51da-cb94-0cc0-b5a8f09c591d"
      },
      "outputs": [],
      "source": [
        "#This is a little more than I care to look it! I'll plot it - a picture is worth at least 1000 words. \n",
        "\n",
        "df['street_address'].value_counts().plot(kind = 'hist', bins = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ebf5f086-e936-c1a9-be75-cf3a85e07d03"
      },
      "source": [
        "Let's try the preprocessing tool! - Note, this example commits a cardinal sin, it configures the label encoder with test and training data. If we think this is a good approach, we'll have to move some things around and fix them in the future! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f4242daa-0ec8-7981-3384-5c01feb5c3b5"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df['building_id'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1b50f3de-38cb-70d0-2d12-1597d3d37bcd"
      },
      "source": [
        "We'll have to transform our data so that the system can use it - "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d482459-62fd-cd6d-b411-90051b04329b"
      },
      "outputs": [],
      "source": [
        "df['building_id'] = le.fit_transform(df['building_id']) \n",
        "\n",
        "df['building_id'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "13852bd6-5613-a204-13ed-40dc40336d8d"
      },
      "source": [
        "## Text analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "76099eaf-00e8-1177-960e-75e3571baeeb"
      },
      "outputs": [],
      "source": [
        "##make models / stuff for each interest level? \n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6077f937-317f-70b3-ed68-1f13c25bb333"
      },
      "outputs": [],
      "source": [
        "## first we need to split the data and get the features so that we can train and validate this process\n",
        "\n",
        "\n",
        "(\n",
        "   X_train_low\n",
        " , X_val_low\n",
        " , y_train_low\n",
        " , y_val_low\n",
        ") = train_test_split(df[\"description\"][df[\"interest_level\"] == \"low\"], df[\"interest_level\"][df[\"interest_level\"] == \"low\"], test_size=0.60)\n",
        "\n",
        "(\n",
        "   X_train_medium\n",
        " , X_val_medium\n",
        " , y_train_medium\n",
        " , y_val_medium\n",
        ") = train_test_split(df[\"description\"][df[\"interest_level\"] == \"low\"], df[\"interest_level\"][df[\"interest_level\"] == \"low\"], test_size=0.60)\n",
        "\n",
        "\n",
        "(\n",
        "   X_train_high\n",
        " , X_val_high\n",
        " , y_train_high\n",
        " , y_val_high\n",
        ") = train_test_split(df[\"description\"][df[\"interest_level\"] == \"low\"], df[\"interest_level\"][df[\"interest_level\"] == \"low\"], test_size=0.60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e3fd45e-fe84-ff14-558c-8d1bb1254bd8"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aabefa0c-a441-a1c5-b0c3-8e2fce9401c0"
      },
      "outputs": [],
      "source": [
        "def description_to_wordlist( description, remove_stopwords=True):\n",
        "    # Function to convert a document to a sequence of words,\n",
        "    # optionally removing stop words.  Returns a list of words.\n",
        "\n",
        "    # 1. Remove non-letters\n",
        "    description_text = re.sub(\"[^a-zA-Z]\",\" \", description)\n",
        "    #\n",
        "    # 2. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "    #\n",
        "    # 3. Optionally remove stop words (True by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    b=[]\n",
        "    stemmer = english_stemmer #PorterStemmer()\n",
        "    for word in words:\n",
        "        b.append(stemmer.stem(word))\n",
        "\n",
        "    # 5. Return a list of words\n",
        "    return(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b00b40e-e82c-f94e-dda9-4a9c64bfd3e9"
      },
      "outputs": [],
      "source": [
        "description_low = []\n",
        "for description in X_train_low['description']:\n",
        "    description_low.append( \" \".join(description_to_wordlist(review)))\n",
        "   \n",
        "description_med = []\n",
        "for description in X_train_med['description']:\n",
        "    description_med.append( \" \".join(description_to_wordlist(review)))\n",
        "\n",
        "description_high = []\n",
        "for description in X_train_high['description']:\n",
        "    description_high.append( \" \".join(description_to_wordlist(review)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1938afbe-c4de-3818-5777-167744aaa566"
      },
      "source": [
        "# naive feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6c29fe75-fcad-6681-e78d-ff88fb785fb2"
      },
      "outputs": [],
      "source": [
        "df[\"num_photos\"] = df[\"photos\"].apply(len)\n",
        "df[\"num_features\"] = df[\"features\"].apply(len)\n",
        "df[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "df[\"created\"] = pd.to_datetime(df[\"created\"])\n",
        "df[\"created_year\"] = df[\"created\"].dt.year\n",
        "df[\"created_month\"] = df[\"created\"].dt.month\n",
        "df[\"created_day\"] = df[\"created\"].dt.day\n",
        "df[\"price_per_bedroom\"] =  df[\"bedrooms\"] / df[\"price\"] \n",
        "df[\"price_per_bathroom\"] = df[\"bathrooms\"] / df[\"price\"] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ebc84cd4-a2f7-36f4-4cfe-263509615704"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5d9c6d2b-3109-1d27-f3a7-6df4330aee32"
      },
      "outputs": [],
      "source": [
        "num_feats = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\n",
        "             \"num_photos\", \"num_features\", \"num_description_words\",\n",
        "             \"created_year\", \"created_month\", \"created_day\", \"building_id\",\n",
        "              \"price_per_bedroom\", \"price_per_bathroom\"]\n",
        "\n",
        "X = df[num_feats]\n",
        "y = df[\"interest_level\"]\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a66624cf-ed69-c745-449c-60ac80da57a6"
      },
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6c39a7de-821a-e570-23e3-d9ea46d5d04c"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.45)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3173b2c8-a451-97a9-4aa2-2144c34f9b96"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=1500, )\n",
        "clf.fit(X_train, y_train)\n",
        "y_val_pred = clf.predict_proba(X_val)\n",
        "log_loss(y_val, y_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f07e6451-e1b4-43d3-2e40-907e530fbb36"
      },
      "source": [
        "# Train another model\n",
        "\n",
        "Let's try a bagging approachd!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4b2a6e53-449a-64b3-91ed-b7b87f8562ac"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "b1 = BaggingClassifier(n_estimators=2000, )\n",
        "b1.fit(X_train, y_train)\n",
        "y_val_pred = b1.predict_proba(X_val)\n",
        "log_loss(y_val, y_val_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79f0ded1-3190-2cbc-858f-ce13d19db5cd"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier()\n",
        "gbc.fit(X_train, y_train)\n",
        "y_val_pred = gbc.predict_proba(X_val)\n",
        "log_loss(y_val, y_val_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1a14d829-bcc8-a63b-3001-30653b31bf84"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC()\n",
        "clf.fit(X_train, y_train)\n",
        "y_val_pred = clf.predict_proba(X_val)\n",
        "log_loss(y_val, y_val_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2cd86abf-e3cf-49f0-da1b-b0acbeba5bea"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X_train, y_train)\n",
        "y_val_pred = neigh.predict_proba(X_val)\n",
        "log_loss(y_val, y_val_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "692a48a7-73e4-ff08-d9aa-eb79c035bdb6"
      },
      "source": [
        "# make prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd806a9a-d8ce-fabf-ae12-ace6a70c3bde"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
        "print(df.shape)\n",
        "df[\"num_photos\"] = df[\"photos\"].apply(len)\n",
        "df[\"num_features\"] = df[\"features\"].apply(len)\n",
        "df[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "df[\"created\"] = pd.to_datetime(df[\"created\"])\n",
        "df[\"created_year\"] = df[\"created\"].dt.year\n",
        "df[\"created_month\"] = df[\"created\"].dt.month\n",
        "df[\"created_day\"] = df[\"created\"].dt.day\n",
        "X = df[num_feats]\n",
        "\n",
        "y = clf.predict_proba(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6a07942f-4c8b-1a08-859c-6d3f5a934edf"
      },
      "outputs": [],
      "source": [
        "labels2idx = {label: i for i, label in enumerate(clf.classes_)}\n",
        "labels2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2a6a7430-56b3-e875-0741-79568dbd2558"
      },
      "outputs": [],
      "source": [
        "sub = pd.DataFrame()\n",
        "sub[\"listing_id\"] = df[\"listing_id\"]\n",
        "for label in [\"high\", \"medium\", \"low\"]:\n",
        "    sub[label] = y[:, labels2idx[label]]\n",
        "sub.to_csv(\"submission_rf.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "313a883b-7dd2-71ec-d3bf-468370965eae"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}