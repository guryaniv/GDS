{"cells":[{"metadata":{"_uuid":"39f05b01870bf7ce3d99306ee1f63b6eed47fbf9"},"cell_type":"markdown","source":"![](http://)<h2>Problem Statement</h2>\nThe objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:\n\n**Supervised:** The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features<br>\n**Classification:** The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)\n\n<h2>Notebook Objectives:</h2>\n<ol>\n<li><font color=\"green\">Understanding Features under application train table</font>\n<li><font color=\"green\">Splitting tables into Categorical, Binary and Continuous features</font>\n<li><font color=\"green\">Creating a simple classification model(without filling null values) and find a baseline <b>roc_auc_score</b></font>\n<li><font color=\"green\">Identifying feature importance by feature type for <b>application_train</b> table</font>\n<li><font color=\"green\">Removing correlated features</font>\n<li><font color=\"red\">Identifying featues strongly correlated with TARGET. This helps in separating features holding linear relationship with TARGET. (work in progress)</font>\n<li><font color=\"green\">Feature selection on categorical data/binary data. Using <b>Chi-squared statistic.</b></font>\n<li><font color=\"green\">Feature selction continuous data. Using <b>ANOVA F-value</b></font>\n<li><font color=\"green\">Visualization on important features</font>\n<li><font color=\"red\">Joining supporting tables and understanding their feature importance as well (work in progress)</font>\n<li><font color=\"red\">Filling null value and again checking accuracy (work in progress)</font>\n<li><font color=\"green\">Preparing <b>bureau and bureau_balance</b> table for joining with application(train/test) data. Check following link ([http://www.kaggle.com/rahullalu/home-credit-default-risk-preparing-bureau-data](http://www.kaggle.com/rahullalu/home-credit-default-risk-preparing-bureau-data))</font>\n<li><font color=\"red\">Preparing <b>previous_application</b> table and associated tables for joining with application(train/test) data. Check following link (work in progress)</font>\n</ol>\n\n<b><I>Note: Moving analysis on bureau and previous_application tables to other notebooks</I></b>"},{"metadata":{"_uuid":"57cdceb22fc1874d242335ea31373520655b8738"},"cell_type":"markdown","source":"<h2>Home Credit Business Model, Counties of Operation and Business Challenges</h2>\n<table><tr><ol>\n<td><li><b>Business Model</b>\n<img src=\"http://www.homecredit.net/~/media/Images/H/Home-Credit-Group/content-images/image-signpost/business-model.jpg\" alt=\"Business Model\"></img></td>\n<td><li><b>Home Credit has presence in 11 countries</b>\n<img src=\"http://www.homecredit.net/~/media/Images/H/Home-Credit-Group/content-images/homepage-map-v1.jpg\" alt=\"Count of Operation\"></img></td>\n</ol></tr></table>"},{"metadata":{"_uuid":"d38e58a2c97e9716ddc7e65e90e8fd98ff4f5d65"},"cell_type":"markdown","source":"<h2>Understanding Data Model</h2>\n<ol>\n<li>We need to join <b>application_{train|test}</b> (train or test) with other tables to get all possible features\n<li>Below model explains which fields can be used for creating joins.\n"},{"metadata":{"_uuid":"d0545fb0c40336117bd6e1223124f8bd24155493"},"cell_type":"markdown","source":"\n<img src=\"https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png\" alt=\"Count of Operation\" height=\"800\" width=\"800\"></img>\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom lightgbm.sklearn import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf1dd50cb02820b8e486fc3837a59c56c1a370fe"},"cell_type":"markdown","source":"<h2>Dataset Overview</h2>\n<ol>\n<li>Number of files\n<li>Type of files\n<li>File size\n</ol>"},{"metadata":{"trusted":true,"_uuid":"a1bd2d27d99db5c792c7f099f24adef6cd0f49a8","_kg_hide-input":true,"scrolled":true},"cell_type":"code","source":"#Dataset view\npath1=\"../input/home-credit-default-risk/\"\ndata_files=list(os.listdir(path1))\ndf_files=pd.DataFrame(data_files,columns=['File_Name'])\ndf_files['Size_in_MB']=df_files.File_Name.apply(lambda x:round(os.stat(path1+x).st_size/(1024*1024),2))\ndf_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c669ea3215231f5204549eb6db54634028ecc8f","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"desc=pd.read_csv(\"../input/columns-description/columns_description.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42d0bc21bcd1ca56727f6bed2b8f29cf823c6c82","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"#All functions\n\n#FUNCTION FOR PROVIDING FEATURE SUMMARY\ndef feature_summary(df_fa):\n    print('DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    col_list=['Null','Unique_Count','Data_type','Max/Min','Mean','Std','Skewness','Sample_values']\n    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n    df['Null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df['Unique_Count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df['Data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.at[col,'Max/Min']=str(round(df_fa[col].max(),2))+'/'+str(round(df_fa[col].min(),2))\n            df.at[col,'Mean']=df_fa[col].mean()\n            df.at[col,'Std']=df_fa[col].std()\n            df.at[col,'Skewness']=df_fa[col].skew()\n        df.at[col,'Sample_values']=list(df_fa[col].unique())\n           \n    return(df.fillna('-'))\n\n\ndef drop_corr_col(df_corr):\n    upper = df_corr.where(np.triu(np.ones(df_corr.shape),\n                          k=1).astype(np.bool))\n    # Find index of feature columns with correlation greater than 0.95\n    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n    return(to_drop)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60fc9f5b6b61952415a2343bc67d770607734031"},"cell_type":"markdown","source":"<h2>Understanding Features in \"application_train.csv\" using FEATURE SUMMARY table:</h2>\n<ol>\n<li>Feature Summary table is generated using custom code\n<li>Idea is to get a consolidated insight on all features. \n<li> Most of the features are Binary, Float values (from 0 to 1) or categorical \n<li>There are lot of missing values. In following sections will be discussing on how to handle missing values..  \n<li>Using summary table we can easily identify following\n<ol>\n<li>Missing values per column\n<li>Unique values per column\n<li>Data type per column\n<li>Minimum and Maximum value\n<li>Mean, Standard Deviation and Skewness\n<li>Sample values\n</ol>\n</ol>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"#Reading training data\ntrain=pd.read_csv(path1+'application_train.csv')\nprint('application_train Feature Summary')\nwith pd.option_context('display.max_rows',train.shape[1]):\n    train_fs=feature_summary(train)   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58ee1acf6fda2368056800b7765ca5d66f8adf45"},"cell_type":"markdown","source":"<h3>Analyzing Target variable: TARGET</h3>\n<ol>\n<li>TARGET variable in binary in nature. So this is a binary classification problem.\n<li>Percentage of Defaulters is <b>8.07%</b>. Case of Imbalanced dataset.\n<li>Imbalanced dataset is a scenario where the number of observations belonging to one class is significantly lower than those belonging to the other classes.\n<li>How to handle imbalance dataset:\n<u>https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/</u>"},{"metadata":{"trusted":true,"_uuid":"a952c6ed176e479953966531e7e42472ed483f59","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"#Understanding target/dependent variable\npie_labels=['Defaulters-'+str(train['TARGET'][train.TARGET==1].count()),'Non Defaulters-'+str(train['TARGET'][train.TARGET==0].count())]\npie_share=[train['TARGET'][train.TARGET==1].count()/train['TARGET'].count(),\n           train['TARGET'][train.TARGET==0].count()/train['TARGET'].count()]\nfigureObject, axesObject = plt.subplots()\npie_colors=('red','green')\npie_explode=(.3,.0)\naxesObject.pie(pie_share,labels=pie_labels,explode=pie_explode,autopct='%.2f%%',colors=pie_colors,startangle=0,shadow=True)\naxesObject.axis('equal')\nplt.title('Percentage of Defaulters and Non Defaulters',color='blue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc538490f7c20357bb86050d7c44ae0c500a3db7","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"print('FEATURE SUMMARY: Categorical Features')\ncat_features=train_fs[train_fs.Data_type=='object'].index\nprint('Total categorical features:',len(cat_features))\ncat_fs=train_fs[train_fs.Data_type=='object']\ncat_fs['Desc']=cat_fs.index\nfor ind in cat_fs['Desc'].values:\n    cat_fs.at[ind,'Desc']=desc.Description[(desc.Table=='application') & (desc.Row==ind)].values[0]\ndisplay(cat_fs.iloc[:,7:9])\ndisplay(cat_fs.iloc[:,:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"63db8ef971e09f29e41c1b7fbf8f27b6da868960","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"#Replacing space with underscore in all categorical values\nfor col in cat_features:\n    train[col]=train[col].apply(lambda x: str(x).replace(\" \",\"_\"))\n   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbf5edd775f13b3521d372398d3842da296a34b8"},"cell_type":"markdown","source":"<h2>Creating dummies from categorical features</h2>\n* We are converting all categorical features into dummies. \n* Null values also treated as a unique feature value and a separate dummy column created for Null values. \n* So we are not filling Null values, but handling them using dummies\n* Treating Null values as a category"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"bca8e9b5b2807bd0d447e5a2aa3f0523af324191","_kg_hide-input":true},"cell_type":"code","source":"#converting call categorical features into dummies \ncat_train=pd.DataFrame()\nfor col in cat_features:\n    dummy=pd.get_dummies(train[col],prefix=col)\n    cat_train=pd.concat([cat_train,dummy],axis=1)\ncat_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5ea3da4c3bd1fee34cac5d9a4af7cf7e67c7807","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"del dummy\ngc.collect()\nprint('Newly created dummy columns:',len(cat_train.columns))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c9ed5d67c64b294dddd8c3c09c39745052b26c3"},"cell_type":"markdown","source":"<h2>Removing correlated categorical features</h2>\n<ul>\n<li>Created correlation matrix to identiy features having correlation higher than 0.95\n<li>And dropping all such features"},{"metadata":{"trusted":true,"_uuid":"2887043c0881d1bdffe97e4999f4768c24d3bf66"},"cell_type":"code","source":"%%time\n#creating correlation matrix with absolute values\ncorr=cat_train.corr().abs()\n#identifying features with high correlation value\nto_drop=drop_corr_col(corr)\ncat_train.drop(to_drop,axis=1,inplace=True)\nprint('Drop following features as they have high correlation other columns:\\n',to_drop,'\\n')\nprint('Categorical Features after dropping correlated features:',cat_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64d78b95f74247276092702133fd13dd67132b5c"},"cell_type":"markdown","source":"<h2>Creating baseline model for Categorical features</h2>\n<ul>\n<li>Calculated roc_auc_score is a baseline value(0.638377) for categorical features.\n<li>After dropping categorical features roc_auc_score slightly improved to (0.638434497)\n<li>After dropping categorical features with Chi-square statistic less than 1 roc_auc_score improved to (0.63889890)\n<li>We will be using Chi-squared statistics to identify best categorical features"},{"metadata":{"trusted":true,"_uuid":"53841388193c34082ba78927635eccf3eaa99faa","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"train_X,test_X,train_y,test_y=train_test_split(cat_train,train['TARGET'],random_state=200)\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Createing a basic LGBM classifier on categorical data. To check can newly created features be consumed by a model')\nprint('roc auc score:',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8c8af85326b0fa3801c02ec7ca2039eb93e0dfef"},"cell_type":"code","source":"indices = np.argsort(model.feature_importances_)[::-1]\nnames = [cat_train.columns[i] for i in indices[:138]]\nfig,ax=plt.subplots(figsize=(20,40))\nplt.title(\"Feature Importance - Categorical Features\",fontsize=35)\nplt.ylabel(\"Categorical Features\",fontsize=35)\nplt.xlabel(\"Feature Importance\",fontsize=35)\ndf_fi_cat=pd.DataFrame(model.feature_importances_[indices[:138]],columns=['Feature_imp'])\ndf_fi_cat['names']=names\ndf_fi_cat.sort_values(by='Feature_imp',inplace=True)\nplt.barh(range(138),df_fi_cat['Feature_imp'],align='edge')\nplt.yticks(range(138),df_fi_cat['names'],color='g',fontsize=15)\nfor i in range(0,138,2):\n    ax.get_yticklabels()[i].set_color(\"red\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"680a889cbaeb0b6ef19e250f000961b8cc345cad"},"cell_type":"markdown","source":"<h2>Feature Selection using [Chi-square](http://www.learn4master.com/machine-learning/chi-square-test-for-feature-selection)</h2>\n* Chi-square is one of the ways of feature selection for categorical features.\n* Chi-square statistics examines the independence of two categorical vectors.\n* We will calculate Chi-square score for all the features and try to visualize it.\n* Chi-square score is calculated for features with respect to target.\n\n"},{"metadata":{"trusted":true,"_uuid":"e0b9cea04919d70ac29f9e1be0458ff41f603530","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"chi2_selector=SelectKBest(chi2,k=138)\nfeature_kbest=chi2_selector.fit_transform(cat_train,train['TARGET'])\ndf_chi=pd.DataFrame(chi2_selector.scores_,columns=['chi_score'])\ndf_chi['columns']=cat_train.columns\ndf_chi_s=df_chi.sort_values(by='chi_score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f844894ff1a1e6c20de64e82313b4c6945c0b241","_kg_hide-input":true,"scrolled":true},"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(20,40))\nplt.title(\"Chi-squared statistics for categorical features\",fontsize=30)\nplt.ylabel(\"Categorical Features\",fontsize=30)\nplt.xlabel(\"Chi-squared statistic\",fontsize=30)\nplt.barh(range(len(df_chi_s['chi_score'])),df_chi_s['chi_score'],align='edge',color='rgbkymc')\nplt.yticks(range(len(df_chi_s['chi_score'])),df_chi_s['columns'],color='g',fontsize=15)\nfor i in range(0,138,2):\n    ax.get_yticklabels()[i].set_color(\"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"882444b4df6e5a5749b788b87e8bac4580e8c4c3","scrolled":true},"cell_type":"code","source":"print('Feature with Chi-square statistic less than 1:',len(df_chi_s[df_chi_s.chi_score<1]['columns']))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"127322fcea259b15c8bc5537a60a01d900c7ee36"},"cell_type":"code","source":"cat_train.drop(df_chi_s[df_chi_s.chi_score<1]['columns'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"70ddcacc3624d4c32ddb0d90d45268f7f443a8ca"},"cell_type":"code","source":"train_X,test_X,train_y,test_y=train_test_split(cat_train,train['TARGET'],random_state=200)\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Score after dropping features with Chi-squared statistic less than 1')\nprint('roc auc score:',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"066d09ce3429a0065023bfd82a5f11b6a106d197"},"cell_type":"markdown","source":"<h2>Identifying top best categorical features (using Chi-squared statistic)</h2>\n<ul>\n<li>Executing a loop from top 10 features to 138 features with step 5 to identify best features.\n<li>Calculating roc_auc score for each set of features.\n<li>Visualizing roc_auc_score against number of features.\n</ul>\n<h3>Conclusion:</h3>\n<ul>\n<li>With correlated features best score was 0.639753 for top 115 features\n<li>After removing correlated feature best score is 0.639378 for top 63 features\n<li>Baseline value for categorical features 0.638377\n<li>So selecting top 63 features will improve overall score\n</ul>"},{"metadata":{"trusted":true,"_uuid":"c2de8953659cae85eb889d2512f44de37938fa44","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\nshape_1=cat_train.shape[1]\nroc_auc=np.zeros([len(range(10,shape_1,5)),2],float)\nk=0\ndf_chi_s.sort_values(by='chi_score',ascending=False,inplace=True)\nfor i in range(10,shape_1,5):\n    train_X,test_X,train_y,test_y=train_test_split(cat_train[df_chi_s['columns'][:i]],train['TARGET'],random_state=200)\n    model =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\n    model.fit(train_X,train_y)\n    roc_auc[k][0]=i\n    roc_auc[k][1]=roc_auc_score(test_y,model.predict_proba(test_X)[:,1])                                \n    k=k+1\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ebb9a2d0fe905c02f380ab242552759a75b20c07","scrolled":false},"cell_type":"code","source":"df_roc=pd.DataFrame(roc_auc,columns=['Features','roc_auc_score'])\ndf_roc.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nprint('Top five roc_auc_scores with Feature count')\ndf_roc.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e947aa93a8fd4ac133adf01be138e4ecdd68e0b7"},"cell_type":"code","source":"df_roc.sort_values(by='Features',inplace=True)\nplt.figure(figsize=(40,10))\nplt.title(\"Categorical Feature selection and roc_auc_score - highlighting top 5\",fontsize=30)\nplt.xlabel(\"Feature count\",fontsize=30)\nplt.ylabel(\"roc_auc_score\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.plot(df_roc['Features'],df_roc['roc_auc_score'],color='b',linewidth=2)\nplt.hlines(xmin=0,xmax=np.max(roc_auc[:,0]),y=np.max(roc_auc[:,1]),color='g',linestyle='dashed')\ndf_roc.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nfor i in range(len(df_roc)):\n    plt.plot(df_roc.iloc[i,0],df_roc.iloc[i,1],'bo')\n    if i<=4:\n        plt.text(df_roc.iloc[i,0],df_roc.iloc[i,1],(('('+str(np.int(df_roc.iloc[i,0]))+','+str(round(df_roc.iloc[i,1],4))+')')),color='r',fontsize=28,rotation=90)\n        plt.vlines(ymin=0.613,ymax=df_roc.iloc[i,1],x=df_roc.iloc[i,0],color='r',linestyle='dotted')\n    if i==16:\n        plt.text(df_roc.iloc[i,0],df_roc.iloc[i,1],(('('+'features'+','+'Score')+')'),color='r',fontsize=28,rotation=90)\n        plt.vlines(ymin=0.613,ymax=df_roc.iloc[i,1],x=df_roc.iloc[i,0],color='r',linestyle='dotted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"416383f1f4d18ca8c2e3e75a67b96aff25bd0902","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"print('FEATURE SUMMARY: Binary Features')\nbin_features=train_fs[((train_fs.Data_type=='int64') | (train_fs.Data_type=='float64')) & (train_fs.Unique_Count==2)].index\nprint('Total binary features (including TARGET):',len(bin_features))\nbin_fs=train_fs[((train_fs.Data_type=='int64') | (train_fs.Data_type=='float64')) & (train_fs.Unique_Count==2)]\nbin_fs['Desc']=bin_fs.index\nfor ind in bin_fs['Desc'].values:\n    bin_fs.at[ind,'Desc']=desc.Description[(desc.Table=='application') & (desc.Row==ind)].values[0]\ndisplay(bin_fs.iloc[:,7:9])\ndisplay(bin_fs.iloc[:,:7])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74a553aa7b37c9769c47c0b5511adb2b93aa8f7d"},"cell_type":"markdown","source":"<h2>Checking for strongly correlated binary features</h2>\n<ul>\n<li>There are no correlated binary features\n</ul>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f71d349ddc5670990dcd859aae8c5eea793a9b0f"},"cell_type":"code","source":"corr_bin=train[bin_features[1:]].corr().abs()\nprint('Number of strongly correlated binary features:',drop_corr_col(corr_bin))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34ae50a9f115f5677b4a6c1a7994397a1b87b408"},"cell_type":"markdown","source":"<h2>Creating baseline model for Binary features</h2>\n<ul>\n<li>Calculated roc_auc_score is a baseline value(0.595040531) for binary features.\n<li>We will be using Chi-squared statistics to identify best binary features"},{"metadata":{"trusted":true,"_uuid":"fc9a2c99d475c713745bb3f486a75850cf8930e7","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"train_X,test_X,train_y,test_y=train_test_split(train[bin_features].drop(['TARGET'],axis=1),train['TARGET'],random_state=200)\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Creating a basic LGBM classifier on binary features')\nprint('roc auc score:',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"54d1cdf03616a964f1244e6f79eeb8d9e1342509","_kg_hide-input":true},"cell_type":"code","source":"indices = np.argsort(model.feature_importances_,)[::-1]\nnames = [train[bin_features].drop(['TARGET'],axis=1).columns[i] for i in indices]\nplt.figure(figsize=(30,10))\nplt.title(\"Feature Importance - Top Binary Features\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.xlabel(\"Binary Features\",fontsize=30)\nplt.ylabel(\"Feature Importance\",fontsize=30)\nplt.bar(range(32), model.feature_importances_[indices])\nplt.xticks(range(32), names,rotation=90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fdcad8fc75e8d98848fd062e3e711390861c506","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"chi2_selector=SelectKBest(chi2,k=32)\nfeature_kbest=chi2_selector.fit_transform(train[bin_features].drop(['TARGET'],axis=1),train['TARGET'])\ndf_chi=pd.DataFrame(chi2_selector.scores_,columns=['chi_score'])\ndf_chi['columns']=bin_features[1:]\ndf_chi_bins=df_chi.sort_values(by='chi_score',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dbec3bf61fa08e973ec32c984010e2663ac6c9f","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(40,10))\nplt.title(\"Chi-squared statistics for binary features\",fontsize=30)\nplt.xlabel(\"Binary Features\",fontsize=30)\nplt.ylabel(\"Chi-squared statistics\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.bar(range(len(df_chi_bins['chi_score'])),df_chi_bins['chi_score'],align='edge',color='rgbkymc')\nplt.xticks(range(len(df_chi_bins['chi_score'])),df_chi_bins['columns'],rotation=90,color='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8f760b0249974494f153d4fa28bd7de771e937b"},"cell_type":"markdown","source":"<h2>Identifying top best binary features (using Chi-squared statistic)</h2>\n<ul>\n<li>Executing a loop from top 5 features to 32 features with step 3 to identify best features.\n<li>Calculating roc_auc score for each set of features.\n<li>Visualizing roc_auc_score against number of features.\n</ul>\n<h3>Conclusion:</h3>\n<ul>\n<li>As per below analysis best score is 0.5950405 for top 32 features\n<li>Baseline value for categorical features 0.5950405\n<li>So in this case we have to select all 32 features\n</ul>"},{"metadata":{"trusted":true,"_uuid":"a157f1a4d87509352c7183fb6d582d31dc40e373","_kg_hide-input":true},"cell_type":"code","source":"%%time\nroc_auc_bin=np.zeros([len(range(5,33,3)),2],float)\nk=0\n\nfor i in range(5,33,3):\n    train_X,test_X,train_y,test_y=train_test_split(train[df_chi_bins['columns'][:i]],train['TARGET'],random_state=200)\n    model =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\n    model.fit(train_X,train_y)\n    roc_auc_bin[k][0]=i\n    roc_auc_bin[k][1]=roc_auc_score(test_y,model.predict_proba(test_X)[:,1])\n    k=k+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"012b6c799d8b62e848948cdd49dda11b2fb3823b","_kg_hide-input":true},"cell_type":"code","source":"df_roc_bin=pd.DataFrame(roc_auc_bin,columns=['Features','roc_auc_score'])\ndf_roc_bin.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nprint('Top five roc_auc_scores with Feature count')\ndf_roc_bin.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82e832ca09915c81ee35798a2bd05ed6c53f3e36","_kg_hide-input":true},"cell_type":"code","source":"df_roc_bin.sort_values(by='Features',inplace=True)\nplt.figure(figsize=(40,10))\nplt.title(\"Binary Feature selection and roc_auc_score - highlighting top 5\",fontsize=30)\nplt.xlabel(\"Feature count\",fontsize=30)\nplt.ylabel(\"roc_auc_score\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.plot(df_roc_bin['Features'],df_roc_bin['roc_auc_score'],color='b',linewidth=3)\nplt.hlines(xmin=0,xmax=np.max(roc_auc_bin[:,0]),y=np.max(roc_auc_bin[:,1]),color='g',linestyle='dashed')\ndf_roc_bin.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nfor i in range(len(df_roc_bin)):\n    plt.plot(df_roc_bin.iloc[i,0],df_roc_bin.iloc[i,1],'bo')\n    if i<=4:\n        plt.text(df_roc_bin.iloc[i,0],df_roc_bin.iloc[i,1],\n                 (('('+str(np.int(df_roc_bin.iloc[i,0]))+','+str(round(df_roc_bin.iloc[i,1],4))+')')),color='r',fontsize=25,rotation=90)\n        plt.vlines(ymin=0.563,ymax=df_roc_bin.iloc[i,1],x=df_roc_bin.iloc[i,0],color='r',linestyle='dotted')\n    if i==8:\n        plt.text(df_roc_bin.iloc[i,0],df_roc_bin.iloc[i,1],(('('+'features'+','+'Score')+')'),color='r',fontsize=25,rotation=90)\n        plt.vlines(ymin=0.563,ymax=df_roc_bin.iloc[i,1],x=df_roc_bin.iloc[i,0],color='r',linestyle='dotted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"454549f3245e44c36dd26738a6302d41b053223a","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"print('FEATURE SUMMARY: Continuous Features')\ncon_features=train_fs[((train_fs.Data_type=='float64') | (train_fs.Data_type=='int64')) & (train_fs.Unique_Count!=2)].index\nprint('Total continuous features:',len(con_features))\ncon_fs=train_fs[((train_fs.Data_type=='float64') | (train_fs.Data_type=='int64')) & (train_fs.Unique_Count!=2)]\ncon_fs['Desc']=con_fs.index\nfor ind in con_fs['Desc'].values:\n    con_fs.at[ind,'Desc']=desc.Description[(desc.Table=='application') & (desc.Row==ind)].values[0]\nwith pd.option_context('display.max_rows',train.shape[1]):\n    display(con_fs.iloc[:,7:9])\n    display(con_fs.iloc[:,:7])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2d7ea9c60a6baf7b2bb7d5e5a7e25b26f5bc560"},"cell_type":"markdown","source":"<h2>Creating baseline model for Continuous features (work in progress)</h2>\n<ul>\n<li>Calculated roc_auc_score is a baseline value with Null ( 0.7533393) for continuous features.\n<li>Baseline value without Null values (0.752309)\n<li>We will be using ANOVA F-value to identify best continuous features"},{"metadata":{"trusted":true,"_uuid":"154edda68570acc4ce0e8aa82aa6d7e4d55b4184","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"train_X,test_X,train_y,test_y=train_test_split(train[con_features].drop(['SK_ID_CURR'],axis=1),train['TARGET'],random_state=200)\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Creating a basic model on continuous features')\nprint('roc auc score',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d4ffebfb7d1553095401c00b3e56992db0a158e5","scrolled":false},"cell_type":"code","source":"indices = np.argsort(model.feature_importances_)[::-1]\nnames = [train[con_features].drop(['SK_ID_CURR'],axis=1).columns[i] for i in indices[:]]\nplt.figure(figsize=(35,10))\nplt.title(\"Feature Importance - Continuous Features\",fontsize=30)\nplt.xlabel(\"Continuous Features\",fontsize=30)\nplt.ylabel(\"Feature importance\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.bar(range(72), model.feature_importances_[indices[:]])\nplt.xticks(range(72), names,rotation=90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d44e4f480b6c06ac6a9af290fca25d21e1d80a09","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"#Updated null values for continuous features their mean value \nfor col in con_features[1:]:\n    if train_fs.at[col,'Null']!=0:\n        train[col]=train[col].fillna(train_fs.at[col,'Mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfb28c9900a63b5339056cb06270171910e9b3f6","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"Fvalue_selector=SelectKBest(f_classif,k=72)\nfeature_kbest=Fvalue_selector.fit_transform(train[con_features[1:]],train['TARGET'])\ndf_Fvalue=pd.DataFrame(Fvalue_selector.scores_,columns=['F-value'])\ndf_Fvalue['columns']=con_features[1:]\ndf_Fvalue_s=df_Fvalue.sort_values(by='F-value',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d894d11202e7651451e93d454c4b714b5b4d9dda","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(40,10))\nplt.title(\"F-value for continuous features\",fontsize=30)\nplt.xlabel(\"Continuous Features\",fontsize=30)\nplt.ylabel(\"F-value statistics\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.bar(range(len(df_Fvalue_s)),df_Fvalue_s['F-value'],align='edge',color='rgbkymc')\nplt.xticks(range(len(df_Fvalue_s)),df_Fvalue_s['columns'],rotation=90,color='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8300c9cfa8d23d26d8fd8ac4ac15482aad1336d"},"cell_type":"markdown","source":"<h2>Identifying top best continuous features (using ANOVA F-value)</h2>\n<ul>\n<li>Executing a loop from top 4 features to 72 features with step 4 to identify best features.\n<li>Calculating roc_auc score for each set of features.\n<li>Visualizing roc_auc_score against number of features.\n</ul>\n<h3>Conclusion:</h3>\n<ul>\n<li>As per below analysis best score is 0.0.752309 for top 72 features\n<li>Baseline value for categorical features  0.7533393 with Null values and 0.0.752309 without Null values.\n<li>So in this case we have to select all 72 features\n</ul>"},{"metadata":{"trusted":true,"_uuid":"d1cc0b497046df34bec77335687f61bcc054b981","_kg_hide-input":true},"cell_type":"code","source":"%%time\nroc_auc_con=np.zeros([len(range(4,73,4)),2],float)\nk=0\n\nfor i in range(4,73,4):\n    train_X,test_X,train_y,test_y=train_test_split(train[df_Fvalue_s['columns'][:i]],train['TARGET'],random_state=200)\n    model =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\n    model.fit(train_X,train_y)\n    roc_auc_con[k][0]=i\n    roc_auc_con[k][1]=roc_auc_score(test_y,model.predict_proba(test_X)[:,1])\n    k=k+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1275a1800c6d1cd079cf7c27827979361c9e057","_kg_hide-input":true},"cell_type":"code","source":"df_roc_con=pd.DataFrame(roc_auc_con,columns=['Features','roc_auc_score'])\ndf_roc_con.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nprint('Top five roc_auc_scores with Feature count')\ndf_roc_con.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"911efdc48796d029290db90cb8441abf01d75645","_kg_hide-input":true},"cell_type":"code","source":"df_roc_con.sort_values(by='Features',inplace=True)\nplt.figure(figsize=(40,10))\nplt.title(\"Continuous Feature selection and roc_auc_score - highlighting top 5\",fontsize=30)\nplt.xlabel(\"Feature count\",fontsize=30)\nplt.ylabel(\"roc_auc_score\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.plot(df_roc_con['Features'],df_roc_con['roc_auc_score'],color='b',linewidth=3)\nplt.hlines(xmin=0,xmax=np.max(roc_auc_con[:,0]),y=np.max(roc_auc_con[:,1]),color='g',linestyle='dashed')\ndf_roc_con.sort_values(by='roc_auc_score',inplace=True,ascending=False)\nfor i in range(len(df_roc_con)):\n    plt.plot(df_roc_con.iloc[i,0],df_roc_con.iloc[i,1],'bo')\n    if i<=4:\n        plt.text(df_roc_con.iloc[i,0],df_roc_con.iloc[i,1],\n                 (('('+str(np.int(df_roc_con.iloc[i,0]))+','+str(round(df_roc_con.iloc[i,1],4))+')')),color='r',fontsize=25,rotation=90)\n        plt.vlines(ymin=0.73,ymax=df_roc_con.iloc[i,1],x=df_roc_con.iloc[i,0],color='r',linestyle='dotted')\n    if i==15:\n        plt.text(df_roc_con.iloc[i,0],df_roc_con.iloc[i,1],(('('+'features'+','+'Score')+')'),color='r',fontsize=25,rotation=90)\n        plt.vlines(ymin=0.73,ymax=df_roc_con.iloc[i,1],x=df_roc_con.iloc[i,0],color='r',linestyle='dotted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"107901f146b6eda50feeaa66a678a477e7f91817","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"print('Concatenating all categorical, binary and continuous features ')\nfinal_train=pd.concat([cat_train,train[bin_features],train[con_features]],axis=1)\nprint('Shape of final training data set:',final_train.shape)\n\n\ndel train,cat_train,bin_features,model,con_features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"f4d4a510f0780ff2c2482a7271ce8391433138a0","trusted":true,"scrolled":false},"cell_type":"code","source":"train_X,test_X,train_y,test_y=train_test_split(final_train.drop(['TARGET','SK_ID_CURR'],axis=1),final_train['TARGET'])\nmodel =LGBMClassifier(learning_rate=0.05,n_estimators=200,n_jobs=-1,reg_alpha=0.1,min_split_gain=.1,verbose=-1)\nmodel.fit(train_X,train_y)\nprint('Creating a final LGBM classifier on final training dataset')\nprint('roc auc score:',roc_auc_score(test_y,model.predict_proba(test_X)[:,1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a2721740a06a09f6ecb0978fbd812c9d6ad1749","_kg_hide-input":true},"cell_type":"code","source":"indices = np.argsort(model.feature_importances_)[::-1]\nnames = [final_train.columns[i] for i in indices[:50]]\nplt.figure(figsize=(30,10))\nplt.title(\"Feature Importance - Top 50 Features\",fontsize=30)\nplt.xlabel(\"Features\",fontsize=30)\nplt.ylabel(\"Feature Importance\",fontsize=30)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.bar(range(50), model.feature_importances_[indices[:50]])\nplt.xticks(range(50), names,rotation=90)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}