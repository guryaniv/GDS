{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# <a id=\"1\">Introduction</a>\n\nAccording to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception.\n\nThe digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner… and often before they´ve even realized they need the service. In their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.\n\nIn this project, we help Santander Group is asking to identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# <a id=\"2\">Load packages</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c8bf45cc990651d97e1baa8bf5c8f5d166d3e4ce"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom math import log1p\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56f86e34d36379bf15257da8bcf2385b8c94093b"},"cell_type":"markdown","source":"# <a id=\"3\">Load Data</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5098e3a6b5bf7a23f0c898c42a32c7c32ed14cc9"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfc223d229ec9668e9ec6542459dfc80771814b4"},"cell_type":"markdown","source":"# <a>Glimpse Of Data</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e7bd85f3b40c95297b9c0279352e373fe413f66d"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b8c8154f2aee122fd148ef62ab392b73f7926af5"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"46571d72358377f9afac0707b136bb6fea2b7308"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6b2a73b228800eb82c3624bde8bd0a2936e8c529"},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f5a30e72737b771c643fba12dac66193a8417df"},"cell_type":"markdown","source":"# <a>Checking Missing Data</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"58d6fb76ea3bee85f32ff8dd76680910be18a8df"},"cell_type":"code","source":"train.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8d22f8e31fac45bfe26c2aaf138ede8ea3a383c7"},"cell_type":"code","source":"test.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2ff573c0e77f6f7ae1a566790ab5c9f458da16e"},"cell_type":"markdown","source":"So, the data has no missing value which is very good, because even when missing values are adjusted there is some logical over-fitting and fitting missing values here would been very difficult because most of the indexes are 0, so for even for more accurate results replacing NaN values with 0 would been most prominent."},{"metadata":{"_uuid":"8f4f3dc2e6b62aae0d96d1b745caf9bbd99298a3"},"cell_type":"markdown","source":"# <a>Supervised Regression</a>"},{"metadata":{"_uuid":"33bc62efed59910816b4d1841629216760907225"},"cell_type":"markdown","source":"The project demands an accurate supervised regressor. We'll try a couple of Regressors including Ensemble Regressors and suggest a good regressor."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0cb5146bedf5f15e0a66edc23b9d6604da404c8e"},"cell_type":"code","source":"columnsList = [ x for x in train.columns if x not in ['ID','target']]\nfeatures=train[columnsList]\ntargets=train['target']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bf7e4ab47b5eb6cbb35e1d444031f3065b0cc8b"},"cell_type":"markdown","source":"# <a> Removing Column with Constant Entries </a>\n*Deleting all columns with no Entries, though it won't matter in the regression, the regressor won't take any input from those columns, it makes data more clear*"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"298457c0bb833d4afb2945505d9748bb8e3441f3"},"cell_type":"code","source":"to_remove_cols=[ x for x in train.columns if train[x].sum()==0]\ncolumnsList = [ x for x in columnsList if x not in to_remove_cols]\nprint(len(columnsList))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21a6c8414bddad2b427e7b29ace284a9a2e005d9"},"cell_type":"markdown","source":"# <a>Removing Duplicate Columns</a>\n*In regression we don't need duplicate columns, duplicate columns will create problem in train_test_split*"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"66c0737436c22ade09624bd39279068c1a0b2d7d"},"cell_type":"code","source":"train=train.T.drop_duplicates().T\ncolumnsList = [ x for x in columnsList if x in train.columns]\nprint(len(columnsList))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46937e7866bdb79d26dea1adf6995ee3efeb15d8"},"cell_type":"markdown","source":"# <a> Evaluation Metric </a>\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\nWe have to reduce RMSLE Value\n\nWe will be passing log1p values. So this metric will calculate RMS value"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"86f5b0d7a121c105f14945850834e637c92827de"},"cell_type":"code","source":"def RMSLE_value(test,pred):\n    return np.sqrt(np.mean(np.power(test-pred, 2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92576555bf50b2c2f18c30e7b9c63025b426e98b"},"cell_type":"markdown","source":"# <a>Support Vector Regression</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ea89b77625c1d344c4264dae36898d1ef198a654"},"cell_type":"code","source":"def Support_Vector_Regression(features_train,features_test,targets_train,targets_test,kernel='rbf',gamma='auto',epsilon=0.1,c=1):\n    reg=SVR(kernel=kernel,gamma=gamma,C=c,epsilon=epsilon)\n    reg.fit(features_train,targets_train)\n    pred=reg.predict(features_test)\n    results=RMSLE_value(targets_test,pred)\n    print('RMSLE_value from Support Vector Regression is',results)\n    return reg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ffbdc7edae59a2dfdecf216dfb616edd59051fd"},"cell_type":"markdown","source":"# <a> Linear Regression </a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"89096bdf34ffa99485ad873504dd7ce4fdfbe579"},"cell_type":"code","source":"def Linear_Regression(features_train,features_test,targets_train,targets_test):\n    reg=LinearRegression()\n    reg.fit(features_train,targets_train)\n    pred=reg.predict(features_test)\n    result=RMSLE_value(targets_test,pred)\n    print(\"RMSLE_value from Linear Regression is \",result)\n    return reg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"268d0a0b22e9f77f59ed4e2b432979a414f09201"},"cell_type":"markdown","source":"# <a> Lasso Regression </a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8c55a59e37cc617232ed3e2ea651d987d88a8d68"},"cell_type":"code","source":"def Lasso_Regression(features_train,features_test,targets_train,targets_test):\n    reg=Lasso()\n    reg.fit(features_train,targets_train)\n    pred=reg.predict(features_test)\n    result=RMSLE_value(targets_test,pred)\n    print(\"RMSLE_value from Lasso Regression is \",result)\n    return reg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bb7df7d271b511472ee54636ce1178b78bf707f"},"cell_type":"markdown","source":"# <a> AdaBoosting using SVR </a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"844f5d2b614f924e55b336594759e6fae881802e"},"cell_type":"code","source":"def AdaBoost_Regression(features_train,features_test,targets_train,targets_test):\n    reg=AdaBoostRegressor(SVR(),n_estimators=13)\n    reg.fit(features_train,targets_train)\n    pred=reg.predict(features_test)\n    result=RMSLE_value(targets_test,pred)\n    print(\"RMSLE_value from AdaBoosting using Support Vector Regressor is \",result)\n    return reg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16bdfe8f38f46718287450f27a1f4a5687134913"},"cell_type":"markdown","source":"# <a> Random Forest Regression  </a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"37aa4ed63182d23db904d9564142a9ab46980403"},"cell_type":"code","source":"def Random_Forest_Regression(features_train,features_test,targets_train,targets_test):\n    reg=RandomForestRegressor(n_estimators=50,min_samples_split=40,max_depth=500)\n    reg.fit(features_train,targets_train)\n    pred=reg.predict(features_test)\n    result=RMSLE_value(targets_test,pred)\n    print(\"RMSLE_value from Random Forest Regression \",result)\n    return reg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f831047d1b317e48eebb04226d1caa9640692807"},"cell_type":"markdown","source":"# <a>Train Test Split</a>\n*I'd have used KFold but it's high on RAM already, train,test sets are huge, not much overfitting expected*"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"42bf42844f8df62d7a6a71586717d7709e0709a6"},"cell_type":"code","source":"features_train, features_test, targets_train, targets_test = train_test_split(train[columnsList],train['target'],test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9cf291a644e92207f3e2020b3335e72d0e3e369a"},"cell_type":"code","source":"features_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"82e1cbb1e313517ed1ecec95459d3511d9743488"},"cell_type":"code","source":"features_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c981b0486d25ffc7e932931abf14a4df36b14b6c"},"cell_type":"code","source":"targets_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f9599c8ec856611cc6bdaee0f5b23ca0dcdefabb"},"cell_type":"code","source":"targets_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b84f68216c59409d3205f4d47b665b9f6fcca64"},"cell_type":"markdown","source":"# <a> Converting Training and Testing Data to array form </a>\nConverting log1p type, we are providing coverted data to the regressors which is actually providing better results."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5f098439ef5839f5464ce9d661ab931cf6c5d45b"},"cell_type":"code","source":"def Convert(data):\n    data=data.values\n    data=data.astype(int)\n    data=np.log1p(data)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6f63acba9e85390d5e7a864c782e495bafbf07f8"},"cell_type":"code","source":"features_train=Convert(features_train)\nfeatures_test=Convert(features_test)\ntargets_train=Convert(targets_train)\ntargets_test=Convert(targets_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70d9494240cd057082f11882db02e5d210d652bd"},"cell_type":"markdown","source":"# <a> Performing Regressions </a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bb24d82048bb7663c22977ef0fa049f698534047"},"cell_type":"code","source":"Support_Vector_Regression(features_train, features_test, targets_train, targets_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8379f1e17a629fdc70b0a17364efcbe3594fb44a"},"cell_type":"code","source":"Linear_Regression(features_train,features_test,targets_train,targets_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ce33d20e992d991c781ceece8a8cd168e6880fe6"},"cell_type":"code","source":"Lasso_Regression(features_train,features_test,targets_train,targets_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a135d9062965c3ce1e76f7211de47d213f4050d7"},"cell_type":"code","source":"AdaBoost_Regression(features_train,features_test,targets_train,targets_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c64c2aa649ecdbadba6c87922c107d783b3c9e5d"},"cell_type":"code","source":"Random_Forest_Regression(features_train,features_test,targets_train,targets_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2811ebbfaa7394253c45e8116e4be1f32ec9e4b"},"cell_type":"markdown","source":"*The results from Support Vector Regression is 1.651663589865719.* \n\n*From Linear Regression it's more than 843 million which is very bad it doesn't do good at all, Note : passing actual data instead of log1p processed data into regressors make the RMSLE value 16 which is still not good but far more satisfactory.  *\n\n*From Lasso Regression it's 1.6951226901280325, so, removing certain features from Linear Regression  helps a lot in this case we can see. It's a comeback. Note : for alpha=0.0 , Lasso regression acts as linear regression. Here standard alpha used is 1.0*\n\n*From AdaBoosting SVR it's 1.6559682182238418, considering it's a boosting regressor it's not very good, and to cinsideration it's more than SVR's score which is indicating overfit* \n\n*From Random Forest it's 1.4456394807462076, some meta regressor things*\n\nWe'll work with Lasso,Random Forest,Support Vector Regressors to check out scores on actual test set."},{"metadata":{"_uuid":"c6a5895e59ca69078cd4f054c901553fb8d531bb"},"cell_type":"markdown","source":"# <a>Processing Test Data</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8d7565c47e193ffd486e92817c6fcc59184b1438"},"cell_type":"code","source":"features=Convert(train[columnsList])\ntargets=Convert(train['target'])\ntest_features=Convert(test[columnsList])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52ab2fbf999445de5f4deed64f5795d1b0ec8078"},"cell_type":"markdown","source":"# <a>Calling Regressors to fit the entire data for submission</a>\nCalling 3 regressors and submit each prediction because top 3 regressors are actually pretty close"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"20cc8dd6d7021d055176b8db600780257d0cb34d"},"cell_type":"code","source":"RFR = RandomForestRegressor(n_estimators=50,min_samples_split=40,max_depth=500).fit(features,targets)\npredRFR=np.expm1(RFR.predict(test_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"de2d3d94679cb24a11adcb857c4342a5ceb3437b"},"cell_type":"code","source":"SVR=SVR().fit(features,targets)\npredSVR=np.expm1(SVR.predict(test_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4d08c2fd77e005aec9c9052c2b18f27c9a9dbb15"},"cell_type":"code","source":"LR=Lasso().fit(features,targets)\npredLR=np.expm1(LR.predict(test_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f869a1bdfd93e1cc212f522e7212bb1189215e76"},"cell_type":"markdown","source":"# <a> Submission </a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e8590f9de7d6d47de788960626d94e64f5501fbb"},"cell_type":"code","source":"sub_SVR = pd.DataFrame()\nsub_SVR['ID'] = test['ID']\nsub_SVR['target'] = predSVR\nsub_SVR.to_csv('sub_SVR.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"61fd5b6c49f88b060642a23940c68dfd57058c72"},"cell_type":"code","source":"sub_RFR = pd.DataFrame()\nsub_RFR['ID'] = test['ID']\nsub_RFR['target'] = predRFR\nsub_RFR.to_csv('sub_RFR.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3a56167a5e66d2fb1e8e1e08ead06e52fe1c05bd"},"cell_type":"code","source":"sub_LR = pd.DataFrame()\nsub_LR['ID'] = test['ID']\nsub_LR['target'] = predLR\nsub_LR.to_csv('sub_LR.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2c66b0f0f602121cb5dac4b1ee3588b01def740"},"cell_type":"markdown","source":"Random Forest Regressor's score is 1.50\n\nSupport Vector Regressor's score is 1.77\n\nLasso Regressor's score is 1.84\n\n**So, Random Forest Regressor  is definitely the choice for the Regression**"},{"metadata":{"_uuid":"1dc4c4cb93df8f1d565e1bdcebf8eb3355fb9dc0"},"cell_type":"markdown","source":"# <a>Exploratory Data Analysis</a>\n\n*For EDA, we start again by reading CSV files*"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a4769f51cd0896f9bdac36e8bdffe884b10dbd0d"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n#Removing Constant and Duplicate Columns, this time keeping 'target'\ncolumnsList = [ x for x in train.columns if x not in ['ID','target']]\nto_remove_cols=[ x for x in train.columns if train[x].sum()==0]\ncolumnsList = [ x for x in columnsList if x not in to_remove_cols]\ntrain_np=train.T.drop_duplicates().T\ncolumnsList = [ x for x in columnsList if x in train_np.columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a940a38e9e9170d9e5e58c7adb087bbf0d7b927"},"cell_type":"markdown","source":"# <a> Defining Some Plot Functions </a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"88a8b2b7f896a970f4486a21a29a94bc61f4e1ca"},"cell_type":"code","source":"def plot_distribution(df,title,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % title)\n    sns.distplot(df.dropna(),color=color, kde=True,bins=100)\n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e4a1752f4ad8f07ce9cb26a52ed5448cc20ea770"},"cell_type":"code","source":"def HeatMap(df,target,columnsList):\n    df=df.astype(int)\n    target=target.astype(int)\n    labels = []\n    values = []\n    for col in columnsList:\n        labels.append(col)\n        values.append(np.corrcoef(df[col].values, target.values)[0,1])\n    corr_df = pd.DataFrame({'columns_labels':labels, 'corr_values':values})\n    corr_df = corr_df.sort_values(by='corr_values')\n    corr_df = corr_df[(corr_df['corr_values']>0.25) | (corr_df['corr_values']<-0.25)]\n    temp = df[corr_df.columns_labels.tolist()]\n    corrmat = temp.corr(method='pearson')\n    f, ax = plt.subplots(figsize=(12, 12))\n    sns.heatmap(corrmat, vmax=1., square=True, cmap=\"YlOrRd\")\n    plt.title(\"Important variables correlation map\", fontsize=15)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acd8b2520c57186c99c64fe53d1ba8247d481adb"},"cell_type":"markdown","source":"# <a> Target Variable </a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0b18c503bb595c4119d5ed35eb570f1217863e98"},"cell_type":"code","source":"plot_distribution(train[\"target\"], \"target\", \"blue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9cb1e805fdd3d535bc3122bac6ae4e2d00dc93de"},"cell_type":"code","source":"plot_distribution(np.log1p(train[\"target\"]), \"log of target\", \"green\")  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c92cd2c5ae22078b4fba5e615b6536f06f8243f"},"cell_type":"markdown","source":"# <a>Distribution Of Non Zero per Row</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"af13ffdc2bb755ede0e2cdf3c0f6bb172de59a82"},"cell_type":"code","source":"non_zeros = train.ne(0).sum(axis=1)\nplot_distribution(np.log1p(non_zeros),\"Distribution of log of non zero indexes per row - train dataset\",\"red\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8857021ae87b86934d04b117f6922edc102f3b60"},"cell_type":"code","source":"non_zeros = test.ne(0).sum(axis=1)\nplot_distribution(np.log1p(non_zeros),\"Distribution of log of non zero indexes per row - test dataset\",\"magenta\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe7c50bb16ddfbaa7ea89a9afecaaa5cc137d88c"},"cell_type":"markdown","source":"# <a>Distribution Of Non Zero per Column</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"759003f43dba6560264dd9d8ec4a4fb8381be7f1"},"cell_type":"code","source":"non_zeros = train.ne(0).sum(axis=0)\nplot_distribution(np.log1p(non_zeros),\"Distribution of non zero indexes per Column - train dataset\",\"cyan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"143dba9746fcb0ee4fd2cb18d1a9836680acd76b"},"cell_type":"code","source":"non_zeros = test.ne(0).sum(axis=0)\nplot_distribution(np.log1p(non_zeros),\"Distribution of non zero indexes per Column - test dataset\",\"green\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d00266661fd0deec1e0b395aaddae0a380629a1"},"cell_type":"markdown","source":"# <a> HeatMap Of Correlation of Important Features of Training Data:</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f0b77a344bcb97629b1b471cc6fe4f2a06e07b6b"},"cell_type":"code","source":"HeatMap(train_np[columnsList],train_np['target'],columnsList)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}