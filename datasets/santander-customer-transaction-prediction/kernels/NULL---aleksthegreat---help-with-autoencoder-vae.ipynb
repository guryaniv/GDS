{"cells":[{"metadata":{"trusted":true,"_uuid":"1b2bb00e59ed9e341ea49a978fd07c134fa44fd2"},"cell_type":"markdown","source":"I wanted to see if I could reconstruct the data and then pass it onto LightGBM for a final model.\nSo far I havent been able to properly tune my VAE and get it to reconstruct the data well enough for\nme to test.  \n\nAny hints or tips would be greatly appreciated!\n\nThe code below is my base model; when I add extra features my score only gets worse :|  \nJust a quick Summary: \n1. Create extra features (optional) \n2.  Transform the data\n3.  PCA and reduction (optional) **Anyone have any luck with this?** \n\n a.  [Reddit post briefly mentioning PCA before autoencoder ](https://www.reddit.com/r/MachineLearning/comments/3wp5pc/results_on_svhn_with_vanilla_vae/)\n4.  SMOTE sampling method (doesn't seem to have any effect)\n5.  added some noise"},{"metadata":{"_uuid":"da5ec03f9d05a539f9ea01a1da4e8b6d1cc9353c"},"cell_type":"markdown","source":"Load Packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Import packages\nfrom __future__ import print_function\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport seaborn as sns\nimport gc\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom scipy.stats import norm, rankdata\n\nimport keras\nfrom keras import regularizers\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add,PReLU, LSTM\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d43c11622de7649d2edc957a360e5382ab517deb"},"cell_type":"markdown","source":"Reduce memory"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d11c2fee213fcb563fb8e75b678cec9cadb925b9"},"cell_type":"markdown","source":"Read in data and merge"},{"metadata":{"trusted":true,"_uuid":"5d7cdf4f441e431e81276cd443f9afb294128785"},"cell_type":"code","source":"#read in data\ntrain = reduce_mem_usage(pd.read_csv('../input/train.csv'))\ntest = reduce_mem_usage(pd.read_csv('../input/test.csv'))\n\n#Select features\nfeatures = [f for f in train if f not in ['ID_code','target']]\n\n#Join for easier feature engineering\ndf_original = pd.concat([train, test],axis=0,sort=False)\ndf = df_original[features]\ntarget = df_original['target'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5013b19011cbda5c272bdd3ff3d48e4d6a54795"},"cell_type":"markdown","source":"Extra features"},{"metadata":{"trusted":true,"_uuid":"bf982a71d887509276193f69ad03472a15879b66","_kg_hide-input":true},"cell_type":"code","source":"#for feature in features:\n#    df['mean_'+feature] = (train[feature].mean()-train[feature])\n#    df['z_'+feature] = (train[feature] - train[feature].mean())/train[feature].std(ddof=0)\n#    df['sq_'+feature] = (train[feature])**2\n#    df['sqrt_'+feature] = np.abs(train[feature])**(1/2)\n#    df['cp_'+feature] = pd.DataFrame(rankdata(train[feature]))\n#    df['cnp_'+feature] = pd.DataFrame((norm.cdf(train[feature])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6093f7e7917c65d9f760e1ee1e428b9317084123"},"cell_type":"markdown","source":"Transform data"},{"metadata":{"trusted":true,"_uuid":"36fc9658660407880d5dd77793cd41ee079e5279","_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"#Guass transformation\nfrom scipy.special import erfinv\ntrafo_columns = [c for c in df.columns if len(df[c].unique()) != 2]\nfor col in trafo_columns:\n    values = sorted(set(df[col]))\n    # Because erfinv(1) is inf, we shrink the range into (-0.9, 0.9)\n    f = pd.Series(np.linspace(-0.9, 0.9, len(values)), index=values)\n    f = np.sqrt(2) * erfinv(f)\n    f -= f.mean()\n    df[col] = df[col].map(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c007c09ec0ece8bd055a8106fe4b7619939eceb8"},"cell_type":"markdown","source":"PCA"},{"metadata":{"trusted":true,"_uuid":"e2583e788c49ffb2d60309e239bd3fbc16769665","_kg_hide-input":true},"cell_type":"code","source":"#from sklearn.decomposition import PCA\n#pca = PCA(n_components=200)\n#pca.fit(df[trafo_columns])\n#df = pca.transform(df[trafo_columns])\n#df = pd.DataFrame(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dede8dd79dc3f0af3423d0de12f9ba2c4e18616d"},"cell_type":"markdown","source":"Add target before we split"},{"metadata":{"trusted":true,"_uuid":"17eb6e166e47a2733551872ff72251321fa82a95","_kg_hide-input":true},"cell_type":"code","source":"df['target'] = df_original.target.values\ndf = reduce_mem_usage(df)\ntrain = df[df['target'].notnull()]\ntarget = train['target']\ntest = df[df['target'].isnull()]\ntrain.shape\n\ntrafo_columns = [c for c in train.columns if c not in ['target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07f98b7da28445479e173767d7555fd26f1bd207"},"cell_type":"markdown","source":"**Here is where I would really appreciate some help!** \nThe Smote sampling happens before the models are built and like I mentioned above, I add some noise as well.\nI decided to stratified the data first and then build VAEs for each of the splits.  Ill aggregate them all together in the end for a final blended reconstruction.  "},{"metadata":{"trusted":true,"_uuid":"21758761abd297e335a82c08e86696dc0896bbbb"},"cell_type":"code","source":"from keras.activations import elu\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Model\nfrom keras.objectives import binary_crossentropy\nfrom keras.callbacks import LearningRateScheduler\nfrom keras import backend as K\nfrom imblearn.keras import balanced_batch_generator\nfrom imblearn.under_sampling import NearMiss, RandomUnderSampler, CondensedNearestNeighbour, AllKNN, InstanceHardnessThreshold\nfrom sklearn.model_selection import KFold\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n\n\nnb_folds = 3\nnb_epoch = 3\nbatch_size = 128\nencoding_dim =1000\nhidden_dim = int(encoding_dim * 2) #i.e. 7\nsgd = SGD(lr=0.001, momentum=0.030, decay=0.76)\nfolds = StratifiedKFold(n_splits=nb_folds, shuffle=True, random_state=420)\n#folds = KFold(n_splits = nb_folds, random_state = 338, shuffle = True)\ntrain_auto = np.zeros(train[trafo_columns].shape)\ntest_auto = np.zeros(test[trafo_columns].shape)\npredictions = np.zeros(len(train))\nlabel_cols = [\"target\"]\ny_split = train[label_cols].values\n\ncp = ModelCheckpoint(filepath=\"autoencoder_0.h5\",\n                               save_best_only=True,\n                               verbose=0)\n\ntb = TensorBoard(log_dir='./logs',\n                histogram_freq=0,\n                write_graph=True,\n                write_images=True)\n\nes= EarlyStopping(monitor='val_acc',\n                  min_delta=0,\n                  patience=20,\n                  verbose=1, mode='min')\n\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(y_split[:,0], y_split[:,0])):\n    print(\"fold {}\".format(fold_))\n#for fold_, (trn_idx, val_idx) in enumerate(folds.split(train)):\n#    print(\"fold {}\".format(fold_))\n\n    trn_data, trn_y = train[trafo_columns].iloc[trn_idx], train['target'].iloc[trn_idx]\n    val_data, val_y = train[trafo_columns].iloc[val_idx], train['target'].iloc[val_idx]\n\n#    classes=[]\n#    for i in np.unique(trn_y):\n#        classes.append(i)\n#        print(\"Before OverSampling, counts of label \" + str(i) + \": {}\".format(sum(trn_y==i)))\n\n #   sm=SMOTE(random_state=2)\n #   trn_data, trn_y = sm.fit_sample(trn_data, trn_y.ravel())\n\n#    print('After OverSampling, the shape of train_X: {}'.format(trn_data.shape))\n#    print('After OverSampling, the shape of train_y: {} \\n'.format(train_y.shape))\n\n#    for eachClass in classes:\n#        print(\"After OverSampling, counts of label \" + str(eachClass) + \": {}\".format(sum(train_y==eachClass)))\n\n    input_dim = trn_data.shape[1] #num of columns, 30\n    input_layer = Input(shape=(input_dim, ))\n    \n    # Q(z|X) -- encoder\n    h_q = Dense(encoding_dim, activation='relu')(input_layer)\n    mu = Dense(hidden_dim, activation='linear')(h_q)\n    log_sigma = Dense(hidden_dim, activation='linear')(h_q)\n    \n    def sample_z(args):\n        mu, log_sigma = args\n        batch = K.shape(mu)[0]\n        dim = K.int_shape(mu)[1]\n        eps = K.random_normal(shape=(batch, dim))\n        return mu + K.exp(0.5 * log_sigma) * eps\n\n    # Sample z ~ Q(z|X)\n    z = Lambda(sample_z)([mu, log_sigma])\n    \n    # P(X|z) -- decoder\n    decoder_hidden = Dense(hidden_dim, activation='relu')\n    decoder_out = Dense(input_dim, activation='softmax')\n    h_p = decoder_hidden(z)\n    outputs = decoder_out(h_p)\n    \n    # Overall VAE model, for reconstruction and training\n    vae = Model(input_layer, outputs)\n    \n    # Encoder model, to encode input into latent variable\n    # We use the mean as the output as it is the center point, the representative of the gaussian\n    encoder = Model(input_layer, mu)\n\n    # Generator model, generate new data given latent variable z\n    d_in = Input(shape=(hidden_dim,))\n    d_h = decoder_hidden(d_in)\n    d_out = decoder_out(d_h)\n    decoder = Model(d_in, d_out)\n    \n    def vae_loss(y_true, y_pred):\n        \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n        # E[log P(X|z)]\n        recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)\n        # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n        kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)\n        return recon + kl\n    \n    vae.compile(optimizer='sgd', loss=vae_loss, metrics=['acc'])\n    \n    def add_noise(series, noise_level):\n        return series * (1 + noise_level * np.random.randn(series.shape[1]))\n    \n    noisy_trn_data = add_noise(trn_data, 0.07)\n#    noisy_val_data = add_noise(val_data, 0.07)\n    \n\n#    training_generator, steps_per_epoch = balanced_batch_generator(trn_data, train_y, sampler=RandomUnderSampler(),\n#                                                batch_size=batch_size, random_state=42)\n\n#    callback_history = vae.fit(training_generator,epochs=nb_epoch,\n#                       validation_data=[val_data, val_data], \n#                       steps_per_epoch=steps_per_epoch, verbose=1,\n#                       callbacks=[cp, tb, es])\n\n\n    vae.fit(noisy_trn_data, trn_data, batch_size=batch_size, epochs=nb_epoch, validation_data=[val_data, val_data], \n                               callbacks=[cp, tb, es]).history\n    train_auto[val_idx] += vae.predict(train.iloc[val_idx][trafo_columns], verbose=1)\n    test_auto += vae.predict(test[trafo_columns], verbose=1)\n\n    mse = vae.predict(train[trafo_columns] / folds.n_splits, verbose=1)\n    predictions += np.mean(np.power(train[trafo_columns] - mse, 2), axis=1)\n\ntrain_auto = pd.DataFrame(train_auto / folds.n_splits)\ntest_auto = pd.DataFrame(test_auto / folds.n_splits)\nerror_df = pd.DataFrame({'Reconstruction_error': predictions,\n                        'True_class': train['target']})\nerror_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"683b63e03066fd98af481d85e5efa7e9f92498cd"},"cell_type":"markdown","source":"Confusion matrix below with reconstruction error threshold: looking for a outlier cutoff.\nEvery variation of this modelI gives me the same output; I cant seem to find a way to improve my score or my model."},{"metadata":{"trusted":true,"_uuid":"d9dea3b5a6eddec5073d61499f37b1cfb19ab4b1"},"cell_type":"code","source":"####Edit thiS!!!!!####\nfrom sklearn.metrics import confusion_matrix\n\nLABELS = [\"Normal\",\"Outlier\"]\nthreshold_fixed = 2.056936\n\npred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\nconf_matrix = confusion_matrix(train['target'], pred_y)\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c7e398b14764a0fab2000a3766559d58d95cbac"},"cell_type":"markdown","source":"Thanks for reading!!!!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}