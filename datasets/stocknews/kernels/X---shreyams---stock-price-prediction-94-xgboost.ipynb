{"nbformat_minor": 1, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "8f80f922-983f-4c43-88dc-47cc235213c7", "_uuid": "31b308c23b0f7aae9687dbb7b9e72accb299f7a0"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<h1>Stock Market Price Prediction with New Data</h1>\n", "\n", "<p><b>Breif Overview:</b> \n", "    <br/><br/>\n", "The model created below is for prediction the stock prices of a Company.\n", "<br/>\n", "There are two datasets\n", "<br/><br/>\n", "1. Stock Prices Dataset for Dow Jones Inc\n", "<br/><br/>\n", "2. Top 25 headlines for everyday for the past 8 years\n", "<br/><br/>\n", "The notebook is briefly summarized as follows:\n", "<br/><br/>\n", "1. Data Preparation - Preparing data for evaluation.\n", "<br/><br/>\n", "2. Data Quality Checks - Performing basic checks on data for better understanding of data.\n", "<br/><br/>\n", "3. Feature inspection and filtering - Correlation and feature Mutual information plots against the target variable. Inspection of the Binary, categorical and other variables.\n", "<br/><br/>\n", "4. Feature importance ranking via learning models\n", "<br/><br/>\n", "5. Training - training data against multiple machine learning algorthms and fine tuning a couple of algorithms for accuracy\n", "    <br/>\n", "</p> </div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "753c7c7e-9532-485c-b594-7d3dc968d355", "_uuid": "beb30caa6b428e2d23ec218757916b042a4fe2b0"}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import warnings\n", "from matplotlib import pyplot\n", "#from pandas import read_csv, set_option\n", "from pandas import Series, datetime\n", "from pandas.tools.plotting import scatter_matrix, autocorrelation_plot\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, TimeSeriesSplit\n", "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.svm import SVC\n", "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n", "from sklearn.metrics import roc_curve, auc\n", "import matplotlib.pyplot as plt\n", "import random\n", "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n", "from statsmodels.tsa.arima_model import ARIMA\n", "from xgboost import XGBClassifier\n", "import seaborn as sns"], "execution_count": 1}, {"cell_type": "markdown", "metadata": {"_cell_guid": "eb0ed3be-170b-417e-9c40-a739d7602fcf", "_uuid": "dc2f721a37fbbe051eac5149e12cfe2fd8a4204a"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<h3>1. Data Preparation:</h3>\n", "<br/>\n", "Imported all the necessary modules for the project\n", "<br/><br/>\n", "Loaded the dataset as a dataframe and parsed the date column to be read by the dataframe as dates type\n", "Checked the top 5 rows of the dataframe to see how the columns are aligned.\n", "<br/><br/>\n", "The 'combined_stock_data.csv' initially only had the headlines(Top1 through Top25). Each row was iterated over an algorithm which generated the Subjectivity, Objectivity, Positive, Negative, Neutral sentiments of the respective headlines of each row.\n", "<br/><br/>\n", "The algorithm was accepting only a single sentence and was providing the respective sentiments in percentage. I modified the algorithm iterate over all of the individuals rows and simultaneously create the Subjectivity, Objectivity, Negative, Positive, Neutral values and assign itself to the columns in the dataframe.\n", "<br/><br/>\n", "The headlines Top1 through Top25 were concatenated and then passed on to the algorithm\n", "<br/><br/>\n", "The original algorithm : https://github.com/nik0spapp/usent\n", "<br/><br/>\n", "Modified algorithm : https://github.com/ShreyamsJain/Stock-Price-Prediction-Model/blob/master/Sentence_Polarity/sentiment.py\n", "<br/>\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "ad52177a-8beb-42aa-a2ff-ed2fc1145c4e", "collapsed": true, "_uuid": "f02a83dfb1b6eaf9aae47dffd881508e082a70b2"}, "outputs": [], "source": ["# Loading the dataset to a dataframe\n", "sentence_file = \"../input/headlinespolarity/combined_stock_data.csv\"\n", "sentence_df = pd.read_csv(sentence_file, parse_dates=[1])"], "execution_count": 2}, {"cell_type": "code", "metadata": {"_cell_guid": "29bebcce-0739-4640-99be-aae365fd2a6b", "_uuid": "76f4245494deea58e81bf79997de2a2eda3696fd"}, "outputs": [], "source": ["sentence_df.head()"], "execution_count": 3}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0e7ac6f4-b56c-4c07-973f-a659c300e802", "_uuid": "7f58e84d56f7cccbf31ddcdf2b3913e6fead05f8"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Checked the datatypes of all of the columns. Below is the list of data types\n", "<p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "51cfb668-b3e4-40d1-9211-fe27650f7e75", "_uuid": "cd95cc6f3a27542a597619ac830b88563106bafd"}, "outputs": [], "source": ["# Check the shape and data types of the dataframe\n", "print(sentence_df.shape)\n", "print(sentence_df.dtypes)"], "execution_count": 4}, {"cell_type": "markdown", "metadata": {"_cell_guid": "3bf5c86e-96fe-41d8-aed4-b74bfa35138b", "_uuid": "f4863ece4805efeb0ad4cda94616aa7ffca26ed2"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Load the Dow Jones dataset to a dataframe stock_data which contains 8 years of Stock Price data.\n", "<br/><br/>\n", "Parse the date as a date type and check the top 5 rows of the dataframe.\n", "<br/><br/>\n", "Checked the top 5 rows of the dataframe\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "c3220239-587e-4177-a7d0-806b1fceacde", "_uuid": "a45877b8c79a4b82b2be3b8762392f7da828ab10"}, "outputs": [], "source": ["# Load the stock prices dataset into a dataframe and check the top 5 rows\n", "stock_prices = \"../input/stocknews/DJIA_table.csv\"\n", "stock_data = pd.read_csv(stock_prices, parse_dates=[0])\n", "stock_data.head()"], "execution_count": 5}, {"cell_type": "markdown", "metadata": {"_cell_guid": "76747a2f-d07f-4507-b360-28125451e672", "_uuid": "27c5192b2682799a1cb779652d22610534f11d15"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Checked the shape and datatypes of the loaded dataset\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "812d3a7e-ca1b-4674-a3f6-b7fab9890c9e", "_uuid": "36ab4e5504dc3165eafb9e67ac661f0fbdc2822f"}, "outputs": [], "source": ["# Check the shape and datatypes of the stock prices dataframe\n", "print(stock_data.shape)\n", "print(stock_data.dtypes)"], "execution_count": 6}, {"cell_type": "markdown", "metadata": {"_cell_guid": "9a9c26bd-38e4-4119-8175-8e5ac4c6387d", "_uuid": "6e7ebe093e51d93130b6c95425a1cb80ab017ace"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Merged the 5 columns(Subjectivity, Objectivity, Positive, Negative, Neutral) with the stock_data dataframe.\n", "<br/><br/>\n", "Validated the merged dataframe to see the 2 dataframes are concatenated by checking the top 5 rows of the merged_dataframe.\n", "</p>\n", "</div> "]}, {"cell_type": "code", "metadata": {"_cell_guid": "d3a9b4c5-b36c-4d52-b2cf-9b94df974b6d", "_uuid": "36c52bdf10b30cfc1a6f33bfbb9b28cf1fa98a37"}, "outputs": [], "source": ["# Create a dataframe by merging the headlines and the stock prices dataframe\n", "merged_dataframe = sentence_df[['Date', 'Label', 'Subjectivity', 'Objectivity', 'Positive', 'Negative', 'Neutral']].merge(stock_data, how='inner', on='Date', left_index=True)\n", "# Check the shape and top 5 rows of the merged dataframe\n", "print(merged_dataframe.shape)\n", "merged_dataframe.head()"], "execution_count": 7}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e6fa16eb-3456-439f-90ab-6c6879e8a5c3", "_uuid": "4b7a0445514836133280d33d9af4ec7fbea33a9d"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "We have the Label(i.e the output column) column in the 2nd position.\n", "<br/><br/>\n", "Lets move it to the end of the dataframe to have a clear view of inputs and outputs\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "0553ad04-a5e7-4498-b6d8-9da9e13b602d", "_uuid": "d2fd797ea4f48d701df5ceaa93428c6397cee25f"}, "outputs": [], "source": ["# Push the Label column to the end of the dataframe\n", "cols = list(merged_dataframe)\n", "print(cols)\n", "cols.append(cols.pop(cols.index('Label')))\n", "merged_dataframe = merged_dataframe.ix[:, cols]\n", "merged_dataframe.head()"], "execution_count": 8}, {"cell_type": "markdown", "metadata": {"_cell_guid": "1f640516-ad70-432c-8bc5-1d1b8adf0a2e", "_uuid": "861a4b4dac2ae2b62a931e53853d0db87f1c0078"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "We have the volumn column in Integer format. Lets change it to float, same as the rest of the columns so we do not have any difficulties in making calculations at a later point.\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "d8755480-5501-442a-976a-07c7e01af645", "_uuid": "64d08ffb756bdfa027c82138c3acf5c781288f76"}, "outputs": [], "source": ["# Change the datatype of the volume column to float\n", "#merged_dataframe['Date'] = pd.to_datetime(merged_dataframe['Date'])\n", "merged_dataframe['Volume'] = merged_dataframe['Volume'].astype(float)\n", "print(cols)\n", "#merged_dataframe = merged_dataframe.set_index(['Date'])\n", "merged_dataframe.index = merged_dataframe.index.sort_values()\n", "merged_dataframe.head()"], "execution_count": 9}, {"cell_type": "markdown", "metadata": {"_cell_guid": "303521cc-acfd-4bf3-9cd4-7fb3939e6425", "_uuid": "f44f205b7286771d1bd2be2a59e9889d435e9c10"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "<h3>2. Data Quality Checks:</h3>\n", "<br/>\n", "Checked the statistics of individual columns in the dataframe.\n", "<br/><br/>\n", "As you can see below there are no outliers in any of the columns, however, some of the columns have NaN values\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "b63aeb60-0594-49a0-a64e-30e74c3f60da", "_uuid": "c99a007b729cf96c0b5bd523207307ebbeeccaf9"}, "outputs": [], "source": ["# Check the statistics of the columns of the merged dataframe and check for outliers\n", "print(merged_dataframe.describe())"], "execution_count": 10}, {"cell_type": "markdown", "metadata": {"_cell_guid": "24f78545-5e25-475c-a667-d990e3fbf9a4", "_uuid": "1e76cfd8e6f07a9abe1352f1fc9453bcc296f2f7"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Plotted histograms for individual columns to see the distribution of values.\n", "<br/><br/>\n", "The x axis is the column values and the y axis is the frequency of those values.\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "cb757964-b848-499d-8517-d1f99b09f68c", "_uuid": "abaef34564c5753f19b7402441827857552796de"}, "outputs": [], "source": ["# Plot a histogram for all the columns of the dataframe. This shows the frequency of values in all the columns\n", "sns.set()\n", "merged_dataframe.hist(sharex = False, sharey = False, xlabelsize = 4, ylabelsize = 4, figsize=(10, 10))\n", "pyplot.show()"], "execution_count": 11}, {"cell_type": "markdown", "metadata": {"_cell_guid": "9845c134-3e70-4168-9fd0-6191449eca42", "_uuid": "993f86363c0b55f707ebea03d3d99e519d827db6"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Plot 1: Scatter plot of Stock Prices vs the Subjectivity.<br/>\n", "        Stock Value of 0 means the Stock Value reduced since the previous day.<br/>\n", "        Stock Value of 1 means the Stock Value increased or remained the same since the previous day.\n", "<br/>        \n", "Plot 2: Scatter plot of Stock Prices vs the Objectivity.<br/>\n", "        Stock Value of 0 means the Stock Value reduced since the previous day.<br/>\n", "        Stock Value of 1 means the Stock Value increased or remained the same since the previous day.\n", "<br/>                \n", "Plot 3: Histogram of Subjectivity column.<br/>\n", "        The x axis are the values of Subjectivity and y axis is its respective frequency.<br/>\n", "        The plot seems to be normally distributed.\n", "<br/>       \n", "Plot 4: Histogram of Objectivity column.<br/>\n", "        The x axis are the values of Objectivity and y axis is its respective frequency.<br/>\n", "        The plot seems to be normally distributed.<br/>\n", "    </p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "a12031e0-f610-44f7-a339-a299bbe6efde", "_uuid": "7b2aa29136fffed22a6b8ff4b82d78c36f9408f6"}, "outputs": [], "source": ["pyplot.scatter(merged_dataframe['Subjectivity'], merged_dataframe['Label'])\n", "pyplot.xlabel('Subjectivity')\n", "pyplot.ylabel('Stock Price Up or Down 0: Down, 1: Up')\n", "pyplot.show()\n", "pyplot.scatter(merged_dataframe['Objectivity'], merged_dataframe['Label'])\n", "pyplot.xlabel('Objectivity')\n", "pyplot.ylabel('Stock Price Up or Down 0: Down, 1: Up')\n", "pyplot.show()\n", "merged_dataframe['Subjectivity'].plot('hist')\n", "pyplot.xlabel('Subjectivity')\n", "pyplot.ylabel('Frequency')\n", "pyplot.show()\n", "merged_dataframe['Objectivity'].plot('hist')\n", "pyplot.xlabel('Subjectivity')\n", "pyplot.ylabel('Frequency')\n", "pyplot.show()\n", "print(\"Size of the Labels column\")\n", "print(merged_dataframe.groupby('Label').size())"], "execution_count": 12}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e2cc4343-be16-44a5-b08c-a94c017a94dc", "_uuid": "7fc30252326a91f80242d2fcbc14877f14ae5cc3"}, "source": ["<div class=\"span5 alert alert-info\"><p>\n", "<h3>3.Feature inspection and filtering</h3>\n", "<br/>\n", "Lets check for NaN values in individual columns of the dataframe.\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "9ff2f1fd-995e-4c59-a9cb-70b3e9202e99", "_uuid": "82f3d3a156a9290650c97dcd588678f615295487"}, "outputs": [], "source": ["md_copy = merged_dataframe\n", "md_copy = md_copy.replace(-1, np.NaN)\n", "import missingno as msno\n", "# Nullity or missing values by columns\n", "msno.matrix(df=md_copy.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))"], "execution_count": 13}, {"cell_type": "markdown", "metadata": {"_cell_guid": "eab7da0c-92bd-4c92-890f-1387f35568cd", "_uuid": "a0c827789a3e74e33e2a2774870cc8d7338f2c95"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "<h4>Correlation Map for features:</h4>\n", "<br/>\n", "Now, we will plot a heat map and a scatter matrix to see the correlation of the columns with each other.\n", "<br/><br/>\n", "You can see the heat map with pearson correlation values in the plot below.\n", "<br/><br/>\n", "This gave me a better understanding to see if there are any dependant variables or if any of the variables are highly correlated.\n", "<br/><br/>\n", "Some variables Subjectivity, Objectivity are negatively correlated. There are very few variables which seem to have a very high correlation. Thus, at this point we can conclude that we do not need any sort of dimensionality reduction technique to be applied.\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "2aec418c-5b88-42ab-a0fc-9c88a743a30e", "scrolled": false, "_uuid": "5a0ccae6a9fab23565e8a926d73ca926007dd259"}, "outputs": [], "source": ["colormap = pyplot.cm.afmhot\n", "pyplot.figure(figsize=(16,12))\n", "pyplot.title('Pearson correlation of continuous features', y=1.05, size=15)\n", "sns.heatmap(merged_dataframe.corr(),linewidths=0.1,vmax=1.0, square=True, \n", "            cmap=colormap, linecolor='white', annot=True)\n", "pyplot.show()"], "execution_count": 14}, {"cell_type": "code", "metadata": {"_cell_guid": "c7af577e-dea8-41d4-9f13-860705048f5f", "_uuid": "90df12bc5f52c85f482d2b5312f1185c00c603a2"}, "outputs": [], "source": ["%matplotlib inline\n", "import plotly.offline as py\n", "py.init_notebook_mode(connected=True)\n", "import plotly.graph_objs as go\n", "import plotly.tools as tls\n", "import warnings\n", "\n", "bin_col = merged_dataframe.columns\n", "zero_list = []\n", "one_list = []\n", "for col in bin_col:\n", "    zero_count = 0\n", "    one_count = 0\n", "    for ix, val in merged_dataframe[col].iteritems():\n", "        if merged_dataframe.loc[ix, 'Label'] == 0:\n", "            zero_count += 1\n", "        else:\n", "            one_count += 1\n", "    zero_list.append(zero_count)\n", "    one_list.append(one_count)\n", "    \n", "trace1 = go.Bar(\n", "    x=bin_col,\n", "    y=zero_list ,\n", "    name='Zero count'\n", ")\n", "trace2 = go.Bar(\n", "    x=bin_col,\n", "    y=one_list,\n", "    name='One count'\n", ")\n", "\n", "data = [trace1, trace2]\n", "layout = go.Layout(\n", "    barmode='stack',\n", "    title='Count of 1 and 0 in binary variables'\n", ")\n", "\n", "fig = go.Figure(data=data, layout=layout)\n", "py.iplot(fig, filename='stacked-bar')"], "execution_count": 15}, {"cell_type": "markdown", "metadata": {"_cell_guid": "23a3f7fe-c18f-420e-a562-0fd8bafb5697", "_uuid": "7f1967f3dc254bd0e13eb66e201bdf913f8f88e3"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "<h3>4. Training</h3>\n", "<br/>\n", "Recheck the dataframe to see if the dataset is ready for train.\n", "<br/><br/>\n", "There are certain NaN values in many columns of the dataframe.\n", "<br/><br/>\n", "Replace the NaN values with the mean values of the respective column.\n", "<br/><br/>\n", "Split the merged dataframe to inputs(X) and outputs(y)\n", "<br/><br/>\n", "In our dataset, we have columns Subjectivity through Adj Close as inputs and the Label column output.\n", "<br/><br/>\n", "Now, we will split our dataset to training and test samples. Lets train out model on first 80% of the data \n", "and test our prediction model on remaining 20% of the data.\n", "<br/><br/>\n", "As this is a time series, it is important we do not randomly pick training and testing samples.\n", "<br/><br/>\n", "Lets consider a few machine learning algorithms to perform our training on.\n", "Logistic Regression\n", "Linear Discriminant Analysis\n", "K Nearest Neighbors\n", "Decision trees\n", "Naive Bayes\n", "Support Vector Classifier\n", "Random Forest Classifier\n", "<br/><br/>\n", "Lets add all of these classifiers to a list 'models'\n", "<br/><br/>\n", "After splitting the dataset, we can see that there are 1393 samples for training and 597 samples for testing\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "bdfdadf2-2bab-4b29-a3b5-9fcbe7a106bb", "_uuid": "11c076755df2cad209dca2723527bb6ba9bc71cf"}, "outputs": [], "source": ["# Print the datatypes and count of the dataframe\n", "print(merged_dataframe.dtypes)\n", "print(merged_dataframe.count())\n", "# Change the NaN values to the mean value of that column\n", "nan_list = ['Subjectivity', 'Objectivity', 'Positive', 'Negative', 'Neutral']\n", "for col in nan_list:\n", "    merged_dataframe[col] = merged_dataframe[col].fillna(merged_dataframe[col].mean())\n", "\n", "# Recheck the count\n", "print(merged_dataframe.count())\n", "# Separate the dataframe for input(X) and output variables(y)\n", "X = merged_dataframe.loc[:,'Subjectivity':'Adj Close']\n", "y = merged_dataframe.loc[:,'Label']\n", "# Set the validation size, i.e the test set to 20%\n", "validation_size = 0.20\n", "# Split the dataset to test and train sets\n", "# Split the initial 70% of the data as training set and the remaining 30% data as the testing set\n", "train_size = int(len(X.index) * 0.7)\n", "print(len(y))\n", "print(train_size)\n", "X_train, X_test = X.loc[0:train_size, :], X.loc[train_size: len(X.index), :]\n", "y_train, y_test = y[0:train_size+1], y.loc[train_size: len(X.index)]\n", "print('Observations: %d' % (len(X.index)))\n", "print('X Training Observations: %d' % (len(X_train.index)))\n", "print('X Testing Observations: %d' % (len(X_test.index)))\n", "print('y Training Observations: %d' % (len(y_train)))\n", "print('y Testing Observations: %d' % (len(y_test)))\n", "pyplot.plot(X_train['Objectivity'])\n", "pyplot.plot([None for i in X_train['Objectivity']] + [x for x in X_test['Objectivity']])\n", "pyplot.show()\n", "num_folds = 10\n", "scoring = 'accuracy'\n", "# Append the models to the models list\n", "models = []\n", "models.append(('LR' , LogisticRegression()))\n", "models.append(('LDA' , LinearDiscriminantAnalysis()))\n", "models.append(('KNN' , KNeighborsClassifier()))\n", "models.append(('CART' , DecisionTreeClassifier()))\n", "models.append(('NB' , GaussianNB()))\n", "models.append(('SVM' , SVC()))\n", "models.append(('RF' , RandomForestClassifier(n_estimators=50)))\n", "models.append(('XGBoost', XGBClassifier()))"], "execution_count": 16}, {"cell_type": "markdown", "metadata": {"_cell_guid": "4c432246-60ab-4553-a138-881f9c2eb046", "_uuid": "e12c71a94c1173f333d34eecc767055038f17fdb"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Now, we will iterate over all of the machine learning classifiers and in each loop , we will train against the\n", "algorithm, predict the outputs with inputs from the testing split.\n", "<br/><br/>\n", "The actual and the predicted outputs are compared to calculate the accuracy.\n", "<br/><br/>\n", "We see that LDA seems to be giving a high accuracy score, but accuracy is still not the most trustworthy measure.\n", "</p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "b8160f68-03a6-4f38-9d09-9ab60f144ba5", "_uuid": "f1267c86f04d6b57a21022e8fc58ea073734ad86"}, "outputs": [], "source": ["# Evaluate each algorithm for accuracy\n", "results = []\n", "names = []\n", "'''\n", "for name, model in models:\n", "    kfold = KFold(n_splits=num_folds, random_state=42)\n", "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg) '''\n", "\n", "for name, model in models:\n", "    clf = model\n", "    clf.fit(X_train, y_train)\n", "    y_pred = clf.predict(X_test)\n", "    accu_score = accuracy_score(y_test, y_pred)\n", "    print(name + \": \" + str(accu_score))"], "execution_count": 17}, {"cell_type": "markdown", "metadata": {"_cell_guid": "7909402a-ea7d-47e2-acde-aa131a62d59f", "_uuid": "d40e06fdc86c110f0b7e991998c37ec402caaa1b"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "As data distributions are in varying ranges, it would be good to scale all of our data and then use it to train our \n", "algorithm.\n", "<br/><br/>\n", "Lets print out the accuracy score, confusion matrix.\n", "    </p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "0205a494-a785-414e-835a-2768ab5aee5f", "_uuid": "e624dab4727b70aa409e0448f744aa8938c5f1b3"}, "outputs": [], "source": ["# prepare the model LDA\n", "scaler = StandardScaler().fit(X_train)\n", "rescaledX = scaler.transform(X_train)\n", "model_lda = LinearDiscriminantAnalysis()\n", "model_lda.fit(rescaledX, y_train)\n", "# estimate accuracy on validation dataset\n", "rescaledValidationX = scaler.transform(X_test)\n", "predictions = model_lda.predict(rescaledValidationX)\n", "print(\"accuracy score:\")\n", "print(accuracy_score(y_test, predictions))\n", "print(\"confusion matrix: \")\n", "print(confusion_matrix(y_test, predictions))\n", "print(\"classification report: \")\n", "print(classification_report(y_test, predictions))\n", "\n", "model_xgb = XGBClassifier()\n", "model_xgb.fit(rescaledX, y_train)\n", "# estimate accuracy on validation dataset\n", "rescaledValidationX = scaler.transform(X_test)\n", "predictions = model_xgb.predict(rescaledValidationX)\n", "print(\"accuracy score:\")\n", "print(accuracy_score(y_test, predictions))\n", "print(\"confusion matrix: \")\n", "print(confusion_matrix(y_test, predictions))\n", "print(\"classification report: \")\n", "print(classification_report(y_test, predictions))\n"], "execution_count": 18}, {"cell_type": "code", "metadata": {"_cell_guid": "c701b10b-e9ba-45ed-b483-a8bf12ff8021", "_uuid": "8abc110725cf391395b5928a075a8576e7870292"}, "outputs": [], "source": ["# Generating the ROC curve\n", "y_pred_proba = model_lda.predict_proba(X_test)[:,1]\n", "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n", "roc_auc = auc(fpr, tpr)\n", "\n", "# Plot ROC curve\n", "print(\"roc auc is :\" + str(roc_auc))\n", "pyplot.plot([0, 1], [0, 1], 'k--')\n", "pyplot.plot(fpr, tpr)\n", "pyplot.xlabel('False Positive Rate')\n", "pyplot.ylabel('True Positive Rate')\n", "pyplot.title('ROC Curve')\n", "pyplot.show()\n", "\n", "# AUC score using cross validation\n", "kfold_val = KFold(n_splits=num_folds, random_state=42)\n", "auc_score = cross_val_score(model_lda, X_test, y_test, cv=kfold_val, scoring='roc_auc')\n", "print(\"AUC using cross val: \" + str(auc_score))\n", "mean_auc = np.mean(auc_score)\n", "print(\"Mean AUC score is: \" + str(mean_auc))"], "execution_count": 19}, {"cell_type": "code", "metadata": {"_cell_guid": "962361a6-3e98-4983-b49e-303548a81c27", "_uuid": "d24a4e01a00a622edc1a19213f02077130723c9e"}, "outputs": [], "source": ["# Scaling Random Forests\n", "\n", "model_rf = RandomForestClassifier(n_estimators=1000)\n", "model_rf.fit(rescaledX, y_train)\n", "# estimate accuracy on validation dataset\n", "rescaledValidationX = scaler.transform(X_test)\n", "predictions = model_rf.predict(rescaledValidationX)\n", "print(\"accuracy score:\")\n", "print(accuracy_score(y_test, predictions))\n", "print(\"confusion matrix: \")\n", "print(confusion_matrix(y_test, predictions))\n", "print(\"classification report: \")\n", "print(classification_report(y_test, predictions))\n"], "execution_count": 20}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b5315ffa-62e6-4c52-835e-d25967793f40", "_uuid": "0f177c02c38ef5081997b347803f9884dc6e4461"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "<h3>5. Feature Importances:</h3>\n", "<br/>    \n", "Below you can find the feature with highest to least important features plotted in the graph.\n", "<br/><br/>\n", "This is for XGBoost.\n", "</p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "e535e555-8da0-4cf0-b1d0-1ef8b51d95eb", "_uuid": "f4459cee5be0caef36ccba1d4766b49b1198b3d5"}, "outputs": [], "source": ["features = merged_dataframe.drop(['Label'],axis=1).columns.values\n", "\n", "x, y = (list(x) for x in zip(*sorted(zip(model_xgb.feature_importances_, features), \n", "                                                            reverse = False)))\n", "trace2 = go.Bar(\n", "    x=x ,\n", "    y=y,\n", "    marker=dict(\n", "        color=x,\n", "        colorscale = 'Viridis',\n", "        reversescale = True\n", "    ),\n", "    name='Feature importance for XGBoost',\n", "    orientation='h',\n", ")\n", "\n", "layout = dict(\n", "    title='Barplot of Feature importances for XGBoost',\n", "     width = 1000, height = 1000,\n", "    yaxis=dict(\n", "        showgrid=False,\n", "        showline=False,\n", "        showticklabels=True,\n", "#         domain=[0, 0.85],\n", "    ))\n", "\n", "fig1 = go.Figure(data=[trace2])\n", "fig1['layout'].update(layout)\n", "py.iplot(fig1, filename='plots')\n"], "execution_count": 21}, {"cell_type": "markdown", "metadata": {"_cell_guid": "2b724f2e-80b3-4fd9-b97b-2d2cc472047d", "_uuid": "6dc8efa4a177d7d3d9e59a1235451a598e9a9755"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Below is the feature importance graph for Random Forests.\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "3e7a8a14-b213-41e9-864e-b6340c8d4cb7", "scrolled": false, "_uuid": "2d25e8b08e457b0cee268b079bff54493dfd84d8"}, "outputs": [], "source": ["x, y = (list(x) for x in zip(*sorted(zip(model_rf.feature_importances_, features), \n", "                                                            reverse = False)))\n", "trace2 = go.Bar(\n", "    x=x ,\n", "    y=y,\n", "    marker=dict(\n", "        color=x,\n", "        colorscale = 'Viridis',\n", "        reversescale = True\n", "    ),\n", "    name='Feature importance for Random Forests',\n", "    orientation='h',\n", ")\n", "\n", "layout = dict(\n", "    title='Barplot of Feature importances for Random Forests',\n", "     width = 1000, height = 1000,\n", "    yaxis=dict(\n", "        showgrid=False,\n", "        showline=False,\n", "        showticklabels=True,\n", "#         domain=[0, 0.85],\n", "    ))\n", "\n", "fig1 = go.Figure(data=[trace2])\n", "fig1['layout'].update(layout)\n", "py.iplot(fig1, filename='plots')\n"], "execution_count": 22}, {"cell_type": "markdown", "metadata": {"_cell_guid": "ab2c0b25-d06c-40b4-b221-b00d23d9b90f", "_uuid": "28e4b8a0852d6906f7d7bb9d92ccfa4264951a1c"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "<h3>Fine Tuning XGBoost</h3>\n", "<br>\n", "As of now the model that seems to be performing the best is the XGBoost model.\n", "<br/><br/>\n", "Lets see if we can fine tune it further to increase the accuracy of the model.\n", "</p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "60962a15-1c86-41bf-8c26-ef4e7a379fe4", "scrolled": false, "_uuid": "f2e017318fca766f0fc06ad5131b57a21913f5a7"}, "outputs": [], "source": ["# XGBoost on Stock Price dataset, Tune n_estimators and max_depth\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.preprocessing import LabelEncoder\n", "import matplotlib\n", "\n", "matplotlib.use('Agg')\n", "model = XGBClassifier()\n", "n_estimators = [150, 200, 250, 450, 500, 550, 1000]\n", "max_depth = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n", "print(max_depth)\n", "best_depth = 0\n", "best_estimator = 0\n", "max_score = 0\n", "for n in n_estimators:\n", "    for md in max_depth:\n", "        model = XGBClassifier(n_estimators=n, max_depth=md)\n", "        model.fit(X_train, y_train)\n", "        y_pred = model.predict(X_test)\n", "        score = accuracy_score(y_test, y_pred)\n", "        if score > max_score:\n", "            max_score = score\n", "            best_depth = md\n", "            best_estimator = n\n", "        print(\"Score is \" + str(score) + \" at depth of \" + str(md) + \" and estimator \" + str(n))\n", "print(\"Best score is \" + str(max_score) + \" at depth of \" + str(best_depth) + \" and estimator of \" + str(best_estimator))\n"], "execution_count": 23}, {"cell_type": "markdown", "metadata": {"_cell_guid": "72c9390e-3db6-478b-b53e-e2bdb9deff8a", "_uuid": "2f12ed6c598ad15b2b7e8c67a6003c8e5b34053b"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<h3> Fine tuning with important features:</h3>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "58995d46-2950-4414-93d8-f7ae38c7207c", "_uuid": "2d18f4829a1ab74a9691cef007328c03c75b42cf"}, "outputs": [], "source": ["imp_features_df = merged_dataframe[['Low', \"Neutral\", 'Close', 'Objectivity', 'Date']]\n", "Xi_train, Xi_test = X.loc[0:train_size, :], X.loc[train_size: len(X.index), :]\n", "clf = XGBClassifier(n_estimators=500, max_depth=3)\n", "clf.fit(Xi_train, y_train)\n", "yi_pred = clf.predict(Xi_test)\n", "score = accuracy_score(y_test, yi_pred)\n", "print(\"Score is \"+ str(score))\n"], "execution_count": 24}, {"cell_type": "markdown", "metadata": {"_cell_guid": "9665a091-e698-4df9-9573-6959c2dd9490", "_uuid": "706734a489eeaa8783d1d00c3a0c017ff3ef7ad2"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<h3>PCA transformation:</h3>\n", "    </div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "67d053a4-dc02-4086-9315-ec14cd12db0d", "_uuid": "752e6c265fd1e9b885f6d4b6b166cc2b23c3e531"}, "outputs": [], "source": ["from sklearn.decomposition import PCA\n", "\n", "pca = PCA(n_components=3)\n", "pca.fit(X)\n", "transformed = pca.transform(X)\n", "\n", "transformed.shape\n", "print(type(transformed))"], "execution_count": 25}, {"cell_type": "code", "metadata": {"_cell_guid": "dc81a32a-2dae-4030-ae69-5b3e94c8eedd", "_uuid": "59d83c3b159ffe928017296398df60a62e6360c4"}, "outputs": [], "source": ["pca_df = pd.DataFrame(transformed)\n", "\n", "X_train_pca, X_test_pca = pca_df.loc[0:train_size, :], pca_df.loc[train_size: len(X.index), :]\n", "\n", "clf = XGBClassifier(n_estimators=500, max_depth=3)\n", "clf.fit(X_train_pca, y_train)\n", "y_pred_pca = clf.predict(X_test_pca)\n", "score = accuracy_score(y_test, y_pred_pca)\n", "print(\"Score is \"+ str(score))\n"], "execution_count": 26}, {"cell_type": "code", "metadata": {"_cell_guid": "7a064ee8-bcaa-4939-80c6-d88a89464863", "_uuid": "8d3c85259402544163076f23afca7451c148e0a2"}, "outputs": [], "source": ["pca_matrix = confusion_matrix(y_test, y_pred_pca)\n", "pca_report = classification_report(y_test, y_pred_pca)\n", "print(\"Confusion Matrix: \\n\" + str(pca_matrix))\n", "print(\"Classification report: \\n\" + str(pca_report))"], "execution_count": 27}, {"cell_type": "code", "metadata": {"_cell_guid": "a57c5e86-3c36-4147-ae34-c824810d2fb4", "_uuid": "4f0494def5eaecdac1b00aa6ce3a8c0c1753616d"}, "outputs": [], "source": ["# Generating the ROC curve\n", "y_pred_proba_pca = clf.predict_proba(X_test_pca)[:,1]\n", "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_pca)\n", "roc_auc = auc(fpr, tpr)\n", "print(\"AUC score is \" + str(roc_auc))\n", "\n", "# Plot ROC curve\n", "print(\"roc auc is :\" + str(roc_auc))\n", "pyplot.plot([0, 1], [0, 1], 'k--')\n", "pyplot.plot(fpr, tpr)\n", "pyplot.xlabel('False Positive Rate')\n", "pyplot.ylabel('True Positive Rate')\n", "pyplot.title('ROC Curve')\n", "pyplot.show()\n"], "execution_count": 28}, {"cell_type": "markdown", "metadata": {"_cell_guid": "502ea190-2e32-4fae-9378-c2137c2bffb1", "_uuid": "af96fd82c109e844b17bbd33411e324a260dd137"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "    <br/>\n", "Now lets try and train our data using a TimeSeriesSplit which is specifically used for splitting the dataset to \n", "training and testing datasets.\n", "<br/><br/>\n", "By specifying the number of splits, we can split the data on a sample of 40%, 70% and 100% of the dataset.\n", "<br/><br/>\n", "The plots below shows the splits of the datasets and the respective number of samples in each split.\n", "</p>\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "66753dc7-b0c5-4a73-8a15-b1ee251ac4bf", "collapsed": true, "_uuid": "332ef93d6c157fc544232bbcb0675d438e4517bf"}, "outputs": [], "source": ["splits = TimeSeriesSplit(n_splits=3)\n", "pyplot.figure(1)\n", "index = 1\n", "print(splits.split(X))\n", "for X_train_index, X_test_index in splits.split(X):\n", "    X1_train = X.loc[X_train_index, :]\n", "    X1_test = X.loc[X_test_index, :]\n", "    print('Observations: %d' % (len(X1_train) + len(X1_test)))\n", "    print('Training Observations: %d' % (len(X1_train)))\n", "    print('Testing Observations: %d' % (len(X1_test)))\n", "    pyplot.subplot(310 + index)\n", "    pyplot.plot(X1_train['Objectivity'])\n", "    pyplot.plot([None for i in X1_train['Objectivity']] + [x for x in X1_test['Objectivity']])\n", "    index += 1\n", "pyplot.show()\n", "\n", "index = 1\n", "#y = np.array(y)\n", "y_ser = list(splits.split(y))\n", "print(y_ser[0])\n", "for y_train_index, y_test_index in y_ser:\n", "    y1_train = y[y_train_index]\n", "    y1_test = y[y_test_index]\n", "    print('Observations: %d' % (len(y1_train) + len(y1_test)))\n", "    print('Training Observations: %d' % (len(y1_train)))\n", "    print('Testing Observations: %d' % (len(y1_test)))\n", "    pyplot.subplot(310 + index)\n", "    pyplot.plot(y1_train)\n", "    pyplot.plot([None for i in y1_train] + [x for x in y1_test])\n", "    index += 1\n", "pyplot.show()\n"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "1317d0e6-0040-4cb3-9d86-be35f978a3a0", "_uuid": "f6e6adcc4d9bc2b4672ce822d2b3a12fe5a8a7af"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Now, we will train our data on the new splits against all algorithms and check for accuracy.\n", "<br/><br/>\n", "We observe that all the algorithms are giving about 53% accuracy.\n", "    </p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "4ff187e4-fefb-43c5-8499-7be037c43a6c", "collapsed": true, "_uuid": "08b1c49c08f56fda4b93d243bc0edba4eaf87a62"}, "outputs": [], "source": ["# Evaluate each algorithm for accuracy\n", "results = []\n", "names = []\n", "\n", "for name, model in models:\n", "    clf1 = model\n", "    clf1.fit(X1_train, y1_train)\n", "    y1_pred = clf.predict(X1_test)\n", "    accu_score1 = accuracy_score(y1_test, y1_pred)\n", "    print(name + \": \" + str(accu_score))"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "b8af7b14-87d0-4018-9d5d-fc15bcd6f7f3", "collapsed": true, "_uuid": "89d11fae19528f2d6de0a7b867cda2930cb8c282"}, "outputs": [], "source": ["# prepare the model LDA\n", "scaler1 = StandardScaler().fit(X1_train)\n", "rescaledX1 = scaler1.transform(X1_train)\n", "model_lda1 = LinearDiscriminantAnalysis()\n", "model_lda1.fit(rescaledX1, y1_train)\n", "# estimate accuracy on validation dataset\n", "rescaledValidationX1 = scaler1.transform(X1_test)\n", "predictions1 = model_lda1.predict(rescaledValidationX1)\n", "print(\"accuracy score:\")\n", "print(accuracy_score(y1_test, predictions1))\n", "print(\"confusion matrix: \")\n", "print(confusion_matrix(y1_test, predictions1))\n", "print(\"classification report: \")\n", "print(classification_report(y1_test, predictions1))"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e5923579-2f0d-4842-b474-e915f667f059", "_uuid": "466ebc391e87c6dd3362a4f22d8185c48fd5f10a"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Lets scale our data using StandardScalar and see how the algorithm performs.\n", "<br/><br/>\n", "Scaling the data makes the accuracy increase to 57%. We can see the respective confusion matrix and accuracy scores below.\n", "    </p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "2a1d7085-5416-4e70-b633-279b19805dec", "collapsed": true, "_uuid": "52414c9b022a8744a1c89717a3646270d16ee238"}, "outputs": [], "source": ["# prepare the model Random Forest\n", "scaler_rf = StandardScaler().fit(X1_train)\n", "rescaledX_rf = scaler_rf.transform(X1_train)\n", "model_rf = RandomForestClassifier(n_estimators=500)\n", "model_rf.fit(rescaledX_rf, y1_train)\n", "# estimate accuracy on validation dataset\n", "rescaledValidationX_rf = scaler_rf.transform(X1_test)\n", "predictions_rf = model_rf.predict(rescaledValidationX_rf)\n", "print(\"accuracy score:\")\n", "print(accuracy_score(y1_test, predictions_rf))\n", "print(\"confusion matrix: \")\n", "print(confusion_matrix(y1_test, predictions_rf))\n", "print(\"classification report: \")\n", "print(classification_report(y1_test, predictions_rf))"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "5b14b363-ef62-411a-9a83-1e9c7636eeaf", "_uuid": "3e37f9dd8f1f8ce9bf21fb34b5df39370a943055"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Plotting the roc auc curve and calculating the score shows a score of 0.5\n", "    </p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "118f8938-0276-4a93-a011-669bd0552330", "collapsed": true, "_uuid": "4179e36de9ed31d196eaca1c291e004a02c1c0d1"}, "outputs": [], "source": ["# Generating the ROC curve\n", "y1_pred_proba = model_lda1.predict_proba(X1_test)[:,1]\n", "fpr1, tpr1, thresholds1 = roc_curve(y1_test, y1_pred_proba)\n", "roc_auc1 = auc(fpr1, tpr1)\n", "\n", "# Plot ROC curve\n", "print(\"roc auc is :\" + str(roc_auc1))\n", "pyplot.plot([0, 1], [0, 1], 'k--')\n", "pyplot.plot(fpr1, tpr1)\n", "pyplot.xlabel('False Positive Rate')\n", "pyplot.ylabel('True Positive Rate')\n", "pyplot.title('ROC Curve')\n", "pyplot.show()"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "f54c4df1-46c6-4303-9593-993109bf5122", "_uuid": "f34e5aeba4c11d3de265e0724d1dd4ebd9353af3"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "<h2>ARIMA model:</h2>\n", "<br/><br/>\n", "Lets create an ARIMA model for our time series.\n", "<br/><br/>\n", "There are a certain checks that needs to be done before applying an ARIMA model on the dataset.\n", "<br/><br/>\n", "We need to check if there is a trend in our dataset. If there a trend, then we need to apply something called as differenciation to make our data stationary.\n", "<br/><br/>\n", "By plotting the outputs of the dataset, we see that there is a clear increasing trend.\n", "<br/><br/>\n", "This means we need to make our data stationary.\n", "</p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "97d1a0ab-aaf8-40e1-acf1-fae3d7609166", "collapsed": true, "_uuid": "bc38b80a39a2fef49045a594bbe9e253cd10ba21"}, "outputs": [], "source": ["pyplot.plot(merged_dataframe['Date'], merged_dataframe['Close'])\n", "pyplot.xlabel('Years')\n", "pyplot.ylabel('Stock Closing Prices')\n", "pyplot.show()\n"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "ce4abfcb-2c9d-497e-b416-13dbdd469ac5", "collapsed": true, "_uuid": "7f38bef8c5e013d19292b22ed84f48cd9d119cf6"}, "outputs": [], "source": ["merged_dataframe.info()"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e85f3516-c0e1-43cf-a15f-d306d6e28116", "_uuid": "4e0873c5ccfd39993fa6e18b6599b16e21112250"}, "source": ["<div class=\"span5 alert alert-info\">\n", "Lets make the 'Date' column as index.\n", "<br/><br/>\n", "Now, we will plot the Autocorrelation plot to see estimate lag in our data.\n", "</div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "f5d9764b-99a7-48d3-9f0d-70f67100712f", "collapsed": true, "_uuid": "4f022837b286fccd04932a94525192e626eb5eec"}, "outputs": [], "source": ["merged_dataframe['Date'] = merged_dataframe['Date'].dt.strftime('%Y-%m')"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "b4271dd5-a2f2-4d7c-9c53-d184b4730b70", "collapsed": true, "_uuid": "a20e2e14574bb40d884a91399ed2056777b5061e"}, "outputs": [], "source": ["df_dateclose = pd.Series(merged_dataframe['Close'].values, index = merged_dataframe['Date'])\n", "print(df_dateclose.index)\n", "print(df_dateclose.head())\n", "autocorrelation_plot(df_dateclose)\n", "pyplot.show()"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "9482bc94-0283-4ea3-94e2-ecd681201d84", "collapsed": true, "_uuid": "98e42d28d6c361635dc6546432e9100e43ac78c2"}, "outputs": [], "source": ["merged_dataframe['Close'].head()"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "4061faac-2ded-423b-9d1f-7ed2c7db7a68", "collapsed": true, "_uuid": "4f965a2bdb6dcf859c601b33eeb81b5962a64dff"}, "outputs": [], "source": ["plot_pacf(df_dateclose, lags=50)"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "31b829d4-e900-41a5-96eb-a06d27259d1f", "collapsed": true, "_uuid": "e7458a06c361aa1f5044863e25175c8688d42ff3"}, "outputs": [], "source": ["plot_acf(df_dateclose)"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "d47cde9d-b93a-4239-a7ce-60b032ca7b04", "collapsed": true, "_uuid": "db520b19c43281955d19ef6e64cbbc006ba40592"}, "outputs": [], "source": ["plot_acf(df_dateclose, lags=50)"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "ee474119-0082-4331-a37f-a5a717906e85", "collapsed": true, "_uuid": "13a3929fb85f74165ccd8b43e67ff17f8915ced8"}, "outputs": [], "source": ["arima_df = pd.DataFrame(merged_dataframe, index=merged_dataframe['Date'])"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "d3053ebc-447a-4c23-ad1d-5e58e3156c18", "collapsed": true, "_uuid": "563c64767c4af7a1a9e98445ac46a3352d7afd54"}, "outputs": [], "source": ["arima_df.index"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "685f23ca-6f95-43ef-bd04-83dae2a11e7b", "_uuid": "de74f5f4bf2ae9d1b962eab0671988aa27baab61"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Lets look at how our ARIMA model performs with different values of p,d,q\n", "<br/><br/>\n", "We will do this by looping over the combination of a different range of parameters and check which of the parameter have the least mean squared error.\n", "    </p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "bbdca68d-b379-4623-a4fc-f02cf8994a0d", "collapsed": true, "_uuid": "3b08d0ff73cc01f323b847bc2a3abf01fef07b66"}, "outputs": [], "source": ["\n", "# evaluate an ARIMA model for a given order (p,d,q)\n", "def evaluate_arima_model(X, arima_order):\n", "    # prepare training dataset\n", "    train_size = int(len(X) * 0.66)\n", "    train, test = X[0:train_size], X[train_size:]\n", "    history = [x for x in train]\n", "    # make predictions\n", "    predictions = list()\n", "    for t in range(len(test)):\n", "        model = ARIMA(history, order=arima_order)\n", "        model_fit = model.fit(disp=0)\n", "        yhat = model_fit.forecast()[0]\n", "        predictions.append(yhat)\n", "        history.append(test[t])\n", "    # calculate out of sample error\n", "    error = mean_squared_error(test, predictions)\n", "    return error\n", "\n", "# evaluate combinations of p, d and q values for an ARIMA model\n", "def evaluate_models(dataset, p_values, d_values, q_values):\n", "    dataset = dataset.astype('float32')\n", "    best_score, best_cfg = float(\"inf\"), None\n", "    for p in p_values:\n", "        for d in d_values:\n", "            for q in q_values:\n", "                order = (p,d,q)\n", "                try:\n", "                    mse = evaluate_arima_model(dataset, order)\n", "                    if mse < best_score:\n", "                        best_score, best_cfg = mse, order\n", "                    print('ARIMA%s MSE=%.3f' % (order,mse))\n", "                except:\n", "                    continue\n", "    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n", "\n", "# evaluate parameters\n", "p_values = [0, 1, 2, 4, 6, 8, 10]\n", "d_values = range(0, 3)\n", "q_values = range(0, 3)\n", "warnings.filterwarnings(\"ignore\")\n", "evaluate_models(df_dateclose.values, p_values, d_values, q_values)"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "820a5ea2-d636-4e7b-9652-868180e2af2d", "_uuid": "9652d86ddc5102c7651ab545db1d7c204f21f694"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Based on our code, we have found out that the ARIMA model is best when the parameters are 0,1,0 for p,d,q respectively.\n", "<br/><br/>\n", "The mean squared error of the model training is about 20000 which is considerably ok as we are dealing with a huge set of numbers.\n", "</p></div>\n"]}, {"cell_type": "code", "metadata": {"_cell_guid": "bdee035a-a50b-4979-98af-e4f810563bb8", "collapsed": true, "_uuid": "4ea51edd3a2b961a04b8f5346fa915db75c0c62f"}, "outputs": [], "source": ["df_dateclose.values"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "ff88da84-7356-4575-8ea2-43c4c31b2a61", "_uuid": "cd7420b92c52572abe3f5e7ce043b6cc737b3880"}, "source": ["<div class=\"span5 alert alert-info\">\n", "<p>\n", "Now ,lets predict the values based on our the model we fit in the previous cell on the test set and plot a graph of the predicted and actual values.\n", "<br/><br/>\n", "We can observe below that the expected and predicted value are very close and it looks like for each of the samples, the previous expected value is much closer to the current predicted.\n", "   <br/><br/>\n", " \n", "The plot below shows the predicted and actual values in red and blue respectively.\n", "<br/><br/>\n", "Its very smooth and seems like the model is doing a good job, but it can definitely do better with a little bit of tweaking.\n", "</p></div>"]}, {"cell_type": "code", "metadata": {"_cell_guid": "0e107c06-eecb-4b01-a72c-6563e3d32c4a", "collapsed": true, "scrolled": false, "_uuid": "d1e29e40fd14bcca3ff6e42623ff83d162d65e63"}, "outputs": [], "source": ["X = df_dateclose.values\n", "size = int(len(X) * 0.66)\n", "train, test = X[0:size], X[size:len(X)]\n", "history = [x for x in train]\n", "predictions = list()\n", "for t in range(len(test)):\n", "    model = ARIMA(history, order=(0,1,0))\n", "    model_fit = model.fit(disp=0)\n", "    output = model_fit.forecast()\n", "    yhat = output[0]\n", "    predictions.append(yhat)\n", "    obs = test[t]\n", "    history.append(obs)\n", "    print('predicted=%f, expected=%f' % (yhat, obs))\n", "error = mean_squared_error(test, predictions)\n", "print('Test MSE: %.3f' % error)\n", "# plot\n", "pyplot.plot(test)\n", "pyplot.plot(predictions, color='red')\n", "pyplot.show()"], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "6fd7c2f8-23dc-49f1-8f75-d4a39fb02814", "collapsed": true, "_uuid": "738e4c4f226f6a403d8a455cac1756c39609e16f"}, "outputs": [], "source": [], "execution_count": null}], "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}}