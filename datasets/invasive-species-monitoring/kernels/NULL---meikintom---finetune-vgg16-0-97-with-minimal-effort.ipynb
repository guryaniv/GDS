{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "_is_fork": false, "language_info": {"pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.0"}, "_change_revision": 0}, "cells": [{"metadata": {"_uuid": "4f9dce2a79fdf7ceabbbd1f00aca2b726ef58e96", "_cell_guid": "8e7c8928-62c9-0abb-781e-edb0eb38ecf3"}, "outputs": [], "cell_type": "markdown", "source": ["### Kaggle Invasive Species Monitoring: Get 0.97 accuracy with minimal effort.\n", "Finetune VGG16 top layers with Keras as described by Francois Chollet here:\n", "\n", "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"], "execution_count": null}, {"metadata": {"_uuid": "4789a096b2c34c88b65f7eae2646439f1e9a1c97", "_cell_guid": "f32bc8ff-1fba-1bec-5c28-ad76fa6302d3"}, "outputs": [], "cell_type": "markdown", "source": ["*chmaxx _ 26.5.17*"], "execution_count": null}, {"metadata": {"_uuid": "46c0bdebe217198fec24f75d29f7a8430de0a651", "_cell_guid": "87c4c624-0670-cbf6-9282-d94bac780c4f"}, "execution_count": null, "cell_type": "code", "source": ["%reset\n", "from keras.preprocessing.image import ImageDataGenerator\n", "from keras.preprocessing import image\n", "from keras.models import Sequential, load_model, Model\n", "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n", "from keras.optimizers import SGD\n", "from keras.callbacks import EarlyStopping\n", "\n", "from keras.callbacks import TensorBoard\n", "from keras_tqdm import TQDMNotebookCallback\n", "from keras import backend as K\n", "\n", "from keras.applications.vgg16 import VGG16\n", "from keras.applications.vgg16 import preprocess_input, decode_predictions\n", "\n", "from datetime import datetime\n", "import os\n", "\n", "import numpy as np\n", "import pandas as pd\n", "\n", "pd.options.mode.chained_assignment = None\n", "pd.options.display.max_rows = 40"], "outputs": []}, {"metadata": {"_uuid": "1a0dbebba1ca371851f8d34615d81d04d40306f8", "_cell_guid": "97873997-b4a4-b84f-72dd-7f316a107459"}, "outputs": [], "cell_type": "markdown", "source": ["Import Labels and check distribution."], "execution_count": null}, {"metadata": {"_uuid": "2665c166e6cb2202d7caf54e627d6a46e37cd34d", "_cell_guid": "32f2db0d-ff38-c04d-c6bd-6c0853d56bc5"}, "execution_count": null, "cell_type": "code", "source": ["file = \"D:/KI/01_keras/_kaggle/_invasiveplants/train_labels.csv\"\n", "df = pd.read_csv(file, sep=\",\", error_bad_lines= True)\n", "df.invasive.value_counts()"], "outputs": []}, {"metadata": {"_uuid": "1b408028b6a75d61badd8a62f7318b5f28679769", "_cell_guid": "b6d8979b-0339-8bee-1393-22ca3f6f4f47"}, "outputs": [], "cell_type": "markdown", "source": ["Separate Images according to their labels. Move them to either class folder **false/** or **true/**."], "execution_count": null}, {"metadata": {"_uuid": "f77e4b237017b7932c9195f51001bf1377cdebf4", "_cell_guid": "4fda719e-f6fb-7dee-4688-f12e51b42655"}, "outputs": [], "cell_type": "markdown", "source": ["After separating the images into two classes I manually moved 400 images to the validation folder (again with separate folders for the two classes)."], "execution_count": null}, {"metadata": {"_uuid": "a4ece94bffd712b147620a2a9a0bc7a1e42063bc", "_cell_guid": "4aea5aea-7ed1-89d7-6787-1aa79f288df4"}, "execution_count": null, "cell_type": "code", "source": ["%cd _kaggle/_invasiveplants/_train\n", "names = df.name\n", "labels = df.invasive\n", "\n", "for idx, label in enumerate(labels):\n", "    iname = str(names[idx]) + \".jpg\"\n", "    if (label == 0):\n", "        !mv $iname false/\n", "    elif (label == 1):\n", "        !mv $iname true/"], "outputs": []}, {"metadata": {"_uuid": "c970faf4d6af72f69373478e77e62c427c739516", "collapsed": true, "_cell_guid": "1f92cddf-7e1a-0c6b-b6ea-891594631fc7"}, "outputs": [], "cell_type": "markdown", "source": ["### Build the CNN\n", "We use the Keras VGG16 application with weights but without top. We add an untrained DNN on top."], "execution_count": null}, {"metadata": {"_uuid": "f1cc782e6c58cdbb1468c85d183a04ea7650777f", "collapsed": true, "_cell_guid": "62f77dcc-aae3-4ad5-b990-bb3ae70f983b"}, "execution_count": null, "cell_type": "code", "source": ["vgg16 = VGG16(weights='imagenet', include_top=False)\n", "\n", "x = vgg16.get_layer('block5_conv3').output\n", "x = GlobalAveragePooling2D()(x)\n", "x = Dense(256, activation='relu')(x)\n", "x = Dropout(0.5)(x)\n", "x = Dense(1, activation='sigmoid')(x)\n", "\n", "model_final = Model(inputs=vgg16.input, outputs=x)"], "outputs": []}, {"metadata": {"_uuid": "9973fb95598981481c33dd7b77f8b9effaf12553", "_cell_guid": "535d287a-e050-69fd-dd04-7ecbd183e131"}, "outputs": [], "cell_type": "markdown", "source": ["Freeze all VGG layers and compile the model."], "execution_count": null}, {"metadata": {"_uuid": "c0a1cff1226a55b8e14f9da65b55b39c157c1010", "_cell_guid": "0f68a5aa-40f9-d6f4-8175-8e830008cfd6"}, "execution_count": null, "cell_type": "code", "source": ["for layer in vgg16.layers:\n", "    layer.trainable = False\n", "\n", "model_final.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"], "outputs": []}, {"metadata": {"_uuid": "067b2229b922a780a038dbf86f0fe4edd2b2d0f6", "_cell_guid": "7542214f-44a6-46ce-c902-4b391c791d5e"}, "outputs": [], "cell_type": "markdown", "source": ["### Setup the Datagenerator \n", "The interesting point here is, **that we seem to be able to feed any image size to VGG16 and not only 224x224px**. This is particularly meant to improve accuracy for hard to detect images when invasive plants appear only very small in the image. "], "execution_count": null}, {"metadata": {"_uuid": "c8e33646428ad4f95111764109b769198301c828", "_cell_guid": "5fa89f4d-5cc2-9bed-ceb6-5bb406bde105"}, "outputs": [], "cell_type": "markdown", "source": ["**Hat tip and thanks to Crequena** for that recommendation. See this thread: https://www.kaggle.com/fujisan/use-keras-pre-trained-vgg16-acc-98/comments"], "execution_count": null}, {"metadata": {"_uuid": "463854ea1ca25db1d6ecc0d25bd1de58eddd4d6a", "_cell_guid": "7435eacc-1bba-e3ab-010e-377ffcdb9857"}, "outputs": [], "cell_type": "markdown", "source": ["I first trained with a small size of 300/225px until early stopping. Than I trained again with 600/450px until early stopping. Feel free to try with even bigger sizes."], "execution_count": null}, {"metadata": {"_uuid": "439bec25e523ae1b7ab3f0a0239cf882ee6ef022", "_cell_guid": "7396ea9f-530e-e36b-a8d1-a40796294d83"}, "execution_count": null, "cell_type": "code", "source": ["# You need to have these three folders each with two subfolders for the two classes.\n", "train_data_dir = \"D:/KI/01_keras/_kaggle/_invasiveplants/_train\"\n", "validation_data_dir = \"D:/KI/01_keras/_kaggle/_invasiveplants/_validate\"\n", "test_data_dir = \"D:/KI/01_keras/_kaggle/_invasiveplants/_test\"\n", "\n", "# 600/450 _ 500/375 _ 400/300 _ 300/225\n", "\n", "img_width = 600  # Change image size for training here\n", "img_height = 450 # Change image size for training here\n", "\n", "batch_size = 5 # i achieved good and fast results with this small minibatch size for training\n", "batch_size_val = 400 # if Tensorflow throws a memory error while validating at end of epoch, decrease validation batch size her\n", "\n", "# set data augmentation parameters here\n", "datagen = ImageDataGenerator(rescale=1., \n", "    featurewise_center=True,\n", "    rotation_range=10,\n", "    width_shift_range=.1,\n", "    height_shift_range=.1,\n", "    shear_range=0.2,\n", "    zoom_range=0.2,\n", "    horizontal_flip=True,\n", "    vertical_flip=False,\n", "    fill_mode=\"reflect\")\n", "\n", "# normalization neccessary for correct image input to VGG16\n", "datagen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n", "\n", "# no data augmentation for validation and test set\n", "validgen = ImageDataGenerator(rescale=1., featurewise_center=True)\n", "validgen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n", "\n", "\n", "train_gen = datagen.flow_from_directory(\n", "        train_data_dir,\n", "        target_size=(img_height, img_width),\n", "        batch_size=batch_size,\n", "        class_mode=\"binary\",\n", "        shuffle=True, \n", "        #save_to_dir=\"_augmented_images/\", \n", "        #save_prefix=\"aug_\"\n", "        )\n", "\n", "val_gen = validgen.flow_from_directory(\n", "        validation_data_dir,\n", "        target_size=(img_height, img_width),\n", "        batch_size=batch_size,\n", "        class_mode=\"binary\",\n", "        shuffle=True)\n", "\n", "test_gen = validgen.flow_from_directory(\n", "        test_data_dir,\n", "        target_size=(img_height, img_width),\n", "        batch_size=1,\n", "        class_mode=\"binary\",\n", "        shuffle=False)\n", "\n", "train_samples = len(train_gen.filenames)\n", "validation_samples = len(val_gen.filenames)\n", "test_samples = len(test_gen.filenames)"], "outputs": []}, {"metadata": {"_uuid": "9995c1a6a1a38f6becd02a9b193507c0d620abb2", "_cell_guid": "8ad1c88e-b3d8-2fc1-2e88-0fdd6d366fc1"}, "execution_count": null, "cell_type": "code", "source": ["now = datetime.now()\n", "\n", "# \"_tf_logs\" is my Tensorboard folder. Change this to your setup if you want to use TB\n", "logdir = \"_tf_logs/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n", "tb = TensorBoard(log_dir=logdir)\n", "\n", "epochs=10\n", "\n", "# I stopped training automagically with EarlyStopping after 3 consecutive epochs without improvement\n", "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n", "\n", "model_final.fit_generator(train_gen, epochs=epochs, \n", "                          steps_per_epoch=int(train_samples/batch_size), \n", "                          validation_data=val_gen, \n", "                          validation_steps=batch_size_val, \n", "                          verbose=0, callbacks=[early_stopping, tb, TQDMNotebookCallback()])"], "outputs": []}, {"metadata": {"_uuid": "627da7068faf99143365ce53e0b6742fd653076c", "_cell_guid": "e1436d45-0681-c1ec-a490-2a361d8ce16c"}, "outputs": [], "cell_type": "markdown", "source": ["After doing two rounds of training until early stopping (one with a small image size, one with a larger size) we do a second round of training that now includes the last convolutional block of the VGG16, that until now was frozen."], "execution_count": null}, {"metadata": {"_uuid": "2a4068387a8f87da42fe473df7cfce4f792ef565", "_cell_guid": "4bdafad6-acfc-2807-014e-66cf690acd77"}, "outputs": [], "cell_type": "markdown", "source": ["First we printout all layers. Than we freeze all layers up to the last conv block and compile again."], "execution_count": null}, {"metadata": {"_uuid": "21da44946b963992e529da358a1ee9d006d3d464", "_cell_guid": "d1f2acc7-b284-acf2-4389-bba96be8a57e"}, "execution_count": null, "cell_type": "code", "source": ["for i, layer in enumerate(model_final.layers):\n", "   print(i, layer.name)\n", "\n", "for layer in model_final.layers[:15]:\n", "   layer.trainable = False\n", "for layer in model_final.layers[15:]:\n", "   layer.trainable = True"], "outputs": []}, {"metadata": {"_uuid": "381dd7e4627975a8fba546f2532c63540e989d26", "collapsed": true, "_cell_guid": "9f781510-b59f-a635-7036-ccd591ea8085"}, "execution_count": null, "cell_type": "code", "source": ["model_final.compile(optimizer=SGD(lr=0.0001, momentum=0.9, nesterov=True),  loss='binary_crossentropy', metrics=['accuracy'])"], "outputs": []}, {"metadata": {"_uuid": "10c4e13167dbc40ad3d9509ebe84473c3937f16b", "_cell_guid": "11badc01-bb30-0a11-0783-17d78d3974b2"}, "outputs": [], "cell_type": "markdown", "source": ["Again I did two rounds of training in this second step: First round with a small image size until early stopping, than a second round with the large image size."], "execution_count": null}, {"metadata": {"_uuid": "e33ce3a0f789276b5464b553deb17122e15acb8b", "_cell_guid": "e4378335-15c7-6c83-3a81-7fc260a90729"}, "execution_count": null, "cell_type": "code", "source": ["now = datetime.now()\n", "\n", "# \"_tf_logs\" is my Tensorboard folder. Change this to your setup if you want to use TB\n", "logdir = \"_tf_logs/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n", "tb = TensorBoard(log_dir=logdir)\n", "\n", "epochs=50\n", "\n", "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n", "\n", "model_final.fit_generator(train_gen, epochs=epochs, \n", "                          steps_per_epoch=int(train_samples/batch_size), \n", "                          validation_data=val_gen, \n", "                          validation_steps=int(validation_samples/batch_size), \n", "                          verbose=0, callbacks=[early_stopping, tb, TQDMNotebookCallback()])"], "outputs": []}, {"metadata": {"_uuid": "47bbd701701bec9dd82a8665fd469dd3a1730f75", "_cell_guid": "b049cfbc-2648-f8b6-afe2-ce235f6991bf"}, "outputs": [], "cell_type": "markdown", "source": ["Make predictions for test images and save as submission CSV."], "execution_count": null}, {"metadata": {"_uuid": "a9e0599b5ba007fd2f8b27ab920d736ea37453fe", "_cell_guid": "3984a18b-eda4-aa5d-f5da-b3c86dd1a2ec"}, "execution_count": null, "cell_type": "code", "source": ["preds = model_final.predict_generator(test_gen, 1531)\n", "preds_rounded = []\n", "\n", "for pred in preds:\n", "    if (pred > .5):\n", "        preds_rounded.append(\"1\")\n", "    else:\n", "        preds_rounded.append(\"0\")\n", "\n", "preds_filenames = [int(x.replace(\"test\\\\\", \"\").replace(\".jpg\", \"\")) for x in test_gen.filenames]   \n", "\n", "data = (list(zip(preds_filenames, preds_rounded)))\n", "\n", "df_result = pd.DataFrame(data, columns=[\"name\", \"invasive\"])\n", "df_result = df_result.sort_values(\"name\")\n", "df_result.index = df_result[\"name\"]\n", "df_result = df_result.drop([\"name\"], axis=1)\n", "\n", "df_result.to_csv(\"_kaggle/_invasiveplants/submission_03.csv\", encoding=\"utf8\", index=True)"], "outputs": []}, {"metadata": {"_uuid": "5c241400fed637e71c0b42f05703345eec119115", "_cell_guid": "b385a71a-70ec-4922-04ef-c2767c8f4a30"}, "execution_count": null, "cell_type": "code", "source": "", "outputs": []}], "nbformat_minor": 0, "nbformat": 4}