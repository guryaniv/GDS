{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "37f61c99-a598-e2c1-0e76-60128c75755b"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3f986c60-7ede-17d9-27d9-d789c1fbb253"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.pipeline import make_pipeline, Pipeline, _name_estimators\n",
        "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "\n",
        "y_train = train['y'].values\n",
        "id_test = test['ID']\n",
        "\n",
        "num_train = len(train)\n",
        "df_all = pd.concat([train, test])\n",
        "df_all.drop(['ID', 'y'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77351d5e-24e0-f6a1-6bf8-f853d83f3ee7"
      },
      "outputs": [],
      "source": [
        "df_all = pd.get_dummies(df_all, drop_first=True)\n",
        "\n",
        "train = df_all[:num_train]\n",
        "test = df_all[num_train:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ee58e189-03b4-a41b-80c8-3f8b73230905"
      },
      "outputs": [],
      "source": [
        "\n",
        "# One-hot encoding of categorical/strings\n",
        "df_all = pd.get_dummies(df_all, drop_first=True)\n",
        "\n",
        "train = df_all[:num_train]\n",
        "test = df_all[num_train:]\n",
        "\n",
        "\n",
        "\n",
        "class LogExpPipeline(Pipeline):\n",
        "    def fit(self, X, y):\n",
        "        super(LogExpPipeline, self).fit(X, np.log1p(y))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.expm1(super(LogExpPipeline, self).predict(X))\n",
        "\n",
        "#\n",
        "# Model/pipeline with scaling,pca,svm\n",
        "#\n",
        "svm_pipe = LogExpPipeline(_name_estimators([RobustScaler(),\n",
        "                                            PCA(),\n",
        "                                            SVR(kernel='rbf', C=1.0, epsilon=0.05)]))\n",
        "                                            \n",
        "# results = cross_val_score(svm_pipe, train, y_train, cv=5, scoring='r2')\n",
        "# print(\"SVM score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
        "# exit()\n",
        "                                            \n",
        "#\n",
        "# Model/pipeline with scaling,pca,ElasticNet\n",
        "#\n",
        "en_pipe = LogExpPipeline(_name_estimators([RobustScaler(),\n",
        "                                           PCA(n_components=125),\n",
        "                                           ElasticNet(alpha=0.001, l1_ratio=0.1)]))\n",
        "\n",
        "#\n",
        "# XGBoost model\n",
        "#\n",
        "xgb_model = xgb.sklearn.XGBRegressor(max_depth=3, learning_rate=0.005, subsample=0.9,\n",
        "                                     colsample_bytree=0.4, objective='reg:linear', n_estimators=1300)\n",
        "\n",
        "# results = cross_val_score(xgb_model, train, y_train, cv=5, scoring='r2')\n",
        "# print(\"XGB score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
        "\n",
        "\n",
        "#\n",
        "# Random Forest\n",
        "#\n",
        "rf_model = RandomForestRegressor(n_estimators=250, n_jobs=4, min_samples_split=25,\n",
        "                                 min_samples_leaf=25, max_depth=3)\n",
        "\n",
        "# results = cross_val_score(rf_model, train, y_train, cv=5, scoring='r2')\n",
        "# print(\"RF score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
        "\n",
        "\n",
        "#\n",
        "# Now the training and stacking part.  In previous version i just tried to train each model and\n",
        "# find the best combination, that lead to a horrible score (Overfit?).  Code below does out-of-fold\n",
        "# training/predictions and then we combine the final results.\n",
        "#\n",
        "# Read here for more explanation (This code was borrowed/adapted) :\n",
        "#\n",
        "\n",
        "class Ensemble(object):\n",
        "    def __init__(self, n_splits, stacker, base_models):\n",
        "        self.n_splits = n_splits\n",
        "        self.stacker = stacker\n",
        "        self.base_models = base_models\n",
        "\n",
        "    def fit_predict(self, X, y, T):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        T = np.array(T)\n",
        "\n",
        "        folds = list(KFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n",
        "\n",
        "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
        "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
        "        for i, clf in enumerate(self.base_models):\n",
        "\n",
        "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
        "\n",
        "            for j, (train_idx, test_idx) in enumerate(folds):\n",
        "                X_train = X[train_idx]\n",
        "                y_train = y[train_idx]\n",
        "                X_holdout = X[test_idx]\n",
        "                y_holdout = y[test_idx]\n",
        "\n",
        "                clf.fit(X_train, y_train)\n",
        "                y_pred = clf.predict(X_holdout)[:]\n",
        "\n",
        "                print (\"Model %d fold %d score %f\" % (i, j, r2_score(y_holdout, y_pred)))\n",
        "\n",
        "                S_train[test_idx, i] = y_pred\n",
        "                S_test_i[:, j] = clf.predict(T)[:]\n",
        "            S_test[:, i] = S_test_i.mean(axis=1)\n",
        "\n",
        "        # results = cross_val_score(self.stacker, S_train, y, cv=5, scoring='r2')\n",
        "        # print(\"Stacker score: %.4f (%.4f)\" % (results.mean(), results.std()))\n",
        "        # exit()\n",
        "\n",
        "        self.stacker.fit(S_train, y)\n",
        "        res = self.stacker.predict(S_test)[:]\n",
        "        return res\n",
        "\n",
        "stack = Ensemble(n_splits=5,\n",
        "                 #stacker=ElasticNetCV(l1_ratio=[x/10.0 for x in range(1,10)]),\n",
        "                 stacker=ElasticNet(l1_ratio=0.1, alpha=1.4),\n",
        "                 base_models=(svm_pipe, en_pipe, xgb_model, rf_model))\n",
        "\n",
        "y_test = stack.fit_predict(train, y_train, test)\n",
        "\n",
        "df_sub = pd.DataFrame({'ID': id_test, 'y': y_test})\n",
        "df_sub.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}