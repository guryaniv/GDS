{"metadata": {"language_info": {"file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "beef65b5bf59b08a68e8ca8524b09fa22184b8fa", "_cell_guid": "fcc696c0-9faf-446b-b8f8-00806f39bae7"}, "cell_type": "markdown", "source": ["This is just  a simple exploratory approach to identify those words that are uniquely used by each author. I have not figured out if this is a reasonable and valid step prior to any sophisticated modeling approach; it's just my first attempt to get familiar with the data. So let's import the necessary libraries first:"]}, {"metadata": {"_uuid": "87b8ff013f6ca0c32e053988de15dde9c7a4e9ca", "_cell_guid": "0e1fb41c-9cb5-44c9-b3b8-363598725b86", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from nltk.tokenize import RegexpTokenizer"], "outputs": []}, {"metadata": {"_uuid": "b73dac5e9699b127e04ee6bed405e166fa0a288b", "_cell_guid": "99456c53-8d99-4a06-8ae7-62f4e37050a4"}, "cell_type": "markdown", "source": ["Get the training set:"]}, {"metadata": {"_uuid": "6fb28720d33461bd13143673dc66585b8c437ff0", "_cell_guid": "4a69cf22-8060-4cf0-a678-557c62b139e6", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["train_df = pd.read_csv(\"../input/train.csv\")"], "outputs": []}, {"metadata": {"_uuid": "b5118659fb5b904823c89a754c7b55e828072d6a", "_cell_guid": "2fe00f42-904e-4d59-832b-2f394af67cc2"}, "cell_type": "markdown", "source": ["Build subsets for each author:"]}, {"metadata": {"_uuid": "413dba1212515d57a2fe4e83d259bb59ecc1f0b9", "_cell_guid": "cc90e933-b739-4aac-9a41-d38c50f1ab6d", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["eap = train_df[train_df[\"author\"] == \"EAP\"]\n", "hpl = train_df[train_df[\"author\"] == \"HPL\"]\n", "mws = train_df[train_df[\"author\"] == \"MWS\"]"], "outputs": []}, {"metadata": {"_uuid": "002b0f33dd64e2d6384ddd48f7c90d73217a8346", "_cell_guid": "260412b0-c8aa-40cc-a2e2-979362ec2b95"}, "cell_type": "markdown", "source": ["Combine all text snippets to one great string for each author separately:"]}, {"metadata": {"_uuid": "6f2c2f71c77baf6b944ac41486ccafc82db41c5d", "_cell_guid": "42623b06-97aa-4ded-9156-e9cef62fb95d", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["all_eap = \"\".join([ text for text in eap[\"text\"] ])\n", "all_hpl = \"\".join([ text for text in hpl[\"text\"] ])\n", "all_mws = \"\".join([ text for text in mws[\"text\"] ])"], "outputs": []}, {"metadata": {"_uuid": "ff4762bf3f0e467ba9308f2df6172bd579b0d33e", "_cell_guid": "7179240b-39e1-4312-8f1d-7a109d79fdcd"}, "cell_type": "markdown", "source": ["Define a simple NLTK tokenizer:"]}, {"metadata": {"_uuid": "919bb2d2fa79358aa8ad3d5b5cc074e41f733eed", "_cell_guid": "0d0c5ce0-e531-4994-819c-3a7f95c568e7", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["tokenizer = RegexpTokenizer(r'\\w+')"], "outputs": []}, {"metadata": {"_uuid": "a82910123c76715c2b444acd272d54768c28196b", "_cell_guid": "0ac7a740-9c9d-422b-a236-f1350fbef1e1"}, "cell_type": "markdown", "source": ["Tokenize the complete texts for each author separately:"]}, {"metadata": {"_uuid": "eeee6c690bbf3f9030cc3d10298f05de45aaaed1", "_cell_guid": "12a7179e-5d87-4bb8-8787-ea2d8d9c0695", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["tokens_eap = tokenizer.tokenize(all_eap)\n", "tokens_hpl = tokenizer.tokenize(all_hpl)\n", "tokens_mws = tokenizer.tokenize(all_mws)"], "outputs": []}, {"metadata": {"_uuid": "c50e172348061942c96e3c39160ab44b71e50d11", "_cell_guid": "da640f25-e8c3-48df-ba99-35537bd1cf8a"}, "cell_type": "markdown", "source": ["Build token sets for all possible author pairs:"]}, {"metadata": {"_uuid": "9501fb6c995cd3474e4c401737f995ed676e02d6", "_cell_guid": "785121fa-50a5-4e77-ab98-8aab14002097", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["set_eap_hpl =  set(tokens_eap + tokens_hpl)\n", "set_eap_mws =  set(tokens_eap + tokens_mws)\n", "set_hpl_mws =  set(tokens_hpl + tokens_mws)"], "outputs": []}, {"metadata": {"_uuid": "9d95211cd6daf17ede25428f5275253eb929f5ee", "_cell_guid": "51fa1968-ac4f-44af-a52a-f686cd01d20c"}, "cell_type": "markdown", "source": ["Keep only those tokens for each author that are uniquely used by the respective author:"]}, {"metadata": {"_uuid": "13d21323d82499dd4666541567541261f9d8359d", "_cell_guid": "f1cdac79-ea51-4731-9001-6c89b2df2772", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["specific_eap = [ token for token in tokens_eap if token not in set_hpl_mws ]\n", "specific_hpl = [ token for token in tokens_hpl if token not in set_eap_mws ]\n", "specific_mws = [ token for token in tokens_mws if token not in set_eap_hpl ]"], "outputs": []}, {"metadata": {"_uuid": "ed1c8834debd7cbfa621d5d3cad08156b3aec86c", "_cell_guid": "83b001e3-6c0d-43f7-be96-934a0aa64da3"}, "cell_type": "markdown", "source": ["Create a data frame for each author that contains the uniquely used tokens; tokens, however, are not unique within the respective data frame, i.e. duplicates have not been removed yet."]}, {"metadata": {"_uuid": "8fc1f403936c0aba9317fcdf8334ebd3957bc25d", "_cell_guid": "89ce17ed-74a6-47ed-a403-451f09b856bf", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["df_eap = pd.DataFrame({\"Token\": specific_eap, \"Author\": \"EAP\"})\n", "df_hpl = pd.DataFrame({\"Token\": specific_hpl, \"Author\": \"HPL\"})\n", "df_mws = pd.DataFrame({\"Token\": specific_mws, \"Author\": \"MWS\"})"], "outputs": []}, {"metadata": {"_uuid": "d7426223271d0c41ec929a20c60cc0a27c61f7a5", "_cell_guid": "99ea946d-8786-4825-8815-08f2e125778c"}, "cell_type": "markdown", "source": ["Count the specific tokens for each data frame and add a column for the counts. I could have done this in one step without subsetting for each author and instead filter for a fixed number of most frequent tokens. However, I wanted to have the same size for each subset, indepentent from any general frequency count; so I decided to count tokens for each author individually and then keep the same number of top rows (see below). More elegant solutions are of course very welcome."]}, {"metadata": {"_uuid": "aa2fd62e743b1291843bdae1e30794730b7c1463", "_cell_guid": "d9d3135b-2f01-4fa5-b07b-8ba9f36481d8", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["df_eap[\"Counts\"] = df_eap.groupby([\"Token\"]).transform(\"count\")\n", "df_hpl[\"Counts\"] = df_hpl.groupby([\"Token\"]).transform(\"count\")\n", "df_mws[\"Counts\"] = df_mws.groupby([\"Token\"]).transform(\"count\")"], "outputs": []}, {"metadata": {"_uuid": "4dac3f6e0d9a69302c79cfe1146decef33505d4c", "_cell_guid": "2ba1d4f2-b356-4727-b260-73d224258b06"}, "cell_type": "markdown", "source": ["Sort by 'Counts' in descending order:"]}, {"metadata": {"_uuid": "89ec4ae9a3c60ad3024d531c97cf4146e3a27b35", "_cell_guid": "2b5229a0-e51d-46cf-a4f8-674de445687c", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["df_eap = df_eap.sort_values(['Counts'], ascending=False)\n", "df_hpl = df_hpl.sort_values(['Counts'], ascending=False)\n", "df_mws = df_mws.sort_values(['Counts'], ascending=False)"], "outputs": []}, {"metadata": {"_uuid": "773c1b9bef9b3fe1d386f4dbb361e80a3bc0250d", "_cell_guid": "444f9d3e-76c7-442f-b106-a8e1bb94c100"}, "cell_type": "markdown", "source": ["Now drop duplicate rows:"]}, {"metadata": {"_uuid": "7d04f6818ccef91b043926ffb70a7f54160801f6", "_cell_guid": "e9a14b7e-150b-4074-b149-40e306a40310", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["df_eap = df_eap.drop_duplicates()\n", "df_hpl = df_hpl.drop_duplicates()\n", "df_mws = df_mws.drop_duplicates()"], "outputs": []}, {"metadata": {"_uuid": "4fb8e7f36e198dab70557ba7d60fe791419c0d8f", "_cell_guid": "60ac0198-4903-492d-8376-b9c1a1ef0ae3"}, "cell_type": "markdown", "source": ["Have a look at the top10 of  words for each author:"]}, {"metadata": {"_uuid": "3fb179ac31598586f6ed28316a0041508f71dba2", "_kg_hide-input": false, "_cell_guid": "e301bc8b-c568-46b4-8442-005c5fcdc8a0", "collapsed": true, "_kg_hide-output": false}, "execution_count": null, "cell_type": "code", "source": ["df_eap.head(10) # Edgar Allan Poe"], "outputs": []}, {"metadata": {"_uuid": "ba98b995cd0c2e9470c81f18a59d1ca25f6a2595", "_cell_guid": "781e4c9a-69cf-4e57-b31e-598cea094b08", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["df_hpl.head(10) # H. P. Lovecraft"], "outputs": []}, {"metadata": {"_uuid": "ededc8dda5da9a33f4a07f05349708e59a8cf489", "_cell_guid": "a69cd68f-7f77-4d8b-bbb1-62424b1208dc", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["df_mws.head(10) # Mary Wollstonecraft Shelley"], "outputs": []}, {"metadata": {"_uuid": "f00f8dd87aa143560b0b7d3987d730ca1feb63d4", "_cell_guid": "d5c19d9e-4ce4-4dbd-bbec-00e0d275f152"}, "cell_type": "markdown", "source": ["Get the head of each data frame, here the first n=10 rows, and combine all three data frames to one:"]}, {"metadata": {"_uuid": "fe88d4a15aebbb21382e8f0166ab177300821a12", "_cell_guid": "59d60d1e-50c8-42e9-8ae8-28ea49e1da6c", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["n = 10\n", "\n", "df_top =  pd.concat([\n", "    df_eap.head(n),\n", "    df_hpl.head(n),\n", "    df_mws.head(n)\n", "])\n", "\n", "df_top.set_index(['Token'], inplace=True)"], "outputs": []}, {"metadata": {"_uuid": "79ed8cd18231ab0ae8ee07006969c24c09149a02", "_cell_guid": "723f7e53-4350-4f13-a6eb-93e1c19ea4f4"}, "cell_type": "markdown", "source": ["Set some plotting parameters:"]}, {"metadata": {"_uuid": "43fea754c7866d5925e75d49a903548a6e1c72a4", "_cell_guid": "c2f29112-39a9-473f-9a25-2349f1325f38", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["params = {'legend.fontsize': 'x-large',\n", "          'figure.figsize': (15, 5),\n", "         'axes.labelsize': 'x-large',\n", "         'axes.titlesize':'x-large',\n", "         'xtick.labelsize':'x-large',\n", "         'ytick.labelsize':'x-large'}\n", "\n", "plt.rcParams.update(params)"], "outputs": []}, {"metadata": {"_uuid": "8718338510553b7313d5c3e9ffec01baa8ddef5b", "_cell_guid": "25170dae-d514-45d7-a7f3-9c9850f1ff45"}, "cell_type": "markdown", "source": ["Make a plot. All subplots have identical y-limits for for comparison reasons:"]}, {"metadata": {"_uuid": "7f5bf4bbae4bd224c529e0946b2e6517613e5451", "_cell_guid": "8d5fddca-e0d4-498c-96b7-522e1b1393d8", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["fig, ax = plt.subplots(1, 3)\n", "\n", "ymax = max(df_top[\"Counts\"] + 10)\n", "\n", "ax[0].set_ylim([0, ymax])\n", "ax[1].set_ylim([0, ymax])\n", "ax[2].set_ylim([0, ymax])\n", "\n", "group_a = df_top[df_top.Author=='EAP']\n", "group_b = df_top[df_top.Author=='HPL']\n", "group_c = df_top[df_top.Author=='MWS']\n", "\n", "group_a.plot(kind='bar', rot=45, title = \"EAP\", ax=ax[0])\n", "group_b.plot(kind='bar', rot=45, title = \"HPL\", ax=ax[1])\n", "group_c.plot(kind='bar', rot=45, title = \"MWS\", ax=ax[2])\n", "\n", "fig.tight_layout()"], "outputs": []}], "nbformat": 4}