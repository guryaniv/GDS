{"cells": [{"metadata": {"_uuid": "5545c16338519fee08965c886d098843e6cc5fbc", "_cell_guid": "079edb7d-6545-4088-821e-1e2eecb4544e"}, "cell_type": "markdown", "source": ["### Using Word2Vec Embeddings with a CNN\n", "\n", "10 Jan 2018\n", "\n", "***\n", "### Summary\n", "\n", "This model scored 0.056 on the leaderboard. This is far from fantastic, however the main purpose of this kernel is to describe a process for creating word2vec embeddings and then using those embeddings to train a Keras cnn.\n", "\n", "Lessons like this are easy to forget so I'm publishing this here mainly for my own future reference. Other beginners may also find this kernel helpful.\n", "\n", "The training time for this model was approx. 90 minutes using a GPU.\n", "\n", "***\n", "\n", "#### References:\n", "\n", "- Kaggle Word2Vec tutorial by Angela Chapman:<br>\n", "This tutorial is excellent.<br>\n", "https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors <br>\n", "\n", "- Blog post by Dr Jason Brownlee:<br>\n", "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/ <br>\n", "\n", "- My previous kernel that used pre trained GloVe embeddings:<br>\n", "https://www.kaggle.com/vbookshelf/keras-cnn-glove-early-stopping-0-048-lb <br>\n", "\n", "- Other helpful info:<br>\n", "https://radimrehurek.com/gensim/models/word2vec.html<br>\n", "https://radimrehurek.com/gensim/models/keyedvectors.html<br>\n", "\n", "\n", "***\n"]}, {"outputs": [], "metadata": {"_uuid": "0c4fcc311a168983671f8193a8dc5d37c9fff44a", "_cell_guid": "7084eff7-873d-49e6-b420-cfeb0a19c79a"}, "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "import math\n", "from sklearn.model_selection import train_test_split\n", "\n", "import nltk\n", "\n", "from gensim.models import word2vec\n", "\n", "from numpy import asarray\n", "from numpy import zeros\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.layers import Flatten\n", "from keras.layers import Embedding\n", "from keras.optimizers import Adam\n", "from keras.layers import BatchNormalization, Flatten, Conv1D, MaxPooling1D\n", "from keras.layers import Dropout\n", "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n", "\n", "# Don't Show Warning Messages\n", "import warnings\n", "warnings.filterwarnings('ignore')"], "execution_count": 1}, {"outputs": [], "metadata": {"_uuid": "6a83175bcfd1124dc7d565238045c394f2ab9913", "_cell_guid": "d5ff48e3-a965-484f-82a1-bd84013c4787"}, "cell_type": "code", "source": ["df_train = pd.read_csv('../input/train.csv')\n", "df_test = pd.read_csv('../input/test.csv')\n", "\n", "df_train.fillna(value='none',inplace=True)\n", "df_test.fillna(value='none',inplace=True)\n", "\n", "print(df_train.shape)\n", "print(df_test.shape)"], "execution_count": 2}, {"outputs": [], "metadata": {"_uuid": "00eb004e8399e772657406b68958181be137d9d6", "_cell_guid": "38647cac-3b1e-4a02-9865-84df781509a0"}, "cell_type": "code", "source": ["# combine the train and test sets for encoding and padding\n", "train_len = len(df_train)\n", "df_combined =  pd.concat(objs=[df_train, df_test], axis=0).reset_index(drop=True)\n", "\n", "print(df_combined.shape)\n", "\n", "# make a copy of df_combined\n", "df_combined_copy = df_combined"], "execution_count": 3}, {"metadata": {"_uuid": "2d525a503bd0d5cce03a01a13a43ca0f0fb4055e", "_cell_guid": "cb5abd6a-4774-4909-952d-625eb3a5bde8"}, "cell_type": "markdown", "source": ["### Format word2vec input\n", "\n", "This is the input format that Word2vec wants:\n", "\n", "[ [Hello, how, are, you.], [I, am, fine, thanks.] ]\n", "\n", "Word2Vec expects single sentences. Each sentence is a list of words. In other words, the input format is a list of lists."]}, {"metadata": {"_uuid": "23af4f78fa66adb9ea6823e7c7481332007391b1", "_cell_guid": "1d3c4d20-6ff1-442e-86f1-ab7639ca7586"}, "cell_type": "markdown", "source": ["### 1. Extract the sentences from each comment"]}, {"outputs": [], "metadata": {"_uuid": "545150a244106a8562b2471b21ef3a316d656380", "_cell_guid": "ef69102f-4a57-4168-aa4d-e4fa5b446c41"}, "cell_type": "code", "source": ["\n", "# initialize the tokenizer for extracting sentences\n", "tok = nltk.data.load('tokenizers/punkt/english.pickle')\n", "\n", "output_list = []\n", "\n", "def sentence_to_list(x):\n", "    \"\"\"\n", "    1. Input: All text in the corpus - i.e. every comment\n", "    2. Output: List of sentences where each sentence is a list of words e.g.\n", "    output = [[hello,how,are,you],[i,am,fine,thanks]]\n", "    3. The output python list contains all sentences from every train and test comment.\n", "    \n", "    \"\"\"\n", "    sentence_list= tok.tokenize(x)\n", "    \n", "    for sentence in sentence_list:\n", "        # convert the sentence into a list of words\n", "        word_list = sentence.split()\n", "        # add the sentence to the list of sentences\n", "        output_list.append(word_list)\n", "        \n", "    return output_list\n", "\n", "\n", "# Run the function\n", "# note that df_combined_copy['comment_text'] is not usable after this step.\n", "# After running this line, a variable called output_list is created in memory...\n", "# Okay, this is not the most pythonic way of doing things but apply() runs fast.\n", "df_combined_copy['comment_text'].apply(sentence_to_list)\n", "\n", "print(len(output_list))"], "execution_count": 4}, {"metadata": {"_uuid": "933417e1a101b9b7717d1455630ee81b9a186ae0", "_cell_guid": "a4d883e9-009b-4681-af37-1ffeca075010"}, "cell_type": "markdown", "source": ["### 2. Create the word2vec embedding"]}, {"outputs": [], "metadata": {"_uuid": "d91f2d8f3b02ae67245491db57c854daa6c6d486", "collapsed": true, "_cell_guid": "4c4b4801-b17d-4cdf-97ba-3597dbd1af5f"}, "cell_type": "code", "source": ["\n", "# Set values for various parameters\n", "num_features = 300    # Word vector dimensionality                      \n", "min_word_count = 4    # Minimum word count                        \n", "num_workers = 4       # Number of threads to run in parallel\n", "context = 10          # Context window size                                                                                    \n", "downsampling = 1e-3   # Downsample setting for frequent words\n", "\n", "# Initialize and train the model\n", "\n", "w2v_model = word2vec.Word2Vec(output_list, workers=num_workers, \n", "            size=num_features, min_count = min_word_count, \n", "            window = context, sample = downsampling)\n", "\n", "w2v_model.init_sims(replace=True)\n", "\n", "# save the model\n", "# model_name = \"300features_4minwords_10context\"\n", "# w2v_model.save(model_name)\n", "\n", "print('Training completed.')"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "a6ad14b47d542cc6a822ca3a3425583bf741bdfe", "collapsed": true, "_cell_guid": "34312436-3db8-4bd5-8f3f-93c394ee525c"}, "cell_type": "code", "source": ["# save the word vectors\n", "\n", "#word_vectors = w2v_model.wv\n", "#word_vectors.save('word2vec_toxic_vectors.csv')\n", "\n", "# load the saved word vectors\n", "#word_vectors = KeyedVectors.load('word2vec_toxic_vectors.csv')\n"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "941c9d69fccfedbea7cd78f4ea88cdd694b64b60", "collapsed": true, "_cell_guid": "6d3d1fe9-6c9a-4edd-b5d9-60ee6dcde90c"}, "cell_type": "code", "source": ["# get the shape of the word2vec embedding matrix\n", "w2v_model.syn1neg.shape"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "7961dfdd442c46b507606e7a02c80e5c8a24c68e", "collapsed": true, "_cell_guid": "4de58051-3e8e-4d74-a339-af04570ee64f"}, "cell_type": "code", "source": ["# Tell me what words are most similar to the word 'man'?\n", "w2v_model.most_similar(\"man\")"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "49670c55c3cc86e4a9db5ad4fd71bac803a26c16", "collapsed": true, "_cell_guid": "35678c5c-cec4-4e66-b109-9eec9dcbe2fc"}, "cell_type": "code", "source": ["# This is how to access the embedding vector for a given word\n", "w2v_model.wv['hello']"], "execution_count": null}, {"metadata": {"_uuid": "db6ab4f7a96e64f595c87dafcaa2730f6f84c351", "_cell_guid": "c72188d9-aa6d-481f-8a73-5d60911591a9"}, "cell_type": "markdown", "source": ["### Now that we have converted our corpus into a word2vec embedding,  how do we actually use it in a machine learning model?\n", "\n", "Good question. One way to do this is to add an embedding matrix to the embedding layer of a neural network. Here's how I did it."]}, {"metadata": {"_uuid": "b0058cc11b693e8bdc13d34bd5c78363ee47a719", "_cell_guid": "fd1a9373-8035-4e92-b39e-e179e611fb9a"}, "cell_type": "markdown", "source": ["### 1. Process the train and test comments again:\n", "1. Each word is assigned a unique integer.\n", "2. Each training example (comment) is transformed into a sequence of these unique integers. This is a vector. (A vector is simply a list of numbers.)\n", "3. Make all vectors the same length by padding the vector with zeros (if too short). Here we set the vector length (max_length) as 500.\n", "4. These padded vectors will be our model inputs: X and X_test"]}, {"outputs": [], "metadata": {"_uuid": "a622b7645927afca07b6c065facbedc82eaf7561", "collapsed": true, "_cell_guid": "914d759b-29e6-4b08-bad8-0e2abe761c91"}, "cell_type": "code", "source": ["# create the padded vectors\n", "\n", "docs_combined = df_combined['comment_text'].astype(str)\n", "\n", "# This tokenizer creates a python list of words\n", "t = Tokenizer()\n", "t.fit_on_texts(docs_combined)\n", "vocab_size = len(t.word_index) + 1\n", "\n", "# integer encode the documents\n", "# assign each word a unique integer\n", "encoded_docs = t.texts_to_sequences(docs_combined)\n", "\n", "# pad documents to a max length of 500 words\n", "max_length = 500 ###\n", "padded_docs_combined = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "a8577e386a0073bf5211669dff39ddc0116a702e", "collapsed": true, "_cell_guid": "878594f4-9487-48d1-b933-d9b05d07feb4"}, "cell_type": "code", "source": ["# seperate the train and test sets\n", "\n", "df_train_padded = padded_docs_combined[:train_len]\n", "df_test_padded = padded_docs_combined[train_len:]\n", "\n", "print(df_train_padded.shape)\n", "print(df_test_padded.shape)"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "39917cea5776a80c44ee1dfdac021c706e158c05", "collapsed": true, "_cell_guid": "f708c926-4703-423a-8ed3-a525753d9877"}, "cell_type": "code", "source": ["# create a embedding matrix for words that are in our combined train and test dataframes\n", "\n", "embedding_matrix = zeros((vocab_size, 300))\n", "\n", "for word, i in t.word_index.items():\n", "    # check if the word is in the word2vec vocab\n", "    if word in w2v_model.wv:\n", "        embedding_vector = w2v_model.wv[word]\n", "        \n", "        if embedding_vector is not None:\n", "            embedding_matrix[i] = embedding_vector"], "execution_count": null}, {"metadata": {"_uuid": "6c6d85307f10aacefcbe554f799e4b965ca7e7a5", "_cell_guid": "2871e84b-59d3-499c-bd69-aa6b54f61549"}, "cell_type": "markdown", "source": ["#### What is the above code doing?\n", "\n", "First, recall that we access a word's embedding like this:<br>\n", "\n", "w2v_model.wv['some_word']\n", "\n", "If a word is in the train or test comments and that word does have a word2vec embedding, then we insert that embedding into our new embedding_matrix. Later, this embedding_matrix will be input into the cnn embedding layer."]}, {"outputs": [], "metadata": {"_uuid": "0c84764d94b73686e16760bf44d5c8fc29b891b5", "collapsed": true, "_cell_guid": "2af1e14f-9a68-472a-9b4d-abb54b0401c4"}, "cell_type": "code", "source": ["# check the shape of the new embedding matrix\n", "embedding_matrix.shape"], "execution_count": null}, {"metadata": {"_uuid": "a8a8a9fc86478ed74c9125342a7163c866e89b0c", "_cell_guid": "0f05077b-c56f-47c1-b12c-ba7a89514fbc"}, "cell_type": "markdown", "source": ["### 2. CNN Model\n", "Finally, we arrive at the cnn model...\n", "\n", "We run the model seperately for each of the 6 targets - toxic, severe_toxic, obscene, threat, insult, identity_hate."]}, {"outputs": [], "metadata": {"_uuid": "9623c01bdb118b845243f8389e92f834f8311efc", "collapsed": true, "_cell_guid": "dd5f0b28-2e6a-41c3-8160-a37437196b2a"}, "cell_type": "code", "source": ["X = df_train_padded\n", "X_test = df_test_padded\n", "\n", "# target columns\n", "y_toxic = df_train['toxic']\n", "y_severe_toxic = df_train['severe_toxic']\n", "y_obscene = df_train['obscene']\n", "y_threat = df_train['threat']\n", "y_insult = df_train['insult']\n", "y_identity_hate = df_train['identity_hate']"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "8729f2131930e150c35aac1d2343f499518c3378", "collapsed": true, "_cell_guid": "5c4548a6-9ac5-49c5-aa45-25201b6144c2"}, "cell_type": "code", "source": ["# target columns for each of the 6 models\n", "target_cols = [y_toxic,y_severe_toxic,y_obscene,y_threat,y_insult,y_identity_hate]\n", "\n", "preds = []\n", "\n", "for col in target_cols:\n", "    \n", "    # set the value of y_train\n", "    y = col\n", "    \n", "    X_train, X_eval, y_train ,y_eval = train_test_split(X, y,test_size=0.25,shuffle=True,\n", "                                                    random_state=5,stratify=y)\n", "\n", "    # define model\n", "    model = Sequential()\n", "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=500, trainable=False)\n", "    model.add(e)\n", "    model.add(Conv1D(128, 3, activation='relu'))\n", "    model.add(MaxPooling1D(pool_size=3, strides=2))\n", "    model.add(Dropout(0.2))\n", "    model.add(Conv1D(64, 3, activation='relu'))\n", "    model.add(MaxPooling1D(pool_size=3, strides=2))\n", "    model.add(Dropout(0.2))\n", "    model.add(Conv1D(64, 3, activation='relu'))\n", "    model.add(MaxPooling1D(pool_size=3, strides=2))\n", "    model.add(Dropout(0.2))\n", "    model.add(Flatten())\n", "    model.add(Dense(1, activation='sigmoid'))\n", "\n", "\n", "    # compile the model\n", "    Adam_new = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n", "    model.compile(optimizer=Adam_new, loss='binary_crossentropy', metrics=['acc'])\n", "\n", "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n", "   \n", "    save_best = ModelCheckpoint('toxic.hdf', save_best_only=True, monitor='val_loss', \n", "                               mode='min')\n", "\n", "    history = model.fit(X_train, y_train, validation_data=(X_eval, y_eval),epochs=100, verbose=1,\n", "                   callbacks=[early_stopping,save_best])\n", "\n", "\n", "    model.load_weights(filepath = 'toxic.hdf')\n", "    \n", "    # make a prediction\n", "    predictions = model.predict(X_test)\n", "\n", "    y_preds = predictions[:,0]\n", "    \n", "    preds.append(y_preds)\n"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "6fe1516c06a60602fa55dc7ddfebb0c240af092e", "collapsed": true, "_cell_guid": "634dd99d-62e0-4ad1-8c0e-fa7026e1293c"}, "cell_type": "code", "source": ["# put the results into a dataframe\n", "\n", "df_results = pd.DataFrame({'id':df_test.id,\n", "                            'toxic':preds[0],\n", "                           'severe_toxic':preds[1],\n", "                           'obscene':preds[2],\n", "                           'threat':preds[3],\n", "                           'insult':preds[4],\n", "                           'identity_hate':preds[5]}).set_index('id')\n", "\n", "# Pandas automatically sorts the columns alphabetically by column name.\n", "# Therefore we need to re-order the columns to match the sample submission file.\n", "df_results = df_results[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n", "\n", "# create a submission csv file\n", "#df_results.to_csv('word2vec_with_cnn.csv', columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']) "], "execution_count": null}, {"metadata": {"_uuid": "e839462a466a0e3879daf1d17d48e6ecc2418781", "_cell_guid": "3452e234-b778-4bfd-b902-5bd2cbb1a735"}, "cell_type": "markdown", "source": ["\n", "***\n", "\n", "Thank you for reading."]}, {"outputs": [], "metadata": {"_uuid": "82e60bb38c799b631aea3048f7b8700a70fac83a", "collapsed": true, "_cell_guid": "3aa94e73-786b-48a9-b44b-d7808abe37eb"}, "cell_type": "code", "source": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"nbconvert_exporter": "python", "version": "3.6.4", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "mimetype": "text/x-python"}}, "nbformat": 4, "nbformat_minor": 1}