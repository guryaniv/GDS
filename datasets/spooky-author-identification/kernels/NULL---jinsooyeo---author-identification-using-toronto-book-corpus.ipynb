{"nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "version": "3.6.3", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "cells": [{"metadata": {"_uuid": "8f95891dfeff0f64d48d9c136cada2a01df4f2ac", "collapsed": true, "_cell_guid": "4996099f-0aa7-4e14-8dcd-05fa84524e4c"}, "cell_type": "markdown", "source": ["# INDEX\n", "1. Introduction\n", "2. Data\n", "3. Preprocessing\n", "4. Training & Result\n", "6. Discuss\n", "\n", "\n", "\n", "# 1. INTRODUCTION\n", "\n", "We executed classification by improving [Simple Keras FastText](https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31) model.\n", "we've got significantly high accuracy by improving baseline model by additional feature engineering method(multi n-gram. text mining methods...) & collecting more data.\n", "\n", "During these process, we had ambitions to utilizing and analyzing the concept of author identification with large corpus of data. \n", "\n", "+ Luckily, we obtained  [Toronto Book Corpus](http://yknzhu.wixsite.com/mbweb) (containing lot of books with several authors), so we could check our hypothesis: \u2019Can we recommend similar author based on one\u2019s sentence or paragraph?\u2019\n", "\n", "According to our competition and great kernels, many contributors proved that a sentence can be classified to unique author. \n", "We interpreted the result of classification as \u2018unique writing style of authors\u2019. \n", "\n", "Let\u2019s check out if there exists some insights base on the Author and Theme classification with Toronto Book Corpus  dataset.\n", "\n", "\n", "\n", "# 2. Additional Data\n", "\n", "Our data is based on  [Toronto Book Corpus](http://yknzhu.wixsite.com/mbweb).\n", "\n", "We extracted 11,038 books & 16 different genres from corpus. \n", "\n", "We decided to consider only 10 out of 16 genres: We excluded (Themes, Young_Adult,New_Adult, Historical , Other,Vampires) from 16 genres (Adventure, Horror, Mystery, Romance, Themes, Young_Adult,Fantasy ,Humor,New_Adult,Science_fiction,Thriller,Historical , Literature, Other, Teen ,Vampires) because the data of those topic were less then other genre.\n", "\n", "\n", "\n", "# 3. Preprocessing Data\n", "\n", "We found that there might be no clue for author in some of the books, so we checked if we can access book author or not. We extracted the books author by using Regular Expression.\n", "\n", "We covered two simple rule covers by regular expression :\n", "    1. There is a sentence starting with 'by' or 'By' in the book\n", "    2. There is a sentence starting with 'Copyright' in the book\n", "In this way, we obtained 2,711 books out of total 11,038 books and finally 1788 authors.\n", "\n", "\n", "## 3-a. How many books are exist in each genre\n", " \n", "\n"]}, {"execution_count": null, "metadata": {"_uuid": "d2774f411c29db00c0869e0eb05ae37674a3c2d6", "_kg_hide-input": false, "_cell_guid": "aa977445-536e-4a0f-8bb8-db021d0b2e3b", "_kg_hide-output": false, "collapsed": true}, "cell_type": "code", "outputs": [], "source": ["import pickle\n", "import matplotlib.pyplot as plt\n", "import sys\n", "from pylab import rcParams\n", "rcParams['figure.figsize'] = 10, 5\n", "\n", "D = { 'Adventure': 284, 'Fantasy': 312, 'Horror': 209, 'Humor': 202, 'Literature': 234,\n", "    'Mystery': 242, 'Romance': 304, 'SF': 350, 'Teen': 286, 'Thriller': 288 }\n", "\n", "plt.bar(range(len(D)), D.values(), align='center')\n", "plt.xticks(range(len(D)), D.keys())\n", "plt.title(\"Books by Topic\")\n", "plt.show()\n", "\n", "total = 0\n", "\n", "for key in D:\n", "    total += D[key]\n", "\n", "print( \"Total: %d\"%(total) )"]}, {"metadata": {"_uuid": "65902ddfee771c12f4682982cc276c1fc1e4df6b", "_cell_guid": "8696983b-2263-4ecf-9f65-64591b37ff88"}, "cell_type": "markdown", "source": ["## 3-b. How many books an Author wrote"]}, {"execution_count": null, "metadata": {"_uuid": "99d8324aa1cd00c03661636b71eb9f02a0b583a9", "collapsed": true, "_cell_guid": "bbf6a727-2151-406f-92f0-995c5328fd6d"}, "cell_type": "code", "outputs": [], "source": ["import pickle\n", "import os,sys,re,shutil\n", "import matplotlib.pyplot as plt\n", "from pylab import rcParams\n", "rcParams['figure.figsize'] = 10, 5\n", "\n", "with open('../input/book-len/book_dict.pickle', 'rb') as handle:\n", "    book_dict = pickle.load(handle)\n", "wrote_list = []\n", "for author in book_dict:\n", "    wrote_list.append( book_dict[author]['num'] )\n", "\n", "plt.hist(wrote_list, bins=25)\n", "plt.title(\"How many books each author wrote (total)\")\n", "plt.show()\n", "plt.clf()\n", "\n", "#Except 1\n", "wrote_list.clear()\n", "for author in book_dict:\n", "    if( book_dict[author]['num'] > 2):\n", "        wrote_list.append( book_dict[author]['num'] )\n", "\n", "plt.hist(wrote_list, bins=20)\n", "plt.title(\"How many books each author wrote (except 1)\")\n", "plt.show()\n", "plt.clf()"]}, {"metadata": {"_uuid": "232e36559831353e1abee69722ef37f3959810bb", "_cell_guid": "a2758dcb-cabb-407c-b46c-9ee45e5a35d4"}, "cell_type": "markdown", "source": ["## 3-c. How many lines exist in each book"]}, {"execution_count": null, "metadata": {"_uuid": "fc740e0f2cf6424482bafbce5ec606df91df0c45", "_kg_hide-input": false, "collapsed": true, "_cell_guid": "04e3e97b-013c-421f-a9c2-e1d5637e2c3a"}, "cell_type": "code", "outputs": [], "source": ["\n", "import pickle\n", "import matplotlib.pyplot as plt\n", "import sys\n", "from pylab import rcParams\n", "rcParams['figure.figsize'] = 15, 5\n", "\n", "with open(\"../input/book-len/book_len.pickle\", \"rb\") as handle:\n", "    data = pickle.load( handle)\n", "\n", "data= sorted(data)[:-1]\n", "\n", "plt.hist(data, bins=100)\n", "plt.title(\"How many lines exist in each book\")\n", "plt.show()\n", "plt.clf()\n"]}, {"metadata": {"_uuid": "ffe22f438fb4c4b3c842cfad0edcfb2439a7260f", "_cell_guid": "259958bf-bb31-404e-a136-d6b693c4799d"}, "cell_type": "markdown", "source": ["There are about 20000 sentence by 3 authors in the original train data. Each author has about 6,000 sentences. \n", "So we choose books which has about 6,000(from 5,000 to 10,000) lines from the book corpus.\n", "\n", "In addition, There are multi-topic books in the book corpus. we exclude these books for the convenience. we've got 112 authors. we exclude 5 authors among the authors: 'All authors named in this book', 'Rusty Fischer, author of Zombies Don\u2019t Cry', 'Gavin Chappell James Rhodes Gav Roach', 'Author:', 'Serenity Valley Publishing'.\n", "\n", "Finally, we have 110 authors ( 107 from the corpus + 3 from the original data )."]}, {"execution_count": null, "metadata": {"_uuid": "e10908f7c0696f215ef30bb99db4c3cf4752e6c5", "collapsed": true, "_cell_guid": "5dc9e10d-64c7-4182-9ec2-3dadb7a10ea2"}, "cell_type": "code", "outputs": [], "source": ["\n", "import pickle\n", "import matplotlib.pyplot as plt\n", "import sys\n", "import operator\n", "from pylab import rcParams\n", "rcParams['figure.figsize'] = 10, 5\n", "\n", "with open(\"../input/book-len/author_dict.pickle\", \"rb\") as handle:\n", "    dict_ = pickle.load( handle)\n", "book_dict= {}\n", "\n", "new_dict = { \"multi_topic\": 0, \"single_topic\":0, \"above5000_below10000\":0}\n", "topic_list = [0] * 10\n", "count = []\n", "author_list = []\n", "for author in dict_:\n", "    if(dict_[author][\"total_sentence\"] == dict_[author][\"no_dup_sentence\"]):\n", "        new_dict[\"single_topic\"] += 1\n", "        count.append( dict_[author][\"no_dup_sentence\"])\n", "        if( dict_[author][\"no_dup_sentence\"] > 5000 and dict_[author][\"no_dup_sentence\"] < 10000):\n", "            new_dict[\"above5000_below10000\"] += 1\n", "            author_list.append(author)\n", "    else:\n", "        new_dict[\"multi_topic\"] += 1\n", "        \n", "plt.bar(range(len(new_dict)), new_dict.values(), align='center')\n", "plt.xticks(range(len(new_dict)), new_dict.keys())\n", "\n", "plt.show()\n", "print( \"authors: %d\" %(len( author_list)) )\n"]}, {"metadata": {"_uuid": "e418eeebdf9b2a1e71c659291fa23844ecba8458", "_cell_guid": "58aa73af-786f-4ed2-ae01-7b17ffa0e44c"}, "cell_type": "markdown", "source": []}, {"metadata": {"_uuid": "f9cdb7570df9ea486703cb42fdf0a0094cd5d15b", "_cell_guid": "3cdd96ea-62ca-48ca-9929-762ab10f4cd2"}, "cell_type": "markdown", "source": ["## 3-d. How long each sentence is"]}, {"execution_count": null, "metadata": {"_uuid": "832e2096cf7938dc1acf4bf456e8daa1acc36aa4", "collapsed": true, "_cell_guid": "1b4b5bbd-1adf-4fba-9c8b-b3bfe8f8fabd"}, "cell_type": "code", "outputs": [], "source": ["import os\n", "import pickle\n", "import matplotlib.pyplot as plt\n", "import sys\n", "import operator\n", "\n", "\n", "with open(\"../input/book-len/sent_length_dict.pickle\", \"rb\") as handle:\n", "    dict_ = pickle.load( handle)\n", "\n", "        \n", "plt.hist(dict_[\"original\"], bins=1000)\n", "plt.title(\"sentence length (Total) \")\n", "# Show and clear plot again\n", "plt.show()\n", "plt.clf()       \n", "\n", "list_ = []\n", "for num in dict_[\"original\"]:\n", "    if( num < 500 and num > 30):\n", "        list_.append(num)\n", "plt.hist(list_, bins=50)\n", "plt.title(\"sentence length ( 30< x < 500) \")\n", "# Show and clear plot again\n", "plt.show()\n", "plt.clf()         \n", "\n", "list_ = []\n", "for num in dict_[\"original\"]:\n", "    if( num < 300 and num > 30):\n", "        list_.append(num)\n", "plt.hist(list_, bins=50)\n", "plt.title(\"sentence length ( 30< x < 300) \")\n", "# Show and clear plot again\n", "plt.show()\n", "plt.clf()   \n"]}, {"metadata": {"_uuid": "c0db6935b2fd08c62240f2a719c6c64c3af6ab9d", "_cell_guid": "ee59efba-053e-4768-8333-b2fd238dbe00"}, "cell_type": "markdown", "source": ["We exclude sentences less than 30 characters, and more than 300 characters."]}, {"metadata": {"_uuid": "2c28e689bfec387544d500994e4ac087973c2051", "_cell_guid": "073b0ece-650f-4c9f-a7e1-29aae941e9f3"}, "cell_type": "markdown", "source": ["# 4. Training & Result\n", "\n", "After the preprocessing step, we've got 464,500 sentence with 110 authors.\n", "we trained our model with these data, and got a softmax value per each sentence.\n", "\n", "To get author embedding, we sum all softmax vectors labeled with same author. We reduce the dimensionality of the vectors by T-SNE to 2D, and make scatter plots using pyplot. Each topic has different color."]}, {"metadata": {"_uuid": "c246e33159c53709be4d1edcd1d6c7a578714a14", "_kg_hide-input": true, "_cell_guid": "a064b2a9-f45e-4a40-a729-403ef3bbbeba", "_kg_hide-output": false, "collapsed": true}, "cell_type": "markdown", "source": ["\n", "![dd](https://www.kaggle.com/jinsooyeo/images/image-preview/sojong_result7.png)"]}]}