{"cells":[{"metadata":{"_uuid":"885112f409f9d6e5da3e2375f5b8ad144afb2766"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"defe27e185fb4d6b162b5c7017d7993709bd85d2"},"cell_type":"markdown","source":"**Hi Guys!**\n\nFor those, who want to play with **automated feature engineering** - please find below small example of how to do it with [featuretools](https://docs.featuretools.com/)\n\nFeaturetools is a framework to perform automated feature engineering. \n<br>It (at least it's stated that) excels at transforming transactional and relational datasets into feature matrices for machine learning."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":132,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# import library for automated feature engineering\nimport featuretools as ft\nimport gc\nfrom os.path import join as pjoin\nfrom os import cpu_count\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":133,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfed6221af6eeb7788c248ea6718d1976892a7d8","collapsed":true},"cell_type":"code","source":"# define filepaths\ndata_dir = '../input'\n\nfilepaths = {\n    'data_desc': pjoin(data_dir, 'HomeCredit_columns_description.csv'),\n    'app_train': pjoin(data_dir, 'application_train.csv'),\n    'app_test': pjoin(data_dir, 'application_test.csv'),\n    'bureau': pjoin(data_dir, 'bureau.csv'),\n    'bureau_bl': pjoin(data_dir, 'bureau_balance.csv'),\n    'credit_bl': pjoin(data_dir, 'credit_card_balance.csv'),\n    'install_pays': pjoin(data_dir, 'installments_payments.csv'),\n    'pc_balance': pjoin(data_dir, 'POS_CASH_balance.csv'),\n    'app_prev': pjoin(data_dir, 'previous_application.csv'),\n    \n}\n\nfilepaths","execution_count":202,"outputs":[]},{"metadata":{"_uuid":"05497ca29c3e0e215c84e19bbcf663d3048b88c1"},"cell_type":"markdown","source":"### Load Main table (Applications)"},{"metadata":{"trusted":true,"_uuid":"d2dd988296912d2d22f4cdf0acc873d82d83a712","collapsed":true},"cell_type":"code","source":"# first X rows are taken for faster calculations, substitute this by whole dataset\nnrows = 30000\n\n# load main datasets\ndf_train = pd.read_csv(\n    filepaths['app_train'], \n    low_memory=False, engine='c',\n    nrows=nrows,\n)\ndf_test = pd.read_csv(\n    filepaths['app_test'], \n    low_memory=False, \n    engine='c',\n)\n\n# concat dataframes together, check shapes\nprint(df_train.shape, df_test.shape)\ndf_joint = pd.concat([df_train, df_test])\nprint(df_joint.shape)\n\ndel df_train, df_test\ngc.collect()\n\nprint('memory usage: {:.2f} MB'.format(df_joint.memory_usage().sum() / 2**20))\n\nint_cols = df_joint.select_dtypes(include=[np.int64]).columns\nfloat_cols = df_joint.select_dtypes(include=[np.float64]).columns \n\ndf_joint[int_cols] = df_joint[int_cols].astype(np.int32)\ndf_joint[float_cols] = df_joint[float_cols].astype(np.float32)\n\nprint('memory usage: {:.2f} MB'.format(df_joint.memory_usage().sum() / 2**20))\n\nprint(df_joint.dtypes.value_counts())\n\n# df_joint.set_index('SK_ID_CURR', inplace=True, drop=True)\ntarget_col = 'TARGET'\n\n# check sample\ndf_joint.head()","execution_count":203,"outputs":[]},{"metadata":{"_uuid":"e7b0d63ef1f8708e92adbe5d5b3c52b672b058cb"},"cell_type":"markdown","source":"### Load previous applications table"},{"metadata":{"trusted":true,"_uuid":"61b9d6eff0632cbd7cc4c5a1988ee97b8ba51554","collapsed":true},"cell_type":"code","source":"df_app_prev = pd.read_csv(\n    filepaths['app_prev'], \n    engine='c', \n    low_memory=False,\n    # first X*3 rows are taken for faster calculations, substitute this by whole dataset\n    nrows=nrows*3,\n)\nprint(df_app_prev.shape)\n\n# optimize memory usage\nprint('memory usage: {:.2f} MB'.format(df_app_prev.memory_usage().sum() / 2**20))\n\nint_cols = df_app_prev.select_dtypes(include=[np.int64]).columns\nfloat_cols = df_app_prev.select_dtypes(include=[np.float64]).columns \n\ndf_app_prev[int_cols] = df_app_prev[int_cols].astype(np.int32)\ndf_app_prev[float_cols] = df_app_prev[float_cols].astype(np.float32)\n\nprint('memory usage: {:.2f} MB'.format(df_app_prev.memory_usage().sum() / 2**20))\nprint(df_app_prev.dtypes.value_counts())\n\n# check sample\ndf_app_prev.head()","execution_count":204,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"97f15cc36ec511dffbb69a4a46b6ec6eb38757ad"},"cell_type":"code","source":"# substitute encoded NaNs by \"real\" np.nan\ndf_app_prev[\n    [c for c in df_app_prev.columns if c.startswith('DAYS_')]\n] = df_app_prev[\n    [c for c in df_app_prev.columns if c.startswith('DAYS_')]\n].replace(365243, np.nan)","execution_count":205,"outputs":[]},{"metadata":{"_uuid":"d001efa888d3cb9a323d8bc2c8b8f8df4ba60a7e"},"cell_type":"markdown","source":"## TABLE JOINING (FEATURE TOOLS)"},{"metadata":{"_uuid":"56f6d3b4aebea7b9cb78e2c9ea6675d218d402bd"},"cell_type":"markdown","source":"An `EntitySet` is a collection of entities and the relationships between them. \n\nThey are useful for preparing raw, structured datasets for feature engineering. \n<br>While many functions in Featuretools take `entities` and `relationships` as separate arguments,\n<br>it is recommended to create an `EntitySet`, so you can more easily manipulate your data as needed."},{"metadata":{"trusted":true,"_uuid":"6aaa751a959d7892181afa4917d06fddc38a29af","collapsed":true},"cell_type":"code","source":"# initialize entityset\nes = ft.EntitySet('application_data')\n\n# add entities (application table itself)\nes.entity_from_dataframe(\n    entity_id='apps', # define entity id\n    dataframe=df_joint.drop('TARGET', axis=1), # select underlying data\n    index='SK_ID_CURR', # define unique index column\n    # specify some datatypes manually (if needed)\n    variable_types={\n        f: ft.variable_types.Categorical \n        for f in df_joint.columns if f.startswith('FLAG_')\n    }\n)","execution_count":206,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9b6fce7a880d2b6b127762e3fb9ef8ef8e278e2","collapsed":true},"cell_type":"code","source":"# trick! substitute relative days by absolute date shift\n# to be used as \"true\" time_index\n# however, prohibit datetime features (like month or year) as they're irrelevant\ntoday = pd.to_datetime('2018-06-11')\ndf_app_prev['DAYS_DECISION'] = today + pd.to_timedelta(df_app_prev['DAYS_DECISION'], unit='d')","execution_count":207,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a864586cb39db479be44e6578b9fad90f6d1ad40"},"cell_type":"code","source":"# add entities (previous applications table)\nes = es.entity_from_dataframe(\n    entity_id = 'prev_apps', \n    dataframe = df_app_prev,\n    index = 'SK_ID_PREV',\n    time_index = 'DAYS_DECISION',\n    variable_types={\n        f: ft.variable_types.Categorical \n        for f in df_app_prev.columns if f.startswith('NFLAG_')\n    }\n)","execution_count":208,"outputs":[]},{"metadata":{"_uuid":"0c46dfb476ae0cfe30f20bbf6064112421131945"},"cell_type":"markdown","source":"In the call to `entity_from_dataframe`, we specified three important parameters\n\n- The `index` parameter specifies the column that uniquely identifies rows in the dataframe\n- The `time_index` parameter tells Featuretools when the data was \"created\".\n- The `variable_types` parameter indicates that some columns should be interpreted as a Categorical variable, \neven though it just an integer in the underlying data.\n"},{"metadata":{"_uuid":"dad074504a276c381adb7c06b45c3fcb8031e75b"},"cell_type":"markdown","source":"**Adding a Relationship**\n\nWith two entities in our entity set, we can add a relationship between them.\n\nWe want to relate these two entities by the columns called “SK_ID_CURR” in each entity. \n<br>Each application has multiple previous applications associated with it, \n<br>so it is called it the parent entity, while the previous applications  entity is known as the child entity. \n<br>When specifying relationships we list the variable in the parent entity first. Note that each ft.Relationship must denote a **one-to-many relationship** rather than a relationship which is one-to-one or many-to-many."},{"metadata":{"trusted":true,"_uuid":"f3a27a1a9b2734aa8b69551ed972afc1f3f2dcfa","collapsed":true},"cell_type":"code","source":"# add relationships\nr_app_cur_to_app_prev = ft.Relationship(\n    es['apps']['SK_ID_CURR'],\n    es['prev_apps']['SK_ID_CURR']\n)\n\n# Add the relationship to the entity set\nes = es.add_relationship(r_app_cur_to_app_prev)\n\n# check constructed entity set\nes","execution_count":209,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"283f5ea0ee51302a246ccbd09927156840715f0a","collapsed":true},"cell_type":"code","source":"# check created entities\nes['apps']","execution_count":210,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4fdc779aa2e449b97396be1c6b77a069ac1498c","collapsed":true},"cell_type":"code","source":"# check created entities\nes['prev_apps']","execution_count":211,"outputs":[]},{"metadata":{"_uuid":"9d8e2341779bae3ef72ebfba2b692b039a381739"},"cell_type":"markdown","source":"**Feature primitives**\n\nFeature primitives are the building blocks of Featuretools. They define individual computations that can be applied to raw datasets to create new features. Because a primitive only constrains the input and output data types, they can be applied across datasets and can stack to create new calculations.\n\n**Why primitives?**\n\nThe space of potential functions that humans use to create a feature is expansive. By breaking common feature engineering calculations down into primitive components, we are able to capture the underlying structure of the features humans create today.\n\nSee [documentation](https://docs.featuretools.com/automated_feature_engineering/primitives.html) for further details"},{"metadata":{"trusted":true,"_uuid":"ddf1febe8211b41f06b1133ebe9e5aee9a5821b6","collapsed":true},"cell_type":"code","source":"# inspect list of all built-in primitives for feature construction\nft.list_primitives()","execution_count":212,"outputs":[]},{"metadata":{"_uuid":"8a64a338e0777f342c0adfd43dc871bd24b3d175"},"cell_type":"markdown","source":"**Handling time**\n\nWhen performing feature engineering to learn a model to predict the future, \n<br>the value to predict will be associated with a time. \nIn this case, it is paramount to only incorporate data prior to this `“cutoff time”` when calculating the feature values.\n\nFeaturetools is designed to take time into consideration when required. \n<br>By specifying a cutoff time, we can control what portions of the data are used when calculating features.\n\nWe can specify the time for each instance of the `target_entity` to calculate features. \n<br>The timestamp represents the last time data can be used for calculating features. This is specified using a dataframe of cutoff times. \n\nRead more [here](https://docs.featuretools.com/automated_feature_engineering/handling_time.html)"},{"metadata":{"trusted":true,"_uuid":"b98dfe6a6c0566179494980ccfe7c81b4b7243cc","collapsed":true},"cell_type":"code","source":"%%time\n# define cut-off times\n# cut-off times are the \"right\" time values to be used for feature calculation without future leaks\n# none in our case\n\ncutoff_times = pd.DataFrame(df_joint.SK_ID_CURR)\ncutoff_times['time'] = today\n\n# add last_time_index\nes.add_last_time_indexes()","execution_count":213,"outputs":[]},{"metadata":{"_uuid":"1b166c736d4a440729f398716238c8195fdf9982"},"cell_type":"markdown","source":"**Running DFS with training windows**\n\nTraining windows are an extension of cutoff times: starting from the cutoff time and moving backwards through time, only data within that window of time will be used to calculate features. We will use events **within 2 month time window**"},{"metadata":{"trusted":true,"_uuid":"a86f05337b38a153691098956a8e163b8906520f","collapsed":true},"cell_type":"code","source":"# see feature set definitions (no actual computations yet)\n# used for faster prototyping\nfeature_defs = ft.dfs(\n    entityset=es, \n    target_entity=\"apps\", \n    features_only=True,\n    agg_primitives=[\n        \"avg_time_between\",\n        \"time_since_last\", \n        \"num_unique\", \n        \"mean\", \n        \"sum\", \n    ],\n    trans_primitives=[\n        \"time_since_previous\",\n        #\"add\",\n    ],\n    max_depth=1,\n    cutoff_time=cutoff_times,\n    training_window=ft.Timedelta(60, \"d\"), # use only last X days in computations\n    max_features=1000,\n    chunk_size=10000,\n    verbose=True,\n)\n\n# check what's been created so far\nfeature_defs","execution_count":214,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3edfebc5bc1db6e43f99930270c15ee6bdc69854","collapsed":true},"cell_type":"code","source":"# calculate actual features\nfm, feature_defs = ft.dfs(\n    entityset=es, \n    target_entity=\"apps\", \n    #features_only=True,\n    agg_primitives=[\n        \"avg_time_between\",\n        \"time_since_last\", \n        \"num_unique\", \n        \"mean\", \n        \"sum\", \n    ],\n    trans_primitives=[\n        \"time_since_previous\",\n        #\"add\",\n    ],\n    max_depth=1,\n    cutoff_time=cutoff_times,\n    training_window=ft.Timedelta(60, \"d\"),\n    max_features=1000,\n    chunk_size=4000,\n    verbose=True,\n)","execution_count":215,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"358c224f0121bd2e56282c26543b4bdb40b1912d","collapsed":true},"cell_type":"code","source":"# check sample of extracted features\nfm = fm.drop_duplicates()\nprint(fm.shape)\nfm[50:100]","execution_count":216,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c7f07b260b842e7623abfeaa2c4edcabc7d747d","collapsed":true},"cell_type":"code","source":"# define validation strategy and run a model atop of generated features\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\n\nskf = StratifiedKFold(5, random_state=42)\n\nprint(fm.dtypes.value_counts())\n# label-encode categorical variables\nfor c in fm.select_dtypes(include=['object']).columns:\n    fm[c], _ = pd.factorize(fm[c])\n\nprint(fm.dtypes.value_counts())","execution_count":217,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04342a87773444f02b31d078b718a7c49755ee3f","collapsed":true},"cell_type":"code","source":"# define train/test datasets\nidx_train = df_joint[~df_joint.TARGET.isnull()].SK_ID_CURR.tolist()\nidx_test = df_joint[df_joint.TARGET.isnull()].SK_ID_CURR.tolist()\n\nfm_train = fm[fm.index.isin(idx_train)]\nfm_test = fm[fm.index.isin(idx_test)]\nfm_train.shape, fm_test.shape","execution_count":218,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e73b0d5f94d38d5c21f14bab77f2a33ea2edfad3","collapsed":true},"cell_type":"code","source":"import lightgbm as lgb\n\n# define lightgbm params\nparams_lgb = {\n    'application': 'binary',\n    'boosting': 'gbdt',\n    'learning_rate': 0.03,\n    'num_leaves': 31,\n    'max_depth': 7,\n    'early_stopping_round': 10,\n    \n    'num_iteration': 2500, \n    'colsample_bytree':.95, \n    'subsample':.87, \n\n    'reg_alpha': 0.04, \n    'reg_lambda': 0.07, \n    'min_split_gain': 0.022, \n    'min_child_weight': 5,\n}\n\n# make dataset\ndata_tr = lgb.Dataset(\n    data=fm_train,\n    label=df_joint[:nrows][target_col],\n)\n\n# run cross-validation\ncv_results = lgb.cv(\n    params_lgb, \n    data_tr, \n    metrics=['auc'], \n    folds=skf.split(fm_train, df_joint[:nrows][target_col]), \n    verbose_eval=25,\n)","execution_count":219,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"813513a5ff525b8369934406d6b6c80fa5bdfa1b","collapsed":true},"cell_type":"code","source":"%%time\n# train model\nparams_lgb['num_iteration'] = int(len(cv_results['auc-mean']) * 5/4)\n\nmodel = lgb.train(\n    params_lgb, \n    data_tr,  \n)","execution_count":220,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4749acb21d09643469a2026e170f86e41503548","collapsed":true},"cell_type":"code","source":"# predict for test\ndf_joint.loc[df_joint.SK_ID_CURR.isin(idx_test), target_col] = model.predict(fm_test)\n\n# sample submission\ndf_joint.loc[df_joint.SK_ID_CURR.isin(idx_test), ['SK_ID_CURR', target_col]].to_csv(\n    'featuretools_example_subm.csv',\n    index=False\n)","execution_count":223,"outputs":[]},{"metadata":{"_uuid":"4af27dc2721bfa513d8b4cc9f1cb57699f60ff17"},"cell_type":"markdown","source":"**Hope this small example inspired you to try this approach yourself! \n<br>Have fun - add custom features, add more tables and relationships, gather hands on experience**\n\n**Likes and comments are welcome :)**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1422b47bd70b7d89bbcbc246b4cb3120ab6c9215"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}