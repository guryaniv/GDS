{"cells":[{"metadata":{"_uuid":"dd06cc5607562de8f4730fbf25e9bc0e17b2e0ad"},"cell_type":"markdown","source":"# All 55 million training data points once and for all\nAll the kernels I've seen so far for this challenge are using `nrows=xxxxx` in pandas `read_csv` which would not make a thorough representation of the training dataset. I decided to load the whole dataset in this kernel to get the general information once and for all. Let's make it short and simple."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64ed3cdb894b3329054c2efec6cd1e7435748956"},"cell_type":"markdown","source":"## Reduce memory usage:\nWe need to reduce our memory usage as mush as possible:\n* We don't need to load the `key` for data exploration whatsoever. So we only use the columns we would want.\n* One good suggestion from [another kernel](https://www.kaggle.com/szelee/how-to-import-a-csv-file-of-55-million-rows) was to set the data types before loading the data. However, unlike that kernel for the latitude and longitudes, in order not to lose any of our data and to capture approximately 16 decimal points we will need float64 not float32. As @szelee mentioned in the coments section, using float32 would not cause much accuracy loss. Thus, you might use float32 for your modeling purposes.\n\nNote that this changes **reduced the memory usage from 3.5GB to 2.3GB**! If float32 was used for all spatial georeferences, memory usage would have been 1.5GB."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"types = {'fare_amount': 'float32',\n         'pickup_longitude': 'float64',\n         'pickup_latitude': 'float64',\n         'dropoff_longitude': 'float64',\n         'dropoff_latitude': 'float64',\n         'passenger_count': 'uint8'}\ncols = ['fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\ntrain_data = pd.read_csv('../input/train.csv', dtype=types, usecols=cols, infer_datetime_format=True, parse_dates=[\"pickup_datetime\"]) # total nrows = 55423855\n#test_data = pd.read_csv('../input/test.csv', nrows=0)\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28267403691b65a2dd06f028e26987c427c38bb1","collapsed":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76cc743eaf7bc6af6205543509c539c70b6913ff"},"cell_type":"markdown","source":"## Inference\n* The minimum fare amount in the entire training dataset is -\\$30 and the maximum is \\$93,963!! This is why we need to see the whole dataset! the maximum fare I saw on other kernels (with limited number of rows) was about $500. \n* The maximum passenger_count is 208! Maybe a ship! who knows?\n* Max(pickup_longitude) = 3457.63 degrees! Min(pickup_longitude) = -3492.26 while latitudes range from 0 to 90; longitudes range from 0 to 180. Compass direction North, South, East or West could be represented by `-` and `+`.\n* The same thing goes with other latitudes and longitudes, their maximum and minimums, or essentially any latitude not in `(-90, 90)` and longitude not in `(-180, 180)`,are for sure false values and need to be dropped."},{"metadata":{"_uuid":"5bd4ec4b64d6ca8e3306bf2f46da4215a68156c8"},"cell_type":"markdown","source":"## Missing data\nAbout 0.00068% of the training data is missing. Drop them! It's safe."},{"metadata":{"trusted":true,"_uuid":"3c906deede91b496bae9f0635964fb49ee3f988c","collapsed":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dda52ae943e4a446da0485a84eecb83fdab23c9"},"cell_type":"markdown","source":"## Histograms"},{"metadata":{"trusted":true,"_uuid":"eabebaae33236be02b0f6be79fa4461878d14aed","collapsed":true},"cell_type":"code","source":"counts = train_data[train_data.passenger_count<6].passenger_count.value_counts()\nplt.bar(counts.index, counts.values)\nplt.xlabel('No. of passengers')\nplt.ylabel('Frequency')\nplt.xticks(range(0,7))\nprint(counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f5c630abeba40a1c08802d6e4e5d315c05edf30","collapsed":true},"cell_type":"code","source":"# to capture 75% of the training dataset\ntrain_data[(train_data.fare_amount<125) & (train_data.fare_amount>0)].fare_amount.hist(bins=175, figsize=(15,4))\nplt.xlabel('fare $USD')\nplt.ylabel('Frequency')\nplt.xlim(xmin=0);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"168fe09956b2f2fb876b0f895f9c08b1bb7309be"},"cell_type":"markdown","source":"## Conclusion\n* There might be very harsh/effective outliers and you might miss them if you are exploring data on `nrows=xxxx`.\n* There are not so many missing data points in general. \n* The last and the most important conclusion: I was curious to have a glance at the entire training dataset at once. If you are too, don't use kaggle's computation power to generate the same results of this kernel again ; )"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}