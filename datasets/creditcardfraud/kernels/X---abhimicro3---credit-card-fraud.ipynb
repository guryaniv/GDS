{"cells":[{"metadata":{"_uuid":"5755e42d0313fb739aac3811316cfcf53783de1d"},"cell_type":"markdown","source":"![](https://zdnet2.cbsistatic.com/hub/i/r/2014/11/28/be5ca1a7-76b6-11e4-b569-d4ae52e95e57/resize/770xauto/a1fc0cd4944953755096a9b4cd0ab5a4/credit-card-fraud-can-be-stopped-heres-how.jpg)"},{"metadata":{"_uuid":"7dba168bd40a756b5ac206a411678a08bd1910d3"},"cell_type":"markdown","source":"# Credit Card Fraud Detection<br>\n### The Problem here is to label each transaction as fraudulent or genuine.The data is skewed so we have adopt a different approach to work for it.<br>I will also demonstrate the use of cross validation for hyperparameter tuning on two different classification models:-<br>\n### 1.SVM<br> 2.RandomForestClassifier"},{"metadata":{"_uuid":"b07bcfd236994a04cee8cf4d13be1b84093d5e50"},"cell_type":"markdown","source":"### Lets Load The Library"},{"metadata":{"trusted":false,"_uuid":"405f27eb99768e3aac6d92d6c52881b663a6d148"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA,FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.svm import SVC \nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom sklearn.preprocessing import StandardScaler \n\nfrom sklearn.cross_validation import train_test_split \nfrom sklearn.cross_validation import KFold \nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom subprocess import check_output\nprint(check_output(['ls','../input']).decode('utf8'))","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"92b6d3c6026804a4e7971344c60bec98b3f5bf0d"},"cell_type":"markdown","source":"### Getting The Data"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4082f90ef3f01913b2905285281174dc3a45d12d"},"cell_type":"code","source":"data=pd.read_csv('../input/creditcard.csv')","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"ad53f9dd734a9c667a961bb0321410277ae6bbdb"},"cell_type":"markdown","source":"Lets see what is the shape of the data"},{"metadata":{"trusted":false,"_uuid":"34764558ebe4c553643f3265a2af38bb7a74c676"},"cell_type":"code","source":"data.shape","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"a1314e923857a924b2d111acc5fa63ea191ba7d3"},"cell_type":"markdown","source":"Have a look on columns of our dataset."},{"metadata":{"trusted":false,"_uuid":"35257c3038919f50cf33f571d80cfd3ad50ce7f7"},"cell_type":"code","source":"print(data.columns)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"81b07919da464906590f8068f194a5d24513102a"},"cell_type":"markdown","source":"Is there any null value here..??"},{"metadata":{"trusted":false,"_uuid":"9d4e2c65042b25f3446ce60989595a84b662cc68"},"cell_type":"code","source":"data.columns[data.isnull().any()]","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"a0518890fcaea8f14bf8b6b5a7d39bc1865521e9"},"cell_type":"markdown","source":"Great...No null value"},{"metadata":{"_uuid":"55c678e0a7f5c2eda453b27d8e26e13601ddd863"},"cell_type":"markdown","source":"## Target variable"},{"metadata":{"_kg_hide-input":true,"trusted":false,"_uuid":"07bb5e08066b35368dd146f228a7d3d88696f149"},"cell_type":"code","source":"tar=data['Class'].value_counts().reset_index()\ntar.columns=['Class','Count']\ntemp=tar.Count\ntag = (np.array(tar.index))\nsizes = (np.array((temp / temp.sum())*100))\nplt.figure(figsize=(10,8))\n\ntrace = go.Bar(x=tag, y=sizes)\nlayout = go.Layout(title='Class Distribution')\ndat1 = [trace]\nfig = go.Figure(data=dat1, layout=layout)\npy.iplot(fig, filename=\"Class Distribution\")","execution_count":7,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false,"_uuid":"a308fc4b40de115cade5a6058ddfc5bb9b3a66b7"},"cell_type":"code","source":"tar=data['Class'].value_counts().reset_index()\ntar.columns=['Class','Count']\ntemp=tar.Count\ntag = (np.array(tar.index))\nsizes = (np.array((temp / temp.sum())*100))\nplt.figure(figsize=(10,8))\n\ntrace = go.Pie(labels=tag, values=sizes)\nlayout = go.Layout(title='Class Distribution')\ndat1 = [trace]\nfig = go.Figure(data=dat1, layout=layout)\npy.iplot(fig, filename=\"Class Distribution\")","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"cccc9d3136938d02d1c7f96d12f1ca54fa563689"},"cell_type":"markdown","source":"### As we can see, the fraud data is only 0.173% of the overall data.This shows that the data is highly imbalance and we just can't split it in random manner."},{"metadata":{"_uuid":"e16fa54ca354186b592eac2e9812a21fa9adba02"},"cell_type":"markdown","source":"## Amount Variable"},{"metadata":{"_uuid":"f3995133428593c6d60bffd486a27b9621f77f75"},"cell_type":"markdown","source":"#### Lets see how the Amount variable varies for both Fraud and Normal Transaction. "},{"metadata":{"trusted":false,"_uuid":"16bdbef39927fcc01e7b6fe71f0bd6c36130016f"},"cell_type":"code","source":"normal=data[data['Class']==0]\nfraud=data[data['Class']==1]\nplt.figure(figsize=(10,6))\nplt.subplot(121)\nfraud.Amount.plot.hist(title=\"Fraud Transacation\")\nplt.subplot(122)\nnormal.Amount.plot.hist(title=\"Normal Transaction\")","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"9322b8d713db7008d828cdb29e0073b4f3ec2393"},"cell_type":"markdown","source":"### Have a look for distribution of other variables."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"8ff2763232b7c28d61cc04f054f85f952515d350"},"cell_type":"code","source":"plt.figure(figsize=(12,8*4))\ngs = gridspec.GridSpec(7, 4)\nflist=[col for col in data.columns if not col in['Time','Class','Amount']]\nfor i, cn in enumerate(data[flist]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(data[cn][data.Class == 1], bins=50)\n    sns.distplot(data[cn][data.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('feature: ' + str(cn))","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"f43c0d789dc7173236580a2d17c057b7dad3acf2"},"cell_type":"markdown","source":"## Preprocess The data<br>\n####  In this step we wil do the following thing:<br>\n1.Drop features with constant value,if any.<br>\n2.Drop duplicate features,if any.<br>\n3.Normalize the data.<br>\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"dca84e1a8695aac29bb1738051cfa3cd67c5fcc0"},"cell_type":"code","source":"def drop_sparse(data):\n    remove=[]\n    for col in data.columns:\n        if data[col].std()==0:\n            remove.append(col)\n        \n    data.drop(remove,axis=1,inplace=True)\n    print(data.shape)\n    return data","execution_count":11,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e7c62d634f835c22e1afbbe8604effa3b4364423"},"cell_type":"code","source":"def drop_duplicate(data):\n    remove = []\n    c = data.columns\n    for i in range(len(c)-1):\n        v = data[c[i]].values\n        for j in range(i+1,len(c)):\n            if np.array_equal(v,data[c[j]].values):\n                remove.append(c[j])\n            \n    data.drop(remove, axis=1, inplace=True) \n    print(data.shape)\n    return data","execution_count":12,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"1a471553bf77d4f930254c03c88549ccd76629e3"},"cell_type":"code","source":"def normalize_data(data):\n    flist=[col for col in data.columns if not col in['Time','Class']]\n    for col in flist:\n         data[col] = StandardScaler().fit_transform(data[col].reshape(-1, 1))\n    return data         ","execution_count":13,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dab14629bb3cb203f9a33ebde31c77a3b7d4c1be"},"cell_type":"code","source":"data=drop_sparse(data)\ndata=drop_duplicate(data)\ndata=normalize_data(data)","execution_count":14,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"11dffb16b37d1edb0de9f3add2837acdeedee1e8"},"cell_type":"code","source":"data=normalize_data(data)","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"aaf28f2c357e4f8e638f3f4bb631026c4cd2e955"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"_uuid":"4cd44d248cecfaee149b3a93a1fadbab1f5e4855"},"cell_type":"markdown","source":"Lets add few more features."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0abb33564bce1c04b174c044763bb0278f01e5cc"},"cell_type":"code","source":"flist = [x for x in data.columns if not x in ['Time','Class']]\n\nflist_kmeans = []\nfor ncl in range(2,4):\n    cls = KMeans(n_clusters=ncl)\n    cls.fit_predict(data[flist].values)\n    data['kmeans_cluster'+str(ncl)] = cls.predict(data[flist].values)\n","execution_count":60,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"1ba08c5a1402ef74be8e4f77956db8eae7f7fff5"},"cell_type":"code","source":"flist = [x for x in data.columns if not x in ['Time','Class']]\n\npca = PCA(n_components=2)\nx_train_projected = pca.fit_transform(data[flist])\n\ndata.insert(1, 'PCAOne', x_train_projected[:, 0])\ndata.insert(1, 'PCATwo', x_train_projected[:, 1])\n","execution_count":61,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"04b323546b09a1010e7c811a505fe487544ed354"},"cell_type":"code","source":"flist = [x for x in data.columns if not x in ['Time','Class']]\n\ntsvd= TruncatedSVD(n_components=2)\nx_train_projected = tsvd.fit_transform(data[flist])\n\ndata.insert(1, 'TSVDOne', x_train_projected[:, 0])\ndata.insert(1, 'TSVDTwo', x_train_projected[:, 1])\n","execution_count":62,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c460ca3f76bbc9b76b85c0bb8683562adb5e1337"},"cell_type":"code","source":"flist = [x for x in data.columns if not x in ['Time','Class']]\n\nica= FastICA(n_components=2)\nx_train_projected = ica.fit_transform(data[flist])\n\ndata.insert(1, 'ICAOne', x_train_projected[:, 0])\ndata.insert(1, 'ICATwo', x_train_projected[:, 1])\n\n ","execution_count":63,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"93bcfdb890ed6aabd7642757350aec2d1c096190"},"cell_type":"code","source":"data.shape","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"22e42ed033ea381a1e8d861819c13446352101eb"},"cell_type":"markdown","source":"There are several ways to approach this classification problem taking into consideration this unbalance.<br>\n1.Collect more data? Nice strategy but not applicable in this case<br><br>\n2.Changing the performance metric:<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Use the confusio nmatrix to calculate Precision, Recall<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F1score (weighted average of precision recall)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Use Kappa - which is a classification accuracy normalized by the imbalance of the classes in the data<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ROC curves - calculates sensitivity/specificity ratio.<br><br>\n3.Resampling the dataset:-<br>\n&nbsp;&nbsp;&nbsp;&nbsp;Essentially this is a method that will process the data to have an approximate 50-50 ratio.<br><br>\n&nbsp;&nbsp;&nbsp;&nbsp;One way to achieve this is by OVER-sampling, which is adding copies of the under-represented class<br> (better when you have little data)<br><br>\n&nbsp;&nbsp;&nbsp;&nbsp;Another is UNDER-sampling, which deletes instances from the over-represented class (better when he have lot's of data)<br>"},{"metadata":{"_uuid":"800b3ec69d46b53d65e014a6ee4bf83d621778c3"},"cell_type":"markdown","source":"### Here we will use the undersampling approach"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"49498cbfe6b754b9713c6f9e959dc6f73a0c6d54"},"cell_type":"code","source":"label=data['Class']\ntrain1=data.drop(['Time','Class'],axis=1)\nnormal=data[data['Class']==0]\nfraud=data[data['Class']==1]","execution_count":15,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e7ae008e0137526eb080355fb4c8e20ae1993b21"},"cell_type":"code","source":"from sklearn.utils import shuffle\nfraud = shuffle(fraud)\nnormal = shuffle(normal)","execution_count":16,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9bc356587d1c23ddd13c31f635de9dcf0884e9f7"},"cell_type":"code","source":"fraud=fraud.reset_index()\nfraud.drop('index',axis=1,inplace=True)","execution_count":17,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b527affa83bc4de0b37a391633532e5c2843668d"},"cell_type":"code","source":"normal=normal.reset_index()\nnormal.drop('index',axis=1,inplace=True)","execution_count":18,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"70f3b8a4298face71babc4d8e21783265162d537"},"cell_type":"code","source":"normal_train=normal.iloc[0:492,:]\nnormal_test=normal.iloc[492:,:]","execution_count":19,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"399911b52de1d295ed2bebe4851dadcede1fe873"},"cell_type":"code","source":"frames = [normal_train,fraud]\ntrain=pd.concat(frames)","execution_count":20,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"88c63539dbb80bbad3c8f34a180bab80271a221d"},"cell_type":"code","source":"from sklearn.utils import shuffle\ntrain = shuffle(train)\n","execution_count":21,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"42920d4734df33eb618fbe0a20fbd5b42540af98"},"cell_type":"code","source":"y_train=train['Class']\nx_train=train.drop(['Time','Class'],axis=1)","execution_count":22,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5cfc1409f73eefb764e55178702f15ba4175ad9a"},"cell_type":"code","source":"## first make a model function for modeling with confusion matrix\ndef model(model,features_train,features_test,labels_train,labels_test):\n    clf= model\n    clf.fit(features_train,labels_train)\n    pred=clf.predict(features_test)\n    cnf_matrix=confusion_matrix(labels_test,pred)\n    print(\"the recall for this model is :\",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n    fig= plt.figure(figsize=(6,3))# to plot the graph\n    print(\"TP\",cnf_matrix[1,1,]) # no of fraud transaction which are predicted fraud\n    print(\"TN\",cnf_matrix[0,0]) # no. of normal transaction which are predited normal\n    print(\"FP\",cnf_matrix[0,1]) # no of normal transaction which are predicted fraud\n    print(\"FN\",cnf_matrix[1,0]) # no of fraud Transaction which are predicted normal\n    sns.heatmap(cnf_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\n    plt.title(\"Confusion_matrix\")\n    plt.xlabel(\"Predicted_class\")\n    plt.ylabel(\"Real class\")\n    plt.show()\n    print(\"\\n----------Classification Report------------------------------------\")\n    print(classification_report(labels_test,pred))","execution_count":23,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b8981131dc6b974418775e257ce8e21b1e23714d"},"cell_type":"code","source":"\n# Split the dataset in two equal parts\nX_train, X_test, Y_train, Y_test = train_test_split(\n    x_train, y_train, test_size=0.5, random_state=0)\n\n# Set the parameters by cross-validation\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    print(\"# Tuning hyper-parameters for %s\" % score)\n    print()\n\n    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n                       scoring='%s_macro' % score)\n    clf.fit(X_train, Y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n","execution_count":24,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"54f016b703ab99e79981d80df3f6b3fd0ea25063"},"cell_type":"code","source":"for i in ([0.5,0.66,0.75]):\n    X_train, X_test, Y_train, Y_test = train_test_split(\n     x_train, y_train, test_size=i)\n    # now use it for modeling\n    clf= SVC(kernel='rbf',C=1000,gamma=0.0001)\n    model(clf,X_train,X_test,Y_train,Y_test)","execution_count":25,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9c622a42cdbabca5352bd1882603e966008399fd"},"cell_type":"code","source":"clf= SVC(kernel='rbf',C=1000,gamma=0.0001)\nmodel(clf,x_train,train1,y_train,label)","execution_count":26,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5dd57d930a3c881232d2c1ed7d09974655f3e983"},"cell_type":"code","source":"# ROC CURVE\nlr =SVC(kernel='rbf',C=1000,gamma=0.0001,probability=True)\ny_pred_undersample_score = lr.fit(x_train,y_train.values.ravel()).predict_proba(train1.values)\n\nfpr, tpr, thresholds = roc_curve(label.values.ravel(),y_pred_undersample_score[:,1])\nroc_auc = auc(fpr,tpr)\n\n# Plot ROC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"e7c4ac0f11981b381fe079cba84a12ba996e7094"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":false,"_uuid":"9328b0e78be90323b566ff72e75d8e7ae07042af"},"cell_type":"code","source":"\n# Split the dataset in two equal parts\nX_train, X_test, Y_train, Y_test = train_test_split(\n    x_train, y_train, test_size=0.5, random_state=0)\n\n# Set the parameters by cross-validation\ntuned_parameters = [{'n_estimators': [1,10,100,1000], 'max_features': ['auto','sqrt','log2'],\n                    }\n                    ]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    print(\"# Tuning hyper-parameters for %s\" % score)\n    print()\n\n    clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=5,\n                       scoring='%s_macro' % score)\n    clf.fit(X_train, Y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n","execution_count":28,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"80a90df17344aab4479de7dd239b60f60cddf8a8"},"cell_type":"code","source":"for i in ([0.5,0.66,0.75]):\n    X_train, X_test, Y_train, Y_test = train_test_split(\n     x_train, y_train, test_size=i)\n    # now use it for modeling\n    clf= RandomForestClassifier(n_estimators=100,max_features='log2')\n    model(clf,X_train,X_test,Y_train,Y_test)","execution_count":29,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5ddd57d93a1819ea9f1d7ea87e4baf5c84ab8f11"},"cell_type":"code","source":"clf= RandomForestClassifier(n_estimators=100,max_features='log2')\nmodel(clf,x_train,train1,y_train,label)","execution_count":31,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"692d1affdb0865f0b8fcbed93a5f9acf49d401ad"},"cell_type":"code","source":"# ROC CURVE\nlr = RandomForestClassifier(n_estimators=100,max_features='log2')\ny_pred_undersample_score = lr.fit(x_train,y_train.values.ravel()).predict_proba(train1.values)\n\nfpr, tpr, thresholds = roc_curve(label.values.ravel(),y_pred_undersample_score[:,1])\nroc_auc = auc(fpr,tpr)\n\n# Plot ROC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":30,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}