{"cells":[{"metadata":{"_uuid":"6b63ad26c7cf91dfbb3ab3163d7494843f568cea"},"cell_type":"markdown","source":"In this kernel I want to demonstrate how to extract features from the pet images using a pretrained network. Since there are often none or multiple images of different resoltuions and aspect ratio I make the following preprocessing steps:\n\n- Take only profile picture (if existing else black)\n- pad to square aspect ratio\n- resize to 256\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm, tqdm_notebook\n\ntrain_df = pd.read_csv('../input/train/train.csv')\nimg_size = 256\nbatch_size = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8a7d78a79a1ef646f5bf73fb6b4059ef5fc0771"},"cell_type":"code","source":"pet_ids = train_df['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe31dbcf0c682ade04a66e2dd19c39fd53cb8591"},"cell_type":"code","source":"from keras.applications.densenet import preprocess_input, DenseNet121","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18987c2d130e6d25de4082cb19d1d2a152c9749e"},"cell_type":"code","source":"def resize_to_square(im):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49a2e974500ebdef071054d74a5cc96baf79e0cd"},"cell_type":"markdown","source":"Lets define our model for feature extraction. Normally DenseNet121 would output 1024 features after GlobalAveragePooling. To further narrow it down, I again pool 4 features each."},{"metadata":{"trusted":true,"_uuid":"9f761b784b4a81feed294b3b713510193edc773f"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"039ac2cb53f2c43465e25160c31f9b8a74246d56"},"cell_type":"code","source":"features = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/train_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72e7fc11b1ecf6873b959d3577496c364919c85b"},"cell_type":"code","source":"train_feats = pd.DataFrame.from_dict(features, orient='index')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e94c0a8b07ed0c8b46ba953636f2398a546d5c"},"cell_type":"markdown","source":"We save the features as a csv to disk, so others can link and join the data frame with their train.csv"},{"metadata":{"trusted":true,"_uuid":"2c676304977cfb548356c32f09f06dbee5f6228b"},"cell_type":"code","source":"train_feats.to_csv('train_img_features.csv')\ntrain_feats.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acae2bcbc492656648ece4ef5a3fd2d18afedffc"},"cell_type":"markdown","source":"and repeat the procedure again for test images"},{"metadata":{"trusted":true,"_uuid":"d371c9e17eabaedc338fe9d46fdb80776792e2e4"},"cell_type":"code","source":"test_df = pd.read_csv('../input/test/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7a13fc482a3b10a6a4dfeed5ee87175c923baf3"},"cell_type":"code","source":"pet_ids = test_df['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c7b0b302a2fc575b32fa6c81c11c5e2ecc5e2d7"},"cell_type":"code","source":"features = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/test_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8e7279ee2ebdbb59f424c90ae0430e5f7b40264"},"cell_type":"code","source":"test_feats = pd.DataFrame.from_dict(features, orient='index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e6263a60f0af03217f9e970deb265add1e52e52"},"cell_type":"code","source":"test_feats.to_csv('test_img_features.csv')\ntest_feats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56f317379a13a3fa94ea3c2c376dbb9186cd1f2a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}