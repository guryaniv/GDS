{"cells":[{"metadata":{"_uuid":"1c42a42b77ac5f9da3a87147dde0d3a4f14a1f14"},"cell_type":"markdown","source":"# Import required packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom numpy.linalg import matrix_power","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"db5c32625c14e314c05afeeb5b7d60c1aac331ae"},"cell_type":"markdown","source":"# Load dataset"},{"metadata":{"trusted":true,"_uuid":"6b8640e1cca8b813eb2e06970b4ded44d91f4e67","collapsed":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_train.head()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"e0d6b73e6fbe1a7ec0e955e318b5db9e9ed50721"},"cell_type":"markdown","source":"# Check if dataset has missing data"},{"metadata":{"trusted":true,"_uuid":"6af21ec6ecd409b68f851e6abc4cf8b41a827bd2","collapsed":true},"cell_type":"code","source":"shape_before = df_train.shape\ndf_train.dropna(inplace=True)\nshape_after = df_train.shape\n\nif shape_before == shape_after:\n    print(\"None missing data found\")\nelse:\n    print('Found and removed missing data')","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"5b2350da058484a441333f6ddc6dc15cd8b957ac"},"cell_type":"markdown","source":"# Group dataset by Author"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"df_authors = df_train.groupby(by='author')\ndf_authors = df_authors.get_group('EAP')\nauthors_initials = ['EAP']","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"61a8d139754d1eb939f1cd13e01a56bb9ec036f6"},"cell_type":"markdown","source":"# Show Authors Initials in this dataset"},{"metadata":{"trusted":true,"_uuid":"d9292e73ad0e01c888a50c60d6b99103c1029fb3","collapsed":true},"cell_type":"code","source":"authors_initials","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"73367d73e90de8b2cd1b01d8a074aad8a5286171"},"cell_type":"markdown","source":"# Tokenize"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"30e957df197b6930d69b8cacd8ab944ec989b779"},"cell_type":"code","source":"def tokenize(string, to_lower=True, is_alpha=True):\n    if to_lower:\n        if is_alpha:\n            return [word.lower() for word in word_tokenize(string)]#if word.isalpha()] \n        else:\n            return [word for word in word_tokenize(string)]\n    return [word for word in word_tokenize(string) if word.isalpha()]","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c75f549658a680f9ca7b6a90d8b315911b4dc65e","collapsed":true},"cell_type":"code","source":"tokenize('test, one ,two, three. four .')","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"fdb5ea034ed3dac59817e4a3a4a37839bd012cd9"},"cell_type":"markdown","source":" # Get tokens for each Author"},{"metadata":{"trusted":true,"_uuid":"d64a0147da4d6e5e73d40ee8f468fbaa88e561fb","collapsed":true},"cell_type":"code","source":"authors_tokens = {}\n\nfor author_initials in authors_initials:\n    authors_tokens[author_initials] = []\n\nfor author_initials in authors_initials:\n    group_text = df_authors['text']    \n    for text in group_text:                        \n        for word in tokenize(text):\n            authors_tokens[author_initials].append(word)    \n\nfor author_initials in authors_initials:\n    authors_tokens[author_initials] = np.unique(authors_tokens[author_initials])\n            \nfor author_initials in authors_initials:\n    print(f'author {author_initials} has {len(authors_tokens[author_initials])} unique tokens')","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"5589ec130ce23d2db8d0fb93554e3c8235b89f2c"},"cell_type":"markdown","source":"# Map token to index for each Author"},{"metadata":{"trusted":true,"_uuid":"52e35d29bbccdf824f541b4cdf614885973910bb","collapsed":true},"cell_type":"code","source":"authors_tokens_map = {}  \nfor author_initials in authors_initials:    \n    tokens_author = len(authors_tokens[author_initials])\n    dict_map = {}\n    for token, token_index in zip(authors_tokens[author_initials], range(tokens_author)):\n        dict_map[token] = token_index\n    authors_tokens_map[author_initials] = dict_map","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"e0f3f91e3044a37b0091834921559bda500907ec"},"cell_type":"markdown","source":"# Create Markov transition matrix for each Author"},{"metadata":{"trusted":true,"_uuid":"f71604890108265e385e24bdeffdb53d1578088e","collapsed":true},"cell_type":"code","source":"authors_matrix = {}\nfor author_initials in authors_initials:\n    matrix_order = len(authors_tokens[author_initials])\n    authors_matrix[author_initials] = np.zeros((matrix_order, matrix_order), dtype=np.float64)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"8b7a5f30f261dc2b32c9be8bb62fa577e277a2da"},"cell_type":"markdown","source":"# Add frequency transition for each token, for each Author, for each text"},{"metadata":{"trusted":true,"_uuid":"d38f8f66255c9cfebcb33ba894b142ebb384e13e","collapsed":true},"cell_type":"code","source":"for author_initials in authors_initials:\n    group_text = df_authors['text']    \n    for text in group_text:        \n        tokens = tokenize(text)\n        try:            \n            for i in range(len(tokens) - 1):\n                token1, token2 = tokens[i], tokens[i + 1]                                              \n                token1_index, token2_index = authors_tokens_map[author_initials][token1], authors_tokens_map[author_initials][token2]            \n                authors_matrix[author_initials][token1_index][token2_index] += 1            \n            token1, token2 = tokens[-1], tokens[0]        \n            token1_index, token2_index = authors_tokens_map[author_initials][token1], authors_tokens_map[author_initials][token2]\n            authors_matrix[author_initials][token1_index][token2_index] += 1\n        except:\n            print(author_initials, token1, token2)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"3b9547772980154933ec07db8f6b0b77417328da"},"cell_type":"markdown","source":"# Make frequency transition matrix be probability transition matrix"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"71eb934b84442d262c36de36cc2f49c65a5fc468"},"cell_type":"code","source":"authors_rows_sum = {}\nfor author_initials in authors_initials:\n    authors_rows_sum[author_initials] = np.sum(authors_matrix[author_initials], axis = 0, dtype=np.float64)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e58de849f7ae2e2111cc059d53634c573e26f72","collapsed":true},"cell_type":"code","source":"for author_initials in authors_initials:    \n    authors_matrix[author_initials] = np.divide(authors_matrix[author_initials], (authors_rows_sum[author_initials][None, :]), dtype=np.float64)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40788639f75e1437b5828da41dd4c18321a07eaa","collapsed":true},"cell_type":"code","source":"for author_initials in authors_initials:    \n    print(f'{author_initials} {np.sum(authors_matrix[author_initials])}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8d83f712c4b905d02ca59c695365c5de3d2fcd80"},"cell_type":"markdown","source":"# Get emanation probability matrix"},{"metadata":{"trusted":true,"_uuid":"929c5603cdace0381eaadc0f05805be3fa0784c4","collapsed":true},"cell_type":"code","source":"matrix_emanation = authors_matrix[author_initials]\n\ndone = False\npot = 1\n\nprint('pot \\t unique_values')\n\nwhile not done:\n    matrix_emanation = matrix_power(matrix_emanation, 2)\n    pot *= 2\n    \n    unique_values = np.sum([len(np.unique(matrix_emanation[:, i])) for i in range(len(matrix_emanation))])\n    \n    print(f'{pot} \\t {unique_values}')\n    \n    if unique_values == matrix_emanation.shape[0]:\n        done = True    \n\nauthors_matrix[author_initials] = matrix_emanation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"684c8eb0e9c63401aa62c536e987b68a8cbb615d"},"cell_type":"code","source":"# np.savetxt('emanation_eap.csv', matrix_emanation, delimiter=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e438b713562ca4e45c8de33dcfcd06f7b5f784b","collapsed":true},"cell_type":"code","source":"# authors_matrix_emanation = {}\n# for author_initials in authors_initials:    \n#     for i in range(1, 65):\n#         temp = matrix_power(authors_matrix[author_initials], i)\n#         print(i, np.count_nonzero(temp), np.sum(temp, axies=0))        \n# #     authors_matrix_emanation[author_initials] = np.power(authors_matrix[author_initials], 60, dtype=np.float64)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c614d948637cb9c55b6c9286c593a67e52bf5354","scrolled":true,"collapsed":true},"cell_type":"code","source":"# np.count_nonzero(authors_matrix_emanation['EAP']), authors_matrix_emanation['EAP'].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d35164611f1001661a7c320f54bcfbc26c572c1e"},"cell_type":"markdown","source":"# Test sample"},{"metadata":{"trusted":true,"_uuid":"27eca6346c6544c25328076512726b22256355d2","collapsed":true},"cell_type":"code","source":"# sample = df_authors['text'][0]\n# sample_tokens = tokenize(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de51b1d0955bd675d284bab621ddf996868da1c3","collapsed":true},"cell_type":"code","source":"# for i in range(len(sample_tokens) - 1):   \n#     token1, token2 = sample_tokens[i], sample_tokens[i + 1]\n#     token1_index, token2_index = authors_tokens_map['EAP'][token1], authors_tokens_map['EAP'][token2]\n#     prob = authors_matrix['EAP'][token1_index][token2_index]\n#     prob_emanation = authors_matrix_emanation['EAP'][token2_index][token1_index]\n#     print(f'prob = {prob:.10f}\\tprob_emanation = {prob_emanation:.10f}\\ttokens = [{token1:<12}, {token2:<12}]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f3fff99e3dae271fa0580d7b8b4ba680cb04c05","collapsed":true},"cell_type":"code","source":"# result = {}\n# for author_initials in authors_initials:\n#     result[author_initials] = np.float64(1.0)\n    \n# for i in range(len(sample_tokens) - 1):     \n#     token1, token2 = sample_tokens[i], sample_tokens[i + 1]\n#     for author_initials in authors_initials:\n#         try:\n#             token1_index, token2_index = authors_tokens_map[author_initials][token1], authors_tokens_map[author_initials][token2]\n#             prob = authors_matrix[author_initials][token1_index][token2_index]\n#             if prob == 0:\n#                 prob = 1e-10\n#             result[author_initials] *= prob    \n#         except:\n#             result[author_initials] *= 1e-10\n# result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef8bd1222d8052b85e87d51f1e9715269fba013b","collapsed":true},"cell_type":"code","source":"# for key, value in result.items():\n#     print(f'{key} {np.log10(value)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffc929b93292f0001a50fb3fd24c5696b02edb26","collapsed":true},"cell_type":"code","source":"# def softmax(x):\n#     return np.exp(x)/np.sum(np.exp(x), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf8abb170cf1df488feb8de4bda1789acfdb6736","collapsed":true},"cell_type":"code","source":"# answer = softmax([np.log10(value) for value in result.values()])\n# np.around(answer, decimals=6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27796334623b9165096e9e6b970722b9e9744674"},"cell_type":"markdown","source":"# Check format output"},{"metadata":{"trusted":true,"_uuid":"b9685c8da4caf9ac4eabd301aabe6a8b93526ff1","collapsed":true},"cell_type":"code","source":"# df_submission = pd.read_csv('../input/sample_submission.csv')\n# df_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f61d12e925b81fb058ce6aeeea5f935f1741a903"},"cell_type":"markdown","source":"# Read test dataset"},{"metadata":{"trusted":true,"_uuid":"2b96d309b17bab8dc98e37c28a8411162a9c06f5","collapsed":true},"cell_type":"code","source":"# df_test = pd.read_csv('../input/test.csv')\n# df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1dda58f6528898fd7898074b2a81638e5d14402"},"cell_type":"markdown","source":"# Setup for output"},{"metadata":{"trusted":true,"_uuid":"02e6bf4b77eed680450fbc30ac6020c948b112e9","collapsed":true},"cell_type":"code","source":"# ids, texts = df_test.loc[:, 'id'], df_test.loc[:,  'text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19c021f4ac6a3cc63b3b6e6f239ab478f2f62ab5"},"cell_type":"code","source":"# result = {}\n# for author_initials in authors_initials:\n#     result[author_initials] = []","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07b06b7dba43f1a96b868c5637b41a312c734a4d"},"cell_type":"markdown","source":"# Process output"},{"metadata":{"trusted":true,"_uuid":"4e594fc7c817e5f4cfb746fd0e3bc7e0edfb1a1d","collapsed":true},"cell_type":"code","source":"# beta = np.float64(1e-3)\n# for id, text in zip(ids, texts):\n#     tokens = [word.lower() for word in word_tokenize(text) if word.isalpha()]\n    \n#     row_result = {}\n#     for author_initials in authors_initials:\n#         row_result[author_initials] = np.float64(1.0)\n        \n#     for author_initials in authors_initials:\n#         for i in range(len(tokens) - 1):   \n#             token1, token2 = tokens[i], tokens[i + 1]\n#             try:\n#                 token1_index, token2_index = authors_tokens_map[author_initials][token1], authors_tokens_map[author_initials][token2]\n#                 prob = authors_matrix[author_initials][token1_index][token2_index]\n#                 if prob <= 0:\n#                     prob = beta\n#             except:\n#                 prob = beta\n#             row_result[author_initials] *= np.float64(prob)\n    \n#     answer = softmax([np.log10(value + 1e-320, dtype=np.float64) for value in row_result.values()])\n#     for author_initials, prob in zip(authors_initials, answer):          \n#         result[author_initials].append(prob)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1a42402c4e4a3e705a24fdf1ab33f7900d66814"},"cell_type":"markdown","source":"# Generate output"},{"metadata":{"trusted":true,"_uuid":"5b969ef9f4f9ae65a9ac243d90b29da209ae5cc9","collapsed":true},"cell_type":"code","source":"# output = []\n# for id, p1, p2, p3 in zip(ids, result['EAP'], result['HPL'], result['MWS']):\n#     output.append([id, p1, p2, p3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6ef44108c471c43d2370904a448ed74334d0d50","collapsed":true},"cell_type":"code","source":"# df_output = pd.DataFrame(output, columns=['id', 'EAP', 'HPL', 'MWS'])\n# df_output.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6ccd501f34c9bf5c85a3f1b31b6f578377b95ce"},"cell_type":"code","source":"# df_output.to_csv('test.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3fc4c6ee172feb07f9f4208c7910a92c8d59eb1","collapsed":true},"cell_type":"code","source":"# df_temp = pd.read_csv('test.csv')\n# df_temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a8449e4f978092f622c04e88cbae40848724bbd","collapsed":true},"cell_type":"code","source":"# df_temp.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6e9fcde33b95c57b418b5acb016db93056b5504","collapsed":true},"cell_type":"code","source":"# np.savetxt(\"eap.csv\", authors_matrix_emanation['MWS'], delimiter=\",\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}