{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c60b8614d39bc1b0851979c91726ecf6d499769"},"cell_type":"code","source":"def getNonEmpty(t):\n\n    for e in t:\n        if e != '':\n            return e\n    return ''\n\ndef NoiseCleansing(DataSet,column):\n    Corpus = []\n    N = DataSet.shape[0]\n    k = 0\n    for s in DataSet[column]:\n        k += 1\n        # 1. Remove ' and \" form the description\n        s = re.sub('[\\'\\\"]', ' ', s)\n        # 2. Remove text inside brackets to get rid of unwanted stuff which can not be labeled\n\n        pat = r'[0-9]+\\.[0-9]+'\n        qt = re.findall(pat, s)\n        if len(qt) > 0:\n            for w in qt:\n                s = s.replace(w, str(round(float(w))))\n\n\n        # this regex removes alpha numeric codes\n        s = re.sub(r'[a-zA-Z]*[0-9]{4}[a-zA-Z0-9]*', '', s)\n        # Remove bad URLS\n        s = re.sub(r'http?s:\\/\\/[a-zA-Z0-9]+[^a-zA-Z0-9]', ' ', s)\n        # Remove EMails\n        s = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', s)\n        # Replace special characters by the space for better tokenization\n        s = re.sub(r'[^a-zA-Z0-9\\*]+', ' ', s)\n        # Remove single char representation of the words\n        Removed_Sgl_Char = []\n        for w in s.split(' '):\n            if re.search(r'[0-9]', w):\n                Removed_Sgl_Char.append(w)\n            elif (len(w) > 1):\n                Removed_Sgl_Char.append(w)\n\n        s = ' '.join(Removed_Sgl_Char)\n        Corpus.append(s)\n    DataSet[column] = Corpus\n    return DataSet","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import json\ninput_data=None\nwith open('../input/train.json', 'r') as f:\n    input_data=json.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d74859bf8ea3c671947a41e4a143ca4df19caa8e"},"cell_type":"code","source":"input_data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f854f0f17c307c0e80497255b6f2111a6881bb44"},"cell_type":"code","source":"y=[el['cuisine'] for el in input_data]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd181a921aaedd0d9643ccc9a23dbed5ce9bb9bf"},"cell_type":"code","source":"input_features=[' '.join([el1 for el1 in el['ingredients']]) for el in input_data]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"495d491df436be36d7244974d19cafc6f7915798"},"cell_type":"code","source":"import pandas as pd \ndataset=pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a3702a7cd951b45c9f0744d929f78eb7b0f1f9a"},"cell_type":"code","source":"import json\noutput_data=None\nwith open('../input/test.json', 'r') as f:\n    output_data=json.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aabaf4824a9c6a81b2759de1004f5978ff39327e"},"cell_type":"code","source":"output_data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e731165211376583b9e58e40ebe539cc905c4473"},"cell_type":"code","source":"dataset['text']=input_features\ndataset['class']=y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07bbc29aa0be727a7955a4b30453b3623459f1fc"},"cell_type":"code","source":"dataset['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c51cef4ec11f22ca1f34d51cf413c6d8b90875c"},"cell_type":"code","source":"dataset=dataset.apply(lambda x: x.astype(str).str.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffaac65b71214f21403ab505074ec470c2b58016"},"cell_type":"code","source":"dataset=NoiseCleansing(dataset,'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70e264df8b6c0eb84e97aff6a83263c5789dc83d"},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fb8e7c1faa54f6ac093c791586288cf551b579c"},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(dataset['text'],dataset['class'],test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cc089c7ae3e6925d1299f2a48c52c5c4005f04c"},"cell_type":"code","source":"text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 1))),\n                      ('tfidf', TfidfTransformer()),\n                     ('clf', SGDClassifier(loss='modified_huber', penalty='l2',\n                                           alpha=1e-4, random_state=42, tol=None)),\n ])\n    \nClassiPipelined=OneVsRestClassifier(text_clf)\nClassiPipelined.fit(x_train,y_train)                 \npredictedClass=ClassiPipelined.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f8d9da0a8eb243655b3e09d58802e638e4664cd"},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"224fd146081547b32ffc61beae8e9b05a72258f0"},"cell_type":"code","source":"np.mean(predictedClass==y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1c4b7b587d4ed424886827615e15f84c6074d4a"},"cell_type":"code","source":"input_features_test=[' '.join([el1 for el1 in el['ingredients']]) for el in output_data]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"263ce6696b88f6ded4252f157afdf355cc9bf09a"},"cell_type":"code","source":"ids=[el['id'] for el in output_data]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8ce14d7b8bf52f6f93cb543f9846108c6ddda7c"},"cell_type":"code","source":"testset=pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce6ca0df36bd9f66970600863fe8aa3303585d05"},"cell_type":"code","source":"testset['id']=ids\ntestset['text']=input_features_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b0f7676d1bab34e3fa325361567596c53df505c"},"cell_type":"code","source":"testset=testset.apply(lambda x: x.astype(str).str.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1a3d2442f78cf3b5779d274ff5582ec265c3eeb"},"cell_type":"code","source":"testset=NoiseCleansing(testset,'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"346459ccdcb8c1f709acf5847ddcb6f326b647ed"},"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.initializers import Constant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae9d8982ff7bc1d500d50fa073c84eade3cf184a"},"cell_type":"code","source":"class LanguageIndex():\n  def __init__(self):\n    self.word2idx = dict()\n    self.idx2word = dict()\n    self.vocab = set()\n  def prepare_mapping(self,text):\n    cnt=1\n    for t in text:\n        for w in t:\n            if w not in self.vocab:\n                self.word2idx[w]=cnt\n                self.idx2word[cnt]=w \n                self.vocab.add(w)\n                cnt+=1\n  def getIds(self,li):\n    return [self.word2idx[el] for el in li]\n  def getList(self,li):\n    return [self.idx2word[el] for el in li]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61be0fcf575a911b8411bd3956517a7d983626e5"},"cell_type":"code","source":"LI=LanguageIndex()\nLI.prepare_mapping([el['ingredients'] for el in input_data])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15f4de316736398df2982ef36b53505198a84324"},"cell_type":"code","source":"input_list=[el['ingredients'] for el in input_data]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fa6d5e804abff501d38644480bb20cd4dad0919"},"cell_type":"code","source":"# finally, vectorize the text samples into a 2D integer tensor\nMAX_NUM_WORDS=len(LI.vocab)+1\nsequences=[LI.getIds(el) for el in input_list]\nMAX_SEQUENCE_LENGTH=max([len(s) for s in sequences])\n# test_sequence=tokenizer.texts_to_sequences(testset['text'])\n\n# word_index = tokenizer.word_index\n# print('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n# data_test=pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nlabels = to_categorical(np.asarray(le.fit_transform(dataset['class'])))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d638e69041e4a309471901b52a6623a2c046654e"},"cell_type":"code","source":"num_words =MAX_NUM_WORDS\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.1\n\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-num_validation_samples]\ny_train = labels[:-num_validation_samples]\nx_val = data[-num_validation_samples:]\ny_val = labels[-num_validation_samples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2491d8135d2264cc13e260d90132d52d2d7aee2"},"cell_type":"code","source":"embedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer='uniform',\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15ed4a4ed952be4749c19fd301c578050186e9af"},"cell_type":"code","source":"len(dataset['class'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46b837b9e0873d8a1fff9dd645bcca1b8b9ec967"},"cell_type":"code","source":"from keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"221070f12ff8bac63b6451124bdd604f2c3a95ba"},"cell_type":"code","source":"print('Training model.')\n\n# train a 1D convnet with global maxpooling\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(5)(x)\n# # x = Conv1D(128, 5, activation='relu')(x)\n# # x = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\nx = Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\nx = Dense(32, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\npreds = Dense(20, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ef452893e578773127b31c4409ea07daeb611a4"},"cell_type":"code","source":"model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=25,validation_data=(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea0241109cde44e08d4ecb547ff6f29c3bb93a04"},"cell_type":"code","source":"from keras import backend as K\nfrom keras.layers import LSTM,Dropout\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1362cce0e60ae1c1a8ea723bc533f340fa1c20c3"},"cell_type":"code","source":"print('Training model.')\n\n# train a 1D convnet with global maxpooling\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx=LSTM(128,activation='tanh',return_sequences=True,kernel_regularizer=regularizers.l2(0.01))(embedded_sequences)\nx=Dropout(0.2)(x)\nx=LSTM(128,activation='tanh',return_state=True,kernel_regularizer=regularizers.l2(0.01))(x)\nx=Dropout(0.2)(x[0])\npreds = Dense(20, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(x)\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=[f1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5781d4537b95c85d688a2e9d7237ad8e6a1d5be2"},"cell_type":"code","source":"model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=25,validation_data=(x_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa0603bbec9e9028436ed391af18d0eb1b7a1776"},"cell_type":"code","source":"y_pred=ClassiPipelined.predict(testset['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1de1b9405eb5d56228ce23ee38ea1cf75bc937af"},"cell_type":"code","source":"del testset['text']\ntestset['cuisine']=y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8079bd693b84fab73b488286f7006e1af5025958"},"cell_type":"code","source":"testset.to_csv('sample_submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}