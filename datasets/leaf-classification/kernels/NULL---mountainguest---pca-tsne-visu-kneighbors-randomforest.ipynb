{"cells":[{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8697b502d822d09459d3156e911fff2f2ba8d2a2"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')   #probably won't be used here","execution_count":1,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8224538275730dedd6405fc7f2a5d94f4e231326"},"cell_type":"code","source":"train.shape, test.shape","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"7b788ebbd07de9559b4a033c5d0b7e4a52ce50ea"},"cell_type":"markdown","source":"----------"},{"metadata":{"_uuid":"e28d93f0198d93291507934ffdcadd1b55c30aaf"},"cell_type":"markdown","source":"Some helper slices:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9aad370d90fd651f9bd478d676538926ccebe010"},"cell_type":"code","source":"margin = slice(2,66)\nshape = slice(66,130)\ntexture = slice(130,None)\nspecies = slice(1,1)","execution_count":3,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b3031a949d04850f13cddcb47768abd75317b403"},"cell_type":"code","source":"from matplotlib import cm\ncolmap = cm.get_cmap('cool', 30)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"9922c35ef5baf2f64a94164a2042b79441990da0"},"cell_type":"markdown","source":"# 1) Reduction dimensionality"},{"metadata":{"trusted":false,"_uuid":"85c53216fb9661eda9cf7e5734e6e7944405df83"},"cell_type":"code","source":"train.head(3)","execution_count":5,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e3fe51d22e0e215c89c23037884d8201be7e0966"},"cell_type":"code","source":"classes = train[\"species\"].value_counts()\n\ncount=len(classes)\navg=classes[:].mean()\nstd=classes[:].std()\n\nprint(\"{0} classes with {1} (+/-{2}) instances per class,\".format(count, avg, std))\nprint(\"{0} training instances with {1} features.\".format(len(train), train.shape[1]))","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"8b1eee12215ef78e4e7c5bbcee8836ba40a1fcd4"},"cell_type":"markdown","source":"That's not great, in this case we can only (try) to reduce the number of features.\n\nLet's try to look at the correlation matrix for the *margin*, *shape* and *texture* features groups."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"008a2205732f1163e5c462e4bf6a7e62efe5ea91"},"cell_type":"code","source":"def corr_sub_plot(ax, df, title=\"\"):\n    corr = df.corr()\n    \n    avg_corr = corr.values[np.triu_indices_from(corr.values,1)].mean()\n    ax.set_title(title+\" ({0:.4})\".format(avg_corr))\n    labels=range(1,len(corr.columns),4)\n    ax.set_yticks(labels)\n    ax.set_yticklabels(labels)\n    return ax.imshow(corr, interpolation=\"nearest\", cmap=colmap, vmin=-1, vmax=1)\n\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"92e8fe7e71d899af72cb0f3c93ab7f3b2b4a9e15"},"cell_type":"code","source":"f, ax = plt.subplots(2, 2,figsize=(10,10))\n\ncorr_sub_plot(ax[0,0], train.iloc[:,margin], title=\"Margin\")\ncorr_sub_plot(ax[0,1], train.iloc[:,shape], \"Shape\")\ncax = corr_sub_plot(ax[1,0], train.iloc[:,texture], \"Texture\")\n\nf.colorbar(cax, ax=ax.ravel().tolist())\n\nax[1,1].set_visible(False)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"7ece5847d3c934cba53832407451e5d2dc406ca8"},"cell_type":"markdown","source":"We printed in parenthesis the average correlation coefficient of the upper right triangle for each sub sets.\n\n- **Margin**: features in the center seems to be more correlated than those on the edges, forming highly positives and negatives correlated rectangular areas.\n- **Shapes**: features next to each others are highly correlated (pink), sounds reasonable. \n- **Texture**: more sporadic than the margin and shape."},{"metadata":{"_uuid":"31545b42ae6816456fcdd66aea3305b90c8cde17"},"cell_type":"markdown","source":"-----------------\n"},{"metadata":{"_uuid":"d422e389b340ef1f65ab00ec13b2cc56d0c2a24e"},"cell_type":"markdown","source":"We can try to reduce the number of features through principal component analysis (PCA). Since PCA tries to maximize the variance, we need to normalize the data."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"7784a0bc32d2cbf279ba5ffc5c647ca7095a8386"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ntexture_n = StandardScaler().fit_transform(train.iloc[:,texture])\nshape_n = StandardScaler().fit_transform(train.iloc[:,shape])\nmargin_n = StandardScaler().fit_transform(train.iloc[:,margin])","execution_count":9,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"cfa07e4ff0c901b99afc09bcae2d26f7f4a4d6e8"},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ndef pca_variance(data, keeped_variance):\n    pca = PCA(n_components=keeped_variance)\n    proj_margin =pca.fit_transform(data)\n    return pca.n_components_","execution_count":10,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f566bb68479de14b78ebc0b357259a384d8ae3af"},"cell_type":"code","source":"pca_red = \"PCA reduced 65 features to {0}, preserving {1}% of the input's variance \"\nprint(pca_red.format(pca_variance(texture_n, 0.95), 0.95))\nprint(pca_red.format(pca_variance(texture_n, 0.99), 0.99))","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"1a1b569fabdd78ad27efe936d7202bf1f9c9837f"},"cell_type":"markdown","source":"Let's plot the number of dimensions in function of the proportion of variance we want to preserve from the original datasets (`n_components`)."},{"metadata":{"trusted":false,"_uuid":"5bac5a106f980219429ff5d3f2529c54fdac5b23"},"cell_type":"code","source":"ranger = np.arange(0.90, 1, 0.005)\ndims_texture = [pca_variance(texture_n, e) for e in ranger]\ndims_margin = [pca_variance(margin_n, e) for e in ranger]\ndims_shape = [pca_variance(shape_n, e) for e in ranger]\n\nplt.plot(ranger, dims_texture, label=\"texture\")\nplt.plot(ranger, dims_margin, label=\"margin\")\nplt.plot(ranger, dims_shape, label=\"shape\")\nplt.legend(loc='upper left')","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"d71d5f8d79360c4d8bd1d8de6e8373c15cf0758b"},"cell_type":"markdown","source":"Since shapes are highly correlated in a regular way, it's easier for PCA to reduce dimensionality efficiently.\nWe can try to reduce dimensions to 2 and draw the result (for fun)."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b46b971635718ca01ff24ffef028bce62c3622cb"},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\npre_X = np.concatenate([texture_n, margin_n, shape_n], axis=1)\nX_reduced = TSNE(n_components=2, random_state=4422, init=\"pca\").fit_transform(pre_X)","execution_count":13,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e9c1acc8f056040f16ea154043bb8ed410109fc4"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n# cat = CategoricalEncoder(encoding=\"ordinal\")  # waiting next sklearn update\nlabels = le.fit_transform(train.iloc[:, 1])","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"fb270e9176298b672f760fdb51da3f9ceea32acf"},"cell_type":"markdown","source":"We link together all instances from the same class."},{"metadata":{"trusted":false,"_uuid":"62d29d26ad6f3f03b191ea4a250cf43e2c97f084"},"cell_type":"code","source":"plt.figure(figsize=(10,10))\n\nfor i in range(0,99):\n    keeped= labels == i\n    plt.plot(X_reduced[keeped, 0], X_reduced[keeped, 1], linestyle=':')\n\nplt.axis('off')","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"8c7b884d2b6ae15d2f8719f3d6f750fa7f659a04"},"cell_type":"markdown","source":"Since we are projeting a 200ish dimensional space into two, it can be worse. Most of the classes are regrouped together, even if PCA's job is not to do so !\n\nWe can also compute `TSNE` in 3D."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"34b3e225fb183900bc41e7c3e122e7b0ca9dca29"},"cell_type":"code","source":"X_reduced_3D = TSNE(n_components=3, random_state=4422).fit_transform(pre_X)","execution_count":16,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"be79bacc5174175d82dd2e63897861e03b80f884"},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(10,10))\nax = Axes3D(fig)\n\n#in order to interact with 3D plots\n#%matplotlib notebook \n\nfor i in range(0,99):\n    keeped= labels == i\n    ax.plot(X_reduced_3D[keeped, 0], X_reduced_3D[keeped, 1], X_reduced_3D[keeped, 2], linestyle=':')\n\nax.view_init(20, 35)\n","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"54cd15fb86b993104c7e08071979afa83bbfbe44"},"cell_type":"markdown","source":"----------------"},{"metadata":{"_uuid":"720b6918e33ee007dd6c1061977ee59a296d4587"},"cell_type":"markdown","source":"### Images"},{"metadata":{"trusted":false,"_uuid":"fdc657bf649a074923dc64340c6d195ee20bc522"},"cell_type":"code","source":"import matplotlib.image as mimg\n\nplt.figure(1)\nimg = mimg.imread('../input/images/99.jpg')\nplt.imshow(img, cmap='cool')","execution_count":10,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e8c7c5b87018d4b871ce115c6b28bd9873eb6739"},"cell_type":"code","source":"from skimage.feature import *\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage.morphology import binary_erosion, generate_binary_structure\n\n\nfig, ax = plt.subplots()\n\n# 130, 12 are nice\nimg_to_process = ndimage.binary_erosion(mimg.imread('../input/images/90.jpg'), structure=np.ones((2,2)))\n\ncenter_of_mass = ndimage.measurements.center_of_mass(img_to_process)\n\ncoords = corner_peaks(corner_harris(img_to_process, k=0), min_distance=4)\ncoords_subpix = corner_subpix(img_to_process, coords, alpha=0.2)\n\nax.imshow(img_to_process, interpolation='nearest', cmap='cool')\n\n# the extraction of sharp edges is still in progress\n# ax.plot(coords_subpix[:, 1], coords_subpix[:, 0], '+w', markersize=15) \nax.plot(center_of_mass[1], center_of_mass[0],'og', markersize=10)\n\nplt.show()","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"3f5ae99340d17306d538dfe4a3a8b437ecc38c4d"},"cell_type":"markdown","source":"We can try to use the center of mass's coordinates (divided by width and height of the image) as a feature. But we won't do it in this version."},{"metadata":{"_uuid":"52447b4092f9536aaf80494b3c61e1f4338e5336"},"cell_type":"markdown","source":"# 2) Preparation"},{"metadata":{"_uuid":"5c8a069c35fc542b3884d680f419000ae7f3ffdb"},"cell_type":"markdown","source":"We first build the pipeline.\n\n`SliceSelector` is a [transformer](http://scikit-learn.org/stable/data_transforms.html). It job is to simply select the desired columns in the data frame."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3f00a8a64ab531640932ba5e07f4360452b2a1c1"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass SliceSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, slice):\n        self.slices = slice\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X.iloc[:,self.slices]","execution_count":20,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c8dcf598d0607cbece80c2d1c3923821d6b5cb9c"},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\n\ndef make_num_pipeline(slices, pca_coef):\n    return Pipeline([('cursor', SliceSelector(slices)), \n                     ('scaler', StandardScaler()), \n                     ('PCA', PCA(n_components=pca_coef)),])\n\nmargin_pipeline = make_num_pipeline(margin, 0.96)\nshape_pipeline = make_num_pipeline(shape, 0.98)\ntexture_pipeline = make_num_pipeline(texture, 0.96)","execution_count":21,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"04221838fd041aeb6384fe494df6f59c070ae522"},"cell_type":"code","source":"full_pipeline = FeatureUnion(transformer_list=[\n    (\"margin_pipeline\", margin_pipeline),\n    (\"shape_pipeline\", shape_pipeline),\n    (\"texture_pipeline\", texture_pipeline),\n])","execution_count":22,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0c3cca1c12db6e5d48d097c18ad0e73176b41041"},"cell_type":"code","source":"y = labels\nX = full_pipeline.fit_transform(train)","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"c65c7fc33e25fe34c949cb175f909db0e92f31f8"},"cell_type":"markdown","source":"Since there's only ten instances per class we have to be carefull while picking instances randomly for the test/train set generation. \n\n`StratifiedShuffleSplit` will help us preserve the class distribution after the test/train split."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8dd5a78e3e43d94e69ee11a525bf296b86b63846"},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(X, y):\n    X_train, y_train = X[train_index], y[train_index]\n    X_test, y_test = X[test_index],  y[test_index]","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"1b77d8bdea137db09533ba34f968f6c5c3ea5304"},"cell_type":"markdown","source":"# 3) Classification\n\nWe can now try to classify, we will test `KNeighbors`, `RandomForest` and `SVC` for the classification.\n\nWe also want to keep track of the accuracy and the cross-entropy loss to measure performance. We also want the cross-entropy because we want to measure the confidence of the classifer in its predictions. Partly because the test set is 20% of the total, which means that we'll have 2 instances per class in the testing set."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"22a6a46ccfdffa9938dfb1c6b379d7dc11095f51"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, log_loss\ndef fast_tests(clf, X, y, power=1):\n    preds = clf.predict(X_test)\n    acc = accuracy_score(y, preds)\n    logloss = log_loss(y_test, np.power(clf.predict_proba(X_test), power), labels=y)\n    return acc, logloss","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"0198a59397cb1a38762b3a34ba8fedd04f7efc5e"},"cell_type":"markdown","source":"### KNeighbors"},{"metadata":{"trusted":false,"_uuid":"8e72e8b1e807e9c045152a03c6bb669b0333e921"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(weights=\"distance\",n_jobs=-1, p=2)\nneigh.fit(X_train, y_train) ","execution_count":26,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c81944b7d94fb07700bd8c1a65ef9b0c9f0c1313"},"cell_type":"code","source":"fast_tests(neigh, X_test, y_test)","execution_count":27,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a8f6ffb4ea0a78355c5ee3acf462c154526600eb"},"cell_type":"code","source":"param_dist = {\"n_neighbors\": range(4,10),\n              \"weights\":[\"distance\"],\n             \"leaf_size\": range(1,10),\n             }\nrnd_n = RandomizedSearchCV(KNeighborsClassifier(p=2), param_dist, n_jobs=-1, n_iter=20)\n\nrnd_n.fit(X_train, y_train) ","execution_count":28,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0a30b991698590b3cb453894c67e953c750a4d63"},"cell_type":"code","source":"optimized_neigh = rnd_n.best_estimator_\nprint(optimized_neigh)\nfast_tests(optimized_neigh, X_test, y_test)","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"e9110670ba19fc0b6c68d2e76e7b2117ae5f398f"},"cell_type":"markdown","source":"### RandomForest Classifier"},{"metadata":{"_uuid":"7847c1aa6b39bbf6c42ebdd96b91951ca886e8ce"},"cell_type":"markdown","source":"Random forest don't require scaling, but it's already done in the pipeline."},{"metadata":{"trusted":false,"_uuid":"32aed11c0153cf2bc50f7a2d1cc44670ef5e30d4"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(max_depth=28, n_estimators=500,random_state=0, max_features=20, n_jobs=-1)\nforest.fit(X_train, y_train)","execution_count":43,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"edcd2b8bd60aa69518b15658d5bfcb03f07071f3"},"cell_type":"code","source":"fast_tests(forest, X_test, y_test)","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"8a8a45c22f841464bb041bc7cec16053efd5f6dc"},"cell_type":"markdown","source":"- Since we have few instances per class, we enable bootstraping; ie, random instances are sampled and replaced in the dataset.\n- Since it's a classification problem, we allow `RandomizedSearchCV` search the minimum samples per leaf at 1 (default) and minimul sample split at 2.\n- The cell below might take half an hour on a regular CPU."},{"metadata":{"trusted":false,"_uuid":"7dfded21e8bbca41133bb811ca6dbf04c8399df6"},"cell_type":"code","source":"from scipy.stats import randint as sp_randint\nparam_dist = {\"max_depth\": sp_randint(20, 30),\n              \"max_features\": sp_randint(15, 60),\n              \"min_samples_split\": sp_randint(2, 10),\n             }\nforest=RandomForestClassifier(n_jobs=-1, random_state=4422, bootstrap=True, criterion=\"entropy\", n_estimators=600)\nrnd_f = RandomizedSearchCV(forest, param_dist, n_iter=15, n_jobs=-1)\n\nrnd_f.fit(X_train, y_train) ","execution_count":34,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"38f1fbaf14444a7e8a41e13c7432be8d59e57e87"},"cell_type":"code","source":"optimized_forest = rnd_f.best_estimator_\nprint(optimized_forest)\noptimized_forest.fit(X_train, y_train) \nfast_tests(optimized_forest, X_test, y_test)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"dc8036de1af8793ed05a6e2d90aacb601da83005"},"cell_type":"markdown","source":"RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n            max_depth=26, max_features=17, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=-1,\n            oob_score=False, random_state=4422, verbose=0,\n            warm_start=False)"},{"metadata":{"_uuid":"89df7f3b365ff52c497a904656d8396005ab8e6d"},"cell_type":"markdown","source":"### Ensemble"},{"metadata":{"_uuid":"fc7432ec2c8121a12136f7b128940d387f29ecf8"},"cell_type":"markdown","source":"Then let's try to combine our optimized classifiers in an soft voting manner."},{"metadata":{"trusted":false,"_uuid":"efc4d1b9a6a617c98a616aaccac6f53730a74e1d"},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nests = [('neigh', optimized_neigh), ('forest', optimized_forest)]\nvoting_clf = VotingClassifier(estimators=ests, voting='soft')\nvoting_clf.fit(X_train, y_train)","execution_count":40,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c44349003cfd9eb662d5c348c2e77243629f0699"},"cell_type":"code","source":"fast_tests(voting_clf, X_test, y_test)","execution_count":41,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}