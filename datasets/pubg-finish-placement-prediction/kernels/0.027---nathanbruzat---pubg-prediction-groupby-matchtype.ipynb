{"cells":[{"metadata":{"_uuid":"8f188553f201ec735b6733a1a963ef45549d1f98"},"cell_type":"markdown","source":"# prediction group by match type\nHello, it's my first public kernel. i will try to make prediction for each match type. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport math\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.models import load_model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport gc\n\nfrom sklearn import preprocessing\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f75f472cbd4ee8183672f13366b29e6ce27bc0b1"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train_V2.csv\")\ntest = pd.read_csv(\"../input/test_V2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d1402c2c4e06531ba5347740706086f8111a880"},"cell_type":"code","source":"train = train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc05f6b6639053fa6e82afbd7d0d288e77f68db8"},"cell_type":"code","source":"train['rideDistance'] = (train['rideDistance']/10).round(0)\ntrain['swimDistance'] = (train['swimDistance']/10).round(0)\ntrain['walkDistance'] = (train['walkDistance']/10).round(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83fa94ea0234ac1fcb17fb08b5d4d3a275ddc06c"},"cell_type":"code","source":"test['rideDistance'] = (test['rideDistance']/10).round(0)\ntest['swimDistance'] = (test['swimDistance']/10).round(0)\ntest['walkDistance'] = (test['walkDistance']/10).round(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80cf4e80f6cf95fca61f74a2a0cad931473ab401"},"cell_type":"code","source":"#a Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df,display=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    if display:\n        start_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    if display:\n        end_mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea34c54c8f6c3ebee8f8dd8ff1e03a3fcaa143b6"},"cell_type":"markdown","source":"# Split by matchType"},{"metadata":{"_uuid":"ee163d59f60cf9f6ebcd704179e36a0160577527"},"cell_type":"markdown","source":"In this part, I merge test and train value to create each matchType dataset. To identified test value, I add winPlacePerc value at -1."},{"metadata":{"trusted":true,"_uuid":"56e1f66a0c41824b85f33ca0a0dabcc9333aa40b"},"cell_type":"code","source":"test[\"winPlacePerc\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67ef0ae34cf6d4fe9248f268c014683714da94ef"},"cell_type":"code","source":"df = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5399dad2afe7a8954b572156145253e04c499210"},"cell_type":"code","source":"del train\ndel test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85542cc04bbebfb350a6a28880d85d2f10ee3c01"},"cell_type":"code","source":"df[\"Id\"] = df.index ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a905cab451747e60a4f7d1c0397e316a39bed7bb"},"cell_type":"markdown","source":"I tried with different combination, second is better. I make dict with name of type and batch size. I serialize each dataset to release RAM."},{"metadata":{"trusted":true,"_uuid":"89ebc806b4ce76d7ac8247b89e01e8765e612bbf"},"cell_type":"code","source":"#squad_fpp = df[(df['matchType']=='squad-fpp') | (df['matchType']=='normal-squad-fpp')]\n#duo = df[(df['matchType']=='duo') | (df['matchType']=='normal-duo')]\n#solo_fpp = df[(df['matchType']=='solo-fpp') | (df['matchType']=='normal-solo-fpp')]\n#squad = df[(df['matchType']=='squad') | (df['matchType']=='normal-squad')]\n#duo_fpp = df[(df['matchType']=='duo-fpp') | (df['matchType']=='normal-duo-fpp')]\n#solo = df[(df['matchType']=='solo') | (df['matchType']=='normal-solo')]\n#flare = df[(df['matchType']=='flaretpp')|(df['matchType']=='flarefpp')]\n#crash = df[(df['matchType']=='crashtpp') | (df['matchType']=='crashfpp')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c78a7c1dae5bc629b7ee5827e63fba828bd07aea"},"cell_type":"code","source":"#dp_by_type = {'squad_fpp':[squad_fpp,39525],\n#              'flare':[flare,62],\n#              'crash':[crash,149],\n#              'solo_fpp':[solo_fpp,12098],\n#              'duo_fpp':[duo_fpp,22586],\n#              'squad':[squad,14110],\n#              'solo':[solo,4067],\n#              'duo':[duo,7105]\n#              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10e040c57e508db8cf0c0b3c1c222e289f1a7f86"},"cell_type":"code","source":"duo = df[(df['matchType']=='duo') | (df['matchType']=='normal-duo')|(df['matchType']=='duo-fpp') | (df['matchType']=='normal-duo-fpp')]\nsquad = df[(df['matchType']=='squad') | (df['matchType']=='normal-squad')|(df['matchType']=='squad-fpp') | (df['matchType']=='normal-squad-fpp')]\nsolo = df[(df['matchType']=='solo') | (df['matchType']=='normal-solo')|(df['matchType']=='solo-fpp') | (df['matchType']=='normal-solo-fpp')]\nflare = df[(df['matchType']=='flaretpp')|(df['matchType']=='flarefpp')]\ncrash = df[(df['matchType']=='crashtpp') | (df['matchType']=='crashfpp')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bff2ab7712ada292fc9bfac754964f422791e37"},"cell_type":"code","source":"dp_by_type = {'flare':[flare,62],\n              'crash':[crash,149],\n              'squad':[squad,53635],\n              'solo':[solo,16165],\n              'duo':[duo,29691]\n              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"715e3d5df00d80b43cc1a339ae7a5ae65b80780c"},"cell_type":"code","source":"for name,ele in dp_by_type.items():\n    print(name + \" : \" + str(len(ele[0])))\n    ele[0].to_csv(name+'.csv', index=False)\n    ele[0] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb24bc20a1052613b0e982bfb7c19f51fc64911b"},"cell_type":"code","source":"#del squad_fpp,duo,solo_fpp,squad,duo_fpp,solo,flare,crash","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77b9799fc3305286aa3ca2a8bdcc22e1fa7a7500"},"cell_type":"code","source":"del duo,squad,solo,flare,crash","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a65091f03d7c9d27f59bb9789f89b379e5e6031d"},"cell_type":"code","source":"del df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92c99f447627695c19be8d0908d4aa0cc3280d0e"},"cell_type":"markdown","source":"# FeatureEngineering"},{"metadata":{"_uuid":"00118058ac0c5995e6f3433b5afaca78cf08f44f"},"cell_type":"markdown","source":"this part is for feature engineering. there are two part, one for make new feartures and the second to coralate data together."},{"metadata":{"trusted":true,"_uuid":"44b385f4d0093ffb43ace369c8a9ff6126de90fb"},"cell_type":"code","source":"def featureEngineering(df):\n    return featureEngineeringSecond(reduce_mem_usage(featureEngineeringFirst(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc51ae5d318857f73d90f3434b558ca266eae866"},"cell_type":"code","source":"def items(df):\n    df['items'] = df['heals'] + df['boosts']\n    return df\n\ndef survival(df):\n    df[\"survival\"] = df[\"revives\"] + df[\"boosts\"] + df[\"heals\"]\n    return df\n\ndef players_in_team(df):\n    agg = df.groupby(['groupId']).size().to_frame('players_in_team')\n    return df.merge(agg, how='left', on=['groupId'])\n\ndef total_distance(df):\n    df['total_distance'] = df['rideDistance'] + df['swimDistance'] + df['walkDistance']\n    return df\n\ndef total_time_by_distance(df):\n    df[\"total_time_by_distance\"] = df[\"rideDistance\"]/5+df[\"walkDistance\"]+df[\"swimDistance\"]*5\n    return df\n\ndef headshotKills_over_kills(df):\n    df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n    df['headshotKills_over_kills'].fillna(0, inplace=True)\n    return df\n\ndef teamwork(df):\n    df['teamwork'] = df['assists'] + df['revives']\n    return df\n\ndef total_items_acquired(df):\n    df['total_items_acquired'] = df[\"boosts\"] + df[\"heals\"] + df[\"weaponsAcquired\"]\n    return df\n\ndef killPlace_over_maxPlace(df):\n    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n    df['killPlace_over_maxPlace'].fillna(0, inplace=True)\n    df['killPlace_over_maxPlace'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef distance_over_heals(df):\n    df['walkDistance_over_heals'] = df['total_distance'] / df['heals']\n    df['walkDistance_over_heals'].fillna(0, inplace=True)\n    df['walkDistance_over_heals'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef distance_over_kills(df):\n    df['walkDistance_over_kills'] = df['total_distance'] / df['kills']\n    df['walkDistance_over_kills'].fillna(0, inplace=True)\n    df['walkDistance_over_kills'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef headshot_kill_rate(df):\n    df['headshot_kill_rate'] = (df['headshotKills']+1)/(df['kills']+1)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49ea8d07d55e350301f6405f5cf78dcca2e93f95"},"cell_type":"code","source":"def featureEngineeringFirst(df):\n    print(\"        Feature Engineering First started...\")\n        \n    df = items(df)\n    gc.collect()\n        \n    df = survival(df)\n    gc.collect()\n    \n    df = players_in_team(df)\n    gc.collect()\n    \n    df = total_distance(df)\n    gc.collect()\n    \n    df = total_time_by_distance(df)\n    gc.collect()\n    \n    #df = headshotKills_over_kills(df)\n    gc.collect()\n\n    df = teamwork(df)\n    gc.collect()\n    \n    df = total_items_acquired(df)\n    gc.collect()\n    \n    #df = killPlace_over_maxPlace(df)\n    gc.collect()\n    \n    #df = distance_over_heals(df)\n    gc.collect()\n    \n    #df = distance_over_kills(df)\n    gc.collect()\n    \n    #df = headshot_kill_rate(df)\n    gc.collect()\n    \n    df = reduce_mem_usage(df)\n    \n    print(\"        Feature Engineering First fished \")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d756a1737a1a84e484df7fef6a99778fc60518e5"},"cell_type":"code","source":"def min_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId','groupId'])[features].min()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank\n\ndef max_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId', 'groupId'])[features].max()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank\n\ndef sum_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId', 'groupId'])[features].sum()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank\n\ndef median_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId', 'groupId'])[features].median()\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank\n\ndef mean_by_team(df):\n    features = list(df.columns)\n    features.remove('Id')\n    features.remove('groupId')\n    features.remove('matchId')\n    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    return agg, agg_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"779608ab1d69ae329cc0f7888a94929d753d2d6d"},"cell_type":"code","source":"def mergeWithAgg(df,agg,agg_rank,name):\n    print(\"            Merge \"+name)\n    df = df.merge(agg, suffixes=[\"\", \"_\"+name], how='left', on=['matchId', 'groupId'])\n    df = df.merge(agg_rank, suffixes=[\"\", \"_\"+name+\"_rank\"], how='left', on=['matchId', 'groupId'])\n    return reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f421c0968dd2ebd60c772c411bc3f4f64a8c7eb"},"cell_type":"code","source":"def featureEngineeringSecond(df):\n    print(\"        Feature Engineering Second started...\")\n    \n    print(\"            Min\")\n    min_, min_rank = min_by_team(df)\n    gc.collect()\n    \n    print(\"            Max\")\n    max_, max_rank = max_by_team(df)\n    gc.collect()\n    \n    print(\"            Sum\")\n    sum_, sum_rank = sum_by_team(df)\n    gc.collect()\n    \n    print(\"            Median\")\n    median_, median_rank = median_by_team(df)\n    gc.collect()\n    \n    print(\"            Mean\")\n    mean_, mean_rank = mean_by_team(df)\n    gc.collect()\n    \n    df = mergeWithAgg(df,min_, min_rank,\"min\")\n    del min_, min_rank\n    df = mergeWithAgg(df,max_, max_rank,\"max\")\n    del max_, max_rank\n    df = mergeWithAgg(df,sum_, sum_rank,\"sum\")\n    del sum_, sum_rank\n    df = mergeWithAgg(df,median_, median_rank,\"median\")\n    del median_, median_rank\n    df = mergeWithAgg(df,mean_, mean_rank,\"mean\")\n    del mean_, mean_rank\n        \n    print(\"        Feature Engineering Second finished\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d53a7465da5a0ef0ea93d6b2043b5163c6d6e51"},"cell_type":"markdown","source":"# Learning part"},{"metadata":{"_uuid":"8f90140f227519366310604379343f4182385638"},"cell_type":"markdown","source":"This part is for training and predicting. I use Sequential model from keras. "},{"metadata":{"trusted":true,"_uuid":"44fe7f8542b9a1726efb9239e6b2b59c985cc640"},"cell_type":"code","source":"def baseline_model(input_dim):\n    model = Sequential()\n    # create model\n    model.add(Dense(32, kernel_initializer='he_normal',input_dim=input_dim , activation='selu'))\n    model.add(Dense(64, kernel_initializer='he_normal', activation='selu'))\n    model.add(Dense(128, kernel_initializer='he_normal', activation='selu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(128, kernel_initializer='he_normal', activation='selu'))\n    model.add(BatchNormalization())\n    model.add(Dense(64, kernel_initializer='he_normal', activation='selu'))\n    model.add(BatchNormalization())\n    model.add(Dense(32, kernel_initializer='he_normal', activation='selu'))\n    model.add(Dense(8, kernel_initializer='he_normal', activation='selu'))\n    model.add(BatchNormalization())\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06c55d7366500b8c8d27702965525603e88ba7d4"},"cell_type":"code","source":"def normalize(df):\n    result = df.copy()\n    for feature_name in df.columns:\n        if df[feature_name].dtype != object:\n            max_value = df[feature_name].max()\n            min_value = df[feature_name].min()\n            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3b9a4694cc471b3d85e2da11ce880666dc08c45"},"cell_type":"code","source":"def learningPart(df,batch_size):\n    print(\"    Data processing started...\")\n    df_winPlacePerc = df.winPlacePerc\n    df = df.drop('winPlacePerc', axis=1)\n    df = df.drop('matchType', axis=1)\n    df = featureEngineering(df)\n    df = df.drop('matchId', axis=1)\n    df = df.drop('groupId', axis=1)\n    gc.collect()\n    df['winPlacePerc'] =  df_winPlacePerc.values\n    \n    del df_winPlacePerc \n    gc.collect()\n    \n    print(\"    Data scaling started...\")\n    df = normalize(df)\n    print(\"    Data scaling finised\")\n    \n    train_df = df[df[\"winPlacePerc\"] != -1]\n    train_df = train_df.dropna()\n    gc.collect()\n    test_df = df[df[\"winPlacePerc\"] == -1]\n    test_df = test_df.drop('winPlacePerc', axis=1)\n    del df\n    gc.collect()\n    \n    train_df_Y = train_df.winPlacePerc\n    train_df_X = train_df.drop('winPlacePerc', axis=1)\n    del train_df\n    print(\"    Data processing finished\")\n    \n\n    \n    print(\"    Data spliting started...\")\n    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(train_df_X, train_df_Y, test_size=0.2)\n    del train_df_Y\n    del train_df_X\n    print(\"    Data spliting finished\")\n    \n    print(\"    Model training started...\")\n    epochs = 80\n    \n    callbacks = [ModelCheckpoint('best_model_df.h5', verbose=0, monitor='val_loss',save_best_only=True, mode='auto')]\n    \n    model_df = baseline_model(X_train_df.shape[1])\n    history_df = model_df.fit(X_train_df, y_train_df, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_test_df, y_test_df),callbacks=callbacks)\n    gc.collect()    \n    print(\"    Model training finished\")\n    \n    print(\"    Prediction started...\")\n    best_model_df = load_model('best_model_df.h5')\n    \n    predict_y_df = best_model_df.predict(t_test_df)\n    predict_y_df = pd.DataFrame({'Id':test_df['Id'].values,'winPlacePerc':predict_y_df.flatten()})\n    print(\"    Prediction finished\")\n    \n    return predict_y_df, history_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17e1ead65acc571a2c323cb3fea759b5c4e80f9c"},"cell_type":"code","source":"def minmax(df):\n    df_minmax = pd.DataFrame()\n    for feature_name in df.columns:\n        v_min = df[feature_name].min()\n        v_max = df[feature_name].max()\n        df_minmax[feature_name] = pd.Series([v_min,v_max])\n    \n    return df_minmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5958ad4c3b7abd2bb10afe3eee1150b51df5264e"},"cell_type":"code","source":"def learningPart(df,batch_size):\n    print(\"    Data processing started...\")\n    df = df.drop('matchType', axis=1)\n    train_df = df[df[\"winPlacePerc\"] != -1]\n    gc.collect()\n    test_df = df[df[\"winPlacePerc\"] == -1]\n    test_df = test_df.drop('winPlacePerc', axis=1)\n    del df\n    gc.collect()\n    \n    train_df_Y = train_df.winPlacePerc\n    train_df_X = train_df.drop('winPlacePerc', axis=1)\n    del train_df\n    \n    train_df_X = featureEngineering(train_df_X)\n    train_df_X = train_df_X.drop('matchId', axis=1)\n    train_df_X = train_df_X.drop('groupId', axis=1)\n       \n    print(\"    Data processing finished\")\n    \n    \n    print(\"    Data spliting started...\")\n    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(train_df_X, train_df_Y, test_size=0.2)\n    del train_df_Y\n    del train_df_X\n    gc.collect()\n    print(\"    Data spliting finished\")\n    \n\n    print(\"    Data test processing started...\")\n    test_df = featureEngineering(test_df)\n    test_df = test_df.drop('matchId', axis=1)\n    test_df = test_df.drop('groupId', axis=1) \n    \n    print(\"    Data test processing finished\")\n    \n    \n    print(\"    Data scaling started...\")\n    minmaxv = pd.concat([minmax(test_df),minmax(X_train_df),minmax(X_test_df)])\n    scaler_df = preprocessing.MinMaxScaler(feature_range=(-1, 1),copy=False).fit(minmaxv)\n    del minmaxv\n    gc.collect()\n    X_train_df = scaler_df.transform(X_train_df)\n    gc.collect()\n    X_test_df = scaler_df.transform(X_test_df)\n    gc.collect()\n    t_test_df = scaler_df.transform(test_df)\n    print(\"    Data scaling finised\")\n    \n    print(\"    Model training started...\")\n    epochs = 80\n    \n    callbacks = [ModelCheckpoint('best_model_df.h5', verbose=0, monitor='val_loss',save_best_only=True, mode='auto')]\n    \n    model_df = baseline_model(X_train_df.shape[1])\n    history_df = model_df.fit(X_train_df, y_train_df, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_test_df, y_test_df),callbacks=callbacks)\n    gc.collect()    \n    print(\"    Model training finished\")\n    \n    print(\"    Prediction started...\")\n    best_model_df = load_model('best_model_df.h5')\n    \n    predict_y_df = best_model_df.predict(t_test_df)\n    predict_y_df = pd.DataFrame({'Id':test_df['Id'].values,'winPlacePerc':predict_y_df.flatten()})\n    print(\"    Prediction finished\")\n    \n    return predict_y_df, history_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0015de7b4d5f081bbd6c604e7918c2d8914e32b0","scrolled":false},"cell_type":"code","source":"result = {}\nfor name,typeGame in dp_by_type.items():\n    print(\"Start \"+ name)\n    predict, history = learningPart(pd.read_csv(name+\".csv\"),typeGame[1])\n    result[name] = [predict,history]\n    print(\"End \"+ name)\n    dp_by_type[name] = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ae763f0eb9b7da58e32198d4644c39f9d6bfa9a"},"cell_type":"markdown","source":"# Result"},{"metadata":{"_uuid":"9ce28e5ce7058e6938998ae88e847d804a3a04ab"},"cell_type":"markdown","source":"This part is for visualied training evolution."},{"metadata":{"trusted":true,"_uuid":"c844f8715b65d234dbe98e4c5d953927664886b6"},"cell_type":"code","source":"def displayHistory(history):\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation mae values\n    plt.plot(history.history['mean_absolute_error'])\n    plt.plot(history.history['val_mean_absolute_error'])\n    plt.title('Mean Abosulte Error')\n    plt.ylabel('Mean absolute error')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d77e0b164ad6f1e509df1827b678ab3dd64d3517"},"cell_type":"code","source":"for key,ele in result.items():\n    print(\"History for \" + str(key))\n    displayHistory(ele[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26dc09d317ddc95142d2b926fcee6c899d709074"},"cell_type":"markdown","source":"# PART concat"},{"metadata":{"_uuid":"6b52807899959b9a1abec78420e314449969d5cc"},"cell_type":"markdown","source":"This part is to merge result and make submission."},{"metadata":{"trusted":true,"_uuid":"1f2fc2e3a37d8c9da9e0a007565f4e174e6ae2dc"},"cell_type":"code","source":"data = []\nfor key,ele in result.items():\n    data.append(ele[0])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6fad0f6d464b4c6413913307606fbe4def6ab25"},"cell_type":"code","source":"df = pd.concat(data, ignore_index=True)\ndf = df.sort_values(by='Id', ascending=[True])\ndf = df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68eb71310af254ebf8974e8b7466aec57be531a6"},"cell_type":"code","source":"submission = pd.read_csv(\"../input/sample_submission_V2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b76d9216c686f95df06065637a70cb0422394feb"},"cell_type":"code","source":"submission[\"winPlacePerc\"] = df.winPlacePerc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3293a45bdd4c89ab2503fc05752000cd877c43d3"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"582e3bb3f7554a86cd64c641da0301abce9640bf"},"cell_type":"markdown","source":"Thank you for reading. If you have any questions/remarks, ask me."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}