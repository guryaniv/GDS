{"cells": [{"source": ["In this Kernel, we want to go through the data in order to develop intuitions while performing a few sanity checks."], "cell_type": "markdown", "metadata": {"_cell_guid": "799c5149-b713-46dc-a080-ce085f9a6cc8", "_uuid": "54fceb5f2476725a04ea83a9d5e6beea54a00765"}}, {"source": ["import pandas as pd\n", "from IPython.display import HTML, display"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "273d2d4f-495e-4197-ae5e-d3bb050c6377", "_uuid": "5eff6b831fe336b7a31d2a732f39c38e74776232", "collapsed": true}}, {"source": ["train_df = pd.read_csv('../input/train.tsv', sep='\\t')\n", "test_df = pd.read_csv('../input/test.tsv', sep='\\t')"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ecb7fefe-ba78-4a62-a85c-73bece3851bb", "_uuid": "6296502ab180a298aa160151abbf2213b5579811", "collapsed": true}}, {"source": ["We start by looking into the datatypes and the number of unique elements for each column and their missing values. We notice immediately that a lot of values are missing in **brand_name**. For now we'll replace them with 'unknown'. Generally speaking, in the context of this dataset, missing values in **brand_name** and **category_name** should be treated as actual values. The rationale behind this is the following: if a listing does not contain a **brand_name** or a **category_name**, it's less likely to be found by customers which in turn means that the price paid is more likely to be below market value."], "cell_type": "markdown", "metadata": {"_cell_guid": "d4a83b04-3a27-4b87-9e7a-d47221a142ae", "_uuid": "ff592f9d47aea4925336e0fd81ffcf81ac45fd63"}}, {"source": ["print('DataFrame shape:')\n", "print(train_df.shape)\n", "print('\\nData types:')\n", "print(train_df.dtypes)\n", "print('\\nUnique Count:')\n", "print(train_df.nunique())\n", "display(train_df.head())\n", "\n", "import missingno as msno\n", "msno.matrix(train_df)\n", "\n", "train_df['brand_name'].fillna('$not available$',inplace=True)\n", "train_df['category_name'].fillna('$not available$',inplace=True)\n", "\n", "test_df['brand_name'].fillna('$not available$',inplace=True)\n", "test_df['category_name'].fillna('$not available$',inplace=True)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e17c2bac-467f-41ac-9703-bc4e5d30fa5a", "_uuid": "f552963656e82520e8d5b09707efa00bbdd5ccff"}}, {"source": ["\n", "\n", "We can learn a couple things here. \n", "1. **train_id**: id of sample (drop during training)\n", "2. **name**: literally the name of the product\n", "3. **item_condition_id**: Encodes item condition on 5 levels\n", "4. **category_name**: The name of the category. Notice that it comes in multiple levels delimited by '/'\n", "5. **brand_name**: Literally the brand name (surprisingly few, considering the total number of rows)\n", "6. **price**: Our target\n", "7. **shipping**: a binary feature probably indicating if shipping costs are included or not\n", "8. **item_description**: the text part of the listing. There are almost as many unique entries here as there are data points\n", "\n", "\n", "Combining these observations with common sense, we can perform a few sanity checks:\n", "1. There should be some correlation between **item_condition_id** and **price**\n", "2. **price** is is unlikely to assume values below 0 so the values are likely to be log normal or something similar\n", "3. Common sense: **brand_name** and **category_name** will show strong biases towards a few select values\n", "4. Common sense: Listings that include the shipping price are likely to be more expensive\n", "\n", "While these points are not mandatory for subsequent analysis, they will show us a few things, e.g. 1. should show us in which order **item_condition_id** is defined and 4. will tell us if **shipping**=1 -> shipping included or shipping separate."], "cell_type": "markdown", "metadata": {"_cell_guid": "ffccf822-90d2-4006-a88a-d05518fba2e4", "_uuid": "e2a4b68b2d4f06d05ba9573c46a0b3b8a0861d22"}}, {"source": ["Sanity check 1: Thee should be some correlation between **item_condition_id** and **price**\n", "\n", "We start with a heatmap of the correlations. As the heatmap alone does not reveal anything, we will direcly plot price against **item_condition_id**. THis shows us that there are generally higher prices for lower **item_condition_id**. This lets us assume that 1 = pristine condition."], "cell_type": "markdown", "metadata": {"_cell_guid": "4a4484ef-4442-417f-8bd4-6a2c89aea76c", "_uuid": "f621f73ecfbf90e6205051064855680e4915c112"}}, {"source": ["import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "df_samp = train_df.sample(10000)\n", "c = train_df.corr()\n", "sns.heatmap(c)\n", "plt.figure()\n", "plt.scatter(df_samp['price'],df_samp['item_condition_id'])\n", "plt.ylabel('item_condition_id')\n", "plt.xlabel('price')"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "fa7958e9-9e64-4f50-857c-4caef83b2ceb", "_uuid": "22802221d02be87053fde7e659d7bffeb77f15c6"}}, {"source": ["Sanity check 2: **Price** is is unlikely to assume values below 0 so the values are likely to be log normal or something similar\n", "\n", "Let's confirm our assumptions about the distribution of the numerical values. Since there are some extreme values we'll also take a look what happens when we clip the bottom and top 1%. The plots confirm that our assumption of log-normality of price is decent albeit not perfect. We'll keep a log-price column as it may come in handy if we later want to use methods that assume normality."], "cell_type": "markdown", "metadata": {"_cell_guid": "6bdb76a0-1fe9-43e4-8ab0-b24cab9cac1b", "_uuid": "354504f79e05786b7f248b79ba839fc98ea06aa1"}}, {"source": ["import numpy as np\n", "plt.figure()\n", "sns.distplot(train_df['price'])\n", "plt.title('All raw prices')\n", "\n", "plt.figure()\n", "s = train_df['price'];\n", "s = s[s > np.percentile(s,1)]\n", "s = s[s < np.percentile(s,99)]\n", "sns.distplot(s)\n", "plt.title('Clipped raw prices')\n", "\n", "plt.figure()\n", "s = train_df['price'];\n", "#s = s[s > 0]\n", "sns.distplot(np.log(s+1))\n", "plt.title('log prices')\n", "\n", "train_df['log_price'] = np.log(train_df['price']+1)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "246565ea-8647-43c7-ab97-508c03958b0e", "_uuid": "64bac64820b384116da192750d240664fe6f2c37"}}, {"source": ["Sanity check 3: Common sense: **brand_name** and **category_name** will show strong biases towards a few select values\n", "\n", "We start by taking a look at the distribution of the counts for each **brand_name** and **category_name**. Since we are already assuming that the distributions are dominated by a few values, we'll opt for pie charts. As expected, the majority of data points fall into a countable number of top **brand_name**s and **categroy_name**s. We keep this in mind as a grouping of 'lesser' brands into one unified 'other' brand might be helpful. A potentially useful detail to keep in mind is that **brand_name** is actually missing for almost half the data."], "cell_type": "markdown", "metadata": {"_cell_guid": "34091d44-7558-4949-b0b6-82a2c0045a29", "_uuid": "d6e6710837fd5de3374d3230b83be98ec04fa5e3"}}, {"source": ["plt.figure()\n", "plt.title('brand name')\n", "grp_brand = train_df.groupby('brand_name')\n", "grp_brand.count()['train_id'].sort_values().plot(kind='pie',labels=None)\n", "\n", "plt.figure()\n", "plt.title('category_name')\n", "grp_cat = train_df.groupby('category_name')\n", "grp_cat.count()['train_id'].sort_values().plot(kind='pie',labels=None)\n", "\n", "print('Top 10 brands:')\n", "print(grp_brand.count()['train_id'].sort_values(ascending=False).head(10))\n", "\n", "print('\\nTop 10 categories:')\n", "print(grp_cat.count()['train_id'].sort_values(ascending=False).head(10))"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0a735494-0b78-48e9-a158-bd2e715791fc", "_uuid": "200c74670f181b433566e8c4939139ed96182808"}}, {"source": ["Sanity check 4: Common sense: Listings that include the shipping price are likely to be more expensive\n", "\n", "Both the mean and median value are higher for **shipping**=0. We can safely assume that **shipping**=0 indicates that shipping costs are included in the item cost. The difference in the median/mean lets us also roughly estimate that the average shipping cost lies somewhere around 3-5 currency units (CU, whatever they may be). While this point may seem trivial, in a market where most items cost around 20 CU, the question of included or exclulded shipping cost has a profound impact on the price fairness."], "cell_type": "markdown", "metadata": {"_cell_guid": "b322bf45-25e8-490d-b6db-099d59ea8b6c", "_uuid": "08e6b4337c1e3b0eb50d332096fa72ec3c8b2559"}}, {"source": ["grp_ship = train_df.groupby('shipping')\n", "\n", "plt.figure()\n", "plt.title('Median price grouped by shipping')\n", "grp_ship['price'].median().plot.bar()\n", "\n", "plt.figure()\n", "plt.title('Mean price grouped by shipping')\n", "grp_ship['price'].mean().plot.bar()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "4b631d8a-73a0-461d-ad55-f863c151cea8", "_uuid": "6f247c6808e7a1a9f2f44e7ad6a6a1508daf6b6d"}}, {"source": ["What's next? Revisiting **category_name**\n", "\n", "Earlier, we have notcied that **category_name** comes with different levels. Essentially this means that **category_name**s represent the leaf nodes of a tree. However, since these inputs are provided by human users, it is possible that some people go to different depths for the same product, e.g. men/apparell/shoes (3 levels) might be as valid as men.apparell/shoes/winter (4 levels). The current category_name column has over 1000 values but the data may be better represented by multiple columns with (hopefully) 10-20 categories (where each column represents a certain tree depth). We start by figuring out the depth.\n", "\n", "Interestingly, it turns out that almost all **categorie_name**s have a depth of 3 (2 delimiters = 3 levels) while the maximum level is 5."], "cell_type": "markdown", "metadata": {"_cell_guid": "0f473d28-cfab-43a6-862c-b7fc421c96e4", "_uuid": "9dfe53f0c6c9b39df0004f93f61a9186e5ef0d33"}}, {"source": ["train_df['category_depth'] = train_df['category_name'].map(lambda s: s.count('/'))\n", "\n", "grp_depth = train_df.groupby('category_depth')\n", "grp_depth['train_id'].count()\n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "867d8fd1-bec1-4130-a946-a9da954c769d", "_uuid": "9581d2643d598edface288fb44aa81176cb2ed61"}}, {"source": ["Next, we determine how many unique values we have in each level. Unfortuntely, it turns out that we still have a  rather large number of unique values, especially in the 2nd level. Nevertheless, we use the groupings to get some intuitions about the overall price distribution. Unsurprisingly, homemade products fetch the lowest prices while electronics come out at the top.****"], "cell_type": "markdown", "metadata": {"_cell_guid": "61cc3fbe-93c9-4616-a5d3-ae6a820d22d6", "_uuid": "7c05c1705d033cf447511e3bb07b2cac38972c96"}}, {"source": ["train_df.category_name[0].split('/')\n", "\n", "def get_level_value(cn, target_depth):\n", "    components = cn.split('/')\n", "    n_components = len(components)\n", "    if target_depth < n_components:\n", "        return components[target_depth]\n", "    else:\n", "        return np.nan\n", "        \n", "    \n", "\n", "print('Number of unique values')\n", "for cat_depth in range(0, train_df['category_depth'].max()+1):\n", "    col_name = 'cat_lvl' + str(cat_depth)\n", "    train_df[col_name] = train_df['category_name'].map(lambda s: get_level_value(s,cat_depth))\n", "    test_df[col_name] = test_df['category_name'].map(lambda s: get_level_value(s,cat_depth))\n", "    print(col_name + ': '+ str(train_df[col_name].nunique()))\n", "\n", "train_df.iloc[:,[3, 10, 11, 12, 13, 14]].head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "34420ba3-5797-4f13-bfa7-ba4764b812c4", "_uuid": "9e1c3accf069b319162e0297dc1b7daf67b7bfd2"}}, {"source": ["Before we prcoeed, we well consolidate cat_lvl2, 3, 4 into one level as the last 2 columns have very few values anyways. We expect that the increase in the number of unique values in **cat_lvl2+** will be marginal over **cat_lvl2**. Actually, it turns out that there is no increase at all. We'll also convert **category_name** and the **cat_lvls** into dummies."], "cell_type": "markdown", "metadata": {"_cell_guid": "6a5d1003-f6a1-403f-a728-f123021d0cb8", "_uuid": "b2c4319a0743be3b5c57737aa11ca169d3b86ec3"}}, {"source": ["train_df['cat_lvl2+'] = (train_df['cat_lvl2'] \n", "                         + '/' + train_df['cat_lvl3'].fillna('')\n", "                         + '/' + train_df['cat_lvl3'].fillna(''))\n", "\n", "test_df['cat_lvl2+'] = (test_df['cat_lvl2'] \n", "                         + '/' + test_df['cat_lvl3'].fillna('')\n", "                         + '/' + test_df['cat_lvl3'].fillna(''))\n", "\n", "display(train_df.loc[train_df['cat_lvl4'].notnull(),['category_name', 'cat_lvl2','cat_lvl2+']].head())\n", "print('#unique values in cat_lvl2: ' +  str(len(train_df['cat_lvl2'].unique())))\n", "print('#unique values in cat_lvl2+: ' +  str(len(train_df['cat_lvl2+'].unique())))"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5443cdad-f901-421d-a2ae-855749390ca5", "_uuid": "784a08ff0cb3f24320f6e9f24896337b8d1591d8"}}, {"source": ["from sklearn import preprocessing\n", "le = preprocessing.LabelEncoder()\n", "to_encode = ['category_name', 'cat_lvl0', 'cat_lvl1', 'cat_lvl2', 'cat_lvl2+', 'cat_lvl3', 'cat_lvl4']\n", "\n", "\n", "for col_name in to_encode:\n", "    le.fit(list(train_df[col_name].fillna('')) + list(test_df[col_name].fillna('')))\n", "    train_df[col_name + '_ENC'] = le.transform(train_df[col_name].fillna(''))\n", "    test_df[col_name + '_ENC'] = le.transform(test_df[col_name].fillna(''))\n", "    #train_df[[col_name, col_name + '_ENC']].head()\n", "\n", "    \n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "084a4874-b47c-406b-8464-0b3add16fedf", "_uuid": "95606607061ad82f00c496f6ef2a846f31b0f772", "collapsed": true}}, {"source": ["It is somewhat questionable if our last exercise has added any value-> tsne"], "cell_type": "markdown", "metadata": {"_cell_guid": "ec545066-fbfb-4c16-b5ac-1f922301c7da", "_uuid": "18afc4d8bd0b70725465b711fb1fce3d11295495"}}, {"source": ["# from sklearn.manifold import TSNE\n", "# from sklearn.preprocessing import StandardScaler\n", "\n", "# n_samples = 10000\n", "\n", "# def perform_TSNE_and_map(df):\n", "#     df_no_price = df_sample.drop(['price','log_price'],axis=1)\n", "#     scaled_data = StandardScaler().fit_transform(df_no_price)\n", "#     embedded_data = TSNE(n_components = 2).fit_transform(scaled_data)\n", "\n", "#     plt.figure(figsize = [15, 5])\n", "#     plt.subplot(1,3,1)\n", "#     plt.scatter(embedded_data[:,0],embedded_data[:,1],c = df['shipping'], s= 1)\n", "#     plt.title('shipping')\n", "\n", "#     plt.subplot(1,3,2)\n", "#     plt.scatter(embedded_data[:,0],embedded_data[:,1],c = df['item_condition_id'], s= 1)\n", "#     plt.title('item_condition_id')\n", "\n", "#     plt.subplot(1,3,3)\n", "#     plt.scatter(embedded_data[:,0],embedded_data[:,1],c = df['log_price'], s=1)\n", "#     plt.title('log_price')\n", "\n", "# # df_sample = train_df[['item_condition_id', 'price', 'log_price', 'shipping', 'category_name_ENC']].sample(n_samples)\n", "# # perform_TSNE_and_map(df_sample)\n", "\n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "56f83a80-568d-49fe-9108-3004d3e56bda", "_uuid": "ad182a9eb13869562b0376512ca0512cd36b793d", "collapsed": true}}, {"source": ["# n_samples = 10000\n", "# df_sample = train_df[['item_condition_id', 'price', 'log_price', 'shipping', \n", "#                       'cat_lvl0_ENC','cat_lvl1_ENC']].sample(n_samples)\n", "# perform_TSNE_and_map(df_sample)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1b402d46-6d02-4235-963a-a691d4257d5d", "_uuid": "a11f9aad8d8176b308774d1d81d2dd4e0d6a8c68", "collapsed": true}}, {"source": ["train_df.head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5c25267b-4d8d-4c9a-8fb8-beb9cf6ff653", "_uuid": "186725a4db1a6f71a765202c5f04a3c3503c81b1"}}, {"source": ["dummy encoding brands"], "cell_type": "markdown", "metadata": {"_cell_guid": "656e8e71-2ff3-46cb-a605-f4d72818e788", "_uuid": "df0d1ab6918bace345afba635b440f7e98a177e5"}}, {"source": ["plt.figure()\n", "grp_brand.count()['price'].sort_values().cumsum().plot()\n", "\n", "plt.figure()\n", "grp_brand.mean()['price'].sort_values().cumsum().plot()\n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "99698eaa-0477-4a34-95bf-3bdd63603e14", "_uuid": "c7ff1df3cd25194d8252be2b264ec5b307585acc"}}, {"source": ["more engineering"], "cell_type": "markdown", "metadata": {"_cell_guid": "2adbe8a3-70e0-440d-92c1-e96cf43e4b5a", "_uuid": "d1f9178a0d4f85a2e14d72309d7e49c02b022f11"}}, {"source": ["# nlp stuff\n", "train_df.item_description.fillna('',inplace=True)\n", "train_df['description_length'] = train_df['item_description'].map(len)\n", "                         \n", "test_df.item_description.fillna('',inplace=True)\n", "test_df['description_length'] = test_df['item_description'].map(len)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0b7b8203-0f56-4441-86f0-7866d74d6390", "_uuid": "844d54502753269b2a3ea5582435f3cee1cdc554", "collapsed": true}}, {"source": ["# testing dummy encoding\n", "train_df = pd.get_dummies(train_df,columns=['cat_lvl0_ENC'])\n", "test_df = pd.get_dummies(test_df,columns=['cat_lvl0_ENC'])"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "9f7686d9-3d93-4799-a7c1-0f879ab6691f", "_uuid": "cbabaf6e84d0d557f45058149c65e79bb06b7a60", "collapsed": true}}, {"source": ["#brand stuff\n", "train_df['is_brand'] = ~(train_df['brand_name'] == '$not available$')\n", "test_df['is_brand'] = ~(test_df['brand_name'] == '$not available$')"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ec1cd904-52ed-49c6-9977-05cbd800bdae", "_uuid": "7b8248ab66082174b726a36de80f949932ce5531", "collapsed": true}}, {"source": ["params for xgb taken from https://www.kaggle.com/maheshdadhich/i-will-sell-everything-for-free-0-55"], "cell_type": "markdown", "metadata": {"_cell_guid": "b735ddf6-1aad-42ac-88bc-65f20f7d0d77", "_uuid": "91427ca298ea05e744c6585590f403b269b884b1"}}, {"source": ["train_df.head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "84fa8344-0648-43a2-9c30-83e4eded7b76", "_uuid": "1809b51e4362edeb4fa4b966db7a23f08e2231bd"}}, {"source": ["features_to_drop = ['train_id', 'name', 'category_name', 'brand_name',\n", "       'price', 'item_description', 'log_price', 'category_depth',\n", "       'cat_lvl0', 'cat_lvl1', 'cat_lvl2', 'cat_lvl3', 'cat_lvl4', 'cat_lvl2+',\n", "       'category_name_ENC', 'cat_lvl2_ENC', \n", "       'cat_lvl3_ENC', 'cat_lvl4_ENC']\n", "\n", "feature_names = np.setdiff1d(train_df.columns,features_to_drop)\n", "feature_names\n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "fb177b0f-4f87-4974-8e47-1a0e9b75986d", "_uuid": "f68b955fdad7f565c0d5077228aa5733135828b6"}}, {"source": ["\n", "\n", "train=train_df.copy()\n", "test=test_df.copy()\n", "\n", "mm_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n", "y = mm_scaler.fit_transform(train['log_price'].reshape(-1,1))\n", "y= train['log_price']\n", "\n", "# XGBoost\n", "import xgboost as xgb\n", "\n", "xgb_par = {'min_child_weight': 20, 'eta': 0.05, 'colsample_bytree': 0.5, 'max_depth': 15,\n", "            'subsample': 0.9, 'lambda': 2.0, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n", "            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n", "\n", "\n", "# light LGB (adapted from 'https://www.kaggle.com/infinitewing/lightgbm-example')\n", "import lightgbm as lgb\n", "\n", "lgb_par = {'n_estimators': 900, 'learning_rate': 0.15, 'max_depth': 9, 'application': 'regression',\n", "               'num_leaves': 256, 'subsample': 0.9, 'colsample_bytree': 0.8,\n", "               'min_child_samples': 50, 'n_jobs': 3, 'metric':'RMSE'}\n", "\n", "# cross validation\n", "from sklearn.model_selection import train_test_split\n", "Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=520)\n", "\n", "dtrain_xgb = xgb.DMatrix(Xtr, label=ytr)\n", "dvalid_xgb = xgb.DMatrix(Xv, label=yv)\n", "dtest_xgb = xgb.DMatrix(test[feature_names].values)\n", "watchlist = [(dtrain_xgb, 'train'), (dvalid_xgb, 'valid')]\n", "model_xgb = xgb.train(xgb_par, dtrain_xgb, 80, watchlist, early_stopping_rounds=30, maximize=False, verbose_eval=20)\n", "# print('Modeling RMSLE by XGB %.5f' % model_xgb.best_score)\n", "\n", "# lightGBM\n", "dtrain_lgb = lgb.Dataset(Xtr, label=ytr, max_bin=8192)\n", "dvalid_lgb = lgb.Dataset(Xv, label=yv, max_bin=8192)\n", "dtest_lgb = lgb.Dataset(test[feature_names].values)\n", "watchlist = [dtrain_lgb, dvalid_lgb]\n", "model_lgb = lgb.train(lgb_par, train_set=dtrain_lgb, num_boost_round=200, valid_sets=watchlist, \\\n", "    early_stopping_rounds=30, verbose_eval=20) "], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "d67dc745-14df-4e25-bb1d-4b04b942efe7", "_uuid": "49352b035ba0f1628db1d180e77e924c3ef2e90c", "scrolled": false}}, {"source": ["# feature_names = ['item_condition_id', 'shipping', \n", "#                  'cat_lvl0_ENC','cat_lvl1_ENC','cat_lvl2+_ENC']\n", "\n", "# train=train_df.copy()\n", "# test=test_df.copy()\n", "# y = train['log_price']\n", "\n", "# from sklearn.model_selection import train_test_split\n", "# import xgboost as xgb\n", "# Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=520)\n", "# dtrain = xgb.DMatrix(Xtr, label=ytr)\n", "# dvalid = xgb.DMatrix(Xv, label=yv)\n", "# dtest = xgb.DMatrix(test[feature_names].values)\n", "# watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n", "\n", "# xgb_par = {'min_child_weight': 20, 'eta': 0.05, 'colsample_bytree': 0.5, 'max_depth': 15,\n", "#             'subsample': 0.9, 'lambda': 2.0, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n", "#             'eval_metric': 'rmse', 'objective': 'reg:linear'}\n", "\n", "# model_1 = xgb.train(xgb_par, dtrain, 80, watchlist, early_stopping_rounds=20, maximize=False, verbose_eval=20)\n", "# print('Modeling RMSLE %.5f' % model_1.best_score)\n", "\n", "# feature_names = ['item_condition_id', 'shipping', \n", "#                  'category_name_ENC']\n", "\n", "# train=train_df.copy()\n", "# test=test_df.copy()\n", "# y = train['log_price']\n", "\n", "# from sklearn.model_selection import train_test_split\n", "# import xgboost as xgb\n", "# Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=520)\n", "# dtrain = xgb.DMatrix(Xtr, label=ytr)\n", "# dvalid = xgb.DMatrix(Xv, label=yv)\n", "# dtest = xgb.DMatrix(test[feature_names].values)\n", "# watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n", "\n", "# xgb_par = {'min_child_weight': 20, 'eta': 0.05, 'colsample_bytree': 0.5, 'max_depth': 15,\n", "#             'subsample': 0.9, 'lambda': 2.0, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n", "#             'eval_metric': 'rmse', 'objective': 'reg:linear'}\n", "\n", "# model_1 = xgb.train(xgb_par, dtrain, 80, watchlist, early_stopping_rounds=20, maximize=False, verbose_eval=20)\n", "# print('Modeling RMSLE %.5f' % model_1.best_score)\n"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"scrolled": true, "_cell_guid": "ae2ae32e-f782-492f-8cb6-c1f386dd6b89", "_uuid": "c12b3c9163e273eb3e5776eb0f7bcc8dc6d9f965", "collapsed": true}}, {"source": ["# prediction\n", "yvalid = 0.6*model_xgb.predict(dvalid_xgb) + 0.4*model_lgb.predict(Xv)\n", "ytest = 0.6*model_xgb.predict(dtest_xgb) + 0.4*model_lgb.predict(test[feature_names].values)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"scrolled": true, "_cell_guid": "e5556d6c-56e5-4ee5-a496-f0cbf2d3c4de", "_uuid": "c6db50f7d63bc82434e5e44d5e88ea5986e50c87", "collapsed": true}}, {"source": ["fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n", "sns.distplot(yvalid, ax=ax[0], color='blue', label='Validation')\n", "sns.distplot(ytest, ax=ax[1], color='green', label='Test')\n", "ax[0].legend(loc=0)\n", "ax[1].legend(loc=0)\n", "plt.show()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "fb47b8be-8f16-42b4-a3a0-e0178389a813", "_uuid": "e377a102f1e7eaefb22f981139287414f4294328"}}, {"source": ["# submission\n", "if test.shape[0] == ytest.shape[0]:\n", "    print('Test shape OK.') \n", "test['price'] = np.exp(ytest) - 1\n", "test[['test_id', 'price']].to_csv('mercari_prototype2.csv', index=False)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "21b586d8-9446-4771-833c-2367d6a95aa9", "_uuid": "637c375b82b47c13266861fba85b9d4f5faa91da"}}, {"source": ["from sklearn.feature_extraction.text import TfidfVectorizer\n", "text_data = train_df.item_description\n", "\n", "tfidf = TfidfVectorizer()\n", "tfidf.fit(text_data) #TODO add test data\n", "transformed = tfidf.transform(text_data)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "90c259fd-e130-460b-89cb-d57f564e245d", "_uuid": "a7cbaf6a940d6d60a455d534f90925b1b44e822b", "collapsed": true}}, {"source": ["print(transformed[1:3,:])"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "33e56b0b-ce74-429b-8c46-4f32b81e641b", "_uuid": "b8cc0a342b4a73591e622f3e49d1c203481864d8"}}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "version": "3.6.3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1}