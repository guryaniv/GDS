{"cells":[{"metadata":{"_uuid":"cda540a26ae5924ae74dbaf88ea931dbc11d61d0"},"cell_type":"markdown","source":"# Bayesian Optimization with XGBoost"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b3f1ece196c81d4932e672f984724a21be12947f"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"664fd6cfdc243b36e381f0a2a8df26723399f193"},"cell_type":"markdown","source":"## Read Data\nUse all data for a better score. However i've not been able to properly configure the K80 gpu available on kaggle to work with xgboost, so I've had to severly limit both the amount of data, and size of model.\n\nThe data appears to be randomized, so reading in the beginning rows is acceptable.\n\nUsing the entire dataset will use around 32gb of memory throughout this notebook, So primarily for this reason I achieved first place on the leaderboard as of July 31,2018 using an AWS EC2 p3.2xlarge instance."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f291b361888117692c878324f79336cc97f887d8"},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv',nrows=2_000_000, usecols=[1,2,3,4,5,6,7])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6331eaf519d93bb329417a39801ee30b4dc717b8"},"cell_type":"markdown","source":"Slicing off unecessary components of the datetime and specifying the date format results in a MUCH more efficiecnt conversion to a datetime object."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"75adeacb8a54ffa5bfa6b900ed9874496d085da8"},"cell_type":"code","source":"df['pickup_datetime'] = df['pickup_datetime'].str.slice(0, 16)\ndf['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"221ae597a851dd2f83965faf2929e0f8751a21c9"},"cell_type":"markdown","source":"## Clean"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"09eb6e85145905ef6960a71033a1355e920db732"},"cell_type":"code","source":"# Remove observations with missing values\n# Since there are only a few of these, i'm not concerned with imputation\ndf.dropna(how='any', axis='rows', inplace=True)\n\n# Removing observations with erroneous values\nmask = df['pickup_longitude'].between(-75, -73)\nmask &= df['dropoff_longitude'].between(-75, -73)\nmask &= df['pickup_latitude'].between(40, 42)\nmask &= df['dropoff_latitude'].between(40, 42)\nmask &= df['passenger_count'].between(0, 8)\nmask &= df['fare_amount'].between(0, 250)\n\ndf = df[mask]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e942cb62da136628d643f992ba2a0aabc88b2010"},"cell_type":"markdown","source":"## Feature Engineering\nManhattan distance provides a better approximation of actual travelled distance than haversine for most trips."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4617497b6b64c20f4f29361fcaccb46663ba105d"},"cell_type":"code","source":"def dist(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  \n    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n    \n    return distance","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a68459806b2649cc5c4c2e19c64536bdee619207"},"cell_type":"markdown","source":"See __[NYC Taxi Fare - Data Exploration](https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration)__ for an excellent EDA on this dataset and the intuition for including airports."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a81eb1b903e65143e9c5af2670235d1e03b09093"},"cell_type":"code","source":"def transform(data):\n    # Extract date attributes and then drop the pickup_datetime column\n    data['hour'] = data['pickup_datetime'].dt.hour\n    data['day'] = data['pickup_datetime'].dt.day\n    data['month'] = data['pickup_datetime'].dt.month\n    data['year'] = data['pickup_datetime'].dt.year\n    data = data.drop('pickup_datetime', axis=1)\n\n    # Distances to nearby airports, and city center\n    # By reporting distances to these points, the model can somewhat triangulate other locations of interest\n    nyc = (-74.0063889, 40.7141667)\n    jfk = (-73.7822222222, 40.6441666667)\n    ewr = (-74.175, 40.69)\n    lgr = (-73.87, 40.77)\n    data['distance_to_center'] = dist(nyc[1], nyc[0],\n                                      data['pickup_latitude'], data['pickup_longitude'])\n    data['pickup_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                         data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    data['pickup_distance_to_ewr'] = dist(ewr[1], ewr[0], \n                                          data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_ewr'] = dist(ewr[1], ewr[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    data['pickup_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                          data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    data['long_dist'] = data['pickup_longitude'] - data['dropoff_longitude']\n    data['lat_dist'] = data['pickup_latitude'] - data['dropoff_latitude']\n    \n    data['dist'] = dist(data['pickup_latitude'], data['pickup_longitude'],\n                        data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    return data\n\n\ndf = transform(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de86eebde9923ffa8d337a0ea5903737e7cdf81a"},"cell_type":"markdown","source":"## Train/Test split"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cf0263609a54a60286ab980786720b278cba0548"},"cell_type":"code","source":"import xgboost as xgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f45e051982e31a057a2945a7e2f7381140bb0484"},"cell_type":"markdown","source":"Being careful about memory management, which is critical when running the entire dataset."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5da11d8c8a2fbbc0d18496c8b9500e2a2f0dd1ac"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop('fare_amount', axis=1),\n                                                    df['fare_amount'], test_size=0.25)\ndel(df)\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndel(X_train)\ndtest = xgb.DMatrix(X_test)\ndel(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dabc51b1618a4dabf440a2c108826078e0f4ba5"},"cell_type":"markdown","source":"## Training\nOptimizing hyperparameters with bayesian optimization. I've tried to limit the scope of the search as much\nas possible since the search space grows exponentially when considering aditional hyperparameters.\n\nGPU acceleration with a few pre tuned hyperparameters speeds up the search a lot."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"911e7524fc9ced3ce909f77da5b1fbf251b7f910"},"cell_type":"code","source":"def xgb_evaluate(max_depth, gamma, colsample_bytree):\n    params = {'eval_metric': 'rmse',\n              'max_depth': int(max_depth),\n              'subsample': 0.8,\n              'eta': 0.1,\n              'gamma': gamma,\n              'colsample_bytree': colsample_bytree}\n    # Used around 1000 boosting rounds in the full model\n    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n    \n    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f2a8ff48bed0f13e05687d2ec694fb5f1f2bcd81","collapsed":true},"cell_type":"code","source":"xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 7), \n                                             'gamma': (0, 1),\n                                             'colsample_bytree': (0.3, 0.9)})\n# Use the expected improvement acquisition function to handle negative numbers\n# Optimally needs quite a few more initiation points and number of iterations\nxgb_bo.maximize(init_points=3, n_iter=5, acq='ei')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1a4ea8f4c4f033c6c31f0a4001be503a535767c"},"cell_type":"markdown","source":"Extract the parameters of the best model."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"5bcdd240ddb16252e8f1504184bff0ab909c2507"},"cell_type":"code","source":"params = xgb_bo.res['max']['max_params']\nparams['max_depth'] = int(params['max_depth'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13442121790f66bc29cd6f1bbb72f3a3195a399d"},"cell_type":"markdown","source":"## Testing"},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"8ac2492334bafc4e5ce14ae4a3551410364734c5"},"cell_type":"code","source":"# Train a new model with the best parameters from the search\nmodel2 = xgb.train(params, dtrain, num_boost_round=250)\n\n# Predict on testing and training set\ny_pred = model2.predict(dtest)\ny_train_pred = model2.predict(dtrain)\n\n# Report testing and training RMSE\nprint(np.sqrt(mean_squared_error(y_test, y_pred)))\nprint(np.sqrt(mean_squared_error(y_train, y_train_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc64e637343735df98c269fc90724afbf23bb91d"},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"scrolled":false,"trusted":false,"collapsed":true,"_uuid":"7cf00b45f4ed10d0bd9228815026ef79201f75fa"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfscores = pd.DataFrame({'X': list(model2.get_fscore().keys()), 'Y': list(model2.get_fscore().values())})\nfscores.sort_values(by='Y').plot.bar(x='X')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"461627feced782418ab2c66b195edb08bf48735b"},"cell_type":"markdown","source":"## Predict on Holdout Set"},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"00e848082e3aa325b0677bf39b4194f5fc221377"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv').set_index('key')\ntest['pickup_datetime'] = test['pickup_datetime'].str.slice(0, 16)\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n\n# Predict on holdout set\ntest = transform(test)\ndtest = xgb.DMatrix(test)\ny_pred_test = model2.predict(dtest)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"725915fef9caac7c0802f64ef0020bdc4baf9dbf"},"cell_type":"markdown","source":"## Submit predictions"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e2f8eef8b3b75620ca617f49e7b640be8d2722f7"},"cell_type":"code","source":"holdout = pd.DataFrame({'key': test.index, 'fare_amount': y_pred_test})\nholdout.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}