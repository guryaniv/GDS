{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"Machine learning models are often considered as black boxes - we set (or guess?) some parameters, give some input data and receive predictions. But in reality those boxes are mostly not completely black. \nPurpose of this notebook is to show some possibilities to view inside the black box and see little bit of what's inside...\n\nThe main questions addressed here:\n* **Which features** are the most important in general and which contributed to some particular prediction most?\n* **How** are features contributing to some particular predictions (lowering, pushing up)?\n* **Why** some particular prediction has the value it has?\n\nLet's take as an example this great kernel with currently highest public LB score: https://www.kaggle.com/poohtls/fork-of-fork-lightgbm-with-simple-features\n(Kudos to it's author,  2 kudos to the kernel's author which that kernel was forked from, 4 kudos to the kernel's author even below, etc).\n\nThat kernel already has feature importances plot, which gives us some basic impression about, how model is transforming features into predictions. \nBut it doesn't provide any possibilities to say something about some individual predictions. Let's try to obtain some intuition for this also.\n\nFirst we run model to get trained LightGBM Booster...\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nsubmission_file_name = \"submission.csv\"\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df, nan_as_category = True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\n# Preprocess application_train.csv and application_test.csv\ndef application_train_test(num_rows = None, nan_as_category = False):\n    # Read data and merge\n    df = pd.read_csv('../input/application_train.csv', nrows= num_rows)\n    #test_df = pd.read_csv('../input/application_test.csv', nrows= num_rows)\n    print(\"Train samples: {}\".format(len(df)))\n    #df = df.append(test_df).reset_index()\n    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n    df = df[df['CODE_GENDER'] != 'XNA']\n    \n    docs = [_f for _f in df.columns if 'FLAG_DOC' in _f]\n    live = [_f for _f in df.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n    \n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n\n    inc_by_org = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n\n    df['NEW_CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n    df['NEW_CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n    df['NEW_DOC_IND_KURT'] = df[docs].kurtosis(axis=1)\n    df['NEW_LIVE_IND_SUM'] = df[live].sum(axis=1)\n    df['NEW_INC_PER_CHLD'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n    df['NEW_INC_BY_ORG'] = df['ORGANIZATION_TYPE'].map(inc_by_org)\n    df['NEW_EMPLOY_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    df['NEW_ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])\n    df['NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n    df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n    df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n    df['NEW_CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n    df['NEW_CAR_TO_EMPLOY_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n    df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n    df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n    df['NEW_CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n    \n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    # Categorical features with One-Hot encode\n    df, cat_cols = one_hot_encoder(df, nan_as_category)\n    \n    # Some simple new features (percentages)\n    # df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    # df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n    # df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n    # df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n    # df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n    return df\n\n# Preprocess bureau.csv and bureau_balance.csv\ndef bureau_and_balance(num_rows = None, nan_as_category = True):\n    bureau = pd.read_csv('../input/bureau.csv', nrows = num_rows)\n    bb = pd.read_csv('../input/bureau_balance.csv', nrows = num_rows)\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n    \n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n    del bb, bb_agg\n    gc.collect()\n    \n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': [ 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': [ 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM': [ 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': [ 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n    }\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    \n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n    del active, active_agg\n    gc.collect()\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n    del closed, closed_agg, bureau\n    gc.collect()\n    return bureau_agg\n\n# Preprocess previous_applications.csv\ndef previous_applications(num_rows = None, nan_as_category = True):\n    prev = pd.read_csv('../input/previous_application.csv', nrows = num_rows)\n    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n    # Days 365.243 values -> nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n    # Add feature: value ask / value received percentage\n    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n    # Previous applications numeric features\n    num_aggregations = {\n        'AMT_ANNUITY': [ 'max', 'mean'],\n        'AMT_APPLICATION': ['min', 'mean'],\n        'AMT_CREDIT': ['min', 'max', 'mean'],\n        'APP_CREDIT_PERC': ['min', 'max', 'mean'],\n        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'DAYS_DECISION': ['min', 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n    }\n    # Previous applications categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n    \n    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n    # Previous Applications: Approved Applications - only numerical features\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n    # Previous Applications: Refused Applications - only numerical features\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n    del refused, refused_agg, approved, approved_agg, prev\n    gc.collect()\n    return prev_agg\n\n# Preprocess POS_CASH_balance.csv\ndef pos_cash(num_rows = None, nan_as_category = True):\n    pos = pd.read_csv('../input/POS_CASH_balance.csv', nrows = num_rows)\n    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n    # Features\n    aggregations = {\n        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n        'SK_DPD': ['max', 'mean'],\n        'SK_DPD_DEF': ['max', 'mean']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    \n    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n    del pos\n    gc.collect()\n    return pos_agg\n    \n# Preprocess installments_payments.csv\ndef installments_payments(num_rows = None, nan_as_category = True):\n    ins = pd.read_csv('../input/installments_payments.csv', nrows = num_rows)\n    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n    # Percentage and difference paid in each installment (amount paid and installment value)\n    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n    # Days past due and days before due (no negative values)\n    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n    # Features: Perform aggregations\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum'],\n        'DBD': ['max', 'mean', 'sum'],\n        'PAYMENT_PERC': [ 'mean', 'sum', 'var'],\n        'PAYMENT_DIFF': [ 'mean', 'sum', 'var'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n    del ins\n    gc.collect()\n    return ins_agg\n\n# Preprocess credit_card_balance.csv\ndef credit_card_balance(num_rows = None, nan_as_category = True):\n    cc = pd.read_csv('../input/credit_card_balance.csv', nrows = num_rows)\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n    # General aggregations\n    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n    del cc\n    gc.collect()\n    return cc_agg\n\n# LightGBM GBDT with KFold or Stratified KFold\n# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\ndef kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n    # Divide in training/validation and test data\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    del df\n    gc.collect()\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    \n    best_rounds = 0\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = lgb.LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=32,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.04,\n            reg_lambda=0.073,\n            min_split_gain=0.0222415,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1, )\n\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric= 'auc', verbose= 100, early_stopping_rounds= 200)\n\n        oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n        best_rounds += clf.best_iteration_\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n    best_rounds = int(best_rounds / num_folds)\n    # Write submission file and plot feature importance\n    if not debug:\n        test_df['TARGET'] = sub_preds\n        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n    return feature_importance_df\n\ndef train_lightgbm(df, num_boost_round=100, debug= False):\n    # Divide in training/validation and test data\n    train_df = df[df['TARGET'].notnull()]\n    print(\"Starting LightGBM. Train shape: {}\".format(train_df.shape))\n    del df\n    gc.collect()\n\n    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    params = {\n        'objective':'binary',\n        'metric':'auc',\n        'nthread':4,\n        'learning_rate':0.02,\n        'num_leaves':32,\n        'colsample_bytree':0.9497036,\n        'subsample':0.8715623,\n        'max_depth':8,\n        'reg_alpha':0.04,\n        'reg_lambda':0.073,\n        'min_split_gain':0.0222415,\n        'min_child_weight':39.3259775,\n        'verbose':-1\n    }\n    \n    train_x = lgb.Dataset(train_df[feats], train_df['TARGET'], silent=True)\n    clf = lgb.train(params, train_x, num_boost_round)\n    return clf, train_df[feats], train_df['TARGET']\n\ndef main(debug = False):\n    num_rows = 10000 if debug else None\n    df = application_train_test(num_rows)\n    with timer(\"Process bureau and bureau_balance\"):\n        bureau = bureau_and_balance(num_rows)\n        print(\"Bureau df shape:\", bureau.shape)\n        df = df.join(bureau, how='left', on='SK_ID_CURR')\n        del bureau\n        gc.collect()\n    with timer(\"Process previous_applications\"):\n        prev = previous_applications(num_rows)\n        print(\"Previous applications df shape:\", prev.shape)\n        df = df.join(prev, how='left', on='SK_ID_CURR')\n        del prev\n        gc.collect()\n    with timer(\"Process POS-CASH balance\"):\n        pos = pos_cash(num_rows)\n        print(\"Pos-cash balance df shape:\", pos.shape)\n        df = df.join(pos, how='left', on='SK_ID_CURR')\n        del pos\n        gc.collect()\n    with timer(\"Process installments payments\"):\n        ins = installments_payments(num_rows)\n        print(\"Installments payments df shape:\", ins.shape)\n        df = df.join(ins, how='left', on='SK_ID_CURR')\n        del ins\n        gc.collect()\n    with timer(\"Process credit card balance\"):\n        cc = credit_card_balance(num_rows)\n        print(\"Credit card balance df shape:\", cc.shape)\n        df = df.join(cc, how='left', on='SK_ID_CURR')\n        del cc\n        gc.collect()\n    with timer(\"Run LightGBM\"):\n        clf, train_df, train_y = train_lightgbm(df, num_boost_round= 1300, debug= debug)\n    return clf, train_df, train_y\n\nwith timer(\"Full model run\"):\n    clf, train_df, train_y = main(True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"519f29a1ad3ac7e9c23e7e3d1702d37a006097b8"},"cell_type":"markdown","source":"Once we have trained Booster, we can get feature importances. LightGBM provides 2 types of feature impotances:\n* split - numbers of times the feature is used in a model;\n* gain - total gains of splits which use the feature.\n\nLet's look at both of them to see if there are any differences..."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"135eea2e6f5a2fc0eed0043018dd7403abe0a128","collapsed":true},"cell_type":"code","source":"def display_importances(clf, feats, importance_type='split'):\n    feature_importance_df = pd.DataFrame()\n    feature_importance_df[\"feature\"] = list(feats)\n    feature_importance_df[\"importance\"] = clf.feature_importance(importance_type=importance_type)\n\n    cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n    plt.figure(figsize=(12, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features ({})'.format(importance_type))\n    plt.tight_layout()\n    plt.show() #if used in notebook\n    #plt.savefig('lgbm_importances.png') #if used in kernel\ndisplay_importances(clf, train_df.columns, 'split')\ndisplay_importances(clf, train_df.columns, 'gain')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e7fb866bba05d828a5338cc33845cc12d0248b8"},"cell_type":"markdown","source":"What we see is that \"NEW_EXT_SOURCES_MEAN\" feature is dominating in the importance of gain. Probably this feature typically has splits in lowest levels of trees. We can check this assumption by plotting some random trees. Below is plot of one tree splits and yes - \"NEW_EXT_SOURCES_MEAN\" is at the root of the tree. You can try viewing some other randomly chosen trees by changing tree_index parameter, and mostly you'll see something similar.\n\n**Note:** in Notebook plotted tree is unreadable due to limited size of image... to see full graph, do right click on the image and view it from there."},{"metadata":{"trusted":true,"_uuid":"2ab0ac7045b3ce3e9e04d2862008060770eace40","collapsed":true},"cell_type":"code","source":"lgb.plot_tree(clf, tree_index=0, figsize=(800, 48), show_info=['split_gain'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2b1378785dc791a7d21eab18ae34ee9f48f3dec"},"cell_type":"markdown","source":"I guess we already have obtained some high level intuition about how model is using features to make predictions. Now let's go deeper and take a look at some individual predictions. We'll use SHAP library (https://github.com/slundberg/shap) for that because of it's nice graphics.\n\nLet's make force plot of first prediction and see what we can say from it..."},{"metadata":{"trusted":true,"_uuid":"b521d19a16beb20326d1e9ac63c8ceac8d7ff2db","collapsed":true},"cell_type":"code","source":"import shap\nshap.initjs()\nshap_values = shap.TreeExplainer(clf).shap_values(train_df.values)\nprint('Truth:', train_y[0])\nshap.force_plot(shap_values[0,:], train_df.iloc[0,:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af9b626f51d77fb5097dd964bde022c98cd69591"},"cell_type":"markdown","source":"What can we see:\n* Predicted value for this item is high, so it is to be most likely a default (and it is, as we see truth is 1).\n* Main features pushing this prediction closer to 1 (in red color) are \"NEW_EXT_SOURCES_MEAN\", \"EXT_SOURCES_3\" and \"DEF_30_CNT_SOCIAL_CIRCLE\".\n* Main features pushing this prediction closer to 0 (in blue color) are \"DAYS_BIRTH\" and \"LIVINGAREA_MODE\".\n    \nSo basically, from this we could even generate some automatic response to the client, which would tell something like - *\"Even if your age is ok and you live in good area, your external scores are too low and risk to have credit default is too high - so sorry, we can't give you credit this time\"*. \n\nOk, let's plot some more cases..."},{"metadata":{"trusted":true,"_uuid":"d39347488deda02dd0052e0e297a643349519e4d","collapsed":true},"cell_type":"code","source":"print('Truth:', train_y[1])\nshap.force_plot(shap_values[1,:], train_df.iloc[1,:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1faf5422519b0041025aa525855d494c256b1898"},"cell_type":"markdown","source":"Predicted value is low, we can give credit here. So, what could we say to this client? *\"Great, you have good credit/annuity ratio, good total income and higher education, therefore risk of credit default is low enough to receive credit!\"*. \n\nOne more?"},{"metadata":{"trusted":true,"_uuid":"dd3bd32e249a558a6266dd1c68983bbc668f3a10","collapsed":true},"cell_type":"code","source":"print('Truth:', train_y[2])\nshap.force_plot(shap_values[2,:], train_df.iloc[2,:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b7e3a3b570be468b4237b7a7c74c4982763d2ec"},"cell_type":"markdown","source":"*\"You have some problems with employment, but your external scores and annuity to income ratio is good, so we can give you a credit!\"*. \n\nOk, generating responses was fun. Let's look what more can we get out of this library. Let's make force plot for more data..."},{"metadata":{"trusted":true,"_uuid":"7b59bd38da069616f6102a061fab9fc3aafed28d","collapsed":true},"cell_type":"code","source":"shap.force_plot(shap_values[:100,:], train_df.iloc[:100,:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a05dea1e7d935c3032df5532f084152cd0a97ff"},"cell_type":"markdown","source":"Graph is interactive - when moving mouse over it, for that specific point info is shown about what feature values are pushing predictions up, and which - down. Playing with the graph a bit helps to get some impression about how model is using features and to see if there are some strange decisions made by model (e.g. that some ID value is used to push prediction up/down, etc).\n\nMore to come, if I'll have time and motivation ;)"},{"metadata":{"_uuid":"8deba2c641d69bdd372b9aec716363cb3cd1bde1"},"cell_type":"markdown","source":"..."},{"metadata":{"_uuid":"9fdcaa5940318deb2bd8699582314fabf109e0aa"},"cell_type":"markdown","source":"..."},{"metadata":{"_uuid":"6e1969320794df45b268f2eb0aed738036ae8864"},"cell_type":"markdown","source":"..."},{"metadata":{"_uuid":"6a92b43af28abe1f43a02bf47d1af20c4b1e176a"},"cell_type":"markdown","source":"..."},{"metadata":{"_uuid":"5b77f8cac680997bce9a720f29755c31fafe1042"},"cell_type":"markdown","source":"..."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}