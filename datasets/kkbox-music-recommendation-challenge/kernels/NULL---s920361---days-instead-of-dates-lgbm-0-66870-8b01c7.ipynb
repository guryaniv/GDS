{"cells": [{"source": "# Days instead of Dates\nOn this notebook I will show you how I improve the score by replacing date data, and converting it into number of days.\nIn this case specifically I will use ALL the fields on the datasets, but only replace the **expiration_date** and **registration_init_time** fields by calculating the difference in days between the two dates.\n\n$$ expiration\\_date - registration\\_init\\_time = membership\\_days$$\n\n## Acknowledgements:\nThis notebook is primarily based on the grate [TalySacc's Script](https://www.kaggle.com/talysacc/simple-lgbm-starter-with-3-fold-cv-lb-0-65249). The modifications are mainly for make understanding easier. I also tried to comment the code as much as I could.", "cell_type": "markdown", "metadata": {"_uuid": "852024499e3c3fa41427fd92c3ee094ee775e34f", "_cell_guid": "058c5db8-be49-465b-90de-fcf78b698eb3"}}, {"source": "# Settings\nThis cell contains the main settings for the notebook.", "cell_type": "markdown", "metadata": {"_uuid": "b918da766e7772d49de4cafea78ff94fb0f0528c", "_cell_guid": "229a0ae7-b4bf-4b53-be1a-2904ae6ae5b6"}}, {"source": "# This value stores the path where all the input data is stored. \n# This is in case you run the notebook on your local computer.\nINPUT_DATA_PATH = '../input/'", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "e8b1cdf69f2d313b77c5d5543e3473d2b0d65c6c", "collapsed": true, "_cell_guid": "65931667-f2ec-471e-b312-f9e81fb0c43f"}, "outputs": []}, {"source": "#  Libraries Import\nFirst, we will import the required libraries.", "cell_type": "markdown", "metadata": {"_uuid": "2a5411c8c1ccd8f633a873de7c5da2952b6d5e21", "_cell_guid": "9188d75f-f9c6-42f5-9df8-93f5707ef2f0"}}, {"source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# SKLEARN\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\n", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "5a05c399dce033e4fc3d6e4cc46d18e32e14bc22", "collapsed": true, "_cell_guid": "2889c032-26b8-4fcb-9e85-820dec22e389"}, "outputs": []}, {"source": "# Read datasets\nNext we read all the datasets into dataframes.", "cell_type": "markdown", "metadata": {"_uuid": "a1f57ea2e61d298116bb792787a3249f32532a37", "_cell_guid": "1605b555-908f-40c0-a795-9396304a9948"}}, {"source": "\ndf_test = pd.read_csv(INPUT_DATA_PATH + 'test.csv',dtype={'msno' : 'category',\n                                                'source_system_tab' : 'category',\n                                                'source_screen_name' : 'category',\n                                                'source_type' : 'category',\n                                                'song_id' : 'category'})\n\ndf_train = pd.read_csv(INPUT_DATA_PATH + 'train.csv',dtype={'msno' : 'category',\n                                                 'source_system_tab' : 'category',\n                                                  'source_screen_name' : 'category',\n                                                  'source_type' : 'category',\n                                                  'target' : np.uint8,\n                                                  'song_id' : 'category'})\n\ndf_members = pd.read_csv(INPUT_DATA_PATH + 'members.csv',dtype={'city' : 'category',\n                                                      'bd' : np.uint8,\n                                                      'gender' : 'category',\n                                                      'registered_via' : 'category'},\n                                                      parse_dates=['registration_init_time','expiration_date'])\nprint(\"OK\")", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "7bdb77e774e38fa897c919bdb06ce47b1fa9a443", "_cell_guid": "ee1d35e7-34f2-4e81-b263-a5e4e2c53e09"}, "outputs": []}, {"source": "# Convert dates to number of days\n### $ expiration\\_date - registration\\_init\\_time = membership\\_days$\nThis is the part where we will convert the dates into days, by calculating the total membership days.", "cell_type": "markdown", "metadata": {"_uuid": "7c0ab024865d435b3f9ddc079d4b32978b8da9f3", "_cell_guid": "807a1a09-0ceb-410c-8678-4303eaeb8ff0"}}, {"source": "# Convert date to number of days\ndf_members['membership_days'] = (df_members['expiration_date'] - df_members['registration_init_time']).dt.days.astype(int)\n\n# Remove both date fieldsa since we already have the number of days between them\ndf_members = df_members.drop(['registration_init_time','expiration_date'], axis=1)\nprint(\"OK\")", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "4eacd12508233d07a8a278595f677f36e021c409", "_cell_guid": "d677d32e-ffe6-46f0-837d-6f812187a9ca"}, "outputs": []}, {"source": "# Merge Dataframes\nLet's merge all the data into the test and train dataframes", "cell_type": "markdown", "metadata": {"_uuid": "c409c1394305c6838359d126328c4a0d830f09a5", "_cell_guid": "7a4fe679-bb09-4aff-b34b-67bcfb48d5e0"}}, {"source": "## Members\nWe will now merge the test and train dataframes with the members dataframe.", "cell_type": "markdown", "metadata": {"_uuid": "aaeef4c3e87ff961d3b77da263054409d09370f1", "_cell_guid": "c7e3d04b-58e6-413e-a1f3-d73045b2d797"}}, {"source": "# Merge the members dataframe into the test dataframe\ndf_test = pd.merge(left = df_test,right = df_members,how='left',on='msno')\ndf_test.msno = df_test.msno.astype('category')\n\n# Merge the member dataframe into the train dataframe\ndf_train = pd.merge(left = df_train,right = df_members,how='left',on='msno')\ndf_train.msno = df_train.msno.astype('category')\n\n# Release memory\ndel df_members\nprint(\"OK\")", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "d0222f14fb9a32596d640e130e87a2fe83f9a685", "_cell_guid": "8d2cf342-5688-42e9-b697-164c4127e0a2"}, "outputs": []}, {"source": "## Songs\nNow we will merge the songs dataframe into the test and train dataframes.", "cell_type": "markdown", "metadata": {"_uuid": "c73c1d5a1e11305579d9aa74299b7bd08d50eaa5", "collapsed": true, "_cell_guid": "88fb46eb-8cb4-4094-bba0-17f30b158aaa"}}, {"source": "# Load the songs dataframe\ndf_songs = pd.read_csv(INPUT_DATA_PATH + 'songs.csv',dtype={'genre_ids': 'category',\n                                                  'language' : 'category',\n                                                  'artist_name' : 'category',\n                                                  'composer' : 'category',\n                                                  'lyricist' : 'category',\n                                                  'song_id' : 'category'})\n\n# Merge the Test Dataframe with the SONGS dataframe\ndf_test = pd.merge(left = df_test,right = df_songs,how = 'left',on='song_id')\ndf_test.song_length.fillna(200000,inplace=True)\ndf_test.song_length = df_test.song_length.astype(np.uint32)\ndf_test.song_id = df_test.song_id.astype('category')\n\n# Merge the Train dataframe with the SONGS dataframe\ndf_train = pd.merge(left = df_train,right = df_songs,how = 'left',on='song_id')\ndf_train.song_length.fillna(200000,inplace=True)\ndf_train.song_length = df_train.song_length.astype(np.uint32)\ndf_train.song_id = df_train.song_id.astype('category')\n\n# Release memory\ndel df_songs\nprint(\"OK\")", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "5c59815adbe56e4fd017f46c44ddac48df4472bb", "_cell_guid": "9f7c7585-7041-4281-888d-3ea72c469464"}, "outputs": []}, {"source": "# LightGBM\nFinally, we create the model, train it, and make the final predictions.", "cell_type": "markdown", "metadata": {"_uuid": "868fc59ed7439b029f45249e944cd5e22b823415", "_cell_guid": "9078694b-76c2-43d7-b0d3-0a999303073b"}}, {"source": "import lightgbm as lgb\n\n# Create a Cross Validation with 3 splits\nkf = KFold(n_splits=3)\n\n# This array will store the predictions made.\npredictions = np.zeros(shape=[len(df_test)])\n\n# For each KFold\nfor train_indices ,validate_indices in kf.split(df_train) : \n    train_data = lgb.Dataset(df_train.drop(['target'],axis=1).loc[train_indices,:],label=df_train.loc[train_indices,'target'])\n    val_data = lgb.Dataset(df_train.drop(['target'],axis=1).loc[validate_indices,:],label=df_train.loc[validate_indices,'target'])\n    \n    # Create the parameters for LGBM\n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting': 'gbdt',\n        'learning_rate': 0.2 ,\n        'verbose': 0,\n        'num_leaves': 108,\n        'bagging_fraction': 0.95,\n        'bagging_freq': 1,\n        'bagging_seed': 1,\n        'feature_fraction': 0.9,\n        'feature_fraction_seed': 1,\n        'max_bin': 128,\n        'max_depth': 12,\n        'num_rounds': 100,\n        'metric' : 'auc',\n        } \n    \n    # Train the model\n    bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n    \n    # Make the predictions storing them on the predictions array\n    predictions += bst.predict(df_test.drop(['id'],axis=1))\n    \n    # Release the model from memory for the next iteration\n    del bst\n\nprint('Training process finished. Generating Output...')\n\n# We get the ammount of predictions from the prediction list, by dividing the predictions by the number of Kfolds.\npredictions = predictions/3\n\n# Read the sample_submission CSV\nsubmission = pd.read_csv(INPUT_DATA_PATH + '/sample_submission.csv')\n# Set the target to our predictions\nsubmission.target=predictions\n# Save the submission file\nsubmission.to_csv('submission.csv',index=False)\n\nprint('Output created.')", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "912df754769c6329b01431349b7a138849116fb0", "_cell_guid": "2efbb855-ab71-4dd5-a6c2-a6f62ed97822"}, "outputs": []}, {"source": "from IPython.display import FileLink, FileLinks\n", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "5fd14b35de48e6c64ff0fbb5c90c53358a877ced", "collapsed": true, "_cell_guid": "3faafb92-f21e-43ed-ba29-d34eddf88ef0"}, "outputs": []}, {"source": "FileLinks('.')", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "57688eb1d624c11d9f24a2cc8d2ad1b1ee479b68", "_cell_guid": "78caaf0d-4807-4cd3-9e14-ed92b14e9b91"}, "outputs": []}, {"source": "def nize(t):\n    if t > 0.7:\n        return 1\n    elif t < 0.3:\n        return 0\n    else:\n        return 0.5\n    \npredictions = np.vectorize(nize)(predictions)\n\n", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "6823a6d8492a8ad82091ac2508419058d500c639", "_cell_guid": "e9d74db6-d440-4a76-a265-1270dc907f0e"}, "outputs": []}, {"source": "predictions[0]", "execution_count": null, "cell_type": "code", "metadata": {"trusted": true, "_uuid": "8f567d1339548e0730df37d3b47eb8799b4712be", "_cell_guid": "4e83eec4-5993-466c-9a6e-070f9d944df4"}, "outputs": []}, {"source": "", "execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "_uuid": "b8f1a5881ac4354334b5975d04a778546fd72f37", "collapsed": true, "_cell_guid": "770b44c6-0e0c-415a-960e-b99466a51ff9"}, "outputs": []}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.3", "name": "python"}}, "nbformat": 4, "nbformat_minor": 1}