{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python", "mimetype": "text/x-python", "version": "3.6.4", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "cells": [{"metadata": {}, "cell_type": "markdown", "source": ["### Welcome to my 2nd kernel for Toxic Comments Classification Challenge\n", "\n", "Here I will test and benchmark some algorithm used in Natural Language Processing. Namely :\n", "1. Logistic Regression\n", "2. Naive Bayes Algorithm\n", "3. Long Short Term Memory Neural networks\n", "\n", "My approach will be \" KIS\" : Keep It Simple, as long as I can. I will make no assumption when testing algorithm in the beginning and then analyze the results before new test\n", "\n", "![](http://www.elpoderdelasideas.com/wp-content/uploads/google-jigsaw-2016.png)"]}, {"metadata": {"_cell_guid": "de3c58cd-2257-4e2f-a906-0da85bc23f68", "_uuid": "8e236cbebab43e8d6f1dd868afb385b57f6dc93f"}, "cell_type": "code", "outputs": [], "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.naive_bayes import BernoulliNB\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn import linear_model\n", "from sklearn.metrics import log_loss\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "\n", "#NLP tools\n", "import re\n", "import string\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from wordcloud import WordCloud\n", "stopwords = nltk.corpus.stopwords.words('english')\n", "\n", "# plot tools\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import seaborn as sns\n", "\n", "sns.set_style(\"darkgrid\")\n", "\n", "path=\"../input/\"\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": 101}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": ["train = pd.read_csv(path+\"train.csv\")\n", "test = pd.read_csv(path+\"test.csv\")"], "execution_count": 84}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": ["replacement_patterns = [\n", " (r'won\\'t', 'will not'),\n", " (r'can\\'t', 'cannot'),\n", " (r'i\\'m', 'i am'),\n", " (r'ain\\'t', 'is not'),\n", " (r'(\\w+)\\'ll', '\\g<1> will'),\n", " (r'(\\w+)n\\'t', '\\g<1> not'),\n", " (r'(\\w+)\\'ve', '\\g<1> have'),\n", " (r'(\\w+)\\'s', '\\g<1> is'),\n", " (r'(\\w+)\\'re', '\\g<1> are'),\n", " (r'(\\w+)\\'d', '\\g<1> would')\n", "]\n", "class RegexpReplacer(object):\n", "    def __init__(self, patterns=replacement_patterns):\n", "         self.patterns = [(re.compile(regex), repl) for (regex, repl) in\n", "         patterns]\n", "     \n", "    def replace(self, text):\n", "        s = text\n", "        for (pattern, repl) in self.patterns:\n", "             s = re.sub(pattern, repl, s)\n", "        return s"], "execution_count": 85}, {"metadata": {}, "cell_type": "code", "outputs": [], "source": ["from nltk.stem import WordNetLemmatizer\n", "lemmer = WordNetLemmatizer()\n", "stopwords = nltk.corpus.stopwords.words('english')\n", "from nltk.tokenize import TweetTokenizer\n", "#from replacers import RegexpReplacer\n", "replacer = RegexpReplacer()\n", "tokenizer=TweetTokenizer()\n", "\n", "\n", "def comment_process(comment):\n", "        comment=tokenizer.tokenize(replacer.replace(comment))\n", "        comment= [word for word in comment if ( word.lower() not in stopwords \n", "                              and word.lower() not in list(string.punctuation) )]\n", "        comment=[lemmer.lemmatize(word, 'v') for word in comment]\n", "        comment.extend(list(comment))\n", "        comment=\" \".join(comment)\n", "        return comment\n", "    \n", "\n", "cleaned_train=train.comment_text.apply(comment_process)\n", "#cleaned_test=test.comment_text.apply(comment_process)\n", "\n", "\n"], "execution_count": 86}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": ["tf = TfidfVectorizer( strip_accents='unicode',analyzer='word', max_features= 50000, ngram_range=(4,4),\n", "            use_idf=True,smooth_idf=True,sublinear_tf=True,\n", "            stop_words = 'english')"], "execution_count": 87}, {"metadata": {}, "cell_type": "code", "outputs": [], "source": ["cols=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "y=train[cols]\n", "\n", "xtrain, xvalid, ytrain, yvalid = train_test_split(cleaned_train, y, \n", "                                                  random_state=42, \n", "                                                  test_size=0.3, shuffle=True)\n", "\n", "xtraintf=tf.fit_transform(xtrain)\n", "xvalidtf=tf.fit_transform(xvalid)\n", "\n", "#xtest=tf.transform(cleaned_test)\n", "\n", "\n", "\n", "\n"], "execution_count": 88}, {"metadata": {}, "cell_type": "markdown", "source": ["### Logistic Regression"]}, {"metadata": {}, "cell_type": "code", "outputs": [], "source": ["prd_valid = np.zeros((xvalidtf.shape[0],yvalid.shape[1]))\n", "prd_train = np.zeros((xtraintf.shape[0],ytrain.shape[1]))\n", "train_loss = []\n", "valid_loss = []\n", "bnb = LogisticRegression(penalty='l2')\n", "for i,col in enumerate(cols):\n", "    print('Building {} model for column:{''}'.format(i,col)) \n", "    bnb.fit(xtraintf,ytrain[col])\n", "    prd_valid[:,i] = bnb.predict_proba(xvalidtf)[:,1]\n", "    prd_train[:,i] = bnb.predict_proba(xtraintf)[:,1]\n", "    train_loss_class=log_loss(ytrain[col],prd_train[:,i])\n", "    valid_loss_class=log_loss(yvalid[col],prd_valid[:,i])\n", "    print('Trainloss=log loss:', train_loss_class)\n", "    print('Validloss=log loss:', valid_loss_class)\n", "    train_loss.append(train_loss_class)\n", "    valid_loss.append(valid_loss_class)\n", "print('mean column-wise log loss:Train dataset', np.mean(train_loss))\n", "print('mean column-wise log loss:Validation dataset', np.mean(valid_loss))"], "execution_count": 89}, {"metadata": {}, "cell_type": "markdown", "source": [".143 mean column-wise log loss... No so bad for this simple model.\n", "## Let's try Naive Bayes Algorithm and compare"]}, {"metadata": {}, "cell_type": "code", "outputs": [], "source": ["prd_valid = np.zeros((xvalidtf.shape[0],yvalid.shape[1]))\n", "prd_train = np.zeros((xtraintf.shape[0],ytrain.shape[1]))\n", "train_loss = []\n", "valid_loss = []\n", "bnb = BernoulliNB()\n", "for i,col in enumerate(cols):\n", "    print('Building {} model for column:{''}'.format(i,col)) \n", "    bnb.fit(xtraintf,ytrain[col])\n", "    prd_valid[:,i] = bnb.predict_proba(xvalidtf)[:,1]\n", "    prd_train[:,i] = bnb.predict_proba(xtraintf)[:,1]\n", "    train_loss_class=log_loss(ytrain[col],prd_train[:,i])\n", "    valid_loss_class=log_loss(yvalid[col],prd_valid[:,i])\n", "    print('Trainloss=log loss:', train_loss_class)\n", "    print('Validloss=log loss:', valid_loss_class)\n", "    train_loss.append(train_loss_class)\n", "    valid_loss.append(valid_loss_class)\n", "print('mean column-wise log loss:Train dataset', np.mean(train_loss))\n", "print('mean column-wise log loss:Validation dataset', np.mean(valid_loss))"], "execution_count": 90}, {"metadata": {}, "cell_type": "markdown", "source": ["10.23 mean column-wise log loss! Much worse than logistic regression. \n", "### My guess:\n", "My guess is that the Naive assumption of the Naive Bayes Algorithm is not valid. This assumption is that the features ( the words) are independent with each other. And obviously it's not valid here. But it would be \"less unvalid\" if the algorithm 2 or 3 words ( or more) instead of just one.  \n", "This picture illustrates the concept of n_grams. Instead of considering only one word we can consider every pair of words or every tree words. \n", "![](https://i.stack.imgur.com/8ARA1.png)\n", "\n", "Of course it will change the TF-IDF scores. The score will now be calculated for 2grams, 3grams... Ngrams."]}, {"metadata": {}, "cell_type": "code", "outputs": [], "source": ["def test_model(model,xtraintf, xvalidtf, ytrain, yvalid ):    \n", "    prd_valid = np.zeros((xvalidtf.shape[0],yvalid.shape[1]))\n", "    prd_train = np.zeros((xtraintf.shape[0],ytrain.shape[1]))\n", "    train_loss = []\n", "    valid_loss = []\n", "    \n", "    if model==\"lr\":\n", "        model= LogisticRegression(penalty=\"l2\")\n", "    if model==\"nb\":\n", "        model=BernoulliNB()\n", "    for i,col in enumerate(cols):\n", "        model.fit(xtraintf,ytrain[col])\n", "        \n", "        prd_valid[:,i] = model.predict_proba(xvalidtf)[:,1]\n", "        prd_train[:,i] = model.predict_proba(xtraintf)[:,1]\n", "        \n", "        train_loss_class=log_loss(ytrain[col],prd_train[:,i])\n", "        valid_loss_class=log_loss(yvalid[col],prd_valid[:,i])\n", "        \n", "        train_loss.append(train_loss_class)\n", "        valid_loss.append(valid_loss_class)\n", "    return(np.mean(train_loss), np.mean(valid_loss))\n", "\n", "\n", "    "], "execution_count": 91}, {"metadata": {}, "cell_type": "code", "outputs": [], "source": ["train_lr, valid_lr=[],[]\n", "train_nb, valid_nb=[],[]\n", "ngram_list=[x for x in range (1,6)]\n", "\n", "for ngram in ngram_list:\n", "    tf = TfidfVectorizer( strip_accents='unicode',analyzer='word', \n", "                         max_features= 50000, ngram_range=(ngram,ngram),\n", "            use_idf=True,smooth_idf=True,sublinear_tf=True,\n", "            stop_words = 'english')\n", "    xtraintf=tf.fit_transform(xtrain)\n", "    xvalidtf=tf.fit_transform(xvalid)\n", "    print(\"testing logistic regression with \"+ str(ngram)+\"grams\")\n", "    score_lr=test_model('lr', xtraintf, xvalidtf, ytrain, yvalid )\n", "    train_lr.append(score_lr[0])\n", "    valid_lr.append(score_lr[1])\n", "    \n", "    print(\"testing naive bayes with \"+ str(ngram)+\"grams\")\n", "    score_nb=test_model('nb', xtraintf, xvalidtf, ytrain, yvalid )\n", "    train_nb.append(score_nb[0])\n", "    valid_nb.append(score_nb[1])\n", "    \n", "\n"], "execution_count": 93}, {"metadata": {}, "cell_type": "code", "outputs": [], "source": ["plt.figure(figsize=(16,12))\n", "plt.suptitle(\"Ngrams comparison\",fontsize=20)\n", "\n", "plt.subplot2grid((2,1),(0,0))\n", "plt.title(\"Logistic Regression\")\n", "plt.plot(ngram_list, train_lr,'xkcd:crimson', label='train', linewidth=3 )\n", "plt.plot(ngram_list, valid_lr, 'xkcd:azure',label='validation', linewidth=3)\n", "plt.legend(fontsize=14)\n", "plt.ylabel('mean column-wise log loss', fontsize=20)\n", "plt.xlabel('Ngrams', fontsize=20)\n", "\n", "plt.subplot2grid((2,1),(1,0))\n", "plt.title(\"Naive Bayes\")\n", "plt.plot(ngram_list, train_nb, 'xkcd:crimson',label='train', linewidth=3 )\n", "plt.plot(ngram_list, valid_nb,'xkcd:azure',label='validation', linewidth=3)\n", "plt.legend(fontsize=14)\n", "plt.ylabel('mean column-wise log loss', fontsize=20)\n", "plt.xlabel('Ngrams', fontsize=20)\n", "\n"], "execution_count": 104}, {"metadata": {}, "cell_type": "markdown", "source": ["## As we can see, the 2 models react differently to Ngrams size\n", "\n", "* The Naive Bayes Algorithm seems to give better results with bigger Ngrams. It validate smy intuition about the non-valid independency hypothesis of this algorithm. Again, the bigger the Ngrams, the less \"unvalid\" this hypothesis is.\n", "* Logistic regression's validation score does not seem to depend on the ngram size\n", "\n", "* The increasing training loss show that bigger ngrams makes the model underfit. Indeed, bigger ngrams makes the model less complex. The bigger the ngrams, the lesser ngrams the model will learn from ==> the lesser complex our model will be.\n", "\n", "## Next step :\n", "* keep on analyzing these results\n", "* trying long short term memory neural network.\n", "I know I could try SVM or Decision trees before LSTM because LSTM  are much more complex. But I have more knowledge on LSTM and I can't wait to try them ahah !"]}], "nbformat_minor": 1, "nbformat": 4}