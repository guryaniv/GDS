{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1e300843-6ca4-4017-3bd3-6f5955f8c20a"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "21aaa023-d6e8-222b-731c-297008bcac52"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7194848b-7ffb-c25e-f1eb-85bc36def14b"
      },
      "outputs": [],
      "source": [
        "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
        "# Modifications copyright (C) 2017 Sijiang Du\n",
        "# Licensed under the Apache License, Version 2.0 (the 'License');\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an 'AS IS' BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Credit Card Fraud Detection\n",
        "Multi-layer neoral network is trained by the data set ofaAnonymized credit card transactions labeled as fraudulent or genuine\n",
        "There are two classes of labeled rows: fraud and normal(true) transactions The total fraud trainsactions are less than 0.2%\n",
        "among overall 284807 records. The program constructs a neural network model to detect the fraud transactions. \n",
        "The training has several epochs. Each training step is done in a batch inputs. Half of the batch data are fraud data and another half\n",
        "are normal transactions data.\n",
        "The data set is divided to two parts: 80% for training set and 20% for testing set.\n",
        "The testing result is evaluated by two measurements: overall accuracy and KS value,\n",
        "The Kolmogorov Smirnov chart is ploted as result and a PNG file is saved in FLAGS.result_dir folder.\n",
        "\n",
        "Tensorflow input_pipeline is being used to streamline and randomlize inputs for the training and testing session.\n",
        "To start: call the train() function, e.g. \"train(5,100)\", means 5 hidden layers, each layer has 100 neurons.  \n",
        "\n",
        "Author: Sijiang Du, Updated in May 2, 2017\n",
        "\n",
        "This file is a modified file of https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n",
        "In this file:\n",
        "The input data are in \"creditcard.csv\" which is downloaded from https://www.kaggle.com/dalpozz/creditcardfraud\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "import numpy as np\n",
        "import os.path\n",
        "import csv\n",
        "import random\n",
        "\n",
        "csv_file_name = \"../input/creditcard.csv\"\n",
        "train_file_name_true = \"creditcard_train_true.csv\"\n",
        "train_file_name_fraud = \"creditcard_train_fraud.csv\"\n",
        "test_file_name_true = \"creditcard_test_true.csv\"\n",
        "test_file_name_fraud = \"creditcard_test_fraud.csv\"\n",
        "test_file_name_both =  \"creditcard_test_all.csv\"\n",
        "\n",
        "batch_size = 128\n",
        "input_channel_num = 30\n",
        "output_channel_num = 2\n",
        "hidden_layer_neuron = 200\n",
        "hidden_layer_num = 3\n",
        "train_epoch = 3\n",
        "csv_lines_train = 0\n",
        "csv_lines_test_both = 0\n",
        " \n",
        "# partition: randomly partion the input file to two files. E.g. partition = 0.8: 80% for training 20% for testing.\n",
        "def csv_partition_train_test(input_file, partition=0.8):\n",
        "  with open(input_file) as data:\n",
        "      with open(FLAGS.data_dir+test_file_name_true, 'w+') as test_true,\\\n",
        "           open(FLAGS.data_dir+test_file_name_fraud, 'w+') as test_fraud,\\\n",
        "           open(FLAGS.data_dir+test_file_name_both, 'w+') as test_both:\n",
        "          with open(FLAGS.data_dir+train_file_name_true, 'w+') as train_true, open(FLAGS.data_dir+train_file_name_fraud, 'w+') as train_fraud:\n",
        "              header = next(data)\n",
        " #             test.write(header)\n",
        " #             train.write(header)           \n",
        "              csv_r = csv.reader(data)\n",
        "              csv_w_train_true = csv.writer(train_true)\n",
        "              csv_w_train_fraud = csv.writer(train_fraud)\n",
        "              csv_w_test_true = csv.writer(test_true)\n",
        "              csv_w_test_fraud = csv.writer(test_fraud)\n",
        "              csv_w_test_both = csv.writer(test_both)             \n",
        "              global csv_lines_test_both\n",
        "              global csv_lines_train\n",
        "              \n",
        "              for line in csv_r:\n",
        "                \n",
        "                  if line[-1]=='0':\n",
        "                      line = line[:-1] + ['0','1']  \n",
        "                  elif line[-1]=='1':\n",
        "                      line = line[:-1] + ['1','0']\n",
        "                       \n",
        "                  if random.random() < partition:\n",
        "                    csv_lines_train +=1\n",
        "                    if line[-1]=='0':\n",
        "                       csv_w_train_fraud.writerow(line)\n",
        "                    else:\n",
        "                       csv_w_train_true.writerow(line)\n",
        "                       \n",
        "                  else:\n",
        "                    csv_w_test_both.writerow(line)\n",
        "                    csv_lines_test_both += 1\n",
        "                    if line[-1]=='0':\n",
        "                       csv_w_test_fraud.writerow(line)\n",
        "                    else:\n",
        "                       csv_w_test_true.writerow(line)\n",
        "\n",
        "\n",
        "def read_creditcard_csv(filename_queue):\n",
        "\n",
        "  reader = tf.TextLineReader(skip_header_lines=1)\n",
        "  key, value = reader.read(filename_queue)\n",
        "  record_defaults = [[0.0]for row in range(32)]\n",
        "  cols = tf.decode_csv(\n",
        "  value, record_defaults=record_defaults)\n",
        "  features = tf.stack(cols[:-2])\n",
        "  label = tf.stack([cols[30],cols[31]])\n",
        "  return features, label\n",
        "\n",
        "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
        "  filename_queue = tf.train.string_input_producer(\n",
        "      filenames, num_epochs=num_epochs, shuffle=True)\n",
        "  example, label = read_creditcard_csv(filename_queue)\n",
        "  # min_after_dequeue defines how big a buffer we will randomly sample\n",
        "  #   from -- bigger means better shuffling but slower start up and more\n",
        "  #   memory used.\n",
        "  # capacity must be larger than min_after_dequeue and the amount larger\n",
        "  #   determines the maximum we will prefetch.  Recommendation:\n",
        "  #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
        "  min_after_dequeue = 10000\n",
        "  capacity = min_after_dequeue + 3 * batch_size\n",
        "  example_batch, label_batch = tf.train.shuffle_batch(\n",
        "      [example, label], batch_size=batch_size, capacity=capacity,\n",
        "      min_after_dequeue=min_after_dequeue)\n",
        "  return example_batch, label_batch\n",
        "\n",
        "def train(layer_num= hidden_layer_num, neuron_num = hidden_layer_neuron):\n",
        "\n",
        "  if    not os.path.exists(FLAGS.data_dir+train_file_name_true)\\\n",
        "     or not os.path.exists(FLAGS.data_dir+train_file_name_fraud)\\\n",
        "     or not os.path.exists(FLAGS.data_dir+test_file_name_true)\\\n",
        "     or not os.path.exists(FLAGS.data_dir+test_file_name_fraud):                    \n",
        "    csv_partition_train_test(csv_file_name, 0.8)\n",
        " \n",
        "  sess = tf.InteractiveSession()\n",
        "  # Create a multilayer model.\n",
        "\n",
        "  # Input placeholders\n",
        "  with tf.name_scope('input'):\n",
        "    x = tf.placeholder(tf.float32, [None, input_channel_num], name='x-input')\n",
        "    y_ = tf.placeholder(tf.float32, [None, output_channel_num], name='y-input')\n",
        "\n",
        "  def weight_variable(shape):\n",
        "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "  def bias_variable(shape):\n",
        "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "  def variable_summaries(var):\n",
        "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "    with tf.name_scope('summaries'):\n",
        "      mean = tf.reduce_mean(var)\n",
        "      tf.summary.scalar('mean', mean)\n",
        "      with tf.name_scope('stddev'):\n",
        "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "      tf.summary.scalar('stddev', stddev)\n",
        "      tf.summary.scalar('max', tf.reduce_max(var))\n",
        "      tf.summary.scalar('min', tf.reduce_min(var))\n",
        "      tf.summary.histogram('histogram', var)\n",
        "\n",
        "  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu, keep_prob=1.0):\n",
        "    \"\"\"Reusable code for making a simple neural net layer.\n",
        "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
        "    It also sets up name scoping so that the resultant graph is easy to read,\n",
        "    and adds a number of summary ops.\n",
        "    \"\"\"\n",
        "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
        "    with tf.name_scope(layer_name):\n",
        "      # This Variable will hold the state of the weights for the layer\n",
        "      with tf.name_scope('weights'):\n",
        "        weights = weight_variable([input_dim, output_dim])\n",
        "        print(weights)\n",
        "        variable_summaries(weights)\n",
        "      with tf.name_scope('biases'):\n",
        "        biases = bias_variable([output_dim])\n",
        "        variable_summaries(biases)\n",
        "      with tf.name_scope('Wx_plus_b'):\n",
        "        preactivate = tf.matmul(input_tensor, weights) + biases\n",
        "      activations = act(preactivate, name='activation')\n",
        "      tf.summary.histogram('activations', activations)\n",
        "      if FLAGS.dropout<1.0: dropped = tf.nn.dropout(activations, keep_prob)\n",
        "      return activations\n",
        "    \n",
        "  keep_prob = tf.placeholder(tf.float32)\n",
        "  \n",
        "  #Add layer_num layers of hidden layer\n",
        "  cur_layer = nn_layer(x, input_channel_num, neuron_num, 'layer_0', tf.identity, keep_prob)\n",
        "  for i in range(1,layer_num):\n",
        "    cur_layer = nn_layer(cur_layer, neuron_num, neuron_num, 'layer_'+str(i), tf.identity, keep_prob)\n",
        "\n",
        "  # the last layer is the output layer\n",
        "  y = nn_layer(cur_layer, neuron_num, output_channel_num, 'output_layer', act=tf.identity)\n",
        "  # scale the activation down by the size of hidden layer\n",
        "  y = y/neuron_num\n",
        "\n",
        "  with tf.name_scope('cross_entropy'):\n",
        "    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
        "    with tf.name_scope('total'):\n",
        "      cross_entropy = tf.reduce_mean(diff)\n",
        "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
        "\n",
        "  with tf.name_scope('train'):\n",
        "    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n",
        "        cross_entropy)\n",
        "\n",
        "  with tf.name_scope('accuracy'):\n",
        "    with tf.name_scope('correct_prediction'):\n",
        "      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "    with tf.name_scope('accuracy'):\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "  #scorecard for the model evaluation. A lower score indictates a higher probability to be fraud.\n",
        "  with tf.name_scope('score_card'):\n",
        "    yc = tf.reshape( y[:,1]-y[:,0], (-1,1))\n",
        "    yc_ = tf.reshape( y_[:,1]-y_[:,0], (-1,1))\n",
        "    score_card = tf.concat([yc, yc_], 1)\n",
        "  \n",
        "  with tf.name_scope('input_examples'):\n",
        "      example_batch_train_true, label_batch_train_true = input_pipeline(tf.constant([FLAGS.data_dir+train_file_name_true]), round(batch_size/2))\n",
        "      example_batch_train_fraud, label_batch_train_fraud = input_pipeline(tf.constant([FLAGS.data_dir+train_file_name_fraud]), batch_size-round(batch_size/2))\n",
        "      example_batch_test_true, label_batch_test_true = input_pipeline(tf.constant([FLAGS.data_dir+test_file_name_true]), batch_size)\n",
        "      example_batch_test_fraud, label_batch_test_fraud = input_pipeline(tf.constant([FLAGS.data_dir+test_file_name_fraud]), batch_size)\n",
        "      example_batch_test_both, label_batch_test_both = input_pipeline(tf.constant([FLAGS.data_dir+test_file_name_both]), batch_size,1)\n",
        "      \n",
        "  init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "\n",
        "  # Merge all the summaries and write them out to /tmp/tensorflow/creditcard/logs/creditcard_with_summaries (by default)\n",
        "  merged = tf.summary.merge_all()\n",
        "  train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)\n",
        "  test_writer_true = tf.summary.FileWriter(FLAGS.log_dir + '/test_true')\n",
        "  test_writer_fraud = tf.summary.FileWriter(FLAGS.log_dir + '/test_fraud')\n",
        "\n",
        "  sess.run(init_op)\n",
        "  # Start input enqueue threads.\n",
        "  coord = tf.train.Coordinator()\n",
        "  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "\n",
        "  # Train the model, and also write summaries.\n",
        "  # Every 10th step, measure test-set accuracy, and write test summaries\n",
        "  # All other steps, run train_step on training data, & add training summaries\n",
        "\n",
        "  def feed_dict(train,test=0):\n",
        "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
        "    if train:\n",
        "      batch_xs_1, batch_ys_1 = sess.run([example_batch_train_true, label_batch_train_true])\n",
        "      batch_xs_2, batch_ys_2 = sess.run([example_batch_train_fraud, label_batch_train_fraud])\n",
        "\n",
        "      xs = np.concatenate ((batch_xs_1,batch_xs_2))\n",
        "      ys = np.concatenate ((batch_ys_1,batch_ys_2))\n",
        "      perm = np.random.permutation(xs.shape[0])\n",
        "      np.take(xs,perm,axis=0,out=xs)\n",
        "      np.take(ys,perm,axis=0,out=ys)\n",
        "      k = FLAGS.dropout\n",
        "    else:\n",
        "      if test==0:\n",
        "        xs, ys = sess.run([example_batch_test_fraud, label_batch_test_fraud])\n",
        "      elif test==1:\n",
        "        xs, ys = sess.run([example_batch_test_true, label_batch_test_true])\n",
        "      elif test==2:\n",
        "        xs, ys = sess.run([example_batch_test_both, label_batch_test_both])\n",
        "      k = 1.0\n",
        "    return {x: xs, y_: ys, keep_prob: k}\n",
        "\n",
        "  scorecard_all = [[0, 0]]\n",
        "  acc_all = []\n",
        "  print('[Training inputs: %s, epoch = %s], [Testing inputs: %s],' % (csv_lines_train, train_epoch,csv_lines_test_both))\n",
        "  try:      \n",
        "      i = 0\n",
        "      train_s = csv_lines_train*train_epoch/batch_size\n",
        "      # plotting\n",
        "      fig1 = plt.figure(1)\n",
        "      ax = fig1.add_subplot(111)\n",
        "      plt.ylim((-0.5, 1.5))\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Training Batches')\n",
        "      ax.text(0.05, 0.90,'Fraud: (red) --', ha='left', va='center', color='red', transform=ax.transAxes)\n",
        "      ax.text(0.05, 0.85,'Normal: (blue) --', ha='left', va='center', color='blue', transform=ax.transAxes)\n",
        "      fig1.suptitle('Tensorflow: Creditcard Fraud Detection Training')\n",
        "      while not coord.should_stop():\n",
        "            i = i+1\n",
        "\n",
        "            if i<=train_s :\n",
        "               #Record train set summaries, and train\n",
        "              if i % 100 == 0:  # Record execution stats\n",
        "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "                run_metadata = tf.RunMetadata()\n",
        "                summary, _= sess.run([merged, train_step],\n",
        "                                      feed_dict=feed_dict(True),\n",
        "                                      options=run_options,\n",
        "                                      run_metadata=run_metadata)\n",
        "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
        "                train_writer.add_summary(summary, i)\n",
        "                print('Adding run metadata for', i)\n",
        "                \n",
        "              else:  # Record a summary\n",
        "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
        "                train_writer.add_summary(summary, i)\n",
        "\n",
        "              if i % 10 == 0:  # Record summaries and test-set accuracy\n",
        "                  summary, acc0 = sess.run([merged, accuracy], feed_dict=feed_dict(False,0))\n",
        "                  test_writer_fraud.add_summary(summary, i)\n",
        "                  summary, acc1 = sess.run([merged, accuracy], feed_dict=feed_dict(False,1))\n",
        "                  test_writer_true.add_summary(summary, i)\n",
        "                  # print('%s: [%s], [%s]' % (i, acc0,acc1))\n",
        "                  \n",
        "                  # plotting\n",
        "                  plt.plot(list(range(i-10,i)),[acc0]*10, 'r', list(range(i-10,i)),[acc1]*10, 'b')\n",
        "                  if i%100 == 0:\n",
        "                    plt.draw()\n",
        "                    plt.pause(0.3)\n",
        "\n",
        "            else:\n",
        "              acc, scorecard = sess.run([accuracy,score_card], feed_dict=feed_dict(False,2))\n",
        "              acc_all.append(acc)\n",
        "              scorecard_all = np.concatenate ((scorecard_all,scorecard))              \n",
        "\n",
        "  except tf.errors.OutOfRangeError:\n",
        "      print('Done training and testing -- epoch limit reached')\n",
        "  finally:\n",
        "      # When done, ask the threads to stop.\n",
        "      coord.request_stop()\n",
        "\n",
        "  scorecard_all = np.delete(scorecard_all, (0), axis=0)\n",
        "  scorecard_all = scorecard_all[scorecard_all[:, 0].argsort()]\n",
        "  count_true = 0\n",
        "  count_fraud = 0\n",
        "  percent_true = np.zeros(scorecard_all.shape[0])\n",
        "  percent_fraud = np.zeros(scorecard_all.shape[0])\n",
        "  score_range = []\n",
        "  score_std = scorecard_all[:,0].std()\n",
        "  \n",
        "  for i in range(scorecard_all.shape[0]):\n",
        "      \n",
        "      if scorecard_all[i,1] > 0:\n",
        "          count_true +=1\n",
        "      else: \n",
        "          count_fraud +=1\n",
        "      percent_true[i] = count_true\n",
        "      percent_fraud[i] = count_fraud\n",
        "  ks = 0\n",
        "  ks_pos = 0\n",
        "  for i in range(scorecard_all.shape[0]):\n",
        "      percent_true[i]  /= count_true\n",
        "      percent_fraud[i] /= count_fraud\n",
        "      diff = percent_fraud[i] - percent_true[i]\n",
        "      if diff > ks:\n",
        "        ks = diff\n",
        "        ks_pos = i\n",
        "  accuracy_mean = np.mean(acc_all)\n",
        "  \n",
        "  acc = 'Accuracy (' + str(csv_lines_test_both) + ' samples): ' + str(accuracy_mean)\n",
        "  # print(acc)\n",
        "  fig2 = plt.figure(2,figsize=(10, 8))\n",
        "  ax = fig2.add_subplot(111)\n",
        "  plt.plot(list(scorecard_all[:, 0]),percent_fraud, 'r', list(scorecard_all[:, 0]),percent_true, 'b')\n",
        "  plt.plot([scorecard_all[ks_pos,0],scorecard_all[ks_pos,0]], [percent_true[ks_pos], percent_fraud[ks_pos]], 'g')\n",
        "  ax.text(0.05, 0.95,'Greenline: KS = '+ str(ks), ha='left', va='center', color='green', transform=ax.transAxes)\n",
        "  ax.text(0.05, 0.90,'Redline: Fraud', ha='left', va='center', color='red', transform=ax.transAxes)\n",
        "  ax.text(0.05, 0.85,'Blueline: Normal', ha='left', va='center', color='blue', transform=ax.transAxes)\n",
        "  ax.text(0.05, 0.80,acc, ha='left', va='center', color='black', transform=ax.transAxes)\n",
        "  plt.ylabel('Cumulative percentage')\n",
        "  plt.xlabel('Scorecard scoring')\n",
        "  fig2.suptitle('KS graph (Creditcard Fraud Detection by Tensorflow)\\n('+str(layer_num) +' hidden layers, '+str(neuron_num)+' neurons per layer)')\n",
        "  result_str = str(round(int(accuracy_mean*1000)))+'_'+str(int(round(ks*1000)))+'_'+str(layer_num)+'_'+str(neuron_num)+'_e'+str(train_epoch)\n",
        "  plt.draw()\n",
        "  plt.savefig(FLAGS.result_dir+'/creditcard_ks_'+result_str+'.png')\n",
        "  plt.pause(3) \n",
        "  # Wait for threads to finish.\n",
        "  coord.join(threads)\n",
        "\n",
        "  # Wait for threads to finish.\n",
        "  coord.join(threads)\n",
        "\n",
        "  sess.close()\n",
        "\n",
        "  train_writer.close()\n",
        "  test_writer_true.close()\n",
        "  test_writer_fraud.close()\n",
        "  return ks,accuracy_mean\n",
        "\n",
        "def main(_):\n",
        "  if tf.gfile.Exists(FLAGS.log_dir):\n",
        "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
        "  tf.gfile.MakeDirs(FLAGS.log_dir)\n",
        "  if tf.gfile.Exists(FLAGS.data_dir):\n",
        "    tf.gfile.DeleteRecursively(FLAGS.data_dir)\n",
        "  tf.gfile.MakeDirs(FLAGS.data_dir)\n",
        "  if not tf.gfile.Exists(FLAGS.result_dir):\n",
        "    tf.gfile.MakeDirs(FLAGS.result_dir)\n",
        " # 4 hidden layers, each layer has 20 output channels. One layer is a (20,20) tensor.\n",
        "  train(4, 20)\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--learning_rate', type=float, default=0.001,\n",
        "                      help='Initial learning rate')\n",
        "  parser.add_argument('--dropout', type=float, default= 1.0,\n",
        "                      help='Keep probability for training dropout.')\n",
        "  parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/creditcard/input_data/',\n",
        "                      help='Directory for storing input data')\n",
        "  parser.add_argument('--log_dir', type=str, default='/tmp/tensorflow/creditcard/logs/creditcard_with_summaries',\n",
        "                      help='Summaries log directory')\n",
        "  parser.add_argument('--result_dir', type=str, default='/tmp/tensorflow/creditcard/result',\n",
        "                      help='Summaries log directory')\n",
        "\n",
        "  FLAGS, unparsed = parser.parse_known_args()\n",
        "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}