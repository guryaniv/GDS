{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "mimetype": "text/x-python", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "cells": [{"source": ["Brace yourselves, friends, cause this is going to be long. I'll try to keep you entertained.\n", "I'm going to create a huge document where I want to accomplish two different goals.\n", "\n", "<h1>Goals</h1>\n", "1. Introduce people that are not necessarily on the field of NLP to the task of text classification.\n", "1. Present my stylometry framework and my specific approach to the author identification task.\n", "\n", "And given the extension of the document, I also provide an index:\n", "1. Introduction to the task\n", "1. Feature Engineering\n", "1. Author Identification (and profiling)\n", "1. Approach description\n", "1. Framework description\n", "\n", "Feel free to skip any part if you feel it is too basic/broad/boring."], "metadata": {"_uuid": "0961ec6584a330b69f76c8f966fb12b6fbf40019", "_cell_guid": "1366880e-b436-4753-b45c-7f9dbb062551"}, "cell_type": "markdown"}, {"source": ["<h1>1. Introduction to the task</h1>\n", "\n", "First of all, lets talk about the classic NLP document classification task. This is a classic task in the field of NLP in which the goal is to classify texts with respect to a predefined set of candidate categories. Each one of these categories will be referred as classes or labels from now on.  \n", "Some examples of document classification tasks:\n", "* Given a web page and a query (google search query for example), determine if a document is relevant.\n", "* Given a text, determine if the content is considered positive or negative.\n", "* Given a word, determine which one of its senses is being used.\n", "* Given a set of assignments, determine if there has been cases of plagarism.\n", "* Given a text, determine if it contains hate speech or not.\n", "...\n", "So, yeah, there are maaaaany examples and each one of them is a whole area of study in the NLP community.\n", "\n", "Let us use this task as an example. We have a corpus of texts written by 3 different authors. The goal is to determine who wrote unseen texts between the three candidates.  \n", "\n", "**How do we do that? \n", "**\n", "\n", "We could manually code some rules to classify them (e.g., if \"Cthulhu\" appears, them H.P. Lovecraft is the answer). This sort of rule-based approach is usually not scalable: if an unexpected case appears, new rules need to be implemented. \n", "A popular alternative is to use machine learning. The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience. Machine learning algorithms learn from the provided data and extract underlying regular patterns from it. The extracted patterns can then be applied to the classification of unseen data instances. This is the approach that I'm going to be using.\n", "\n", "The typical machine learning-driven text classification approach works like this:\n", "1) we determine which features of the text are distinctive (i.e., can help distinguishing between text classes).\n", "2) we extract said features.       \n", "3) we use the feature vectors along with the correct classes of the training set to learn (i'm assuming there exists a set of texts in which \"the correct answer\" is known, usually known as training set), feeding them to a standard supervised machine learning algorithm (such as SVM, Random Forests, etc.)\n", "4) we use the model that is extracted by the machine learning algorithm to classify unseen instances. \n", "5) if we have the correct classes of the unseen instances, we evaluate how well we did.\n", "\n", "All right, still with me folks? So, given the presented steps, which one do you think is the critical step? \n", "\n", "There are plenty of machine learning algorithms that can be easily used off-the-shelf and that work very well, extracting features is just a matter of coding, the evaluation usually consists in applying a formula that considers the output of the classifier. The choice of feature sets is the key component in this type of approach. This feature selection process  is often called \"feature engineering\".\n", "\n", "<h1>2 Feature Engineering </h1>\n", "Feature engineering is a vaguely defined set of tasks related to designing feature sets for machine learning applications (which in some cases, is considered an art). The first important task to do, in order to correctly design a feature set is to understand the properties of the problem at hand and assess how they might interact with the chosen classifier. After understanding the problem, hypotheses need to be drawn. Feature engineering is thus a cycle, in which a set of features is proposed, experiments with this feature set are performed, and, after analyzing the results, the feature set is modified to improve the performance until the results are satisfactory. \n", "\n", "Although it is often possible to obtain competitive performance using fairly simple and obvious sets of features, there is room for significant performance improvement. Carefully constructed feature sets require thorough understanding of the task at hand, but can significantly outperform basic feature sets. In short, better features mean better results.\n", "\n", "The data needs to be characterized by a group of features that differentiate between the instances that belong to a class with respect to the other classes. Irrelevant or partially relevant features can negatively impact the performance of the classifier. An example of an irrelevant feature would be one that takes a fixed value for any instance in the input data.\n", "\n", "Optimal feature selection helps the algorithms extract patterns that generalize to unseen instances without needing complex parametrization of the classifier to perform competitively, preventing overfitting. Models created by the machine learning algorithm which contain the  \"knowledge\" extracted from training data are faster to run, easier to understand and to maintain if the feature set is appropriate.\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "metadata": {"_uuid": "506521e523eaf55f2214d1c97d1a37db6c2174fc", "_cell_guid": "4a62b0bc-3f4a-43c9-9f4f-a6a4db450590"}, "cell_type": "markdown"}, {"source": ["After talking about the general task of text classification, let us talk about the specific task at hand.\n", "\n", "<h1>3. Author Identification</h1>\n", "\n", "The task at hand can be considered an author identification problem. The goal is to determine the author of a text with a predefined set of authors.\n", "\n", "So, the question is,  are we the first ones that thought about that? \n", "\n", "The answer is...\n", "\n", "Yeah, no.\n", "\n", "Given how wrong you are if you thought the answer was YES! Let me give you a....\n", "\n", "<div style=\"color:red;background:black;\">**HISTORY LESSON!**</div>\n", "\n", "(real shame the marquee tag didnt work)\n", "\n", "One of the first proposals that implemented a data-driven author identification system is:\n", "\n", "Frederick Mosteller and David L. Wallace, \u2018Inference in an authorship problem\u2019,Journal of the Ameri-can Statistical Association,58(302), 275\u2013309, (1963). \n", "\n", "Yeah, 1963! (some of us were not even planned yet)\n", "\n", "The authors tried to clarify the authorship of the Federalist Papers drawing upon function words and Naive Bayes classification.  The Federalist Papers is a collection of 85 articles and essays written by Alexander Hamilton, James Madison, and John Jay to promote the ratification of the United States Constitution.  The authorship of seventy-three of The Federalist essays is fairly certain. The remaining 12  are the subjects of study of several scholars.\n", "\n", "The other classic problem involves this guy:\n", "![](https://www.biography.com/.image/c_fill%2Ccs_srgb%2Cg_face%2Ch_300%2Cq_80%2Cw_300/MTE1ODA0OTcxNzgzMzkwNzMz/william-shakespeare-194895-1-402.jpg)\n", "\n", "He looks like he is hiding something.\n", "\n", "Long story short, people believe that William Shakespeare did not actually write some of his best plays. His biography, humble origins, obscure life (not much is known of his personal life) made people question how, given his background, could he be the greatest writer of all time.  So, many researchers used NLP to research on the topic. None of the results are absolutely conclusive, but it seems that some of his works are stylistically related to other authors. If you want to read more about the topic, refer to:\n", "\n", "* Hierarchical and Non-Hierarchical Linear and Non-Linear Clustering Methods to \"Shakespeare Authorship Question\" by Refat Aljumly \n", "\n", "and \n", "\n", "* Neural computation in stylometry I: An application to the works of Shakespeare and Fletcher by Robert Matthews and Thomas Merriam.\n", "\n", "Another very interesting authorship problem that was in the news a couple of years ago is the case of Robert Galbraith. In 2013 a novel  called The Cuckoo\u2019s Calling was published by an unknown author called Robert Galbraith. A newspaper received an anonymous tip that this author was actually J.K Rowling, which wanted to publish a crime novel without the influence of the whole \"Harry Potter\" universe to affect its success. Said newspaper wanted to prove it and hired a researcher in the field of author identification. The researcher observed many similarities between the styles of J.K. Rowling and Robert Galbraith.  J.K Rowling admitted it was her all along (bummed out probably to be outed by us nerds).\n", "\n", "<div style=\"color:red;background:black;\">**END OF HISTORY LESSON!**</div>\n", "\n", "So yeah, these are some of the mainstream cases. But there are MANY other previous works on the task. Author identification  is often applied in forensic linguistic scenarios (here you can see the kind of linguistic analysis that can be done in police investigation: [https://www2.fbi.gov/publications/leb/1996/oct964.txt](http://))\n", "\n", "How do some of these approaches tackle this task? What features do they use? Let me present some examples:\n", "* Character/token n-grams (frequencies of sequences of characters/words)\n", "* Frequency of specific words (the most relevant, look up function words, tf-idf computation and all of that good stuff)\n", "* Usage of specific parts of speech (frequencies of adjectives, nouns, etc).\n", "* Punctuation mark usage (very stylistic in some languages).\n", "* Syntactic structural features (analysis on the syntactic trees of the sentences).\n", "* Topic models\n", "* Word embeddings\n", "...\n", "\n", "and a large ETC.\n"], "metadata": {"_uuid": "f0ad716766796bb68b8d27fafb164a2a9b93243e", "_cell_guid": "5d8f1f22-e6e2-4929-b550-9eafadf15d8a"}, "cell_type": "markdown"}, {"source": ["<h1>4. Approach description</h1>\n", "\n", "At this point you might be thinking: \"WTF IS THIS, I WANT TO SEE CODE\" or \"WOW, THIS IS MARVELOUS\". I am specially eloquent, basically because I presented my PhD on author identification and profiling (instead of identifying the author of texts, try to identify demographic traits of the author: gender, age, etc.) a couple of months ago. \n", "\n", "So now that I have put everything in a bit of context, let me introduce my approach and then I'll explain the Python framework I created (during the development of my PhD thesis) and show you some of the things it can do. \n", "\n", "An overview of the flow of the system is shown in the following image:\n", "\n", "![](https://i.imgur.com/ljqGT9Y.png)\n", "\n", "So, it seems like a standard text classification machine learning flow. The only peculiar thing is that I use both syntactic parsing and some dictionaries. \n", "\n", "The important part of the system is the selected feature set. \n", "\n", "The feature set is composed of six subgroups of features:\n", "\n", "**Character-based features\n", "**\n", "\n", "are composed of the ratios between upper case characters, periods, commas, parentheses, exclamations, colons,\n", "number digits, semicolons, hyphens and quotation marks and the total number of characters in a text.\n", "\n", "**Word-based features **\n", "\n", "are composed of the mean number of characters per word, vocabulary richness, acronyms, stopwords, first person pronouns, usage of words composed by two or three characters, standard deviation of word length and the difference between the longest and shortest words.\n", "\n", "**Sentence-based features**\n", "\n", "are composed of the mean number of words per sentence, standard deviation of words per sentence and the difference between the maximum and minimum number of words per sentence in a text.\n", "\n", "**Dictionary-based features**\n", "\n", "consist of the ratios of discourse markers, interjections, abbreviations, curse words, and polar words (positive and negative words in polarity dictionaries) with respect to the total number of words in a text.\n", "\n", "**Syntactic features**\n", "\n", "Three types of syntactic features are distinguished:\n", "\n", "Part-of-Speech features are given by  the relative frequency of each PoS tag. We use the Penn Treebank tagset  ([http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](http://)) in a text,  the relative frequency of comparative/superlative adjectives and adverbs and the relative frequency of the present and past tenses. In addition to the fine-grained Penn Treebank tags, we introduce general grammatical categories (such as \"verb\", \"noun\", etc.) and calculate their frequencies.\n", "\n", "Dependency features reflect the occurrence of syntactic dependency relations in the dependency trees of the text. The tagset used by the parser is the standard penn treebank dependency tagset. We extract the frequency of each individual dependency relation per sentence, the percentage of modifier relations used per tree,  the frequency of adverbial dependencies (they give information on manner, direction, purpose, etc.), the ratio of modal verbs with respect to the total number of verbs, and the percentage of verbs that appear in complex tenses referred to as \"verb chains\" (VCs).\n", "\n", "Tree features measure the tree width, the tree depth and the ramification factor. Tree depth is defined as the maximum number of nodes between the root and a leaf node; the width is the maximum number of siblings at any of the levels of the tree; and the ramification factor is the mean number of children per level. In other words, the tree  features characterize the complexity of the dependency structure of the sentences. \n", "\n", "These measures are also applied to subordinate and coordinate clauses.\n", "\n", "(I usually use discourse features as well, but I'm going to pass in this case)\n", "\n", "**Lexical features**\n", "\n", "Super simple, just the frequency of the N most frequent words in the training set.\n", "\n", "The full set is composed of less than 200 features and in several tasks, it performs at state-of-the-art level.\n", "\n"], "metadata": {"_uuid": "48a41234d7240ba2da3d3b818bb23f4b1aee730c", "_cell_guid": "942d093f-1763-49fe-b5f5-5db69200f63f"}, "cell_type": "markdown"}, {"source": ["<h1>5. Framework Description</h1>\n", "\n", "All right, if you have jumped to this section directly, welcome!\n", "\n", "If you have stuck around in my long rant, you deserve a treat!\n", "![](https://media.giphy.com/media/HlYYLuI3WsAW4/giphy.gif)\n", "\n", "Lets get to the code.\n", "\n", "In my PhD research I had to code a lot, so I ended up creating a framework to perform author profiling/identification/text classification problems. \n", "\n", "Lets see the general file system organization\n", "\n", "TreeLib/\n", "\n", "        tree.py\n", "        treeOperations.py\n", "\n", "dicts/       \n", "\n", "        several dictionaries\n", "\n", "featureClasses/\n", "\n", "        characterBasedFeatures.py\n", "        dictionaryBasedFeatures.py\n", "        lexicalFeatures.py\n", "        sentenceBasedFeatures.py\n", "        syntacticFeatures.py\n", "        utils.py\n", "        wordBasedFeatures.py\n", "\n", "featureManager.py\n", "\n", "instanceManager.py\n", "\n", "\n", "The instance and feature manager classes are key, so let us see some code (FINALLY).\n", "\n", "\n", "\n"], "metadata": {"_uuid": "e39f77603ea67a851c2ea45b9348865fa7d768c4", "_cell_guid": "7668239c-c3a2-4429-b8f7-86c5fd70205f"}, "cell_type": "markdown"}, {"source": ["This code models each feature individually and the concept of a feature set.\n", "Each Feature has a name and a value.\n", "The feature set has a feature dict, in which you can access a specific feature like this:\n", "\n", "self.featureDict[\"typeOfFeature\"][\"nameOfFeature\"]\n", "Each group of features will be a type of feature: characterBased, wordBased..."], "metadata": {"_uuid": "c6b66bf6a68c674b2da730b786899786db31df9d", "_cell_guid": "ba082d81-9c08-4bb2-8b64-c962a8a284a2"}, "cell_type": "markdown"}, {"source": ["class Feature:\n", "\n", "\tdef __init__(self, featureName, featureValue):\n", "\t\tself.name = featureName\n", "\t\tself.value = featureValue\n", "\n", "\tdef __repr__(self):\n", "\t\treturn str(self.value)\n", "\n", "class FeatureSet:\n", "\n", "\tdef __init__(self):\n", "\t\tself.featureDict = {}\n", "\n", "\tdef __repr__(self):\n", "\t\treturn str(self.featureDict)\n", "\n", "\tdef initFeatureType(self, featureType):\n", "\t\tself.featureDict[featureType] = {}\n", "\n", "\tdef addFeature(self, featureType, featureName, featureValue):\n", "\t\tself.featureDict[featureType][featureName] = Feature(featureName, featureValue)\n", "\n", "\tdef updateFeature(self, featureType, featureName, increment, operation=\"sum\"):\n", "\t\tif operation == \"sum\":\n", "\t\t\tself.featureDict[featureType][featureName].value += increment\n", "\t\telif operation == \"division\":\n", "\t\t\tself.featureDict[featureType][featureName].value /= increment\n", "\t\telse:\n", "\t\t\traise ValueError(\"Incorrect Operation\")\n", "\n", "\tdef getFeatureNames(self, featuresSelected=None):\n", "\t\tfeatureNames = []\n", "\t\t\n", "\t\tif featuresSelected is None:\n", "\t\t\tfeaturesSelected = self.featureDict.keys()\n", "\t\t\n", "\t\tfor featType in featuresSelected:\n", "\t\t\tfeatureNames.expand(self.featureDict[featType].keys())\n", "\n", "\t\treturn featureNames\n", "\n", "\tdef getFeatureTypeNames(self,featuresSelected=None):\n", "\t\tfeatureTypeNames = []\n", "\t\tif featuresSelected is None:\n", "\t\t\tfeaturesSelected = self.featureDict.keys()\n", "\n", "\t\tfor featType in featuresSelected:\n", "\t\t\tfor featName in self.featureDict[featType].keys():\n", "\t\t\t\tfeatureTypeNames.append((featType,featName))\n", "\n", "\t\treturn featureTypeNames\n", "\n", "\tdef getFeatureVector(self, featureTypeNames):\n", "\t\tfeatureVector = []\n", "\n", "\t\tfor featType, featName in featureTypeNames:\n", "\t\t\tfeatValue = self.featureDict[featType][featName].value\n", "\t\t\tfeatureVector.append(featValue)\n", "\n", "\t\treturn featureVector"], "outputs": [], "metadata": {"collapsed": true, "_uuid": "46cf5f2e8433d2a940cd18243d309c8e13cc6272", "_cell_guid": "70fad83e-3e85-4615-9e51-d596ad725fed"}, "execution_count": null, "cell_type": "code"}, {"source": ["Now, we have the classes that represent an Instance (a text transformed into a feature vector) and an Instance collection (the representation of a corpus in vectors.\n", "\n", "Each instance has a name (the file name), a FeatureSet, the correct label, the text itself, the tokens and sentences of the text and the tokens in lower case. This way, we have the tokenization and sentence splitting precomputed. The conll that represents the syntactic trees is also stored inside the instance (we will talk about conll later).\n", "\n", "The instance collection class, contains an array of instances, a instance dict (to directly access by name), and has  functions that, for instance, transform the instance collection to sklearn input format."], "metadata": {"_uuid": "6ed5292ef312382d315888ada07e88a02a6f057f", "_cell_guid": "dac1f33e-3ae9-44b0-9c9c-a42e19f70491"}, "cell_type": "markdown"}, {"source": ["from nltk import word_tokenize\n", "import codecs\n", "import nltk\n", "import os\n", "\n", "class Instance:\n", "\n", "\tdef __init__(self, name, label, paths):\n", "\t\tself.name = name\n", "\t\tself.featureSet = FeatureSet()\n", "\t\tself.label = label\n", "\t\ttokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\t\n", "\t\tself.paths = paths\n", "\t\tself.text = codecs.open(self.paths[\"clean\"],\"r\", encoding=\"utf-8\").read()\n", "\t\tif \"synParsed\" in self.paths:\n", "\t\t\tself.conll = codecs.open(self.paths[\"synParsed\"],\"r\", encoding=\"utf-8\").read()\n", "\t\telse:\n", "\t\t\tself.conll = None\n", "\t\tself.tokens = word_tokenize(self.text)\n", "\t\tself.lowerTokens = self.text.lower().split()\n", "\t\tself.sentences = tokenizer.tokenize(self.text)\n", "\n", "\tdef getFeaturenames(self, featuresSelected):\n", "\t\treturn self.featureSet.getFeaturenames(featuresSelected)\n", "\n", "\tdef getFeatureTypeNames(self, featuresSelected):\n", "\t\treturn self.featureSet.getFeatureTypeNames(featuresSelected)\n", "\n", "\tdef getFeatureVector(self, featuresSelected):\n", "\t\treturn self.featureSet.getFeatureVector(featuresSelected)\n", "\n", "\tdef initFeatureType(self, featureType):\n", "\t\tself.featureSet.initFeatureType(featureType)\n", "\n", "\tdef addFeature(self, featureType, featureName, featureValue):\n", "\t\tself.featureSet.addFeature(featureType, featureName, featureValue)\n", "\n", "\tdef updateFeature(self, featureType, featureName, increment, operation=\"sum\"):\n", "\t\tself.featureSet.updateFeature(featureType, featureName, increment, operation)\n", "\n", "\tdef __repr__(self):\n", "\t\treturn self.name + \"\\n\" + self.label+ \"\\n\" + str(self.featureSet) #+ \"\\n\"+str(self.tokens)\n", "\n", "\tdef getSklearnInput(self):\n", "\t\tfeatureTypeNames = self.getFeatureTypeNames(None)\n", "\t\tX = self.getFeatureVector(featureTypeNames)\n", "\t\treturn X, self.label, self.name.split(\"_\")[0]\n", "\n", "class InstanceCollection:\n", "\n", "\tdef __init__(self):\n", "\t\tself.instances = []\n", "\t\tself.labels = set()\n", "\t\tself.instanceDict = {}\n", "\t\tself.featurePath = \"path to store precomputed features\"\n", "\n", "\tdef __repr__(self):\n", "\t\tstrCollection = \"\"\n", "\t\tfor instance in self.instances:\n", "\t\t\tstrCollection += \"---------\\n\"+ str(instance) +\"\\n---------\"\n", "\t\treturn strCollection\n", "    \n", "\tdef initFeatureType(self, featureType):\n", "\t\tfor instance in self.instances:\n", "\t\t\tinstance.initFeatureType(featureType)\n", "\n", "\tdef addInstance(self, instance):\n", "\t\tself.instances.append(instance)\n", "\t\tself.instanceDict[instance.name] = instance\n", "\t\tself.labels.add(instance.label)\n", "\n", "\n", "\tdef getFeatureNames(self, featuresSelected):\n", "\t\treturn self.instances[0].getFeatureNames(featuresSelected)\n", "\n", "\tdef getFeatureTypeNames(self, featuresSelected):\n", "\t\treturn self.instances[0].getFeatureTypeNames(featuresSelected)\n", "\n", "\tdef getSklearnInput(self, featuresSelected = None):\n", "\t\tX = []\n", "\t\tY = []\n", "\n", "\t\tfeatureTypeNames = self.getFeatureTypeNames(featuresSelected)\n", "\n", "\t\tfor instance in self.instances:\n", "\t\t\tfeatureVector = instance.getFeatureVector(featureTypeNames)\n", "\t\t\tX.append(featureVector)\n", "\t\t\tY.append(instance.label)\n", "\n", "\t\treturn X, Y\n", "\n", "\tdef getMeanFeatValuesPerClass(self, featuresSelected=None):\n", "\t\tfeatureTypeNames = self.getFeatureTypeNames(featuresSelected)\n", "\t\tnFeats = len(featureTypeNames)\n", "\n", "\t\tdictPerClass = {}\n", "\n", "\t\tfor instance in self.instances:\n", "\t\t\tfeatureVector = instance.getFeatureVector(featureTypeNames)\n", "\t\t\tlabel = instance.label\n", "\t\t\tif label not in dictPerClass:\n", "\t\t\t\tdictPerClass[label] = np.array([featureVector],dtype=np.float64)\n", "\t\t\telse:\n", "\t\t\t\tdictPerClass[label] = np.append(dictPerClass[label],[featureVector],axis=0)\n", "\n", "\t\toutDict = {}\n", "\t\tfor label, matrix in dictPerClass.items():\n", "\t\t\ti=0\n", "\t\t\toutDict[label] = {}\n", "\t\t\twhile i < nFeats:\n", "\t\t\t\t\n", "\t\t\t\tfeatureValues = matrix[:,i]\n", "\t\t\t\tfeatureType, featureName = featureTypeNames[i]\n", "\t\t\t\t\n", "\t\t\t\tmean = np.mean(featureValues)\n", "\t\t\t\tmedian = np.median(featureValues)\n", "\t\t\t\tstd = np.std(featureValues)\n", "\t\t\t\t\n", "\t\t\t\toutDict[label][featureName] = {}\n", "\t\t\t\toutDict[label][featureName][\"mean\"] = mean\n", "\t\t\t\toutDict[label][featureName][\"median\"] = median\n", "\t\t\t\toutDict[label][featureName][\"std\"] = std\n", "\n", "\t\t\t\ti+=1\n", "\n", "\t\treturn outDict"], "outputs": [], "metadata": {"collapsed": true, "_uuid": "920a01d5a6c1ed085d7a14774f8af170ac304944", "_cell_guid": "ea45d5bd-5660-4055-a327-ee01e9bc69b3"}, "execution_count": null, "cell_type": "code"}, {"source": ["All right, so now we have this sort of class structure\n", "\n", "Instance Collection\n", "        Instance\n", "                FeatureSet\n", "\n", "Let us create an empty instance collection using some sample files that I uploaded (4 txt files with their corresponding 4 conll dependency parses) which contain literary texts from project gutenberg."], "metadata": {"_uuid": "e93a7818ee3362722a1da39f5171b26f1929565b", "_cell_guid": "ec76262a-220f-4540-bc82-57944e5ccc03"}, "cell_type": "markdown"}, {"source": ["\n", "def createInstanceCollection(path, labelPosition=1, separator = \"_\", selectedLabels = None):\n", "    iC = InstanceCollection()\n", "    for fname in os.listdir(path):\n", "        if fname.endswith(\".txt\"):\n", "            paths = {}\n", "            paths[\"clean\"] = path+fname\n", "            paths[\"synParsed\"] = path+fname.replace(\".txt\",\".conll\")\n", "            pieces = fname.split(separator)\n", "            label = pieces[labelPosition]\n", "            instance = Instance(fname, label, paths)\n", "            iC.addInstance(instance)\n", "\n", "    return iC\n", "\n", "labelPosition = 3\n", "path = \"../input/test-data/\"\n", "\n", "iC = createInstanceCollection(path,labelPosition)\n", "print(iC)"], "outputs": [], "metadata": {"_kg_hide-output": true, "_cell_guid": "2ac9a742-b959-4b0c-b41a-e017ad68f187", "_uuid": "6e0505632aac04070a9ae3be64759fb9aebf5364"}, "execution_count": null, "cell_type": "code"}, {"source": ["All right, now we have an empty instance collection. Lets fill it with some features.\n", "Each kind of feature has a specific class that computes them. \n", "We will start with the simplest group of features, the SentenceBasedFeatures.\n", "\n", "Each class that we want to define in this framework receives the instance collection and the model name (we could call this \"kaggle_author_identification\", for instance).\n", "In the initialization, we set the class basic info, such as the type of feature that this class contains.\n", "\n", "Then we have the words per sentence feature function, which computes the mean number of words per sentence, as well as the statistical range and standard deviation of this value for all of the instances in our instance collection."], "metadata": {"_uuid": "b3ba7b7d498a44c6c856c6ab47643efd5df621d5", "_cell_guid": "b4dd2701-5e32-40be-8fc0-e073f4053e44"}, "cell_type": "markdown"}, {"source": ["import numpy as np\n", "\n", "class SentenceBasedFeatures:\n", "\n", "\tdef __init__(self,iC, modelName):\n", "\t\tself.iC = iC\n", "\t\tself.type = \"SentenceBasedFeatures\"\n", "\t\tself.iC.initFeatureType(self.type)\n", "\t\tself.modelName = modelName\n", "\t\t\t\n", "\tdef get_wordsPerSentence_stdandrange(self):\n", "\t\tfor instance in self.iC.instances:\n", "\t\t\tsentences = instance.sentences\n", "\t\t\tlengths = []\n", "\t\t\tfor sentence in sentences:\n", "\t\t\t\tlengths.append(len(word_tokenize(sentence)))\n", "\t\t\t\n", "\t\t\tstd = np.std(lengths)\n", "\t\t\tmean = np.mean(lengths)\n", "\t\t\trng = np.amax(lengths) - np.amin(lengths)\n", "\n", "\t\t\tinstance.addFeature(self.type, self.type+\"_STD\", std)\n", "\t\t\tinstance.addFeature(self.type, self.type+\"_Range\", rng)\n", "\t\t\tinstance.addFeature(self.type, self.type+\"_wordsPerSentence\", mean)"], "outputs": [], "metadata": {"collapsed": true, "_kg_hide-output": false, "_cell_guid": "4a8174c3-2a4f-492f-aa6e-29a0e1b54c76", "_uuid": "659a55a065e1c8a012b80527fd0f61f4d84e1f3c"}, "execution_count": null, "cell_type": "code"}, {"source": ["Easy right? we do our thing, and then add the feature to the instance. Lets use this class and see how the features look like."], "metadata": {"_uuid": "0c7fcb930eb2620c3a339db221a3ba998c996d79", "_cell_guid": "36cd6f4a-bb6c-4e8a-9f25-4de37f84f0ef"}, "cell_type": "markdown"}, {"source": ["from pprint import pprint\n", "modelName = \"kaggle_author_identification\"\n", "iSent = SentenceBasedFeatures(iC,modelName)\n", "iSent.get_wordsPerSentence_stdandrange()\n", "pprint(iC)"], "outputs": [], "metadata": {"_kg_hide-output": true, "_cell_guid": "6de002c6-b364-4e4d-b2e9-839875fbfe16", "_uuid": "d3db37d908773dc48f0a334a18e8be09ad705f63"}, "execution_count": null, "cell_type": "code"}, {"source": ["All right, this is looking better. Let us add some character-based features, which are simple but stupidly effective. We can see how the class is very similar to the previous one (same initialization, but different feature functions)."], "metadata": {"_uuid": "da6102170056e5bda8b38bc08b525987720f5da3", "_cell_guid": "ff103f5c-5e78-4154-b326-59323fe73059"}, "cell_type": "markdown"}, {"source": ["import re\n", "class CharacterBasedFeatures:\n", "\n", "\tdef __init__(self,iC, modelName):\n", "\t\tself.iC = iC\n", "\t\tself.type = \"CharacterBasedFeatures\"\n", "\t\tself.iC.initFeatureType(self.type)\n", "\t\tself.modelName = modelName\n", "\n", "\tdef get_uppers(self):\n", "\t\tfor instance in self.iC.instances:\n", "\t\t\tfeatValue = 0.0\n", "\t\t\tmatches = re.findall(\"[A-Z]\",instance.text,re.DOTALL)\n", "\t\t\tupperCases = len(matches)\n", "\t\t\tratio = upperCases / len(instance.text)\n", "\t\t\tinstance.addFeature(self.type, self.type+\"_UpperCases\", ratio)\n", "\n", "\tdef get_in_parenthesis_stats(self):\n", "\t\tfor instance in self.iC.instances:\n", "\t\t\tmatches = re.findall(\"\\((.*?)\\)\", instance.text)\n", "\t\t\tnpar = len(matches)\n", "\t\t\ttotalchars = 0\n", "\t\t\ttotalwords = 0\n", "\n", "\t\t\tfor match in matches:\n", "\t\t\t\ttotalchars += len(match)\n", "\t\t\t\twords = word_tokenize(match)\n", "\t\t\t\ttotalwords = len(words)\n", "\n", "\t\t\tcharsInParenthesis = 0.0\n", "\t\t\twordsInParenthesis = 0.0\n", "\t\t\tif npar > 0:\n", "\t\t\t\tcharsInParenthesis = totalchars / npar\n", "\t\t\t\twordsInParenthesis = totalwords / npar\n", "\n", "\t\t\tinstance.addFeature(self.type, self.type+\"_charsinparenthesis\", charsInParenthesis)\n", "\t\t\tinstance.addFeature(self.type, self.type+\"_wordsinparenthesis\", wordsInParenthesis)\n", "\t\t\n", "\tdef get_numbers(self):\n", "\t\tfor instance in self.iC.instances:\n", "\t\t\tmatches = re.findall(\"[0-9]\", instance.text)\n", "\t\t\tratio = 0.0\n", "\t\t\tnchars = len(instance.text)\n", "\n", "\t\t\tif nchars > 0:\n", "\t\t\t\tratio = len(matches) / nchars\n", "\n", "\t\t\tinstance.addFeature(self.type, self.type+\"_Numbers\", ratio)\n", "\n", "\tdef get_symbols(self,symbols, featureName):\n", "\t\tfor instance in self.iC.instances:\n", "\t\t\tnChars = len(instance.text)\n", "\t\t\tmatches = 0\n", "\t\t\tratio = 0.0\n", "\t\t\t\n", "\t\t\tfor char in instance.text:\n", "\t\t\t\tif char in symbols:\n", "\t\t\t\t\tmatches = matches + 1\n", "\t\t\t\n", "\t\t\tif nChars > 0:\n", "\t\t\t\tratio = matches / nChars\n", "\n", "\t\t\tinstance.addFeature(self.type, self.type+\"_\"+featureName, ratio)      "], "outputs": [], "metadata": {"collapsed": true, "_uuid": "985fe41bde7071232bb1b8205053ab2839fbe9ec", "_cell_guid": "3f5ef311-7642-47dd-be9b-8f6fe965a868"}, "execution_count": null, "cell_type": "code"}, {"source": ["Lets now extract some character-based features."], "metadata": {"_uuid": "3979aafc200c11e69981a940447fde1bf41d28a5", "_cell_guid": "f0e58220-4f38-4072-be75-51d9cfafb03f"}, "cell_type": "markdown"}, {"source": ["iChar = CharacterBasedFeatures(iC,modelName)\n", "iChar.get_uppers()\n", "iChar.get_numbers()\n", "iChar.get_symbols([\",\"],\"commas\")\n", "iChar.get_symbols([\".\"],\"dots\")\n", "iChar.get_symbols(['?',\"\u00bf\"],\"questions\")\n", "iChar.get_symbols(['!','\u00a1'],\"exclamations\")\n", "iChar.get_symbols([\":\"],\"colons\")\n", "iChar.get_symbols([\";\"],\"semicolons\")\n", "iChar.get_symbols(['\"',\"'\",\"\u201d\",\"\u201c\", \"\u2019\"],\"quotations\")\n", "iChar.get_symbols([\"\u2014\",\"-\",\"_\"],\"hyphens\")\n", "iChar.get_symbols([\"(\",\")\"],\"parenthesis\")\n", "iChar.get_in_parenthesis_stats()\n", "print(iC)"], "outputs": [], "metadata": {"_kg_hide-output": true, "_cell_guid": "060713ee-8886-4bde-80e3-fbfccdf94d45", "_uuid": "c11aee496feaa84a9f71009e2aa81fe7e62ae4ac"}, "execution_count": null, "cell_type": "code"}, {"source": ["We have more features now, which is cool.  But I know you guys are pretty smart and you want to see more complex stuff.\n", "All right, let us talk about the syntactic features. First of all, disclaimer, all of this framework assumes that you have 2 things:\n", "- collection of raw texts\n", "- their conll files with the dependency parses\n", "So, before executing everything, you need to have everything parsed (I use mate-tools). \n", "Before going into more detail, lets see what kind of file is a conll file:\n", "\n", "\n"], "metadata": {"_uuid": "ce3c519d5f2bbb6bc176b66563d77a281cd6fd72", "_cell_guid": "7f413e0f-fa80-4460-ac3f-08b7e968f055"}, "cell_type": "markdown"}, {"source": ["conll = \"1\\tThe\\tthe\\tthe\\tDT\\tDT\\tend_string=3|spos=DT|start_string=0\\tspos=DT\\t2\\t2\\tNMOD\\tNMOD\\t_\\t_\\n2\\tman\\tman\\tman\\tNN\\tNN\\tend_string=7|number=SG|spos=NN|start_string=4\\tnumber=SG|spos=NN\\t5\\t5\\tSBJ\\tSBJ\\t_\\t_\\n3\\tin\\tin\\tin\\tIN\\tIN\\tend_string=10|spos=IN|start_string=8\\tspos=IN\\t2\\t2\\tNMOD\\tNMOD\\t_\\t_\\n4\\tblack\\tblack\\tblack\\tNN\\tNN\\tend_string=16|number=SG|spos=NN|start_string=11\\tnumber=SG|spos=NN\\t3\\t3\\tPMOD\\tPMOD\\t_\\t_\\n5\\tfled\\tflee\\tflee\\tVBD\\tVBD\\tend_string=21|finiteness=FIN|person=3|spos=VV|start_string=17|tense=PAST\\tfiniteness=FIN|person=3|spos=VV|tense=PAST\\t0\\t0\\tROOT\\tROOT\\t_\\t_\\n6\\tacross\\tacross\\tacross\\tIN\\tIN\\tend_string=28|spos=IN|start_string=22\\tspos=IN\\t5\\t5\\tLOC\\tLOC\\t_\\t_\\n7\\tthe\\tthe\\tthe\\tDT\\tDT\\tend_string=32|spos=DT|start_string=29\\tspos=DT\\t8\\t8\\tNMOD\\tNMOD\\t_\\t_\\n8\\tdesert\\tdesert\\tdesert\\tNN\\tNN\\tend_string=39|number=SG|spos=NN|start_string=33\\tnumber=SG|spos=NN\\t6\\t6\\tPMOD\\tPMOD\\t_\\t_\\n9\\t,\\t,\\t,\\t,\\t,\\tend_string=40|spos=,|start_string=39\\tspos=,\\t5\\t5\\tP\\tP\\t_\\t_\\n10\\tand\\tand\\tand\\tCC\\tCC\\tend_string=44|spos=CC|start_string=41\\tspos=CC\\t5\\t5\\tCOORD\\tCOORD\\t_\\t_\\n11\\tthe\\tthe\\tthe\\tDT\\tDT\\tend_string=48|spos=DT|start_string=45\\tspos=DT\\t12\\t12\\tNMOD\\tNMOD\\t_\\t_\\n12\\tgunslinger\\tgunslinger\\tgunslinger\\tNN\\tNN\\tend_string=59|number=SG|spos=NN|start_string=49\\tnumber=SG|spos=NN\\t13\\t13\\tSBJ\\tSBJ\\t_\\t_\\n13\\tfollowed\\tfollow\\tfollow\\tVBD\\tVBD\\tend_string=68|finiteness=FIN|person=3|spos=VV|start_string=60|tense=PAST\\tfiniteness=FIN|person=3|spos=VV|tense=PAST\\t10\\t10\\tCONJ\\tCONJ\\t_\\t_\\n14\\t.\\t.\\t.\\t.\\t.\\tend_string=69|spos=.|start_string=68\\tspos=.\\t5\\t5\\tP\\tP\\t_\\t_\\n\\n\"\n", "print(conll)"], "outputs": [], "metadata": {"collapsed": true, "_kg_hide-output": true, "_cell_guid": "9f418fb7-bdf2-4531-9dcb-22b8e09972fe", "_uuid": "9ca80dd823c17fd9e22cc8ccfd84a4cd354d029c"}, "execution_count": null, "cell_type": "code"}, {"source": ["All right, maybe that looks weird (bonus points for recognizing the sentence shown). Basically a conll file is a tab separated file that contains info such as the part of speech of each token, the syntactic dependencies that link two words, the lemma of the word, etc.  The important part, is that each sentence is represented as a tree in this file. Now, I'm going to show you the tree, which will look much nicer.\n", "\n", "![](https://i.imgur.com/DwFOZQ4.png)\n", "\n", "Much nicer right? (now you might now where the sentence comes from. If anyone talks about the movie, I'm going to be so pissed).\n", "\n", "Let us now see how we use this sort of files. Let me introduce the classes that manipulate the trees and extract our syntactic features.\n", "First, the Tree class. A Tree is a root node and a node dict. A node contains meta data, an array of nodes that are their children, a parent id, the id of the node and the label of the arc that reach the node. This is all info that can be found in the conll string. We can also see that the Node class has a subclass that is called SyntacticNode, which has specific characteristics found in the conll file.\n", "\n"], "metadata": {"_uuid": "2dcb5eb9cf39939a835bbd5534dd8289394de4e5", "_cell_guid": "5f77caec-5e20-4b42-bf99-e5c9700c3f9b"}, "cell_type": "markdown"}, {"source": ["class Tree:\n", "\n", "\tdef __init__(self, rootNode, nodeDict={}):\n", "\t\tself.nodeDict = nodeDict\n", "\t\tself.root = rootNode\n", "\n", "\t\tif self.root:\n", "\t\t\tif not self.root.id in self.nodeDict:\n", "\t\t\t\tself.nodeDict[self.root.id] = rootNode\n", "\n", "\tdef getDepthIterator(self, initNode = None):\n", "\t\tif not initNode:\n", "\t\t\tinitNode = self.root\n", "\t\t\n", "\t\tstack = []\n", "\t\tstack.append(initNode)\n", "\n", "\t\twhile stack:\n", "\t\t\tcurrent = stack.pop(0)\n", "\t\t\tif current:\n", "\t\t\t\tyield current\n", "\t\t\t\tfor child in current.children:\n", "\t\t\t\t\tstack.insert(0,child)\n", "\n", "\tdef getWidthIterator(self, initNode = None):\n", "\t\tif not initNode:\n", "\t\t\tinitNode = self.root\n", "\n", "\t\tqueue = []\n", "\t\tqueue.append(initNode)\n", "\n", "\t\twhile queue:\n", "\t\t\tcurrent = queue.pop()\n", "\t\t\tif current:\n", "\t\t\t\tyield current\n", "\t\t\t\tfor child in current.children:\n", "\t\t\t\t\tqueue.insert(0,child)\n", "\n", "\tdef __str__(self):\n", "\t\tstrRepr = \"\"\n", "\t\t\n", "\t\tqueue = []\n", "\t\tqueue.append(self.root)\n", "\t\tstrRepr += \"ROOT-> \"+ str(self.root.id) + \"\\n\"\n", "\t\ti = 1\n", "\t\twhile queue:\n", "\t\t\tcurrent = queue.pop()\n", "\t\t\tstrRepr += \"CHILDREN OF \"+str(current.id)+\" -> \"\n", "\t\t\tfor child in current.children:\n", "\t\t\t\tstrRepr += str(child.id) + \"\\t\"\n", "\t\t\t\tqueue.insert(0,child)\n", "\t\n", "\t\t\tstrRepr +=\"\\n\"\n", "\t\t\ti+=1\n", "\n", "\t\treturn strRepr\n", "\n", "class Node:\n", "\tdef __init__(self, meta, idNode, arcLabel, parentId):\n", "\t\tself.meta = meta\n", "\t\tself.children = []\n", "\t\tself.parent = parentId\n", "\t\tself.id = idNode\n", "\t\tself.arcLabel = arcLabel\n", "\n", "\tdef setParent(self, parentNode):\n", "\t\tself.parent = parentNode\n", "\n", "\tdef addChild(self, childNode):\n", "\t\tself.children.append(childNode)\n", "\n", "\tdef __str__(self):\n", "\t\tstrRepr = \"\"\n", "\t\tstrRepr += self.id + \" \" + self.meta + \" \" + self.arcLabel\n", "\t\treturn strRepr\n", "\n", "\n", "class SyntacticNode(Node):\n", "\n", "\tdef __init__(self, meta, idNode, arcLabel, parentId):\n", "\t\tself.meta = meta\n", "\t\tself.children = []\n", "\t\tself.parent = parentId\n", "\t\tself.id = idNode\n", "\t\tself.arcLabel = arcLabel\n", "\n", "\t\tpieces = meta.split(\"\\t\")\n", "\t\tself.word = pieces[1]\n", "\t\tself.lemma = pieces[2]\n", "\t\tself.pos = pieces[4]\n", "\t\tself.features = pieces[6]\n", "\t\tself.parentid = pieces[8]\n", "\n", "\tdef __str__(self):\n", "\t\tstrRepr = \"\"\n", "\t\tstrRepr += self.word + \" \" + self.pos + \" \" + self.features + \" \" + self.parentid + \" \" + self.arcLabel\n", "\t\treturn strRepr"], "outputs": [], "metadata": {"collapsed": true, "_uuid": "5eea2c3021573a352c0d04ff45af7b973026af94", "_cell_guid": "5ca17f50-8670-4174-b24c-0b8419d417b8"}, "execution_count": null, "cell_type": "code"}, {"source": ["Now, I present the TreeOperations class and SyntacticTreeOperations subclass, which uses the Tree class and has many functions that extract information from the syntactic tree."], "metadata": {"_uuid": "1d566bfb955f96f0a8b235ce2dcb58c2d27be99e", "_cell_guid": "ec4bf356-32a6-4779-8777-6c80e01d3680"}, "cell_type": "markdown"}, {"source": ["class TreeOperations:\n", "\n", "\tdef __init__(self, conllStringSentence):\n", "\t\tconllStringSentence = conllStringSentence.strip()\n", "\t\tif not conllStringSentence:\n", "\t\t\traise ValueError(\"Please input a correct conll sentence\")\n", "\t\t\treturn\n", "\t\tself.tree = self.conll_to_tree(conllStringSentence)\n", "\n", "\tdef conll_to_tree(self, conllString):\n", "\t\tconllArray = conllString.split(\"\\n\")\n", "\t\tnodes, root = self.create_nodes(conllArray)\n", "\t\tself.link_nodes(nodes)\n", "\t\treturn Tree(root, nodes)\n", "\n", "\tdef create_nodes(self, conllArray):\n", "\t\tnodeDict = {}\n", "\t\troot = None\n", "\n", "\t\tfor line in conllArray:\n", "\t\t\tpieces = line.split(\"\\t\")\n", "\t\t\tidNode = int(pieces[0])\n", "\t\t\tarcLabel = pieces[10]\n", "\t\t\tparentId = int(pieces[9])\n", "\t\t\tiNode = Node(line, idNode, arcLabel, parentId)\n", "\t\t\tnodeDict[idNode] = iNode\n", "\t\t\tif parentId == 0:\n", "\t\t\t\troot = iNode\n", "\n", "\t\treturn nodeDict, root\n", "\n", "\n", "\tdef link_nodes(self, nodeDict):\n", "\t\tfor idNode, iNode in nodeDict.items():\n", "\t\t\tif iNode.parent > 0:\n", "\t\t\t\tiParent = nodeDict[iNode.parent]\n", "\t\t\t\tiParent.addChild(iNode)\n", "\t\t\t\tiNode.setParent(iParent)\n", "\n", "\t\n", "\tdef get_ramification_factor(self, initNode = None):\n", "\t\tif initNode:\n", "\t\t\tit = self.tree.getWidthIterator(initNode)\n", "\t\telse:\n", "\t\t\tit = self.tree.getWidthIterator()\n", "\n", "\t\tacumChilds = 0\n", "\t\tlevels = 1\n", "\t\tfor current in it:\n", "\t\t\tnchilds = len(current.children)\n", "\t\t\tif nchilds > 0:\n", "\t\t\t\tacumChilds+=nchilds\n", "\t\t\t\tlevels+=1\n", "\n", "\t\treturn acumChilds / levels\n", "\n", "\tdef get_max_width(self, initNode = None):\n", "\t\tit = self.tree.getWidthIterator(initNode)\n", "\t\tmaxWidth = 0\n", "\n", "\t\tfor current in it:\n", "\t\t\tnchilds = len(current.children)\n", "\t\t\tif nchilds > maxWidth:\n", "\t\t\t\tmaxWidth = nchilds\n", "\n", "\t\treturn maxWidth\n", "\n", "\tdef get_max_depth(self, initNode = None):\n", "\t\tif not initNode:\n", "\t\t\tinitNode = self.tree.root\n", "\n", "\t\treturn self.get_max_depth_recursive(initNode)\n", "\n", "\n", "\tdef get_max_depth_recursive(self, node):\n", "\t\tdepth = []\n", "\n", "\t\tif node:\n", "\t\t\tif not node.children:\n", "\t\t\t\treturn 0\n", "\t\tif not node:\n", "\t\t\treturn 0\n", "\t\t\n", "\t\tfor child in node.children:\n", "\t\t\tdepth.append(self.get_max_depth_recursive(child))\n", "\n", "\t\treturn 1 + max(depth)\n", "\n", "\tdef get_node_depth(self, node):\n", "\t\tcurrent = node\n", "\t\tdepth = 0\n", "\t\twhile current.parent:\n", "\t\t\tdepth+=1\n", "\t\t\tcurrent = current.parent\n", "\t\treturn depth\n", "\n", "class SyntacticTreeOperations(TreeOperations):\n", "\n", "\t'''\n", "\t\tGets the maximum width and depth below a node that has a given relation \n", "\t\twith its father. EX: For every subordinate clause, we get the maximum value\n", "\t\tof width and depth of the subtree BELOW the node which has a SUB relation with its father.\n", "\t'''\n", "\tdef get_relation_width_depth(self, relation):\n", "\t\t\n", "\t\tit = self.tree.getWidthIterator()\n", "\t\twidthDepths = []\n", "\n", "\t\tfor current in it:\n", "\t\t\tif current.arcLabel == relation:\n", "\t\t\t\twidth = self.get_max_width(current)\n", "\t\t\t\tdepth = self.get_max_depth(current)\n", "\t\t\t\twidthDepths.append((width,depth))\n", "\n", "\t\treturn widthDepths\n", "\n", "\tdef get_relation_depth_level(self, relation):\n", "\t\tit = self.tree.getWidthIterator()\n", "\t\tlevels = []\n", "\t\tfor current in it:\n", "\t\t\tif current.arcLabel == relation:\n", "\t\t\t\tlevel = self.get_node_depth(current)\n", "\t\t\t\tlevels.append(level)\n", "\n", "\t\treturn levels\n", "\n", "\tdef get_relation_ramification_factor(self, relation):\n", "\t\tit = self.tree.getWidthIterator()\n", "\t\tramFactors = []\n", "\t\tfor current in it:\n", "\t\t\tif current.arcLabel == relation:\n", "\t\t\t\tramFactor = self.get_ramification_factor(current)\n", "\t\t\t\tramFactors.append(ramFactor)\n", "\n", "\t\treturn ramFactors\n", "\n", "\tdef search_deps_frequency(self, searchedRels = []):\n", "\n", "\t\tit = self.tree.getWidthIterator()\n", "\t\trelFreq = {}\n", "\t\tsearchAll = False\n", "\t\tif not searchedRels:\n", "\t\t\tsearchAll = True\n", "\n", "\t\ttotal = 0\n", "\t\tfor current in it:\n", "\t\t\tif current:\n", "\t\t\t\tfor child in current.children:\n", "\t\t\t\t\tif child.arcLabel in searchedRels or searchAll:\n", "\t\t\t\t\t\tif child.arcLabel in relFreq:\n", "\t\t\t\t\t\t\trelFreq[child.arcLabel] +=1\n", "\t\t\t\t\t\telse:\n", "\t\t\t\t\t\t\trelFreq[child.arcLabel] =1\n", "\t\t\t\t\t\ttotal+=1\n", "\n", "\t\treturn relFreq, total\n", "\n", "\tdef search_pos_frequency(self, searchedPos = []):\n", "\t\tit = self.tree.getWidthIterator()\n", "\t\tposFreq = {}\n", "\n", "\t\tsearchAll = False\n", "\t\tif not searchedPos:\n", "\t\t\tsearchAll = True\n", "\n", "\t\ttotal = 0\n", "\t\tfor current in it:\n", "\t\t\tif current:\n", "\t\t\t\tfor child in current.children:\n", "\t\t\t\t\tif child.pos in searchedPos or searchAll:\n", "\t\t\t\t\t\tif child.pos in posFreq:\n", "\t\t\t\t\t\t\tposFreq[child.pos] +=1\n", "\t\t\t\t\t\telse:\n", "\t\t\t\t\t\t\tposFreq[child.pos] =1\n", "\t\t\t\t\t\ttotal+=1\n", "\n", "\t\treturn posFreq, total\n", "\n", "\tdef get_composed_verb_ratio(self):\n", "\t\tverbTags = [\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\", \"MD\"]\n", "\t\tverbFreq, total = self.search_pos_frequency(verbTags)\n", "\t\tdepFreq, vcFreq = self.search_deps_frequency([\"VC\"])\n", "\n", "\t\tif vcFreq > 0 and total > 0:\n", "\t\t\tcomposedVerbRatio = vcFreq / total\n", "\t\telse:\n", "\t\t\tcomposedVerbRatio = 0.0\n", "\n", "\t\treturn composedVerbRatio\n", "\n", "\tdef get_modal_ratio(self):\n", "\t\tverbTags = [\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\", \"MD\"]\n", "\t\tverbFreq, total = self.search_pos_frequency(verbTags)\n", "\n", "\t\tif total > 0 and \"MD\" in verbFreq:\n", "\t\t\tmodalRatio = verbFreq[\"MD\"]/ total\n", "\t\telse:\n", "\t\t\tmodalRatio = 0.0\n", "\n", "\t\treturn modalRatio\n", "\n", "\tdef create_nodes(self, conllArray):\n", "\t\t\n", "\t\tnodeDict = {}\n", "\t\troot = None\n", "\t\tfor line in conllArray:\n", "\t\t\tpieces = line.split(\"\\t\")\n", "\t\t\tidNode = int(pieces[0])\n", "\t\t\tarcLabel = pieces[11]\n", "\t\t\tparentId = int(pieces[9])\n", "\n", "\t\t\tiNode = SyntacticNode(line, idNode, arcLabel, parentId)\n", "\t\t\tnodeDict[idNode] = iNode\n", "\t\t\tif parentId == 0:\n", "\t\t\t\troot = iNode\n", "\n", "\t\treturn nodeDict, root"], "outputs": [], "metadata": {"collapsed": true, "_uuid": "9160d1bdf2ca5a59824a1e1a22416f0b3d8deee2", "_cell_guid": "68397565-bde6-4c06-8176-7e0aa16b11a3"}, "execution_count": null, "cell_type": "code"}, {"source": ["All right, this might seem a bit convoluted. Lets see how we use all of this stuff. Presenting, the SyntacticFeatures class."], "metadata": {"_uuid": "1962e8d5034cab8817410237a2453246d59e3a38", "_cell_guid": "bacc8b59-faaf-4c7b-afd8-b430f9ad46ec"}, "cell_type": "markdown"}, {"source": ["class SyntacticFeatures:\n", "\n", "\tadverbialRelations = [\"ADV\",\"TMP\",\"LOC\",\"DIR\",\"MNR\",\"PRP\",\"EXT\"]\n", "\tmodifierRelations = [\"NMOD\",\"PMOD\",\"AMOD\"]\n", "\n", "\tverbTags = [\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\", \"MD\"]\n", "\tnounTags = [\"NN\",\"NNS\",\"NNP\",\"NNPS\"]\n", "\tadverbTags = [\"RB\",\"RBR\",\"RBS\",\"WRB\"]\n", "\tadjectiveTags = [\"JJ\",\"JJR\",\"JJS\"]\n", "\tpronounTags = [\"PRP\",\"PRP$\",\"WP\",\"WP$\"]\n", "\tdeterminerTags = [\"DT\",\"PDT\",\"WDT\"]\n", "\tconjunctionTags = [\"CC\",\"IN\"]\n", "\n", "\tsuperlatives = [\"JJS\",\"RBS\"]\n", "\tcomparatives = [\"JJR\",\"RBR\"]\n", "\t\n", "\tpastVerbs = [\"VBD\",\"VBN\"]\n", "\tpresentVerbs = [\"VBG\",\"VBP\",\"VBZ\"]\n", "\n", "\tdef __init__(self,iC, modelName, load=True):\t\n", "\t\t\n", "\t\tself.iC = iC\n", "\t\tself.type = \"SyntacticFeatures\"\n", "\t\tself.iC.initFeatureType(self.type)\n", "\t\tself.allRelationsPos = open(\"../input/dictss/allRelationsPos.txt\",\"r\").read().split(\"\\n\")\n", "\t\tself.modelName = modelName\n", "\t\tself.load = load\n", "\n", "\tdef compute_syntactic_features(self):\n", "\t\tnPosts = len(self.iC.instances)\n", "\t\tnProcessed = 0\n", "\t\tfor instance in self.iC.instances:\n", "\t\t\tconllSents = instance.conll.split(\"\\n\\n\")\n", "\t\t\tiTrees = []\n", "\t\t\tconllSents = conllSents[:-1]\n", "\t\t\tfor conllSent in conllSents:\n", "\t\t\t\ttry:\n", "\t\t\t\t\tiTree = SyntacticTreeOperations(conllSent)\n", "\t\t\t\t\tiTrees.append(iTree)\n", "\t\t\t\texcept ValueError as e:\n", "\t\t\t\t\tcontinue\n", "\n", "\t\t\tself.get_relation_usage(iTrees, instance)\n", "\t\t\tself.get_relationgroup_usage(iTrees, instance)\n", "\t\t\tself.get_pos_usage(iTrees, instance)\n", "\t\t\tself.get_posgroup_usage(iTrees, instance)\n", "\t\t\t\n", "\t\t\tself.get_shape_features(iTrees, instance)\n", "\t\t\tself.get_subcoord_features(iTrees, instance)\n", "\t\t\tself.get_verb_features(iTrees, instance)\n", "\t\t\tnProcessed +=1\n", "\n", "\t\tself.adjust_features()\n", "\t\t\n", "\t#to be used after get_relation_usage and get_pos_usage\n", "\tdef adjust_features(self):\n", "\t\tfor instance in self.iC.instances:\n", "\t\t\tfor featName in self.allRelationsPos:\n", "\t\t\t\tif featName not in instance.featureSet.featureDict[\"SyntacticFeatures\"]:\n", "\t\t\t\t\tinstance.addFeature(self.type, featName, 0.0)\n", "\n", "\tdef get_relation_usage(self, iTrees, instance):\n", "\t\tnTrees = len(iTrees)\n", "\t\tfor iTree in iTrees:\n", "\t\t\tdepFreq,_ = iTree.search_deps_frequency()\n", "\t\t\tfor dep, freq in depFreq.items():\n", "\t\t\t\tif \"SYNDEP_\"+ dep in self.allRelationsPos:\t\n", "\t\t\t\t\tif \"SYNDEP_\"+ dep not in instance.featureSet.featureDict[\"SyntacticFeatures\"].keys():\n", "\t\t\t\t\t\tinstance.addFeature(self.type, \"SYNDEP_\"+dep, 0.0)\n", "\t\t\t\t\t\n", "\t\t\t\t\tinstance.updateFeature(self.type, \"SYNDEP_\"+dep, freq / nTrees)\n", "\n", "\t\t\t\t\n", "\n", "\tdef get_relationgroup_usage(self,iTrees, instance):\n", "\t\tnTrees = len(iTrees)\n", "\t\tinstance.addFeature(self.type, \"SYNDEP_modifierRelations\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNDEP_adverbialRelations\", 0.0)\n", "\n", "\t\tfor iTree in iTrees:\n", "\t\t\tdepFreq, total = iTree.search_deps_frequency(self.adverbialRelations)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNDEP_adverbialRelations\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_deps_frequency(self.modifierRelations)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNDEP_modifierRelations\", total / nTrees)\n", "\n", "\n", "\tdef get_posgroup_usage(self, iTrees, instance):\n", "\t\tnTrees = len(iTrees)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_verbTags\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_nounTags\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_adverbTags\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_adjectiveTags\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_pronounTags\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_determinerTags\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_conjunctionTags\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_superlatives\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_comparatives\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_pastVerbs\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNPOS_presentVerbs\", 0.0)\n", "\n", "\n", "\t\tfor iTree in iTrees:\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.verbTags)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_verbTags\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.nounTags)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_nounTags\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.adverbTags)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_adverbTags\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.adjectiveTags)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_adjectiveTags\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.pronounTags)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_pronounTags\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.determinerTags)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_determinerTags\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.conjunctionTags)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_conjunctionTags\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.superlatives)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_superlatives\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.comparatives)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_comparatives\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.pastVerbs)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_pastVerbs\", total / nTrees)\n", "\n", "\t\t\tdepFreq, total = iTree.search_pos_frequency(self.presentVerbs)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNPOS_presentVerbs\", total / nTrees)\n", "\n", "\n", "\tdef get_pos_usage(self,iTrees, instance):\n", "\t\tnTrees = len(iTrees)\n", "\t\tfor iTree in iTrees:\n", "\t\t\tposFreq, _ = iTree.search_pos_frequency()\n", "\t\t\tfor pos, freq in posFreq.items():\n", "\t\t\t\tif \"SYNPOS_\"+ pos in self.allRelationsPos:\n", "\t\t\t\t\tif \"SYNPOS_\"+pos not in instance.featureSet.featureDict[\"SyntacticFeatures\"]:\n", "\t\t\t\t\t\tinstance.addFeature(self.type, \"SYNPOS_\"+pos, 0.0)\n", "\n", "\t\t\t\t\tinstance.updateFeature(self.type, \"SYNPOS_\"+pos, freq / nTrees)\n", "\n", "\n", "\tdef get_shape_features(self,iTrees, instance):\n", "\t\tnTrees = len(iTrees)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_width\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_depth\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_ramFactor\", 0.0)\n", "\n", "\t\tfor iTree in iTrees:\n", "\t\t\tramFact = iTree.get_ramification_factor()\n", "\t\t\twidth = iTree.get_max_width()\n", "\t\t\tdepth = iTree.get_max_depth()\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_width\", width / nTrees)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_depth\", depth / nTrees)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_ramFactor\", ramFact / nTrees)\n", "\n", "\tdef get_subcoord_features(self, iTrees, instance):\n", "\t\tnSubs = 0\n", "\t\tnCoords = 0\n", "\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_subDepth\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_subWidth\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_subRamFact\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_subLevel\", 0.0)\n", "\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_coordDepth\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_coordWidth\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_coordRamFact\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_coordLevel\", 0.0)\n", "\n", "\n", "\t\tfor iTree in iTrees:\n", "\t\t\tsubFreq, numS =  iTree.search_deps_frequency([\"SUB\"])\n", "\t\t\tif subFreq:\n", "\t\t\t\tnSubs += numS\n", "\n", "\t\t\tcoordFreq, numC =  iTree.search_deps_frequency([\"COORD\"])\n", "\t\t\tif coordFreq:\n", "\t\t\t\tnCoords += numC\n", "\n", "\t\t\twidthDepth = iTree.get_relation_width_depth(\"SUB\")\n", "\t\t\tif widthDepth:\n", "\t\t\t\tincrementW = sum([pair[0] for pair in widthDepth]) / len(widthDepth)\n", "\t\t\t\tincrementD = sum([pair[1] for pair in widthDepth]) / len(widthDepth)\n", "\n", "\t\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_subWidth\", incrementW)\n", "\t\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_subDepth\", incrementD)\n", "\n", "\t\t\tramFactors = iTree.get_relation_ramification_factor(\"SUB\")\n", "\t\t\tif ramFactors:\n", "\t\t\t\tincrementR = np.sum(np.array(ramFactors)) / len(ramFactors)\n", "\t\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_subRamFact\", incrementR)\n", "\n", "\t\t\tlevels = iTree.get_relation_depth_level(\"SUB\")\n", "\t\t\tif levels:\n", "\t\t\t\tincrementSL = np.sum(np.array(levels)) / len(levels)\n", "\t\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_subLevel\", incrementSL)\n", "\n", "\t\t\twidthDepth = iTree.get_relation_width_depth(\"COORD\")\n", "\t\t\tif widthDepth:\n", "\t\t\t\tincrementCW = sum([pair[0] for pair in widthDepth]) / len(widthDepth)\n", "\t\t\t\tincrementCD = sum([pair[1] for pair in widthDepth]) / len(widthDepth)\n", "\n", "\t\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_coordWidth\", incrementCW)\n", "\t\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_coordDepth\", incrementCD)\n", "\n", "\t\t\tramFactors = iTree.get_relation_ramification_factor(\"COORD\")\n", "\t\t\tif ramFactors:\n", "\t\t\t\tincrementCR = np.sum(np.array(ramFactors)) / len(ramFactors)\n", "\t\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_coordRamFact\", incrementCR)\n", "\n", "\t\t\tlevels = iTree.get_relation_depth_level(\"COORD\")\n", "\t\t\tif levels:\n", "\t\t\t\tincrementCL = np.sum(np.array(levels)) / len(levels)\n", "\t\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_coordLevel\", incrementCL)\n", "\n", "\t\tif nSubs > 0:\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_subDepth\", nSubs, \"division\")\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_subWidth\", nSubs, \"division\")\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_subRamFact\", nSubs, \"division\")\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_subLevel\", nSubs, \"division\")\n", "\n", "\t\tif nCoords > 0:\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_coordDepth\", nCoords, \"division\")\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_coordWidth\", nCoords, \"division\")\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_coordRamFact\", nCoords, \"division\")\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_coordLevel\", nCoords, \"division\")\n", "\n", "\tdef get_verb_features(self, iTrees, instance):\n", "\t\tnTrees = len(iTrees)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_composedVerbRatio\", 0.0)\n", "\t\tinstance.addFeature(self.type, \"SYNSHAPE_modalRatio\", 0.0)\n", "\n", "\t\tfor iTree in iTrees:\n", "\t\t\tcomposedVerbRatio = iTree.get_composed_verb_ratio()\n", "\t\t\tmodalRatio = iTree.get_modal_ratio()\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_composedVerbRatio\", composedVerbRatio / nTrees)\n", "\t\t\tinstance.updateFeature(self.type, \"SYNSHAPE_modalRatio\", modalRatio / nTrees)\n"], "outputs": [], "metadata": {"collapsed": true, "_uuid": "6b4d760ace2920178174a4d1f41412cb6d00cb92", "_cell_guid": "0f998d72-511c-41ac-b2ca-2a213e5e0bde"}, "execution_count": null, "cell_type": "code"}, {"source": ["Lets add these bad boys to our feature set."], "metadata": {"_uuid": "ef5745390cd7cd7bb822153384d52f23f3c47d7f", "_cell_guid": "d5869dd5-706e-49f3-b19e-fdf07e636036"}, "cell_type": "markdown"}, {"source": ["iSyntactic = SyntacticFeatures(iC,modelName)\n", "iSyntactic.compute_syntactic_features()\n", "pprint(iC)"], "outputs": [], "metadata": {"_kg_hide-output": true, "_cell_guid": "e803b0e9-0a87-4860-a210-d40f6d0c8567", "_uuid": "bbcc9e98b9c63492090d1363c5cf5bc4c60717cd"}, "execution_count": null, "cell_type": "code"}, {"source": ["Now, we have a TON of features. Everything we computed can be used to classify. An easy way to do it, is to get the sklearn input and train a classifier that can be used to predict the class of an unseen instance."], "metadata": {"_uuid": "15f455e09668eddb9c5dd921c86fd0b53cd0197d", "_cell_guid": "f2154ab9-605d-4a19-b94f-4acaff2dbe44"}, "cell_type": "markdown"}, {"source": ["from sklearn.svm import SVC\n", "X, Y = iC.getSklearnInput()\n", "clfLinear = SVC(C=1.0, kernel=\"linear\", gamma='auto', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False)\n", "clfLinear.fit(X, Y)\n", "\n", "path = \"../input/testtest/\"\n", "labelPosition = 3\n", "\n", "iCTest = createInstanceCollection(path,labelPosition)\n", "\n", "iSent = SentenceBasedFeatures(iCTest,modelName)\n", "iSent.get_wordsPerSentence_stdandrange()\n", "\n", "iChar = CharacterBasedFeatures(iCTest,modelName)\n", "iChar.get_uppers()\n", "iChar.get_numbers()\n", "iChar.get_symbols([\",\"],\"commas\")\n", "iChar.get_symbols([\".\"],\"dots\")\n", "iChar.get_symbols(['?',\"\u00bf\"],\"questions\")\n", "iChar.get_symbols(['!','\u00a1'],\"exclamations\")\n", "iChar.get_symbols([\":\"],\"colons\")\n", "iChar.get_symbols([\";\"],\"semicolons\")\n", "iChar.get_symbols(['\"',\"'\",\"\u201d\",\"\u201c\", \"\u2019\"],\"quotations\")\n", "iChar.get_symbols([\"\u2014\",\"-\",\"_\"],\"hyphens\")\n", "iChar.get_symbols([\"(\",\")\"],\"parenthesis\")\n", "iChar.get_in_parenthesis_stats()\n", "\n", "iSyntactic = SyntacticFeatures(iCTest,modelName)\n", "iSyntactic.compute_syntactic_features()\n", "\n", "for instance in iCTest.instances:\n", "\tX, _ , idx = instance.getSklearnInput()\n", "\tprint(clfLinear.predict([X]).tolist())\n"], "outputs": [], "metadata": {"_kg_hide-output": false, "_cell_guid": "e369677b-99b4-4989-9faf-ca54399207cc", "_uuid": "73e5a6c3a1c2be6ccccc1a70ea741702b08b7c96"}, "execution_count": null, "cell_type": "code"}, {"source": ["And as you see, the system tells us that the test instance is from arthur conan doyle, which in fact, it is actually true. \n", "Now you know how my framework and my approach works. If you want to use the full version of the code, please help yourselves, [Link to code](https://github.com/joanSolCom/author_profiling_tools/tree/master/author_profiling_code) .\n", "\n", "If you are interested in my research and you want to read my articles, visit my researchgate: [Link to researchgate profile](https://www.researchgate.net/profile/Juan_Soler_Company). "], "metadata": {"_uuid": "1a391517043fb0f4a9fbe34af11a7c367e7585d7", "_cell_guid": "bab53669-df99-4a93-9407-59099f78f5e9"}, "cell_type": "markdown"}], "nbformat": 4, "nbformat_minor": 1}