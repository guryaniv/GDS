{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "147802a5-1f0c-80da-6ca5-cc2b69218b5e"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2b54c7f9-54a8-9b24-c068-616de394ed2f"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import gensim, logging\n",
        "from gensim.models import word2vec\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "simbolos = ['.','.','...','@','$','(',')','\"',':',';','?']\n",
        "%matplotlib inline\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "pal = sns.color_palette()\n",
        "from subprocess import check_output\n",
        "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "37956a50-b4b5-e362-1245-e8e517b256cc"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('../input/train.csv')\n",
        "df_test = pd.read_csv('../input/test.csv')\n",
        "df_part = df_train[0:2500]\n",
        "len(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e0929273-838d-6aa5-ea25-03e9de557de8"
      },
      "outputs": [],
      "source": [
        "#comparador de palabras y procesado simple de texto: tokenizador, filtrado(lower y alfanumerico) \n",
        "# y limpieza(no stops) \n",
        "#probar despues con el stemer y el lemmatizador\n",
        "\n",
        "def same_word_ratio_2(row):\n",
        "    \n",
        "    ratiow = []\n",
        "    #question1 = row['question1'].values.tolist()\n",
        "    #question2 = row['question2'].values.tolist()\n",
        "    for i in range(len(row)):\n",
        "        n_words = 0     \n",
        "        q1 = (pd.Series(row['question1'][i]).astype(str))[0]\n",
        "        q2 = (pd.Series(row['question2'][i]).astype(str))[0] \n",
        "        if q2 == []: \n",
        "            q2 =\"\"\n",
        "        if q1 == []:\n",
        "            q1 =\"\" \n",
        "        q_tokens1 = word_tokenize(q1) #tokenizar frase por frase, las frases con sus tokens separados\n",
        "        q_tokens2 = word_tokenize(q2)\n",
        "        \n",
        "        filtered_tokens1 = [token.lower() for token in q_tokens1 if token.isalnum()]#para limpiar las frases(minusculas y alfanumeric)\n",
        "        filtered_tokens2 = [token.lower() for token in q_tokens2 if token.isalnum()]\n",
        "        \n",
        "        clean_tokens1 = [token for token in filtered_tokens1 if token not in stops] #quitar stopwords\n",
        "        clean_tokens2 = [token for token in filtered_tokens2 if token not in stops]\n",
        "        \n",
        "            #meter lo de word2vec--->>> most_similar() y model.wv.simila\n",
        "        \n",
        "        if (len(clean_tokens1) + len(clean_tokens2)) == 0:\n",
        "            r = len(set(clean_tokens1) & set(clean_tokens2))\n",
        "        else:         \n",
        "            r = len(set(clean_tokens1) & set(clean_tokens2))/(len(set(clean_tokens1)) + len(set(clean_tokens2)))\n",
        "    \n",
        "        ratiow.append(r)\n",
        "        \n",
        "    return ratiow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad192701-61eb-28b5-d78f-21e2ef4716cc"
      },
      "outputs": [],
      "source": [
        "def get_clean_tokens(df, question):\n",
        "     \n",
        "    clean_tokens = []\n",
        "    #question1 = row['question1'].values.tolist()\n",
        "    #question2 = row['question2'].values.tolist()\n",
        "    for i in range(len(df)):    \n",
        "        q = (pd.Series(df[question][i]).astype(str))[0] \n",
        "        q_tokens = word_tokenize(q) #tokenizar frase por frase, las frases con sus tokens separados        \n",
        "        filtered_tokens = [token.lower() for token in q_tokens if token not in simbolos  ]#para limpiar las frases(minusculas y alfanumeric)\n",
        "        cleans = [token for token in filtered_tokens if token not in stops] #quitar stopwords\n",
        "        clean_tokens.append(cleans)\n",
        "        \n",
        "        \n",
        "    return clean_tokens\n",
        "#devuelve una matriz con los clean tokens por frases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0bd2a658-46dd-6b86-e750-3683cad0e8cd"
      },
      "outputs": [],
      "source": [
        "#same_word_raio_2 reducida->>>>\n",
        "\n",
        "def same_word_ratio(df):#df:matriz de datos (df_train)\n",
        "    \n",
        "    ratiow = []\n",
        "    clean_tokens1 = get_clean_tokens(df, 'question1')\n",
        "    clean_tokens2 = get_clean_tokens(df, 'question2')\n",
        "    for i in range(len(clean_tokens1)):\n",
        "        \n",
        "        if (len(clean_tokens1[i]) + len(clean_tokens2[i])) == 0:\n",
        "            r = len(set(clean_tokens1[i]) & set(clean_tokens2[i]))\n",
        "        else:         \n",
        "            r = len(set(clean_tokens1[i]) & set(clean_tokens2[i]))/(len(set(clean_tokens1[i])) + len(set(clean_tokens2[i])))\n",
        "    \n",
        "        ratiow.append(r)\n",
        "        \n",
        "    return ratiow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4c7834a5-7f33-3635-d94c-5cd63fba1a33"
      },
      "outputs": [],
      "source": [
        "q1 = (pd.Series(df_part['question1'][42]).astype(str))[0]   \n",
        "q2 = (pd.Series(df_part['question2'][42]).astype(str))[0]   \n",
        "x = df_part['question1'].values.tolist()\n",
        "y = df_part['question2'].values.tolist()\n",
        "words_q1 = (\" \".join(q1)).lower().split()\n",
        "print(x[42])\n",
        "q11 = word_tokenize(q1)\n",
        "print(q11)\n",
        "filt1 = [token.lower() for token in q11 if token.isalnum()]\n",
        "print(filt1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b8dc77ed-9918-c715-80fb-b0494b4129af"
      },
      "outputs": [],
      "source": [
        "q_tokens = word_tokenize(q) #tokenizar frase por frase, las frases con sus tokens separados        \n",
        "filtered_tokens = [token.lower() for token in q_tokens if token.isalnum()]#para limpiar las frases(minusculas y alfanumeric)\n",
        "cleans = [token for token in filtered_tokens if token not in stops] #quitar stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e13b4f85-da20-f29f-a819-67a2e0f528ce"
      },
      "outputs": [],
      "source": [
        "clean_tokens1 = get_clean_tokens(df_part, 'question1')\n",
        "clean_tokens2 = get_clean_tokens(df_part, 'question2')\n",
        "\n",
        "print(clean_tokens1[42])\n",
        "print(clean_tokens2[42])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "172efc2c-7609-d640-54ed-36c4b1fdb889"
      },
      "outputs": [],
      "source": [
        "num = '50,000'\n",
        "if(num.isalnum()):\n",
        "    print('siiiiiiiiiiiiiiiiiiiiii')\n",
        "else:\n",
        "    print('nooooooooooooooooooo')\n",
        "print(num.lower())\n",
        "\n",
        "num.isalnum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a5bdfdbe-64dd-41a1-9f78-e885f6fe4c6b"
      },
      "outputs": [],
      "source": [
        "#FUNCI\u00d3N DE WORD2VEC\n",
        "def word2vec_sim(df):\n",
        "    ratios = []\n",
        "    clean1_redu = []\n",
        "    clean2_redu = []\n",
        "    clean1 = get_clean_tokens(df, 'question1')\n",
        "    clean2 = get_clean_tokens(df, 'question2')\n",
        "    \n",
        "    \n",
        "    for c in range(len(clean1)):\n",
        "        count = 0\n",
        "        sim = 0\n",
        "        dif = 0\n",
        "        for i in range(len(clean1[c])):\n",
        "            for j in range(len(clean2[c])):\n",
        "                \n",
        "                try:\n",
        "                    if(model.similarity(clean1[c][i], clean2[c][j] ) == 1):\n",
        "                        clean1.remove(clean1[c][i])\n",
        "                        clean2.remove(clean2[c][j])\n",
        "\n",
        "                except KeyError:\n",
        "                    pass \n",
        "                    \n",
        "        minimo = min(len(clean2[c]), len(clean1[c])) \n",
        "        if (minimo == 0):\n",
        "            ratio = 0\n",
        "            sim = 0\n",
        "            dif = 0\n",
        "        else:\n",
        "            ratio = count / minimo\n",
        "            sim = sim /minimo\n",
        "            dif = dif / minimo\n",
        "        ratios.append(dif)\n",
        "        #ratios.append(sim)\n",
        "        \n",
        "    return clean1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "517c689e-2a4d-a15c-dd6c-7abfc77abc64"
      },
      "outputs": [],
      "source": [
        "cleans = word2vec_sim(df_part)\n",
        "print(cleans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7936fe36-9708-9916-d8a4-c3f157dfa6cd"
      },
      "outputs": [],
      "source": [
        "#pruebas same_word_ratio reducida\n",
        "ratios = same_word_ratio(df_part) #matriz de ratios para decidir 1 o 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4090b083-6a89-78f2-f60d-e3141471963a"
      },
      "outputs": [],
      "source": [
        "print(ratios[53])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "54b3e7a7-b36f-3c12-10df-d8fc4db12767"
      },
      "outputs": [],
      "source": [
        "#PRUEBA PARA EL WORD2VEC\n",
        "\"\"\"\n",
        "def word2vec_sim(clean1, clean2):\n",
        "    ratios = []\n",
        "    count = 0\n",
        "    for c in range(len(clean1)):\n",
        "        for i in range(len(clean1[c])):\n",
        "            for j in range(len(clean2[c])):\n",
        "                if(model.wv.similarity(clean1[c][i], clean2[c][j] ) > 0.7):\n",
        "                    count = count + 1\n",
        "        minimo = min(len(clean2[c]), len(clean1[c])) \n",
        "        if (minimo == 0):\n",
        "            ratio = 0\n",
        "        else:\n",
        "            ratio = count / minimo\n",
        "            \n",
        "        ratios.append(ratio)\n",
        "        \n",
        "    return ratios \n",
        "    \n",
        "\"\"\"                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "54a71d2b-e274-de7c-472a-4e7d0fddb0b0"
      },
      "outputs": [],
      "source": [
        "def decision(matriz_est, umbral):\n",
        "    decision = {} #tabla con los datos estimados, duplicados o no.\n",
        "    #matriz_est: matriz de \"scores\" de la que se saca una matriz decision (ratiow)\n",
        "    for index in range(len(matriz_est)):\n",
        "        if (matriz_est[index] > umbral):\n",
        "            decision[index] = 1\n",
        "        else:\n",
        "            decision[index] = 0\n",
        "    #print (decision)\n",
        "    return decision\n",
        "\n",
        "def porcentaje_acierto(matriz_dec, matriz_df): \n",
        "    acierto =  {} #tabla para porcentajes de aciertos en la estimaci\u00f3n\n",
        "    counter = 0\n",
        "    for index in range(len(matriz_dec)):\n",
        "        if (matriz_dec[index] == matriz_df['is_duplicate'][index]):\n",
        "            acierto[index]=1\n",
        "            counter = counter +1\n",
        "        else:\n",
        "            acierto[index]=0\n",
        "        \n",
        "    porcentaje = (counter*100)/len(matriz_df) \n",
        "    return porcentaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "158feec3-5940-c891-ab45-2eef2afad7d4"
      },
      "outputs": [],
      "source": [
        "ratiow = same_word_ratio_2(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e334b164-5d9d-210e-6f80-b4eb2a1341e0"
      },
      "outputs": [],
      "source": [
        "decision = decision(ratiow, 0.25)\n",
        "porcentaje = porcentaje_acierto(decision, df_train) \n",
        "print (porcentaje)\n",
        "#0.25 es el umbral ideal para df_part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "68310fd3-cc39-4d72-6dcc-bf9f9f3d976b"
      },
      "outputs": [],
      "source": [
        "x = [0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9]\n",
        "y = [0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0,3]\n",
        "for w in x:\n",
        "    decision = decision(ratiow, w)\n",
        "    p = porcentaje_acierto(decision,df_train)\n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e551845c-3ab3-94d6-9f72-1eac8e5c822b"
      },
      "source": [
        "**FUNCIONES PARA OBTENER LAS NO DETECCIONES Y LAS FALSAS ALARMAS M\u00c1S GRAVES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "27b2ada1-6475-01ba-eb0c-bf355c8a89c5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def no_detections_graves(ratios, df_x ): #ratios: una matriz de decision con ratios calculados\n",
        "                               #df_x: matriz original con las preguntas\n",
        "    \n",
        "    no_detections = {}\n",
        "    no_detections_q = {}\n",
        "    no_detections_q1 = {}\n",
        "    no_detections_q2 = {}\n",
        "    no_detections_q3 = {}\n",
        "    counter = 0\n",
        "    \n",
        "    for index in range(len(ratios)):\n",
        "        if ((ratios[index] < 0.15) and (df_x['is_duplicate'][index] == 1)):\n",
        "            no_detections[index] = 1\n",
        "            #no_detections_q2 = pd.DataFrame({'id': df_x['test_id'][index], 'question1': df_x['question1'][index], 'question2': df_x['question2'][index]})\n",
        "            no_detections_q1[index] = df_x['question1'][index]\n",
        "            no_detections_q2[index] = df_x['question2'][index]\n",
        "            no_detections_q3[index] = ratios[index]\n",
        "\n",
        "            counter = counter + 1\n",
        "        else:\n",
        "            no_detections[index] = 0\n",
        "            \n",
        "    #no_detections_q = pd.DataFrame({'question1': no_detections_q1, 'question2': no_detections_q2, 'ratio': no_detections_q3})   \n",
        "    no_detections_q['question1'] = no_detections_q1\n",
        "    no_detections_q['question2'] = no_detections_q2\n",
        "    no_detections_q['ratio'] = no_detections_q3\n",
        "\n",
        "    return no_detections_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c040ba5-a819-f2c7-6573-3e687e72e73f"
      },
      "outputs": [],
      "source": [
        "def falsas_alarmas_graves(ratios, df_x ): #ratios: una matriz de decision con ratios calculados\n",
        "                               #df_x: matriz original con las preguntas\n",
        "    \n",
        "    falsa_alarma = {}\n",
        "    falsa_alarma_q = {}\n",
        "    falsa_alarma_q1 = {}\n",
        "    falsa_alarma_q2 = {}\n",
        "    falsa_alarma_q3 = {}\n",
        "    counter = 0\n",
        "    \n",
        "    for index in range(len(ratios)):\n",
        "        if ((ratios[index] > 0.46) and (df_x['is_duplicate'][index] == 0)):\n",
        "            falsa_alarma[index] = 1\n",
        "            #falsa_alarma_q2 = pd.DataFrame({'id': df_x['test_id'][index], 'question1': df_x['question1'][index], 'question2': df_x['question2'][index]})\n",
        "            falsa_alarma_q1[index] = df_x['question1'][index]\n",
        "            falsa_alarma_q2[index] = df_x['question2'][index]\n",
        "            falsa_alarma_q3[index] = ratios[index]\n",
        "            counter = counter + 1\n",
        "        else:\n",
        "            falsa_alarma[index]=0\n",
        "            \n",
        "    #falsa_alarma_q = pd.DataFrame({'question1': falsa_alarma_q1, 'question2': falsa_alarma_q2, 'ratio': falsa_alarma_q3})\n",
        "    falsa_alarma_q['question1'] = falsa_alarma_q1\n",
        "    falsa_alarma_q['question2'] = falsa_alarma_q2\n",
        "    falsa_alarma_q['ratio'] = falsa_alarma_q3\n",
        "    return falsa_alarma_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "24e12cf6-914e-456c-045b-edd980159f91"
      },
      "outputs": [],
      "source": [
        "no_detections_g = no_detections_graves(ratios , df_part)\n",
        "falsas_alarmas_g = falsas_alarmas_graves(ratios, df_part )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d0ffaa29-b749-538a-a25f-7e2f2e01d176"
      },
      "outputs": [],
      "source": [
        "#no_detections_g\n",
        "falsas_alarmas_g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ffc0539f-d9f2-ec8c-d44b-d61946a8c424"
      },
      "outputs": [],
      "source": [
        "ratiow_test = same_word_ratio_2(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a3c640c7-a751-23d5-4790-f10593601e98"
      },
      "outputs": [],
      "source": [
        "decision_test = decision(ratiow_test, 0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d27a307-d65e-7d1e-0595-78be82f8474b"
      },
      "outputs": [],
      "source": [
        "decision_test = decision(ratiow_test, 0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "41a11a0f-d6ab-7089-513e-689c64e32767"
      },
      "outputs": [],
      "source": [
        "#submission = pd.DataFrame({'test_id': df_test['id'],'is_duplicate': decision_test})\n",
        "submission = pd.DataFrame({'test_id': df_train['id'], 'is_duplicate': decision})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "#sub = pd.DataFrame()\n",
        "#sub['test_id'] = df_test['id']\n",
        "#sub['is_duplicate'] = decision_test\n",
        "#sub.to_csv('simple_counw.csv', index=False)\n",
        "submission.head()"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}