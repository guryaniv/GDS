{"cells":[{"metadata":{"_uuid":"038de4bc15a508865e0166981678280bd13458c3"},"cell_type":"markdown","source":"**Learning rate test:  Cosine annealing learning rate scheduler with periodic restarts.**\n\nKeras Callback for implementing Stochastic Gradient Descent with Restarts\n\nhttps://gist.github.com/jeremyjordan/5a222e04bb78c242f5763ad40626c452\n\nRef: https://arxiv.org/pdf/1608.03983.pdf"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import sys\nimport random\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\nimport time\nt_start = time.time()\n%matplotlib inline\n\n# import cv2\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm_notebook #, tnrange\n#from itertools import chain\nfrom skimage.io import imread, imshow #, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom keras.models import Model, load_model, save_model\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add, Flatten, Dense\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam, SGD\nfrom keras import backend as K\nfrom keras import optimizers\nimport tensorflow as tf\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img,save_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf25dea3209c809868d54c4e4f046fbb24b7be6"},"cell_type":"code","source":"version = 1\nbasic_name = f'LR_test_study_v{version}'\nsave_model_name = basic_name + '.model'\nsubmission_file = basic_name + '.csv'\n\nprint(save_model_name)\nprint(submission_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35bdafc01382dedb738935d0b5c1957bb7c23c8b"},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 101\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76064cac97236b7d7c4b025d50f08a339fdb0070"},"cell_type":"code","source":"# Loading of training/testing ids and depths\ntrain_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\n\nprint(len(train_df),len(depths_df),len(test_df)) \n#train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30e9712d795fa39f237349d45cada1b262f88b6a"},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), color_mode = \"grayscale\"))/255  for idx in tqdm_notebook(train_df.index)]\ntrain_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), color_mode = \"grayscale\"))/255  for idx in tqdm_notebook(train_df.index)]\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e01057b8cc44c0e8854fd3cda7b42b5bc4baeaa"},"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\ndef cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce37d97ec0ce0565971e7ee419c08b9f4cc11795"},"cell_type":"code","source":"# Create train/validation split stratified by salt coverage\nids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.1, stratify=train_df.coverage_class, random_state= 44)\nprint(x_train.shape,y_train.shape,x_valid.shape,y_valid.shape)\nprint(np.mean(x_train),np.mean(y_train),np.std(x_train),np.std(y_train))\nprint(np.mean(x_valid),np.mean(y_valid),np.std(x_valid),np.std(y_valid))\nprint(np.max(x_train),np.max(y_train),np.max(x_valid),np.max(y_valid))\nprint(np.min(x_train),np.min(y_train),np.min(x_valid),np.min(y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3f4dee806630cbffe06f496e2895b1f9ad23f08"},"cell_type":"code","source":"def BatchActivate(x):\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = BatchActivate(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16, batch_activate = False):\n    x = BatchActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    if batch_activate:\n        x = BatchActivate(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7133e0eb3ee904f8d9d05aad85932215a4250590"},"cell_type":"code","source":"# Build model\ndef build_model(input_layer, start_neurons, DropoutRatio = 0.05):\n    # 101 -> 50\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(input_layer)\n    conv1 = residual_block(conv1,start_neurons * 1)\n    conv1 = residual_block(conv1,start_neurons * 1, True)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(DropoutRatio/2)(pool1)\n\n    # 50 -> 25\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n    conv2 = residual_block(conv2,start_neurons * 2)\n    conv2 = residual_block(conv2,start_neurons * 2, True)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(DropoutRatio)(pool2)\n\n    # 25 -> 12\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n    conv3 = residual_block(conv3,start_neurons * 4)\n    conv3 = residual_block(conv3,start_neurons * 4, True)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(DropoutRatio)(pool3)\n\n    # 12 -> 6\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n    conv4 = residual_block(conv4,start_neurons * 8)\n    conv4 = residual_block(conv4,start_neurons * 8, True)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(DropoutRatio)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 16)\n    convm = residual_block(convm,start_neurons * 16, True)\n    \n    # 6 -> 12\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(DropoutRatio)(uconv4)\n    \n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 8)\n    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n    \n    # 12 -> 25\n    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])    \n    uconv3 = Dropout(DropoutRatio)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 4)\n    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n\n    # 25 -> 50\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n        \n    uconv2 = Dropout(DropoutRatio)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 2)\n    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n\n    # 50 -> 101\n    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    \n    uconv1 = Dropout(DropoutRatio)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 1)\n    uconv2 = residual_block(uconv1,start_neurons * 1, True)\n\n    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv2)\n    output_layer =  Activation('sigmoid')(output_layer_noActi)\n    \n    return output_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5d610af56c3495634c9a0e89229d1a0b5f27a59"},"cell_type":"code","source":"def get_iou_vector(A, B):\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch]>0, B[batch]>0\n#         if np.count_nonzero(t) == 0 and np.count_nonzero(p) > 0:\n#             metric.append(0)\n#             continue\n#         if np.count_nonzero(t) >= 1 and np.count_nonzero(p) == 0:\n#             metric.append(0)\n#             continue\n#         if np.count_nonzero(t) == 0 and np.count_nonzero(p) == 0:\n#             metric.append(1)\n#             continue\n        \n        intersection = np.logical_and(t, p)\n        union = np.logical_or(t, p)\n        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n\n    return np.mean(metric)\n\ndef my_iou_metric(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n\ndef my_iou_metric_2(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9880abec3a5003d3d95dbcf83cf57b63cb666850"},"cell_type":"code","source":"# code download from: https://github.com/bermanmaxim/LovaszSoftmax\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=\"loss\"\n                   )\n    return loss\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vscores, vlabels\n\ndef lovasz_loss(y_true, y_pred):\n    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n    #logits = K.log(y_pred / (1. - y_pred))\n    logits = y_pred #Jiaxin\n    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85452a9351639530050ef840652e6f51dc39bc1a"},"cell_type":"code","source":"#Implementation\n#Both finding the optimal range of learning rates and assigning a learning rate schedule can be implemented quite trivially using Keras Callbacks.\n#Finding the optimal learning rate range\n#We can write a Keras Callback which tracks the loss associated with a learning rate varied linearly over a defined range.\nfrom keras.callbacks import Callback\nimport matplotlib.pyplot as plt\n\nclass LRFinder(Callback):\n    \n    '''\n    A simple callback for finding the optimal learning rate range for your model + dataset. \n    \n    # Usage\n        ```python\n            lr_finder = LRFinder(min_lr=1e-5, \n                                 max_lr=1e-2, \n                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n                                 epochs=3)\n            model.fit(X_train, Y_train, callbacks=[lr_finder])\n            \n            lr_finder.plot_loss()\n        ```\n    \n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n        \n    # References\n        Blog post: jeremyjordan.me/nn-learning-rate\n        Original paper: https://arxiv.org/abs/1506.01186\n    '''\n\n    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n        super().__init__()\n        \n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.total_iterations = steps_per_epoch * epochs\n        self.iteration = 0\n        self.history = {}\n        \n    def clr(self):\n        '''Calculate the learning rate.'''\n        x = self.iteration / self.total_iterations \n        return self.min_lr + (self.max_lr-self.min_lr) * x\n        \n    def on_train_begin(self, logs=None):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.min_lr)\n        \n    def on_batch_end(self, epoch, logs=None):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.iteration += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iteration)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n            \n        K.set_value(self.model.optimizer.lr, self.clr())\n    \n    def plot_lr(self):\n        '''Helper function to quickly inspect the learning rate schedule.'''\n        plt.plot(self.history['iterations'], self.history['lr'])\n        plt.yscale('log')\n        plt.xlabel('Iteration')\n        plt.ylabel('Learning rate')\n        \n    def plot_loss(self):\n        '''Helper function to quickly observe the learning rate experiment results.'''\n        plt.plot(self.history['lr'], self.history['loss'])\n        plt.xscale('log')\n        plt.xlabel('Learning rate')\n        plt.ylabel('Loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20eceb5d2283db037b90dea70a54ee7405cf42fb"},"cell_type":"code","source":"from keras.callbacks import Callback\nimport keras.backend as K\nimport numpy as np\n\nclass SGDRScheduler(Callback):\n    '''Cosine annealing learning rate scheduler with periodic restarts.\n    # Usage\n        ```python\n            schedule = SGDRScheduler(min_lr=1e-5,\n                                     max_lr=1e-2,\n                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n                                     lr_decay=0.9,\n                                     cycle_length=5,\n                                     mult_factor=1.5)\n            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n        ```\n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n        lr_decay: Reduce the max_lr after the completion of each cycle.\n                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n        cycle_length: Initial number of epochs in a cycle.\n        mult_factor: Scale epochs_to_restart after each full cycle completion.\n    # References\n        Blog post: jeremyjordan.me/nn-learning-rate\n        Original paper: http://arxiv.org/abs/1608.03983\n    '''\n    def __init__(self,\n                 min_lr,\n                 max_lr,\n                 steps_per_epoch,\n                 lr_decay=1,\n                 cycle_length=10,\n                 mult_factor=2):\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.lr_decay = lr_decay\n        self.batch_since_restart = 0\n        self.next_restart = cycle_length\n        self.steps_per_epoch = steps_per_epoch\n        self.cycle_length = cycle_length\n        self.mult_factor = mult_factor\n        self.history = {}\n        \n    def clr(self):\n        '''Calculate the learning rate.'''\n        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n        return lr\n\n    def on_train_begin(self, logs={}):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.max_lr)\n\n    def on_batch_end(self, batch, logs={}):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        self.batch_since_restart += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_epoch_end(self, epoch, logs={}):\n        '''Check for end of current cycle, apply restarts when necessary.'''\n        if epoch + 1 == self.next_restart:\n            self.batch_since_restart = 0\n            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n            self.next_restart += self.cycle_length\n            self.max_lr *= self.lr_decay\n            self.best_weights = self.model.get_weights()\n            \n    def on_train_end(self, logs={}):\n        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n        self.model.set_weights(self.best_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4140a51a004a558c53a4cbe432f8d17ef0d03e4"},"cell_type":"code","source":"x_trn, x_val, y_trn, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=79)\nprint(x_trn.shape,y_trn.shape,x_val.shape,y_val.shape)\nprint(x_train.shape,y_train.shape,x_valid.shape,y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1467c656775f443808ed2b3b34a06fcd8e465c9"},"cell_type":"code","source":"# LR Search -- ResUnet with binary_crossentropy loss\nmt1 = time.time()\ninput_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16,0.5)\nmodel = Model(input_layer, output_layer)\n\nreduce_lr = ReduceLROnPlateau(monitor='my_iou_metric', mode = 'max',factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n\nepochs = 5\nbatch_size = 32\nepoch_size = len(x_trn)\nlr=0.01\nlr_finder = LRFinder(min_lr=1e-5, \n                    max_lr=1e-2, \n                    steps_per_epoch=np.ceil(epoch_size/batch_size), \n                    epochs=epochs)\n\n#optimizer = SGD(lr=lr, momentum=0.8, decay=0.001, nesterov=False)\noptimizer = optimizers.adam(lr = lr)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[my_iou_metric])\n\nhistory = model.fit(x_trn, y_trn,\n                    validation_data=[x_val, y_val], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[lr_finder,reduce_lr], \n                    verbose=2)\nlr_finder.plot_loss()\nmt2 = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f4e23f7bbba81db020c88673fb1bfed9d23d387"},"cell_type":"code","source":"# Test1-1 -- ResUnet with binary_crossentropy loss\n# With cosine annealing learning rate scheduler with periodic restarts\n\nmt1 = time.time()\nsave_model_name = basic_name + '.model1'\ninput_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16,0.25)\nmodel = Model(input_layer, output_layer)\n\nmodel_checkpoint = ModelCheckpoint(save_model_name,monitor='my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='my_iou_metric', mode = 'max',factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\nepochs = 30\nbatch_size = 32\nepoch_size = len(x_trn)\nlr=0.01\nschedule = SGDRScheduler(min_lr=1e-4,\n                        max_lr=1e-2,\n                        steps_per_epoch=np.ceil(epoch_size/batch_size),\n                        lr_decay=0.9,\n                        cycle_length=5,\n                        mult_factor=1.5)\n\n#optimizer = SGD(lr=lr, momentum=0.8, decay=0.001, nesterov=False)\noptimizer = optimizers.adam(lr = lr)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[my_iou_metric])\n\nhistory = model.fit(x_trn, y_trn,\n                    validation_data=[x_val, y_val], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[schedule,reduce_lr, model_checkpoint], \n                    verbose=2)\nmt2 = time.time()\nprint(f\"Test1-1 {epochs,batch_size} runtime = {(mt2-mt1)/60} mins\")\nprint(\"Evaluation on X_valid:\", model.evaluate(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1915cf1d5e515580c51ce9540dd5827b69f3a8f4"},"cell_type":"code","source":"#Test1-1 Performance\nfig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_loss.legend()\nplt.title('ResUNet: Binary Crossentropy Loss')\nax_score.plot(history.epoch, history.history[\"my_iou_metric\"], label=\"Train score\")\nax_score.plot(history.epoch, history.history[\"val_my_iou_metric\"], label=\"Validation score\")\nax_score.legend()\nplt.title('Cosine annealing learning rate scheduler with periodic restarts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"202d107970486ed27e20fbb02b552a5de99f7db9"},"cell_type":"code","source":"save_model_name = basic_name + '.model1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04e1b37b1312f12bfc5f152b4b55b27139381b06"},"cell_type":"code","source":"# Test1-2: continue with Lovasz Loss, LR Search\nmt1 = time.time()\nmodel = load_model(save_model_name,custom_objects={'my_iou_metric': my_iou_metric})\ninput_x = model.layers[0].input\noutput_layer = model.layers[-1].input\nmodel = Model(input_x, output_layer)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric_2', mode = 'max',factor=0.5, patience=2, min_lr=0.00001, verbose=1)\nepochs = 30\nbatch_size = 32\nepoch_size = len(x_trn)\nlr=0.001\nlr_finder = LRFinder(min_lr=1e-5, \n                    max_lr=1e-2, \n                    steps_per_epoch=np.ceil(epoch_size/batch_size), \n                    epochs=epochs)\n\nc = optimizers.adam(lr = lr)\nmodel.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\nhistory = model.fit(x_trn, y_trn,\n                    validation_data=[x_val, y_val], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[lr_finder,reduce_lr], \n                    verbose=2)\nlr_finder.plot_loss()\nmt2 = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"041d3bc480910796ca0d1f8fa95d0b6e58ef0fdd"},"cell_type":"code","source":"# Test1-2: continue with Lovasz Loss\nmt1 = time.time()\nmodel = load_model(save_model_name,custom_objects={'my_iou_metric': my_iou_metric})\ninput_x = model.layers[0].input\noutput_layer = model.layers[-1].input\nmodel = Model(input_x, output_layer)\n\nsave_model_name = basic_name + '.model2'\nearly_stopping = EarlyStopping(monitor='val_my_iou_metric_2', mode = 'max',patience=20, verbose=1)\nmodel_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric_2', \n                                   mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric_2', mode = 'max',factor=0.5, patience=2, min_lr=0.00001, verbose=1)\nepochs = 30\nbatch_size = 32\nepoch_size = len(x_trn)\nlr=0.001\nschedule = SGDRScheduler(min_lr=1e-5,\n                        max_lr=1e-3,\n                        steps_per_epoch=np.ceil(epoch_size/batch_size),\n                        lr_decay=0.9,\n                        cycle_length=5,\n                        mult_factor=1.5)\nc = optimizers.adam(lr = lr)\nmodel.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\nhistory = model.fit(x_trn, y_trn,\n                    validation_data=[x_val, y_val], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[schedule,reduce_lr,model_checkpoint], \n                    verbose=2)\nmt2 = time.time()\nprint(f\"Test1-2 {epochs,batch_size} runtime = {(mt2-mt1)/60} mins\")\nprint(\"Evaluation on X_valid:\", model.evaluate(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76ee0d2dddb134821e73fcf6db0267090bc4361d"},"cell_type":"code","source":"#Test1-2 Performance\nfig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_loss.legend()\nplt.title('ResUNet: Continue on Lovasz Loss')\nax_score.plot(history.epoch, history.history[\"my_iou_metric_2\"], label=\"Train score\")\nax_score.plot(history.epoch, history.history[\"val_my_iou_metric_2\"], label=\"Validation score\")\nax_score.legend()\nplt.title('Cosine annealing learning rate scheduler with periodic restarts')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0398b2874a48dc25e7d7516c40aff69c04dc3ff6"},"cell_type":"markdown","source":"For comparison, test2 does not have SGDR schedule "},{"metadata":{"trusted":true,"_uuid":"5820313c4059ab91debaadd842326d83d513f058"},"cell_type":"code","source":"# Test2-1 -- ResUnet with binary_crossentropy loss\n# Without cosine annealing learning rate scheduler with periodic restarts\n\nmt1 = time.time()\nsave_model_name = basic_name + '.model3'\ninput_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16,0.25)\nmodel = Model(input_layer, output_layer)\n\nmodel_checkpoint = ModelCheckpoint(save_model_name,monitor='my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='my_iou_metric', mode = 'max',factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\nepochs = 30\nbatch_size = 32\nepoch_size = len(x_trn)\nlr=0.01\n\noptimizer = optimizers.adam(lr = lr)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[my_iou_metric])\n\nhistory = model.fit(x_trn, y_trn,\n                    validation_data=[x_val, y_val], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[reduce_lr, model_checkpoint], \n                    verbose=2)\nmt2 = time.time()\nprint(f\"Test2-1 {epochs,batch_size} runtime = {(mt2-mt1)/60} mins\")\nprint(\"Evaluation on X_valid:\", model.evaluate(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52f8727b05983f095ade5ffa3444b23cf6dec426"},"cell_type":"code","source":"#Test2-1 Performance\nfig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_loss.legend()\nplt.title('ResUNet: Binary Crossentropy Loss')\nax_score.plot(history.epoch, history.history[\"my_iou_metric\"], label=\"Train score\")\nax_score.plot(history.epoch, history.history[\"val_my_iou_metric\"], label=\"Validation score\")\nax_score.legend()\nplt.title('Without cosine annealing learning rate scheduler with periodic restarts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7b24c0d38e3d2887f169909c86a8ec0e15ed2b4"},"cell_type":"code","source":"# Test2-2: continue with Lovasz Loss\nmt1 = time.time()\nmodel = load_model(save_model_name,custom_objects={'my_iou_metric': my_iou_metric})\ninput_x = model.layers[0].input\noutput_layer = model.layers[-1].input\nmodel = Model(input_x, output_layer)\n\nsave_model_name = basic_name + '.model2'\nearly_stopping = EarlyStopping(monitor='val_my_iou_metric_2', mode = 'max',patience=20, verbose=1)\nmodel_checkpoint = ModelCheckpoint(save_model_name,monitor='val_my_iou_metric_2', \n                                   mode = 'max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_my_iou_metric_2', mode = 'max',factor=0.5, patience=2, min_lr=0.00001, verbose=1)\nepochs = 30\nbatch_size = 32\nepoch_size = len(x_trn)\nlr=0.001\nc = optimizers.adam(lr = lr)\nmodel.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\nhistory = model.fit(x_trn, y_trn,\n                    validation_data=[x_val, y_val], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[reduce_lr,model_checkpoint], \n                    verbose=2)\nmt2 = time.time()\nprint(f\"Test2-2 {epochs,batch_size} runtime = {(mt2-mt1)/60} mins\")\nprint(\"Evaluation on X_valid:\", model.evaluate(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2008659ee38e0175d9fcbc01d45f918b8b428b96"},"cell_type":"code","source":"#Test2-2 Performance\nfig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_loss.legend()\nplt.title('ResUNet: Lavasz Loss')\nax_score.plot(history.epoch, history.history[\"my_iou_metric_2\"], label=\"Train score\")\nax_score.plot(history.epoch, history.history[\"val_my_iou_metric_2\"], label=\"Validation score\")\nax_score.legend()\nplt.title('Without cosine annealing learning rate scheduler with periodic restarts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"206ad5e7c183da191e5b1391ce5d56a0640b2124"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}