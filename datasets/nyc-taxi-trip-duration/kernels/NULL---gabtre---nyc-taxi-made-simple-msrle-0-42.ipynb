{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "version": "3.6.1", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "cells": [{"outputs": [], "cell_type": "code", "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime as dt # to handle dates and time\nfrom datetime import datetime, timedelta, date\n\nfrom functools import reduce\n\n# data visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#import cufflinks as cf\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.", "metadata": {"_uuid": "60c7f00f3b7319146a88830c6768c654c3f3369b", "_cell_guid": "025439d3-3b80-44f3-b262-92d2cb736d76", "trusted": false}, "execution_count": 1}, {"outputs": [], "cell_type": "code", "source": "# sklearn\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\nfrom sklearn.preprocessing import Imputer, OneHotEncoder, LabelBinarizer, StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb", "metadata": {"collapsed": true, "_uuid": "fc40f50750624ae63d967b2350fdc54ac1eff8d0", "_cell_guid": "977cfca7-8c84-4e7a-b8ca-f005e9c3abd5", "trusted": false}, "execution_count": 2}, {"outputs": [], "cell_type": "markdown", "source": " ## Custom Functions", "metadata": {"_uuid": "70936cc02cfa8c97ce26d48fce65fadfa0bf2883", "_cell_guid": "b3597393-7f58-4381-b8ea-40204724fa7c"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "def mean_cross_score(x):\n    print(\"Accuracy: %0.3f +/- %0.3f\" % (np.mean(x), np.std(x)))", "metadata": {"collapsed": true, "_uuid": "040731d5a590fd7ebe7d51932d052741aa2b67ce", "_cell_guid": "66d77bf0-9aa4-476e-a073-5a6016534bc4", "trusted": false}, "execution_count": 3}, {"outputs": [], "cell_type": "code", "source": "class DataFrameSelector(BaseEstimator, TransformerMixin): # to select dataframes in a pipeline.\n    def __init__(self, attributes): \n        self.attributes = attributes\n    def fit(self, df, y=None): \n        return self\n    def transform(self, df):\n        return df[self.attributes].values", "metadata": {"collapsed": true, "_uuid": "5af4394767b2ab19943d54d1c68286dc47d5d76e", "_cell_guid": "f98382b2-c543-40a1-b642-3e94217d714e", "trusted": false}, "execution_count": 4}, {"outputs": [], "cell_type": "code", "source": "def RemoveOutliers(df,cols,n_sigma): # keep only instances that are within p\\m n_sigma in columns cols\n    new_df = df.copy()\n    for col in cols:\n        new_df = new_df[np.abs(new_df[col]-new_df[col].mean())<=(n_sigma*new_df[col].std())]\n    print('%i instances have been removed' %(df.shape[0]-new_df.shape[0]))\n    return new_df", "metadata": {"collapsed": true, "_uuid": "934fbea9bc1c0055c854671fbdbfdb16ddc91dc6", "_cell_guid": "1fc6ff37-a8e9-4fe4-a3c0-74f829f4931c", "trusted": false}, "execution_count": 5}, {"outputs": [], "cell_type": "code", "source": "def my_pipeline(df, func_list):\n    new_df = df.copy()\n    return reduce(lambda x, func: func(x), func_list, new_df)", "metadata": {"collapsed": true, "_uuid": "2e6bfc80993ed07c908b9c30b8024c8a0d65ab6c", "_cell_guid": "a3a7b605-9de3-4a05-a10a-faae0a9b2c5e", "trusted": false}, "execution_count": 6}, {"outputs": [], "cell_type": "markdown", "source": "## Loading Data", "metadata": {"_uuid": "06e8bb7157cdd5e3ca1aa9b5d12825c5eafb14f8", "_cell_guid": "9fd1b366-3796-49db-841c-829ca4185e66"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "train_df = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\ntest_df = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\n\nholidays_df = pd.read_csv('../input/nyc2016holidays/NYC_2016Holidays.csv', sep=';')", "metadata": {"collapsed": true, "_uuid": "f1a6f9af21ea88a1f9d78db64676e718094c3c3e", "_cell_guid": "bc379c0e-3593-4ebd-bcd3-d5b5e9b84bde", "trusted": false}, "execution_count": 7}, {"outputs": [], "cell_type": "code", "source": "# Geo locations taken from Google maps\nNYC = np.array([-74.0059,40.7128]) # google maps coordinates of NYC\n\nfifth_ave = np.array([0.58926996811979,0.8079362008674332]) # versor of Fifth Av. digitized from google maps\nort_fifth_ave = np.array([-0.8079362008674332,0.58926996811979]) # orthogonal versor\n\nEastRiver = np.array([-73.955921,40.755157])\nHudsonRiver = np.array([-74.012226,40.755677])\nLeftBound = np.array([-74.020485,40.701463])\nRightBound = np.array([-73.932614,40.818593])", "metadata": {"collapsed": true, "_uuid": "3d8f13903fdd119f1f81bd605a958788dd7161ca", "_cell_guid": "e9116765-879b-4b1c-9bfd-5ed0b1015b0a", "trusted": false}, "execution_count": 8}, {"outputs": [], "cell_type": "markdown", "source": "## Initial Exploration of the Data", "metadata": {"_uuid": "63abbdec37c1bd1f9018764b74f3b12253fb0e30", "_cell_guid": "dbdb2f62-9c53-4f76-a87a-6fe6aafed1de"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "train_df.info()", "metadata": {"_uuid": "a7cc775bcecc6ed06d3360b012d1b05748ee414f", "_cell_guid": "c15c4029-02c8-4244-888c-bdac0e71fa56", "trusted": false}, "execution_count": 9}, {"outputs": [], "cell_type": "markdown", "source": "There are 1.5 millions instances in this dataset, which means it is fairly big. Notice also that there are no *null* values in the training set. Given the size of this set I think it is fair to assume that also the test set won't have any null value, but you never know...\n\nThe labels are pretty much self explenatory, so we want go into details. \n\nNotice also that there are 4 objects that need to be transformed for ML purposes:\n- **id**: this will probably be useless\n- **pickup_datetime** and **dropoff_datetime**: this can be transformed in with the *datetime* library\n- **store_and_fwd_flag**: this attribute flags whether the instance was uploaded immidiately or not. I don't know if it will be usefull or not...\n\nLets do some consistency check on the train set", "metadata": {"_uuid": "67b75cfa5650324b20a10dd2b3155db5837ea79c", "_cell_guid": "375d2cfb-9df7-4f6d-954c-b630ec27a6c0"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "train_df['id'].value_counts().shape", "metadata": {"_uuid": "6eb9b217089623d44bc3fe42e94289506a7d9905", "_cell_guid": "d757424f-f698-4edf-8a04-f420e9978663", "trusted": false}, "execution_count": 10}, {"outputs": [], "cell_type": "markdown", "source": "This matches the total number if instances, so each instance is unique.\n\nWhat about the **store_and_fwd_flag**?", "metadata": {"_uuid": "481bbb2b778847a642c0e723e753dd9f2f92fafb", "_cell_guid": "77fba246-6ac3-4ec8-9690-242c2fba1fbe"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "train_df['store_and_fwd_flag'].value_counts()", "metadata": {"_uuid": "375f1b5a9183320ee3a18c66f3f86aa85532a444", "_cell_guid": "2d9df92d-0801-450c-bf15-8b075677b3b5", "trusted": false}, "execution_count": 11}, {"outputs": [], "cell_type": "markdown", "source": "There are just 8000 instances flagged 'Yes' out of 1.5 millions. This suggests that this attribute will probably be useless for our ML. However, we should check whether this instances are peculiar in some respect.  ", "metadata": {"_uuid": "264edab464637a145c40739e493bc323a63aa47d", "_cell_guid": "888b5fd9-cce8-4c59-874e-ed848bbef231"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "train_df.describe()", "metadata": {"_uuid": "47c371c83d3031dbe5c6de9532e3bfdc7e3421c6", "_cell_guid": "13e8ea85-ee1a-4af7-b1f2-81e9506537e9", "trusted": false}, "execution_count": 13}, {"outputs": [], "cell_type": "markdown", "source": "Lets take a better look at the **vendor_id** and **passenger_count** attributes:", "metadata": {"_uuid": "0d0584bb5675f9d2a2b472a1754aeef461f2c7a2", "_cell_guid": "b0ffcd19-9663-401b-a03e-ac8191f835cb"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "plt.figure(figsize=(12,4))\n\nplt.subplot(1,2,1)\ndata = train_df['vendor_id'].value_counts().sort_index()\ndata.plot(kind='bar')\nplt.xlabel('vendor_id')\nplt.ylabel('events')\n\nplt.subplot(1,2,2)\ndata = train_df['passenger_count'].value_counts().sort_index()\ndata.plot(kind='bar')\nplt.xlabel('passenger_count')\n\nplt.show()", "metadata": {"_uuid": "a3ba2a9c518acbc17054854b7cae8897749a5e30", "_cell_guid": "4f685f50-5a81-4946-a4f1-3884bd07fd7d", "trusted": false}, "execution_count": 14}, {"outputs": [], "cell_type": "markdown", "source": "Notice that *value_counts()* givs only the non-null values... so apparently there were rides with zero passengers!\nLets keep in mind this when we exclude outliers.", "metadata": {"_uuid": "775b014474a56b89d8feaa659a85859859a8b710", "_cell_guid": "ad1969a7-5583-4e7a-9344-a0d69f86df10"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "This is probably going to be one of the most eye catching plots. Lets look at the pickup location:", "metadata": {"_uuid": "08a8ea70528a737e8a82057780d4d4d3aa5f2b4a", "_cell_guid": "dfbc4965-79b8-4385-abfb-241126ec6649"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "plt.figure(figsize=(10,10))\n\nplt.scatter(x=train_df['pickup_longitude'].values,y=train_df['pickup_latitude'].values, marker='^',s=1,alpha=.3)\nplt.xlim([-74.1,-73.7])\nplt.ylim([40.6, 40.9])\nplt.axis('off')\n\n#plt.scatter(x=train_df['dropoff_longitude'].values,y=train_df['dropoff_latitude'].values, marker='v',s=1,alpha=.1)\n#plt.xlim([-74.05,-73.75])\n#plt.ylim([40.6, 40.9])\n#plt.axis('off')\n\nplt.show()", "metadata": {"collapsed": true, "_uuid": "27cc0c01250d3f14d0d13856e33d3cb6bb97a759", "_cell_guid": "7a51c1ab-3244-49cc-9c10-c7d844be9d5b", "trusted": false}, "execution_count": 15}, {"outputs": [], "cell_type": "markdown", "source": "One million rides... \n\nEven without a map beneath one can recognize Manhattan, Brooklyn and the airports JFK and La Guardia!\nIn fact, from this plot one already can see that most of the rides happen in Manhatthan. We'll check this quantitatively later. \n\nThere are also some weird instances of cabs going into the ocean! and a couple of NY cabs in SF! outliers...\nWe could remove them more carefully later, but instead lets just remove the 3 $\\sigma$ tails.", "metadata": {"_uuid": "cd123a3375fab1f239b54978ce185064df025442", "_cell_guid": "35661a00-77cf-4191-814a-481c1a3617b8"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "clean_att = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']\ntrain_df_clean = RemoveOutliers(train_df,clean_att,5)", "metadata": {"collapsed": true, "_uuid": "a39ed19c171b00d5657ae00d0df8372e9cadf779", "_cell_guid": "06ff6651-9798-497b-98f5-504328719b2a", "trusted": false}, "execution_count": 16}, {"outputs": [], "cell_type": "markdown", "source": "Next we have **trip_duration**, that is what we have to predict. Notice that the competition calculates the MSR Log Error. So we can directly take the log of **trip_duration**.", "metadata": {"_uuid": "493dae5a42d5f94d5f4e202586d217b124762260", "_cell_guid": "512980e7-e0d8-48fc-8531-834dc334fd5a"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "train_df_clean['trip_duration'] = np.log(train_df_clean['trip_duration'])", "metadata": {"collapsed": true, "_uuid": "fd62a563dcfb97f9ecf384dde706a8b2fb52523f", "_cell_guid": "e361cdfd-6d25-4074-83fa-e6a3e20a9709", "trusted": false}, "execution_count": 17}, {"outputs": [], "cell_type": "code", "source": "plt.figure(figsize=(8,6))\n\nplt.hist(train_df_clean['trip_duration'], bins=100)\n#plt.yscale('log', nonposy='clip')\nplt.xlabel('Trip duration (log)')\nplt.ylabel('events')\n\nplt.show()", "metadata": {"collapsed": true, "_uuid": "551ec62bad90036068589bd1b330063fa54094f2", "_cell_guid": "a93f8583-b50a-46a8-a154-0b9c1a4b1d30", "trusted": false}, "execution_count": 18}, {"outputs": [], "cell_type": "markdown", "source": "The duration is a nice bell-shaped distribution in log space, however there is some small peak around log(duration)=11. Lets clean a bit more the data by removing the outliers", "metadata": {"_uuid": "3e96d857f2e2deb27711589b9ce79f66999a622b", "_cell_guid": "e28b6f45-2a10-49cf-b66e-746ec40674bd"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "clean_att = ['trip_duration']\ntrain_df_clean = RemoveOutliers(train_df_clean,clean_att,5)", "metadata": {"collapsed": true, "_uuid": "f1d982929db2d327751143a1b41a9007bdac22d1", "_cell_guid": "c13aa074-270c-4718-9640-cf82e6f63d9f", "trusted": false}, "execution_count": 19}, {"outputs": [], "cell_type": "markdown", "source": "This is the result after cleaning", "metadata": {"_uuid": "7c9a096a9daa94d7d3d36e9a4af5ab87a145e0f2", "_cell_guid": "53975436-2494-4f88-97e2-b283750f8807"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "plt.figure(figsize=(8,6))\n\nplt.hist(train_df_clean['trip_duration'], bins=100)\n#plt.yscale('log', nonposy='clip')\nplt.xlabel('Trip duration (log)')\nplt.ylabel('events')\n\nplt.show()", "metadata": {"collapsed": true, "_uuid": "956d18092391f57f39a6703a1b9e206066a189d2", "_cell_guid": "74e6662b-5b6f-4645-8a92-a67135ccf9d7", "trusted": false}, "execution_count": 20}, {"outputs": [], "cell_type": "markdown", "source": "## Data Preparation for ML", "metadata": {"_uuid": "33f24896c43eb9584191673eb74b20ed28fb803c", "_cell_guid": "4241601a-a260-46a4-83e6-c0c87eefacdb"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "Most ML algorithm and sklearn class works with numberical attributes only, so we need to transform the object-type attributes.\nLets create a list of numerical and categorical attributes", "metadata": {"_uuid": "7ef3e012668e959ab083cb0ff0d3e9355448170c", "_cell_guid": "2c5a9f9c-e711-42f0-98ba-c3d9c561fd1f"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "num_att = [f for f in train_df_clean.columns if train_df_clean.dtypes[f] != 'object']\ncat_att = [f for f in train_df_clean.columns if train_df_clean.dtypes[f] == 'object']\nprint(\"-\"*10+\" numerical attributes \"+\"-\"*10)\nprint(num_att)\nprint('')\nprint(\"-\"*10+\" categorical attributes \"+\"-\"*10)\nprint(cat_att)", "metadata": {"collapsed": true, "_uuid": "170fa5aa4e80a65ec6ae1678b0856f93306c0d48", "_cell_guid": "8e570a3c-77a9-4ee4-abcd-d910eb0d8e75", "trusted": false}, "execution_count": 21}, {"outputs": [], "cell_type": "markdown", "source": "### Trip Duration", "metadata": {"_uuid": "61786aaa9d176c749fc78aa24a4e36c31c494eab", "_cell_guid": "8f91ddc0-fc1c-4a71-bad0-f6002e1ea0df"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "First lets run a consistency check on the data", "metadata": {"_uuid": "df9478311a7a379b2b0329177432cee80cf251cf", "_cell_guid": "5208d391-3a5a-4157-b4f6-35e8682bae64"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "train_df_clean['pickup_datetime'] = pd.to_datetime(train_df_clean['pickup_datetime'])\ntrain_df_clean['dropoff_datetime'] = pd.to_datetime(train_df_clean['dropoff_datetime'])\n\ndelta_t = np.log((train_df_clean['dropoff_datetime']-train_df_clean['pickup_datetime']).dt.total_seconds())\nprint(\"Number of wrong trip durations: %i\" %train_df_clean[np.round(delta_t,5)!=np.round(train_df_clean['trip_duration'],5)].shape[0])", "metadata": {"collapsed": true, "_uuid": "53fad1b2a34bc77426547ea1bb5e63cea97b81b7", "_cell_guid": "11459f1e-db94-43e4-b2f5-392463551fde", "trusted": false}, "execution_count": 22}, {"outputs": [], "cell_type": "markdown", "source": "The train_set is consitent, so we can drop the dropoff time and split the dataframes in X (the features) and Y (what we have to predict)", "metadata": {"_uuid": "f75f532ec635b70a6a6bdddce7862b649088e7bb", "_cell_guid": "140b052e-989b-4018-b682-00831a556565"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "X_train = train_df_clean.drop(['id','dropoff_datetime','trip_duration'], axis=1)\nY_train = train_df_clean['trip_duration'].copy()\n\nX_test = test_df.drop(['id'], axis=1)\nX_test_id = test_df['id'].copy()\n\n\ncat_att = ['store_and_fwd_flag']\ndate_att = ['pickup_datetime']\ncoord_att = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']", "metadata": {"collapsed": true, "_uuid": "ecac87239356acb44e84a56148a8d69a66316eb8", "_cell_guid": "2fe84b67-16dd-43b3-86ab-ef45bcb66cfe", "trusted": false}, "execution_count": 23}, {"outputs": [], "cell_type": "markdown", "source": "### Categorical attributes", "metadata": {"_uuid": "1fc0f2633e7f8c19048a5ca08297602cf8b73e1c", "_cell_guid": "6634c0ea-a9c4-44b0-9d85-9c2d39d20743"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "def FlagEncoder(df):\n    new_df = df.copy()\n    new_df['store_and_fwd_flag'] = (new_df['store_and_fwd_flag']=='Y')*1\n    return new_df", "metadata": {"collapsed": true, "_uuid": "e74701fc66c1b07e4cc99b99f6164ec6b8fca370", "_cell_guid": "4f09116f-954a-4b86-bcc0-7d4f39b72b25", "trusted": false}, "execution_count": 24}, {"outputs": [], "cell_type": "markdown", "source": "### Datetime Attributes", "metadata": {"_uuid": "d3c904db29646bc70a26e1bca2a88992ecc8054a", "_cell_guid": "bf5bd10b-5b0c-4480-84ea-f25ac3b196cc"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "Here we'll use the holidays dataset to add a 'weekend' attribute. We'll also transform the pick_datetime into 'day' (i.e. the day of the week) and 'time' (the time of the pickup).", "metadata": {"_uuid": "636db77f9a67fe7689558d37aca17a1d7edf8e71", "_cell_guid": "fd0a57d7-0b5f-49b2-bb75-f92fbbceb71d"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "holidays = (pd.to_datetime('2016 '+ holidays_df['Date']))", "metadata": {"collapsed": true, "_uuid": "0c8dbad9d13c17d5107e237ef5260b24bfee1eb6", "_cell_guid": "b33d5860-5302-4a1c-97ce-bf010be473da", "trusted": false}, "execution_count": 25}, {"outputs": [], "cell_type": "code", "source": "def DateAttributes(df):\n    new_df = df.copy()\n    new_df['pickup_datetime'] = pd.to_datetime(new_df['pickup_datetime'])\n    new_df['day'] = new_df['pickup_datetime'].dt.weekday\n    new_df['weekend'] = 1*((new_df['day']>=5)|(new_df['pickup_datetime'].dt.date.isin(holidays.dt.date.values)))\n    new_df['time'] = np.round(new_df['pickup_datetime'].dt.time.apply(lambda x: x.hour + x.minute/60.0),1)\n    return new_df", "metadata": {"collapsed": true, "_uuid": "206619e45c43abde4e7d2971a6730fac628a1754", "_cell_guid": "efbaf692-68eb-4d16-822b-d3655ff3835b", "trusted": false}, "execution_count": 26}, {"outputs": [], "cell_type": "markdown", "source": "### Numerical Attributes", "metadata": {"_uuid": "508ef1e44a862a815843d62d915d11c7f5ec653f", "_cell_guid": "af1897da-2b58-4395-a9c9-76c27fb332df"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "#### Distance\n\nFrom Physics 101, distance = speed * time, so we should try to find the distance and the speed of each ride to help the ML algorith. This is probably the most difficult part of the game.\n\nWe will define two kind of distances called L1 and L2. The L1 distance is the typical distance you experience in Mahatthan going from point A to point B. The L2 distance is just the Euclidean distance. \n\nI've seen many people using very complex definition of distances on this competition, for example the Haversine distance. This is the distance on a a great circle from a point A on a sphere of radius R to another point B. I think this is quite an overdoing. The size of NYC compared to the radius of the Earth is minuscle. so the error that one makes in treating the Earth as flat is more that acceptable. \n\nSo the easy option is just to use longitude and latitude as coordinates on a plane! No Haversine needed! We only have to express latitude and longitude with the same (arbitrary) unit of measurement. In fact, I've already used this approximation in the plot above. ", "metadata": {"_uuid": "5a2325db14a8039baedd5cefcc3d6219c9284f7f", "_cell_guid": "079f3efa-e18d-4b91-b522-d8ff34afb578"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "def distance(coords): #  L1 and L2 distances (in arbitrary units)\n    units = np.array([np.cos(np.radians(NYC[1])),1]) # multiply by 111.2 to get km \n    picks = np.split(coords.transpose(),2)[0].transpose()*units\n    drops = np.split(coords.transpose(),2)[1].transpose()*units\n    x1 = np.dot(picks,fifth_ave*units)\n    y1 = np.dot(picks,ort_fifth_ave*units)    \n    x2 = np.dot(drops,fifth_ave*units)\n    y2 = np.dot(drops,ort_fifth_ave*units)\n    dist_L1 = abs(x1-x2) + abs(y1-y2)\n    dist_L2 = np.sqrt((x1-x2)**2 + (y1-y2)**2)\n    return [dist_L1,dist_L2]", "metadata": {"collapsed": true, "_uuid": "aa55d037e7c4d8b518d9f33b1bdb1e02063cc9ee", "_cell_guid": "881507f3-d5fd-46a6-a21b-846860eb684f", "trusted": false}, "execution_count": 27}, {"outputs": [], "cell_type": "code", "source": "def DistanceAttribute(df):\n    new_df = df.copy()\n    coords = new_df[coord_att].values\n    new_df['dist_L1'] = distance(coords)[0]\n    new_df['dist_L2'] = distance(coords)[1]\n    return new_df", "metadata": {"collapsed": true, "_uuid": "119d551039e243bbc771f10d20c1f162dbcc2f8c", "_cell_guid": "b967348d-6ae3-4b1e-b83a-6c2108652db9", "trusted": false}, "execution_count": 28}, {"outputs": [], "cell_type": "code", "source": "# Adding attributes\npipe_list = [FlagEncoder, DateAttributes, DistanceAttribute]\n\nX_train_prepared = my_pipeline(X_train,pipe_list)\nX_test_prepared = my_pipeline(X_test,pipe_list)", "metadata": {"collapsed": true, "_uuid": "23ca979b7c384d05b073242a24cbde607d98c250", "_cell_guid": "5aacdf2f-a2f6-426c-9536-00067f36d90c", "trusted": false}, "execution_count": 29}, {"outputs": [], "cell_type": "markdown", "source": "But how many rides actually happen within Manhattan?\n\nTo answer this question one could use a clustering algorith. but we'll make it simpler here: we'll just define a strip on the map and call it Manhattan. Here it is:", "metadata": {"_uuid": "8db82a75da8b04ca0dbfc1311dfa1e6efa051257", "_cell_guid": "4fd9e99d-cd48-4af3-b848-12bd1fd67663"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "X=X_train_prepared\n\nc1 = (X['pickup_longitude']>LeftBound[0])&(X['pickup_longitude']<RightBound[0])\nc2 = (X['pickup_longitude']>LeftBound[0])&(X['dropoff_longitude']<RightBound[0])\nc3 = ((X['pickup_longitude']-EastRiver[0])*ort_fifth_ave[0]+(X['pickup_latitude']-EastRiver[1])*ort_fifth_ave[1])>0\nc4 = ((X['pickup_longitude']-HudsonRiver[0])*ort_fifth_ave[0]+(X['pickup_latitude']-HudsonRiver[1])*ort_fifth_ave[1])<0\nc5 = ((X['dropoff_longitude']-EastRiver[0])*ort_fifth_ave[0]+(X['dropoff_latitude']-EastRiver[1])*ort_fifth_ave[1])>0\nc6 = ((X['dropoff_longitude']-HudsonRiver[0])*ort_fifth_ave[0]+(X['dropoff_latitude']-HudsonRiver[1])*ort_fifth_ave[1])<0\n\nManhattan_df = X[c1&c2&c3&c4&c5&c6]\nY_Manhattan = Y_train.values[c1&c2&c3&c4&c5&c6]\nprint('Percentage of trips within Manhattan: %.2f' %(1.*Manhattan_df.shape[0]/X.shape[0])) ", "metadata": {"collapsed": true, "_uuid": "28df6989f0acca1eea1bd6d1c593bffdf0c9d069", "_cell_guid": "8a604140-5630-479c-9c86-0c9e1e735055", "trusted": false}, "execution_count": 30}, {"outputs": [], "cell_type": "markdown", "source": "Almost all rides are within Manhattan! The L1 distance will be a quite accurate estimate of the true distance for these instances. \n\nWhat about JFK and La Guardia?", "metadata": {"_uuid": "5dd54df240359d2b61def9ef06f86bbb20112671", "_cell_guid": "0bfd87e8-e7ed-4dc5-a7d0-876a3ced27e1"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "JFK = np.array([-73.779148,40.653416])\nLaGuardia = np.array([-73.873890,40.775341])", "metadata": {"collapsed": true, "_uuid": "13e42f6df61dc5e78640e60b738fb6a91fda3103", "_cell_guid": "be52f8db-9469-493d-a243-87da3ae162d5", "trusted": false}, "execution_count": 31}, {"outputs": [], "cell_type": "code", "source": "X=X_train_prepared\n\nc1 = (X['pickup_longitude']>(JFK[0]-0.05))&(X['pickup_longitude']<(JFK[0]+0.05))\nc2 = (X['pickup_latitude']>(JFK[1]-0.05))&(X['pickup_latitude']<(JFK[1]+0.05))\nc3 = (X['dropoff_longitude']>(JFK[0]-0.05))&(X['dropoff_longitude']<(JFK[0]+0.05))\nc4 = (X['dropoff_latitude']>(JFK[1]-0.05))&(X['dropoff_latitude']<(JFK[1]+0.05))\n\nJFK_df = X[(c1&c2)|(c3&c4)]\nY_JFK = Y_train.values[(c1&c2)|(c3&c4)]\nprint('Percentage of trips to/from JFK: %.2f' %(1.*JFK_df.shape[0]/X.shape[0])) ", "metadata": {"collapsed": true, "_uuid": "a881c88726ca35b7630b2781a98fc602ba4ade4c", "_cell_guid": "00d83e74-d8a6-46f0-808c-855c0542413b", "trusted": false}, "execution_count": 32}, {"outputs": [], "cell_type": "code", "source": "X=X_train_prepared\n\nc1 = (X['pickup_longitude']>(LaGuardia[0]-0.02))&(X['pickup_longitude']<(LaGuardia[0]+0.02))\nc2 = (X['pickup_latitude']>(LaGuardia[1]-0.02))&(X['pickup_latitude']<(LaGuardia[1]+0.02))\nc3 = (X['dropoff_longitude']>(LaGuardia[0]-0.02))&(X['dropoff_longitude']<(LaGuardia[0]+0.02))\nc4 = (X['dropoff_latitude']>(LaGuardia[1]-0.02))&(X['dropoff_latitude']<(LaGuardia[1]+0.02))\n\nLaGuardia_df = X[(c1&c2)|(c3&c4)]\nY_LaGuardia = Y_train.values[(c1&c2)|(c3&c4)]\nprint('Percentage of trips to/from La Guardia: %.2f' %(1.*LaGuardia_df.shape[0]/X.shape[0])) ", "metadata": {"collapsed": true, "_uuid": "d31e6cc4cfbc0cd2f3d55d55a81ce41c3c81bb77", "_cell_guid": "865b142e-ed5b-405d-9041-e439619b3eb9", "trusted": false}, "execution_count": 33}, {"outputs": [], "cell_type": "markdown", "source": "That is interesting La Guardia as twice as many rides as JFK, but from [this link](http://http://laguardiaairport.com/about-us/facts-and-statistics/) and [this link](http://https://www.panynj.gov/airports/pdf/stats/JFK_DEC_20016.pdf), JFK has twice as many passengers as La Guardia. SO people going to La Guardia are more inclined to take a Taxi... if you have been to La Guardia you know what I'm talking about.", "metadata": {"_uuid": "93ae54729d325c50b13520ce58a5713ec1240d3d", "_cell_guid": "e6c5e16e-757d-4b01-b64e-455feebd05f5"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "# Dataframe consolidation\nManhattan_df = DistanceAttribute(Manhattan_df)\nJFK_df = DistanceAttribute(JFK_df)\nLaGuardia_df = DistanceAttribute(LaGuardia_df)", "metadata": {"collapsed": true, "_uuid": "6ba151df75dd007e6468177fbcd13d9e0d527911", "_cell_guid": "141f0d76-6a3a-4932-b420-6ac733f2db71", "trusted": false}, "execution_count": 34}, {"outputs": [], "cell_type": "markdown", "source": "Lets now look at how well the distance attribute predict the trip duration. A scatter plot is a good starting point.", "metadata": {"_uuid": "0048386d60acb22b5b53d8699ae8e4db22ebf2d1", "_cell_guid": "a051de0c-5b77-4869-8fff-899942bd0e8e"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "plt.figure(figsize=(8,8))\n\nplt.scatter(x=X_train_prepared['dist_L2'].values,y=np.exp(Y_train).values,s=1,alpha=0.1)\nplt.xlim([0,0.3])\nplt.ylim([0, 6000])\n#plt.axis('off')\nplt.xlabel('dist_L2')\nplt.ylabel('Trip duration (log)')\n\nplt.show()", "metadata": {"collapsed": true, "_uuid": "acbd3628c8b6432dc249e369c05cbbff65270ea9", "_cell_guid": "f8feb355-7623-4637-9d37-ea89fc951ae7", "trusted": false}, "execution_count": 35}, {"outputs": [], "cell_type": "markdown", "source": "This is another interesting plot. One can see a bulk of instances with large scatter at distance L2 less than 0.1, and then 3 clear spikes: one at 0 distance (outliers?), one at 0.12 and one at 0.22. Notice that a spike in the distance means a fixed prefered length of travelling... I already know where people is going!", "metadata": {"_uuid": "105d7cd4be6a5f91ba3d89d870627f5e0c3cc282", "_cell_guid": "546c85f7-682b-4673-808d-d925d1cb19bc"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "plt.figure(figsize=(8,8))\n\nplt.scatter(x=X_train_prepared['dist_L2'].values,y=np.exp(Y_train),s=1,alpha=0.1)\nplt.scatter(x=Manhattan_df['dist_L2'].values,y=np.exp(Y_Manhattan),s=1,alpha=0.1)\nplt.scatter(x=JFK_df['dist_L2'].values,y=np.exp(Y_JFK),s=1,alpha=0.1)\nplt.scatter(x=LaGuardia_df['dist_L2'].values,y=np.exp(Y_LaGuardia),s=1,alpha=0.1)\n\nplt.xlim([0,0.3])\nplt.ylim([0, 6000])\nplt.xlabel('dist_L2')\nplt.ylabel('Trip duration')\n\nplt.show()", "metadata": {"collapsed": true, "_uuid": "9d5197f35e42211dc7fc2f7a1bde9747f80236ee", "_cell_guid": "cb555c42-a3c6-4831-bac0-1be731a9d94a", "trusted": false}, "execution_count": 36}, {"outputs": [], "cell_type": "markdown", "source": "The data divides well in three clusters: rides within Manhattan and rides to/from the airport. This make around the 90% of the data. so lets define new attribute with this information encoded a la OneHot. Furthermore lets clean the spike at zero distance.", "metadata": {"_uuid": "377f3c1d5a603d37d21bdd8314817328c40f0e65", "_cell_guid": "b0516d42-7e4c-43f5-aab5-5154e07c521b"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "c1 = X_train_prepared['dist_L1']>0.0001\nc2 = X_train_prepared['dist_L2']>0.0001\n\nX_train = X_train[(c1&c2)]\nY_train = Y_train[c1&c2]", "metadata": {"collapsed": true, "_uuid": "9c83934af3db1e96bb58d73826e73cf4bad41411", "_cell_guid": "f0071e0f-0ea2-4f8a-a2ca-1a3dd782a20b", "trusted": false}, "execution_count": 37}, {"outputs": [], "cell_type": "code", "source": "def RideScope(df):\n    X = df.copy()\n    \n    # Manhatthan only\n    c1 = (X['pickup_longitude']>LeftBound[0])&(X['pickup_longitude']<RightBound[0])\n    c2 = (X['dropoff_longitude']>LeftBound[0])&(X['dropoff_longitude']<RightBound[0])\n    c3 = ((X['pickup_longitude']-EastRiver[0])*ort_fifth_ave[0]+(X['pickup_latitude']-EastRiver[1])*ort_fifth_ave[1])>0\n    c4 = ((X['pickup_longitude']-HudsonRiver[0])*ort_fifth_ave[0]+(X['pickup_latitude']-HudsonRiver[1])*ort_fifth_ave[1])<0\n    c5 = ((X['dropoff_longitude']-EastRiver[0])*ort_fifth_ave[0]+(X['dropoff_latitude']-EastRiver[1])*ort_fifth_ave[1])>0\n    c6 = ((X['dropoff_longitude']-HudsonRiver[0])*ort_fifth_ave[0]+(X['dropoff_latitude']-HudsonRiver[1])*ort_fifth_ave[1])<0\n\n    X['M&M'] = (c1&c2&c3&c4&c5&c6)*1\n    \n    # JFK\n    c1 = (X['pickup_longitude']>(JFK[0]-0.05))&(X['pickup_longitude']<(JFK[0]+0.05))\n    c2 = (X['pickup_latitude']>(JFK[1]-0.05))&(X['pickup_latitude']<(JFK[1]+0.05))\n    c3 = (X['dropoff_longitude']>(JFK[0]-0.05))&(X['dropoff_longitude']<(JFK[0]+0.05))\n    c4 = (X['dropoff_latitude']>(JFK[1]-0.05))&(X['dropoff_latitude']<(JFK[1]+0.05))\n    \n    X['JFK'] = ((c1&c2)|(c3&c4))*1\n    \n    #LaGuardia\n    c1 = (X['pickup_longitude']>(LaGuardia[0]-0.02))&(X['pickup_longitude']<(LaGuardia[0]+0.02))\n    c2 = (X['pickup_latitude']>(LaGuardia[1]-0.02))&(X['pickup_latitude']<(LaGuardia[1]+0.02))\n    c3 = (X['dropoff_longitude']>(LaGuardia[0]-0.02))&(X['dropoff_longitude']<(LaGuardia[0]+0.02))\n    c4 = (X['dropoff_latitude']>(LaGuardia[1]-0.02))&(X['dropoff_latitude']<(LaGuardia[1]+0.02))\n\n    X['LaG'] = ((c1&c2)|(c3&c4))*1\n    \n    return X", "metadata": {"collapsed": true, "_uuid": "959aa330889718f0a9e96eb92d128d4d9bbfd851", "_cell_guid": "90de0f96-1a8e-4252-b432-65310038f1b3", "trusted": false}, "execution_count": 38}, {"outputs": [], "cell_type": "code", "source": "pipe_list = pipe_list + [RideScope]\n\nX_train_prepared = my_pipeline(X_train,pipe_list)\nX_test_prepared = my_pipeline(X_test,pipe_list)", "metadata": {"collapsed": true, "_uuid": "0c4e7bc0c8ccf40119cbb9807c1a9c1d816afaa0", "_cell_guid": "71ff8b71-4559-4e5e-a902-e89fdb5664b0", "trusted": false}, "execution_count": 39}, {"outputs": [], "cell_type": "markdown", "source": "Everybody knows that a ride from JFK is unpredictable, but can we understand the dispersion better? It surely depends on the traffic along the way, which is probably correlated with the time of the pickup. Lets check.", "metadata": {"_uuid": "380fbd8064da8bbf6495e5aef7ba6fdd6c2adc70", "_cell_guid": "bd05e515-b63a-47b3-8e86-d6bf167b3cdb"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "plt.figure(figsize=(8,8))\n\nsc = plt.scatter(JFK_df['dist_L2'].values,np.exp(Y_JFK),c=JFK_df['time'],s=1,cmap=plt.get_cmap('jet'),alpha=0.5)\nplt.xlim([0,0.3])\nplt.ylim([0, 6000])\ncb = plt.colorbar(sc)\nplt.xlabel('dist_L2')\nplt.ylabel('Trip duration')\ncb.set_label('Time of the day')\n\nplt.show()", "metadata": {"collapsed": true, "_uuid": "f250f7a1cc21a024e206a52af511eab5c5318200", "_cell_guid": "bff9de93-5dc2-496b-b9c0-ec24c533e96e", "trusted": false}, "execution_count": 40}, {"outputs": [], "cell_type": "markdown", "source": "As expected, trips that start at night have much less scatter than trips that happen during the day. So better to include the time in the attribute for ML. ", "metadata": {"_uuid": "8a9d088d0b8178ee3a7cf8dba94f666a9a48a4cd", "_cell_guid": "4040b386-2b88-4505-96a3-4828ad3c1457"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "#### Speed\n\nThe previous plot suggesed that the time of the day is (obviously) important. So lets take a look at the average speed of each ride vs the time of the day", "metadata": {"_uuid": "517aacf6dd4b0693224226338f0481edca2076d9", "_cell_guid": "060e6d4e-023b-41c5-b441-78311c5e3c56"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "def Speeds(df,Y):\n    new_df = df.copy()\n    new_df['speed_L1'] = new_df['dist_L1']/np.exp(Y)\n    new_df['speed_L2'] = new_df['dist_L2']/np.exp(Y)\n    return new_df", "metadata": {"collapsed": true, "_uuid": "589ff704b10b1e328ec67f5347d67fe837303e72", "_cell_guid": "d48edd1f-a7f9-4dbd-ab7d-d96c2d363122", "trusted": false}, "execution_count": 41}, {"outputs": [], "cell_type": "code", "source": "speed_df = Speeds(X_train_prepared,Y_train)\n\nweekday_speed = speed_df.groupby(['weekend','time']).agg(['mean','std'])[['speed_L1','speed_L2']].loc[0].reset_index()\nweekend_speed = speed_df.groupby(['weekend','time']).agg(['mean','std'])[['speed_L1','speed_L2']].loc[1].reset_index()", "metadata": {"collapsed": true, "_uuid": "7caf3fcb465a19f1038c38a6be702893f73f73b2", "_cell_guid": "f6406b4e-88a3-468b-921b-ddb9d5fb82b0", "trusted": false}, "execution_count": 42}, {"outputs": [], "cell_type": "code", "source": "plt.figure(figsize=(6,6))\n\nplt.errorbar(weekday_speed['time'],weekday_speed['speed_L2']['mean'],yerr=0, label='weekday')\nplt.errorbar(weekend_speed['time'],weekend_speed['speed_L2']['mean'],yerr=0, label='weekend')\nplt.ylim([0, 0.000125])\n\n#plt.errorbar(weekday_speed['time'],weekday_speed['speed_L2']['mean'],yerr=weekday_speed['speed_L2']['std'])\n#plt.errorbar(weekend_speed['time'],weekend_speed['speed_L2']['mean'],yerr=weekend_speed['speed_L2']['std'])\n#plt.ylim([0, 0.000125])\nplt.ylim([0, 0.0001])\nplt.xlabel('time')\nplt.ylabel('speed_L2')\nplt.legend()\n\nplt.show()", "metadata": {"collapsed": true, "_uuid": "f41beae3aed9e7185af6763324e48f6087dd95b9", "_cell_guid": "744e8154-dd3a-44ff-892c-603085f6e3e9", "trusted": false}, "execution_count": 43}, {"outputs": [], "cell_type": "markdown", "source": "Notice the speed plateau during 9 -19 of weekdays and the lower speed at night during weekend.", "metadata": {"_uuid": "2aa1e7f789032c7dd9fd63caf5a1fe81b172b953", "_cell_guid": "2a987763-2e7e-4e92-b8ec-a4b01e18c189"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "We can define a speed attribute (actually two, wrt L1 and L2) only on the train set, since there is no known duration on the test. Therfore we need a rule to define it also for the test set. 'time' and 'weekend' seems to obvious selection rules.", "metadata": {"_uuid": "48b19c575565118a005336e5a1d3e2df6a2d8e5d", "_cell_guid": "03373562-3516-4389-853e-a2f425aa958e"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "speeds = speed_df.groupby(['weekend','time']).mean()[['speed_L1','speed_L2']].reset_index()", "metadata": {"collapsed": true, "_uuid": "3ef35ada8379e83c5ad7a701bbaf986c9ecb3d8b", "_cell_guid": "f0625490-02e2-4790-a3ba-df80f3180efd", "trusted": false}, "execution_count": 44}, {"outputs": [], "cell_type": "code", "source": "def SpeedAttribute(df):\n    new_df = df.copy()\n    new_df = pd.merge(new_df, speeds, how='left', on=['weekend','time'])\n    return new_df", "metadata": {"collapsed": true, "_uuid": "fe361d246c2ca6270f14f1c3a03d48a3ec3515a1", "_cell_guid": "7e3c2dbf-2cf2-4222-a9d3-40127f2defe0", "trusted": false}, "execution_count": 45}, {"outputs": [], "cell_type": "code", "source": "pipe_list = pipe_list + [SpeedAttribute]\n\nX_train_prepared = my_pipeline(X_train,pipe_list)\nX_test_prepared = my_pipeline(X_test,pipe_list)", "metadata": {"collapsed": true, "_uuid": "ed2b48293ccca2b2577d792863dc839a896f9c07", "_cell_guid": "1ab327fb-d52a-4276-8dc5-fc5f7e0afdf9", "trusted": false}, "execution_count": 46}, {"outputs": [], "cell_type": "markdown", "source": "## Attributes selection and scaling", "metadata": {"_uuid": "534243415687b6eb91736acdfc18b5cdad2637e6", "_cell_guid": "ceeabbbb-a160-4c86-accc-79a7832b2982"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "X_train_prepared.columns", "metadata": {"collapsed": true, "_uuid": "3bdd16983bec1c3c99b2b647749df0e20f1afe4e", "_cell_guid": "469b9b81-866c-48c9-b3ec-ba22d4a1d239", "trusted": false}, "execution_count": 47}, {"outputs": [], "cell_type": "code", "source": "num_att = ['passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','dist_L1','dist_L2','speed_L1','speed_L2','time']\nOneHot = ['weekend','M&M','JFK','LaG']", "metadata": {"collapsed": true, "_uuid": "073d503d57dde109f5f0fcc4747e58a59af8f646", "_cell_guid": "a3b59702-ce4b-4adc-99e3-1845df0452f2", "trusted": false}, "execution_count": 48}, {"outputs": [], "cell_type": "code", "source": "num_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_att)),\n    ('imputer', Imputer(strategy=\"median\")),\n    ('std_scaler', StandardScaler()),\n])\n\nOneHot_pipeline = Pipeline([\n    ('selector', DataFrameSelector(OneHot)),\n])", "metadata": {"collapsed": true, "_uuid": "bc2ead75bf699723e2cb3cf37e21b33f4eb36876", "_cell_guid": "a7b957e8-b085-479a-9009-45125d8fa5b9", "trusted": false}, "execution_count": 49}, {"outputs": [], "cell_type": "code", "source": "full_pipeline = FeatureUnion(transformer_list=[\n           (\"num_pipeline\", num_pipeline),\n           (\"cat_pipeline\", OneHot_pipeline),\n])", "metadata": {"collapsed": true, "_uuid": "71b893bc21861f3c91f8768b557cd9fb5ae79eae", "_cell_guid": "cc4e49c3-8ee2-49c6-95fb-54e855107eaa", "trusted": false}, "execution_count": 50}, {"outputs": [], "cell_type": "code", "source": "X_train_scaled = full_pipeline.fit_transform(X_train_prepared)\nX_test_scaled = full_pipeline.transform(X_test_prepared)", "metadata": {"collapsed": true, "_uuid": "b994e7351c68de96c98ad8635063854864027472", "_cell_guid": "8af44281-130b-4bb4-b9a4-c600b92628eb", "trusted": false}, "execution_count": 51}, {"outputs": [], "cell_type": "markdown", "source": "## Model Selection and Training", "metadata": {"_uuid": "b43cc4a39a97e28a34d6b6854b8b4169ba0afb87", "_cell_guid": "c39a2d33-5694-41de-a003-0de6d6ff7362"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#tree_reg = DecisionTreeRegressor()\n\n#scores = cross_val_score(tree_reg, X_train_scaled, Y_train, cv=3, scoring='neg_mean_squared_error')\n#mean_cross_score(-scores)", "metadata": {"collapsed": true, "_uuid": "e9be24322f2d3fb7b78eeb017263c869443d47df", "_cell_guid": "88135ed2-443f-4c40-ad7c-91f33197727d", "trusted": false}, "execution_count": 52}, {"outputs": [], "cell_type": "code", "source": "rnd_reg = RandomForestRegressor()\n\nscores = cross_val_score(rnd_reg, X_train_scaled, Y_train, cv=3, scoring='neg_mean_squared_error')\nmean_cross_score(-scores)", "metadata": {"collapsed": true, "_uuid": "cbcab6458ca2438003be36fa28e97c27fa33f417", "_cell_guid": "6b6e8a97-f691-4ca1-985f-402629463de2", "trusted": false}, "execution_count": 53}, {"outputs": [], "cell_type": "code", "source": "rnd_reg = RandomForestRegressor()\nrnd_reg.fit(X_train_scaled, Y_train)", "metadata": {"collapsed": true, "_uuid": "68efcd7b300d4372550f016a2dbf7bf56e5c77b6", "_cell_guid": "990ac9c8-cf60-4ef6-93dc-552ad9026152", "trusted": false}, "execution_count": 60}, {"outputs": [], "cell_type": "markdown", "source": "## Submission", "metadata": {"_uuid": "db44cf8ff6064c445fed22bcb97f27077492ab79", "_cell_guid": "5e6c413b-373e-4881-b927-ef60233e45f9"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "Y_test_pred = rnd_reg.predict(X_test_scaled)\n\nsubmission = pd.DataFrame({\n        \"id\": X_test_id,\n        \"trip_duration\": np.exp(Y_test_pred)\n    })\nsubmission.to_csv('submission2.csv', index=False)", "metadata": {"collapsed": true, "_uuid": "4c6e18fbbd725028edd332a492afa81ab4385dd2", "_cell_guid": "7394ee4d-58db-42ef-b46e-49ef715200a0", "trusted": false}, "execution_count": 63}, {"outputs": [], "cell_type": "markdown", "source": "**This notebook scored on 0.42 on the leaderboard.**\n\nNotice however that the score I get here is much better thatn the one on the leaderboard. This should be investigate. Also one could performe some gridsearch to tune the model a check for overfitting. \n\nNevertheless I think the result is fairly good: with just a few feature we can predict quite well the trip duration. \nA lot more could be done. Especially crossing with other dataset. \n\nFor example there are dataset of traffic in NYC given by the DOT. I tried to use them, but the measurement are geographically to sparse to be usefull, but maybe there are better dataset. \n\nThe weather might be relevant as well\n\n**Anyway, if you like the job done here and want to continue, fork and upvote!**\n", "metadata": {"_uuid": "0f24256c21559572fba41918ba0e9e1469cb436d", "_cell_guid": "94a32d58-400a-4201-a290-833711662e24"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "", "metadata": {"collapsed": true, "_uuid": "c2228482a2c1cd886804166397bab1228b21223c", "_cell_guid": "ce79f92b-1d85-4749-b984-7c63523275f5", "trusted": false}, "execution_count": null}], "nbformat_minor": 1, "nbformat": 4}