{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport pickle\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport os\nfrom tqdm import tqdm\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"814880d3ca25c4b465597a70f959bb262495c031"},"cell_type":"markdown","source":"Add weather data from: ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ \nFrom this, use daily maximum and minimum temperatures (encoded as binary 'extreme' days), average wind speed, precipitation, and snow (all as numeric)."},{"metadata":{"trusted":true,"_uuid":"a53258524d9d2596ac075d0568f06364af5e70f9"},"cell_type":"code","source":"nyc_weather = pd.read_csv('../input/nyc-weather/nyc_weather.csv')\nweather_cols = ['DATE','AWND','PRCP','SNOW','TMAX','TMIN']\nnyc_weather = nyc_weather[weather_cols].copy()\nnyc_weather['DATE'] = pd.to_datetime(nyc_weather['DATE'], utc=True, format='%m/%d/%Y') \nnyc_weather.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3507c1ddd6673bc27429c1144eccd29dfa484fc9"},"cell_type":"markdown","source":"The plot below shows the distribution of daily high and low temperatures. This highlights the temperatures that will be considered extreme. "},{"metadata":{"trusted":true,"_uuid":"c752156f88013fdda8b7dec3265ea317f098a957","_kg_hide-input":true},"cell_type":"code","source":"plt.rc('figure', figsize=(15, 8))\nplt.subplot(1,2,1)\nplt.hist(nyc_weather.TMAX, bins =  30)\nplt.xlabel('Temperature (C)')\nplt.ylabel('Frequency Count')\nplt.title('Max Daily Temperature')\nplt.subplot(1,2,2)\nplt.hist(nyc_weather.TMIN, bins =  30)\nplt.xlabel('Temperature (C)')\nplt.ylabel('Frequency Count')\nplt.title('Min Daily Temperature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bd8888a58eae0701a94f13b5d65521cd83448a2"},"cell_type":"code","source":"nyc_weather.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a545217025c4b896040ca67e64d9bd8e0f199b00"},"cell_type":"code","source":"holidays = pd.read_csv('../input/us-bank-holidays-20092018/US Bank Holidays 2012-2018.csv')\nholidays['Date'] = pd.to_datetime(holidays['Date'], utc=True, format='%m/%d/%y') \nholidays.head(12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caeea8f330ad6f2abc93364456ca48504242c143"},"cell_type":"markdown","source":"The training set has ~55M rows so I take a sample of 12M.  After cleaning up some formatting and changing the data types to improve efficiency, it's time to begin feature engineering.  I break the pickup times down into categorical features, including year, month, and day/hour combinations (e.g. Friday 5pm, Saturday 7am, etc.).  I join the weather data and use wind speed, precipitation and snow as numeric features. I encode the extreme temperature days as binary features. I calculate the distance between pickup and dropoff using the Haversine. Lastly, I add the various bank holidays as categorical features. \n\n*Hiding this code for ease of reading the kernel. "},{"metadata":{"trusted":true,"_uuid":"18367bc3b22817d1481854e58d179d5fb023ea09"},"cell_type":"code","source":"%%time\n#import sample of train and full test\nimport random\n\nn = sum(1 for line in open('../input/new-york-city-taxi-fare-prediction/train.csv')) - 1 #number of records in file (excludes header)\ns = 15000000 #desired sample size\nskip = sorted(random.sample(range(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n\ntrain = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', skiprows=skip) \ntest = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv') \ntest_id = test.key.values #set this value for final submission\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c58595c04be110d35aa83ca166b2ce0dfbacbf01"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f59b6a8da69136f7cbd0afefb7c11360616664b4","_kg_hide-input":true},"cell_type":"code","source":"%%time\n#truncate datetime string for efficiency converting to datetime format\ntrain['pickup_datetime'] = train['pickup_datetime'].str.slice(0, 16)\ntest['pickup_datetime'] = test['pickup_datetime'].str.slice(0, 16)\n\ntrain['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M') \ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], utc=True,format='%Y-%m-%d %H:%M') \n\ntrain_sample = train.dropna()\ndel train\n\n#get rid of unnecessary memory consuming column\ntrain_sample.drop(labels='key', axis=1, inplace=True)\ntest.drop(labels='key', axis=1, inplace=True)\n\n#convert data to less memory intensive types\ntrain_sample.loc[:,'passenger_count'] = train_sample.passenger_count.astype(dtype = 'uint8')\ntrain_sample['pickup_longitude'] = train_sample.pickup_longitude.astype(dtype = 'float32')\ntrain_sample['pickup_latitude'] = train_sample.pickup_latitude.astype(dtype = 'float32')\ntrain_sample['dropoff_longitude'] = train_sample.dropoff_longitude.astype(dtype = 'float32')\ntrain_sample['dropoff_latitude'] = train_sample.dropoff_latitude.astype(dtype = 'float32')\ntrain_sample['fare_amount'] = train_sample.fare_amount.astype(dtype = 'float32')\n\ntest['pickup_longitude'] = test.pickup_longitude.astype(dtype = 'float32')\ntest['pickup_latitude'] = test.pickup_latitude.astype(dtype = 'float32')\ntest['dropoff_longitude'] = test.dropoff_longitude.astype(dtype = 'float32')\ntest['dropoff_latitude'] = test.dropoff_latitude.astype(dtype = 'float32')\n\n#filter training set to be within full range of test set\ntrain_sample = train_sample.loc[train_sample.pickup_longitude.between(test.pickup_longitude.min(), test.pickup_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.pickup_latitude.between(test.pickup_latitude.min(), test.pickup_latitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_longitude.between(test.dropoff_longitude.min(), test.dropoff_longitude.max())]\ntrain_sample = train_sample.loc[train_sample.dropoff_latitude.between(test.dropoff_latitude.min(), test.dropoff_latitude.max())]\n\n#convert timestamp to features to be used as categorial\ntrain_sample['hour'] = train_sample['pickup_datetime'].apply(lambda time: time.hour)\ntrain_sample['month'] = train_sample['pickup_datetime'].apply(lambda time: time.month)\ntrain_sample['day_of_week'] = train_sample['pickup_datetime'].apply(lambda time: time.dayofweek)\ntrain_sample['year'] = train_sample['pickup_datetime'].apply(lambda t: t.year)\n\n\ntest['hour'] = test['pickup_datetime'].apply(lambda time: time.hour)\ntest['month'] = test['pickup_datetime'].apply(lambda time: time.month)\ntest['day_of_week'] = test['pickup_datetime'].apply(lambda time: time.dayofweek)\ntest['year'] = test['pickup_datetime'].apply(lambda t: t.year)\n\n#reduce memory by converting datatypes\ntrain_sample['hour'] = train_sample.hour.astype(dtype = 'uint8')\ntrain_sample['month'] = train_sample.month.astype(dtype = 'uint8')\ntrain_sample['day_of_week'] = train_sample.day_of_week.astype(dtype = 'uint8')\ntrain_sample['year'] = train_sample.year.astype(dtype = 'uint16')\n\n\ntest['hour'] = test.hour.astype(dtype = 'uint8')\ntest['month'] = test.month.astype(dtype = 'uint8')\ntest['day_of_week'] = test.day_of_week.astype(dtype = 'uint8')\ntest['year'] = test.year.astype(dtype = 'uint16')\n\n\n# Join Weather data\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(nyc_weather, how = 'left', left_on ='pickup_day', right_on = 'DATE')\ntrain_sample.drop(columns = ['pickup_day','DATE'], axis = 0, inplace = True)\n\ntest['pickup_day'] = test.pickup_datetime.dt.floor('d')\ntest = test.merge(nyc_weather, how = 'left', left_on ='pickup_day', right_on = 'DATE')\ntest.drop(columns = ['pickup_day','DATE'], axis = 0, inplace = True)\n\ntrain_sample['AWND'] = train_sample.AWND.astype(dtype = 'float16')\ntrain_sample['PRCP'] = train_sample.PRCP.astype(dtype = 'float16')\ntrain_sample['SNOW'] = train_sample.day_of_week.astype(dtype = 'float16')\ntrain_sample['TMAX'] = train_sample.TMAX.astype(dtype = 'float16')\ntrain_sample['TMIN'] = train_sample.TMAX.astype(dtype = 'float16')\n\ntest['AWND'] = test.AWND.astype(dtype = 'float16')\ntest['PRCP'] = test.PRCP.astype(dtype = 'float16')\ntest['SNOW'] = test.day_of_week.astype(dtype = 'float16')\ntest['TMAX'] = test.TMAX.astype(dtype = 'float16')\ntest['TMIN'] = test.TMAX.astype(dtype = 'float16')\n\n#create weather features\n#extreme temps\ntrain_sample['hot_day'] = np.where(train_sample.TMAX >= 30,1,0)\ntrain_sample['cold_day'] = np.where(train_sample.TMIN <= 0,1,0)\ntest['hot_day'] =  np.where(test.TMAX >= 30,1,0)\ntest['cold_day'] = np.where(test.TMIN <= 0,1,0)\ntrain_sample['hot_day'] = train_sample.hot_day.astype(dtype = 'uint8')\ntrain_sample['cold_day'] = train_sample.cold_day.astype(dtype = 'uint8')\ntest['hot_day'] = test.hot_day.astype(dtype = 'uint8')\ntest['cold_day'] = test.cold_day.astype(dtype = 'uint8')\n\n#rain and snow\ntrain_sample['rainy_day'] = np.where(train_sample.PRCP >= 0,1,0)\ntrain_sample['snowy_day'] = np.where(train_sample.SNOW <= 0,1,0)\ntest['rainy_day'] =  np.where(test.PRCP >= 0,1,0)\ntest['snowy_day'] = np.where(test.SNOW <= 0,1,0)\ntrain_sample['rainy_day'] = train_sample.rainy_day.astype(dtype = 'uint8')\ntrain_sample['snowy_day'] = train_sample.snowy_day.astype(dtype = 'uint8')\ntest['rainy_day'] = test.rainy_day.astype(dtype = 'uint8')\ntest['snowy_day'] = test.snowy_day.astype(dtype = 'uint8')\n\n#windy days\ntrain_sample['windy_day'] = np.where(train_sample.AWND >= 0,1,0)\ntest['windy_day'] =  np.where(test.AWND >= 0,1,0)\ntrain_sample['windy_day'] = train_sample.windy_day.astype(dtype = 'uint8')\ntest['windy_day'] = test.windy_day.astype(dtype = 'uint8')\n\n\n#calculate distance between pickup and dropoff\ndef degree_to_radion(degree):\n    return degree*(np.pi/180)\n\ndef calculate_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n    \n    from_lat = degree_to_radion(pickup_latitude)\n    from_long = degree_to_radion(pickup_longitude)\n    to_lat = degree_to_radion(dropoff_latitude)\n    to_long = degree_to_radion(dropoff_longitude)\n    \n    radius = 6371.01\n    \n    lat_diff = to_lat - from_lat\n    long_diff = to_long - from_long\n\n    a = np.sin(lat_diff / 2)**2 + np.cos(degree_to_radion(from_lat)) * np.cos(degree_to_radion(to_lat)) * np.sin(long_diff / 2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    \n    return radius * c\n\ntrain_sample['distance'] = calculate_distance(train_sample.pickup_latitude, train_sample.pickup_longitude, train_sample.dropoff_latitude, train_sample.dropoff_longitude)\ntest['distance'] = calculate_distance(test.pickup_latitude, test.pickup_longitude, test.dropoff_latitude, test.dropoff_longitude)\n\ntrain_sample['distance'] = train_sample.distance.astype(dtype = 'float32')\ntest['distance'] = test.distance.astype(dtype = 'float32')\n\n\n#combine day and hour to make every hour of the week a binary feature\ntrain_sample['day_hour'] = train_sample.day_of_week.astype(str) + \"_\" + train_sample.hour.astype(str)\ntrain_sample['day_hour'] = train_sample['day_hour'].astype('category')\n\ntest['day_hour'] = test.day_of_week.astype(str) + test.hour.astype(str)\ntest['day_hour'] = test['day_hour'].astype('category')\n\n#filter out negative fares\ntrain_sample = train_sample[train_sample.fare_amount > 0]\n\n#holidays\ntrain_sample['pickup_day'] = train_sample.pickup_datetime.dt.floor('d')\ntrain_sample = train_sample.merge(holidays, left_on = 'pickup_day', right_on = 'Date', how = 'left')\ntrain_sample['Holiday'] =train_sample.Holiday.fillna('None')\n\nle = LabelEncoder()\ntrain_sample['holiday'] = le.fit_transform(train_sample.Holiday.values)\ntrain_sample.drop(['Holiday','Date','pickup_day'], axis = 1, inplace = True)\n\ntest['pickup_day'] = test.pickup_datetime.dt.floor('d')\ntest = test.merge(holidays, left_on = 'pickup_day', right_on = 'Date', how = 'left')\ntest['Holiday'] =test.Holiday.fillna('None')\n\ntest['holiday'] = le.fit_transform(test.Holiday.values)\ntest.drop(['Holiday','Date','pickup_day'], axis = 1, inplace = True)\n\n\ntrain_sample['holiday'] = train_sample.holiday.astype(dtype = 'uint8')\ntest['holiday'] = test.holiday.astype(dtype = 'uint8')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3070464e94a6efccecc0629e7505ecfd4835a0e"},"cell_type":"code","source":"train_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c33259e06e349c3084186adef96b31949f459387"},"cell_type":"markdown","source":"Here, I identify 200 'neighborhoods' throughout the city using Kmeans clustering. I include all possible coordinate pairs from both the training sample and the test set, rounded to 4 decimal places, and take the unique values (note: I am loading a model from an earlier kernel, which was trained on the full test set and a sample of 25M rows from the training set) Rounding and taking unique values makes the neighborhoods more evenly distributed, instead of being concentrated in the most frequent pickup/dropoff points. Without doing this, the cluster centers are much more densely located in Manhattan and even more sparse in the outer boroughs. Rounding to 4 decimal places performed best on the test set. \n\nWith the trained model, I label each pickup/dropoff 'neighborhood' on the training and test set, which I will encode as categorical features."},{"metadata":{"trusted":true,"_uuid":"ad31710bbff94deb192ac6df294fb0dd2cf655be","_kg_hide-input":false},"cell_type":"code","source":"# Create set of unique locations rounded to 4 decimal places. This prevents the model from biasing towards more frequently used pickup/dropoff spots\nfull_pickups = pd.concat([train_sample[['pickup_longitude','pickup_latitude']],test[['pickup_longitude','pickup_latitude']]], axis = 0)\nfull_pickups.columns = ['x','y']\nfull_dropoffs = pd.concat([train_sample[['dropoff_longitude','dropoff_latitude']],test[['dropoff_longitude','dropoff_latitude']]], axis = 0)\nfull_dropoffs.columns = ['x','y']\nfull_locs = pd.concat([full_pickups,full_dropoffs], axis = 0)\nfull_locs = full_locs.sample(10000000)\nfull_locs['x'] = full_locs.x.round(4)\nfull_locs['y'] = full_locs.y.round(4)\n\nfull_locs = full_locs.groupby(['x','y']).count().reset_index()\nfull_locs.info()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf3f4b916b73fe35764f84d897cb18df28c13eca"},"cell_type":"markdown","source":"The plots below show the resulting clusters of locations and their centers. These are the rounded/unique values, not the training set. "},{"metadata":{"trusted":true,"_uuid":"a7437bc26a1d3e94e9a1da79c9080ef77b18f2a9","_kg_hide-input":true},"cell_type":"code","source":"%%time\n\nX_df = full_locs.copy()\nX_kmeans = full_locs.values\ndel full_locs, full_pickups, full_dropoffs \n\nnum_clusters = 100\n\n#fit the model (done in a previous kernel on a larger set, since the number of rows gets reduced with rounding and taking unique values)\nkmeans = KMeans(n_clusters=num_clusters)\nkmeans.fit(X_kmeans)\n\n#load model from previous kernel trained on 25M observations\n#with open('../input/taxi-weather-holidays-kmeans-neighborhoods/kmeans_200_round4.pkl', 'rb') as fid:\n#    kmeans = pickle.load(fid)\n\n#create labels for graph below\nz = kmeans.predict(X_kmeans)\n\ncenters = kmeans.cluster_centers_\n\nx_centers = [pair[0] for pair in centers]\ny_centers = [pair[1] for pair in centers]\nz_centers = np.arange(num_clusters)\n\n#locations plotted with clusters as different shades\nplt.subplot(1,2,1)\nplt.scatter(X_df['x'], X_df['y'], c=z)\nplt.gray()\nplt.xlabel('Pickup/Dropoff Longitude')\nplt.ylabel('Pickup/Dropoff Latitude')\nplt.title('Clusters of NYC locations')\nplt.subplot(1,2,2)\n#plot of cluster center locations\nplt.scatter(x_centers, y_centers, c=z_centers)\nplt.gray()\nplt.xlabel('Pickup/Dropoff Longitude')\nplt.ylabel('Pickup/Dropoff Latitude')\nplt.title('Cluster Centers of NYC locations')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19d7d3daa7aee4580eb664c9df43856edb2c1d61","_kg_hide-input":true},"cell_type":"code","source":"#add cluster labels as new features\ndel X_kmeans, X_df\n#train_sample = train_sample.sample(10000000)\n\ntrain_sample['pickup_neighborhood'] = kmeans.predict(np.column_stack([train_sample.pickup_longitude.values,train_sample.pickup_latitude.values]))\ntrain_sample['dropoff_neighborhood'] = kmeans.predict(np.column_stack([train_sample.dropoff_longitude.values,train_sample.dropoff_latitude.values]))\n\ntest['pickup_neighborhood'] =  kmeans.predict(np.column_stack([test.pickup_longitude.values, test.pickup_latitude.values]))\ntest['dropoff_neighborhood'] = kmeans.predict(np.column_stack([test.dropoff_longitude.values,test.dropoff_latitude.values]))\n\ntrain_sample['pickup_neighborhood'] = train_sample.pickup_neighborhood.astype(dtype = 'uint8')\ntrain_sample['dropoff_neighborhood'] = train_sample.dropoff_neighborhood.astype(dtype = 'uint8')\n\ntest['pickup_neighborhood'] = test.pickup_neighborhood.astype(dtype = 'uint8')\ntest['dropoff_neighborhood'] = test.dropoff_neighborhood.astype(dtype = 'uint8')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28a232cfcb052babc005006111acd288354d10a3"},"cell_type":"code","source":"train_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dc5b8a1daa7c094b194153148d22aaf285b46a1","_kg_hide-input":true},"cell_type":"code","source":"#save kmeans model for future use\nwith open('kmeans_100_round4_v2.pkl', 'wb') as fid:\n    pickle.dump(kmeans, fid)    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a171105b0c357bb4ae663a8939b226e9fb22f84"},"cell_type":"markdown","source":"Before I train the final model, I need to convert the training set into the proper format. Since there are a large number of binary features resulting from the neighborhoods and day/hour combinations, I use sparse matricies.  I impute any missing values with the mean, then split the set 90/10 for cross validation, in this case just to see how the test score compares to the submission. "},{"metadata":{"trusted":true,"_uuid":"6ac33ced3e49b4b58e98900c13e2eab034a2f613","_kg_hide-input":true},"cell_type":"code","source":"#create final array for model\ncategorical_cols = ['day_hour','month','year','pickup_neighborhood','dropoff_neighborhood','passenger_count','holiday','hot_day','cold_day','rainy_day','snowy_day','windy_day'] #'day_hour','hot_day','cold_day','jfk_pickup','jfk_dropoff','lga_pickup','lga_dropoff','ewr_pickup','ewr_dropoff'     'hour','day_of_week', 'pickup_lat_round','pickup_long_round'\nnumerical_cols = ['distance'] # 'delta_lat','delta_long', , 'pickup_latitude','pickup_longitude','AWND','PRCP','SNOW','TMAX','TMIN'\n\n#subset categorical features for onehot encoding, return sparse matrix\nX_cats = train_sample[categorical_cols].values\nX_cats_test = test[categorical_cols].values\nX_cats_full = np.append(X_cats, X_cats_test, axis = 0)\n\nohe = OneHotEncoder(categories = 'auto')\nX_onehot = ohe.fit_transform(X_cats_full)\ndel X_cats,X_cats_test, X_cats_full\n\n#subset numerical columns and convert to sparse in order to combine with categorical subset\nX_nums = train_sample[numerical_cols].values\nX_nums_test = test[numerical_cols].values\nX_nums_full = np.append(X_nums, X_nums_test, axis = 0)\n\nX_nums_sparse = csr_matrix(X_nums_full)\ndel X_nums, X_nums_test, X_nums_full\n\n#combine sparse matricies\nX_full = hstack([X_onehot, X_nums_sparse]).tocsr()\n\n#impute any missing data\nsi = SimpleImputer()\nX_full_imputed = si.fit_transform(X_full)\n\nX = X_full_imputed[:train_sample.shape[0],:]\nX_public = X_full_imputed[train_sample.shape[0]:,:]\n\ny = train_sample.fare_amount.values\ndel X_onehot, X_nums_sparse\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .1)\n#del X,y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43ba71adb991542fad2f121c5e89dfab80a60114"},"cell_type":"markdown","source":"Finally, train the Light GBM. "},{"metadata":{"trusted":true,"_uuid":"3d84a9835457b0c808db8149f983c24ce2f1463a"},"cell_type":"code","source":"%%time\n\nparams = {'objective': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 50,\n          'max_depth': 8,\n          'learning_rate': 0.5,\n          'bagging_fraction': 0.8,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.02,\n          'min_child_samples': 10, \n          'min_child_weight': 0.02, \n          'lambda_l2': 0.0475,\n          'verbosity': -1,\n          'data_random_seed': 17,\n          'early_stop': 100,\n          'verbose_eval': 100,\n          'num_rounds': 100} \n\nd_train = lgb.Dataset(X_train, label=y_train)\nd_test = lgb.Dataset(X_test, label=y_test)\nwatchlist = [d_train, d_test]\nnum_rounds = 100\nverbose_eval = 100\nearly_stop = 100\nmodel_lgb = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    \npred_test_y_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n\nprint(\"LGB Loss = \" + str(sqrt(mean_squared_error(y_test,pred_test_y_lgb))))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0effb19c2e28d60c7eddc08a8618440e95f0ffbe"},"cell_type":"markdown","source":"Format the final predictions on the public set for submission, then plot a distribution of the fare amount predictions. "},{"metadata":{"trusted":true,"_uuid":"5a63dcb4390722de089b505da0df34c9c4d9fa9f"},"cell_type":"code","source":"lgb_public= model_lgb.predict(X_public, num_iteration=model_lgb.best_iteration)\n\nfinal_pred_public =lgb_public.flatten()\n\n#clean and format final submission\ntest_predictions_lgb = [float(np.asscalar(x)) for x in final_pred_public]\ntest_predictions_lgb = [x if x>0 else 0 for x in test_predictions_lgb]\nsample = pd.DataFrame({'key': test_id,'fare_amount':test_predictions_lgb})\nsample = sample.reindex(['key', 'fare_amount'], axis=1)\nsample.to_csv('submission_lgb.csv', index=False)\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9a37b1198d21a0995ae13c9e5d71ecc7fb641d6"},"cell_type":"code","source":"plt.rc('figure', figsize=(10, 10))\nplt.hist(test_predictions_lgb, bins = 100)\nplt.xlabel('Predicticted Price')\nplt.ylabel('Frequency')\nplt.title('Predictions from LGB')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}