{"cells":[{"metadata":{"_cell_guid":"56fbfcbd-7cee-4054-9142-48ecc8f689c3","_uuid":"78d4e02d62194c4b78f419ac6b332e02fd1bf6a7"},"cell_type":"markdown","source":"# Predicting sales with a nested LSTM"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"97ff7f5e-de74-4a7c-b034-bcf1432480f2","_uuid":"2dd003abe546efb14933e446b6854697ac10dfca"},"cell_type":"markdown","source":"## Create training and test sets"},{"metadata":{"_cell_guid":"deb3ac11-c600-48dc-b9d7-2b31d802caab","_uuid":"6ed50a5c0665dc88ac7a6d0c8f4f9c57187fd2e6","scrolled":true,"trusted":true},"cell_type":"code","source":"# First we create a dataframe with the raw sales data, which we'll reformat later\nDATA = '../input/'\nsales = pd.read_csv(DATA+'sales_train.csv', parse_dates=['date'], infer_datetime_format=True, dayfirst=True)\nsales.head()","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"e64d8618-78fd-41cf-8361-c12f35ee75db","_uuid":"9069d629e7479bdae511d9f7303762b8fa85a2c1","trusted":true},"cell_type":"code","source":"# Let's also get the test data\ntest = pd.read_csv(DATA+'test.csv')\ntest.head()","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"d55fc349-4a95-4d4a-8aaf-827d57bce777","_uuid":"6b3e74f88c77bf2e0eb76f00482fce1cf70b3b64","trusted":true},"cell_type":"code","source":"# Now we convert the raw sales data to monthly sales, broken out by item & shop\n# This placeholder dataframe will be used later to create the actual training set\ndf = sales.groupby([sales.date.apply(lambda x: x.strftime('%Y-%m')),'item_id','shop_id']).sum().reset_index()\ndf = df[['date','item_id','shop_id','item_cnt_day']]\ndf = df.pivot_table(index=['item_id','shop_id'], columns='date',values='item_cnt_day',fill_value=0).reset_index()\ndf.head()","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"910d25e3-8401-4815-8248-043d44139e13","_uuid":"8983b18b147317811bd7d1ceeec0b1502caab8ce","trusted":true},"cell_type":"code","source":"# Merge the monthly sales data to the test data\n# This placeholder dataframe now looks similar in format to our training data\ndf_test = pd.merge(test, df, on=['item_id','shop_id'], how='left')\ndf_test = df_test.fillna(0)\ndf_test.head()","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"2565c11e-6b3a-4268-becd-5cab11fee861","_uuid":"303707b5b9d2c54fc771ecaae2117ce1b33d067c","scrolled":true,"trusted":true},"cell_type":"code","source":"# Remove the categorical data from our test data, we're not using it\ndf_test = df_test.drop(labels=['ID', 'shop_id', 'item_id'], axis=1)\ndf_test.head()","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"fde5595e-0cf6-4620-87e2-6031ba8ea68d","_uuid":"005d1e880ce7bea66471a7fb0e67d051cc133550","trusted":true},"cell_type":"code","source":"# Now we finally create the actual training set\n# Let's use the '2015-10' sales column as the target to predict\nTARGET = '2015-10'\ny_train = df_test[TARGET]\nX_train = df_test.drop(labels=[TARGET], axis=1)\n\nprint(y_train.shape)\nprint(X_train.shape)\nX_train.head()","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"fe54e321-f3d2-4217-a396-4416285ff1ff","_uuid":"846ff06038a2cf228b383d3c3c75989ae3a726be","trusted":true},"cell_type":"code","source":"# To make the training set friendly for keras, we convert it to a numpy matrix\nX_train = X_train.as_matrix()\nX_train = X_train.reshape((214200, 33, 1))\n\ny_train = y_train.as_matrix()\ny_train = y_train.reshape(214200, 1)\n\nprint(y_train.shape)\nprint(X_train.shape)\n\nX_train[:1]","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"d7e744b1-4707-4df7-b8b4-4c2b92b0856b","_uuid":"2214c539e1f1348b9fb5e903df514434c0e4191e","trusted":true},"cell_type":"code","source":"# Lastly we create the test set by converting the test data to a numpy matrix\n# We drop the first month so that our trained LSTM can output predictions beyond the known time range\nX_test = df_test.drop(labels=['2013-01'],axis=1)\nX_test = X_test.as_matrix()\nX_test = X_test.reshape((214200, 33, 1))\nprint(X_test.shape)","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"0c696412-ce99-457c-871f-2a021ee9a779","_uuid":"9c08c7366882bfb520eda5a36e0b8a1fa1dc1c43"},"cell_type":"markdown","source":"## Keras implementation of Nested LSTMs\n\nWe will train a [nested LSTM](https://arxiv.org/abs/1801.10308) model using keras, as implemented by [Somshubra Majumdar](https://github.com/titu1994).  \n(see: https://github.com/titu1994/Nested-LSTM)\n\nThe below code cell contains the implementation of the `NestedLSTM` class that we'll use to create our model."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nimport warnings\n\nfrom keras import backend as K\nfrom keras import activations\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.engine import Layer\nfrom keras.engine import InputSpec\nfrom keras.legacy import interfaces\nfrom keras.layers import RNN\nfrom keras.layers.recurrent import _generate_dropout_mask, _generate_dropout_ones\nfrom keras.layers import LSTMCell, LSTM, Bidirectional\n\n\n\nclass NestedLSTMCell(Layer):\n    \"\"\"Nested NestedLSTM Cell class.\n\n    Derived from the paper [Nested LSTMs](https://arxiv.org/abs/1801.10308)\n    Ref: [Tensorflow implementation](https://github.com/hannw/nlstm)\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        depth: Depth of nesting of the memory component.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        cell_activation: Activation function of the first cell gate.\n            Note that in the paper only the first cell_activation is identity.\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, must be 2.\n            Mode 1 will structure its operations as a larger number of\n            smaller dot products and additions, whereas mode 2 will\n            batch them into fewer, larger operations. These modes will\n            have different performance profiles on different hardware and\n            for different applications.\n    \"\"\"\n\n    def __init__(self, units, depth,\n                 activation='tanh',\n                 recurrent_activation='sigmoid',\n                 cell_activation='linear',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=False,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=2,\n                 **kwargs):\n        super(NestedLSTMCell, self).__init__(**kwargs)\n\n        if depth < 1:\n            raise ValueError(\"`depth` must be at least 1. For better performance, consider using depth > 1.\")\n\n        if implementation != 1:\n            warnings.warn(\n                \"Nested LSTMs only supports implementation 2 for the moment. Defaulting to implementation = 2\")\n            implementation = 2\n\n        self.units = units\n        self.depth = depth\n        self.activation = activations.get(activation)\n        self.recurrent_activation = activations.get(recurrent_activation)\n        self.cell_activation = activations.get(cell_activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.unit_forget_bias = unit_forget_bias\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.implementation = implementation\n        self.state_size = tuple([self.units] * (self.depth + 1))\n        self._dropout_mask = None\n        self._nested_recurrent_masks = None\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.kernels = []\n        self.biases = []\n\n        for i in range(self.depth):\n            if i == 0:\n                input_kernel = self.add_weight(shape=(input_dim, self.units * 4),\n                                               name='input_kernel_%d' % (i + 1),\n                                               initializer=self.kernel_initializer,\n                                               regularizer=self.kernel_regularizer,\n                                               constraint=self.kernel_constraint)\n                hidden_kernel = self.add_weight(shape=(self.units, self.units * 4),\n                                                name='kernel_%d' % (i + 1),\n                                                initializer=self.recurrent_initializer,\n                                                regularizer=self.recurrent_regularizer,\n                                                constraint=self.recurrent_constraint)\n                kernel = K.concatenate([input_kernel, hidden_kernel], axis=0)\n            else:\n                kernel = self.add_weight(shape=(self.units * 2, self.units * 4),\n                                         name='kernel_%d' % (i + 1),\n                                         initializer=self.recurrent_initializer,\n                                         regularizer=self.recurrent_regularizer,\n                                         constraint=self.recurrent_constraint)\n            self.kernels.append(kernel)\n\n        if self.use_bias:\n            if self.unit_forget_bias:\n                def bias_initializer(_, *args, **kwargs):\n                    return K.concatenate([\n                        self.bias_initializer((self.units,), *args, **kwargs),\n                        initializers.Ones()((self.units,), *args, **kwargs),\n                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n                    ])\n            else:\n                bias_initializer = self.bias_initializer\n\n            for i in range(self.depth):\n                bias = self.add_weight(shape=(self.units * 4,),\n                                       name='bias_%d' % (i + 1),\n                                       initializer=bias_initializer,\n                                       regularizer=self.bias_regularizer,\n                                       constraint=self.bias_constraint)\n                self.biases.append(bias)\n        else:\n            self.biases = None\n\n        self.built = True\n\n    def call(self, inputs, states, training=None):\n        if 0 < self.dropout < 1 and self._dropout_mask is None:\n            self._dropout_mask = _generate_dropout_mask(\n                _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n                self.dropout,\n                training=training,\n                count=1)\n        if (0 < self.recurrent_dropout < 1 and\n                self._nested_recurrent_masks is None):\n            _nested_recurrent_mask = _generate_dropout_mask(\n                _generate_dropout_ones(inputs, self.units),\n                self.recurrent_dropout,\n                training=training,\n                count=self.depth)\n            self._nested_recurrent_masks = _nested_recurrent_mask\n\n        # dropout matrices for input units\n        dp_mask = self._dropout_mask\n        # dropout matrices for recurrent units\n        rec_dp_masks = self._nested_recurrent_masks\n\n        h_tm1 = states[0]  # previous memory state\n        c_tm1 = states[1:self.depth + 1]  # previous carry states\n\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n\n        h, c = self.nested_recurrence(inputs,\n                                      hidden_state=h_tm1,\n                                      cell_states=c_tm1,\n                                      recurrent_masks=rec_dp_masks,\n                                      current_depth=0)\n\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                h._uses_learning_phase = True\n        return h, c\n\n    def nested_recurrence(self, inputs, hidden_state, cell_states, recurrent_masks, current_depth):\n        h_state = hidden_state\n        c_state = cell_states[current_depth]\n\n        if 0.0 < self.recurrent_dropout <= 1. and recurrent_masks is not None:\n            hidden_state = h_state * recurrent_masks[current_depth]\n\n        ip = K.concatenate([inputs, hidden_state], axis=-1)\n        gate_inputs = K.dot(ip, self.kernels[current_depth])\n\n        if self.use_bias:\n            gate_inputs = K.bias_add(gate_inputs, self.biases[current_depth])\n\n        i = gate_inputs[:, :self.units]  # input gate\n        f = gate_inputs[:, self.units * 2: self.units * 3]  # forget gate\n        c = gate_inputs[:, self.units: 2 * self.units]  # new input\n        o = gate_inputs[:, self.units * 3: self.units * 4]  # output gate\n\n        inner_hidden = c_state * self.recurrent_activation(f)\n\n        if current_depth == 0:\n            inner_input = self.recurrent_activation(i) + self.cell_activation(c)\n        else:\n            inner_input = self.recurrent_activation(i) + self.activation(c)\n\n        if (current_depth == self.depth - 1):\n            new_c = inner_hidden + inner_input\n            new_cs = [new_c]\n        else:\n            new_c, new_cs = self.nested_recurrence(inner_input,\n                                                   hidden_state=inner_hidden,\n                                                   cell_states=cell_states,\n                                                   recurrent_masks=recurrent_masks,\n                                                   current_depth=current_depth + 1)\n\n        new_h = self.activation(new_c) * self.recurrent_activation(o)\n        new_cs = [new_h] + new_cs\n\n        return new_h, new_cs\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'depth': self.depth,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'cell_activation': activations.serialize(self.cell_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(NestedLSTMCell, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass NestedLSTM(RNN):\n    \"\"\"Nested Long-Short-Term-Memory layer - [Nested LSTMs](https://arxiv.org/abs/1801.10308).\n\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        depth: Depth of nesting of the memory component.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        recurrent_activation: Activation function to use\n            for the recurrent step\n            (see [activations](../activations.md)).\n        cell_activation: Activation function of the first cell gate.\n            Note that in the paper only the first cell_activation is identity.\n            (see [activations](../activations.md)).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, either 1 or 2.\n            Mode 1 will structure its operations as a larger number of\n            smaller dot products and additions, whereas mode 2 will\n            batch them into fewer, larger operations. These modes will\n            have different performance profiles on different hardware and\n            for different applications.\n        return_sequences: Boolean. Whether to return the last output.\n            in the output sequence, or the full sequence.\n        return_state: Boolean. Whether to return the last state\n            in addition to the output.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n\n    # References\n        - [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (original 1997 paper)\n        - [Learning to forget: Continual prediction with NestedLSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n        - [Nested LSTMs](https://arxiv.org/abs/1801.10308)\n    \"\"\"\n\n    @interfaces.legacy_recurrent_support\n    def __init__(self, units, depth,\n                 activation='tanh',\n                 recurrent_activation='sigmoid',\n                 cell_activation='linear',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 unit_forget_bias=False,\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=1,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 **kwargs):\n        if implementation == 0:\n            warnings.warn('`implementation=0` has been deprecated, '\n                          'and now defaults to `implementation=2`.'\n                          'Please update your layer call.')\n        if K.backend() == 'theano':\n            warnings.warn(\n                'RNN dropout is no longer supported with the Theano backend '\n                'due to technical limitations. '\n                'You can either set `dropout` and `recurrent_dropout` to 0, '\n                'or use the TensorFlow backend.')\n            dropout = 0.\n            recurrent_dropout = 0.\n\n        cell = NestedLSTMCell(units, depth,\n                              activation=activation,\n                              recurrent_activation=recurrent_activation,\n                              cell_activation=cell_activation,\n                              use_bias=use_bias,\n                              kernel_initializer=kernel_initializer,\n                              recurrent_initializer=recurrent_initializer,\n                              unit_forget_bias=unit_forget_bias,\n                              bias_initializer=bias_initializer,\n                              kernel_regularizer=kernel_regularizer,\n                              recurrent_regularizer=recurrent_regularizer,\n                              bias_regularizer=bias_regularizer,\n                              kernel_constraint=kernel_constraint,\n                              recurrent_constraint=recurrent_constraint,\n                              bias_constraint=bias_constraint,\n                              dropout=dropout,\n                              recurrent_dropout=recurrent_dropout,\n                              implementation=implementation)\n        super(NestedLSTM, self).__init__(cell,\n                                         return_sequences=return_sequences,\n                                         return_state=return_state,\n                                         go_backwards=go_backwards,\n                                         stateful=stateful,\n                                         unroll=unroll,\n                                         **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):\n        self.cell._dropout_mask = None\n        self.cell._nested_recurrent_masks = None\n        return super(NestedLSTM, self).call(inputs,\n                                            mask=mask,\n                                            training=training,\n                                            initial_state=initial_state,\n                                            constants=constants)\n\n    @property\n    def units(self):\n        return self.cell.units\n\n    @property\n    def depth(self):\n        return self.cell.depth\n\n    @property\n    def activation(self):\n        return self.cell.activation\n\n    @property\n    def recurrent_activation(self):\n        return self.cell.recurrent_activation\n\n    @property\n    def cell_activation(self):\n        return self.cell.cell_activation\n\n    @property\n    def use_bias(self):\n        return self.cell.use_bias\n\n    @property\n    def kernel_initializer(self):\n        return self.cell.kernel_initializer\n\n    @property\n    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer\n\n    @property\n    def bias_initializer(self):\n        return self.cell.bias_initializer\n\n    @property\n    def unit_forget_bias(self):\n        return self.cell.unit_forget_bias\n\n    @property\n    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer\n\n    @property\n    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer\n\n    @property\n    def bias_regularizer(self):\n        return self.cell.bias_regularizer\n\n    @property\n    def kernel_constraint(self):\n        return self.cell.kernel_constraint\n\n    @property\n    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint\n\n    @property\n    def bias_constraint(self):\n        return self.cell.bias_constraint\n\n    @property\n    def dropout(self):\n        return self.cell.dropout\n\n    @property\n    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout\n\n    @property\n    def implementation(self):\n        return self.cell.implementation\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'depth': self.depth,\n                  'activation': activations.serialize(self.activation),\n                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n                  'cell_activation': activations.serialize(self.cell_activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'unit_forget_bias': self.unit_forget_bias,\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(NestedLSTM, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config):\n        if 'implementation' in config and config['implementation'] == 0:\n            config['implementation'] = 2\n        return cls(**config)","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"e8ac0b5c-09f3-4bf1-8ceb-8f54fb81f9a3","_uuid":"2c9ac0c8f0f2ba3bf312c5fc6224facce415402e"},"cell_type":"markdown","source":"## Build and Train the model"},{"metadata":{"_cell_guid":"d59a768d-c77d-44b4-9d48-14a752962b91","_uuid":"512b649b54d56cad4651e57083735121808bc459","collapsed":true,"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Activation\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras import backend as K","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"6c3898a6-e583-4426-a2bd-27b40e33f0c5","_uuid":"37595bce4deffcd88c38c5eef6e0678a5eadbef8","trusted":true},"cell_type":"code","source":"# Create the model using the NestedLSTM class - two layers are a good starting point\n# Feel free to play around with the number of nodes & other model parameters\nmodel = Sequential()\nmodel.add(NestedLSTM(64, input_shape=(33, 1), depth=3, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1))\n\n# The adam optimizer works pretty well, although you might try RMSProp as well\nmodel.compile(loss='mse',\n              optimizer='adam',\n              metrics=['mean_squared_error'])\nmodel.summary()","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"08d56185-5f4e-44d0-8802-8eddac643569","_uuid":"51b1a7acdbd3ceb2fef16d24385778ed1d68e4ed","trusted":true},"cell_type":"code","source":"# It's training time!\nBATCH = 2000\n\nprint('Training time, it is...')\nmodel.fit(X_train, y_train,\n          batch_size=BATCH,\n          epochs=10\n         )","execution_count":58,"outputs":[]},{"metadata":{"_cell_guid":"4e4c11ad-2a76-4ced-bfff-a24e8ba5ab5c","_uuid":"272b8e352e7c606b1cc3da4f1b12dd2b1c0e4f62"},"cell_type":"markdown","source":"## Get test set predictions and Create submission"},{"metadata":{"_cell_guid":"71f93b2d-f548-4362-a04d-0f5923eabd06","_uuid":"b801abe918b33a85f55f040235c94dbbadb8d045","collapsed":true,"trusted":true},"cell_type":"code","source":"# Get the test set predictions and clip values to the specified range\ny_pred = model.predict(X_test).clip(0., 20.)\n\n# Create the submission file and submit!\npreds = pd.DataFrame(y_pred, columns=['item_cnt_month'])\npreds.to_csv('submission.csv',index_label='ID')","execution_count":59,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}