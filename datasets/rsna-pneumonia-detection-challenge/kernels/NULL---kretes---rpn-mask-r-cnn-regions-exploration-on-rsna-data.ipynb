{"cells":[{"metadata":{"_uuid":"86bb5e47115706aa4e83ff1031ec096aa4d39716"},"cell_type":"markdown","source":"# RPN explortion from mask_rcnn model:\nBased on:\n1. official exploration: https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/inspect_data.ipynb\n2. starter notebook for mask-rcnn: https://www.kaggle.com/drt2290078/mask-rcnn-sample-starter-code\n\nWith that we can look at what is the expected resolution and what should be some hyperparams regarding region proposal network\n    "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input'\n\n# Directory to save logs and trained model\nROOT_DIR = '/kaggle/working'\n\nimport os\nimport sys\nimport itertools\nimport math\nimport logging\nimport json\nimport re\nimport random\nfrom collections import OrderedDict\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.lines as lines\nfrom matplotlib.patches import Polygon\n\n!git clone https://www.github.com/matterport/Mask_RCNN.git\nos.chdir('Mask_RCNN')\nsys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))\nfrom mrcnn import utils\nfrom mrcnn import visualize\nfrom mrcnn.visualize import display_images\nimport mrcnn.model as modellib\nfrom mrcnn.model import log\nfrom mrcnn.config import Config\n\nimport pandas as pd  \nimport glob\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dfa2b6277f177010f9b8df12ed36d24a425b817"},"cell_type":"code","source":"\nimport cv2\nimport pydicom\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b40b6610fc8958fd5a31e721505e667b016eebdc"},"cell_type":"code","source":"# The following parameters have been selected to reduce running time for demonstration purposes \n# These are not optimal \n\nclass DetectorConfig(Config):\n    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n    Overrides values in the base Config class.\n    \"\"\"\n    \n    # Give the configuration a recognizable name  \n    NAME = 'pneumonia'\n    \n    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 8 \n    \n    BACKBONE = 'resnet50'\n    \n    NUM_CLASSES = 2  # background + 1 pneumonia classes\n    \n    # Use small images for faster training. Set the limits of the small side\n    # the large side, and that determines the image shape.\n    IMAGE_MIN_DIM = 384\n    IMAGE_MAX_DIM = 384\n    \n    RPN_ANCHOR_SCALES = (32, 64)\n    \n    TRAIN_ROIS_PER_IMAGE = 16\n    \n    MAX_GT_INSTANCES = 3\n    \n    DETECTION_MAX_INSTANCES = 3\n    DETECTION_MIN_CONFIDENCE = 0.9\n    DETECTION_NMS_THRESHOLD = 0.1\n    \n    RPN_TRAIN_ANCHORS_PER_IMAGE = 16\n    STEPS_PER_EPOCH = 100 \n    TOP_DOWN_PYRAMID_SIZE = 32\n    STEPS_PER_EPOCH = 100\n    \n    \nconfig = DetectorConfig()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"032155c362e0872a33fa510d106dcb6f142967d9"},"cell_type":"code","source":"train_dicom_dir = os.path.join(DATA_DIR, 'stage_1_train_images')\ntest_dicom_dir = os.path.join(DATA_DIR, 'stage_1_test_images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb5882b62d542094f2b549672f09cd4219150baf"},"cell_type":"code","source":"def get_dicom_fps(dicom_dir):\n    dicom_fps = glob.glob(dicom_dir+'/'+'*.dcm')\n    return list(set(dicom_fps))\n\ndef parse_dataset(dicom_dir, anns): \n    image_fps = get_dicom_fps(dicom_dir)\n    image_annotations = {fp: [] for fp in image_fps}\n    for index, row in anns.iterrows(): \n        fp = os.path.join(dicom_dir, row['patientId']+'.dcm')\n        image_annotations[fp].append(row)\n    return image_fps, image_annotations \n\nclass DetectorDataset(utils.Dataset):\n    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n    \"\"\"\n\n    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n        super().__init__(self)\n        \n        # Add classes\n        self.add_class('pneumonia', 1, 'Lung Opacity')\n   \n        # add images \n        for i, fp in enumerate(image_fps):\n            annotations = image_annotations[fp]\n            self.add_image('pneumonia', image_id=i, path=fp, \n                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def load_image(self, image_id):\n        info = self.image_info[image_id]\n        fp = info['path']\n        ds = pydicom.read_file(fp)\n        image = ds.pixel_array\n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1)\n        return image\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotations = info['annotations']\n        count = len(annotations)\n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            for i, a in enumerate(annotations):\n                if a['Target'] == 1:\n                    x = int(a['x'])\n                    y = int(a['y'])\n                    w = int(a['width'])\n                    h = int(a['height'])\n                    mask_instance = mask[:, :, i].copy()\n                    cv2.rectangle(mask_instance, (x, y), (x+w, y+h), 255, -1)\n                    mask[:, :, i] = mask_instance\n                    class_ids[i] = 1\n        return mask.astype(np.bool), class_ids.astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f75b79d15bd9cd6e1a340500e67ada66fe8c9187"},"cell_type":"code","source":"anns = pd.read_csv(os.path.join(DATA_DIR, 'stage_1_train_labels.csv'))\nimage_fps, image_annotations = parse_dataset(train_dicom_dir, anns=anns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbedeeb342fba07a8b186f68ce605cd66422be2a"},"cell_type":"code","source":"# Original DICOM image size: 1024 x 1024\nORIG_SIZE = 1024\n\n######################################################################\n# Modify this line to use more or fewer images for training/validation. \n# To use all images, do: image_fps_list = list(image_fps)\nimage_fps_list = list(image_fps[:5000]) \n#####################################################################\n\n# split dataset into training vs. validation dataset \n# split ratio is set to 0.9 vs. 0.1 (train vs. validation, respectively)\nsorted(image_fps_list)\nrandom.seed(42)\nrandom.shuffle(image_fps_list)\n\nvalidation_split = 0.1\nsplit_index = int((1 - validation_split) * len(image_fps_list))\n\nimage_fps_train = image_fps_list[:split_index]\nimage_fps_val = image_fps_list[split_index:]\n\nprint(len(image_fps_train), len(image_fps_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad0f97a8ed2b64520ac33e85c49e2a80ed34c031"},"cell_type":"code","source":"dataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19a619cdd057a79dc1ba4d3c90953e23ce1c2e52"},"cell_type":"code","source":"dataset = dataset_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48cc4fa8e0313202b5f6da518e3fed1965b78ce6"},"cell_type":"code","source":"masked = [i for i in dataset.image_ids if np.sum(dataset.load_mask(i)[0]) > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8693cf1b6664e58cd03e856b9b65eb4311555648"},"cell_type":"code","source":"len(dataset.image_ids), len(masked)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c061b07415566f3ba399ffc146974d7a1871c3ea"},"cell_type":"code","source":"np.hstack([np.random.choice(dataset.image_ids, 4), np.random.choice(masked, 4)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"477810ebf54e9901e27d0c875e18f2a57f9671b8"},"cell_type":"code","source":"# Load random image and mask.\nimage_id = random.choice(masked)\n# 1572\nimage = dataset.load_image(image_id)\nmask, class_ids = dataset.load_mask(image_id)\n# Compute Bounding box\nbbox = utils.extract_bboxes(mask)\n\n# Display image and additional stats\nprint(\"image_id \", image_id, dataset.image_reference(image_id))\nlog(\"image\", image)\nlog(\"mask\", mask)\nlog(\"class_ids\", class_ids)\nlog(\"bbox\", bbox)\n# Display image and instances\nvisualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, show_mask=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd18e6d863e7a2a1eb56730a8edb016eee2ae087"},"cell_type":"markdown","source":"# Choosing the minimal resolution\n\nI want to see how do the xrays look after resizing, and whether it is theoretically possibe to localize opacity for a resized image.\n\nI found out that 256x256 is too small at least for some images while 384 might be enough. I choose this as a minimal acceptable resolution for convolutional models."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"1c2e539d5c5b0843bcb70a0d6afda46827a599a0"},"cell_type":"code","source":"# Load random image and mask.\ndef show_image_with_resized(image_id):\n\n    # image_id = 1572\n    image = dataset.load_image(image_id)\n    mask, class_ids = dataset.load_mask(image_id)\n\n    # Compute Bounding box\n    bbox = utils.extract_bboxes(mask)\n    # Display image and instances\n    visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, show_mask=False)\n\n    original_shape = image.shape\n    # Resize\n    image, window, scale, padding, _ = utils.resize_image(\n        image, \n        min_dim=config.IMAGE_MIN_DIM, \n        max_dim=config.IMAGE_MAX_DIM,\n        mode=config.IMAGE_RESIZE_MODE)\n    mask = utils.resize_mask(mask, scale, padding)\n    # Compute Bounding box\n    bbox = utils.extract_bboxes(mask)\n\n    # Display image and additional stats\n    print(\"image_id: \", image_id, dataset.image_reference(image_id))\n\n    # Display image and instances\n    visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, show_mask=False)\n    \nshow_image_with_resized(random.choice(dataset.image_ids))\nshow_image_with_resized(random.choice(masked))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44e6695c66e2a61063b2c82287f66213090fb386"},"cell_type":"markdown","source":"# what should candidate region look like...\n\n## First let's look at the prior..."},{"metadata":{"trusted":true,"_uuid":"887386f1ea65cf5c1c0f1acbc3e2ff8b48f337b1"},"cell_type":"code","source":"from https://www.kaggle.com/thomasjpfan/q-a-with-only-pictures it seems that ratio should start at 0.3\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c131257238f1465cfcaef3b8c956ab2b98e5163"},"cell_type":"code","source":"# are there any images with bbox but not diagnosed?\nnp.sum((anns['width'] > 0) & (anns['Target'] == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b58944a416e04bbd2f1e028a2c72871d8c93b3da"},"cell_type":"code","source":"# are there any images without bbox but diagnosed?\nnp.sum(~(anns['width'] > 0) & (anns['Target'] > 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"569feee6eb0d17dfe6bb2bed933459b72f3f5ae7"},"cell_type":"code","source":"predicted_anns = anns[anns.Target == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7c562dae5a2e3b215dfea6d5a2bae5d23178674"},"cell_type":"code","source":"import seaborn as sns\nsns.distplot(predicted_anns['width'])\nsns.distplot(predicted_anns['height'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a09c69a9f55bac58f69d26d4969769db9196205"},"cell_type":"code","source":"sns.distplot((predicted_anns['width'] / predicted_anns['height']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"666d0cf8649bd3284016d820503771b95fec42a0"},"cell_type":"code","source":"# lets pick this configuration:\n\n#Ratios of anchors at each cell (width/height)\nconfig.RPN_ANCHOR_RATIOS = [0.3,0.5,1,2]\nconfig.RPN_ANCHOR_SCALES = np.array([96, 192, 256, 384, 512]) * (config.IMAGE_MIN_DIM / 1024)\nconfig.RPN_ANCHOR_SCALES","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82c2c6325f2388bca383ffae95f7471238a02876"},"cell_type":"code","source":"# Generate Anchors\nbackbone_shapes = modellib.compute_backbone_shapes(config, config.IMAGE_SHAPE)\nanchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, \n                                          config.RPN_ANCHOR_RATIOS,\n                                          backbone_shapes,\n                                          config.BACKBONE_STRIDES, \n                                          config.RPN_ANCHOR_STRIDE)\n\n# Print summary of anchors\nnum_levels = len(backbone_shapes)\nanchors_per_cell = len(config.RPN_ANCHOR_RATIOS)\nprint(\"Count: \", anchors.shape[0])\nprint(\"Scales: \", config.RPN_ANCHOR_SCALES)\nprint(\"ratios: \", config.RPN_ANCHOR_RATIOS)\nprint(\"Anchors per Cell: \", anchors_per_cell)\nprint(\"Levels: \", num_levels)\nanchors_per_level = []\nfor l in range(num_levels):\n    num_cells = backbone_shapes[l][0] * backbone_shapes[l][1]\n    anchors_per_level.append(anchors_per_cell * num_cells // config.RPN_ANCHOR_STRIDE**2)\n    print(\"Anchors in Level {}: {}\".format(l, anchors_per_level[l]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67e4e581e86b5b3d8cef26bf029488aefad589b9"},"cell_type":"code","source":"## Visualize anchors of one cell at the center of the feature map of a specific level\n\n# Load and draw random image\nimage_id = np.random.choice(dataset.image_ids, 1)[0]\nimage, image_meta, _, _, _ = modellib.load_image_gt(dataset, config, image_id)\nfig, ax = plt.subplots(1, figsize=(10, 10))\nax.imshow(image)\nlevels = len(backbone_shapes)\n\nfor level in range(levels):\n    colors = visualize.random_colors(levels)\n    # Compute the index of the anchors at the center of the image\n    level_start = sum(anchors_per_level[:level]) # sum of anchors of previous levels\n    level_anchors = anchors[level_start:level_start+anchors_per_level[level]]\n    print(\"Level {}. Anchors: {:6}  Feature map Shape: {}\".format(level, level_anchors.shape[0], \n                                                                  backbone_shapes[level]))\n    center_cell = backbone_shapes[level] // 2\n    center_cell_index = (center_cell[0] * backbone_shapes[level][1] + center_cell[1])\n    level_center = center_cell_index * anchors_per_cell \n    center_anchor = anchors_per_cell * (\n        (center_cell[0] * backbone_shapes[level][1] / config.RPN_ANCHOR_STRIDE**2) \\\n        + center_cell[1] / config.RPN_ANCHOR_STRIDE)\n    level_center = int(center_anchor)\n\n    # Draw anchors. Brightness show the order in the array, dark to bright.\n    for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]):\n        y1, x1, y2, x2 = rect\n        p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor='none',\n                              edgecolor=(i+1)*np.array(colors[level]) / anchors_per_cell)\n        ax.add_patch(p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad7e0d4dae7dc793761788031c1c4468463c3da1"},"cell_type":"code","source":"# let's display some image with regions:\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b84c7429ca3ec8ba591ff4cecc7f497ffb89ece8"},"cell_type":"code","source":"# Create data generator\nrandom_rois = 2000\ng = modellib.data_generator(\n    dataset, config, shuffle=True, random_rois=random_rois, \n    batch_size=4,\n    detection_targets=True)\n\n# Uncomment to run the generator through a lot of images\n# to catch rare errors\n# for i in range(1000):\n#     print(i)\n#     _, _ = next(g)\n\n# Get Next Image\nif random_rois:\n    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks, rpn_rois, rois], \\\n    [mrcnn_class_ids, mrcnn_bbox, mrcnn_mask] = next(g)\n    \n    log(\"rois\", rois)\n    log(\"mrcnn_class_ids\", mrcnn_class_ids)\n    log(\"mrcnn_bbox\", mrcnn_bbox)\n    log(\"mrcnn_mask\", mrcnn_mask)\nelse:\n    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_boxes, gt_masks], _ = next(g)\n    \nlog(\"gt_class_ids\", gt_class_ids)\nlog(\"gt_boxes\", gt_boxes)\nlog(\"gt_masks\", gt_masks)\nlog(\"rpn_match\", rpn_match, )\nlog(\"rpn_bbox\", rpn_bbox)\nimage_id = modellib.parse_image_meta(image_meta)[\"image_id\"][0]\nprint(\"image_id: \", image_id, dataset.image_reference(image_id))\n\n# Remove the last dim in mrcnn_class_ids. It's only added\n# to satisfy Keras restriction on target shape.\nmrcnn_class_ids = mrcnn_class_ids[:,:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73852ea360fdca3339eb0e11db42a34ffbb3f5c3"},"cell_type":"code","source":"b = 0\n\n# Restore original image (reverse normalization)\nsample_image = modellib.unmold_image(normalized_images[b], config)\n\n# Compute anchor shifts.\nindices = np.where(rpn_match[b] == 1)[0]\nrefined_anchors = utils.apply_box_deltas(anchors[indices], rpn_bbox[b, :len(indices)] * config.RPN_BBOX_STD_DEV)\nlog(\"anchors\", anchors)\nlog(\"refined_anchors\", refined_anchors)\n\n# Get list of positive anchors\npositive_anchor_ids = np.where(rpn_match[b] == 1)[0]\nprint(\"Positive anchors: {}\".format(len(positive_anchor_ids)))\nnegative_anchor_ids = np.where(rpn_match[b] == -1)[0]\nprint(\"Negative anchors: {}\".format(len(negative_anchor_ids)))\nneutral_anchor_ids = np.where(rpn_match[b] == 0)[0]\nprint(\"Neutral anchors: {}\".format(len(neutral_anchor_ids)))\n\n# ROI breakdown by class\nfor c, n in zip(dataset.class_names, np.bincount(mrcnn_class_ids[b].flatten())):\n    if n:\n        print(\"{:23}: {}\".format(c[:20], n))\n\n# Show positive anchors\nvisualize.draw_boxes(sample_image, boxes=anchors[positive_anchor_ids], \n                     refined_boxes=refined_anchors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"470422c138ba752475479c414d083faeffbd3314"},"cell_type":"code","source":"visualize.draw_boxes(sample_image, boxes=anchors[negative_anchor_ids])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1dcdd63b20d95e01c640ebcbfbeb8010d195eaf"},"cell_type":"code","source":"visualize.draw_boxes(sample_image, boxes=anchors[np.random.choice(neutral_anchor_ids, 100)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdbce5d87d5b196c26f1b40c1c383b1a297a4d0e"},"cell_type":"code","source":"if random_rois:\n    # Class aware bboxes\n    bbox_specific = mrcnn_bbox[b, np.arange(mrcnn_bbox.shape[1]), mrcnn_class_ids[b], :]\n\n    # Refined ROIs\n    refined_rois = utils.apply_box_deltas(rois[b].astype(np.float32), bbox_specific[:,:4] * config.BBOX_STD_DEV)\n\n    # Class aware masks\n    mask_specific = mrcnn_mask[b, np.arange(mrcnn_mask.shape[1]), :, :, mrcnn_class_ids[b]]\n\n    visualize.draw_rois(sample_image, rois[b], refined_rois, mask_specific, mrcnn_class_ids[b], dataset.class_names)\n    \n    # Any repeated ROIs?\n    rows = np.ascontiguousarray(rois[b]).view(np.dtype((np.void, rois.dtype.itemsize * rois.shape[-1])))\n    _, idx = np.unique(rows, return_index=True)\n    print(\"Unique ROIs: {} out of {}\".format(len(idx), rois.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74167456ec996fb6834b76770a04df5d12282c6f"},"cell_type":"code","source":"if random_rois:\n    # Dispalay ROIs and corresponding masks and bounding boxes\n    ids = random.sample(range(rois.shape[1]), 8)\n\n    images = []\n    titles = []\n    for i in ids:\n        image = visualize.draw_box(sample_image.copy(), rois[b,i,:4].astype(np.int32), [255, 0, 0])\n        image = visualize.draw_box(image, refined_rois[i].astype(np.int64), [0, 255, 0])\n        images.append(image)\n        titles.append(\"ROI {}\".format(i))\n        images.append(mask_specific[i] * 255)\n        titles.append(dataset.class_names[mrcnn_class_ids[b,i]][:20])\n\n    display_images(images, titles, cols=4, cmap=\"Blues\", interpolation=\"none\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"324abfb724d20744bc3e9bac8786298d49f36470"},"cell_type":"code","source":"# Check ratio of positive ROIs in a set of images.\nif random_rois:\n    limit = 10\n    temp_g = modellib.data_generator(\n        dataset, config, shuffle=True, random_rois=10000, \n        batch_size=1, detection_targets=True)\n    total = 0\n    for i in range(limit):\n        _, [ids, _, _] = next(temp_g)\n        positive_rois = np.sum(ids[0] > 0)\n        total += positive_rois\n        print(\"{:5} {:5.2f}\".format(positive_rois, positive_rois/ids.shape[1]))\n    print(\"Average percent: {:.2f}\".format(total/(limit*ids.shape[1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23d39b587cd788a53602d52ab2aea630527c34b1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7be94176bf7f83ab14b32e8afcb3480135987e05"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16282176134386fef75d494aefade6761d183140"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"! rm -rf /kaggle/working/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f1ff1ae2e94b873f617dfb851b092c1005a6e7d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}