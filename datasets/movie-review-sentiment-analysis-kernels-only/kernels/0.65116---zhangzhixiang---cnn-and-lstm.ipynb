{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, SpatialDropout1D\n\nimport tensorflow as tf\nsession_config = tf.ConfigProto(\n    log_device_placement=True,\n    inter_op_parallelism_threads=0,\n    intra_op_parallelism_threads=0,\n    allow_soft_placement=True)\nsess = tf.Session(config=session_config)\n\n\nprint(os.listdir('../input/'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.tsv', sep='\\t')\n\nprint('train set: {0}'.format(train.shape))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc13284ee04df6bff72dc13e57b6f5fcaa29183d"},"cell_type":"code","source":"test = pd.read_csv('../input/test.tsv', sep='\\t')\n\nprint('test set: {0}'.format(train.shape))\ntest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"384ea3b082abe7b9422b02ddd19ccd8af03ba4dc"},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.hist(train['Sentiment'], 5, alpha=0.2, density=True)\n\nplt.hist(\n    [train.loc[train['Phrase'].apply(lambda p: ',' in p), 'Sentiment'],\n     train.loc[train['Phrase'].apply(lambda p: '.' in p), 'Sentiment'],\n     train.loc[train['Phrase'].apply(lambda p: '!' in p), 'Sentiment'],\n     train.loc[train['Phrase'].apply(lambda p: '?' in p), 'Sentiment']],\n    5, alpha=0.5, density=True,\n    label=[',', '.', '!', '?'])\n\nplt.xlabel('sentiment')\nplt.ylabel('probability')\nplt.grid(alpha=0.25)\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26de8763e1ad9adadefac3083f307815cd9096aa"},"cell_type":"code","source":"replace_list = {r\"i'm\": 'i am',\n                r\"'re\": ' are',\n                r\"let’s\": 'let us',\n                r\"'s\":  ' is',\n                r\"'ve\": ' have',\n                r\"can't\": 'can not',\n                r\"cannot\": 'can not',\n                r\"shan’t\": 'shall not',\n                r\"n't\": ' not',\n                r\"'d\": ' would',\n                r\"'ll\": ' will',\n                r\"'scuse\": 'excuse',\n                ',': ' ,',\n                '.': ' .',\n                '!': ' !',\n                '?': ' ?',\n                '\\s+': ' '}\n\ndef clean_text(text):\n    text = text.lower()\n    for s in replace_list:\n        text = text.replace(s, replace_list[s])\n    text = ' '.join(text.split())\n    return text\n\nX_train = train['Phrase'].apply(lambda p: clean_text(p))\nphrase_len = X_train.apply(lambda p: len(p.split(' ')))\nmax_phrase_len = phrase_len.max()+10\nprint('max phrase len: {0}'.format(max_phrase_len-10)+\n      '\\nuse maxlen: {0}'.format(max_phrase_len))\n\nplt.figure(figsize=(10, 8))\nplt.hist(phrase_len, alpha=0.2, density=True)\nplt.xlabel('phrase len')\nplt.ylabel('probability')\nplt.grid(alpha=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fe98a6425edc144eda5a171555fe4d4dab6e663"},"cell_type":"code","source":"y_train = train['Sentiment']\n\ntokenizer = Tokenizer(num_words=8192,\n                      filters='\"#$%&()*+-/:;<=>@[\\]^_`{|}~')\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train, maxlen=max_phrase_len)\ny_train = to_categorical(y_train.values)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\nprint('X_train size: {0}, '.format(X_train.shape)+\n      'y_train size: {0}\\n'.format(y_train.shape)+\n      'X_val size: {0}, '.format(X_val.shape)+\n      'y_val size: {0}'.format(y_val.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"975013c7fe0924b3e13f63e8e925a306f8b58339"},"cell_type":"code","source":"model_cnn = Sequential()\nmodel_cnn.add(Embedding(8192, 256))\nmodel_cnn.add(SpatialDropout1D(0.5))\nmodel_cnn.add(Conv1D(128, 3, padding='same', activation='relu', strides=1))\nmodel_cnn.add(GlobalMaxPooling1D())\nmodel_cnn.add(Dense(256, activation='relu'))\nmodel_cnn.add(Dropout(0.5))\nmodel_cnn.add(Dense(5, activation='softmax'))\nmodel_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_cnn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e3399efd7095ee0c36baff545ab1bebc9316823"},"cell_type":"code","source":"model_cnn.fit(X_train, y_train,\n              validation_data=(X_val, y_val),\n              epochs=8, batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b84bde20ea666462842752a2f469e6b0f846e44"},"cell_type":"code","source":"model_lstm = Sequential()\nmodel_lstm.add(Embedding(8192, 256))\nmodel_lstm.add(SpatialDropout1D(0.3))\nmodel_lstm.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3))\nmodel_lstm.add(Dense(256, activation='relu'))\nmodel_lstm.add(Dropout(0.3))\nmodel_lstm.add(Dense(5, activation='softmax'))\nmodel_lstm.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel_lstm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e67c57f3019f7f68a0f870aeac4968ea1d6618bc"},"cell_type":"code","source":"# for lstm batch_size should not be too large\nmodel_lstm.fit(X_train, y_train,\n               validation_data=(X_val, y_val),\n               epochs=8, batch_size=512, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"398c146585870063ef0f6e460a0479d1ac4f5963"},"cell_type":"code","source":"X_test = test['Phrase'].apply(lambda p: clean_text(p))\n\nprint('X_train size: {0}'.format(X_test.apply(lambda p: len(p.split(' '))).max()))\n\nX_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen=max_phrase_len)\n\nsub = pd.read_csv('../input/sampleSubmission.csv')\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2a4e8bef93ecb4ebe7cb20b36eb26b4f72f2b38"},"cell_type":"code","source":"y_cnn = model_cnn.predict(X_test)\ny_lstm = model_lstm.predict(X_test)\n\ny = y_cnn+y_lstm\ny = np.argmax(y, axis=1)\ny[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"066a1baaedd587d11f64620db3fe6bdb56cc9555"},"cell_type":"code","source":"sub = pd.read_csv('../input/sampleSubmission.csv')\nsub['Sentiment'] = y\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04b581d4b90cab539e61cfa9d517247b473f62b3"},"cell_type":"code","source":"sub.to_csv('test_Submission_3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af9364ce1ad5f15f36d496377b749b2847cfc44f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}