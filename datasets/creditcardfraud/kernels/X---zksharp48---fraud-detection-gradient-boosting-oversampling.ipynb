{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a6d66bb5-3758-07c5-ec62-5449eb03be9e"
      },
      "source": [
        "In this kernel, I demonstrate the performance of applying a Gradient Boosting Classification Tree (GBCT) by using Scikit-Learn. In addition, I use Synthetic Minority Over-Sampling Technique (SMOTE) to oversample the minority class, ie. fraud transactions, to combat the high skewness of this data set. \n",
        "\n",
        "Credit: The general pipeline is built upon [the work](https://www.kaggle.com/joparga3/d/dalpozz/creditcardfraud/in-depth-skewed-data-classif-93-recall-acc-now) of `joparga3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "49c236a5-95b7-14ac-a17a-b3989fb550f6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2fda4e0d-1a46-ac62-86df-7d1e8007ca58"
      },
      "source": [
        "A) DATA PREPROCESSING\n",
        "===\n",
        "Loading\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1e377b5e-c268-aa7d-01f5-70b10d48c330"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../input/creditcard.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "64c126a8-ec44-8561-5c46-fe32f3f4cdcb"
      },
      "source": [
        "Normalization\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69836016-d9d1-142d-0943-3b0b1c4026ff"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "df_scaled = df #Make Duplicate\n",
        "\n",
        "df_scaled['normAmount'] =  StandardScaler().fit_transform(df_scaled['Amount'].values.reshape(-1, 1))\n",
        "df_scaled = df_scaled.drop(['Amount'],axis=1)\n",
        "df_scaled.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c420f07f-b23b-59c9-fe8f-e79088018119"
      },
      "outputs": [],
      "source": [
        "# Number of data points in the minority class && Indices Picking\n",
        "number_records_fraud = len(df_scaled[df_scaled.Class == 1]) #492 Fraud Cases\n",
        "fraud_indices = np.array(df_scaled[df_scaled.Class == 1].index)\n",
        "print(\"Number of Fraud Cases: \", number_records_fraud)\n",
        "\n",
        "\n",
        "# Number of data points in the majority class && Indices Picking\n",
        "number_records_normal = len(df_scaled[df_scaled.Class != 1])\n",
        "normal_indices = df_scaled[df_scaled.Class == 0].index\n",
        "print(\"Number of Normal Cases: \", number_records_normal)\n",
        "\n",
        "# Get fraud Transactions by Filtering\n",
        "df_fraud = df_scaled.iloc[fraud_indices] \n",
        "X_fraud = df_fraud.ix[:,df_fraud.columns != 'Class']\n",
        "y_fraud = df_fraud.ix[:,df_fraud.columns == 'Class']\n",
        "\n",
        "# Get Normal Transactions by Filtering\n",
        "df_normal = df_scaled.iloc[normal_indices] #Get normal Transaction by Filtering\n",
        "X_normal = df_normal.ix[:,df_normal.columns != 'Class']\n",
        "y_normal = df_normal.ix[:,df_normal.columns == 'Class']\n",
        "\n",
        "# Make X,y for classfication\n",
        "X = df_scaled.ix[:, df_scaled.columns != 'Class']\n",
        "yy = df_scaled.ix[:, df_scaled.columns == 'Class']\n",
        "y = np.asarray(yy['Class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7383f91c-5c2e-c891-c213-bb91ead9a01f"
      },
      "source": [
        "\n",
        "Resampling: oversampling using SMOTE\n",
        "---\n",
        "In order to balance the number of samples in our two classes, we will oversample the fraud transaction class by artificially synthesizing fraud transactions using [SMOTE](http://contrib.scikit-learn.org/imbalanced-learn/auto_examples/over-sampling/plot_smote.html#sphx-glr-auto-examples-over-sampling-plot-smote-py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da14ff58-4146-e2c2-c7aa-f031e97ad3b6"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "# Apply SMOTE's\n",
        "kind = 'regular'\n",
        "sm = SMOTE(kind='regular')\n",
        "X_res, y_res = sm.fit_sample(X, y)\n",
        "\n",
        "print(\"esampled Dataset has shape: \", X_res.shape)\n",
        "print(\"Number of Fraud Cases (Real && Synthetic): \", np.sum(y_res))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0ae0feff-fccd-aca6-ba04-5fe2d25274fc"
      },
      "source": [
        "Training Classifier\n",
        "===\n",
        "We apply the train-split operation to both the resampled data set (balanced) and the raw data set (highly skewed). The training will be conducted upon the resampled data set and we want to test the output classifier against the raw data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23581b08-a715-4c75-15c9-90cb4a744baf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y)\n",
        "\n",
        "print(\"Number transactions train dataset: \", len(X_train))\n",
        "print(\"Number transactions test dataset: \", len(X_test))\n",
        "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
        "\n",
        "\n",
        "\n",
        "X_train_res, X_test_res, y_train_res, y_test_res= train_test_split(X_res, y_res)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Number transactions train dataset: \", len(X_train_res))\n",
        "print(\"Number transactions test dataset: \", len(X_test_res))\n",
        "print(\"Total number of transactions: \", len(X_train_res)+len(X_test_res))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d64d755-c614-3059-11e8-28742c1ff264"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "est = GradientBoostingClassifier(n_estimators=200, max_depth=3, learning_rate=1,\n",
        "                                random_state=0, verbose = 1)\n",
        "est.fit(X_train_res, y_train_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b7d3e76f-e1af-4370-4b5a-10704ad84274"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        1#print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cbd8d992-a42f-0f46-70db-d43ef44651cd"
      },
      "source": [
        "On Original Data Space\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "be727f66-7e79-bba7-a7a5-016d0fd62790"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "75be00bf-ed7a-bb73-f456-41b3e4c956d6"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7229a40b-6e67-a867-f90d-5d8565bdab2d"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4721b8f7-4fe0-27a7-d822-67c29048f71e"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1a6351c-31bc-36b1-1c69-c5ae8b670803"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}