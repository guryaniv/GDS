{"cells":[{"metadata":{"collapsed":true,"trusted":true,"_uuid":"550f51721724ce0e1c9ffb1f0effb33c70beb331"},"cell_type":"code","source":"import os ; os.environ['OMP_NUM_THREADS'] = '4'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"57fb0406-c299-45a5-8a79-8f2d0555291e","_uuid":"a99c3c72d52d41b8970e830abbf5af3175e6cbf5","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport operator\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport nltk\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6a88c217-09fb-46e4-8c40-be87e211f41e","_uuid":"01c2d15145496ad3dbde81c45d3329f6cfc6fd9a","collapsed":true,"trusted":true},"cell_type":"code","source":"start = time.time()\ndef print_time(start):\n    time_now = time.time() - start \n    minutes = int(time_now / 60)\n    seconds = int(time_now % 60)\n    if seconds < 10:\n        print('Elapsed time was %d:0%d.' % (minutes, seconds))\n    else:\n        print('Elapsed time was %d:%d.' % (minutes, seconds))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6a9b1e0e-7175-4edd-8421-a87b0ca7b343","_uuid":"423a15cb025b2c7ca242ed61c3bde0c23cef8a15","collapsed":true,"trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv('../input/train.tsv', sep='\\t')\ndf_sub = pd.read_csv('../input/test.tsv', sep='\\t')\n\nsubmission = pd.DataFrame()\nsubmission['test_id'] = df_sub.test_id.copy()\n\ny_target = list(df.price)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be6e48c6-5544-4259-8664-0aa694abbd51","_uuid":"2daa6091569576448d7d4a456f150a8488553ed5"},"cell_type":"markdown","source":"## Impute Missing Values","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"217deb9b-d7d0-4bd8-82b6-c38241d141ca","_uuid":"c245eca9a7e5a9020f55605f4de6cb689a45cb47","trusted":true,"collapsed":true},"cell_type":"code","source":"def null_percentage(column):\n    df_name = column.name\n    nans = np.count_nonzero(column.isnull().values)\n    total = column.size\n    frac = nans / total\n    perc = int(frac * 100)\n    print('%d%% or %d missing from %s column.' % \n          (perc, nans, df_name))\n\ndef check_null(df, columns):\n    for col in columns:\n        null_percentage(df[col])\n        \ncheck_null(df, df.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"94baeead-4275-4e7a-afe3-0f3e8214d341","_uuid":"6106a00e9deb1906416cfc312c0d5d28e4ad122f","collapsed":true,"trusted":true},"cell_type":"code","source":"def merc_imputer(df_temp):\n    df_temp.brand_name = df_temp.brand_name.replace(np.nan, 'no_brand')\n    df_temp.category_name = df_temp.category_name.replace(np.nan, 'uncategorized/uncategorized')\n    df_temp.item_description = df_temp.item_description.replace(np.nan, 'No description yet')\n    df_temp.item_description = df_temp.item_description.replace('No description yet', 'no_description')\n    return df_temp\n\ndf = merc_imputer(df)\ndf_sub = merc_imputer(df_sub)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53f69f2d-4a34-47e4-bb17-c1b3dca68634","_uuid":"01a10d70478cd9de13ccf2c3cafd0c98aa3007cf","trusted":true,"collapsed":true},"cell_type":"code","source":"print('Training Data')\ncheck_null(df, df.columns)\nprint('Submission Data')\ncheck_null(df_sub, df_sub.columns)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8b902c1f-9ee7-447a-bcbe-264779d458eb","_uuid":"6e584e0b8b39a0f5dafd8d09a5f104ddfa619702"},"cell_type":"markdown","source":"# EDA ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"07f984e0-28d2-48ec-9a1b-762d3e41a847","_uuid":"c47630038253f0fecb89f54c35010e8e095a46d4"},"cell_type":"markdown","source":"## Shipping","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"448471b6-e7a6-4601-861b-7e04f854bcb7","_uuid":"3269a6a7b3d4e59591b297f1dc109d5ad6df56ef","trusted":true,"collapsed":true},"cell_type":"code","source":"df.shipping.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e17b44e-6b2d-4bce-a040-aa6c97586545","_uuid":"04ec78c4e703016b9d47b4fd225012ef74601131","trusted":true,"collapsed":true},"cell_type":"code","source":"print('%.1f%% of items have free shipping.' % ((663100 / len(df))*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0dd97aad-82e5-481b-9dc9-877291690ab0","_uuid":"d49f8433429a50b04eac5b57ee2e83466eeb9257"},"cell_type":"markdown","source":"Free shipping items should be priced higher because shipping is included in the price. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0a31f100-28fa-4fe7-9b79-d8ba1cca7fcb","_uuid":"7c1d12d0c01e6bb05437709b61fa19c3a7795a81"},"cell_type":"markdown","source":"### Price","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b4654984-c7b3-4cac-ba16-e11ed4e4a441","_uuid":"0a8391d960231b95ff6d646d081e249276e69ee2","trusted":true,"collapsed":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7640bb53-4fd6-40a2-ab49-68fdf786bc58","_uuid":"79a14a4d834d835739231d3d5234b0650106ea67","trusted":true,"collapsed":true},"cell_type":"code","source":"print('$1 items: ' + str(df.price[df.price == 1].count()))\nprint('$2 items: ' + str(df.price[df.price == 2].count()))\nprint('$3 items: ' + str(df.price[df.price == 3].count()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6daa410c-4429-4c30-bea3-3fc9c7a603e6","_uuid":"a30461d3467758e13e00f1f8ca5bdd907b45205e"},"cell_type":"markdown","source":"There is a minimum price of $3.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"98ccf40e-daf8-42bf-a1db-3698eb968780","_uuid":"03d13c05d98f0a0c906a801c8d4d1dd65175624a","trusted":true,"collapsed":true},"cell_type":"code","source":"plt.figure('Training Price Dist', figsize=(30,10))\nplt.title('Price Distribution for Training - 3 Standard Deviations', fontsize=32)\nplt.hist(df.price.values, bins=145, normed=False, \n         range=[0, (np.mean(df.price.values) + 3 * np.std(df.price.values))])\nplt.axvline(df.price.values.mean(), color='b', linestyle='dashed', linewidth=2)\nplt.xticks(fontsize=24)\nplt.yticks(fontsize=26)\nplt.show()\n\nprint('Line indicates mean price.')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"381bde6c-b7c5-4748-98c2-3a906fc17a67","_uuid":"9df319c16590cc1b92ec36940cad27bd783ad558"},"cell_type":"markdown","source":"Most prices are on the lower end of the spectrum, and items priced above 145 are outliers that make up less that 0.3% of the data. Are there free items? ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1b99134e-e146-4b0d-a7dd-88ea8137a432","_uuid":"911535f6f2536b9f1620c65ad67500354b06ea49","trusted":true,"collapsed":true},"cell_type":"code","source":"print('Free items: %d, representing %.5f%% of all items.' % \n      (df.price[df.price == 0].count(), \n        (df.price[df.price == 0].count() / df.shape[0])))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6feb2169-6e8d-45f0-8a33-8e20ffab072d","_uuid":"b9b2ddcc60add1c75fa33b8644213fae1f7869bc"},"cell_type":"markdown","source":"What does free even mean here? ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4595d434-7888-40cc-8174-783e899c4455","_uuid":"c84d75c0a451c17ab9278eb2124d3b18a7b1222a","trusted":true,"collapsed":true},"cell_type":"code","source":"print('Free items where seller pays shipping: %d.' % \n      df.price[operator.and_(df.price == 0, df.shipping == 1)].count())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2420d35-716e-463e-ab9e-1d0791cad79b","_uuid":"852add93ecca74cc29aa258bf7e20c23c75dd779"},"cell_type":"markdown","source":"This is a tiny outlier. And it seems like some items the sellers actually paid to give away. I'd like to see how many items are listed for a low price but the seller is actually making money off shipping to avoid fees, a common eBay practice. Unfortunately, without data about the actual shipping price, we can't extrapolate any insights here. My approach would be to look at items that are priced lower than average yet have higher than average shipping prices for their name and descriptions. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b7bb949b-fe0d-4b34-b2a5-546fa732e0ba","_uuid":"36b2f3d44ac77d2f14894560132fb7a4e6ffeba1","trusted":true,"collapsed":true},"cell_type":"code","source":"print('No description:', str(df.item_description[df.item_description == 'no_description'].count()))\nprint('Uncategorized:',str(df.category_name[df.category_name == 'uncategorized/uncategorized'].count()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"224e3678-5867-42ca-a169-789d0c530577","_uuid":"eee24d77c2a3aefa482977b020d89e84abc87f9b"},"cell_type":"markdown","source":" Many items lack a description, but few lack a category. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0fffd73f-db93-4868-83a3-798c3062b441","_uuid":"5cb0060e48c8273d213743051730147996783328"},"cell_type":"markdown","source":"### Category Name","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"87f81f05-4280-4749-a0a3-e22d0dfa9465","_uuid":"67b537839fb3328ea78e031f7ea2a0c0c96025fd","trusted":true,"collapsed":true},"cell_type":"code","source":"cat_counts = np.sort(df.category_name.value_counts())\nprint(str(len(cat_counts)) + ' categories total.')\nprint(str(df.shape[0]) + ' records total.')\nprint('Category frequency percentiles, marked by lines: \\n25%%: %d, 50%%: %d, 75%%: %d, 95%%: %d, 97.5%%: %d.' % \n     (cat_counts[int(len(cat_counts)*0.25)], \n      cat_counts[int(len(cat_counts)*0.5)],\n      cat_counts[int(len(cat_counts)*0.75)],\n      cat_counts[int(len(cat_counts)*0.9)],\n      cat_counts[int(len(cat_counts)*0.95)]))\n\ntitle = 'Category Quantity ECDF Without Top 15 Outliers'\nplt.figure(title, figsize=(30,10))\nplt.title(title, fontsize=32)\nx = np.sort(df.category_name.value_counts())\nx = x[0:-15]\ny = np.arange(1, len(x) + 1) / len(x)\nplt.plot(x, y, marker='.', linestyle='none')\nplt.xticks(fontsize=24)\nplt.yticks(fontsize=26)\nplt.axvline(x=x[int(len(x)*0.25)], linewidth=1, color='b')\nplt.axvline(x=x[int(len(x)*0.5)], linewidth=1, color='b')\nplt.axvline(x=x[int(len(x)*0.75)], linewidth=1, color='b')\nplt.axvline(x=x[int(len(x)*0.95)], linewidth=1, color='b')\nplt.axvline(x=x[int(len(x)*0.975)], linewidth=1, color='b')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83424e3d-784c-46d9-ab9e-4c1e8d513a7e","_uuid":"5ab7952ef4a3ce1cf058529101dedf9a85f547e9","trusted":true,"collapsed":true},"cell_type":"code","source":"print('The top 75%% of categories represent %.1f%% of the dataset, and the top 50%% represent %.1f%%.' % \n      ((sum([count for count in cat_counts if count > 10]) / len(df))*100, \n       (sum([count for count in cat_counts if count > 76]) / len(df))*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"712c053b-c107-4952-b327-d4cffdeb4dd2","_uuid":"d92a9ee0f2eb28781fe4b57329aaf32418168c5b"},"cell_type":"markdown","source":"There are a lot of uncommon or unique categories that make up a small percentage of the data. If dimensionality reduction needs to happen here, I think it would be safe to keep only the top half of category names and the remaining ~10th of a percent of data will be grouped together as items with an uncommon category. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"89a1f4a1-eaa5-43fa-bd2e-7133bdb9d3ce","_uuid":"77203c9465dcff71b4a70ab16544d17d21a9a99e","trusted":true,"collapsed":true},"cell_type":"code","source":"title = 'Top 35 Categories'\nplt.figure(title, figsize=(30,10))\ndf.category_name.value_counts()[0:35].plot(kind='bar')\nplt.title(title, fontsize=30)\nplt.yticks(fontsize=18)\nplt.xticks(fontsize=18, rotation=35, ha='right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ebf86b2-f481-4946-bdc5-2d9e853afcab","_uuid":"8bcabd71f1fd0d5b71a7834b442732564ead88d2","collapsed":true},"cell_type":"markdown","source":"## Brand Name","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3570808d-7af6-4304-b12e-a9ee83404265","_uuid":"8b0b933ef975f856605268c6e45ab409bdfdbd25","trusted":true,"collapsed":true},"cell_type":"code","source":"brand_counts = np.sort(df.brand_name.value_counts())\nprint(str(len(brand_counts)) + ' brands total.')\nprint(str(df.shape[0]) + ' records total.')\nprint('Category frequency percentiles, marked by lines: \\n25%%: %d, 50%%: %d, 75%%: %d, 95%%: %d, 97.5%%: %d.' % \n     (brand_counts[int(len(brand_counts)*0.25)], \n      brand_counts[int(len(brand_counts)*0.5)],\n      brand_counts[int(len(brand_counts)*0.75)],\n      brand_counts[int(len(brand_counts)*0.9)],\n      brand_counts[int(len(brand_counts)*0.95)]))\n\ntitle = 'Brand Quantity ECDF Without Top 25 Outliers'\nplt.figure(title, figsize=(30,10))\nplt.title(title, fontsize=32)\nx = np.sort(df.brand_name.value_counts())\nx = x[0:-25]\ny = np.arange(1, len(x) + 1) / len(x)\nplt.plot(x, y, marker='.', linestyle='none')\nplt.xticks(fontsize=24)\nplt.yticks(fontsize=26)\nplt.axvline(x=x[int(len(x)*0.25)], linewidth=1, color='b')\nplt.axvline(x=x[int(len(x)*0.5)], linewidth=1, color='b')\nplt.axvline(x=x[int(len(x)*0.75)], linewidth=1, color='b')\nplt.axvline(x=x[int(len(x)*0.95)], linewidth=1, color='b')\nplt.axvline(x=x[int(len(x)*0.975)], linewidth=1, color='b')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e1f587a3-3aad-4d1c-ae8e-d9ee4b7105a3","_uuid":"8b2ed59251f3a159dc9c78b4104c4c1400bcafc6","trusted":true,"collapsed":true},"cell_type":"code","source":"print('The top 75%% of categories represent %.1f%% of the dataset, and the top 50%% represent %.1f%%.' % \n      ((sum([count for count in brand_counts if count > 1]) / len(df))*100, \n       (sum([count for count in brand_counts if count > 4]) / len(df))*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e19b0b8d-97e2-40cd-9375-2b9661391973","_uuid":"215ab6f24ace8a3107f7ede6f16fde2cff2a107e"},"cell_type":"markdown","source":"A story similar to category_name.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"dd1d19d2-c409-4872-a52c-2452a2bfcb3a","_uuid":"46c6ce276951d807facae15c9e78b3e865aa5375","trusted":true,"collapsed":true},"cell_type":"code","source":"print('%d items, or %.2f%%, are missing a brand name.' % \n      (len(df[df.brand_name == 'no_brand']), \n       len(df[df.brand_name == 'no_brand']) / len(df)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0fa4a609-e971-4b2e-b60e-0742418da99d","_uuid":"ddb0f277f8fb5371330909ea2e511cc8e16f1c10","trusted":true,"collapsed":true},"cell_type":"code","source":"title = 'Top 35 Brands'\nplt.figure(title, figsize=(30,10))\ndf.brand_name.value_counts()[1:70].plot(kind='bar')\nplt.title(title, fontsize=30)\nplt.yticks(fontsize=18)\nplt.xticks(fontsize=18, rotation=45, ha='right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3affcf71-7759-47de-9ae0-375f8d8e75f7","_uuid":"ee4148d218b3414231b1e03f2701496a6a6145ba"},"cell_type":"markdown","source":"The most popular brands, PINK and Nike, are an order of magnitude less frequent than unbranded items. It seems there's a mix of company brands and individual product line brands, as we can see both Victoria's Secret and Pink as well as Nintendo and Pokemon. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"837c8f4f-feb6-4610-9cf3-54a3355d4995","_uuid":"63397bd0c3012b2ee52b5ac8e5baf76e205e6a99","trusted":true,"collapsed":true},"cell_type":"code","source":"title = 'Top Half of Brands'\nplt.figure(title, figsize=(30,10))\ndf.brand_name.value_counts()[50:2500].plot(kind='bar')\nplt.title(title, fontsize=30)\nplt.yticks(fontsize=18)\nplt.xticks(fontsize=0, rotation=45, ha='right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"75f6100c-d2ff-4eae-850c-ae5927b94054","_uuid":"7c2416517a028fe6f5b4751323ff6738341e858d"},"cell_type":"markdown","source":"An exponential growth curve that explodes at the end. I just like making huge charts like this. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"696e583f-3c8d-4873-94eb-93debd32dbb6","_uuid":"a32dd56d73ac301a23f67d01e92ff38f6587fb33","trusted":true,"collapsed":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c0b5c387-679b-4d1c-8a59-ac1faf872f28","_uuid":"f0675756dcc1e3729f3ab3fee8f7df7dd0b0cf77"},"cell_type":"markdown","source":"# Preprocessing ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c6ebea97-ea9c-40c5-a91b-5f2c5f4a6178","_uuid":"40f43437d94b6446195023d37558cfaf9848fdb3"},"cell_type":"markdown","source":"## Natural Language Processing ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4e98a472-f5a9-4b2e-8750-ee4300add57c","_uuid":"6baf79e20d2d3e2342338398ad63fce9465c9d1b","collapsed":true,"trusted":true},"cell_type":"code","source":"import nltk\nnltk.data.path.append(r'D:\\Python\\Data Sets\\nltk_data')\nfrom nltk.corpus import stopwords \nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom scipy import sparse ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51613ff9-478f-4c1d-bdf9-da44ea088f21","_uuid":"c4c332fdfd2707658a03938a05590ee90fdd7109"},"cell_type":"markdown","source":"### Category Name","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"18ea0680-f179-49f1-966c-6b2e794a7cd8","_uuid":"35f48bdf53d47a43b6af2ad0247dd56ba9deb371"},"cell_type":"markdown","source":"This is pretty straightforward. Make dummy categories for each categorical value, with uncommon values just zero. But since this dataset is big and there's a large number of categories, the best way to use this is to use CountVectorizer() because it returns a sparse matrix instead of a dense one. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"baef1164-4e5a-436d-9e1e-a136d1e18ade","_uuid":"4944ff3dd9f5ffe62415db55f8144bd12e9b5c55","collapsed":true,"trusted":true},"cell_type":"code","source":"cat_vec = CountVectorizer(stop_words=[stopwords, string.punctuation], max_features=int(len(cat_counts)*0.5))\ncat_matrix = cat_vec.fit_transform(df.category_name)\ncat_matrix_sub = cat_vec.transform(df_sub.category_name)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cf7741e4-ab5e-498a-8564-7fd57ff57933","_uuid":"d7960e61c1db39e8f1f45729021de144a3d4e9fd","collapsed":true,"trusted":true},"cell_type":"code","source":"# For exploring the tokens. The array is an array inside of an array of one, ravel pulls it out. \ncat_tokens = list(zip(cat_vec.get_feature_names(), np.array(cat_matrix.sum(axis=0)).ravel()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"11493430-e77c-47a9-a3b3-ad477e239b00","_uuid":"351838f228e57cf7511b4084c8271acc3b2bd419"},"cell_type":"markdown","source":"### Brand Name ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a4894f50-9c33-44bf-a599-1f4e8afeb66c","_uuid":"c03084fa7433489c68a5cb4554cb1f33cfa8cf0f","collapsed":true,"trusted":true},"cell_type":"code","source":"brand_vec = CountVectorizer(stop_words=[stopwords, string.punctuation], max_features=int(len(brand_counts)*0.5))\nbrand_matrix = brand_vec.fit_transform(df.brand_name)\nbrand_matrix_sub = brand_vec.transform(df_sub.brand_name)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e143dbde-60ca-4feb-a982-4e4f8349878b","_uuid":"e55876850edd43506111f9bbc3f7e91fe8b3166f","collapsed":true,"trusted":true},"cell_type":"code","source":"brand_tokens = list(zip(brand_vec.get_feature_names(), np.array(brand_matrix.sum(axis=0)).ravel()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b69e55ea-30c3-41db-9af8-cbcb3957d552","_uuid":"d54266e1403d52809cb2cc22c7368f7036dcf4ca"},"cell_type":"markdown","source":"### Item Name","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6e30ff63-e9b5-46c8-9ec4-79c422cdd6ed","_uuid":"428dacdf5ba58ce33d1f9e66efa8b6cb8f18ee57"},"cell_type":"markdown","source":"Item name and description are more complicated. As they are phrases and sentences, the number of words is going to be exponentially larger and the words themselves don't hold equal weight. I'm going to use a statistical method called Term Frequency - Inverse Document Frequency (TF-IDF) that combines the bag of words approach with a weight adjustment based on the overall frequency of each term in the dataset. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8780af06-cf99-4f43-a098-37024880690c","_uuid":"86fd774e5508a5349e2bd676fc16a9b4f44a65c8","collapsed":true,"trusted":true},"cell_type":"code","source":"name_vec = TfidfVectorizer(min_df=15, stop_words=[stopwords, string.punctuation])\nname_matrix = name_vec.fit_transform(df.name)\nname_matrix_sub = name_vec.transform(df_sub.name)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"213dbd4e-ba49-4353-a66f-5797b16f624f","_uuid":"7824b2aa6c216bee4bf64e6e5fbaccac6693b3ac","trusted":true,"collapsed":true},"cell_type":"code","source":"print('Kept %d words.' % len(name_vec.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15003bd3-35c8-458b-8d7a-c40b0e151a7d","_uuid":"3bf3289aadba02b4206195c9087ddf69aa8c53e0"},"cell_type":"markdown","source":"### Description","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f349418e-f927-434b-852f-526febeb35fe","_uuid":"02072505902076e687fbd3e63d9330cb5a1af411","collapsed":true,"trusted":true},"cell_type":"code","source":"desc_vec = TfidfVectorizer(max_features=100000,\n                           stop_words=[stopwords, string.punctuation])\ndesc_matrix = desc_vec.fit_transform(df.item_description)\ndesc_matrix_sub= desc_vec.transform(df_sub.item_description)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12610310-ef3d-432f-b85f-f00a7a216d42","_uuid":"30a15d411357e8742b4b3bc164f6d83f117554e0"},"cell_type":"markdown","source":"### Condition and Shipping ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"fc610fec-7981-4ab8-968d-58031c2c77ad","_uuid":"31d74eba8ffa2d030076ff2dd60fe8304fbf9f5d","collapsed":true,"trusted":true},"cell_type":"code","source":"cond_matrix = sparse.csr_matrix(pd.get_dummies(df.item_condition_id, sparse=True, drop_first=True))\ncond_matrix_sub = sparse.csr_matrix(pd.get_dummies(df_sub.item_condition_id, sparse=True, drop_first=True))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ab8bc2ac-4a9d-4450-be0c-d44e1437e17f","_uuid":"f65cad8df625e302a9ae7f820a61080c863e8673","collapsed":true,"trusted":true},"cell_type":"code","source":"ship_matrix = sparse.csr_matrix(df.shipping).transpose()\nship_matrix_sub = sparse.csr_matrix(df_sub.shipping).transpose()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49c91ef2-4cbc-4bc5-9e4b-ad407b0b7498","_uuid":"e49b24b4b5b143ca89ae3613a762311d57cdc127"},"cell_type":"markdown","source":"### Combine Sparse Matrices ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"65885bc9-ffb6-4ae0-bed6-1881ef7f6b73","_uuid":"b77725c9e6bae163fddfd6ba9e0c871d762174ee","collapsed":true,"trusted":true},"cell_type":"code","source":"sparse_matrix = sparse.csr_matrix(sparse.hstack([cat_matrix, brand_matrix, name_matrix, desc_matrix, \n                               cond_matrix, ship_matrix]))\nsparse_matrix_sub = sparse.csr_matrix(sparse.hstack([cat_matrix_sub, brand_matrix_sub, name_matrix_sub, \n                                   desc_matrix_sub, cond_matrix_sub, ship_matrix_sub]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f6c3bb0-b3f0-4f2c-af45-093c680c7b04","_uuid":"b237d5633fbdab2d33b493910571ca06a4e5eb13","trusted":true,"collapsed":true},"cell_type":"code","source":"if sparse_matrix.shape[1] == sparse_matrix_sub.shape[1]:\n    print('Features check out.')\nelse:\n    print(\"The number of features in training and test set don't match.\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"808e70b5-a16e-4504-9477-309babbcbbd4","_uuid":"c0a7a64f1e66b2d97cc00d87d45659eb7d8b50c5"},"cell_type":"markdown","source":"### Garbage Collection","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"05ba56bb-9b8a-4009-b099-c32d8d129180","_uuid":"741279d536c90b5ed466ccc63f96254117b50960","trusted":true,"collapsed":true},"cell_type":"code","source":"import gc\ndel(cat_matrix, brand_matrix, name_matrix, desc_matrix, cond_matrix, ship_matrix)\ndel(cat_matrix_sub, brand_matrix_sub, name_matrix_sub, desc_matrix_sub, cond_matrix_sub, ship_matrix_sub)\ndel(df, df_sub)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e1d73663-71bd-446d-8665-b1b25b3b4328","_uuid":"6a9ffd7eb7550b67fa64e096992e8cf3524d9419","trusted":true,"collapsed":true},"cell_type":"code","source":"print_time(start)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8b6c5f5d-23c6-4854-adf1-fbcf8f476088","_uuid":"c67995b41e83b7af29c99adef2ee53c461bd32dd","collapsed":true},"cell_type":"markdown","source":"# Training","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"abc8d377-2f82-4bde-a91e-ea5570270ed9","_uuid":"a4a34219f9d3fc8263db593a307467ce2e9d6440","collapsed":true,"trusted":true},"cell_type":"code","source":"def rmsle(true, pred):\n    assert len(pred) == len(true)\n    return np.sqrt(np.mean(np.power(np.log1p(pred)-np.log1p(true), 2)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4cb60f72-ee35-4b79-a6a4-440898968d52","_uuid":"5d243f3e167917ca6a3d649fe9e631e658f18842"},"cell_type":"markdown","source":"Take the log of the target data to boost training accuracy. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"dc7db9a9-1442-4392-bd75-10cb6dec98bb","_uuid":"f0e1f4e5f66ec5e2675c526dc149933d1ff04cf9","collapsed":true,"trusted":true},"cell_type":"code","source":"y_target = np.log10(np.array(y_target) + 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29ccb25e-f76d-4c7a-bf5f-a6c218948f9d","_uuid":"cdf7a16f8e3be148036e7ce0cd1142a4e10da8cd"},"cell_type":"markdown","source":"Split training and test set ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a4c019cd-a667-4fad-bb03-0dacb23001b4","_uuid":"d1c3f768996d49a181aafa8e03179582c7a2c6e6","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(sparse_matrix, y_target, test_size = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dc5cf210-1ecd-4289-8bf8-13baaccd9170","_uuid":"94b6dd35f3d66fb70d95802024fa1ccca810b78b"},"cell_type":"markdown","source":"### Ridge Regression","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"cd1a96a1-4024-4e7a-8f02-276ba0271a33","_uuid":"7b0d81838e386d8dc126fe3756ae58c33a72dccd","trusted":true,"collapsed":true},"cell_type":"code","source":"start = time.time()\nfrom sklearn.linear_model import Ridge\nreg_ridge = Ridge(solver='sag', alpha=5)\nreg_ridge.fit(X_train, y_train)\ny_pred = reg_ridge.predict(X_test)\nprint(rmsle(10 ** y_test - 1, 10 ** y_pred - 1))\nprint_time(start)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0056c8ae-acc2-46df-bfa4-3b15a727216b","_uuid":"a51226e67c1fb2991ed59a2b984c32ec2ec30b36"},"cell_type":"markdown","source":"### LightGBM","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"ec2c433ba3379af3d75fa971cc4c1773de35c9fe"},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0b0c0854-f061-49bb-93e5-1efdc310bf78","_uuid":"e470141dceca1505a747c7902e77c88eb5540690","collapsed":true,"trusted":true},"cell_type":"code","source":"def rmsle_lgb(y_true, y_pred):\n    assert len(y_true) == len(y_pred)\n    return np.sqrt(np.mean(np.power(np.log1p(y_true + 1) - np.log1p(y_pred + 1), 2)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"45b5c08e-aae1-4116-9500-77ac675c1b2b","_uuid":"024e7fc2838df7dcfad20f6d264aad766579418a","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Cell didn't crash... \")\nstart = time.time()\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'verbose': 1,\n    'num_leaves': 31,\n    'n_estimators': 1000, \n    'learning_rate': 0.5, \n    'max_depth': 10\n}\n\nreg_lgbm = lgb.LGBMRegressor(**params, n_jobs=4)\nprint('check')\nreg_lgbm.fit(X_train, y_train)\ny_pred = reg_lgbm.predict(X_test)\n\nprint(rmsle(10 ** y_test - 1, 10 ** y_pred - 1))\nprint_time(start)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b067431-d292-423f-9458-56c8e2987273","_uuid":"9c9ace23aa843aa1e5fca5d05c8ef69dd7943e60"},"cell_type":"markdown","source":"### Stacking ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4c72e623-67b6-44d6-acf8-c9aa2a7d249d","_uuid":"e281cf886afbaddd4a90c50d8f1cd8ab9c737470"},"cell_type":"markdown","source":"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f1e42338-f62e-4d32-99e9-2785e9a4e9c4","_uuid":"b18b01f7f7ea980b806c457ed2a9cd9dcb6f306a","collapsed":true},"cell_type":"markdown","source":"Stacking Function. This function returns dataframes of predictions from each input model that can be merged into the train, test, and submission datasets. I used scikit-learn's 'clone' method, so all the weights will be stripped from the input models. This lets you input either fresh models or previously used models without worrying about it. A cloned model is generated for each training fold. I learned how to do this here, and it's an article worth reading: \nhttp://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/\n\nIn my experiments, I've gotten 2-3% better performance from stacking a few models together than the best model does on it's own. Interestingly, even adding a crap model (like the Lasso regression with a score of 0.75) increases the overall performance. The stacked models seem to work better with tree based models than regression models. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7e379a0b-d3f8-43f1-9fe5-08cec5d75f7d","_uuid":"0dec41e7069cb724e42ce8ea76dc9e759991d073","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.base import clone\ndef stack_predictions(X_train, y_train, X_test, submit, K, *models):\n    train_preds = pd.DataFrame(index=np.array(range(X_train.shape[0])))\n    test_preds = pd.DataFrame(index=np.array(range(X_test.shape[0])))\n    submit_preds = pd.DataFrame(index=np.array(range(submit.shape[0])))\n    folds = KFold(n_splits=K, shuffle=True)\n    \n    fold_n = 0\n    train_folds = np.zeros(len(train_preds))\n    for train_index, test_index in folds.split(X_train):\n        train_folds[test_index] = fold_n\n        fold_n += 1\n    \n    fold_n = 0\n    test_folds = np.zeros(len(test_preds))\n    for train_index, test_index in folds.split(X_test):\n        test_folds[test_index] = fold_n\n        fold_n += 1\n    \n    fold_n = 0\n    submit_folds = np.zeros(len(submit_preds))\n    for train_index, test_index in folds.split(submit):\n        submit_folds[test_index] = fold_n\n        fold_n += 1\n    \n    for m, model in enumerate(models):\n        print('Selecting model %d.' % (m+1))\n        col = 'pred_col_' + str(m)\n        train_preds[col] = np.nan\n        test_preds[col] = np.nan\n        submit_preds[col] = np.nan\n        \n        for fold in range(K):\n            print('Processing a fold...')\n            current_model = clone(model)\n            current_model.fit(X_train[np.where(train_folds!=fold)], y_train[np.where(train_folds!=fold)])\n            \n            train_preds[col].iloc[np.where(train_folds==fold)] = current_model.predict(\n                X_train[np.where(train_folds==fold)])\n            \n            test_preds[col].iloc[np.where(test_folds==fold)] = current_model.predict(\n                X_test[np.where(test_folds==fold)])\n            \n            submit_preds[col].iloc[np.where(submit_folds==fold)] = current_model.predict(\n                submit[np.where(submit_folds==fold)])  \n\n    return train_preds, test_preds, submit_preds","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a1b672e1-b998-4f60-87e3-191f2f5d7a91","_uuid":"d859696338e6bc25dd4a0dd566177b37cbb88cac","collapsed":true},"cell_type":"markdown","source":"### Create Stacked Models ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7edf27e5-a410-4373-a68f-c5cf93c52c5f","_uuid":"d7e9747c711d1f2a87ea0b7377baa2eedff827f7","collapsed":true,"trusted":true},"cell_type":"code","source":"reg_ridge = Ridge(solver='sag', alpha=5)\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'verbose': 0,\n    'num_leaves': 31\n}\n\nreg_lgbm = lgb.LGBMRegressor(**params)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b6cc365-f612-431c-bce4-416acda0de34","_uuid":"d24fdb0d3140cf631df4bf40742526f0c1cc8489","collapsed":true,"trusted":true},"cell_type":"code","source":"start = time.time()\ntrain_preds, test_preds, sub_preds = stack_predictions(X_train, y_train, X_test, \n                                                       sparse_matrix_sub, 5, \n                                                       reg_ridge, reg_lgbm)\nprint_time(start)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c2de5eb0-19f8-406a-876b-8818c362b7a0","_uuid":"dc0d19ef69d1dc68c2f2c50a53fc36c3f3bc0a9f","collapsed":true,"trusted":true},"cell_type":"code","source":"X_train_stacked = sparse.csr_matrix(sparse.hstack([X_train, sparse.csr_matrix(train_preds)]))\nX_test_stacked = sparse.csr_matrix(sparse.hstack([X_test, sparse.csr_matrix(test_preds)]))\nsub_stacked = sparse.csr_matrix(sparse.hstack([sparse_matrix_sub, sparse.csr_matrix(sub_preds)]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b4e2a621-edc9-40fa-b5ab-ad9f3409b64f","_uuid":"50a4c82104719d2ccceaffe9d1f76cfd309973fc","collapsed":true,"trusted":true},"cell_type":"code","source":"start = time.time()\nfrom sklearn.linear_model import Ridge\nreg_ridge = Ridge(solver='sag', alpha=5)\nreg_ridge.fit(X_train_stacked, y_train)\ny_pred = reg_ridge.predict(X_test_stacked)\nprint(rmsle(10 ** y_test - 1, 10 ** y_pred - 1))\nprint_time(start)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77510322-0a2c-4abe-8ded-68c216c6bf8c","_uuid":"d47194e003c55914d724394c82d9980aca280bcb","collapsed":true,"trusted":true},"cell_type":"code","source":"start = time.time()\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'verbose': 0,\n    'num_leaves': 31,\n    'n_estimators': 1000, \n    'learning_rate': 0.5, \n    'max_depth': 10,\n}\n\nreg_lgbm = lgb.LGBMRegressor(**params, n_jobs=4)\nreg_lgbm.fit(X_train_stacked, y_train)\nr_pred = reg_lgbm.predict(X_test_stacked)\n\nprint(rmsle(10 ** y_test - 1,10 ** r_pred - 1))\nprint_time(start)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9cddfb20-756a-47cc-8f83-3c98ad477a58","_uuid":"afd58f18582daa2534b25fb9040c737bae9fcafa"},"cell_type":"markdown","source":"Another LGBM model trained with the additional features from the input Ridge and LGBM models. I think it can evaluate how accurate the LGBM and Ridge predictions are along with the context of all the previous features, and make a more informed prediction. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3d60784d-72fe-465b-90a9-e5f896d99373","_uuid":"2549eaa2c63d18b7a2929043fe06ba1f12d0a530","collapsed":true,"trusted":true},"cell_type":"code","source":"pred_sub = reg_lgbm.predict(sub_stacked)\nlightgbm_submission = submission.copy()\nlightgbm_submission['price'] = pd.DataFrame(10 ** pred_sub - 1)\n\nlightgbm_submission.to_csv('stacked_submission_1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}