{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3ec0b941-0147-c12f-0b68-8ac6b329ac9a"
      },
      "source": [
        "*This script is based on the earlier conclusions from the script here : https://www.kaggle.com/artimous/d/quora/question-pairs-dataset/deciphering-the-quora-bot*\n",
        "\n",
        "\n",
        "Getting common words\n",
        "--------------------\n",
        "\n",
        "No syntax and semantics analysis here. Going simple to analyse all the common words in the two question sets and visualizing them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "299e6b9d-c7d1-0809-bb3d-ac116ebccd3b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    t_file = pd.read_csv('../input/test.csv', encoding='ISO-8859-1')\n",
        "    tr_file = pd.read_csv('../input/train.csv', encoding ='ISO-8859-1')\n",
        "    print('File load: Success')\n",
        "except:\n",
        "    print('File load: Failed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2effab24-1144-513d-b255-59bd1b8a7068"
      },
      "source": [
        "**Removing stop words from ntlk**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7d036380-4b13-f06c-d918-5df92c784e82"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "print(stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cd311de6-c227-215d-ae9c-80e2528858d3"
      },
      "source": [
        "CLEANING\n",
        "--------\n",
        "\n",
        "Simply dropped null values, split each of the question strings and removed stops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60810b63-7e3e-46b2-ba6f-0c9a9f1853f1"
      },
      "outputs": [],
      "source": [
        "t_file = t_file.dropna()\n",
        "t_file['question1'] = t_file['question1'].apply(lambda x: x.rstrip('?'))\n",
        "t_file['question2'] = t_file['question2'].apply(lambda x: x.rstrip('?'))\n",
        "t_file['question1'] = t_file['question1'].str.lower().str.split()\n",
        "t_file['question2'] = t_file['question2'].str.lower().str.split()\n",
        "t_file['question1'] = t_file['question1'].apply(lambda x: [item for item in x if item not in stop])\n",
        "t_file['question2'] = t_file['question2'].apply(lambda x: [item for item in x if item not in stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c33b8643-9135-6a15-e976-d8c7ae7da47a"
      },
      "outputs": [],
      "source": [
        "tr_file = tr_file.dropna()\n",
        "tr_file['question1'] = tr_file['question1'].apply(lambda x: x.rstrip('?'))\n",
        "tr_file['question2'] = tr_file['question2'].apply(lambda x: x.rstrip('?'))\n",
        "tr_file['question1'] = tr_file['question1'].str.lower().str.split()\n",
        "tr_file['question2'] = tr_file['question2'].str.lower().str.split()\n",
        "tr_file['question1'] = tr_file['question1'].apply(lambda x: [item for item in x if item not in stop])\n",
        "tr_file['question2'] = tr_file['question2'].apply(lambda x: [item for item in x if item not in stop])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c44a5b29-2111-d0ab-4063-22728357c78f"
      },
      "source": [
        "**Finding common word percentage and average word lengths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf08abe7-a8a9-2441-4537-28436e4d56b6"
      },
      "outputs": [],
      "source": [
        "tr_file['Common'] = tr_file.apply(lambda row: len(list(set(row['question1']).intersection(row['question2']))), axis=1)\n",
        "tr_file['Average'] = tr_file.apply(lambda row: 0.5*(len(row['question1'])+len(row['question2'])), axis=1)\n",
        "tr_file['Percentage'] = tr_file.apply(lambda row: row['Common']*100.0/(row['Average']+1), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8c78fccf-b8ba-810c-3024-ce8e190a44ad"
      },
      "outputs": [],
      "source": [
        "t_file['Common'] = t_file.apply(lambda row: len(list(set(row['question1']).intersection(row['question2']))), axis=1)\n",
        "t_file['Average'] = t_file.apply(lambda row: 0.5*(len(row['question1'])+len(row['question2'])), axis=1)\n",
        "t_file['Percentage'] = t_file.apply(lambda row: 1 if row['Average'] == 0 else row['Common']/(row['Average']), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "94650a8a-3187-3431-eb4a-8e2a19c5471a"
      },
      "source": [
        "**True and False plotting of data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "36a531be-a15d-8d6e-d602-23a1181a4bd3"
      },
      "source": [
        "Cheating the title\n",
        "------------------\n",
        "\n",
        "We can take a look at the training file right? No training still counts as good work? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dc6e0b1e-6d8d-86b4-96f7-c5ce749b2b63"
      },
      "outputs": [],
      "source": [
        "y = tr_file['Percentage'][tr_file['is_duplicate']==0].values\n",
        "x = tr_file['Average'][tr_file['is_duplicate']==0].values\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, sharey=True, figsize=(7, 4))\n",
        "fig.subplots_adjust(hspace=0.5, left=0.07, right=0.93)\n",
        "ax = axs[0]\n",
        "hb = ax.hexbin(x, y, gridsize=70, bins='log', cmap='inferno')\n",
        "ax.axis([0, 20, 0, 100])\n",
        "ax.set_title(\"Duplicates\")\n",
        "cb = fig.colorbar(hb, ax=ax)\n",
        "cb.set_label('log10(N)')\n",
        "\n",
        "\n",
        "y = tr_file['Percentage'][tr_file['is_duplicate']==1].values\n",
        "x = tr_file['Average'][tr_file['is_duplicate']==1].values\n",
        "ax = axs[1]\n",
        "hb = ax.hexbin(x, y, gridsize=70, bins='log', cmap='inferno')\n",
        "ax.axis([0, 20, 0, 100])\n",
        "ax.set_title(\"Not duplicates\")\n",
        "cb = fig.colorbar(hb, ax=ax)\n",
        "cb.set_label('log10(N)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0e9e5251-5720-c99d-f5cc-aa317d587118"
      },
      "source": [
        "**Scatters are a must**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e2ee5b6-82c8-9b28-ab77-46e9123aeb5d"
      },
      "outputs": [],
      "source": [
        "x = tr_file['Percentage'][tr_file['is_duplicate']==0].values\n",
        "y = tr_file['qid1'][tr_file['is_duplicate']==0].values\n",
        "area = tr_file['Average'][tr_file['is_duplicate']==0].values\n",
        "\n",
        "plt.scatter(x, y, s=area*3, c='r', alpha=0.1)\n",
        "\n",
        "x = tr_file['Percentage'][tr_file['is_duplicate']==1].values\n",
        "y = tr_file['qid1'][tr_file['is_duplicate']==1].values\n",
        "area = tr_file['Average'][tr_file['is_duplicate']==1].values\n",
        "\n",
        "plt.scatter(x, y, s=area*3, c='b', alpha=0.1)\n",
        "\n",
        "plt.ylabel('Question IDs')\n",
        "plt.xlabel('Percentage of common words')\n",
        "\n",
        "plt.title(\"Percentages of common words in questions\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "119d261f-8936-e14b-9da6-7dfea623b986"
      },
      "source": [
        "**Observations**\n",
        "----------------\n",
        "\n",
        "From the final plot it is pretty clearly visible that the ones that are clustered towards the 100% mark are nearly all blue. This states the fact that questions having a lot of common strings are termed as equivalent more often than not.\n",
        "Also as seen in the hex plot, non duplicates are clustered towards the 100% area more than the duplicate ones.\n",
        "Have we decoded the Quora bot? Not at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f38f8dd3-b3de-894a-0b10-39de25002ee0"
      },
      "source": [
        "The final step\n",
        "--------------\n",
        "\n",
        "Penning down the final result into and output file. This approach is just naive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5d3ebc3a-5d37-8478-6eb1-df860d7a33d4"
      },
      "outputs": [],
      "source": [
        "df2 = pd.DataFrame({'test_id' : range(0,2345796)})\n",
        "df2['is_duplicate']=pd.Series(t_file['Percentage'])\n",
        "df2.fillna(0, inplace = True)\n",
        "print(df2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c403481e-82d5-4e19-5629-f00d74a506a2"
      },
      "outputs": [],
      "source": [
        "df2.to_csv('submit_naive.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "034b3dba-ea12-9990-e8d8-5d83cfa36070"
      },
      "source": [
        " Cosine, Jaccard and Shingling\n",
        "--------------------------------\n",
        "\n",
        "The first, naive approach towards identifying question pairs -- Strip the stopwords, stem the remaining and do a simple Cosine/Jaccard Test. K-Shingling is also a popular technique, where continuous subsets of \"k\" words are matched between the two documents.\n",
        "However, a major drawback with the above is that of a lack of semantic understanding -- There might be two questions with a high percentage of common words, but different meanings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "acbce3bf-b76a-455a-97d2-13b79abdbd22"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re, math\n",
        "def get_cosine(vec1, vec2):\n",
        "    vec1 = Counter(vec1)\n",
        "    vec2 = Counter(vec2)\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "135c6d9c-5a27-3f97-9397-7a0be548ac7b"
      },
      "outputs": [],
      "source": [
        "t_file['Cosine'] = t_file.apply(lambda row: get_cosine(row['question1'],row['question2']), axis=1)\n",
        "print(t_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "db2fe43b-b5d4-4d8c-2403-659c515a7086"
      },
      "outputs": [],
      "source": [
        "df3 = pd.DataFrame({'test_id' : range(0,2345796)})\n",
        "df3['is_duplicate']=pd.Series(t_file['Cosine'])\n",
        "df3.fillna(0, inplace = True)\n",
        "print(df3.shape)\n",
        "df3.to_csv('submit_cosine.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e9640aab-7bdc-c0e3-c846-13a6d58e5ee5"
      },
      "source": [
        "Jaccard Similarity\n",
        "------------------\n",
        "\n",
        "Jaccard Similarity is given by s=p/(p+q+r)\n",
        "where,\n",
        "\n",
        "- p = # of attributes positive for both objects \n",
        "- q = # of attributes 1 for i and 0 for j \n",
        "- r = # of attributes 0 for i and 1 for j "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5c2b3207-03ff-0376-2970-aea5397ea993"
      },
      "outputs": [],
      "source": [
        "t_file['Jaccard'] = t_file.apply(lambda row: 0 if (len(row['question1'])+len(row['question2'])-row['Common']) == 0  else float(row['Common'])/((len(row['question1'])+len(row['question2'])-row['Common'])), axis=1)\n",
        "print(t_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "62905238-363e-1a22-d41d-7a4f3d55508c"
      },
      "outputs": [],
      "source": [
        "df4 = pd.DataFrame({'test_id' : range(0,2345796)})\n",
        "df4['is_duplicate']=pd.Series(t_file['Jaccard'])\n",
        "df4.fillna(0, inplace = True)\n",
        "print(df4.shape)\n",
        "df4.to_csv('submit_jaccard.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f9468b60-9a3c-63d2-fac5-a3012520f881"
      },
      "source": [
        "Semantic Similarity via Wordnet\n",
        "-------------------------------\n",
        "\n",
        "Wordnet is a huge library of synsets for almost all words in the English dictionary. The synsets for each word describe its meaning, part of speeches, and synonyms/antonyms. The synonyms help in identifying the semantic meaning of the sentence, when all words are taken together.\n",
        "\n",
        "[This][1] paper describes how wordnet is used to calculate a matrix similarity between two sentences. Later a thresholding for paraphrases is done, they could come up with a F-Score of 82.4 on the Microsoft Research Paraphrase Corpus, the industry standard.\n",
        "\n",
        "\n",
        "  [1]: http://staffwww.dcs.shef.ac.uk/people/S.Fernando/pubs/clukPaper.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4187ea54-adbe-1f10-930b-9d65dd62146c"
      },
      "source": [
        "Word Embeddings\n",
        "---------------\n",
        "\n",
        "A recent trend in the Deep NLP community, starting with the famous Word2Vec and CBOW, and now Doc2Vec, Paragraph2Vec, skip-thought vectors coming along! These are extremely powerful models which have changed the scope of NLP models in the last 3-4 years.\n",
        "\n",
        "- CNN over Word Embeddings: [This][1] research paper explains the approach of applying Convolutional Neural Nets over the word embeddings (using a large collection of unlabeled data), building vector representations for question pairs. They tested their results over AskUbuntu, witnessing a 92.4% test accuracy!\n",
        "\n",
        "- Skip-thought Vectors: This model backs upon its ability to semantically understand a sentence, thus the transition from the old skip-gram to skip-thought. Ryan Kiros is the lead developer behind this model, do have a look at his [Github Repo][2] and its application on Paraphrase Detection. And let me know if you can replicate his results!\n",
        "\n",
        "\n",
        "  [1]: https://aclweb.org/anthology/K15-1013\n",
        "  [2]: https://github.com/ryankiros/skip-thoughts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "37ec29d6-576e-7c1e-2dab-f2e50b41af8e"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f0601470-0866-83e0-9943-bb4f631fe15c"
      },
      "source": [
        "**Special mention : [Script][1] by [Shubh24][2]**\n",
        "\n",
        "\n",
        "  [1]: https://www.kaggle.com/shubh24/d/quora/question-pairs-dataset/everything-you-wanna-know\n",
        "  [2]: https://www.kaggle.com/shubh24"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}