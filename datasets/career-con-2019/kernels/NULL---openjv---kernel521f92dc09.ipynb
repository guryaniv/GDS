{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# pull training X and y data\nX_train = pd.read_csv('../input/X_test.csv')\ny_train = pd.read_csv('../input/y_train.csv')\nX_test = pd.read_csv('../input/X_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87d18ba83f9ba52502e2b549bed92968cf49db24"},"cell_type":"code","source":"def MIN_MAX(frame, _columns, columns_):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    _frame = frame[_columns]\n    frame_ = frame[columns_]\n    frameV = frame_.values\n    frameVScaled = min_max_scaler.fit_transform(frameV)\n    frameVScaledframe = pd.DataFrame(frameVScaled)\n    _frameframeVScaledframe = pd.concat([_frame, frameVScaledframe], axis = 1)\n    _frameframeVScaledframe.columns = frame.columns\n    return _frameframeVScaledframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9916f7c7106e802070c85ce789368a3a2706653d"},"cell_type":"code","source":"X_train = MIN_MAX(X_train, X_train.columns[:3], X_train.columns[3:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d83bde7a529215e13fde5bf95502efccb585acdb"},"cell_type":"code","source":"labels = y_train['surface'].unique()\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4d7be10aa6e332224e0f68f1ad30c70067d2dff"},"cell_type":"code","source":"def get_label_sid(label_index):\n    return y_train[y_train['surface'] == labels[label_index]]['series_id'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"297a1d26885dd44749d5c022c3f6c223450b56b7"},"cell_type":"code","source":"X_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b4df5c79441a50385dc7ca815819bb321fe3d93"},"cell_type":"code","source":"def construct_corrcoef_XXt(series_id, X, columns):\n    return np.corrcoef(X[X['series_id'] == series_id][columns])\n\ndef construct_corrcoef_XtX(series_id, X, columns):\n    return np.corrcoef(X[X['series_id'] == series_id][columns].T)\n\ndef construct_X(series_id, X, columns):\n    return X[X['series_id'] == series_id][columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48ba0f7571acd36ee1b3f84374587e179e11e041"},"cell_type":"code","source":"cols = X_train.columns[3:]\ncols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d6baaeeb450068cd9f6a969293f1a09b1413113"},"cell_type":"markdown","source":"## Correleation Matrices as a basis for classifiaction\nHere, we seek to construct a Convolutional Autoencoder for the constructed Correleation Matrices. The latent vectors will contribute to our loss function through the maximization of a Calinski-Harabaz Index and a Minimization of Mean Square Error with the output.\n\n$$ \\text{Loss} = \\text{min}_{MSE}\\text{max}_{CHI} \\sum_{i} \\sum_{j} (y_{ij} - y_{ij}^q)^{2} + \\lambda \\text{Calinski-Harabaz Index on latent vector space} $$\n\nI'm not sure if this will work, but I think it's worth a try...\n\nIf this doesn't work, lets use the standard Varitational Autoencoder paradigm with the Calinkski-Harabaz Index as an additional regulrization term."},{"metadata":{"trusted":true,"_uuid":"f78633c026a74a1dd4936f882de0cf67ccfd484e"},"cell_type":"code","source":"X = np.stack([construct_X(series_id, X_train, cols) for series_id in X_train['series_id'].unique()], axis = 2)\nXXt = np.stack([construct_corrcoef_XXt(series_id, X_train, cols) for series_id in X_train['series_id'].unique()], axis = 2)\nXtX = np.stack([construct_corrcoef_XtX(series_id, X_train, cols) for series_id in X_train['series_id'].unique()], axis = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"229e5e01a94a2de48f598d063b347f7f45634ff4"},"cell_type":"code","source":"XXt.shape, XtX.shape, X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f562a66a24a6805f1ec780f3520f30a2bf562b9"},"cell_type":"code","source":"series_id = 21","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08caf2adfb2295a3ed420de64fdb64d047d33bf1"},"cell_type":"code","source":"def plot_corrcoef():\n    fig = plt.figure(figsize = (16, 8))\n    \n    ax1 = plt.subplot2grid((2, 2), (0, 0), colspan = 2)\n    ax2 = plt.subplot2grid((2, 2), (1, 1))\n    ax3 = plt.subplot2grid((2, 2), (1, 0))\n    \n    ax1.matshow(X[:,:,series_id].T)\n    ax1.set_title('X')\n    ax2.matshow(XXt[:,:,series_id])\n    ax2.set_title('XXt')\n    ax3.matshow(XtX[:,:,series_id])\n    ax3.set_title('XtX')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cec3aeff6fb4e08b03e4052911e7e18b92c1151c"},"cell_type":"code","source":"plot_corrcoef()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be515088b99af8291a6c96a8eb7511f61ada2c90"},"cell_type":"markdown","source":"## Difference Tensor Structure\nLook to construct a tensor whose elements are the normalized difference vectors ie.\n$$ T_{i, j} = \\frac{A_i - A_j^T}{|A_i - A_j|} \\text{ where } i \\neq j, \\text{ else } \\frac{A_i}{|A_i|} $$\nThis structure is an attempt to capture information on the nature of the change we see between data. Notice that $T_{i,j,:}$ is equivelent to the direction of change between the $i^{th}$ observation and the $j^{th}$ observation. Consider the addition of padding which represents the lengths of each of these vectors. Otherwise, this will be lost."},{"metadata":{"trusted":true,"_uuid":"ec83323667b68eb53b45f91deca9d661a88de26e"},"cell_type":"code","source":"def APS(A):\n    T = np.zeros(shape = (A.shape[0], A.shape[0], A.shape[1]))\n    for i in range(A.shape[0]):\n        for j in range(A.shape[0]):\n            if i != j:\n                T[i, j, :] = (A[i, :] - A.T[:, j]) / (np.linalg.norm(A[i, :] - A.T[:, j]))\n            else:\n                T[i, j, :] = A[i, :] / (np.linalg.norm(A[i, :]))\n    return T\n\ndef APS_NN(A):\n    T = np.zeros(shape = (A.shape[0], A.shape[0], A.shape[1]))\n    for i in range(A.shape[0]):\n        for j in range(A.shape[0]):\n            if i != j:\n                T[i, j, :] = (A[i, :] - A.T[:, j])\n            else:\n                T[i, j, :] = A[i, :]\n    return T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad4773ed4b4a6a6256da893e8d4dde052906ecd2"},"cell_type":"code","source":"TBulk = np.stack([APS(X[:,:,i]) for i in X_train['series_id'].unique()], axis = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de06f6c5bd7fb358cca6282712f035868be9dd20"},"cell_type":"code","source":"T = TBulk[:,:,:,series_id]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3401d2ae22232b05ca2cefa4b20083d5c18b4455"},"cell_type":"code","source":"def plot_tensor_T(T_):\n    fig, axes = plt.subplots(2, 5, figsize = (16, 8))\n    for i in range(axes.shape[0]):\n        for j in range(axes.shape[1]):\n            axes[i,j].matshow(T_[:,:,axes.shape[1]*i + j])\n            axes[i,j].set_title(cols[axes.shape[1]*i + j]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24c9de5deac385a44749d37e70251f693551df3f"},"cell_type":"code","source":"plot_tensor_T(T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"474864738a9096a0524300d4b8418834482ea5a5"},"cell_type":"code","source":"def pullSubIntervals(t, interval_num):\n    return np.stack([t[i:(i+interval_num),i:(i+interval_num),:] for i in range(128 - interval_num)], axis = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"018cd59cf0d7811bdd0b6d5578cca51fda00c0a8"},"cell_type":"code","source":"pullSubIntervals(TBulk[:,:,:,series_id], 64).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9f654ef527751f4b03ed515cc8186100861df2e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}