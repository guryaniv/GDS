{"cells":[{"metadata":{"_uuid":"b64e880bd17c6853e64b2ffb70154a2218229ca0"},"cell_type":"markdown","source":"Reference of this - https://www.kaggle.com/anqitu/feature-engineer-and-model-ensemble-top-10/"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"Validation = False\nreduce_size = False\nnum_first_level_models = 3 \nSEED = 0\n\nimport time\nstart_time = time.time()\n\nimport pandas as pd\nimport numpy as np\nimport gc\nfrom tqdm import tqdm\n\npd.set_option('display.max_rows', 99)\npd.set_option('display.max_columns', 50)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data path\ndata_path = '../input'\nsubmission_path = ''","execution_count":63,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int16)\n\n    return df","execution_count":64,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7407f609d2eb1943274c01056c57db0e62c389c"},"cell_type":"code","source":"# # 0. Load data ----------------------------------------------------------------\n# print('%0.2f min: Start loading result'%((time.time() - start_time)/60))\n# result = pd.read_csv('ver6_lr_stacking.csv' % data_path)\n# result.to_csv('ver6_lr_stacking.csv', index = False)\n# print('%0.2f min: Finish loading result'%((time.time() - start_time)/60))","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"98a8a05cca1269551765667d10b9d7337097f673"},"cell_type":"markdown","source":"# 1. Load data"},{"metadata":{"trusted":true,"_uuid":"162c855ff975f4786ff044086d351016760f0daf"},"cell_type":"code","source":"# It takes long to run the cope, so i comment them and upload my result\n\nprint('%0.2f min: Start loading data'%((time.time() - start_time)/60))\nsale_train = pd.read_csv('%s/sales_train.csv' % data_path)\ntest  = pd.read_csv('%s/test.csv' % data_path)\n\nsale_train[sale_train['item_id'] == 11373][['item_price']].sort_values(['item_price'])\nsale_train[sale_train['item_id'] == 11365].sort_values(['item_price'])\n# Correct sale_train values\nsale_train['item_price'][2909818] = np.nan\nsale_train['item_cnt_day'][2909818] = np.nan\nsale_train['item_price'][2909818] = sale_train[(sale_train['shop_id'] ==12) & (sale_train['item_id'] == 11373) & (sale_train['date_block_num'] == 33)]['item_price'].median()\nsale_train['item_cnt_day'][2909818] = round(sale_train[(sale_train['shop_id'] ==12) & (sale_train['item_id'] == 11373) & (sale_train['date_block_num'] == 33)]['item_cnt_day'].median())\nsale_train['item_price'][885138] = np.nan\nsale_train['item_price'][885138] = sale_train[(sale_train['item_id'] == 11365) & (sale_train['shop_id'] ==12) & (sale_train['date_block_num'] == 8)]['item_price'].median()\ntest_nrow = test.shape[0]\nsale_train = sale_train.merge(test[['shop_id']].drop_duplicates(), how = 'inner')\nsale_train['date'] = pd.to_datetime(sale_train['date'], format = '%d.%m.%Y')\nprint('%0.2f min: Finish loading data'%((time.time() - start_time)/60))\n","execution_count":67,"outputs":[]},{"metadata":{"_uuid":"4cc56a0e8e186512f3fad9b1144c2d3e5acc0b0a"},"cell_type":"markdown","source":"# 2. Aggregate data"},{"metadata":{"trusted":true,"_uuid":"625e150db91b14ff77968d5e31458fea529b1e64"},"cell_type":"code","source":"from itertools import product\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = []\nfor block_num in sale_train['date_block_num'].unique():\n    cur_shops = sale_train[sale_train['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = sale_train[sale_train['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n#turn the grid into pandas dataframe\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\nprint('%0.2f min: Finish creating the grid'%((time.time() - start_time)/60))","execution_count":68,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d4bd1492424909316d39cc91865f5c59775c5fa"},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\nsale_train['item_cnt_day'] = sale_train['item_cnt_day'].clip(0,20)\ngb_cnt = sale_train.groupby(index_cols)['item_cnt_day'].agg(['sum']).reset_index().rename(columns = {'sum': 'item_cnt_month'})\ngb_cnt['item_cnt_month'] = gb_cnt['item_cnt_month'].clip(0,20).astype(np.int)\n\n#join aggregated data to the grid\ntrain = pd.merge(grid,gb_cnt,how='left',on=index_cols).fillna(0)\ntrain['item_cnt_month'] = train['item_cnt_month'].astype(int)\ntrain = downcast_dtypes(train)\n\n#sort the data\ntrain.sort_values(['date_block_num','shop_id','item_id'],inplace=True)\nprint('%0.2f min: Finish joining gb_cnt'%((time.time() - start_time)/60))","execution_count":69,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00a61401d35c592fe9638565e15a9867d66a4866"},"cell_type":"code","source":"# # Sanity check\nprint(sale_train['item_cnt_day'].sum())\nprint(train['item_cnt_month'].sum())\nprint(gb_cnt['item_cnt_month'].sum())","execution_count":70,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aee5614b0e8f17c6c3f4645c0003c15c054bc4a6"},"cell_type":"code","source":"item = pd.read_csv('%s/items.csv' % data_path)\ntrain = train.merge(item[['item_id', 'item_category_id']], on = ['item_id'], how = 'left')\ntest = test.merge(item[['item_id', 'item_category_id']], on = ['item_id'], how = 'left')\nprint('%0.2f min: Finish adding item_category_id'%((time.time() - start_time)/60))","execution_count":71,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"83b91bf5c09ad13bf3743cd1784ae88f13264965"},"cell_type":"code","source":"item_cat = pd.read_csv('%s/item_categories.csv' % data_path)\nl_cat = list(item_cat.item_category_name)\nfor ind in range(0,1):\n    l_cat[ind] = 'PC Headsets / Headphones'\nfor ind in range(1,8):\n    l_cat[ind] = 'Access'\nl_cat[8] = 'Tickets (figure)'\nl_cat[9] = 'Delivery of goods'\nfor ind in range(10,18):\n    l_cat[ind] = 'Consoles'\nfor ind in range(18,25):\n    l_cat[ind] = 'Consoles Games'\nl_cat[25] = 'Accessories for games'\nfor ind in range(26,28):\n    l_cat[ind] = 'phone games'\nfor ind in range(28,32):\n    l_cat[ind] = 'CD games'\nfor ind in range(32,37):\n    l_cat[ind] = 'Card'\nfor ind in range(37,43):\n    l_cat[ind] = 'Movie'\nfor ind in range(43,55):\n    l_cat[ind] = 'Books'\nfor ind in range(55,61):\n    l_cat[ind] = 'Music'\nfor ind in range(61,73):\n    l_cat[ind] = 'Gifts'\nfor ind in range(73,79):\n    l_cat[ind] = 'Soft'\nfor ind in range(79,81):\n    l_cat[ind] = 'Office'\nfor ind in range(81,83):\n    l_cat[ind] = 'Clean'\nl_cat[83] = 'Elements of a food'","execution_count":72,"outputs":[]},{"metadata":{"_uuid":"641d95f8ffe92e47ef29e3f5d9eaee93826a7400"},"cell_type":"markdown","source":"# 3. Label Encoding"},{"metadata":{"trusted":true,"_uuid":"ec661c870a37f9943718fdc8a4459254d2837c90"},"cell_type":"code","source":"from sklearn import preprocessing\nlb = preprocessing.LabelEncoder()\nitem_cat['item_cat_id_fix'] = lb.fit_transform(l_cat)\n\ntrain = train.merge(item_cat[['item_cat_id_fix', 'item_category_id']], on = ['item_category_id'], how = 'left')\ntest = test.merge(item_cat[['item_cat_id_fix', 'item_category_id']], on = ['item_category_id'], how = 'left')\n\ndel item, item_cat, grid, gb_cnt\ngc.collect()\nprint('%0.2f min: Finish adding item_cat_id_fix'%((time.time() - start_time)/60))","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"b2b3479b388ccaffaab914052e2fe6ec70658843"},"cell_type":"markdown","source":"# 4. Add item/shop pair mean-encodings"},{"metadata":{"trusted":true,"_uuid":"de69325a23c2328ab71869615470cebcbdc1d646"},"cell_type":"code","source":"print('%0.2f min: Start adding mean-encoding for item_cnt_month'%((time.time() - start_time)/60))\nTarget = 'item_cnt_month'\nglobal_mean =  train[Target].mean()\ny_tr = train[Target].values","execution_count":74,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e4a83329fb3fa638c7acfa1805498bcd56c18d9"},"cell_type":"code","source":"mean_encoded_col = ['shop_id', 'item_id', 'item_category_id', 'item_cat_id_fix']\nfor col in tqdm(mean_encoded_col):\n        col_tr = train[[col] + [Target]]\n        corrcoefs = pd.DataFrame(columns = ['Cor'])\n","execution_count":75,"outputs":[]},{"metadata":{"_uuid":"f2bc0c2d94e930040b2c32891703601d8b2d4f5f"},"cell_type":"markdown","source":"# 4.1.1 Mean encodings - KFold scheme\n    "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"333884c38df4fff224f71e15c5d7a3a2a243d1a2"},"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(n_splits = 5, shuffle = False, random_state = SEED)\ncol_tr[col + '_cnt_month_mean_Kfold'] = global_mean\nfor tr_ind, val_ind in kf.split(col_tr):\n    X_tr, X_val = col_tr.iloc[tr_ind], col_tr.iloc[val_ind]\n    means = X_val[col].map(X_tr.groupby(col)[Target].mean())\n    X_val[col + '_cnt_month_mean_Kfold'] = means\n    col_tr.iloc[val_ind] = X_val\n    X_val.head()\ncol_tr.fillna(global_mean, inplace = True)\ncorrcoefs.loc[col + '_cnt_month_mean_Kfold'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Kfold'])[0][1]","execution_count":76,"outputs":[]},{"metadata":{"_uuid":"48ac38876f306e4d8a2a2565c53f7354a5a2e897"},"cell_type":"markdown","source":"# 4.1.2 Mean encodings - Leave-one-out scheme"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ac149ef86d5846f60173a7331dd4dc455a2c2af5"},"cell_type":"code","source":"item_id_target_sum = col_tr.groupby(col)[Target].sum()\nitem_id_target_count = col_tr.groupby(col)[Target].count()\ncol_tr[col + '_cnt_month_sum'] = col_tr[col].map(item_id_target_sum)\ncol_tr[col + '_cnt_month_count'] = col_tr[col].map(item_id_target_count)\ncol_tr[col + '_target_mean_LOO'] = (col_tr[col + '_cnt_month_sum'] - col_tr[Target]) / (col_tr[col + '_cnt_month_count'] - 1)\ncol_tr.fillna(global_mean, inplace = True)\ncorrcoefs.loc[col + '_target_mean_LOO'] = np.corrcoef(y_tr, col_tr[col + '_target_mean_LOO'])[0][1]","execution_count":77,"outputs":[]},{"metadata":{"_uuid":"6e5edc943295427517e06e757ab8b8d8dba61988"},"cell_type":"markdown","source":"# 4.1.3 Mean encodings - Smoothing"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d45172e2c07ade6e5b4c0d5e3d29c599130b9a65"},"cell_type":"code","source":"item_id_target_mean = col_tr.groupby(col)[Target].mean()\nitem_id_target_count = col_tr.groupby(col)[Target].count()\ncol_tr[col + '_cnt_month_mean'] = col_tr[col].map(item_id_target_mean)\ncol_tr[col + '_cnt_month_count'] = col_tr[col].map(item_id_target_count)\nalpha = 100\ncol_tr[col + '_cnt_month_mean_Smooth'] = (col_tr[col + '_cnt_month_mean'] *  col_tr[col + '_cnt_month_count'] + global_mean * alpha) / (alpha + col_tr[col + '_cnt_month_count'])\ncol_tr[col + '_cnt_month_mean_Smooth'].fillna(global_mean, inplace=True)\ncorrcoefs.loc[col + '_cnt_month_mean_Smooth'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Smooth'])[0][1]","execution_count":78,"outputs":[]},{"metadata":{"_uuid":"1f35f5a345ee8a0b10afec89a0b3e5b5bb614a3b"},"cell_type":"markdown","source":"# 4.1.4 Mean encodings - Expanding mean scheme"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"748dacba4884e675b149729182ba5e327ccfd36a"},"cell_type":"code","source":"cumsum = col_tr.groupby(col)[Target].cumsum() - col_tr[Target]\nsumcnt = col_tr.groupby(col).cumcount()\ncol_tr[col + '_cnt_month_mean_Expanding'] = cumsum / sumcnt\ncol_tr[col + '_cnt_month_mean_Expanding'].fillna(global_mean, inplace=True)\ncorrcoefs.loc[col + '_cnt_month_mean_Expanding'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Expanding'])[0][1]","execution_count":79,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71264d83afd7bd594a16349768013dcd67762cd2"},"cell_type":"code","source":"train = pd.concat([train, col_tr[corrcoefs['Cor'].idxmax()]], axis = 1)\nprint(corrcoefs.sort_values('Cor'))\nprint('%0.2f min: Finish encoding %s'%((time.time() - start_time)/60, col))\n\nprint('%0.2f min: Finish adding mean-encoding'%((time.time() - start_time)/60))","execution_count":80,"outputs":[]},{"metadata":{"_uuid":"5bd5b0b65f9768f5bf17a3006805234163908ed6"},"cell_type":"markdown","source":"# 5. Feature Engineering\n\n## 5.1 Combine Train and Test set"},{"metadata":{"trusted":true,"_uuid":"93d53cabcf6214f6bfbd1f38f2d3bc8d4c14463b"},"cell_type":"code","source":"print('%0.2f min: Start combining data'%((time.time() - start_time)/60))\nif Validation == False:\n    test['date_block_num'] = 34\n    all_data = pd.concat([train, test], axis = 0)\n    all_data = all_data.drop(columns = ['ID'])\nelse:\n    all_data = train\n\ndel train, test, col_tr\ngc.collect()\n\nall_data = downcast_dtypes(all_data)","execution_count":81,"outputs":[]},{"metadata":{"_uuid":"13b34c4f668443ad5ac12e9d18c8fedc5b6e5f91"},"cell_type":"markdown","source":"## 5.2 Creating item/shop pair lags lag-based features"},{"metadata":{"trusted":true,"_uuid":"706b947f141287b7ade887e867fc0d4847fcf129"},"cell_type":"code","source":"print('%0.2f min: Start adding lag-based feature'%((time.time() - start_time)/60))\nindex_cols = ['shop_id', 'item_id', 'item_category_id', 'item_cat_id_fix', 'date_block_num']\ncols_to_rename = list(all_data.columns.difference(index_cols))\nprint(cols_to_rename)\nshift_range = [1, 2, 3, 4, 12]\n\nfor month_shift in tqdm(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n\ntrain_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n\nfoo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\ntrain_shift = train_shift.rename(columns=foo)\n\nall_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\ngc.collect()\n\nall_data = all_data[all_data['date_block_num'] >= 12]  #Don't use old data from year 2013\nlag_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]]\nall_data = downcast_dtypes(all_data)\nprint('%0.2f min: Finish generating lag features'%((time.time() - start_time)/60))","execution_count":82,"outputs":[]},{"metadata":{"_uuid":"fdce1343317bf44fb6fa1b98338a9fcbae7238fe"},"cell_type":"markdown","source":"## 5.3 Creating date features"},{"metadata":{"trusted":true,"_uuid":"797e8f9e60c2a811346046360e6d9d51f866fd4c"},"cell_type":"code","source":"print('%0.2f min: Start getting date features'%((time.time() - start_time)/60))\n\ndates_train = sale_train[['date', 'date_block_num']].drop_duplicates()\ndates_test = dates_train[dates_train['date_block_num'] == 34-12]\ndates_test['date_block_num'] = 34\ndates_test['date'] = dates_test['date'] + pd.DateOffset(years=1)\ndates_all = pd.concat([dates_train, dates_test])\n\ndates_all['dow'] = dates_all['date'].dt.dayofweek\ndates_all['year'] = dates_all['date'].dt.year\ndates_all['month'] = dates_all['date'].dt.month\ndates_all = pd.get_dummies(dates_all, columns=['dow'])\ndow_col = ['dow_' + str(x) for x in range(7)]\ndate_features = dates_all.groupby(['year', 'month', 'date_block_num'])[dow_col].agg('sum').reset_index()\ndate_features['days_of_month'] = date_features[dow_col].sum(axis=1)\ndate_features['year'] = date_features['year'] - 2013\n\ndate_features = date_features[['month', 'year', 'days_of_month', 'date_block_num']]\nall_data = all_data.merge(date_features, on = 'date_block_num', how = 'left')\ndate_columns = date_features.columns.difference(set(index_cols))\nprint('%0.2f min: Finish getting date features'%((time.time() - start_time)/60))","execution_count":83,"outputs":[]},{"metadata":{"_uuid":"52ae9b304c918d11001948892c2d6eaced2607ab"},"cell_type":"markdown","source":"# 5.4 Scale feature columns "},{"metadata":{"trusted":true,"_uuid":"4389a2e1c5d80f885d8635f70e252b6605e9a124"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ntrain = all_data[all_data['date_block_num']!= all_data['date_block_num'].max()]\ntest = all_data[all_data['date_block_num']== all_data['date_block_num'].max()]\nsc = StandardScaler()\n\nto_drop_cols = ['date_block_num']\nfeature_columns = list(set(lag_cols + index_cols + list(date_columns)).difference(to_drop_cols))\n\ntrain[feature_columns] = sc.fit_transform(train[feature_columns])\ntest[feature_columns] = sc.transform(test[feature_columns])\nall_data = pd.concat([train, test], axis = 0)\nall_data = downcast_dtypes(all_data)\n\ndel train, test, date_features, sale_train\ngc.collect()\nprint('%0.2f min: Finish scaling features'%((time.time() - start_time)/60))","execution_count":84,"outputs":[]},{"metadata":{"_uuid":"484778b71c1e0921fb0e767ded853a81a27d44ca"},"cell_type":"markdown","source":"# 6. Different Level model\n## 6.1 first Model"},{"metadata":{"trusted":true,"_uuid":"f3af6d7eaddfa316d3ce16a68a8819211a7a38ac"},"cell_type":"code","source":"# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts\ndates = all_data['date_block_num']\nlast_block = dates.max()\nprint('Test `date_block_num` is %d' % last_block)\nprint(feature_columns)\n\nprint('%0.2f min: Start training First level models'%((time.time() - start_time)/60))\nstart_first_level_total = time.perf_counter()\n\nscoringMethod = 'r2'; \nfrom sklearn.metrics import mean_squared_error; from math import sqrt\n# Train meta-features M = 15 (12 + 15 = 27)\nmonths_to_generate_meta_features = range(27,last_block +1)\nmask = dates.isin(months_to_generate_meta_features)\nTarget = 'item_cnt_month'\ny_all_level2 = all_data[Target][mask].values\nX_all_level2 = np.zeros([y_all_level2.shape[0], num_first_level_models])","execution_count":85,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"786f6cfdaa7e3de39b983dcb5542760ee3013b21"},"cell_type":"code","source":"# Now fill `X_train_level2` with metafeatures\nslice_start = 0\n\nfor cur_block_num in tqdm(months_to_generate_meta_features):\n\n    print('-' * 50)\n    print('Start training for month%d'% cur_block_num)\n    start_cur_month = time.perf_counter()\n\n    cur_X_train = all_data.loc[dates <  cur_block_num][feature_columns]\n    cur_X_test =  all_data.loc[dates == cur_block_num][feature_columns]\n\n    cur_y_train = all_data.loc[dates <  cur_block_num, Target].values\n    cur_y_test =  all_data.loc[dates == cur_block_num, Target].values\n\n    # Create Numpy arrays of train, test and target dataframes to feed into models\n    train_x = cur_X_train.values\n    train_y = cur_y_train.ravel()\n    test_x = cur_X_test.values\n    test_y = cur_y_test.ravel()\n\n    preds = []","execution_count":86,"outputs":[]},{"metadata":{"_uuid":"d789f55251ad1a29c3d90ec8cfcecb3690ad4e7d"},"cell_type":"markdown","source":"## 6.2 Second Model"},{"metadata":{"trusted":true,"_uuid":"15d7baa5e15c1c50739ce75cd5b80338ea19b4bc"},"cell_type":"code","source":"from sklearn.linear_model import (LinearRegression, SGDRegressor)\nimport lightgbm as lgb\n\nsgdr= SGDRegressor(\n    penalty = 'l2' ,\n    random_state = SEED )\nlgb_params = {\n              'feature_fraction': 0.75,\n              'metric': 'rmse',\n              'nthread':1,\n              'min_data_in_leaf': 2**7,\n              'bagging_fraction': 0.75,\n              'learning_rate': 0.03,\n              'objective': 'mse',\n              'bagging_seed': 2**7,\n              'num_leaves': 2**7,\n              'bagging_freq':1,\n              'verbose':0\n              }\n\nestimators = [sgdr]\n\nfor estimator in estimators:\n    print('Training Model %d: %s'%(len(preds), estimator.__class__.__name__))\n    start = time.perf_counter()\n    estimator.fit(train_x, train_y)\n    pred_test = estimator.predict(test_x)\n    preds.append(pred_test)\n    # pred_train = estimator.predict(train_x)\n    # print('Train RMSE for %s is %f' % (estimator.__class__.__name__, sqrt(mean_squared_error(cur_y_train, pred_train))))\n    # print('Test RMSE for %s is %f' % (estimator.__class__.__name__, sqrt(mean_squared_error(cur_y_test, pred_test))))\n    run = time.perf_counter() - start\n    print('{} runs for {:.2f} seconds.'.format(estimator.__class__.__name__, run))\n    print()\n\n\nprint('Training Model %d: %s'%(len(preds), 'lightgbm'))\nstart = time.perf_counter()\nestimator = lgb.train(lgb_params, lgb.Dataset(train_x, label=train_y), 300)\npred_test = estimator.predict(test_x)\npreds.append(pred_test)\n# pred_train = estimator.predict(train_x)\n# print('Train RMSE for %s is %f' % ('lightgbm', sqrt(mean_squared_error(cur_y_train, pred_train))))\n# print('Test RMSE for %s is %f' % ('lightgbm', sqrt(mean_squared_error(cur_y_test, pred_test))))\nrun = time.perf_counter() - start\nprint('{} runs for {:.2f} seconds.'.format('lightgbm', run))\nprint()","execution_count":87,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"453cc7b0c0b89de07061ab953f9b853725ec2aa6"},"cell_type":"code","source":"print('Training Model %d: %s'%(len(preds), 'keras'))\nstart = time.perf_counter()\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(20, input_dim=train_x.shape[1], kernel_initializer='uniform', activation='softplus'))\n    model.add(Dense(1, kernel_initializer='uniform', activation = 'relu'))\n    # Compile model\n    model.compile(loss='mse', optimizer='Nadam', metrics=['mse'])\n    # model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\nestimator = KerasRegressor(build_fn=baseline_model, verbose=1, epochs=5, batch_size = 55000)\n\nestimator.fit(train_x, train_y)\npred_test = estimator.predict(test_x)\npreds.append(pred_test)\n\nrun = time.perf_counter() - start\nprint('{} runs for {:.2f} seconds.'.format('lightgbm', run))\n\n\ncur_month_run_total = time.perf_counter() - start_cur_month\nprint('Total running time was {:.2f} minutes.'.format(cur_month_run_total/60))\nprint('-' * 50)","execution_count":88,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa163e8ddd880d72c0c7095ee7399409f563479"},"cell_type":"code","source":"# #slice_end = slice_start + cur_X_test.shape[0]\n# X_all_level2[ slice_start : slice_end , :] = np.c_[preds].transpose()\n# slice_start = slice_end\n\n\n# Split train and test\ntest_nrow = len(preds[0])\nX_train_level2 = X_all_level2[ : -test_nrow, :]\nX_test_level2 = X_all_level2[ -test_nrow: , :]\ny_train_level2 = y_all_level2[ : -test_nrow]\ny_test_level2 = y_all_level2[ -test_nrow : ]\n\nprint('%0.2f min: Finish training First level models'%((time.perf_counter() - start_first_level_total)/60))","execution_count":89,"outputs":[]},{"metadata":{"_uuid":"0b8268ac2a42cacb38cbbdf88fbba4b17a86d573"},"cell_type":"markdown","source":"# 4. Ensembling"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4d082063dd3ca89c65feb300b6fc9bd94e4958a3"},"cell_type":"code","source":"pred_list = {}\n\n#A. Second level learning model via linear regression\nprint('Training Second level learning model via linear regression')\n\nfrom sklearn.linear_model import (LinearRegression, SGDRegressor)\nlr = LinearRegression()\nlr.fit(X_train_level2, y_train_level2)\n#Compute R-squared on the train and test sets.\nprint('Train R-squared for %s is %f' %('test_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, lr.predict(X_train_level2)))))\ntest_preds_lr_stacking = lr.predict(X_test_level2)\ntrain_preds_lr_stacking = lr.predict(X_train_level2)\nprint('Train R-squared for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_lr_stacking))))\n\npred_list['test_preds_lr_stacking'] = test_preds_lr_stacking\nif Validation:\n    print('Test R-squared for %s is %f' %('test_preds_lr_stacking', sqrt(mean_squared_error(y_test_level2, test_preds_lr_stacking))))","execution_count":90,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93460138a4d2ec3441fd20b228eb08df26c1d92b"},"cell_type":"code","source":"# B. Second level learning model via SGDRegressor\nprint('Training Second level learning model via SGDRegressor')\nsgdr= SGDRegressor(\n    penalty = 'l2' ,\n    random_state = SEED )\n\nsgdr.fit(X_train_level2, y_train_level2)\n# Compute R-squared on the train and test sets.\n# print('Train R-squared for %s is %f' %('test_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, lr.predict(X_train_level2)))))\ntest_preds_sgdr_stacking = sgdr.predict(X_test_level2)\ntrain_preds_sgdr_stacking = sgdr.predict(X_train_level2)\nprint('Train R-squared for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_sgdr_stacking))))\n\npred_list['test_preds_sgdr_stacking'] = test_preds_sgdr_stacking\nif Validation:\n    print('Test R-squared for %s is %f' %('test_preds_sgdr_stacking', sqrt(mean_squared_error(y_test_level2, test_preds_sgdr_stacking))))\n\n\nprint('%0.2f min: Finish training second level model'%((time.time() - start_time)/60))","execution_count":91,"outputs":[]},{"metadata":{"_uuid":"01bd0de71457baf97ea7d5dbf4fe51cf5af0fb83"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"6fbbb47eb3a04b2ce1cebd6cacfebba718eed780"},"cell_type":"code","source":"if not Validation:\n    submission = pd.read_csv('%s/sample_submission.csv' % data_path)\n\nver = 6\nfor pred_ver in ['lr_stacking', 'sgdr_stacking']:\n    print(pred_list['test_preds_' + pred_ver].clip(0,20).mean())\n    submission['item_cnt_month'] = pred_list['test_preds_' + pred_ver].clip(0,20)\n    submission[['ID', 'item_cnt_month']].to_csv('submission.csv', index = False)\n\nprint('%0.2f min: Finish running scripts'%((time.time() - start_time)/60))","execution_count":92,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd3d26310a1579a77ae77d6e6521132f8d327f5f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}