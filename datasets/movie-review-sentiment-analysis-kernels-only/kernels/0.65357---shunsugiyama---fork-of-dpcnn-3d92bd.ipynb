{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.tsv\", sep=\"\\t\")\ndf_test = pd.read_csv(\"../input/test.tsv\", sep=\"\\t\")\n\ndef transform(df, train=True):\n    NUM_FOLDS = 5\n    df[\"phrase_count\"] = df.groupby(\"SentenceId\")[\"Phrase\"].transform(\"count\")\n    df[\"word_count\"] = df[\"Phrase\"].apply(lambda x: len(x.split()))\n    df[\"has_upper\"] = df[\"Phrase\"].apply(lambda x: x.lower() != x)\n    df[\"sentence_end\"] = df[\"Phrase\"].apply(lambda x: x.endswith(\".\"))\n    df[\"after_comma\"] = df[\"Phrase\"].apply(lambda x: x.startswith(\",\"))\n    df[\"sentence_start\"] = df[\"Phrase\"].apply(lambda x: \"A\" <= x[0] <= \"Z\")\n    if train:\n        df[\"fold_id\"] = df[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)\n    return df\n\ndf = transform(df)\ndf_test = transform(df_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14374c81f355d9150083b6b0729daa1af2a5168c"},"cell_type":"code","source":"from sklearn.utils import shuffle\nfrom keras.utils import to_categorical\n\ndf = shuffle(df, random_state=42)\n#X = df['Phrase']\n#y = df['Sentiment']\n\n#y = to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ab921fb0c6fee413cfd202f28dc450ec9e6b94b"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\nmax_features=15000\nmaxlen = 57\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(df['Phrase']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"524b392c77eeb19305f5086cbad8677669127a74"},"cell_type":"code","source":"from keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, MaxPooling1D, Conv1D, SpatialDropout1D\nfrom keras.layers import add, Dropout, PReLU, BatchNormalization, GlobalMaxPooling1D, concatenate\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback, EarlyStopping\nfrom keras import optimizers\nfrom keras import initializers, regularizers, constraints, callbacks\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7301b47b8585f6bd526c608f22a8173eb1bebf0"},"cell_type":"code","source":"max_features=15000\nmaxlen = 57\n\nseq = tokenizer.texts_to_sequences(df['Phrase'])\nseq = pad_sequences(seq, maxlen=maxlen)\nseq_test = tokenizer.texts_to_sequences(df_test['Phrase'])\nseq_test = pad_sequences(seq_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"127a6f57a54f44ec671a1994698c7f010b2d4e57"},"cell_type":"code","source":"def build_model():\n    filter_nr = 64\n    filter_size = 3\n    max_pool_size = 3\n    max_pool_strides = 2\n    dense_nr = 256\n    spatial_dropout = 0.2\n    dense_dropout = 0.5\n    train_embed = False\n    conv_kern_reg = regularizers.l2(0.00001)\n    conv_bias_reg = regularizers.l2(0.00001)\n\n    inp1 = Input(shape=(6, ))\n    dense_vector = BatchNormalization()(inp1)\n\n    comment = Input(shape=(maxlen,))\n    emb_comment = Embedding(max_features, 64, input_length=maxlen)(comment)\n    emb_comment = SpatialDropout1D(spatial_dropout)(emb_comment)\n\n    block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\n    block1 = BatchNormalization()(block1)\n    block1 = PReLU()(block1)\n    block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1)\n    block1 = BatchNormalization()(block1)\n    block1 = PReLU()(block1)\n\n    block1_output = add([block1, emb_comment])\n    block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output)\n\n    block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1_output)\n    block2 = BatchNormalization()(block2)\n    block2 = PReLU()(block2)\n    block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2)\n    block2 = BatchNormalization()(block2)\n    block2 = PReLU()(block2)\n\n    block2_output = add([block2, block1_output])\n    block2_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output)\n\n    block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2_output)\n    block3 = BatchNormalization()(block3)\n    block3 = PReLU()(block3)\n    block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block3)\n    block3 = BatchNormalization()(block3)\n    block3 = PReLU()(block3)\n\n    block3_output = add([block3, block2_output])\n    block3_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output)\n\n    block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block3_output)\n    block4 = BatchNormalization()(block4)\n    block4 = PReLU()(block4)\n    block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block4)\n    block4 = BatchNormalization()(block4)\n    block4 = PReLU()(block4)\n\n    block4_output = add([block4, block3_output])\n    block4_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block4_output)\n\n    output = GlobalMaxPooling1D()(block4_output)\n    output = concatenate([dense_vector, output])\n    output = Dense(50, activation=\"relu\")(output)\n    output = Dense(20, activation=\"relu\")(output)\n    output = Dense(5, activation='softmax')(output)\n\n    model = Model([inp1, comment], output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1f445facf79911e3d80afc2c7eb3be319d2edd2"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\ndense_features = [\"phrase_count\", \"word_count\", \"has_upper\", \"after_comma\", \"sentence_start\", \"sentence_end\"]\nepochs = 15\nbatch_size = 1024\ntest_preds = np.zeros((df_test.shape[0], 5))\n\nfor i in range(5):\n    print(\"FOLD\"+str(i+1))\n    train_seq, val_seq = seq[df[\"fold_id\"] != i], seq[df[\"fold_id\"] == i]\n    train_dense, val_dense = df[df[\"fold_id\"] != i][dense_features], df[df[\"fold_id\"] == i][dense_features]\n    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=3, verbose=1)\n    y_train = to_categorical(df[df[\"fold_id\"] != i][\"Sentiment\"])\n    y_val = to_categorical(df[df[\"fold_id\"] == i][\"Sentiment\"])\n    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n    model = build_model()\n    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(), metrics=['accuracy'])\n    model.fit([train_dense, train_seq],y_train, validation_data=([val_dense, val_seq], y_val), epochs=epochs, batch_size=batch_size, \n              shuffle=True,callbacks=[early_stopping], verbose=1)\n    test_preds += model.predict([df_test[dense_features], seq_test], batch_size=1024, verbose=1)\ntest_preds /= 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"390ae00ab870caa0521c11ec4cfd24e13c087763"},"cell_type":"code","source":"df_test['Sentiment'] = np.argmax(test_preds, axis=1)\ndf_test[[\"PhraseId\", \"Sentiment\"]].to_csv(\"submittion.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f984a8f4f923c0add64cf191aa7c47023eb85e65"},"cell_type":"code","source":"df_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a44fedf6dd278ca3e16fe2cdcdd8bb6066f2a7f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32c7e05f3b004c1ed63a7f1f28f2c955bf3a6c2d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}