{"cells":[{"metadata":{"trusted":true,"_uuid":"57dc77b28b32a2f77704fe7e2e2ebaf9b4dbb295"},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import ElasticNetCV, ElasticNet\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n#sns.set_palette(\"bright\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12a348e7fe9c4ded6e4082592c544881c75bb99d"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12ed70cc1151d1df079a3e4ee7716513724c8483"},"cell_type":"code","source":"# this function reduces the memory print for dataset. it helps since we are using gridsearch\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93160acadbd6c6fb707b0960383a6ad2f6469776"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c975cd5e4093bdb85299c2aeff54d700952b4e11"},"cell_type":"code","source":"# drop the columns which are not required for modelling like ID. \ntarget = train_df[\"target\"]\ntrain_df = train_df.drop([\"target\",\"id\"],axis=1)\ntest_id = test_df[\"id\"]\ntest_df = test_df.drop([\"id\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e27b138527608258e2742071afa71649d1098431"},"cell_type":"code","source":"#sc = StandardScaler()\n#train_df = pd.DataFrame(sc.fit_transform(train_df))\n#test_df = pd.DataFrame(sc.transform(test_df))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e090aee11b0ff54d1442fa1e7e81fda0ad5b86f"},"cell_type":"code","source":"type(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05d228925fa2ece356861c9fdd34ddf7b2947138"},"cell_type":"code","source":"#Reduce memoray usage\ntrain_df=reduce_mem_usage(train_df)\ntest_df=reduce_mem_usage(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a813ed91124979b23d11b498f68ae0268f2a7f0"},"cell_type":"code","source":"#lets define grid for elastic net\n# split the training into 0.75 train and 0.25 test for cross validation\nfrom sklearn.model_selection import StratifiedShuffleSplit,RepeatedStratifiedKFold\nshuffle_split = StratifiedShuffleSplit(test_size=0.25,train_size=0.75,n_splits=35,random_state=87951)\n#kfold = RepeatedStratifiedKFold(n_splits=25, n_repeats=10, random_state=87951)\nparam_grid = {\n                'alpha'     : [0.1,1,10,0.01],\n                'l1_ratio'  :  np.arange(0.40,1.00,0.10),\n                'tol'       : [0.0001,0.001]\n            }\neNet = ElasticNet(max_iter=10000)\ngrid_search = GridSearchCV(eNet, \n                           param_grid, \n                           scoring='roc_auc', \n                           cv = shuffle_split,\n                           return_train_score=True,\n                           n_jobs = -1)\ngrid_search.fit(train_df,target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cab1a0b379fcd87b0d2bc0e7a854c79aa175d4c"},"cell_type":"code","source":"print(\"Best parameters : {}\".format(grid_search.best_params_))\nprint(\"Best cross validation score: {:.2f}\".format(grid_search.best_score_))\nprint(\"Best estimator: {}\".format(grid_search.best_estimator_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0df205a5012b62b591828560cfc6718b0bef7492"},"cell_type":"code","source":"results = pd.DataFrame(grid_search.cv_results_)\nresults.sort_values(['mean_test_score'],ascending = False)[:10]\n#results.loc[\"params\", \"mean_test_score\", \"std_test_score\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faf6900be9dca1108ad0939580a4cb6d1408ff6b"},"cell_type":"code","source":"scores_mean = np.array(results.mean_test_score).reshape(-1)\nscores_std = np.array(results.std_test_score).reshape(-1)\nprint(\"mean CV scores for each fold {} \".format(scores_mean))\nprint(\"std CV scores for each fold {} \".format(scores_std))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c7cc9c4dbb0179cc93d7305192009bbba24361a"},"cell_type":"code","source":"clf = grid_search.best_estimator_\ntype(clf)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#plot the features and their coeff which model has found\nel_df =pd.Series(clf.coef_,index=train_df.columns)\nel_df = el_df[clf.coef_!=0]\nplt.figure(figsize=(8,6))\nel_df.plot(kind='barh')\nplt.xlabel(\"Importance\",fontsize=12)\nplt.ylabel(\"Features\",fontsize=12)\nplt.title(\"Top Features\",fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae32be9aa666d6575c99c647fc2e16eb6d68be22"},"cell_type":"markdown","source":"lets analyze results for the model. we can take these 12 features and do some feature engineeting."},{"metadata":{"trusted":true,"_uuid":"98f69e900f01a27c06e499757b0d52575c78397a"},"cell_type":"code","source":"train_short=train_df.iloc[:,clf.coef_!=0]\ntest_short=test_df.iloc[:,clf.coef_!=0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20aa96d746e0e5fee29e7f764831dc5c8a92dec4"},"cell_type":"code","source":"train_short.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca4769592786e4691881f976c45667e22ecc96a1"},"cell_type":"markdown","source":"RF top 15 features -->  https://www.kaggle.com/tomehta/feature-selection-with-random-forest\n['33',\n '65',\n '117',\n '217',\n '91',\n '295',\n '214',\n '268',\n '189',\n '199',\n '24',\n '56',\n '39',\n '237',\n '201']"},{"metadata":{"trusted":true,"_uuid":"28f89aef953741f6c305d31ba65b8c976ce77872"},"cell_type":"code","source":"# we can try bucketing the features,,may be another day\n#cmb_df= pd.concat([train_short,test_short])\n#now plan is to bin the continous features. before that again check if anything co-related.\n#sns.heatmap(cmb_df.corr())\n# bin the columns as per their percentile\n#for col in cmb_df.columns:\n    #bins = np.linspace(-5,5,11)\n    #bins = np.percentile(cmb_df[col],range(0,101,10))\n    #cmb_df[col+'_binned' ] = pd.cut(cmb_df[col], bins=bins)\n    #cmb_df[col+'_binned' ] = pd.qcut(cmb_df[col],10, duplicates='drop')\n#create dummies for binned cols\n#cmb_df = pd.get_dummies(cmb_df,dummy_na= False)\n#train_X = cmb_df.iloc[:250]\n#test_X = cmb_df.iloc[250:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcfe6ad6e33fce67822b568d71bd2dbd1fb0e5d2"},"cell_type":"code","source":"pred_el = grid_search.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f95c92ec0d05596b47b338eb903926be0044bd1e"},"cell_type":"code","source":"pred_el.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4e6952cf578e9b076fd31741bbc42c36abeea7e"},"cell_type":"code","source":"#print test file \nsub_df = pd.DataFrame()\nsub_df[\"id\"] = test_id \nsub_df[\"target\"] = pred_el\nsub_df.to_csv(\"baseline_el.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61f598c7a88490bb87002637d59cb18cf3cb4cd4"},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b1b9185ebb4fc58ca6b6aedfad92a58d255e95a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}