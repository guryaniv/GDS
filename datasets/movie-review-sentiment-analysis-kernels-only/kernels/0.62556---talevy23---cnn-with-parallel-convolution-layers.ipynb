{"cells":[{"metadata":{"_uuid":"f09b9d9b86f19f7ac5263912742d1c3d0a68a445"},"cell_type":"markdown","source":"Paralel convolution layers followed by maxpool (sizes 3,4 and 5) concatenated to dense layer.\n\nReference: Yoon Kim https://github.com/yoonkim/CNN_sentence"},{"metadata":{"trusted":true,"_uuid":"099cef438c884c9d7ab326fed0225d7b9784bfe2"},"cell_type":"code","source":"# references:\n# https://www.kaggle.com/antmarakis/cnn-baseline-model\n# http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n# https://github.com/yoonkim/CNN_sentence\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nseed = 12345\nimport random\nimport numpy as np\nfrom tensorflow import set_random_seed\n\nrandom.seed(seed)\nnp.random.seed(seed)\nset_random_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d3f692a5ec11bb372088b55b0a1c95937b58fbd"},"cell_type":"markdown","source":"# Input"},{"metadata":{"trusted":true,"_uuid":"0dfe6bc0b59d4fd21ab888482e630f0220b42f3d"},"cell_type":"code","source":"train,test,sampleSubmission = pd.read_csv('../input/train.tsv', sep = '\\t'),pd.read_csv('../input/test.tsv', sep = '\\t'),pd.read_csv('../input/sampleSubmission.csv')\n\n# train,test,sampleSubmission = pd.read_csv('all/train.tsv', sep = '\\t'),pd.read_csv('all/test.tsv', sep = '\\t'),pd.read_csv('all/sampleSubmission.csv')\ntrain.head(3)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5df48ffba0cd8a34d8a713e1c7ede41005a03816"},"cell_type":"markdown","source":"# Sentence preprocessing"},{"metadata":{"trusted":true,"_uuid":"dd0933ef1d842ce17c33a1d3a44518eeed8a64f8"},"cell_type":"code","source":"# From Yoon Kim work:\n# https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\nimport re\ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n    string = re.sub(r\"\\'s\", \" \\'s\", string) \n    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n    string = re.sub(r\"\\'re\", \" \\'re\", string) \n    string = re.sub(r\"\\'d\", \" \\'d\", string) \n    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n    string = re.sub(r\",\", \" , \", string) \n    string = re.sub(r\"!\", \" ! \", string) \n    string = re.sub(r\"\\(\", \" \\( \", string) \n    string = re.sub(r\"\\)\", \" \\) \", string) \n    string = re.sub(r\"\\?\", \" \\? \", string) \n    string = re.sub(r\"\\s{2,}\", \" \", string)    \n    return string.strip().lower()\n\nphrases = [clean_str(s) for s in train['Phrase']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db3b9ecfe040a2d1e05091f537d136d1373c925f"},"cell_type":"code","source":"type(train['Phrase'])\nlen(phrases[2].split())\nphrases[2].split()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3710e5647a91020d9cfe8ad06735dda64641132"},"cell_type":"markdown","source":"# One Hot Labels"},{"metadata":{"trusted":true,"_uuid":"16557b8c2002d644cc265e64f05b95cdebcdcdaa"},"cell_type":"code","source":"from keras.utils import to_categorical\nY = to_categorical(train['Sentiment'].values)\nprint(Y[155:165])\nprint(train['Sentiment'].values[155:165])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1be6330aa501d1baa23fc9df99cb406cad51de8e"},"cell_type":"markdown","source":"# Embedding Layer"},{"metadata":{"_uuid":"fe7a0a7d3f62e7b93d761e6979a88adf5b83e913"},"cell_type":"markdown","source":"## Text to Tokens"},{"metadata":{"trusted":true,"_uuid":"53a57be7780e9524ff00d28e95eadfe31e3d623a"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# tokenizer = Tokenizer(num_words=max_features)\nt = Tokenizer()\nt.fit_on_texts(phrases)\nvocab_size = len(t.word_index) + 1\nX = t.texts_to_sequences(phrases)\n# print(X)\nmax_length = max([len(test.split()) for test in phrases ])\nX = pad_sequences(X,maxlen=max_length,padding = 'post')\n# print(X)\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f069da5ee8ff4a7bb921d3f4e8569eef0223d03"},"cell_type":"markdown","source":"## Word Embedding + Model"},{"metadata":{"_uuid":"b56e56c26222b467af240bc19b42b323bd8c77c8"},"cell_type":"markdown","source":"- 52 words max length (52 length vector)\n- embedding dim of 8 ([52,8] embedding layer)\n- 3 parallel convolution layers to capture 3,4 and 5 words ([50,5],[49,5],[48,5] covolution output)\n- Each convolution layer is followed by a maxpool to capture 1 feature ([1,5] output from each convolution layer)\n- concatenate parallel lines\n- flatten and dense layer 20% dropout rate (15 neurons to 5 outputs)\n\n"},{"metadata":{"trusted":true,"_uuid":"9ae88fb0061d918f3f9bd276f4ef7805bfe19b58"},"cell_type":"code","source":"from keras.layers import Embedding\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation\nfrom keras.layers import Flatten, Conv1D, SpatialDropout1D, MaxPooling1D,AveragePooling1D, merge, concatenate, Input, Dropout\n\n# ONE LAYER\ndef model(output_dim=8, max_length=50, y_dim=5, num_filters=5, filter_sizes = [3,5], pooling = 'max', pool_padding = 'valid', dropout = 0.2):\n    # Input Layer\n#     embed_input = Input(shape=(max_length,output_dim))\n    embed_input = Input(shape=(max_length,))\n    x = Embedding(vocab_size,output_dim,input_length=max_length)(embed_input)\n#     x = SpatialDropout1D(0.2)(x)\n    ## concat\n    pooled_outputs = []\n    for i in range(len(filter_sizes)):\n        conv = Conv1D(num_filters, kernel_size=filter_sizes[i], padding='valid', activation='relu')(x)\n        if pooling=='max':\n            conv = MaxPooling1D(pool_size=max_length-filter_sizes[i]+1, strides=1, padding = pool_padding)(conv)\n        else:\n            conv = AveragePooling1D(pool_size=max_length-filter_sizes[i]+1, strides=1, padding = pool_padding)(conv)            \n        pooled_outputs.append(conv)\n    merge = concatenate(pooled_outputs)\n        \n    x = Flatten()(merge)\n    x = Dropout(dropout)(x)\n    predictions = Dense(y_dim, activation = 'sigmoid')(x)\n    \n    model = Model(inputs=embed_input,outputs=predictions)\n\n    model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n    print(model.summary())\n    \n    from keras.utils import plot_model\n    plot_model(model, to_file='shared_input_layer.png')\n    \n    return model\n\n\nmodel = model(output_dim=16, max_length=max_length,y_dim=5,filter_sizes = [3,4,5],pooling = 'max',dropout=0.5)\nfrom IPython.display import Image\nImage(filename='shared_input_layer.png') \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b17dd72b744422932cae73fe14799b6ac22f3247"},"cell_type":"code","source":"# ## PREVIOUS MODELS\n# def model1(output_dim=8, max_length=50, y_dim=5, filter_sizes = [3]):\n#     model = Sequential()\n#     model.add(Embedding(vocab_size,output_dim,input_length=max_length))\n#     model.add(Flatten())\n#     model.add(Dense(y_dim, activation = 'sigmoid'))\n\n#     model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n#     print(model.summary())\n#     return model\n\n# def model2(output_dim=8, max_length=50, y_dim=5, num_filters=5, filter_sizes = [3,5]):\n#     model = Sequential()\n#     model.add(Embedding(vocab_size,output_dim,input_length=max_length))\n\n#     model.add(SpatialDropout1D(0.2))\n\n#     ## GOOD\n#     model.add(Conv1D(num_filters, kernel_size=filter_sizes[0], padding='valid', activation='relu'))\n#     model.add(MaxPooling1D(pool_size=max_length-filter_sizes[0]+1, strides=1, padding='valid'))\n\n#     model.add(Flatten())\n#     model.add(Dense(y_dim, activation = 'sigmoid'))\n\n#     model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n#     print(model.summary())\n#     return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e57d0a7ff7691692546869bf81a8b4218a3018d"},"cell_type":"markdown","source":"# Train/Validatio Split"},{"metadata":{"trusted":true,"_uuid":"185ef5375be38687f1afe4b0f1dee0c5d0c880ce"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"837869f4f93bbcf9c58e2bf0a8f6145abc7b48b8"},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true,"_uuid":"307029d9a02d7a9ac175ad5b377031fc5f42f23a"},"cell_type":"code","source":"epochs = 5\nbatch_size = 32\n\nmodel.fit(X_train,Y_train,epochs = epochs, validation_data=(X_val,Y_val), batch_size=batch_size, verbose = 1)\nloss,accuracy = model.evaluate(X_val,Y_val)\nprint('Accuracy: %f' % (accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa47e4eec481f32a94029b0f9b577e74aedfbfdf"},"cell_type":"markdown","source":"## Same with average pooling"},{"metadata":{"trusted":true,"_uuid":"be5555f6219ffad7b158b97013a0b31a64953dc2"},"cell_type":"code","source":"# epochs = 10\n# batch_size = 32\n\n# model_avg = model(output_dim=16, max_length=max_length,y_dim=5,filter_sizes = [3,4,5],pooling = 'avg',dropout=0.5)\n\n# model_avg.fit(X_train,Y_train,epochs = epochs, validation_data=(X_val,Y_val), batch_size=batch_size, verbose = 1)\n# loss,accuracy = model_avg.evaluate(X_val,Y_val)\n# print('Accuracy: %f' % (accuracy*100))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85def27f852d252c0439abc6e7c889204b136350"},"cell_type":"markdown","source":"Same model where max pooling layer creates catches 5 words at a time.\nPrevious models maxpool was on the entire text, outputing 1 feature.\nThis will require 'same' convolution padding so all filters outputs will be same length and concatenate will be on same size metrices.\nWe also have more weights so lets increase dropout."},{"metadata":{"trusted":true,"_uuid":"ede29b7c70273c645ff6f45c72e1c36df4eb8429"},"cell_type":"code","source":"# ONE LAYER\ndef model_constant_poolsize(output_dim=8, max_length=50, y_dim=5, num_filters=5, filter_sizes = [3,5], pooling = 'max', pool_padding = 'valid', pool_size=5,dropout=0.4):\n    # Input Layer\n#     embed_input = Input(shape=(max_length,output_dim))\n    embed_input = Input(shape=(max_length,))\n    x = Embedding(vocab_size,output_dim,input_length=max_length)(embed_input)\n#     x = SpatialDropout1D(0.2)(x)\n    ## concat\n    pooled_outputs = []\n    for i in range(len(filter_sizes)):\n        conv = Conv1D(num_filters, kernel_size=filter_sizes[i], padding='same', activation='relu')(x)\n        if pooling=='max':\n            conv = MaxPooling1D(pool_size=pool_size, strides=1, padding = pool_padding)(conv)\n        else:\n            conv = AveragePooling1D(pool_size=pool_size, strides=1, padding = pool_padding)(conv)            \n        pooled_outputs.append(conv)\n    merge = concatenate(pooled_outputs)\n        \n    x = Flatten()(merge)\n    x = Dropout(dropout)(x)\n    predictions = Dense(y_dim, activation = 'sigmoid')(x)\n    \n    model = Model(inputs=embed_input,outputs=predictions)\n\n    model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n    print(model.summary())\n    \n    from keras.utils import plot_model\n    plot_model(model, to_file='shared_input_layer_model_constant_poolsize.png')\n    \n    return model\n\n\nmodel_constant_poolsize = model_constant_poolsize(output_dim=16, max_length=max_length,y_dim=5,filter_sizes = [3,4,5],pooling = 'max', pool_size=5,dropout = 0.5)\nfrom IPython.display import Image\nImage(filename='shared_input_layer_model_constant_poolsize.png') \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd7fede568478743c512cb852554afb9da371572"},"cell_type":"code","source":"epochs = 5\nmodel_constant_poolsize.fit(X_train,Y_train,epochs = epochs, validation_data=(X_val,Y_val), batch_size=batch_size, verbose = 1)\nloss,accuracy = model_constant_poolsize.evaluate(X_val,Y_val)\nprint('Accuracy: %f' % (accuracy*100))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52f5febc93b2814dc9bc467bd6d371cc7e00f8d7"},"cell_type":"markdown","source":"# Create Submission"},{"metadata":{"_uuid":"4e79c0584934c1bdde4006b3751cd210759c7d43"},"cell_type":"markdown","source":"Looks like best results came from the model where maxpooling outputs more than one feature.\nWe'll score test data on that model"},{"metadata":{"trusted":true,"_uuid":"f38d43b67fe9b8df9685217caa8428fff8bde2bc"},"cell_type":"code","source":"# pre processing\ntest_phrases = [clean_str(s) for s in test['Phrase']]\n# text to tokens\nX_test = t.texts_to_sequences(test_phrases)\nX_test = pad_sequences(X_test,maxlen=max_length,padding = 'post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"758e175b8b03bc65daef574f719d9c2ba177468c"},"cell_type":"code","source":"# sampleSubmission['Sentiment'] = model.predict_classes(X_test,verbose=1)\nsampleSubmission['Sentiment'] = model_constant_poolsize.predict(X_test,verbose=1).argmax(axis=-1)\nsampleSubmission.to_csv('sub_cnn_constant_maxpool.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0e76b9964de23bcac45448d7160a2259a5dc32f","collapsed":true},"cell_type":"markdown","source":"## Submission on model trained on full data"},{"metadata":{"trusted":true,"_uuid":"486d1ec1f0f9e0fdfe498836b4a8f0ff44f77913"},"cell_type":"code","source":"# train\nmodel_constant_poolsize.fit(X,Y,epochs = epochs, validation_data=(X_val,Y_val), batch_size=batch_size, verbose = 1)\n\n# submission\nsampleSubmission['Sentiment'] = model_constant_poolsize.predict(X_test,verbose=1).argmax(axis=-1)\nsampleSubmission.to_csv('sub_cnn_constant_maxpool_FULL.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28cc09f2d9f9e53bf305a573ef63ebbe68287630"},"cell_type":"markdown","source":"Val_acc here is meaningless as we are validating on subset of train data (definitely not best practice)\nTODO:\nCheck missclassified samples"},{"metadata":{"trusted":true,"_uuid":"e07520305e07ae08d7ee1031020eacf65f6c62c1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}