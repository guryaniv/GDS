{"cells":[{"metadata":{"_uuid":"c6405a9ba97dc7bfae5437fea9ae4943ccd0cb55"},"cell_type":"markdown","source":"# Taxifare competition using TF.data to process full dataset #\n\nThis is an exercise  using TF.data API, that let us process huge datasets without memory costraints. Infact this Kernel uses less than 8Gbytes, but can process the full 55 million records of the *train.csv* file.\nThere are a few things I am not happy with this Kernel (no data cleaning, slow separation of train and eval,...), so any comments are welcome."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os; os.environ['OMP_NUM_THREADS'] = '4'\n\nimport tensorflow as tf\nimport numpy as np\nimport shutil\nimport pandas as pd\n\nprint(\"Tensorflow v\",tf.__version__)\nassert tf.__version__ >= \"1.8\" or tf.__version__ >= \"1.10\"\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# List the CSV columns\nCSV_COLUMNS = ['fare_amount', 'pickup_datetime','pickup_longitude','pickup_latitude',\n               'dropoff_longitude','dropoff_latitude', 'passenger_count', 'key']\n\n#Choose which column is your label\nLABEL_COLUMN = 'fare_amount'\nTRAIN_LINES = 55423856","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41690bb0b2e4fe9a2429efd13f517c194d1641e2"},"cell_type":"code","source":"from contextlib import contextmanager\nimport time\n#Utility generator to time operations like training and evaluation\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d316db9def3e86baeb1830b6dd70a47e843aa5c"},"cell_type":"code","source":"#This is just to have a look at the input data\nPATH = '../input'\ntrain_df = pd.read_csv(f'{PATH}/train.csv', nrows=10000)\ntrain_df.head()\n#train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63eba5f8f1a91979ca0b843b67e7c086606c891c"},"cell_type":"markdown","source":"# Read the dataset from a csv file\nLet's start using the new API (introduced in core Tensorflow in version 1.8) to read a dataset directly from a CSV file.\nAs it is difficult to debug with Tensorflow  I use below cell just as a safety check to be sure I am reading the data correctly."},{"metadata":{"trusted":true,"_uuid":"973ceacc4cd789f6ad067e6b2842e7b7898828a1"},"cell_type":"code","source":"BATCH_SIZE=8 \ndataset = tf.contrib.data.make_csv_dataset(\n    file_pattern=f'{PATH}/train.csv',\n    batch_size=BATCH_SIZE,\n    column_names=None,\n    column_defaults=None,\n    label_name='fare_amount',\n    select_columns=[1, 2, 3, 4, 5, 6, 7],\n    field_delim=',',\n    use_quote_delim=True,\n    na_value='',\n    header=True,\n    num_epochs=None,\n    shuffle=True,\n    shuffle_buffer_size=10000,\n    shuffle_seed=None,\n    prefetch_buffer_size=1,\n    num_parallel_reads=1,\n    num_parallel_parser_calls=2,\n    sloppy=False,\n    num_rows_for_inference=100\n)\n\nnext_element = dataset.make_one_shot_iterator().get_next()\nwith tf.Session() as sess:\n    features, label = sess.run(next_element)\n    print(\"Features:\\n\", features, \"\\n\\nLabel:\\n\", label)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4afe5686d6c7a91aef34dd1a5c241d30b3e2f23e"},"cell_type":"markdown","source":"Python function to calculate the day of the week. It will be used when mapping the dataset. \nIt'd be more efficient to use tensorflow operators, but it would require much more coding and I am lazy :-) "},{"metadata":{"trusted":true,"_uuid":"227a11c9cb10a34d769700835e9aaf1450e356f0"},"cell_type":"code","source":"\ndef pd_weekDay(year, month, day):\n    df = pd.DataFrame({'year': year,\n                       'month': month,\n                       'day': day})\n    date_df = pd.to_datetime(df)\n    return date_df.dt.weekday.astype(np.int32)\n\n#Let's check that the function is working correctly\nyears=np.array([2018, 2018, 2018])\nmonths=np.array([8, 11, 1])\ndays=np.array([20, 6, 8])\nprint(pd_weekDay(years, months, days))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c19f6f5f3bfc3395a5615ee6757fc710d65776d3"},"cell_type":"markdown","source":"The function is used when mapping the dataset. It creates new boolean features \nstating when coordinates are at one of the NYC airports"},{"metadata":{"trusted":true,"_uuid":"6951ccbd7cb40912a67a66b0ef1aa3043fcc399c"},"cell_type":"code","source":"def tf_isAirport(latitude,longitude,airport_name='JFK'):\n    jfkcoord = tf.constant([-73.8352, -73.7401, 40.6195, 40.6659])\n    ewrcoord = tf.constant([-74.1925, -74.1531, 40.6700, 40.7081])\n    lgucoord = tf.constant([-73.8895, -73.8550, 40.7664, 40.7931])\n    if airport_name=='JFK':\n        coord = jfkcoord\n    elif airport_name=='EWR':\n        coord = ewrcoord\n    elif airport_name=='LGU':\n        coord = lgucoord\n    else:\n        raise ValueError( f'Unknown NYC Airport {airport_name}' )\n        \n    is_airport = \\\n    tf.logical_and(\n        tf.logical_and(\n            tf.greater(latitude, coord[0]), tf.less(latitude, coord[1])\n        ),\n        tf.logical_and(\n            tf.greater(longitude, coord[2]), tf.less(longitude, coord[3])\n        )\n    )\n    return is_airport\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50ffcade37dd808b986be2843b464a3eb9ae592c"},"cell_type":"markdown","source":"# Feature Engineering\nHere is the function called by dataset.map(). It does all the feature engineering work.\nIt works also for prediction, when there is no label."},{"metadata":{"trusted":true,"_uuid":"930d90d91583479c0613eb91f8150e02ddf525ec"},"cell_type":"code","source":"def feat_eng_func(features, label=None):\n    print(\"Feature Engineered Label:\", label)\n    #New features based on pickup datetime\n    features['pickup_year'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 0, 4), tf.int32)\n    features['pickup_month'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 5, 2), tf.int32)\n    features['pickup_day'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 8, 2), tf.int32)\n    features['pickup_hour'] = tf.string_to_number(tf.substr(features['pickup_datetime'], 11, 2), tf.int32)\n    #TODO is there an easy way to perform below calculation using TF APIs?\n    features['pickup_weekday'] = tf.py_func(pd_weekDay,\n                                            [features['pickup_year'], features['pickup_month'], features['pickup_day']],\n                                            tf.int32,\n                                            stateful=False,\n                                            name='Weekday'\n                                           )\n    #TODO features['pickup_dayofyear'] = tf.cast(features['pickup_month'] * 30 + features['pickup_day'], tf.int32 )\n    #Normalize year and add decimals for months. This is because fares increase with time\n    features['pickup_dense_year'] = (\n                tf.cast(features['pickup_year'], tf.float32) + \\\n                tf.cast(features['pickup_month'], tf.float32) / tf.constant(12.0, tf.float32) -  \\\n                 tf.constant(2009.0, tf.float32) ) /  \\\n                 tf.constant(6.0, tf.float32) \n   \n    #Clip latitudes and longitudes\n    minlat = tf.constant(38.0)\n    maxlat = tf.constant(42.0)\n    minlon = tf.constant(-76.0)\n    maxlon = tf.constant(-72.0)\n    features['pickup_longitude'] = tf.clip_by_value(features['pickup_longitude'], minlon, maxlon)\n    features['pickup_latitude'] = tf.clip_by_value(features['pickup_latitude'], minlat, maxlat)\n    features['dropoff_longitude'] = tf.clip_by_value(features['dropoff_longitude'], minlon, maxlon)\n    features['dropoff_latitude'] = tf.clip_by_value(features['dropoff_latitude'], minlat, maxlat)\n    #Clip passengers \n    minpass = tf.constant(1.0)\n    maxpass = tf.constant(6.0)\n    features['passenger_count'] = tf.clip_by_value(tf.cast(features['passenger_count'], tf.float32), minpass, maxpass)\n    #Clip fare_amount\n    #TODO normalize or tf.log the fare_amount\n    if label != None:\n        minfare = tf.constant(1.0)\n        maxfare = tf.constant(300.0)\n        label = tf.clip_by_value(label,  minfare, maxfare) \n    #New features based on pickup and dropoff position\n    features['longitude_dist'] = tf.abs(features['pickup_longitude'] - features['dropoff_longitude'])\n    features['latitude_dist'] = tf.abs(features['pickup_latitude'] - features['dropoff_latitude'])\n    #compute euclidean distance of the trip \n    features['distance'] = tf.sqrt(features['longitude_dist']**2 + features['latitude_dist']**2)\n    #features for airport locations\n    features['is_JFK_pickup'] = tf_isAirport(features['pickup_latitude'], \n                                             features['pickup_longitude'],\n                                             airport_name='JFK')\n    features['is_JFK_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n                                             features['dropoff_longitude'],\n                                             airport_name='JFK')\n    features['is_EWR_pickup'] = tf_isAirport(features['pickup_latitude'], \n                                             features['pickup_longitude'],\n                                             airport_name='EWR')\n    features['is_EWR_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n                                             features['dropoff_longitude'],\n                                             airport_name='EWR')\n    features['is_LGU_pickup'] = tf_isAirport(features['pickup_latitude'], \n                                             features['pickup_longitude'],\n                                             airport_name='LGU')\n    features['is_LGU_dropoff'] = tf_isAirport(features['dropoff_latitude'], \n                                             features['dropoff_longitude'],\n                                             airport_name='LGU')\n    features['is_NYC_airport'] = tf.logical_or(\n        tf.logical_or(\n            tf.logical_or(features['is_JFK_pickup'], features['is_JFK_dropoff']),\n            tf.logical_or(features['is_EWR_pickup'], features['is_EWR_dropoff'])),\n        tf.logical_or(features['is_LGU_pickup'], features['is_LGU_dropoff'])\n    )\n    \n    if label == None:\n        return features\n    return (features, label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"561828952619939a3aa218dc5721c4b05e4e8c32"},"cell_type":"markdown","source":"Create an input function that reads a csv file into a dataset.\nIt is used to create the input functions for training, evaluating and predicting\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def read_dataset(filename, mode, batch_size = 512):\n    def _input_fn():    \n        if mode == tf.estimator.ModeKeys.TRAIN:\n            num_epochs = None # indefinitely\n            shuffle = False   # I assume the train records are already shuffled. Not sure this is correct.\n        else:\n            num_epochs = 1 # end-of-input after this\n            shuffle = False\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            label_name=None\n            select_columns=[1, 2, 3, 4, 5, 6]\n        else:\n            label_name ='fare_amount'\n            select_columns = [1, 2, 3, 4, 5, 6, 7]\n\n        # Create list of files that match pattern\n        file_list = tf.gfile.Glob(filename)\n        # Create Dataset from the CSV files\n        dataset = tf.contrib.data.make_csv_dataset(\n            file_pattern=file_list,\n            batch_size=batch_size, \n            column_names=None,\n            column_defaults=None,\n            label_name=label_name,\n            select_columns=select_columns,\n            field_delim=',',\n            use_quote_delim=True,\n            na_value='',\n            header=True,\n            num_epochs=num_epochs,\n            shuffle=shuffle,\n            shuffle_buffer_size=128*batch_size,\n            shuffle_seed=None,\n            prefetch_buffer_size=1,\n            num_parallel_reads=1,\n            num_parallel_parser_calls=3,\n            sloppy=False,\n            num_rows_for_inference=100\n        )\n#This is necessary to split train and eval\n        skip_train_lines = TRAIN_LINES // batch_size // 100 * 10 #skip first 10% lines of train data set\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            dataset = dataset.skip(skip_train_lines) #this is very slow. I don't know if there are better ways.\n        elif mode == tf.estimator.ModeKeys.EVAL:\n            dataset = dataset.take(skip_train_lines) \n\n        dataset = dataset.map(feat_eng_func) #do all the feature engineering\n        #TODO filter the dataset removing outliers and dirty data: \n        #TODO I tried with dataset.filter\n#        dataset = dataset.repeat(3)\n        return dataset.make_one_shot_iterator().get_next()\n    return _input_fn\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a32fd922b6521ad6a44bb54506487ac6d11331e"},"cell_type":"markdown","source":"Again, it is difficult to debug with Tensorflow so I sometimes run this cell just as a safety check to be sure \nI am transforming the data correctly."},{"metadata":{"trusted":true,"_uuid":"d06e3f13a201c8dbcb80a8069abbf5f6b8930776"},"cell_type":"code","source":"train_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.EVAL, batch_size = 8)\nwith timer('Evaluating'):\n    with tf.Session() as sess:\n        features, label = sess.run(train_input_fn())\n        print(\"Features:\\n\", features, \"\\n\\nLabel:\\n\", label)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8791b029ab63b672e4941d414663ffbd13f4509"},"cell_type":"markdown","source":"# Feature Columns\nHere are the functions that return the feature columns. The first function contains all features, the other two functions split the features by sparsity so you can use them with a Wide and Deep model."},{"metadata":{"trusted":true,"_uuid":"35fc0403cde57b11b4fa0a00879b842ca634e328"},"cell_type":"code","source":"# Define your feature columns\ndef create_feature_cols():\n    hour_cat = tf.feature_column.categorical_column_with_identity('pickup_hour', 24 )\n    weekday_cat = tf.feature_column.categorical_column_with_identity('pickup_weekday', 7)\n    hour_X_weekday = tf.feature_column.crossed_column([hour_cat, weekday_cat], 500)\n#    days_list = range(367)\n#    yearday = tf.feature_column.categorical_column_with_vocabulary_list('pickup_dayofyear', days_list)\n    return [\n#    tf.feature_column.numeric_column('pickup_longitude'),\n#    tf.feature_column.numeric_column('pickup_latitude'),\n#    tf.feature_column.numeric_column('dropoff_longitude'),\n#    tf.feature_column.numeric_column('dropoff_latitude'),\n    tf.feature_column.numeric_column('passenger_count'),\n    tf.feature_column.numeric_column('pickup_dense_year'),\n#TODO    tf.feature_column.numeric_column('pickup_dayofyear'),\n#TODO    tf.feature_column.embedding_column(yearday, 2),\n#    tf.feature_column.numeric_column('pickup_year'),\n#    tf.feature_column.numeric_column('pickup_month'),\n#    tf.feature_column.numeric_column('pickup_day'),\n    #TODO use embeddings for the hour\n    #tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('pickup_hour', (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n    #                                                                        11, 12, 13, 14, 15, 16, 17, 18,\n    #                                                                         19, 20, 21, 22, 23) )\n    #                                  ),\n    #tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('pickup_weekday', (0, 1, 2, 3, 4, 5, 6)\n    #                                                                                            )),\n    tf.feature_column.embedding_column(hour_X_weekday, 2),\n    tf.feature_column.embedding_column(\n        tf.feature_column.categorical_column_with_vocabulary_list('pickup_month', (0, 1, 2, 3, 4, 5, 6, \n                                                                                   7, 8, 9, 10, 11, 12)),\n        2),\n    tf.feature_column.numeric_column('longitude_dist'),\n    tf.feature_column.numeric_column('latitude_dist'),\n    tf.feature_column.numeric_column('distance'),\n    tf.feature_column.numeric_column('is_JFK_pickup'),\n    tf.feature_column.numeric_column('is_JFK_dropoff'),\n    tf.feature_column.numeric_column('is_EWR_pickup'),\n    tf.feature_column.numeric_column('is_EWR_dropoff'),\n    tf.feature_column.numeric_column('is_LGU_pickup'),\n    tf.feature_column.numeric_column('is_LGU_dropoff'),\n    tf.feature_column.numeric_column('is_NYC_airport'),\n#    tf.feature_column.numeric_column('is_long_distance')\n  ]\n\n# Define the sparse feature columns to be used with DNNLinearCombinedRegressor\ndef create_sparse_feature_cols():\n    return [\n    tf.feature_column.numeric_column('is_JFK_pickup'),\n    tf.feature_column.numeric_column('is_JFK_dropoff'),\n    tf.feature_column.numeric_column('is_EWR_pickup'),\n    tf.feature_column.numeric_column('is_EWR_dropoff'),\n    tf.feature_column.numeric_column('is_LGU_pickup'),\n    tf.feature_column.numeric_column('is_LGU_dropoff'),\n    tf.feature_column.numeric_column('is_NYC_airport'),\n#    tf.feature_column.numeric_column('is_long_distance')\n  ]\n\n# Define the dense feature columns DNNLinearCombinedRegressor\ndef create_dense_feature_cols():\n    hour_cat = tf.feature_column.categorical_column_with_identity('pickup_hour', 24 )\n    weekday_cat = tf.feature_column.categorical_column_with_identity('pickup_weekday', 7)\n    hour_X_weekday = tf.feature_column.crossed_column([hour_cat, weekday_cat], 500)\n    month_cat = tf.feature_column.categorical_column_with_identity('pickup_month', 13 )\n    return [\n    tf.feature_column.embedding_column(hour_X_weekday, 2),\n    tf.feature_column.embedding_column(month_cat, 2),\n    tf.feature_column.numeric_column('pickup_longitude'),\n    tf.feature_column.numeric_column('pickup_latitude'),\n    tf.feature_column.numeric_column('dropoff_longitude'),\n    tf.feature_column.numeric_column('dropoff_latitude'),\n    tf.feature_column.numeric_column('passenger_count'),\n    tf.feature_column.numeric_column('pickup_dense_year'),\n    tf.feature_column.numeric_column('longitude_dist'),\n    tf.feature_column.numeric_column('latitude_dist'),\n    tf.feature_column.numeric_column('distance'),\n  ]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfcce62d3841b4053610265c4e7f6d9ce03ffb52"},"cell_type":"markdown","source":"# Training and Evaluating\nThe evaluation is done on the *train.csv* file, but the read_dataset function has logic to differentiate the records that will be read according to the TRAIN or EVAL mode. "},{"metadata":{"trusted":true,"_uuid":"7394b8ab69057c7131c3761c08e82f923bb8c5ee","scrolled":true},"cell_type":"code","source":"BATCH_SIZE = 512\nOUTDIR = './trained_model'\ntrain_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.TRAIN, batch_size = BATCH_SIZE)\neval_input_fn = read_dataset(f'{PATH}/train.csv', tf.estimator.ModeKeys.EVAL, batch_size = BATCH_SIZE)\nshutil.rmtree(OUTDIR, ignore_errors = True)\n#estimator = tf.estimator.LinearRegressor(model_dir = OUTDIR, feature_columns = create_feature_cols())\nestimator = tf.estimator.DNNRegressor(model_dir = OUTDIR, feature_columns = create_feature_cols(),\n                                     hidden_units=[177, 73, 32],\n                                     optimizer='Ftrl', \n                                     batch_norm=False, \n                                     dropout=0.1) \n#TODO I haven't got good results with the Wide and Deep model, but may be I have to experiment even more                                     \n#estimator = tf.estimator.DNNLinearCombinedRegressor(model_dir = OUTDIR, \n#                                                    linear_feature_columns=create_sparse_feature_cols(),\n#                                                    dnn_feature_columns=create_dense_feature_cols(),\n#                                                    dnn_hidden_units=[128, 64, 32],\n#                                                    dnn_dropout=None\n#                                                   )\nwith timer('Training...'):\n#increasing the steps you can increase the amount of processed records that is = BATCH_SIZE * max_steps\n    estimator.train(train_input_fn, max_steps=120000) \nwith timer('Evaluating'):\n    evaluation = estimator.evaluate(eval_input_fn, name='train_eval')\nprint(evaluation)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0ab7cc25d9aff50ff2b4adcbab300ebc0985d60"},"cell_type":"markdown","source":"# Predicting and saving the submission file"},{"metadata":{"trusted":true,"_uuid":"b0f0b1b2aa59ecaca8479a212fb3bb6859d0a7b1","_kg_hide-output":true},"cell_type":"code","source":"avg_loss = evaluation['average_loss']\npredict_input_fn = read_dataset(f'{PATH}/test.csv', tf.estimator.ModeKeys.PREDICT, batch_size=128)\npredictions = estimator.predict(predict_input_fn)\n\ntest_df = pd.read_csv(f'{PATH}/test.csv', nrows=10000)\n#test_df.head()\n\ns = pd.Series()\nfor i, p in enumerate(predictions):\n    s.at[i] = p['predictions'][0]\n#s.describe()\ntest_df['fare_amount'] = s\nsub = test_df[['key', 'fare_amount']]\nsub.to_csv(f'DNNregr-{avg_loss:4.4}.csv', index=False)\n#    print(\"Prediction %s: %s\" % (i + 1, p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"424ec19726fd2aff6f4da3d3d5f2621bc3220869"},"cell_type":"code","source":"s.describe()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}