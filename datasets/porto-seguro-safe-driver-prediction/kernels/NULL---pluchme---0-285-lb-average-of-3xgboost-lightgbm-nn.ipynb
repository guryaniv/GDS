{"metadata": {"language_info": {"file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3", "mimetype": "text/x-python", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat": 4, "cells": [{"source": ["**You can find final submission in the input data.** \n"], "metadata": {"_cell_guid": "748d7f4d-f1b1-4bc7-bf2c-0f4f5c443686", "_uuid": "04a142c0e29d07f336a5a5fc0fd25d3e8f2dce1b"}, "cell_type": "markdown"}, {"source": ["Hello everyone. In this kernel I want to share one of our solutions. \n", "\n", "Here will be fragments of the code with changed model parametrs because of forgotten values, but all submission files will be provided. "], "metadata": {"_cell_guid": "302bd0c4-37e5-4099-a011-a661e772b536", "_uuid": "ddbaa04d0af865ed7f52ea84569b3fa6b2cd3f7e", "_kg_hide-input": true}, "cell_type": "markdown"}, {"source": ["We took 3 different XGBoost models:\n", "- 2 models with 5Kfolds, but with different parametrs (eta, max_depth)\n", "- 1 model with 4Kfolds\n"], "metadata": {"_cell_guid": "8f4a1a49-6148-44a9-bc1d-3f44f4e0c820", "_uuid": "e163e1985dc896b2a4283de937c966c82ced8a74"}, "cell_type": "markdown"}, {"source": ["Also, we made feature binarization and scaling by my class:"], "metadata": {"_cell_guid": "807beaff-9483-4f47-a02d-6d458331771b", "_uuid": "4953a8776fd809719df45b74eff9afa0520a17db"}, "cell_type": "markdown"}, {"execution_count": null, "source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn.model_selection import KFold\n", "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n", "import xgboost as xgb\n", "class FeatureBinarizatorAndScaler:\n", "    \"\"\" This class needed for scaling and binarization features\n", "    \"\"\"\n", "    NUMERICAL_FEATURES = list()\n", "    CATEGORICAL_FEATURES = list()\n", "    BIN_FEATURES = list()\n", "    binarizers = dict()\n", "    scalers = dict()\n", "\n", "    def __init__(self, numerical=list(), categorical=list(), binfeatures = list(), binarizers=dict(), scalers=dict()):\n", "        self.NUMERICAL_FEATURES = numerical\n", "        self.CATEGORICAL_FEATURES = categorical\n", "        self.BIN_FEATURES = binfeatures\n", "        self.binarizers = binarizers\n", "        self.scalers = scalers\n", "\n", "    def fit(self, train_set):\n", "        for feature in train_set.columns:\n", "\n", "            if feature.split('_')[-1] == 'cat':\n", "                self.CATEGORICAL_FEATURES.append(feature)\n", "            elif feature.split('_')[-1] != 'bin':\n", "                self.NUMERICAL_FEATURES.append(feature)\n", "\n", "            else:\n", "                self.BIN_FEATURES.append(feature)\n", "        for feature in self.NUMERICAL_FEATURES:\n", "            scaler = StandardScaler()\n", "            self.scalers[feature] = scaler.fit(np.float64(train_set[feature]).reshape((len(train_set[feature]), 1)))\n", "        for feature in self.CATEGORICAL_FEATURES:\n", "            binarizer = LabelBinarizer()\n", "            self.binarizers[feature] = binarizer.fit(train_set[feature])\n", "\n", "\n", "    def transform(self, data):\n", "        binarizedAndScaledFeatures = np.empty((0, 0))\n", "        for feature in self.NUMERICAL_FEATURES:\n", "            if feature == self.NUMERICAL_FEATURES[0]:\n", "                binarizedAndScaledFeatures = self.scalers[feature].transform(np.float64(data[feature]).reshape(\n", "                    (len(data[feature]), 1)))\n", "            else:\n", "                binarizedAndScaledFeatures = np.concatenate((\n", "                    binarizedAndScaledFeatures,\n", "                    self.scalers[feature].transform(np.float64(data[feature]).reshape((len(data[feature]),\n", "                                                                                       1)))), axis=1)\n", "        for feature in self.CATEGORICAL_FEATURES:\n", "\n", "            binarizedAndScaledFeatures = np.concatenate((binarizedAndScaledFeatures,\n", "                                                         self.binarizers[feature].transform(data[feature])), axis=1)\n", "\n", "        for feature in self.BIN_FEATURES:\n", "            binarizedAndScaledFeatures = np.concatenate((binarizedAndScaledFeatures, np.array(data[feature]).reshape((\n", "                len(data[feature]), 1))), axis=1)\n", "        print(binarizedAndScaledFeatures.shape)\n", "        return binarizedAndScaledFeatures"], "metadata": {"_cell_guid": "cd34eb71-d013-4e3e-a399-522ab0577fec", "_uuid": "958dfd51abb4671c66dabdc61988ce58f126d6ee", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["Gini coefficient:"], "metadata": {"_cell_guid": "4f48a756-3e0d-45f7-96cc-f4a118c28a2a", "_uuid": "7abae331092757be9df1f3fef5d7ded9dabdf0d2"}, "cell_type": "markdown"}, {"execution_count": null, "source": ["def gini(actual, pred, cmpcol=0, sortcol=1):\n", "    assert (len(actual) == len(pred))\n", "    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n", "    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n", "    totalLosses = all[:, 0].sum()\n", "    giniSum = all[:, 0].cumsum().sum() / totalLosses\n", "\n", "    giniSum -= (len(actual) + 1) / 2.\n", "    return giniSum / len(actual)\n", "\n", "\n", "def gini_normalized(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "\n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = gini_normalized(labels, preds)\n", "    return [('gini', gini_score)]\n"], "metadata": {"_cell_guid": "c3324459-2b8e-40d7-93cf-07c8d436eff4", "_uuid": "d9bbba07c9912ffa50bac99c1734821c8712b788", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["Let's create the submission file. All models will be after:"], "metadata": {"_cell_guid": "63ba53b0-f071-4dda-88f4-374da829c88d", "_uuid": "ae9b6c7a12e92001de991491b1a8a0b6c0f7a692"}, "cell_type": "markdown"}, {"execution_count": null, "source": ["first = pd.read_csv('../input/results/5folds(v. 1).csv')\n", "second  = pd.read_csv('../input/results/5folds(v. 2).csv')\n", "third =  pd.read_csv('../input/results/4folds.csv')\n", "fourth = pd.read_csv('../input/results/LightGBM.csv')\n", "fifth = pd.read_csv('../input/results/NN.csv')\n", "\n", "a = pd.DataFrame()\n", "a['id'] = first['id']\n", "a['target'] = (first['target']+second['target']+third['target']+fourth['target']+fifth['target'])/5\n", "a.to_csv('submission.csv')"], "metadata": {"_cell_guid": "1485ef66-c612-4c37-8b51-bd968347edd1", "_uuid": "09f73dc763281feb762d034b5db275e8192d750e", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["Here is our preprocessing approach:"], "metadata": {"_cell_guid": "b76927d7-cf8f-4d4f-8c58-d4872c6f7f4c", "_uuid": "f17e021b725d5f46d8ced3d5439861d5aa33526e"}, "cell_type": "markdown"}, {"execution_count": null, "source": ["def preproc(X_train):\n", "    # Adding new features and deleting features with low importance\n", "    multreg = X_train['ps_reg_01'] * X_train['ps_reg_03'] * X_train['ps_reg_02']\n", "    ps_car_reg = X_train['ps_car_13'] * X_train['ps_reg_03'] * X_train['ps_car_13']\n", "    X_train = X_train.drop(['ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n", "                            'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12',\n", "                            'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin',\n", "                            'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin', 'ps_car_10_cat', 'ps_ind_10_bin',\n", "                            'ps_ind_13_bin', 'ps_ind_12_bin'], axis=1)\n", "    X_train['mult'] = multreg\n", "    X_train['ps_car'] = ps_car_reg\n", "    X_train['ps_ind'] = X_train['ps_ind_03'] * X_train['ps_ind_15']\n", "    return X_train\n"], "metadata": {"_cell_guid": "58640db0-5f60-48a4-b5e6-5ca71196985b", "_uuid": "ab6895cfc80078fa46eaff13343b7828258b1b68", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["And main XGBoost body:\n"], "metadata": {"_cell_guid": "5c568415-cac6-4b7c-b725-3bb3c21a09f2", "_uuid": "8a5bafb824d5cf029a7c42abc4c5477a9d0f2f66"}, "cell_type": "markdown"}, {"execution_count": null, "source": ["X_train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\n", "y_train = X_train['target']\n", "X_train = X_train.drop(['id', 'target'], axis=1)\n", "X_test = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')\n", "X_test = X_test.drop(['id'], axis=1)\n", "X_train = preproc(X_train)\n", "X_test = preproc(X_test)\n", "\n", "binarizerandscaler = FeatureBinarizatorAndScaler()\n", "binarizerandscaler.fit(X_train)\n", "X_train = binarizerandscaler.transform(X_train)\n", "X_test = binarizerandscaler.transform(X_test)\n", "\n", "# Kinetic features https://www.kaggle.com/alexandrudaia/kinetic-and-transforms-0-482-up-the-board\n", "kinetic_train = []\n", "for i in range(4):\n", "    kinetic_train = pd.read_csv('../input/kinetic-features/'+str(i+1)+'k.csv').iloc[:, 1:2]\n", "    X_train = np.concatenate((X_train, np.array(kinetic_train).reshape((len(kinetic_train), 1))), axis=1)\n", "    kinetic_test = pd.read_csv('../input/kinetic-features/'+str(i+1)+'kt.csv').iloc[:, 1:2]\n", "    X_test = np.concatenate((X_test, np.array(kinetic_test).reshape((len(kinetic_test), 1))), axis=1)\n", "\n", "i = 0\n", "K = 5\n", "kf = KFold(n_splits=K, random_state=42, shuffle=True)\n", "# 5 Cross Validation\n", "results = []\n", "for train_index, test_index in kf.split(X_train):\n", "    train_X, valid_X = X_train[train_index], X_train[test_index]\n", "    train_y, valid_y = y_train[train_index], y_train[test_index]\n", "    weights = np.zeros(len(y_train))\n", "    weights[y_train == 0] = 1\n", "    weights[y_train == 1] = 1\n", "    print(weights, np.mean(weights))\n", "    watchlist = [(xgb.DMatrix(train_X, train_y, weight=weights), 'train'), (xgb.DMatrix(valid_X, valid_y), 'valid')]\n", "    # Setting parameters for XGBoost model\n", "    params = {'eta': 0.03, 'max_depth': 4, 'objective': 'binary:logistic', 'seed': 42, 'silent': True}\n", "    model = xgb.train(params, xgb.DMatrix(train_X, train_y, weight=weights), 1500, watchlist,  maximize=True, verbose_eval=5,\n", "                        feval=gini_xgb, early_stopping_rounds=100)\n", "    resy = pd.DataFrame(model.predict(xgb.DMatrix(X_test)))\n", "    i += 1\n", "    # Saving results for all CV models\n", "    results.append(resy)\n", "    # resy.to_csv(str(i)+'fold.csv')\n", "\n", "# Creating the submission file\n", "submission = pd.DataFrame((results[0]+results[1]+results[2]+results[3]+results[4])/5)\n", "#submission.to_csv('sumbission.csv')"], "metadata": {"_cell_guid": "0ce2555a-9263-4571-b7ad-ce4702debc0c", "_uuid": "cdb0c393b7ef2169e395d5b2d7fead27bfe6629e", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": [], "metadata": {"_cell_guid": "3053e599-8577-4476-be46-d4c18fd3b0cf", "_uuid": "7dfc3c9756c826d25d4cf4990f7e588dbfc9b757"}, "cell_type": "markdown"}, {"source": ["I've took this LightGBM https://www.kaggle.com/the1owl/forza-baseline-lightgbm-example but I also change weights. Many thanks to the1owl."], "metadata": {"_cell_guid": "a64a9076-62da-4eeb-8703-521c037eae6b", "_uuid": "1491416aae4349c9cafc67cf422ffd9b8e79ad6d", "collapsed": true}, "cell_type": "markdown"}, {"source": ["We took about 20 NN models and calculated average.\n", "Here is one of the examples:"], "metadata": {"_cell_guid": "ff35f58f-d99c-4b42-9713-df5088cac5c5", "_uuid": "eea0469dee2fd9d23223cc06da7fc264b566c742"}, "cell_type": "markdown"}, {"execution_count": null, "source": ["from keras.models import Sequential\n", "from keras.layers import Dense, Dropout\n", "from keras.optimizers import SGD\n", "from keras.initializers import random_uniform\n", "\n", "import pandas as pd\n", "\n", "X_train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\n", "y_train = X_train['target']\n", "X_test = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')\n", "X_test = X_test.drop(['id'], axis=1)\n", "X_train = X_train.drop(['id', 'target'], axis = 1)\n", "y_train1 = abs(-1+y_train)\n", "y_train = pd.concat([y_train, y_train1], axis=1)\n", "binarizerandscaler = FeatureBinarizatorAndScaler()\n", "binarizerandscaler.fit(X_train)\n", "X_train = binarizerandscaler.transform(X_train)\n", "X_test = binarizerandscaler.transform(X_test)\n", "y_train = y_train.as_matrix()\n", "\n", "\n", "#hyperparameters\n", "input_dimension = 226\n", "learning_rate = 0.0025\n", "momentum = 0.85\n", "hidden_initializer = random_uniform(seed=SEED)\n", "dropout_rate = 0.2\n", "\n", "\n", "# create model\n", "model = Sequential()\n", "model.add(Dense(128, input_dim=input_dimension, kernel_initializer=hidden_initializer, activation='relu'))\n", "model.add(Dropout(dropout_rate))\n", "model.add(Dense(64, kernel_initializer=hidden_initializer, activation='relu'))\n", "model.add(Dense(2, kernel_initializer=hidden_initializer, activation='softmax'))\n", "\n", "sgd = SGD(lr=learning_rate, momentum=momentum)\n", "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['acc'])\n", "model.fit(X_train, y_train, epochs=130, batch_size=128)\n", "predictions = model.predict_proba(X_test)\n", "\n", "ans = pd.DataFrame(predictions)\n", "ans = ans[0]\n", "#ans.to_csv('./ans.csv', index=False)\n", "\n", "\n", "    #save the model\n", "model_json = model.to_json()\n", "#with open(\"./ans.json\", \"w\") as json_file:\n", "#    json_file.write(model_json)\n", "\n", " #   model.save_weights(\"./ans.h5\")\n"], "metadata": {"_cell_guid": "c2120a75-203d-4b52-ba83-f6d4cb367acb", "_uuid": "f46fe561148ae557387be23423e08c4a7b0c622b", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": [], "metadata": {"_cell_guid": "bd512109-1105-4520-84b4-3c498ef0d9b8", "_uuid": "6c96a86f7f155628024329ed7c0956f92d42b798"}, "cell_type": "markdown"}], "nbformat_minor": 1}