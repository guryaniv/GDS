{"metadata": {"language_info": {"version": "3.6.3", "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "anaconda-cloud": {}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "code", "source": ["import tensorflow as tf\n", "import pandas as pd \n", "import numpy as np\n", "from string import punctuation\n", "from collections import Counter\n", "from tqdm import tqdm\n", "%matplotlib inline"], "outputs": [], "metadata": {"_cell_guid": "ca5062ab-58a3-483e-85b9-c78fb4541678", "_uuid": "7c00afcb40878f45ca6453a456805850cc569077"}, "execution_count": 1}, {"cell_type": "markdown", "source": ["## Step 1. Dataset preparation\n", "\n", "\n", "#### Step 1.1 Loading data"], "metadata": {"_cell_guid": "e15d3267-dcdd-40e7-9016-5e66923f98aa", "_uuid": "b9d998bd6d0aab05ee7d538982c276db977a7054"}}, {"cell_type": "code", "source": ["sentiment_data = pd.read_csv('../input/train.csv')\n"], "outputs": [], "metadata": {"_cell_guid": "e3abd6f8-f237-47dc-b54b-ef04d457d8ff", "_uuid": "58bbf8f841851fdf01d0226d19b4f4ce9251e0e5", "collapsed": true}, "execution_count": 2}, {"cell_type": "code", "source": ["sentiment_data.head()"], "outputs": [], "metadata": {"_cell_guid": "5b79409a-7e8b-4582-bed0-40361f246f9e", "_uuid": "2c0c327118cb3809b56e77772588ae11295880c3"}, "execution_count": 3}, {"cell_type": "markdown", "source": ["#### Step 1.2 Shuffling data"], "metadata": {"_cell_guid": "ab621cec-0959-4fe3-8775-f640462ba868", "_uuid": "7847b984ba922aedc4bc9d367492699629ba624e"}}, {"cell_type": "code", "source": ["from sklearn.utils import shuffle\n", "sentiment_data = shuffle(sentiment_data)"], "outputs": [], "metadata": {"_cell_guid": "1323b905-9029-4733-a54f-7d6f43b84659", "_uuid": "cddf7a1386265e58044ce3f50578a7148745cb10", "collapsed": true}, "execution_count": 4}, {"cell_type": "markdown", "source": ["#### Step 1.3 Creating the Vocab and the vocab2int"], "metadata": {"_cell_guid": "1cc4980c-eaa0-432d-b71f-65cfdd568f41", "_uuid": "06503c17fc9613024ce0c49b765be9c645dba1d6"}}, {"cell_type": "code", "source": ["labels = sentiment_data.iloc[:, 0].values\n", "reviews = sentiment_data.iloc[:, 1].values"], "outputs": [], "metadata": {"_cell_guid": "02eb2fd3-38de-4539-b221-fce114861d1a", "_uuid": "75d202724514b34253a758f5a697dd4d6303ecd0", "collapsed": true}, "execution_count": 5}, {"cell_type": "code", "source": ["reviews_processed = []\n", "unlabeled_processed = [] \n", "for review in reviews:\n", "    review_cool_one = ''.join([char for char in review if char not in punctuation])\n", "    reviews_processed.append(review_cool_one)"], "outputs": [], "metadata": {"_cell_guid": "0fefd71d-19ab-48c6-aa8d-2582e116e167", "_uuid": "1da5a2cf03119c344ab7c794d5059a558305a5d9", "collapsed": true}, "execution_count": 6}, {"cell_type": "code", "source": ["word_reviews = []\n", "all_words = []\n", "for review in reviews_processed:\n", "    word_reviews.append(review.lower().split())\n", "    for word in review.split():\n", "        all_words.append(word.lower())\n", "    \n", "counter = Counter(all_words)\n", "vocab = sorted(counter, key=counter.get, reverse=True)\n", "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}"], "outputs": [], "metadata": {"_cell_guid": "698bd5e8-8450-4e8e-977a-d46a90208384", "_uuid": "13363a209b55c41a8be3e94f3c51c6534dfac569", "collapsed": true}, "execution_count": 7}, {"cell_type": "markdown", "source": ["#### Step 1.4 Encoding words to ints"], "metadata": {"_cell_guid": "4caaf351-6499-4b93-a97b-05bf5d534250", "_uuid": "d5dddbaf623c23ba5b634244e1929ea68be2f105"}}, {"cell_type": "code", "source": ["reviews_to_ints = []\n", "for review in word_reviews:\n", "    reviews_to_ints.append([vocab_to_int[word] for word in review])"], "outputs": [], "metadata": {"_cell_guid": "202339ee-07bb-49da-8fde-7c703f93931e", "_uuid": "899d1178d64afaf9cd61256316768c96e74b6483", "collapsed": true}, "execution_count": 8}, {"cell_type": "markdown", "source": ["#### Step 1.5 Checking if there was any review with length == 0"], "metadata": {"_cell_guid": "ea09f4d4-2791-4b60-aa87-1e7cd83690ad", "_uuid": "b1959cde73e5ee7f23a0da5829b7b49a7335b4f8"}}, {"cell_type": "code", "source": ["reviews_lens = Counter([len(x) for x in reviews_to_ints])\n", "print('Zero-length {}'.format(reviews_lens[0]))\n", "print(\"Max review length {}\".format(max(reviews_lens)))"], "outputs": [], "metadata": {"_cell_guid": "ea05a229-801d-4dc2-b570-b34a16bf9200", "_uuid": "1ef44eb0c41433e9456ab9927124c2051453d2e7"}, "execution_count": 9}, {"cell_type": "markdown", "source": ["#### Step 1.6 Padding the data to the same sequence length"], "metadata": {"_cell_guid": "ea667d07-ab1b-451f-a70a-bf396d85dfe9", "_uuid": "2ad88fe184c6a33b02844e4c55692a6203bcd351"}}, {"cell_type": "code", "source": ["seq_len = 250\n", "\n", "features = np.zeros((len(reviews_to_ints), seq_len), dtype=int)\n", "for i, review in enumerate(reviews_to_ints):\n", "    features[i, -len(review):] = np.array(review)[:seq_len]"], "outputs": [], "metadata": {"_cell_guid": "a9f2b8b3-aefc-4d90-a685-cd7d71441248", "_uuid": "5e4935bcdf905e668b918b69087b8043b8db7c10", "collapsed": true}, "execution_count": 10}, {"cell_type": "markdown", "source": ["#### Step 1.7 Creating training and testing sets"], "metadata": {"_cell_guid": "0e5df796-44c1-4b6d-8e55-ef62ec4e43dd", "_uuid": "500de716e9329962da7079ac4cc3873fcc1e3275"}}, {"cell_type": "code", "source": ["X_train = features[:6400]\n", "y_train = labels[:6400]\n", "\n", "X_test = features[6400:]\n", "y_test = labels[6400:]\n", "\n", "print('X_trian shape {}'.format(X_train.shape))"], "outputs": [], "metadata": {"_cell_guid": "a5c56a32-19aa-44fb-a6fb-fdf11454e33e", "_uuid": "d6bba777a2b2dd7f650f4ca2061b078be6d24d81"}, "execution_count": 11}, {"cell_type": "markdown", "source": ["## Step 2 Define a model\n", "\n", "\n", "#### Step 2.1 Define functions for creating weights and biases"], "metadata": {"_cell_guid": "bc3f5dac-3d8c-4694-be50-2bbc63b259b8", "_uuid": "97ffbd7cdf7b6c0bea08da2843ac9f4bec943226"}}, {"cell_type": "code", "source": ["def weights_init(shape):\n", "    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.05))"], "outputs": [], "metadata": {"_cell_guid": "1f2a72f8-3482-47d7-aca7-86513049c341", "_uuid": "ed98934ddd2ca97b4e547fad30f56cbf95044645", "collapsed": true}, "execution_count": 12}, {"cell_type": "code", "source": ["def bias_init(shape):\n", "    return tf.Variable(tf.zeros(shape=shape))"], "outputs": [], "metadata": {"_cell_guid": "ee2ed01a-07ae-4d2e-9d43-35b7ffbc03fb", "_uuid": "f80a047d0b91c82d80611b0ca0a7b65f0008ccbb", "collapsed": true}, "execution_count": 13}, {"cell_type": "markdown", "source": ["#### Step 2.2 Define helper functions for the model"], "metadata": {"_cell_guid": "01a3a5b4-e20e-433f-aaeb-506bebf76b06", "_uuid": "d78bcf5e1d81e0d856976f263cc71d745867003e"}}, {"cell_type": "code", "source": ["def define_inputs(batch_size, sequence_len):\n", "    '''\n", "    This function is used to define all placeholders used in the network.\n", "    \n", "    Input(s): batch_size - number of samples that we are feeding to the network per step\n", "              sequence_len - number of timesteps in the RNN loop\n", "              \n", "    Output(s): inputs - the placeholder for reviews\n", "               targets - the placeholder for classes (sentiments)\n", "               keep_probs - the placeholder used to enter value for dropout in the model    \n", "    '''\n", "    inputs = tf.placeholder(tf.int32, [batch_size, sequence_len], name='inputs_reviews')\n", "    targets = tf.placeholder(tf.float32, [batch_size, 1], name='target_sentiment')\n", "    keep_probs = tf.placeholder(tf.float32, name='keep_probs')\n", "    \n", "    return inputs, targets, keep_probs"], "outputs": [], "metadata": {"_cell_guid": "d1149031-eba7-492f-a72a-1a8f40df2d42", "_uuid": "1d4831c851e2dd0041a8a5cd5b7d1b9af9022e96", "collapsed": true}, "execution_count": 14}, {"cell_type": "code", "source": ["def embeding_layer(vocab_size, embeding_size, inputs):\n", "    '''\n", "    Function used for creating word embedings (word vectors)\n", "    \n", "    Input(s): vocab_size - number of words in the vocab\n", "              embeding_size - length of a vector used to represent a single word from vocab\n", "              inputs - inputs placeholder\n", "    \n", "    Output(s): embed_expended -  word embedings expended to be 4D tensor so we can perform Convolution operation on it\n", "    '''\n", "    word_embedings = tf.Variable(tf.random_uniform([vocab_size, embeding_size]))\n", "    embed = tf.nn.embedding_lookup(word_embedings, inputs)\n", "    embed_expended = tf.expand_dims(embed, -1) #expend dims to 4d for conv layer\n", "    return embed_expended"], "outputs": [], "metadata": {"_cell_guid": "4b728176-bfd3-4045-bd76-6a8858b5fce1", "_uuid": "3d0a80bf8dc03fac30d5022b7a038426004b1531", "collapsed": true}, "execution_count": 15}, {"cell_type": "code", "source": ["def text_conv(input, filter_size, number_of_channels, number_of_filters, strides=(1, 1), activation=tf.nn.relu, max_pool=True):\n", "    '''\n", "    This is classical CNN layer used to convolve over embedings tensor and gether useful information from it.\n", "    \n", "    Input(s): input - word_embedings\n", "              filter_size - size of width and height of the Conv kernel\n", "              number_of_channels - in this case it is always 1\n", "              number_of_filters - how many representation of the input review are we going to output from this layer \n", "              strides - how many pixels does kernel move to the side and up/down\n", "              activation - a activation function\n", "              max_pool - boolean value which will trigger a max_pool operation on the output tensor\n", "    \n", "    Output(s): text_conv layer\n", "    \n", "    '''\n", "    weights = weights_init([filter_size, filter_size, number_of_channels, number_of_filters])\n", "    bias = bias_init([number_of_filters])\n", "    \n", "    layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding='SAME')\n", "    \n", "    if activation != None:\n", "        layer = activation(layer)\n", "    \n", "    if max_pool:\n", "        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2 ,1], strides=[1, 2, 2, 1], padding='SAME')\n", "    \n", "    return layer"], "outputs": [], "metadata": {"_cell_guid": "a484f01d-b043-42bd-9d7c-dda2b979e5e0", "_uuid": "4d9ee289e9d00056b3b3cbd24abe60aa503fe951", "collapsed": true}, "execution_count": 16}, {"cell_type": "code", "source": ["def lstm_layer(lstm_size, number_of_layers, batch_size, dropout_rate):\n", "    '''\n", "    This method is used to create LSTM layer/s for PixelRNN\n", "    \n", "    Input(s): lstm_cell_unitis - used to define the number of units in a LSTM layer\n", "              number_of_layers - used to define how many of LSTM layers do we want in the network\n", "              batch_size - in this method this information is used to build starting state for the network\n", "              dropout_rate - used to define how many cells in a layer do we want to 'turn off'\n", "              \n", "    Output(s): cell - lstm layer\n", "               init_state - zero vectors used as a starting state for the network\n", "    '''\n", "    def cell(size, dropout_rate=None):\n", "        layer = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n", "        \n", "        return tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n", "            \n", "    cell = tf.contrib.rnn.MultiRNNCell([cell(lstm_size, dropout_rate) for _ in range(number_of_layers)])\n", "    \n", "    init_state = cell.zero_state(batch_size, tf.float32)\n", "    return cell, init_state"], "outputs": [], "metadata": {"_cell_guid": "9bce5266-b5e3-4d20-82cd-9f002853583f", "_uuid": "d9da952a296b67623c2e27710217f0c211bb19c1", "collapsed": true}, "execution_count": 17}, {"cell_type": "code", "source": ["def flatten(layer, batch_size, seq_len):\n", "    '''\n", "    Used to transform/reshape 4d conv output to 2d matrix\n", "    \n", "    Input(s): Layer - text_cnn layer\n", "              batch_size - how many samples do we feed at once\n", "              seq_len - number of time steps\n", "              \n", "    Output(s): reshaped_layer - the layer with new shape\n", "               number_of_elements - this param is used as a in_size for next layer\n", "    '''\n", "    dims = layer.get_shape()\n", "    number_of_elements = dims[2:].num_elements()\n", "    \n", "    reshaped_layer = tf.reshape(layer, [batch_size, int(seq_len/2), number_of_elements])\n", "    return reshaped_layer, number_of_elements"], "outputs": [], "metadata": {"_cell_guid": "c6a3bc7f-754f-4ef9-a748-9a42f58d6e09", "_uuid": "653e0d182fbc8ebe1686db953d506088bbd3b795", "collapsed": true}, "execution_count": 18}, {"cell_type": "code", "source": ["def dense_layer(input, in_size, out_size, dropout=False, activation=tf.nn.relu):\n", "    '''\n", "    Output layer for the lstm netowrk\n", "    \n", "    Input(s): lstm_outputs - outputs from the RNN part of the network\n", "              input_size - in this case it is RNN size (number of neuros in RNN layer)\n", "              output_size - number of neuros for the output layer == number of classes\n", "              \n", "    Output(s) - logits, \n", "    '''\n", "    weights = weights_init([in_size, out_size])\n", "    bias = bias_init([out_size])\n", "    \n", "    layer = tf.matmul(input, weights) + bias\n", "    \n", "    if activation != None:\n", "        layer = activation(layer)\n", "    \n", "    if dropout:\n", "        layer = tf.nn.dropout(layer, 0.5)\n", "        \n", "    return layer"], "outputs": [], "metadata": {"_cell_guid": "54bc9c0d-04ba-479e-b97e-d4c7609fa4be", "_uuid": "39fcf3ef8923aa6bcfcd656a0f4609a228aa6ce1", "collapsed": true}, "execution_count": 19}, {"cell_type": "code", "source": ["def loss_optimizer(logits, targets, learning_rate, ):\n", "    '''\n", "    Function used to calculate loss and minimize it\n", "    \n", "    Input(s): rnn_out - logits from the fully_connected layer\n", "              targets - targets used to train network\n", "              learning_rate/step_size\n", "    \n", "    \n", "    Output(s): optimizer - optimizer of choice\n", "               loss - calculated loss function\n", "    '''\n", "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets))\n", "    \n", "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n", "    return loss, optimizer"], "outputs": [], "metadata": {"_cell_guid": "6fc1ffce-1ed8-41bb-890d-be2056be4ac9", "_uuid": "f3fad6dd3d45171a0865ef502e9c74180f399fd2", "collapsed": true}, "execution_count": 20}, {"cell_type": "code", "source": ["class SentimentCNN(object):\n", "    \n", "    def __init__(self, learning_rate=0.001, batch_size=100, seq_len=250, vocab_size=10000, embed_size=300,\n", "                conv_filters=32, conv_filter_size=5, number_of_lstm_layers=1, lstm_units=128):\n", "        \n", "        \n", "        '''\n", "        To created Sentiment embed network CNN-LSTM create object of this class.\n", "        \n", "        Input(s): learning_rate/step_size - how fast are we going to find global minima\n", "                  batch_size -  the nuber of samples to feed at once\n", "                  seq_len - the number of timesteps in unrolled RNN\n", "                  vocab_size - the number of nunique words in the vocab\n", "                  embed_size - length of word embed vectors\n", "                  conv_filters - number of filters in output tensor from CNN layer\n", "                  conv_filter_size - height and width of conv kernel\n", "                  number_of_lstm_layers - the number of layers used in the LSTM part of the network\n", "                  lstm_units - the number of neurons/cells in a LSTM layer\n", "        \n", "        '''\n", "        tf.reset_default_graph()\n", "        self.inputs, self.targets, self.keep_probs = define_inputs(batch_size, seq_len)\n", "        \n", "        embed = embeding_layer(vocab_size, embed_size, self.inputs)\n", "        \n", "        #Building the network\n", "        convolutional_part = text_conv(embed, conv_filter_size, 1, conv_filters)\n", "        conv_flatten, num_elements = flatten(convolutional_part, batch_size, seq_len)\n", "        \n", "        cell, init_state = lstm_layer(lstm_units, number_of_lstm_layers, batch_size, self.keep_probs)\n", "        \n", "        outputs, states = tf.nn.dynamic_rnn(cell, conv_flatten, initial_state=init_state)\n", "        \n", "        review_outputs = outputs[:, -1, :]\n", "        \n", "        logits = dense_layer(review_outputs, lstm_units, 1, activation=None)\n", "        \n", "        self.loss, self.opt = loss_optimizer(logits, self.targets, learning_rate)\n", "        \n", "        preds = tf.nn.sigmoid(logits)\n", "        currect_pred = tf.equal(tf.cast(tf.round(preds), tf.int32), tf.cast(self.targets, tf.int32))\n", "        self.accuracy = tf.reduce_mean(tf.cast(currect_pred, tf.float32))"], "outputs": [], "metadata": {"_cell_guid": "f1ce760e-9da7-4efd-8b34-5d44cf4fb249", "_uuid": "74ede758801cada2b6b54230e1688a5adca4f9ac", "collapsed": true}, "execution_count": 21}, {"cell_type": "markdown", "source": ["## Step 3 Training and testing"], "metadata": {"_cell_guid": "ce9a258d-f0b3-407d-9604-5cfa80bb0458", "_uuid": "36baae535afea8cb57dd42d2cb594fe5a6191abf"}}, {"cell_type": "code", "source": ["model = SentimentCNN(learning_rate=0.001, \n", "                     batch_size=50, \n", "                     seq_len=250, \n", "                     vocab_size=len(vocab_to_int) + 1, \n", "                     embed_size=300,\n", "                     conv_filters=32, \n", "                     conv_filter_size=5, \n", "                     number_of_lstm_layers=1, \n", "                     lstm_units=128)"], "outputs": [], "metadata": {"_cell_guid": "449450ec-459f-4f2c-b3d2-ea70480f3a7f", "_uuid": "1c71ce777be8f32f9f0e2dc113ed6e90e6bb3910", "collapsed": true}, "execution_count": 22}, {"cell_type": "code", "source": ["session = tf.Session()"], "outputs": [], "metadata": {"_cell_guid": "bdce7439-b8ad-4652-8f8e-20e617777fe7", "_uuid": "53f672b4bdd7f3e43da11f09b8c8d5a70533fd58", "collapsed": true}, "execution_count": 23}, {"cell_type": "code", "source": ["session.run(tf.global_variables_initializer())"], "outputs": [], "metadata": {"_cell_guid": "381e8d66-eae6-4a6a-bc01-9c06a686d086", "_uuid": "a6f41197c6d8ae09cc1f6b56bf5f4cfc2e5d8c90", "collapsed": true}, "execution_count": 24}, {"cell_type": "code", "source": ["epochs = 5\n", "batch_size = 50\n", "drop_rate = 0.7"], "outputs": [], "metadata": {"_cell_guid": "14a7c9d5-1962-4d9b-9541-1f0aa7bcb3b8", "_uuid": "f218965a7934400a38f441f07c1b8277047ea4e8", "collapsed": true}, "execution_count": null}, {"cell_type": "markdown", "source": ["#### Step 3.1 Training process"], "metadata": {"_cell_guid": "7df4d495-f6b9-4d7d-899f-71e894326dee", "_uuid": "6ac025e0c4456cc574003f1dcaa859187346c258"}}, {"cell_type": "code", "source": ["for i in range(epochs):\n", "    epoch_loss = []\n", "    train_accuracy = []\n", "    for ii in tqdm(range(0, len(X_train), batch_size)):\n", "        X_batch = X_train[ii:ii+batch_size]\n", "        y_batch = y_train[ii:ii+batch_size].reshape(-1, 1)\n", "        \n", "        c, _, a = session.run([model.loss, model.opt, model.accuracy], feed_dict={model.inputs:X_batch, \n", "                                                                                  model.targets:y_batch,\n", "                                                                                  model.keep_probs:drop_rate})\n", "        \n", "        epoch_loss.append(c)\n", "        train_accuracy.append(a)\n", "        \n", "    \n", "    print(\"Epoch: {}/{}\".format(i, epochs), \" | Epoch loss: {}\".format(np.mean(epoch_loss)), \n", "          \" | Mean train accuracy: {}\".format(np.mean(train_accuracy)))"], "outputs": [], "metadata": {"_cell_guid": "73a5229b-c1d5-4bf7-96b1-57ef25840c6c", "_uuid": "a367f9d87d7ef592c84295f29371b20ba5dd37ee"}, "execution_count": null}, {"cell_type": "markdown", "source": ["#### Step 3.2 Testing process"], "metadata": {"_cell_guid": "0f338f38-d9e7-4a9b-8f7a-fff832fc0c7f", "_uuid": "9a6fe82c27efb5392b83b14302732b4f1b091903"}}, {"cell_type": "code", "source": ["test_accuracy = []\n", "\n", "ii = 0\n", "while ii + batch_size <= len(X_test):\n", "    X_batch = X_test[ii:ii+batch_size]\n", "    y_batch = y_test[ii:ii+batch_size].reshape(-1, 1)\n", "\n", "    a = session.run([model.accuracy], feed_dict={model.inputs:X_batch, \n", "                                                 model.targets:y_batch, \n", "                                                 model.keep_probs:1.0})\n", "    \n", "    test_accuracy.append(a)\n", "    ii += batch_size"], "outputs": [], "metadata": {"_cell_guid": "a8c79fe4-875e-4686-9c7e-037ad4edd393", "_uuid": "3829858836c94ee424461fa11fd29f7927509005", "collapsed": true}, "execution_count": null}, {"cell_type": "code", "source": ["print(\"Test accuracy: {}\".format(np.mean(test_accuracy)))"], "outputs": [], "metadata": {"_cell_guid": "3515473b-6e6a-454d-9ccc-241fa1b6f553", "_uuid": "f8a985f71a1c851feccdf355febedcb3f454f486", "collapsed": true}, "execution_count": null}, {"cell_type": "code", "source": ["session.close()"], "outputs": [], "metadata": {"_cell_guid": "30552f06-4463-4592-b8cc-ceac18f33490", "_uuid": "021e48993338ac23d5547874b38b2a4ad88b782e", "collapsed": true}, "execution_count": null}], "nbformat_minor": 1, "nbformat": 4}