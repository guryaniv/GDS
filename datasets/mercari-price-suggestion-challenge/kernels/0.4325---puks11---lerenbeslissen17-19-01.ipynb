{"nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.preprocessing import LabelBinarizer\n", "from sklearn.model_selection import train_test_split\n", "from scipy.sparse import hstack\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from scipy.sparse import csr_matrix\n", "import math\n", "import gc\n", "import time\n", "#------------------------------------------------------------------------------------------------#\n", "\n", "start_time = time.time()\n", "\n", "# Read in the datafiles\n", "df_train = pd.read_csv('../input/train.tsv', sep='\\t') \n", "df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n", "\n", "#Mercari only lets users sell products with a price higher than 3.0, so we only want the \n", "#data to contain products with a price of 3.0 or higher.\n", "df_train[df_train.price >= 3.0]\n", "\n", "# Concatenate the training and test data but save the length of the training data for later use\n", "train_rows = len(df_train)\n", "all_data = pd.concat([df_train, df_test])\n", "print(\"[{}] Data concatenated.\".format(time.time() - start_time))\n", "\n", "# Function for filling the NaN values in the dataframe\n", "def transform_values(data):\n", "    data.fillna(value='missing', inplace = True)\n", "    return data\n", "\n", "all_data = transform_values(all_data)\n", "\n", "# Split the categories into a maincategory, subcategory 1 and subcategory 2\n", "def transform_category_name(category_name):\n", "    # If there is a category split it\n", "    try:\n", "        main, sub1, sub2= category_name.split('/')\n", "        return main, sub1, sub2\n", "    # Else return \"missing\" for each (sub-)category\n", "    except:\n", "        return 'missing', 'missing', 'missing'\n", "\n", "all_data['category_main'], all_data['category_sub1'], all_data['category_sub2'] = zip(*all_data['category_name'].apply(transform_category_name))\n", "# Delete all the entries without an item description\n", "all_data[all_data.item_description != 'missing']\n", "print(\"[{}] Data transformed.\".format(time.time() - start_time))\n", "\n", "# Create a vector for all the names with count vectorizer\n", "name_cv = CountVectorizer(ngram_range=(1,2), min_df=7, stop_words='english')\n", "X_name = name_cv.fit_transform(all_data['name'])\n", "print(\"[{}] Name vector created.\".format(time.time() - start_time))\n", "\n", "# Create seperate vectors for all the (sub-)categories using a CountVectorizer\n", "cat_cv = CountVectorizer()\n", "X_catmain = cat_cv.fit_transform(all_data['category_main'],)\n", "X_catsub1 = cat_cv.fit_transform(all_data['category_sub1'])\n", "X_catsub2 = cat_cv.fit_transform(all_data['category_sub2'])\n", "print(\"[{}] Category vectors created.\".format(time.time() - start_time))\n", "\n", "# Function to get length of item descriptions\n", "def descrLength(description):\n", "    try:\n", "        if description == \"missing\":\n", "            return 0\n", "        else:\n", "            words = [w for w in description.split(\" \")]\n", "            return len(words)\n", "    except:\n", "        return 0\n", "      \n", "# Create an extra column in the dataframe that keeps track of the length of the descriptions\n", "all_data['descr_length'] = all_data['item_description'].apply(lambda x: descrLength(x))\n", "all_data['descr_length'] = all_data['descr_length'].astype(str)\n", "X_length = cat_cv.fit_transform(all_data['descr_length'])\n", "print(\"[{}] Description length vector created.\".format(time.time() - start_time))\n", "\n", "# Create a vector with tf-idf values of the words in the descriptions.\n", "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=90000, strip_accents=\"ascii\",\n", "                             stop_words='english')\n", "X_description = vectorizer.fit_transform(all_data['item_description'])\n", "print(\"[{}] Description vector created.\".format(time.time() - start_time))\n", "       \n", "# Make a vector of the brands using a label binarizer.\n", "lb = LabelBinarizer(sparse_output=True)\n", "X_brand = lb.fit_transform(all_data['brand_name'])\n", "print(\"[{}] Brand vector created.\".format(time.time() - start_time))\n", "\n", "# Use get dummies to create a sparse csr matrix of the shipping and item condition values.\n", "X_dummies = csr_matrix(pd.get_dummies(all_data[['item_condition_id', 'shipping']],\n", "                                          sparse=True).values)\n", "print(\"[{}] Shipping and condition vector created.\".format(time.time() - start_time))\n", "\n", "train_X = hstack((X_name[:train_rows], X_catmain[:train_rows], X_catsub1[:train_rows], \n", "                X_catsub2[:train_rows], X_brand[:train_rows], X_length[:train_rows], X_description[:train_rows],\n", "                X_dummies[:train_rows]))\n", "print('[{}] Training data stacked'.format(time.time() - start_time))\n", "\n", "test_X = hstack((X_name[train_rows:], X_catmain[train_rows:], X_catsub1[train_rows:], \n", "                X_catsub2[train_rows:], X_brand[train_rows:], X_length[train_rows:], X_description[train_rows:],\n", "                X_dummies[train_rows:]))\n", "print('[{}] Test data stacked.'.format(time.time() - start_time))\n", "\n", "Y_train = np.log1p(df_train['price']) #log1p because is nice\n", "print(\"[{}] Data processing and feature extraction complete.\".format(time.time() - start_time))"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "e191eed6-6285-4c36-a41c-b76cbd0b427a", "_uuid": "c6a5f2de3a77de4639000155a6f1e5fcc3d50e0f", "collapsed": true, "scrolled": false}, "cell_type": "code"}, {"source": ["from sklearn.linear_model import Ridge\n", "from sklearn.model_selection import train_test_split\n", "\n", "ridge_time = time.time()\n", "\n", "# Split the training data in training and validation sets to use cross validation\n", "data_train_X, data_val_X, data_train_Y, data_val_Y = train_test_split(train_X, Y_train)\n", "print('[{}] Data splitted.'.format(time.time() - ridge_time))\n", "\n", "# Training the Ridge algorithm on the trainingsset\n", "clf1 = Ridge(alpha=5.3, fit_intercept=True, normalize=False, \n", "      copy_X=True, max_iter=None, tol=0.01, solver='auto', random_state=None)\n", "clf1.fit(train_X, Y_train)\n", "print(\"[{}] Data 1 fitted.\".format(time.time() - ridge_time))\n", "\n", "clf2 = Ridge(alpha=.75, fit_intercept=True, normalize=False, \n", "      copy_X=True, max_iter=None, tol=0.01, solver='auto', random_state=100)\n", "clf2.fit(train_X, Y_train)\n", "print(\"[{}] Data 2 fitted.\".format(time.time() - ridge_time))\n", "\n", "# Use the Ridge model to predict the Y values for the validation set\n", "#Y_pred1 = clf1.predict(data_val_X)\n", "#Y_pred2 = clf2.predict(data_val_X)\n", "\n", "#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\n", "'''def rmsle(y, y0):\n", "     assert len(y) == len(y0)\n", "     return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n", "\n", "print(\"Error on 1:\", rmsle(np.expm1(Y_pred1), np.expm1(data_val_Y)))\n", "print(\"Error on 2:\", rmsle(np.expm1(Y_pred2), np.expm1(data_val_Y)))\n", "'''"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "5e6e4492-dacd-4ec6-84b5-1df914c32ce9", "_uuid": "5a7354828d06f7669a46e5247d06f65c896a1bdb", "collapsed": true}, "cell_type": "code"}, {"source": ["5.3 = 0.4512697\n", "\n", "0.53 = 0.4590716\n", "\n", "with everything = 0.4504744\n", "\n", "without brand = 0.45662041 --> slechter\n", "\n", "without category = 0.4595244 --> slechter\n", "\n", "without shipping = 0.457 --> slechter\n", "\n", "without item condition = \n", "\n", "price higher than 3.0 = 0.4488579\n", "\n", "missing ipv no description = 0.4500646/0.4504321/0.4496406 --> geen verbetering\n", "\n", "gewoon no description = 0.4507507/0.4493476/0.4503064\n", "\n", "with description length = 0.4503645/0.44881417/0.4481222 --> beetje verbetering\n", "\n", "with normal brands = 0.4499219\n", "\n", "with extra brands from name = 0.4492130/0.450550142/0.4502868\n"], "metadata": {"_cell_guid": "046339cc-9038-4778-896e-97e3ea8b82e2", "_uuid": "a22f9b54e084980de5304648bdd6aa2cc2d83c7c"}, "cell_type": "markdown"}, {"source": ["from sklearn.model_selection import train_test_split\n", "import lightgbm as lgb\n", "\n", "lgb_time = time.time()\n", "\n", "# Split the training data in training and validation sets to use cross validation\n", "data_train_X, data_val_X, data_train_Y, data_val_Y = train_test_split(train_X, Y_train)\n", "print('[{}] Data splitted.'.format(time.time() - lgb_time))\n", "\n", "# Create the datasets to use for LGBM\n", "dataset_train = lgb.Dataset(data_train_X, label=data_train_Y)\n", "dataset_val = lgb.Dataset(data_val_X, label=data_val_Y)\n", "# The list of data that LGBM uses to evaluate its results to avoid overfitting\n", "watchlist = [dataset_train, dataset_val]\n", "\n", "# Parameters for the LGBM model.\n", "params = {\n", "        'learning_rate': 0.35,\n", "        'application': 'regression',\n", "        'max_depth': 6,\n", "        'num_leaves': 60,\n", "        'verbosity': -1,\n", "        'metric': 'RMSE',\n", "        'data_random_seed': 1,\n", "        'bagging_fraction': 0.5,\n", "        'nthread': 4\n", "    }\n", "\n", "# Train the LGBM model on the trainingdata\n", "lgb_model = lgb.train(params, train_set=dataset_train, num_boost_round=7500, valid_sets=watchlist, \n", "            early_stopping_rounds=1000, verbose_eval=500) \n", "print(\"[{}] Model completed.\".format(time.time() - lgb_time))\n", "\n", "# Predict the values of Y for the validation set with the created model\n", "#lgb_pred =lgb_model.predict(data_val_X)\n", "print(\"[{}] Prediction completed.\".format(time.time() - lgb_time))\n", "\n", "#print(rmsle(np.expm1(lgb_pred), np.expm1(data_val_Y)))\n", "\n"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "65877f2c-eed5-4a15-8412-98210e003bf5", "_uuid": "aa663c47def33e7aaea85a23e4f7ef3c7a15d4fc", "collapsed": true, "scrolled": false}, "cell_type": "code"}, {"source": ["final_time = time.time()\n", "\n", "final_ridge_pred1 = clf1.predict(test_X)\n", "print(\"[{}] First Ridge prediction completed.\".format(time.time() - final_time))\n", "final_ridge_pred2 = clf2.predict(test_X)\n", "print(\"[{}] Second Ridge prediction completed.\".format(time.time() - final_time))\n", "final_lgb_pred = lgb_model.predict(test_X)\n", "print(\"[{}] Lgb prediction completed.\".format(time.time() - final_time))\n", "\n", "final_pred = 0.25*final_ridge_pred1 + 0.25*final_ridge_pred2 + 0.5*final_lgb_pred\n", "\n", "submission = pd.DataFrame(data=df_test[['test_id']])\n", "submission['price'] = np.expm1(final_pred)\n", "submission.to_csv(\"submission_ridge_lgbm.csv\", index=False)"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "a784e465-656f-4943-b5f5-1e1eb16ca663", "_uuid": "e3c6316e6aaba7c4f697de040a274b72e9ce4135", "collapsed": true}, "cell_type": "code"}, {"source": ["'''\n", "# Try and find brand names in product names\n", "all_brands = set(all_data['brand_name'])\n", "def brands_in_name(entry):\n", "    name = entry[0]\n", "    brand = entry[1]\n", "    new_brand = ''\n", "    split_name = name.split(' ')\n", "    if brand == 'missing':\n", "        for words in split_name:\n", "            if words in all_brands:\n", "                new_brand += words\n", "        if new_brand == '':\n", "            return 'missing'\n", "        else:\n", "            return new_brand\n", "    return brand\n", "\n", "all_data['brand_name'] = all_data[['name', 'brand_name']].apply(brands_in_name, axis = 1) \n", "print('new brands extracted')\n", "'''"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "d4e750d1-0d46-4c43-a2c9-50003afc3124", "_uuid": "20865ff665fabb1d616ee77e34fdf19c91c3ae6d", "collapsed": true}, "cell_type": "code"}, {"source": ["* alpha 5 = 0.45512\n", "* alpha 5.1 = 0.45513\n", "* alpha 5.2 = 0.45511\n", "* alpha 5.3 = 0.45510\n", "* alpha 5.4 = 0.45512\n", "* alpha 5.5 = 0.45511\n", "* alpha 0.5 = 0.461\n", "* alpha 0.6 = 0.460\n", "* alpha 15 = 0.458\n", "* alpha 100 = 0.481"], "metadata": {"_cell_guid": "da1f02c0-1850-42f2-9989-8dc1411f0e1f", "_uuid": "9c6e28d9a2af656d12301919dd00c94bf36d9a00", "collapsed": true}, "cell_type": "markdown"}], "metadata": {"language_info": {"file_extension": ".py", "name": "python", "version": "3.6.4", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}}