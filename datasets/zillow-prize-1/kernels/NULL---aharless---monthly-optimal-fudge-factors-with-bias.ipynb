{"metadata": {"language_info": {"name": "python", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "version": "3.6.1", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "cells": [{"source": ["The important thing is that the optimal fudge factors for 2017 turn out to be similar to those for 2016, which suggests that fudge factors derived from probing the public leaderboard will work for the private leaderboard."], "metadata": {"_cell_guid": "d4564fe2-06e6-4642-b2a6-a3bd3725c21e", "_uuid": "724353a43137568165f0a953ab30f7b13e9881c3"}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "f16ac0de-de1c-4b65-a85e-a3227fbe0c47", "_uuid": "47787be314e3cc68e7071ae827aef9710fc8341a"}, "source": ["LEARNING_RATE = 0.02            # shrinkage rate for boosting roudns\n", "ROUNDS_PER_ETA = 20             # maximum number of boosting rounds times learning rate\n", "VAL_SPLIT_DATE = '2016-09-15'   # First 2016 date not comparable to 2017 training data"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "1c1a4c8e-9379-47b8-aa2e-3df8e5d22918", "_uuid": "a6d445499b3664bf165ab992f309a417471707f4", "_execution_state": "idle"}, "source": ["import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.metrics import mean_absolute_error\n", "import datetime as dt\n", "from datetime import datetime\n", "import gc\n", "import patsy\n", "import statsmodels.api as sm \n", "import statsmodels.formula.api as smf\n", "from statsmodels.regression.quantile_regression import QuantReg"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "f288b2d1-010c-4f6b-9401-f849e1433380", "_uuid": "6d5c6e151491103206a9c72ab7d4ecd1de50a1ec"}, "source": ["def calculate_aggregates(properties):\n", "    # Number of properties in the zip\n", "    zip_count = properties['regionidzip'].value_counts().to_dict()\n", "    # Number of properties in the city\n", "    city_count = properties['regionidcity'].value_counts().to_dict()\n", "    # Median year of construction by neighborhood\n", "    medyear = properties.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n", "    # Mean square feet by neighborhood\n", "    meanarea = properties.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n", "    # Neighborhood latitude and longitude\n", "    medlat = properties.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n", "    medlong = properties.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n", "    return( zip_count, city_count, medyear, meanarea, medlat, medlong )\n"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "10a33853-c574-4346-ad6a-bb7b9ca80e7e", "_uuid": "cb614dcab64ab0cec6b11a2c72c7fc7abe56786c"}, "source": ["def munge(properties):\n", "    for c in properties.columns:\n", "        properties[c]=properties[c].fillna(-1)\n", "        if properties[c].dtype == 'object':\n", "            lbl = LabelEncoder()\n", "            lbl.fit(list(properties[c].values))\n", "            properties[c] = lbl.transform(list(properties[c].values))\n"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "180e44c4-2cff-41a0-a118-181b4a71b23e", "_uuid": "9557f96e6cd253ea1ad8e7e631eef334355c8cc2"}, "source": ["def calculate_target_aggreagates(df):\n", "    # Standard deviation of target value for properties in the city/zip/neighborhood\n", "    citystd = df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n", "    zipstd = df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n", "    hoodstd = df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n", "    return( citystd, zipstd, hoodstd )"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "058d3299-005e-4458-a8cd-cd3c4d854190", "_uuid": "4b9916ad9eb1d2a0b1aab78e71191c34a78a9b77"}, "source": ["def calculate_features(df):\n", "    # Nikunj's features\n", "    # Number of properties in the zip\n", "    df['N-zip_count'] = df['regionidzip'].map(zip_count)\n", "    # Number of properties in the city\n", "    df['N-city_count'] = df['regionidcity'].map(city_count)\n", "    # Does property have a garage, pool or hot tub and AC?\n", "    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n", "                         (df['pooltypeid10']>0) & \\\n", "                         (df['airconditioningtypeid']!=5))*1 \n", "\n", "    # More features\n", "    # Mean square feet of neighborhood properties\n", "    df['mean_area'] = df['regionidneighborhood'].map(meanarea)\n", "    # Median year of construction of neighborhood properties\n", "    df['med_year'] = df['regionidneighborhood'].map(medyear)\n", "    # Neighborhood latitude and longitude\n", "    df['med_lat'] = df['regionidneighborhood'].map(medlat)\n", "    df['med_long'] = df['regionidneighborhood'].map(medlong)\n", "\n", "    df['zip_std'] = df['regionidzip'].map(zipstd)\n", "    df['city_std'] = df['regionidcity'].map(citystd)\n", "    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "e389a360-b1dc-4257-b3c3-fea69026863d", "_uuid": "d816890fe0edcab13c415144e76ecd12938ab17e"}, "source": ["dropvars = ['parcelid', 'airconditioningtypeid', 'buildingclasstypeid',\n", "            'buildingqualitytypeid', 'regionidcity']\n", "droptrain = ['logerror', 'transactiondate']"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "9a9fccf6-f446-4480-9e42-470559c3162f", "_uuid": "ecbb50d5b99e6db35232e2757f7982da3dfb3d46"}, "source": ["xgb_params = {  # best as of 2017-09-28 13:20 UTC\n", "    'eta': LEARNING_RATE,\n", "    'max_depth': 7, \n", "    'subsample': 0.6,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'lambda': 5.0,\n", "    'alpha': 0.65,\n", "    'colsample_bytree': 0.5,\n", "    'silent': 1\n", "}"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"scrolled": true, "collapsed": true, "_cell_guid": "de8e97e6-d681-465b-a68a-6178e7593e49", "_uuid": "fe6761fd44aaf7a5c063897c9ca3b90b957b7108"}, "source": ["num_boost_rounds = round( ROUNDS_PER_ETA / xgb_params['eta'] )\n", "early_stopping_rounds = round( num_boost_rounds / 20 )\n", "print('Boosting rounds: {}'.format(num_boost_rounds))\n", "print('Early stoping rounds: {}'.format(early_stopping_rounds))"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "a6cc2bdd-9d38-4f66-8903-3ce019f7109a", "_uuid": "43ea1cf09b04ca53bb99c95d428b4624572ba75a", "_execution_state": "idle"}, "source": ["properties = pd.read_csv('../input/properties_2016.csv')\n", "aggs = calculate_aggregates(properties)\n", "zip_count, city_count, medyear, meanarea, medlat, medlong = aggs\n", "munge(properties)\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\")\n", "train_df = train.merge(properties, how='left', on='parcelid')\n", "del train\n", "gc.collect()"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "af6766ec-9ad8-463f-bf2f-fd611e86114d", "_uuid": "3a8d09d2e92d506d08039a9c98d5719ee40b3013"}, "source": ["for m in range(1,12+1):\n", "    \n", "    print( \"\\n\\nFUDGE FACTOR ANALYSIS FOR 2016 MONTH \", m)\n", "    select_mon = pd.to_datetime(train_df[\"transactiondate\"]).dt.month==m\n", "\n", "    select_data = select_mon & (pd.to_datetime(train_df[\"transactiondate\"]) >= VAL_SPLIT_DATE)\n", "    target_aggs = calculate_target_aggreagates(train_df[~select_data])\n", "    citystd, zipstd, hoodstd = target_aggs\n", "\n", "    train1 = train_df.copy()\n", "    calculate_features(train1)\n", "\n", "    x_valid = train1.drop(dropvars+droptrain, axis=1)[select_mon]\n", "    y_valid = train1[\"logerror\"].values.astype(np.float32)[select_mon]\n", "    n_valid = x_valid.shape[0]\n", "\n", "    train1=train1[ train1.logerror > -0.4 ]\n", "    train1=train1[ train1.logerror < 0.419 ]\n", "\n", "    train1=train1[~select_mon]\n", "\n", "    # Use only training data comparable to what is available for 2017\n", "    select_qtr4 = pd.to_datetime(train1[\"transactiondate\"]) >= VAL_SPLIT_DATE\n", "    train1=train1[~select_qtr4]\n", "    \n", "    x_train=train1.drop(dropvars+droptrain, axis=1)\n", "    y_train = train1[\"logerror\"].values.astype(np.float32)\n", "    y_mean = np.mean(y_train)\n", "    xgb_params['base_score'] = y_mean\n", "    \n", "    n_train = x_train.shape[0]\n", "\n", "    print('Fitting model to {} points & using {} to fit fudge factor'.format(n_train, n_valid))\n", "\n", "    dtrain = xgb.DMatrix(x_train, y_train)\n", "    dvalid_x = xgb.DMatrix(x_valid)\n", "    dvalid_xy = xgb.DMatrix(x_valid, y_valid)\n", "\n", "    evals = [(dtrain,'train'),(dvalid_xy,'eval')]\n", "    model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n", "                      evals=evals, early_stopping_rounds=early_stopping_rounds,\n", "                      verbose_eval=False)\n", "\n", "    valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\n", "\n", "    fudges = QuantReg(y_valid, sm.add_constant(valid_pred)).fit(q=.5).params\n", "    fudge0 = fudges[0]\n", "    fudge1 = fudges[1]\n", "    rawerr = mean_absolute_error(y_valid, valid_pred)\n", "    fudgerr = mean_absolute_error(y_valid, fudge0 + fudge1*valid_pred)\n", "    print(\"Fudge factors reduce MAE from {0:9.6f} to {1:9.6f}\".format(rawerr, fudgerr))\n", "    print(\"Optimized fudge factors for month {0}: {1:9.6f}, {2:9.6f}\".format(m, fudge0, fudge1))\n", "    "], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "a6cc2bdd-9d38-4f66-8903-3ce019f7109a", "_uuid": "43ea1cf09b04ca53bb99c95d428b4624572ba75a", "_execution_state": "idle"}, "source": ["properties = pd.read_csv('../input/properties_2017.csv')\n", "aggs = calculate_aggregates(properties)\n", "zip_count, city_count, medyear, meanarea, medlat, medlong = aggs\n", "munge(properties)\n", "train = pd.read_csv(\"../input/train_2017.csv\")\n", "train_df = train.merge(properties, how='left', on='parcelid')\n", "del train\n", "gc.collect()"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "e7d9595d-13e9-46a8-a22e-51e96e0eaf91", "_uuid": "3a95f73a66e5bfdbed8d3e4fb7658a05f85d8fb5"}, "source": ["for m in range(1,9+1):\n", "    \n", "    print( \"\\n\\nFUDGE FACTOR ANALYSIS FOR 2017 MONTH \", m)\n", "    select_mon = pd.to_datetime(train_df[\"transactiondate\"]).dt.month==m\n", "\n", "    target_aggs = calculate_target_aggreagates(train_df[~select_mon])\n", "    citystd, zipstd, hoodstd = target_aggs\n", "\n", "    train1 = train_df.copy()\n", "    calculate_features(train1)\n", "\n", "    x_valid = train1.drop(dropvars+droptrain, axis=1)[select_mon]\n", "    y_valid = train1[\"logerror\"].values.astype(np.float32)[select_mon]\n", "    n_valid = x_valid.shape[0]\n", "\n", "    train1=train1[ train1.logerror > -0.4 ]\n", "    train1=train1[ train1.logerror < 0.419 ]\n", "\n", "    train1=train1[~select_mon]\n", "    \n", "    x_train=train1.drop(dropvars+droptrain, axis=1)\n", "    y_train = train1[\"logerror\"].values.astype(np.float32)\n", "    y_mean = np.mean(y_train)\n", "    xgb_params['base_score'] = y_mean\n", "    \n", "    n_train = x_train.shape[0]\n", "\n", "    print('Fitting model to {} points & using {} to fit fudge factor'.format(n_train, n_valid))\n", "\n", "    dtrain = xgb.DMatrix(x_train, y_train)\n", "    dvalid_x = xgb.DMatrix(x_valid)\n", "    dvalid_xy = xgb.DMatrix(x_valid, y_valid)\n", "\n", "    evals = [(dtrain,'train'),(dvalid_xy,'eval')]\n", "    model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n", "                      evals=evals, early_stopping_rounds=early_stopping_rounds,\n", "                      verbose_eval=False)\n", "\n", "    valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\n", "    fudges = QuantReg(y_valid, sm.add_constant(valid_pred)).fit(q=.5).params\n", "    fudge0 = fudges[0]\n", "    fudge1 = fudges[1]\n", "    rawerr = mean_absolute_error(y_valid, valid_pred)\n", "    fudgerr = mean_absolute_error(y_valid, fudge0 + fudge1*valid_pred)\n", "    print(\"Fudge factors reduce MAE from {0:9.6f} to {1:9.6f}\".format(rawerr, fudgerr))\n", "    print(\"Optimized fudge factors for month {0}: {1:9.6f}, {2:9.6f}\".format(m, fudge0, fudge1))\n"], "cell_type": "code"}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "a9d5754f-03b2-4fb6-9f50-b6e906cd65d3", "_uuid": "c6e555efa782a348ba44d9bc35b0ddb19809c0fc"}, "source": [], "cell_type": "code"}], "nbformat_minor": 1, "nbformat": 4}