{"cells":[{"metadata":{"_uuid":"54748c999175280d887b807f5ebc51784906daa8","_cell_guid":"0fccc944-288a-4f50-aef3-084dd7770d20"},"cell_type":"markdown","source":"# Toxic Comment\n\nSource : https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda\n"},{"metadata":{"_uuid":"3ed5d0148a86a3370d577ec0f17bb6e248d35658","_cell_guid":"ae464e6a-c4db-48f2-af08-1b941bc51ac8","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport collections\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nimport warnings\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom bs4 import BeautifulSoup\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom scipy import sparse\nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\n#settings\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\n\nstopword_list = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"208a10f1139823380ba73499ff35a4073a642e31","_cell_guid":"584ae88d-bbb9-48f0-9df4-86ea747092ea","trusted":true},"cell_type":"code","source":"# Load Dataset\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(\"DIMENSION OF DATABASE : \")\nprint(\">>> Dimension du train :\", train.shape) # (159571, 8)\nprint(\">>> Dimension du train :\", test.shape) # (153164, 2)\n\n# Unique ID ? \ng=train['id'].value_counts()\ng.where(g>1).dropna()\n\ng=test['id'].value_counts()\ng.where(g>1).dropna()\n\n# Traitement des valeurs manquantes\nprint(\"\\nMISSING VALUES : \")\nprint(\">>> Check for missing values in Train dataset\")\nnull_check=train.isnull().sum()\nprint(null_check)\nprint(\">>> Check for missing values in Test dataset\")\nnull_check=test.isnull().sum()\nprint(null_check)\nprint(\">>> Filling NA with \\\"unknown\\\"\")\ntrain[\"comment_text\"].fillna(\"unknown\", inplace=True)\ntest[\"comment_text\"].fillna(\"unknown\", inplace=True)\n\n# Repartition des différentes classes : \nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nfor col in list_classes:\n    print(\"\\nRépartition pour la variable \", col, \" : \\n\", collections.Counter(train[col]))\n    \n#marking comments without any tags as \"clean\"\nrowsums=train.iloc[:,2:].sum(axis=1)\ntrain['total_toxicity'] = rowsums\ntrain['clean']=(rowsums==0)\ntrain.head()\n\nprint('\\nDistribution of Total Toxicity Labels (important for validation)')\nprint('On train set : ',pd.value_counts(train.total_toxicity))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f81c4048ca024a5e3988587ba0f20ca19cc832e6","_cell_guid":"2117e2ab-bfa4-4b55-b78e-9fee788e58c4"},"cell_type":"markdown","source":"# Indirect Features : \n\nExtract and analyze emoji\n\nNettoyage des commentaires ; suppression des \\n, ...\n\nCreate news features such as : \n* Numbers of words, unique words, stopwords, ...\n* Numbers of  punctuation, especially exclamation marks, questions marks ...\n* Numbers of  symbols\n* Numbers of  smiley\n* etc ...\n\nOther source : https://github.com/anthonyray/sentiment-analysis"},{"metadata":{"_uuid":"b8f7c527b9ec936da23a44c44f38c764b7cdd33c","_cell_guid":"0ddedcba-b149-4706-8111-12543a7c6f8a"},"cell_type":"markdown","source":"## Emoji"},{"metadata":{"_uuid":"8fbf8991faccdc9107ebf223b63916339fdaa08a","collapsed":true,"_cell_guid":"035271f7-7b54-4325-b0bd-24dc74ee4507","trusted":false},"cell_type":"code","source":"# Packages require \nimport emoji\n\n\"\"\"\nDescription de la fonction : \n1. Identifier les emoticones dans les texts comments\n2. Lister et compter les emoticones \n3. Supprimer les emoticones dans la phrase\n\"\"\"\n\n# Fonction 1 : Extraire tous les emoji de la phrase\ndef extract_emojis(str):\n  return ' '.join(c for c in str if c in emoji.UNICODE_EMOJI)\n\n# Definition des pattern unicode pour les emojis \n'''\nTO DO:\npour bien nettoyer la liste, il va falloir faire un dico des unicodes pour les emoticons\n'''\nemoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\u2122\"\n                           u'\\u260E'\n                           \"]+\", flags=re.UNICODE)\n# Fonction 2 : \ndef pipeline_emoji(df):\n    # Lister tous les emojis présents dans la phrase\n    df['list_emoji'] = df[\"comment_text\"].apply(lambda x: extract_emojis(x))\n    # Compter le nombre d'emoji\n    df['count_emoji'] = df[\"comment_text\"].apply(lambda x: len(extract_emojis(x)))\n    # Supprimer les emojis - on remplace\n    df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: emoji_pattern.sub(r'', x)) \n    \n\"\"\"\npipeline_emoji(train)\nprint(list(train.columns))\n\n# Recuperation des obs qui ont un emoji\nm = np.array(train['nb_emoji'])\nidx = np.where(m == 1)\ntrain.iloc[idx]\ntrain.comment_text.iloc[599]\n# 137 et 143 et 599\nprint(train.comment_text.iloc[599].encode('ascii', 'backslashreplace'))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d3c3a25ade6c7f48e5b6afb690635a08a37d74a","collapsed":true,"_cell_guid":"04516601-a8d5-4c42-a1e7-24eca9cc3f98","trusted":false},"cell_type":"code","source":"def indirect_features(df):\n    \n    # ========================================================================================\n    # >>> Retreat IP Adress\n    # ========================================================================================\n    print(\">>>> Retreat IP Address ----------------- \")\n    ip_pattern = re.compile(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\", flags=re.UNICODE)\n    df['ip']=df[\"comment_text\"].apply(lambda x: \n                                      re.findall(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\",str(x)))\n    # Count of ip addresses\n    df['count_ip']=df[\"ip\"].apply(lambda x: len(x))\n    # Delete IP Adress\n    df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: ip_pattern.sub(r'', x)) \n\n    # ========================================================================================\n    # >>> Retreat Link\n    # ========================================================================================\n    # Links\n    df['complete_link']=df[\"comment_text\"].apply(lambda x: \n                                                 \" \".join(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n                                                             str(x))))\n\n    \"\"\"\n    TO DO : A REVOIR >>> >\n    df['domain_name']=df['complete_link'].apply(lambda x:\n                                               urlparse(x))\n    # ValueError: Invalid IPv6 URL\n    \"\"\"\n\n    # Count of links\n    df['count_links']=df[\"complete_link\"].apply(lambda x: len(x))\n\n    \"\"\"\n    TO DO : delete links\n    \"\"\"\n    \n    # ========================================================================================\n    # >>> Retreat time and date\n    # ========================================================================================\n    \n    \"\"\" \n    \n    TO DO : A REVOIR CAR D'AUTRES TYPES DE REGEX SUR LES DATES \n    Exemple : L612 : 5-Mar 15 \n    \n    \"\"\"\n    \n    time_pattern = re.compile(\"\\d{1,2}:\\d{1,2}\", \n                              flags=re.UNICODE)\n    date_pattern = re.compile(r'\\d\\d\\s(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|January|February|March|April|May|June|July|August|September|October|November|December|january|february|march|april|may|june|july|august|september|october|november|december)\\s\\d{4}', \n                              flags=re.UNICODE)\n\n    df['time']=df[\"comment_text\"].apply(lambda x: re.findall(\"\\d{1,2}:\\d{1,2}\",str(x)))\n    df['time_flag']=df.time.apply(lambda x: len(x))\n    df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: time_pattern.sub(r'', x)) \n\n    df['date']=df[\"comment_text\"].apply(lambda x: \n                                        re.findall(r'\\d\\d\\s(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}',\n                                                   str(x)))\n    df['date_flag'] = df.date.apply(lambda x: len(x))\n    df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: date_pattern.sub(r'', x)) \n\n    # ========================================================================================\n    # >>> Retreat ID User \n    # ========================================================================================\n    # TO DO : Delete username >>> Not very interesting\n    user_pattern = re.compile(\"\\[\\[User(.*)\", flags=re.UNICODE)\n\n    df['username']=df[\"comment_text\"].apply(lambda x: re.findall(\"\\[\\[User(.*)\",str(x)))\n    df['count_usernames']=df[\"username\"].apply(lambda x: len(x))\n    df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: user_pattern.sub(r'', x)) \n\n    # ========================================================================================\n    # >>> Retreat divers pattern \n    # ========================================================================================\n    divers_pattern = re.compile(\"\\(UTC\\)|\\(utc\\)\", flags=re.UNICODE)\n    text = divers_pattern.sub(r'', text)\n\n    # ========================================================================================\n    # >>> Retreat emoji\n    # ========================================================================================\n    pipeline_emoji(df)\n     \n    # ========================================================================================\n    # Retraitement si case vide\n    # ========================================================================================\n    df.comment_text = df.comment_text.replace(r'^\\s*$', \"NAN\", regex=True)\n    \n    # ========================================================================================\n    # >>> Retreat words\n    # ========================================================================================\n    # 1- '\\n' can be used to count the number of sentences in each comment\n    df['count_sent']=df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n    # 2- Word count in each comment:\n    df['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n    # 3- Unique word count\n    df['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n    # 4- Letter count\n    df['count_letters']=df[\"comment_text\"].apply(lambda x: len(str(x)))\n    # 5- upper case words count\n    df[\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n    # 6- title case words count\n    df[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n    # 7- Number of stopwords\n    df[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopword_list]))\n    # 8- Average length of the words\n    df[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    \n    df['total_length'] = df['comment_text'].apply(len)\n    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment \n                                                                  if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n                                    axis=1)\n    \n    # ========================================================================================\n    # >>> Symbols, punctuation and emoticon\n    # ========================================================================================\n    # 1- punctuation count\n    df[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) \n                                                                      if c in string.punctuation]))\n    # 2- Exclamation marks count\n    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n    # 3- Question marks count\n    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n    # 4- Symbols counts\n    df['num_symbols'] = df['comment_text'].apply(\n        lambda comment: sum(comment.count(w) for w in '*&$%'))\n    # 5- Smiley count\n    df['num_smilies'] = df['comment_text'].apply(\n        lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n\n    # ========================================================================================\n    # >>> Derived features\n    # ========================================================================================\n    # 1- Word count percent in each comment:\n    df['word_unique_percent']=df['count_unique_word']*100/df['count_word']\n    # 2- Punct percent in each comment:\n    df['punct_percent']=df['count_punctuations']*100/df['count_word']\n    \nindirect_features(train)\nindirect_features(test)\nprint(\"FIN >>> > \")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59a2da2cf2b1298942c5d44f397e2fc736cd8ca5","_cell_guid":"11804969-d56a-4d03-ac44-761781af73be"},"cell_type":"markdown","source":" # Clean Comment text and create feature"},{"metadata":{"_uuid":"987aa5ba4c61dc4eaff3d15817f4ba946cfd4660","_kg_hide-input":true,"collapsed":true,"_cell_guid":"ba8244b7-f030-4d53-b658-4d0c5510b292","trusted":false},"cell_type":"code","source":"CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n                   \"this's\": \"this is\",\n                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } \n\ndef expand_contractions(sentence, contraction_mapping): \n     \n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),  \n                                      flags=re.IGNORECASE|re.DOTALL) \n    def expand_match(contraction): \n        match = contraction.group(0) \n        first_char = match[0] \n        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())                        \n        expanded_contraction = first_char+expanded_contraction[1:] \n        return expanded_contraction \n         \n    expanded_sentence = contractions_pattern.sub(expand_match, sentence) \n    return expanded_sentence \n\n# Remove accent and diacritics \nimport unidecode\ndef remove_accent_before_tokens(sentences):\n    res = unidecode.unidecode(sentences)\n    return(res)\n\n# Remove accent and punctuation\ndef remove_before_token(sentence, keep_apostrophe = False):\n    sentence = sentence.strip()\n    if keep_apostrophe:\n        PATTERN = r'[?|$|&|*|%|@|(|)|~]'\n        filtered_sentence = re.sub(PATTERN, r' ', sentence)\n    else :\n        PATTERN = r'[^a-zA-Z0-9]'\n        filtered_sentence = re.sub(PATTERN, r' ', sentence)\n    return(filtered_sentence)\n\nprint(\"Fin\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aae017123e378a9bff42cd3f62051edfc62febe0","collapsed":true,"_cell_guid":"9860b63b-f81e-4518-873e-4e3accabaa9e","trusted":false},"cell_type":"code","source":"\"\"\"\n\nTO DO >>> Dans les extracts de features indirectes, on a supprimer des éléments dans les commentaires.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c71941f11f1b577edf13e3f471730fb761c1b2b","collapsed":true,"_cell_guid":"ea30bef0-84f1-4f69-9371-5bbea70615c2","trusted":false},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nfrom bs4 import BeautifulSoup\n\ndef preprocessing_clean(comment):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    \n    # >>> Remove html \n    comment=BeautifulSoup(comment).get_text()\n    \n    # >>> Convert to lower case , so that Hi and hi are the same\n    comment=comment.lower()\n    \n    # >>> Suppression des leak elements \n    # remove \\n\n    comment=re.sub(\"\\\\n\",\"\",comment)\n    # remove leaky elements like ip,user\n    comment=re.sub(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\",\"\",comment)\n    #removing usernames\n    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n    \n    #Split the sentences into words\n    words=tokenizer.tokenize(comment)    \n    words=[CONTRACTION_MAP[word] if word in CONTRACTION_MAP else word for word in words]\n\n    # Stopwords\n    words = [w for w in words if not w in stopword_list]\n    \n    # Lemmatization\n    # words= [lem.lemmatize(word, \"v\") for word in words]\n\n    clean_sent=\" \".join(words)\n\n    # remove any non alphanum,digit character\n    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n    return(clean_sent)\n\nprint(\">>> Before cleaning\")\nprint(train.comment_text.iloc[23])\n\nprint(\"\\n>>> After cleaning\")\npreprocessing_clean(train.comment_text.iloc[23])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"332dd53992c8ff9a71d13bfbb73b2bd2f12fd211","collapsed":true,"_cell_guid":"63072abc-8c1c-4057-9091-8773f6a002d8","trusted":false},"cell_type":"code","source":"\"\"\"\n\nTO DO  :\n\n- Faire une colonne text lemma\n- Faire une colonne texte stem \n\nEssayer les modeles sur ces deux versions \n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e22728742b2cdf4ef2cd5c4bd7f81be963c11c04","collapsed":true,"_cell_guid":"1bae28f8-a54d-457a-a6f2-de532a5548e5","trusted":false},"cell_type":"code","source":"# Application sur tout le corpus : \n# type(train.comment_text) = pandas.core.series.Series\n# type(corpus) = pandas.core.series.Series\n\n# Clean des comments sur le train\nclean_corpus=train.comment_text.apply(lambda x :preprocessing_clean(x))\nprint(\"Not cleaned : \", clean_corpus[42])\nprint(\"\\nCleaned : \", clean_corpus[42])\n\nprint(\"FIN\")\n\n\"\"\"\n# Clean des comments sur le test\nclean_corpus=test.comment_text.apply(lambda x :preprocessing_clean(x))\nprint(\"Not cleaned : \", clean_corpus[42])\nprint(\"Cleaned : \", clean_corpus[42])\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b449805f14d43826b9de447a93a7d0369c6439ed","collapsed":true,"_cell_guid":"e20e58d1-e743-46f9-a841-279fbb10ecb9","trusted":false},"cell_type":"code","source":"print(\"Not cleaned : \", clean_corpus[23])\nprint(\"\\nCleaned : \", clean_corpus[23])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45696259f302bbd8f4ec5d30e030df2a60595286","collapsed":true,"_cell_guid":"f4edba3e-2533-40df-b53f-8c6cbfd573e8","trusted":false},"cell_type":"code","source":"\"\"\"\nCreate final dataset \n\"\"\"\n\n\"\"\"\n\n# Transform series into dataframe\ndf_clean_corpus = clean_corpus.reset_index()\n\n# Merge\ndf_final = pd.concat([df.drop(\"comment_text\",axis = 1),\n                      df_clean_corpus.drop(\"index\",axis = 1)], \n                     axis =1 )\ndf_final.head()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abe7f4f9e7ec89e2b7c85ad66583b24b0a0f671a","_cell_guid":"058caeb8-41c4-4c75-b224-f7699a5e4aff"},"cell_type":"markdown","source":"Source : https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"},{"metadata":{"_uuid":"8f78ffa931cf34e5b00affd59e713e7bc8920589","collapsed":true,"_cell_guid":"6ff2ad58-a83d-4487-87dc-21eccb84dae3","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79b1cc56f9bf43a9e07206b752ce84816c009eda","collapsed":true,"_cell_guid":"329ed10e-8f08-42bc-91de-ca53d993c48b","trusted":false},"cell_type":"code","source":"# TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)\n# Exemple de lancement : \n# tfidf_word = TfidfVectorizer()\n# X_tfidf_word = tfidf_word.fit_transform(X[:, 1])\n\n# TF-IDF sur les mots :\ntfidf_word = TfidfVectorizer()\nX_tfidf_word = tfidf_word.fit_transform(clean_corpus)\n\n# TF-IDF sur les character n-grams :\ntfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)\nX_tfidf_char = tfidf_char.fit_transform(clean_corpus)\n\n# Concatenation des deux bases \nX_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n\n# Affichages des features dans les deux trucs \nfeatures_tfidf_word = np.array(tfidf_word.get_feature_names())\nprint(list(features_tfidf_word))\nfeatures_tfidf_word = np.array(tfidf_char.get_feature_names())\nprint(list(features_tfidf_word))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d2879e716550694ea59f779dd36a41598b1f2b0","collapsed":true,"_cell_guid":"2950d53f-480d-479b-bfe7-f22ed9599a39","trusted":false},"cell_type":"code","source":"clean_corpus","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2b8953678f89320077c9ba9d9b4893762d01abd","collapsed":true,"_cell_guid":"60c7fc7d-5c8d-4427-a290-95fc1cb0d2d2","trusted":false},"cell_type":"code","source":"barack-obama-mother.jpg\n159559","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d773206ff22b2fafe3dc11b9e856a4bf783ed96","_cell_guid":"1612ad71-d7fb-46a3-b8d3-2d952758f2d8"},"cell_type":"markdown","source":"ON constate qu'il y a beaucoup de chiffre, de fautes d'orthographe"},{"metadata":{"_uuid":"9d46ae16e2695d57f50904bb09a9067492ce1e01","collapsed":true,"_cell_guid":"a9208efc-883b-46c9-adc4-f5b99b46f0dc","trusted":false},"cell_type":"code","source":"# Avant de lancer une régression logistique ou autre : \nX_tfidf_word = tfidf_word.transform(clean_corpus)\ntype(X_tfidf_word)\nX_tfidf_char = tfidf_char.transform(clean_corpus)\ntype(X_tfidf_char)\nX_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n\n# Après on peut : LogisticRegression().fit(X_tfidf, y[:, i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bc97cfe32c567412779baef4f452f72c0463652","collapsed":true,"_cell_guid":"3bf6afad-5c11-406e-80a0-c60aa4ef8a25","trusted":false},"cell_type":"code","source":"# Pour ajouter les features indirectes dans la base TF IDF: \nfeatures = ['count_sent',\n 'count_word',\n 'count_unique_word',\n 'count_letters',\n 'count_words_upper',\n 'count_words_title',\n 'count_stopwords',\n 'mean_word_len',\n 'total_length',\n 'capitals',\n 'caps_vs_length',\n 'count_punctuations',\n 'num_exclamation_marks',\n 'num_question_marks',\n 'num_symbols',\n 'num_smilies',\n 'word_unique_percent',\n 'punct_percent']\n\nx_feat_indirect = train[features]\nfrom scipy.sparse import hstack\nX_train_dtm = hstack([X_tfidf,np.array(x_feat_indirect)])\nX_train_dtm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84a2cdef20d2512e6efffc2f9c2261d42bca3f4e","_kg_hide-input":true,"collapsed":true,"_cell_guid":"1f885de1-0b5f-4573-b3c7-f5c95e8b23a7","trusted":false},"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38c18206d9cbcfce11e0696efb0b02f481df17da","collapsed":true,"_cell_guid":"e2657772-1c1e-4ce0-ac13-9e12660fa0cb","trusted":false},"cell_type":"code","source":"TARGET_COLS=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nlst_drop = TARGET_COLS\nprint(TARGET_COLS)\nlst_drop.append(\"id\")\nlst_drop.append(\"comment_text\")\nlst_drop.append(\"total_toxicity\")\nlst_drop.append(\"clean\")\nprint(lst_drop)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f758c782a2a5bd6e4ce234b379dea196dca6858d","collapsed":true,"_cell_guid":"3a153897-2d14-409d-9d9a-979aee738da2","trusted":false},"cell_type":"code","source":"# Sur la base de train\ntrain_x  = train.drop(lst_drop, axis = 1)\nprint(list(train_x.columns))\ntarget_y = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\nprint(list(target_y.columns))\n\n#Sur la base de test\ntest_x = test.drop([\"id\",\"comment_text\"],axis = 1)\nprint(list(test_x.columns))\ntest_x.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"121a3f8c5451f1f2113b89f243ef169bbe71c084","collapsed":true,"_cell_guid":"41741d4a-ed8c-4084-991a-b663850df88b","trusted":false},"cell_type":"code","source":"# Fitting a simple Logistic Regression indirect feature\nprint(\"Using only Indirect features\")\nmodel = LogisticRegression(C=3)\nX_train, X_valid, y_train, y_valid = train_test_split(train_x, \n                                                      target_y, \n                                                      test_size=0.25,\n                                                      random_state=42)\ntrain_loss = []\nvalid_loss = []\nimportance=[]\n\nsubmission_binary = pd.read_csv('../input/sample_submission.csv')\n\nfor i, j in enumerate(TARGET_COLS):\n    print('Class:= '+j)\n    model.fit(X_train,y_train[j])\n    preds_valid = model.predict_proba(X_valid)\n    preds_train = model.predict_proba(X_train)\n    train_loss_class=log_loss(y_train[j],preds_train)\n    valid_loss_class=log_loss(y_valid[j],preds_valid)\n    print('Trainloss=log loss:', train_loss_class)\n    print('Validloss=log loss:', valid_loss_class)\n    \"\"\"\n    importance.append(model.coef_)\n    train_loss.append(train_loss_class)\n    valid_loss.append(valid_loss_class)\n    \"\"\"\n    \n    # Prediction in same time in order to create submission file\n    test_y_prob = model.predict_proba(test_x)\n    submission_binary[j] = test_y_prob\n    \nprint('mean column-wise log loss:Train dataset', np.mean(train_loss))\nprint('mean column-wise log loss:Validation dataset', np.mean(valid_loss))\n\n# generate submission file\n# submission_binary.to_csv('submission_binary.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55baddc4d4f123f745dac03be7c4c8ff2b77674a","collapsed":true,"_cell_guid":"90d9cacd-bf7d-494b-87d2-526df8ea0c30","trusted":false},"cell_type":"code","source":"\"\"\"\n# Create a train and test set\nsize = int(len(brown_tagged_sents) * 0.9)\ntrain_sents = brown_tagged_sents[:size]\ntest_sents = brown_tagged_sents[size:]\n\n# Train the model \nfrom nltk.tag.perceptron import PerceptronTagger\npct_tag = PerceptronTagger(load=False)\npct_tag.train(train_sents)\n\n# Check the performance \nprint (\"Evaluation Own PerceptronTagger on train set \", pct_tag.evaluate(train_sents))\nprint (\"Evaluation Own PerceptronTagger on test set \", pct_tag.evaluate(test_sents))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5f7e9ec6e75ada5a74ba3944c417eeb47470c38","_cell_guid":"20984624-86eb-49ef-bf7a-6eb0b000e64f"},"cell_type":"markdown","source":"# Simple regression with TF-IDF"},{"metadata":{"_uuid":"8bac7f9405b53d694898f378705b533a75da0c33","collapsed":true,"_cell_guid":"5a4ee50e-512d-4964-9b6b-90ea5da32563","trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubm = pd.read_csv('../input/sample_submission.csv')\n\n\n\ndf = pd.concat([train['comment_text'], test['comment_text']], axis=0)\ndf = df.fillna(\"unknown\")\nnrow_train = train.shape[0]\n\n\nvectorizer = TfidfVectorizer(stop_words='english', max_features=50000)\ndata = vectorizer.fit_transform(df)\nX = MaxAbsScaler().fit_transform(data)\n\ncol = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\npreds = np.zeros((test.shape[0], len(col)))\n\n\n\nloss = []\n\nfor i, j in enumerate(col):\n    print('===Fit '+j)\n    model = LogisticRegression()\n    model.fit(X[:nrow_train], train[j])\n    preds[:,i] = model.predict_proba(X[nrow_train:])[:,1]\n    \n    pred_train = model.predict_proba(X[:nrow_train])[:,1]\n    print('log loss:', log_loss(train[j], pred_train))\n    loss.append(log_loss(train[j], pred_train))\n    \nprint('mean column-wise log loss:', np.mean(loss))\n    \n    \nsubmid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = col)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e991df448eb32121fc36177ebbe7b9873b2541b5","collapsed":true,"_cell_guid":"873201a7-5338-4c6f-8873-c7f9c542196b","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}