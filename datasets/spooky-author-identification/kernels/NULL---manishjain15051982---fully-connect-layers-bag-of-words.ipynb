{"cells": [{"metadata": {"_uuid": "f00336a6bee77687bef4301cc4002db87222b4bd", "_cell_guid": "979a54fc-a42b-491b-af3a-589f6f268b03"}, "cell_type": "markdown", "source": ["We will perform text classificaiton using bag of words and finally load this as input into Fully connected layers of Neural Net. We shall use pandas,numpy for data operations and Keras for Fully connected layers of neural net"]}, {"metadata": {"_uuid": "451283cf35e7d2c731afc390021152233c26541f", "_cell_guid": "c7a9d9dc-22f0-411f-8375-1f4bce809187"}, "cell_type": "markdown", "source": ["We will perform below steps\n", "1. Clean Training data for punctuations and standard stopwords\n", "2. Create vocab list \n", "3. Convert text to matrix usng Keras Tokenizer\n", "4. Train neural net \n", "5. Predicton on test data and submission"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "308263d332c3bd21fc619f6a0d5da127e14042eb", "_cell_guid": "4a6c0b05-36ee-490d-9456-c02bca277f99"}, "source": ["# Load the libraries\n", "import numpy as np\n", "import pandas as pd\n", "\n", "from pandas import DataFrame\n", "from collections import Counter\n", "from nltk.corpus import stopwords\n", "\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.layers import Dense, Dropout,BatchNormalization,Input,Activation\n", "from keras.models import Model\n", "from keras import optimizers\n", "\n", "from matplotlib import pyplot\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "4ede5bd3941f88a1697e56ab8713bb8c86649f52", "_cell_guid": "50b1a240-db47-4c4a-820c-1e851f4f52e1"}, "source": ["# turn a text into clean tokens\n", "def clean_text(text):\n", "    # remove punctuation from the text\n", "    text = text.replace(\"' \", \" ' \")\n", "    signs = set(',.:;\"?!')\n", "    prods = set(text) & signs\n", "    if not prods:\n", "        return text\n", "    for sign in prods:\n", "        text = text.replace(sign, ' {} '.format(sign) )\n", "    # split into tokens by white space\n", "    tokens = text.split()\n", "    # remove remaining tokens that are not alphabetic\n", "    tokens = [word for word in tokens if word.isalpha()]\n", "    # filter out stop words\n", "    stop_words = set(stopwords.words('english'))\n", "    tokens = [w for w in tokens if not w in stop_words]\n", "    # filter out short tokens\n", "    tokens = [word for word in tokens if len(word) > 1]\n", "    return tokens\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "0f8008f84d0f63f869b7f0a70a521e9b7154057f", "_cell_guid": "4576c116-938d-4eab-ad97-de0fcc4771dd"}, "source": ["#Define column names\n", "TEXT =\"text\"\n", "AUTHOR = \"author\"\n", "\n", "# Create fucntion to load words into vocab\n", "\n", "def add_text_to_vocab(X_train, vocab):\n", "    for text in X_Train[TEXT]:\n", "      tokens = clean_text(text)\n", "      # update counts\n", "      vocab.update(tokens)\n", " "]}, {"metadata": {"_uuid": "57cbd0ea18754d07c59d884cd705912cb80debdd", "_cell_guid": "5a981350-e1bc-4475-a086-ceff2d9d6ef6"}, "cell_type": "markdown", "source": ["Now, we will load the training data, clean up each text and load create a vocab list after filtering on punctuations, stopwords,short length tokens and minimum occurance."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "bd68422500190ff4a4bbe6a46957bec88a3f497e", "_cell_guid": "a743b614-0e09-4603-8c9d-05476bf4e64c"}, "source": ["# Read text from training data\n", "X_Train= pd.read_csv(\"../input/train.csv\")\n", "\n", "# define vocab\n", "vocab = Counter()\n", "\n", "# add all text to vocab\n", "add_text_to_vocab(X_Train,vocab)\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "854b5fdf694947c1c18376a2a3f0e95aeb38298f", "_cell_guid": "b82afbb5-3227-47ca-a3fe-0c7dfed02dbd"}, "source": ["# keep tokens with a min occurrence \n", "min_occurane = 2\n", "tokens = [k for k,c in vocab.items() if c >= min_occurane]"]}, {"metadata": {"_uuid": "2b9e6ebf0cbbb09b7b847126bfbcef6baf0b1a3c", "_cell_guid": "afb4848b-31ca-44ec-bab0-23054fc3ede3"}, "cell_type": "markdown", "source": ["Vocab list is now created. We will store that into a file for further reference"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "2c24e0768775a08e9821dae010c1679694b2ecbd", "_cell_guid": "610628d7-113b-481b-8237-af0ac7c970d8"}, "source": ["# save list to file\n", "def save_list(lines, filename):\n", "    # convert lines to a single blob of text\n", "    data = '\\n'.join(lines)\n", "    # open file\n", "    file = open(filename, 'w')\n", "    # write text\n", "    file.write(data)\n", "    # close file\n", "    file.close()\n", "\n", "# save tokens to a vocabulary file\n", "save_list(tokens, 'vocab.txt')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "ceb14260fbc7fb31bc2522c196a9bb1f9609f5ed", "_cell_guid": "8505bca3-b030-4aa5-b7d2-ac39807cb67b"}, "source": ["# load file into memory\n", "def load_file(filename):\n", "    # open the file as read only\n", "    file = open(filename, 'r')\n", "    # read all text\n", "    text = file.read()\n", "    # close the file\n", "    file.close()\n", "    return text\n", "\n", "# load the vocabulary back into memory to be used for model training\n", "vocab_filename = 'vocab.txt'\n", "vocab = load_file(vocab_filename)\n", "vocab = vocab.split()\n", "vocab = set(vocab)"]}, {"metadata": {"_uuid": "7c0215f4adcdc8de1bcc7bfcfe0bf8ccb49f5374", "_cell_guid": "5cbd77b1-a1a4-4fdf-8909-c1608c4a533d"}, "cell_type": "markdown", "source": ["Now, we can start data preparation for model training. We will use Keras Tokenizer with mode as 'freq', I have tried Binary, count and tfidf as well. Freq modes seems to work best with lowest loss among these options"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "b089af78abe2105652dea62331c11ec2725fdfa0", "_cell_guid": "e64a5088-f9b0-4009-b67a-99512d4b98fc"}, "source": ["# create the tokenizer\n", "tokenizer = Tokenizer()\n", "\n", "# prepare bag of words encoding of docs\n", "def prepare_data(train_docs, mode):\n", "    # fit the tokenizer on the documents\n", "    tokenizer.fit_on_texts(train_docs)\n", "    # encode training data set\n", "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n", "    return Xtrain\n"]}, {"metadata": {"_uuid": "cb00bc85065e47115a8178bd99076a2d77907dd9", "_cell_guid": "2ee72991-840c-420e-888e-622ff7eb9c8b"}, "cell_type": "markdown", "source": ["Now we will create neural network definition model with Keras. This will include multiple set of Dense, BatchNorm, Activation and dropout layer. You can decide on number of layers. \n", "\n", "Dropout is included in input latyer and hidden layers to reduce variance and overfitting on training data."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "6b801d650897e69884f897be2e3a094d72e72541", "_cell_guid": "ab235c9c-0a4f-45a9-901a-c1bf3f0a8aab"}, "source": ["# Create a neural network model\n", "def create_model(xtrain):\n", "    \n", "    X_input = Input(shape=(xtrain.shape[1],))\n", "   \n", "    # dropout on input layer\n", "    X = Dropout(0.5)(X_input)\n", "    \n", "    # Dense -> BN -> RELU-> Dropout Block applied to X - First set\n", "    X = Dense(900, kernel_initializer='he_normal', name='D0')(X)\n", "    X = BatchNormalization(axis=1, name='bn0')(X)\n", "    X = Activation('relu')(X)\n", "    X = Dropout(0.5)(X)\n", "\n", "    # Dense -> BN -> RELU-> Dropout Block applied to X - Second set\n", "    X = Dense(600, kernel_initializer='he_normal', name='D1')(X)\n", "    X = BatchNormalization(axis=1, name='bn1')(X)\n", "    X = Activation('relu')(X)\n", "    X = Dropout(0.5)(X)\n", "    \n", "    # Dense -> BN -> RELU-> Dropout Block applied to X - Third set\n", "    X = Dense(300, kernel_initializer='he_normal', name='D2')(X)\n", "    X = BatchNormalization(axis=1, name='bn2')(X)\n", "    X = Activation('relu')(X)\n", "    X = Dropout(0.5)(X)\n", "\n", "    # output layer with softmax function for 3 classes prediction\n", "    X = Dense(3, kernel_initializer='he_normal', activation='softmax')(X)\n", "\n", "    Spookymodel = Model(inputs=X_input, outputs=X, name='SpookyAuthor')\n", "    \n", "    return Spookymodel"]}, {"metadata": {"_uuid": "69aec2bb747306cb1d8fa5a9bcd303dd08010e67", "_cell_guid": "5b5ac5ab-93b8-4794-a9d9-a5357d4059d5"}, "cell_type": "markdown", "source": ["We are ready to transform training data and create model"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a467c93f4c2337ea0a122b2042c1a79bed64f599", "_cell_guid": "40fbdc69-29e7-40b2-b842-b42b2630ee99"}, "source": ["#Prepare data and and create model \n", "# You can try with other modes 'binary','count','tfidf'\n", "mode = 'freq'\n", "\n", "#Training \n", "train_texts = X_Train[TEXT]\n", "# Training labels (coverted into seperate columsn for each other with 0,1)\n", "ytrain = np.array(pd.get_dummies(X_Train[AUTHOR]))\n", "\n", "# prepare data for mode\n", "xtrain = prepare_data(train_texts, mode)\n", "\n", "# model defination creation\n", "Spookymodel = create_model(xtrain)\n", "    "]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "5b8a95fd6b9d1b2ef62e3cbb21fdf87d2bfb76d4", "_cell_guid": "7071d1e4-113f-4084-a405-87dbb080bd34"}, "source": ["# Summarize the model\n", "Spookymodel.summary()"]}, {"metadata": {"_uuid": "1aac753e17bb93e21fff8780ba67355062e55444", "_cell_guid": "ab88e543-a423-4ad8-8737-2378c241a568"}, "cell_type": "markdown", "source": ["Now we are ready to the model. Let's complie and train the model.\n", "\n", "Optimizer - We will use \"Adamax\" optimizer with learning rate of 0.05 and rate decay of 0.001. You can tune these as per your preferences.\n", "\n", "Loss function - Categorical_crossentropy.\n", "\n", "Batch size - 64\n", "\n", "validation split- 20%"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a48f0fb05f14763c4a8a79146337f8ffd6e9a0d6", "_cell_guid": "02453157-8f99-4994-b14a-533821cd1752"}, "source": ["optimzer = optimizers.Adamax(lr=0.05, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.001)\n", "\n", "# compile network\n", "Spookymodel.compile(loss='categorical_crossentropy', optimizer=optimzer, metrics=['accuracy'])\n", "\n", "# fitnetwork\n", "train_history = Spookymodel.fit(x= xtrain, y=ytrain, epochs=5,verbose=2,batch_size=64, validation_split=0.2)\n", "\n", "# Plot the training and validation loss at each epoch\n", "loss = train_history.history['loss']\n", "val_loss = train_history.history['val_loss']\n", "pyplot.plot(loss)\n", "pyplot.plot(val_loss)\n", "pyplot.legend(['loss', 'val_loss'])\n", "pyplot.show()"]}, {"metadata": {"_uuid": "81bb36b00f69cc719d78e8c7bdaf2be050a74e33", "_cell_guid": "18e28c76-fdd8-40a7-93b9-de201152c996"}, "cell_type": "markdown", "source": ["Validation loss of 0.43 is achieved with 5 epochs. It can be minimized further with hyperparameter tunning and more training."]}, {"metadata": {"_uuid": "00c7d432d0246d32e88a9fd4aff2268dbfb0ca17", "_cell_guid": "f75ff7ee-a92b-4474-81b8-58c4c113d5c8"}, "cell_type": "markdown", "source": ["**Prediction on test data**"]}, {"metadata": {"_uuid": "8b4baf31ca84e3f43fb54a516178ee1fe23b1b5e", "_cell_guid": "12bd10a3-3b07-48e1-94dd-8bc27aaccec7"}, "cell_type": "markdown", "source": ["Now, we are ready to predict on test data and submit the results"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "1179365b22f1fad5da54cd2814f6c91ad40a3ceb", "_cell_guid": "2a021f86-2453-47b3-a335-5ed3a4676713"}, "source": ["# Function to predict the author class on test data\n", "def predict_author(text, vocab,tokenizer, model):\n", "    # clean\n", "    tokens = clean_text(text)\n", "    # filter by vocab\n", "    tokens = [w for w in tokens if w in vocab]\n", "    # convert to line\n", "    line = ' '.join(tokens)\n", "    # encode\n", "    encoded = tokenizer.texts_to_matrix([line], mode='freq')\n", "    # prediction\n", "    yhat = model.predict(encoded, verbose=0)\n", "    return yhat\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "61639890bc41429407c66e38451dd85e3d516f64", "_cell_guid": "2af03511-5834-481b-9865-0136f8c870cb"}, "source": ["# Load test data into dataframe\n", "X_sub = pd.read_csv(\"../input/test.csv\")\n", "\n", "# Initilize prediction matrix \n", "y_pred = np.zeros((X_sub.shape[0],3))\n", "\n", "# Predict for each  sample in test dataset\n", "i = 0\n", "for text in X_sub[TEXT]:\n", "    y_pred[i]=predict_author(text,vocab,tokenizer,Spookymodel)\n", "    i +=1\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "cb4900c568e52e88161284c9532fe743e1e0bbea", "_cell_guid": "e8f131c2-e1e6-46e9-b01c-e9bfe3536d72"}, "source": ["#Creating submission datafram\n", "submission = pd.DataFrame(y_pred,dtype=float)\n", "submission=submission.rename(index=int, columns={0: \"EAP\", 1: \"HPL\", 2: \"MWS\"})\n", "submission.insert(0,'id',X_sub[\"id\"])\n", "\n", "\n", "#Save as CSV file for submission\n", "submission.to_csv(\"sub.csv\",sep=',', encoding='utf-8')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "44d4b0b9c39bd2a7117fb1a19b1ffb866eef3117", "_cell_guid": "408adfd3-a2f4-4bf5-adff-6ac1ab87cfe7"}, "source": []}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "version": "3.6.3", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1}