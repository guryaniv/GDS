{"cells": [{"metadata": {"_uuid": "61e6abb3027184b728050e2bf6b51925db8abdc6", "_cell_guid": "3637caa0-ccaf-440d-93a4-d4554501be74"}, "outputs": [], "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running thiimport numpy as np \n", "import pandas as pd\n", "from pandas import Series, DataFrame \n", "from tensorflow.python.framework import ops\n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "import math\n", "import sklearn.metrics as skm\n", "%matplotlib inline\n", "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n", "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n", "plt.rcParams['image.interpolation'] = 'nearest'\n", "plt.rcParams['image.cmap'] = 'gray'\n", "from sklearn.linear_model import LogisticRegression\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": 1}, {"metadata": {"_uuid": "fd79ccaf2d9b40caea038a3ade80321e3064fa78", "_cell_guid": "bd32dd47-bf47-4199-9989-9f90590d7c03"}, "cell_type": "markdown", "source": []}, {"metadata": {"_uuid": "c64a9555323f185d67fc90b703dacb235ff2aabb", "_cell_guid": "36703564-d313-4cb3-a356-3ab21b300793"}, "cell_type": "markdown", "source": ["Since the data iteslf doesn't tell us about the features.  We will use Autoencoders to build features of importance and use them using logistic regression. We will see the jump in accuracy by using the Autoencoder generated features. Autoencoder will pass the features of X(currently 198) to a neual netwok layer of nodes= 100, thus shinking features by 100 and also those features would contain the core features of training dataset."]}, {"metadata": {"_uuid": "33710f4ae2f921b64348fbb6670a12828d7a34ec", "collapsed": true, "_cell_guid": "34044484-cd76-4442-8cae-aff953533250"}, "outputs": [], "cell_type": "code", "source": ["#Load the train and test csv files.\n", "train = pd.read_csv('../input/testcsv/train.csv')\n", "test = pd.read_csv('../input/testcsv/test.csv')"], "execution_count": 3}, {"metadata": {"_uuid": "5d8634a9a466506a3614058868836345930fae41", "_cell_guid": "fee38698-0da1-4777-98ea-bb05c856adbc"}, "outputs": [], "cell_type": "code", "source": ["# Preprocessing Step taken by Forca\n", "id_test = test['id'].values\n", "target_train = train['target'].values\n", "train = train.drop(['target','id'], axis = 1)\n", "test = test.drop(['id'], axis = 1)\n", "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n", "train = train.drop(col_to_drop, axis=1)\n", "test = test.drop(col_to_drop, axis=1)  \n", "train = train.replace(-1, np.nan)\n", "test = test.replace(-1, np.nan)\n", "cat_features = [a for a in train.columns if a.endswith('cat')]\n", "for column in cat_features:\n", "    temp = pd.get_dummies(pd.Series(train[column]))\n", "    train = pd.concat([train,temp],axis=1)\n", "    train = train.drop([column],axis=1)\n", "for column in cat_features:\n", "    temp = pd.get_dummies(pd.Series(test[column]))\n", "    test = pd.concat([test,temp],axis=1)\n", "    test = test.drop([column],axis=1)\n", "test=test.fillna(method='pad')\n", "train=train.fillna(method='pad')\n", "print(train.values.shape, test.values.shape)\n", "print(target_train.shape)"], "execution_count": 4}, {"metadata": {"_uuid": "5c9db3f245921fb78da8869e1258cf870f5e367c", "_cell_guid": "6bb93a28-cbd8-41c4-9c34-257c91f387a0"}, "outputs": [], "cell_type": "code", "source": ["#Function to evaluate the Gini Coefficients\n", "def eval_gini(y_true, y_prob):\n", "    y_true = np.asarray(y_true)\n", "    y_true = y_true[np.argsort(y_prob)]\n", "    ntrue = 0\n", "    gini = 0\n", "    delta = 0\n", "    n = len(y_true)\n", "    for i in range(n-1, -1, -1):\n", "        y_i = y_true[i]\n", "        ntrue += y_i\n", "        gini += y_i * delta\n", "        delta += 1 - y_i\n", "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n", "    return gini\n", "\n", "\n", "#Applying vanila logistic regression.\n", "from sklearn.cross_validation import train_test_split\n", "x_train ,x_test, y_train, y_test = train_test_split(train, target_train,test_size=0.3)\n", "\n", "log_model=LogisticRegression()\n", "log_model.fit(x_train,y_train)\n", "pred_log = log_model.predict_proba(x_test)[:,1]\n", "print( \"Gini log  Test= \", eval_gini(y_test, pred_log))"], "execution_count": 8}, {"metadata": {"_uuid": "4f2781cc2f0d0b68e964634dc5706d3f5005b851", "_cell_guid": "2ed23b69-b7dd-4ad4-803b-34a7c681a767"}, "cell_type": "markdown", "source": ["We can see that vanila logistic regression doesnt perform well on the dataset. For this we need to find the features. autoencoder has the ability to provide the non linear core principle dimension of the data. Lets build the autoencoder. Most of the parts are learned from Andrew Ng's deep learning specialization."]}, {"metadata": {"_uuid": "c0639cb9f17e9e3ca8db40343daf513f10f545db", "collapsed": true, "_cell_guid": "d5afd898-69de-4aa3-a0c7-e718bb4416ac"}, "outputs": [], "cell_type": "code", "source": ["#Mini Batches Generation for TensorFlow. Lots of code ahead, don't worry, its just helper functions.\n", "def random_mini_batches(X, Y, mini_batch_size = 1024, seed = 0):\n", "    np.random.seed(seed)            \n", "    m = X.shape[1]                  \n", "    mini_batches = []\n", "    \n", "    permutation = list(np.random.permutation(m))\n", "    shuffled_X = X[:, permutation]\n", "    shuffled_Y = Y[:, permutation]\n", "    \n", "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n", "    for k in range(0, num_complete_minibatches):\n", "        mini_batch_X = shuffled_X[:, k*mini_batch_size : mini_batch_size*(k+1)]\n", "        mini_batch_Y = shuffled_Y[:,mini_batch_size*k : mini_batch_size*(k+1)]\n", "        mini_batch = (mini_batch_X, mini_batch_Y)\n", "        mini_batches.append(mini_batch)\n", "    if m % mini_batch_size != 0:\n", "        mini_batch_X = shuffled_X[:, mini_batch_size*num_complete_minibatches : m]\n", "        mini_batch_Y = shuffled_Y[:, mini_batch_size*num_complete_minibatches : m]\n", "        mini_batch = (mini_batch_X, mini_batch_Y)\n", "        mini_batches.append(mini_batch)\n", "    return mini_batches"], "execution_count": 9}, {"metadata": {"_uuid": "2209bfce429bc62895a45901f414929bbe914125", "collapsed": true, "_cell_guid": "836e2dd6-aa21-498f-8bc9-dad8a2637809"}, "outputs": [], "cell_type": "code", "source": ["# creating placeholders for TensorFlow\n", "def create_placeholders(n_x, n_y):\n", "    X = tf.placeholder(dtype=\"float\", shape=(n_x, None), name='X')\n", "    Y = tf.placeholder(dtype=\"float\", shape=(n_y, None), name='Y')\n", "    return X, Y"], "execution_count": 10}, {"metadata": {"_uuid": "f3de9014bce2b6dfbb7bfa5c729ca2f04db644d3", "collapsed": true, "_cell_guid": "f9b0470e-2181-4dae-b0b7-4b2025229b42"}, "outputs": [], "cell_type": "code", "source": ["# initialize_parameters\n", "tf.reset_default_graph()\n", "def initialize_parameters(f1=198, f2=100, f3=50):\n", "    tf.set_random_seed(1)  \n", "    W1 = tf.get_variable(\"W1\", [f2,f1], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n", "    b1 = tf.get_variable('b1', [f2,1], initializer= tf.zeros_initializer())\n", "    W2 = tf.get_variable(\"W2\", [f3,f2], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n", "    b2 = tf.get_variable('b2', [f3,1], initializer= tf.zeros_initializer())\n", "    W3 = tf.get_variable('W3', [f2,f3], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n", "    b3 = tf.get_variable('b3', [f2,1], initializer= tf.zeros_initializer())\n", "    W4 = tf.get_variable('W4', [f1,f2], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n", "    b4 = tf.get_variable('b4', [f1,1], initializer= tf.zeros_initializer())\n", "    parameters = {\"W1\": W1,\n", "                  \"b1\": b1,\n", "                  \"W2\": W2,\n", "                  \"b2\": b2,\n", "                  \"W4\": W4,\n", "                  \"b4\": b4,\n", "                  \"W3\": W3,\n", "                  \"b3\": b3}\n", "    \n", "    return parameters"], "execution_count": 11}, {"metadata": {"_uuid": "f62767b5ff9c13409fa2ec93caa7972ba885e282", "collapsed": true, "_cell_guid": "d24f70dc-abed-4d03-ae22-e68c011f7d7d"}, "outputs": [], "cell_type": "code", "source": ["#Forward Prop steps for tensorflow\n", "def forward_propagation(X, parameters):\n", "    W1 = parameters['W1']\n", "    b1 = parameters['b1']\n", "    W2 = parameters['W2']\n", "    b2 = parameters['b2']\n", "    W3 = parameters['W3']\n", "    b3 = parameters['b3']\n", "    W4 = parameters['W4']\n", "    b4 = parameters['b4']\n", "    \n", "    Z1 = tf.add(tf.matmul(W1, X), b1)                                  # Z1 = np.dot(W1, X) + b1\n", "    A1 = tf.nn.tanh(Z1)\n", "    Z2 = tf.add(tf.matmul(W2, A1), b2)                                  # Z1 = np.dot(W1, X) + b1\n", "    A2 = tf.nn.relu(Z2)# A1 = relu(Z1)                                              # A2 = relu(Z2)\n", "    Z3 = tf.add(tf.matmul(W3, A2), b3)                           # Z3 = np.dot(W3,Z2) + b3\n", "    A3=  tf.nn.tanh(Z3)\n", "    Z4 = tf.add(tf.matmul(W4, A3), b4)                           # Z3 = np.dot(W3,Z2) + b3\n", "    A4=  tf.nn.relu(Z4)\n", "    return A4, A2\n"], "execution_count": 12}, {"metadata": {"_uuid": "043428805edd537a75cdebf41004135ea7c62e09", "collapsed": true, "_cell_guid": "bbcc0cbe-5253-4a3c-8754-7a76d6b8e0ea"}, "outputs": [], "cell_type": "code", "source": ["#Cost computation for tensorflow\n", "def compute_cost(A4, Y, parameters):\n", "    m = Y.shape[1]\n", "    W1 = parameters['W1']\n", "    W2 = parameters['W2']\n", "    W3 = parameters['W3']\n", "    W4 = parameters['W4']\n", "    #Cost is the the difference of output with input.\n", "    cost = tf.reduce_mean(tf.pow(Y - A4, 2))\n", "    return cost"], "execution_count": 13}, {"metadata": {"_uuid": "87e29b41d77d586e3f15b7a1e158445973ffd7e3", "collapsed": true, "_cell_guid": "0a81beb6-53dd-400c-b3f0-23f9deb21807"}, "outputs": [], "cell_type": "code", "source": ["#Final AE Model\n", "def model(X_train, Y_train, f_2, f_3, learning_rate,\n", "          num_epochs , minibatch_size , print_cost):\n", "    ops.reset_default_graph()                         \n", "    tf.set_random_seed(1)                             \n", "    seed = 3                                          \n", "    (n_x, m) = X_train.shape \n", "    n_y = Y_train.shape[0]                            \n", "    costs = []                                        \n", "    X, Y = create_placeholders(n_x, n_y)\n", "    parameters = initialize_parameters(f2=f_2,f3=f_3)\n", "    A4, A2 = forward_propagation(X, parameters)\n", "    cost = compute_cost(A4, Y, parameters)\n", "    #Tensorflow optimizer\n", "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n", "    init = tf.global_variables_initializer()\n", "    with tf.Session() as sess:\n", "        sess.run(init) \n", "        # Do the training loop\n", "        for epoch in range(num_epochs):\n", "            epoch_cost = 0.                      \n", "            num_minibatches = int(m / minibatch_size)\n", "            seed = seed + 1\n", "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n", "            for minibatch in minibatches:\n", "                # Select a minibatch\n", "                (minibatch_X, minibatch_Y) = minibatch\n", "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n", "                epoch_cost += minibatch_cost / num_minibatches\n", "\n", "            # Print the cost every epoch\n", "            if print_cost == True:\n", "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n", "            if print_cost == True:\n", "                costs.append(epoch_cost)\n", "                \n", "        plt.plot(np.squeeze(costs))\n", "        plt.ylabel('cost')\n", "        plt.xlabel('iterations (per tens)')\n", "        plt.title(\"Learning rate =\" + str(learning_rate))\n", "        plt.show()\n", "        parameters = sess.run(parameters)\n", "        return parameters, A2"], "execution_count": 14}, {"metadata": {"_uuid": "e63782f4060c374159c4e3daf3d58dca69cb89a5", "collapsed": true, "_cell_guid": "d6216be5-3eec-407f-8145-02dbb95da08d"}, "outputs": [], "cell_type": "code", "source": ["train=train.as_matrix()\n", "test=test.as_matrix()"], "execution_count": 15}, {"metadata": {"_uuid": "7b1141fc36f6a564893cad25d2550f508255ab94", "_cell_guid": "f8881d2d-6c3b-4793-8e0c-6f017c35a568"}, "outputs": [], "cell_type": "code", "source": ["#Run the MODEL\n", "parameters, _ = model(train.T,train.T,160,100,minibatch_size=512,num_epochs=10, learning_rate=0.001, print_cost=True)"], "execution_count": 16}, {"metadata": {"_uuid": "b59f99108f864c486c9974ad94e498296b1f625e", "_cell_guid": "bdab0529-2454-4d9e-822e-02e64a7255ac"}, "cell_type": "markdown", "source": ["The cost here is the reconstruction cost of y using reconstructed using this NN.\n", "We can see the cost converging when we train the Autoencoder to X with X itself. The middle layer which has the size of 130 nodes. So the final feature count would be 130. We call them A2 featuers and these features will be a input to Logistic regression model. And we will see improved accuracy."]}, {"metadata": {"_uuid": "176df102261fc36a5f6c4f16230eeac7ae05ba77", "collapsed": true, "_cell_guid": "5c535d4a-b40d-4556-bce8-bd1e5ee12e4a"}, "outputs": [], "cell_type": "code", "source": ["#Helper functions to calculate the features using parameters\n", "\n", "def forward_propagationout(X, parameters):\n", "    # retrieve parameters\n", "    W1 = parameters['W1']\n", "    b1 = parameters['b1']\n", "    W2 = parameters['W2']\n", "    b2 = parameters['b2']\n", "    W3 = parameters['W3']\n", "    b3 = parameters['b3']\n", "    W4 = parameters['W4']\n", "    b4 = parameters['b4']\n", "    \n", "    Z1 = np.dot(W1, X) + b1                                  # Z1 = np.dot(W1, X) + b1\n", "    A1 = np.tanh(Z1)\n", "    Z2 = np.dot(W2, A1) + b2                                   # Z1 = np.dot(W1, X) + b1\n", "    A2 = relu(Z2)                                             # A2 = relu(Z2)\n", "    Z3 = np.dot(W3, A2) + b3                           # Z3 = np.dot(W3,Z2) + b3\n", "    A3=  np.tanh(Z3)\n", "    Z4 = np.dot(W4, A3) + b4                            # Z3 = np.dot(W3,Z2) + b3\n", "    A4=  relu(Z4)\n", "    return A2\n", "def relu(x):\n", "    s = np.maximum(0,x)\n", "    return s\n", "\n"], "execution_count": 17}, {"metadata": {"_uuid": "e2ff35c4f687305c462399f5174fd07e1a3e942e", "_cell_guid": "20b0e697-6025-4bb7-bb7e-5980be1f64d8"}, "outputs": [], "cell_type": "code", "source": ["#A2 featues from the middle layer of AE\n", "A2=forward_propagationout(train.T.astype('float32'), parameters)\n", "x_train ,x_test, y_train, y_test = train_test_split(A2.T, target_train,test_size=0.3)\n", "log_model=LogisticRegression()\n", "log_model.fit(x_train,y_train)\n", "pred_log = log_model.predict_proba(x_test)[:,1]\n", "print( \"Gini log  Test= \", eval_gini(y_test, pred_log))"], "execution_count": 18}, {"metadata": {"_uuid": "95681e4c9f62d3634d009e8cae3d9432607b0034", "collapsed": true, "_cell_guid": "10a959f5-8c86-417b-8d83-933296eae1cc"}, "cell_type": "markdown", "source": ["I hope you must have got the significance of Autoencoders by now, \n", "We have just increased the **Test Gini index from 0.255 to 0.2678.** . This is a drastic improvement.\n", "This happened because of better features created by AE.\n", "Though this increase is not going to help you score better rank. But its difficult to create featues when no information is given and specifically in this case only Autoencoders are here to help you. You can reduce the amount of code using Keras.\n", "You can now stack this Logistic regession model to any other model to get better rank.  If needs, I can provide the parameters file to create those features and do the regression.\n", "Thanks for reading."]}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1}