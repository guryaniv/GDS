{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adadelta\nfrom keras import regularizers\n\nfrom sklearn.preprocessing import PolynomialFeatures","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e68351700a85cef78482152591de9e25c16ba550"},"cell_type":"markdown","source":"## Loading data"},{"metadata":{"trusted":true,"_uuid":"d60ab6db6c81d295cbb7da8673d0995275cb58ad","collapsed":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv', nrows = 10 ** 6)\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0055df6db737bd013e804c362c03a5aa9740e53"},"cell_type":"markdown","source":"## Checking for missing values"},{"metadata":{"trusted":true,"_uuid":"3e07fed26c7f9f8c8fc55bc176ccaca44885bb71","collapsed":true},"cell_type":"code","source":"datasets = [train_df, test_df]\n\nfor df in datasets:\n    missing_values = df.isnull().sum().to_frame().sort_values(0, ascending = False)\n    display(missing_values.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2a3fb3fdd4b3d4d1bef58f967341588e5069b6e"},"cell_type":"markdown","source":"## Filtering values for train and test datasets have same distribution"},{"metadata":{"trusted":true,"_uuid":"624597bc003a157d7631e41c80f0d0bc0b9255eb","collapsed":true},"cell_type":"code","source":"print(\"Train before cleaning:\")\ndisplay(train_df.describe())\n\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')\ntrain_df = train_df[(train_df.pickup_longitude > -75.0) & (train_df.pickup_longitude < -73.0)]\ntrain_df = train_df[(train_df.pickup_latitude > 40.0) & (train_df.pickup_latitude < 42.0)]\ntrain_df = train_df[(train_df.dropoff_longitude > -75.0) & (train_df.dropoff_longitude < -73.0)]\ntrain_df = train_df[(train_df.dropoff_latitude > 40.0) & (train_df.dropoff_latitude < 42.0)]\ntrain_df = train_df[(train_df.passenger_count > 0.0) & (train_df.passenger_count <= 6.0)]\n\n\nprint(\"Train after cleaning:\")\ndisplay(train_df.describe())\n\nprint(\"Test for comparison:\")\ndisplay(test_df.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5167c66f23f4843db84423287841bc296f6164d6"},"cell_type":"markdown","source":"## Calculate haversine"},{"metadata":{"trusted":true,"_uuid":"00fbb31b44531b14972aa5dc1feb1fa6ea20304f","collapsed":true},"cell_type":"code","source":"def calc_haversine(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n\n    df['dlat'] = np.radians(df.dropoff_latitude - df.pickup_latitude)\n    df['dlon'] = np.radians(df.dropoff_longitude - df.pickup_longitude)\n    df['haversine_a'] = np.sin(df.dlat/2) * np.sin(df.dlat/2) + np.cos(np.radians(df.pickup_latitude)) \\\n            * np.cos(np.radians(df.dropoff_latitude)) * np.sin(df.dlon/2) * np.sin(df.dlon/2)\n    df['haversine'] = 6371 * 2 * np.arctan2(np.sqrt(df.haversine_a), np.sqrt(1-df.haversine_a))\n\n    return df.drop(columns=['pickup_datetime'])\n\ntrain_df = calc_haversine(train_df)\ntest_df = calc_haversine(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b546921a979e99033abb9190189474cf92232207"},"cell_type":"markdown","source":"## Correlation Visualization"},{"metadata":{"trusted":true,"_uuid":"b67639a325895b497cd7205ee8a37efb8afe4948","collapsed":true},"cell_type":"code","source":"corr = train_df.corr()\nf, ax = plt.subplots(figsize=(10, 10)) \ncmap = sns.diverging_palette(220, 10, as_cmap=True) \nsns.heatmap(corr, cmap=cmap, vmax=1.0, square=True, linewidths=.3, cbar_kws={\"shrink\": .5}, ax=ax) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c7f6a414381a3c717624028d3ae27240e8bac06"},"cell_type":"markdown","source":"## Prepare X and Y for magic"},{"metadata":{"trusted":true,"_uuid":"c135c307f66f34570d22471b8d25d6a9dcf1180d","collapsed":true},"cell_type":"code","source":"# filter interesting columns and label\ntrain_y = np.array(train_df['fare_amount'])\ntrain_X = train_df.drop(columns=['fare_amount','key'])\n\nprint(\"Shape for X:\")\nprint(train_X.shape)\nprint(\"Shape for Y:\")\nprint(train_y.shape)\n\ntest_X = test_df.drop(columns=['key'])\nprint(\"Shape for test X:\")\nprint(test_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6ab3f376f8c26b4a93d0ec37a94fb74be21d402"},"cell_type":"markdown","source":"## Build DNN model on Keras"},{"metadata":{"trusted":true,"_uuid":"85abbb09a27d2e1e2a15b261264b3c7cbdde39e4","collapsed":true},"cell_type":"code","source":"def run_model(X, Y, dnn_layers_size, dropout_value, batch_size, epochs):\n    \n    input_size = X.shape[1]\n    \n    model = Sequential()\n    \n    for idx, l in enumerate(dnn_layers_size):\n        model.add(Dense(l, input_dim=input_size,\n                           kernel_initializer='normal',\n                           activation='selu'))\n        model.add(Dropout(dropout_value))\n        input_size = l\n        \n    model.add(Dense(1, kernel_initializer='normal'))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    \n    train_history = model.fit([X], Y, epochs=epochs, batch_size=batch_size, validation_split=0.1, shuffle=True)\n    \n    return train_history, model\n\ndef build_layers(layers, n_features):\n    if len(layers) == 0:\n        n_features = int(n_features * 2.5)\n    else:\n        n_features = int(math.sqrt(n_features))\n        \n    if n_features < 3:\n        return layers\n    else:\n        layers.append(n_features)\n        return build_layers(layers, n_features)\n    \ndef plot_build(train_history):    \n    \n    # plotting train_history\n    plt.figure(0)\n    axes = plt.gca()\n    axes.set_ylim([0,90])\n    plt.plot(train_history.history['loss'],'g')\n    plt.plot(train_history.history['val_loss'],'b')\n    plt.rcParams['figure.figsize'] = (8, 6) \n    plt.xlabel(\"Num of Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss vs Validation Loss\")\n    plt.grid()\n    plt.legend(['train','validation'])\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0415b9da67249b5cfaac61c41b6f58677cb3707"},"cell_type":"markdown","source":"## Show training for X with no poly-transformation"},{"metadata":{"trusted":true,"_uuid":"d6dcdb6aae44a263358ec63663ee722bf190f5dd","collapsed":true},"cell_type":"code","source":"layers = build_layers([],train_X.shape[1])\nprint('Layers:', layers)\nprint('-' * 15)\ntrain_history, model = run_model(train_X, train_y, layers, 0.2, batch_size = 32, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a459dc2b3a2b2393f2d7c18e1c7aede32c147a98","collapsed":true},"cell_type":"code","source":"plot_build(train_history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d58e4fbae8fe61ae90bff30389507c82bbf47db9"},"cell_type":"markdown","source":"## Generating submissions for both model"},{"metadata":{"trusted":true,"_uuid":"ddba4a856ff617411a641dfdf7635e47f969dff8","collapsed":true},"cell_type":"code","source":"# Generating DNN submission\npred_y = model.predict([test_X])\ntest_df['pred'] = pred_y\n\nsubmission = pd.DataFrame(\n    {'key': test_df.key, 'fare_amount': test_df.pred},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission_dnn.csv', index = False)\n\nprint(os.listdir('.'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3b7277b6f0106e0e5052aad113eac2d25d4b235"},"cell_type":"markdown","source":"## Adventures with XGB"},{"metadata":{"trusted":true,"_uuid":"c67c88decf069b68b64737d551fcdb3d8e634a1c","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as msq\nimport xgboost as xgb\n\nstdscaler = StandardScaler()\nxgb_train_X = stdscaler.fit_transform(train_X)\nxgb_test_X  = stdscaler.fit_transform(test_X)\n\nx_train, x_test, y_train, y_test = train_test_split(xgb_train_X, train_y, random_state=70, test_size=0.2)\n\ndef XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train, label=y_train)\n    matrix_test = xgb.DMatrix(x_test, label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'},\n                    dtrain=matrix_train,\n                    num_boost_round=100, \n                    early_stopping_rounds=10,\n                    evals=[(matrix_test,'test')])\n    return model\n\nxgb_model = XGBmodel(x_train,x_test,y_train,y_test)\nxgb_pred = xgb_model.predict(xgb.DMatrix(xgb_test_X), ntree_limit = xgb_model.best_ntree_limit)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cbd9ff6a7d686eae49c01b71bd8ae86a35fc36d","collapsed":true},"cell_type":"code","source":"# Generating XGB submission\ntest_df['pred_xgb'] = xgb_pred\n\nsubmission = pd.DataFrame(\n    {'key': test_df.key, 'fare_amount': test_df.pred_xgb},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission_xgb.csv', index = False)\n\nprint(os.listdir('.'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73da005d0a005c4983ddcb10b3c4fa4095e18e9f"},"cell_type":"markdown","source":"## Combine previous results (DNN and XGB) in a simplistic average ensemble scheme"},{"metadata":{"trusted":true,"_uuid":"c152bae4dce70a466d98f18a4e11b4cd2b51d31c","collapsed":true},"cell_type":"code","source":"test_df['ensemble'] = (test_df.pred + test_df.pred_xgb) / 2.0\n\nsubmission = pd.DataFrame(\n    {'key': test_df.key, 'fare_amount': test_df.ensemble},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission_ensemble.csv', index = False)\n\nprint(os.listdir('.'))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fd559ff5ca72a73091d5dfd5b7032522832e999"},"cell_type":"markdown","source":"Special thanks to Dan Becker, Will Cukierski, and Julia Elliot for reviewing this Kernel and providing suggestions!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}