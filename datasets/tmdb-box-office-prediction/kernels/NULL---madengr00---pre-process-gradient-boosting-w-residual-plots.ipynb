{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Cleaning and Preprocessing Data for Machine Learning"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom datetime import date, datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daf931a13554a23a099f42ce63e69f77ec89b114"},"cell_type":"code","source":"# Read the csv files into a pandas DataFrame\n# Our goal is to predict the movie revenue without adding additional data to the dataset\ntrain = pd.read_csv(\"../input/tmdb-box-office-prediction/train.csv\")\ntest = pd.read_csv(\"../input/tmdb-box-office-prediction/test.csv\")\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47ea9e2a2079971653b1a0144773c36422495387"},"cell_type":"markdown","source":"## Correcting Dates"},{"metadata":{"trusted":true,"_uuid":"35ca6b51bea9cd8a71686844969a24bc5582e080"},"cell_type":"code","source":"#Good catch & correction found here\n#https://www.kaggle.com/jiegeng94/simple-tmdb-prediction-with-gradient-boosting\n#The year values have only two digits and the years before 1969 are denoted as ones of 2000's. Make it correct. \ndef expand_release_date(df):\n    df.release_date = pd.to_datetime(df.release_date)\n\n    df['year'] = df.release_date.dt.year\n    df['year'] = df.year.apply(lambda x: x-100 if x > 2020 else x)\n    \n    df['month'] = df.release_date.dt.month\n    df['day'] = df.release_date.dt.dayofweek\n    df['quarter'] = df.release_date.dt.quarter\n    \n    return df\n\ntrain = expand_release_date(train)\ntest = expand_release_date(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7df8fe5a02bac5e7888f98776224cb566d0fe9b1"},"cell_type":"markdown","source":"## Assigning weights to Cast, Crew, Production Companies, and Keyword by # appearances\nSimilar to scalers used for ML models later, we decided to weight the cast, crew, and keywords by number of appearances in the dataset.  We created tables of the unique values, then assigned a weight over the range.  "},{"metadata":{"trusted":true,"_uuid":"9ed6d6e9244e9e3e896d1a4551122a26154bfebb"},"cell_type":"code","source":"# Import Cast Table with Counts of Appearances\nfile = \"../input/weighttables/Cast_Data.csv\"\ncastData = pd.read_csv(file)\ncastData.rename(columns = {\"Num\":\"numtimesDS\"}, inplace = True)\ncastData.sort_values(by='numtimesDS', ascending=False, inplace = True)\n# calculation to create the actor wt\ncastData['actorWt'] = castData['numtimesDS']/castData['numtimesDS'].max().astype(np.float64)\ncastData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dfd36ed8a074a8c18bd0090aaa92b97db000939"},"cell_type":"code","source":"# Import Production Company Table with Counts of Appearances\nfile = \"../input/prodcotable/ProdCo_Data.csv\"\nprodcoData = pd.read_csv(file)\nprodcoData.rename(columns = {\"Prod_Co\":\"numtimesDS\"}, inplace = True)\nprodcoData.sort_values(by='numtimesDS', ascending=False, inplace = True)\n\nprodcoData['prodcoWt'] = prodcoData['numtimesDS']/prodcoData['numtimesDS'].max().astype(np.float64)\nprodcoData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89b24dfddcd809c3aab88b9c3e1d35bda1c4ccfe"},"cell_type":"code","source":"# Import Keyword Table with Counts of Appearances\nfile = \"../input/weighttables/Keyword_Data.csv\"\nkeywordData = pd.read_csv(file)\nkeywordData.rename(columns = {\"# of Uses in DS\":\"numtimesDS\"}, inplace = True)\nkeywordData.sort_values(by='numtimesDS', ascending=False, inplace = True)\n\nkeywordData['keywordWt'] = keywordData['numtimesDS']/keywordData['numtimesDS'].max().astype(np.float64)\nkeywordData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4edc10c05f760dc948e17c3b9e6baf0d2d928970"},"cell_type":"code","source":"# Import Crew Table with Counts of Appearances\nfile = \"../input/weighttables/Crew_Data_clean.csv\"\ncrewData = pd.read_csv(file)\ncrewData.rename(columns = {\"Num\":\"numtimesDS\"}, inplace = True)\ncrewData.sort_values(by='numtimesDS', ascending=False, inplace = True)\n\ncrewData['crewWt'] = crewData['numtimesDS']/crewData['numtimesDS'].max().astype(np.float64)\ncrewData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaf7e8b4d7cdeff07ca3ff255639f38517075184"},"cell_type":"code","source":"# In order to apply weights to the field with JSON data, we first flatten, then convert to list, then iterate to produce a wt. \n# We are assuming that bigger \"stars\" appear in the dataset more times. \n#https://www.kaggle.com/rajuspartan/exploratory-data-analysis-with-reusable-functions\n#Flatening JSON columns\ndef get_dictionary(s):\n    try:\n        d = eval(s)\n    except:\n        d = {}\n    return d\ntrain[\"castList\"] = train.cast.map(lambda x: sorted([d['id'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\ntest[\"castList\"] = test.cast.map(lambda x: sorted([d['id'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\n\ntrain[\"keywordList\"] = train.Keywords.map(lambda x: sorted([d['id'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\ntest[\"keywordList\"] = test.Keywords.map(lambda x: sorted([d['id'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\n\ntrain[\"prodcoList\"] = train.production_companies.map(lambda x: sorted([d['id'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\ntest[\"prodcoList\"] = test.production_companies.map(lambda x: sorted([d['id'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\n\ntrain[\"crewList\"] = train.crew.map(lambda x: sorted([d['id'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\ntest[\"crewList\"] = test.crew.map(lambda x: sorted([d['id'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\n\ntrain['castList'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3723cc1cba34ed277ad62dc8b79075e491f15e30"},"cell_type":"code","source":"#The castList is a string, convert this to a list for easy function application\n#Convert string to list, separate on commas\ntrain['castList'] = train['castList'].apply(lambda x: x[1:-1].split(','))\ntest['castList'] = test['castList'].apply(lambda x: x[1:-1].split(','))\n\ntrain['keywordList'] = train['keywordList'].apply(lambda x: x[1:-1].split(','))\ntest['keywordList'] = test['keywordList'].apply(lambda x: x[1:-1].split(','))\n\ntrain['prodcoList'] = train['prodcoList'].apply(lambda x: x[1:-1].split(','))\ntest['prodcoList'] = test['prodcoList'].apply(lambda x: x[1:-1].split(','))\n\ntrain['crewList'] = train['crewList'].apply(lambda x: x[1:-1].split(','))\ntest['crewList'] = test['crewList'].apply(lambda x: x[1:-1].split(','))\n\ntrain['castList'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdf226d8ac0a89881f435b2730aa82e66cf12a09"},"cell_type":"code","source":"#create sum of weights column\ndef weight(data):\n    wt = 0\n    for row in data:\n        for x in row:\n            wt = wt + int(x)\n            \n    return wt\n\ntrain['castWt'] = train['castList'].apply(weight)\ntest['castWt'] = test['castList'].apply(weight)\n\ntrain['keywordWt'] = train['keywordList'].apply(weight)\ntest['keywordWt'] = test['keywordList'].apply(weight)  \n\ntrain['prodcoWt'] = train['prodcoList'].apply(weight)\ntest['prodcoWt'] = test['prodcoList'].apply(weight)\n\ntrain['crewWt'] = train['crewList'].apply(weight)\ntest['crewWt'] = test['crewList'].apply(weight)\n\n#Add columns for Team weights\ntrain['teamWt'] = train['castWt']+train['crewWt']+train['prodcoWt']\ntest['teamWt'] = test['castWt']+test['crewWt']+test['prodcoWt']\n\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9628afe19212b079fa750e57d3621fc694b22538"},"cell_type":"markdown","source":"## Create Count Features\nSome great information found here!\n#https://www.kaggle.com/jiegeng94/machine-learning-beginner-tutorial"},{"metadata":{"trusted":true,"_uuid":"1aaf54d9a77ec6996b20c7701a5127690923f35a"},"cell_type":"code","source":"def proc_json_len(string):\n    try:\n        data = eval(string)\n        return len(data)\n    except:\n        return 0\n\ntrain['count_genre'] = train.genres.apply(proc_json_len)\ntrain['count_country'] = train.production_countries.apply(proc_json_len)\ntrain['count_company'] = train.production_companies.apply(proc_json_len)\ntrain['count_splang'] = train.spoken_languages.apply(proc_json_len)\ntrain['count_cast'] = train.cast.apply(proc_json_len)\ntrain['count_crew'] = train.crew.apply(proc_json_len)\ntrain['count_staff'] = train.count_cast + train.count_crew\ntrain['count_keyword'] = train.Keywords.apply(proc_json_len)\ntest['count_genre'] = test.genres.apply(proc_json_len)\ntest['count_country'] = test.production_countries.apply(proc_json_len)\ntest['count_company'] = test.production_companies.apply(proc_json_len)\ntest['count_splang'] = test.spoken_languages.apply(proc_json_len)\ntest['count_cast'] = test.cast.apply(proc_json_len)\ntest['count_crew'] = test.crew.apply(proc_json_len)\ntest['count_staff'] = test.count_cast + test.count_crew\ntest['count_keyword'] = test.Keywords.apply(proc_json_len)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08263e517c249cd3f01533143d93fe4109d4d3d6"},"cell_type":"markdown","source":"## Part of a Collection, or Not?\nCreate a boolean column"},{"metadata":{"trusted":true,"_uuid":"19024c7c4ea38d18b64c94345f97cef7b90299d6"},"cell_type":"code","source":"#Collection or not?\ntrain['belongs_to_collection'] = train['belongs_to_collection'].notna()\ntest['belongs_to_collection'] = test['belongs_to_collection'].notna()\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c3c25f3b6564b0c277915a195256f488a5fc314"},"cell_type":"markdown","source":"## Genres and Spoken Languages"},{"metadata":{"trusted":true,"_uuid":"5bed963596ec5cf578efa5cbd811c03c27c26998"},"cell_type":"code","source":"new_genres_train = pd.DataFrame(train['genres'])\nnew_genres_test = pd.DataFrame(test['genres'])\nnew_splang_train = pd.DataFrame(train['spoken_languages'])\nnew_splang_test = pd.DataFrame(test['spoken_languages'])\nnew_genres_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"297bb248f3fc170843366f8724c13a676049fc21"},"cell_type":"code","source":"def get_dictionary(s):\n    try:\n        d = eval(s)\n    except:\n        d = {}\n    return d\n#https://www.kaggle.com/rajuspartan/exploratory-data-analysis-with-reusable-functions\n#Flatening JSON columns\nnew_genres_train.genres = new_genres_train.genres.map(lambda x: sorted([d['name'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\nnew_genres_test.genres = new_genres_test.genres.map(lambda x: sorted([d['name'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\n\nnew_splang_train.spoken_languages = new_splang_train.spoken_languages.map(lambda x: sorted([d['iso_639_1'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\nnew_splang_test.spoken_languages = new_splang_test.spoken_languages.map(lambda x: sorted([d['iso_639_1'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\nnew_splang_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad537f9200af2f796d4cad91e73d22a05144e899"},"cell_type":"code","source":"#We used this approach from Stack Overflow\n#https://stackoverflow.com/questions/50394099/separate-columns-based-on-genre\n#featurize the genre column\nnew_genres_train = new_genres_train['genres'].str.get_dummies(',')\nnew_genres_test = new_genres_test['genres'].str.get_dummies(',')\nprint(new_genres_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db399d621427314e2ac03985de9c37843c471eb1"},"cell_type":"code","source":"#featurize the spoken language column\n# Is english in the spoken language?\ndef proc_json_len2(string):\n        if ('en' in string):\n            return 1\n        else:\n            return 0\n  \nnew_splang_train['inEnglish'] = new_splang_train['spoken_languages'].apply(proc_json_len2)\nnew_splang_test['inEnglish'] = new_splang_test['spoken_languages'].apply(proc_json_len2)\nnew_splang_train.columns\nnew_splang_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34ec5498605d44cc494a504c6811d870eb8e9471"},"cell_type":"code","source":"#add genres back to data (join)\ntrain = pd.concat([train, new_genres_train], axis = 1, sort = False)\ntest = pd.concat([test, new_genres_test], axis = 1, sort = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d86ddce2a92bf8a987dce03862aad6b5e3e5ab2d"},"cell_type":"code","source":"#add spoken languages back to data (join)\ntrain = pd.concat([train, new_splang_train], axis = 1, sort = False)\ntest = pd.concat([test, new_splang_test], axis = 1, sort = False)\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a3b14eccbfc0f17b3d6c78704529953aa63fbbe"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"003d1e807b8b47b0bcd4b96ced72bef6668fe2e1"},"cell_type":"code","source":"#Select subset of columns\ntrain = train[['id','belongs_to_collection','budget', 'original_language', 'popularity', 'status','year', 'month', 'Action', 'Adventure', 'Animation', 'Comedy',\n       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Foreign',\n       'History', 'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction',\n       'TV Movie', 'Thriller', 'War', 'Western', 'count_genre',\n       'count_country', 'count_company', 'count_splang', 'count_cast',\n       'count_crew', 'count_staff', 'count_keyword', \"castWt\",'prodcoWt','keywordWt', 'crewWt','teamWt','inEnglish', \"revenue\"]]\ntest = test[['id','belongs_to_collection','budget', 'original_language', 'popularity', 'status','year', 'month', 'Action', 'Adventure', 'Animation', 'Comedy',\n       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Foreign',\n       'History', 'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction',\n       'Thriller', 'War', 'Western', 'count_genre',\n       'count_country', 'count_company', 'count_splang', 'count_cast',\n       'count_crew', 'count_staff', 'count_keyword', \"castWt\",'prodcoWt','keywordWt', 'crewWt','teamWt','inEnglish']]\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9421a7f74402c086eabfe80a8dd3951134f694d6"},"cell_type":"markdown","source":"## Dummy Encoding to transform categorical features"},{"metadata":{"trusted":true,"_uuid":"f0499d634b05128703722f97781aa6b083dc80b5"},"cell_type":"code","source":"data = train.copy()\ndata2 = test.copy()\ndata_binary_encoded = pd.get_dummies(data, columns=[\"belongs_to_collection\", \"status\"])\ndata2_binary_encoded = pd.get_dummies(data2, columns=[\"belongs_to_collection\", \"status\"])\n#Select subset of columns\ntrain = data_binary_encoded[['id','belongs_to_collection_True', 'budget', 'original_language', 'popularity', 'status_Released', 'year', 'month', 'Action', 'Adventure', 'Animation', 'Comedy',\n       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Foreign',\n       'History', 'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', 'Thriller', 'War', 'Western', 'count_genre',\n       'count_country', 'count_company', 'count_splang', 'count_cast',\n       'count_crew', 'count_staff', 'count_keyword', \"castWt\",'prodcoWt','keywordWt', 'crewWt','teamWt','inEnglish', 'revenue']]\ntest = data2_binary_encoded[['id','belongs_to_collection_True', 'budget', 'original_language', 'popularity', 'status_Released', 'year', 'month', 'Action', 'Adventure', 'Animation', 'Comedy',\n       'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Foreign',\n       'History', 'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction','Thriller', 'War', 'Western', 'count_genre',\n       'count_country', 'count_company', 'count_splang', 'count_cast',\n       'count_crew', 'count_staff', 'count_keyword', \"castWt\", 'prodcoWt','keywordWt', 'crewWt','teamWt','inEnglish']]\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5a5b95b881dca0db624a72f819cef65506bc8c7"},"cell_type":"markdown","source":"## Factorize to map each categorical item in a column to a value"},{"metadata":{"trusted":true,"_uuid":"795888f69ad72d6c6ca4b19aed75cb0af1dae01f"},"cell_type":"code","source":"train['language_enc'] = pd.factorize(train['original_language'])[0]\ntest['language_enc'] = pd.factorize(test['original_language'])[0]\n#This is just a list of all of the languages listed\ncatenc = pd.factorize(train['original_language'])\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"320417e18fdb6ec7f07326755c91125663d7976e"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true,"_uuid":"9b229f72fc62b74547003daf85c4cef7a4aa36d7"},"cell_type":"code","source":"X = train[['id','belongs_to_collection_True', 'budget', 'language_enc', 'popularity','year', 'month', 'count_genre',\n       'count_country', 'count_company', 'count_splang', 'count_cast',\n       'count_crew', 'count_staff', 'count_keyword', \"castWt\",'prodcoWt','keywordWt', 'crewWt','teamWt','inEnglish']]\ny = train['revenue'].values.reshape(-1,1)\nprint(X.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"629abd76eb2623243c817f06e04719d805e6d967"},"cell_type":"code","source":"#looking at a subset of attributes only\ntrain2 = train[['belongs_to_collection_True', 'budget', 'popularity', 'year','month', 'revenue']]\ntrain2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dc7a1e40af6700fad27416028c21fb270316ff3"},"cell_type":"code","source":"#explore datasets\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\npairplots = sns.pairplot(train2, diag_kind = 'kde', hue = \"month\",palette = \"Accent\", plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},\n             height = 4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d13dd0af21eedb1b888b9b3d43e3e54004988a9"},"cell_type":"markdown","source":"# TEST / TRAIN SPLIT OF TRAIN Dataset"},{"metadata":{"trusted":true,"_uuid":"65ea63bf5f3465be42527d0811bd65bc9856111d"},"cell_type":"code","source":"# Use train_test_split to create training and testing data from our \"train\" dataset\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b86567359b1078cc4b487d144f6f1e6c2efb5a7"},"cell_type":"markdown","source":"# LINEAR REGRESSION - No Scaling"},{"metadata":{"trusted":true,"_uuid":"8b689315c139d15f886e20faa7dcd7cbe8b314b6"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n#create the linear regression object\nmodel = LinearRegression(fit_intercept = True)\n#train the model\nmodel.fit(X_train, y_train)\ntraining_score = model.score(X_train, y_train)\nprint(f\"R2 Score: {training_score}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb308fd2bc751e544f53a730819987fef5ea494a"},"cell_type":"markdown","source":"### Residuals\n"},{"metadata":{"trusted":true,"_uuid":"02f4aa443db6cb276f1edd446295e78bfe01ccce"},"cell_type":"code","source":"# Plot the Residuals for the Training and Testing data\n#Residuals are the difference between the true values of y and the predicted values of y.\n#make predictions using the testing set\nprediction = model.predict(X_test)\n#plot residuals\nplt.scatter(model.predict(X_train), model.predict(X_train) - y_train, c=\"black\", label=\"Training Data\")\nplt.scatter(prediction, prediction - y_test, c=\"grey\", label=\"Testing Data\")\nplt.legend()\nplt.hlines(y=0, xmin=y_test.min(), xmax=y_test.max())\nplt.title(\"Residual Plot\")\nplt.show()\n\nMSE = mean_squared_error(y_test, prediction)\nr2 = model.score(X_test, y_test)\n### END SOLUTION\nprint(f\"MSE: {MSE}, R2: {r2}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dad593793efa95d63364b043cdbf32e975d775d6"},"cell_type":"markdown","source":"## Gradient Boosting Regressor - No Scaler"},{"metadata":{"trusted":true,"_uuid":"d96c4e7152e5aaa5f051d485cdc6f19665ad2262"},"cell_type":"code","source":"# Fit regression model\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': .01, 'loss': 'ls'} \nclf = ensemble.GradientBoostingRegressor(**params)\npredictions2 = clf.fit(X_train,y_train)\ntraining_score = clf.score(X_train, y_train)\nprint(f\"Training Score: {training_score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee2e8d9f434a011ed7bdf654c17b4bef2917e33c"},"cell_type":"code","source":"# Plot the Residuals for the Training and Testing data\n### BEGIN SOLUTION\npredictions2 = np.expand_dims(clf.predict(X_test), axis = 1)\nplt.scatter((np.expand_dims(clf.predict(X_train), axis = 1)), (np.expand_dims(clf.predict(X_train), axis = 1)) - y_train, c=\"black\", label=\"Training Data\")\nplt.scatter(predictions2, predictions2 - y_test, c=\"grey\", label=\"Testing Data\")\nplt.legend()\nplt.hlines(y=0, xmin=y_test.min(), xmax=y_test.max())\nplt.title(\"Residual Plot\")\nplt.show()\n\nMSE = mean_squared_error(y_test, predictions2)\nr2 = clf.score(X_test, y_test)\nprint(f\"MSE: {MSE}, R2: {r2}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e371d2a0e9f30e8f4f83738c204c938afec59ae"},"cell_type":"code","source":"#Predictions for the test data\nrevenue_predictions = clf.predict(X_test)\ngbr_predictions = pd.DataFrame(revenue_predictions, columns = ['revenue'])\ngbr_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c77b1d8d7c660919f9e342622296fe3e6088f93"},"cell_type":"code","source":"test2 = pd.concat([test, gbr_predictions], axis = 1, join_axes = [test.index])\n#look at top values only\ntest2 = test2[['belongs_to_collection_True', 'budget', 'popularity', 'year','month', 'revenue']]\ntest2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c8b583ccfaca26ea714af7072f522c6a3274e44"},"cell_type":"code","source":"#explore datasets\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\npairplots = sns.pairplot(test2, diag_kind = 'kde', hue = \"month\",palette = \"Accent\", plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},\n             height = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d477e9a7b0b36128e7f2340d2fd17221586b0e8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}