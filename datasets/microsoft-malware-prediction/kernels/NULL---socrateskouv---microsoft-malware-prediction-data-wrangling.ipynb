{"cells":[{"metadata":{"_uuid":"365c9eb1e03db3748f02a81bf57fb6210ae45c10"},"cell_type":"markdown","source":"![](https://imgur.com/J2wx5Pg)\n"},{"metadata":{"trusted":true,"_uuid":"5802dfed27787cad6c69871d4382d3824a12ed86"},"cell_type":"markdown","source":"One of the biggest problems for those who want to participate in Microsoft's competition is going to be the size of this massive dataset.With a train set of almost 9 million rows and 83 columns the first thing every Kaggler will notice is that we can't load the data in a single dataframe.While there are methods to get past this ( like using the chunksize option ) i thought it would be usefull to  load a part of the data and then decide which features we are going to load when we are going to train a model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc  # Garbage Collector\nnumber_of_rows = 2000000 # We are loading 2 million rows and skip the others.\n\ndef unique_columns(dataframe): # A function to drop all columns where all rows are the same.\n    list_name = []\n    temp = list(dataframe.nunique().values)\n    for i in range(len(temp)):\n        if temp[i] == dataframe.shape[0] or temp[i] == 1:\n            list_name.append(dataframe.columns[i])\n    print('Columns that were dropped:',list_name)\n    return list_name\n\ndef get_top_features(fscore,df,model): # A function to get the features of a trained model with\n    temp=[]                            # fscore greater than the fscore parameter.\n    for i in range(len(model.feature_importances_)):\n        if model.feature_importances_[i]>=fscore:\n            temp.append(df.columns[i])\n    print('Features with fscore greater than ',fscore,':',temp)\n    return temp\n\ntrain = pd.read_csv( '../input/train.csv' , skiprows =  lambda x: x > number_of_rows)\ntrain.tail() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c6968ce43887162eadddada1dc1730237167cc8"},"cell_type":"markdown","source":"The 'MachineIdentifier' feature is a unique ID for every machine so it's not going to add much information so we can drop this column.We can also drop all columns where all rows are the same."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.drop(unique_columns(train),axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffb008c7f2a1f4b3f54702c018c3af22d2e6b9fa"},"cell_type":"markdown","source":"There are many categorical features in our dataset like 'ProductName' and 'EngineVersion'. We will have to use a Label Encoder hor those in order to be able to train a model.Before doing that let's see all the features that are of type 'object' (which means that are parsed as strings)."},{"metadata":{"trusted":true,"_uuid":"afc3d7421a4235dbc5673739d4a743bcd0d2e1fc","scrolled":true},"cell_type":"code","source":"train.select_dtypes(include = 'object').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e012b342a54ffac2d518dcac4215608eb682d93"},"cell_type":"markdown","source":"Columns AppVersion,EngineVersion, AvSigVersion and OsVersion can be turned into an meaningfull number if we take out the dot."},{"metadata":{"trusted":true,"_uuid":"03ed9d63903319e86248798fa50d150663f7e783"},"cell_type":"code","source":"train.OsVer = train.OsVer.apply(func = lambda x:x.replace(\".\",\"\")).astype(float)\ntrain.AppVersion = train.AppVersion.apply(func = lambda x:x.replace(\".\",\"\")).astype(float)\ntrain.EngineVersion = train.EngineVersion.apply(func = lambda x:x.replace(\".\",\"\")).astype(float)\ntrain.AvSigVersion = train.AvSigVersion.apply(func = lambda x:x.replace(\".\",\"\")).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d551fcdc4229e53490cfcb15fa667529bb4cd8b"},"cell_type":"markdown","source":"Census_OSEdition feature and "},{"metadata":{"_uuid":"f8d249f1dd1fbec6e1a94657dbc24a8a9bbf326b"},"cell_type":"markdown","source":"> Features Census_OSEdition and Census_OSSkuName contain the same information throughout the dataset so we can drop one and keep the other."},{"metadata":{"trusted":true,"_uuid":"3e36d655ae7fbc4eb94705fafe49190e56b3a5ab"},"cell_type":"code","source":"train.drop(['Census_OSEdition'],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55b967f2e76cf8a388eb6c150b149057e24668f5"},"cell_type":"markdown","source":"We are going to turn the rest feature columns into categorical columns and let pandas encode them ( Label Encoder was slower)."},{"metadata":{"trusted":true,"_uuid":"dc46a71ec8de3881b440205461f4d7eb4e1329aa"},"cell_type":"code","source":"for column in train.select_dtypes(include = 'object').columns:\n    print(column)\n    train[column]=train[column].astype('category')\n    train[column]=train[column].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4b13e66fe0c8ddecad4dfdf0cd18e1281cf8ffc4"},"cell_type":"code","source":"import seaborn as sns\ncorrelation_matrix = train.corr()\nmask = np.zeros_like(correlation_matrix)\nmask[np.triu_indices_from(mask)] = True \nsns.heatmap(correlation_matrix, mask = mask)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a770bad7d41e28db301b1d8ff12f44e7b16b4bf7"},"cell_type":"markdown","source":"We are only interested in the last row of the correlation matrix."},{"metadata":{"trusted":true,"_uuid":"3323b31cff4b753ee3d02a13ac0179a8a3ca7a42"},"cell_type":"code","source":"correlation_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ef9dfe94c603e48b4d42f932e58929a5684b113"},"cell_type":"code","source":"y_train = train['HasDetections']\nX_train = train.drop(['HasDetections'],axis=1)\ndel(train) # save some space\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1582f89cf0369800b4bd3322ff2d699f01a7ff0"},"cell_type":"markdown","source":"We are training a simple XGBoost model to get the feature importances."},{"metadata":{"trusted":true,"_uuid":"3240c3a93b62ac83ed0aab3e78d077eba9ff52cf"},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import plot_importance\nxgbo = xgb.XGBClassifier()\nxgbo.fit(X_train,y_train)\nplot_importance(xgbo)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e2d433bc24773d670c37e44fa031cad0d432d9c"},"cell_type":"markdown","source":"A more clean visualization."},{"metadata":{"trusted":true,"_uuid":"d2e1938e40e6dddd8722d11c29bf898acaed5168","scrolled":true},"cell_type":"code","source":"from matplotlib import pyplot\npyplot.bar(range(len(xgbo.feature_importances_)), xgbo.feature_importances_)\npyplot.show()\ncolumns_to_be_used = get_top_features(fscore = 0.04 , df = X_train, model = xgbo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"feffe8530250df6fa61086b14e2672945579a914"},"cell_type":"code","source":"del(X_train)\ndel(y_train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4849b9741ad1cfb6dadf4473fd2c57bd7f3677c"},"cell_type":"markdown","source":"Now we can load all rows and only the columns of the features with and fscore of 0.04 or greater."},{"metadata":{"trusted":true,"_uuid":"6522277604e8177bfda37b38ecfda90c1f68fcfb"},"cell_type":"code","source":"train = pd.read_csv( '../input/train.csv' , usecols = columns_to_be_used)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b91d67396d4aeb7ee7edc729c69f6f229df8a63"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}