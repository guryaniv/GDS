{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import KFold\n\nimport matplotlib.pyplot as plt\nfrom haversine import haversine\n%matplotlib inline\nimport gc \nimport os\nprint(os.listdir(\"../input\"))\n\n\n# Class, for use in pipelines, to select certain columns from a DataFrame and convert to a numpy array\n# From A. Geron: Hands-On Machine Learning with Scikit-Learn & TensorFlow, O'Reilly, 2017\n# Modified by Derek Bridge to allow for casting in the same ways as pandas.DatFrame.astype\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names, dtype=None):\n        self.attribute_names = attribute_names\n        self.dtype = dtype\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X_selected = X[self.attribute_names]\n        if self.dtype:\n            return X_selected.astype(self.dtype).values\n        return X_selected.values\n    \n# Class, for use in pipelines, to binarize nominal-valued features (while avoiding the dummy variable trap)\n# By Derek Bridge, 2017\nclass FeatureBinarizer(BaseEstimator, TransformerMixin):\n    def __init__(self, features_values):\n        self.features_values = features_values\n        self.num_features = len(features_values)\n        self.labelencodings = [LabelEncoder().fit(feature_values) for feature_values in features_values]\n        self.onehotencoder = OneHotEncoder(sparse=False,n_values=[len(feature_values) for feature_values in features_values])\n        self.last_indexes = np.cumsum([len(feature_values) - 1 for feature_values in self.features_values])\n    def fit(self, X, y=None):\n        for i in range(0, self.num_features):\n            X[:, i] = self.labelencodings[i].transform(X[:, i])\n        return self.onehotencoder.fit(X)\n    def transform(self, X, y=None):\n        for i in range(0, self.num_features):\n            X[:, i] = self.labelencodings[i].transform(X[:, i])\n            onehotencoded = self.onehotencoder.transform(X)\n        return np.delete(onehotencoded, self.last_indexes, axis=1)\n    def fit_transform(self, X, y=None):\n        onehotencoded = self.fit(X).transform(X)\n        return np.delete(onehotencoded, self.last_indexes, axis=1)\n    def get_params(self, deep=True):\n        return {\"features_values\" : self.features_values}\n    def set_params(self, **parameters):\n        for parameter, value in parameters.items():\n            self.setattr(parameter, value)\n        return self\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38ac811524c78255df22cf7be6abe32db95f7a5e"},"cell_type":"markdown","source":"<h2>Import Train Set</h2>\n<p>Importing data from the **train.csv** file.<p>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# fields\nfields = ['fare_amount', 'pickup_datetime', 'passenger_count', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n\n# Use pandas to read our training set\ndf = pd.read_csv(\"../input/train.csv\",skipinitialspace=True, usecols=fields, nrows=10000000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"4162f6bf42718d5332f0223ee58ec192df15fb9d"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"077a2a3631817b4f1ffdad5205ef5741ec95cad5"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9184d25663725ba1dd7bf0c3beee068dd27eed8"},"cell_type":"markdown","source":"<h2>Feature Engineering</h2>\n<p>Splitting **date_time** column into two separate columns. One column for the date and one column for the time of day (24hr).</p>"},{"metadata":{"trusted":true,"_uuid":"05c325af13186a5dc7c1199a079f1f701c1cb98c"},"cell_type":"code","source":"df['pickup_datetime'] = df['pickup_datetime'].str.replace(' UTC', '')\ndf['pickup_datetime'] = df['pickup_datetime'].str.replace('-', '')\ndf['pickup_datetime'] = df['pickup_datetime'].str.replace(':', '')\n\ndate_time = df['pickup_datetime'].str.split(' ', n=1, expand=True)\ndf.drop(df.columns[[1]], axis=1, inplace=True)\ndate_time.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4d00cd393170ffc77be066d0de177fc7515b856"},"cell_type":"markdown","source":"<p>Then downcasting columns to smaller sizes to save memory.</p>"},{"metadata":{"trusted":true,"_uuid":"2fb5b8fb87014abd921d47e49de4de0ccbffe914"},"cell_type":"code","source":"# converting and then downcast the new columns to smaller sizes\ndf['passenger_count'] = pd.to_numeric(df['passenger_count'], downcast='integer')\ndf['fare_amount'] = pd.to_numeric(df['fare_amount'], downcast='float')\ndf['pickup_latitude'] = pd.to_numeric(df['pickup_latitude'], downcast='float')\ndf['pickup_longitude'] = pd.to_numeric(df['pickup_longitude'], downcast='float')\ndf['dropoff_latitude'] = pd.to_numeric(df['dropoff_latitude'], downcast='float')\ndf['dropoff_longitude'] = pd.to_numeric(df['dropoff_longitude'], downcast='float')\n\n# converting and then downcasting the new columns\ndate_time[0] = pd.to_numeric(date_time[0], downcast='integer')\ndate_time[1] = pd.to_numeric(date_time[1], downcast='integer')\n\n# adding the new columns to the original dataframe\ndf['date'] = date_time[0].values\ndf[\"time\"] = date_time[1].values\n\n# free up the space used by date_time\ndate_time = None\n\n# checkout the new df\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"371d0a5d1169a26dd3880a5f298d71b7d5a8ed67"},"cell_type":"markdown","source":"<h2>Calculate Distances</h2>"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"93fdcc326c8af2b56defb340b48243e56a0b1acc"},"cell_type":"code","source":"def calc_hav(row):\n    point1= (row['pickup_latitude'],row['pickup_longitude'])\n    point2= (row['dropoff_latitude'],row['dropoff_longitude'])\n    return(haversine(point1,point2))\n\ndf['distance'] = df.apply(calc_hav, axis=1)\n\n\n# downcast to save memory\ndf['distance'] = pd.to_numeric(df['distance'], downcast='float')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9348e1366c0357999f6ebd4591998f486cd0b953"},"cell_type":"code","source":"# missing_dist = (df['distance'].isnull().sum())\n# print('Missing distance values: ', missing_dist)\n\n# # impute missing values for distance\n# df['distance'] = df['distance'].fillna(df['distance'].mean())\n# df.info()\n\n# missing_dist = (df['distance'].isnull().sum())\n# print('Missing distance values: ', missing_dist)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0a81d84c8c39f3855e4bf9c9a09a37209942f2d"},"cell_type":"markdown","source":"<h2>Data Cleaning</h2>"},{"metadata":{"trusted":true,"_uuid":"a4cf7a9d66fb49dea0a3e99436d403314e36dd1b"},"cell_type":"code","source":"# remove any negative fares, zero passengers and impossible coordinates\ndf = df[\n    (df.fare_amount > 0) & \n    (df.passenger_count > 0) &\n    (df.pickup_latitude > 40.5) &\n    (df.pickup_latitude < 41) &\n    (df.pickup_longitude > -75) &\n    (df.pickup_longitude < -73) &\n    (df.dropoff_latitude > 40.5) &\n    (df.dropoff_latitude < 41) &\n    (df.dropoff_longitude > -75) &\n    (df.dropoff_longitude < -73) &\n    (df.pickup_latitude != 0) &\n    (df.pickup_longitude != 0) &\n    (df.dropoff_latitude != 0) &\n    (df.dropoff_latitude != 0) &\n    (df.distance > 0)\n       ]\n\n# coordinates should fall within these confines \n\n#40.507754, -74.255323 # westernmost point\n#40.739021, -73.700556 # easternmost point\n#40.914862, -73.909555 # northernmost point\n#40.496218, -74.247699 # southernmost point\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4d7c99b4d47231770542c090c36c6dc907a6ff10"},"cell_type":"code","source":"df = df.reset_index(drop=True)\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f61185b9926cd9776231065ff8b7d9daec2ba275"},"cell_type":"markdown","source":"<h2>Build Pipeline</h2>"},{"metadata":{"trusted":true,"_uuid":"ea0505054c702350b683d8027c367c0cb91afc82"},"cell_type":"code","source":"# get the labels\ny = df.pop('fare_amount').values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"658a471efa73019e743320d4dcc2dcbb243403ee"},"cell_type":"code","source":"# create the object that splits the data\nkf = KFold(n_splits= 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9054d7fc5bd8198a426bcfcd2c1d5ec9d8615c50"},"cell_type":"code","source":"# features I want to select\nnumeric_features = ['passenger_count', 'date', 'time','distance', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n\n# create pipelines\nnumeric_pipeline = Pipeline([\n    (\"selector\", DataFrameSelector(numeric_features))\n])\n\n#nominal_pipeline = Pipeline([\n#   (\"selector\", DataFrameSelector(nominal_features)),\n#   (\"binarizer\", FeatureBinarizer([df[feature].unique() for feature in nominal_features]))\n#)\n\n# union the pipelines\npipeline = Pipeline([\n    #\"union\", FeatureUnion([\n    (\"numeric_pipeline\", numeric_pipeline),\n    #\"nominal_pipeline\", nominal_pipeline)])),\n    (\"estimator\", SGDRegressor(max_iter=100, verbose=1, shuffle=True, early_stopping=True, alpha=0.0003, learning_rate='invscaling', penalty='l2'))])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53a0cc971121eb9172f1cf7151608de5b3e16612"},"cell_type":"markdown","source":"<h2>Train The Model</h2>"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b5b5f0fd30e47a0288c7ac2f475dcff1b7dd0e47"},"cell_type":"code","source":"# run the pipeline\nimport warnings\nimport time\n\nstart = time.time()\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    print((np.sqrt(cross_val_score(pipeline, df, y, scoring=\"neg_mean_squared_error\", cv=kf) ** 2)).mean())\nend = time.time()\nprint((end - start)/60, 'mins')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af39e7358b0ae9040f58e46ceffcdb9e4fdf1cd6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"618807c291b0d1f4f4e20d08d462f1419d2d0039"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}