{"cells":[{"metadata":{"_cell_guid":"47f3f00d-9a12-48cf-978e-fbaf2d9a222a","_uuid":"2fd41531c269d30f8ba0eee9f139e724599a690d"},"cell_type":"markdown","source":"hello"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nfrom scipy import stats, linalg\n\nimport seaborn as sns\n\nmydata = pd.read_csv(\"../input/kc_house_data.csv\", parse_dates = ['date'])\n\n#make a table of the data!\n\n#categorize/clear the data\n\n#zip codes are strings\nmydata['zipcode'] = mydata['zipcode'].astype(str)\n#the other non ratio data are \"categories\" thanks pandas :D\nmydata['waterfront'] = mydata['waterfront'].astype('category',ordered=True)\nmydata['condition'] = mydata['condition'].astype('category',ordered=True)\nmydata['view'] = mydata['view'].astype('category',ordered=True)\nmydata['grade'] = mydata['grade'].astype('category',ordered=False)\n\n#drop ID\nmydata = mydata.drop(['id', 'date'],axis=1)\nmydata = mydata.dropna()\nmydata = mydata[mydata.bedrooms < 15]\n#display a table of all the data for refernce (handy)\ndf = pd.DataFrame(data = mydata)\nstr_list = [] # empty list to contain columns with strings (words)\nfor colname, colvalue in mydata.iteritems():\n    if type(colvalue[1]) == str:\n         str_list.append(colname)\n# Get to the numeric columns by inversion            \nnum_list = mydata.columns.difference(str_list) \n# Create Dataframe containing only numerical features\nnumData = mydata[num_list]\n\n#and then remove more stuff\ninterestingCol =['price','bedrooms','bathrooms','sqft_above','sqft_living']\nnumData = numData[interestingCol]\noriginalData = numData.copy()\n\n#reduce the number of data points\nnumData = numData.sample(n=11000, random_state = 13)\n#figure out what the standardized million dollars is and save that\noneMillSTD = (numData['price'].median()-numData['price'].mean())/numData['price'].std()\nnumData =(numData - numData.mean()) / numData.std()\n\n\n\n","execution_count":49,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"Okay, we just imported a bunch of stuff and cleared it, let's show what we have so we're on the same page"},{"metadata":{"_cell_guid":"71366c12-a4a4-43bf-9353-ea5f49fc0437","_uuid":"f117c5be98608e214a652eebdbeed14105223456","trusted":true},"cell_type":"code","source":"numData.fillna(method='backfill', inplace=True)\n\nnumData.describe()\noriginalData.head()\n\n","execution_count":50,"outputs":[]},{"metadata":{"_cell_guid":"1a2a5062-2c91-4c38-aff0-355a564c851b","_uuid":"93c54695c8ba0215b82a1a1e3d43c8448589d77c"},"cell_type":"markdown","source":"cool! Let's prepare to do a decision tree!\n\nWe need:\n* X\n* y\n* attributeNames\n\n**classification**\nAnd finally, some way to categorize the data (since we're working with a regression problem and we need to find a way to do a categorization).\n\nLet's do million dollar homes! (with the two classes being price < 1.0 e+6 and >= e+6)\n"},{"metadata":{"_cell_guid":"6ee5585f-65da-404f-8019-ac71adab4d49","_uuid":"8ba6bcaca81a7907fe4713413ea9b86e1437f0b8","collapsed":true,"trusted":true},"cell_type":"code","source":"X = numData.drop(['price'],axis=1)\ny = numData['price']\n\nattributeNames = list(X)\n\nclassNames = ['MillionDollarHome','notMDH']\n\nfrom sklearn import model_selection\nX, X_testglb, y, y_testglb = model_selection.train_test_split(X,y, test_size = (1/11),random_state = 42)\n\nN, M = X.shape\nXpd = X.copy()\nX = X.as_matrix()\n","execution_count":51,"outputs":[]},{"metadata":{"_cell_guid":"934347b1-d7ba-47e3-9d9c-169e63e3127c","_uuid":"4bd5213c0475ef665a8892215f4259d87f7f9c08"},"cell_type":"markdown","source":"<h2>Decision Tree</h2>\nuse the decision tree to predict if the home is a million dollars or not!"},{"metadata":{"_cell_guid":"9dabf724-b220-4e73-baa6-0e89d6d9718d","_uuid":"2288cc39538758ab01e377f18917e8fb72912a7a","trusted":true},"cell_type":"code","source":"from sklearn import tree\n\n#first we have to create a new y for million dollars or not!\ndef millionDollars(money):\n    #returns false if the price is less than a million\n    #returns true if the price is equal to or greater than a million dollars\n    if(money < oneMillSTD):\n        return 0\n    else:\n        return 1\n\n#create the new classification data set\ny_dtc = y.apply(millionDollars)\n\ny_dtc = y_dtc.as_matrix()\nyCopy = y.copy()\ny = y_dtc\n\n#use the SKLEARN decision tree maker\ndtc = tree.DecisionTreeClassifier(criterion='gini', min_samples_split=1000)\ndtc = dtc.fit(X,y_dtc)\n\n#visualize\n#code borroed from \n#https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\n\ndef drawTree(datree):\n    with open(\"tree1.dot\", 'w') as f:\n         f = tree.export_graphviz(datree,\n                              out_file=f,\n                              rounded = True,\n                              filled= True )\n    #Convert .dot to .png to allow display in web notebook\n    check_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n    return(\"tree1.png\")\n    \n#use the function to draw the tree\nPImage(drawTree(dtc))\n\n###some code qualifying the data\n\n","execution_count":52,"outputs":[]},{"metadata":{"_cell_guid":"3f230f67-262a-4e4b-bf97-346e52747485","_uuid":"742c1d8d74ad5ab3747515568ca26e4fca5561bd"},"cell_type":"markdown","source":"<h3>Let's optimize the pruning level of the code</h3>"},{"metadata":{"_cell_guid":"659128d8-357c-41ad-a679-1c57b5c39e75","_uuid":"2870c87b2853434e5ee363b0819670c5a24d5359","scrolled":true,"trusted":true},"cell_type":"code","source":"### some optimization code using cross validation!?\n# Tree complexity parameter - constraint on maximum depth\ntc = np.arange(2, 41, 1)\n\n# K-fold crossvalidation\nK = 10\nCV = model_selection.KFold(n_splits=K,shuffle=True)\n\n# Initialize variable\nError_train = np.empty((len(tc),K))\nError_test = np.empty((len(tc),K))\n\nk=0\nfor train_index, test_index in CV.split(X):\n    print('Computing CV fold: {0}/{1}..'.format(k+1,K))\n\n    # extract training and test set for current CV fold\n    X_train, y_train = X[train_index,:], y[train_index]\n    X_test, y_test = X[test_index,:], y[test_index]\n\n    for i, t in enumerate(tc):\n        # Fit decision tree classifier, Gini split criterion, different pruning levels\n        dtc = tree.DecisionTreeClassifier(criterion='gini', max_depth=t)\n        dtc = dtc.fit(X_train,y_train.ravel())\n        y_est_test = dtc.predict(X_test)\n        y_est_train = dtc.predict(X_train)\n        # Evaluate misclassification rate over train/test data (in this CV fold)\n        misclass_rate_test = sum(np.abs(y_est_test - y_test)) / float(len(y_est_test))\n        misclass_rate_train = sum(np.abs(y_est_train - y_train)) / float(len(y_est_train))\n        Error_test[i,k], Error_train[i,k] = misclass_rate_test, misclass_rate_train\n    k+=1\n\n    \n\nplt.boxplot(Error_test.T)\nplt.xlabel('Model complexity (max tree depth)')\nplt.ylabel('Test error across CV folds, K={0})'.format(K))\nplt.show()\n\n\nplt.plot(tc, Error_train.mean(1))\nplt.plot(tc, Error_test.mean(1))\nError_tot = Error_train.mean(1) + Error_test.mean(1)\nplt.plot(tc, Error_tot)\nplt.xlabel('Model complexity (max tree depth)')\nplt.ylabel('Error (misclassification rate, CV K={0})'.format(K))\nplt.legend(['Error_train','Error_test','Error_total'])","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"ebd6cf7a-a728-400e-87d4-d660ee999d95","_uuid":"fb0d4a4d58f496d514515f2bb3f9a6839202b01c"},"cell_type":"markdown","source":"<h3>So let's show the least error prone tree!</h3>\n\n"},{"metadata":{"_cell_guid":"7991578f-7616-4291-9183-a16c60f8d758","_uuid":"55da9d2e6976d085acaaa4514fdc1ebf7c501adc","trusted":true},"cell_type":"code","source":"optimalDepth = np.argmin(Error_tot)\nprint(optimalDepth)\n\nCV = model_selection.KFold(n_splits=2,shuffle=True)\n\nfor train_index, test_index in CV.split(X):\n    #print('Computing CV fold: {0}/{1}..'.format(k+1,K))\n\n    # extract training and test set for current CV fold\n    X_train, y_train = X[train_index,:], y[train_index]\n    X_test, y_test = X[test_index,:], y[test_index]\n\n    dtc = tree.DecisionTreeClassifier(criterion='gini', max_depth=optimalDepth)\n    dtc = dtc.fit(X_train,y_train.ravel())\n    y_est_test = dtc.predict(X_test)\n    y_est_train = dtc.predict(X_train)\n\n\n#y_est_test = dtc.predict(X_testglb)\n\n#using the confusion matrix not actually graphically\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score\n\ncm = confusion_matrix(y_test,y_est_test)\nprint('Confusion matrix: \\n',cm)\nsns.heatmap(cm, annot=True)\nplt.title('Optimal Decision Tree confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\npercision = (precision_score(y_test,y_est_test))\nrecall =(recall_score(y_test,y_est_test))\nprint( \"%.2f\" % percision)\nprint( \"%.2f\" % recall)\n#PImage(drawTree(dtc))\n\ny_optdtc = y_est_test","execution_count":81,"outputs":[]},{"metadata":{"_cell_guid":"ff4140f6-5d9c-4af2-8802-d1d0a40a5b31","_uuid":"76d4df1af1c780dad6ac2861fe82418918938a59"},"cell_type":"markdown","source":"<h2>Let's try nearest neighbors</h2>"},{"metadata":{"_cell_guid":"55739995-d038-4d5d-b798-73087422db6d","_uuid":"9a72e4edc89641a7a4850a9370a785c9e0390feb","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"275a4bff-7af6-4809-a579-df73ad188f6f","_uuid":"2cc642088ab27e00d9f3468f8f20dc82f1abdcfd","trusted":true},"cell_type":"code","source":"from sklearn import neighbors\nfrom sklearn import model_selection\n\n#max number of neighbors\nL = 40\n\nCV = model_selection.KFold(n_splits=40)\nerrors = np.zeros((N,L))\ni=0\n\nfor train_index, test_index in CV.split(X):\n    print('Crossvalidation fold: {0}/{1}'.format(i+1,L))    \n    \n    # extract training and test set for current CV fold\n    X_train = X[train_index,:]\n    y_train = y_dtc[train_index]\n    X_test = X[test_index,:]\n    y_test = y_dtc[test_index]\n\n    # Fit classifier and classify the test points (consider 1 to 40 neighbors)\n    for l in range(1,L+1):\n        knclassifier = neighbors.KNeighborsClassifier(n_neighbors=l);\n        knclassifier.fit(X_train, y_train);\n        y_est = knclassifier.predict(X_test);\n        errors[i,l-1] = np.sum(y_est[0]!=y_test[0])\n\n    i+=1\n\nerrorOfKNN = 100*sum(errors,0)/N\nplt.plot(errorOfKNN)\nplt.xlabel('Number of neighbors')\nplt.ylabel('Classification error rate (%)')\n","execution_count":55,"outputs":[]},{"metadata":{"_cell_guid":"fe006c9c-bcb2-4ef5-8d9e-59383f937ebc","_uuid":"da5e07f6f5a6c4db378f9264e73c8b2bf22dffdf","trusted":true},"cell_type":"code","source":"optimalK = np.argmin(errorOfKNN)\nprint(optimalK)\nprint(min(errorOfKNN*100))\n","execution_count":56,"outputs":[]},{"metadata":{"_cell_guid":"96477767-384b-4ec0-8ec8-fade7026fa75","_uuid":"ef5f9e8809b8ffeb3389c0844445bcd2f4a0260e"},"cell_type":"markdown","source":"So we continue with that above state number of neighbors"},{"metadata":{"_cell_guid":"109600d6-3ff1-426b-a988-6590fb59dd6e","_uuid":"fdb78adfbd304a0ddca13ed8e129c695ea08d74d","trusted":true},"cell_type":"code","source":"knclassifier = neighbors.KNeighborsClassifier(n_neighbors=5);\nknclassifier.fit(X, y);\n\nCV = model_selection.KFold(n_splits=2)\ni = 0\nfor train_index, test_index in CV.split(X):\n    print('Crossvalidation fold: {0}/{1}'.format(i+1,L))    \n    \n    # extract training and test set for current CV fold\n    X_train = X[train_index,:]\n    y_train = y_dtc[train_index]\n    X_test = X[test_index,:]\n    y_test = y_dtc[test_index]\n\n    # Fit classifier and classify the test points (consider 1 to 40 neighbors)\n    \n    knclassifier = neighbors.KNeighborsClassifier(n_neighbors=optimalK);\n    knclassifier.fit(X_train, y_train);\n    y_est = knclassifier.predict(X_test);\n    cm = confusion_matrix(y_test,y_est)\n    print('Confusion matrix: \\n',cm)\n    i += 1\n\n\ny_optKnn = y_est\nsns.heatmap(cm, annot=True)\nplt.title('Optimal Nearest Neighbor confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":76,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e1abfd0b54fe1a5eb919da74f3f9a569df46a80"},"cell_type":"code","source":"percision = (precision_score(y_test,y_est))\nrecall =(recall_score(y_test,y_est))\nprint( \"%.2f\" % percision)\nprint( \"%.2f\" % recall)","execution_count":58,"outputs":[]},{"metadata":{"_cell_guid":"30402a63-c368-4597-bbdc-6458a849b8c7","_uuid":"835cc8d2d89195a5042ecb412827d3427c4c2b67","collapsed":true},"cell_type":"markdown","source":"Now it's time for logistic regression"},{"metadata":{"_cell_guid":"f93b9cc9-e02c-477b-a442-728481d2e027","_uuid":"b710d8dd789ba01d2d88664ee41be1b3373e1ab5","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n  \n    \n","execution_count":59,"outputs":[]},{"metadata":{"_cell_guid":"1232ec2e-fd5d-4147-ae40-eeb5fb4887a3","_uuid":"4107729fd1850aab1d20f1418a146cdb22fe9449","scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.feature_selection import RFE, RFECV\n\n#redfine X to get a pandas dictionary\n\n\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\n#print('Selected features: %s' % list(attributeNames[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()\n","execution_count":60,"outputs":[]},{"metadata":{"_cell_guid":"4872c0be-5181-477c-a3ad-58581a0a1726","_uuid":"2bd4d2fb3990fcc9e2b654f0c944875c777e784f","trusted":true},"cell_type":"code","source":"print(rfecv.support_)","execution_count":61,"outputs":[]},{"metadata":{"_cell_guid":"c4a24aec-ebd9-46d2-b97c-086a261b91de","_uuid":"fd52204306b3549adcbeb36f79074b0270487197","collapsed":true,"trusted":false},"cell_type":"code","source":"optimalCol = ['bedrooms', 'sqft_above','sqft_living']\nXreg = Xpd[optimalCol].as_matrix()","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"efce3278-0177-447d-bd15-d7530e51eb52","_uuid":"1b0126bbcb520d3d720586dfe2845f514a1fabcb","trusted":true},"cell_type":"code","source":"logModel = LogisticRegression()\n\n#max number of neighbors\nK = 40\n\nCV = model_selection.KFold(n_splits=2)\nerrors = np.zeros((N,L))\nError_logreg = np.empty((K,1))\n\n\nk=0\nfor train_index, test_index in CV.split(Xreg):\n    print('CV-fold {0} of {1}'.format(k+1,K))\n    \n    # extract training and test set for current CV fold\n    X_train = Xreg[train_index,:]\n    y_train = y[train_index]\n    X_test = Xreg[test_index,:]\n    y_test = y[test_index]\n    model = LogisticRegression(C=N)\n    model.fit(X_train,y_train)\n    y_logreg = model.predict(X_test)\n    Error_logreg[k] = 100*(y_logreg!=y_test).sum().astype(float)/len(y_test)\n    cm = confusion_matrix(y_test,y_logreg)\n    print('Confusion matrix: \\n',cm)\n    k+=1\n\n\ny_globaltest = y_test\ny_optlog = y_logreg\nsns.heatmap(cm, annot=True)\nplt.title('Optimal Nearest Neighbor confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n","execution_count":77,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9580beda2fdbd27ae3d999635b0f0fceaf20529b"},"cell_type":"code","source":"percision = (precision_score(y_test,y_logreg))\nrecall =(recall_score(y_test,y_logreg))\nprint( \"%.2f\" % percision)\nprint( \"%.2f\" % recall)","execution_count":63,"outputs":[]},{"metadata":{"_cell_guid":"2115f98b-0248-4ef0-970c-978ba497e794","_uuid":"48a80f95201033c7d7d14aff1444c81480d66277"},"cell_type":"markdown","source":"visualize the logistic regression"},{"metadata":{"_cell_guid":"acc9f43f-6244-489c-9e52-4d8a8d285d8b","_uuid":"681d3492f87b98784658c6b04dbb9effe9793773","collapsed":true,"trusted":true},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\n\nplotCol = ['sqft_above', 'sqft_living']\nXplot = Xpd[plotCol].as_matrix()\n\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\nh = .02  # step size in the mesh","execution_count":64,"outputs":[]},{"metadata":{"_cell_guid":"e165087c-023e-4f0a-a6bd-0fed9c2d5d79","_uuid":"0f4396bc84b735e5babda7de32151b27ad98483a","trusted":true},"cell_type":"code","source":"model.fit(Xplot,y)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = Xplot[:, 0].min() - .5, Xplot[:, 0].max() + .5\ny_min, y_max = Xplot[:, 1].min() - .5, Xplot[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(10,6))\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(Xplot[:, 0], Xplot[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\nplt.xlabel('sqft_above')\nplt.ylabel('sqft_living')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\n\nplt.show()","execution_count":65,"outputs":[]},{"metadata":{"_cell_guid":"a01a320d-9868-415b-805e-675b6ab57193","_uuid":"53f11ca9586ea5fe54ab390200462cc76e353694","collapsed":true},"cell_type":"markdown","source":"visualization for nearest neighbors\n"},{"metadata":{"_cell_guid":"0ff04b36-1d75-4c3b-92f3-87239e2f21dd","_uuid":"e0839fe94bd3ee7da9177a41c0027b818a2651c8","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9357db26-2040-4725-bb36-6ae455b2a967","_uuid":"31537a126589f70e0311dd6110be8bfad27b8761","collapsed":true,"trusted":false},"cell_type":"code","source":"\n\nclf = neighbors.KNeighborsClassifier(n_neighbors = 5)\nclf.fit(Xplot, y)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = Xplot[:, 0].min() - 1, Xplot[:, 0].max() + 1\ny_min, y_max = Xplot[:, 1].min() - 1, Xplot[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\nnp.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(Xplot[:, 0], Xplot[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xlabel(\"sqft_living\")\nplt.ylabel(\"bedrooms\")\nplt.title(\"House Price Classifier\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89dbf9d4-1598-4489-9826-d493fa173e60","_uuid":"15e016d95a908ccfd9e9c60afaf9104dbf90363e","collapsed":true},"cell_type":"markdown","source":"Final part of the report! let's do some things to clean up!\n"},{"metadata":{"trusted":false,"_uuid":"1c6b48ca6b57187b1417f044cbe53e8acef93056"},"cell_type":"code","source":"import sklearn.linear_model as lm\nK = 40\nCV = model_selection.KFold(n_splits=K,shuffle=True)\n\n# Initialize variables\nError_logreg = np.empty((K,1))\nError_nearestn = np.empty((K,1))\nn_tested=0\n\n\n\nk=0\nfor train_index, test_index in CV.split(X,y):\n    print('CV-fold {0} of {1}'.format(k+1,K))\n    \n    # extract training and test set for current CV fold\n    X_train = X[train_index,:]\n    y_train = y[train_index]\n    X_test = X[test_index,:]\n    y_test = y[test_index]\n    X_trainreg = Xreg[train_index,:]\n    X_testreg = Xreg[test_index,:]\n\n    # Fit and evaluate Logistic Regression classifier\n    model = lm.logistic.LogisticRegression(C=N)\n    model = model.fit(X_trainreg, y_train)\n    y_logreg = model.predict(X_testreg)\n    Error_logreg[k] = 100*(y_logreg!=y_test).sum().astype(float)/len(y_test)\n    \n    # Fit and evaluate Decision Tree classifier\n    model2 = neighbors.KNeighborsClassifier(n_neighbors=optimalK);\n    model2 = model2.fit(X_train, y_train)\n    y_nearestn = model2.predict(X_test)\n    Error_nearestn[k] = 100*(y_nearestn!=y_test).sum().astype(float)/len(y_test)\n\n    k+=1\n\n# Test if classifiers are significantly different using methods in section 9.3.3\n# by computing credibility interval. Notice this can also be accomplished by computing the p-value using\n# [tstatistic, pvalue] = stats.ttest_ind(Error_logreg,Error_nearestn)\n# and test if the p-value is less than alpha=0.05. \nz = (Error_logreg-Error_nearestn)\nzb = z.mean()\nnu = K-1\nsig =  (z-zb).std()  / np.sqrt(K-1)\nalpha = 0.05\n\nzL = zb + sig * stats.t.ppf(alpha/2, nu);\nzH = zb + sig * stats.t.ppf(1-alpha/2, nu);\n\nif zL <= 0 and zH >= 0 :\n    print('Classifiers are not significantly different')        \nelse:\n    print('Classifiers are significantly different.')\n    \n# Boxplot to compare classifier error distributions\nplt.figure()\nplt.boxplot(np.concatenate((Error_logreg, Error_nearestn),axis=1))\nplt.xlabel('Logistic Regression   vs.   Nearest Neighbor')\nplt.ylabel('Cross-validation error [%]')\n\nplt.show()\n","execution_count":32,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7dc7a787320365fedb12425ff9f390745df3067d"},"cell_type":"code","source":"from scipy import stats, integrate\nsns.distplot(Error_logreg, label=\"Logistic Regression\", hist = False, rug = True)\nsns.distplot(Error_nearestn, label =\"Nearest Neighbor\", hist = False, rug = True)\n\n","execution_count":45,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c21c13cbba9475cbfca1efef0ad521ff4797b728"},"cell_type":"code","source":"print(sum(y))\nprint(10000-sum(y))","execution_count":47,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbee0f4e5c1c0745bd3c970f878ef725e437c348"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\nfpr,tpr,_ = roc_curve(y_globaltest.ravel(),y_est_test.ravel())\n\n#y_optlog = y_logreg\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: Decision Tree')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":82,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e359543f511ab3e2d8df422f2e92ab48e16cd7e1"},"cell_type":"code","source":"print(y_logreg)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3c62eb442e30000348617f5a9662560dd61997f7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}