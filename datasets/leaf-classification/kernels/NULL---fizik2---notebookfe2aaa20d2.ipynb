{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "92573c64-943b-a7af-c782-79140887a0ae"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# A large amount of the data loading code is based on najeebkhan's kernel\n",
        "# Check it out at https://www.kaggle.com/najeebkhan/leaf-classification/neural-network-through-keras\n",
        "root = '../input'\n",
        "np.random.seed(2016)\n",
        "split_random_state = 7\n",
        "split = .9\n",
        "\n",
        "\n",
        "def load_numeric_training(standardize=True):\n",
        "    \"\"\"\n",
        "    Loads the pre-extracted features for the training data\n",
        "    and returns a tuple of the image ids, the data, and the labels\n",
        "    \"\"\"\n",
        "    # Read data from the CSV file\n",
        "    data = pd.read_csv(os.path.join(root, 'train.csv'))\n",
        "    ID = data.pop('id')\n",
        "\n",
        "    # Since the labels are textual, so we encode them categorically\n",
        "    y = data.pop('species')\n",
        "    y = LabelEncoder().fit(y).transform(y)\n",
        "    # standardize the data by setting the mean to 0 and std to 1\n",
        "    X = StandardScaler().fit(data).transform(data) if standardize else data.values\n",
        "\n",
        "    return ID, X, y\n",
        "\n",
        "\n",
        "def load_numeric_test(standardize=True):\n",
        "    \"\"\"\n",
        "    Loads the pre-extracted features for the test data\n",
        "    and returns a tuple of the image ids, the data\n",
        "    \"\"\"\n",
        "    test = pd.read_csv(os.path.join(root, 'test.csv'))\n",
        "    ID = test.pop('id')\n",
        "    # standardize the data by setting the mean to 0 and std to 1\n",
        "    test = StandardScaler().fit(test).transform(test) if standardize else test.values\n",
        "    return ID, test\n",
        "\n",
        "\n",
        "def resize_img(img, max_dim=96):\n",
        "    \"\"\"\n",
        "    Resize the image to so the maximum side is of size max_dim\n",
        "    Returns a new image of the right size\n",
        "    \"\"\"\n",
        "    # Get the axis with the larger dimension\n",
        "    max_ax = max((0, 1), key=lambda i: img.size[i])\n",
        "    # Scale both axes so the image's largest dimension is max_dim\n",
        "    scale = max_dim / float(img.size[max_ax])\n",
        "    return img.resize((int(img.size[0] * scale), int(img.size[1] * scale)))\n",
        "\n",
        "\n",
        "def load_image_data(ids, max_dim=96, center=True):\n",
        "    \"\"\"\n",
        "    Takes as input an array of image ids and loads the images as numpy\n",
        "    arrays with the images resized so the longest side is max-dim length.\n",
        "    If center is True, then will place the image in the center of\n",
        "    the output array, otherwise it will be placed at the top-left corner.\n",
        "    \"\"\"\n",
        "    # Initialize the output array\n",
        "    # NOTE: Theano users comment line below and\n",
        "    X = np.empty((len(ids), max_dim, max_dim, 1))\n",
        "    # X = np.empty((len(ids), 1, max_dim, max_dim)) # uncomment this\n",
        "    for i, idee in enumerate(ids):\n",
        "        # Turn the image into an array\n",
        "        x = resize_img(load_img(os.path.join(root, 'images', str(idee) + '.jpg'), grayscale=True), max_dim=max_dim)\n",
        "        x = img_to_array(x)\n",
        "        # Get the corners of the bounding box for the image\n",
        "        # NOTE: Theano users comment the two lines below and\n",
        "        length = x.shape[0]\n",
        "        width = x.shape[1]\n",
        "        # length = x.shape[1] # uncomment this\n",
        "        # width = x.shape[2] # uncomment this\n",
        "        if center:\n",
        "            h1 = int((max_dim - length) / 2)\n",
        "            h2 = h1 + length\n",
        "            w1 = int((max_dim - width) / 2)\n",
        "            w2 = w1 + width\n",
        "        else:\n",
        "            h1, w1 = 0, 0\n",
        "            h2, w2 = (length, width)\n",
        "        # Insert into image matrix\n",
        "        # NOTE: Theano users comment line below and\n",
        "        X[i, h1:h2, w1:w2, 0:1] = x\n",
        "        # X[i, 0:1, h1:h2, w1:w2] = x  # uncomment this\n",
        "    # Scale the array values so they are between 0 and 1\n",
        "    return np.around(X / 255.0)\n",
        "\n",
        "\n",
        "def load_train_data(split=split, random_state=None):\n",
        "    \"\"\"\n",
        "    Loads the pre-extracted feature and image training data and\n",
        "    splits them into training and cross-validation.\n",
        "    Returns one tuple for the training data and one for the validation\n",
        "    data. Each tuple is in the order pre-extracted features, images,\n",
        "    and labels.\n",
        "    \"\"\"\n",
        "    # Load the pre-extracted features\n",
        "    ID, X_num_tr, y = load_numeric_training()\n",
        "    # Load the image data\n",
        "    X_img_tr = load_image_data(ID)\n",
        "    # Split them into validation and cross-validation\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, train_size=split, random_state=random_state)\n",
        "    train_ind, test_ind = next(sss.split(X_num_tr, y))\n",
        "    X_num_val, X_img_val, y_val = X_num_tr[test_ind], X_img_tr[test_ind], y[test_ind]\n",
        "    X_num_tr, X_img_tr, y_tr = X_num_tr[train_ind], X_img_tr[train_ind], y[train_ind]\n",
        "    return (X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val)\n",
        "\n",
        "\n",
        "def load_test_data():\n",
        "    \"\"\"\n",
        "    Loads the pre-extracted feature and image test data.\n",
        "    Returns a tuple in the order ids, pre-extracted features,\n",
        "    and images.\n",
        "    \"\"\"\n",
        "    # Load the pre-extracted features\n",
        "    ID, X_num_te = load_numeric_test()\n",
        "    # Load the image data\n",
        "    X_img_te = load_image_data(ID)\n",
        "    return ID, X_num_te, X_img_te\n",
        "\n",
        "print('Loading the training data...')\n",
        "(X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val) = load_train_data(random_state=split_random_state)\n",
        "y_tr_cat = to_categorical(y_tr)\n",
        "y_val_cat = to_categorical(y_val)\n",
        "print('Training data loaded!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f85cf76e-4ee4-03a1-2f10-98da59bb2138"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0dcc5f0d-a3bd-1649-0cc5-c13b9123732a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as scipy\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2ef3f9c9-f346-4669-f3df-a7b5d14f0c3a"
      },
      "outputs": [],
      "source": [
        "def iterate_minibatches(inputs, inputs_img, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(inputs))\n",
        "        np.random.shuffle(indices)\n",
        "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], inputs_img[excerpt], targets[excerpt]\n",
        "\n",
        "def weight_variable(shape):\n",
        "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def bias_variable(shape):\n",
        "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
        "    \"\"\"Reusable code for making a simple neural net layer.\n",
        "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
        "    It also sets up name scoping so that the resultant graph is easy to read,\n",
        "    and adds a number of summary ops.\n",
        "    \"\"\"\n",
        "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
        "    weights = weight_variable([input_dim, output_dim])\n",
        "    biases = bias_variable([output_dim])\n",
        "    preactivate = tf.matmul(input_tensor, weights) + biases\n",
        "    activations = act(preactivate, name='activation')\n",
        "    return activations\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1], padding='SAME')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aa806af0-840b-ccf0-92cb-16d33b4a71f9"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../input/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "78dae40f-1891-5053-8c59-1e940900d61d"
      },
      "outputs": [],
      "source": [
        "num_classes = 99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "adf4a8b9-2818-cd31-095a-94728bf3e2a6"
      },
      "outputs": [],
      "source": [
        "train_df = df\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6bebfc3f-e06b-b931-49a2-41b5fd363540"
      },
      "outputs": [],
      "source": [
        "print(train_df.species.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fab2347c-6de3-71ef-30f0-7849aaecf5b8"
      },
      "outputs": [],
      "source": [
        "species = train_df['species'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0d2b57d-888c-ec99-3754-cf65caeaa749"
      },
      "outputs": [],
      "source": [
        "species.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c343bf35-e6ad-e617-eeda-245fb7abde66"
      },
      "outputs": [],
      "source": [
        "species[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69e791ef-cc12-bcec-320f-ce680c28d09c"
      },
      "outputs": [],
      "source": [
        "print(Y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a05b4edd-16b7-466d-cd01-d439e4ccec36"
      },
      "outputs": [],
      "source": [
        "X_train = train_df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6f52509f-5038-822f-c435-8e619b6672b8"
      },
      "outputs": [],
      "source": [
        "X_train = scipy.delete(X_train, 0, 1) # Remove first and second columns\n",
        "X_train = scipy.delete(X_train, 0, 1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2fab3e93-cb8d-2bb6-8ec4-c8498478c4dc"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aa199b79-6242-ef56-a1a4-8aa4ab6668e3"
      },
      "outputs": [],
      "source": [
        "X_train, X_val = X_train[:-200], X_train[-200:]\n",
        "X_train, X_test = X_train[:-100], X_train[-100:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "253037b0-497e-807e-1d64-7ee07165093a"
      },
      "outputs": [],
      "source": [
        "Y_train, Y_val = Y_train[:-200], Y_train[-200:]\n",
        "Y_train, Y_test = Y_train[:-100], Y_train[-100:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c28ca8a9-70af-0f95-ccc7-3b4216668436"
      },
      "outputs": [],
      "source": [
        "sess = tf.InteractiveSession()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "46a42469-329b-8c54-814a-6d3cf3202fc9"
      },
      "outputs": [],
      "source": [
        "## Create the model\n",
        "num_classes = 99\n",
        "x_feature = tf.placeholder(tf.float32, [None, 192])\n",
        "x_img = tf.placeholder(tf.float32, [None, 96, 96, 1])\n",
        "y_ = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "net = tf.reshape(x_img, [-1, 96*96*1])\n",
        "net = nn_layer(net, 96*96*1, 1024, 'layer1')\n",
        "net = nn_layer(net, 1024, 512, 'layer2')\n",
        "net = nn_layer(net, 512, 256, 'layer3')\n",
        "net = tf.concat(1, [net, x_feature])\n",
        "net = tf.nn.dropout(net, keep_prob)\n",
        "net = nn_layer(net, 256+192, 512, 'layer4')\n",
        "net = tf.nn.dropout(net, keep_prob)\n",
        "net = nn_layer(net, 512, 1024, 'layer5')\n",
        "y = nn_layer(net, 1024, num_classes, 'layer6', act=tf.identity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c15d3d9-128d-0a06-03e4-aa72dd27db32"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
        "train_step = tf.train.AdamOptimizer(\n",
        "    learning_rate=1e-4, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\\\n",
        ".minimize(cross_entropy)\n",
        "\n",
        "prediction = tf.nn.softmax(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca172a9b-763a-0999-4aab-a94352decee5"
      },
      "outputs": [],
      "source": [
        "tf.global_variables_initializer().run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d5d3d99-5439-c14c-63fa-339ad1094810"
      },
      "outputs": [],
      "source": [
        "min_loss = 100\n",
        "saver = tf.train.Saver()\n",
        "save_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e11848c5-0f81-fc6a-9d1d-a1b0525f588c"
      },
      "outputs": [],
      "source": [
        "#X_num_tr, X_img_tr, y_tr, X_num_val, X_img_val, y_val\n",
        "\n",
        "\n",
        "for i in range(50):\n",
        "    if i % 5 == 0:\n",
        "        print(i)\n",
        "    for batch in iterate_minibatches(X_num_tr, X_img_tr, y_tr_cat, 50, shuffle=True):\n",
        "        batch_xs, batch_xs_img, batch_ys = batch\n",
        "        sess.run(train_step, feed_dict={x_img: batch_xs_img, x_feature: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
        "\n",
        "        # Test trained model\n",
        "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    print(\"tr:{}\".format(sess.run(accuracy, feed_dict={x_img: X_img_tr, \n",
        "                                                       x_feature: X_num_tr, y_: y_tr_cat, keep_prob: 1.0})))\n",
        "    print(\"val:{}\".format(sess.run(accuracy, feed_dict={x_img: X_img_val, \n",
        "                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})))\n",
        "    cur_loss = sess.run(cross_entropy, feed_dict={x_img: X_img_val, \n",
        "                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})\n",
        "    if cur_loss < min_loss:\n",
        "        min_loss = cur_loss\n",
        "        save_path = saver.save(sess, 'my-model')\n",
        "        print(\"!!!NEW MIN LOSS {}. Saved at {}\".format(cur_loss, save_path))\n",
        "\n",
        "    else: \n",
        "        print(\"val_los:{}\".format(cur_loss))\n",
        "\n",
        "print(\"val:{}\".format(sess.run(accuracy, feed_dict={x_img: X_img_val, \n",
        "                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c57299b2-8f91-073a-0f3e-1e7b54e976a5"
      },
      "outputs": [],
      "source": [
        "with tf.Session() as sess2:\n",
        "    # Initialize variables\n",
        "    #sess.run(init)\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    # Restore model weights from previously saved model\n",
        "    saver.restore(sess2, save_path)\n",
        "    print(\"Model restored from file: %s\" % save_path)\n",
        "\n",
        "    loss = sess2.run(cross_entropy, feed_dict={x_img: X_img_val, \n",
        "                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})\n",
        "    \n",
        "    print(\"val:{}\".format(loss))\n",
        "    \n",
        "    \n",
        "    LABELS = sorted(pd.read_csv(os.path.join(root, 'train.csv')).species.unique())\n",
        "\n",
        "\n",
        "    index, test, X_img_te = load_test_data()\n",
        "    yPred_proba = sess2.run(prediction, feed_dict={x_img: X_img_te, \n",
        "                                                      x_feature: test, keep_prob: 1.0})\n",
        "\n",
        "    # Converting the test predictions in a dataframe as depicted by sample submission\n",
        "    yPred = pd.DataFrame(yPred_proba,index=index,columns=LABELS)\n",
        "\n",
        "    print('Creating and writing submission...')\n",
        "    #fp = open('submit.csv', 'w')\n",
        "    #fp.write(yPred.to_csv())\n",
        "    #print('Finished writing submission')\n",
        "    # Display the submission\n",
        "    yPred.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "653ce417-39b0-3ed8-f26e-10e863264c96"
      },
      "outputs": [],
      "source": [
        "loss = sess.run(cross_entropy, feed_dict={x_img: X_img_val, \n",
        "                                                        x_feature: X_num_val, y_: y_val_cat, keep_prob: 1.0})\n",
        "print(\"val:{}\".format(loss))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fcee3a3b-c105-398f-e4e1-993456275f80"
      },
      "outputs": [],
      "source": [
        "LABELS = sorted(pd.read_csv(os.path.join(root, 'train.csv')).species.unique())\n",
        "\n",
        "\n",
        "index, test, X_img_te = load_test_data()\n",
        "yPred_proba = sess.run(prediction, feed_dict={x_img: X_img_te, \n",
        "                                                  x_feature: test, keep_prob: 1.0})\n",
        "\n",
        "# Converting the test predictions in a dataframe as depicted by sample submission\n",
        "yPred = pd.DataFrame(yPred_proba,index=index,columns=LABELS)\n",
        "\n",
        "print('Creating and writing submission...')\n",
        "fp = open('submit.csv', 'w')\n",
        "fp.write(yPred.to_csv())\n",
        "print('Finished writing submission')\n",
        "# Display the submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "903e221f-2882-a42e-5f44-c531ee62f46c"
      },
      "outputs": [],
      "source": [
        "fp = open('submit.csv', 'w')\n",
        "fp.write(yPred.to_csv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f4a71b3-9e5d-0609-5598-130cdaa25b28"
      },
      "outputs": [],
      "source": [
        "yPred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd907334-7c33-cc58-f6f1-3ee763f1aeba"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}