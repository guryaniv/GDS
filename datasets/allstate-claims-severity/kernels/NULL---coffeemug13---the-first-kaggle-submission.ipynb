{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "347dc395-7d3f-169f-8ac7-d2947098ddf6"
      },
      "source": [
        "# Allstate Claims Severity\n",
        "\n",
        "## Task\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "> Submissions are evaluated on the [mean absolute error (MAE)](https://www.kaggle.com/wiki/MeanAbsoluteError) between the predicted loss and the actual loss. ([Source: Kaggle.com](https://www.kaggle.com/c/allstate-claims-severity/details/evaluation)).\n",
        "\n",
        "# General Notes\n",
        "\n",
        "**Visualize everything you can!** We are much better in interpreting and analyzing visual representations of data than looking at the bare numbers. However, make sure to pick visualizations, scales and colors that are suited for your particular question.\n",
        "\n",
        "**Look at the Kernels and Forums on Kaggle!** Kaggle is a platform for learning and sharing ideas in the domain of data mining, data processing and machine learning. Many users share their work and ideas, so please use these resources. Also don't be shy to post questions and share your approaches and ideas in the Forum if you are not in the Top 10%. It's all about learning!\n",
        "\n",
        "**Always cite (link) your sources!** Many of the ideas that you implement are not yours, and others would love to know where you took them from.\n",
        "\n",
        "You can also use our [gitter channel](https://gitter.im/ViennaKaggle/allstate-claims-severity) to discuss your approach. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4b5f3e5c-2951-f533-49b7-b3cd802975c3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3fbc9c9a-5707-c6d1-ae46-0e88f59301b0"
      },
      "source": [
        "# Competition Datasets\n",
        "\n",
        "Each competition provides at least 2 datasets for a competition - a training set and a testing set.\n",
        "\n",
        "## Kaggle Training Set\n",
        "\n",
        "The training set contains `n` input variables (number of columns - 2) for `p` observation (number of rows) including an index column and a ground truth column. We will refer to this set as *Kaggle Training Set* to avoid confusion during Cross Validation. It can be loaded and inspected using the `pandas` library.\n",
        "\n",
        "    train_data = pd.read_csv('../input/train.csv')\n",
        "\n",
        "## Kaggle Testing Set\n",
        "\n",
        "The testing set contains `n` input variables (number of columns - 1) for `q` observation (number of rows) including an index column (also called *ID*) and **without** a ground truth column. The task of the Kaggle participant is to predict this missing column. We will refer to this set as *Kaggle Testing Set* to avoid confusion during Cross Validation.\n",
        "\n",
        "    test_data = pd.read_csv('../input/test.csv')\n",
        "\n",
        "## Additional Sets\n",
        "\n",
        "Some competitions offer additional data that can/should be joined with the training data. However, this competition doesn't provide additional training sets.\n",
        "\n",
        "# Submission\n",
        "\n",
        "In the end, a Kaggle participant should upload a CSV file containing `q` predictions (number of rows - one for each observation in the testing set) with an index column and one (or multiple) prediction column(s). The prediction dimensions depends on the competition and task (e.g. classification vs. regression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "50f8a48d-7661-3a6d-2a28-5f3bdf3bf1a4"
      },
      "outputs": [],
      "source": [
        "# Let's load both sets\n",
        "train_data = pd.read_csv('../input/train.csv')\n",
        "test_data = pd.read_csv('../input/test.csv')\n",
        "\n",
        "# Let's load the sample submission\n",
        "submission = pd.read_csv('../input/sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7fac464e-6f91-2afa-c2d0-e1a4533d891f"
      },
      "outputs": [],
      "source": [
        "for col in train_data:\n",
        "    values = {}\n",
        "    print('column: ' + col)\n",
        "    for val in train_data[col]:\n",
        "        if val.isnumeric():\n",
        "            break\n",
        "        if val not in values:\n",
        "            values[str(val)] = 1\n",
        "    print('values for column <' + col + '>:',end='')\n",
        "    print(\";\".join(values.keys()))\n",
        " \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7a12a340-d9c2-6a11-7751-f3b886a5f14e"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the first 5 entries of the Kaggle training set\n",
        "train_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e058dabb-0618-538c-9ba9-968d83d3cba0"
      },
      "outputs": [],
      "source": [
        "print(\"Number of observations: %i\" % len(train_data))\n",
        "print(\"List of columns: %s\" % \", \".join(train_data.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "be7b75dd-36d8-05ce-47fa-775770e1f257"
      },
      "source": [
        "We observe that the Kaggle training set contains 188,318 observations with 132 columns\n",
        "\n",
        " - 1 index column **id**\n",
        " - 130 feature columns **cat{0-99}**, **cont{0-99}**, etc.\n",
        " - 1 ground truth column **loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad78d501-c3d7-f065-1294-c2ed654b1d3f"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the first 5 entries of the Kaggle testing set\n",
        "test_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8f7ef982-73e9-4ef5-06b2-2d3660796550"
      },
      "outputs": [],
      "source": [
        "print(\"Number of observations: %i\" % len(test_data))\n",
        "print(\"List of columns: %s\" % \", \".join(test_data.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "46205d09-f537-4636-a78f-44f1ba3c98c6"
      },
      "source": [
        "We observe that the Kaggle training set contains 125,546 observations with 131 columns\n",
        "\n",
        " - 1 index column **id**\n",
        " - 130 feature columns **cat{0-99}**, **cont{0-99}**, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d42c4b7-0cf5-492e-ddb8-d0dfb85797bd"
      },
      "outputs": [],
      "source": [
        "submission.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8f0ad9ba-ba37-48a2-c955-00afcad9ba47"
      },
      "outputs": [],
      "source": [
        "print(\"Number of observations: %i\" % len(submission))\n",
        "print(\"List of columns: %s\" % \", \".join(submission.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "125bfa39-d98c-318a-6fec-1d35d896402b"
      },
      "source": [
        "The sample submission should contain 125,546 rows with 2 columns\n",
        "\n",
        " - 1 index column **id**\n",
        " - 1 prediction column **loss**\n",
        "\n",
        "where **id** should correspond with the index of the observation in the Kaggle testing set and **loss** should be predicted by your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3920174b-c23a-5cec-a1e7-a88fb8fdca65"
      },
      "source": [
        "# Analyzing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fcefb65d-65a0-075f-808c-239212caad52"
      },
      "source": [
        "**Task A >** What do the columns mean (age, hometown, packages, price, etc.)? -> you can find this by looking at the values, distributions, or feature importance (you can use this as an order for analyzing the columns)\n",
        "\n",
        "**Task B >** Can we use external data? If yes, which datasets can help us (demographics, BIP, number of hospitals, money spent in health)?\n",
        "\n",
        "**Task C >** What does the value distributions tell us? What could be useful bins for the features? Are there log dependencies? how many unique values? how often do they occur? Are there outliers? Correlations between columns? Columns with unique values? Spikes?\n",
        "\n",
        "**Task D >** Are there missing values? how many are there, and in which columns do they occur? What could be a reason that the values are missing -> how can we replace them? 0, Min, Max, Mean, Median, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "63fcb1cb-af2d-d684-4dd9-dab914cdea00"
      },
      "outputs": [],
      "source": [
        "train_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "919bb1a4-539a-5fb3-6a04-eddbc8d0d798"
      },
      "source": [
        "# Preparing the Dataset\n",
        "\n",
        "As we have seen in the previous part, the dataset contains numerical and categorical values. Most Machine Learning algorithms can only work with numeric values as they are computing distances. Keep in mind that whenever you compute distances over multiple features you should normalize the features. The goal of this part is to clean the dataset and convert it to something that the Machine Learning algorithms can use.\n",
        "\n",
        "## Notes\n",
        "\n",
        "Be careful when mixing pandas, numpy, and XGB matrices. We usually add a **_df** suffix when dealing with pandas dataframes and a **xgdmat_** prefix when dealing with XGB matrices.\n",
        "\n",
        "When you apply a transformation (e.g. Log-Transformation) to the ground truth, you need to apply as well an inverse transform to the result of your prediction (we will do this in the end)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ee1b3d17-a3f0-4fea-b78d-ddae906c7efb"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "label_encoders = {}\n",
        "category_labels = {}\n",
        "\n",
        "def transform_x(data_df, phase=\"train\"):\n",
        "    \"\"\"Transforms the input dataframe to a dataframe containing\n",
        "    the input variables (= features)\"\"\"\n",
        "    X = data_df.drop(['id'], axis=1)\n",
        "    \n",
        "    if 'loss' in X.columns:\n",
        "        X = X.drop(['loss'], axis=1)\n",
        "    \n",
        "    # List of categorical features\n",
        "    cat_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # List of numerical features\n",
        "    num_features = X.select_dtypes(exclude=['object']).columns\n",
        "    \n",
        "    # Replace each categorical feature with encoded labels\n",
        "    for cat in cat_features:\n",
        "        if phase == \"train\":\n",
        "            # Let's store the used labels\n",
        "            category_labels[cat] = list(set(X[cat]))     \n",
        "  \n",
        "            # We need to fit the Label Encoder in the training phase\n",
        "            label_encoders[cat] = preprocessing.LabelEncoder()\n",
        "            label_encoders[cat].fit(X[cat])\n",
        "        \n",
        "        # We replace unseen labels by the first label\n",
        "        mask = X[cat].apply(lambda x: x not in category_labels[cat])\n",
        "        X.loc[mask, cat] = category_labels[cat][0]\n",
        "        \n",
        "        X[cat] = label_encoders[cat].transform(X[cat])\n",
        "    \n",
        "    return X\n",
        "\n",
        "def transform_y(data_df):\n",
        "    \"\"\"Transforms the input dataframe to a dataframe containing\n",
        "    the ground truth data\"\"\"\n",
        "    y = data_df['loss']\n",
        "    \n",
        "    # You can do some crazy stuff here\n",
        "    # y = np.log(y)\n",
        "    \n",
        "    return y\n",
        "\n",
        "def inverse_transform_y(data):\n",
        "    \"\"\"Inverse transforms the y values to match the original\n",
        "    Kaggle testing set\"\"\"\n",
        "    y = data\n",
        "    \n",
        "    # You should invert all the crazy stuff\n",
        "    # y = np.exp(y)\n",
        "    \n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7a1abe0-4b9e-44ca-c3eb-7a241008bdfa"
      },
      "outputs": [],
      "source": [
        "X_train_df = transform_x(train_data)\n",
        "y_train_df = transform_y(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f45899e0-9cde-5259-fd6d-8270ee4dd2c4"
      },
      "source": [
        "# Cross-Validation\n",
        "\n",
        "Cross-Validation is a technique used in parameter optimization to avoid overfitting (having a low training error and a high tesing error). Make sure you use the right evaluation metric for your problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "34361dc6-5250-3588-a763-1621e0fa1f80"
      },
      "source": [
        "**Task E >** Split the Kaggle training set into a training and validation set. The training set should be used for paramter tuning using Cross Validation and the results should be verified on the validation set. The goal is to generate a validation set that gives similar results as the public/private Kaggle testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3d542541-b87e-0630-ff20-3f83eb2268ce"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79cbb224-d44f-02ba-349f-c4fd805fa0c3"
      },
      "source": [
        "# Regression using XGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "26863f92-8793-b8bf-5f97-df54d9b43971"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create our DMatrix to make XGBoost more efficient\n",
        "xgdmat_train = xgb.DMatrix(X_train_df.values, y_train_df.values)\n",
        "\n",
        "params = {'eta': 0.01, 'seed':0, 'subsample': 0.5, 'colsample_bytree': 0.5, \n",
        "             'objective': 'reg:linear', 'max_depth':6, 'min_child_weight':3} \n",
        "\n",
        "num_rounds = 100\n",
        "mdl = xgb.train(params, xgdmat_train, num_boost_round=num_rounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2b02ef23-589d-47c0-4bd9-6cbf551738ea"
      },
      "outputs": [],
      "source": [
        "X_test_df = transform_x(test_data, phase=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "81a7e678-cabb-36bc-34c8-5cf0dbdd17ea"
      },
      "outputs": [],
      "source": [
        "xgdmat_test = xgb.DMatrix(X_test_df.values)\n",
        "y_pred = mdl.predict(xgdmat_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4124087b-080f-8047-da23-b0fb3001666f"
      },
      "outputs": [],
      "source": [
        "submission.iloc[:, 1] = inverse_transform_y(y_pred)\n",
        "submission.to_csv('vienna_kaggle_submission.csv', index=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8cd1925a-7c9d-dc55-8bef-6eb5c4e13537"
      },
      "source": [
        "# References\n",
        "\n",
        " - https://www.kaggle.com/guyko81/allstate-claims-severity/just-an-easy-solution"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}