{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "99acd28f-708b-628c-3796-4ac3732fb4d7"
      },
      "source": [
        "## EDA and Fraud detection\n",
        "\n",
        "The provided data has the financial transaction data as well as the target variable **isFraud**, which is the actual fraud status of the transaction and **isFlaggedFraud** is the indicator which the simulation is used to flag the transaction using some threshold.\n",
        "\n",
        "The goal should be how we can improve and come up with better threshold to capture the fraud transaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3973397b-e562-879a-aeb8-5c94ad02c7c2"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\")\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from scipy.stats import skew, boxcox\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "db05bb33-2f0f-641e-d745-1d6c0e6905dd"
      },
      "outputs": [],
      "source": [
        "# Utilities-related functions\n",
        "def now():\n",
        "    tmp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    return tmp\n",
        "\n",
        "def my_file_read(file):\n",
        "    df = pd.read_csv(file)\n",
        "    print(\"{}: {} has {} observations and {} columns\".format(now(), file, df.shape[0], df.shape[1]))\n",
        "    print(\"{}: Column name checking::: {}\".format(now(), df.columns.tolist()))\n",
        "    return df\n",
        "\n",
        "# Self-defined function to read dataframe and find the missing data on the columns and # of missing\n",
        "def checking_na(df):\n",
        "    try:\n",
        "        if (isinstance(df, pd.DataFrame)):\n",
        "            df_na_bool = pd.concat([df.isnull().any(), df.isnull().sum(), (df.isnull().sum()/df.shape[0])*100],\n",
        "                                   axis=1, keys=['df_bool', 'df_amt', 'missing_ratio_percent'])\n",
        "            df_na_bool = df_na_bool.loc[df_na_bool['df_bool'] == True]\n",
        "            return df_na_bool\n",
        "        else:\n",
        "            print(\"{}: The input is not panda DataFrame\".format(now()))\n",
        "\n",
        "    except (UnboundLocalError, RuntimeError):\n",
        "        print(\"{}: Something is wrong\".format(now()))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4a1c4468-d934-12ad-10a3-e34db68dd06d"
      },
      "outputs": [],
      "source": [
        "raw_data = my_file_read(\"../input/PS_20174392719_1491204439457_log.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c9011fd9-d8cf-2a6a-594f-2615566136da"
      },
      "source": [
        "Let's check the dataset if there's any null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7296a623-d327-d558-d680-fa6aa329288f"
      },
      "outputs": [],
      "source": [
        "print(checking_na(raw_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e34e854b-f5f7-58ec-06f5-276fd30828f2"
      },
      "source": [
        "Quickly look at the dataset sample and other properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d14c657-d93e-f8a6-f70f-27af26167785"
      },
      "outputs": [],
      "source": [
        "print(raw_data.head(5))\n",
        "print(raw_data.describe())\n",
        "print(raw_data.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2239d356-c415-540c-5de1-603ebdffbd0a"
      },
      "source": [
        "### 1. EDA (exploratory data analysis )\n",
        "\n",
        "In this section, we will do EDA to understand the data more. From the simulation, there are 5 transaction types as per illustrated below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "64c3a763-cca6-42d8-24b3-21f12523edca"
      },
      "outputs": [],
      "source": [
        "print(raw_data.type.value_counts())\n",
        "\n",
        "f, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "raw_data.type.value_counts().plot(kind='bar', title=\"Transaction type\", ax=ax, figsize=(8,8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c751d532-af83-ffd8-ef8e-6b72a016a37d"
      },
      "source": [
        "There are 2 flags which stand out to me and it's interesting to look onto: **isFraud** and **isFlaggedFraud** column.\n",
        "From the hypothesis, *isFraud* is the indicator which indicates the actual fraud transactions whereas *isFlaggedFraud* is what the system prevents the transaction due to some thresholds being triggered. \n",
        "\n",
        "Let's quickly what kinds of transaction are being flagged and are fraud..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3dac0c68-809f-bf99-c363-01c91ab67622"
      },
      "outputs": [],
      "source": [
        "ax = raw_data.groupby(['type', 'isFraud']).size().plot(kind='bar')\n",
        "ax.set_title(\"# of transaction which are the actual fraud per transaction type\")\n",
        "ax.set_xlabel(\"(Type, isFraud)\")\n",
        "ax.set_ylabel(\"Count of transaction\")\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "baff5c12-ef37-21fc-d7e8-e3c109cd09ed"
      },
      "outputs": [],
      "source": [
        "ax = raw_data.groupby(['type', 'isFlaggedFraud']).size().plot(kind='bar')\n",
        "ax.set_title(\"# of transaction which is flagged as fraud per transaction type\")\n",
        "ax.set_xlabel(\"(Type, isFlaggedFraud)\")\n",
        "ax.set_ylabel(\"Count of transaction\")\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f88b03c2-37bd-757f-5ebc-2651a3f9acb2"
      },
      "source": [
        "So it looks the simulation can flag only 16 transfer transactions as fraud. Let's look at those records and compare with the records which the system cannot catch'em.\n",
        "\n",
        "The plot below will also focus only on *transfer* transaction type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15decf9a-5c41-e91c-9a4f-dbe4da07acf7"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "tmp = raw_data.loc[(raw_data.type == 'TRANSFER'), :]\n",
        "\n",
        "a = sns.boxplot(x = 'isFlaggedFraud', y = 'amount', data = tmp, ax=axs[0][0])\n",
        "axs[0][0].set_yscale('log')\n",
        "b = sns.boxplot(x = 'isFlaggedFraud', y = 'oldbalanceDest', data = tmp, ax=axs[0][1])\n",
        "axs[0][1].set(ylim=(0, 0.5e8))\n",
        "c = sns.boxplot(x = 'isFlaggedFraud', y = 'oldbalanceOrg', data=tmp, ax=axs[1][0])\n",
        "axs[1][0].set(ylim=(0, 3e7))\n",
        "d = sns.regplot(x = 'oldbalanceOrg', y = 'amount', data=tmp.loc[(tmp.isFlaggedFraud ==1), :], ax=axs[1][1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0a871a8f-038c-939d-330b-68df42fd9f44"
      },
      "source": [
        "It looks like **isFlaggedFraud** variable is relied on oldbalanceDest, which is 0 and some threshold on the amount variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bb35aa27-58fe-193a-1fca-cfbe3d137de3"
      },
      "source": [
        "## 2. Modeling\n",
        "\n",
        "In this section, we will focus only Transfer and Cash out transaction types, as they have been identified as fraud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb4da622-1804-2064-85b4-76b55597a917"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tools import categorical\n",
        "\n",
        "# 1. Keep only interested transaction type ('TRANSFER', 'CASH_OUT')\n",
        "# 2. Drop some columns\n",
        "# 3. Convert categorical variables to numeric variable\n",
        "tmp = raw_data.loc[(raw_data['type'].isin(['TRANSFER', 'CASH_OUT'])),:]\n",
        "tmp.drop(['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1, inplace=True)\n",
        "tmp = tmp.reset_index(drop=True)\n",
        "a = np.array(tmp['type'])\n",
        "b = categorical(a, drop=True)\n",
        "tmp['type_num'] = b.argmax(1)\n",
        "\n",
        "print(tmp.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "89f2e5fa-91e2-71e4-ff0e-805d21514a41"
      },
      "source": [
        "Let's see the correlation of the selected datapoint from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cbcdc8a1-2d85-7848-373a-148ca5ce759a"
      },
      "outputs": [],
      "source": [
        "def correlation_plot(df):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax1 = fig.add_subplot(111)\n",
        "    cmap = cm.get_cmap('jet', 30)\n",
        "    cax = ax1.imshow(df.corr(), interpolation = \"nearest\", cmap = cmap)\n",
        "    ax1.grid(True)\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    labels = df.columns.tolist()\n",
        "    ax1.set_xticklabels(labels, fontsize=13, rotation=45)\n",
        "    ax1.set_yticklabels(labels, fontsize=13)\n",
        "    fig.colorbar(cax)\n",
        "    plt.show()\n",
        "    \n",
        "correlation_plot(tmp)\n",
        "\n",
        "# Alternatively, we can use quick seaborn\n",
        "# plot the heatmap\n",
        "sns.heatmap(tmp.corr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6abf25e8-549d-7b27-285a-ddf84fa42c55"
      },
      "source": [
        "Quickly get the count and the target variable count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7a015980-509b-c870-f042-7f436d607cf1"
      },
      "outputs": [],
      "source": [
        "ax = tmp.type.value_counts().plot(kind='bar', title=\"Transaction type\", figsize=(6,6))\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "ax = pd.value_counts(tmp['isFraud'], sort = True).sort_index().plot(kind='bar', title=\"Fraud transaction count\")\n",
        "for p in ax.patches:\n",
        "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()))\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "577e26ff-24c5-0c30-550f-6c2ab5b9ca5a"
      },
      "source": [
        "Based on the dataset, the numeric variables are quite skew, in this case. I will try to scale it with 2 methods and compare them on the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6b75388-5006-20c8-8159-4a8a6883ba30"
      },
      "outputs": [],
      "source": [
        "tmp['amount_boxcox'] = preprocessing.scale(boxcox(tmp['amount']+1)[0])\n",
        "\n",
        "figure = plt.figure(figsize=(16, 5))\n",
        "figure.add_subplot(131) \n",
        "plt.hist(tmp['amount'] ,facecolor='blue',alpha=0.75) \n",
        "plt.xlabel(\"Transaction amount\") \n",
        "plt.title(\"Transaction amount \") \n",
        "plt.text(10,100000,\"Skewness: {0:.2f}\".format(skew(tmp['amount'])))\n",
        "\n",
        "figure.add_subplot(132)\n",
        "plt.hist(np.sqrt(tmp['amount']), facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Square root of amount\")\n",
        "plt.title(\"Using SQRT on amount\")\n",
        "plt.text(10, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['amount']))))\n",
        "\n",
        "figure.add_subplot(133)\n",
        "plt.hist(tmp['amount_boxcox'], facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Box cox of amount\")\n",
        "plt.title(\"Using Box cox on amount\")\n",
        "plt.text(10, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['amount_boxcox'])))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "752ddfd7-7a88-2745-adff-8078ee984579"
      },
      "outputs": [],
      "source": [
        "tmp['oldbalanceOrg_boxcox'] = preprocessing.scale(boxcox(tmp['oldbalanceOrg']+1)[0])\n",
        "\n",
        "figure = plt.figure(figsize=(16, 5))\n",
        "figure.add_subplot(131) \n",
        "plt.hist(tmp['oldbalanceOrg'] ,facecolor='blue',alpha=0.75) \n",
        "plt.xlabel(\"old balance originated\") \n",
        "plt.title(\"Old balance org\") \n",
        "plt.text(2,100000,\"Skewness: {0:.2f}\".format(skew(tmp['oldbalanceOrg'])))\n",
        "\n",
        "\n",
        "figure.add_subplot(132)\n",
        "plt.hist(np.sqrt(tmp['oldbalanceOrg']), facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Square root of oldBal\")\n",
        "plt.title(\"SQRT on oldbalanceOrg\")\n",
        "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['oldbalanceOrg']))))\n",
        "\n",
        "figure.add_subplot(133)\n",
        "plt.hist(tmp['oldbalanceOrg_boxcox'], facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Box cox of oldBal\")\n",
        "plt.title(\"Box cox on oldbalanceOrg\")\n",
        "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['oldbalanceOrg_boxcox'])))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9e4b2340-4bf7-7310-43fb-32710bbbfc94"
      },
      "outputs": [],
      "source": [
        "tmp['newbalanceOrg_boxcox'] = preprocessing.scale(boxcox(tmp['newbalanceOrig']+1)[0])\n",
        "\n",
        "figure = plt.figure(figsize=(16, 5))\n",
        "figure.add_subplot(131) \n",
        "plt.hist(tmp['newbalanceOrig'] ,facecolor='blue',alpha=0.75) \n",
        "plt.xlabel(\"New balance originated\") \n",
        "plt.title(\"New balance org\") \n",
        "plt.text(2,100000,\"Skewness: {0:.2f}\".format(skew(tmp['newbalanceOrig'])))\n",
        "\n",
        "\n",
        "figure.add_subplot(132)\n",
        "plt.hist(np.sqrt(tmp['newbalanceOrig']), facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Square root of newBal\")\n",
        "plt.title(\"SQRT on newbalanceOrig\")\n",
        "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['newbalanceOrig']))))\n",
        "\n",
        "figure.add_subplot(133)\n",
        "plt.hist(tmp['newbalanceOrg_boxcox'], facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Box cox of newBal\")\n",
        "plt.title(\"Box cox on newbalanceOrig\")\n",
        "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['newbalanceOrg_boxcox'])))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eab9f09a-767d-fa1e-34a4-59aaae0923c8"
      },
      "outputs": [],
      "source": [
        "tmp['oldbalanceDest_boxcox'] = preprocessing.scale(boxcox(tmp['oldbalanceDest']+1)[0])\n",
        "\n",
        "figure = plt.figure(figsize=(16, 5))\n",
        "figure.add_subplot(131) \n",
        "plt.hist(tmp['oldbalanceDest'] ,facecolor='blue',alpha=0.75) \n",
        "plt.xlabel(\"Old balance desinated\") \n",
        "plt.title(\"Old balance dest\") \n",
        "plt.text(2,100000,\"Skewness: {0:.2f}\".format(skew(tmp['oldbalanceDest'])))\n",
        "\n",
        "\n",
        "figure.add_subplot(132)\n",
        "plt.hist(np.sqrt(tmp['oldbalanceDest']), facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Square root of oldBalDest\")\n",
        "plt.title(\"SQRT on oldbalanceDest\")\n",
        "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['oldbalanceDest']))))\n",
        "\n",
        "figure.add_subplot(133)\n",
        "plt.hist(tmp['oldbalanceDest_boxcox'], facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Box cox of oldbalanceDest\")\n",
        "plt.title(\"Box cox on oldbalanceDest\")\n",
        "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['oldbalanceDest_boxcox'])))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6b8abd24-213a-fb3c-942c-1301478c98f2"
      },
      "outputs": [],
      "source": [
        "tmp['newbalanceDest_boxcox'] = preprocessing.scale(boxcox(tmp['newbalanceDest']+1)[0])\n",
        "\n",
        "figure = plt.figure(figsize=(16, 5))\n",
        "figure.add_subplot(131) \n",
        "plt.hist(tmp['newbalanceDest'] ,facecolor='blue',alpha=0.75) \n",
        "plt.xlabel(\"newbalanceDest\") \n",
        "plt.title(\"newbalanceDest\") \n",
        "plt.text(2,100000,\"Skewness: {0:.2f}\".format(skew(tmp['newbalanceDest'])))\n",
        "\n",
        "\n",
        "figure.add_subplot(132)\n",
        "plt.hist(np.sqrt(tmp['newbalanceDest']), facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Square root of newbalanceDest\")\n",
        "plt.title(\"SQRT on newbalanceDest\")\n",
        "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(np.sqrt(tmp['newbalanceDest']))))\n",
        "\n",
        "figure.add_subplot(133)\n",
        "plt.hist(tmp['newbalanceDest_boxcox'], facecolor = 'red', alpha=0.5)\n",
        "plt.xlabel(\"Box cox of newbalanceDest\")\n",
        "plt.title(\"Box cox on newbalanceDest\")\n",
        "plt.text(2, 100000, \"Skewness: {0:.2f}\".format(skew(tmp['newbalanceDest_boxcox'])))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5a7807f3-4b47-47a9-b787-55932e9068a8"
      },
      "outputs": [],
      "source": [
        "print(\"The fraud transaction of the filtered dataset: {0:.4f}%\".format((len(tmp[tmp.isFraud == 1])/len(tmp)) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e5c1a76c-b40e-c6f4-4f53-9e6ee6413619"
      },
      "source": [
        "As can be seen, I have already filtered unrelated transaction type out and keep only relevant. There're only actual fraud of 0.3%. This is very imbalance data.\n",
        "\n",
        "In this notebook, I will quickly use traditional *under*-sampling method (there are several other ways; under and over sampling, SMOTE, etc).\n",
        "\n",
        "I will under sample the dataset by creating a 50-50 ratio of randomly selecting 'x' amount of sample from majority class, with 'x' being the total number of records with the minority class.\n",
        "\n",
        "Also we will use only the boxcox data transformation for prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4025730d-e0cb-2d65-aa84-cfde79ddce6c"
      },
      "outputs": [],
      "source": [
        "tmp.drop(['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'amount', 'type'], axis=1, inplace=True)\n",
        "\n",
        "X = tmp.ix[:, tmp.columns != 'isFraud']\n",
        "y = tmp.ix[:, tmp.columns == 'isFraud']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c2f43870-9711-9a41-e159-b1051420e6f2"
      },
      "outputs": [],
      "source": [
        "# Number of data points in the minority class\n",
        "number_records_fraud = len(tmp[tmp.isFraud == 1])\n",
        "fraud_indices = tmp[tmp.isFraud == 1].index.values\n",
        "\n",
        "# Picking the indices of the normal classes\n",
        "normal_indices = tmp[tmp.isFraud == 0].index\n",
        "\n",
        "# Out of the indices we picked, randomly select \"x\" number (x - same as total fraud)\n",
        "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
        "random_normal_indices = np.array(random_normal_indices)\n",
        "\n",
        "# Appending the 2 indices\n",
        "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
        "under_sample_data = tmp.iloc[under_sample_indices, :]\n",
        "\n",
        "X_undersample = under_sample_data.ix[:, under_sample_data.columns != 'isFraud']\n",
        "y_undersample = under_sample_data.ix[:, under_sample_data.columns == 'isFraud']\n",
        "\n",
        "# Showing ratio\n",
        "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.isFraud == 0])/len(under_sample_data))\n",
        "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.isFraud == 1])/len(under_sample_data))\n",
        "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d14cf8b8-c616-5978-bbe1-708b912212c5"
      },
      "source": [
        "## Logistic regression classifier - Manual under-sampling data\n",
        "\n",
        "From the model evaluation (or confusion matrix), we know that\n",
        "\n",
        " 1. Accuracy = (TP + TN) / Total\n",
        " 2. Presicion = TP / (TP + FP)\n",
        " 3. Recall = TP / (TP + FN)\n",
        "\n",
        "As such, specifically for this problem, we are interested in the recall score to capture the most fraudulent transactions. As we know, due to the imbalance of the data, many observations could be predicted as False Negatives, being, that we predict a normal transaction, but it is in fact a fraudulent one. Recall captures this.\n",
        "\n",
        "Obviously, trying to increase recall, tends to come with a decrease of precision. However, in our case, if we predict that a transaction is fraudulent and turns out not to be, is not a massive problem compared to the opposite.\n",
        "\n",
        "Due to this, many evaluation will be based on **recall score**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9892a8f9-93a0-7029-f3f4-dfb6ce8e3b2d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Whole dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "print(\"Number transactions train dataset: \", format(len(X_train),',d'))\n",
        "print(\"Number transactions test dataset: \", format(len(X_test), ',d'))\n",
        "print(\"Total number of transactions: \", format(len(X_train)+len(X_test), ',d'))\n",
        "\n",
        "# Undersampled dataset\n",
        "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
        "                                                                                                   ,y_undersample\n",
        "                                                                                                   ,test_size = 0.3\n",
        "                                                                                                   ,random_state = 0)\n",
        "print(\"\")\n",
        "print(\"Number transactions train dataset: \", format(len(X_train_undersample),',d'))\n",
        "print(\"Number transactions test dataset: \", format(len(X_test_undersample),',d'))\n",
        "print(\"Total number of transactions: \", format(len(X_train_undersample)+len(X_test_undersample),',d'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "035e78e9-ec7e-743d-8498-f194e57a86a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cross_validation import KFold, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, precision_score, precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \n",
        "\n",
        "\n",
        "def printing_Kfold_scores(x_train_data, y_train_data, kfoldnum, c_array):\n",
        "    # define K-Fold\n",
        "    fold = KFold(len(y_train_data), kfoldnum,shuffle=False) \n",
        "\n",
        "    results_table = pd.DataFrame(index = range(len(c_array),3), columns = ['C_parameter','Mean recall score', 'Mean precision score'])\n",
        "    results_table['C_parameter'] = c_array\n",
        "\n",
        "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
        "    j = 0\n",
        "    for c_param in c_array:\n",
        "        print('-------------------------------------------')\n",
        "        print('C parameter: ', c_param)\n",
        "        print('-------------------------------------------')\n",
        "        print('')\n",
        "\n",
        "        recall_accs = []\n",
        "        precision_accs = []\n",
        "        for iteration, indices in enumerate(fold,start=1):\n",
        "\n",
        "            # Call the logistic regression model with a certain C parameter\n",
        "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n",
        "\n",
        "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
        "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
        "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
        "\n",
        "            # Predict values using the test indices in the training data\n",
        "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
        "\n",
        "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n",
        "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
        "            recall_accs.append(recall_acc)\n",
        "            \n",
        "            precision_acc = precision_score(y_train_data.iloc[indices[1], :].values, y_pred_undersample)\n",
        "            precision_accs.append(precision_acc)\n",
        "            print(\"Iteration {}: recall score = {:.4f}, precision score = {:.4f}\".format(iteration, recall_acc, precision_acc))\n",
        "\n",
        "        # The mean value of those recall scores is the metric we want to save and get hold of.\n",
        "        results_table.ix[j,'Mean recall score'] = np.mean(recall_accs)\n",
        "        results_table.ix[j, 'Mean precision score'] = np.mean(precision_accs)\n",
        "        j += 1\n",
        "        print('')\n",
        "        print('Mean recall score {:.4f}'.format(np.mean(recall_accs)))\n",
        "        print('Mean precision score {:.4f}'.format(np.mean(precision_accs)))\n",
        "        print('')\n",
        "\n",
        "    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n",
        "    \n",
        "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
        "    print('*********************************************************************************')\n",
        "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
        "    print('*********************************************************************************')\n",
        "    \n",
        "    return best_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "694ae9f3-417f-e08d-3a4a-f23288d1f799"
      },
      "source": [
        "Let's find the C value to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7acf208e-11b5-90a4-c430-da8e73b77a57"
      },
      "outputs": [],
      "source": [
        "c_param_range = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "k_fold = 5\n",
        "best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample, k_fold, c_param_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "72034804-5ed1-7b56-b97f-0b65845b8b20"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        1#print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "201a4af8-e6b6-fb4a-d504-d2e0f90f9e21"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
        "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
        "y_pred_undersample = lr.predict(X_test_undersample.values)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
        "\n",
        "print(\"Recall metric in the testing dataset: {0:.4f}\".format(cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "890cda0d-65cd-5f72-8338-3cb13c5ac5d4"
      },
      "outputs": [],
      "source": [
        "\n",
        "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
        "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
        "y_pred = lr.predict(X_test.values)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3050879e-42c0-83d6-1e39-8b74723083d6"
      },
      "outputs": [],
      "source": [
        "# ROC CURVE\n",
        "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
        "y_pred_undersample_score = lr.fit(X_train_undersample,y_train_undersample.values.ravel()).decision_function(X_test_undersample.values)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test_undersample.values.ravel(),y_pred_undersample_score)\n",
        "roc_auc = auc(fpr,tpr)\n",
        "\n",
        "# Plot ROC\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.xlim([-0.1,1.0])\n",
        "plt.ylim([-0.1,1.01])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e73a4f7-34e0-7d19-c3d5-9dec70f6ad35"
      },
      "outputs": [],
      "source": [
        "print(lr)\n",
        "print(lr.intercept_ )\n",
        "print(lr.coef_)\n",
        "print(X.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d84b28d6-eca7-b0a9-8ba9-e9818754214e"
      },
      "source": [
        "**Next steps**:\n",
        "\n",
        " - What if we use all features (boxcox transformation and original data)\n",
        " - Using SVC or other methodologies\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}