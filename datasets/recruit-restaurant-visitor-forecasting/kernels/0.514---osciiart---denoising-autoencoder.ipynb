{"cells":[{"metadata":{"_uuid":"11be6678b4cda3ac3ad5726393e54d1dd468fd6e","_cell_guid":"a01e8033-94e6-4c05-a390-5aefa2ec988e"},"cell_type":"markdown","source":"This is an inperfect implimentation of the 1st solution of Porto Seguro competition.  \n<https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629>\n\nThis is not as a strong model as the original. But still it's useful to mix in ensemble.  \n\nscore of models trained at my local  \nmodel name /local CV/public LB/private LB  \nnr2/0.501/0.487/0.559  \nnr3/0.501/0.486/0.557  \nnr5/0.503/0.490/0.541  \nnr6/0.468/0.492/0.557  \n\nModel size is reduced for kernel.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3d7965f7620d3f6012baeffae448691b696903e0","_cell_guid":"b9e26266-3404-4878-875a-cb3daf65800d","trusted":true},"cell_type":"code","source":"### preprocessing\n\"\"\"\ncode is taken from\ntunguz - Surprise Me 2!\nhttps://www.kaggle.com/tunguz/surprise-me-2/code\n\"\"\"\nimport glob, re\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nfrom datetime import datetime\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9648127fd6b1c57305e8fe5102ef141a41d6c350","_cell_guid":"1e84b5a3-07bf-4188-8c02-a00aaead35bb","trusted":true,"collapsed":true},"cell_type":"code","source":"data = {\n    'tra': pd.read_csv('../input/air_visit_data.csv'),\n    'as': pd.read_csv('../input/air_store_info.csv'),\n    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n    'ar': pd.read_csv('../input/air_reserve.csv'),\n    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n    'id': pd.read_csv('../input/store_id_relation.csv'),\n    'tes': pd.read_csv('../input/sample_submission.csv'),\n    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n    }\n\ndata['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n\nfor df in ['ar','hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n    data[df]['visit_dow'] = data[df]['visit_datetime'].dt.dayofweek\n    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n    # Exclude same-week reservations\n    data[df] = data[df][data[df]['reserve_datetime_diff'] > data[df]['visit_dow']]\n    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n\ndata['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\ndata['tra']['doy'] = data['tra']['visit_date'].dt.dayofyear\ndata['tra']['year'] = data['tra']['visit_date'].dt.year\ndata['tra']['month'] = data['tra']['visit_date'].dt.month\ndata['tra']['week'] = data['tra']['visit_date'].dt.week\ndata['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n\ndata['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['doy'] = data['tes']['visit_date'].dt.dayofyear\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['week'] = data['tes']['visit_date'].dt.week\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n\nunique_stores = data['tes']['air_store_id'].unique()\nstores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n\n#sure it can be compressed...\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n#tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n#stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n\nstores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n# NEW FEATURES FROM Georgii Vyshnia\nstores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\nstores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\nlbl = preprocessing.LabelEncoder()\nfor i in range(10):\n    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\nstores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\nstores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ntrain = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n\ntrain = pd.merge(train, stores, how='inner', on=['air_store_id','dow']) \ntest = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n\nfor df in ['ar','hr']:\n    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n\ntrain['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n\ntrain['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\ntrain['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\ntrain['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n\ntest['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\ntest['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\ntest['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n\n# NEW FEATURES FROM JMBULL\ntrain['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntrain['var_max_lat'] = train['latitude'].max() - train['latitude']\ntrain['var_max_long'] = train['longitude'].max() - train['longitude']\ntest['var_max_lat'] = test['latitude'].max() - test['latitude']\ntest['var_max_long'] = test['longitude'].max() - test['longitude']\n\n# NEW FEATURES FROM Georgii Vyshnia\ntrain['lon_plus_lat'] = train['longitude'] + train['latitude'] \ntest['lon_plus_lat'] = test['longitude'] + test['latitude']\n\nlbl = preprocessing.LabelEncoder()\ntrain['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\ntest['air_store_id2'] = lbl.transform(test['air_store_id'])\n\ncol = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\ntrain = train.fillna(-1)\ntest = test.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"356f102dc42959563e5e2e46755f598dbe72c61d","_cell_guid":"d545fc9a-72fe-4645-9b97-de36b8930b71","trusted":true},"cell_type":"code","source":"# concatenate train data and test data and convert category data into one-hot\ntraintest = pd.concat([train, test])\nvalue_col = ['holiday_flg','min_visitors','mean_visitors','median_visitors', # 'max_visitors',\n             'count_observations',\n'rs1_x','rv1_x','rs2_x','rv2_x','rs1_y','rv1_y','rs2_y','rv2_y','total_reserv_sum','total_reserv_mean',\n'total_reserv_dt_diff_mean','date_int','var_max_lat','var_max_long','lon_plus_lat']\n\ncat_col =  ['dow', 'year', 'month', 'air_store_id2', 'air_area_name', 'air_genre_name',\n'air_area_name0', 'air_area_name1', 'air_area_name2', 'air_area_name3', 'air_area_name4',\n'air_area_name5', 'air_area_name6', 'air_genre_name0', 'air_genre_name1',\n'air_genre_name2', 'air_genre_name3', 'air_genre_name4']\n\nnn_col = value_col + cat_col\n\ndummys = []\nfor col in cat_col:\n    dummy = pd.get_dummies(traintest[col], drop_first=False)\n    dummys.append(dummy.as_matrix())\ndummys = np.concatenate(dummys, axis=1)\nvalue_scaler = preprocessing.MinMaxScaler() # normalization method of original is rank gaussian\nfor vcol in value_col:\n    traintest[vcol] = value_scaler.fit_transform(traintest[vcol].values.astype(np.float64).reshape(-1, 1))\nX_value = traintest[value_col].as_matrix()\nX = np.concatenate([dummys, X_value], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"fd9402a1d463a7f82025950d012250ce306b3ca8","_cell_guid":"4e527e78-b825-4277-9792-1b299be4a84d","trusted":true},"cell_type":"code","source":"def RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"771e2304a92c9586f0b2c862dc157c0311ff231a","_cell_guid":"0e74ef7c-0ba8-4951-b16a-d18160986f06","trusted":true},"cell_type":"code","source":"### train denoising autoencoder\nfrom keras.layers import Input, Dense\nfrom keras import Model\n\ndef get_DAE():\n    # denoising autoencoder\n    inputs = Input((X.shape[1],))\n    x = Dense(500, activation='relu')(inputs) # 1500 original\n    x = Dense(500, activation='relu', name=\"feature\")(x) # 1500 original\n    x = Dense(500, activation='relu')(x) # 1500 original\n    outputs = Dense(X.shape[1], activation='relu')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='mse')\n\n    return model\n\n\ndef x_generator(x, batch_size, shuffle=True):\n    # batch generator of input\n    batch_index = 0\n    n = x.shape[0]\n    while True:\n        if batch_index == 0:\n            index_array = np.arange(n)\n            if shuffle:\n                index_array = np.random.permutation(n)\n\n        current_index = (batch_index * batch_size) % n\n        if n >= current_index + batch_size:\n            current_batch_size = batch_size\n            batch_index += 1\n        else:\n            current_batch_size = n - current_index\n            batch_index = 0\n\n        batch_x = x[index_array[current_index: current_index + current_batch_size]]\n\n        yield batch_x\n\n\ndef mix_generator(x, batch_size, swaprate=0.15, shuffle=True):\n    # generator of noized input and output\n    # swap 0.15% of values of data with values of another\n    num_value = X.shape[1]\n    num_swap = int(num_value * swaprate)\n    gen1 = x_generator(x, batch_size, shuffle)\n    gen2 = x_generator(x, batch_size, shuffle)\n    while True:\n        batch1 = next(gen1)\n        batch2 = next(gen2)\n        new_batch = batch1.copy()\n        for i in range(batch1.shape[0]):\n            swap_idx = np.random.choice(num_value, num_swap, replace=False)\n            new_batch[i, swap_idx] = batch2[i, swap_idx]\n\n        yield (new_batch, batch1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f3dd6c92b467c5fdf21c0611b3829262839f495","_cell_guid":"31298848-e9f8-4fa5-b60f-a505955ee860","trusted":true,"scrolled":true},"cell_type":"code","source":"# training\nbatch_size = 128\nnum_epoch = 5 # 1000 original\ngen = mix_generator(X, batch_size)\ndae = get_DAE()\ndae.fit_generator(generator=gen,\n                  steps_per_epoch=np.ceil(X.shape[0] / batch_size),\n                  epochs=num_epoch,\n                  verbose=1,)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1432807f9763b8cbf62abedf8ef7a453f0d16e7d","_cell_guid":"6893059a-a733-4332-8d9b-68c08735958d","trusted":true},"cell_type":"code","source":"### train NN with feature of DAE\nfrom keras.layers import Dropout\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom  keras.regularizers import l2\n\ndef get_NN(DAE):\n    l2_loss = l2(0.05)\n    DAE.trainable = False\n    x = dae.get_layer(\"feature\").output\n    x = Dropout(0.1)(x)\n    x = Dense(500, activation='relu', kernel_regularizer=l2_loss)(x) # 4500 original\n    x = Dropout(0.5)(x)\n    x = Dense(100, activation='relu', kernel_regularizer=l2_loss)(x) # 1000 original\n    x = Dropout(0.5)(x)\n    x = Dense(100, activation='relu', kernel_regularizer=l2_loss)(x) # 1000 original\n    x = Dropout(0.5)(x)\n    predictions = Dense(1, activation='relu', kernel_regularizer=l2_loss)(x)\n\n    model = Model(inputs=dae.input, outputs=predictions)\n    model.compile(loss='mse',optimizer='adam')\n\n    return model\n\n\ndef train_generator(x, y, batch_size, shuffle=True):\n    batch_index = 0\n    n = x.shape[0]\n    while True:\n        if batch_index == 0:\n            index_array = np.arange(n)\n            if shuffle:\n                index_array = np.random.permutation(n)\n\n        current_index = (batch_index * batch_size) % n\n        if n >= current_index + batch_size:\n            current_batch_size = batch_size\n            batch_index += 1\n        else:\n            current_batch_size = n - current_index\n            batch_index = 0\n\n        batch_x = x[index_array[current_index: current_index + current_batch_size]]\n        batch_y = y[index_array[current_index: current_index + current_batch_size]]\n\n        yield batch_x, batch_y\n\n\ndef test_generator(x, batch_size, shuffle=False):\n    batch_index = 0\n    n = x.shape[0]\n    while True:\n        if batch_index == 0:\n            index_array = np.arange(n)\n            if shuffle:\n                index_array = np.random.permutation(n)\n\n        current_index = (batch_index * batch_size) % n\n        if n >= current_index + batch_size:\n            current_batch_size = batch_size\n            batch_index += 1\n        else:\n            current_batch_size = n - current_index\n            batch_index = 0\n\n        batch_x = x[index_array[current_index: current_index + current_batch_size]]\n\n        yield batch_x\n\n        \ndef get_callbacks(save_path):\n    save_checkpoint = ModelCheckpoint(filepath=save_path, monitor='val_loss', save_best_only=True)\n    early_stopping = EarlyStopping(monitor='val_loss',\n                                   patience=4,\n                                   verbose=1,\n                                   min_delta=1e-4,\n                                   mode='min')\n    Callbacks = [ save_checkpoint, early_stopping]\n    return Callbacks","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d18d6721a0664f646d0ef80bfe0420c309e7868","_cell_guid":"db676c01-c680-46a2-aa85-01cc196ff21e","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nbatch_size = 128\nnum_epoch = 5 # 150 original\nnum_fold = 3 # 5 original\n\nY_train = np.log1p(train['visitors']).values\nX_train = X[:Y_train.shape[0]]\nX_test = X[Y_train.shape[0]:]\n\ny_test = np.zeros([num_fold, X_test.shape[0]])\ny_valid = np.zeros([X_train.shape[0]])\n\nfolds = list(KFold(n_splits=num_fold, shuffle=True, random_state=42).split(X_train))\nfor j, (ids_train_split, ids_valid_split) in enumerate(folds):\n    print(\"fold\", j+1, \"==================\")\n    model = get_NN(dae)\n    gen_train = train_generator(X_train[ids_train_split], Y_train[ids_train_split], batch_size)\n    gen_val = train_generator(X_train[ids_valid_split], Y_train[ids_valid_split], batch_size, shuffle=False)\n    gen_val_pred = test_generator(X_train[ids_valid_split], batch_size, shuffle=False)\n    gen_test_pred = test_generator(X_test, batch_size, shuffle=False)\n\n    # Fit model\n    callbacks = get_callbacks(\"weight\" + str(j) + \".hdf5\")\n    model.fit_generator(generator=gen_train,\n                        steps_per_epoch=np.ceil(ids_train_split.shape[0] / batch_size),\n                        epochs=num_epoch,\n                        verbose=1,\n#                         callbacks=callbacks,\n                        validation_data=gen_val,\n                        validation_steps=np.ceil(ids_valid_split.shape[0] / batch_size),\n                        )\n    # Predict on train, val and test\n#     model.load_weights(\"weight\" + str(j) + \".hdf5\") # load best epoch weight\n    y_valid[ids_valid_split] = model.predict_generator(generator=gen_val_pred,\n                                        steps=np.ceil(ids_valid_split.shape[0] / batch_size))[:,0]\n    y_test[j] = model.predict_generator(generator=gen_test_pred,\n                                        steps=np.ceil(X_test.shape[0] / batch_size))[:,0]\n\nscore = RMSLE(y_valid, Y_train)\nprint(\"valid score\", score)\ny_test_mean = np.mean(y_test, axis=0)\nid = pd.read_csv(\"../input/sample_submission.csv\")['id']\nsubmission = pd.DataFrame({'id': id, 'visitors': np.expm1(y_test_mean)})\nsubmission.to_csv('submission_dae.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}