{"cells":[{"metadata":{"_uuid":"aaf6eaf5e3c8a35621be1c42512a6c8685dd343b","trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\n\ntrainDataSet = pd.read_csv('../input/train.csv')\ntestDataSet = pd.read_csv('../input/test.csv')\ntrainDataSet.head\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nprint (trainDataSet.shape)\nprint (testDataSet.shape)\nprint (trainDataSet.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a1b11197f8dd78a1ad888a08b081412f15be90b"},"cell_type":"code","source":"def cleanText(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub('\\d+', ' ', text)\n    text = text.strip(' ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f514a1d46af6853578c1a2d56b105e1b8a6c4f57"},"cell_type":"code","source":"for className in class_names:\n    print (trainDataSet[className].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dd65fc070634028eba546468f8d0f0abb1f030e"},"cell_type":"code","source":"print('Percentage of comments that are not labelled:')\nprint(len(trainDataSet[(trainDataSet['toxic']==0) & \n             (trainDataSet['severe_toxic']==0) & \n             (trainDataSet['obscene']==0) & \n             (trainDataSet['threat']== 0) & \n             (trainDataSet['insult']==0) &\n             (trainDataSet['identity_hate']==0)]) / len(trainDataSet))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b1115e0219deef5da54ae9e228b1c90edc9293a"},"cell_type":"code","source":"tempToxicDataSet = trainDataSet[trainDataSet['toxic'] == 0][0:1]\ntempInsultDataSet = trainDataSet[trainDataSet['toxic'] == 1][0:1]\nframes = [tempToxicDataSet, tempInsultDataSet]\ntempTrainDataSet = pd.concat(frames)\nprint (tempTrainDataSet.shape)\n\ntempTestDataSet = testDataSet[0:1]\nprint (tempTestDataSet.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce390e67966423ccb2e061937a595bbf785c4f93","scrolled":true},"cell_type":"code","source":"train_text = tempTrainDataSet['comment_text']\ntrain_target = tempTrainDataSet.loc[:, class_names]\ntest_text = tempTestDataSet['comment_text']\nall_text = pd.concat([train_text, test_text])\nStringData = \"\"\nfor i in all_text:\n    StringData += i\nStringData","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90df9ca5c29dece156af5816b6f200084e6da55c"},"cell_type":"markdown","source":"|### SENT TOKENIZE - Convert the paragraph into sentences\n### WORD TOKENIZE - Convert a sentence or paragraph into words"},{"metadata":{"trusted":true,"_uuid":"b27a6b96084c6da45cb09763cb51b5838eaf6329"},"cell_type":"code","source":"print (\"Original Text - \",StringData,\"\\n\")\nsentences = nltk.sent_tokenize(StringData)\nprint (len(sentences))\nsentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"609c0b03398f7a47c42ead4243d9cdb99c385524"},"cell_type":"code","source":"words = nltk.word_tokenize(StringData)\nprint (len(words))\nwords","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b70fdb042d41dbf3f1ae983d43ea2f942a13783c"},"cell_type":"markdown","source":"#### STEMMING - Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n#### Example - (Intelligence or Intelligently => Intelligen)\n#### Problem - Produced intermediate representation of the word may not have any meaning \n#### LEMMATIZATION - Same as stemming but the intermediate representation has a meaning\n#### Example - (Intelligence or Intelligently => Intelligent)"},{"metadata":{"trusted":true,"_uuid":"86fc9f3275f0c346202b1cbe890d052ae10e2fdd"},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\nfor i in range(len(sentences)):\n    print (\"Actual Sentence - \",sentences[i],\"\\n\")\n    stemmingWords = nltk.word_tokenize(sentences[i])\n    lemmatizingWords = nltk.word_tokenize(sentences[i])\n    stemmedWords = [stemmer.stem(word) for word in stemmingWords]\n    lemmatizedWords = [lemmatizer.lemmatize(word) for word in lemmatizingWords]\n    print (\"Stemmed Words - \\n\")\n    print (stemmedWords)\n    print (\"Lemmatized Words - \\n\")\n    print (lemmatizedWords)\n    print (\"____________________________________________\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19f53c4659245fb5e6f7e37900c5a3acb7b568a8"},"cell_type":"markdown","source":"### REMOVAL OF STOP WORDS\n### Example - (to, be etc....)\n### Stopwords are not required while doing sentiment analysis and we need to carry out this step for better performance"},{"metadata":{"trusted":true,"_uuid":"09ef363aea8732711b955d91f908a15789d44221","scrolled":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nfor i in range(len(sentences)):\n    print (\"Actual Sentence - \",sentences[i],\"\\n\")\n    words = nltk.word_tokenize(sentences[i])\n    newwords = [word for word in words if word not in stopwords.words('english')]\n    print (newwords)\n    print (\"____________________________________________\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6f2ea5800a9027a383a5e35a8dcb22d8dfb1ae2"},"cell_type":"markdown","source":"### PARTS OF SPEECH TAGGING"},{"metadata":{"trusted":true,"_uuid":"7abe677a8a9e13d97b8c59ba469004a85862b8a5","scrolled":false},"cell_type":"code","source":"words = nltk.word_tokenize(StringData)\ntagged_words = nltk.pos_tag(words)\nwords_tags = []\nfor tw in tagged_words:\n    words_tags.append(tw[0]+\"_\"+tw[1])\nprint (words_tags)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b12387449942b2107971c6e0fa3d353f542a1e91"},"cell_type":"markdown","source":"### NAMED ENTITY RECOGNITION"},{"metadata":{"trusted":true,"_uuid":"4cddb69a8fae91709b6f43e4086f20ed7f525b8e","scrolled":false},"cell_type":"code","source":"import matplotlib\n\nwords = nltk.word_tokenize(StringData)\ntagged_words = nltk.pos_tag(words)\nnamedEntity = nltk.ne_chunk(tagged_words)\nprint (namedEntity)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43e563368e3b0f60be63c16b4a9c1ee3379eacbd"},"cell_type":"markdown","source":"\n### BAG OF WORDS MODEL\n### The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms.\n### Study Link  - https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n"},{"metadata":{"_uuid":"dfe4a942bf8954a7f1f6c970e2a11e335a691c53"},"cell_type":"markdown","source":"1. Convert all letters to lower case\n1. Remove all special characters\n1. Remove all extra spaces"},{"metadata":{"trusted":true,"_uuid":"175dec7f3a2e75908082e7b7ef13721c466ce334"},"cell_type":"code","source":"import re\n\nsentences = nltk.sent_tokenize(StringData)\nfor i in range(len(sentences)):\n    sentences[i] = sentences[i].lower()\n    sentences[i] = re.sub(r'\\W',' ',sentences[i])\n    sentences[i] = re.sub(r'\\s+',' ',sentences[i])\nsentences","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffd109a35ff8fe2eb53cfd3c07c26b86abde906b"},"cell_type":"markdown","source":"### 1. Create the word count dictionary\n### 2. Get the first 5 frequently occuring words."},{"metadata":{"trusted":true,"_uuid":"6ad742ffb8b0d7978062495fa41ce1496e019849"},"cell_type":"code","source":"word2count = {}\nfor data in sentences:\n    words = nltk.word_tokenize(data)\n    for word in words:\n        if word not in word2count.keys():\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\nimport heapq\nfreq_words = heapq.nlargest(30, word2count, key=word2count.get)\nfreq_words","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a36b520e68184738f8baa6d357a852901c87babb"},"cell_type":"markdown","source":"### Create the bag of words model"},{"metadata":{"trusted":true,"_uuid":"d024ecdb1d766de7b9e309a1449d5310e179fb17"},"cell_type":"code","source":"bagOfWords = []\nfor data in sentences:\n    vector = []\n    for word in freq_words:\n        if word in nltk.word_tokenize(data):\n            vector.append(1)\n        else:\n            vector.append(0)\n    bagOfWords.append(vector)\nbagOfWords = np.asarray(bagOfWords)\nbagOfWords","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1da64fc97a441da1be0f4217d29884b5a22ef714"},"cell_type":"markdown","source":"### Drawbacks of Bag Of Words model -\n### 1.  All words have same importance. \n### 2. No semantic information is preserved\n### Solution to the above problems is - TF-IDF model (Term Frequency - Inverse Document Frequency )\n### TF-IDF = (Number of occurences of word in the document / Total Number of words in a document)\n"},{"metadata":{"trusted":true,"_uuid":"2e862461013d083c0afee7eaac0d759d89b5f222"},"cell_type":"code","source":"# IDF Matrix\nword_idfs = {}\nfor word in freq_words:\n    document_count = 0\n    for data in sentences:\n        if word in nltk.word_tokenize(data):\n            document_count += 1\n    word_idfs[word] = np.log((len(sentences)/document_count)+1) # +1 is the bias and standard way of calulating TF-IDF\nword_idfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4783a79bf0c6cc029a7295a267e88e3d3f24527c"},"cell_type":"code","source":"# TF Matrix\ntf_matrix = {}\nfor word in freq_words:\n    doc_tf = []\n    for data in sentences:\n        frequency = 0\n        for w in nltk.word_tokenize(data):\n            if w == word:\n                frequency += 1\n        tf_word = frequency/len(nltk.word_tokenize(data))\n        doc_tf.append(tf_word)\n    tf_matrix[word] = doc_tf\n# tf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f3cc7a65b8dd1fece125bccf0766e89694455ee"},"cell_type":"code","source":"# TF_IDF MATRIX\ntfidf_matrix = []\nfor word in tf_matrix.keys():\n    tfidf = []\n    for value in tf_matrix[word]:\n        score = value * word_idfs[word]\n        tfidf.append(score)\n    tfidf_matrix.append(tfidf)\ntfidf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c273ea0863e8e692f61027b4631a797043177d4b"},"cell_type":"code","source":"X = np.asarray(tfidf_matrix)\nX = np.transpose(X)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8c8a428dcfb25499834e240fc40ee652c1d403a"},"cell_type":"markdown","source":"####  N-GRAM MODEL - Need to know the Markov chains.  An n-gram is a contiguous sequence of n items from a given sample of text or speech. N-grams of texts are extensively used in text mining and natural language processing tasks. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios). For example, for the sentence \"The cow jumps over the moon\". If N=2 (known as bigrams), then the ngrams would be:\n* the cow\n* cow jumps\n* jumps over\n* over the\n* the moon\n\n#### Generating a word n-gram in the given code"},{"metadata":{"trusted":true,"_uuid":"d247db8e58459d1ded0ec31da1c938a3bd684f01"},"cell_type":"code","source":"import random\n\ntext = train_text[0]\nwords = nltk.word_tokenize(text)\nprint (text)\nprint (\"\\n\")\nngrams = {}\nn = 3\nfor i in range(len(words) - n):\n    gram = ' '.join(words[i:i+n])\n    if gram not in ngrams.keys():\n        ngrams[gram] = []\n    ngrams[gram].append(words[i+n])\n\ncurrentgram = ' '.join(words[0:n])\nresult = currentgram\nfor i in range(10):\n    if currentgram not in ngrams.keys():\n        break\n    possibilities = ngrams[currentgram]\n    nextItem = possibilities[random.randrange(len(possibilities))]\n    result  += ' '+nextItem\n    rwords = nltk.word_tokenize(result)\n    currentGram = ' '.join(rwords[len(rwords)-n:len(rwords)])\n    \nprint (ngrams)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84ba05392dba0ce04f08ee91ad066d5ed7e746ff"},"cell_type":"markdown","source":"## LATENT SYMANTIC ANALYSIS (LSA) - Latent Semantic Analysis is a technique for creating a vector representation of a document. Having a vector representation of a document gives you a way to compare documents for their similarity by calculating the distance between the vectors. This in turn means you can do handy things like classifying documents to determine which of a set of known topics they most likely belong to.\n#### We need to build SVD model . SVD - Singular Value Decomposition. Refer to the link for more information - https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/\n#### LSA  is used for following applications:\n#### 1.  Article Bucketing in websites\n#### 2. Finding relationships between articles/words\n#### 3. Page indexing in search engines"},{"metadata":{"trusted":true,"_uuid":"aa749741cd52c137d1e2fe2cd2850e0ad412b557"},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\ndataset = [\"The amount of population is increasing day by day\",\n           \"The concert was great\",\n           \"I love to see Gordan Ramsay cook\",\n           \"Google is introducing a new technology\",\n           \"AI Robots are the example of great technology present today\",\n           \"All of us were singing in the concert\",\n           \"We have launch campaigns to stop pollution and global warming\"\n          ]\ndataset = [line.lower() for line in dataset]\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(dataset)\n#print (X[0]) # (documentNumber , position) tfidf value\n\nlsa = TruncatedSVD(n_components = 4, n_iter = 100) # n_components are the number of concepts that you want to find from the data\nlsa.fit(X)\n#print (lsa.components_[3])\nterms = vectorizer.get_feature_names()\nconcept_words = {}\nfor i,comp in enumerate(lsa.components_):\n    componentTerms = zip(terms,comp)\n    sortedTerms = sorted(componentTerms, key=lambda x:x[1], reverse=True)\n    sortedTerms =  sortedTerms[:10]\n    concept_words[\"Concept \"+str(i)] = sortedTerms\n\nfor key in concept_words.keys():\n    sentence_scores = []\n    for sentence in dataset:\n        words = nltk.word_tokenize(sentence)\n        score = 0\n        for word in words:\n            for word_with_score in concept_words[key]:\n                if word == word_with_score[0]:\n                    score += word_with_score[1]\n        sentence_scores.append(score)\n    print(\"\\n\"+key+\"\\n\")\n    for sent_score in sentence_scores:\n        print (sent_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eadcaa7fd5ec987b43946e385452bb1c376aaa04"},"cell_type":"markdown","source":"#### Find the synonyms and antonyms for words - Refer to link - https://wordnet.princeton.edu/"},{"metadata":{"trusted":true,"_uuid":"e4b86ef76156c8a90f88ab98e84816a61c7dafbc"},"cell_type":"code","source":"from nltk.corpus import wordnet\n\nsynonyms = []\nantonyms = []\n\nfor syn in wordnet.synsets(\"good\"):\n    for s in syn.lemmas():\n        synonyms.append(s.name())\n        for a in s.antonyms():\n            antonyms.append(a.name())\nprint (set(antonyms))\nprint (set(synonyms))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3c512fe0fe8210ef8a3d90ea6f0b2633c729a4b"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nwc = WordCloud().generate(StringData)\nplt.figure(figsize=(15,15))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4a99d7b699015ae322ee79be93057c7f206f6bd"},"cell_type":"code","source":"from nltk.corpus import stopwords\nimport string\n\noneSetOfStopWords = set(stopwords.words('english')+['``',\"''\"])\ntotalWords =[]\nSentences = trainDataSet['comment_text'].values\ncleanedSentences = \"\"\nfor i in range(0,5000):\n    cleanedText = cleanText(Sentences[i])\n    cleanedSentences += cleanedText\n    requiredWords = nltk.word_tokenize(cleanedText)\n    for word in requiredWords:\n        if word not in oneSetOfStopWords and word not in string.punctuation:\n            totalWords.append(word)\n    \nwordfreqdist = nltk.FreqDist(totalWords)\nmostcommon = wordfreqdist.most_common(50)\nprint(mostcommon)\n\nwc = WordCloud().generate(cleanedSentences)\nplt.figure(figsize=(15,15))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb8e478f5740b7ac3a7d540fe2454d6275fa6a40"},"cell_type":"code","source":"import string\nprint (string.punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67eb10bcd90d137bff6c363035e870772748e3fe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}