{"nbformat": 4, "cells": [{"outputs": [], "metadata": {"collapsed": true, "_uuid": "cf57e98aa619eb82a530700313f13ff45d422e65", "_cell_guid": "b687c75d-144d-478a-b400-28b8a09290b9"}, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "# setup\n", "# -------------------------------------------------------------\n", "# system\n", "import os\n", "# -------------------------------------------------------------\n", "# fundamental modules\n", "import operator\n", "import pandas as pd\n", "import numpy as np\n", "from collections import namedtuple\n", "Batch = namedtuple('Batch', ['data'])\n", "\n", "# ds tools\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn.metrics import mean_absolute_error\n", "from sklearn.metrics import make_scorer\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve, roc_curve, average_precision_score\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.utils import shuffle\n", "from sklearn import preprocessing\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", "# models\n", "import xgboost as xgb\n", "from xgboost import XGBClassifier\n", "import lightgbm as lgb\n", "from lightgbm import LGBMClassifier\n", "import sklearn.linear_model as lm\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC\n", "import mxnet as mx\n", "\n", "# for gini\n", "from numba import jit\n", "\n", "# -------------------------------------------------------------\n", "import gc\n", "import logging\n", "import sys\n", "root_logger = logging.getLogger()\n", "stdout_handler = logging.StreamHandler(sys.stdout)\n", "root_logger.addHandler(stdout_handler)\n", "root_logger.setLevel(logging.DEBUG)\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "#from subprocess import check_output\n", "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "# Any results you write to the current directory are saved as output.\n", "\n", "# =================================================================================\n", "# custom objective function (similar to auc)\n", "def gini(y, pred):\n", "    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n", "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n", "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n", "    gs -= (len(y) + 1) / 2.\n", "    return gs / len(y)\n", "\n", "def gini_nn(y, pred):\n", "    return gini(y, pred) / gini(y, y)\n", "\n", "# -------------------------------------------------------------\n", "# for sklearn api\n", "@jit\n", "def eval_gini(y_true, y_prob):\n", "    \"\"\"\n", "    Original author CPMP : https://www.kaggle.com/cpmpml\n", "    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n", "    \"\"\"\n", "    y_true = np.asarray(y_true)\n", "    y_true = y_true[np.argsort(y_prob)]\n", "    ntrue = 0\n", "    gini = 0\n", "    delta = 0\n", "    n = len(y_true)\n", "    for i in range(n-1, -1, -1):\n", "        y_i = y_true[i]\n", "        ntrue += y_i\n", "        gini += y_i * delta\n", "        delta += 1 - y_i\n", "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n", "    return gini\n", "\n", "def gini_xgb_sklearn_api(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = eval_gini(labels, preds)\n", "    return [('gini', gini_score)]\n"], "execution_count": 1}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "6695c59350b1d4563dd4316c13f969023f953ea1", "_cell_guid": "856de70d-1d7e-4bba-8645-ba17b562fbd8"}, "cell_type": "code", "source": ["# read in files\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "\n", "# basic understanding of the dataset\n", "print (\"Dimension of train data {}\".format(train.shape))\n", "print (\"Dimension of test data {}\".format(test.shape))"], "execution_count": 2}, {"cell_type": "raw", "source": ["====================================================================================\n", "Work with features\n", "credit to:\n", "Heads or Tails's Steering Wheel of Fortune - Porto Seguro EDA notebook\n", "Cam Nugent's deep neural network: insurance claims (~0.268)"], "metadata": {"_uuid": "72170f02d8472c92c57938954a4354cba1ea6e52", "_cell_guid": "e729daae-b40e-442f-9c3c-3da55329027d"}}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "a030275f1114dc879071d4bcc6bfcbc6babaa99e", "_cell_guid": "b9827ae7-dcd9-4136-a0e9-662121c9f802"}, "cell_type": "code", "source": ["# list columns that have missing values\n", "for col in train.columns:\n", "    null_sum = (train[col] == -1).sum()\n", "    if null_sum > 0:\n", "        print('feature %s has %i missing entries' %(col, null_sum))"], "execution_count": 3}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "7a8e06adc8aed0343383f760a07d371311113d0e", "_cell_guid": "2f526b16-aaab-4fbc-8ae9-7fbbc89bdf53"}, "cell_type": "code", "source": ["# olivier's feature selection and \n", "feat_olivier_peatle = [\n", "    \"ps_car_13\",    # xgb #1  lgb #1\n", "    \"ps_reg_03\",    # xgb #2  lgb #2\n", "    \"ps_ind_03\",    # xgb #3  lgb #3\n", "    \"ps_car_14\",    # xgb #4  lgb #5\n", "    \"ps_ind_15\",    # xgb #5  lgb #4\n", "    \"ps_reg_02\",    # xgb #6  lgb #8\n", "    \"ps_ind_05_cat\",# xgb #7  lgb #6\n", "    \"ps_ind_01\",    # xgb #8  lgb #9\n", "    \"ps_car_11_cat\",# xgb #9  lgb #10\n", "    \"ps_car_01_cat\",# xgb #10 lgb #11\n", "    \"ps_reg_01\",    # xgb #11 lgb #7\n", "    \"ps_car_12\",    # xgb #12 lgb #15\n", "    \"ps_car_15\",    # xgb #13 lgb #12\n", "    \"ps_car_06_cat\",# xgb #14 lgb #13\n", "    \"ps_car_09_cat\",# xgb #15 lgb #19\n", "    \"ps_ind_02_cat\",# xgb #16 lgb #18\n", "    \n", "    \"ps_car_07_cat\",# xgb #18 lgb #16\n", "    \"ps_car_11\",    # xgb #19 lgb #20\n", "    \"ps_car_03_cat\",# xgb #20 lgb #17\n", "    \"ps_ind_04_cat\",# xgb #21 lgb #24\n", "    \"ps_car_04_cat\",# xgb #22 lgb #25\n", "\n", "    \n", "    'ps_car_05_cat',# xgb #27 lgb #27\n", "    \n", "    \"ps_car_02_cat\",# xgb #29 lgb #29\n", "    \"ps_car_08_cat\",# xgb #30 lgb #30\n", "    \n", "    \n", "    'ps_car_10_cat',# xgb #33 lgb #34\n", "    \n", "    \n", "\n", "    'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', # these three sums to ps_ind_14, so ps_ind_14 removed\n", "    'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', # these four sums to 1\n", "    'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', #If one of ps_ind_16_bin, ps_ind_17_bin and ps_ind_18_bin is 1, then the others are 0, but they can all be 0\n", "]"], "execution_count": 4}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "77b02a338c9cc9f10364743dc27bae65b10b4791", "_cell_guid": "437c8ba7-dd33-42c7-86cc-c4e1d7ce8c9a"}, "cell_type": "code", "source": ["# data preparation for training\n", "\n", "# drop id and target column\n", "train_y = train['target']\n", "train_x = train.drop(['id', 'target'], axis = 1)\n", "\n", "# prepare testing dataset\n", "test_x = test.drop(['id'], axis = 1)#test[feat_olivier_peatle]"], "execution_count": 5}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "2ae766197cd59f450e9883863328a50a309d576b", "_cell_guid": "5fee528e-d5e5-4887-9c14-94ee31e720a7"}, "cell_type": "code", "source": ["merged_dat = pd.concat([train_x, test_x], axis = 0)\n", "print(merged_dat.shape)"], "execution_count": 6}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "4e2497d0e4b70372c77132246abf699741c91406", "_cell_guid": "0d30e296-b3e0-4b61-a1ad-bea0b0cd7207"}, "cell_type": "code", "source": ["# add feature \"https://www.kaggle.com/headsortails/steering-wheel-of-fortune-porto-seguro-eda\"\n", "# Number of NAs per ID\n", "merged_dat['nona_calc_'] = (merged_dat == -1).sum(axis = 1)\n", "# Sum of binary features\n", "clmn_fltr = [col for col in merged_dat.columns if 'ind' in col and 'bin' in col]\n", "merged_dat['sumbin_calc_'] = merged_dat[clmn_fltr].sum(axis = 1)\n", "# Difference measure for binary features\n", "fltr_df = merged_dat[clmn_fltr]\n", "diff_df = fltr_df.apply(lambda x: x - fltr_df.iloc[0, :], axis = 1).fillna(0)\n", "merged_dat['diffbin_calc_'] = diff_df.sum(axis = 1)"], "execution_count": 7}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "567d8b2b25b0f6f2181f366da567a4602f9fd6db", "_cell_guid": "8f636c50-6fa6-4a70-9f6b-601ddbeecd42"}, "cell_type": "code", "source": ["merged_dat = merged_dat[feat_olivier_peatle + ['nona_calc_', 'sumbin_calc_', 'diffbin_calc_']]"], "execution_count": 8}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "667b6476b210b9af5f236211d1f6c7b44f2558d0", "_cell_guid": "ca627216-8a24-4260-ab05-6c14bb3bc256"}, "cell_type": "code", "source": ["#change data to float32\n", "for c, dtype in zip(merged_dat.columns, merged_dat.dtypes): \n", "    if dtype == np.float64:     \n", "        merged_dat[c] = merged_dat[c].astype(np.float32)"], "execution_count": 9}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "2f9b2e76c9eb5009aaf669df8fe19f069b6bc48c", "_cell_guid": "867980a3-ed2e-431d-9e8d-cecc7c2494e2"}, "cell_type": "code", "source": ["#one hot encode the categoricals\n", "cat_features = [col for col in merged_dat.columns if col.endswith('cat')]\n", "# count ind (no cat nor bin) column as categoricals\n", "cat_features = cat_features + \\\n", "               [col for col in merged_dat.columns if 'ind' in col and 'cat' not in col and 'bin' not in col]\n", "for column in cat_features:\n", "    temp = pd.get_dummies(pd.Series(merged_dat[column]))\n", "    merged_dat = pd.concat([merged_dat, temp],axis=1)\n", "    merged_dat.drop([column], axis=1, inplace = True)"], "execution_count": 10}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "35b3f0069794dab570f368e0510cba25460f3f55", "_cell_guid": "b49cc59a-87f0-4140-ad8a-98f06b05fbf4"}, "cell_type": "code", "source": ["merged_dat.replace(-1, 0, inplace = True)"], "execution_count": 11}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "38ebcc71e49dce50d5b0dfa995578f1fb9127321", "_cell_guid": "ad1733e2-e351-4c5f-a9bb-e9d2d6d0c81e"}, "cell_type": "code", "source": ["# normailise the scale of the numericals\n", "scaler = MinMaxScaler()\n", "merged_dat = scaler.fit_transform(merged_dat)"], "execution_count": 12}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "91a9ba57e84f9c2a68d57fdef4d9f8484e557544", "_cell_guid": "c94a7348-3ff1-4996-911d-9d444f6298ba"}, "cell_type": "code", "source": ["train_X = merged_dat[:train_x.shape[0]]\n", "test_X = merged_dat[train_x.shape[0]:]"], "execution_count": 13}, {"cell_type": "markdown", "source": ["====================================================================================\n", "Mxnet MLP"], "metadata": {"_uuid": "72170f02d8472c92c57938954a4354cba1ea6e52", "_cell_guid": "e729daae-b40e-442f-9c3c-3da55329027d"}}, {"cell_type": "markdown", "source": ["**Please run this notebook locally as it has to save model checkpoints and the best epoch is selected manually for prediction**"], "metadata": {"_uuid": "3881cd0e906a0e3dba68fbe25d320399a485acaa", "_cell_guid": "3e93f752-d28b-4f95-8df5-6c1fe6bf1b5d"}}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "96f2811bf638befe5caa9c09421210926900ad4d", "_cell_guid": "bc9cd9b0-36a7-4231-a414-785bacd5090a"}, "cell_type": "code", "source": ["# ------------------------------------------------\n", "gc.collect()\n", "\n", "# process data\n", "X = train_X\n", "y = train_y.values\n", "\n", "# k fold training\n", "k = 5\n", "skf = StratifiedKFold(n_splits = k, random_state = 0, shuffle  = True)\n", "\n", "# ------------------------------------------------\n", "# setup neural mlp model\n", "def get_mlp(n_in):\n", "    \"\"\"\n", "    multi-layer perceptron\n", "    \"\"\"\n", "    n_out = 1\n", "    data = mx.symbol.Variable('data')\n", "    fc1  = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden = n_in)\n", "    act1 = mx.symbol.Activation(data = fc1, name = 'relu1', act_type = \"relu\")\n", "    fc2  = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 3 * int((n_in - n_out) / 4 // 1 + n_out))\n", "    act2 = mx.symbol.Activation(data = fc2, name = 'relu2', act_type = \"relu\")\n", "    fc3  = mx.symbol.FullyConnected(data = act2, name = 'fc3', num_hidden = 2 * int((n_in - n_out) / 4 // 1 + n_out))\n", "    act3 = mx.symbol.Activation(data = fc3, name = 'relu3', act_type = \"relu\")\n", "    fc4  = mx.symbol.FullyConnected(data = act3, name = 'fc4', num_hidden = int((n_in - n_out) / 4 // 1 + n_out))\n", "    act4 = mx.symbol.Activation(data = fc4, name = 'relu4', act_type = \"relu\")\n", "    fc5  = mx.symbol.FullyConnected(data = act4, name = 'fc5', num_hidden = n_out)\n", "    mlp  = mx.symbol.LogisticRegressionOutput(data = fc5, name = 'softmax')\n", "    return mlp\n", "\n", "optimizer_params = {'learning_rate': 0.1, \n", "                    'momentum' : 0.9,\n", "                    'wd' : 0.0001, \n", "                    'lr_scheduler': mx.lr_scheduler.FactorScheduler(step = 1000, factor = 0.5)}\n", "\n", "gini_metric = mx.metric.create(gini_nn)\n", "\n", "print('---training using multi layer perception---')\n", "for i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n", "    print('=====================================================')\n", "    print('kfold learning: {}  of  {} : '.format(i + 1, k))\n", "    \n", "    # process data\n", "    X_train, X_valid = X[train_index], X[valid_index]\n", "    y_train, y_valid = y[train_index], y[valid_index]\n", "    \n", "    # apply upsampling using olivier's approach: XGB classifier, upsampling LB 0.283\n", "    upsampling = False\n", "    if upsampling:\n", "        X_train = np.append(X_train, X_train[np.where(y_train == 1)[0], :], axis = 0)\n", "        y_train = np.append(y_train, y_train[np.where(y_train == 1)[0]], axis = 0)\n", "        X_train, y_train = shuffle(X_train, y_train)\n", "    \n", "    # mxnet ndarray\n", "    mx_train = mx.io.NDArrayIter(X_train, y_train, batch_size = 50000)\n", "    mx_valid = mx.io.NDArrayIter(X_valid, y_valid, batch_size = 50000)\n", "    #---------------------------------\n", "    model_mlp = mx.mod.Module(symbol = get_mlp(X.shape[1]), context = mx.gpu())\n", "    model_mlp.fit(mx_train,\n", "                  num_epoch          = 400,\n", "                  optimizer          = 'sgd',\n", "                  eval_data          = mx_valid,\n", "                  eval_metric        = gini_metric,\n", "                  optimizer_params   = optimizer_params,\n", "                  initializer        = mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2),\n", "                  epoch_end_callback = mx.callback.do_checkpoint('fold_' + str(i), 10),\n", "                 )"], "execution_count": 16}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "9dc6788534bd7d4ff17bca3c9e5264f396a6523b", "_cell_guid": "8471f424-985f-46e0-8994-fc7263287366"}, "cell_type": "code", "source": ["# collate results\n", "fold_pred_collator = pd.DataFrame()\n", "\n", "# setup prediction\n", "best_epoch = [170, 150, 120, 150, 150]\n", "for i in range(0, 5):\n", "    print('generating prediction using fold # %i model.' %i)\n", "    sym, arg_params, aux_params = mx.model.load_checkpoint('fold_' + str(i), best_epoch[i])\n", "    mdl_pred = mx.mod.Module(symbol = sym, context = mx.gpu(), label_names = None)\n", "    mdl_pred.bind(for_training = False, data_shapes = [('data', test_X.shape)], label_shapes = None)\n", "    mdl_pred.set_params(arg_params, aux_params, allow_missing=True)\n", "    mdl_pred.forward(Batch([mx.nd.array(test_X)]))\n", "    fold_pred_collator['fold_' + str(i)] = mdl_pred.get_outputs()[0].asnumpy().squeeze()\n", "    "], "execution_count": 35}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "1e1989a093f4c51337eba0205e5a68eb256cb554", "_cell_guid": "369a4068-cf87-436b-acf7-dffd0d85f7e8"}, "cell_type": "code", "source": ["sbmtn_result_df = test['id'].to_frame()\n", "sbmtn_result_df['target'] = fold_pred_collator.mean(axis = 1)"], "execution_count": 41}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "7a499111fd2b79b4da7a46402c7279539aca39ac", "_cell_guid": "2edabc48-b86d-4bf0-9c77-89fead56dbb9"}, "cell_type": "code", "source": ["sbmtn_result_df.to_csv('nn_submission.csv', index = False)"], "execution_count": 42}], "nbformat_minor": 1, "metadata": {"language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "name": "python", "version": "3.6.3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}}