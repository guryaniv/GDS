{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"## Introduction\nThis is an attempt to create a benchmark using Tensorflow and the Unet. **The 0.38 score is because the kernel is not training at all (actually, it predicts all 0 at the output).** Any help to solve this problem would be really appreciated."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aec92f72580464260d8d9634bcec3058264d8a21"},"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport pandas as pd\nimport tensorflow as tf\nimport gc\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom skimage.feature import canny\nfrom skimage import exposure\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc159597bdbe7def5be9c1e5ebf8b7504263899c"},"cell_type":"markdown","source":"## Usefull functions"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6a9278b925f34345dcbad756cec744e23685f3f3"},"cell_type":"code","source":"def print_progress(it, mIoU,loss):\n    # Calculate the accuracy on the training-set.\n    now = time.strftime(\"%c\")\n    print(\"Iteration \" + str(it) + \" --- mIoU: \" + str(mIoU) + \" --- Loss: \" + str(loss) + \" --- \" + now);\n\n\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs\n\n\ndef create_submission(pred,test_fns,score, threshold):\n    pred_dict = {fn[:-4]: RLenc(np.round(pred[i, :, :, 0] > threshold)) for i, fn in enumerate(test_fns)};\n    sub = pd.DataFrame.from_dict(pred_dict, orient='index')\n    sub.index.names = ['id']\n    sub.columns = ['rle_mask']\n    sub.to_csv('submission_'+str(score)+'.csv')\n\n\ndef transform_images(x_train,y_train,depths):\n    x = [i for i in x_train];\n    y = [i for i in y_train];\n    d = [i for i in depths];\n\n    for i in range(len(x_train)):\n        (h, w) = x_train[i].shape[:2];\n        center = (w / 2, h / 2);\n        # flip h\n        x.append(np.array(cv2.flip(x_train[i],0)))\n        y.append(np.array(cv2.flip(y_train[i],0)))\n        d.append(depths[i]);\n        # flip v\n        x.append(np.array(cv2.flip(x_train[i], 1)))\n        y.append(np.array(cv2.flip(y_train[i], 1)))\n        d.append(depths[i]);\n        # flip h & v\n        x.append(np.array(cv2.flip(x_train[i], -1)))\n        y.append(np.array(cv2.flip(y_train[i], -1)))\n        d.append(depths[i]);\n        '''\n        # rotate 90\n        M = cv2.getRotationMatrix2D(center,90,1.0)\n        x.append(np.array(cv2.warpAffine(x_train[i], M,(h,w))))\n        y.append(np.array(cv2.warpAffine(y_train[i], M,(h,w))))\n        d.append(depths[i]);\n        # rotate 180\n        M = cv2.getRotationMatrix2D(center, 180, 1.0)\n        x.append(np.array(cv2.warpAffine(x_train[i], M, (h, w))))\n        y.append(np.array(cv2.warpAffine(y_train[i], M, (h, w))))\n        d.append(depths[i]);\n        # rotate 270\n        M = cv2.getRotationMatrix2D(center, 270, 1.0)\n        x.append(np.array(cv2.warpAffine(x_train[i], M, (h, w))))\n        y.append(np.array(cv2.warpAffine(y_train[i], M, (h, w))))\n        d.append(depths[i]);\n        '''\n    return x, y, d;\n\n\ndef conv_layer(input, filters, kernel_size, strides, k_init, k_reg, activation=tf.nn.relu, dropout=0.,\n               p_type=None,p_size=(2,2),p_stride=(2,2)):\n\n    l = tf.layers.conv2d(inputs=input, filters=filters, kernel_size=kernel_size, strides=strides, activation=activation,\n                         kernel_initializer=k_init, kernel_regularizer=k_reg,padding='same');\n\n    if p_type == None:\n        d = tf.layers.dropout(inputs=l, rate=dropout);\n    elif p_type == 'avg':\n        a = tf.layers.average_pooling2d(inputs=l, pool_size=p_size, strides=p_stride);\n        d = tf.layers.dropout(inputs=a, rate=dropout);\n    elif p_type == 'max':\n        m = tf.layers.max_pooling2d(inputs=l, pool_size=p_size, strides=p_stride);\n        d = tf.layers.dropout(inputs=m, rate=dropout);\n\n    print(\"Layer created with shape: \" + str(d.shape))\n\n    return d;\n\n\ndef convt_layer(input, filters, kernel_size, strides, k_init, k_reg, activation=tf.nn.relu, dropout=0.,\n               p_type=None,p_size=(2,2),p_stride=(2,2)):\n\n    l = tf.layers.conv2d_transpose(inputs=input, filters=filters, kernel_size=kernel_size, strides=strides,\n                                   activation=activation,kernel_initializer=k_init, kernel_regularizer=k_reg, padding='same');\n\n    if p_type == None:\n        d = tf.layers.dropout(inputs=l, rate=dropout);\n    elif p_type == 'avg':\n        a = tf.layers.average_pooling2d(inputs=l, pool_size=p_size, strides=p_stride);\n        d = tf.layers.dropout(inputs=a, rate=dropout);\n    elif p_type == 'max':\n        m = tf.layers.max_pooling2d(inputs=l, pool_size=p_size, strides=p_stride);\n        d = tf.layers.dropout(inputs=m, rate=dropout);\n\n    print(\"Layer created with shape: \" + str(d.shape))\n\n    return d;","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e93f2290b6abb2b6d3bb8ce3f701404d82192c7"},"cell_type":"markdown","source":"## Data class"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1a72e1c70f960aee4469a7564524b1793e1e630b"},"cell_type":"code","source":"class Data():\n    def __init__(self):\n        self.width = 128;\n        self.channels = 1;\n        self.x_train = None;\n        self.y_train = None;\n        self.depths = None;\n        self.x_val = None;\n        self.y_val = None;\n        self.x_test = None;\n        self.next_batch = 0;\n        self.end_epoch = False;\n        self.test_fns = None;\n        self.feats = None;\n\n    def load_train(self):\n        print(\"Loading train data...\");\n        TRAIN_IMAGE_DIR = '../input/train/images/'\n        TRAIN_MASK_DIR = '../input/train/masks/'\n\n        train_fns = os.listdir(TRAIN_IMAGE_DIR)\n\n        depths = pd.read_csv('../input/depths.csv');\n        max_depth = np.max(depths['z'].values);\n        print('Max depth: ' + str(max_depth));\n        depths['z'] = np.array(depths['z'].values)/max_depth;\n\n        x_train = [np.array(cv2.resize(cv2.imread(TRAIN_IMAGE_DIR + p, cv2.IMREAD_GRAYSCALE),(128,128)),dtype=np.uint8) for p in train_fns]\n        x_train = [exposure.equalize_adapthist(x) for x in x_train];\n        x_train = np.array(x_train / 255\n        #x_train = np.expand_dims(x_train, axis=3)\n\n        y_train = [np.array(cv2.resize(cv2.imread(TRAIN_MASK_DIR + p, cv2.IMREAD_GRAYSCALE),(128,128)),dtype=np.uint8) for p in train_fns]\n        y_train = np.array(y_train) / 255\n        #y_train = np.expand_dims(y_train, axis=3)\n\n        self.width = x_train.shape[1];\n\n        depths = [np.full((self.width,self.width),depths[depths['id'] == p[:-4]]['z'].values) for p in train_fns];\n        #depths = np.expand_dims(depths, axis=3)\n\n        #self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train, self.y_train,\n        #                                                                      random_state=23, test_size=0.2)\n\n        x_train, y_train, depths = transform_images(x_train,y_train,depths);\n\n        feats = [canny(x) for x in x_train];\n\n        self.x_train = np.array(x_train);\n        del x_train\n        gc.collect()\n        self.y_train = np.array(y_train);\n        del y_train\n        gc.collect()\n        self.depths = np.array(depths);\n        del depths\n        gc.collect()\n        self.feats = np.array(feats);\n        del feats\n        gc.collect()\n\n        self.x_train = np.expand_dims(self.x_train, axis=3)\n        self.y_train = np.expand_dims(self.y_train, axis=3)\n        self.depths = np.expand_dims(self.depths, axis=3)\n        self.feats = np.expand_dims(self.feats, axis=3)\n        self.channels = self.x_train.shape[3];\n\n        self.shuffleTrainData();\n\n        print(\"Finish loading!\");\n        print(\"x_train shape:\");\n        print(self.x_train.shape);\n        print(\"y_train shape:\");\n        print(self.y_train.shape);\n        print(\"depths shape:\");\n        print(self.depths.shape);\n        print(\"feats shape:\");\n        print(self.feats.shape);\n\n    def load_test(self):\n        del self.x_train, self.y_train\n        gc.collect()\n        self.x_train = None;\n        self.y_train = None;\n        gc.collect()\n\n        print(\"Loading test data...\");\n        TEST_IMAGE_DIR = '../input/test/images/'\n\n        self.test_fns = os.listdir(TEST_IMAGE_DIR)\n\n        depths = pd.read_csv('../input/depths.csv');\n        max_depth = np.max(depths['z'].values);\n        print('Max depth: ' + str(max_depth));\n        depths['z'] = np.array(depths['z'].values) / max_depth;\n\n        self.depths = depths;\n\n        self.next_batch = 0;\n        self.end_epoch = False;\n\n        print(\"Finish loading!\");\n\n    def getNextTrainBatch(self,batch_size):\n        init = int(self.next_batch * batch_size);\n        end = int(init + batch_size);\n\n        self.next_batch += 1;\n\n        if end > len(self.x_train):\n            end = int(len(self.x_train));\n            init = int(end - batch_size);\n            self.end_epoch = True;\n            self.next_batch = 0;\n\n        x = self.x_train[init:end];\n        y = self.y_train[init:end];\n        z = self.depths[init:end];\n        f = self.feats[init:end];\n\n        return x, y, z, f;\n\n    def getNextTestBatch(self,batch_size):\n        TEST_IMAGE_DIR = '../input/test/images/';\n        init = int(self.next_batch * batch_size);\n        end = int(init + batch_size);\n\n        self.next_batch += 1;\n\n        if end >= len(self.test_fns):\n            end = int(len(self.test_fns));\n            self.end_epoch = True;\n            self.next_batch = 0;\n\n        test_fns = self.test_fns[init:end];\n        #print(\"Length test_fns: \" + str(len(test_fns)));\n        x_test = [\n            np.array(cv2.resize(cv2.imread(TEST_IMAGE_DIR + p, cv2.IMREAD_GRAYSCALE), (128, 128)), dtype=np.uint8) for p\n            in test_fns]\n\n        x_test = [exposure.equalize_adapthist(x) for x in x_test];\n\n        x_test = np.array(x_test) / 255\n\n        feats = [canny(x) for x in x_test];\n\n        x_test = np.expand_dims(x_test, axis=3)\n        #print(\"Length x_test: \" + str(len(x_test)));\n        width = x_test.shape[1];\n\n        depths = [np.full((width, width), self.depths[self.depths['id'] == p[:-4]]['z'].values) for p in test_fns];\n        depths = np.expand_dims(depths, axis=3)\n\n        feats = np.expand_dims(feats, axis=3)\n\n        return x_test, depths, feats, np.arange(init,end,1);\n\n    def shuffleTrainData(self):\n        rng_state = np.random.get_state();\n        np.random.shuffle(self.x_train);\n        np.random.set_state(rng_state);\n        np.random.shuffle(self.y_train);\n        np.random.set_state(rng_state);\n        np.random.shuffle(self.depths);\n        np.random.set_state(rng_state);\n        np.random.shuffle(self.feats);\n        self.next_batch = 0;\n        return;","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5c9bf740df0e5a3fe5a6fdfafeac90396c487de"},"cell_type":"markdown","source":"## Parameters and data loading"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b0554105d3d04deb605dcf76b588834f123342e2"},"cell_type":"code","source":"batch_size = 100;\ndropout_rate = .0;\nlearning_rate = .01;\nlr_decay = .9;\nreg = .0;\nmax_epochs = 1;\nsalt_threshold = .5;\n\ndata = Data();\ndata.load_train();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26eed2f0e77914fab3f2b64c113f9ae950720609"},"cell_type":"markdown","source":"## Building the computational graph"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"637620c2f495a6691f11a2084b43fb18fbfe8f61"},"cell_type":"code","source":"tf.reset_default_graph();\ntf.logging.set_verbosity(tf.logging.ERROR)\n# inputs\nwith tf.variable_scope('input'):\n    x = tf.placeholder(tf.float32, shape=[None, data.width, data.width, data.channels], name='x');\n    y_true = tf.placeholder(tf.float32, shape=[None, data.width, data.width, data.channels], name='y_true');\n    z = tf.placeholder(tf.float32, shape=[None, data.width, data.width, data.channels], name='z');\n    feats = tf.placeholder(tf.float32, shape=[None, data.width, data.width, 1], name='feats');\n    lr = tf.placeholder(tf.float32, name='learning_rate');\n    dropout = tf.placeholder(tf.float32, name='dropout');\n\nx_ = tf.concat([x,z,feats],axis=3);\n\nprint(\"Total input shape: \" + str(x_.shape));\n\ntf_batchsize = tf.shape(x)[0];\nxavier = tf.contrib.layers.xavier_initializer();\nreg_regr = tf.contrib.layers.l2_regularizer(scale=reg)\n\n# conv layers\nwith tf.variable_scope('encoder'):\n    e1 = conv_layer(input=x_, filters=8, kernel_size=(3,3), strides=(1,1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e2 = conv_layer(input=e1, filters=8, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout,p_type='avg');\n\n    e3 = conv_layer(input=e2, filters=16, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e4 = conv_layer(input=e3, filters=16, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout, p_type='avg');\n\n    e5 = conv_layer(input=e4, filters=32, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e6 = conv_layer(input=e5, filters=32, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout, p_type='avg');\n\n    e7 = conv_layer(input=e6, filters=64, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e8 = conv_layer(input=e7, filters=64, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout, p_type='avg');\n\n    e9 = conv_layer(input=e8, filters=128, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    e10 = conv_layer(input=e9, filters=128, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\nwith tf.variable_scope('decoder'):\n    d1 = convt_layer(input=e10, filters=64, kernel_size=(2,2), strides=(2,2), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    c1 = tf.concat([d1,e7],axis=3);\n    d2 = conv_layer(input=c1, filters=64, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    d3 = conv_layer(input=d2, filters=64, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                         dropout=dropout);\n\n    d4 = convt_layer(input=d3, filters=32, kernel_size=(2,2), strides=(2,2), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    c2 = tf.concat([d4,e5],axis=3);\n    d5 = conv_layer(input=c2, filters=32, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    d6 = conv_layer(input=d5, filters=32, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    d7 = convt_layer(input=d6, filters=16, kernel_size=(2,2), strides=(2,2), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    c3 = tf.concat([d7, e3], axis=3);\n    d8 = conv_layer(input=c3, filters=16, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    d9 = conv_layer(input=d8, filters=16, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\n    d10 = convt_layer(input=d9, filters=8, kernel_size=(2, 2), strides=(2, 2), k_init=xavier, k_reg=reg_regr,\n                         dropout=dropout);\n\n    c4 = tf.concat([d10, e1], axis=3);\n    d11 = conv_layer(input=c4, filters=8, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n    d12 = conv_layer(input=d11, filters=8, kernel_size=(3, 3), strides=(1, 1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout);\n\nwith tf.variable_scope('output'):\n    y_pred = convt_layer(input=d12, filters=1, kernel_size=(1,1), strides=(1,1), k_init=xavier, k_reg=reg_regr,\n                        dropout=dropout,activation=tf.nn.sigmoid);\n\nwith tf.variable_scope('loss'):\n    y_true_flat = tf.reshape(y_true,[tf_batchsize,-1]);\n    y_pred_flat = tf.reshape(y_pred, [tf_batchsize, -1]);\n    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=y_true_flat,\n                                               logits=y_pred_flat);\n        #loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred);\n\nwith tf.variable_scope('metrics'):\n    IoU, IoU_op = tf.metrics.mean_iou(labels=tf.cast(y_true,tf.int32),\n                                          predictions=tf.to_int32(y_pred > salt_threshold),num_classes=2,name='mIoU');\n\noptimizer = tf.train.AdamOptimizer(learning_rate=lr, name='op').minimize(loss);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"376d88433d3eedb271c97c4a845a7da40ae84392"},"cell_type":"markdown","source":"## Initializing variables"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e530ce9cdff5d2b3fa7702546d2165c919368d2c"},"cell_type":"code","source":"    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    session.run(tf.local_variables_initializer())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8427666e1a75d19a48323671c2cf05e587466d8"},"cell_type":"markdown","source":"## Starting training"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cbfd0c9510c10cfbba7d092b35083826195d56c4"},"cell_type":"code","source":"print(\"Starting training...\");\n\nepoch = 1;\nlr = learning_rate;\nstep = 0;\nmIoU = [];\nmIoU_over_steps = [];\nxent = [];\nloss_over_steps = [];\n\nfeed_dict_val = {'input/x:0': data.x_val, 'input/y_true:0': data.y_val, 'input/dropout:0': 0};\n\nwhile epoch <= max_epochs:\n\n    x_batch, y_batch, z_batch, f_batch = data.getNextTrainBatch(batch_size);\n\n    feed_dict_train = {'input/x:0': x_batch, 'input/y_true:0': y_batch, 'input/z:0': z_batch,\n                           'input/feats:0': f_batch,'input/learning_rate:0': lr,\n                           'input/dropout:0': dropout_rate};\n\n    _ = session.run(['op'], feed_dict=feed_dict_train);\n    _ = session.run([IoU_op], feed_dict=feed_dict_train);\n    mIoU,xent = session.run([IoU,loss], feed_dict=feed_dict_train);\n    mIoU_over_steps.append(mIoU);\n    loss_over_steps.append(xent);\n\n    if step % 10 == 0:\n            #_ = session.run([IoU_op], feed_dict=feed_dict_val);\n            #mIoU, xent = session.run([IoU, loss], feed_dict=feed_dict_val);\n        print_progress(step, np.mean(mIoU_over_steps), np.mean(loss_over_steps));\n        mIoU_over_steps = [];\n        loss_over_steps = [];\n        plt.imshow(y_batch[0]);\n    if data.end_epoch:\n        print('End of epoch ' + str(epoch));\n        epoch += 1;\n        lr *= lr_decay;\n        data.end_epoch = False;\n        data.shuffleTrainData();\n\n    step += 1;\n\ndel data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92d0479affb10013b9860f0a9c93d867fa118b7d"},"cell_type":"markdown","source":"## Predicting"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8562497d93f925b071c68cfbdfe460e8a6c6d6ea"},"cell_type":"code","source":"data = Data();\ndata.load_test();\nsub = np.zeros((len(data.test_fns), data.width, data.width, data.channels));\n\nwhile not data.end_epoch:\n    x, z, f, idx = data.getNextTestBatch(batch_size);\n    feed_dict_test = {'input/x:0': x, 'input/z:0': z, 'input/feats:0': f, 'input/dropout:0': 0};\n    sub[idx] = session.run([y_pred],feed_dict_test);\n\nsession.close();\n\nsubs_resized = [];\n\nfor i in range(len(sub)):\n    subs_resized.append(np.array(cv2.resize(sub[i],dsize=(101,101)),dtype=np.int8));\n\nsubs_resized = np.array(subs_resized);\nsubs_resized = np.expand_dims(subs_resized, axis=3)\nprint(subs_resized.shape)\n\ncreate_submission(subs_resized,data.test_fns,mIoU,salt_threshold)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}