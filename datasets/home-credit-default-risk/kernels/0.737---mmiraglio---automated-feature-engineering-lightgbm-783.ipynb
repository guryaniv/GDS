{"cells":[{"metadata":{"collapsed":true,"trusted":true,"_uuid":"26e1e582d8746c7b057a73ec64d3557f4da709bf"},"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nimport featuretools as ft\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cf78fbea4a1feb8c4d099b744a67850aa2a6d3a7"},"cell_type":"code","source":"\"\"\"\nI created this kernel to test the Featuretools framework to perform automated feature engineering\nhttps://docs.featuretools.com/index.html\n\nI got inspired by the following Towards Data Science post\nhttps://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219\n\nand I borrowed some code from James Shepherd's kernel --- LightGBM with weighted averages ---\nthat was of big help\nhttps://www.kaggle.com/shep312/lightgbm-with-weighted-averages-dropout-787\n\nI'm new to Data Science and machine learning so fork this kernel at your own risk ;)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d78f61b74d5c52ed33c56ea17146861faa9bf4a","collapsed":true},"cell_type":"code","source":"\"\"\" \nSample Size set to 25k to avoid Kaggle's Kernel execution timeout (6 hours)\nSetting this variable to None will process all lines from the input files, it took\nmore or less 5 hours on my machine (i7 processor with 16GB RAM)\n\"\"\"\n\n#sample_size = None\nsample_size = 25000 \n\n\"\"\" Load and process inputs \"\"\"\ninput_dir = os.path.join(os.pardir, 'input')\nprint('Input files:\\n{}'.format(os.listdir(input_dir)))\nprint('Loading data sets...')\napp_train_df = pd.read_csv(os.path.join(input_dir,'application_train.csv'), nrows=sample_size)\napp_test_df = pd.read_csv(os.path.join(input_dir,'application_test.csv'))\nprev_app_df = pd.read_csv(os.path.join(input_dir,'previous_application.csv'), nrows=sample_size)\nbureau_df = pd.read_csv(os.path.join(input_dir,'bureau.csv'), nrows=sample_size)\nbureau_balance_df = pd.read_csv(os.path.join(input_dir,'bureau_balance.csv'), nrows=sample_size)\ninstallments_df = pd.read_csv(os.path.join(input_dir,'installments_payments.csv'), nrows=sample_size)\ncc_balance_df = pd.read_csv(os.path.join(input_dir,'credit_card_balance.csv'), nrows=sample_size)\npos_balance_df = pd.read_csv(os.path.join(input_dir,'POS_CASH_balance.csv'), nrows=sample_size)\n\nprint('Data loaded.\\nMain application training data set shape = {}'.format(app_train_df.shape))\nprint('Main application test data set shape = {}'.format(app_test_df.shape))\nprint('Positive target proportion = {:.2f}'.format(app_train_df['TARGET'].mean()))","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf3f42fe0b917fe2beebff01061a38f5862d5bde","collapsed":true},"cell_type":"code","source":"# Merge the datasets into a single one for training\napp_both = pd.concat([app_train_df, app_test_df])","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c6c5f7843d269eaa599ca2a5709a9708d67ad30f"},"cell_type":"code","source":"# A lot of the continuous days variables have integers as missing value indicators.\nprev_app_df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42b7a9301b7f1514631369e8a605b195bc2fd993","collapsed":true},"cell_type":"code","source":"#Add new features\n# Amount loaned relative to salary\napp_both['LOAN_INCOME_RATIO'] = app_both['AMT_CREDIT'] / app_both['AMT_INCOME_TOTAL']\napp_both['ANNUITY_INCOME_RATIO'] = app_both['AMT_ANNUITY'] / app_both['AMT_INCOME_TOTAL']\n    \n# Number of overall payments (I think!)\napp_both['ANNUITY LENGTH'] = app_both['AMT_CREDIT'] / app_both['AMT_ANNUITY']\n    \n# Social features\napp_both['WORKING_LIFE_RATIO'] = app_both['DAYS_EMPLOYED'] / app_both['DAYS_BIRTH']\napp_both['INCOME_PER_FAM'] = app_both['AMT_INCOME_TOTAL'] / app_both['CNT_FAM_MEMBERS']\napp_both['CHILDREN_RATIO'] = app_both['CNT_CHILDREN'] / app_both['CNT_FAM_MEMBERS']","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9be7cd837eaf5337a69e0c0c2b02f13d59054aa6","collapsed":true},"cell_type":"code","source":"# Create new entityset\nes = ft.EntitySet(id='home_credit_default_risk')\n\napplications_var_types = {'FLAG_CONT_MOBILE': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_10': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_11': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_12': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_13': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_14': ft.variable_types.Boolean, \n                          'FLAG_DOCUMENT_15': ft.variable_types.Boolean, \n                          'FLAG_DOCUMENT_16': ft.variable_types.Boolean, \n                          'FLAG_DOCUMENT_17': ft.variable_types.Boolean, \n                          'FLAG_DOCUMENT_18': ft.variable_types.Boolean, \n                          'FLAG_DOCUMENT_19': ft.variable_types.Boolean, \n                          'FLAG_DOCUMENT_2': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_20': ft.variable_types.Boolean, \n                          'FLAG_DOCUMENT_21': ft.variable_types.Boolean, \n                          'FLAG_DOCUMENT_3': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_4': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_5': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_6': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_7': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_8': ft.variable_types.Boolean,\n                          'FLAG_DOCUMENT_9': ft.variable_types.Boolean,\n                          'FLAG_EMAIL': ft.variable_types.Boolean,\n                          'FLAG_EMP_PHONE': ft.variable_types.Boolean,\n                          'FLAG_MOBIL': ft.variable_types.Boolean,\n                          'FLAG_PHONE': ft.variable_types.Boolean,\n                          'FLAG_WORK_PHONE': ft.variable_types.Boolean,\n                          'LIVE_CITY_NOT_WORK_CITY': ft.variable_types.Boolean,\n                          'LIVE_REGION_NOT_WORK_REGION': ft.variable_types.Boolean,\n                          'REG_CITY_NOT_LIVE_CITY': ft.variable_types.Boolean,\n                          'REG_CITY_NOT_WORK_CITY': ft.variable_types.Boolean,\n                          'REG_REGION_NOT_LIVE_REGION': ft.variable_types.Boolean,\n                          'REG_REGION_NOT_WORK_REGION': ft.variable_types.Boolean,\n                          'TARGET': ft.variable_types.Discrete}\n\n# Create an entity from the applications (app_both) dataframe\n# This dataframe already has an index\nes = es.entity_from_dataframe(entity_id='applications',\n                              variable_types = applications_var_types,\n                              dataframe=app_both, index='SK_ID_CURR')\n\nbureau_var_types = {'CREDIT_ACTIVE': ft.variable_types.Categorical, \n                    'CREDIT_CURRENCY': ft.variable_types.Categorical,\n                    'CREDIT_TYPE': ft.variable_types.Categorical}\n\n# Create an entity from the bureau dataframe\n# This dataframe already has an index\nes = es.entity_from_dataframe(entity_id='bureau', \n                              variable_types = bureau_var_types,\n                              dataframe=bureau_df, index='SK_ID_BUREAU')\n\n# Create an entity from the bureau balance dataframe\nes = es.entity_from_dataframe(entity_id='bureau_balance', \n                              variable_types = {'MONTHS_BALANCE': ft.variable_types.Ordinal},\n                              make_index = True,\n                              dataframe=bureau_balance_df, index='bureau_balance_id')\n\n# Create an entity from the installments dataframe\nes = es.entity_from_dataframe(entity_id='installments',\n                              make_index = True,\n                              dataframe=installments_df, index='installment_id')\n\nprev_app_var_types = {'NFLAG_LAST_APPL_IN_DAY': ft.variable_types.Boolean,\n                      'NFLAG_INSURED_ON_APPROVAL': ft.variable_types.Boolean,\n                      'SELLERPLACE_AREA': ft.variable_types.Categorical}\n\n# Create an entity from the previous applications dataframe\nes = es.entity_from_dataframe(entity_id='previous_application',\n                              variable_types = prev_app_var_types,\n                              make_index = True,\n                              dataframe=prev_app_df, index='prev_app_id')\n\n# Create an entity from the credit card balance dataframe\nes = es.entity_from_dataframe(entity_id='cc_balance',\n                              variable_types = {'MONTHS_BALANCE': ft.variable_types.Ordinal},\n                              make_index = True,\n                              dataframe=cc_balance_df, index='cc_balance_id')\n\n# Create an entity from the POS Cash balance dataframe\nes = es.entity_from_dataframe(entity_id='pos_balance',\n                              variable_types = {'MONTHS_BALANCE': ft.variable_types.Ordinal},\n                              make_index = True,\n                              dataframe=pos_balance_df, index='pos_balance_id')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee9d2b8a5dec25797fd3fb12e8d587a4fce39d44","collapsed":true},"cell_type":"code","source":"# Relationship between applications and credits bureau\nr_applications_bureau = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['bureau']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_bureau)\n\n# Relationship between applications and credits bureau\nr_applications_installment = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['installments']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_installment)\n\n# Relationship between applications and credits bureau\nr_bureau_bureaubalance = ft.Relationship(es['bureau']['SK_ID_BUREAU'],\n                                    es['bureau_balance']['SK_ID_BUREAU'])\nes = es.add_relationship(r_bureau_bureaubalance)\n\n# Relationship between applications and previous applications\nr_applications_prev_apps = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['previous_application']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_prev_apps)\n\n# Relationship between applications and credit card balance\nr_applications_cc_balance = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['cc_balance']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_cc_balance)\n\n# Relationship between applications and POS cash balance\nr_applications_pos_balance = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['pos_balance']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_pos_balance)\n\nprint(es)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9fa4c1626caa119bd9499333d9a8ef538ce3e7d","collapsed":true},"cell_type":"code","source":"\"\"\"\nDeep Feature Synthesis (DFS) is an automated method for performing feature engineering on relational and transactional data.\nhttps://docs.featuretools.com/automated_feature_engineering/afe.html\n\"\"\"\n# Create new features using specified primitives\nfeature_matrix, feature_defs = ft.dfs(entityset = es, target_entity = 'applications',\n                                      drop_contains=['SK_ID_PREV'], max_depth=2, verbose=True)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bbdb9b962584178aa21f9092c4cb35934116f60a"},"cell_type":"code","source":"def process_dataframe(input_df, encoder_dict=None):\n    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n\n    # Label encode categoricals\n    print('Label encoding categorical features...')\n    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n    for feat in categorical_feats:\n        encoder = LabelEncoder()\n        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n    print('Label encoding complete.')\n\n    return input_df, categorical_feats.tolist(), encoder_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a18aa17666023a2496124913bba90c1158d207b6","collapsed":true},"cell_type":"code","source":"# Process the data set.\nfeature_matrix_enc, categorical_feats, encoder_dict = process_dataframe(input_df=feature_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c2a480c334470ebbe674276dc41e3e600eef592","collapsed":true},"cell_type":"code","source":"# Separate into train and test\ntrain_df = feature_matrix_enc[feature_matrix_enc['TARGET'].notnull()].copy()\n\ntest_df = feature_matrix_enc[feature_matrix_enc['TARGET'].isnull()].copy()\ntest_df.drop(['TARGET'], axis=1, inplace=True)\n\ndel feature_matrix, feature_defs, feature_matrix_enc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dfe91a06a170385bb1f0a028122ab573cfa5128","collapsed":true},"cell_type":"code","source":"\"\"\" Train the model \"\"\"\ntarget = train_df.pop('TARGET')\n\nlgbm_train = lgbm.Dataset(data=train_df,\n                          label=target,\n                          categorical_feature=categorical_feats,\n                          free_raw_data=False)\nlgbm_params = {\n    'boosting': 'dart',\n    'application': 'binary',\n    'learning_rate': 0.1,\n    'min_data_in_leaf': 30,\n    'num_leaves': 31,\n    'max_depth': -1,\n    'feature_fraction': 0.5,\n    'scale_pos_weight': 2,\n    'drop_rate': 0.02\n}\n\ncv_results = lgbm.cv(train_set=lgbm_train,\n                     params=lgbm_params,\n                     nfold=5,\n                     num_boost_round=600,\n                     early_stopping_rounds=50,\n                     verbose_eval=20,\n                     metrics=['auc'])\n\noptimum_boost_rounds = np.argmax(cv_results['auc-mean'])\nprint('Optimum boost rounds = {}'.format(optimum_boost_rounds))\nprint('Best CV result = {}'.format(np.max(cv_results['auc-mean'])))\n\nclf = lgbm.train(train_set=lgbm_train,\n                 params=lgbm_params,\n                 num_boost_round=optimum_boost_rounds)\n\n\"\"\" Predict on test set and create submission \"\"\"\ny_pred = clf.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"74534f456ac2aa869f5ff3f9b2e7f80529c24b26"},"cell_type":"code","source":"out_df = pd.DataFrame({'SK_ID_CURR': test_df.index, 'TARGET': y_pred})\nout_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}