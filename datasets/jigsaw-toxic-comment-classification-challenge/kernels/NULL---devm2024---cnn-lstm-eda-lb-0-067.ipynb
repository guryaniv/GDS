{"nbformat_minor": 1, "nbformat": 4, "cells": [{"cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "import pandas as pd\n", "import numpy as np\n", "import re\n", "from nltk import word_tokenize\n", "from nltk.corpus import stopwords\n", "from nltk.corpus import wordnet as wn\n", "from keras.layers import Bidirectional\n", "import codecs\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.text import text_to_word_sequence\n", "from sklearn.model_selection import train_test_split\n", "from keras.preprocessing.sequence import pad_sequences\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "metadata": {"_cell_guid": "83a4355c-1634-47f2-b05d-872900e20799", "collapsed": true, "_uuid": "37f13b65d17cd5afea513607f0181ddae2d280a6"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a5e8f789-deed-4aa1-a8e6-8ae759beba9c", "_uuid": "fa319c3c6260a43541fb42b15cc7d98cd054e80f"}, "source": ["TLDR: Inspired by the Jeremy's lstm, i tried using the convolutional layer on the top of that.\n", "Collectivley these type of codes are Long-term Recurrent Convolutional Networks or CNN-LSTM networks\n", "Here are few links explaining its effectiveness \n", "https://machinelearningmastery.com/cnn-long-short-term-memory-networks/\n", "\n", "https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n", "\n", "https://yerevann.github.io/2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/\n", "\n", "Adding the CNN on top of LSTM helps in the sense that CNN combined with pooling layers can bring out important temporal features devoid of any noise which the LSTM can use more effectively.\n", "In the end, bidirectional LSTM will help in classifying the data.\n", "\n", "Let's go to the code directly."]}, {"cell_type": "code", "source": ["train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n", "test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")"], "execution_count": null, "metadata": {"_cell_guid": "689d4aba-0ec7-4c86-9822-20e8a221de02", "collapsed": true, "_uuid": "bd5d04f9d0b5033601ff5bc9ee8d880af98d606f"}, "outputs": []}, {"cell_type": "code", "source": ["#Some data cleaning\n", "train.comment_text = train.comment_text.apply(lambda x : re.sub(' u ', 'you', x))\n", "train.comment_text = train.comment_text.apply(lambda x : re.sub('\\nu ', 'you', x))\n", "train.comment_text = train.comment_text.apply(lambda x : re.sub(' u\\n', 'you', x))\n", "train.comment_text = train.comment_text.apply(lambda x : re.sub(\"i'm\", 'i am', x))\n", "train.comment_text = train.comment_text.apply(lambda x : re.sub(\"fucksex\", 'fuck sex', x))\n", "\n", "test.comment_text = test.comment_text.apply(lambda x : re.sub(' u ', 'you', x))\n", "test.comment_text = test.comment_text.apply(lambda x : re.sub('\\nu ', 'you', x))\n", "test.comment_text = test.comment_text.apply(lambda x : re.sub(' u\\n', 'you', x))\n", "test.comment_text = test.comment_text.apply(lambda x : re.sub(\"i'm\", 'i am', x))\n", "test.comment_text = test.comment_text.apply(lambda x : re.sub(\"fucksex\", 'fuck sex', x))"], "execution_count": null, "metadata": {"_cell_guid": "48400efc-a312-44ca-b3a7-76ca0b2deec4", "collapsed": true, "_uuid": "cb418c4ba6f65b0b13d6ccbf847e8d6522caa83a"}, "outputs": []}, {"cell_type": "code", "source": ["from IPython.core.display import display, HTML\n", "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"], "execution_count": null, "metadata": {"_cell_guid": "36daad23-6f78-47ff-8bcd-276dd76fe061", "collapsed": true, "_uuid": "242f22632cfbb6f94c8d5da396e36915da9c8081"}, "outputs": []}, {"cell_type": "code", "source": ["#Let's see the words which constitutes the toxic comments\n", "def getwordcountdf(data, key):\n", "    filtered=data[data[key]==1]\n", "    sequence=[]\n", "    tr_words=set(stopwords.words('english'))\n", "    for x in filtered.comment_text:\n", "        sequence+=text_to_word_sequence(x, \n", "                                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n", "                                        lower=True,\n", "                                        split=\" \")\n", "    filtered_words = [word for word in sequence if word not in tr_words]\n", "    df= pd.DataFrame({'words': filtered_words})\n", "    z= df.groupby('words').size().reset_index(name='counts')\n", "    return z.sort_values('counts',ascending=False)\n"], "execution_count": null, "metadata": {"_cell_guid": "e1fb7b85-10d8-43d0-86a8-8142e245c3a9", "collapsed": true, "_uuid": "3f8cb02e393df02b0930a78e247cc6e6cf524d86"}, "outputs": []}, {"cell_type": "code", "source": ["obscene=getwordcountdf(train, 'obscene')\n", "toxic=getwordcountdf(train, 'toxic')\n", "severe_toxic=getwordcountdf(train, 'severe_toxic')\n", "threat=getwordcountdf(train, 'threat')\n", "insult=getwordcountdf(train, 'insult')\n", "identity_hate=getwordcountdf(train, 'identity_hate')"], "execution_count": null, "metadata": {"_cell_guid": "fa90e952-9ecc-4341-bb6c-08c08119a7e3", "collapsed": true, "_uuid": "1219f116407e7063b0218dadb85c3ce9433a8cc9"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e75f1a26-c53b-497c-a093-822087db19cc", "_uuid": "b4d02b8b7e4c757f17bb37b7c746d684b88465e9"}, "source": ["#### Lets look at the various keywords which are most frequent in each category of toxic comments"]}, {"cell_type": "code", "source": ["obscene.head(50).plot.bar(x='words', y='counts', figsize=(20,4))"], "execution_count": null, "metadata": {"_cell_guid": "693834d5-c258-4a45-9338-6dba8ee930cb", "collapsed": true, "_uuid": "a836cf9f683b5b6c65affbaa26aa2b99cc4d63d6"}, "outputs": []}, {"cell_type": "code", "source": ["severe_toxic.head(50).plot.bar(x='words', y='counts', figsize=(20,4))"], "execution_count": null, "metadata": {"_cell_guid": "f9d08cbd-16f1-4a0a-9ec7-ee583ee97e00", "collapsed": true, "_uuid": "93333c6ac0db662806933c8ce61a452af872c818"}, "outputs": []}, {"cell_type": "code", "source": ["threat.head(50).plot.bar(x='words', y='counts', figsize=(20,4))"], "execution_count": null, "metadata": {"_cell_guid": "1556b6d1-0a02-4e8e-bab0-04f6e8ae1b81", "collapsed": true, "_uuid": "32737fa7ed4210b8305b4ebf029d0b70de499493"}, "outputs": []}, {"cell_type": "code", "source": ["insult.head(50).plot.bar(x='words', y='counts', figsize=(20,4))"], "execution_count": null, "metadata": {"_cell_guid": "155e2b55-5b89-489b-82a2-ca47d6406f9b", "collapsed": true, "_uuid": "5500b11a41403f15ee70b5393d0fe877ef6a62af"}, "outputs": []}, {"cell_type": "code", "source": ["identity_hate.head(50).plot.bar(x='words', y='counts', figsize=(20,4))"], "execution_count": null, "metadata": {"_cell_guid": "9df808e1-ee23-4fdf-83e9-457823b40c23", "collapsed": true, "_uuid": "f39ed2b58c4dfcab80df6325e2390e1f65a16e5a"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "262faf05-611e-4065-bfdd-a8536bf1998b", "_uuid": "4c2a806613405f0d344e563abaaab9af2790dd30"}, "source": ["We can see that most frequent words do corresponds to the class they have been assigned to. \n", "These are really toxic words and we should get rid of these comments anyhow.\n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "1d3e0b3d-b100-4b88-bbb7-3827f62ae982", "_uuid": "56b03814aca1f15b28d54a65f3a834c945724f5f"}, "source": ["### Let's now remove the stopwords from the data.\n", "#### I know its not good idea to remove the stopwords from a LSTM but it seems to be working better this way.\n"]}, {"cell_type": "code", "source": ["def cleanupDoc(s):\n", "    stopset = set(stopwords.words('english'))\n", "    stopset.add('wikipedia')\n", "    tokens =sequence=text_to_word_sequence(s, \n", "                                        filters=\"\\\"!'#$%&()*+,-\u02da\u02d9./:;\u2018\u201c<=\u00b7>?@[]^_`{|}~\\t\\n\",\n", "                                        lower=True,\n", "                                        split=\" \")\n", "    cleanup = \" \".join(filter(lambda word: word not in stopset, tokens))\n", "    return cleanup\n", "\n", "test.comment_text=test.comment_text.apply(cleanupDoc)\n", "train.comment_text=train.comment_text.apply(cleanupDoc)"], "execution_count": null, "metadata": {"_cell_guid": "12215cc0-3426-4408-b1a2-fd0aa280eb4b", "collapsed": true, "_uuid": "651857b7c71909873a347574d6ec5f03d1de5609"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "f73a8140-41e7-4a20-b78a-120b17980862", "_uuid": "a9199a0751028fc991b63e2e02be7daef78507ed"}, "source": ["##### Standard tokenization of data to get the indexes of embedding matrix"]}, {"cell_type": "code", "source": ["tokenizer = Tokenizer()\n", "tokenizer.fit_on_texts(train.comment_text)\n", "sequences = tokenizer.texts_to_sequences(train.comment_text)\n", "test_sequence=tokenizer.texts_to_sequences(test.comment_text)"], "execution_count": null, "metadata": {"_cell_guid": "bede56fa-7331-4d17-9e83-711188356426", "collapsed": true, "_uuid": "6b177e756899943e5ec05b08995e7aeaf595576a"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "9a25a673-6e0f-4a5c-891b-f2d1fb8f3d3d", "_uuid": "a22736fe82db04beb33cb65e48d8f384ecaac6dc"}, "source": ["###### Pad the sequence so that we have the same length input, since keras supports same length input only"]}, {"cell_type": "code", "source": ["data = pad_sequences(sequences, maxlen=150)"], "execution_count": null, "metadata": {"_cell_guid": "b2d409c7-0f1c-4dab-8a7e-a55a9b4d2c74", "collapsed": true, "_uuid": "e103e368c0082948ad1a4e02fce6237a74553763"}, "outputs": []}, {"cell_type": "code", "source": ["t_data = pad_sequences(test_sequence, maxlen=150)"], "execution_count": null, "metadata": {"_cell_guid": "1a7bf316-97ed-45fd-9057-caedc060fc8e", "collapsed": true, "_uuid": "3f636568baa8d661d457814b9e93db8c767601ab"}, "outputs": []}, {"cell_type": "code", "source": ["### Load the pretrained glove vectors\n", "print('Indexing word vectors.')\n", "embeddings_index = {}\n", "f = codecs.open('../input/glove6b300dtxt/glove.6B.300d.txt', encoding='utf-8')\n", "for line in f:\n", "    values = line.split(' ')\n", "    word = values[0]\n", "    coefs = np.asarray(values[1:], dtype='float32')\n", "    embeddings_index[word] = coefs\n", "f.close()\n", "print('Found %s word vectors.' % len(embeddings_index))"], "execution_count": null, "metadata": {"_cell_guid": "1dc4998a-aac5-4a8a-a7fc-e59a27359f58", "collapsed": true, "_uuid": "14b6925ebcc5a1648c80b4e20182cc9dad27481e"}, "outputs": []}, {"cell_type": "code", "source": ["vector_length=300\n", "length=150\n", "num_classes=6"], "execution_count": null, "metadata": {"_cell_guid": "b780254c-531a-4472-b7c2-15b296869315", "collapsed": true, "_uuid": "2eeb292be304613c4939e8887aac0efae94084c7"}, "outputs": []}, {"cell_type": "code", "source": ["word_index = tokenizer.word_index\n", "print('Preparing embedding matrix.')\n", "# prepare embedding matrix\n", "nb_words = min(200000, len(word_index))\n", "notfound=[]\n", "embedding_matrix = np.zeros((nb_words, vector_length))\n", "for word, i in word_index.items():\n", "    if i >= nb_words:\n", "        continue\n", "    embedding_vector = embeddings_index.get(word)\n", "    if embedding_vector is not None:\n", "        # words not found in embedding index will be all-zeros.\n", "        embedding_matrix[i] = embedding_vector\n", "    else:\n", "        notfound.append(word)\n", "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"], "execution_count": null, "metadata": {"_cell_guid": "7092e7cb-3707-40c5-9734-2e22c0e0c162", "collapsed": true, "_uuid": "1775cb72ebf04cfb24fc4208ee2a1abd57232dbe"}, "outputs": []}, {"cell_type": "code", "source": ["embedding_matrix.shape"], "execution_count": null, "metadata": {"_cell_guid": "05452c4a-67b4-4aac-a029-c3ab2d9b70f4", "collapsed": true, "_uuid": "2b8e69e74850018efdad39e31893aabf8cffe679"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "28992d9e-7eb9-447c-b7a5-19d369b18469", "_uuid": "35a59ea7289d1f6c80cc81f19b70daeec99c5bb2"}, "source": ["### Here is the keras model with CNN + LSTM"]}, {"cell_type": "code", "source": [], "execution_count": null, "metadata": {"_cell_guid": "1956a0d3-e710-4419-8dc3-b438acd67592", "collapsed": true, "_uuid": "329848b9f4c53091d3eaa2074c434ac0453ea76e"}, "outputs": []}, {"cell_type": "code", "source": ["#keras stuff now\n", "import os\n", "import csv\n", "import codecs\n", "from keras.layers import LSTM, Convolution1D,Convolution2D, Flatten, Dropout, Dense, Input, Conv1D, GRU, GlobalMaxPooling1D\n", "from keras.layers.embeddings import Embedding\n", "from keras.models import Sequential\n", "from keras.layers import Merge, merge, concatenate\n", "from keras.models import Model\n", "from keras.layers.normalization import BatchNormalization\n", "from keras import backend as K\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.utils.np_utils import to_categorical\n", "from keras.layers import Conv1D, MaxPooling1D, Embedding\n", "#Using Pretrained Embedding Matrix"], "execution_count": null, "metadata": {"_cell_guid": "2bae0bae-f0aa-44a8-b1e4-b3b89a87f113", "collapsed": true, "_uuid": "7dcc704d2c76e2b4585e84d39414150f5236b760"}, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Since it is a multi label classification, we need to have 6 sigmoid units in the last layer rather than having a softmax layer."]}, {"cell_type": "code", "source": ["def getjmodel():\n", "    inp = Input(shape=(length,))\n", "    x = Embedding(nb_words, vector_length, weights=[embedding_matrix])(inp)\n", "    x = Conv1D(256, 3, activation='relu')(x)\n", "    x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n", "    x = GlobalMaxPooling1D()(x)\n", "    x = Dense(50, activation=\"relu\")(x)\n", "    x = Dropout(0.1)(x)\n", "    x = Dense(6, activation=\"sigmoid\")(x)\n", "    model = Model(inputs=inp, outputs=x)\n", "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "    return model"], "execution_count": null, "metadata": {"_cell_guid": "feb67a35-d8d1-4a94-ab0b-b732182ccbfb", "collapsed": true, "_uuid": "363fd9a10c70ba9df90ee35d5697c38abed38bec"}, "outputs": []}, {"cell_type": "code", "source": ["labels=train[['toxic','severe_toxic', 'obscene','threat','insult', 'identity_hate' ]]\n", "data_train, data_test, y_train, y_test, comm_train, comm_trst =train_test_split(data, np.array(labels),train.comment_text, test_size=0.20, random_state=22)"], "execution_count": null, "metadata": {"_cell_guid": "799015d2-71e9-47cd-b1f8-62d9fd4a7892", "collapsed": true, "_uuid": "cacd9365851fe8a6d699ed8e24da29c49811108a"}, "outputs": []}, {"cell_type": "code", "source": ["galaxyModel=getjmodel()\n", "galaxyModel.fit(data_train, y_train, 1024, epochs=3, validation_data=(data_test, y_test))"], "execution_count": null, "metadata": {"_cell_guid": "c38dcc1f-5d2c-4206-bd82-fe2430cfa1bb", "collapsed": true, "_uuid": "fbbffb628bf2ce4e123ec04f97f22809ead08ec8"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "bc9566dc-fc5f-4592-ac48-2c64c0faf093", "_uuid": "ad89006a766276055a9572cf7b819a4b2b473119"}, "source": ["### submission script"]}, {"cell_type": "code", "source": ["preds=galaxyModel.predict(t_data)\n", "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n", "sample_submission = pd.read_csv('sample_submission.csv')\n", "sample_submission[list_classes] = preds\n", "sample_submission.to_csv('mysubmission.csv', index=False)"], "execution_count": null, "metadata": {"_cell_guid": "1e3cc6bd-f1ed-4dbb-ad85-369dfccd8240", "collapsed": true, "_uuid": "a492497720350c36511e1e4aa7ce9856a3633b19"}, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "de43d097-7882-4b44-bfcc-d29771341c87", "_uuid": "9fef63f0a9c1ed9098ae2825c0325fbb7704ce0b"}, "source": ["# Future tasks:\n", "1. Add data augmentation\n", "2. Improve model hyperparameters\n", "3. Correct misspelled words. \n", "\n", "Thanks for looking at the kernel, hit the like button if you found it useful."]}, {"cell_type": "code", "source": [], "execution_count": null, "metadata": {"_cell_guid": "d3a8d693-45ce-4dab-9d47-06887a3d3748", "collapsed": true, "_uuid": "094e323e1ffbdb797ac34c5ebcca2d903fda45b3"}, "outputs": []}], "metadata": {"language_info": {"pygments_lexer": "ipython3", "version": "3.6.4", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "name": "python", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}}