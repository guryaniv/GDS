{"cells": [{"source": ["# Introduction\n", "\n", "In this notebook, we will be representing our documents as a TF-IDF vector , which is a way to model your text data into vectors , because all the libraries which you will generally use, tends to represent the data  into mathematical models. Then we will train a classifer(naiveBayes, SVM) on the dataset and then will evaluate our model. The tutorial will progress as follows:\n", "\n", "\n", "1. **Preprocess your Data with Gensim** - We will be using gensim library along with nltk, to quickly and efficiently preprocess our data. \n", "\n", "2. **Building the Model** - We will be training Naive Bayes classifier on our data\n", "\n", "3. **Train the Model** - We will be training Naive Bayes classifier on our data\n", "\n", "4. **Prediction**\n", "\n", "5. **Evaluating the Model** - We will be evaluating our model\n", "\n", "**Note:** I won't be doing a lot of Exploratory Data analysis as there are already public kernels available. Please go through **Spooky NLP and Topic Modelling tutorial** for data visualization purpose"], "cell_type": "markdown", "metadata": {"_cell_guid": "9e222406-13a5-4b68-9a6f-8aa1dc7357b3", "_uuid": "b3289510811141b46a0a923361f9b84f9a6a1611"}}, {"outputs": [], "metadata": {"_cell_guid": "f528cef0-7c8c-4c87-bcc4-be3cc215fbb2", "_uuid": "86d55df4366588c592aa929b419fc83cb66b14f4", "collapsed": true}, "cell_type": "code", "source": ["# read in some helpful libraries\n", "import nltk                       # the natural langauage toolkit, open-source NLP\n", "import pandas as pd               # pandas dataframe\n", "import re                         # regular expression\n", "from nltk.corpus import stopwords  \n", "from gensim import parsing        # Help in preprocessing the data, very efficiently\n", "import gensim\n", "import numpy as np\n", "\n", "# Loading in the training data with Pandas\n", "df_train = pd.read_csv(\"../input/train.csv\")"], "execution_count": 2}, {"outputs": [], "metadata": {"_cell_guid": "7f8673ec-ad86-4ae3-a4f7-4fc9d57febd1", "_uuid": "71b523d00ef5c1b0b84400944d55eff35674e191"}, "cell_type": "code", "source": ["# look at the first few rows and how the text looks like\n", "print (df_train['text'][2]) , '\\n'\n", "df_train.head()"], "execution_count": 3}, {"outputs": [], "metadata": {"_cell_guid": "a1379b65-2bcb-4d2b-9678-9f103212d2bb", "_uuid": "36f8b401b35888936af29ab799be4e52ab89d311"}, "cell_type": "code", "source": ["## check the dimensions of the table\n", "print (\"Shape:\", df_train.shape, '\\n')\n", "\n", "## Check if there is any NULL values inside the dataset\n", "print (\"Null Value Statistics:\", '\\n \\n', df_train.isnull().sum()) ## Sum will tell the total number of NULL values inside the dataset\n", "print ('\\n')\n", "\n", "## Explore the data types of your dataset\n", "print (\"Data Type of All Columns:\" '\\n \\n', df_train.dtypes)"], "execution_count": 4}, {"outputs": [], "metadata": {"_cell_guid": "60e1ef11-9ee4-4544-94da-1731fb9c438b", "_uuid": "66596dd7e693966f5a7ce4049fc227a6ebf50172"}, "cell_type": "code", "source": ["## Collect all unique author names from author column\n", "author_names = df_train['author'].unique()\n", "print (author_names)"], "execution_count": 5}, {"source": ["For performance reason it is good to convert the target variable(author name) in **some sort of coding format** "], "cell_type": "markdown", "metadata": {"_cell_guid": "afc6df98-c926-4d01-b13a-b65dd0b46d0e", "_uuid": "c8f583e89a5a103afd0c24e7b8998922c0d28b3c"}}, {"outputs": [], "metadata": {"_cell_guid": "741abbf8-189a-45b9-b54d-d905bd29418d", "_uuid": "110d06796aa8c411294d01f4cad656006c4c62ec"}, "cell_type": "code", "source": ["\"\"\"\n", "MWS 2\n", "EAP 0\n", "HPL 1\n", "\"\"\" \n", "authorname_to_id = {}\n", "assign_id = 0\n", "for name in author_names:\n", "    authorname_to_id[name] = assign_id\n", "    assign_id += 1  ## Get a new id for new author\n", "    \n", "##  Print the dictionary created\n", "for key, values in authorname_to_id.items():\n", "    print (key, values)"], "execution_count": 6}, {"outputs": [], "metadata": {"_cell_guid": "6fe6e541-ba72-4333-85d5-2a5743839cd8", "_uuid": "d85960d4bc8a3075588db56e9bc7556faeaf19d5"}, "cell_type": "code", "source": ["## convert the author name to id --> So when we predict the result humans can understand\n", "\"\"\"\n", "0 EAP\n", "1 HPL\n", "2 MWS\n", "\"\"\" \n", "id_to_author_name = {v: k for k, v in authorname_to_id.items()}\n", "for key, values in id_to_author_name.items():\n", "    print (key, values)"], "execution_count": 7}, {"outputs": [], "metadata": {"_cell_guid": "c54e47b3-99a3-4ba6-8769-d7c3ab746334", "_uuid": "ae0284a84c7abb3f6957ffe0b5cf842bf4738135", "collapsed": true}, "cell_type": "code", "source": ["## Add a new column to pandas dataframe, with the author name mapping\n", "def get_author_id(author_name):\n", "    return authorname_to_id[author_name]\n", "\n", "df_train['author_id'] = df_train['author'].map(get_author_id)"], "execution_count": 8}, {"outputs": [], "metadata": {"_cell_guid": "5d7e431d-ee32-45b1-a82c-29fcd5636ddc", "_uuid": "88c6113db1f09a935424327b7c93bfe0982e8285"}, "cell_type": "code", "source": ["df_train.head()"], "execution_count": 9}, {"source": ["## 1. Preprocessing the Data"], "cell_type": "markdown", "metadata": {"_cell_guid": "5dde30b9-5bd4-4971-aa5f-370c6d5710b2", "_uuid": "fecce442fe2dc3ac996f93fe7a39c2ba34ebe71d"}}, {"outputs": [], "metadata": {"_cell_guid": "12d6cb4f-728e-4c67-8b1c-43d65b1bb73d", "_uuid": "f5b25b9237f6669fb9cdb5e8c1fb7cf02b0aeffa", "collapsed": true}, "cell_type": "code", "source": ["def transformText(text):\n", "    \n", "    stops = set(stopwords.words(\"english\"))\n", "    \n", "    # Convert text to lower\n", "    text = text.lower()\n", "    # Removing non ASCII chars    \n", "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n", "    \n", "    # Strip multiple whitespaces\n", "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n", "    \n", "    # Removing all the stopwords\n", "    filtered_words = [word for word in text.split() if word not in stops]\n", "    \n", "    # Removing all the tokens with lesser than 3 characters\n", "    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n", "    \n", "    # Preprocessed text after stop words removal\n", "    text = \" \".join(filtered_words)\n", "    \n", "    # Remove the punctuation\n", "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n", "    \n", "    # Strip all the numerics\n", "    text = gensim.parsing.preprocessing.strip_numeric(text)\n", "    \n", "    # Strip multiple whitespaces\n", "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n", "    \n", "    # Stemming\n", "    return gensim.parsing.preprocessing.stem_text(text)"], "execution_count": 10}, {"outputs": [], "metadata": {"_cell_guid": "56297a46-d6de-40a7-9ea2-f6c987c78594", "_uuid": "bb8fa68a7d30696f0467d2517c330043e78f9d86", "collapsed": true}, "cell_type": "code", "source": ["df_train['text'] = df_train['text'].map(transformText)"], "execution_count": 11}, {"outputs": [], "metadata": {"_cell_guid": "1f3921e2-6ce3-42cb-b303-14f3098affcb", "_uuid": "86cda089fe81da4b0150bddb901454f588ff245e"}, "cell_type": "code", "source": ["## Print a couple of rows after the preprocessing of the data is done\n", "\n", "print (df_train['text'][0] , '\\n')\n", "print (df_train['text'][1] , '\\n')\n", "print (df_train['text'][2])"], "execution_count": 12}, {"source": ["### We will be dividing our training data into **(Train, Test)**. So that we can evaluate the model"], "cell_type": "markdown", "metadata": {"_cell_guid": "53b5d611-1754-40e1-a7e5-f0689420c814", "_uuid": "b62b2b8c42a46094e3973828a5e01aae687e7698"}}, {"outputs": [], "metadata": {"_cell_guid": "6c08ecc9-a5f6-4f44-a435-e3783185627d", "_uuid": "c1f27b1f953869150f66fc11158c0b03a1cd5e63", "collapsed": true}, "cell_type": "code", "source": ["## Split the data \n", "from sklearn.model_selection import train_test_split\n", "X_train, X_test, y_train, y_test = train_test_split(df_train['text'], df_train['author_id'], \n", "                                                    test_size=0.33, random_state=42)"], "execution_count": 13}, {"outputs": [], "metadata": {"_cell_guid": "84694c85-9871-4ea1-8d45-65b3d815d215", "_uuid": "5418839ceb96d24881279e4e9bf4466f6f802d1a"}, "cell_type": "code", "source": ["print (\"Training Sample Size:\", len(X_train), ' ', \"Test Sample Size:\" ,len(X_test))"], "execution_count": 14}, {"source": ["## 2. Building the Model"], "cell_type": "markdown", "metadata": {"_cell_guid": "662d46ec-01d7-439c-8235-77dfebc8e810", "_uuid": "b8d8f1b42718c05b0b57c6ad6631a33a34b80444"}}, {"outputs": [], "metadata": {"_cell_guid": "b75d3944-d141-4552-9822-b4e0185ccf90", "_uuid": "7f8713fa43f2579cdc55149849ec232bf3bf65cc"}, "cell_type": "code", "source": ["## Get the word vocabulary out of the data\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "count_vect = CountVectorizer()\n", "X_train_counts = count_vect.fit_transform(X_train)\n", "X_train_counts.shape\n", "\n", "## Count of 'mistak' in corpus (mistake -> mistak after stemming)\n", "print ('mistak appears:', count_vect.vocabulary_.get(u'mistak') , 'in the corpus')"], "execution_count": 15}, {"outputs": [], "metadata": {"_cell_guid": "b17dabb2-42bc-403a-af23-32ec7301693c", "_uuid": "852b61e4f1fcd834dc02ab7eed54b8e0537a537f"}, "cell_type": "code", "source": ["## Get the TF-IDF vector representation of the data\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "tfidf_transformer = TfidfTransformer()\n", "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n", "print ('Dimension of TF-IDF vector :' , X_train_tfidf.shape)"], "execution_count": 16}, {"source": ["## 3. Training a classifier"], "cell_type": "markdown", "metadata": {"_cell_guid": "01a8c859-362a-46f8-ae30-b193913dc83d", "_uuid": "d56be71925c003ce43127ee425ce861255c7c8eb"}}, {"outputs": [], "metadata": {"_cell_guid": "3c5cae4e-8a77-45df-83d4-0ec05aeb461f", "_uuid": "df3cc66ac7bea646148caac88ba0c0b786bd46a9", "collapsed": true}, "cell_type": "code", "source": ["from sklearn.naive_bayes import MultinomialNB\n", "clf = MultinomialNB().fit(X_train_tfidf, y_train)"], "execution_count": 17}, {"source": ["## 4. Prediction"], "cell_type": "markdown", "metadata": {"_cell_guid": "25b06766-4d93-4b42-a3e8-b6dcf2894b68", "_uuid": "1788457e6ddfcb6e7d1e11e984f5aba64e2778a6"}}, {"outputs": [], "metadata": {"_cell_guid": "7d4086f0-55ec-4f30-b170-22a529e180c9", "_uuid": "cc7d05c7ec10eee14e13de3c3b22be36f51441ec", "collapsed": true}, "cell_type": "code", "source": ["## Prediction part\n", "\n", "X_new_counts = count_vect.transform(X_test)\n", "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n", "\n", "predicted = clf.predict(X_new_tfidf)"], "execution_count": 18}, {"outputs": [], "metadata": {"_cell_guid": "292e151c-4cf9-4784-96ef-3ea6f91e5a8d", "_uuid": "1a2fe39f7554cbcaecc3a7f5871bc1e14acd2157"}, "cell_type": "code", "source": ["## predictions for first 10 test samples\n", "\n", "counter  = 0\n", "for doc, category in zip(X_test, predicted):\n", "    print('%r => %s' % (doc, id_to_author_name[category]))\n", "    if(counter == 10):\n", "        break\n", "    counter += 1    "], "execution_count": 19}, {"source": ["## 5. Evaluation \n", "\n", "We will be doing simple evaluation scheme and will conclude  mean of the **correct predictions** as our accuracy"], "cell_type": "markdown", "metadata": {"_cell_guid": "38420227-83e9-484e-bbae-329a8cde5bd1", "_uuid": "24c7df3e9f0ea00c1f61556952a9e4dd69226491"}}, {"outputs": [], "metadata": {"_cell_guid": "ea161d8c-5e6d-477a-8284-790649979340", "_uuid": "fe6e0e9c320b4d56ce719f0166f863f41b8eff94"}, "cell_type": "code", "source": ["np.mean(predicted == y_test) ## 80% sounds good only "], "execution_count": 20}, {"source": ["# Where to go from Here:\n", "Here are my couple of ideas to try:\n", "1. Use a better classifier (SVM)\n", "2. Go with word2vec representation of the data rather than TF-IDF\n", "\n", "**Note:** These are my possible ideas, feel free to comment and let me know the sections of the code which \n", "                    can be improved."], "cell_type": "markdown", "metadata": {"_cell_guid": "c1e92cd9-daab-477b-b478-d43490a65493", "_uuid": "24cc73bea878dc8a94a8a46778c7a10ec316dd93"}}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.3", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python"}}}