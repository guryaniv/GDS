{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport datetime as dt\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport os\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Load dataset**\n\nFirst we will load the train.csv dataset. Since this dataset has 55M rows, we will only use the first 1M to build our model to prevent memory issues and speed up preprocessing and model building."},{"metadata":{"trusted":true,"_uuid":"41e7502eb90efe0bf213974dd45d8bf686b5da7c","collapsed":true},"cell_type":"code","source":"train_df =  pd.read_csv('../input/train.csv', nrows = 1_000_000)\ntrain_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67a50c3072827032399c98571e1563975e9fab08"},"cell_type":"markdown","source":"**Data exploration**\n\nNow we will explore the loaded data to identify outliers and other problems that might need fixing such as null values."},{"metadata":{"trusted":true,"_uuid":"13dc61c19a6fd7ec53fb0c4c1947f9e973c745c9","collapsed":true},"cell_type":"code","source":"#Identify null values\nprint(train_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ed9ec358e20228ebef5f3c79b080f862a247c35"},"cell_type":"markdown","source":"We have a few rows with null values so it is safe to remove them."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"63af8f6b05047dbca96363a13068b082f17bf33d"},"cell_type":"code","source":"#Drop rows with null values\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cdc7b60c4ffa97154278cb7d912ada941d4b004"},"cell_type":"markdown","source":"Now let's explore the variables in the dataset. First we will look at the first rows to get an idea of the format of the values and then we will plot them to get a sense of their distribution and identify outliers."},{"metadata":{"trusted":true,"_uuid":"b0a5761b710bf2b1465c732f5869ad521f84dd1f","collapsed":true},"cell_type":"code","source":"#Look at the first rows\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7e63eddd28a5786b1cb9f2fced34e260a2fbde6","collapsed":true},"cell_type":"code","source":"#Plot variables using only 1000 rows for efficiency\ntrain_df.iloc[:1000].plot.scatter('pickup_longitude', 'pickup_latitude')\ntrain_df.iloc[:1000].plot.scatter('dropoff_longitude', 'dropoff_latitude')\n\n#Get distribution of values\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8df4397ae5fee4edb2c7fdd0e03b40b0ec1c7182"},"cell_type":"markdown","source":"Okay, that was interesting. We learned a few things about the dataset:\n- Fare_amount has negative values. We will remove those.\n- Latitudes and longitudes have values near 0 that cannot be correct since NYC is at (40,-74) aprox. We will remove points not near these coordinates.\n- Passenger_count has values of 0 and as high as 200, which are also unrealistic. We will remove those.\n"},{"metadata":{"trusted":true,"_uuid":"286761fca37f938207d1bee8a7b4ede99388fdd1","collapsed":true},"cell_type":"code","source":"#Clean dataset\ndef clean_df(df):\n    return df[(df.fare_amount > 0) & \n            (df.pickup_longitude > -80) & (df.pickup_longitude < -70) &\n            (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &\n            (df.dropoff_longitude > -80) & (df.dropoff_longitude < -70) &\n            (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45) &\n            (df.passenger_count > 0) & (df.passenger_count < 10)]\n\ntrain_df = clean_df(train_df)\nprint(len(train_df))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"759a91c11c07b15d5b7f8c6b1ee61cb3aa3bd61e"},"cell_type":"markdown","source":"**Feature engineering**\n\nNow that we have cleaned some extreme values, we will add some interesting features in the dataset.\n- total_distance: distance from pickup to dropoff\n- Extract information from datetime (day of week, month, hour, day)"},{"metadata":{"trusted":true,"_uuid":"cf4dc24dc12477bb24c32ee52f722fe12902ea51","collapsed":true},"cell_type":"code","source":"def sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    #Define earth radius (km)\n    R_earth = 6371\n    #Convert degrees to radians\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = dropoff_lon - pickup_lon\n    \n    #Compute haversine distance\n    a = np.sin(dlat/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon/2.0)**2\n    \n    return 2 * R_earth * np.arcsin(np.sqrt(a))\n\ndef add_datetime_info(dataset):\n    #Convert to datetime format\n    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'])\n    \n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['weekday'] = dataset.pickup_datetime.dt.weekday\n    \n    return dataset\n\ntrain_df['distance'] = sphere_dist(train_df['pickup_latitude'], train_df['pickup_longitude'], \n                                   train_df['dropoff_latitude'] , train_df['dropoff_longitude'])\n\ntrain_df = add_datetime_info(train_df)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158fd6b7a6d9a584418ec1a0be2b0cbe9311e629","collapsed":true},"cell_type":"markdown","source":"Now we need to drop the columns that we will not use to train our model.\n- key\n- pickup_datetime"},{"metadata":{"_uuid":"2a7ceec12850b9eaddc6e6f5500d8916459cc6cb","trusted":true,"collapsed":true},"cell_type":"code","source":"train_df.drop(columns=['key', 'pickup_datetime'], inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"443a99877d718d52e25bde3876f8ad98cbb2ca0b"},"cell_type":"markdown","source":"### Add rotational latitude and longitudes"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0a122590b9e48cd0121c414d222176f5b32088f5"},"cell_type":"code","source":"#y' = y*cos(a) - x*sin(a)\n#x' = y*sin(a) + x*cos(a)\ntrain_df['pickup_long_15'] = train_df['pickup_longitude']*np.cos(15* np.pi / 180) - train_df['pickup_latitude']*np.sin(15* np.pi/180)\ntrain_df['pickup_long_30'] = train_df['pickup_longitude']*np.cos(30* np.pi / 180) - train_df['pickup_latitude']*np.sin(30* np.pi/180)\ntrain_df['pickup_long_45'] = train_df['pickup_longitude']*np.cos(45* np.pi / 180) - train_df['pickup_latitude']*np.sin(45* np.pi/180)\ntrain_df['pickup_long_60'] = train_df['pickup_longitude']*np.cos(60* np.pi / 180) - train_df['pickup_latitude']*np.sin(60* np.pi/180)\ntrain_df['pickup_long_75'] = train_df['pickup_longitude']*np.cos(75* np.pi / 180) - train_df['pickup_latitude']*np.sin(75* np.pi/180)\n\ntrain_df['pickup_lat_15'] = train_df['pickup_longitude']*np.sin(15* np.pi / 180) + train_df['pickup_latitude']*np.cos(15* np.pi/180)\ntrain_df['pickup_lat_30'] = train_df['pickup_longitude']*np.sin(30* np.pi / 180) + train_df['pickup_latitude']*np.cos(30* np.pi/180)\ntrain_df['pickup_lat_45'] = train_df['pickup_longitude']*np.sin(45* np.pi / 180) + train_df['pickup_latitude']*np.cos(45* np.pi/180)\ntrain_df['pickup_lat_60'] = train_df['pickup_longitude']*np.sin(60* np.pi / 180) + train_df['pickup_latitude']*np.cos(60* np.pi/180)\ntrain_df['pickup_lat_75'] = train_df['pickup_longitude']*np.sin(75* np.pi / 180) + train_df['pickup_latitude']*np.cos(75* np.pi/180)\n\ntrain_df['dropoff_long_15'] = train_df['dropoff_longitude']*np.cos(15* np.pi / 180) - train_df['dropoff_latitude']*np.sin(15* np.pi/180)\ntrain_df['dropoff_long_30'] = train_df['dropoff_longitude']*np.cos(30* np.pi / 180) - train_df['dropoff_latitude']*np.sin(30* np.pi/180)\ntrain_df['dropoff_long_45'] = train_df['dropoff_longitude']*np.cos(45* np.pi / 180) - train_df['dropoff_latitude']*np.sin(45* np.pi/180)\ntrain_df['dropoff_long_60'] = train_df['dropoff_longitude']*np.cos(60* np.pi / 180) - train_df['dropoff_latitude']*np.sin(60* np.pi/180)\ntrain_df['dropoff_long_75'] = train_df['dropoff_longitude']*np.cos(75* np.pi / 180) - train_df['dropoff_latitude']*np.sin(75* np.pi/180)\n\ntrain_df['dropoff_lat_15'] = train_df['dropoff_longitude']*np.sin(15* np.pi / 180) + train_df['dropoff_latitude']*np.cos(15* np.pi/180)\ntrain_df['dropoff_lat_30'] = train_df['dropoff_longitude']*np.sin(30* np.pi / 180) + train_df['dropoff_latitude']*np.cos(30* np.pi/180)\ntrain_df['dropoff_lat_45'] = train_df['dropoff_longitude']*np.sin(45* np.pi / 180) + train_df['dropoff_latitude']*np.cos(45* np.pi/180)\ntrain_df['dropoff_lat_60'] = train_df['dropoff_longitude']*np.sin(60* np.pi / 180) + train_df['dropoff_latitude']*np.cos(60* np.pi/180)\ntrain_df['dropoff_lat_75'] = train_df['dropoff_longitude']*np.sin(75* np.pi / 180) + train_df['dropoff_latitude']*np.cos(75* np.pi/180)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3775b9a6c2de1f613672b5b65ed47017462cf0b"},"cell_type":"markdown","source":"**Model training**\n\nNow that we have the dataframe that we wanted we can start to train the XGBoost model. First we will split the dataset into train (80%)  and test (20%). "},{"metadata":{"trusted":true,"_uuid":"c2e97a94627615364085834bc9bc282be060599d","collapsed":true},"cell_type":"code","source":"y = train_df['fare_amount']\ntrain = train_df.drop(columns=['fare_amount'])\n\nx_train,x_test,y_train,y_test = train_test_split(train,y,random_state=0,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cfa5cd524f30e6fbcc4c4c788b2a8c8703221de","collapsed":true},"cell_type":"code","source":"def XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'},\n                    dtrain=matrix_train,num_boost_round=100, \n                    early_stopping_rounds=100,evals=[(matrix_test,'test')])\n    return model\n\nmodel = XGBmodel(x_train,x_test,y_train,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecdfbfa119de80585d53df3a1dddc3771b438a72"},"cell_type":"markdown","source":"**Prediction**\n\nFinally we can use our trained model to predict the submission. First we will need to load and preprocess the test dataset just like we did for the training dataset."},{"metadata":{"trusted":true,"_uuid":"5c326448d0f49e7a1547da0820ac931c20a5dabb","collapsed":true},"cell_type":"code","source":"#Read and preprocess test set\ntest_df =  pd.read_csv('../input/test.csv')\ntest_df['distance'] = sphere_dist(test_df['pickup_latitude'], test_df['pickup_longitude'], \n                                   test_df['dropoff_latitude'] , test_df['dropoff_longitude'])\ntest_df = add_datetime_info(test_df)\n#y' = y*cos(a) - x*sin(a)\n#x' = y*sin(a) + x*cos(a)\ntest_df['pickup_long_15'] = test_df['pickup_longitude']*np.cos(15* np.pi / 180) - test_df['pickup_latitude']*np.sin(15* np.pi/180)\ntest_df['pickup_long_30'] = test_df['pickup_longitude']*np.cos(30* np.pi / 180) - test_df['pickup_latitude']*np.sin(30* np.pi/180)\ntest_df['pickup_long_45'] = test_df['pickup_longitude']*np.cos(45* np.pi / 180) - test_df['pickup_latitude']*np.sin(45* np.pi/180)\ntest_df['pickup_long_60'] = test_df['pickup_longitude']*np.cos(60* np.pi / 180) - test_df['pickup_latitude']*np.sin(60* np.pi/180)\ntest_df['pickup_long_75'] = test_df['pickup_longitude']*np.cos(75* np.pi / 180) - test_df['pickup_latitude']*np.sin(75* np.pi/180)\n\ntest_df['pickup_lat_15'] = test_df['pickup_longitude']*np.sin(15* np.pi / 180) + test_df['pickup_latitude']*np.cos(15* np.pi/180)\ntest_df['pickup_lat_30'] = test_df['pickup_longitude']*np.sin(30* np.pi / 180) + test_df['pickup_latitude']*np.cos(30* np.pi/180)\ntest_df['pickup_lat_45'] = test_df['pickup_longitude']*np.sin(45* np.pi / 180) + test_df['pickup_latitude']*np.cos(45* np.pi/180)\ntest_df['pickup_lat_60'] = test_df['pickup_longitude']*np.sin(60* np.pi / 180) + test_df['pickup_latitude']*np.cos(60* np.pi/180)\ntest_df['pickup_lat_75'] = test_df['pickup_longitude']*np.sin(75* np.pi / 180) + test_df['pickup_latitude']*np.cos(75* np.pi/180)\n\ntest_df['dropoff_long_15'] = test_df['dropoff_longitude']*np.cos(15* np.pi / 180) - test_df['dropoff_latitude']*np.sin(15* np.pi/180)\ntest_df['dropoff_long_30'] = test_df['dropoff_longitude']*np.cos(30* np.pi / 180) - test_df['dropoff_latitude']*np.sin(30* np.pi/180)\ntest_df['dropoff_long_45'] = test_df['dropoff_longitude']*np.cos(45* np.pi / 180) - test_df['dropoff_latitude']*np.sin(45* np.pi/180)\ntest_df['dropoff_long_60'] = test_df['dropoff_longitude']*np.cos(60* np.pi / 180) - test_df['dropoff_latitude']*np.sin(60* np.pi/180)\ntest_df['dropoff_long_75'] = test_df['dropoff_longitude']*np.cos(75* np.pi / 180) - test_df['dropoff_latitude']*np.sin(75* np.pi/180)\n\ntest_df['dropoff_lat_15'] = test_df['dropoff_longitude']*np.sin(15* np.pi / 180) + test_df['dropoff_latitude']*np.cos(15* np.pi/180)\ntest_df['dropoff_lat_30'] = test_df['dropoff_longitude']*np.sin(30* np.pi / 180) + test_df['dropoff_latitude']*np.cos(30* np.pi/180)\ntest_df['dropoff_lat_45'] = test_df['dropoff_longitude']*np.sin(45* np.pi / 180) + test_df['dropoff_latitude']*np.cos(45* np.pi/180)\ntest_df['dropoff_lat_60'] = test_df['dropoff_longitude']*np.sin(60* np.pi / 180) + test_df['dropoff_latitude']*np.cos(60* np.pi/180)\ntest_df['dropoff_lat_75'] = test_df['dropoff_longitude']*np.sin(75* np.pi / 180) + test_df['dropoff_latitude']*np.cos(75* np.pi/180)\n\ntest_key = test_df['key']\nx_pred = test_df.drop(columns=['key', 'pickup_datetime'])\n\n#Predict from test set\nprediction = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e0bd0c431cb912aa91092c9f6fdb5e2cbf40669","collapsed":true},"cell_type":"code","source":"#Create submission file\nsubmission = pd.DataFrame({\n        \"key\": test_key,\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"575db49166dea9d6fd01f16ebb01ec411d30f6f4"},"cell_type":"markdown","source":"**Possible improvements**\n\n- Right now, converting the 'pickup_datetime' column to datetime format is a real bottleneck. Try to find a way to better scale this part to be able to train with a larger number of traning samples.\n- Use cross-validation to tune the hyperparameters of the model for better performance."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}