{"cells":[{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"Thanks to @ AhmetArdem\n\nhttps://github.com/aerdem4/mercari-price-suggestion\nhttps://www.kaggle.com/c/mercari-price-suggestion-challenge/discussion/49968\n\n1.Fit data for RNN tokenization & Sklearn Vectorizations was done only on training data \n2. Prediction of RNN and WordBatch done in Batches \n3. Memory Optimization \n3. Wordbatch findings The various feature concantenations produce a significant improvement For ex: In the below code, Adding X_desc3, X_cat_brand, X_name, X_name2 to the regular fields available in MPC training data improves score from 0.43X to 0.41X (Without RNN ensemble) \n4. Increased batch_size for successive epochs for RNN \n5. Absolutely no word processing was applied ( Except for basic NaN handling ) Tried some regex replacements based on ELI5 observations but did not give a huge boost so removed them"},{"metadata":{"_cell_guid":"279b6543-e454-4cb8-be5e-43226d6e8eb5","_uuid":"7b12b239f78dcb22e18bb72c3bb6d9e5fea76169","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport os\nimport gc\nimport time\nstart_time = time.time()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\n\nos.environ['MKL_NUM_THREADS'] = '4'\nos.environ['OMP_NUM_THREADS'] = '4'\n\n# Sklearn model definition \n\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.model_selection import train_test_split\n\n# Keras model definition \n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, GlobalAveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras import initializers\nfrom keras.layers import Activation, BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\n\n# Tensor Flow \n\nimport tensorflow as tf\n\nstart_time = time.time()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f73b62c5-637a-4ddb-8f2f-8a9cc3aa5de7","_uuid":"ec1c5ba41c2eaccfe8719c47cbfc9638c0da3d98","trusted":false},"cell_type":"code","source":"# RNN Preprocessing, Transformation methods \n\ndef preprocess_RNN(dataset):\n    \n    print(\"Filling Missing Values\")\n    dataset['category_name'].fillna(value='missing', inplace=True)\n    dataset['brand_name'].fillna(value='missing', inplace=True)\n    dataset['item_description'].fillna(value='missing', inplace=True)\n    \n    print(\"Casting data types to type Category\")\n    dataset['category_name'] = dataset['category_name'].astype('category')\n    dataset['brand_name'] = dataset['brand_name'].astype('category')\n    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')  \n    print(\"RNN PreProcessing completed\")\n    \n    return dataset \n\ndef fit_RNN_text(dataset):\n\n    print(\"Fit Name and Item description fields for Tokenization\")\n    raw_text = np.hstack([dataset.item_description.str.lower(), dataset.name.str.lower()])\n    word_token = Tokenizer()\n    word_token.fit_on_texts(raw_text)\n    print(\"RNN Data fit completed\")\n    \n    return word_token\n\ndef fit_RNN_label(dataset):\n\n    print(\"Fit Categorical variables on full Merged Test and Train data\")\n    \n    le_name = LabelEncoder()\n    le_name.fit(dataset.category_name)\n    le_brand = LabelEncoder()\n    le_brand.fit(dataset.brand_name)\n    \n    print(\"Completed Label fitting\")\n    return le_name, le_brand\n\ndef transform_RNN(dataset, le_name, le_brand, word_token):\n    print(\"Use Defined Label encoders to Encode brand and category_name\")\n    dataset['category'] = le_name.transform(dataset.category_name)\n    dataset['brand'] = le_brand.transform(dataset.brand_name)\n    print(\"Convert Text to sequences\")\n    dataset[\"seq_item_description\"] = word_token.texts_to_sequences(dataset.item_description.str.lower())\n    dataset[\"seq_name\"] = word_token.texts_to_sequences(dataset.name.str.lower())\n    print(\"Sequence Conversion Completed\")\n    \n    return dataset \n\ndef get_keras_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n        ,'item_desc': pad_sequences(dataset.seq_item_description\n                                    , maxlen=MAX_ITEM_DESC_SEQ)\n        ,'brand': np.array(dataset.brand)\n        ,'category': np.array(dataset.category)\n        ,'item_condition': np.array(dataset.item_condition_id)\n        ,'num_vars': np.array(dataset[[\"shipping\"]])\n    }\n    \n    print(\"Data ready for Vectorization\")\n    \n    return X","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e37c38d5-d6c1-4d83-9c37-2939cee56359","_uuid":"cbecb1635cd7fb5869c5a7556c53234b8af71d96","trusted":false,"collapsed":true},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        # self.init = initializations.get('glorot_uniform')\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7ddc07f5-06fa-4e80-9f1d-69e1053dccbe","_uuid":"2942af14095d19963de9cef0afcd9a7bf7f9c4f1","trusted":false},"cell_type":"code","source":"def RNN_model():\n\n    #Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand = Input(shape=[1], name=\"brand\")\n    category = Input(shape=[1], name=\"category\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    \n    #Embeddings layers\n    emb_size = 60\n    \n    emb_name = Embedding(MAX_TEXT, emb_size//3)(name)\n    emb_item_desc = Embedding(MAX_TEXT, emb_size)(item_desc)\n    emb_brand = Embedding(MAX_BRAND, 10)(brand)\n    emb_category = Embedding(MAX_CATEGORY, 10)(category)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    \n    # Try Global Average Max Pooling \n    \n    #emb_name = GlobalAveragePooling1D()(emb_name)\n    #emb_item_desc = GlobalAveragePooling1D()(emb_item_desc)\n    \n    # RNN For item desc and name \n    rnn_layer1 = GRU(25) (emb_item_desc)\n    #rnn_layer1 = Attention(X_train['item_desc'].shape[1]-1)(rnn_layer1)\n    rnn_layer2 = GRU(12) (emb_name)\n    #rnn_layer2 = Attention(X_train['name'].shape[1]-1)(rnn_layer2)\n    \n    #main layer\n    main_l = concatenate([\n         Flatten() (emb_brand)\n        , Flatten() (emb_category)\n        , Flatten() (emb_item_condition)\n        , rnn_layer1\n        , rnn_layer2\n        , num_vars\n    ])\n    \n    main_l = BatchNormalization()(main_l)\n    main_l = Dropout(0.1)(Dense(512,activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(64,activation='relu') (main_l))\n \n    #output\n    output = Dense(1,activation=\"linear\") (main_l)\n    \n    #model\n    model = Model([brand, category, item_condition, item_desc, name, num_vars], output)\n    optimizer = optimizers.Adam()\n    model.compile(loss=\"mse\", \n                  optimizer=optimizer)\n    return model\n\ndef rmsle(y, y_pred):\n    import math\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 \\\n              for i, pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n\ndef eval_model(model):\n    val_preds = model.predict(X_valid)\n    val_preds = np.expm1(val_preds)\n    y_pred = val_preds[:, 0]\n    \n    y_true = np.array(valid_prices)\n    \n    yt = pd.DataFrame(y_true)\n    yp = pd.DataFrame(y_pred)\n    \n    print(yt.isnull().any())\n    print(yp.isnull().any())\n    \n    v_rmsle = rmsle(y_true, y_pred)\n    print(\" RMSLE error on dev test: \"+str(v_rmsle))\n    return v_rmsle\n\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96699281-f8d3-464f-b8da-43a0c330d5d0","_uuid":"1978bcb043f38bfbfe16a103e33999e4ae2fba54","trusted":false,"collapsed":true},"cell_type":"code","source":"# Loading data \nt1 = time.time()\n\ntrain = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv', sep='\\t')\ntrain = train[train['price'] !=0 ]\nntrain = train.shape[0]\ntrain['target'] = np.log1p(train['price'])\ntest = pd.read_table('../input/mercari-price-suggestion-challenge/test_stg2.tsv', sep='\\t')\nntest = test.shape[0]\nmerge = pd.concat([train, test], 0, ignore_index = True)\n\ndel test, train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1d73e9fe-f24a-4bc2-bc25-041d435996c3","_uuid":"5044964a1f89088b0f062062f482d608535df557","trusted":false,"collapsed":true},"cell_type":"code","source":"# Training RNN \n# Pre Process RNN Data - Missing data, Tokenization \n\nt1 = time.time()\nmerge = preprocess_RNN(merge)\nt2 = time.time()\nprint(\"Time taken for RNN preprocess \"+str(t2-t1))\nprint(merge.shape)\n\n# Fitting Categorical columns on test and train data \nt1 = time.time()\nle_name, le_brand = fit_RNN_label(merge)\nt2 = time.time()\nprint(\"Time taken to Fit on RNN \"+str(t2-t1))\n\n# Fitting Text data on Train data only \nt1 = time.time()\ntrain = merge[:ntrain]\nword_token = fit_RNN_text(train)\n\n# Transforming training data to be readied for RNN TRAINING \ntrain = transform_RNN(train, le_name, le_brand, word_token)\nt2 = time.time()\nprint(\"Time taken to Fit and Transform on RNN \"+str(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"363f0a76-5d34-4b93-860c-bc1b6b1c4341","_uuid":"f10f176d4ee770d5985db4f94fc9f4e3fd92386b","trusted":false,"collapsed":true},"cell_type":"code","source":"print(train.shape, merge.shape)\ndel merge\ngc.collect","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0ebc9a56-223c-47f1-9872-c6d26c9105c5","_uuid":"0565d61b55804a91355a714acd550ad6d1051b83","trusted":false},"cell_type":"code","source":"#EMBEDDINGS MAX VALUE\n#Max_text is USED TO CALCULATE VOCABULARY LENGTH for EMBEDDING \n#MAX_Seq values are USED TO CALCULATE PADDING LENGTHS \n# For padding \nMAX_NAME_SEQ = 20 \nMAX_ITEM_DESC_SEQ = 60 \nMAX_CATEGORY_NAME_SEQ = 20 \nMAX_CATEGORY = np.max(train.category.max())+1\nMAX_BRAND = np.max(train.brand.max())+1\nMAX_CONDITION = 6\n# For Vocab length --> THE VALUE OF MAX_TEXT is THE VALUE OF THE VOCABULARY LENGTH \nMAX_TEXT = np.max([np.max(train.seq_name.max()) , np.max(train.seq_item_description.max())])+2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49ba373d-ef46-41d1-9779-7f576082c136","_uuid":"b76df888fdfe6b0bdd51b8e1fa970b3e949743ec","trusted":false,"collapsed":true},"cell_type":"code","source":"# Split into test and train data \n\ndtrain, dvalid = train_test_split(train, random_state=233, train_size=0.99)\ndtrain['target'] =np.log1p(dtrain['price'])\ntarget  = np.array(dtrain.target)\ndvalid['target'] =np.log1p(dvalid['price'])\nvalid_prices = np.array(dvalid.target)\nprint(dvalid.shape, dtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d3c5355-cadb-43cc-a026-18b3d00929ba","_uuid":"d05937a7719c67955b351f15f1f220659299428f","trusted":false,"collapsed":true},"cell_type":"code","source":"#Sequence padding \ndel train\nX_train = get_keras_data(dtrain)\nX_valid = get_keras_data(dvalid)\ndel dtrain, dvalid\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09a65a08-4544-4c8d-bb14-92446f63b21d","_uuid":"9cbb1635ff9b9e04b7e41c2e91e2699aa75621d5","trusted":false,"collapsed":true},"cell_type":"code","source":"#FITTING THE MODEL\nepochs = 1\nBATCH_SIZE = 512 * 3\nsteps = int(len(X_train['name'])/BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.009, 0.0045\nlr_decay = exp_decay(lr_init, lr_fin, steps)\nmodelRNN = RNN_model()\nK.set_value(modelRNN.optimizer.lr, lr_init)\nK.set_value(modelRNN.optimizer.decay, lr_decay)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dcd7f6bd-69fe-4e3c-8747-cd683ef1ad63","_uuid":"27b8864e69ab5c624f7b250fb92311c7e3eed3bf","trusted":false,"collapsed":true},"cell_type":"code","source":"# Fitting RNN \nos.environ['OMP_NUM_THREADS'] = '4'\nos.environ['MKL_NUM_THREADS'] = '4'\nfor i in range(3):\n    history = modelRNN.fit(X_train, target\n                    , epochs=epochs\n                    , batch_size=BATCH_SIZE+(512*i)\n                    , validation_data = (X_valid, valid_prices)\n                    , verbose=1\n                    )\n    # Evaluate RMSLE \n    v_rmsle = eval_model(modelRNN)\n    print('[{}] Finished predicting valid set...'.format(time.time() - start_time))\n    \nprint(\"Finished Fitting the model\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a845efb6-9bf4-4926-b1f7-1b60f0245951","_uuid":"25b36fc7c06f02aedd68f942e707bea7998d37bc","trusted":false},"cell_type":"code","source":"del X_train, X_valid\ndel target, valid_prices\ndel epochs, lr_init, lr_fin, exp_decay, lr_decay\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"426b71dd-7b9d-418b-ae40-2f880bf0a369","_uuid":"84b680c2ac4bd15c85006b0bfd7643c5b2b48360","trusted":false},"cell_type":"code","source":"t1 = time.time()\ntrain = pd.read_csv('../input/mercari-price-suggestion-challenge/train.tsv', sep='\\t')\ntrain = train[train['price'] != 0]\ntrain['target'] = np.log1p(train['price'])\nn_train = train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6274f4c2-31fb-410d-ada8-f1ca9fad90ff","_uuid":"89ebc105a10df43cdf6fb754baf60ad8739cf647","trusted":false},"cell_type":"code","source":"def split_cat(text):\n    try: \n        s1, s2, s3 = text.split(\"/\")\n        return s1, s2, s3\n    \n    except: return (\"No Label\", \"No Label\", \"No Label\")\n\ndef wordbatch_preprocess(df):\n    df[\"category_name\"] = df[\"category_name\"].fillna(value=\"missing\").astype(str)\n    df[\"name\"] = df[\"name\"].fillna(value=\"missing\").astype(str)\n    df[\"brand_name\"] = df[\"brand_name\"].fillna(value=\"missing\").astype(str)\n    df[\"item_description\"] = df[\"item_description\"].fillna(value=\"missing\").astype(str)\n    df[\"item_condition_id\"] = df[\"item_condition_id\"].astype(int)\n    df[\"shipping\"] = df[\"shipping\"].astype(int)\n    print(\"Got so far\")\n    \n    print(df.dtypes)\n    df['subcat_0'], df['subcat_1'], df['subcat_2'] = zip(*df['category_name'].apply(lambda x: split_cat(x)))\n    print(df.dtypes)\n    \n    return df\n\n# Pre Processing for word batch \ntrain = wordbatch_preprocess(train)\n\nprint(\"Pre Process for Word batch is done\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"824bfd6f-9f78-4703-842a-7e0c3ceeecdd","_uuid":"027adc23a5aede042892f6ef5cb3d2f3c4e18de1","trusted":false},"cell_type":"code","source":"import numpy as np\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport wordbatch\nfrom wordbatch.extractors import WordBag\nfrom wordbatch.models import FM_FTRL\n\nclass WordBatchModel(object):\n    def __init__(self):\n        self.wb_desc = None\n        self.desc_indices = None\n        self.cv_name, self.cv_name2 = None, None\n        self.cv_cat0, self.cv_cat1, self.cv_cat2 = None, None, None\n        self.cv_brand = None\n        self.cv_condition = None\n        self.cv_cat_brand = None\n        self.desc3 = None\n        self.model = None\n\n    def train_wbm(self, df):\n\n        self.wb_desc = wordbatch.WordBatch(None,\n                                           extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n                                                                \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n                                                                \"idf\": None}), procs=8)\n        self.wb_desc.dictionary_freeze = True\n        X_desc = self.wb_desc.fit_transform(df['item_description'])\n        self.desc_indices = np.array(np.clip(X_desc.getnnz(axis=0) - 1, 0, 1), dtype=bool)\n        X_desc = X_desc[:, self.desc_indices]\n        \n        self.cv_cat0 = CountVectorizer(min_df=2)\n        X_category0 = self.cv_cat0.fit_transform(df['subcat_0'])\n        \n        self.cv_cat1 = CountVectorizer(min_df=2)\n        X_category1 = self.cv_cat1.fit_transform(df['subcat_1'])\n        \n        self.cv_cat2 = CountVectorizer(min_df=2)\n        X_category2 = self.cv_cat2.fit_transform(df['subcat_2'])\n\n        self.cv_brand = CountVectorizer(min_df=2, token_pattern=\".+\")\n        X_brand = self.cv_brand.fit_transform(df['brand_name'])\n\n        # Variations \n\n        self.cv_name = CountVectorizer(min_df=2, ngram_range=(1, 1),binary=True, token_pattern=\"\\w+\")\n        X_name = 2 * self.cv_name.fit_transform(df['name'])\n        \n        self.cv_name2 = CountVectorizer(min_df=2, ngram_range=(2, 2), binary=True, token_pattern=\"\\w+\")\n        X_name2 = 0.5 * self.cv_name2.fit_transform(df['name'])\n                                                      \n        df[\"cat_brand\"] = [a + \" \" + b for a, b in zip(df[\"category_name\"], df[\"brand_name\"])]\n        self.cv_cat_brand = CountVectorizer(min_df=10, token_pattern=\".+\")\n        X_cat_brand = self.cv_cat_brand.fit_transform(df[\"cat_brand\"])\n        \n        self.cv_condition = CountVectorizer(token_pattern=\".+\")\n        X_condition = self.cv_condition.fit_transform((df['item_condition_id'] + 10 * df[\"shipping\"]).apply(str))\n            \n        self.desc3 = CountVectorizer(ngram_range=(3, 3), max_features=1000, binary=True, token_pattern=\"\\w+\")\n        X_desc3 = self.desc3.fit_transform(df[\"item_description\"])\n\n        X = hstack((X_condition,\n                    X_desc, X_brand,\n                    X_category0, X_category1, X_category2,\n                    X_name, X_name2,\n                    X_cat_brand, \n                    X_desc3\n                    )).tocsr()\n\n        print(\"X Reconstructed\")\n        y = df['target'].values\n        #y = y.reshape(y.shape[0],1)\n        print(\"Y created\")\n\n        self.model = FM_FTRL(alpha=0.01, beta=0.01, L1=0.00001, L2=0.1, D=X.shape[1], alpha_fm=0.02, L2_fm=0.0,\n                             init_fm=0.01, D_fm=200, e_noise=0.0001, iters=10, inv_link=\"identity\", threads=4)\n        print(\"Model Defined\")\n        self.model.fit(X, y)\n\n    def predict(self, df):\n        X_desc = self.wb_desc.transform(df[\"item_description\"])\n        X_desc = X_desc[:, self.desc_indices]\n        \n        X_brand = self.cv_brand.transform(df['brand_name'])\n\n        X_name = 2 * self.cv_name.transform(df[\"name\"])\n        X_name2 = 0.5 * self.cv_name2.transform(df[\"name\"])\n\n        X_category0 = self.cv_cat0.transform(df['subcat_0'])\n        X_category1 = self.cv_cat1.transform(df['subcat_1'])\n        X_category2 = self.cv_cat2.transform(df['subcat_2'])\n        \n        X_condition = self.cv_condition.transform((df['item_condition_id'] + 10 * df[\"shipping\"]).apply(str))\n        \n        df[\"cat_brand\"] = [a + \" \" + b for a, b in zip(df[\"category_name\"], df[\"brand_name\"])]\n        X_cat_brand = self.cv_cat_brand.transform(df[\"cat_brand\"])\n        \n        X_desc3 = self.desc3.transform(df[\"item_description\"])\n\n        X = hstack((X_condition,\n                    X_desc, X_brand,\n                    X_category0, X_category1, X_category2,\n                    X_name, X_name2,\n                    X_cat_brand, \n                    X_desc3\n                   )).tocsr()\n\n        return self.model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e18f5910-6bc9-4f4a-b4c0-b1e3dc31171f","_uuid":"fbc861ea2a632afa2e62d6ba842637d6b54d6647","trusted":false},"cell_type":"code","source":"# Training WordBatch \nos.environ['MKL_NUM_THREADS'] = '4'\nos.environ['OMP_NUM_THREADS'] = '4'\nprint(train.shape)\nt1 = time.time()\nwbm = WordBatchModel()\nwbm.train_wbm(train)\nt2 = time.time()\nprint(\"Time taken for Word Batch is \"+str(t2-t1))\ndel train \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a37022e7-2fa6-47b4-beec-78554a907cd4","_uuid":"4bc33039edbbf3c825452a3c966338907389a41f","trusted":false},"cell_type":"code","source":"import time\nt1 = time.time()\ndef load_test():\n    for df in pd.read_csv('../input/mercari-price-suggestion-challenge/test_stg2.tsv', sep='\\t', chunksize= 1000000):\n        yield df\n\ntest_ids = np.array([], dtype=np.int32)\npreds= np.array([], dtype=np.float32)\n\ni = 0 \n    \nfor df in load_test():\n    \n    i +=1\n    print(\" Chunk number is \"+str(i))\n    df1 = df2 = df\n    testRNN = preprocess_RNN(df1)\n    print(df1.dtypes)\n    print(testRNN.dtypes)\n    testRNN = transform_RNN(testRNN, le_name, le_brand, word_token)\n    print(testRNN.dtypes)\n    X_testRNN = get_keras_data(testRNN)\n    predsRNN1 = modelRNN.predict(X_testRNN, batch_size = BATCH_SIZE, verbose = 1)\n    test_id = df['test_id']\n    del testRNN\n    del df['seq_item_description'], df['seq_name'], df['brand'], df['category']\n    gc.collect()\n    print(\"RNN Prediction is done\")\n    \n    print(df.isnull().any())\n    print(df.dtypes)\n    test_WB = wordbatch_preprocess(df2)\n    print(df.dtypes)\n    print(\"Word batch preprocess done\")\n    predsWB1 = wbm.predict(test_WB)\n    print(\"Word Batch Prediction done\")\n\n    predsRNN = np.expm1(predsRNN1)\n    predsWB = np.expm1(predsWB1)\n    \n    predsRNN = predsRNN.reshape(predsRNN.shape[0],1)\n    predsWB = predsWB.reshape(predsWB.shape[0],1)\n\n    predsRNN = np.clip(predsRNN, 0, predsRNN.max())\n    predsWB = np.clip(predsWB, 0, predsWB.max())\n    \n    preds= np.append(preds, ((predsRNN*0.4) + (predsWB*0.6)))\n    test_ids = np.append(test_ids, test_id)\n    \nprint(\"All chunks done\")\nt2 = time.time()\nprint(\"Total time for Parallel Batch Prediction is \"+str(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4f6bb780-c522-4da5-931f-c92b4326e885","_uuid":"2617748761e3605803f1f39ce21eee836555efaf","trusted":false},"cell_type":"code","source":"import pandas as pd \n\nsubmission = pd.DataFrame( columns = ['test_id', 'price'])\nsubmission['test_id'] = test_ids\nsubmission['price'] = preds\n\nprint(\"Check Submission NOW!!!!!!!!@\")\nsubmission.to_csv(\"BatchPrediction_MemoryCoreOptimized.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6f691046-7eca-4ab0-a9f1-fbe6833a234b","_uuid":"53cb83a5121c0b32f1f8c92f7b8831818049b51f","trusted":false},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c757aca3-7377-4d8f-9ac6-5e636b4b15dc","_uuid":"b5513961c4e11b83fabc10ccb9398790fe06d508","trusted":false},"cell_type":"code","source":"t2 = time.time()\nprint(\"Total time taken is \"+str(t2-start_time))","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}