{"cells":[{"metadata":{"trusted":true,"_uuid":"d01baac3757b22b92747599ed87524c1ddc0ad1d"},"cell_type":"code","source":"'''\nThe kernel demonstrates the clustering and expending we used to get to 7# place\nRunning this kernel on training event 1000 will score ~0.635 after the clustering stage, and 0.735 after expending stage.\nEvery stage takes about 8-10 min on Kaggle, and about half the time on my laptop.\nIn the clustering part of the kernel the algorithm 5500 pairs of z0, 1/2R (More of it below) by increasing the number to about 100,000 \nthe score will plateau at about 0.765 (after expending).\nHow does it work:\nIn each clustering loop the algorithm try to find all tracks originating from (0,0,z0) and with a radius of 1/(2*kt).\nIf a hit (x,y,z) is on a track the helix can be fully defined by the following features (1), (2)\n\nrr=(x**2+y**2)**0.5\ntheta_=arctan(y/x)\ndtheta = arcsin(kt*rr)\n(1)\tTheta=theta_+dtheta\n(2)\t(z-z0)*kt/dtheta\n\nTo solve the +pi,-pi problem we use sin, cos for theta.\nTo make (2) more uniform, we use arctan((z-z0)/(3.3*dtheta/kt))\n\nAfter calculating the features, the algorithm tries to cluster all the hits with the same features. \nThis is done by sparse binning – using np.unique.\nThe disadvantage of sparse binning over dbscan is it’s sensitivity, the advantages are its speed and its sensitivity (almost no outliners).\nAfter clustering every hit choose if his cluster is good according to the clusters length.\nEvery 500 loops all hits belonging to tracks which are long enough are removed from the dataset\nIf two hits from the same detector are on the same track, the one which is closest to the track’s center of mass is chosen.\nThe z0, kt pairs a chosen randomly\nWhile running, the algorithm changes the bin width and the length of the minimum track to be extracted from the dataset.\n\nExpending is done by selecting the un-clustered hits which are close to the center of mass of the track.\n\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport os\n#print(os.listdir(\"../input\"))\nfrom ipywidgets import FloatProgress,FloatText\nfrom IPython.display import display\n\nimport time\nimport pdb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom trackml.dataset import load_event\nfrom trackml.randomize import shuffle_hits\nfrom trackml.score import score_event\nfrom trackml.dataset import load_dataset\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom itertools import product\nimport gc\nimport cProfile\nfrom tqdm import tqdm\n\n%matplotlib inline\n#make wider graphs\nsns.set(rc={'figure.figsize':(12,5)})\nplt.figure(figsize=(12,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d60502a63403cc916c67b1137eeb36ca591c129"},"cell_type":"code","source":"path = '../input/train_1/'\n\nlabel_shift_M=1000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06cbcc6c3210db2daf69b2ae244f8d0e07512bbc"},"cell_type":"code","source":"def create_one_event_submission(event_id, hits, labels):\n    sub_data = np.column_stack(([event_id]*len(hits), hits.hit_id.values, labels))\n    submission = pd.DataFrame(data=sub_data, columns=[\"event_id\", \"hit_id\", \"track_id\"]).astype(int)\n    return submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"225efbd270270d28321b3a0a5dd0fb0c6fed49b7"},"cell_type":"code","source":"def hit_score(res,truth):\n    tt=res.merge(truth[['hit_id','particle_id','weight']],on='hit_id',how='left')\n    un,inv,count = np.unique(tt['track_id'],return_inverse=True, return_counts=True)\n    tt['track_len']=count[inv]\n    un,inv,count = np.unique(tt['particle_id'],return_inverse=True, return_counts=True)\n    tt['real_track_len']=count[inv]\n    gp=tt.groupby('track_id')\n    gp=gp['particle_id'].value_counts().rename('par_freq').reset_index()\n    tt=tt.merge(gp,on=['track_id','particle_id'],how='left')\n    gp=gp.groupby('track_id').head(1)\n    gp=gp.rename(index=str, columns={'particle_id': 'common_particle_id'})\n    tt = tt.merge(gp.drop(['par_freq'],axis=1),on='track_id',how='left')\n    tt['to_score']=(2*tt['par_freq']>tt['track_len']) & (2*tt['par_freq']>tt['real_track_len'])\n    tt['score']=tt['weight']*tt['to_score']\n    return tt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd911596627b039c2af3d2c06912ee8a5ee7246b"},"cell_type":"code","source":"def calc_features(hits,hipos,phik,double_sided=False):\n    \n    if not 'rr' in list(hits.columns):\n        hits['theta_']=np.arctan2(hits.y,hits.x)\n        hits['rr']=np.sqrt(np.square(hits.x)+np.square(hits.y))\n        hits['correct']=1.005 - (abs(hits.z + 200) / 6000)**2.4\n    ktrr=hits.rr*hipos.kt\n    hits['dtheta']=np.where((np.abs(ktrr)<1),np.arcsin(ktrr,where=(np.abs(ktrr)<1) ),ktrr)\n    hits['theta'] = hits.theta_+hits.dtheta*hits['correct']\n    hits['phi'] = np.arctan2((hits.z-hipos.z0) ,phik*hits.dtheta/hipos.kt)*2.0/np.pi\n    hits['sint']=np.sin(hits['theta'])\n    hits['cost']=np.cos(hits['theta'])\n    hits['fault']=(np.abs(ktrr)>1).astype('int')\n    if double_sided:\n        hits['phi2'] = np.arctan2((hits.z-hipos.z0) ,phik*(np.pi-hits.dtheta)/hipos.kt)*2.0/np.pi\n        hits['theta2'] = hits.theta_+np.pi-hits.dtheta*hits['correct']\n        hits['sint2']=np.sin(hits['theta2'])\n        hits['cost2']=np.cos(hits['theta2'])\n    return hits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2283196746e1d9f8e4889505a5954b5b4c4971f3"},"cell_type":"code","source":"def tag_bins(cat):\n    un,inv,count = np.unique(cat,return_inverse=True, return_counts=True)\n    bin_tag=inv\n    bin_count=count[inv]\n    return bin_tag,bin_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2eabe9ad6b83c925244a344b13682180c49aa502"},"cell_type":"code","source":"def sparse_bin(features,bin_num,randomize=True,fault=None):\n    err=np.random.rand(features.shape[1])*randomize\n    cat=np.zeros(features.shape[0]).astype('int64')\n    factore=1\n    for i,feature in enumerate(features.columns):\n        cat=cat+(features[feature]*bin_num._asdict()[feature]+err[i]).astype('int64')*factore\n        factore=factore*(2*bin_num._asdict()[feature]+1)\n    if not fault is None:\n        cat=cat+(factore*features.index*fault).astype('int64')\n    return tag_bins(cat)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c276e6101f8de47741f3e97873cc061bcb0a3f0"},"cell_type":"code","source":"def clustering(hits,stds,filters,phik=1.0,nu=500,weights=None,res=None,truth=None,history=None,pre_test_points=None):\n    start = time.time()\n    rest = hits.copy()\n    if weights is None:\n        weights={'phi':1, 'theta':0.15}\n    calc_score = not truth is None\n    if not history is None:\n        hist_list=[]\n    if calc_score:\n        rest = rest.merge(truth[['hit_id','particle_id','weight']],on='hit_id',how='left')\n        dum,rest['particle_track_len']=tag_bins(rest['particle_id'])\n        score = 0 \n        hit_num=0\n        total_num=0\n        frs=FloatText(value=0, description=\"full score:\")\n        display(frs)\n        fs=FloatText(value=0, description=\"score:\")\n        display(fs)\n        fss=FloatText(value=0, description=\"s rate:\")\n        display(fss)\n        fsd=FloatText(value=0, description=\"add score:\")\n        display(fsd)\n    ft = FloatText(value=rest.shape[0], description=\"Rest size:\")\n    display(ft)\n    fg = FloatText(value=rest.shape[0], description=\"Group size:\")\n    display(fg)\n    fgss = FloatText(description=\"filter:\")\n    display(fgss)\n\n    if res is None:\n        rest['track_len']=1\n        rest['track_id']=-rest.index\n        rest['kt']=1e-6\n        rest['z0']=0\n    else:\n        rest=rest.merge(res[['hit_id','track_id','kt','z0']],on='hit_id',how='left')\n        dum,rest['track_len']=tag_bins(rest['track_id'])\n\n    res_list=[]\n    rest['sensor']=rest.volume_id+rest.layer_id*100+100000*rest.module_id\n    rest['layers']=rest.volume_id+rest.layer_id*100\n    if pre_test_points is None:\n        maxprog= filters.npoints.sum()\n    else:\n        maxprog = filters.shape[0]*pre_test_points.shape[0]\n    pbar = tqdm(total=maxprog,mininterval=5.0)\n    rest['pre_track_id']=rest['track_id']\n    p=-1\n    feature_cols=['theta','sint','cost','phi','rr','theta_','dtheta','fault']\n    for filt in filters.itertuples():\n        if pre_test_points is None:\n            test_points=pd.DataFrame()\n            for col in stds:\n                test_points[col] = np.random.normal(scale=stds[col],size=filt.npoints)\n        else:\n            test_points=pre_test_points.sample(frac=filt.npoints).reset_index(drop=True)\n        \n        for row in test_points.itertuples():\n            p=p+1\n            pbar.update()\n            calc_features(rest,row,phik)\n            rest['new_track_id'],rest['new_track_len']=sparse_bin(rest[['phi','sint','cost']],filt,fault=rest.fault)\n            rest['new_track_id']=rest['new_track_id']+(p+1)*label_shift_M\n            better = (rest.new_track_len>rest.track_len) & (rest.new_track_len<19)\n            rest['new_track_id']=rest['new_track_id'].where(better,rest.track_id)\n            dum,rest['new_track_len']=tag_bins(rest['new_track_id'])\n            better = (rest.new_track_len>rest.track_len) & (rest.new_track_len<19)\n            rest['track_id']=rest['track_id'].where(~better,rest['new_track_id']) \n            rest['track_len']=rest['track_len'].where(~better,rest['new_track_len'])\n            rest['kt']=rest['kt'].where(~better,row.kt)\n            rest['z0']=rest['z0'].where(~better,row.z0)\n            \n            if (((row.Index+1)%nu == 0) or (row.Index + 1 == test_points.shape[0])):\n                dum,rest['track_len']=tag_bins(rest['track_id'])\n                calc_features(rest,rest[['kt','z0']],phik)\n                gp = rest.groupby(['track_id']).agg({'phi': np.mean , \n                    'sint':np.mean, 'cost':np.mean}).rename(columns={ 'phi': 'mean_phi', \n                                'sint':'mean_sint', 'cost':'mean_cost'}).reset_index()\n                cols_to_drop = rest.columns.intersection(gp.columns).drop('track_id')\n                rest = rest.drop(cols_to_drop,axis=1).reset_index().merge(gp,on=['track_id'],how = 'left').set_index('index')\n                rest['dist'] = weights['theta']*np.square(rest.sint-rest.mean_sint)+ weights['theta']*np.square(rest.cost-rest.mean_cost)+ weights['phi']*np.square(rest.phi-rest.mean_phi)\n                rest=rest.sort_values('dist')\n                rest['closest']=rest.groupby(['track_id','sensor'])['dist'].cumcount()\n                rest['closest2']=rest.groupby(['track_id','layers'])['dist'].cumcount()\n                select = (rest['closest']!=0) | (rest['closest2']>2)  \n                rest['track_id']=rest['track_id'].where(~select,rest['pre_track_id'])\n                dum,rest['track_len']=tag_bins(rest['track_id'])\n                fgss.value=filt.phi\n                fg.value=filt.min_group\n                ft.value = rest[rest.track_len<=filt.min_group].shape[0]\n\n                select = (rest['track_len']>filt.min_group)\n                #The next lines are just for printing\n                if calc_score:\n                    tm=rest[select]                   \n                    gp = tm.groupby(['track_id','particle_id'])['hit_id'].count().rename('par_count').reset_index()\n                    tm=tm.merge(gp,on=['track_id','particle_id'],how='left')\n                    gp = rest.groupby(['track_id','particle_id'])['hit_id'].count().rename('par_count').reset_index()\n                    rs=rest.merge(gp,on=['track_id','particle_id'],how='left')\n                    to_full_score=(rs.weight*((rs.par_count*2>rs.track_len) & (rs.par_count*2>rs.particle_track_len)))\n                    frs.value=to_full_score.sum()+fs.value\n                    to_score=(tm.weight*((tm.par_count*2>tm.track_len) & (tm.par_count*2>tm.particle_track_len)))\n                    hit_num=hit_num+(to_score>0).sum()\n                    total_num=total_num+tm.weight.sum()\n                    fs.value=fs.value+to_score.sum()\n                    fss.value=fs.value/total_num\n                    fsd.value=to_score.sum()\n                    gp = rest.groupby(['track_id','particle_id'])['hit_id'].count().rename('par_count').reset_index()\n                    rs=rest.merge(gp,on=['track_id','particle_id'],how='left')\n                    to_full_score=(rs.weight*((rs.par_count*2>rs.track_len) & (rs.par_count*2>rs.particle_track_len)))\n                    frs.value=to_full_score.sum()+fs.value-to_score.sum()\n                    if not history is None:\n                        hist_list.append(pd.DataFrame({'P':p,'ftheta':filt.phi,'added_score':to_score.sum(),'min_group':filt.min_group,\n                                                    'full_score':frs.value,'score':fsd.value,'correct':fss.value,\n                                                    'clustered':tm.shape[0],'left':rest.shape[0]-tm.shape[0]}, index=[0]))\n\n                #end of printing part \n                tm=rest[select][['hit_id','track_id','kt','z0']]\n                res_list.append(tm)\n                rest = rest[~select]\n                dum,rest['track_len']=tag_bins(rest['track_id'])\n                rest['pre_track_id']=rest['track_id']\n\n    ft.value = rest.shape[0]\n    res_list.append(rest[['hit_id','track_id','kt','z0']].copy())\n    res = pd.concat(res_list, ignore_index=True)\n    pbar.close()\n    rest['track_id'],dum=tag_bins(rest['track_id'])\n     \n    if not history is None:\n        history.append(pd.concat(hist_list,ignore_index=False))\n    print ('took {:.5f} sec'.format(time.time()-start))\n    return res \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"028e58874978f0689caa5d9e95198154ef003436"},"cell_type":"code","source":"def refine_hipos(res,hits,stds,nhipos,phik=3.3,weights=None): \n    cols=list(res.columns)\n    if weights is None:\n        weights={'theta':0.15, 'phi':1.0}\n\n    groups = res.merge(hits,on='hit_id',how='left')\n    if not groups.columns.contains('kt'):\n        groups['kt']=0\n        groups['z0']=0\n        print(\"No kt's, calculating\")\n    calc_features(groups,groups[['kt','z0']],phik)\n\n    gp=groups.groupby('track_id').agg({'phi': np.std , 'sint' : np.std,\n            'cost' : np.std}).rename(columns={ 'phi': 'phi_std', \n            'sint' : 'sint_std', 'cost':'cost_std'}).reset_index()\n    groups=groups.merge(gp,on='track_id',how='left')\n    groups['theta_std']=np.sqrt(weights['theta']*np.square(groups.sint_std)+weights['theta']*np.square(groups.cost_std))\n    hipos=pd.DataFrame()\n    for col in stds:\n        hipos[col]=np.random.normal(scale=stds[col],size=nhipos)\n\n    for hipo in tqdm(hipos.itertuples(),total=nhipos):\n\n        groups['kt_new']=groups['kt']+hipo.kt\n        groups['z0_new']=groups['z0']+hipo.z0\n        calc_features(groups,groups[['kt_new','z0_new']].rename(columns={\"kt_new\": \"kt\", \"z0_new\": \"z0\"}),phik)\n        gp=groups.groupby('track_id').agg({'phi': np.std , 'sint' : np.std,\n            'cost' : np.std}).rename(columns={ 'phi': 'new_phi_std', \n            'sint' : 'new_sint_std', 'cost':'new_cost_std'}).reset_index()\n        groups=groups.merge(gp,on='track_id',how='left')\n        groups['new_theta_std']=np.sqrt(weights['theta']*np.square(groups.new_sint_std)+weights['theta']*np.square(groups.new_cost_std))\n\n        old_std=np.sqrt(np.square(groups.theta_std)+weights['phi']*np.square(groups.phi_std))\n        new_std=np.sqrt(np.square(groups.new_theta_std)+np.square(groups.new_phi_std))\n        cond=(old_std<=new_std) \n        groups['kt']=groups['kt'].where(cond,groups.kt_new)\n        groups['z0']=groups['z0'].where(cond,groups.z0_new)\n        groups['theta_std']=groups['theta_std'].where(cond,groups.new_theta_std)\n        groups['sint_std']=groups['sint_std'].where(cond,groups.new_sint_std)\n        groups['cost_std']=groups['cost_std'].where(cond,groups.new_cost_std)\n        groups['phi_std']=groups['phi_std'].where(cond,groups.new_phi_std)\n        groups=groups.drop(['new_theta_std','new_phi_std','new_sint_std','new_cost_std'],axis=1)\n\n        #pdb.set_trace()\n    to_return=groups[cols+['theta_std','phi_std','sint_std','cost_std']]\n    return to_return\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1681033621feb237981cd38709db1fe91ab1620"},"cell_type":"code","source":"def expand_tracks(res,hits,min_track_len,max_track_len,max_expand,to_track_len,mstd=1.0,dstd=0.0,phik=3.3,max_dtheta=10,mstd_size=None,mstd_vol=None,drop=0,nhipo=1000,weights=None):\n    if weights is None:\n        weights={'theta':0.25, 'phi':1.0}\n\n    if mstd_size is None:\n        mstd_size=[0 for i in range(20)]\n    if mstd_vol is None:\n        mstd_vol={7:0,8:0,9:0,12:0,13:0,14:0,16:0,17:0,18:0}\n    gp=res.groupby('track_id').first().reset_index()\n    orig_hipo=gp[['track_id','kt','z0']]\n    eres=res.copy()\n    res_list=[]\n    stds={'kt':7e-5,'z0':0.8}\n    eres=refine_hipos(eres,hits,stds,nhipo,phik=phik,weights=weights)\n    dum,eres['track_len']=tag_bins(eres['track_id'])\n    eres['max_track_len']=np.clip(eres.track_len+max_expand,0,max_track_len) \n    eres['max_track_len']=2*(  eres['max_track_len']/2).astype('int')+1\n    eres=eres.sort_values('track_len')\n    eres = eres.merge(hits,on='hit_id',how='left')\n    eres['sensor']=eres.volume_id+eres.layer_id*100+100000*eres.module_id\n    group_sensors=eres.groupby('track_id').sensor.unique()\n    groups=eres[eres.track_len>min_track_len].groupby('track_id').first().reset_index().copy()\n    groups['order']=-groups.track_len \n    groups=groups.sort_values('order').reset_index(drop=True)\n    groups=groups.head(int((1.0-drop)*groups.shape[0])).copy()\n    select=eres.track_len<to_track_len\n    grouped=eres[~select]\n    regrouped=eres[select].copy()\n    regrouped['min_dist']=100\n    regrouped['new_track_len']=0\n    regrouped['new_track_id']=regrouped['track_id']\n    regrouped['new_kt']=regrouped['kt']\n    regrouped['new_z0']=regrouped['z0']\n    regrouped['new_max_size'] = max_track_len\n\n    f = FloatProgress(min=0, max=groups.shape[0], description='calculating:') # instantiate the bar\n    display(f) # display the bar\n\n    for group_tul in tqdm(groups.itertuples(),total=groups.shape[0]):\n        if group_tul.Index%20 ==0: f.value=group_tul.Index\n        if group_tul.track_len>=max_track_len: continue\n        group=eres[eres.track_id==group_tul.track_id].copy()\n        calc_features(group,group[['kt','z0']],phik)\n        group['abs_z']=np.abs(group.z)\n        group['abs_theta']=np.abs(group.theta)\n        phi_mean=group.phi.mean()\n        sint_mean=group.sint.mean()            \n        cost_mean=group.cost.mean()\n        max_z=group.abs_z.max()\n        max_theta=group.abs_theta.max()\n        regrouped['abs_z']=np.abs(regrouped.z)\n        calc_features(regrouped,group_tul,phik,double_sided=True)\n        regrouped['dist'] =np.sqrt(weights['theta']*np.square(regrouped.sint-sint_mean)+weights['theta']*np.square(regrouped.cost-cost_mean)+weights['phi']*np.square(regrouped.phi-phi_mean))\n        regrouped['dist2'] =np.sqrt(weights['theta']*np.square(regrouped.sint2-sint_mean)+weights['theta']*np.square(regrouped.cost2-cost_mean)+weights['phi']*np.square(regrouped.phi2-phi_mean))\n        select = (regrouped.abs_z>max_z)  & (max_dtheta >max_dtheta) & (regrouped.dist2<regrouped.dist)\n        regrouped['dist']=regrouped['dist'].where(~select,regrouped['dist2'])    \n        cmstd=regrouped.volume_id.map(mstd_vol)+mstd_size[group_tul.track_len]+mstd\n        if (dstd==0.0):\n            sdstd==group.dstd\n        else:\n            sdstd=dstd\n        better =( regrouped.dist<cmstd*sdstd) & ( regrouped.dist<regrouped.min_dist) & (~regrouped.sensor.isin(group_sensors.loc[group_tul.track_id]))\n        regrouped['min_dist']=np.where(better,regrouped.dist,regrouped.min_dist)\n        regrouped['new_track_id']=np.where(better,group_tul.track_id,regrouped.new_track_id)\n        regrouped['new_z0']=np.where(better,group_tul.z0,regrouped.new_z0)\n        regrouped['new_kt']=np.where(better,group_tul.kt,regrouped.new_kt)\n        regrouped['new_track_len']=np.where(better,group_tul.track_len,regrouped.new_track_len)\n        regrouped['new_max_size']=np.where(better,group_tul.max_track_len,regrouped.new_max_size)\n    f.value=group_tul.Index\n    regrouped=regrouped.sort_values('min_dist')\n    regrouped['closest']=regrouped.groupby('new_track_id')['min_dist'].cumcount()\n    better=regrouped.closest+regrouped.new_track_len>=regrouped.new_max_size\n    regrouped['track_id']=regrouped['track_id'].where(better,regrouped['new_track_id'])\n    res_list.append(grouped[['hit_id','track_id']])\n    res_list.append(regrouped[['hit_id','track_id']])\n    to_return=pd.concat(res_list)\n    to_return=to_return.merge(orig_hipo,on='track_id',how='left')\n    return to_return\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00cff1de64d7a7f24df2e9430344f5ae90dc953c"},"cell_type":"code","source":"history=[]\nevent_num = 0\nevent_prefix = 'event00000100{}'.format(event_num)\nhits, cells, particles, truth = load_event(os.path.join(path, event_prefix))\nweights={'pi':1,'theta':0.15}\nstds={'z0':7.5, 'kt':7.5e-4}\nd =    {'sint':[225,110,110,110,110,110],\n        'cost':[225,110,110,110,110,110],\n          'phi':[550,260,260,260,260,260],\n        'min_group':[11,11,10,9,8,7],\n        'npoints':[500,2000,1000,1000,500,500]}\n\n\nfilters=pd.DataFrame(d)\nnu=500\nresa=clustering(hits,stds,filters,phik=3.3,nu=nu,truth=truth,history=history)\nresa[\"event_id\"]=event_num\nscore = score_event(truth, resa.rename(index=str, columns={\"label\": \"track_id\"}))\nprint(\"Your score: \", score)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f17bf5518006da909b788b04fd6646317b65dc9b"},"cell_type":"code","source":"mstd_vol={7:0,8:0,9:0,12:2,13:1,14:2,16:3,17:2,18:3}\nmstd_size=[4,4,4,4,3,3,3,2,2,2,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\nweights={'theta':0.1, 'phi':1}\nnresa=expand_tracks(resa,hits,5,16,5,7,mstd=8,dstd=0.00085,phik=3.3,max_dtheta=0.9*np.pi/2,mstd_vol=mstd_vol,mstd_size=mstd_size,weights=weights,nhipo=1000)\nnresa['event_id']=0\nscore = score_event(truth, nresa)\nprint(\"Your score: \", score)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"317879ec4824598fad62b7faba8f981eed065681"},"cell_type":"code","source":"import numba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5800822ec8d5798c6bc9ecba2df5f20964fe13cc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}