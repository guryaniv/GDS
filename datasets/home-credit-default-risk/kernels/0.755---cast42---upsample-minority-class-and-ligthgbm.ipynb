{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import resample\n\nfrom lightgbm import LGBMClassifier\nimport gc\nimport pandas_profiling\nimport matplotlib.pyplot as plt","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"aa6ae96e41a469010166bf81334dafe06b3a7239"},"cell_type":"markdown","source":"# Feature selection\n![](http://)Overlap (or misclassification rate) and \"probability of superiority\" have two good properties:\n* As probabilities, they don't depend on units of measure, so they are comparable between studies.\n* They are expressed in operational terms, so a reader has a sense of what practical effect the difference makes.\n\n### Cohen's effect size\nThere is one other common way to express the difference between distributions.  Cohen's $d$ is the difference in means, standardized by dividing by the standard deviation.  Here's the math notation:\n \n $ d = \\frac{\\bar{x}_1 - \\bar{x}_2} s $\n \nwhere $s$ is the pooled standard deviation:\n\n$s = \\sqrt{\\frac{n_1 s^2_1 + n_2 s^2_2}{n_1+n_2}}$\n\n Here's a function that computes it:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b56411cbba9583d3f7bc8a6fcac52491b9698111"},"cell_type":"code","source":"def cohen_effect_size(X, y):\n    \"\"\"Calculates the Cohen effect size of each feature.\n    \n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target vector relative to X\n        Returns\n        -------\n        cohen_effect_size : array, shape = [n_features,]\n            The set of Cohen effect values.\n        Notes\n        -----\n        Based on https://github.com/AllenDowney/CompStats/blob/master/effect_size.ipynb\n    \"\"\"\n    group1, group2 = X[y==0], X[y==1]\n    diff = group1.mean() - group2.mean()\n    var1, var2 = group1.var(), group2.var()\n    n1, n2 = group1.shape[0], group2.shape[0]\n    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n    d = diff / np.sqrt(pooled_var)\n    return d","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5700b7043f53632cf6d57e4e0526da26afac99f0"},"cell_type":"code","source":"LoanID = 'SK_ID_CURR'\ndata   = pd.read_csv('../input/application_train.csv').set_index(LoanID)\ntest   = pd.read_csv('../input/application_test.csv').set_index(LoanID)\nprev   = pd.read_csv('../input/previous_application.csv')\nburo   = pd.read_csv('../input/bureau.csv')\nburobl = pd.read_csv('../input/bureau_balance.csv')\ncredit = pd.read_csv('../input/credit_card_balance.csv')","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"84a0ad02ee4a9b10f22637e7804997705e860767"},"cell_type":"markdown","source":"# Data preparation based on [fork-of-good-fun-with-ligthgbm-more-features](https://www.kaggle.com/cttsai/fork-of-good-fun-with-ligthgbm-more-features)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3fac535e9dab514e3b1b5909d240cfdd3725a6ec"},"cell_type":"code","source":"def PivotGroupBy(df, groupby_id, target_id, feature_name='', cutoff=0.05):\n    cnt_name = 'cnt_{0}'.format(target_id)\n    tmp = df.groupby([groupby_id])[target_id].value_counts(normalize=True)\n    tmp = tmp.loc[tmp >= cutoff].rename(cnt_name).reset_index()\n    tmp = tmp.pivot(index=groupby_id, columns=target_id, values=cnt_name)\n    tmp.rename(columns={f:'{0}_r={1}'.format(feature_name, f) for f in tmp.columns}, inplace=True)\n    return tmp\n\ndef CatMeanEnc(df, index_name, groupby_ids):\n###################################\n# PLEASE DON'T DO THIS AT HOME LOL\n# Averaging factorized categorical features defeats my own reasoning\n################################### \n    cat_features = [f_ for f_ in df.columns if df[f_].dtype == 'object']\n    \n    df_pivots = [PivotGroupBy(df, index_name, f_, feature_name=f_) for f_ in cat_features]\n    df_pivots = pd.concat(df_pivots, axis=1)\n#    for f_ in cat_features:\n#        df[f_], _ = pd.factorize(df[f_])\n\n    df_ret = df[[f for f in df.columns if f not in cat_features]].groupby(LoanID).mean().join(df_pivots, how='left')\n    print(df_ret.describe())\n    df_ret['cnt_{:}'.format(index_name)] = df[[groupby_ids, index_name]].groupby(groupby_ids).count()[index_name]\n    del df_ret[index_name]\n    return df_ret\n\ndef JoinMeanEnc(main_df, join_dfs=[]):\n    for df in join_dfs:\n        print(main_df.shape, df.shape)\n        f_join = [f_ for f_ in df.columns if f_ not in main_df.columns]\n        main_df = main_df.join(df[f_join], how='left')\n    return main_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9c622eac03d162aed7c6472e5119f69ae15e7cd","collapsed":true},"cell_type":"code","source":"# Attach bureau_balance to Bureau\ntmp = PivotGroupBy(burobl, 'SK_ID_BUREAU', 'STATUS', feature_name='bureau_balance')\ntmp['LONGEST_MONTHS'] = burobl.groupby(['SK_ID_BUREAU'])['MONTHS_BALANCE'].size()\nburo = buro.join(tmp, how='left', on='SK_ID_BUREAU')\nprint(buro.head())\ndel burobl, tmp\n\n# factorize         \ncategorical_feats = [f for f in data.columns if data[f].dtype == 'object']\nfor f_ in categorical_feats:\n    data[f_], indexer = pd.factorize(data[f_])\n    test[f_] = indexer.get_indexer(test[f_])\n    \nprint(data.shape, test.shape)\n\ny = data['TARGET']\ndel data['TARGET']\n\navg_dfs = [CatMeanEnc(prev,   index_name='SK_ID_PREV', groupby_ids=LoanID), \n           CatMeanEnc(buro,   index_name='SK_ID_BUREAU', groupby_ids=LoanID), \n           CatMeanEnc(credit, index_name='SK_ID_PREV', groupby_ids=LoanID)]\ndata = JoinMeanEnc(data, join_dfs=avg_dfs)\ntest = JoinMeanEnc(test, join_dfs=avg_dfs)\n\nexcluded_feats = [] #['SK_ID_CURR']\nfeatures = [f_ for f_ in data.columns if f_ not in excluded_feats]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09c8551ffc86ed3ca4cec35fdffc38092c8df961","collapsed":true},"cell_type":"code","source":"print('Number of features %d' % len(features))\neffect_sizes = cohen_effect_size(data[features], y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b0c8fcbbd9fbd470438aec99373ab8871761e94","collapsed":true},"cell_type":"code","source":"effect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(30).index)[::-1].plot.barh(figsize=(6, 10));\nplt.title('Features with the 30 largest effect sizes');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a542446ac95d27ab115ed3a6ebd1774d7431952","collapsed":true},"cell_type":"code","source":"significant_features = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.1]\nprint('Significant features %d: %s' % (len(significant_features), significant_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6356dc2f8e2c5b07369b38b7bc93a7819840cc0d"},"cell_type":"markdown","source":"# Explore the data"},{"metadata":{"trusted":true,"_uuid":"065853d88aabba262a0b252789f0f0b916a99a4e","collapsed":true},"cell_type":"code","source":"profile = pandas_profiling.ProfileReport(data[significant_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b9ca55b460cb1ba7bff3e75827842693415e6d6b"},"cell_type":"code","source":"profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b2a635213c4d91f03b617702461ecd078ecae01b"},"cell_type":"code","source":"rejected_variables = profile.get_rejected_variables(threshold=0.9)\nrejected_variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3add9021e35b78d8e2d5dce431e4419d11d5ad8d","collapsed":true},"cell_type":"code","source":"selected_features = list(set(significant_features) - set(rejected_variables))\nselected_features","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"adda3cde94ad409b3e8382f8bd4a7c642bdf1226"},"cell_type":"markdown","source":"# Impute the data"},{"metadata":{"trusted":true,"_uuid":"b8dd0a8a07d66b1816d0976a096ccf35aea3c343","collapsed":true},"cell_type":"code","source":"X = data[selected_features].copy()\n# X['EXT_SOURCE_1'].fillna((X['EXT_SOURCE_1'].mean()), inplace=True)\n# X['EXT_SOURCE_2'].fillna((X['EXT_SOURCE_2'].median()), inplace=True)\n# X['EXT_SOURCE_3'].fillna((X['EXT_SOURCE_3'].median()), inplace=True)\n# X['CNT_DRAWINGS_ATM_CURRENT'].fillna((X['CNT_DRAWINGS_ATM_CURRENT'].median()), inplace=True)\n# X['DAYS_CREDIT'].fillna((X['DAYS_CREDIT'].mean()), inplace=True)\n# X['AMT_BALANCE'].fillna(0, inplace=True)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7c5ee1965922471a6c6aa9f985514a9a67176f48"},"cell_type":"code","source":"# Fill the remaining Nan's with zero\nX.fillna(0, inplace=True)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"152763deef7cf95e0b72d747c9647827fbb51571"},"cell_type":"markdown","source":"# Upsample the minority class to match the majority class with SMOTE"},{"metadata":{"trusted":true,"_uuid":"be4edd47527d6d5d2aabf3a5708c89d9b5edddb8","collapsed":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d0ecfdb20f9b8691adc5a05fae585edd195396b1"},"cell_type":"code","source":"X_resampled, y_resampled = SMOTE().fit_sample(X, y)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7276937ec1143ccbbd0dfecf54aeda57c7d873c3"},"cell_type":"code","source":"params_LGBM = {\n    'n_estimators'     : 4000,\n    'learning_rate'    : 0.03,\n    'num_leaves'       : 70,\n    'colsample_bytree' : 0.8,\n    'subsample'        : 0.9,\n    'max_depth'        : 7,\n    'reg_alpha'        : 0.1,\n    'reg_lambda'       : 0.1,\n    'min_split_gain'   : 0.01,\n    'min_child_weight' : 2,\n    'silent'           : True,\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9610bd90822186584cc14f54e023b20061d3ba69","collapsed":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1301)\noof_preds = np.zeros(X_resampled.shape[0])\nsub_preds = np.zeros(test.shape[0])\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(X_resampled, y_resampled)):\n    trn_x, trn_y = X_resampled[trn_idx], y_resampled[trn_idx]\n    val_x, val_y = X_resampled[val_idx], y_resampled[val_idx]\n    \n    clf = LGBMClassifier(**params_LGBM)\n    \n    clf.fit(trn_x, trn_y, \n            eval_set= [(trn_x, trn_y), (val_x, val_y)], \n            eval_metric='auc', verbose=100, early_stopping_rounds=150\n           )\n    \n    oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n    sub_preds += clf.predict_proba(test[selected_features], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n    \n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    del clf, trn_x, trn_y, val_x, val_y\n    gc.collect()\n\nscore = roc_auc_score(y_resampled, oof_preds)\nprint('Full AUC score %.6f' % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d08799fd8b9aed7a31745aefa1ecec617b0ad5b"},"cell_type":"markdown","source":"# Prepare for submission"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6496cc7b30b0af8bef6a524554c08e545a7c2c91"},"cell_type":"code","source":"test['TARGET'] = sub_preds\ntest[['TARGET']].to_csv('subm_lgbm_auc{:.8f}.csv'.format(score), index=True, float_format='%.8f')","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b739e9d068d6689ee254e1c618ab2a53fcb2768d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}