{"nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"nbconvert_exporter": "python", "name": "python", "version": "3.6.3", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "9dacb7a2a5a9e582d7e597be4ea479cedd79489a", "_cell_guid": "d49b7eaa-0bf7-48e0-9fe6-51ba03036ba1"}, "source": ["# Classifying multi-label comments with Logistic Regression\n", "#### Rhodium Beng\n", "Started on 20 December 2017\n", "\n", "This kernel is inspired by:\n", "- kernel by Jeremy Howard : _NB-SVM strong linear baseline + EDA (0.052 lb)_\n", "- kernel by Issac : _logistic regression (0.055 lb)_\n", "- _Solving Multi-Label Classification problems_, https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "5c5f4cc8865644748e11336736bbe584adebe7b1", "_cell_guid": "8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6", "collapsed": true}, "source": ["import numpy as np\n", "import pandas as pd\n", "from matplotlib import pyplot as plt\n", "%matplotlib inline\n", "import seaborn as sns\n", "import re"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "4f65d03ddbfd127307d3e415003346eb898b4d6b", "_cell_guid": "80d61838-9025-4cba-bb0e-58175586b21b"}, "source": ["## Load training and test data"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "4e35cd5fcae1581dbd6bc51f14728e27fe63fe70", "_cell_guid": "094fff47-db10-447c-965e-08056f718bde", "collapsed": true}, "source": ["train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "89d1c9a4f9598427e8a20d66fa9e56796ad720f6", "_cell_guid": "09986b08-eda6-4438-9cbe-52a61d8d57fa"}, "source": ["## Examine the data (EDA)"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "9e53b7599d707a9420a75c37c7ac6d05bed9df7b", "_cell_guid": "c4c7137d-6bc7-4b41-b50a-e511883155e9", "_kg_hide-output": true}, "source": ["train_df.sample(5)"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "6c824d91ae1e801e1489e93e1a9932c8c0cb0e0a", "_cell_guid": "40597119-1274-4d5b-a054-b4b17dbcbb36"}, "source": ["In the training data, the comments are labelled as one or more of the six categories; toxic, severe toxic, obscene, threat, insult and identity hate. This is essentially a multi-label classification problem."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "7f7f2581edb2f42a64812dec31622a011dceff80", "_cell_guid": "7e29bebb-d9b7-44ab-a6ba-9f98e6507d5e", "collapsed": true}, "source": ["cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "e4a226272e319458391e117e8fb7f16b17c4884f", "_cell_guid": "ce00e980-da07-4412-ae4c-5152cc2036e0"}, "source": ["# check missing values in numeric columns\n", "train_df.describe()"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "474f4cf26190a2f2011ca0908052973fec1b1520", "_cell_guid": "b2be6443-eb4e-4e12-81b3-faf2ea692ca4"}, "source": ["There are no missing numeric values. Based on the mean values, it also looks like there are many comments which are not labelled in any of the six categories."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "99f1db4864c5c538f2a925d7b6733bb4b1c68707", "_cell_guid": "997cc605-71a0-4ceb-a64b-e3e44c172aea"}, "source": ["# check for any 'null' comment\n", "no_comment = train_df[train_df['comment_text'].isnull()]\n", "len(no_comment)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "964e57595bb6e87a47aa82557c9d63aae3c24bd0", "_cell_guid": "af5ba625-bd5d-4ad3-948b-5641b10d62fb", "_kg_hide-output": true}, "source": ["test_df.head()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "c616bcb0dcf611679dcb5009def9d71d6727cd0e", "_cell_guid": "6bd65c22-9c8a-4756-a7bf-16187a6044d1"}, "source": ["no_comment = test_df[test_df['comment_text'].isnull()]\n", "no_comment"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "ad7f31ff8035dc60d37c8070f8bbaa9e7afac32f", "_cell_guid": "ecce8833-b5b4-40b4-a164-015b7380b8b1"}, "source": ["There is a row in the test data which does not contain any comment, so let's put 'unknown' in its place."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "ffeacaea28588d64762dc2a67aedddb4b04088b5", "_cell_guid": "ec4d0216-9bae-4d23-bbb6-c4afddf34815", "collapsed": true}, "source": ["# fill NaN with string \"unknown\"\n", "test_df.fillna('unknown',inplace=True)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "e497fc6688e16602e3f49a11939f32324d295a8d", "_cell_guid": "b3dcbd9f-dbb8-4a5a-96b2-b93882516e27"}, "source": ["# let's see the total rows in train, test data and the numbers for the various categories\n", "print('Total rows in train is {}'.format(len(train_df)))\n", "print('Total rows in test is {}'.format(len(test_df)))"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "c5aea4341bf7da46627704b28f162776fc2a8c59", "_cell_guid": "26e7c4a8-bf9d-4105-9e84-b48f9f056015"}, "source": ["print(train_df[cols_target].sum())"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "2ff9ed83ba328872d446add97695285dc49f4165", "_cell_guid": "981dff09-3acf-4014-b38a-22afc02a6654", "collapsed": true}, "source": ["Majority of the comments are not labelled in one or more of these categories."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "1e97432af65b6b75b436daabc83bdf57775a59c1", "_cell_guid": "b26588a7-9a7f-4183-98b2-fb94a70bedaa", "collapsed": true}, "source": ["# Let's look at the character length for the rows and record these\n", "train_df['char_length'] = train_df['comment_text'].str.len()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"scrolled": true, "_uuid": "448c3492fc2fe24f30bd7b97047d69f16b58ca2f", "_cell_guid": "d5ac5111-5bba-44c9-a039-7bb01a5bfd59"}, "source": ["# look at the histogram plot for text length\n", "sns.set()\n", "train_df['char_length'].hist()\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "d28a801b4057518f87e20c46a77f9dc8757ab944", "_cell_guid": "5b482de7-5fd1-4ef7-b7b4-2abf84ebdc25"}, "source": ["Most of the text length are within 500 characters, with some up to 5,000 characters long."], "cell_type": "markdown"}, {"metadata": {"_uuid": "e1e0ecc1df6989b0ee73874f273f0dbbfc4c9d5e", "_cell_guid": "f16a287f-27d4-4afa-82a1-dd441c2fd36c"}, "source": ["Next, let's examine the correlations among the target variables."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "fab22b3f850c80a10d665ae43ee09b5107a79887", "_cell_guid": "64164a2c-770f-469d-9020-e91714a9b2a8", "collapsed": true}, "source": ["data = train_df[cols_target]"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"scrolled": true, "_uuid": "58968a44d8fdb3b93ac57f1ca20f81ffb71d164f", "_cell_guid": "7fc7803b-a7f1-414d-84fa-d1d2201c8bb7", "_kg_hide-output": false}, "source": ["colormap = plt.cm.magma\n", "plt.figure(figsize=(7,7))\n", "plt.title('Correlation of features & targets',y=1.05,size=14)\n", "sns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n", "           linecolor='white',annot=True)"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "4f9da9651b4e007088b19a06165d0727fb4f4e07", "_cell_guid": "095fd031-32a0-400b-9357-af0c8f50dd6b"}, "source": ["Indeed, it looks like the various labels are correlated, e.g. insult-obscene has the highest at 0.74, followed by toxic-obscene and toxic-insult."], "cell_type": "markdown"}, {"metadata": {"_uuid": "b677d6a32b7b72e08ba3b46bce572a223db964de", "_cell_guid": "caa00f5d-7e26-48cb-92b3-acc1f1de0aca"}, "source": ["What about the character length & distribution of the comment text in the test data?"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "36e4d00a5afc15e6b04ffa1e79421396d051f614", "_cell_guid": "0993d06b-495e-428a-933a-6ffec6bdcef3", "collapsed": true}, "source": ["test_df['char_length'] = test_df['comment_text'].str.len()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "1cc05875b88e54b5a1079eafc088207536dea2f3", "_cell_guid": "828b9990-d78e-46c7-b011-1c66e6e6be79"}, "source": ["plt.figure(figsize=(20,5))\n", "plt.hist(test_df['char_length'])\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "e429fb37db620abf3de274ccdd274c4f1517c5fe", "_cell_guid": "495c87cf-6764-4d87-bb32-0e48cd799e63"}, "source": ["Looks like there are several very long comments in the test data. Let's see what they are."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "b7403f24f1632e296c2d46190b46f82fdb926b4b", "_cell_guid": "5398026f-ac8d-43ce-bc55-e78ad89294eb", "_kg_hide-output": true}, "source": ["test_df[test_df['char_length']>5000]"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "edf30fd8b2192e4e67a277b5d76c20b6e82d6283", "_cell_guid": "77f3d54a-efd3-4251-b74b-3eea98edaed0"}, "source": ["Let's truncate char length in test_df to 5,000 characters and see if the distribution would be similar to train_df."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "6696494aacc851e303a8d724428688e2a2ac7751", "_cell_guid": "5102345d-3d98-4759-876c-a76eb687a82c", "collapsed": true}, "source": ["test_comment = test_df['comment_text'].apply(lambda x: x[:5000])\n", "char_length = test_comment.str.len()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "deba592d146256b223dc32143ad5f2be269e8279", "_cell_guid": "600871e5-ceae-4689-b1b3-b5298558ddea"}, "source": ["plt.figure()\n", "plt.hist(char_length)\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "db69eb685cd1be44de2224c399cbdeabb5ceaa06", "_cell_guid": "b2bbf246-f098-425a-99e3-2eb2126b975c"}, "source": ["Now, the shape of character length distribution looks similar to the train data. I guess the train data were clipped to 5,000 characters to facilitate the folks who did the labelling of comment categories."], "cell_type": "markdown"}, {"metadata": {"_uuid": "d88d9cea99dbd77e81a5b3c4b9309df88b04550b", "_cell_guid": "fdf9d2f6-d248-452f-8f94-562755e3a3f3"}, "source": ["## Clean up the comment text"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "b42586552d4cdc79793b0de8e630f863f2b2c456", "_cell_guid": "24392feb-0adc-4e27-bd20-41ed8cadce37", "collapsed": true}, "source": ["def clean_text(text):\n", "    text = re.sub(r\"what's\", \"what is \", text)\n", "    text = re.sub(r\"\\'s\", \" \", text)\n", "    text = re.sub(r\"\\'ve\", \" have \", text)\n", "    text = re.sub(r\"can't\", \"cannot \", text)\n", "    text = re.sub(r\"n't\", \" not \", text)\n", "    text = re.sub(r\"i'm\", \"i am \", text)\n", "    text = re.sub(r\"\\'re\", \" are \", text)\n", "    text = re.sub(r\"\\'d\", \" would \", text)\n", "    text = re.sub(r\"\\'ll\", \" will \", text)\n", "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n", "    text = re.sub(r\"\\.\", \" \", text)\n", "    text = re.sub('\\W', ' ', text)\n", "    text = re.sub('\\s+', ' ', text)\n", "    text = text.strip()\n", "    return text"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "e67944653b23b6267fdb0634c03a5b8702ae6d26", "_cell_guid": "ec11fdb5-22a8-4889-9a8d-c83d4179a0cf", "_kg_hide-output": false}, "source": ["# clean the comment_text in train_df\n", "cleaned_train_comment = []\n", "for i in range(0,len(train_df)):\n", "    cleaned_comment = clean_text(train_df['comment_text'][i])\n", "    cleaned_train_comment.append(cleaned_comment)\n", "train_df['comment_text'] = pd.Series(cleaned_train_comment).astype(str)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "3ed1fff4601eb381b0c9f2da0dddef0453b248d3", "_cell_guid": "5292f0f4-cdaf-4e9e-88f4-28efc3aa224d", "collapsed": true}, "source": ["# clean the comment_text in test_df\n", "cleaned_test_comment = []\n", "for i in range(0,len(test_df)):\n", "    cleaned_comment = clean_text(test_df['comment_text'][i])\n", "    cleaned_test_comment.append(cleaned_comment)\n", "test_df['comment_text'] = pd.Series(cleaned_test_comment).astype(str)"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "cae81f6b1d9bb475fc486d5fbb81981025cc3672", "_cell_guid": "f5abe72a-13a7-41f6-ae8e-1c34dca97110"}, "source": ["## Define X from entire train & test data for use in tokenization by Vectorizer"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "b15dba3906f67e764af0a627e46d7b7e486888c5", "_cell_guid": "30bf94de-36aa-4e8d-8d12-64172d8dc446", "collapsed": true}, "source": ["train_df = train_df.drop('char_length',axis=1)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "061d59552c6ef83bea8ecf9ffbf203286aeab6f8", "_cell_guid": "49547fd7-9633-4f6a-bd46-84d8966f1e8b", "collapsed": true}, "source": ["X = train_df.comment_text\n", "test_X = test_df.comment_text"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "6b1e91df6163a437c5f55e9c4de88dc11a89e5ba", "_cell_guid": "083686b8-483a-4fbd-8584-cb9da8357c57"}, "source": ["print(X.shape, test_X.shape)"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "cc4c2aeef221f5a97e1bd5ea1154052172175351", "_cell_guid": "b2d898ae-79bc-48b8-8544-fb077f876c67"}, "source": ["## Vectorize the data"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "7b7adf15d16408eb99689884906d0d687c2f8407", "_cell_guid": "9be5a4f2-0e85-4f9a-ac00-b8e9916116cb"}, "source": ["# import and instantiate CountVectorizer\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "vect = TfidfVectorizer(max_features=20000,min_df=2)\n", "vect"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "b755b1d4db58eeb5b0ab668d1aaf4a651d3de441", "_cell_guid": "283c1d48-c267-431b-834e-37c8d9222b3c"}, "source": ["# learn the vocabulary in the training data, then use it to create a document-term matrix\n", "X_dtm = vect.fit_transform(X)\n", "# examine the document-term matrix created from X_train\n", "X_dtm"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "408301ccb78e3f4056a6d2ebbd239594d1a59da0", "_cell_guid": "54050711-560a-47cf-b4f9-2fbaf59bc2e4"}, "source": ["# transform the test data using the earlier fitted vocabulary, into a document-term matrix\n", "test_X_dtm = vect.transform(test_X)\n", "# examine the document-term matrix from X_test\n", "test_X_dtm"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "98a540b42db71f1639d47f31d2a4ea851aa1b9e5", "_cell_guid": "c84eb9f4-4fa5-418c-883c-9d9316092db0"}, "source": ["## Solving a multi-label classification problem\n", "One way to approach a multi-label classification problem is to transform the problem into separate single-class classifier problems. This is known as 'problem transformation'. There are three methods:\n", "* _**Binary Relevance.**_ This is probably the simplest which treats each label as a separate single classification problems. The key assumption here though, is that there are no correlation among the various labels.\n", "* _**Classifier Chains.**_ In this method, the first classifier is trained on the input X. Then the subsequent classifiers are trained on the input X and all previous classifiers' predictions in the chain. This method attempts to draw the signals from the correlation among preceding target variables.\n", "* _**Label Powerset.**_ This method transforms the problem into a multi-class problem  where the multi-class labels are essentially all the unique label combinations. In our case here, where there are six labels, Label Powerset would in effect turn this into a six-factorial or 720-class problem!"], "cell_type": "markdown"}, {"metadata": {"_uuid": "389245398dce9573dfa0f7c2facd2849d2174f1f", "_cell_guid": "7abd771a-b0f3-47bc-b4ff-13a78aff48e7"}, "source": ["## Binary Relevance - build a multi-label classifier using Logistic Regression"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "e7e5707b19c35c8371a0cff7351bc8aadc33acd1", "_cell_guid": "e30be87f-e0a6-4fd7-bff2-b3c37737cbfe"}, "source": ["# import and instantiate the Logistic Regression model\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import accuracy_score\n", "logreg = LogisticRegression(C=6.0,random_state=123)\n", "\n", "# create submission file\n", "submission_binary = pd.read_csv('../input/sample_submission.csv')\n", "\n", "for label in cols_target:\n", "    print('... Processing {}'.format(label))\n", "    y = train_df[label]\n", "    # train the model using X_dtm & y\n", "    logreg.fit(X_dtm, y)\n", "    # compute the training accuracy\n", "    y_pred_X = logreg.predict(X_dtm)\n", "    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n", "    # compute the predicted probabilities for X_test_dtm\n", "    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n", "    submission_binary[label] = test_y_prob"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "ee6d942b861235107e0d94174a8d21c2bad0e9ea", "_cell_guid": "5d0970eb-bc44-4fad-91b7-e2a23c2f986d"}, "source": ["### Create submission file"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "b89e22016a7b4f282940dc23253cbf343c1fbf83", "_cell_guid": "a2816552-c314-4584-ad49-8464e80b1b29"}, "source": ["submission_binary.head()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "75d9571cc77eb1805d897af6ca0d86fd28405c6d", "_cell_guid": "fb1bef7d-586e-437d-a1d3-b0868e7ac321", "collapsed": true}, "source": ["# generate submission file\n", "submission_binary.to_csv('submission_binary.csv',index=False)"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "18644d484121f3bab7a06207bd7fa739b15feff5", "_cell_guid": "be8c8e90-f798-46d8-b77c-cd0668292646"}, "source": ["#### Binary Relevance with Logistic Regression classifier scored 0.062 on the public leaderboard."], "cell_type": "markdown"}, {"metadata": {"_uuid": "0393cd387f508609c0d068c68fb7dbb0be659383", "_cell_guid": "5018c7ea-dd27-4d4c-b9ad-ef2140c773e5"}, "source": ["## Classifier Chains - build a multi-label classifier using Logistic Regression"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "f1b4fea94c83661d7bbad5c6bc7a5994643128e2", "_cell_guid": "b2172222-42b8-4f0a-8d20-160d52af6f62", "collapsed": true}, "source": ["# create submission file\n", "submission_chains = pd.read_csv('../input/sample_submission.csv')\n", "\n", "# create a function to add features\n", "def add_feature(X, feature_to_add):\n", "    '''\n", "    Returns sparse feature matrix with added feature.\n", "    feature_to_add can also be a list of features.\n", "    '''\n", "    from scipy.sparse import csr_matrix, hstack\n", "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "3cdc6de563643c7a86f0c54ecff2f720695fe81d", "_cell_guid": "20c3ff4a-8925-4c78-a8f1-74f080f0b890"}, "source": ["for label in cols_target:\n", "    print('... Processing {}'.format(label))\n", "    y = train_df[label]\n", "    # train the model using X_dtm & y\n", "    logreg.fit(X_dtm,y)\n", "    # compute the training accuracy\n", "    y_pred_X = logreg.predict(X_dtm)\n", "    print('Training Accuracy is {}'.format(accuracy_score(y,y_pred_X)))\n", "    # make predictions from test_X\n", "    test_y = logreg.predict(test_X_dtm)\n", "    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n", "    submission_chains[label] = test_y_prob\n", "    # chain current label to X_dtm\n", "    X_dtm = add_feature(X_dtm, y)\n", "    print('Shape of X_dtm is now {}'.format(X_dtm.shape))\n", "    # chain current label predictions to test_X_dtm\n", "    test_X_dtm = add_feature(test_X_dtm, test_y)\n", "    print('Shape of test_X_dtm is now {}'.format(test_X_dtm.shape))"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "a0be9c78f3facd8215c0d83c446dcef61af3fc43", "_cell_guid": "d917c9c5-5140-4b08-86b3-64e0b566bc1b"}, "source": ["### Create submission file"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "f1d85021f64b145d28a55f65e0d3e806a6c91625", "_cell_guid": "4b7db16c-ca27-4fa8-933c-369711f60ea3"}, "source": ["submission_chains.head()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "da35f9fd53f310366d0b760eb7573ae1b4e122c4", "_cell_guid": "3e1dfebe-ecb4-4a0e-958c-56a2d27e6a7d", "collapsed": true}, "source": ["# generate submission file\n", "submission_chains.to_csv('submission_chains.csv', index=False)"], "cell_type": "code", "outputs": []}, {"metadata": {"_uuid": "cbbce014f73f54e9bf3e88e096ad66b89b88fcf4", "_cell_guid": "7a012382-ad72-4c76-b53c-1f756544e23e"}, "source": ["### That's all for now. Would like to work on the last problem transformation method Label Powerset next, but right now, I can't think of how I could generate the prediction probability numbers in the format required for submission.\n", "### Tips and comments are most welcomed & appreciated.\n", "### Please upvote if you find it useful. Happy Holidays!"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "21a21a9ff3c174753c8d48495e058a1782bf7a76", "_cell_guid": "2b370cff-d22e-4393-94d5-98b4883a32d8", "collapsed": true}, "source": [], "cell_type": "code", "outputs": []}]}