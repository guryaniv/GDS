{"cells":[{"metadata":{"_uuid":"a0cfc6214ae35bb72678b6dc71fd725e0514c349"},"cell_type":"markdown","source":"# Titanic prediction - competition\n"},{"metadata":{"_uuid":"f2fd1f4111450b5eb2fe95b947343cbd7c662351"},"cell_type":"markdown","source":"## Define useful global functions\n\nCollecting global functions to be used throughout here. "},{"metadata":{"trusted":true,"_uuid":"c930a8f0aed0d9452a1cb9bf022e54d8dc68c83f"},"cell_type":"code","source":"def create_submission(test, preds_test, file_name):\n    predictions = []\n    for pred in preds_test:\n        if pred == 1:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    submission = pd.concat([test_orig['PassengerId'], pd.Series(predictions).astype(\"int\")], axis=1)\n    submission.columns = ['PassengerId', 'Survived']\n    submission['Survived'] = submission['Survived'].astype(\"int\")\n    # Not here since we do not submit it.\n    submission.to_csv(file_name, index = False)\n\n# Returns a fitted and tuned model. Will also create predictions. \ndef run_model(train, test, cv_grid_params,file_name = \"submission.csv\", regression = False, gauss_proc = False):\n    from sklearn.model_selection import cross_val_score, GridSearchCV\n    from sklearn.metrics import confusion_matrix\n    \n    X_train = train.drop(['Survived'], axis = 1)\n    y_train = train['Survived']\n    X_test = test.drop(['Survived'], axis = 1)\n    y_test = test['Survived']\n\n    grid_search = GridSearchCV(**cv_grid_params)\n    grid_search.fit(X_train,y_train)\n    \n    model = cv_grid_params['estimator']\n    \n    model.set_params(**grid_search.best_params_)\n    model.fit(X_train,y_train)\n    cv_scores = cross_val_score(model, X_train, y_train, cv = 10)\n    print(\"Cross validation scores:\" + str(cv_scores))\n    print(\"Mean score: \" + str(cv_scores.mean()))\n    test_preds = model.predict(X_test)\n    if regression == False:\n        print(\"Train confusion matrix:\" )\n        print(confusion_matrix(y_train.astype(\"int\"), model.predict(X_train).astype(\"int\")))\n        print(\"Predicting and creating submission. \")\n        create_submission(X_test, test_preds.astype(\"float\"), file_name)\n    else:\n        train_preds = model.predict(X_train)\n        \n        preds_test = []\n        preds_train = []\n        for i in range(train_preds.shape[0]):\n            if train_preds[i] < 0.5:\n                preds_train.append(0)\n            else:\n                preds_train.append(1)\n        print(\"Train confusion matrix:\" )\n        print(confusion_matrix(y_train, np.array(preds_train)))\n        for i in range(test_preds.shape[0]):\n            if test_preds[i] < 0.5:\n                preds_test.append(0)\n            else:\n                preds_test.append(1)\n        print(\"Predicting and creating ridge submission. \")\n        create_submission(X_test, preds_test, file_name)\n    \n    return test_preds, model\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e22a3a326b12f106f0acc97e7f87880b1779660"},"cell_type":"markdown","source":"## Start with processing data, feature fixing etc. "},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"ef707d031d2b5f094c062b37ccead2f7356de751"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Set seed to always be used\nseed = 123\n\ntrain_orig = pd.read_csv(\"../input/train.csv\")\ntest_orig = pd.read_csv(\"../input/test.csv\")\ndata = pd.concat([test_orig,train_orig], sort = False)\ndata = data.reset_index()\n\nclass_weights = {\n    0: (train_orig['Survived'] == 0).sum()/train_orig.shape[0],\n    1: (train_orig['Survived'] == 1).sum()/train_orig.shape[0]\n}\n\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"155f03154a631e0e2f5407a42262ba82c06354e5"},"cell_type":"markdown","source":"### Check for null values\n\nOkay, so we need to fix age, Fare, Cabin and Embarked. Survived's null values is the test set. "},{"metadata":{"trusted":true,"_uuid":"221ed92c3f8666150cc075938c3036cbb2e7561f"},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab68434c884bf4d1180146b977e07ee741ec4036"},"cell_type":"code","source":"# Fill the missing one. \ndata['Fare'] = data['Fare'].fillna(data['Fare'].dropna().median())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7247fb56b729af952cf2dbffc162d4246ece82e"},"cell_type":"markdown","source":"## Fixing Cabin department out of Cabin feature\n\nWe can get which department people lived, as well as their enumeration. The most probable is that the letter accounted for different sections, "},{"metadata":{"trusted":true,"_uuid":"655454e22c3a76a18515f86c87f3da54995f243e"},"cell_type":"code","source":"print(data['Cabin'].isnull().sum())\ndata['Cabin_dep'] = [cabin_no[0] for cabin_no in data['Cabin'].astype(\"str\")]\ndata['Cabin_dep'] = data['Cabin_dep'].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce66d7a4be01e9c36a7e8731cb644f5a301704e7"},"cell_type":"markdown","source":"## Fixing age variable\n\nAs we saw, we have many ages that are NaN. To compensate for this, we simulate the distribution and obtain new samples. Make sure to use seed.  \n\nWe sample for the training set and test set individually. For the test set, we just sample out of the distribution it has. For the training and test set, we sample out of densities for each class. \n\nThe test distribution is, as seen, similar to the ones who died, which *might* indicate that we will rather have more people dying in the test set. "},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4770637d9e1e7e12609c991518f50f89341cf7d6"},"cell_type":"code","source":"from matplotlib.pyplot import hist\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import gaussian_kde\nhist(data['Age'].dropna(), density = True)\nfrom scipy.stats import poisson\n\ndens0 = gaussian_kde(data.Age[data['Survived'] == 0].dropna())\ndens1 = gaussian_kde(data.Age[data['Survived'] == 1].dropna())\ndensNA = gaussian_kde(data.Age[data['Survived'].isna()].dropna())\n\nx0 = np.arange(0,data.Age[data['Survived'] == 0].dropna().max())\nx1 = np.arange(0,data.Age[data['Survived'] == 1].dropna().max())\nxNA = np.arange(0,data.Age[data['Survived'].isna()].dropna().max())\n\nplt.plot(x0, dens0.evaluate(x0), 'r')\nplt.plot(x1, dens1.evaluate(x1), 'g')\nplt.plot(xNA, densNA.evaluate(xNA), 'b')\nplt.legend([\"Died\", \"Survived\", \"Test dist\"])\nplt.title(\"Density slightly different, but not very. Still worth to separate sampling though. \")\n\ndist0 = dens0.evaluate(x0)\n# Normalize\ndist0 = np.divide(dist0,np.sum(dist0))\ndist0 = dens0.evaluate(x0)\n# Normalize\ndist0 = np.divide(dist0,np.sum(dist0))\n\ndist1 = dens1.evaluate(x1)\n# Normalize\ndist1 = np.divide(dist1,np.sum(dist1))\ndist1 = dens1.evaluate(x1)\n# Normalize\ndist1 = np.divide(dist1,np.sum(dist1))\n\ndistNA = densNA.evaluate(xNA)\n# Normalize\ndistNA = np.divide(distNA,np.sum(distNA))\ndistNA = dens1.evaluate(xNA)\n# Normalize\ndistNA = np.divide(distNA,np.sum(distNA))\n\n\n# We should sample out of this distribution to compensate. \nnp.random.seed(seed)\nnan_ages0 = np.random.choice(x0, p = dist0, size = data['Age'].isnull().sum())\nnan_ages1 = np.random.choice(x1, p = dist1, size = data['Age'].isnull().sum())\nnan_agesNA = np.random.choice(xNA, p = distNA, size = data['Age'].isnull().sum())\n\ncount = 0\nfor i in list(data.Age[data['Survived'] == 0].index[np.where(data.Age[data['Survived'] == 0].isna())]):\n    data['Age'][i] = nan_ages0[count]\n    count += 1\n        \ncount = 0\nfor i in list(data.Age[data['Survived'] == 1].index[np.where(data.Age[data['Survived'] == 1].isna())]):\n    data['Age'][i] = nan_ages1[count]\n    count += 1\n\ncount = 0\nfor i in list(data.Age[data['Survived'].isna()].index[np.where(data.Age[data['Survived'].isna()].isna())]):\n    data['Age'][i] = nan_agesNA[count]\n    count += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bda7b7c3eef3be85d0c8e3aa39c6d1a394cdbc6a"},"cell_type":"markdown","source":"## Remove outliers \n\nFirst of all, I discovered while doing the things below, that there are many outliers in the dataset. To do this, I use the Z-score as in [this article from Towards Data Science](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba). This is only done on the numerical features. \n\nA critique towards this is that we assume a normal distribution of these, which might be far from the case. However, we'll do it, and see if it works well. "},{"metadata":{"trusted":true,"_uuid":"680ba94f21f0fbabd8d32406ef69f61004a00bd6"},"cell_type":"code","source":"from scipy.stats import zscore\n\nindices_age = data.Age[data['Age'].isnull() == False].index\nz_scores_age = np.abs(zscore(data.Age[data['Age'].isnull() == False]))\n\n\nz_scores = pd.DataFrame(columns=['Age','SibSp','Parch','Fare'])\nz_scores['SibSp'] = zscore(data.SibSp)\nz_scores['Parch'] = zscore(data.Parch)\nz_scores['Fare'] = zscore(data.Fare)\nz_scores['Age'] = zscore(data.Age)\n    \noutlier_rows, outlier_cols = np.where(z_scores > 3)\n\noutliers = {}\n\nfor i,col in enumerate(outlier_rows):\n    if outlier_rows[i] not in outliers:\n        outliers[outlier_rows[i]] = [outlier_cols[i]]\n    else:\n        outliers[outlier_rows[i]].append(outlier_cols[i])\n\n        \n# Remove all being outlier in 2 or more columns\nto_del = []\nfor outlier in outliers:\n    if len(outliers[outlier]) >= 2 and outlier >= test_orig.shape[0]:\n        to_del.append(outlier)\n\nprint(len(to_del))\n# Now delete all in list to_del\nfor i in to_del:\n    data.drop(data.index[i], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f28c52724551c1faaeb7dab08d2fb83ad4482c3"},"cell_type":"markdown","source":"Okay, we only removed 1. Hopefully this will help the model. "},{"metadata":{"trusted":true,"_uuid":"23f37a01843d4b4909f32e9fa15ebde491ff6b96"},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9953990985a77496c1157e310acfc7504e2cd813"},"cell_type":"markdown","source":"### Sex variable\n\nThe sex feature does not have any null values. However, we clearly see that men died to a much larger extent than women. Women clearly seem to have been prioritised. "},{"metadata":{"trusted":true,"_uuid":"0b9dafb418a544b3643054876c7b2a6d49e3d177"},"cell_type":"code","source":"data['Sex'].isnull().sum()\ndata['Sex'] = data['Sex'].astype('str')\ndata['Sex'] = data['Sex'].replace({'male':1, 'female':-1})\nx=np.arange(1,4,2)\nplt.bar(x, data.Sex[data['Survived'] == 1].value_counts().sort_index(), width = 0.5)\nplt.title(\"Gender distributions of people dying and surviving\")\nplt.xticks(x - 0.25,[\"Female\", \"Male\"])\nplt.bar(x-0.5, data.Sex[data['Survived'] == 0].value_counts().sort_index(), width = 0.5, color = \"r\")\nplt.legend([\"Survived\",\"Died\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5380df18130fcfd4ce7bc2377ab0b03c734a069"},"cell_type":"markdown","source":"### Fare variable\n\nWhat can be seen with the fare variable? \n\nThere is one missing, in the test set. Just replace this one with the mean to compensate for it. "},{"metadata":{"trusted":true,"_uuid":"5f682ee83cb6770ea82893c7c9b61b9649248b8e"},"cell_type":"code","source":"plt.plot(data.Fare[data['Survived'] == 1], 'ro')\nplt.plot(data.Fare[data['Survived'] == 0], 'bo')\nplt.legend([\"Survived\", \"Died\"])\nprint(\"Survived: Mean: \"+str(np.mean(data.Fare[data['Survived'] == 1]))+ \", sd: \" + str(np.std(data.Fare[data['Survived'] == 1])))\nprint(\"Died: Mean: \"+str(np.mean(data.Fare[data['Survived'] == 0]))+ \", sd: \" + str(np.std(data.Fare[data['Survived'] == 0])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f3feaf298de7cc80886c6dac8fe0dee67da8740"},"cell_type":"markdown","source":"### Parch feature\n\nThe parch feature apparently indicates how many parents and children aboard the passenger in question had. This variable might give a good indication - a person with no family probably did not have to collect any family members. It is also quite probable that big, poor families were travelling. "},{"metadata":{"trusted":true,"_uuid":"5abe8a1ebbaabed72ec45c38c5f20078faab7b9c"},"cell_type":"code","source":"x = np.arange(0,14,2)\nplt.bar(x - 0.25, data.Parch[data['Survived'] == 0].value_counts().sort_index(), width = 0.5)\nplt.bar(2*data.Parch[data['Survived'] == 1].value_counts().sort_index().index + 0.25, data.Parch[data['Survived'] == 1].value_counts().sort_index(), color = 'r', width = 0.5)\nplt.xticks(x,np.arange(0,7,1))\nplt.legend(['Survived', 'Died'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e02aa965db80b45388498f8dcc8cdbe3a8a5c67"},"cell_type":"markdown","source":"Let's reduce the variance on this, and let 3 be \"3+\", as these indicate outliers. Also, we only have 2 that have the value 6, which is weird. This probably gives a better, more general model input. "},{"metadata":{"trusted":true,"_uuid":"0467095b1997c447991c673bda0027723f8050fd"},"cell_type":"code","source":"data.loc[data['Parch'] >= 3, 'Parch'] = 3\nprint(data.Parch.value_counts().sort_index())\nx = np.arange(0,8,2)\nplt.bar(x - 0.25, data.Parch[data['Survived'] == 0].value_counts().sort_index(), width = 0.5, color = \"r\")\nplt.bar(2*data.Parch[data['Survived'] == 1].value_counts().sort_index().index + 0.25, data.Parch[data['Survived'] == 1].value_counts().sort_index(), color = 'b', width = 0.5)\nplt.xticks(x,np.arange(0,7,1))\nplt.legend(['Died', 'Survived'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae508f2f86320950e930bb816f804892f7e5cc2b"},"cell_type":"markdown","source":"### SibSp feature\n\nThe SibSp indicates how many siblings and spouses the person had onboard. Again, this probably indicates whether they had to go back for someone or not. \n\n"},{"metadata":{"trusted":true,"_uuid":"432337f4187b0fc3c290f2f528cc54ca022fc02c"},"cell_type":"code","source":"x = np.array([0,1,2,3,4,5,8])\n\nplt.bar(np.array([0,2,4,6,8]) - 0.25,data.SibSp[data['Survived'] == 1].value_counts().sort_index(), color=\"b\", width=0.5)\nplt.bar(2*np.array([0,1,2,3,4,5,8]) + 0.25,data.SibSp[data['Survived'] == 0].value_counts().sort_index(), color = \"r\", width=0.5)\nplt.legend([\"Survived\", \"Died\"])\nplt.xticks(np.arange(0,18,2),np.arange(0,9))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a485af24bd700fbadaf2599ce638b0f2c933361b"},"cell_type":"markdown","source":"We have clear outliers. I think we will do 4+ on this one. "},{"metadata":{"trusted":true,"_uuid":"69aff44b255d59be7b13450c0dea4632f5617cf3"},"cell_type":"code","source":"data.loc[data['SibSp'] >= 4, 'SibSp'] = 4\n\nx = np.arange(0,10,2)\n\nplt.bar(x - 0.25,data.SibSp[data['Survived'] == 1].value_counts().sort_index(),width=0.5, color=\"b\")\nplt.bar(x + 0.25,data.SibSp[data['Survived'] == 0].value_counts().sort_index(),width=0.5, color=\"r\")\nplt.legend(['Survived', 'Died'])\nplt.xticks(x,x/2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d76b5514e98d6cf8dba1696922884942b79c9ca"},"cell_type":"markdown","source":"### Name feature\n\nThe name feature is interesting, from which we can draw some other features perhaps. "},{"metadata":{"trusted":true,"_uuid":"4ab38477fd5ed7f237816603e7badbea6a9f55ec"},"cell_type":"code","source":"data.Name.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ff702c9babefc0dc2d2504aaef5373f6a55d824"},"cell_type":"markdown","source":"It seems like many has titles in their names. "},{"metadata":{"_uuid":"f01a2dded86efe2b75dd076510f6396e257d367a"},"cell_type":"markdown","source":"#### Create Title feature\n\nFirst of all, everyone seems to have a title \"Mr.\", \"Mrs.\", \"Master.\" or \"Miss.\". Thus, we create a new one with their title. "},{"metadata":{"trusted":true,"_uuid":"c331b3aad510b3e3f0aad585b816d343367c4813"},"cell_type":"code","source":"#print(['yes' for x in [x.lower() for x in list(data['Name'].values)] if 'master' in x])\ndef create_title(name_col):\n    import re\n    for name in name_col:\n        matches = re.findall(\"\\w+\\.\",name)\n        yield matches[0]\n\ndata['Title'] = list(create_title(data['Name']))\n#print(data['Title'].value_counts())\ndata['Title'] = data['Title'].replace({'Mlle.':'Miss.','Ms.':'Miss.','Jonkheer.':\"Mr.\",\"Major.\":\"Mr.\",\"Countess.\":\"Mrs.\",\"Don.\":\"Mr.\", \"Dona.\":\"Mrs.\",\"Sir.\":\"Mr.\",\"Mme.\":\"Miss.\", \"Col.\":\"Military\", \"Major.\":\"Military\",\"Capt.\":\"Military\", \"Lady.\":\"Mrs.\"})\nprint(data['Title'].value_counts())\n\nx = np.arange(0,14,2)\n\nplt.bar(x - 0.25, data.Title[data['Survived'] == 0].value_counts().sort_index(), color = \"r\", width = 0.5)\nplt.bar(x[:6] + 0.25, data.Title[data['Survived'] == 1].value_counts().sort_index(), color = \"b\", width = 0.5)\nplt.xticks(x,[\"Dr.\", \"Master.\", \"Military\",\"Miss.\",\"Mr.\",\"Mrs.\",\"Revenant\"])\nplt.legend([\"Died\",\"Survived\"])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38a1842f01c16152604e29a8700ef10c8726cb6d"},"cell_type":"markdown","source":"Interesting is that *all* the revenants died. **Military** is a collection of military title. I did find many noble title, and that might have been interesting to separate. "},{"metadata":{"_uuid":"81f13bf4c597e140525fa5830b293024756c0f50"},"cell_type":"markdown","source":"## Ticket feature\nLet us look at the ticket feature. \n\nTickets seem to be a number or with letters and a number, indicating some sort of special ticket. Let's categorize them into into \"Numbered\" and their letters. Since we have so many different types, I will just base them based on the first letter. "},{"metadata":{"trusted":true,"_uuid":"8a05eabea086f34eecb70b4cd17e418b936003f7"},"cell_type":"code","source":"data.Ticket.head(20)\ndata.Ticket.loc[20]\n\nticket_type = []\n\nimport re\n\n#print(data.Ticket)\nfor ticket in data.Ticket.values:\n    if ticket.isdigit():\n        ticket_type.append(\"Numbered\")\n    else:\n        ticket_type.append([re.sub('[^a-zA-Z_]','', x)[0:1] for x in ticket.split(\" \")][0])\n        \ndata['Ticket_type'] = ticket_type\nx = np.arange(0,16,2)\nplt.bar(x - 0.25,data.Ticket_type[data['Survived'] == 0].value_counts().sort_index(), color = \"r\", width = 0.5)\nplt.bar(x + 0.25, data.Ticket_type[data['Survived'] == 1].value_counts().sort_index(), color = \"b\", width = 0.5)\nplt.legend(['Died', 'Survived'])\nplt.title(\"Ticket type dying vs \")\nplt.xticks(x,data.Ticket_type[data['Survived'] == 0].value_counts().sort_index().index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa8cce8a379596804e9d18bb13d212c62b419b85"},"cell_type":"markdown","source":"There actually does seem to be some kind of useful information here. P seems to have survived to larger extent. However, the other seem to, unfortunately, follow the same distribution as the whole training set more or less. "},{"metadata":{"_uuid":"1a236196484a48578e690c88d4bbc13b32daa19e"},"cell_type":"markdown","source":"### Fix dummy variables for categories that we that"},{"metadata":{"trusted":true,"_uuid":"5207f6de2d77f51b64b4333a949ecd7dc87e4b90"},"cell_type":"code","source":"data = pd.get_dummies(data.drop(['Cabin','Ticket','Name'], axis = 1))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"231f95d998b1cc09ac0e629f3760435960380a66"},"cell_type":"markdown","source":"### Split into training and test set\n\nWe should also upsample to balance the class weights. This does however make the training set's confusion matrix not necessarily representative. \n\nIt can be done with upsampling, which was tried. However, I will use the class weights for those models where this is available instead, to induce a loss function. "},{"metadata":{"trusted":true,"_uuid":"87b1fa0bbbc47ac8e93370a37d3c43d92bbf8613"},"cell_type":"code","source":"train, test = [x for _, x in data.groupby(data['Survived'].isnull())]\ntrain = train.drop(['index', 'PassengerId'], axis = 1)\ntest = test.drop(['index', 'PassengerId'], axis = 1)\n\"\"\"\n# The upsampling, which later was chosen not to use. \nn_dying = train[train['Survived'] == 0].shape[0]\nn_surviving = train[train['Survived'] == 1].shape[0]\nn_to_sample = n_dying - n_surviving\nprint(\"Resampling \"+str(n_to_sample)+\" samples from training set. \")\n\nresamples = train[train['Survived'] == 1].sample(n_to_sample, axis = 0)\n\ntrain = pd.concat([train, resamples])\"\"\"\n\ntrain.Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29710be70ef39811ea667c7a1f836c655b32ee63"},"cell_type":"markdown","source":"## Data processing done - time for modelling\n\nFirst off, we simply try a ridge regression with an rbf kernel and see its performance. This is kind of unchristly as it is actually regression, but it might actually perform well. "},{"metadata":{"trusted":true,"_uuid":"d430316aac062eeff55bc1302d0272f361ec6a3b"},"cell_type":"code","source":"X_train = train.drop(['Survived'], axis = 1)\ny_train = train['Survived']\nX_test = test.drop(['Survived'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eba51e89df2d41a63d25c7670109a75f1ad1b94b"},"cell_type":"markdown","source":"## Ridge kernel regression\n\nRidge kernel regression might give a nice result. Although it is not classification, this has interestingly worked well before. Important to note that the cross-validation scores here does not reflect the accuracy of the estimates. "},{"metadata":{"trusted":true,"_uuid":"d39da34b9cb5fa4e8fa9bd3803165a1105680dc4"},"cell_type":"code","source":"from sklearn.kernel_ridge import KernelRidge\n\nkern_ridge = KernelRidge()\n\nparams = {'alpha': [0.01,0.1, 1.0],\n 'coef0': [0,0.1,1],\n 'degree': [1,2,3],\n 'gamma': [0.01,1,10],\n 'kernel': ['rbf']\n}\n\ngrid_params = {\n    'estimator':kern_ridge,\n    'param_grid':params,\n    'n_jobs':5,\n    'iid':False,\n    'verbose':True,\n    'scoring':'neg_mean_squared_error',\n    'cv':10\n}\n\ny_preds_ridge, ridge_model = run_model(train, test, grid_params, file_name = \"submission_ridge.csv\", regression = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60f94944f94c5f88a91bb0311753ad1e61faaa88"},"cell_type":"markdown","source":"## Logistic regression\n\nNow lets try logistic regression. "},{"metadata":{"trusted":true,"_uuid":"c9089270d1e265097c8199b9e8143f2fa11cb839"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg_model = LogisticRegression()\n\nlog_reg_params = {'C': [0.1,1.0,10,100], \n                  'class_weight': [None], \n                  'dual': [None], \n                  'fit_intercept': [True,False], \n                  'max_iter': [10000], \n                  'multi_class': ['ovr'],\n                  'class_weight':[class_weights],\n                  'n_jobs': [1],\n                  'penalty': ['l1','l2'],\n                  'random_state': [123],  \n                  'tol': [0.0001, 0.0005,0.001], \n                  'solver':['saga'],\n                  'warm_start': [False]}\ngrid_params['estimator'] = log_reg_model\ngrid_params['param_grid'] = log_reg_params\ngrid_params['scoring'] = 'accuracy'\n\ntest_preds_log_reg, log_reg_model = run_model(train, test, grid_params, file_name = \"submission_log_reg.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1193cd1cfe12676073b2b4ff45972fe36b7a1675"},"cell_type":"markdown","source":"So Logistic regression performed worse. Let's choose the Ridge Regression for the classification. "},{"metadata":{"_uuid":"528191b1414fabc00e8b39a3e40f51da1da0a9e0"},"cell_type":"markdown","source":"### XGBoost \n\nLet's try XGBoost instead. The score, without tuning hyperparameters or any cross-validation, resulted in 0.77, quite a good score. \n\n#### Tuning the parameters of XGBoost\n\nLet's tune the parameters for XGBoost and see how well it performs. "},{"metadata":{"trusted":true,"_uuid":"5af016b84c587e09a0bee8084720effb498b10a8","scrolled":false},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb_model = XGBClassifier()\n\nparams_xgb = {\n    'base_score':[0.3,0.5],\n    'colsample_bytree':[0.4,0.7],\n    'gamma':[0.01,0.5,0.9],\n    'min_child_weight':[1,3],\n    'learning_rate':[0.01,0.1,1],\n    'max_depth':[3,4,5],\n    'n_estimators':[500],\n    'reg_alpha':[1e-5, 0.1],\n    'reg_lambda':[1e-5, 0.1],\n    'subsample':[0.8]\n}\n\ngrid_params['estimator'] = xgb_model\ngrid_params['param_grid'] = params_xgb\ngrid_params['n_jobs'] = 5\n\ntest_preds_xgb, xgb_model = run_model(train, test, grid_params, file_name = \"submission_xgb.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ff7d1676f0ef4509a5606a6190c7a5b6bcd5836"},"cell_type":"markdown","source":"Let's use the best model to predict and submit it. Tuning as above led to an improvement of 2 %. Not much, but still something. "},{"metadata":{"_uuid":"eda912b386831b60aa0263ad868cb5986ad66195"},"cell_type":"markdown","source":"## Gaussian process\n\nLet's try gaussian processes instead. We use the same features as previously, i.e. X_train, y_train and X_test. However, we need to scale the numerical variables. \n\nAlso, in gaussian processes, output $y$ is assumed to have mean 0. Since we have a balanced dataset through upsampling, "},{"metadata":{"trusted":true,"_uuid":"cb1954e3f53346a8764f594838f27b15a7180cf2"},"cell_type":"code","source":"from sklearn.preprocessing import scale\n# Scale the data \ntest_gp = test.copy()\ntrain_gp = train.copy()\ndata_gp = pd.concat([test_gp,train_gp])\ndata_gp[['Age','Fare','SibSp','Parch']] = scale(data_gp[['Age','Fare','SibSp','Parch']])\n\n\nprint(data_gp.head())\n\ndata_gp['Survived'].replace({1:1, 0:-1})\n\nfrom sklearn.preprocessing import scale\n# Time to scale the numerical variables!\ntrain_gp, test_gp = [x for _, x in data_gp.groupby(data_gp['Survived'].isnull())]\n\nprint(train_gp.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4a8d7028c41ab5c538e530b3f68d0a94c373a24"},"cell_type":"code","source":"from sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nparams = {'copy_X_train': [True],\n 'kernel': [RBF(1.0),RBF(0.01),RBF(0.001),RBF(10), RBF(5), RBF(3), RBF(2)],\n 'max_iter_predict': [100],\n 'multi_class': ['one_vs_rest'],\n 'n_jobs': [3],\n 'n_restarts_optimizer': [0],\n 'optimizer': ['fmin_l_bfgs_b'],\n 'random_state': [123],\n 'warm_start': [False]\n         }\n\ngrid_params['estimator'] = GaussianProcessClassifier()\ngrid_params['param_grid'] = params\n\ntest_preds_gauss, gp_model = run_model(train_gp, test_gp, grid_params, file_name = \"submission_gauss_process.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c76901799e0411f2c9a9c62271673679aad38f34"},"cell_type":"markdown","source":"This actually yielded some pretty good results; this made me pass above the 0.8 line on the test set. "},{"metadata":{"_uuid":"3d6ffac78545928e31431e3de02ec0f6b917259f"},"cell_type":"markdown","source":"### SVM classifier \n\nLet's use an SVM aswell to classify, with *scaled* numerical features. "},{"metadata":{"trusted":true,"_uuid":"93f2e4a2ac72c5a87f25c8232f471d7da2506b38"},"cell_type":"code","source":"from sklearn.svm import SVC\n\nparams = {'C': [0.01,0.1,0.5,1.0,10,50],\n 'class_weight': [class_weights],\n 'coef0': [0.0,0.1,0.5,1],\n 'decision_function_shape': ['ovr'],\n 'degree': [2,3],\n 'gamma': ['auto_deprecated'],\n 'kernel': ['rbf','poly'],\n 'max_iter': [-1],\n 'probability': [False],\n 'random_state': [123],\n 'shrinking': [True],\n 'tol': [0.001,0.003,0.005,0.01]}\n\ngrid_params['estimator'] = SVC()\ngrid_params['param_grid'] = params\n\ntest_preds_svm, svm_model = run_model(train_gp, test_gp, grid_params, file_name = \"submission_gauss_process.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56e57d5cdb3b9c48240162fe8dac3c7008526d73"},"cell_type":"markdown","source":"The SVM yielded a test score result of about 0.78468, in other words, not too shabby. "},{"metadata":{"_uuid":"e8f30690e39e50933d01975ba1f4d51696c62d19"},"cell_type":"markdown","source":"### Random Forest\n\nLet us try a random forest"},{"metadata":{"trusted":true,"_uuid":"0c1dbc3e84b7f7bba6b2a8f8f1776c90242ccc59"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier()\n\nparams_rf = {\n     'bootstrap': [True,False],\n     'class_weight': [None],\n     'criterion': ['gini','entropy'],\n     'max_depth': [None],\n     'max_features': ['auto'],\n     'max_leaf_nodes': [None],\n     'min_impurity_decrease': [0.0,0.01,0.1,1],\n     'min_impurity_split': [None],\n     'min_samples_leaf': [1,5,10,15],\n     'min_samples_split': [2,5,10],\n     'min_weight_fraction_leaf': [0.0],\n     'n_estimators': [5,10,20,100],\n     'n_jobs': [None],\n     'oob_score': [False],\n     'random_state': [123]\n            }\n\ngrid_params['estimator'] = rf_model\ngrid_params['param_grid'] = params_rf\n\ntest_preds_rf, rf_model = run_model(train, test, grid_params, file_name = \"submission_rf.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"143ebbc188b9f01af256cdf155b2f866848ed8fb"},"cell_type":"markdown","source":"Hmm, this one yielded pretty good test results of about 0.78. "},{"metadata":{"_uuid":"02119fa99d6c75893d4f6d709b3e7e51557f8c86"},"cell_type":"markdown","source":"## Okay, let's go for an ensemble model\n\nSome of the models has worked quite well, but none has been perfect. Let's combine them into an ensemble model and see how good that one will be. \n\nI will not combine ridge regression, as it require modifications of the data for a result. However, I will combine the others. Logistic regression is probably not very good though, so I might exclude that one. \n\nI will also not include logistic regression due to its poor results previously. \n\nAlso noteworthy is that the scaled data will be used. Hopefully, it will not hurt the accuracy of the models that did not use scaled data for their best results. \n\nHowever, let's first check out how the predictions of the test set correlates. "},{"metadata":{"trusted":true,"_uuid":"521828c1e82f2d60b25a05956f48611e8fec973b"},"cell_type":"code","source":"preds = pd.concat([pd.Series(test_preds_rf),pd.Series(test_preds_gauss),pd.Series(test_preds_xgb),pd.Series(test_preds_svm)], axis = 1)\n\npreds.columns = ['Random Forest', 'Gaussian Process', 'XGBoost', 'SVM']\npreds.corr()\n\nimport seaborn as sns\n\nsns.heatmap(preds.corr(), xticklabels=preds.columns, yticklabels=preds.columns, annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5988261cc1e10338df4dc7b60329b67e6224d777"},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nsvm_model.set_params(probability = True)\nvc_model = VotingClassifier([('rf',rf_model),('xgb',xgb_model),('gp',gp_model),('svm',svm_model)], voting = 'soft')\n\nX_train = train_gp.drop(['Survived'], axis = 1)\ny_train = train_gp['Survived']\nX_test = test_gp.drop(['Survived'],axis = 1)\n\nvc_model.fit(X_train, y_train)\n\npreds_train = vc_model.predict(X_train)\n\nfrom sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_train, preds_train))\ny_preds_test = vc_model.predict(X_test)\n\ncreate_submission(test_gp,y_preds_test, \"submission_vc_soft.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78823dbe550c6532cb21f8fb2e8647c078bc8f92"},"cell_type":"markdown","source":"## Critique, questions and problems \n\nIn this work, I did some feature analysis, feature engineering and tried a few different models. \n\n- I do introduce a bias by drawing from the classes' respective distributions in age. This might be questionable, and other methods might be more appropriate. For example, just filling in with the median, but I don't think that is the way to go either. \n- Upsampling is, to be honest, probably not completely needed here. The data set is already *quite* balanced, and introducing the bias through upsampling of people surviving might actually hurt the results. I did it first, but decided not to, which actually improved my scores. \n- Feature selection needs to be improved. Currently, I have 36 columns. While this is still ~20 times less than the number of training samples, many of these matrices are sparse. \n- How I choose my parameters for my gridsearch for the models can still be improved. I would love to receive feedback on the grid input parameters for the models. \n- Although I did remove some outliers, I am not sure as to how much these actually affect the models that I specifically chose to implement, apart from logistic regression, which is sensitive to outliers. \n\nSo far, the RandomForest, the Gaussian Process and XGBoost have yielded the best scores. I think there might be some feature engineering left to do. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}