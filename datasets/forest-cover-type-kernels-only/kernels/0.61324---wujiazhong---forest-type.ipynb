{"cells":[{"metadata":{"trusted":true,"_uuid":"a5806b16aab5679758987003e0027bc4e09c9af9"},"cell_type":"code","source":"import pandas as pd\nimport sklearn as sk\nimport numpy as np\n\nprint(pd.__version__)\nprint(sk.__version__)\n\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1283837e99c277f37c3571933cc341c96ab8930d","scrolled":true},"cell_type":"code","source":"soil_type_cols = []\nwilderness_area_cols = []\nfor col in train_data.columns:\n    if 'Soil_Type' in col:\n        soil_type_cols.append(col)\n    elif 'Wilderness_Area' in col:\n        wilderness_area_cols.append(col)\nprint(soil_type_cols)\nprint(wilderness_area_cols)\ntrain_data_cate = train_data.loc[:, soil_type_cols + wilderness_area_cols]\ntest_data_cate = test_data.loc[:, soil_type_cols + wilderness_area_cols]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"be1c58487530802f9806414a47a8719bf746abcd"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_data_num = train_data.drop(['Id', 'Cover_Type'] + soil_type_cols + wilderness_area_cols, axis=1)\ntest_data_num = test_data.drop(['Id'] + soil_type_cols + wilderness_area_cols, axis=1)\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X[self.attribute_names]\n\nnum_pipeline = Pipeline([\n    ('scaler', StandardScaler())\n])\ntrain_data_num.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1280538175c2eda75452816df3333d7151338cc"},"cell_type":"code","source":"train_data_num_tr = pd.DataFrame(num_pipeline.fit_transform(train_data_num))\ntrain_data_num_tr.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d605bfc218111f495194a3ce00fa70dae2c3315"},"cell_type":"code","source":"test_data_num_tr = pd.DataFrame(num_pipeline.fit_transform(test_data_num))\ntest_data_num_tr.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd88e5c853809b23d6c5b3502f3d4e3da5b0e016"},"cell_type":"code","source":"train_data_tr = train_data_num_tr.join(train_data_cate)\ntrain_data_tr.describe()\nX_data = train_data_tr.values\ny_data = train_data['Cover_Type'].values\n\ntest_data_tr = test_data_num_tr.join(test_data_cate)\ntest_data_tr.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca64e6f14f919caad139a418a09236f2b1cdfefd"},"cell_type":"code","source":"from xgboost import XGBClassifier\nimport os\n\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n# Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, gaussian_process, discriminant_analysis\n\nMLA = [\n    # Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    # Gaussian Processes\n#     gaussian_process.GaussianProcessClassifier(),  Failed if features are over 10 or more\n\n    # GLM\n#     linear_model.PassiveAggressiveClassifier(),\n#     linear_model.RidgeClassifierCV(),\n#     linear_model.SGDClassifier(),\n#     linear_model.Perceptron(),\n\n    # Nearest Neighbor\n#     neighbors.KNeighborsClassifier(),\n\n    # Trees\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n\n\n    # xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"327b09c61e426e1746a09a001982b8c3d47b81ef"},"cell_type":"code","source":"from sklearn import model_selection\nimport seaborn as sns\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = train_data['Cover_Type']\n\n# a = ensemble.ExtraTreesClassifier()\n# a.fit(X_train, y_train)\n# print(a.score(X_test, y_test))\nrow_index = 0\nfor alg in MLA:\n#     fit = alg.fit(X_train, y_train)\n#     predicted = fit.predict(X_test)\n    # fp, tp, th = roc_curve(y_test, predicted)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    print(MLA_name)\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, X_data, y_data, cv=cv_split, return_train_score=True)\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    print(MLA_compare.loc[row_index, 'MLA Time'])\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    print(\"MLA Train Accuracy Mean\", MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'])\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()\n    print(\"MLA Test Accuracy Mean\", MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'])\n    # if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std() * 3  \n    # let's know the worst that can happen!\n\n    # save MLA predictions - see section 6 for usage\n    alg.fit(X_data, y_data)\n    MLA_predict[MLA_name] = alg.predict(X_data)\n\n    row_index += 1\npd.set_option('display.max_columns', None)\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"810647e38b04181c06b186f33879f6450a6503df"},"cell_type":"code","source":"# This is an example how to use random search on GradientBoostingClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n\n#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\ngrid_n_estimator = [50, 100]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\nprint(X_data.shape[1])\nprint(y_data.shape[0])\nprint(len(test_data_tr.columns))\nvote_test = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n    \n#     xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n   ('xgb', XGBClassifier())\n\n]\n\nvote_param = [{\n    #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n    'etc__n_estimators': grid_n_estimator,\n    'etc__criterion': grid_criterion,\n    'etc__max_depth': grid_max_depth,\n    'etc__random_state': grid_seed,\n\n\n    #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n    'gbc__loss': ['deviance', 'exponential'],\n    'gbc__learning_rate': grid_ratio,\n    'gbc__n_estimators': grid_n_estimator,\n    'gbc__criterion': ['friedman_mse', 'mse', 'mae'],\n    'gbc__max_depth': grid_max_depth,\n    'gbc__min_samples_split': grid_min_samples,\n    'gbc__min_samples_leaf': grid_min_samples,      \n    'gbc__random_state': grid_seed,\n\n    #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n    'rfc__n_estimators': grid_n_estimator,\n    'rfc__criterion': grid_criterion,\n    'rfc__max_depth': grid_max_depth,\n    'rfc__min_samples_split': grid_min_samples,\n    'rfc__min_samples_leaf': grid_min_samples,   \n    'rfc__bootstrap': grid_bool,\n    'rfc__oob_score': grid_bool, \n    'rfc__random_state': grid_seed,\n\n    #http://xgboost.readthedocs.io/en/latest/parameter.html\n    'xgb__learning_rate': grid_ratio,\n    'xgb__max_depth': [2,4,6,8,10],\n    'xgb__tree_method': ['exact', 'approx', 'hist'],\n    'xgb__objective': ['reg:linear', 'reg:logistic', 'binary:logistic'],\n    'xgb__seed': grid_seed    \n\n}]\n\ngrid_param = [\n    [{\n        #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n        'n_estimators': grid_n_estimator, #default=10\n        'criterion': grid_criterion, #default=”gini”\n        'max_depth': grid_max_depth, #default=None\n        'random_state': grid_seed\n     }],\n    [{\n        #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n#         'loss': ['deviance', 'exponential'], #default=’deviance’\n        'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n        'n_estimators': [100], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n#         'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n        'max_depth': grid_max_depth, #default=3   \n        'random_state': grid_seed\n    }],\n    [{\n        #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n        'n_estimators': grid_n_estimator, #default=10\n        'criterion': grid_criterion, #default=”gini”\n        'max_depth': grid_max_depth, #default=None\n        'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n        'random_state': grid_seed\n     }],\n    [{\n        #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n        'learning_rate': grid_learn, #default: .3\n        'max_depth': [1,2,4,6,8,10], #default 2\n        'n_estimators': grid_n_estimator, \n        'seed': grid_seed  \n     }]\n]\n\nimport time\nstart_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\nfor clf, param in zip(vote_test, grid_param): #https://docs.python.org/3/library/functions.html#zip\n\n    #print(clf[1]) #vote_test is a list of tuples, index 0 is the name and index 1 is the algorithm\n    #print(param)\n    \n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split)\n    best_search.fit(X_data, y_data)\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75d795905c2c43997d816ba1cea52a2d3f6869b6"},"cell_type":"code","source":"#Hard Vote or majority rules w/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_test , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X_data, y_data, cv  = cv_split)\ngrid_hard.fit(X_data, y_data)\nprint(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\npred_type = grid_hard.predict(test_data_tr.values)\noutput_data = np.column_stack((test_data['Id'].values, pred_type))\nnp.savetxt(\"submit.csv\", output_data, fmt='%i', delimiter=\",\", header='Id,Cover_Type', comments='')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}