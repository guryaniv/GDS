{"cells":[{"metadata":{"_cell_guid":"e7243711-c5fd-47cd-8f5f-4c141e761654","_execution_state":"idle","_uuid":"db8358ab1beb4c0ab8246a5b8f675829cbd83e2d"},"cell_type":"markdown","source":"This notebook provides a baseline to compare ML models with.\n------------------------------------------------------------\n\nA null submission (predicting a logerror of 0.0 for all properties) gives a public leaderboard score of 0.0663010. \n\nThe average of the logerror in the training data is 0.0114572195563: So one can trivially improve this score by predicting 0.0114572195563 for every property. This gives a public leaderboard score of 0.0651279.\n\nThis notebook goes one step further and uses the mean logerror for every \"landusetypeID\" (commercial property, single family residential, empty lot, etc...), for which we have a statistically significant sample, as prediction. The resulting public leaderboard score is 0.0651405. **This is slightly worse than using the overall average logerror** So zillow's algorithm doesn't seem to have a constant bias between the landusetypeID logerrors. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"12c9430c-4506-4075-9423-d4a822aaa8b2","collapsed":true,"_execution_state":"idle","_uuid":"7f24998aefacd3a8867d3aa7a0ad786681eb2bb8","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\n\n\n##### READ IN RAW DATA\ndef read_data():\n    # see https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options\n    # for the dtype option and why it is important\n    print( \"Reading data from disk ...\")\n    dtype={'parcelid': np.int32, 'logerror': np.float32, 'transactiondate': str, 'transaction_month': np.int32, 'airconditioningtypeid': np.float32, 'architecturalstyletypeid': np.float32, 'basementsqft': np.float32, 'bathroomcnt': np.float32, 'bedroomcnt': np.float32,  'buildingclasstypeid': np.float32, 'buildingqualitytypeid': np.float32,  'calculatedbathnbr': np.float32, 'decktypeid': np.float32, 'finishedfloor1squarefeet': np.float32,  'calculatedfinishedsquarefeet': np.float32, 'finishedsquarefeet12': np.float32,  'finishedsquarefeet13': np.float32, 'finishedsquarefeet15': np.float32,  'finishedsquarefeet50': np.float32, 'finishedsquarefeet6': np.float32, 'fips': np.float32,  'fireplacecnt': np.float32, 'fullbathcnt': np.float32, 'garagecarcnt': np.float32,  'garagetotalsqft': np.float32, 'hashottuborspa': object, 'heatingorsystemtypeid': np.float32,  'latitude': np.float32, 'longitude': np.float32, 'lotsizesquarefeet': np.float32, 'poolcnt': np.float32,  'poolsizesum': np.float32, 'pooltypeid10': np.float32, 'pooltypeid2': np.float32, 'pooltypeid7': np.float32,  'propertycountylandusecode': object, 'propertylandusetypeid': np.float32,  'propertyzoningdesc': object, 'rawcensustractandblock': np.float32, 'regionidcity': np.float32,  'regionidcounty': np.float32, 'regionidneighborhood': np.float32, 'regionidzip': np.float32,  'roomcnt': np.float32, 'storytypeid': np.float32, 'threequarterbathnbr': np.float32,  'typeconstructiontypeid': np.float32, 'unitcnt': np.float32, 'yardbuildingsqft17': np.float32,  'yardbuildingsqft26': np.float32, 'yearbuilt': np.float32, 'numberofstories': np.float32,  'fireplaceflag': object, 'structuretaxvaluedollarcnt': np.float32,  'taxvaluedollarcnt': np.float32, 'assessmentyear': np.float32, 'landtaxvaluedollarcnt': np.float32,  'taxamount': np.float32, 'taxdelinquencyflag': object, 'taxdelinquencyyear': np.float32,  'censustractandblock': np.float32}\n    train_full = pd.read_csv(\"../input/train_2016_v2.csv\",dtype=dtype,parse_dates=['transactiondate'])\n    prop_full = pd.read_csv('../input/properties_2016.csv',dtype=dtype)\n    sample_full = pd.read_csv('../input/sample_submission.csv')\n    sample_full['parcelid'] = sample_full['ParcelId'] #rename so that parcelid spelling is the same\n    print(\"shape of full training data:\",train_full.shape)\n    print(\"shape of full property data:\",prop_full.shape)\n    print(\"shape of full sample data:\",sample_full.shape)\n    print(\"\\nFinished reading data from disk ...\")\n    return prop_full,train_full,sample_full\n\nprop_full,train_full,sample_full=read_data()\nlandusetypeIDs=np.sort(prop_full['propertylandusetypeid'].unique())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0ba7848b-6e2b-469b-8643-370c9ef4339f","collapsed":true,"_execution_state":"idle","_uuid":"03025c0ffc2307fa83eeddaf2950d1ede6fe9932","trusted":false},"cell_type":"code","source":"print(\"Mean log error for entire data set:\")\nprint(sum(train_full['logerror'])/len(train_full['logerror']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b91554d-584b-43e7-a013-95d8a9015aba","collapsed":true,"_execution_state":"idle","_uuid":"7594dc421fdd6920f2962020f04c1ec780b3b62d","trusted":false},"cell_type":"code","source":"#calculate and store the mean logerror for every propertylandusetypeid\nmeandict={}\n\nfor typeId in landusetypeIDs:\n    print(\"propertylandusetypeid =\", typeId)\n    props=prop_full.loc[prop_full['propertylandusetypeid'] == typeId]\n    propst=props.loc[props['parcelid'].isin(train_full.parcelid)]\n    print(\"    number of properties with that typeID in (test,train) data =\",len(props.index),len(propst.index))\n    meandict[typeId]=0.0114572195563 #see description at the beginning\n    if(len(propst.index)>1000): #we want more than 1000 properties in the training data for the respective landusetypeid so that the bias is hopefully statistically significant. Change this to whatever value you feel comfortable with \n        tprops=train_full.loc[train_full['parcelid'].isin(propst.parcelid)]    \n        meandict[typeId]=sum(tprops['logerror'])/len(tprops.index)\n        print(\"    average abs logerror = \",sum(abs(tprops['logerror']))/len(tprops.index))\n        print(\"    average logerror = \",sum(tprops['logerror'])/len(tprops.index))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4860fb5-f810-4438-a0a5-8ebb5b826b59","collapsed":true,"_execution_state":"idle","_uuid":"93277cb11176adb95a38c2508d28ef79b8fe28ea","trusted":false},"cell_type":"code","source":"train_full=train_full.drop_duplicates(subset='parcelid',keep='first') #need to drop duplicates and triplicates\n\ndf_sample_full = sample_full.merge(prop_full, how='left', on='parcelid')\ndf_sample_full = df_sample_full.merge(train_full, how='left', on='parcelid')\ndf_sample_full = df_sample_full[['parcelid','propertylandusetypeid','logerror']] \ndel prop_full;gc.collect();\ndel train_full;gc.collect();\ndel sample_full;gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6deaa4b5-b6dd-49e1-a371-34c052e8a811","collapsed":true,"_execution_state":"idle","_uuid":"3a5edea9cae441b3213de6a48ac57865d567b3d1","trusted":false},"cell_type":"code","source":"df_sample_full.head(n=20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70b74acd-84ad-408c-a25a-30698eb455da","collapsed":true,"_execution_state":"idle","_uuid":"3f362c83549e80ef338340a258affbf91bca64c6","trusted":false},"cell_type":"code","source":"# predict the logerror based on the mean for the respective propertylandusetypeid\ndef myfillna2(logerr,propid):\n    # almost all properties have a propertylandusetypeid set. However some very few don't \n    if np.isnan(propid):\n        return 0.0114572195563 #see description at the beginning\n    else:\n        return meandict[propid]\n\npred = df_sample_full.apply(lambda x: myfillna2(x.logerror,x.propertylandusetypeid), axis=1)   \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff73c1b4-87a8-4d7b-a9e0-aa10516cafe6","collapsed":true,"_execution_state":"idle","_uuid":"49bf900d63e2b758ac9aec19e6f38d2b6401948a","trusted":false},"cell_type":"code","source":"pred.head(n=20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19bbd477-12b8-488e-8f1a-1a8a7d70292c","collapsed":true,"_execution_state":"idle","_uuid":"7642bfb587148ba36b11f8ee4de39d8596f4bee7","trusted":false},"cell_type":"code","source":"##### WRITE THE RESULTS\n\nprint( \"\\nPreparing results for write ...\" )\ny_pred=[]\n\nfor i,predict in enumerate(pred):\n    y_pred.append(str(round(predict,4)))\ny_pred=np.array(y_pred)\n\noutput = pd.DataFrame({'ParcelId': df_sample_full['parcelid'],\n        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n# set col 'ParceID' to first col\ncols = output.columns.tolist()\ncols = cols[-1:] + cols[:-1]\noutput = output[cols]\nfrom datetime import datetime\n\nprint( \"\\nWriting results to disk ...\" )\noutput.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n\nprint( \"\\nFinished ...\" )","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}