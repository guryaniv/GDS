{"cells":[{"metadata":{"_uuid":"860b372e064aa95391d8ac370318c154c57f7305"},"cell_type":"markdown","source":"# Playing with ensemble classifiers based on seven different CNN models (~99.5 % accuracy)"},{"metadata":{"_uuid":"d9138bdb00bbff95013233a7d451bb6964ecef1d"},"cell_type":"markdown","source":"Hellouu peoples.\n\nThis notebook presents my results of playing around with CNNs in Keras. I train seven different architectures and use them two create an ensemble classifier in order to compare accuracies. Why seven? I am so glad you asked. The number is motivated by a mixture of random coincidence and [this epic movie](https://en.wikipedia.org/wiki/Seven_Samurai). \n\nBefore I start I want to give credit to [Yassine Ghouzam's notebook](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6), which I used as inspiration and which is probably the better notbook if you are working with Keras for the first time. You might also want to have a look at [this](http://cs231n.github.io/convolutional-networks/#architectures), which helped me a lot.\n\nThis is what you're going to find here:\n\n__1. Import of libraries and import of the data__\n\n__2. First overview of data__\n  1. Plot images\n  2. Check distribution\n  3. Check for missing values\n  \n__3. Create train, validation and test datasets__\n\n__4. Define and train the convolutional neural networks__\n  1. Model 1\n  2. Model 2\n  3. Model 3\n  4. Model 4\n  5. Model 5\n  6. Model 6\n  7. Model 7\n  \n__5. Ensemble classifiers and confusion matrices__\n  1. Overview of model performance so far:\n  2. Ensemble classifier based on summing the probabilities\n  3. Ensemble classifier based on a majority vote\n  \n__6. Conlusion__\n\n__7. Output routines__"},{"metadata":{"_uuid":"3baacb792ab330b33411e70dfc91f6a5c297fcc4"},"cell_type":"markdown","source":"# Import of libraries and import of the data"},{"metadata":{"trusted":false,"_uuid":"ae31b5e9247fd1bf770c4eaf6aec11e5710b09d3"},"cell_type":"code","source":"#Data\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport sys\n\n#Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport colorlover as cl\nfrom IPython.display import HTML, SVG\nimport random\n\nrandom.seed(42)\ninit_notebook_mode(connected=True)\n#%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ef4e93eb02aa6c2c695709949fb34507d6aec796"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_comp = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ad334edb2e39446ce540b712baa6800d42ffe7b2"},"cell_type":"code","source":"iplot(ff.create_table(df_train.iloc[0:10,0:10]), filename='jupyter-table1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd5255b967e4dafbcf2ab7a6d9561e964ede621d"},"cell_type":"markdown","source":"# First overview of data"},{"metadata":{"_uuid":"1a0622f432031024fe8ce91dc5cd31ceb60f54b3"},"cell_type":"markdown","source":"### Plot images"},{"metadata":{"_uuid":"7885484d816d57f8f981355e2098c13d9edc5f71"},"cell_type":"markdown","source":"Let's create an overview of the data so that we get a feeling of what we're dealing with. You can change the displayed number by changing *sel_int*."},{"metadata":{"trusted":false,"_uuid":"aba622489026f7fdf48a065b172ddeb5746c372c"},"cell_type":"code","source":"n_size = 10\nsel_int = 5\nnum_array = df_train[df_train.label == sel_int]\n\nfig = tools.make_subplots(rows=10, cols=10, print_grid=False)\nfor row in range(1,n_size+1):\n    for column in range(1,n_size+1):\n        trace = go.Heatmap(z=num_array.iloc[row*10-10+column-1, 1:].values.reshape((28,28))[::-1], colorscale=[[0,'rgb(0,0,0)'],[1,'rgb(255,255,255)']], showscale=False)\n        fig.append_trace(trace, row, column)\n        fig['layout']['xaxis'+str(((row-1)*10 + column))].update(showticklabels=False, ticks='')\n        fig['layout']['yaxis'+str(((row-1)*10 + column))].update(showticklabels=False, ticks='')\n        \nfig['layout'].update(height=500, width=500)\nfig['layout']['margin'].update(l=10, r=10, b=10, t=10)\niplot(fig, filename='number_plot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5377e8208e0aebc4d9a633134e2b8db57192ebf"},"cell_type":"markdown","source":"### Check distribution"},{"metadata":{"_uuid":"19dab8fdb2534379cdb63306614be819a261ecd8"},"cell_type":"markdown","source":"Let's check whether the labels are evenly distributed:"},{"metadata":{"trusted":false,"_uuid":"063e3e8a037b6e0bc13d60e97e7d688d26340f4a"},"cell_type":"code","source":"#df.label.value_counts().values\ntrace = go.Bar(x=df_train.label.value_counts().index,y=df_train.label.value_counts().values)\nlayout = go.Layout(xaxis=dict(title='Number', nticks=10),\n                  yaxis=dict(title='# Occurance'),\n                  width = 600,\n                  height = 400\n                  )\nfigure = go.Figure(data = [trace],\n                  layout = layout)\nfigure['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31256a2de93f3d8c20fd0d884991770b82f1566e"},"cell_type":"markdown","source":"### Check for missing values"},{"metadata":{"trusted":false,"_uuid":"9fa08e8aab5b859e4c89db7913e69b0abf68a932"},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1a84588d25bada0d819ddadd2fb272aa8a2b716b"},"cell_type":"code","source":"df_train.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c06264a4db27801c25da76377f8742c92c5dd071"},"cell_type":"markdown","source":"No missing values - perfect."},{"metadata":{"_uuid":"9a92e1d436da9f8d2af2339815f1ca3ea3368cc9"},"cell_type":"markdown","source":"# Create train, validation and test datasets"},{"metadata":{"_uuid":"e8fd60568cfbd128b16ba0efe54869e1eb437722"},"cell_type":"markdown","source":"As usual we need to split our data into a training, validation and test data set."},{"metadata":{"trusted":false,"_uuid":"fcc9d8ca9200af2cb03ddd15b07d9032a8a6e9d5"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"862aab60afee38e27d04b9b9c2267d8c49780314"},"cell_type":"code","source":"Y = df_train.label\nX = df_train.drop('label', axis=1)\n\nX = X / 255\nX_comp = df_comp / 255\n\nX_train, X_cross, Y_train, Y_cross = train_test_split(X, Y,test_size=0.1, random_state=42)\nX_valid, X_test, Y_valid, Y_test = train_test_split(X_cross, Y_cross, test_size=0.5, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caebc8f2c5bccd191b37a249975aecac06aeae66"},"cell_type":"markdown","source":"Let's make sure there is nothing going wrong with the distribution of the labels in the three sets:"},{"metadata":{"trusted":false,"_uuid":"6c73786d854c4d173d6edca6703bfab7da3930ac"},"cell_type":"code","source":"trace1 = go.Bar(x=Y_train.value_counts().index,y=Y_train.value_counts().values/Y_train.value_counts().values.sum(), name='Training set')\ntrace2 = go.Bar(x=Y_valid.value_counts().index,y=Y_valid.value_counts().values/Y_valid.value_counts().values.sum(), name='Validation set')\ntrace3 = go.Bar(x=Y_test.value_counts().index,y=Y_test.value_counts().values/Y_test.value_counts().values.sum(), name='Test set')\nfig = go.Figure(data=[trace1, trace2, trace3])\nfig['layout'].update(xaxis=dict(title='Number', nticks=10), yaxis=dict(title='# Occurance'), width = 600, height = 400)\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56f9d4ac702ed71ace4cb5f9661e7ad56df2984d"},"cell_type":"markdown","source":"# Define and train the convolutional neural networks"},{"metadata":{"_uuid":"c00c1566310d0b7dfce8a0caeac0b6e27c63f141"},"cell_type":"markdown","source":"Here's where the meat comes on the table. In the next sections the six CNNs are defined and trained using a GPU and the ImageDataGenerator from Keras to make the networks more robust."},{"metadata":{"trusted":false,"_uuid":"89f00feb38d3f07fac108b626e4c4a11da0beda7"},"cell_type":"code","source":"from keras.models import Sequential, load_model\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten\nfrom keras.utils import plot_model, to_categorical\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"07f173eddddb7ba0e1592b4291548faa3e547c29"},"cell_type":"code","source":"X_train = X_train.values.reshape(X_train.shape[0],28,28,1)\nX_valid = X_valid.values.reshape(X_valid.shape[0],28,28,1)\nX_test = X_test.values.reshape(X_test.shape[0],28,28,1)\nX_comp = X_comp.values.reshape(X_comp.shape[0],28,28,1)\n\nY_train = to_categorical(Y_train)\nY_valid = to_categorical(Y_valid)\nY_test = to_categorical(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"78289563e3647810b37556a0a70353f70a277d96"},"cell_type":"code","source":"datagen = ImageDataGenerator(height_shift_range=0.1,\n                             width_shift_range=0.1,\n                             #brightness_range=(0,0.1),\n                             rotation_range=10,\n                             zoom_range=0.1,\n                             fill_mode='constant',\n                             cval=0\n                            )\n\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90a66bd6df60bbc3252750318f4872b6d9451a79"},"cell_type":"markdown","source":"## Model 1"},{"metadata":{"trusted":false,"_uuid":"7aa1700a4491a81686766d1175ee095d5c32dc39"},"cell_type":"code","source":"model = Sequential()\ndroprate = 0.175\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel1 = model\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9561ba76496a6540dd8624407bb4b87c5d844a22"},"cell_type":"raw","source":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))"},{"metadata":{"trusted":false,"_uuid":"6534fad29d1560e6e4f54cd62974e45107dcf1e2"},"cell_type":"code","source":"epochsN = 25\nbatch_sizeN = 63\nhistory1 = model1.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)/batch_sizeN, epochs=epochsN, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d2be50e4e184cd220bc27e0ff06fe1ba43bca440"},"cell_type":"code","source":"model1.evaluate(X_test, Y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4470a78e7ebdc6fc635442f4f7be591ac66e5fd4"},"cell_type":"code","source":"model1.save('model_1.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5a77b161cc126d86bccd2eccc642c286bc3d2e42"},"cell_type":"code","source":"history = history1\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2cee733490884d048ddcac380970bfa2bf3cdf0"},"cell_type":"markdown","source":"## Model 2"},{"metadata":{"trusted":false,"_uuid":"7d474fa1cdc998cabaa700a43ae4f509745544f7"},"cell_type":"code","source":"del model\nmodel = Sequential()\ndroprate = 0.15\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\n#model.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\n#model.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\n#model.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel2 = model\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23f51dcedee5848db12baaa4c29c1d3aa1b1d0eb"},"cell_type":"raw","source":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))"},{"metadata":{"trusted":false,"_uuid":"d0b61c7638ff482907107ca46df5109e7573da9d"},"cell_type":"code","source":"epochsN = 35\nbatch_sizeN = 63\nhistory2 = model2.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)/batch_sizeN, epochs=epochsN, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2cd164e05178be61195b73d0795cb660b562963e"},"cell_type":"code","source":"model2.evaluate(X_test, Y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1603e5922d4dc5f978a37d04aa3d0e48235c3d3b"},"cell_type":"code","source":"model2.save('model_2.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b1f448f15fbc32d16f5e235afaa2374fdd917c1f"},"cell_type":"code","source":"history = history2\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55fe6acb9a439a0930e67c5b3fde6f7612638355"},"cell_type":"markdown","source":"## Model 3"},{"metadata":{"trusted":false,"_uuid":"7aedaaa913e1a36e4b0a201677547c2932fcc998"},"cell_type":"code","source":"del model\nmodel = Sequential()\ndroprate = 0.2\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel3 = model\nmodel3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6af5cbd3494237e5c5ea6384414b9923a704378f"},"cell_type":"raw","source":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))"},{"metadata":{"trusted":false,"_uuid":"c29e2028f1090080e12721396dbb2750a422cafb"},"cell_type":"code","source":"epochsN = 40\nbatch_sizeN = 63\nhistory3 = model3.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)/batch_sizeN, epochs=epochsN, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9dc28b22799f7dbeadfb3d5f9f312da6a937eabb"},"cell_type":"code","source":"model3.evaluate(X_test, Y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"821adbe2dd6d93d57e9199622c6419368f747121"},"cell_type":"code","source":"model3.save('model_3.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"586410d7219900bd5927f53aa8409e853b404038"},"cell_type":"code","source":"history = history3\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b24cbd68db58166fc1bbf4881ab7f107f456d7a"},"cell_type":"markdown","source":"## Model 4"},{"metadata":{"trusted":false,"_uuid":"efaa3b4d5afdb9c2de3c5136223e000ef775638c"},"cell_type":"code","source":"del model\nmodel = Sequential()\ndroprate = 0.20\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=16, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=16, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel4 = model\nmodel4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b83872d8a17e8d8fb99e7f040b752db8dad30cda"},"cell_type":"raw","source":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))"},{"metadata":{"trusted":false,"_uuid":"94e3c70b7208203150087495971df5cf84691117"},"cell_type":"code","source":"epochsN = 90\nbatch_sizeN = 63\nhistory4 = model4.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)/batch_sizeN, epochs=epochsN, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e77903785f76b62529720cb9696c021b49ccd65e"},"cell_type":"code","source":"model4.evaluate(X_test, Y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ca8723c003c2e67280306476321be579acb23bd4"},"cell_type":"code","source":"model4.save('model_4.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ae2e2975b5b92e315f2e334e3091c715cec19e5e"},"cell_type":"code","source":"history = history4\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"132726d08d17993519d63a301955bcafca453af9"},"cell_type":"markdown","source":"## Model 5"},{"metadata":{"trusted":false,"_uuid":"d33c152f2acfff17e9986c84bc6fc4e17aa018c3"},"cell_type":"code","source":"del model\nmodel = Sequential()\ndroprate = 0.1\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=16, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(3,3), filters=16, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dropout(droprate))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel5 = model\nmodel5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edfcd745953026432d948ab1384db549ae5c6121"},"cell_type":"raw","source":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))"},{"metadata":{"trusted":false,"_uuid":"c1a7e5cf70ee2408b3b84ecd153b23d79ccebbd4"},"cell_type":"code","source":"epochsN = 90\nbatch_sizeN = 63\nhistory5 = model5.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)/batch_sizeN, epochs=epochsN, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"41b65281674bac8ab4fe7a2746b3e58684258a51"},"cell_type":"code","source":"model5.evaluate(X_test, Y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"285a5ec1c34735bffde3cfbf6ba65ae12b4dea6a"},"cell_type":"code","source":"model5.save('model_5.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"20e27df6ab6f86081a53995cdf8d595b2b6cc5e4"},"cell_type":"code","source":"history = history5\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0805ef9779d47db5958f8c5c3af70533e633803b"},"cell_type":"markdown","source":"## Model 6"},{"metadata":{"trusted":false,"_uuid":"bc35c2c227fbd83be6edf5fc6b0325e34419fd83"},"cell_type":"code","source":"del model\nmodel = Sequential()\ndroprate = 0.15\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=16, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dropout(droprate))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel6 = model\nmodel6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69d524c49c91ee3216e77aa020a8d6e671f5fdd6"},"cell_type":"raw","source":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))"},{"metadata":{"trusted":false,"_uuid":"0f0a8e1772024ab6031d10f5620d620482066f76"},"cell_type":"code","source":"epochsN = 45\nbatch_sizeN = 63\nhistory6 = model6.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)/batch_sizeN, epochs=epochsN, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d974a34b7cc353fde36354bb4742e4738fd84d12"},"cell_type":"code","source":"model6.evaluate(X_test, Y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"21e5b5fd2b29fade34c405a9d4d17f834b65c3ae"},"cell_type":"code","source":"model6.save('model_6.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8066405ea7d10fe8a708016cbd3fb4b121895f1d"},"cell_type":"code","source":"history = history6\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbccf6b0acf8610d2c71fd2658c9820c88d0f98a"},"cell_type":"markdown","source":"## Model 7"},{"metadata":{"trusted":false,"_uuid":"51f325604a6096d9b9dde15f5a7140a00d4d6d27"},"cell_type":"code","source":"del model\nmodel = Sequential()\ndroprate = 0.35\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\n#model.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(3,3), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(3,3), filters=128, strides=(1,1), padding='same',activation='relu'))\n#model.add(Conv2D(kernel_size=(3,3), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(3,3), filters=256, strides=(1,1), padding='valid',activation='relu'))\nmodel.add(Conv2D(kernel_size=(3,3), filters=256, strides=(1,1), padding='valid',activation='relu'))\n#model.add(Conv2D(kernel_size=(3,3), filters=256, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(3,3), filters=256, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='softmax'))\n\nmodel7 = model\nmodel7.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51474b52343891a830bc56cb2631da6a4536f4e4"},"cell_type":"raw","source":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))"},{"metadata":{"trusted":false,"_uuid":"6f8a6f96c07fb2f616fb29457e4353a0df564d78"},"cell_type":"code","source":"epochsN = 60\nbatch_sizeN = 63\nhistory7 = model7.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)/batch_sizeN, epochs=epochsN, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"31f84ae32359b9565631a863aa2c9ef10235dec9"},"cell_type":"code","source":"model7.evaluate(X_test, Y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dd84712df20ffc74c01e02eea740d831de14cbac"},"cell_type":"code","source":"model7.save('model_7.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"64a9c2898c503e73d8dc8ce77543155c19953df0"},"cell_type":"code","source":"history = history7\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eeec29e093e4230aee84c4ff0ac7bd5295784579"},"cell_type":"markdown","source":"# Ensemble classifiers and confusion matrices"},{"metadata":{"_uuid":"3630d3cd8afea23d23fd525ec6f2fc84c1e931cd"},"cell_type":"markdown","source":"#Optionally load the models from disk:\nmodel1 = load_model('model_1.h5')\nmodel2 = load_model('model_2.h5')\nmodel3 = load_model('model_3.h5')\nmodel4 = load_model('model_4.h5')\nmodel5 = load_model('model_5.h5')\nmodel6 = load_model('model_6.h5')\nmodel7 = load_model('model_7.h5')"},{"metadata":{"_uuid":"a0392b4f0337d3e69479cce785990605664ce7f8"},"cell_type":"markdown","source":"## Overview of model performance so far:"},{"metadata":{"_uuid":"42c834046afe0560c1c8ad86c25651905e3f17c3"},"cell_type":"markdown","source":"Let's make a list and plot the performance of the created models:"},{"metadata":{"trusted":false,"_uuid":"81e8887cfc085c2701d5179e2b0c1ddea3e2286a"},"cell_type":"code","source":"trained_models = [model1, model2, model3, model4, model5, model6, model7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"09644599a9aefc0f40d3462dc131921c283419ff"},"cell_type":"code","source":"acc_scores = pd.Series()\nfor num, model in enumerate(trained_models):\n    acc_scores.loc['Model ' + str(num + 1)] = accuracy_score(np.argmax(Y_test, axis=1), np.argmax(model.predict(X_test), axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b759f735a39b074e0bddffcda671c4156e9aed3f"},"cell_type":"code","source":"trace = go.Bar(x=acc_scores.values ,y=acc_scores.index, orientation='h')\nlayout = go.Layout(xaxis=dict(title='Accuracy', nticks=10, range=[0.985, 1]),\n                  #yaxis=dict(title='Model'),\n                  width = 600,\n                  height = 400\n                  )\nfigure = go.Figure(data = [trace],\n                  layout = layout)\nfigure['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81474fdd71b2bbf0ff2361ebf5455ce3187f276a"},"cell_type":"markdown","source":"Let's get the score of the best performing model:"},{"metadata":{"trusted":false,"_uuid":"0434a7928bd701717b0610cf72785df78edf0b7b"},"cell_type":"code","source":"print(acc_scores.idxmax(), ': ', acc_scores[acc_scores.idxmax()])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"764ef9d313dba2ba7f49bf2307b1daf961da7cad"},"cell_type":"markdown","source":"As a reference, let's plot the confusion matrix for this model:"},{"metadata":{"trusted":false,"_uuid":"2ef92a8ff54e840c2468d14f0673085d262ea55b"},"cell_type":"code","source":"ind_best_model = acc_scores.reset_index().loc[:, 0].idxmax(axis=0)\nY_test_pred = trained_models[ind_best_model].predict(X_test)\nconfM = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_test_pred, axis=1))\n\nfig = ff.create_annotated_heatmap(x=list(map(str, range(0,10))), y=list(map(str, range(0,10))), z=np.log(confM+1), annotation_text=confM, colorscale='Jet')\nfig['layout'].update(xaxis=dict(title='Predictated Label'), yaxis=dict(title='Actual Label', autorange='reversed'), width = 600, height = 600)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03f21aab488ac03e5111818cfa7cd2d0092b6879"},"cell_type":"markdown","source":"## Ensemble classifier based on summing the probabilities:"},{"metadata":{"_uuid":"5ca67c899c4f7f59a7c6d4c23baa8625fc381a16"},"cell_type":"markdown","source":"Now lets see whether we can get a better classification by summing up the probabilities of the six different models:"},{"metadata":{"trusted":false,"_uuid":"2d0bea913b8fd725e50a68d2bd108fb112dc3f0c"},"cell_type":"code","source":"def summing_classifier(data, model_list):\n    total_pred_prob = model_list[0].predict(data)\n    for model in model_list[1:]:\n        total_pred_prob += model.predict(data)\n        \n    return np.argmax(total_pred_prob, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f7c1e5fdb7debfef9349b8865ab6ddb0f1c2461"},"cell_type":"markdown","source":"This gives a score of:"},{"metadata":{"trusted":false,"_uuid":"1e75755611650dcfe9a4dddeacfbeb5c9d14bc7a"},"cell_type":"code","source":"acc_scores.loc['Summing Classifier'] = accuracy_score(np.argmax(Y_test, axis=1), summing_classifier(X_test, trained_models))\nacc_scores.loc['Summing Classifier']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52f832cc35f131762966c8d3d0ba18a2605a762a"},"cell_type":"markdown","source":"Which we can compare to the average of the six models:"},{"metadata":{"trusted":false,"_uuid":"67e3694fdf84c06296223177336b5742808a6dbe"},"cell_type":"code","source":"acc_scores.iloc[0:6].mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3228e6060f7864e2ecf2a7962216a2f8c9e778c"},"cell_type":"markdown","source":"The confusion matrix for the summing classifier looks like this:"},{"metadata":{"trusted":false,"_uuid":"2c779e1e04b6374fef19f571350604369f22eca7"},"cell_type":"code","source":"confM = confusion_matrix(np.argmax(Y_test, axis=1), summing_classifier(X_test, trained_models))\n\nfig = ff.create_annotated_heatmap(x=list(map(str, range(0,10))), y=list(map(str, range(0,10))), z=np.log(confM+1), annotation_text=confM, colorscale='Jet')\nfig['layout'].update(xaxis=dict(title='Predictated Label'), yaxis=dict(title='Actual Label', autorange='reversed'), width = 600, height = 600)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"529cf10c27b5730a9ef47b6990d200fb6684dee8"},"cell_type":"markdown","source":"## Ensemble classifier based on majority vote:"},{"metadata":{"_uuid":"f5d7f07a5472b738527de928a8e860718e90eb42"},"cell_type":"markdown","source":"Let's try if we get a different result by determining the label with a majority vote of the six different models:"},{"metadata":{"trusted":false,"_uuid":"3bcfaf81e19be4c21261472f8eac11651649d2bc"},"cell_type":"code","source":"def voting_classifier(data, model_list):\n    pred_list = np.argmax(model_list[0].predict(data), axis=1).reshape((1,len(data)))\n    for model in model_list[1:]:\n        pred_list = np.append(pred_list, [np.argmax(model.predict(data), axis=1)], axis=0)\n    return np.array(list(map(lambda x: np.bincount(x).argmax(), pred_list.T)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2761995c63fd98a3a627831fa8e7cc23bfe2ff85"},"cell_type":"markdown","source":"This results in a accuracy of:"},{"metadata":{"trusted":false,"_uuid":"df133d6cf329720238bf3ca9183413cb97cb49b3"},"cell_type":"code","source":"acc_scores.loc['Voting Classifier'] = accuracy_score(np.argmax(Y_test, axis=1), voting_classifier(X_test, trained_models))\nacc_scores.loc['Voting Classifier']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74e96d6b0781b541264a5133433cffa65e6fbe91"},"cell_type":"markdown","source":"And here is the confusing matrix we see that we make different mistakes (but not more or less):"},{"metadata":{"trusted":false,"_uuid":"0a02af6932f898a13548db84dd8092429155579b"},"cell_type":"code","source":"confM = confusion_matrix(np.argmax(Y_test, axis=1), voting_classifier(X_test, trained_models))\n\nfig = ff.create_annotated_heatmap(x=list(map(str, range(0,10))), y=list(map(str, range(0,10))), z=np.log(confM+1), annotation_text=confM, colorscale='Jet')\nfig['layout'].update(xaxis=dict(title='Predictated Label'), yaxis=dict(title='Actual Label', autorange='reversed'), width = 600, height = 600)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f41c4fe4876f69bc2a0e4e0abe9e162f8ba1f25"},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"_uuid":"9713497c88d0e80cfb642da52fbd9e7918544695"},"cell_type":"markdown","source":"Let's plot the accuracies again:"},{"metadata":{"trusted":false,"_uuid":"5c900c7dd9870078608fcccfeca8ad60d32ecf25"},"cell_type":"code","source":"trace = go.Bar(x=acc_scores.sort_values(ascending=True).values ,y=acc_scores.sort_values(ascending=True).index, orientation='h')\nlayout = go.Layout(xaxis=dict(title='Accuracy', nticks=10, range=[0.985, 1]),\n                  #yaxis=dict(title='Model'),\n                  width = 600,\n                  height = 400\n                  )\nfigure = go.Figure(data = [trace],\n                  layout = layout)\nfigure['layout']['margin'].update(l=130, r=50, b=50, t=50)\niplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b347197b3fef8789f3b6bf4ba380359c118eb852"},"cell_type":"markdown","source":"So that's it. While I wouldn't trust the accuracies determined from a small test set of ~2000 images too much, it seems that the ensemble enhances the accurance beyond what the best single model can do."},{"metadata":{"_uuid":"d72f1adb00991a0ebd52e4d8945f28cb78953116"},"cell_type":"markdown","source":"# Output routines"},{"metadata":{"_uuid":"03d6cbbe05e777adf2fe9bfd69269e7f6b6f073b"},"cell_type":"markdown","source":"### Competition predictions of the best single model:"},{"metadata":{"trusted":false,"_uuid":"08b787137d437226994675c52975054559796c4e"},"cell_type":"code","source":"best_model_results = pd.DataFrame({'Label' : np.argmax(trained_models[ind_best_model].predict(X_comp), axis=1)})\nbest_model_results = best_model_results.reset_index().rename(columns={'index' : 'ImageId'})\nbest_model_results['ImageId'] = best_model_results['ImageId'] + 1\nbest_model_results.to_csv('best_model_result_kaggle.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9158c49f60f67fc4f1876c29f23f707dd72c1f5"},"cell_type":"markdown","source":"### Competition predictions of the ensemble based on the sum:"},{"metadata":{"trusted":false,"_uuid":"3e4a098556253807ae2861a3d4fbf1458dacf5c6"},"cell_type":"code","source":"esmbl_sum_results = pd.DataFrame({'Label' : summing_classifier(X_comp, trained_models)})\nesmbl_sum_results = esmbl_sum_results.reset_index().rename(columns={'index' : 'ImageId'})\nesmbl_sum_results['ImageId'] = esmbl_sum_results['ImageId'] + 1\nesmbl_sum_results.to_csv('esmbl_sum_result_kaggle.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24257f93fde40c7c5290af440a0d0d03f70c3931"},"cell_type":"markdown","source":"### Competition predictions of the ensemble based on the vote:"},{"metadata":{"trusted":false,"_uuid":"eb893bea0efcdab35c1e819ea106ee4a567edb69"},"cell_type":"code","source":"esmbl_vote_results = pd.DataFrame({'Label' : voting_classifier(X_comp, trained_models)})\nesmbl_vote_results = esmbl_vote_results.reset_index().rename(columns={'index' : 'ImageId'})\nesmbl_vote_results['ImageId'] = esmbl_vote_results['ImageId'] + 1\nesmbl_vote_results.to_csv('esmbl_vote_result_kaggle.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}