{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "version": "3.6.3"}}, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "9dacb7a2a5a9e582d7e597be4ea479cedd79489a", "_cell_guid": "d49b7eaa-0bf7-48e0-9fe6-51ba03036ba1"}, "source": ["# Who wrote that? - Spooky author identification\n", "Started on 30 Oct 2017\n", "\n", "This notebook is inspired by:\n", "* Machine Learning: Classification - Coursera course by University of Washington,\n", "https://www.coursera.org/learn/ml-classification\n", "* Machine Learning with Text in scikit-learn - Kevin Markham's tutorial at Pycon 2016, \n", "https://m.youtube.com/watch?t=185s&v=ZiKMIuYidY0\n", "* Kernel by bshivanni - \"Predict the author of the story\", \n", "https://www.kaggle.com/bsivavenu/predict-the-author-of-the-story\n", "* Kernel by SRK - \"Simple Engg Feature Notebook - Spooky Author\",\n", "https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author"]}, {"cell_type": "markdown", "metadata": {"_uuid": "8f9dbcf8df12e40c0daee0f48d91fd07cd084cae", "_cell_guid": "06e8c4a5-feea-41ea-8831-13ab7089dcc5"}, "source": ["Comments:\n", "\n", "* In this kernel, I will also do a weighted averaging of the 'proba' of the 2 models to see the performance.\n"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from matplotlib import pyplot as plt\n", "%matplotlib inline"], "metadata": {"_uuid": "5c5f4cc8865644748e11336736bbe584adebe7b1", "collapsed": true, "_cell_guid": "8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6"}}, {"cell_type": "markdown", "metadata": {"_uuid": "4f65d03ddbfd127307d3e415003346eb898b4d6b", "_cell_guid": "80d61838-9025-4cba-bb0e-58175586b21b"}, "source": ["## Read \"train.csv\" and \"test.csv into pandas"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')"], "metadata": {"_uuid": "4e35cd5fcae1581dbd6bc51f14728e27fe63fe70", "collapsed": true, "_cell_guid": "094fff47-db10-447c-965e-08056f718bde"}}, {"cell_type": "markdown", "metadata": {"_uuid": "89d1c9a4f9598427e8a20d66fa9e56796ad720f6", "_cell_guid": "09986b08-eda6-4438-9cbe-52a61d8d57fa"}, "source": ["## Examine the train data"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train_df.shape"], "metadata": {"_uuid": "4cf1c0650ee00566fa75e9c100393cac437d48d1", "_cell_guid": "e8a8b1d3-36c0-4026-b91d-1181dda8bbfb"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train_df.head()"], "metadata": {"_uuid": "9e53b7599d707a9420a75c37c7ac6d05bed9df7b", "_cell_guid": "c4c7137d-6bc7-4b41-b50a-e511883155e9"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# check the class distribution for the author label in train_df?\n", "train_df['author'].value_counts()"], "metadata": {"_uuid": "2ff9ed83ba328872d446add97695285dc49f4165", "_cell_guid": "981dff09-3acf-4014-b38a-22afc02a6654"}}, {"cell_type": "markdown", "metadata": {"_uuid": "d2bd8ddc6445aad024d327d7b78004f80cfd83f6", "_cell_guid": "391dfeb1-5549-4f85-ac4c-4ed66e9cce6b"}, "source": ["#### The class distribution looks balanced."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# compute the text length for the rows and record these\n", "train_df['text_length'] = train_df['text'].apply(lambda x: len(str(x).split()))\n", "train_df.head()"], "metadata": {"_uuid": "1e97432af65b6b75b436daabc83bdf57775a59c1", "_cell_guid": "b26588a7-9a7f-4183-98b2-fb94a70bedaa"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# look at the histogram plot for text length\n", "train_df.hist()\n", "plt.show()"], "metadata": {"_uuid": "448c3492fc2fe24f30bd7b97047d69f16b58ca2f", "_cell_guid": "d5ac5111-5bba-44c9-a039-7bb01a5bfd59"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# number of text length that are greater than 100, 150 & 200\n", "G100 = sum(i > 100 for i in train_df['text_length'])\n", "G150 = sum(i > 150 for i in train_df['text_length'])\n", "G200 = sum(i > 200 for i in train_df['text_length'])\n", "print('Text length greater than 100, 150 & 200 are ',G100,',',G150,'&',G200, ' respectively.')\n", "print('In percentages, they are %.2f, %.2f & %.2f' %(G100/len(train_df)*100, \n", "      G150/len(train_df)*100, G200/len(train_df)*100))"], "metadata": {"_uuid": "0bccd37457d2688f5709b48c1101eb452fb02e5b", "_cell_guid": "71c554b8-96f5-4910-9c99-0b9e27ea2210"}}, {"cell_type": "markdown", "metadata": {"_uuid": "d28a801b4057518f87e20c46a77f9dc8757ab944", "_cell_guid": "5b482de7-5fd1-4ef7-b7b4-2abf84ebdc25"}, "source": ["#### Most of the text length are within 200 words and less. Let's look at the summary statistics of the text lengths by author."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["EAP = train_df[train_df['author'] =='EAP']['text_length']\n", "EAP.describe()"], "metadata": {"_uuid": "d5d5bf3219d1a53fae0bfab69844e118cda32bab", "_cell_guid": "5da0987d-d871-4f53-b605-8f89ab4dfd94"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["EAP.hist()\n", "plt.show()"], "metadata": {"_uuid": "4600bd930b16cd5e576701c7baa4304c6b6b0836", "_cell_guid": "4f17e03d-50dd-4aae-b579-a8f0d68ead5a"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["MWS = train_df[train_df['author'] == 'MWS']['text_length']\n", "MWS.describe()"], "metadata": {"_uuid": "a3e02aec85a3817124145d6c4ca602b9e0a73223", "_cell_guid": "a9d8733d-c134-4f9b-8184-26492b995602"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["MWS.hist()\n", "plt.show()"], "metadata": {"_uuid": "487062ed9cca7724c86631174501c4bf2c2b90ad", "_cell_guid": "4befa810-7786-451d-b476-b41dc5036869"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["HPL = train_df[train_df['author'] == 'HPL']['text_length']\n", "HPL.describe()"], "metadata": {"_uuid": "e3be2b92e83f2a4336bd20677395930597fea6c6", "_cell_guid": "73db4ab6-3395-4972-be02-36d4bc393899"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["HPL.hist()\n", "plt.show()"], "metadata": {"_uuid": "f8cf435292bd01efe234667d55239a8733b60950", "_cell_guid": "6f58ad06-6ce6-4921-bcbf-536cc7c009b3"}}, {"cell_type": "markdown", "metadata": {"_uuid": "71efe8c7cf1fa93eec845046b031852fcc4ed8ac", "_cell_guid": "7c25d701-f75d-4902-b51d-6ed46cdb27a8"}, "source": ["## Similarly examine the text length & distribution in test data"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["test_df.shape"], "metadata": {"_uuid": "8359291e49919941659a26679e19630192707508", "_cell_guid": "d0776a75-0bac-4508-b60f-66e0dc6735d4"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["test_df.head()"], "metadata": {"_uuid": "5b17f1fccfe69cda599ad77b061481b6c3c1a590", "_cell_guid": "9006b12c-110c-49e4-a5b7-cd824f9306ac"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# examine the text length in test_df and record these\n", "test_df['text_length'] = test_df['text'].apply(lambda x: len(str(x).split()))\n", "test_df.head()"], "metadata": {"_uuid": "36e4d00a5afc15e6b04ffa1e79421396d051f614", "_cell_guid": "0993d06b-495e-428a-933a-6ffec6bdcef3"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["test_df.hist()\n", "plt.show()"], "metadata": {"_uuid": "1cc05875b88e54b5a1079eafc088207536dea2f3", "_cell_guid": "828b9990-d78e-46c7-b011-1c66e6e6be79"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# number of text length that are greater than 100, 150 & 200\n", "G100 = sum(i > 100 for i in test_df['text_length'])\n", "G150 = sum(i > 150 for i in test_df['text_length'])\n", "G200 = sum(i > 200 for i in test_df['text_length'])\n", "print('Text length greater than 100, 150 & 200 are ',G100,',',G150,'&',G200, ' respectively.')\n", "print('In percentages, they are {:.2f}, {:.2f} & {:.2f}'.format(G100/len(test_df)*100, \n", "      G150/len(test_df)*100, G200/len(test_df)*100))"], "metadata": {"_uuid": "d51acd1488ff54db3037cd0259ffe51aa755c092", "_cell_guid": "cda4bb66-766e-4219-87be-45aed5036114"}}, {"cell_type": "markdown", "metadata": {"_uuid": "db69eb685cd1be44de2224c399cbdeabb5ceaa06", "_cell_guid": "b2bbf246-f098-425a-99e3-2eb2126b975c"}, "source": ["#### The proportion of text which are long in the test data is very similar to that in the train data."]}, {"cell_type": "markdown", "metadata": {"_uuid": "d0709c2d7c606f27b7b1abeeed821aaeee00fb6e", "_cell_guid": "3c3f238d-1b6a-48a0-bcee-40031e4a2536"}, "source": ["## Some preprocessing of the target variable to facilitate modelling"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# convert author labels into numerical variables\n", "train_df['author_num'] = train_df.author.map({'EAP':0, 'HPL':1, 'MWS':2})\n", "# Check conversion for first 5 rows\n", "train_df.head()"], "metadata": {"_uuid": "46438c851ef26de2bd7a3736c13abeb938519800", "_cell_guid": "1a1a7fb8-9dfc-47c9-9114-f4708d109854"}}, {"cell_type": "markdown", "metadata": {"_uuid": "cae81f6b1d9bb475fc486d5fbb81981025cc3672", "_cell_guid": "f5abe72a-13a7-41f6-ae8e-1c34dca97110"}, "source": ["## Define X and y from train data for use in tokenization by CountVectorizer"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["X = train_df['text']\n", "y = train_df['author_num']\n", "print(X.shape)\n", "print(y.shape)"], "metadata": {"_uuid": "061d59552c6ef83bea8ecf9ffbf203286aeab6f8", "_cell_guid": "49547fd7-9633-4f6a-bd46-84d8966f1e8b"}}, {"cell_type": "markdown", "metadata": {"_uuid": "903c319c5b83198edf0af59d49818bd9071ec8dc", "_cell_guid": "4a6e6a69-91b7-43a9-8e91-3c8fd20d2790"}, "source": ["## Split train data into a training and a test set"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n", "print(X_train.shape)\n", "print(X_test.shape)\n", "print(y_train.shape)\n", "print(y_test.shape)"], "metadata": {"_uuid": "32e74067044fb018d0a71ed97c3326d578b1c0a6", "_cell_guid": "438ceb39-2687-41b5-b7cb-8784a1aab35d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# examine the class distribution in y_train and y_test\n", "print(y_train.value_counts())\n", "print(y_test.value_counts())"], "metadata": {"_uuid": "40e308981530947ddb7b68ac0075b36c7decde43", "_cell_guid": "7010f557-ccce-4d8a-a289-d081d2c3c0c9"}}, {"cell_type": "markdown", "metadata": {"_uuid": "cc4c2aeef221f5a97e1bd5ea1154052172175351", "_cell_guid": "b2d898ae-79bc-48b8-8544-fb077f876c67"}, "source": ["## Vectorize the data using CountVectorizer"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# import and instantiate CountVectorizer\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\;|\\:')\n", "# vect = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\?|\\;|\\:|\\!|\\'')\n", "vect"], "metadata": {"_uuid": "7b7adf15d16408eb99689884906d0d687c2f8407", "_cell_guid": "9be5a4f2-0e85-4f9a-ac00-b8e9916116cb"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# learn the vocabulary in the training data, then use it to create a document-term matrix\n", "X_train_dtm = vect.fit_transform(X_train)\n", "# examine the document-term matrix created from X_train\n", "X_train_dtm"], "metadata": {"_uuid": "b755b1d4db58eeb5b0ab668d1aaf4a651d3de441", "_cell_guid": "283c1d48-c267-431b-834e-37c8d9222b3c"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# transform the test data using the earlier fitted vocabulary, into a document-term matrix\n", "X_test_dtm = vect.transform(X_test)\n", "# examine the document-term matrix from X_test\n", "X_test_dtm"], "metadata": {"_uuid": "408301ccb78e3f4056a6d2ebbd239594d1a59da0", "_cell_guid": "54050711-560a-47cf-b4f9-2fbaf59bc2e4"}}, {"cell_type": "markdown", "metadata": {"_uuid": "bfd0cc30c28d8750c2b4fc395c00d9f3b186a4fe", "_cell_guid": "74e5e92e-9964-47bd-89fb-d39b62730954"}, "source": ["## Build and evaluate an author classification model using Multinomial Naive Bayes"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# import and instantiate the Multinomial Naive Bayes model\n", "from sklearn.naive_bayes import MultinomialNB\n", "nb = MultinomialNB()\n", "nb"], "metadata": {"_uuid": "5498742f48ffcb39e09d347f5bf315fd882627c0", "_cell_guid": "24e76bcf-3f0c-46b0-8370-e47ec1377b31"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# train the model using X_train_dtm & y_train\n", "nb.fit(X_train_dtm, y_train)"], "metadata": {"_uuid": "2eb2a105d06304cc43486e396c5e57bcfa6a5da5", "_cell_guid": "21f72378-5d13-4c70-9ac5-74cede2bfc34"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# make author (class) predictions for X_test_dtm\n", "y_pred_test = nb.predict(X_test_dtm)"], "metadata": {"_uuid": "20b7974ad1b6b3bd4c03d0110950410c491366e6", "collapsed": true, "_cell_guid": "521690d7-19e4-408e-b837-76582be3b408"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# compute the accuracy of the predictions with y_test\n", "from sklearn import metrics\n", "metrics.accuracy_score(y_test, y_pred_test)"], "metadata": {"_uuid": "75c9bf8f66093512b8ca8275e88f38750c6d6128", "_cell_guid": "022f4b59-ef5e-413e-b4ef-090d81ac2223"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# compute the accuracy of training data predictions\n", "y_pred_train = nb.predict(X_train_dtm)\n", "metrics.accuracy_score(y_train, y_pred_train)"], "metadata": {"_uuid": "280a7405b142bba27d5a9b9cd2f778191158649b", "_cell_guid": "9a538513-0860-48db-8626-5117499ea237"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# look at the confusion matrix for y_test\n", "metrics.confusion_matrix(y_test, y_pred_test)"], "metadata": {"_uuid": "be747bd888c6f48c9e1581c5341fa0eaba4b9753", "_cell_guid": "6d92d210-7255-4c33-ba42-bee0c32de1bb"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# calculate predicted probabilities for X_test_dtm\n", "y_pred_prob = nb.predict_proba(X_test_dtm)\n", "y_pred_prob[:10]"], "metadata": {"_uuid": "3b87ae14e8f809a509207fee3c3e29925f19a2e4", "_cell_guid": "fa8ac435-be40-48a6-8b46-b6e3860dbbee"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# compute the log loss number\n", "metrics.log_loss(y_test, y_pred_prob)"], "metadata": {"_uuid": "9fed846b715fb77ee2549c63b02e3adce66b4f5f", "_cell_guid": "48b380e8-5822-4376-aeea-295b229d62e9"}}, {"cell_type": "markdown", "metadata": {"_uuid": "a989e534fe575ebeb3d5952e221c5b445631738a", "_cell_guid": "04ab10ee-4c43-41a2-87a8-ea3c3400a546"}, "source": ["## Build and evaluate an author classification model using Logistic Regression"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# import and instantiate the Logistic Regression model\n", "from sklearn.linear_model import LogisticRegression\n", "logreg = LogisticRegression()\n", "logreg"], "metadata": {"_uuid": "e7e5707b19c35c8371a0cff7351bc8aadc33acd1", "_cell_guid": "e30be87f-e0a6-4fd7-bff2-b3c37737cbfe"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# train the model using X_train_dtm and y_train\n", "logreg.fit(X_train_dtm, y_train)"], "metadata": {"_uuid": "b0273bf7105237551f62d03cd9a46bc245e99b8b", "_cell_guid": "fb392d0d-0d4b-4c0b-9690-2eac8cacc607"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# make class predictions for X_test_dtm\n", "y_pred_test = logreg.predict(X_test_dtm)"], "metadata": {"_uuid": "98f1dfbb75b68b65d49fa9f8fa69ebf64554e894", "collapsed": true, "_cell_guid": "4a298037-97bc-49eb-8cf7-bf52d4894f11"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# compute the accuracy of the predictions\n", "metrics.accuracy_score(y_test, y_pred_test)"], "metadata": {"_uuid": "5000bb48f01b5632c5b3d4387154e32b6be4410a", "_cell_guid": "9c459dae-783d-49e0-9b0c-dffd35419dbc"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# compute the accuracy of predictions with the training data\n", "y_pred_train = logreg.predict(X_train_dtm)\n", "metrics.accuracy_score(y_train, y_pred_train)"], "metadata": {"_uuid": "847ef102dcb57e88f06003603a4ec7bd7cdde87f", "_cell_guid": "816ea321-cdfd-41d0-b2f8-b89fcc513c43"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# look at the confusion matrix for y_test\n", "metrics.confusion_matrix(y_test, y_pred_test)"], "metadata": {"_uuid": "302632e4ecd6790e1cbb92baab53f98517028bcc", "_cell_guid": "c6f4a678-7072-44c4-a415-de7ba1d55d52"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# compute the predicted probabilities for X_test_dtm\n", "y_pred_prob = logreg.predict_proba(X_test_dtm)\n", "y_pred_prob[:10]"], "metadata": {"_uuid": "c6a1543f17f4031e31b250aef5d98ddb6b1f1d0d", "_cell_guid": "8a238463-87eb-44c5-afcd-794accd04d71"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# compute the log loss number\n", "metrics.log_loss(y_test, y_pred_prob)"], "metadata": {"_uuid": "ad9cfb160671a6fbcf92b8ca6e800440948cf232", "_cell_guid": "91e879ac-744b-4d10-b798-c99535e5c23b"}}, {"cell_type": "markdown", "metadata": {"_uuid": "ef4f2b854c0a2c346a6615ac8de40e5e4bcbe868", "_cell_guid": "962928a6-503e-4757-8518-b7b23f74d09a"}, "source": ["## Train the Logistic Regression model with the entire dataset from \"train.csv\""]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Learn the vocabulary in the entire training data, and create the document-term matrix\n", "X_dtm = vect.fit_transform(X)\n", "# Examine the document-term matrix created from X_train\n", "X_dtm"], "metadata": {"_uuid": "d7e09c9fe6524a9f964d50d8048c5226a4a67475", "_cell_guid": "5018c932-1fc4-401b-8993-2f1a48f2415e"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Train the Logistic Regression model using X_dtm & y\n", "logreg.fit(X_dtm, y)"], "metadata": {"_uuid": "ab43f652017097166901c83c679485b444c6c192", "_cell_guid": "69cacde4-b731-40a8-b754-e3ecd4decff7"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Compute the accuracy of training data predictions\n", "y_pred_train = logreg.predict(X_dtm)\n", "metrics.accuracy_score(y, y_pred_train)"], "metadata": {"_uuid": "65f85a9921ce7d0664762fddb1b592a8bb005cae", "_cell_guid": "48c91a38-87d7-4b72-a418-829730c9ee37"}}, {"cell_type": "markdown", "metadata": {"_uuid": "968c394fe3ba44506a2a65606317b8d1983d24ee", "_cell_guid": "8bc6812d-6592-412e-80a5-ae339488a0bf"}, "source": ["## Make predictions on the test data and compute the probabilities for submission"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["test_df.head()"], "metadata": {"_uuid": "e0398012e2ba55ec849fe618eaf8a89c4d796623", "_cell_guid": "34f000be-b71e-4e4b-8af9-9962217e66b8"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# transform the test data using the earlier fitted vocabulary, into a document-term matrix\n", "test_dtm = vect.transform(test_df['text'])\n", "# examine the document-term matrix from X_test\n", "test_dtm"], "metadata": {"_uuid": "2e45db51b1b8d7ea951db649770e33cc725608eb", "_cell_guid": "4b22b19b-fbf5-429b-b980-fc954000dd1c"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# make author (class) predictions for test_dtm\n", "LR_y_pred = logreg.predict(test_dtm)\n", "print(LR_y_pred)"], "metadata": {"_uuid": "3290ddf6fe948d9b4a0ed26419ef94e35f295e84", "_cell_guid": "19cc4c8b-ac52-4ed8-b7b3-9d0d1d96099d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# calculate predicted probabilities for test_dtm\n", "LR_y_pred_prob = logreg.predict_proba(test_dtm)\n", "LR_y_pred_prob[:10]"], "metadata": {"_uuid": "abaefe7c47cac96b429315d480900e8e9e800f98", "_cell_guid": "0c051ec9-b17b-42b6-8449-6b692431d6c5"}}, {"cell_type": "markdown", "metadata": {"_uuid": "7ab689bd05df65f3a5e09fcefe7d80ba66615add", "_cell_guid": "73999e46-b1f2-47ed-8c13-2871dfbd8541"}, "source": ["## Train the Naive Bayes model with the entire dataset \"train.csv\""]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["nb.fit(X_dtm, y)"], "metadata": {"_uuid": "2bca382cdca372e9ba4b294b54336efcfb15c181", "_cell_guid": "7c9287c2-87f0-4344-8cf2-8cdf56c5e943"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# compute the accuracy of training data predictions\n", "y_pred_train = nb.predict(X_dtm)\n", "metrics.accuracy_score(y, y_pred_train)"], "metadata": {"_uuid": "9a6a36102a4269e6e182375eeabcf4ce63ac94ee", "_cell_guid": "c3e5dd33-a75e-44f7-b211-1cced31c033d"}}, {"cell_type": "markdown", "metadata": {"_uuid": "7e9ed7c4b05fed9aafcec960e4414ddaaac852c3", "_cell_guid": "326c0751-6036-451b-8b23-fdec9ef4631a"}, "source": ["## Make predictions on test data"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# make author (class) predictions for test_dtm\n", "NB_y_pred = nb.predict(test_dtm)\n", "print(NB_y_pred)"], "metadata": {"_uuid": "6336049258e441b9507def0b9173570abd17e04c", "_cell_guid": "fe35c157-77cf-48a3-9791-023725cd9784"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# calculate predicted probablilities for test_dtm\n", "NB_y_pred_prob = nb.predict_proba(test_dtm)\n", "NB_y_pred_prob[:10]"], "metadata": {"_uuid": "db1ff0ec20bcbdead790ace8ac5254775ca60e95", "_cell_guid": "ae1ce383-766a-4ef8-99cf-9df8f2bdfcf9"}}, {"cell_type": "markdown", "metadata": {"_uuid": "ee6d942b861235107e0d94174a8d21c2bad0e9ea", "_cell_guid": "5d0970eb-bc44-4fad-91b7-e2a23c2f986d"}, "source": ["## Create submission file"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["alpha = 0.6\n", "y_pred_prob = ((1-alpha)*LR_y_pred_prob + alpha*NB_y_pred_prob)\n", "y_pred_prob[:10]"], "metadata": {"_uuid": "a2a24232c56e0d0567493401972c11df9fbdeec9", "_cell_guid": "4865057d-c375-4f3d-a8b3-fa36b18a3f7c", "scrolled": false}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["result = pd.DataFrame(y_pred_prob, columns=['EAP','HPL','MWS'])\n", "result.insert(0, 'id', test_df['id'])\n", "result.head()"], "metadata": {"_uuid": "ee067d4b61d323f1589970031ca121364e52a9f6", "_cell_guid": "ef99474c-ee91-40af-a4e0-7f86445e6841"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Generate submission file in csv format\n", "result.to_csv('rhodium_submission_14.csv', index=False, float_format='%.20f')"], "metadata": {"_uuid": "86d7d2fa2b5671183438fe0769cfb7594eb4efa5", "collapsed": true, "_cell_guid": "bef9209e-c843-4ca6-be4f-35820ffa258d"}}, {"cell_type": "markdown", "metadata": {"_uuid": "cbbce014f73f54e9bf3e88e096ad66b89b88fcf4", "_cell_guid": "7a012382-ad72-4c76-b53c-1f756544e23e"}, "source": ["### Will work on this further.\n", "### Comments and tips are most welcomed.\n", "### Please upvote if you find it useful. Cheers!"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": [], "metadata": {"_uuid": "21a21a9ff3c174753c8d48495e058a1782bf7a76", "collapsed": true, "_cell_guid": "2b370cff-d22e-4393-94d5-98b4883a32d8"}}], "nbformat_minor": 1, "nbformat": 4}