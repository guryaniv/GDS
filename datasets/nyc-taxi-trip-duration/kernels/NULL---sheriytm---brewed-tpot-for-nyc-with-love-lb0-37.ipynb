{"cells": [{"cell_type": "markdown", "source": ["**New York City Taxi Trip Duration**\n", "\n", "This notebook goes through a simple process of extracting basic features, building a model and then make predictions.\n", "\n", "I am using the TPOT package to create a pipeline. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. I thought it is useful to add this to the list of kernels available in this competition."], "metadata": {"_uuid": "d9163d403d777f932311c57e27a9bdc2a771e90a", "_execution_state": "idle", "_cell_guid": "9631385a-8e31-4600-b207-bca33c81e0de"}}, {"cell_type": "markdown", "source": ["**Objective:**\n", "\n", "The challenge is to build a model that predicts the total ride duration of taxi trips in New York City. The primary data set is released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables.\n", "This challenge is a little different from most because kagglers are encouraged to publish additional training data that other participants can use for their predictions.\n", "\n", "First things first, lets look at the data.\n", "\n", "I borrowed some feature engineering codes from Nir Mabin's [\"Dark Taxi - TripDurationPrediction\"](https://www.kaggle.com/donniedarko/darktaxi-tripdurationprediction-lb-0-385)\n", "and Beluga's [\"From EDA to the Top\"](https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367).\n", "\n", "**If this helped you, some up-votes would be very much appreciated - it would keep me motivated.**"], "metadata": {"_uuid": "a1481fd13c682a054e31df2da430fd7fc20e655f", "_execution_state": "idle", "_cell_guid": "b896ae0a-1ad1-4472-95e7-ffbaf296f196"}}, {"cell_type": "code", "source": ["# Import libraries\n", "import os\n", "mingw_path = 'g:/mingw64/bin'\n", "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n", "import xgboost as xgb\n", "\n", "import numpy as np\n", "import os\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "color = sns.color_palette()\n", "%matplotlib inline\n", "from haversine import haversine\n", "import datetime as dt\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input/new-york-city-taxi-with-osrm/\"]).decode(\"utf8\"))"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "bc3cb35d1c95b5f81407156c85d691559df4c2fb", "_execution_state": "idle", "_cell_guid": "c6ca08fe-ba84-4292-9c09-4d6720e10af2"}}, {"cell_type": "markdown", "source": ["****Load the data****"], "metadata": {"_uuid": "62ab351c1741632952aa349cb52c91e305e3d190", "_execution_state": "idle", "_cell_guid": "cbea5ce4-b349-42db-8762-c123ed704a6a"}}, {"cell_type": "code", "source": ["# Load training data as train\n", "#trainDF = pd.read_csv('../input/train.csv')\n", "#Use the line above instead of the one below if not running on Kaggle i.e. running locally.\n", "trainDF = pd.read_csv('../input/new-york-city-taxi-with-osrm/train.csv', nrows=50000)\n", "\n", "# Load testing data as test\n", "testDF = pd.read_csv('../input/new-york-city-taxi-with-osrm/test.csv')\n", "\n", "# Print size as well as the top 5 observation of training dataset\n", "print('Size of the training set is: {} rows and {} columns'.format(*trainDF.shape))\n", "print (\"\\n\", trainDF.head(5))"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "cc8143e47759a830349c82045653235ad9e3f97b", "_execution_state": "idle", "_cell_guid": "dcd8b901-2123-4f5f-ab71-b9706df5f792"}}, {"cell_type": "markdown", "source": ["We have only 11 columns but 1458644 rows which is a reasonable size. We also have two types of ids namely id and vendor_id. Lets try to understand this data.\n", "\n", "NOTE:\n", "This statement reflects if you load the whole training data. In order to be able to run the whole script on Kaggle, I had to load only 50,000 rows of the training data to show the notebook in action."], "metadata": {"_uuid": "a5830681606fa6057a6ac5ce8b8ab6fec101aa3e", "_execution_state": "idle", "_cell_guid": "b4f97a85-79ac-4dd2-ad17-853fa59f1aa8"}}, {"cell_type": "markdown", "source": ["****Understanding Data****"], "metadata": {"_uuid": "78c5c414e2aa9aead7a639ee4793d064a643d7e8", "_execution_state": "idle", "_cell_guid": "19117eee-b89a-4a91-a71a-40922ecb318c"}}, {"cell_type": "code", "source": ["# Look at the summary of numerical variables for train data set\n", "df = trainDF.describe()\n", "print (df)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "9099489cd1c546977df6a8ec00eb59faf9e78d48", "scrolled": true, "_execution_state": "idle", "_cell_guid": "126c91ff-1488-4852-af9f-b915cde786fa"}}, {"cell_type": "code", "source": ["# Print the shape of the data\n", "print(\"Train shape : \", trainDF.shape)\n", "print(\"Test shape : \", testDF.shape)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "25fc29b2ad674f3d46741bb3de18e77cc8905813", "_execution_state": "idle", "_cell_guid": "f2e90c7b-0a11-40f1-b1f6-ab7c0708ede1"}}, {"cell_type": "markdown", "source": ["**What are the data types ?**"], "metadata": {"_uuid": "3f14dd4ad48140155bc9db2dae667e3984f13bf8", "_execution_state": "idle", "_cell_guid": "7b7efd88-9742-4537-882b-9b6c4dc6db51"}}, {"cell_type": "code", "source": ["dtypeDF = trainDF.dtypes.reset_index()\n", "dtypeDF.columns = [\"Count\", \"Column Type\"]\n", "dtypeDF.groupby(\"Column Type\").aggregate('count').reset_index()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "b853e6e929a3ae90e40118d10be8fb1a7e46cc9e", "_execution_state": "idle", "_cell_guid": "111ef821-d49a-4a88-9c64-0db1207f3c48"}}, {"cell_type": "markdown", "source": ["There are 3 integers, 4 floats and four categorical variables."], "metadata": {"_uuid": "1b81a39c4261b0b4183c58755f212ceae5081145", "_execution_state": "idle", "_cell_guid": "9a6952ab-11de-40a1-b3ee-2a34ba5b01b0"}}, {"cell_type": "markdown", "source": ["****Understanding distribution of target variable i.e trip duration.****"], "metadata": {"_uuid": "3f42d244578085349383ed65e49848986eb47f81", "_execution_state": "idle", "_cell_guid": "27d127dc-f6d9-4f78-9415-f5cacbc8f423"}}, {"cell_type": "code", "source": ["# Plot the trip duration\n", "plt.figure(figsize=(8,6))\n", "plt.scatter(range(trainDF.shape[0]), np.sort(trainDF.trip_duration.values))\n", "plt.xlabel('index', fontsize=12)\n", "plt.ylabel('trip duration', fontsize=12)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "28810e6fb387ee629e4fa2211a90396cb2668ddd", "_execution_state": "idle", "_cell_guid": "bdbe5553-b1ef-4fc8-a3fd-c9209e358def"}}, {"cell_type": "markdown", "source": ["We can see a few outliers with one way out there compared to the other 3 outliers. Lets remove those points so that we can zoom in."], "metadata": {"_uuid": "cbcb72cd175bd1b7c2b7d8272d542955ba3266ae", "_execution_state": "idle", "_cell_guid": "7e720e22-60d6-4dc9-a75f-1ab4ac800e84"}}, {"cell_type": "code", "source": ["# in train dataset some trip duration are very high (I consider them outliers and remove them before replotting it)\n", "th = trainDF.trip_duration.quantile(0.99)\n", "tempDF = trainDF\n", "tempDF = tempDF[tempDF['trip_duration'] < th]\n", "plt.figure(figsize=(8,6))\n", "plt.scatter(range(tempDF.shape[0]), np.sort(tempDF.trip_duration.values))\n", "plt.xlabel('index', fontsize=12)\n", "plt.ylabel('trip duration', fontsize=12)\n", "plt.show()\n", "\n", "del tempDF"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "ba00d0d2b723f12f7b02d93b7ec3f7b280b98114", "_execution_state": "idle", "_cell_guid": "8a2c8f87-bff5-468c-85e1-c5f7bc065274"}}, {"cell_type": "code", "source": ["# Lets remove the outliers from the train data target\n", "trainDF = trainDF[trainDF['trip_duration'] < th]"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "406447b01bc8f9879a36815974fdc8d494de7b0a", "_execution_state": "idle", "_cell_guid": "3e48b250-389c-4d4a-9f68-c23712eb37c1", "collapsed": true}}, {"cell_type": "markdown", "source": ["Dealing with missing values"], "metadata": {"_uuid": "3bde232817e116ad3e5a2723e65840a8a920bc47", "_execution_state": "idle", "_cell_guid": "e7a0a6e6-d0d9-4cc8-9f9b-a84a28cc605b"}}, {"cell_type": "code", "source": ["# Number of variables with missing values\n", "variables_missing_value = trainDF.isnull().sum()\n", "variables_missing_value "], "outputs": [], "execution_count": null, "metadata": {"_uuid": "8a30f78d5efbb2a3062d3f40474855ade2d5d2ae", "_execution_state": "idle", "_cell_guid": "e2e1463b-4fcc-446e-9e52-8cdeb922f891"}}, {"cell_type": "markdown", "source": ["Nice, there are no missing values in the training data. What about the test data?"], "metadata": {"_uuid": "f9e1b25035a3b98b34a2e5abe6127aa0cb504d20", "_execution_state": "idle", "_cell_guid": "8b68d25b-05b5-4949-9c23-23110af1ae80"}}, {"cell_type": "code", "source": ["variables_missing_value = testDF.isnull().sum()\n", "variables_missing_value "], "outputs": [], "execution_count": null, "metadata": {"_uuid": "94cf2e30978d5d279268f7166db39f6b33716108", "_execution_state": "idle", "_cell_guid": "b785004b-94b9-4aac-9f50-3a691e1ba8bd"}}, {"cell_type": "markdown", "source": ["There are no missing values in the test data either. So lets get to the modelling bit and brew that tea for NYC.\n", "\n", "First we have to do a quick data pre-processing and feature selection."], "metadata": {"_uuid": "9d7cc45a9f840931de9d4601106905c36bdb8321", "_execution_state": "idle", "_cell_guid": "e3fd1357-f702-4d72-9517-2d86b4475f9d"}}, {"cell_type": "markdown", "source": ["****Feature Generation****\n", "\n", "Here we make a very simple data pre-processing and feature selection."], "metadata": {"_uuid": "d287f589fb044e91701ecfbdc76df8825827a800", "_execution_state": "idle", "_cell_guid": "73fd5d9c-db98-4a7c-9860-777c052db459"}}, {"cell_type": "code", "source": ["from sklearn.decomposition import PCA\n", "from sklearn.cluster import MiniBatchKMeans\n", "\n", "t0 = dt.datetime.now()\n", "\n", "train = trainDF\n", "test = testDF\n", "del trainDF, testDF\n", "\n", "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n", "test['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\n", "train.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\n", "test.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date\n", "train['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\n", "train['store_and_fwd_flag'] = 1 * (train.store_and_fwd_flag.values == 'Y')\n", "test['store_and_fwd_flag'] = 1 * (test.store_and_fwd_flag.values == 'Y')\n", "train['check_trip_duration'] = (train['dropoff_datetime'] - train['pickup_datetime']).map(lambda x: x.total_seconds())\n", "duration_difference = train[np.abs(train['check_trip_duration'].values  - train['trip_duration'].values) > 1]\n", "print('Trip_duration and datetimes are ok.') if len(duration_difference[['pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration']]) == 0 else print('Ooops.')\n", "\n", "train['trip_duration'].describe()\n", "\n", "train['log_trip_duration'] = np.log(train['trip_duration'].values + 1)\n", "\n", "# Feature Extraction\n", "coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n", "                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n", "                    test[['pickup_latitude', 'pickup_longitude']].values,\n", "                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n", "\n", "pca = PCA().fit(coords)\n", "train['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\n", "train['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\n", "train['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n", "train['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n", "test['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\n", "test['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\n", "test['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n", "test['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n", "\n", "# Distance\n", "def haversine_array(lat1, lng1, lat2, lng2):\n", "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n", "    AVG_EARTH_RADIUS = 6371  # in km\n", "    lat = lat2 - lat1\n", "    lng = lng2 - lng1\n", "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n", "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n", "    return h\n", "\n", "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n", "    a = haversine_array(lat1, lng1, lat1, lng2)\n", "    b = haversine_array(lat1, lng1, lat2, lng1)\n", "    return a + b\n", "\n", "def bearing_array(lat1, lng1, lat2, lng2):\n", "    AVG_EARTH_RADIUS = 6371  # in km\n", "    lng_delta_rad = np.radians(lng2 - lng1)\n", "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n", "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n", "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n", "    return np.degrees(np.arctan2(y, x))\n", "\n", "train.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "train.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "train.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "train.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])\n", "\n", "test.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n", "test.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n", "test.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n", "test.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])\n", "\n", "train.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2\n", "train.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2\n", "test.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2\n", "test.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2\n", "\n", "# Datetime features\n", "train.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday\n", "train.loc[:, 'pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear\n", "train.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\n", "train.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\n", "train.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n", "train.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n", "\n", "test.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\n", "test.loc[:, 'pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\n", "test.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\n", "test.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\n", "test.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n", "test.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']\n", "\n", "train.loc[:,'week_delta'] = train['pickup_datetime'].dt.weekday + \\\n", "    ((train['pickup_datetime'].dt.hour + (train['pickup_datetime'].dt.minute / 60.0)) / 24.0)\n", "test.loc[:,'week_delta'] = test['pickup_datetime'].dt.weekday + \\\n", "    ((test['pickup_datetime'].dt.hour + (test['pickup_datetime'].dt.minute / 60.0)) / 24.0)\n", "\n", "# Make time features cyclic\n", "train.loc[:,'week_delta_sin'] = np.sin((train['week_delta'] / 7) * np.pi)**2\n", "train.loc[:,'hour_sin'] = np.sin((train['pickup_hour'] / 24) * np.pi)**2\n", "\n", "test.loc[:,'week_delta_sin'] = np.sin((test['week_delta'] / 7) * np.pi)**2\n", "test.loc[:,'hour_sin'] = np.sin((test['pickup_hour'] / 24) * np.pi)**2\n", "\n", "# Speed\n", "train.loc[:, 'avg_speed_h'] = 1000 * train['distance_haversine'] / train['trip_duration']\n", "train.loc[:, 'avg_speed_m'] = 1000 * train['distance_dummy_manhattan'] / train['trip_duration']\n", "\n", "train.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 3)\n", "train.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 3)\n", "# Average speed for regions\n", "gby_cols = ['pickup_lat_bin', 'pickup_long_bin']\n", "coord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\n", "coord_count = train.groupby(gby_cols).count()[['id']].reset_index()\n", "coord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\n", "coord_stats = coord_stats[coord_stats['id'] > 100]\n", "\n", "train.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 2)\n", "train.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 2)\n", "train.loc[:, 'center_lat_bin'] = np.round(train['center_latitude'], 2)\n", "train.loc[:, 'center_long_bin'] = np.round(train['center_longitude'], 2)\n", "train.loc[:, 'pickup_dt_bin'] = (train['pickup_dt'] // (3 * 3600))\n", "test.loc[:, 'pickup_lat_bin'] = np.round(test['pickup_latitude'], 2)\n", "test.loc[:, 'pickup_long_bin'] = np.round(test['pickup_longitude'], 2)\n", "test.loc[:, 'center_lat_bin'] = np.round(test['center_latitude'], 2)\n", "test.loc[:, 'center_long_bin'] = np.round(test['center_longitude'], 2)\n", "test.loc[:, 'pickup_dt_bin'] = (test['pickup_dt'] // (3 * 3600))\n", "\n", "# Clustering\n", "sample_ind = np.random.permutation(len(coords))[:500000]\n", "kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])\n", "\n", "train.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])\n", "train.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])\n", "test.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\n", "test.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])\n", "t1 = dt.datetime.now()\n", "print('Time till clustering: %i seconds' % (t1 - t0).seconds)\n", "\n", "# Temporal and geospatial aggregation\n", "for gby_col in ['pickup_hour', 'pickup_date', 'pickup_dt_bin',\n", "               'pickup_week_hour', 'pickup_cluster', 'dropoff_cluster']:\n", "    gby = train.groupby(gby_col).mean()[['avg_speed_h', 'avg_speed_m', 'log_trip_duration']]\n", "    gby.columns = ['%s_gby_%s' % (col, gby_col) for col in gby.columns]\n", "    train = pd.merge(train, gby, how='left', left_on=gby_col, right_index=True)\n", "    test = pd.merge(test, gby, how='left', left_on=gby_col, right_index=True)\n", "\n", "for gby_cols in [['center_lat_bin', 'center_long_bin'],\n", "                 ['pickup_hour', 'center_lat_bin', 'center_long_bin'],\n", "                 ['pickup_hour', 'pickup_cluster'],  ['pickup_hour', 'dropoff_cluster'],\n", "                 ['pickup_cluster', 'dropoff_cluster']]:\n", "    coord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\n", "    coord_count = train.groupby(gby_cols).count()[['id']].reset_index()\n", "    coord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\n", "    coord_stats = coord_stats[coord_stats['id'] > 100]\n", "    coord_stats.columns = gby_cols + ['avg_speed_h_%s' % '_'.join(gby_cols), 'cnt_%s' %  '_'.join(gby_cols)]\n", "    train = pd.merge(train, coord_stats, how='left', on=gby_cols)\n", "    test = pd.merge(test, coord_stats, how='left', on=gby_cols)\n", "\n", "group_freq = '60min'\n", "df_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\n", "train.loc[:, 'pickup_datetime_group'] = train['pickup_datetime'].dt.round(group_freq)\n", "test.loc[:, 'pickup_datetime_group'] = test['pickup_datetime'].dt.round(group_freq)\n", "\n", "# Count trips over 60min\n", "df_counts = df_all.set_index('pickup_datetime')[['id']].sort_index()\n", "df_counts['count_60min'] = df_counts.isnull().rolling(group_freq).count()['id']\n", "train = train.merge(df_counts, on='id', how='left')\n", "test = test.merge(df_counts, on='id', how='left')\n", "\n", "# Count how many trips are going to each cluster over time\n", "dropoff_counts = df_all \\\n", "    .set_index('pickup_datetime') \\\n", "    .groupby([pd.TimeGrouper(group_freq), 'dropoff_cluster']) \\\n", "    .agg({'id': 'count'}) \\\n", "    .reset_index().set_index('pickup_datetime') \\\n", "    .groupby('dropoff_cluster').rolling('240min').mean() \\\n", "    .drop('dropoff_cluster', axis=1) \\\n", "    .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n", "    .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'dropoff_cluster_count'})\n", "\n", "train['dropoff_cluster_count'] = train[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\n", "test['dropoff_cluster_count'] = test[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\n", "\n", "# Count how many trips are going from each cluster over time\n", "df_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\n", "pickup_counts = df_all \\\n", "    .set_index('pickup_datetime') \\\n", "    .groupby([pd.TimeGrouper(group_freq), 'pickup_cluster']) \\\n", "    .agg({'id': 'count'}) \\\n", "    .reset_index().set_index('pickup_datetime') \\\n", "    .groupby('pickup_cluster').rolling('240min').mean() \\\n", "    .drop('pickup_cluster', axis=1) \\\n", "    .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n", "    .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'pickup_cluster_count'})\n", "\n", "train['pickup_cluster_count'] = train[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)\n", "test['pickup_cluster_count'] = test[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)\n", "\n", "# For this particular problem we can add OSRM ([Open Source Routing Machine](http://project-osrm.org/ \n", "# \"OSRM\")) features. This data contains the fastest routes from specific starting points in NY.\n", "fr1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv', usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps'])\n", "fr2 = pd.read_csv('../input/new-york-city-taxi-with-osrm//fastest_routes_train_part_2.csv', usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n", "test_street_info = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv',\n", "                               usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n", "train_street_info = pd.concat((fr1, fr2))\n", "train = train.merge(train_street_info, how='left', on='id')\n", "test = test.merge(test_street_info, how='left', on='id')\n", "\n", "\n"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "34db7cb4bc2f7175a634e065edb328f781bac53d", "_cell_guid": "db13e472-90de-41b4-8a67-588bca58d737"}}, {"cell_type": "code", "source": ["feature_names = list(train.columns)\n", "print(np.setdiff1d(train.columns, test.columns))\n", "do_not_use_for_training = ['id', 'log_trip_duration', 'pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration',\n", "                           'pickup_date', 'avg_speed_h', 'avg_speed_m', 'pickup_lat_bin', 'pickup_long_bin',\n", "                           'center_lat_bin', 'center_long_bin', 'pickup_dt_bin', 'pickup_datetime_group']\n", "feature_names = [f for f in train.columns if f not in do_not_use_for_training]\n", "# print(feature_names)\n", "print('We have %i features.' % len(feature_names))\n", "train[feature_names].count()\n", "#ytrain = np.log(train['trip_duration'].values + 1)\n", "#ytrain = train[train['trip_duration'].notnull()]['trip_duration_log'].values\n", "\n", "\n", "t1 = dt.datetime.now()\n", "print('Feature extraction time: %i seconds' % (t1 - t0).seconds)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "989ec5b4871eadac9f097bf8d470358c9b902b8a", "_cell_guid": "e577d3a7-2813-49b1-bd16-25f7c496f8e8"}}, {"cell_type": "markdown", "source": ["****Feature check before modeling****"], "metadata": {"_uuid": "8f03a44ff9f5a35ccc6b7d4886ad56017101fde2", "_cell_guid": "aecd2d05-b748-42f2-9383-f1b602f027ce"}}, {"cell_type": "code", "source": ["feature_stats = pd.DataFrame({'feature': feature_names})\n", "feature_stats.loc[:, 'train_mean'] = np.nanmean(train[feature_names].values, axis=0).round(4)\n", "feature_stats.loc[:, 'test_mean'] = np.nanmean(test[feature_names].values, axis=0).round(4)\n", "feature_stats.loc[:, 'train_std'] = np.nanstd(train[feature_names].values, axis=0).round(4)\n", "feature_stats.loc[:, 'test_std'] = np.nanstd(test[feature_names].values, axis=0).round(4)\n", "feature_stats.loc[:, 'train_nan'] = np.mean(np.isnan(train[feature_names].values), axis=0).round(3)\n", "feature_stats.loc[:, 'test_nan'] = np.mean(np.isnan(test[feature_names].values), axis=0).round(3)\n", "feature_stats.loc[:, 'train_test_mean_diff'] = np.abs(feature_stats['train_mean'] - feature_stats['test_mean']) / np.abs(feature_stats['train_std'] + feature_stats['test_std'])  * 2\n", "feature_stats.loc[:, 'train_test_nan_diff'] = np.abs(feature_stats['train_nan'] - feature_stats['test_nan'])\n", "feature_stats = feature_stats.sort_values(by='train_test_mean_diff')\n", "feature_stats[['feature', 'train_test_mean_diff']].tail()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "579ca306990e7173c892d40b18a2f32265954905", "_cell_guid": "acd51790-de3c-496f-94ed-1c2e09227f5a"}}, {"cell_type": "markdown", "source": ["****Train a simple classifier****\n", "\n", "We use the TPOT package to handle the cross validation and hyperparameters for us"], "metadata": {"_uuid": "284a903e1c8fdd77376dcb0ad40c80dda206375f", "_execution_state": "idle", "_cell_guid": "88e7cecd-c924-49ce-9148-1c60caf7b158"}}, {"cell_type": "code", "source": ["from tpot import TPOTRegressor\n", "auto_classifier = TPOTRegressor(generations=3, population_size=9, verbosity=2)\n", "from sklearn.model_selection import train_test_split"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "a78cf0849fb37f4f853863b9e2f9d44fdaf1f0e1", "_execution_state": "idle", "_cell_guid": "fb6ae6a0-249a-42e3-9bd6-282505f9bf41", "collapsed": true}}, {"cell_type": "markdown", "source": ["**Split the training data to train and validate**"], "metadata": {"_uuid": "3aca2423d4e71da6603a3d9caa581df0abbae451", "_execution_state": "idle", "_cell_guid": "aa28099d-91cb-4f33-b802-193b83f94a8b"}}, {"cell_type": "code", "source": ["# K Fold Cross Validation\n", "from sklearn.model_selection import KFold\n", "\n", "X = train[feature_names].values\n", "y = np.log(train['trip_duration'].values + 1)  \n", "\n", "\n", "kf = KFold(n_splits=10)\n", "kf.get_n_splits(X)\n", "\n", "print(kf)  \n", "\n", "KFold(n_splits=10, random_state=None, shuffle=False)\n", "for train_index, test_index in kf.split(X):\n", "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n", "    X_train, X_valid = X[train_index], X[test_index]\n", "    y_train, y_valid = y[train_index], y[test_index]"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "4aa44f013328df33350cebc54c27169ef0e372a7", "_execution_state": "idle", "_cell_guid": "b7603ffe-f702-456a-821f-4f1d11f19533"}}, {"cell_type": "code", "source": ["auto_classifier.fit(X_train, y_train)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "7512c52267292580a375a03f5bd973318c77c6cc", "_execution_state": "idle", "_cell_guid": "3daca28b-7ac0-4f0b-a235-f1929698c9a4"}}, {"cell_type": "code", "source": ["#print(\"The cross-validation MSE\")\n", "#print(auto_classifier.score(X_valid, y_valid))"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "d110174069ff841b8d69602022051c0c4ba09149", "_execution_state": "idle", "_cell_guid": "a760e6e4-ba80-4420-b046-3620c63d7680", "collapsed": true}}, {"cell_type": "code", "source": ["# Now do the prediction\n", "test_result = auto_classifier.predict(test[feature_names].values)\n", "sub = pd.DataFrame()\n", "sub['id'] = test['id']\n", "sub['trip_duration'] = np.exp(test_result)\n", "sub.to_csv('NYCTaxi_TpotModels.csv', index=False)\n", "sub.head()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "eb674e18fcd03006bbe7bb7f70cba688e6ca349c", "_execution_state": "idle", "_cell_guid": "205919a7-80bb-477f-93c8-95f4f75ed399"}}, {"cell_type": "code", "source": ["# Export the model\n", "auto_classifier.export('NYCTaxi_pipeline.py')"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "30c05fa69862c8167d67220d5eaacc401d982db3", "_execution_state": "idle", "_cell_guid": "573a2b82-cced-4424-9f10-7a8e2276cedf", "collapsed": true}}, {"cell_type": "markdown", "source": ["That is it for now. You can run locally with more number of generations, population, etc. to get a better result. Because of Kaggle time limitations I could not choose parameters that take longer to run.\n", "\n", "I hope you like it. If so please up-vote. \n", "\n", "**Stay tuned !!!**"], "metadata": {"_uuid": "433f20ca962f987564cf825e31a3bfead0967fea", "_execution_state": "idle", "_cell_guid": "ff96a60b-c2ef-4731-a9eb-da327e63e697"}}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python", "file_extension": ".py", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}}