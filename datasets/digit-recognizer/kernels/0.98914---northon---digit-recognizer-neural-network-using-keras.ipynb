{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"cb69d40b29c5c8d35adaa1c0e9f90f34f973ec4e"},"cell_type":"markdown","source":"A Convolutional Neural Network is better suited when you have data that doesn’t neatly align into columns. This is typical for image processing. CNN's are less sensitive to where in the image the pattern is that we're looking for.\n\nWith a multi-layer perceptron, we achieved around 97% accuracy. Let's see if we can beat that.\n\nWhy Keras on top of TensorFlow?\nKeras is a layer on top of TensorFlow that makes things a lot easier. Not only is it easier to use, it's easier to tune.\n\nI'll start by importing the stuff I need, including the new layer needed in a CNN:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e62c9fc9028c9f038d288e2c02919a8994ec6be5"},"cell_type":"code","source":"import keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import RMSprop","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"7130f6e40b7f4146d23050a0c87b6aeebb1f4d3a"},"cell_type":"markdown","source":"## Downloading the data"},{"metadata":{"trusted":true,"_uuid":"030aa406d95c15938982e0443f7ef4f96b5039b7"},"cell_type":"code","source":"import pandas as pd\ninput_file = (\"../input/train.csv\")\n\ndf_train=pd.read_csv(input_file)\ndf_train.shape # (42000, 785)\ndf_train.head()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9acd70e87b63eefc0e08898333de7978a2ad8645"},"cell_type":"code","source":"import pandas as pd\ninput_file = (\"../input/test.csv\")\n\ndf_test=pd.read_csv(input_file)\ndf_test.shape # gives (28000, 784)\ndf_test.head() #gives a dataframe","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"c79c429d60c1c0c44eace5053e67a0e258e7a8d3"},"cell_type":"markdown","source":"### Making a Train-Test -split\nThe original test.csv -file does not contain any label data; it cannot be used in testing the accuracy of the model. I need therefore to use the original train.csv -file for creating a Train- and Test dataset. Using a standard 80/20 -split the sizes of the files will be as follows\n\nTrain: 0.80 x 42000 = 33600<br>\nTest: 0.20 x 42000 = 8400"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"204514e677c1682dd2a8e855be209be79b5d81bf"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split # the needed split-function imported from scikit-learn\n\ntrain_set, test_set = train_test_split(df_train, test_size=0.20, random_state=42)\n\nX_train_set = train_set.drop(['label'], axis=1) #Dropping 'label', the predicted variable \ny_train_set = train_set['label'] # keeping 'label', the predicted variable \n\nX_test_set = test_set.drop(['label'], axis=1)\ny_test_set = test_set['label']","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"9bcd23cff7ab67d94893210366ee35f8bd6911d2"},"cell_type":"markdown","source":"### Converting the dataframes into numpy arrays\nSince I need to use the reshape function (see following cell), which cannot be used on a dataframe, I need to convert the dataframes first into numpy arrays."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b321bcaf59a47b8c094febf35e737cc041c3fa48"},"cell_type":"code","source":"df_train_label_array = y_train_set.as_matrix() #creates a numpy array of the df\ndf_train_image_array = X_train_set.as_matrix() #creates a numpy array of the df\n\ndf_test_image_array = X_test_set.as_matrix()\ndf_test_label_array = y_test_set.as_matrix()","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"1fd41983a84def5a80866f5dae18af9e3824a486"},"cell_type":"markdown","source":"### 2D images vs. flattened 1D streams\nWe need to shape the data differently than in an \"ordinary\" Neural Network. Since we're treating the data as 2D images of 28x28 pixels instead of a flattened stream of 784 pixels, we need to shape it accordingly. Depending on the data format Keras is set up for, this may be 1x28x28 or 28x28x1 (the \"1\" indicates a single color channel, as this is just grayscale. If we were dealing with color images, it would be 3 instead of 1 since we'd have red, green, and blue color channels)"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b9597e7a5d4ceb3dbf5f038efb678e3705caef4b"},"cell_type":"code","source":"from keras import backend as K\n\nif K.image_data_format() == 'channels_first':\n    train_images = df_train_image_array.reshape(df_train_image_array.shape[0], 1, 28, 28)\n    test_images =  df_test_image_array.reshape(df_test_image_array.shape[0], 1, 28, 28)\n    input_shape = (1, 28, 28)\nelse:\n    train_images = df_train_image_array.reshape(df_train_image_array.shape[0], 28, 28, 1)\n    test_images = df_test_image_array.reshape(df_test_image_array.shape[0], 28, 28, 1)\n    input_shape = (28, 28, 1)\n    \ntrain_images = train_images.astype('float32')\ntest_images = test_images.astype('float32')\ntrain_images /= 255\ntest_images /= 255","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9f0e8871d89834c785d1b9126975663fbe821af"},"cell_type":"code","source":"test_images.shape","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"362d0f507e81e755818a1d496293280b94a68300"},"cell_type":"markdown","source":"### Converting the Train and Test labels\nNext I need to convert my train and test labels to be categorical in *one-hot vector* format. One-hot encoding is a process by which categorical variables - here: integers - are converted into a form that could be provided to ML algorithms to do a better job in prediction."},{"metadata":{"trusted":true,"_uuid":"2cf31bd82032f63165d9fc555296378d8ad6bd49"},"cell_type":"code","source":"train_labels = keras.utils.to_categorical(df_train_label_array, 10)\ntest_labels = keras.utils.to_categorical(df_test_label_array, 10)\ntest_labels","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"d284233b75d33143aaded032db45568c3ad89976"},"cell_type":"markdown","source":"### As a sanity check let's print out a few of the training images with its label:"},{"metadata":{"trusted":true,"_uuid":"6eaca674c3f28fac14a20321d0deb1634616095d"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef display_sample(num):\n    #Print the one-hot array of this sample's label \n    print(train_labels[num])  \n    #Print the label converted back to a number\n    label = train_labels[num].argmax(axis=0)\n    #Reshape the 768 values to a 28x28 image\n    image = train_images[num].reshape([28,28])\n    plt.title('Sample: %d  Label: %d' % (num, label))\n    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n    plt.show()\n    \ndisplay_sample(1111) #the 1111th image in the Training set\ndisplay_sample(2222) #the 2222nd image in the Training set\ndisplay_sample(3333) #the 3333rd image in the Training set","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"43304f25042ef6a74aa5e2b9539212c56c76af14"},"cell_type":"markdown","source":"Now for the meat of the problem. Setting up a convolutional neural network involves more layers. Not all of these are strictly necessary; you could run without pooling and dropout, but those extra steps help avoid overfitting and help things run faster.\n\nI'll start with a 2D convolution of the image - it's set up to take 32 windows, or \"filters\", of each image, each filter being 3x3 in size.\n\nWe then run a second convolution on top of that with 64 3x3 windows. Please note! This topology is just what comes recommended within Keras's own examples. Again you want to re-use previous research whenever possible while tuning CNN's, as it is hard to do.\n\nNext I apply a MaxPooling2D layer that takes the maximum of each 2x2 result to distill the results down into something more manageable.\n\nA dropout filter is then applied to prevent overfitting.\n\nNext I flatten the 2D layer I have at this stage into a 1D layer. So at this point I can just pretend we have a traditional multi-layer perceptron, an \"ordinary\" neural network\n\n... and feed that into a hidden, flat layer of 128 units.\n\nI then apply dropout again to further prevent overfitting.\n\nAnd finally, I feed that into our final 10 units where softmax is applied to choose our category of 0-9."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"85118c9d35cfcb50b20e41915c26708e65848549"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\n# 64 3x3 kernels\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\n# Reduce by taking the max of each 2x2 block\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# Dropout to avoid overfitting\nmodel.add(Dropout(0.25))\n# Flatten the results to one dimension for passing into our final layer\nmodel.add(Flatten())\n# A hidden layer to learn with\nmodel.add(Dense(128, activation='relu'))\n# Another dropout\nmodel.add(Dropout(0.5))\n# Final categorization from 0-9 with softmax\nmodel.add(Dense(10, activation='softmax'))","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"b032724700c7a523af2a55dc43ac99a915dfdd11"},"cell_type":"markdown","source":"### Let's double check the model description:"},{"metadata":{"trusted":true,"_uuid":"7869bd89bfed93838ed030d54ec204a6a0d1e615"},"cell_type":"code","source":"model.summary()","execution_count":24,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"4f5f56cad2b6684695d61d624e65f33fc4d535ab"},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"501fd2fb5f8985d02d23ff19358dacab2e984296"},"cell_type":"markdown","source":"And now for the training of the model. To make things go a little faster, I'll use batches of 32.\n\n## Warning !\n\n**Running these 10 epochs on a CPU took me around 30 minutes.** Don't run the next block unless you can tie up your computer for at least half-an-hour. It will print progress as each epoch is run, but each epoch can take several minutes. Perhaps runnig this on a GPU would make it faster?"},{"metadata":{"trusted":true,"_uuid":"416da8366bc09ddfb9c6046edcc12f35c54c997f"},"cell_type":"code","source":"history = model.fit(train_images, train_labels,\n                    batch_size=32,\n                    epochs=10,\n                    verbose=2,\n                    validation_data=(test_images, test_labels))","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"e243b622aa740e85ea1c715b9eb1af39b75799da"},"cell_type":"markdown","source":"#### Was it worth the wait? "},{"metadata":{"trusted":true,"_uuid":"178cc984718a2be0744c79ad881fcd6432c45516"},"cell_type":"code","source":"score = model.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"b8b105b62732abb0858ef8bd9f3d448ba30cd854"},"cell_type":"markdown","source":"Around 99%! And that's with just 10 epochs! It came at a significant cost in terms of computing power, but when you start distributing things over multiple computers each with multiple GPU's, that cost might start to feel less bad. If you're building something where life and death are on the line, like a self-driving car, every fraction of a percent matters!<br><br>\nBefore submitting this to Kaggle.com the  original test file *df_test* needs to be converted into a numpy array to be able  to receive the predictions. Testing here first with a numpy array conversion: "},{"metadata":{"trusted":true,"_uuid":"0a6dbe34b3db418a04bb251da43a575ee3ca27eb"},"cell_type":"code","source":"df_test_RESULTS = df_test.as_matrix()\ndf_test_RESULTS.shape","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58ef1c95eeba1557425c7151a154ea4b4abf13c3"},"cell_type":"code","source":"testX = df_test_RESULTS.reshape(df_test_RESULTS.shape[0], 28, 28, 1)\ntestX = testX.astype(float)\ntestX /= 255.0\ntestX.shape","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"20119be55c725d27c2f7604cc8a3b51d7fc299cc"},"cell_type":"markdown","source":"### Kaggle submission cell "},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"08b8cb59a7d66d449465b3bd844e4cb1ec54c21b"},"cell_type":"code","source":"predictions = model.predict_classes(testX, verbose=0)\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"DR.csv\", index=False, header=True)\n","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}