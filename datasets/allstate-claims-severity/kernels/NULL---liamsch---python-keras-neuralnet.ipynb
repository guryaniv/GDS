{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d07426dc-7c90-312c-6dc7-462d68d24145"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "84a1d0c7-a0ef-c8e2-46fa-19f8f60fc6fd"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Reshape, Flatten\n",
        "from keras.regularizers import l2, activity_l2\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "np.random.seed(1)  # for reproducibility\n",
        "\n",
        "train_in = pd.read_csv('../input/train.csv')\n",
        "test_in = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b6909ee3-bc53-eb89-f925-8e2684c683e3"
      },
      "outputs": [],
      "source": [
        "### Create dummy variables and prepare training/test data frames...\n",
        "\n",
        "# Save our target variable column before we drop it\n",
        "y_train = train_in.iloc[:, -1]\n",
        "\n",
        "# Stack training and test data into one big data frame so we cover all categories found in both\n",
        "data = pd.concat((train_in.iloc[:, :-1], test_in))\n",
        "\n",
        "# Get dummy variables for all categorical columns\n",
        "colnames = []\n",
        "X_data = []\n",
        "\n",
        "# For each categorical column...\n",
        "for c in [i for i in data.columns[1:-1] if 'cat' in i]:\n",
        "    # Get dummy variables for that column\n",
        "    dummies = pd.get_dummies(data[c])\n",
        "    # Drop the last dummy, as its value is implied by the others (all 0's = 1)\n",
        "    dummies = dummies.iloc[:, :-1].values.astype(np.bool)\n",
        "    X_data.append(dummies)\n",
        "    # Create column names for those dummy variables\n",
        "    colnames += [c + '_' + str(i) for i in range(dummies.shape[1])]\n",
        "    \n",
        "# Stack all dummy variables into big dataframe with the colnames\n",
        "X_data = pd.DataFrame(np.hstack(X_data), columns=colnames)\n",
        "\n",
        "# Drop any columns with only 1 value (so drop unused categories, if any)\n",
        "X_data = X_data.iloc[:, [len(pd.unique(X_data.loc[:,c]))>1 for c in X_data.columns]]\n",
        "\n",
        "# Get the other (continuous) columns\n",
        "X_data_cont = np.vstack([data[c].values.astype(np.float32) \\\n",
        "                         for c in data.columns[1:-1] if 'cat' not in c]).T\n",
        "\n",
        "# Final data frame is the dummy variables + the continuous variables\n",
        "X_data = X_data.join(pd.DataFrame(X_data_cont, \n",
        "                    columns=[c for c in data.columns[1:-1] if 'cat' not in c]))\n",
        "\n",
        "# Split into train and test data frames\n",
        "train = X_data[:len(y_train)].join(y_train)\n",
        "test = X_data[len(y_train):]\n",
        "\n",
        "# Create X train and y train arrays for input to NN\n",
        "Xt = train.iloc[:, :-1].values\n",
        "yt = train.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2800811-7996-65ce-06c3-b63d76b614fe"
      },
      "outputs": [],
      "source": [
        "### Define and compile a fairly deep neural network...\n",
        "dropout_p=0.5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1, input_shape=[Xt.shape[1]]))\n",
        "model.add(Dense(2048))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout_p))\n",
        "model.add(Dense(1024))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout_p))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout_p))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout_p))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()\n",
        "model.compile(loss='mae', optimizer=Adam())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "798a6519-c295-a059-c272-6791b48c5b4a"
      },
      "outputs": [],
      "source": [
        "### Fit the neural network model (takes 3-4 mins per epoch, 1 epoch seems to provide best results,\n",
        "### though I only tried with 1 or 2)\n",
        "history = model.fit(Xt, yt,\n",
        "                    batch_size=96,\n",
        "                    nb_epoch=1,\n",
        "                    verbose=2, \n",
        "                    #validation_data=(Xv.values, yv.values),\n",
        "                    shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9ebd0e3f-8a84-744b-11fb-84c3011b35ab"
      },
      "outputs": [],
      "source": [
        "### Predict the test set\n",
        "# This NN submission got me to 1214 on LB\n",
        "# Taking logs of the loss before training and then exponentiating the predictions degraded performance\n",
        "# Running two epochs also degraded performance\n",
        "preds_nn = model.predict(test.values)\n",
        "submission_nn = pd.DataFrame(test_in['id'])\n",
        "submission_nn['loss'] = preds_nn\n",
        "submission_nn.to_csv('submission_nn.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}