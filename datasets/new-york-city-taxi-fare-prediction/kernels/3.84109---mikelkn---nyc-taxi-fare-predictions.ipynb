{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', nrows = 1000000)\n# out of the 55million + data, i have picked a sample and will be working with it\n\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"566b4a49a01547d9d041961b3397c71fa931b219"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7cccf46ee300c56dff3d91717e15b307e9fdbc8"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d47079142f59eb139286eba32498ab77859b9b94"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e7c0fa440877ee613cfb3b06312381c2e351801d"},"cell_type":"code","source":"#We check the datatypes \ntrain.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"125ee905b58c3e4a30f42c86460fc370fd08f1ce"},"cell_type":"code","source":"test.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3d373430105ccdfa2dcfc808b309f527f725da5"},"cell_type":"markdown","source":"As we can see from above, we have 2 columns with the object datatype. We will have to take a closer look at them later. \nFirst we take a look at the data to see if there are any null or NAN values\n"},{"metadata":{"trusted":true,"_uuid":"c0a492be357cd7664e82a383f98b16d1ac6491de"},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3513c6b688a483323b0ca8729f4c86f24391540e"},"cell_type":"code","source":"#I will simply drop the nan columns\ntrain = train.dropna()\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1ceba943eab612d925d9674bde30d726e09887c"},"cell_type":"code","source":"#The we check for zeros in our train and test data\n(train ==0).astype(int).sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31bed33fa491e959f15bb1fa1cfeabd54fe571a1"},"cell_type":"markdown","source":" we are simply going to drop the zero rows becasue they may result in wrong outpur down the road'"},{"metadata":{"trusted":true,"_uuid":"755e40e905d47ab201c3ae7ed5041b3b983ff6df"},"cell_type":"code","source":"train = train.loc[~(train==0).any(axis =1)]\n#we take a look at what we have done\n#(train ==0).astype(int).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b50f20ab12209ffa20ef7371ad33f2ddc7e31e92"},"cell_type":"code","source":"(train ==0).astype(int).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb09cbac5abaf645cbfd8e16cacbc9069173b4c8"},"cell_type":"code","source":"#we started out with 1 million data point. Lets see what we have now.\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6cf0f0196e364f5120a131fe47497b584f68f5a"},"cell_type":"markdown","source":"\n\nWHAT we do next is to see how the data points are distributed. By using the the .describe method, we check the min and the max value of the dataset.\n\n"},{"metadata":{"trusted":true,"_uuid":"4f4793b1474e4095dfb2e7420d61e81c4d4881a2"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67f1032324e9955310785554b30865f6da8ac201"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce1369548666afabb807ad9d267b5653ffd7d0a1"},"cell_type":"markdown","source":"waouw!!! checkout the difference/ range between the min and the max . it is ver large. This means we could have a lot of outlier in out dataset, this may lead to wrong out;puts"},{"metadata":{"_uuid":"da25bf6c7ef551a0a45d2a45a3dec611394de582"},"cell_type":"markdown","source":"Lets find the absolut distance travelled by each passenger, by taking the difference between their pickup latitudes and dropoff latitudes. We do the same for latitude"},{"metadata":{"trusted":true,"_uuid":"44bf57d76d1f690557593c984ee3db4dc43e6576"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9b3d7eb69a6c2f0bac597bfe7f62127acbb7896"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d3a291cb2bc6c8410ecc5946c570572c926c1b7"},"cell_type":"code","source":"train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"218adb5386825efa549e16a38713140e0b95109f"},"cell_type":"code","source":"#lets take care of the object data first\nobject_data = train.dtypes == np.object\ncategoricals = train.columns[object_data]\ncategoricals\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e347cf423a18e73f53e9ea99229a3d71630c0e6"},"cell_type":"code","source":"#I will drop the key column since i do not really need it \ntrain.drop('key', axis = 1, inplace = True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efbe8131495c98f0440cec00bfe4f978ff3e8edf"},"cell_type":"markdown","source":"Now we take care of the pickup_datetime column, we extract the dates separately and drop the pickup_datetime column\n"},{"metadata":{"trusted":true,"_uuid":"004693fd9951294e2b51afa8db214b955c3e54c8"},"cell_type":"code","source":"import datetime as dt\n\ndef date_extraction(data):\n    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n    data['year'] = data['pickup_datetime'].dt.year\n    data['month'] = data['pickup_datetime'].dt.month\n    data['weekday'] = data['pickup_datetime'].dt.day\n    data['hour'] = data['pickup_datetime'].dt.hour\n    data = data.drop('pickup_datetime', axis = 1, inplace = True)\n    \n    return data\n    \n#Apply this to both the train and the test data\ndate_extraction(train)\n#test = date_extraction(test)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6584c1ff3e1a62d07adbff9e72bf3b521420da03"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef9f30b3318e6515d002a2af35852d75a2b0e69f"},"cell_type":"code","source":"date_extraction(test)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0b5575d09df76cbee155d8d80ac1faa6593acab"},"cell_type":"markdown","source":"Found this on Quora and i am going to use this to find the distance travelled using the longitudes and latitudes given. You can check it out at:\n\nhttps://www.quora.com/How-to-measure-the-distance-traveled-using-latitude-and-longitude\nhttps://community.esri.com/groups/coordinate-reference-systems/blog/2017/10/05/haversine-formula"},{"metadata":{"trusted":true,"_uuid":"f98215eea97decdae999378bbc019d0c3ef8e659"},"cell_type":"code","source":"\n\n#First i will define the Haversine function\n#radii of earth in meters = 6371e3 meters\n# def long_lat_distance (x):\n#     x['Longitude_distance'] = x['pickup_longitude'] - x['dropoff_longitude']\n#     x['Latitude_distance'] = x['pickup_latitude'] - x['dropoff_latitude'] \n    \n#     return x   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aeb04de10c62deb0162ebb71c029c9ad8ed8b97a"},"cell_type":"code","source":"def long_lat_distance (x):\n    x['Longitude_distance'] = np.radians(x['pickup_longitude'] - x['dropoff_longitude'])\n    x['Latitude_distance'] = np.radians(x['pickup_latitude'] - x['dropoff_latitude']) \n    x['distance_travelled/10e3'] = ((x['Longitude_distance']**2 + x['Latitude_distance']**2)**0.5) *1000\n    return x   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"811c7d0229cbdc8b174d4702008faf061b1f2d49"},"cell_type":"code","source":"for x in [train, test]:\n    long_lat_distance(x)\n    \ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e1c9697470da88be3e1a989b979cd5a7c604cfa"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"96537da4fd1d1db4260cb2e118c4740f366e574d"},"cell_type":"code","source":"def harvesine(x):\n    #radii of earth in meters = \n    r = 6371000 \n    d = x['distance_travelled/10e3']\n    theta_1 = np.radians(x['dropoff_latitude'])\n    theta_2 = np.radians(x['pickup_latitude'])\n    lambda_1 = np.radians(x['dropoff_longitude'])\n    lambda_2 = np.radians(x['dropoff_longitude'])\n    theta_diff = x['Longitude_distance']\n    lambda_diff = x['Latitude_distance']\n    \n    a = np.sin(theta_diff/2)**2 + np.cos(theta_1)*np.cos(theta_2)*np.sin(lambda_diff/2)**2\n    c = 2 * np.arctan2(a**0.5, (1-a)**0.5)\n    x['harvesine/km'] = (r * c)/1000\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d83134ce687eb68624c4dc36a0a7bafc81d3238d"},"cell_type":"code","source":"for x in [train, test]:\n    harvesine(x)\n    \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0750ab2e425f38740be686eb8506e509e8ca041"},"cell_type":"code","source":"train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f924057d40cf6bba5983089784ade11eedb48d2"},"cell_type":"markdown","source":"Now that we do not have any categorical features left, we are going to use the standard scaler to mitigate the effect ot the outliers on our data set"},{"metadata":{"trusted":true,"_uuid":"56baae0e06bf9377dc929b99163d7403a4547a8d"},"cell_type":"code","source":"# #not sure if this is rrally necessary. will have to see\n\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# scaler.fit_transform(train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ea01dd35b8e707ec364d83bea86528fdace5d28"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"996ea9d276d814b5bffb589d71661ebb041e63df"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b5974a8c840f29d20445804fc9ce96f329c4a49"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fccb09b7aa0340f88097d252f01e6a5df74cf61"},"cell_type":"code","source":"print('Are there any nulls\\nan in the train data: ')\nprint(train.isnull().sum())\n\nprint('\\nAre there any nulls\\nans in the test data: ')\nprint(test.isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92a1302a1c35289bdcee30fc135fed35e2008488"},"cell_type":"code","source":"#as we can see there are 3 nulls in the train data. Lets replace them with the mean\ntrain['harvesine/km'] = train['harvesine/km'].fillna(train['harvesine/km'].median())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"942b92e5f6ba7c60eb19c045fa4a56cfecbc0888"},"cell_type":"markdown","source":"## FEATURE SELECTION  and  RANDOM FOREST"},{"metadata":{"trusted":true,"_uuid":"0c10a02c62683ee4c6488b8418422e5563be99cd"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n#split the train features \nfeature_cols = [x for x in train.columns if x!= 'fare_amount']\nX = train[feature_cols]\ny = train['fare_amount']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41f659d6c9a1fbf22bb599ba1d4b4e624b219c6a"},"cell_type":"code","source":"correlations = X.corrwith(y)\ncorrelations = abs(correlations*100)\ncorrelations.sort_values(ascending = False, inplace= True)\n\ncorrelations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fb4e40d2fe3d9aa275b56426d75649e9620dc44"},"cell_type":"code","source":"#lets plot and see what we've got\nax = correlations.plot(kind='bar')\nax.set(ylim=[-1, 1], ylabel='pearson correlation');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2da36fb99f0b7e076d107f39d8de66ac9b2a113e"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"cebbf38d5c54ce159d0dfa8bab26e621b09ca1ab"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c3479509198a3739f8297769032125a53e33892"},"cell_type":"code","source":"#From the diagram above, i will use the 5 most important features\n\ntrain_1 = train.drop(['pickup_longitude', 'dropoff_longitude','pickup_latitude','dropoff_latitude',\n                    'Longitude_distance', 'Latitude_distance'], axis =1)\n\ntrain_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d618fa35c750a7fd48df82514f1efe394edc0f0"},"cell_type":"code","source":"train_1['harvesine/km'] =train_1['harvesine/km'].round(2) \ntrain_1['distance_travelled/10e3'] =train_1['distance_travelled/10e3'].round(2) \n\ntrain_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17b884f23e1be9a6e40a39f539c030102621b407"},"cell_type":"code","source":"\n# #not sure if this is rrally necessary. will have to see\n\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# scaler.fit_transform(train_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0c607497009336545c9937eecc66a6dbd678ad3"},"cell_type":"code","source":"train_1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62dc961cfbbe9dc22a3578c991f2e8ebffa037a1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27fde97491772d3f566b59df54da1f3819418526"},"cell_type":"code","source":"test_1 = test.drop(['pickup_longitude', 'dropoff_longitude','pickup_latitude','dropoff_latitude',\n                    'Longitude_distance', 'Latitude_distance'], axis =1)\n\ntest_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40619ca1b75a9449d08ad88df55def3f62fb33df"},"cell_type":"markdown","source":"## Modelling and Prediction"},{"metadata":{"trusted":true,"_uuid":"2774b4271b6dabcec64c57e0c3ec2845aa65209e"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"61b9f5b23b2a8936469214a53108a6cff571e555"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfeat_cols = [x for x in train_1.columns if x!= 'fare_amount']\nX_1 = train_1[feat_cols]\ny_1 = train_1['fare_amount']\nX_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size = 0.25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a6d04a959239dbb5c4036f22d193f8dd6c68fba"},"cell_type":"code","source":"#Random forest\nrf = RandomForestRegressor(n_estimators = 100, max_features = 5)\nrf = rf.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f88379b46b7b1abc4b70c6884bce23595329ddb4"},"cell_type":"markdown","source":"With the previous version 14, i used Linear regression and i got a score of 9.38 but with random forest i got 3.89"},{"metadata":{"trusted":true,"_uuid":"71e68d5ab7ab08bd3d5ee9ac5363e5bbf4e6a1c3"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49663febabc77b52778dfd8492476cf8a06bcd8f"},"cell_type":"markdown","source":"## Preparing submission file\n"},{"metadata":{"trusted":true,"_uuid":"66ed57b21d4f56d36662e012c0a834d765e2bed8"},"cell_type":"code","source":"final_prediction = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3893b04d78c2e27cb4cf6a883da13845bcf404f"},"cell_type":"code","source":"test_1.drop('key', axis = 1, inplace = True)\ntest_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e34d67297c9cdef2e7d347862ffd00601a92f23"},"cell_type":"code","source":"#random forest\nfinal_prediction = rf.predict(test_1)\n\nNYCtaxiFare_submission = pd.DataFrame({'key': test.key, 'fare_amount': final_prediction})\nNYCtaxiFare_submission.to_csv('NYCtaxiFare_prediction.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4bc9c9821923be9936678d5783855fbf2199a9f"},"cell_type":"code","source":"NYCtaxiFare_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a0c8a67acafe443bf67233a10610376472b8cb5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d43276df634366d6b24c912e8f10d816fdac6cd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}