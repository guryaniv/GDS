{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9e598214dce2f7aff9389c58f6871a8506966d8"},"cell_type":"code","source":"data =  pd.read_csv('../input/train.csv', nrows = 15_000_000)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Given a dataframe, add two new features 'abs_diff_longitude' and\n# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n# the pickup location to the dropoff location.\ndef add_travel_vector_features(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n\nadd_travel_vector_features(data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3501b1a8a57342293c6ab6f3e5779858b9cf60ff","scrolled":true},"cell_type":"code","source":"print('Old size: %d' % len(data))\ndata = data.dropna(how = 'any', axis = 'rows')\nprint('New size: %d' % len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c51a15f4a365862f62dea455f4c2462d2190ba4e"},"cell_type":"code","source":"from datetime import datetime\ndata['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in data['pickup_datetime']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4fc9f4307938fa4acf3a91b5a54363c14357d501"},"cell_type":"code","source":"#print(data.describe())\ndef distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295 # Pi/180\n    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a)) # 2*R*asin...\n\n# add new column to dataframe with distance in miles\ndata['distance_miles'] = distance(data.pickup_latitude, data.pickup_longitude, \\\n                                      data.dropoff_latitude, data.dropoff_longitude)\n\n(data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dc7077a9940f07baf0cdcda99f2c01d991894b9","scrolled":false},"cell_type":"code","source":"print('Old size: %d' % len(data))\ndata = data[(data.abs_diff_longitude < 3.0) & (data.abs_diff_latitude < 3.0)]\ndata = data[(data.pickup_longitude >= -74.3) & (data.pickup_longitude <= -72.9)]  # nyc coordinates\ndata = data[(data.dropoff_longitude >= -74.3) & (data.dropoff_longitude <= -72.9)]\ndata = data[(data.pickup_latitude >= 40.5) & (data.pickup_latitude <= 41.8)]\ndata = data[(data.dropoff_latitude >= 40.5) & (data.dropoff_latitude <= 41.8)]\ndata = data[(data.fare_amount>=2) & (data.fare_amount<=500)]\ndata = data[(data.passenger_count>0) & (data.passenger_count <=6)]\ndata = data[(data.distance_miles<=100.0) & (data.distance_miles>0.05)]\nnyc = (-74.0063889, 40.7141667)\ndata['distance_to_center'] = distance(nyc[1], nyc[0],data.dropoff_latitude, data.dropoff_longitude)\ndata = data[data.distance_to_center<15.0]\nprint('New size: %d' % len(data))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0587baf20567e6855e9a8342646eb3cc607b6cda"},"cell_type":"code","source":"def late_night (row):\n    if (row['hour'] <= 6) or (row['hour'] >= 20):\n        return 1\n    else:\n        return 0\n\n\ndef night (row):\n    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):\n        return 1\n    else:\n        return 0\n    \n\n\n#data.distance_miles.hist(bins=50, figsize=(12,4))\n#plt.xlabel('distance miles')\n#plt.title('Histogram ride distances in miles')\n#data.groupby('passenger_count')['distance_miles', 'fare_amount'].mean()\n#print(\"Average $USD/Mile : {:0.2f}\".format(data.fare_amount.sum()/data.distance_miles.sum()))\n#data['fare_per_mile'] = data.fare_amount / data.distance_miles\ndata['hour'] = [date.hour for date in data['datetime_object']]\ndata['year'] = [date.year for date in data['datetime_object']]\ndata['day'] = [date.day for date in data['datetime_object']]\ndata['weekday'] = data['datetime_object'].apply(lambda x: x.weekday())\ndata['night'] = data.apply (lambda x: night(x), axis=1)\ndata['late_night'] = data.apply (lambda x: late_night(x), axis=1)   \n# There is a $1 surcharge from 4pm to 8pm on weekdays, excluding holidays.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c647154ffae4423a6626432c846c8fed188e9cb5","scrolled":true},"cell_type":"code","source":"rangeA = 1.5\nrangeN = 20.0\nrangeS = 48.7\nrangeR = 14.1\nrangeD = 28.7\nrangeO = 29\nrangeP = 15.7\nrangeW = 20.0\njfk = (-73.7822222222, 40.6441666667) #JFK Airport\newr = (-74.175, 40.69) # Newark Liberty International Airport\nlgr = (-73.87, 40.77) # LaGuardia Airport\n\n # county\nNassau = (-73.5594, 40.6546)\nSuffolk = (-72.6151, 40.9849)\nWestchester = (-73.7949, 41.1220)\nRockland = (-73.9830, 41.1489)\nDutchess = (-73.7478, 41.7784)\nOrange = (-74.3118, 41.3912)\nPutnam = (-73.7949, 41.4351) \n\ndata_air=data\n\ndef add_checkpoint(point, point_name,rangeA):\n    data_air[point_name] = (distance(data.pickup_latitude, data.pickup_longitude, point[1], point[0]) <= rangeA) | ((distance(data.dropoff_latitude, data.dropoff_longitude, point[1], point[0]) <= rangeA))\n    data_air[point_name].replace(False, 0, inplace=True)\n    data_air[point_name] = data_air[point_name].astype(int)\n\nadd_checkpoint(jfk, 'jfk',rangeA)\nadd_checkpoint(ewr, 'ewr',rangeA)\nadd_checkpoint(lgr, 'lgr',rangeA)\nadd_checkpoint(Nassau, 'Nassau',rangeN)\nadd_checkpoint(Suffolk, 'Suffolk',rangeS)\nadd_checkpoint(Westchester, 'Westchester',rangeW)\nadd_checkpoint(Rockland, 'Rockland',rangeR )\nadd_checkpoint(Dutchess, 'Dutchess',rangeD)\nadd_checkpoint(Orange, 'Orange',rangeO)\nadd_checkpoint(Putnam, 'Putnam',rangeP)\n\ndata_air = data[(data_air.jfk | data_air.ewr | data_air.lgr | data_air.Nassau | data_air.Suffolk | data_air.Westchester | data_air.Rockland | data_air.Dutchess | data_air.Orange | data_air.Putnam)==1]\ndata_air['airport'] = (data_air.jfk | data_air.ewr | data_air.lgr )==1\ndata_air['airport'].replace(False, 0, inplace=True)\ndata_air['airport'] = data_air['airport'].astype(int)\ndata_air['county1'] = (data_air.jfk | data_air.ewr | data_air.lgr )==0\ndata_air['county1'] = (data_air.Nassau | data_air.Westchester)==1\ndata_air['county1'].replace(False, 0, inplace=True)\ndata_air['county1'] = data_air['county1'].astype(int)\ndata_air['county2'] = (data_air.jfk | data_air.ewr | data_air.lgr | data_air.Nassau | data_air.Westchester)==0\ndata_air['county2'].replace(False, 0, inplace=True)\ndata_air['county2'] = data_air['county2'].astype(int)\ndata = data[(data.jfk | data.ewr | data.lgr | data.Nassau | data.Suffolk | data.Westchester | data.Rockland | data.Dutchess | data.Orange | data.Putnam)==0]\ndata_air.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17624e0b3112e5507991a417f0ccbd520ba3cc20","scrolled":true},"cell_type":"code","source":"'''import seaborn as sns\nimport matplotlib.pyplot as plt\ncorrmat = data_air.corr()\nf, ax = plt.subplots(figsize=(12, 9))\n\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'fare_amount')['fare_amount'].index\ncm = np.corrcoef(data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47700a4563d5bfe42fc5e0956cb10c90c3dab29f","scrolled":false},"cell_type":"code","source":"\n\n#data.year_2015.hist(bins=50, figsize=(12,4))\n#plt.xlabel('distance miles')\n#plt.title('Histogram ride hour')\nplt.scatter(data['year'][:1000], data['fare_amount'][:1000])\nplt.show()\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c995ab67329370d8d459ff7668414a5385602c7d","scrolled":true},"cell_type":"code","source":"# Drop unwanted columns\n\n\n\ndropped_columns_air = ['day','pickup_longitude','pickup_latitude','dropoff_latitude','dropoff_longitude',\n                       'distance_to_center','passenger_count','Nassau','Westchester',\n                  'datetime_object','abs_diff_longitude','abs_diff_latitude','key','pickup_datetime']\n\ndropped_columns = ['day','pickup_longitude','pickup_latitude','dropoff_latitude','dropoff_longitude','distance_to_center',\n                  'datetime_object','abs_diff_longitude','abs_diff_latitude','key','pickup_datetime',\n                  'jfk','ewr','lgr', 'Nassau','Suffolk','Westchester','Rockland','Dutchess','Orange','Putnam'\n                  ]\ntrain_clean = data.drop(dropped_columns, axis=1)\ntrain_air_clean = data_air.drop(dropped_columns_air, axis=1)\ntrain_air_clean.head()\ndata_air.head()\n#train_clean.head()\n#test_clean = test.drop(dropped_columns + ['key', 'passenger_count'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"521021ba33e6acb69ca70236837c7f4f33df431b"},"cell_type":"code","source":"# split data in train and validation (90% ~ 10%)\nfrom sklearn.model_selection import train_test_split\ntrain_df, validation_df = train_test_split(train_clean, test_size=0.10, random_state=1)\n\n# Get labels\ntrain_labels = train_df['fare_amount'].values\nvalidation_labels = validation_df['fare_amount'].values\ntrain_df = train_df.drop(['fare_amount'], axis=1)\nvalidation_df = validation_df.drop(['fare_amount'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7bb2897578992fc5e9b298f7ebe93364ab329a8"},"cell_type":"code","source":"# split data in train and validation (90% ~ 10%)\ntrain_air_df, validation_air_df = train_test_split(train_air_clean, test_size=0.10, random_state=1)\n\n# Get labels\ntrain_air_labels = train_air_df['fare_amount'].values\nvalidation_air_labels = validation_air_df['fare_amount'].values\ntrain_air_df = train_air_df.drop(['fare_amount'], axis=1)\nvalidation_air_df = validation_air_df.drop(['fare_amount'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8fb1d988ffe3f9c0edee12a7d2a0e56389eb6c9","scrolled":false},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\ntest_df['distance_miles'] = distance(test_df.pickup_latitude, test_df.pickup_longitude, \\\n                                      test_df.dropoff_latitude, test_df.dropoff_longitude)\ntest_df['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in test_df['pickup_datetime']]\ntest_df['hour'] = [date.hour for date in test_df['datetime_object']]\ntest_df['year'] = [date.year for date in test_df['datetime_object']]\ntest_df['day'] = [date.day for date in test_df['datetime_object']]\ntest_df['weekday'] = test_df['datetime_object'].apply(lambda x: x.weekday())\ntest_df['night'] = test_df.apply (lambda x: night(x), axis=1)\ntest_df['late_night'] = test_df.apply (lambda x: late_night(x), axis=1)\n\n#test_df['distance_to_center'] = distance(nyc[1], nyc[0],test_df.dropoff_latitude, test_df.dropoff_longitude)\n\ndef add_checkpoint_test(point, point_name,rangeA):\n    test_df[point_name] = (distance(test_df.pickup_latitude, test_df.pickup_longitude, point[1], point[0]) <= rangeA) | ((distance(test_df.dropoff_latitude, test_df.dropoff_longitude, point[1], point[0]) <= rangeA))\n    test_df[point_name].replace(False, 0, inplace=True)\n    test_df[point_name] = test_df[point_name].astype(int)\n\nadd_checkpoint_test(jfk, 'jfk',rangeA)\nadd_checkpoint_test(ewr, 'ewr',rangeA)\nadd_checkpoint_test(lgr, 'lgr',rangeA)\nadd_checkpoint_test(Nassau, 'Nassau',rangeN)\nadd_checkpoint_test(Suffolk, 'Suffolk',rangeS)\nadd_checkpoint_test(Westchester, 'Westchester',rangeW)\nadd_checkpoint_test(Rockland, 'Rockland',rangeR)\nadd_checkpoint_test(Dutchess, 'Dutchess',rangeD)\nadd_checkpoint_test(Orange, 'Orange',rangeO)\nadd_checkpoint_test(Putnam, 'Putnam',rangeP)\n\n\n\n#test_df['euclidean'] = minkowski_distance(test_df['pickup_longitude'], test_df['dropoff_longitude'],\n#                                       test_df['pickup_latitude'], test_df['dropoff_latitude'], 2)\n\ntest_air_df = test_df[(test_df.jfk | test_df.ewr | test_df.lgr | test_df.Nassau | test_df.Suffolk | test_df.Westchester | test_df.Rockland | test_df.Dutchess | test_df.Orange | test_df.Putnam)==1]\ntest_df = test_df[(test_df.jfk | test_df.ewr | test_df.lgr | test_df.Nassau | test_df.Suffolk | test_df.Westchester | test_df.Rockland | test_df.Dutchess | test_df.Orange | test_df.Putnam)==0]\n\ndropped_columns_test = ['pickup_longitude', 'pickup_latitude', 'day','key',\n                        'jfk','ewr','lgr','Nassau','Suffolk','Westchester','Rockland','Dutchess','Orange','Putnam',\n                   'dropoff_longitude', 'dropoff_latitude' ,'datetime_object','pickup_datetime'\n                  ]\ntest_clean = test_df.drop(dropped_columns_test, axis=1)\ntest_clean.head()\n\ntest_air_df['airport'] = (test_air_df.jfk | test_air_df.ewr | test_air_df.lgr )==1\ntest_air_df['airport'].replace(False, 0, inplace=True)\ntest_air_df['airport'] = test_air_df['airport'].astype(int)\ntest_air_df['county1'] = (test_air_df.jfk | test_air_df.ewr | test_air_df.lgr )==0 \ntest_air_df['county1'] = (test_air_df.Nassau | test_air_df.Westchester)==1\ntest_air_df['county1'].replace(False, 0, inplace=True)\ntest_air_df['county1'] = test_air_df['county1'].astype(int)\ntest_air_df['county2'] = (test_air_df.jfk | test_air_df.ewr | test_air_df.lgr | test_air_df.Nassau | test_air_df.Westchester)==0\ntest_air_df['county2'].replace(False, 0, inplace=True)\ntest_air_df['county2'] = test_air_df['county2'].astype(int)\n\n\ndropped_columns_test_air = ['pickup_longitude', 'pickup_latitude', 'day','key','passenger_count','Nassau','Westchester',\n                   'dropoff_longitude', 'dropoff_latitude' ,'datetime_object','pickup_datetime'\n                  ]\ntest_air_clean = test_air_df.drop(dropped_columns_test_air, axis=1)\ntest_air_clean.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d14dd4cd4418db0eb7784fa253fbd6aad3a22629","scrolled":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LeakyReLU\nfrom keras import optimizers\nfrom keras import regularizers\n# Scale data\n# Note: im doing this here with sklearn scaler but, on the Coursera code the scaling is done with Dataflow and Tensorflow\nscaler = preprocessing.MinMaxScaler()\ntrain_df_scaled = scaler.fit_transform(train_df)\nvalidation_df_scaled = scaler.transform(validation_df)\ntest_scaled = scaler.transform(test_clean)\n\ntrain_air_df_scaled = scaler.fit_transform(train_air_df)\nvalidation_air_df_scaled = scaler.transform(validation_air_df)\ntest_air_scaled = scaler.transform(test_air_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b7ac029887665e1615477d1dd53909b7b8bd4b2","scrolled":false},"cell_type":"code","source":"BATCH_SIZE = 256\nEPOCHS = 5\nLEARNING_RATE = 0.0001\nDATASET_SIZE = 6000000\n\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_dim=train_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1))\n\nadam = optimizers.adam(lr=LEARNING_RATE)\nmodel.compile(loss='mse', optimizer=adam, metrics=['mae'])\nprint(train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ec3f622b3dd6541f15603423fed272419e5ce52","scrolled":true},"cell_type":"code","source":"BATCH_SIZE = 256\nEPOCHS = 5\nLEARNING_RATE = 0.0001\nDATASET_SIZE = 6000000\n\nmodel_air = Sequential()\nmodel_air.add(Dense(256, activation='relu', input_dim=train_air_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))\nmodel_air.add(BatchNormalization())\nmodel_air.add(Dense(128, activation='relu'))\nmodel_air.add(BatchNormalization())\nmodel_air.add(Dense(64, activation='relu'))\nmodel_air.add(BatchNormalization())\nmodel_air.add(Dense(32, activation='relu'))\nmodel_air.add(BatchNormalization())\nmodel_air.add(Dense(16, activation='relu'))\nmodel_air.add(BatchNormalization())\nmodel_air.add(Dense(1))\n\nadam = optimizers.adam(lr=LEARNING_RATE)\nmodel_air.compile(loss='mse', optimizer=adam, metrics=['mae'])\nprint(train_air_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f852870068ff992bd974bb7bf9fa5c582ffe49a5","scrolled":false},"cell_type":"code","source":"print('Dataset size: %s' % DATASET_SIZE)\nprint('Epochs: %s' % EPOCHS)\nprint('Learning rate: %s' % LEARNING_RATE)\nprint('Batch size: %s' % BATCH_SIZE)\nprint('Input dimension: %s' % train_df_scaled.shape[1])\nprint('Features used: %s' % train_df.columns)\nmodel.summary()\nhistory = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, \n                    verbose=1, validation_data=(validation_df_scaled, validation_labels), \n                    shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bdca0dc697498768f6e18c8f5df9e7f7b65f386","scrolled":true},"cell_type":"code","source":"print('Dataset size: %s' % DATASET_SIZE)\nprint('Epochs: %s' % EPOCHS)\nprint('Learning rate: %s' % LEARNING_RATE)\nprint('Batch size: %s' % BATCH_SIZE)\nprint('Input dimension: %s' % train_air_df_scaled.shape[1])\nprint('Features used: %s' % train_air_df.columns)\nmodel_air.summary()\nhistory_air = model_air.fit(x=train_air_df_scaled, y=train_air_labels, batch_size=BATCH_SIZE, epochs=EPOCHS*2, \n                    verbose=1, validation_data=(validation_air_df_scaled, validation_air_labels), \n                    shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dd3285487937f33f4f7bb4b77517319498e036c","scrolled":true},"cell_type":"code","source":"#plot_loss_accuracy(history)\n\nSUBMISSION_NAME = 'submission.csv'\ndef output_submission(raw_test,prediction,  file_name):\n    df = pd.DataFrame(prediction, columns=['fare_amount'])\n    df['key'] = raw_test['key']\n                  \n    #raw_test = raw_test.drop(dropped_columns, axis=1)\n    df[['key','fare_amount']].to_csv((file_name), index=False)\n    \n    #print(df)\n    print('Output complete')\n    print(df)\n    \nprediction = model.predict(test_scaled, batch_size=128, verbose=1)\nprediction_air = model_air.predict(test_air_scaled, batch_size=128, verbose=1)\n#prediction_air = model_air.predict(test_air_scaled, num_iteration = model_air.best_iteration)\n#print(prediction_air)\nframes = [test_df, test_air_df]\ntest_final = pd.concat(frames)\nframes = [prediction, prediction_air]\nprediction_final = np.concatenate(frames)\n\n\n#test_df = pd.read_csv('../input/test.csv')\nresult=dict()\nprint(len(test_final))\ni=0\n#print(test_df[0])\nfor index, row in test_final.iterrows():\n    result[row['key']]=prediction_final[i]\n    i=i+1\ntest_df1 = pd.read_csv('../input/test.csv')\n\npred=[]\nfor index, row in test_df1.iterrows():\n    pred.append(result[row['key']])\n    \ntest_df1.head()\noutput_submission(test_df1,pred, SUBMISSION_NAME)\n\n\n#from sklearn.model_selection import train_test_split\n#y = data.fare_amount\n#X = data.drop('fare_amount', axis=1)\n#train_df, val_df, train_y, val_y = train_test_split(X, y,test_size=0.2)\n#train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f2c800f5a6a167ee32508b65479c808044aa171"},"cell_type":"code","source":"# Construct and return an Nx3 input matrix for our linear model\n# using the travel vector, plus a 1.0 for a constant bias term.\ndef get_input_matrix(df):\n    return np.column_stack((df.distance_miles, df.passenger_count,df.hour,df.year, np.ones(len(df))))\n\n#train_X = get_input_matrix(train_df)\n#train_y = np.array(train_df['fare_amount'])\n\n#print(train_X.shape)\n#print(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45b8b93121272d9e53b87c65656e4664189119f5"},"cell_type":"code","source":"# The lstsq function returns several things, and we only care about the actual weight vector w.\n#(w, _, _, _) = np.linalg.lstsq(train_X, train_y, rcond = None)\n#print(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c013ad4ef4170783901cd43f8ec7460b092bb782"},"cell_type":"code","source":"#w_OLS = np.matmul(np.matmul(np.linalg.inv(np.matmul(train_X.T, train_X)), train_X.T), train_y)\n#print(w_OLS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72c92552f4eab983aec23cb227ac2e454b66d8e0"},"cell_type":"code","source":"#test_df = pd.read_csv('../input/test.csv')\n#test_df['distance_miles'] = distance(test_df.pickup_latitude, test_df.pickup_longitude, \\\n                                      test_df.dropoff_latitude, test_df.dropoff_longitude)\n#test_df['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in test_df['pickup_datetime']]\n#test_df['hour'] = [date.hour for date in test_df['datetime_object']]\n#test_df['year'] = [date.year for date in test_df['datetime_object']]\n\n#val_df['distance_miles'] = distance(val_df.pickup_latitude, val_df.pickup_longitude, \\\n                                      val_df.dropoff_latitude, val_df.dropoff_longitude)\n#val_df['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in val_df['pickup_datetime']]\n#val_df['hour'] = [date.hour for date in val_df['datetime_object']]\n#val_df['year'] = [date.year for date in val_df['datetime_object']]\ntest_df.dtypes\n#val_df = pd.read_csv('../input/train.csv', nrows = 10000000)\n#val_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e94990202009366436f78fdd66d546f40d4e85d"},"cell_type":"code","source":"# Reuse the above helper functions to add our features and generate the input matrix.\n#add_travel_vector_features(test_df)\n#test_X = get_input_matrix(test_df)\n#add_travel_vector_features(val_df)\n#val_df = val_df.dropna(how = 'any', axis = 'rows')\n#val_df = val_df[(val_df.abs_diff_longitude < 5.0) & (val_df.abs_diff_latitude < 5.0)]\n#val_X = get_input_matrix(val_df)\n# Predict fare_amount on the test set using our model (w) trained on the training set.\n#test_y_predictions = np.matmul(test_X, w).round(decimals = 2)\n#val_y_predictions = np.matmul(val_X, w).round(decimals = 2)\n#val_y = np.array(val_df['fare_amount'])\n\n#from sklearn.metrics import mean_squared_error\n#print(np.sqrt(mean_squared_error(val_y, val_y_predictions)))\n# Write the predictions to a CSV file which we can submit to the competition.\n#submission = pd.DataFrame(\n#    {'key': test_df.key, 'fare_amount': test_y_predictions},\n#    columns = ['key', 'fare_amount'])\n#submission.to_csv('submission.csv', index = False)\n\n#print(os.listdir('.'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}