{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "mimetype": "text/x-python", "version": "3.6.4", "pygments_lexer": "ipython3"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "cells": [{"source": ["Not necessarily useful, but I found this [visualization code](https://github.com/philipperemy/keras-visualize-activations) on Github and wanted to try it out.  This example is based on [Nitin Surya's neural network](https://www.kaggle.com/nitinsurya/surprise-me-2-neural-networks-keras)."], "metadata": {"_uuid": "1d9fa65697c084b8ea20545c64e430bf044db6ba", "collapsed": true, "_cell_guid": "b8654a8b-643b-4d2b-bf4a-b1fc6c06f2a9"}, "cell_type": "markdown"}, {"source": ["NWEEKS_TO_EXCLUDE = 1"], "metadata": {"_uuid": "556618fcb333d6b8e68e7a26ede7d349c10a4536", "collapsed": true, "_cell_guid": "1666bebe-2dc7-4128-a699-827233b25ca7"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["\"\"\"\n", "Contributions from:\n", "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n", "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n", "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n", "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n", "hklee - weighted mean comparisons, LB 0.497, 1ST\n", "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n", "tunguz - Surprise Me 2!\n", "https://www.kaggle.com/tunguz/surprise-me-2/code\n", "\n", "Also all comments for changes, encouragement, and forked scripts rock\n", "\n", "Keep the Surprise Going\n", "\"\"\"\n", "\n", "import glob, re\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn import *\n", "from datetime import datetime\n", "from xgboost import XGBRegressor\n", "\n", "from keras.layers import Embedding, Input, Dense\n", "import keras\n", "import keras.backend as K\n", "\n", "import matplotlib.pyplot as plt"], "metadata": {"_uuid": "6ae68db95c99261f9d9235185aaaf2344c335169", "collapsed": true, "_cell_guid": "dd50fef0-4659-49ce-9e19-a8f3c16b9542"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["# From Philippe R\u00e9my's Github  https://github.com/philipperemy\n", "# Revised to print layer names when displaying\n", "\n", "def get_activations(model, model_inputs, print_shape_only=False, layer_name=None):\n", "    print('----- activations -----')\n", "    activations = []\n", "    inp = model.input\n", "\n", "    model_multi_inputs_cond = True\n", "    if not isinstance(inp, list):\n", "        # only one input! let's wrap it in a list.\n", "        inp = [inp]\n", "        model_multi_inputs_cond = False\n", "\n", "    outputs = [layer.output for layer in model.layers if\n", "               layer.name == layer_name or layer_name is None]  # all layer outputs\n", "\n", "    funcs = [K.function(inp + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n", "\n", "    if model_multi_inputs_cond:\n", "        list_inputs = []\n", "        list_inputs.extend(model_inputs)\n", "        list_inputs.append(0.)\n", "    else:\n", "        list_inputs = [model_inputs, 0.]\n", "\n", "    # Learning phase. 0 = Test mode (no dropout or batch normalization)\n", "    # layer_outputs = [func([model_inputs, 0.])[0] for func in funcs]\n", "    layer_outputs = [func(list_inputs)[0] for func in funcs]\n", "    for layer_activations in layer_outputs:\n", "        activations.append(layer_activations)\n", "        if print_shape_only:\n", "            print(layer_activations.shape)\n", "        else:\n", "            print(layer_activations)\n", "    return activations\n", "\n", "\n", "def display_activations_with_names(activation_maps, layer_names):\n", "    import numpy as np\n", "    import matplotlib.pyplot as plt\n", "    \"\"\"\n", "    (1, 26, 26, 32)\n", "    (1, 24, 24, 64)\n", "    (1, 12, 12, 64)\n", "    (1, 12, 12, 64)\n", "    (1, 9216)\n", "    (1, 128)\n", "    (1, 128)\n", "    (1, 10)\n", "    \"\"\"\n", "    batch_size = activation_maps[0].shape[0]\n", "    assert batch_size == 1, 'One image at a time to visualize.'\n", "    for i, activation_map in enumerate(activation_maps):\n", "        print('Displaying activation map {}'.format(layer_names[i]))\n", "        shape = activation_map.shape\n", "        if len(shape) == 4:\n", "            activations = np.hstack(np.transpose(activation_map[0], (2, 0, 1)))\n", "        elif len(shape) == 2:\n", "            # try to make it square as much as possible. we can skip some activations.\n", "            activations = activation_map[0]\n", "            num_activations = len(activations)\n", "            if num_activations > 1024:  # too hard to display it on the screen.\n", "                square_param = int(np.floor(np.sqrt(num_activations)))\n", "                activations = activations[0: square_param * square_param]\n", "                activations = np.reshape(activations, (square_param, square_param))\n", "            else:\n", "                activations = np.expand_dims(activations, axis=0)\n", "        else:\n", "            raise Exception('len(shape) = 3 has not been implemented.')\n", "        plt.imshow(activations, interpolation='None', cmap='jet')\n", "        plt.show()"], "metadata": {"_uuid": "d25e1479527e1d07555b3cb7c239c9725d115581", "collapsed": true, "_cell_guid": "5bc330b2-44cc-4ea8-9fad-bec74c521225"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["data = {\n", "    'tra': pd.read_csv('../input/air_visit_data.csv'),\n", "    'as': pd.read_csv('../input/air_store_info.csv'),\n", "    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n", "    'ar': pd.read_csv('../input/air_reserve.csv'),\n", "    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n", "    'id': pd.read_csv('../input/store_id_relation.csv'),\n", "    'tes': pd.read_csv('../input/sample_submission.csv'),\n", "    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n", "    }\n", "\n", "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n", "\n", "for df in ['ar','hr']:\n", "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n", "    data[df]['visit_dow'] = data[df]['visit_datetime'].dt.dayofweek\n", "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n", "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n", "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n", "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n", "    # Exclude late reservations\n", "    ne = 7*(NWEEKS_TO_EXCLUDE - 1)\n", "    data[df] = data[df][data[df]['reserve_datetime_diff'] > data[df]['visit_dow']+ne]\n", "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n", "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n", "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n", "\n", "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n", "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n", "data['tra']['year'] = data['tra']['visit_date'].dt.year\n", "data['tra']['month'] = data['tra']['visit_date'].dt.month\n", "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n", "\n", "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n", "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n", "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n", "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n", "data['tes']['year'] = data['tes']['visit_date'].dt.year\n", "data['tes']['month'] = data['tes']['visit_date'].dt.month\n", "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n", "\n", "unique_stores = data['tes']['air_store_id'].unique()\n", "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n", "\n", "#sure it can be compressed...\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n", "\n", "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n", "# NEW FEATURES FROM Georgii Vyshnia\n", "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n", "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n", "lbl = preprocessing.LabelEncoder()\n", "for i in range(10):\n", "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n", "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n", "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n", "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n", "\n", "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n", "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n", "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n", "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n", "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n", "\n", "train = pd.merge(train, stores, how='inner', on=['air_store_id','dow']) \n", "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n", "\n", "for df in ['ar','hr']:\n", "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n", "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n", "\n", "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n", "\n", "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n", "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n", "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n", "\n", "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n", "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n", "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n", "\n", "# NEW FEATURES FROM JMBULL\n", "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n", "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n", "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n", "train['var_max_long'] = train['longitude'].max() - train['longitude']\n", "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n", "test['var_max_long'] = test['longitude'].max() - test['longitude']\n", "\n", "# NEW FEATURES FROM Georgii Vyshnia\n", "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n", "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n", "\n", "lbl = preprocessing.LabelEncoder()\n", "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n", "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n", "\n", "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n", "train = train.fillna(-1)\n", "test = test.fillna(-1)"], "metadata": {"_uuid": "4e1d66863fc7513cbc78f9a392da989994e62c1c", "collapsed": true, "_cell_guid": "ba6c11e9-8f34-44c8-88f8-36f6b055caab"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["def RMSLE(y, pred):\n", "    return metrics.mean_squared_error(y, pred)**0.5"], "metadata": {"_uuid": "d9d356a59c967c4a4d4c6a481f5fbbd1da1243c9", "collapsed": true, "_cell_guid": "381168b6-ea8b-4335-a5c2-5eebd2c853d9"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["Here we prepare data required for the neural network model.\n", "\n", "**value_col**:  taken as float input(which are normalized)\n", "\n", "**nn_col - value_col**: taken as categorical inputs(embedding layers used)"], "metadata": {"_uuid": "52e4a8487080e0d8f347774ffdcf67307584fa83", "_cell_guid": "aa727038-6fd5-47c0-9cda-197c4016ff8a"}, "cell_type": "markdown"}, {"source": ["value_col = ['holiday_flg','min_visitors','mean_visitors','median_visitors','max_visitors','count_observations',\n", "'rs1_x','rv1_x','rs2_x','rv2_x','rs1_y','rv1_y','rs2_y','rv2_y','total_reserv_sum','total_reserv_mean',\n", "'total_reserv_dt_diff_mean','date_int','var_max_lat','var_max_long','lon_plus_lat']\n", "\n", "nn_col = value_col + ['dow', 'year', 'month', 'air_store_id2', 'air_area_name', 'air_genre_name',\n", "'air_area_name0', 'air_area_name1', 'air_area_name2', 'air_area_name3', 'air_area_name4',\n", "'air_area_name5', 'air_area_name6', 'air_genre_name0', 'air_genre_name1',\n", "'air_genre_name2', 'air_genre_name3', 'air_genre_name4']\n", "\n", "\n", "X = train.copy()\n", "X_test = test[nn_col].copy()\n", "\n", "value_scaler = preprocessing.MinMaxScaler()\n", "for vcol in value_col:\n", "    X[vcol] = value_scaler.fit_transform(X[vcol].values.astype(np.float64).reshape(-1, 1))\n", "    X_test[vcol] = value_scaler.transform(X_test[vcol].values.astype(np.float64).reshape(-1, 1))\n", "\n", "X_train = list(X[nn_col].T.as_matrix())\n", "Y_train = np.log1p(X['visitors']).values\n", "nn_train = [X_train, Y_train]\n", "nn_test = [list(X_test[nn_col].T.as_matrix())]\n", "print(\"Train and test data prepared\")"], "metadata": {"_uuid": "a5de3f5c884c46772f44e76c2ff6053fbc9b2b4c", "collapsed": true, "_cell_guid": "20cd2b5a-990f-4ad7-a043-b17d70e49b08"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["Following function implements the Keras neural network model.\n", "\n", "Basic structure:\n", "- categorical columns get independent inputs, passed through embedding layer and then flattened.\n", "- numeric columns are simply taken as float32 inputs\n", "- the final tensors of categorical and numerical are then concatenated together\n", "- following the concatenated layer and simple feed forward neural network is implemented.\n", "- output layer has 'ReLU' activation function"], "metadata": {"_uuid": "fb1b7bd33d1a74ac26abe06bccc747fa930a5722", "_cell_guid": "0f3c1ff0-c4be-4732-81a5-db1a250e5296"}, "cell_type": "markdown"}, {"source": ["def get_nn_complete_model(train, hidden1_neurons=35, hidden2_neurons=15):\n", "    \"\"\"\n", "    Input:\n", "        train:           train dataframe(used to define the input size of the embedding layer)\n", "        hidden1_neurons: number of neurons in the first hidden layer\n", "        hidden2_neurons: number of neurons in the first hidden layer\n", "    Output:\n", "        return 'keras neural network model'\n", "    \"\"\"\n", "    K.clear_session()\n", "\n", "    air_store_id = Input(shape=(1,), dtype='int32', name='air_store_id')\n", "    air_store_id_emb = Embedding(len(train['air_store_id2'].unique()) + 1, 15, input_shape=(1,),\n", "                                 name='air_store_id_emb')(air_store_id)\n", "    air_store_id_emb = keras.layers.Flatten(name='air_store_id_emb_flatten')(air_store_id_emb)\n", "\n", "    dow = Input(shape=(1,), dtype='int32', name='dow')\n", "    dow_emb = Embedding(8, 3, input_shape=(1,), name='dow_emb')(dow)\n", "    dow_emb = keras.layers.Flatten(name='dow_emb_flatten')(dow_emb)\n", "\n", "    month = Input(shape=(1,), dtype='int32', name='month')\n", "    month_emb = Embedding(13, 3, input_shape=(1,), name='month_emb')(month)\n", "    month_emb = keras.layers.Flatten(name='month_emb_flatten')(month_emb)\n", "\n", "    air_area_name, air_genre_name = [], []\n", "    air_area_name_emb, air_genre_name_emb = [], []\n", "    for i in range(7):\n", "        area_name_col = 'air_area_name' + str(i)\n", "        air_area_name.append(Input(shape=(1,), dtype='int32', name=area_name_col))\n", "        tmp = Embedding(len(train[area_name_col].unique()), 3, input_shape=(1,),\n", "                        name=area_name_col + '_emb')(air_area_name[-1])\n", "        tmp = keras.layers.Flatten(name=area_name_col + '_emb_flatten')(tmp)\n", "        air_area_name_emb.append(tmp)\n", "\n", "        if i > 4:\n", "            continue\n", "        area_genre_col = 'air_genre_name' + str(i)\n", "        air_genre_name.append(Input(shape=(1,), dtype='int32', name=area_genre_col))\n", "        tmp = Embedding(len(train[area_genre_col].unique()), 3, input_shape=(1,),\n", "                        name=area_genre_col + '_emb')(air_genre_name[-1])\n", "        tmp = keras.layers.Flatten(name=area_genre_col + '_emb_flatten')(tmp)\n", "        air_genre_name_emb.append(tmp)\n", "\n", "    air_genre_name_emb = keras.layers.concatenate(air_genre_name_emb)\n", "    air_genre_name_emb = Dense(4, activation='sigmoid', name='final_air_genre_emb')(air_genre_name_emb)\n", "\n", "    air_area_name_emb = keras.layers.concatenate(air_area_name_emb)\n", "    air_area_name_emb = Dense(4, activation='sigmoid', name='final_air_area_emb')(air_area_name_emb)\n", "    \n", "    air_area_code = Input(shape=(1,), dtype='int32', name='air_area_code')\n", "    air_area_code_emb = Embedding(len(train['air_area_name'].unique()), 8, input_shape=(1,), name='air_area_code_emb')(air_area_code)\n", "    air_area_code_emb = keras.layers.Flatten(name='air_area_code_emb_flatten')(air_area_code_emb)\n", "    \n", "    air_genre_code = Input(shape=(1,), dtype='int32', name='air_genre_code')\n", "    air_genre_code_emb = Embedding(len(train['air_genre_name'].unique()), 5, input_shape=(1,),\n", "                                   name='air_genre_code_emb')(air_genre_code)\n", "    air_genre_code_emb = keras.layers.Flatten(name='air_genre_code_emb_flatten')(air_genre_code_emb)\n", "\n", "    \n", "    holiday_flg = Input(shape=(1,), dtype='float32', name='holiday_flg')\n", "    year = Input(shape=(1,), dtype='float32', name='year')\n", "    min_visitors = Input(shape=(1,), dtype='float32', name='min_visitors')\n", "    mean_visitors = Input(shape=(1,), dtype='float32', name='mean_visitors')\n", "    median_visitors = Input(shape=(1,), dtype='float32', name='median_visitors')\n", "    max_visitors = Input(shape=(1,), dtype='float32', name='max_visitors')\n", "    count_observations = Input(shape=(1,), dtype='float32', name='count_observations')\n", "    rs1_x = Input(shape=(1,), dtype='float32', name='rs1_x')\n", "    rv1_x = Input(shape=(1,), dtype='float32', name='rv1_x')\n", "    rs2_x = Input(shape=(1,), dtype='float32', name='rs2_x')\n", "    rv2_x = Input(shape=(1,), dtype='float32', name='rv2_x')\n", "    rs1_y = Input(shape=(1,), dtype='float32', name='rs1_y')\n", "    rv1_y = Input(shape=(1,), dtype='float32', name='rv1_y')\n", "    rs2_y = Input(shape=(1,), dtype='float32', name='rs2_y')\n", "    rv2_y = Input(shape=(1,), dtype='float32', name='rv2_y')\n", "    total_reserv_sum = Input(shape=(1,), dtype='float32', name='total_reserv_sum')\n", "    total_reserv_mean = Input(shape=(1,), dtype='float32', name='total_reserv_mean')\n", "    total_reserv_dt_diff_mean = Input(shape=(1,), dtype='float32', name='total_reserv_dt_diff_mean')\n", "    date_int = Input(shape=(1,), dtype='float32', name='date_int')\n", "    var_max_lat = Input(shape=(1,), dtype='float32', name='var_max_lat')\n", "    var_max_long = Input(shape=(1,), dtype='float32', name='var_max_long')\n", "    lon_plus_lat = Input(shape=(1,), dtype='float32', name='lon_plus_lat')\n", "\n", "    date_emb = keras.layers.concatenate([dow_emb, month_emb, year, holiday_flg])\n", "    date_emb = Dense(5, activation='sigmoid', name='date_merged_emb')(date_emb)\n", "\n", "    cat_layer = keras.layers.concatenate([holiday_flg, min_visitors, mean_visitors,\n", "                    median_visitors, max_visitors, count_observations, rs1_x, rv1_x,\n", "                    rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y,\n", "                    total_reserv_sum, total_reserv_mean, total_reserv_dt_diff_mean,\n", "                    date_int, var_max_lat, var_max_long, lon_plus_lat,\n", "                    date_emb, air_area_name_emb, air_genre_name_emb,\n", "                    air_area_code_emb, air_genre_code_emb, air_store_id_emb])\n", "\n", "    m = Dense(hidden1_neurons, name='hidden1',\n", "             kernel_initializer=keras.initializers.RandomNormal(mean=0.0,\n", "                            stddev=0.05, seed=None))(cat_layer)\n", "    m = keras.layers.PReLU()(m)\n", "    m = keras.layers.BatchNormalization()(m)\n", "    \n", "    m1 = Dense(hidden2_neurons, name='sub1')(m)\n", "    m1 = keras.layers.PReLU()(m1)\n", "    m = Dense(1, activation='relu')(m1)\n", "\n", "    inp_ten = [\n", "        holiday_flg, min_visitors, mean_visitors, median_visitors, max_visitors, count_observations,\n", "        rs1_x, rv1_x, rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y, total_reserv_sum, total_reserv_mean,\n", "        total_reserv_dt_diff_mean, date_int, var_max_lat, var_max_long, lon_plus_lat,\n", "        dow, year, month, air_store_id, air_area_code, air_genre_code\n", "    ]\n", "    inp_ten += air_area_name\n", "    inp_ten += air_genre_name\n", "    model = keras.Model(inp_ten, m)\n", "    model.compile(loss='mse', optimizer='rmsprop', metrics=['acc'])\n", "\n", "    return model"], "metadata": {"_uuid": "6df537d60eca75ceb2445359aa67ba8c1688ccda", "collapsed": true, "_cell_guid": "282b5af0-fcfc-419e-9afa-6ca86756173c"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["model4 = get_nn_complete_model(train, hidden2_neurons=12)\n", "for i in range(5):\n", "    model4.fit(nn_train[0], nn_train[1], epochs=3, verbose=1,\n", "        batch_size=256, shuffle=True, validation_split=0.15)\n", "    model4.fit(nn_train[0], nn_train[1], epochs=8, verbose=0,\n", "        batch_size=256, shuffle=True)\n", "print(\"Model4 trained\")"], "metadata": {"_uuid": "9dc55a87c7bb1b2c7c4f599e3f58bd35c9a6d6d7", "collapsed": true, "_cell_guid": "4694a89d-0629-4d6e-b646-79ad0b5a7b83"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["preds4 = pd.Series(model4.predict(nn_train[0]).reshape(-1)).clip(0, 6.8).values\n", "\n", "print('RMSE NeuralNetwork: ', RMSLE(np.log1p(train['visitors'].values), preds4))"], "metadata": {"_uuid": "513635a989d672572fb3e30740ebc33785beade7", "collapsed": true, "scrolled": true, "_cell_guid": "45f736a7-30df-4339-a803-251e6e818734"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["model4.summary()"], "metadata": {"_uuid": "02cec58e287996c25bc64249b4bbd212ddb9666b", "collapsed": true, "_cell_guid": "ada1ed4f-b6a4-4232-ade4-2e20dc2886fc"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["layer_names = {  # layers to display\n", "               '  Area Embedding':    'air_area_code_emb_flatten',\n", "               '  Genre Embedding':   'air_genre_code_emb_flatten', \n", "               '  Store Embedding':   'air_store_id_emb_flatten',\n", "               '  Concatenated Data': 'concatenate_4',\n", "               '  1st Dense Input':   'hidden1',\n", "               '  1st Dense Output':  'p_re_lu_1', \n", "               '  2nd Dense Input':   'sub1', \n", "               '  2nd Dense Output':  'p_re_lu_2'\n", "              }"], "metadata": {"_uuid": "4d658f4bcba886c22e1993a3d82edc34b390ac86", "collapsed": true, "_cell_guid": "c0884d20-45f1-407e-9d65-30fdcfa89894"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["for i in [0,5,13,14,15]:       # Visualize several test data points\n", "   # Get test data point\n", "    inp = []\n", "    for arr in nn_test[0]:\n", "        inp.append( arr[i].reshape(1, 1) )\n", "\n", "    print( '\\nCalculating activations for test case ', i)\n", "    activs = list()\n", "    for n in layer_names.keys():\n", "        activs += get_activations(model4, inp, print_shape_only=True, layer_name=layer_names[n])\n", "    \n", "    result = get_activations(model4, inp, print_shape_only=True, layer_name='dense_1')\n", "\n", "    display_activations_with_names(activs, [n for n in layer_names.keys()])\n", "    print( 'Final result for test case', i, ': ', result[0].item(0), '\\n\\n\\n')"], "metadata": {"_uuid": "4074fcef371e628088ce7f3793e57764f55a0bcd", "collapsed": true, "scrolled": false, "_cell_guid": "88f68b6f-014f-4454-93ba-6e4e81124f97"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["# Final layer connections\n", "model4.get_weights()[-2]"], "metadata": {"_uuid": "4713f26686446549e2a9eec88b91a79723a8f0c1", "collapsed": true, "_cell_guid": "4dc90b4a-abfe-4802-9dec-109a16eb3010"}, "cell_type": "code", "execution_count": null, "outputs": []}, {"source": ["The specific arrangement of weights depends randomly on the particular training run, so I can't make specific comments, but it's interesting to look at how the weights map the final hidden layer to the result."], "metadata": {"_uuid": "f17f25b44de0bccc30f5b62dbe43e6924b03f3e3", "_cell_guid": "be15d9bc-1c50-47a7-8eef-a84667f36978"}, "cell_type": "markdown"}], "nbformat_minor": 1, "nbformat": 4}