{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"I will be using my learning from data exploration excercise at https://www.kaggle.com/lokendradevangan/home-credit-initial-data-exploration  for developing my first model.  I will also borrow information from other Kagglers to improvise my model "},{"metadata":{"trusted":true,"_uuid":"4f143601af1d59c51e720390878b5358764e2a76","collapsed":true},"cell_type":"code","source":"##Initial data understanding \n#this is training dataset. I will be creating two sample using below data sets as my training and validation dataset. This will be done after preprocessing of the data.\ndataset=pd.read_csv(\"../input/application_train.csv\")\n##test dataset which is to be predicted\ntest=pd.read_csv(\"../input/application_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47ae3589b3e560dda17add47467a2314df1b68d4","collapsed":true},"cell_type":"code","source":"#reducing the unique values in occupation by grouping by skill level. This grouping can differ based on more information about each occupation\ndataset['NAME_TYPE_SUITE'].replace({'Children':'Family',\n                                    'Group of people':'Other',\n                                    'Other_A':'Other',\n                                    'Other_B':'Other',\n                                    'Spouse, partner':'Family'},inplace=True)\n\ntest['NAME_TYPE_SUITE'].replace({'Children':'Family',\n                                    'Group of people':'Other',\n                                    'Other_A':'Other',\n                                    'Other_B':'Other',\n                                    'Spouse, partner':'Family'},inplace=True)\ndataset.groupby(['OCCUPATION_TYPE']).SK_ID_CURR.count()\n\ndataset.groupby(['NAME_EDUCATION_TYPE']).SK_ID_CURR.count()\ndataset.groupby(['NAME_EDUCATION_TYPE']).TARGET.mean() \ndataset['NAME_EDUCATION_TYPE'].replace({'Academic degree':'Higher education '},inplace=True)\ntest['NAME_EDUCATION_TYPE'].replace({'Academic degree':'Higher education '},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4ccd6f0c1d28440d3f48b753cbe741fc2f4692b"},"cell_type":"code","source":"#reducing the unique values in occupation by grouping by skill level. This grouping can differ based on more information about each occupation\ndataset['OCCUPATION_TYPE'].replace({'High skill tech staff':'High_Skill',\n                                    'Managers':'High_Skill',\n                                    'Accountants':'High_Med_Skill',\n                                    'HR staff':'High_Med_Skill',\n                                    'Core staff':'Med_Skill',\n                                   'Cooking staff':'Med_Skill',\n                                    'Realty agents':'Med_Skill',\n                                    'Sales staff':'Med_Skill',\n                                    'IT staff':'High_Med_Skill',\n                                    'Medicine staff':'High_Med_Skill',\n                                    'Secretaries':'Med_Skill',\n                                    'Security staff':'Med_Skill',\n                                    'Cleaning staff':'Low_Skill',\n                                      'Laborers':'Low_Skill',\n                                      'Low-skill Laborers':'Low_Skill',\n                                      'Cleaning staff':'Low_Skill',\n                                    'Waiters/barmen staff':'Low_Skill',\n                                    'Private service staff':'Low_Skill',\n                                    'Drivers':'Med_Skill'\n                                   },inplace=True)\ntest['OCCUPATION_TYPE'].replace({'High skill tech staff':'High_Skill',\n                                    'Managers':'High_Skill',\n                                    'Accountants':'High_Med_Skill',\n                                    'HR staff':'High_Med_Skill',\n                                    'Core staff':'Med_Skill',\n                                   'Cooking staff':'Med_Skill',\n                                    'Realty agents':'Med_Skill',\n                                    'Sales staff':'Med_Skill',\n                                    'IT staff':'High_Med_Skill',\n                                    'Medicine staff':'High_Med_Skill',\n                                    'Secretaries':'Med_Skill',\n                                    'Security staff':'Med_Skill',\n                                    'Cleaning staff':'Low_Skill',\n                                      'Laborers':'Low_Skill',\n                                      'Low-skill Laborers':'Low_Skill',\n                                      'Cleaning staff':'Low_Skill',\n                                    'Waiters/barmen staff':'Low_Skill',\n                                    'Private service staff':'Low_Skill',\n                                    'Drivers':'Med_Skill'\n                                   },inplace=True)\ndataset.groupby(['OCCUPATION_TYPE']).SK_ID_CURR.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8f3191cc2ca928834f03963967a7a2a8e2d196da"},"cell_type":"code","source":"#grouping\ndataset['NAME_INCOME_TYPE'].replace({'Businessman':'Other','Student':'Other','Maternity leave':'Other'},inplace=True)\ntest['NAME_INCOME_TYPE'].replace({'Businessman':'Other','Student':'Other','Maternity leave':'Other'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8ab9e31ec4eb9729752b82f77478992fb0d2a3e"},"cell_type":"code","source":"print('Testing Features shape: ', test.shape)\nprint('Training Features shape: ', dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa1a6a30e51d1cfe6864909271c818d3941c6b7d"},"cell_type":"code","source":"# Create a label encoder object\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\napp_train=dataset\napp_test=test\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in dataset:\n    if dataset[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(dataset[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(dataset[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(dataset[col])\n            app_test[col] = le.transform(test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b03e209c3b703401a98593d929727789f4f23dc7"},"cell_type":"code","source":"app_train= pd.get_dummies(app_train)\napp_test= pd.get_dummies(app_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1adaa45c4e6d45588796094f43ed1e4c45c2e1d8"},"cell_type":"code","source":"app_train.dtypes.value_counts()\napp_test.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47923b25d57e57516e938e3566fed745f3e6e71a"},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_train.fillna(dataset.median(),inplace = True)\n\napp_test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\napp_test.fillna(dataset.median(),inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1f994379b5cca2fc7bae7cf9f7cf53dc8ed7364"},"cell_type":"code","source":"#replace all  NaN in the var_list with zero\nVar_List=('OBS_30_CNT_SOCIAL_CIRCLE','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE',\n        'DAYS_LAST_PHONE_CHANGE','AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK',\n         'AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR')\ndef missing_val_replace(data,Var_List):\n    for col in data:\n        for i in Var_List:\n            if col==i:\n                data[col].fillna(0)\n                print (col)\n    return data\napp_train=missing_val_replace(app_train,Var_List) \n#replace all other NaN with median values\napp_train=app_train.fillna(app_train.median)\napp_test=missing_val_replace(app_test,Var_List) \n#replace all other NaN with median values\napp_test=app_test.fillna(app_test.median)\n\napp_train.dtypes.value_counts()\napp_test.dtypes.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1fc8e76d9159258c81e948c09c75ed8420eea36"},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\napp_trainv2=app_train\n# Add the target back in\n#app_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Training Features shape: ', app_trainv2.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bec07b0ab506133441e28a5f9fd2f905d9977f3"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns=['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54dc5f3a0c5d0bb3aadc84c006d41c63c9681b9c"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02c531b19636aa58568cec6bea0b0668b76f44af","collapsed":true},"cell_type":"code","source":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99a0a8ac449dece2ab83a86d01c647676fb77523"},"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4ade11ca16d915facc7f638b5e81760cf91dad49"},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"41e96ce0faa618d3c7ab89d3dcbfbfb81b4d0000"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"69d287b396b08eb232d4de3657569221b59a030c"},"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"af44ac0d21b9b21c75a03c0e461e999f150d3dcb"},"cell_type":"code","source":" #Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\nsubmit.to_csv('random_forest_baseline.csv', index = False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}