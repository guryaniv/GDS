{"cells":[{"metadata":{"trusted":true,"_uuid":"4c6ee3c5349e368a806a570f6d69de26ce91640c"},"cell_type":"code","source":"import os\nprint(os.listdir()) #list files avalilable\n\nimport numpy as np\nimport pandas as pd\n#import the training data\ntrain_data=pd.read_csv(\"../input/application_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f8d8501dab6d4707e83c3d86f1c16e367086aa1"},"cell_type":"code","source":"test=pd.read_csv('../input//application_test.csv') #import test data set \ntest.head() # to show first 5 rows of test data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ee518b5d905dde8a4dc95ed2e9de59487dccb791"},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fbe042f1defbf17723bc417ed82497d1b52572be"},"cell_type":"code","source":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n         # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1016f935feb099de01e641eb5ae537675d21d70c"},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(train_data)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a2101e7aa4e0e81f3dff1ee6d7c13094e952330b"},"cell_type":"code","source":"# Number of each type of column\nnp.where(train_data.dtypes=='object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"24fc5d9b0778431fc8a0e7ae00e5238c18263b24"},"cell_type":"code","source":"# Number of each type of column\ntrain_data.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"55261e7b21cee1bb7d29b34a040d9425fc344feb"},"cell_type":"code","source":"train_data.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b878e320ee3f7e3f0ebb04a686b4fe28c4482f91"},"cell_type":"code","source":"train_data.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"21f0d8a4f01c0479c40ef20d1427b669fb38829a"},"cell_type":"code","source":"# Create a label encoder object\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in train_data:\n    if train_data[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(train_data[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(train_data[col])\n            # Transform both training and testing data\n            train_data[col] = le.transform(train_data[col])\n            test[col] = le.transform(test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2057ab4809e00aa6fddf8f740446aded8761da6b"},"cell_type":"code","source":"for i in train_data.columns:\n    if train_data[i].dtypes=='object':\n        print(train_data[i].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e825f06e3687ec8e0c2c05dfbd4715e26d6750cf"},"cell_type":"code","source":"#one hot encoding\ntrain_data= pd.get_dummies(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5102b4aad4f45277c3046e6833ccbe470bbab81b"},"cell_type":"code","source":"test= pd.get_dummies(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"06bee94c73a3b14e92e3462faa7f64faf94a4c38"},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"72de8bb596464721d23d5e4f6dc466993a827df5"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9897c6f566a2180079c94fc244c0edd144c713fa"},"cell_type":"markdown","source":"<h1>Aligning Training and Testing Data</h1>\n<body>There need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!</body>"},{"metadata":{"trusted":false,"_uuid":"c792d16d46e35039e99bd5f7f9396193c5304474"},"cell_type":"code","source":"train_target=train_data['TARGET']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"828845e3b04926d321a97430856ba338ca71f0b9"},"cell_type":"code","source":"train_data, test = train_data.align(test, join = 'inner', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c18df8084b7d05b12e5baf283e238fb01eb9bd1f"},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3db58982a1c98b8e7aee9627e31e1c3f27f559a8"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"02bf1da2cad0fc881c57eba490057dbe2e40b0fc"},"cell_type":"code","source":"train_data['TARGET']=train_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3a691547e54ad446e807dcc3f8264ddae0b40273"},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"680fe10218a5c36fdf0dec6b87e1f992da7aba6a"},"cell_type":"markdown","source":"<h1>Exterior sources</h1>"},{"metadata":{"trusted":false,"_uuid":"f339175d769bb7876708146e2449b47ee78316ae"},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = train_data[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bca04968a052cf7dc7b750c809b8ae2cc8a3dc97"},"cell_type":"code","source":"poly_features= train_data[['TARGET','EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"99ad7916b806f3a9c0b35cbd44755b2e700c6e91"},"cell_type":"code","source":"poly_features_test= test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c0a89ba7aa2840794a1e3ec351846053dc617fa4"},"cell_type":"code","source":"# to impute missing values we gonna use sklearn.imputer method \n\nfrom sklearn.preprocessing import Imputer\nimputer= Imputer(strategy='median')\n#poly_features['TARGET']=poly_target\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns = ['TARGET'])\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\n\n# Create the polynomial object with specified degree\n#poly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0ed86aaccc233f9fa4762d8f2edc7551d4911a0b"},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"92c9e84124a557b46035133abbf3bd27851dca85"},"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"66fdf142267224d869704f5647b68d3b76872a02"},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ce0cdcab0d7ddba1c896a748742a7cd1760d6ad4"},"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"368d41dff1d84f5e8c2b2f60019faf8918290d53"},"cell_type":"code","source":"poly_features.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6056cf804dac42fcc030e675465abf2cd60dc1c7"},"cell_type":"code","source":"# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"757dfbbcd40e69a14789c2e4e00bf95d5e4d4edb"},"cell_type":"code","source":"poly_features.to_pickle(\"Poly_features.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fd474cb6d008da0d48e252ddd9a98a76bdfe3deb"},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = train_data['SK_ID_CURR']\ntrain_data_poly = train_data.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = test['SK_ID_CURR']\ntest_poly = test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\ntrain_data_poly, test_poly = train_data_poly.align(test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', train_data_poly.shape)\nprint('Testing data with polynomial features shape:  ', test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"eb82d77d1a427bd0b4ec30276a6e082c80ea3c6a"},"cell_type":"code","source":"train_data_poly.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d0b30d7db75389e6b31077ce2a757607672072f4"},"cell_type":"code","source":"test_poly.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0db5a237e9fc983674f43ea18960e7f0b99f6d71"},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5a5c8f02bd9c5d162aecc6d2f235136ab8059a3"},"cell_type":"markdown","source":"<h1>Logistic regression implementation</h1>\n"},{"metadata":{"trusted":false,"_uuid":"d5af9f2f86a96701cc7a726cdb0a3924a383bd38"},"cell_type":"code","source":"from sklearn.preprocessing import  Imputer\n\n# Drop the target from the training data\nif 'TARGET' in train_data:\n    train = train_data.drop(columns = ['TARGET'])\nelse:\n    train = train_data.copy()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d5db7ab5169934443225ba93d826c2681c15c9e5"},"cell_type":"code","source":"# Feature names\nfeatures = list(train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"405dbd831ac165cfdc2d90d995534e5baa571360"},"cell_type":"code","source":"# Copy of the testing data\ntest_test = test.copy() # after imputation test set would be converted to numpy array which in turn would be made difficult \n#for visualization.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d44e40f4056c3dfda8f1cb4f462f281d728dce75"},"cell_type":"code","source":"test_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8c461441ea7c87ec77cabc5c45d4f1530933d9ad"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c1f433ed5be2c28945279a064c4baad43fb2dc30"},"cell_type":"code","source":"#ignore_col= ['SK_ID_CURR']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1be6ab5e0ef8711c712714fe1f7378935c22a129"},"cell_type":"code","source":"# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n\n#train[\"AMT_GOODS_PRICE\"].fillna(trainX[\"AMT_GOODS_PRICE\"].mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fc892e897321f5f37b034c2f7f52e9043a3d2210"},"cell_type":"code","source":"# Fit on the training data\nimputer.fit(train)\n# Transform both training and testing data\ntrain = imputer.transform(train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2222afcad3c3731cd56db3c04980b23af63b21a6"},"cell_type":"code","source":"test = imputer.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b0d87443490aabdc26cd4d7c7c1bf10f7f351180"},"cell_type":"code","source":"print('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c110cc4d3a886146c43759fce7dd3177ac990cfb"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\ntrain_X,test_X,train_y,test_y= train_test_split(train,train_target,test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d0374b85724db737aabf3f9ea8a24c3688728564"},"cell_type":"code","source":"# Make the model with the specified regularization parameter\nClassifier = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nClassifier.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4fd30a122467dca6c767c2920fdf795de70aa38e"},"cell_type":"code","source":"ypred= Classifier.predict(test_X) # predicting on validation set(test_X) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"409115294f6998625b85263cb23337f0ccd59c23"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy= accuracy_score(test_y,ypred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a597ca1acafb3efda54b6496d82ada9716cc9e82"},"cell_type":"code","source":"accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59172f9a722890fcb0d710570971d9cc2a6357cc"},"cell_type":"markdown","source":"### K fold cross validation"},{"metadata":{"trusted":false,"_uuid":"b99fb99310142c0fb4c9e6c87a4f6b12405b3cff"},"cell_type":"code","source":"# k fold cross validation\nfrom sklearn.model_selection import cross_val_score\ncv_score= cross_val_score(Classifier, train,train_target,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dff66efbf8f05b7030edf912de47a67787396069"},"cell_type":"code","source":"cv_score\ncv_score.mean()\n# after performing 10 fold cross validation cv_score is 91.9 which is similar to the accuracy_score of the logistic regresion \n#classifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"536861d0eebe4cee53ab7a85532bbf440250d1aa"},"cell_type":"markdown","source":"### Applying logreg on test set"},{"metadata":{"trusted":false,"_uuid":"4784bfbddfc0381fe39735c7f3e0f4efa4be457c"},"cell_type":"code","source":"test_pred= Classifier.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"20569cd1514cbed90c0a8cbaf7ff2159132fac09"},"cell_type":"code","source":"test_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"05f3baa5a0f72b4cd61b307951b55e164a185785"},"cell_type":"code","source":"test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"915ee7357166b7ba3fa0d455c1cd86c0c120333e"},"cell_type":"code","source":"# Make predictions\n# Make sure to select the second column only\n#Classifier_pred = Classifier.predict_proba(test)[:, 1]\n#ypred=  Classifier.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"25269eda317517680d124bdc1234266ab58a8bdc"},"cell_type":"code","source":"# Submission dataframe\nsubmit = test_test[['SK_ID_CURR']]\nsubmit['TARGET'] = test_pred\n\nsubmit.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1fd4b1b7bfa459c95b474c868e2530c7fb9668f0"},"cell_type":"code","source":"#pred=np.apply_along_axis((lambda X: 1 if X>0.5 else 0),0,Classifier_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e4a8b2317780642399ed2790e6782ec6f7ae59c0"},"cell_type":"code","source":"submit.to_csv(\"submit1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fb6c75d99c5c16bf8bc09093711e4da4c1446bf7"},"cell_type":"code","source":"submit.to_csv(\"Logreg.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bec19bbf852c1fa1d3d73f1ce52c115d48ec7ed3"},"cell_type":"code","source":"### Logistic regression performs bad on test set. completely a biased model. shouldn't be deployed.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4816efbec3a3423daf34449f4fe54b19188595fc"},"cell_type":"markdown","source":"### Boosting Algorithm"},{"metadata":{"trusted":false,"_uuid":"9338edab5b0d5308dd4cbdfee000aedacc4bfd2f"},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3181b1b985b637f34a48698cb71f0aed8bd36eea"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nXtrain, Xval, ytrain, yval = train_test_split(train_data_poly, train_target, test_size = 0.2,\n                                             random_state = 1982)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6bf5335cfa87fddabc67d8242a76ee141f4c87f9"},"cell_type":"code","source":"#total rows if we append test set to the training set. just for assumption no use in later code\n\ntrain_1=pd.concat([train_data_poly, test_poly], axis= 0)\ntrain_1.shape\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2373f9d304a3b344727a42d3f7942c218338b73"},"cell_type":"markdown","source":"### Dmarices for train, val and test set"},{"metadata":{"trusted":false,"_uuid":"d7c4d17335ef574681e0e1cc1e35b7f8bb493e40"},"cell_type":"code","source":"# to feed data to xgboost first training set is transformed into Dmatrix. in code below train, validation and test sets are transformed.\nxgtrain = xgb.DMatrix(Xtrain, label = ytrain)\nxgval = xgb.DMatrix(Xval, label = yval) \nxgtest = xgb.DMatrix(test_poly)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"869e62b760ef5fb08db4c0127909e87f490392af"},"cell_type":"code","source":"watchlist = [(xgtrain,'train'),(xgval, 'eval')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"17d405a8eb2a301fbcb3a8cb1f2a4ed7528bddd6"},"cell_type":"code","source":"params = {}\nparams[\"objective\"] =  \"binary:logistic\"\nparams[\"booster\"] = \"gbtree\"\nparams[\"max_depth\"] = 7\nparams[\"eval_metric\"] = 'auc'\nparams[\"subsample\"] = 0.8\nparams[\"colsample_bytree\"] = 0.8\nparams[\"silent\"] = 1\nparams[\"seed\"] = 4\nparams[\"eta\"] = 0.1\n\nplst = list(params.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5d0c7845fd8bda875838b332edb224f0ad847baf"},"cell_type":"code","source":"num_rounds = 500\nmodel_cv = xgb.train(plst, xgtrain, num_rounds, evals = watchlist, early_stopping_rounds = 10, verbose_eval = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"afd0affca8cd04e63840a648ff8f9b61095f5141"},"cell_type":"code","source":"testxg_pred = model_cv.predict(xgtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"386e3b80a20e63ae8715939f0133e2edaa275015"},"cell_type":"code","source":"test_id=test_test['SK_ID_CURR'] #this code is to run the above code","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"05ae61c085a55172d89ef26300b104583dc22f98"},"cell_type":"code","source":"preds = pd.DataFrame({\"SK_ID_CURR\": test_id, \"TARGET\": testxg_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"758a50e716a525a9613291bfb78c5d80a4ad25e7"},"cell_type":"code","source":"preds.to_csv(\"xgb_model22aug.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"63f84cbeba688383784cce1f9b2ccf1277b789e0"},"cell_type":"code","source":"testxg_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"73a0a57b378cedb0bf75926c6bf6f6a41a1c1bd1"},"cell_type":"code","source":"#  to determine the acccuracy or any metric we need test set of target variable which we don't have with us. so I'll submit the \n# xgb_model22aug.csv to check how good the model predicts on unseen data.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbfb9cddc9a956de18e6a2b46b5cce6a9d061e80"},"cell_type":"markdown","source":"### feature importance"},{"metadata":{"trusted":false,"_uuid":"e526c8ec26da1a361bc51844156947eada2c2140"},"cell_type":"code","source":"feat_imp = pd.Series(model_cv.get_fscore()).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a2c50afd12ec494688da0f498c7b701fc5a89b5c"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfeat_imp[:25].plot(kind='bar', title='Feature Importances')\nplt.ylabel('Feature Importance Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9138fab38fa4c872ad49935f5cc6dc2ed4f89e3e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"64245f4c690061cb63373e6121b7d14f164d162e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8ad4bf6b7382ca2ecabb16eab34b308e47da4675"},"cell_type":"code","source":"import pickle\npickle.dump(model_cv, open(\"xgb_model13july.pickle.dat\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a45ca585231471d013a2b365846aa7c1dd83f33c"},"cell_type":"code","source":"loaded_model = pickle.load(open(\"xgb_model13july.pickle.dat\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fe2d23d23cf4a439b185fbb0e2b0a83600650b68"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}