{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ed90a97c-db49-08fa-ad38-7812143922a8"
      },
      "source": [
        "# A pipeline for predictive analytics using text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "25269837-97b9-e4a4-15ed-5f11058a4656"
      },
      "source": [
        "## Before we start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "568cc4f2-d603-356d-404a-4b7182516e1e"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.cross_validation import train_test_split\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "df = pd.read_csv('../input/Combined_News_DJIA.csv')\n",
        "print(df.shape)\n",
        "import matplotlib\n",
        "matplotlib.rcParams[\"figure.figsize\"] = \"8, 8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f4146ff8-6427-906e-5737-0608e9287583"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8f17b4a1-285b-cd07-24e0-6848de22581f"
      },
      "source": [
        "Credits to **Kate**, for the great method of combining all headlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e0cc03b-79ff-d3ed-26ed-ba0d85e9284e"
      },
      "outputs": [],
      "source": [
        "df['Combined']=df.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3d6c99ae-c180-ad8a-59cf-35fed9ed7928"
      },
      "outputs": [],
      "source": [
        "train,test = train_test_split(df,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "41156b20-23b2-723a-88a1-af5a53e5ee29"
      },
      "source": [
        "## Simple EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1b7363b6-b7ff-1669-c1c5-52444a786ff7"
      },
      "source": [
        "We first have a quick look at the text, looking for words that can be the indicator of decrease on stock price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "38acab01-47eb-6f15-46e9-339873debc09"
      },
      "outputs": [],
      "source": [
        "non_decrease = train[train['Label']==1]\n",
        "decrease = train[train['Label']==0]\n",
        "print(len(non_decrease)/len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "708699e0-eba0-5444-9694-99853ffd5b00"
      },
      "source": [
        "We can see that the occurrence of non-decrease situation is almost the **same** as that of a decrease market. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dcb87cfe-cfc6-9468-5a61-3845ec3aaa05"
      },
      "outputs": [],
      "source": [
        "def to_words(content):\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", content) \n",
        "    words = letters_only.lower().split()                             \n",
        "    stops = set(stopwords.words(\"english\"))                  \n",
        "    meaningful_words = [w for w in words if not w in stops] \n",
        "    return( \" \".join( meaningful_words )) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "57b78e88-5a85-77da-c1c8-6b32d240675a"
      },
      "outputs": [],
      "source": [
        "non_decrease_word=[]\n",
        "decrease_word=[]\n",
        "for each in non_decrease['Combined']:\n",
        "    non_decrease_word.append(to_words(each))\n",
        "\n",
        "for each in decrease['Combined']:\n",
        "    decrease_word.append(to_words(each))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4da1e092-1aaf-357c-d34d-f7e8ff68d162"
      },
      "outputs": [],
      "source": [
        "wordcloud1 = WordCloud(background_color='black',\n",
        "                      width=3000,\n",
        "                      height=2500\n",
        "                     ).generate(decrease_word[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad657f1f-4f6c-7d39-1668-9c3b16a918a2"
      },
      "outputs": [],
      "source": [
        "plt.figure(1,figsize=(8,8))\n",
        "plt.imshow(wordcloud1)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "246cd815-d945-b2d3-5a67-612eb539f619"
      },
      "outputs": [],
      "source": [
        "wordcloud2 = WordCloud(background_color='white',\n",
        "                      width=3000,\n",
        "                      height=2500\n",
        "                     ).generate(non_decrease_word[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b183a274-8301-587a-debe-7be03d4ae710"
      },
      "outputs": [],
      "source": [
        "plt.figure(1,figsize=(8,8))\n",
        "plt.imshow(wordcloud2)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6778e1c2-6b5e-ea73-1a90-db189ed24893"
      },
      "source": [
        "If you look at two word clouds, you may find that even in the case of non-decreased situation the **Political sensitive words** are dominant.  Therefore, I have a feeling that the overall results of the classification may not be quite good since the features of two classes are **not that distinct**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4bf6e36d-9ef9-067f-0494-9a881631bc25"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a1e6b4ea-258b-60a2-a676-4e35a115a744"
      },
      "source": [
        "I choose to use **tf-idf** model for feature extraction as it can make the informative features weighted enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d515a6cf-0a26-ac0b-d2d6-b10d82785ce1"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "train_text = []\n",
        "test_text = []\n",
        "for each in train['Combined']:\n",
        "    train_text.append(to_words(each))\n",
        "\n",
        "for each in test['Combined']:\n",
        "    test_text.append(to_words(each))\n",
        "train_features = tfidf.fit_transform(train_text)\n",
        "test_features = tfidf.transform(test_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "59201790-7841-da44-04de-ada5436ccafc"
      },
      "source": [
        "## Model fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1abb84a3-67e1-1f5b-5765-a03d73c94b03"
      },
      "source": [
        "`In this part, I will fit seven different classifiers on the training set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6086feaa-ac14-ee51-4511-981484eef54a"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from ggplot import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d047899b-af56-c55d-9398-336d892644b0"
      },
      "outputs": [],
      "source": [
        "Classifiers = [\n",
        "    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(n_estimators=200),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1d9edb07-b840-ed14-3840-51c426fb3fe7"
      },
      "outputs": [],
      "source": [
        "dense_features=train_features.toarray()\n",
        "dense_test= test_features.toarray()\n",
        "Accuracy=[]\n",
        "Model=[]\n",
        "for classifier in Classifiers:\n",
        "    try:\n",
        "        fit = classifier.fit(train_features,train['Label'])\n",
        "        pred = fit.predict(test_features)\n",
        "        prob = fit.predict_proba(test_features)[:,1]\n",
        "    except Exception:\n",
        "        fit = classifier.fit(dense_features,train['Label'])\n",
        "        pred = fit.predict(dense_test)\n",
        "        prob = fit.predict_proba(dense_test)[:,1]\n",
        "    accuracy = accuracy_score(pred,test['Label'])\n",
        "    Accuracy.append(accuracy)\n",
        "    Model.append(classifier.__class__.__name__)\n",
        "    print('Accuracy of '+classifier.__class__.__name__+' is '+str(accuracy))\n",
        "    fpr, tpr, _ = roc_curve(test['Label'],prob)\n",
        "    tmp = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n",
        "    g = ggplot(tmp, aes(x='fpr', y='tpr')) +geom_line() +geom_abline(linetype='dashed')+ ggtitle('Roc Curve of '+classifier.__class__.__name__)\n",
        "    print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "da665177-8979-8c22-74c4-2faefacd83d1"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "db557c90-4aec-7cf9-d29d-d33daa5d5043"
      },
      "source": [
        "Obviously, the overall performances of seven models are not quite good. The features are not distinct enough, therefore resulting in a bad model fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7456e452-245f-509d-59e5-53730aeb2ced"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}