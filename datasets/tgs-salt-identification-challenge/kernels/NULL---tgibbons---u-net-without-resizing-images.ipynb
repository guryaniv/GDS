{"cells":[{"metadata":{"_uuid":"3ca70217a19cf7a2c79f0ee71124a31ad2cc6846"},"cell_type":"markdown","source":"\n# Basic U-Net model without resizing images\n\nMost of the U-Net samples resize the images from 101x101 to 128x128 because U-net layers are easier to design when images are a factor of 2 in size. But the upsampling and downsampling introduces some issues. Here is an example of a u-net model that uses the starting 101x101 images without resizing. There is no upsampling or downsampling\n<br>\nThe U-net buffering on a few layers is changed from \"same\" to \"valid\" to handle the layers where the image size rounds down to a different size. For example, the traditional model goes from 50x50 -> 25x25 -> 12x12 -> 24x24 -> 48x48. Chaning the padding fixes this.\n\n## This is based on \"U-net, dropout, augmentation, stratification\" by Peter Hönigschmid \nhttps://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification\n<br>\nRather than resizing the images from 101x101 to 128x128, this kernel adjusts the padding on U-net \n"},{"metadata":{"_uuid":"fa525d982fdb7e518de47cd8ec52f6ee868d312b"},"cell_type":"markdown","source":"# Changelog\n- Changed uncov to uconv, but removed the dropout in the last layer\n- Corrected sanity check of predicted validation data (changed from ids_train to ids_valid)\n- Used correct mask (from original train_df) for threshold tuning (inserted y_valid_ori)\n## More Changes\n- Removed resize code, keep images at 101x101 and changes some layer padding to valid to fix size issues\n- Removed some of the displays and sanity checks to focus on the u-net code\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"},"cell_type":"markdown","source":"# Params and helpers"},{"metadata":{"_uuid":"e54e151245d665e42bb95d9cf2e1a33cb9440e48","trusted":true,"collapsed":true},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 101\n\n# removed upsample and downsample code, since resizing is not used","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530c358f2868a444e8233936996463a66c2cc4f3"},"cell_type":"markdown","source":"# Loading of training/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n\ntrain_df = train_df.join(depths_df)\n\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24d7f3d982bfa582b222f012129acdda55282b6d"},"cell_type":"markdown","source":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255."},{"metadata":{"_uuid":"b18c1f50cefd7504eae7e7b9605be3814c7cad6d","trusted":true},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86620c6a070571895f4f36ec050a25803915ed74","trusted":true},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e604848016fd6c57141c09235391fccc1ef52be6"},"cell_type":"code","source":"# Simple split of images into training and testing sets\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63ac58ab47921b4e4f54102e2c8b85fa318225f1"},"cell_type":"markdown","source":"# Build model"},{"metadata":{"_uuid":"a517622135321d17e4aaad749def999205da358c","trusted":false,"collapsed":true},"cell_type":"code","source":"def build_model(input_layer, start_neurons):\n    # standard size 128 -> 64   custome size 101 -> 50\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n    \n    # standard size 64 -> 32       custome size 50 -> 25\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    # standard size 32 -> 16       custome size 25 -> 12\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = Conv2D(start_n-neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n    \n    # standard size 16 -> 8       custome size 12 -> 6\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n    \n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n    \n    # standard size 8 -> 16         custome size 6-> 12\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    \n    # standard size 16 -> 32        custome size 12 -> 25\n    # Changed padding from \"same\" to \"valid\" to round up image to next size\n    #deconv3a = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])\n    uconv3 = Dropout(0.5)(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n    \n    # standard size 32 -> 64   custome size 25 -> 50\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n    uconv2 = Dropout(0.5)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n\n    # standard size 64 -> 128   custome size 50 -> 101\n    # Changed padding from \"same\" to \"valid\" to round up image to next size\n    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    uconv1 = Dropout(0.5)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n\n    #uconv1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aa78bd7c607e1f0e0235e4b2f82056c0361dac5","trusted":false,"collapsed":true},"cell_type":"code","source":"input_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16)\nmodel = Model(input_layer, output_layer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3399029adb039b049e3d6ca01fef30ed8653482b","trusted":false,"collapsed":true},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7ded4adc1757c88a1bea59ea36b1a9f7941bd28","trusted":false,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c007157c2fd3d7dadcaeee2a6376351852d1e565"},"cell_type":"markdown","source":"# Data augmentation"},{"metadata":{"_uuid":"88b3f57eac3ec3719b401730dc6d8d2d89d09ccc","trusted":false,"collapsed":true},"cell_type":"code","source":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7040f72549212dd4f71c13dfbd8bf013481ea369","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a6b1abaa4681cba3b608bc5f33cf260370d82a"},"cell_type":"markdown","source":"# Training"},{"metadata":{"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=10, verbose=2)\nfilepath=\"unet_wo_resizing-{epoch:02d}-{val_acc:.2f}.hdf5\"\nmodel_checkpoint = ModelCheckpoint(filepath, save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.000001)\nepochs = 200\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c824f6bca47f051500966c433ce7fb5a9528f6d7","trusted":false,"collapsed":true},"cell_type":"code","source":"# May want to load in top saved model here\n# model = load_model(\"./keras.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cf70d3c2fdab389bb566248ea14f0ab90de722f3"},"cell_type":"code","source":"# Predict the validation set ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"484d679067ad731f3cfc51dc570052f8085fe542"},"cell_type":"code","source":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([x for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd973023204ebf921fe1f23748856e6a6f692aa4","collapsed":true},"cell_type":"markdown","source":"# Scoring\nScore the model and do a threshold optimization by the best IoU."},{"metadata":{"_uuid":"d261beec66b6867ac0d5c94684f12aa08b70d638","trusted":false,"collapsed":true},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85f6d9567cec0ef8976730a6834b6569b6e108a0","trusted":false,"collapsed":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"183d37ad32bc2f1f0d17a9538702c45a826ccefc","trusted":false,"collapsed":true},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ced29761f2d1760245112a30a7abd4783b373dd","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"332a614c0ae837c115ec6563f355753ffbb8cd83"},"cell_type":"markdown","source":"# Submission\nLoad, predict and submit the test image predictions."},{"metadata":{"_uuid":"72128add82c6853441671fde67e7e66601a01787","trusted":false,"collapsed":true},"cell_type":"code","source":"# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ecb152b492c7126d12c5ef2c701eec8ea3d86f1","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"x_test = np.array([np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f278d0b87320c117b4ed7c116a991782b82ba5a7","trusted":false,"collapsed":true},"cell_type":"code","source":"preds_test = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"113f816f9db8b87ca7f6845fe6e61328ab606f41","trusted":false,"collapsed":true},"cell_type":"code","source":"pred_dict = {idx: RLenc(np.round(preds_test[i] > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4243166f91c4bcb4da00208f4f53dd912dbb429f","trusted":false,"collapsed":true},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6f2c5ca7f1dee414026794187af108b2e4f94e11"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}