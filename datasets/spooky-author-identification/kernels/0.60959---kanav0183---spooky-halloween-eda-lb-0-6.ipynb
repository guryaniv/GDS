{"nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["## 1)EDA\n", "##### 1.1)Importing LIbraries.\n", "##### 1.2)Exploration and Wordclouds.\n", "##### 1.3)Making some new features.\n", "## 2)LB~(0.6)\n", "##### 2.1)Countvectorizers\n", "##### 2.1)Baseline Logistic Regression Model"], "metadata": {"_uuid": "cd620708e85d3963d63a416b1f6e0ad8455ba29e", "_cell_guid": "f44d96e9-1d32-41f0-8067-e7ccd318cd8d"}, "cell_type": "markdown"}, {"outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import nltk\n", "from wordcloud import WordCloud, STOPWORDS\n", "from nltk.corpus import stopwords\n", "from wordcloud import WordCloud, STOPWORDS\n", "import pandas as pd\n", "import numpy as np\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n"], "execution_count": null, "metadata": {"_uuid": "e1a38479b7d02df2a8a086de9d8ec60bfffe5427", "collapsed": true, "_cell_guid": "0ad9554d-7295-4588-93a9-6b80d425538a"}, "cell_type": "code"}, {"source": ["##### Glimpse of Data"], "metadata": {"_uuid": "1648f8542570c26f14ce823ee9c13e089ba2643a", "_cell_guid": "76f636fa-9c16-4269-9f55-e704373d31a9"}, "cell_type": "markdown"}, {"outputs": [], "source": ["train.head()"], "execution_count": null, "metadata": {"_uuid": "6139d50ee519427a46eef37d22d721bc959e6998", "_cell_guid": "a729a474-3e05-47d1-b7ea-65727ad9b420"}, "cell_type": "code"}, {"outputs": [], "source": ["print(train.id[0],'---')\n", "print(train.text[0])"], "execution_count": null, "metadata": {"_uuid": "8090781d8df981b8c74a1a92ba70e3ccc4d01794", "_cell_guid": "06c84804-b900-4a41-99ae-031bfdb476b8"}, "cell_type": "code"}, {"source": ["## Is it Balanced Data?"], "metadata": {"_uuid": "9806cb013b117c5a46c08314123a86555e788f29", "_cell_guid": "5ef5ef83-43e1-4275-8dd8-5dea0f0a6531"}, "cell_type": "markdown"}, {"outputs": [], "source": ["plt.figure(figsize=(10,6))\n", "sns.countplot(train['author']);\n", "plt.title('Countplot for Authours_');\n", "plt.xlabel('Authors_',fontsize=20);"], "execution_count": null, "metadata": {"_uuid": "0dbf7b1856e4ecf89448a15779a06a48fd5ad61f", "_cell_guid": "d503ed56-a260-40a5-90eb-91784c1c1557"}, "cell_type": "code"}, {"source": ["So the anwer is yes!"], "metadata": {"_uuid": "7ce2494a3ad2de2aec75c9587e060a15df9ad71c", "_cell_guid": "016e5237-6dd8-4a34-b4a7-3d7f75a2be22"}, "cell_type": "markdown"}, {"outputs": [], "source": ["df_eap = train[train.author=='EAP']\n", "df_hpl = train[train.author=='HPL']\n", "df_mws = train[train.author=='MWS']"], "execution_count": null, "metadata": {"_uuid": "8634dc03b954d16dd49c25c25011a8ca1f4b2e7e", "collapsed": true, "_cell_guid": "bcc6e155-79fa-478f-b252-70a3fb3a5210"}, "cell_type": "code"}, {"source": ["## Generating the word cloud from Authors Dictionary!\n", "1)EAP"], "metadata": {"_uuid": "ae2aa8b6f2cf2e27cad811cae6b96d89ccb55141", "_cell_guid": "0d59115b-be81-4d23-ba3f-aff30412e49d"}, "cell_type": "markdown"}, {"outputs": [], "source": ["df_eap.text\n", "dic= (' '.join(df_eap['text']))\n", "\n", "wordcloud = WordCloud(width = 1000, height = 500,stopwords=STOPWORDS).generate(dic)\n", "\n", "plt.figure(figsize=(15,5));\n", "plt.imshow(wordcloud);\n", "plt.axis('off');\n", "plt.title('Word Cloud for EAP');"], "execution_count": null, "metadata": {"_uuid": "2b68c5c1bc683690600ce4fdbb869b719dbedd41", "_cell_guid": "a4680cff-9b49-4dc9-95f9-974e37a583e9"}, "cell_type": "code"}, {"source": ["*2)HPL*"], "metadata": {"_uuid": "63aafed071a3b4744b817f3fbf57ecc3140f674a", "_cell_guid": "56635a01-e45c-42db-9f9f-d8c6f1aad38e"}, "cell_type": "markdown"}, {"outputs": [], "source": ["dic= (' '.join(df_hpl['text']))\n", "\n", "wordcloud = WordCloud(width = 1000, height = 500,stopwords=STOPWORDS).generate(dic)\n", "\n", "plt.figure(figsize=(15,5));\n", "plt.imshow(wordcloud);\n", "plt.axis('off');\n", "plt.title('Word Cloud for HPL');"], "execution_count": null, "metadata": {"_uuid": "e8fa851039f623d278d3346caf66c0ab4379927b", "_cell_guid": "5fd32776-57d6-4ae1-a60b-277e187cef5a"}, "cell_type": "code"}, {"source": ["*MWS*"], "metadata": {"_uuid": "61ec4eae8e513ed5e6ba90ce5db859dabf73f0fb", "_cell_guid": "3dd0d753-1629-49ad-8197-3826a5c1ed46"}, "cell_type": "markdown"}, {"outputs": [], "source": ["\n", "dic= (' '.join(df_mws['text']))\n", "\n", "wordcloud = WordCloud(width = 1000, height = 500,stopwords=STOPWORDS).generate(dic)\n", "\n", "plt.figure(figsize=(15,5));\n", "plt.imshow(wordcloud);\n", "plt.axis('off');\n", "plt.title('Word Cloud for MWS');"], "execution_count": null, "metadata": {"_uuid": "40bde56ec80892d9c1837d9c5657036b3786c759", "_cell_guid": "1bfab6e2-4d18-4c34-a598-a5646c71a1ce"}, "cell_type": "code"}, {"source": ["### Its Enough EDA author wise, Now lets jump to train DataSet and get sum features."], "metadata": {"_uuid": "9918b8c3ca9241f8cf810d4593622f79eeee654a", "_cell_guid": "e6f5af80-59c7-431e-b7ce-48e7f41f160e"}, "cell_type": "markdown"}, {"source": ["**1)Lenght of Words**"], "metadata": {"_uuid": "847ea8d3e701aaf3a3b87bec83ffbd7db0bcd97a", "_cell_guid": "3b2bb7b2-dfa1-419d-b4d2-794f7ef79b10"}, "cell_type": "markdown"}, {"outputs": [], "source": ["train[\"length\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n", "test[\"length\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n", "print(train['length'].head())"], "execution_count": null, "metadata": {"_uuid": "3a5afe4652e242ee7722f9afe96ffeb36581c02c", "_cell_guid": "8aff4d4a-58de-4898-9f80-5709faef31ad"}, "cell_type": "code"}, {"source": ["**2)Removing the Stopwords**"], "metadata": {"_uuid": "50e314a322b64431674614bb5c101f89ce16b510", "_cell_guid": "3fd662a4-7c25-49f8-bde4-418a1c29af5c"}, "cell_type": "markdown"}, {"outputs": [], "source": ["train[\"stp_len\"] = train[\"text\"].apply(lambda x: len([i for i in str(x).lower().split() if i in stopwords.words(\"english\")]))\n", "test[\"stp_len\"] = test[\"text\"].apply(lambda x: len([i for i in str(x).lower().split() if i in stopwords.words(\"english\")]))\n", "print(train['stp_len'].head())"], "execution_count": null, "metadata": {"_uuid": "ecab45bd7f76d4337e532e267eeb020c0f71d33c", "_cell_guid": "42353aae-0cff-49ab-8c0d-6f4ba04d20fc"}, "cell_type": "code"}, {"source": ["## Lets see mean length of each writter."], "metadata": {"_uuid": "836f4871305ff0cd766325bf3e036d92f3512212", "_cell_guid": "6195bddc-a780-43b1-8e78-521aed2e4840"}, "cell_type": "markdown"}, {"outputs": [], "source": ["print(train.groupby(by=['author'])['length'].mean())\n", "train.groupby(by=['author'])['length'].mean().plot(kind='bar');"], "execution_count": null, "metadata": {"_uuid": "d0b36b272c1daf562e2625c887c08e4811d3d45c", "_cell_guid": "5def86fe-df2b-4816-bb77-058f3bedef47"}, "cell_type": "code"}, {"source": ["## Lets see which user has widest dictionary!"], "metadata": {"_uuid": "5b7cb54cfa67b58d59ec2a48b72047c56951af32", "_cell_guid": "bfabff61-a331-42d8-8b9b-7c93ffa0c27a"}, "cell_type": "markdown"}, {"outputs": [], "source": ["print(train.groupby(by=['author'])['stp_len'].mean())\n", "train.groupby(by=['author'])['stp_len'].mean().plot(kind='bar');"], "execution_count": null, "metadata": {"_uuid": "4705fca7ea3942807a4d9679d44319bcf21e4c4b", "_cell_guid": "710de250-7bd8-4ad6-ae43-935fe3c07ffd"}, "cell_type": "code"}, {"source": ["## In our analysis the punctuation marks can play a very role"], "metadata": {"_uuid": "53e791fc4920c222653d855e56b215b9ac7ede23", "_cell_guid": "2696cde2-c31d-460f-ad5d-f9be802ea734"}, "cell_type": "markdown"}, {"outputs": [], "source": ["import string\n", "train[\"punct\"] =train['text'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]) )\n", "test[\"punct\"] =test['text'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]) )"], "execution_count": null, "metadata": {"_uuid": "56448b07c9948e315de82210e82f363359faabd5", "collapsed": true, "_cell_guid": "fcdb12b9-8e08-4cf8-b82a-33e9a08d2ba1"}, "cell_type": "code"}, {"outputs": [], "source": ["import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "print(train.groupby(by=['author'])['punct'].mean())\n", "train.groupby(by=['author'])['punct'].mean().plot(kind='bar');"], "execution_count": null, "metadata": {"_uuid": "0df6fa18b3be021100d1a82590e91064a214e090", "_cell_guid": "ad7b1e1d-466a-4001-b991-a7769b09bc20"}, "cell_type": "code"}, {"source": ["# Predictions on test Data-"], "metadata": {"_uuid": "68fd45712516ef05ac865cef1d97c6be258b7f8d", "_cell_guid": "1d912ed3-e679-4cb7-a1e2-bb2bf1b56c55"}, "cell_type": "markdown"}, {"source": ["### CountVectorizer\n", "<p>We call <strong>vectorization</strong> the general process of turning a collection\n", "of text documents into numerical feature vectors. This specific strategy\n", "(tokenization, counting and normalization) is called the <strong>Bag of Words</strong>\n", "or \u201cBag of n-grams\u201d representation. Documents are described by word\n", "occurrences while completely ignoring the relative position information\n", "of the words in the document.</p>"], "metadata": {"_uuid": "8841c3ab69d4e348fb78509343974ca81a61852a", "_cell_guid": "5eff38fb-7470-4c9b-af6d-103734c6f5f2"}, "cell_type": "markdown"}, {"outputs": [], "source": ["vect = CountVectorizer(ngram_range=(1,2),min_df=5).fit(train['text'])\n", "train_vectorized = vect.transform(train['text'])\n", "len(vect.get_feature_names())"], "execution_count": null, "metadata": {"_uuid": "8d0bcb7b6add777b800326e1a9cccb9acff88e97", "_cell_guid": "53cb658e-b014-49a7-a31f-16dd972a3abd"}, "cell_type": "code"}, {"source": ["### This technique called Stacking will help us to combine our extracted features with X_test"], "metadata": {"_uuid": "1ae96ebb695bba877a815548ccee49f584c18744", "_cell_guid": "124622ab-6836-4407-b257-3b9f43f35c18"}, "cell_type": "markdown"}, {"outputs": [], "source": ["k=['length', 'stp_len', 'punct']\n", "arr = np.array(train[k])\n", "import scipy\n", "stack = train[['length', 'stp_len', 'punct']]\n", "train_vectorized = scipy.sparse.hstack([train_vectorized, stack])"], "execution_count": null, "metadata": {"_uuid": "ad3d32a1bda33411dcb5d2b477201716bb2fb048", "collapsed": true, "_cell_guid": "d2717a59-b00d-4390-a6af-80e5b7764a0a"}, "cell_type": "code"}, {"outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "model = LogisticRegression(C=.03,n_jobs=-1)\n", "model.fit(train_vectorized, train['author'])"], "execution_count": null, "metadata": {"_uuid": "aadbf0ef29258d760cddf28e794792b5134d2741", "_cell_guid": "b5481aa0-408b-43a7-a6b8-7af790640f86"}, "cell_type": "code"}, {"source": ["### This technique called Stacking will help us to combine our extracted features with X_test"], "metadata": {"_uuid": "18454c93e93554bcfbe70f31507be1d387ba5ede", "_cell_guid": "ae324a29-703c-4135-8ea6-0f86bfba1ba2"}, "cell_type": "markdown"}, {"outputs": [], "source": ["k=np.array(test[['length', 'stp_len', 'punct']])\n", "X_test = vect.transform(test['text'] )\n", "X_test = scipy.sparse.hstack([X_test, k])"], "execution_count": null, "metadata": {"_uuid": "9ecb7f8ebfa3beadcd158395f44871311d6668f6", "collapsed": true, "_cell_guid": "daca33c7-9f0a-443f-8d51-7e9ef8c2e483"}, "cell_type": "code"}, {"source": ["## Making Predictions on test set"], "metadata": {"_uuid": "3c6aa7a72aa2d539661db6647152ecfe7fbb28eb", "_cell_guid": "8cb4e604-3b33-4fd7-8a74-8c24c1a1814b"}, "cell_type": "markdown"}, {"outputs": [], "source": ["df = pd.DataFrame(model.predict_proba(X_test),index=test['id'],columns=['EAP','HPL','MWS'])\n", "df.head()"], "execution_count": null, "metadata": {"_uuid": "e2aa4b7aeea7a1e904983eed4099a70e68f68971", "_cell_guid": "7880d56c-08b7-46d5-a2dc-3501a8fb06ac"}, "cell_type": "code"}, {"source": ["1. ## Saving as Submission file LB ~0.6"], "metadata": {"_uuid": "0f1b2f9f06fd2c46e2f95031edb3b9a5a18e74a6", "_cell_guid": "4a55b115-f522-4b47-8508-1412148ab860"}, "cell_type": "markdown"}, {"outputs": [], "source": ["df.to_csv('Submission1.csv')"], "execution_count": null, "metadata": {"_uuid": "52923a5c32b98b164b034eb0c99e0a2ee78186c3", "collapsed": true, "_cell_guid": "d72d3236-ee15-4df3-918e-fe122f258ad5"}, "cell_type": "code"}, {"source": ["#### This was baseline score.\n", "#### I will keep on updating it."], "metadata": {"_uuid": "de9f076a2f1a5f2c62321681abb1f93ee285b644", "_cell_guid": "a085235d-bf6d-4509-92b2-be2d1b33c9e9"}, "cell_type": "markdown"}, {"outputs": [], "source": [], "execution_count": null, "metadata": {"_uuid": "1c91b96caf7c6e67f1023ea5c11ec7299ea478a9", "collapsed": true, "_cell_guid": "93d1a87d-033e-4b53-8298-1dded49addc6"}, "cell_type": "code"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "version": "3.6.3"}}}