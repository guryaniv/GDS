{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\n#del train,test\n#train=pd.read_csv(\"../input/train.csv\")\n#test=pd.read_csv(\"../input/test.csv\")\n#del train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c94149444a14ba732d0b11868c9b1a85bdcd751"},"cell_type":"code","source":"train=pd.read_csv(\"../input/train.csv\")[:3000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b12545f5445b6d51c9c99b2cbc12477dcf02d522"},"cell_type":"code","source":"train['eeg_fp1'].rolling(256).kurt().plot()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"fa5fd6b481d97841b96ed1070c2da9628f79bb20"},"cell_type":"code","source":"dtypes = {\"crew\": \"int8\",\n          \"experiment\": \"category\",\n          \"time\": \"float32\",\n          \"seat\": \"int8\",\n          \"eeg_fp1\": \"float32\",\n          \"eeg_f7\": \"float32\",\n          \"eeg_f8\": \"float32\",\n          \"eeg_t4\": \"float32\",\n          \"eeg_t6\": \"float32\",\n          \"eeg_t5\": \"float32\",\n          \"eeg_t3\": \"float32\",\n          \"eeg_fp2\": \"float32\",\n          \"eeg_o1\": \"float32\",\n          \"eeg_p3\": \"float32\",\n          \"eeg_pz\": \"float32\",\n          \"eeg_f3\": \"float32\",\n          \"eeg_fz\": \"float32\",\n          \"eeg_f4\": \"float32\",\n          \"eeg_c4\": \"float32\",\n          \"eeg_p4\": \"float32\",\n          \"eeg_poz\": \"float32\",\n          \"eeg_c3\": \"float32\",\n          \"eeg_cz\": \"float32\",\n          \"eeg_o2\": \"float32\",\n          \"ecg\": \"float32\",\n          \"r\": \"float32\",\n          \"gsr\": \"float32\",\n          \"event\": \"category\",\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61a2976fb94dc0a793204179073de0e39e035bf2"},"cell_type":"code","source":"def corranal(train,test):\n    excluded_feats = ['Name', 'RescuerID', 'Description', 'PetID','AdoptionSpeed','label_description'] #['SK_ID_CURR']\n    features = [f_ for f_ in train.columns if f_ not in excluded_feats]\n\n    print('Number of features %d  ' % (len(features)),train.shape,train.AdoptionSpeed.shape) \n    #effect_sizes = cohen_effect_size(Xtrain[:len(ytrain)], ytrain)\n    effect_sizes = cohen_effect_size(train[features],train.AdoptionSpeed.values)\n    effect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(50).index)[::-1].plot.barh(figsize=(6, 10));\n    print('Features with the 30 largest effect sizes')\n    significant_features = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.1]\n    print('Significant features %d: %s' % (len(significant_features), significant_features))\n    return significant_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54c988eb928d86ae27eea66588ff50151d13840c","scrolled":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn import preprocessing\nsvd = TruncatedSVD(n_components=300,n_iter=7, random_state=42)\n\ndef kodeer(dfdir):\n    chunk=5000000\n    colo=256\n    traine=[]\n    for start in range(0,5000000,chunk):\n        mtrain=pd.read_csv(dfdir,dtype=dtypes)[start:start+chunk]\n        leng=int( len(mtrain)/colo)-1\n\n        traint=mtrain[\"eeg_fp1\"][:leng*colo].values.reshape(leng,colo)\n        if len(traint)>0:\n            for ci in  [ \"eeg_f7\", \"eeg_f8\", \"eeg_t4\", \"eeg_t6\", \"eeg_t5\", \"eeg_t3\", \"eeg_fp2\", \"eeg_o1\", \"eeg_p3\", \"eeg_pz\", \"eeg_f3\", \"eeg_fz\", \"eeg_f4\", \"eeg_c4\", \"eeg_p4\", \"eeg_poz\", \"eeg_c3\", \"eeg_cz\", \"eeg_o2\", \"ecg\", \"r\", \"gsr\"]:\n                trainm=mtrain[ci].rolling(256).kurt()\n                trainm=trainm.dropna()[:leng*colo].values.reshape(leng,colo)\n                #print(trainm.shape,traint.shape)\n                traint=np.hstack((traint,trainm))\n            \n\n        traint= traint.reshape(-1,23,256,1)\n\n        \n        if 'event' in mtrain.columns:    \n            le = preprocessing.LabelEncoder()\n            Yle=le.fit_transform(mtrain['event'])\n            Yle=Yle[:leng*colo].reshape(leng,colo).mean(axis=1) \n            Yle=[np.int(x) for x in Yle]        \n        else:\n            Yle='' \n\n        \n    return traint,Yle\n\n#test,_=kodeer(\"../input/test.csv\")\ntrain,Yle=kodeer(\"../input/train.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"354b86e84e272246d0bd32191740ce373ce38a5f"},"cell_type":"code","source":"import keras\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\n\n\nmodel = Sequential()\n# The kernel filter matrix is applied on the whole image\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (23,256,1)))\n\n# The CNN can isolate features that are useful everywhere from these transformed\n# images(feature maps)\n\n# The Second important layer in CNN is the pooling(MaxPool2D) layer.\n# This layer simply acts as downsampling filter. \nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2))) # we have to choose the area size-\n# -pooled each time(pool_size) more the pooling dimension is high, more the\n# downsampling is important\n\n# Dropout is a regularization technique for neural network model.\n# A simple way to prevent neural network from overfitting\nmodel.add(Dropout(0.25))\n\n# Combining convolutional and pooling layers, CNN are able to combine local\n# features and learn more global features of the image\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\n# 'relu' is the rectifier(activation function = max(0,x)) is used to add \n# non-linearity to the network\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n# The Flatten layer is used to convert the final feature maps into \n# a one single 1D Vector\nmodel.add(Flatten())\n\n# Two Fully-connected(Dense =  10, activation = softmax) layers\n# the net outputs distribution of probability of each class\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5, activation = \"softmax\"))\n\n# In summary() we can understand the architechture we build and \n# we can understand about the totol number of parameters \nmodel.summary()\n\n# total_params = (filter_height * filter_width * input_image_channels + 1) * number_of_filters\n# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n\n# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd650b7b49ded947e7509d3f7b5fe576516b7ba9"},"cell_type":"code","source":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)\nepochs = 2\nbatch_size = 86","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d4f7bc73edb6a79bd44a70cfedbc9b3211f2e3f"},"cell_type":"code","source":"# With data augmentation to prevent overfitting\n\ndatagen = ImageDataGenerator(\n        featurewise_center=True,  # set input mean to 0 over the dataset\n        samplewise_center=True,  # set each sample mean to 0\n        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n        samplewise_std_normalization=True,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        #rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        #height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b571f72b845c760caface3c2045b253c2902eb05"},"cell_type":"code","source":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(Yle, num_classes = 5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c353fc22a837f14eae38d8c61fa146f22c3fab0"},"cell_type":"code","source":"# Fit the model\nhistory = model.fit_generator(datagen.flow(train[:14000],Y_train[:14000], batch_size=batch_size),\n                              epochs = epochs, validation_data = (train[14000:],Y_train[14000:]),\n                              verbose = 2, steps_per_epoch=train.shape[0] // batch_size\n                              , callbacks=[learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"603ca041290389a36ad55bea925ad221eda43cdc"},"cell_type":"code","source":"test,_=kodeer(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"855ea549806c418c95b4a12d625a7e8bb18c6339"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdfb07509155582b83a398db0e03d752b9136fb1"},"cell_type":"code","source":"# Predict the values from the validation dataset\nY_pred = model.predict(test)\nY_pred","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}