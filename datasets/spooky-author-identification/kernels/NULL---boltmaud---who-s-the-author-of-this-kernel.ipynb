{"nbformat": 4, "metadata": {"language_info": {"mimetype": "text/x-python", "name": "python", "version": "3.6.3", "file_extension": ".py", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "28445d0f-ccac-4a52-96de-833927f95700", "_uuid": "dd46d2f885afef3adeb862468e3bc7834852c11a"}, "source": ["*Hello everyone, this is my first Kernel and I'm very motivated to work on this project! ***\n", "\n", "# **Introduction** : sngrams\n", "\n", "In this tutorial, we will try to use the**  [#SNGRAMS ](https://pdfs.semanticscholar.org/4e5a/778e0e45a4bddb81916a33d8e3b380fbb836.pdf) **to find which author wrote which sentences ! Grigori Sidorov, Francisco Velasquez, Efstathios Stamatatos, Alexander Gelbukh and Liliana Chanona did the same research with the same number of authors : THREE ! Strange, isn't it ? \n", "\n", "# ** 1. What are sngrams ? **\n", "##  1.1. Stanford Parser\n", "\n", "Syntactic N-grams are using the output of the ** [ #Stanford Parser](http://nlp.stanford.edu:8080/parser/) ** which is a probabilistic parser that use knowledge of language. The parser exists in English and extrats groups of words that have a grammatical relation. The model is based on the structure of the sentences which depends of the language. \n", "\n", "The output of this parser for a sentence like ' It never once occurred to me that the fumbling might be a mere mistake.' (which is the second sentence of our train set) would look like : \n", "\n", "nsubj(occurred-4, It-1)<br>\n", "neg(occurred-4, never-2)<br>\n", "advmod(occurred-4, once-3)<br>\n", "root(ROOT-0, occurred-4)<br>\n", "case(me-6, to-5)<br>\n", "nmod(occurred-4, me-6)<br>\n", "mark(mistake-14, that-7)<br>\n", "det(fumbling-9, the-8)<br>\n", "nsubj(mistake-14, fumbling-9)<br>\n", "aux(mistake-14, might-10)<br>\n", "cop(mistake-14, be-11)<br>\n", "det(mistake-14, a-12)<br>\n", "amod(mistake-14, mere-13)<br>\n", "ccomp(occurred-4, mistake-14)<br>\n", "\n", "The following tree is also defined :\n", "![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/graphstanford.bmp)\n", "\n", "*This graphe was designed by the librairy : nltk.draw.tree. * \n", "*The name 'nsubj', 'neg' .. is the relation between the words in the parenthesis. The number indicates the position of the words in the sentence.   \n", "\n", "## 1.2. n-grams using the stanford parser\n", "\n", "### 1.2.1 Normal n-grams\n", "Using the sentence 'It never once occurred to me that the fumbling might be a mere mistake' the normal 2-grams is : \n", "\n", "It never ;  never once ;  once occurred ; occured to ;  to me ; me that ;  that the ;  the fumbling ; fumbling might ; might be ; be a ; a mere ;  mere mistake\n", "\n", "For a 3-grams : \n", "\n", "It never once ;never once occurred ; once occurred to ; occurred to me ;to me that ; me that the ;that the fumbling ;the fumbling  might ; fumbling  might be ; might be a ; be a mere ; a mere mistake\n", " \n", " ### 1.2.2 Sn-grams \n", " The sn-grams use the result of the stanford parser and gives :\n", " \n", " For a s2grams, the couples are all the couple from the roots :\n", " occured once, occured never, occured it , occured to, occured mistake, mistake mere,mistake a, mistake be, mistake might, mistake fumbling, mistake that, fumbing the, me to \n", " \n", " For a s3grams the (occured to) and (to me) become (occured to me) and (mistake fumbling) and (fumbling the) become (mistake fumbling the) \n", " \n", " # ** 2. How to use sngrams **\n", " \n", " I tried this solution on my computer but to generate all the features, the programme needed 5 hours. I decided to try without the syntaxic ngrams.\n", " \n", " # ** 3. Programme with ngrams ** \n", " \n", " "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "261e93f5-1dda-4811-8e8b-95ef455ea184", "_uuid": "889c9f81b2c443189cf620602ce412bf2b636c5e"}, "source": ["First, we import the dataset and the libs : "]}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "7195c9b3-7782-4fe9-81f7-c382a75ad7f9", "collapsed": true, "_uuid": "64e544ad7f624a97c71f2292d4047f3a77e28b53"}, "outputs": [], "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.naive_bayes import MultinomialNB\n", "import pandas as pd\n", "\n", "dfTrain = pd.read_csv(\"../input/train.csv\") # importing train dataset\n", "dfTest = pd.read_csv(\"../input/test.csv\") # importing test dataset"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "84de8e9c-82b7-498a-b6a8-b1e43ed3b224", "_uuid": "e08ac651131e14a854fe74aa5a25bba2fb290d08"}, "source": ["Then we create a matrix with the ngrams using TiDf. We delete the stop-word and accepte ngrams from 1 word to 3. \n", "The function fit_tranform will tranform the data into a matrix and fit the model. "]}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "a83ba9cd-9494-4138-8835-146159468137", "collapsed": true, "_uuid": "c92344023b091ee0491b31716b3987b6c3c37a92"}, "outputs": [], "source": ["vectorizer = CountVectorizer(stop_words=\"english\",analyzer='word', ngram_range=(1,3))\n", "train_counts = vectorizer.fit_transform(dfTrain.text)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "237b16d3-7a13-4244-b8e8-de91fe2c88e1", "_uuid": "7717f4a39bc5b82ff9668ff62df17ecb465f00e8"}, "source": ["We fit the model with the Multinomiale method because it's the best one for the problems using TiDf and ngrams."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "eb638e63-872d-415a-9208-257f052e383e", "collapsed": true, "_uuid": "b6b673f252d80c78efd61c310db9852cbbe4b697"}, "outputs": [], "source": ["classifier = MultinomialNB()\n", "classifier.fit(train_counts, dfTrain.author)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "fdf46071-772a-4db4-add6-0d671f523096", "_uuid": "78fcda81b5061764ad9dcbcab843710059d6021b"}, "source": ["The test set need to be transform too and the model is ready to predict : "]}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "90d1c01b-bb55-4633-851b-f325a64a6e49", "collapsed": true, "_uuid": "20cff2c59b1cfa37c422bf1a76746eb189c59761"}, "outputs": [], "source": ["tests_counts = vectorizer.transform(dfTest.text)\n", "predicted = pd.DataFrame(classifier.predict_proba(tests_counts) )"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "3e9c82db-3b44-4d2f-a278-5c7c3abc52e0", "_uuid": "93dd7bc285a9678c555bd3f08d5874af9ad82d1a"}, "source": ["Finally, we prepare the submit file :"]}, {"execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "_cell_guid": "8df0b583-8633-4513-8576-250632f30cf7", "_kg_hide-output": false, "collapsed": true, "_uuid": "1990aefc7571f7d7044c2ab90293d04c27eb8ae6"}, "outputs": [], "source": ["submit=pd.DataFrame({})\n", "submit[\"id\"]=dfTest.id\n", "submit[\"EAP\"]=predicted[0]\n", "submit[\"HPL\"]=predicted[1]\n", "submit[\"MWS\"]=predicted[2]\n", "\n", "print(submitfinal)\n", "submit.to_csv(\"submit.csv\", sep=',',index=False)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e4787ed4-68c1-41e2-9ab7-8478c4613d05", "_uuid": "53645af1ac21c88e31a9aeaafc7162e1bd50eca0"}, "source": ["# ** 4. Vizualisation ** \n", "\n", "In this part, I'm not sure to keep the rules but I didn't see anything that forbidd to use everything we know. \n", "\n", "## 4.1 LDA Vizualisation \n", "\n", "Firstly, I used knime to extract the words topics for each authors. I created a vizualisation with the colors of Halloween. The size of a word depend of its weight at the output of LDA algorithm. \n", "\n", "![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/viz1.png)\n", "\n", "Then I used [#Tropes](http://www.tropes.fr/) to get some interesting information about the grammar.\n", "\n", "** THE PRONOUNS **\n", "![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/pronouns.png)\n", "\n", "** THE CONNECTORS **\n", "![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/connectors_.png)\n", "\n", "** THE MODALIZATION ** \n", "![](https://raw.githubusercontent.com/BoltMaud/Kaggle_images/master/modalities_.png)\n", "\n", "\n", "\n"]}]}