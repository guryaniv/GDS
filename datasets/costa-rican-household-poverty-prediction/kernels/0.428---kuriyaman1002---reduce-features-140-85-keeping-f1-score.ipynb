{"cells":[{"metadata":{"_uuid":"a25dad79d0e0ac40da4ab1ffa5bf3e24008ca367"},"cell_type":"markdown","source":"# Reduce features without degrading F1 score\nI found many needless columns in dataset. I removed them and ran prediction. Finally, the F1 score was almost the same as full features prediction with LightGBM.\n\n**Leaderboard score**\n* 0.428 : reduced(85 features)\n* 0.425 : full(140 features)\n\nThis data processing gives us efficient predictions and saving time in future analyses.\n\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"input_dir = '../input/'\nworking_dir = '../working/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9632d88df34a7b6eed6332ca307520251a697eb0"},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d041cc6d400253f8d3cc705f8d5c4497a6f4d6d","scrolled":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv(os.path.join(input_dir, 'train.csv'))\ntest = pd.read_csv(os.path.join(input_dir, 'test.csv'))\n\n# Set index\ntrain.index = train['Id'].values\ntest.index = test['Id'].values\n\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2e4e7b1c6ce13fc445773495b5bb44ec08f3541"},"cell_type":"markdown","source":"### Clean Data\nI picked the following function from [\"Data cleaning and random forest\n\"](https://www.kaggle.com/katacs/data-cleaning-and-random-forest)\n\n* dependency 'no' -> 0\n* edjefa, edjefe 'no' -> 0,  'yes' -> 1\n* meaneduc NaN -> mean escolari of household\n* v2a1 NaN -> 0\n* v18q1 NaN -> 0\n* rez_esc NaN -> 0"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4a4adc68074bb7af195886dc16cf6dc77a5da612"},"cell_type":"code","source":"# copy from https://www.kaggle.com/katacs/data-cleaning-and-random-forest\ndef data_cleaning(data):\n    data['dependency']=np.sqrt(data['SQBdependency'])\n    data['rez_esc']=data['rez_esc'].fillna(0)\n    data['v18q1']=data['v18q1'].fillna(0)\n    data['v2a1']=data['v2a1'].fillna(0)\n    \n    conditions = [\n    (data['edjefe']=='no') & (data['edjefa']=='no'), #both no\n    (data['edjefe']=='yes') & (data['edjefa']=='no'), # yes and no\n    (data['edjefe']=='no') & (data['edjefa']=='yes'), #no and yes \n    (data['edjefe']!='no') & (data['edjefe']!='yes') & (data['edjefa']=='no'), # number and no\n    (data['edjefe']=='no') & (data['edjefa']!='no') # no and number\n    ]\n    choices = [0, 1, 1, data['edjefe'], data['edjefa']]\n    data['edjefx']=np.select(conditions, choices)\n    data['edjefx']=data['edjefx'].astype(int)\n    data.drop(['edjefe', 'edjefa'], axis=1, inplace=True)\n    \n    meaneduc_nan=data[data['meaneduc'].isnull()][['Id','idhogar','escolari']]\n    me=meaneduc_nan.groupby('idhogar')['escolari'].mean().reset_index()\n    for row in meaneduc_nan.iterrows():\n        idx=row[0]\n        idhogar=row[1]['idhogar']\n        m=me[me['idhogar']==idhogar]['escolari'].tolist()[0]\n        data.at[idx, 'meaneduc']=m\n        data.at[idx, 'SQBmeaned']=m*m\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65d0f54bfb04f6474ad8c8826a581660f8493f4c","collapsed":true},"cell_type":"code","source":"train = data_cleaning(train)\ntest = data_cleaning(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55b0e86aa4c10b87aea29e461aac91e5dafe8486"},"cell_type":"markdown","source":"### Extract heads of household"},{"metadata":{"trusted":true,"_uuid":"d7a7790f7240d3ef1800f9fd1575c5bf300e3284","collapsed":true},"cell_type":"code","source":"train = train.query('parentesco1==1')\ntrain = train.drop('parentesco1', axis=1)\ntest = test.drop('parentesco1', axis=1)\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f0cc575d0b191738049be0043ac8828b2ad35a6"},"cell_type":"markdown","source":"## Convert one-hot variables into numeric\n* 'epared', 'etecho', 'eviv' and 'instlevel' can be converted into numeric\n*  like (bad, regular, good) -> (0 ,1, 2)"},{"metadata":{"trusted":true,"_uuid":"88699adf384a5c2577796e8039e24c609843bdc8","collapsed":true},"cell_type":"code","source":"def get_numeric(data, status_name):\n    # make a list of column names containing 'sataus_name'\n    status_cols = [s for s in data.columns.tolist() if status_name in s]\n    print('status column names')\n    print(status_cols)\n    # make a DataFrame with only status_cols\n    status_df = data[status_cols]\n    # change its column name like ['epared1', 'epared2', 'epared3'] -> [0, 1, 2]\n    status_df.columns = list(range(status_df.shape[1]))\n    # get the column name which has the biggest value in every row\n    # see https://stackoverflow.com/questions/26762100/reconstruct-a-categorical-variable-from-dummies-in-pandas\n    # this is pandas.Series\n    status_numeric = status_df.idxmax(1)\n    # set Series name\n    status_numeric.name = status_name\n    # add status_numeric as a new column\n    data = pd.concat([data, status_numeric], axis=1)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"326b6387157241de563ef523bb5fc21f65fd6751","collapsed":true},"cell_type":"code","source":"status_name_list = ['epared', 'etecho', 'eviv', 'instlevel']\nfor status_name in status_name_list:\n    train = get_numeric(train, status_name)\n    test = get_numeric(test, status_name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0212b17aa4d3b0d3c05b8da8a1f94ba42a9b0690"},"cell_type":"markdown","source":"## Delete needless columns\n### redundant columns\n* r4t3, tamviv, tamhog, hhsize ... almost the same as hogar_total\n* v14a ... almost the same as saniatrio1\n* v18q, mobilephone ... can be generated by v18q1, qmobilephone\n* SQBxxx, agesq ... squared values\n* parentescoxxx ... only heads of household are in dataset now\n\n### extra columns\n(One-hot variables should be linearly independent. For example, female (or male) column is needless, because whether the sample is female or not can be explained only with male (or female) column.)\n* paredother, pisoother, abastaguano, energcocinar1, techootro, sanitario6, elimbasu6, estadocivil7, parentesco12, tipovivi5, lugar1, area1, female\n\n### obsolete columns\n* epared1~3, etecho1~3, eviv1~3, instlevel1~9 ... we don't use these columns anymore.\n"},{"metadata":{"trusted":true,"_uuid":"3126c495f7a132be4202caaa7203f0336cc0252c","collapsed":true},"cell_type":"code","source":"needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n                 'mobilephone', 'paredother', 'pisoother', 'abastaguano',\n                 'energcocinar1', 'techootro', 'sanitario6', 'elimbasu6',\n                 'estadocivil7', 'parentesco12', 'tipovivi5',\n                 'lugar1', 'area1', 'female', 'epared1', 'epared2',\n                 'epared3', 'etecho1', 'etecho2', 'etecho3',\n                 'eviv1', 'eviv2', 'eviv3', 'instlevel1', 'instlevel2',\n                 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6',\n                 'instlevel7', 'instlevel8', 'instlevel9']\nSQB_cols = [s for s in train.columns.tolist() if 'SQB' in s]\nparentesco_cols = [s for s in train.columns.tolist() if 'parentesco' in s]\n\nneedless_cols.extend(SQB_cols)\nneedless_cols.extend(parentesco_cols)\n\ntrain = train.drop(needless_cols, axis=1)\ntest = test.drop(needless_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e7c8a7762826c98455074ec0998ec8728cac65f","collapsed":true},"cell_type":"code","source":"ori_train = pd.read_csv(os.path.join(input_dir, 'train.csv'))\nori_train_X = ori_train.drop(['Id', 'Target', 'idhogar'], axis=1)\n\ntrain_X = train.drop(['Id', 'Target', 'idhogar'], axis=1)\n\nprint('feature columns \\n {} -> {}'.format(ori_train_X.shape[1], train_X.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf976c03d25d41b8ea09160786fd4db5048743e2"},"cell_type":"markdown","source":"## Simple LightGBM"},{"metadata":{"trusted":true,"_uuid":"0d60bc2fc0b86b1c935865dea92c3a5f874f1d64","scrolled":false,"collapsed":true},"cell_type":"code","source":"# Split data\ntrain_Id = train['Id'] # individual ID\ntrain_idhogar = train['idhogar'] # household ID\ntrain_y = train['Target'] # Target value\ntrain_X = train.drop(['Id', 'Target', 'idhogar'], axis=1) # features\n\ntest_Id = test['Id'] # individual ID\ntest_idhogar = test['idhogar'] # household ID\ntest_X = test.drop(['Id', 'idhogar'], axis=1) # features\n\n# Union train and test\nall_Id = pd.concat([train_Id, test_Id], axis=0, sort=False)\nall_idhogar = pd.concat([train_idhogar, test_idhogar], axis=0, sort=False)\nall_X = pd.concat([train_X, test_X], axis=0, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddc14d963f2d16e376f15d50557b848d0f9fd2ec","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, f1_score, make_scorer\nimport lightgbm as lgb\n\nX_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.1, random_state=0)\n\nF1_scorer = make_scorer(f1_score, greater_is_better=True, average='macro')\n\n# gbm_param = {\n#     'num_leaves':[210]\n#     ,'min_data_in_leaf':[9]\n#     ,'max_depth':[14]\n# }\n# gbm = GridSearchCV(\n#     lgb.LGBMClassifier(objective='multiclassova', class_weight='balanced', seed=0)\n#     , gbm_param\n#     , scoring=F1_scorer\n# )\n\n\n# params = {'num_leaves': 13, 'min_data_in_leaf': 23, 'max_depth': 11, 'learning_rate': 0.09, 'feature_fraction': 0.74}\ngbm = lgb.LGBMClassifier(boosting_type='dart', objective='multiclassova', class_weight='balanced', random_state=0)\n# gbm.set_params(**params)\n\ngbm.fit(X_train, y_train)\n# gbm.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"458423a2b5c5151f1fb397c98cf3f2cc6a6d251a"},"cell_type":"code","source":"import pickle\nwith open(os.path.join(working_dir, '20180801_lgbm.pickle'), mode='wb') as f:\n    pickle.dump(gbm, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e133dcb8f8c1eadf0503e3c5338c09fe89ce449","collapsed":true},"cell_type":"code","source":"y_test_pred = gbm.predict(X_test)\ncm = confusion_matrix(y_test, y_test_pred)\nf1 = f1_score(y_test, y_test_pred, average='macro')\nprint(\"confusion matrix: \\n\", cm)\nprint(\"macro F1 score: \\n\", f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2e25bb77387bb8c9773930f3f06871bdd70413d0"},"cell_type":"code","source":"pred = gbm.predict(test_X)\npred = pd.Series(data=pred, index=test_Id.values, name='Target')\npred = pd.concat([test_Id, pred], axis=1, join_axes=[test_Id.index])\npred.to_csv('20180801_lgbm.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}