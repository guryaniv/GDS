{"cells":[{"metadata":{"_uuid":"442fe863b3b6efa19790c552c13ef14eb42d3f50"},"cell_type":"markdown","source":"I try to do classification with neural networks, but the lightGBM result is always better than NN result. \nIn the following example, for simplicity, I use only one feature and as you can see I can't reach lightGBM level of ROC. I have tried to use RNN cells, LSTM... batch normalization.. different data transformation - result is always the same, lightGBM result much more better.\n\nMaybe someone uses neural networks and can share an information how we can improve the result. Should I do some specific transformation of the data, or maybe should I use some special network architecture.. or something else.. \nany advice will be helpful. \nThank you."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print('Importing data...')\ndata = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')\n\nprint('preparation..')\ny = data['TARGET']\ndel data['TARGET']\ncl = [\n# 'AMT_INCOME_TOTAL',\n 'AMT_CREDIT',]\n\ndata = data[cl]\ntest = test[cl]\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\n#Impute missing\nprint('Imputing data...')\ndata = data.fillna(value = -1)\ntest = test.fillna(value = -1)\n\n#Create new, balanced train set\ndata['y'] = y\ndata_all_ones = data[data.y==1]\ndata_all_zeros = data[data.y==0]\ndata_all_zeros2 = data_all_zeros.iloc[0:data_all_ones.shape[0],:]\ndata = pd.concat([data_all_ones,data_all_zeros2], axis = 0)\ny= data.y\ndel data['y']\n\n\n#Scale data to feed Neural Net\nprint('scaling...')\nscaler = MinMaxScaler().fit(data)\ndata = scaler.transform(data)\ntest= scaler.transform(test)\n\nprint('finished')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c7457f96f0c6f1e35f32e735735fed57769c899"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(data, y, test_size=0.15, shuffle=True, random_state=42)\n\nX_train_a = X_train\ny_train_a = y_train.values.reshape(-1,1)\n\nX_valid_a = X_valid\ny_valid_a = y_valid.values.reshape(-1,1)\n\nX_test_a = test","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"4326894a263d53fbe3198e6b26280363501c7b5d"},"cell_type":"markdown","source":"**Simple DNN with 2 hidden layers**"},{"metadata":{"trusted":true,"_uuid":"b05cb4d3f4669f515c2f11176908f5ba4a348d09"},"cell_type":"code","source":"#tensorflow model\nimport tensorflow as tf\n# to make this notebook's output stable across runs\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n    \nreset_graph()\n\nn_inputs = X_train.shape[1]\nn_neurons = 10\nn_outputs = 1\n\nlearning_rate = 0.001\n\nX = tf.placeholder(tf.float32, [None, n_inputs])\ny = tf.placeholder(tf.int32, shape=[None, 1])\ntf_is_training = tf.placeholder(tf.bool, None)  # to control dropout when training and testing\n\n\n\n#he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\")\nhe_init = tf.contrib.layers.xavier_initializer()\n\no_input = tf.layers.dense(X, n_inputs, activation=tf.nn.elu)\no_input = tf.layers.dropout(o_input, rate=0.5, training=tf_is_training)   # drop out 50% of inputs\n\no_hidden_1 = tf.layers.dense(o_input, n_neurons, activation=tf.nn.elu, kernel_initializer=he_init)\no_hidden_1 = tf.layers.dropout(o_hidden_1, rate=0.5, training=tf_is_training)   # drop out 50% of inputs\n\no_hidden_2 = tf.layers.dense(o_hidden_1, n_neurons, activation=tf.nn.elu, kernel_initializer=he_init)\no_hidden_2 = tf.layers.dropout(o_hidden_2, rate=0.5, training=tf_is_training)   # drop out 50% of input\n\nlogits = tf.layers.dense(o_hidden_2, n_outputs)\ny_proba = tf.nn.sigmoid(logits)\n\ny_as_float = tf.cast(y, tf.float32)\nxentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\nloss = tf.reduce_mean(xentropy)\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntraining_op = optimizer.minimize(loss)\n\ny_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)\n\ny_pred_correct = tf.equal(y_pred, y)\naccuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))\n\ninit = tf.global_variables_initializer()\n\nn_ecpochs = 100\nbatch_size = 1000\n\ndef random_batch(X_train, y_train, batch_size):\n    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n    X_batch = X_train[rnd_indices]\n    y_batch = y_train[rnd_indices]\n    return X_batch, y_batch\n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_ecpochs):\n        for iteration in range(len(X_train_a) // batch_size):\n            X_batch, y_batch = random_batch(X_train_a, y_train_a, batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, tf_is_training: True})\n            \n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch, tf_is_training: False})\n        \n        acc_val = accuracy.eval(feed_dict={X: X_valid_a, y: y_valid_a, tf_is_training: False})\n        y_proba_val = y_proba.eval(feed_dict={X:X_valid_a, y: y_valid_a, tf_is_training: False})\n        print(epoch, \"Train accuracy:\", acc_train, 'Validation accuracy:',acc_val)\n    \n   # acc_test = accuracy.eval(feed_dict={X: data_test_a})\n    y_proba_test = y_proba.eval(feed_dict={X: X_test_a, tf_is_training: False})\n    print(epoch, \"Test prob:\", y_proba_test)","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"51008267d74884a1fcd46ece3bae1267978bb214"},"cell_type":"markdown","source":"**Simple LightGBM Classifier**"},{"metadata":{"trusted":true,"_uuid":"4d0556aa2266108e8cf429cfc505b8cba1c766f5"},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nclf = LGBMClassifier(\n        learning_rate=0.05,\n    )\n    \nclf.fit(X_train_a, y_train_a.ravel(), \n            eval_set= [(X_train_a, y_train_a.ravel()), (X_valid_a, y_valid_a.ravel())], \n            eval_metric='auc', verbose=250, early_stopping_rounds=250\n           )\n    \ny_pred_p = clf.predict_proba(X_valid_a, num_iteration=clf.best_iteration_)[:, 1]\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d84cc17658becc74c073b8c8aff0684e07291a50"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\n\nimport matplotlib.pyplot as plt\n\n\n# Plot data\ndef generate_results(y_test, y_score):\n    # print(y_score)\n    fpr, tpr, _ = roc_curve(y_test, y_score)\n    roc_auc = auc(fpr, tpr)\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"01f3ab80445216744af118fb647ae9b10e7fc484"},"cell_type":"markdown","source":"**DNN ROC Curve**"},{"metadata":{"trusted":true,"_uuid":"0e88ca4842f7d90e6fdc19280a7753821ad0f8fe"},"cell_type":"code","source":"generate_results(y_valid, y_proba_val)","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"3b74e1fe906bf32f61832fb5ff4ca22a052aa74b"},"cell_type":"markdown","source":"**LightGBM ROC Curve**"},{"metadata":{"trusted":true,"_uuid":"a056c09c573f88f5d5929570117963d377999415"},"cell_type":"code","source":"generate_results(y_valid.ravel(), y_pred_p)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cdb3fbc14b4e851e6aaf72655a309f020d06ebf0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}