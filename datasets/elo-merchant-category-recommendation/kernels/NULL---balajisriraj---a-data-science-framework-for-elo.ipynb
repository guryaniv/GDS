{"cells":[{"metadata":{"_uuid":"a8f9622945156d6337ba73c481da2de7efef7384"},"cell_type":"markdown","source":"# <div style=\"text-align: center\">A Data Science Framework for Elo </div>\n### <div align=\"center\"><b>Quite Practical and Far from any Theoretical Concepts</b></div>\n<div style=\"text-align:center\">last update: <b>11/28/2018</b></div>\n<img src='http://s8.picofile.com/file/8344134250/KOpng.png'>\nYou can Fork and Run this kernel on **Github**:\n> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n"},{"metadata":{"_uuid":"750903cc2679d39058f56df6c6c040be02b748df"},"cell_type":"markdown","source":" <a id=\"1\"></a> <br>\n## 1- Introduction\n**[Elo](https://www.cartaoelo.com.br/)** has defined a competition in **Kaggle**. A realistic and attractive data set for data scientists.\non this notebook, I will provide a **comprehensive** approach to solve Elo Recommendation problem.\n\nI am open to getting your feedback for improving this **kernel**."},{"metadata":{"_uuid":"cda11210a88d6484112cbe2c3624225328326c6a"},"cell_type":"markdown","source":"<a id=\"top\"></a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n1. [Data Science Workflow for Elo](#2)\n1. [Problem Definition](#3)\n    1. [Business View](#4)\n        1. [Real world Application Vs Competitions](#31)\n1. [Problem feature](#7)\n    1. [Aim](#8)\n    1. [Variables](#9)\n    1. [ Inputs & Outputs](#10)\n1. [Select Framework](#11)\n    1. [Import](#12)\n    1. [Version](#13)\n    1. [Setup](#14)\n1. [Exploratory data analysis](#15)\n    1. [Data Collection](#16)\n        1. [Features](#17)\n        1. [Explorer Dataset](#18)\n    1. [Data Cleaning](#19)\n    1. [Data Preprocessing](#20)\n    1. [Data Visualization](#23)\n        1. [countplot](#61)\n        1. [pie plot](#62)\n        1. [Histogram](#63)\n        1. [violin plot](#64)\n        1. [kdeplot](#65)\n1. [Apply Learning](#24)\n1. [Conclusion](#25)\n1. [References](#26)"},{"metadata":{"_uuid":"e9438d850fcacb93c4dc1f7873255803ecbf521c"},"cell_type":"markdown","source":"-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n \n -----------"},{"metadata":{"_uuid":"e11b73b618b0f6e4335520ef80267c6d577d1ba5"},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## 2- A Data Science Workflow for Elo\nOf course, the same solution can not be provided for all problems, so the best way is to create a **general framework** and adapt it to new problem.\n\n**You can see my workflow in the below image** :\n\n <img src=\"http://s8.picofile.com/file/8342707700/workflow2.png\"  />\n\n**You should feel free\tto\tadjust \tthis\tchecklist \tto\tyour needs**\n###### [Go to top](#top)"},{"metadata":{"_uuid":"600be852c0d28e7c0c5ebb718904ab15a536342c"},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n## 3- Problem Definition\nI think one of the important things when you start a new machine learning project is Defining your problem. that means you should understand business problem.( **Problem Formalization**)\n> **we will be predicting whether a question asked on Quora is sincere or not.**\n\n## 3-1 About Elo\n [Elo](https://www.cartaoelo.com.br/) is one of the largest **payment brands** in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But \n1. do these promotions work for either the consumer or the merchant?\n1. Do customers enjoy their experience? \n1. Do merchants see repeat business? \n\n**Personalization is key**.\n<a id=\"4\"></a> <br>\n## 3-2 Business View \nElo has built machine learning models to understand the most important aspects and preferences in their customers’ lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in.\n<a id=\"31\"></a> <br>\n### 3-2-1 Real world Application Vs Competitions\nJust a simple comparison between real-world apps with competitions:\n<img src=\"http://s9.picofile.com/file/8339956300/reallife.png\" height=\"600\" width=\"500\" />\n\n###### [Go to top](#top)"},{"metadata":{"_uuid":"556980c672d2f7b2a4ee943b9d13b88de6e41e04"},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n## 4- Problem Feature\nProblem Definition has three steps that have illustrated in the picture below:\n\n1. Aim\n1. Variable\n1. Inputs & Outputs\n\n\n\n\n\n<a id=\"8\"></a> <br>\n### 4-1 Aim\nDevelop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty.\nWe are predicting a **loyalty score** for each card_id represented in test.csv and sample_submission.csv.\n\n<a id=\"9\"></a> <br>\n### 4-2 Variables\nThe data is formatted as follows:\n\ntrain.csv and test.csv contain card_ids and information about the card itself - the first month the card was active, etc. train.csv also contains the target.\n\nhistorical_transactions.csv and new_merchant_transactions.csv are designed to be joined with train.csv, test.csv, and merchants.csv. They contain information about transactions for each card, as described above.\n\nmerchants can be joined with the transaction sets to provide additional merchant-level information.\n\n\n<a id=\"10\"></a> <br>\n### 4-3 Inputs & Outputs\nwe use train.csv and test.csv as Input and we should upload a  submission.csv as Output\n\n### 4-4 Evaluation\nSubmissions are scored on the root mean squared error. RMSE(Root Mean Squared Error) is defined as:\n<img src='https://www.includehelp.com/ml-ai/Images/rmse-1.jpg'>\nwhere y^ is the predicted loyalty score for each card_id, and y is the actual loyalty score assigned to a card_id.\n\n**<< Note >>**\n> You must answer the following question:\nHow does your company expect to use and benefit from **your model**.\n###### [Go to top](#top)"},{"metadata":{"_uuid":"fbedcae8843986c2139f18dad4b5f313e6535ac5"},"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n## 5- Select Framework\nAfter problem definition and problem feature, we should select our framework to solve the problem.\nWhat we mean by the framework is that  the programming languages you use and by what modules the problem will be solved.\n###### [Go to top](#top)"},{"metadata":{"_uuid":"c90e261f3b150e10aaec1f34ab3be768acf7aa25"},"cell_type":"markdown","source":"<a id=\"12\"></a> <br>\n## 5-2 Import"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport string\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c2beac253f7ddddcc2e1aa26dc850d5b87268f3"},"cell_type":"markdown","source":"<a id=\"13\"></a> <br>\n## 5-3 version"},{"metadata":{"_kg_hide-input":true,"_uuid":"9ffe2f1e5995150c8138f9e98509c7525fb230b4","trusted":true},"cell_type":"code","source":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"431bf889ae401c1089a13835356c13f2b6a06f6c"},"cell_type":"markdown","source":"<a id=\"14\"></a> <br>\n## 5-4 Setup\n\nA few tiny adjustments for better **code readability**"},{"metadata":{"_uuid":"04ff1a533119d589baee777c21194a951168b0c7"},"cell_type":"markdown","source":"<a id=\"15\"></a> <br>\n## 6- EDA\n In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n \n* Which variables suggest interesting relationships?\n* Which observations are unusual?\n* Analysis of the features!\n\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n\n1. Data Collection\n1. Visualization\n1. Data Cleaning\n1. Data Preprocessing\n<img src=\"http://s9.picofile.com/file/8338476134/EDA.png\">\n\n ###### [Go to top](#top)"},{"metadata":{"_uuid":"cedecea930b278f86292367cc28d2996a235a169"},"cell_type":"markdown","source":"<a id=\"16\"></a> <br>\n## 6-1 Data Collection\n**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n\nI start Collection Data by the training and testing datasets into **Pandas DataFrames**.\n###### [Go to top](#top)"},{"metadata":{"_kg_hide-input":true,"_uuid":"9269ae851b744856bce56840637030a16a5877e1","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', parse_dates=[\"first_active_month\"] )\ntest = pd.read_csv('../input/test.csv' ,parse_dates=[\"first_active_month\"] )\nmerchants=pd.read_csv('../input/merchants.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58ed9c838069f54de5cf90b20a774c3e236149b3"},"cell_type":"markdown","source":"**<< Note 1 >>**\n\n* Each **row** is an observation (also known as : sample, example, instance, record).\n* Each **column** is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate).\n###### [Go to top](#top)"},{"metadata":{"_uuid":"1699b28d58f13f12fafa535d0fae345baa044b12"},"cell_type":"markdown","source":"## data_dictionary Analysis"},{"metadata":{"trusted":true,"_uuid":"14df0c955a8127dbd57fe13dda247d6f8f47dda6"},"cell_type":"code","source":"data_dictionary_train=pd.read_excel('../input/Data_Dictionary.xlsx',sheet_name='train')\ndata_dictionary_history=pd.read_excel('../input/Data_Dictionary.xlsx',sheet_name='history')\ndata_dictionary_new_merchant_period=pd.read_excel('../input/Data_Dictionary.xlsx',sheet_name='new_merchant_period')\ndata_dictionary_merchant=pd.read_excel('../input/Data_Dictionary.xlsx',sheet_name='merchant')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa1855fc1c1f2f0d77c7e3f253325bfe90ce4d03"},"cell_type":"markdown","source":"### data_dictionary_train"},{"metadata":{"trusted":true,"_uuid":"582ef3b40f27fa577c545ebace4bba543a385dae"},"cell_type":"code","source":"data_dictionary_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"284845f840636073478a14709346a3b205f7fc32"},"cell_type":"markdown","source":"### data_dictionary_history"},{"metadata":{"trusted":true,"_uuid":"ccde2ad10228c572c981dd4f206573aea1aea2c1"},"cell_type":"code","source":"data_dictionary_history.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c5074662da70213137b1f65531acf59f15399a0"},"cell_type":"markdown","source":"### data_dictionary_new_merchant_period"},{"metadata":{"trusted":true,"_uuid":"c40a573cd5c7f0b8a5e8a8acca76b4427bb2e14b"},"cell_type":"code","source":"data_dictionary_new_merchant_period.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63975f9b0e35051b49888400908b75bbb34786c3"},"cell_type":"markdown","source":"### data_dictionary_merchant"},{"metadata":{"trusted":true,"_uuid":"a3ba3933f4bc24f5089f6b1e639b592a43ca1e0c"},"cell_type":"code","source":"data_dictionary_merchant.head(30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19e963687091990b49c0a40e19941f7b60c3672f"},"cell_type":"markdown","source":"## Train Analysis"},{"metadata":{"_uuid":"4708d70e39d1ae861bbf34411cf03d07f261fceb","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.sample(1) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8e7a84ab982504d7263b1812fa66bba78bddbdc","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test.sample(1) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3483fbc1e932d9f387703a796248963e77cefa1d"},"cell_type":"markdown","source":"Or you can use others command to explorer dataset, such as "},{"metadata":{"_uuid":"08a94b16129d4c231b64d4691374e18aa80f1d80","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.tail(1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"581b90e6a869c3793472c7edd59091d6d6342fb2"},"cell_type":"markdown","source":"<a id=\"17\"></a> <br>\n## 6-1-1 Features\nFeatures can be from following types:\n* numeric\n* categorical\n* ordinal\n* datetime\n* coordinates\n\nFind the type of features in **Qoura dataset**?!\n\nFor getting some information about the dataset you can use **info()** command."},{"metadata":{"_kg_hide-input":true,"_uuid":"ca840f02925751186f87e402fcb5f637ab1ab8a0","trusted":true},"cell_type":"code","source":"print(train.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cbcf76344a6e3c8e841ccf1f43bf00d040a06a1","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(test.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73ab30f86273b590a51fc363d9bf78c2709558fa"},"cell_type":"markdown","source":"<a id=\"18\"></a> <br>\n## 6-1-2 Explorer Dataset\n1- Dimensions of the dataset.\n\n2- Peek at the data itself.\n\n3- Statistical summary of all attributes.\n\n4- Breakdown of the data by the class variable.\n\nDon’t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects.\n###### [Go to top](#top)"},{"metadata":{"_kg_hide-input":true,"_uuid":"4b45251be7be77333051fe738639104ae1005fa5","trusted":true},"cell_type":"code","source":"# shape for train and test\nprint('Shape of train:',train.shape)\nprint('Shape of test:',test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"c64e9d3e0bf394fb833de94a0fc5c34f69fce24c","trusted":true},"cell_type":"code","source":"#columns*rows\ntrain.size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b5fd1034cd591ebd29fba1c77d342ec2b408d13"},"cell_type":"markdown","source":"After loading the data via **pandas**, we should checkout what the content is, description and via the following:"},{"metadata":{"_kg_hide-input":true,"_uuid":"edd043f8feb76cfe51b79785302ca4936ceb7b51","trusted":true},"cell_type":"code","source":"type(train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"edd043f8feb76cfe51b79785302ca4936ceb7b51","trusted":true},"cell_type":"code","source":"type(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b8b6f0c962a59e5258e74ed9e740a4aaf7c8113","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c288c3dc8656a872a8529368812546e434d3a22"},"cell_type":"markdown","source":"To pop up 5 random rows from the data set, we can use **sample(5)**  function and find the type of features."},{"metadata":{"_uuid":"09eb18d1fcf4a2b73ba2f5ddce99dfa521681140","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.sample(5) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8280749a19af32869978c61941d1dea306632d71"},"cell_type":"markdown","source":"<a id=\"19\"></a> <br>\n## 6-2 Data Cleaning\nWhen dealing with real-world data, dirty data is the norm rather than the exception. We continuously need to predict correct values, impute missing ones, and find links between various data artefacts such as schemas and records. We need to stop treating data cleaning as a piecemeal exercise (resolving different types of errors in isolation), and instead leverage all signals and resources (such as constraints, available statistics, and dictionaries) to accurately predict corrective actions.\n\nThe primary goal of data cleaning is to detect and remove errors and **anomalies** to increase the value of data in analytics and decision making. While it has been the focus of many researchers for several years, individual problems have been addressed separately. These include missing value imputation, outliers detection, transformations, integrity constraints violations detection and repair, consistent query answering, deduplication, and many other related problems such as profiling and constraints mining.[4]\n###### [Go to top](#top)"},{"metadata":{"_uuid":"a6315bf510cecb907b2d23aad25faf6ccad32ac4"},"cell_type":"markdown","source":"How many NA elements in every column!!\n\nGood news, it is Zero!\n\nTo check out how many null info are on the dataset, we can use **isnull().sum()**."},{"metadata":{"_kg_hide-input":true,"_uuid":"675f72fb58d83c527f71819e71ed8e17f81126f5","trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5faa6528c6667060c05268757ff46e211b4fea3f"},"cell_type":"markdown","source":"But if we had , we can just use **dropna()**(be careful sometimes you should not do this!)"},{"metadata":{"_kg_hide-input":true,"_uuid":"e8e124ca20643ad307d9bfdc34328d548c6ddcbc","trusted":true},"cell_type":"code","source":"# remove rows that have NA's\nprint('Before Droping',train.shape)\ntrain = train.dropna()\nprint('After Droping',train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"277e1998627d6a3ddeff4e913a6b8c3dc81dec96"},"cell_type":"markdown","source":"\nWe can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property."},{"metadata":{"_uuid":"c2f1eaf0b6dfdc7cc4dace04614e99ed56425d00"},"cell_type":"markdown","source":"To print dataset **columns**, we can use columns atribute."},{"metadata":{"_uuid":"909d61b33ec06249d0842e6115597bbacf21163f","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3458838205be4c7fbff88e95ef69934e13e2199b"},"cell_type":"markdown","source":"You see number of unique item for Target  with command below:"},{"metadata":{"_kg_hide-input":true,"_uuid":"c7937700664991b29bdb0b3f04942c59498da760","trusted":true},"cell_type":"code","source":"train_target = train['target'].values\n\nnp.unique(train_target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae08b544a8d4202c7d0a47ec83d685e81c91a66d"},"cell_type":"markdown","source":"To check the first 5 rows of the data set, we can use head(5)."},{"metadata":{"_kg_hide-input":true,"_uuid":"5899889553c3416b27e93efceddb106eb71f5156","trusted":true},"cell_type":"code","source":"train.head(5) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1150b6ac3d82562aefd5c64f9f01accee5eace4d"},"cell_type":"markdown","source":"Or to check out last 5 row of the data set, we use tail() function."},{"metadata":{"_uuid":"79339442ff1f53ae1054d794337b9541295d3305","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.tail() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8a1cc36348c68fb98d6cb28aa9919fc5f2892f3"},"cell_type":"markdown","source":"To give a **statistical summary** about the dataset, we can use **describe()**\n"},{"metadata":{"_uuid":"3f7211e96627b9a81c5b620a9ba61446f7719ea3","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.describe() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10bdb8246f66c14043392806cae714f688cc8251"},"cell_type":"markdown","source":"As you can see, the statistical information that this command gives us is not suitable for this type of data\n**describe() is more useful for numerical data sets**"},{"metadata":{"_uuid":"91dda1f631cf4ed362162501aaaac6d19cfd6cc7"},"cell_type":"markdown","source":"<a id=\"20\"></a> <br>\n## 6-3 Data Preprocessing\n**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n \nData Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\nthere are plenty of steps for data preprocessing and we just listed some of them in general(Not just for Quora) :\n1. removing Target column (id)\n1. Sampling (without replacement)\n1. Making part of iris unbalanced and balancing (with undersampling and SMOTE)\n1. Introducing missing values and treating them (replacing by average values)\n1. Noise filtering\n1. Data discretization\n1. Normalization and standardization\n1. PCA analysis\n1. Feature selection (filter, embedded, wrapper)\n1. Etc.\n\nWhat methods of preprocessing can we run on  Quora?! \n###### [Go to top](#top)"},{"metadata":{"_uuid":"6c8c838f497c66a227975fb9a2f588e431f0c568"},"cell_type":"markdown","source":"**<< Note 2 >>**\nin pandas's data frame you can perform some query such as \"where\""},{"metadata":{"_uuid":"c8c8d9fd63d9bdb601183aeb4f1435affeb8a596","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.where(train ['target']==1).count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33fc33a18489b438a884819d99dc00a02b113be8"},"cell_type":"markdown","source":"As you can see in the below in python, it is so easy perform some query on the dataframe:"},{"metadata":{"_uuid":"8b545ff7e8367c5ab9c1db710f70b6936ac8422c","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train[train['target']<-32]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2788d023986eca622f7db9e1d64c2a4e02737ddb"},"cell_type":"markdown","source":"Some examples of questions that they are insincere"},{"metadata":{"_uuid":"d517b2b99a455a6b89c238faf1647515b8a67d87","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train[train['target']==1].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa882e5bcdc7d5f440489eff75d1d225269655a4"},"cell_type":"markdown","source":"**<< Note >>**\n>**Preprocessing and generation pipelines depend on a model type**"},{"metadata":{"_uuid":"055772bd170aa8018aabd85106b76675802c33b3"},"cell_type":"markdown","source":"<a id=\"23\"></a> <br>\n## 6-4 Data Visualization\n**Data visualization**  is the presentation of data in a pictorial or graphical format. It enables decision makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns.\n\n> * Two** important rules** for Data visualization:\n>     1. Do not put too little information\n>     1. Do not put too much information\n\n###### [Go to top](#top)"},{"metadata":{"_uuid":"fbe8c50bcc1b632f42dd249e27a9a7c14517fd29"},"cell_type":"markdown","source":"<a id=\"63\"></a> <br>\n## 6-4-3  Histogram"},{"metadata":{"_kg_hide-input":true,"_uuid":"e065ebff5374a9ab83df9c099a05962eb3645934","trusted":true},"cell_type":"code","source":"train[\"target\"].hist();","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d6c35cd011504530293e6c258b99be971797ca38"},"cell_type":"code","source":"sns.distplot(train['target'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bd6263adedda9a34b77e33bd2a69678ae205b946"},"cell_type":"code","source":"sns.violinplot(data=train, x=\"feature_1\", y='target')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72cc7c7b60a33390a85b16bc34e3b9e424650cdd"},"cell_type":"markdown","source":"<a id=\"24\"></a> <br>\n## 7- Apply Learning\nHow to understand what is the best way to solve our problem?!\n\nThe answer is always \"**It depends**.\" It depends on the **size**, **quality**, and **nature** of the **data**. It depends on what you want to do with the answer. It depends on how the **math** of the algorithm was translated into instructions for the computer you are using. And it depends on how much **time** you have. Even the most **experienced data scientists** can't tell which algorithm will perform best before trying them.(see a nice [cheatsheet](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/microsoft-machine-learning-algorithm-cheat-sheet-v7.pdf) for this section)\nCategorize the problem\nThe next step is to categorize the problem. This is a two-step process.\n\n1. **Categorize by input**:\n    1. If you have labelled data, it’s a supervised learning problem.\n    1. If you have unlabelled data and want to find structure, it’s an unsupervised learning problem.\n    1. If you want to optimize an objective function by interacting with an environment, it’s a reinforcement learning problem.\n1. **Categorize by output**.\n    1. If the output of your model is a number, it’s a regression problem.\n    1. If the output of your model is a class, it’s a classification problem.\n    1. If the output of your model is a set of input groups, it’s a clustering problem.\n    1. Do you want to detect an anomaly ? That’s anomaly detection\n1. **Understand your constraints**\n    1. What is your data storage capacity? Depending on the storage capacity of your system, you might not be able to store gigabytes of classification/regression models or gigabytes of data to clusterize. This is the case, for instance, for embedded systems.\n    1. Does the prediction have to be fast? In real time applications, it is obviously very important to have a prediction as fast as possible. For instance, in autonomous driving, it’s important that the classification of road signs be as fast as possible to avoid accidents.\n    1. Does the learning have to be fast? In some circumstances, training models quickly is necessary: sometimes, you need to rapidly update, on the fly, your model with a different dataset.\n1. **Find the available algorithms**\n    1. Now that you a clear understanding of where you stand, you can identify the algorithms that are applicable and practical to implement using the tools at your disposal. Some of the factors affecting the choice of a model are:\n\n    1. Whether the model meets the business goals\n    1. How much pre processing the model needs\n    1. How accurate the model is\n    1. How explainable the model is\n    1. How fast the model is: How long does it take to build a model, and how long does the model take to make predictions.\n    1. How scalable the model is\n"},{"metadata":{"_uuid":"cf3679a51c72dbe2d2549b5fe97e4ac5f1fa0fa0"},"cell_type":"markdown","source":"you can Fork and Run this kernel on **Github**:\n> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n"},{"metadata":{"_uuid":"3218340bb7dfc4ab53987820284a5c2b1c34eb45"},"cell_type":"markdown","source":"#### The kernel is not completed and will be updated soon  !!!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}