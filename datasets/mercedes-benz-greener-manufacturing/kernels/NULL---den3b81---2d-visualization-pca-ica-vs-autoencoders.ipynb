{"nbformat_minor": 0, "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.1", "name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "cells": [{"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "83a3729b473c315f7bc2c66640768092ccb798c4", "_cell_guid": "bdd82027-9d53-4d05-b6ec-cf95300cbaaa"}, "cell_type": "markdown", "execution_count": null, "source": "Hello there!\n\n*Autoencoders* (AE) are neural networks which are trained to learn a compressed representation of the inputs they are fed. For instance, they can be employed for compressing images... although they are no match for specific algorithms such as JPEG. I employed them successfully in the past for anomaly detection in real-time signal processing. \n\nAE have a symmetric hour-glass shape. The two symmetric parts are known as the **encoder** and than **decoder**, respectively (..oh really?). The encoder takes the original inputs and compress them into **codes**. The decoder tries to reconstruct the original inputs from these codes. AE are trained by minimizing a *reconstruction error*, i.e. some measure of the difference between the original patterns and the reconstructed ones.\n\nIn this notebook I will show you how **the dimensionality reduction performed by a simple AE (made using Keras) is more interesting than that of PCA or ICA**, at least for visualization purposes. In particular, I will be comparing the codes of an AE with 1 hidden layer of 2 neurons (yeah... that corresponds to a massive compression, over 150x) against the first 2 principal components of the Mercedes dataset many of you are already familiar with.\n\nIt won't take long, so bear with me for a while and upvote this notebook if you liked it. If you are interested in Autoencoders I strongly suggest you to have a look at [this Keras blog][1]. \n\n\n  [1]: https://blog.keras.io/building-autoencoders-in-keras.html"}, {"outputs": [], "source": "# import base modules\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_cell_guid": "bb7bfc89-f3f8-4ff7-b60e-aaec7b0fee90", "_uuid": "ada647bbccdd5afc7c026999c420e3b290f40098", "trusted": false}}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "4746f0bf4999e8d8e986dce7186246e5984a4737", "trusted": false, "_cell_guid": "56af20f4-e184-43e9-b869-f2cf60c97be4"}, "cell_type": "code", "execution_count": null, "source": "# load training data\nprint('Load data...')\ndf_train = pd.read_csv('../input/train.csv')"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "7d0cc326bf65ceab0998e28fe74b0586ea678108", "trusted": false, "_cell_guid": "8d19401b-1d95-419b-b771-cef013b961ef"}, "cell_type": "code", "execution_count": null, "source": "# let's keep just the boolean features\ntrain = df_train.iloc[:,10:]\n# ... and the y\ny = df_train.y"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "e552d5681d092b00d62d19052ba2b902ce87d03c", "trusted": false, "_cell_guid": "57261003-ceeb-4e13-aeef-8815648d6359"}, "cell_type": "code", "execution_count": null, "source": "# Let's quickly build an Autoencoder with 1 hidden layer and only 2 hidden neurons. \nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\nencoding_dim = 2\ninput_layer = Input(shape=(train.shape[1],))\nencoded = Dense(encoding_dim, activation='relu')(input_layer)\ndecoded = Dense(train.shape[1], activation='sigmoid')(encoded)\n\n# let's create and compile the autoencoder\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "fad30237b93b0a0c59bb23d017562af625e8ca08", "trusted": false, "_cell_guid": "b8e2507d-a15a-42bb-a220-4d8c4241e7da"}, "cell_type": "code", "execution_count": null, "source": "# let's train the autoencoder, checking the progress on a validation dataset \nfrom sklearn.model_selection import train_test_split\nX1, X2, Y1, Y2 = train_test_split(train, train, test_size=0.2, random_state=42)\n\n# these parameters seems to work for the Mercedes dataset\nautoencoder.fit(X1.values, Y1.values,\n                epochs=300,\n                batch_size=200,\n                shuffle=False,\n                verbose = 2,\n                validation_data=(X2.values, Y2.values))"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "0dba21ec6a2643ea2c3b468ce21d7af3e980d6e3", "trusted": false, "_cell_guid": "4c90c16e-c58d-4906-9519-1d7a0ad11482"}, "cell_type": "code", "execution_count": null, "source": "# now let's evaluate the coding of the initial features\nencoder = Model(input_layer, encoded)\npreds = encoder.predict(train.values)"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "d850b3ec10c079adb041b6d9c3b828ea3ce67644", "trusted": false, "_cell_guid": "7429cfb4-a2d3-4810-8341-87359e433716"}, "cell_type": "code", "execution_count": null, "source": "#... and let's plot the two components of the compression on a scatter plot that also shows \n# the y value associated to each point. PCA decomposition is provided as well for comparison\nplt.figure(figsize = (17,5))\nplt.subplot(131)\nplt.scatter(preds[:,0],preds[:,1],  c = y, cmap = \"RdGy\", \n            edgecolor = \"None\", alpha=1, vmin = 75, vmax = 150)\nplt.colorbar()\nplt.title('AE Scatter Plot')\n\n\n# ICA and PCA (first 2 components)\nfrom sklearn.decomposition import PCA, FastICA # Principal Component Analysis module\nica = FastICA(n_components=2)\nica_2d = ica.fit_transform(train.values)\n\nplt.subplot(132)\nplt.scatter(ica_2d[:,0],ica_2d[:,1],  c = y, cmap = \"RdGy\",\n            edgecolor = \"None\", alpha=1, vmin = 75, vmax = 150)\nplt.colorbar()\nplt.title('ICA Scatter Plot')\n\npca = PCA(n_components=2)\npca_2d = pca.fit_transform(train.values)\n\nplt.subplot(133)\nplt.scatter(pca_2d[:,0],pca_2d[:,1],  c = y, cmap = \"RdGy\",\n            edgecolor = \"None\", alpha=1, vmin = 75, vmax = 150)\nplt.colorbar()\nplt.title('PCA Scatter Plot')\n\nplt.show()"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "7371966fec5affa8e646da5fe85017341464155a", "_cell_guid": "cd2bf21a-38f0-417f-9116-e1c59c26b77a"}, "cell_type": "markdown", "execution_count": null, "source": "### Can you see how the cluster with very low y is clearer when considering the AE compressed representation? In the same way, the AE representation helps identify at least another cluster with low-to-medium y."}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "96d73819c5cf208411abac5dc2cadd8efa38a352", "_cell_guid": "f79ad1ee-86f6-4f5e-9bf2-2b689b7fe900"}, "cell_type": "markdown", "execution_count": null, "source": "### Btw, that cluster is easily easily identified by variable X232... but I think many of you already know this. If not, see below..."}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "a943f6ba0fb69b94b4d602b22bc6641406247df8", "trusted": false, "_cell_guid": "4e64813a-af14-4f1b-ac00-f65412ade005"}, "cell_type": "code", "execution_count": null, "source": "var = df_train.X232\nplt.figure(figsize = (15,5))\nplt.subplot(131)\nplt.scatter(preds[:,0],preds[:,1],  c = y, cmap = \"RdGy\", \n            edgecolor = \"None\", alpha=1, vmin = 75, vmax = 150)\nplt.title('AE Scatter Plot')\n\nplt.subplot(132)\nplt.scatter(preds[:,0],preds[:,1],  c = var, cmap = \"jet\", \n            edgecolor = \"None\", alpha=1, vmin = 0, vmax = 1)\nplt.title('X232')\n\nplt.subplot(133)\nbins = np.linspace(75, 275, 51)\nplt.hist(y[var==0], bins, alpha=0.5, label='0', color = plt.cm.jet(0))\nplt.hist(y[var==1], bins, alpha=0.75, label='1', color = plt.cm.jet(255))\nplt.title('X232')\nplt.legend(loc='upper right')\n\nplt.show()"}, {"outputs": [], "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "283538a11cfa0a08abe6a3dd164cab1473bc3051", "trusted": false, "_cell_guid": "a98be400-3d26-4084-90ab-0c516ef27529"}, "cell_type": "code", "execution_count": null, "source": ""}]}