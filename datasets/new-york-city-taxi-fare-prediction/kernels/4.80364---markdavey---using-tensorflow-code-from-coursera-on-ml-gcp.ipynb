{"cells":[{"metadata":{"_uuid":"7d9d7b443e11f6265d0376ced1b9103a7bc3a017","colab_type":"text","id":"4f3CKqFUqL2-","slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# The code is using feature engineering from github"},{"metadata":{"_uuid":"1d39544ae022f6ad776d4df355427d58e6cea103"},"cell_type":"markdown","source":"**Learning Objectives:**\n  * Use a pre-definfed DNNLinearCombinedRegressor  estimator of the `Estimator` class in TensorFlow to predict taxi rides\n"},{"metadata":{"_uuid":"e0709085cb56dea9623a4d2635af16cdcdac4c9f"},"cell_type":"markdown","source":"The data is based on taxi fares. This code was set up as a pipeline in the GCP code. I've refactored it into a Jupyter notebook\n<p>\nUsing code from the coursera class which is at github: https://github.com/kariato/training-data-analyst With the actual code being at https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/machine_learning/feateng/taxifare\n\n<p>\nThis is my first python note books so it has bugs please fork and tell me were I went wrong"},{"metadata":{"_uuid":"48b8f8da190f5e428ba3951c618dd9c2bb816ebc","colab_type":"text","id":"6TjLjL9IU80G"},"cell_type":"markdown","source":"## Set Up\nIn this first cell, we'll load the necessary libraries."},{"metadata":{"_uuid":"33e1a28bb7db9ed0f3549add28d1c1c5ab22d6cb","trusted":true,"collapsed":true},"cell_type":"code","source":"import math\nimport shutil\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport os\nfrom dateutil.parser import parse\nfrom pytz import timezone\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03905d999d6479e320b930cbd07ef7ae29156f7b"},"cell_type":"markdown","source":"# Constants and Hyper-parameters"},{"metadata":{"_uuid":"68a198c1839bdd3502907c165fcc5d29898a3079","trusted":true,"collapsed":true},"cell_type":"code","source":"#Constants\nMIN_LONG = -74.3\nMAX_LONG = -73.0\nMIN_LAT = 40.6\nMAX_LAT = 41.7\nMAX_PASSENGER = 10 \nMIN_FARE = 0.0 \noutput_dir = \".\"\nOUTDIR = \".\"\nOUTPUT_RESULT=\"submission.csv\"\n#Hyper prameters\nBUCKETS=20\nHIDDEN_UNITS = \"128 32 4\"\nSCALE = 10\nBATCH_SIZE=32\nROWS_TO_READ=40000\nROWS_TO_SKIP=10\nLEARNING_RATE=0.04\nSTEPS_TO_PROCESS=40000","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbeef361a56f2a82ec18a70812e227dd505dd890","colab_type":"text","id":"ipRyUHjhU80Q"},"cell_type":"markdown","source":"Next, we'll load our data set."},{"metadata":{"_uuid":"99b19a5c142a80a276a83f5e24e55a3856c64fc1","trusted":true,"collapsed":true},"cell_type":"code","source":"df = type('', (), {})()\nprint(datetime.now())\ndf.train = pd.read_csv('../input/train.csv', sep=\",\", skiprows=range(1,ROWS_TO_SKIP),nrows=ROWS_TO_READ)\nprint(datetime.now())\ndf.test = pd.read_csv('../input/test.csv', sep=\",\")\nprint(datetime.now())\n#df.train.head(10)\ndf.train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d5a84f293985f11c3058b9d3244221f908986f1","colab_type":"text","id":"HzzlSs3PtTmt","slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"## Examine the data\n\nIt's a good idea to get to know your data a little bit before you work with it.\n\nWe'll print out a quick summary of a few useful statistics on each column.\n\nThis will include things like mean, standard deviation, max, min, and various quantiles."},{"metadata":{"_uuid":"556a8f92e163f4df5802e049b39a25650dd1e3f6","trusted":true,"collapsed":true},"cell_type":"code","source":"df.train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83c9905534084e0466d4049e877ee0c5d89d1c21"},"cell_type":"markdown","source":"We need to clean up the data to match the test data"},{"metadata":{"_uuid":"0787b46ae54857e57d615b029120ecc99025ccc1","trusted":true,"collapsed":true},"cell_type":"code","source":"#The test data is clean the training data has quite a bit of not from new york locations\n#I know google thinks you should not do this since the bad data mean something but try\ndef clean(dfn):\n    dfn=dfn[  ((MIN_LONG) <= dfn['pickup_longitude']) & (dfn['pickup_longitude'] <= (MAX_LONG)) ]\n    dfn=dfn[ (MIN_LAT <= dfn['pickup_latitude']) & (dfn['pickup_latitude'] <= MAX_LAT) ]\n    dfn=dfn[ ((MIN_LONG) <= dfn['dropoff_longitude']) & (dfn['dropoff_longitude'] <= (MAX_LONG) )]\n    dfn=dfn[ (MIN_LAT <= dfn['dropoff_latitude'])  & (dfn['dropoff_latitude'] <= MAX_LAT) ]\n    dfn=dfn[dfn['passenger_count'] <= MAX_PASSENGER ]\n    dfn=dfn[dfn['fare_amount'] >= MIN_FARE ]\n    return dfn\ndf.train=clean(df.train)\ndf.train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62a020154bb669f0825196e9195e3e91678ee63d"},"cell_type":"markdown","source":"# Calculate Time of each ride\nThe calcuating the hour and week day for millions of rows is costly so we pre-calcualte all possible values"},{"metadata":{"_uuid":"4a1ad5b31a537b9695538a4d84390772b2c60f81","trusted":true,"scrolled":false,"collapsed":true},"cell_type":"code","source":"##calculate times\ndf.train['nyctime'] = df.train.apply(lambda row: row['pickup_datetime'][:14]+'00:00 UTC', axis=1)\ndf.test['nyctime'] = df.test.apply(lambda row: row['pickup_datetime'][:14]+'00:00 UTC', axis=1)\n\nnycTimes = []\ndef findTimes(timeStr, nycDict, field):\n    if not(timeStr[:14]+'00:00 UTC' in nycDict):\n        nycTime = {}\n        nycTime['time'] = parse(timeStr).astimezone(timezone('US/Eastern'))\n        nycTime['weekday'] = int(nycTime['time'].weekday())\n        nycTime['hour'] = int(nycTime['time'].hour)\n        nycTime['hourSince2000'] = int(((nycTime['time'].year-2009)*366+int(nycTime['time'].strftime(\"%j\")))*25+nycTime['time'].hour)\n        nycTime['nyctime'] = timeStr[:14]+'00:00 UTC'\n        nycTimes.append(nycTime)\n    return \n\nminDate=parse(df.train['pickup_datetime'].min())\nmaxDate=parse(df.train['pickup_datetime'].max())\nwhile (minDate < maxDate):\n    findTimes(minDate.strftime(\"%Y-%m-%d %H:%M:%S%z\"),nycTimes,'time')\n    minDate = minDate + timedelta(hours=1)\n\ndf.times = pd.DataFrame(nycTimes)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d746523ec083bfad1ee20d8472bb743988a82e2"},"cell_type":"markdown","source":"Now join the data frames on the hourly time key"},{"metadata":{"trusted":true,"_uuid":"b83419cc337ae30fc3b50f138876159ccc0befaf","collapsed":true},"cell_type":"code","source":"df.train=df.train.join(df.times.set_index('nyctime'), on='nyctime')\ndf.test=df.test.join(df.times.set_index('nyctime'), on='nyctime')\ndf.times.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97f6c0188c807316844f1fa43a398f56dd8dfeb1","collapsed":true},"cell_type":"code","source":"df.train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"240ad3b15c755938b6ecaf4219e76370ca3af5f0"},"cell_type":"markdown","source":"# Feature Engineering on data set"},{"metadata":{"_uuid":"9fa5c8e1f1bb6123684d68fd00a292611ab8cdad","trusted":true,"collapsed":true},"cell_type":"code","source":"# Create feature engineering function that will be used in the input and serving input functions\ndef add_engineered(features):\n    # this is how you can do feature engineering in TensorFlow\n    lat1 = features['pickup_latitude']\n    lat2 = features['dropoff_latitude']\n    lon1 = features['pickup_longitude']\n    lon2 = features['dropoff_longitude']\n    latdiff = (lat1 - lat2)\n    londiff = (lon1 - lon2)\n    \n    # set features for distance with sign that indicates direction\n    features['latdiff'] = latdiff\n    features['londiff'] = londiff\n    dist = (latdiff * latdiff + londiff * londiff)**(0.5)\n    features['euclidean'] = dist\n    features['cityBlockDist'] = abs(latdiff) + abs(londiff)\n    return features\n\ndf.train = add_engineered(df.train)\ndf.test = add_engineered(df.test)\n\ndf.train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"006db9e7e369a888d6a59021ee487b7956c1a1f1"},"cell_type":"markdown","source":"This is the measure used to see how close the data is to actual taxi fares"},{"metadata":{"trusted":true,"_uuid":"b27ae53ab973cf48188ad09900b5119d434bf8e1","collapsed":true},"cell_type":"code","source":"def rmse(labels, predictions):\n    pred_values = tf.cast(predictions['predictions'],tf.float64)\n    return {'rmse': tf.metrics.root_mean_squared_error(labels*SCALE, pred_values*SCALE)}\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6430185575da0e46e6f9e3555522c3f91a4958d7"},"cell_type":"markdown","source":"# Build an estimator starting from INPUT COLUMNS.\n     These include feature transformations and synthetic features.\n     The model is a wide-and-deep model."},{"metadata":{"trusted":true,"_uuid":"d086db9de87ed0a67522afad6d168a20060894cb","collapsed":true},"cell_type":"code","source":"# These are the raw input columns, and will be provided for prediction also\nINPUT_COLUMNS = [\n    # Define features\n    \n    # Numeric columns\n    tf.feature_column.numeric_column('weekday'),\n    tf.feature_column.numeric_column('hour'),\n    tf.feature_column.numeric_column('pickup_latitude'),\n    tf.feature_column.numeric_column('pickup_longitude'),\n    tf.feature_column.numeric_column('dropoff_latitude'),\n    tf.feature_column.numeric_column('dropoff_longitude'),\n    tf.feature_column.numeric_column('passenger_count'),\n    #tf.feature_column.numeric_column('hourSince2000'),\n    \n    # Engineered features that are created in the input_fn\n    tf.feature_column.numeric_column('latdiff'),\n    tf.feature_column.numeric_column('londiff'),\n    tf.feature_column.numeric_column('euclidean'),\n    tf.feature_column.numeric_column('cityBlockDist')\n]\n# Build the estimator\ndef build_estimator(model_dir, nbuckets, hidden_units):\n    \"\"\"\n     \n  \"\"\"\n\n    # Input columns   hourSince2000,\n    (dayofweek, hourofday, plat, plon, dlat, dlon, pcount, latdiff, londiff, euclidean,cityBlockDist) = INPUT_COLUMNS\n\n    # Bucketize the times \n    hourbuckets = np.linspace(0.0, 23.0, 24).tolist()\n    b_hourofday = tf.feature_column.bucketized_column(hourofday, hourbuckets)\n    weekdaybuckets = np.linspace(0.0, 6.0, 7).tolist()\n    b_dayofweek = tf.feature_column.bucketized_column(dayofweek, weekdaybuckets)\n    #since2000buckets = np.linspace(0.0, 599999, 60000).tolist()\n    #b_hourSince2000 = tf.feature_column.bucketized_column(hourSince2000, since2000buckets)\n    \n    # Bucketize the lats & lons\n    latbuckets = np.linspace(38.0, 42.0, nbuckets).tolist()\n    lonbuckets = np.linspace(-76.0, -72.0, nbuckets).tolist()\n    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\n    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\n    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\n    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\n   \n    # Feature cross\n    ploc = tf.feature_column.crossed_column([b_plat, b_plon], nbuckets * nbuckets)\n    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], nbuckets * nbuckets)\n    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4 )\n    day_hr =  tf.feature_column.crossed_column([b_dayofweek, b_hourofday], 24 * 7)\n\n    # Wide columns and deep columns.\n    wide_columns = [\n        # Feature crosses\n        dloc, ploc, pd_pair,\n        day_hr,\n\n        # Sparse columns\n        b_dayofweek, b_hourofday,\n        #b_hourSince2000,\n\n        # Anything with a linear relationship\n        pcount \n    ]\n\n    deep_columns = [\n        # Embedding_column to \"group\" together ...\n        tf.feature_column.embedding_column(pd_pair, 10),\n        tf.feature_column.embedding_column(day_hr, 10),\n        #tf.feature_column.embedding_column(b_hourSince2000, 60000),\n        # Numeric columns\n        plat, plon, dlat, dlon,\n        latdiff, londiff, euclidean,cityBlockDist\n    ]\n    \n    estimator = tf.estimator.DNNLinearCombinedRegressor(\n        model_dir = model_dir,\n        linear_feature_columns = wide_columns,\n        dnn_feature_columns = deep_columns,\n        dnn_hidden_units = hidden_units)\n\n    # add extra evaluation metric for hyperparameter tuning\n      \n    estimator = tf.contrib.estimator.add_metrics(estimator, rmse)\n    return estimator\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14a72f99da3bcfd8922f4463cb41d7e6cbe87dea","colab_type":"text","id":"Lr6wYl2bt2Ep","slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"## **Build a neural network model**\n\nIn this exercise, we'll be trying to predicttaxi fares. Ok get all the features into a dictionary"},{"metadata":{"trusted":true,"_uuid":"06477f8f5988f45c875ebf2e1aa6c8170db4f848","collapsed":true},"cell_type":"code","source":"feature_columns={}\nfor i in INPUT_COLUMNS:\n    feature_columns[i.key]=i\nlist(feature_columns.keys())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79975fc20e81da73159c5ca9de09fe5e6bc33f7e"},"cell_type":"markdown","source":"Take the panda data and use the estimator functions to turn it into processed data"},{"metadata":{"_uuid":"8edb375ceb93ad7dd2a4aa9ade918e63df6a7c4a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Split into train and eval and create input functions\nmsk = np.random.rand(len(df.train)) < 0.8\ntraindf = df.train[msk]\nevaldf = df.train[~msk]\n\ntrain_input_fn = tf.estimator.inputs.pandas_input_fn(x = traindf[list(feature_columns.keys())],\n                                                    y = traindf[\"fare_amount\"] / SCALE,\n                                                    num_epochs = 1,\n                                                    batch_size = BATCH_SIZE,\n                                                    shuffle = True)\neval_input_fn = tf.estimator.inputs.pandas_input_fn(x = evaldf[list(feature_columns.keys())],\n                                                    y = evaldf[\"fare_amount\"] / SCALE,  # note the scaling\n                                                    num_epochs = 1, \n                                                    batch_size = len(evaldf), \n                                                    shuffle=False)\npredict_input_fn = tf.estimator.inputs.pandas_input_fn(x = df.test[list(feature_columns.keys())],\n                                                    y = None,  # note the scaling\n                                                    num_epochs = 1, \n                                                    batch_size = len(df.test), \n                                                    shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d82f22d4e8ba83b0124c25e19eaf6b3681b8e772"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19f21c50e5cf9856dede876eca056de4cdfbe90c","scrolled":false,"collapsed":true},"cell_type":"code","source":"  tf.logging.set_verbosity(tf.logging.INFO)\n  myopt = tf.train.FtrlOptimizer(learning_rate = LEARNING_RATE) # note the learning rate\n  estimator = estimator = build_estimator(OUTDIR, BUCKETS, HIDDEN_UNITS.split(' '))\n    \n  estimator = tf.contrib.estimator.add_metrics(estimator,rmse)\n  \n  train_spec=tf.estimator.TrainSpec(\n                       input_fn = train_input_fn,max_steps = STEPS_TO_PROCESS)\n  eval_spec=tf.estimator.EvalSpec(\n                       input_fn = eval_input_fn,\n                       steps = None,\n                       start_delay_secs = 1, # start evaluating after N seconds\n                       throttle_secs = 10,  # evaluate every N seconds\n                       )\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da140b9ef33a1b29410395fbcd45456ccf9ff5ab"},"cell_type":"markdown","source":"# Predict using the estimator"},{"metadata":{"trusted":true,"_uuid":"44ae62cd892ce451cd534116598f960925ef75ce","collapsed":true},"cell_type":"code","source":"  predictions=estimator.predict(input_fn=predict_input_fn)\n  pred = pd.DataFrame({'fare_amount':[i['predictions'][0]*SCALE for i in predictions]})\n  submission=pd.concat([df.test['key'],pred],axis=1)\n  submission.to_csv(OUTPUT_RESULT,index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b0922795cd31b9a1aa69dc36a7a4ac7d0716f55a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"default_view":{},"name":"first_steps_with_tensor_flow.ipynb","provenance":[],"version":"0.3.2","views":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}