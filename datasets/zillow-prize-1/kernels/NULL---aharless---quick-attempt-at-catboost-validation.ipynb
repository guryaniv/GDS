{"metadata": {"language_info": {"name": "python", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1, "cells": [{"source": ["Based on See--'s notebook, of course...."], "metadata": {"_uuid": "d7e9311bbd809ba0aa88479996abbe89ad4e5b1d", "_cell_guid": "6a2abb6d-e906-4c9f-8923-19112d298b67"}, "cell_type": "markdown"}, {"source": ["import pandas as pd\n", "import numpy as np\n", "from catboost import CatBoostRegressor\n", "from sklearn.metrics import mean_absolute_error"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "00e6bfcd2da6d0199d09e9f1c3341745029c7235", "collapsed": true, "_cell_guid": "d0f0ce4d-75b9-475d-add9-5bc6f24357f9"}, "cell_type": "code"}, {"source": ["> ## Data loading, we parse transactiondate"], "metadata": {"_uuid": "b13eb7681d1664b73e4d54ca80ce0a7d5331aa57", "_cell_guid": "b36ca855-dcf0-4956-b3a3-1badba0a4cd5"}, "cell_type": "markdown"}, {"source": ["train_df = pd.read_csv('../input/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\n", "test_df = pd.read_csv('../input/sample_submission.csv', low_memory=False)\n", "properties = pd.read_csv('../input/properties_2016.csv', low_memory=False)\n", "# field is named differently in submission\n", "test_df['parcelid'] = test_df['ParcelId']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "a7a5c89651983966017b964bd25853c026e79613", "collapsed": true, "_cell_guid": "0b0142aa-8a69-407c-8ed6-647f52c2c79c"}, "cell_type": "code"}, {"source": ["# similar to the1owl\n", "def add_date_features(df):\n", "    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n", "    df[\"transaction_month\"] = df[\"transactiondate\"].dt.month\n", "    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n", "    df[\"transaction_quarter\"] = df[\"transactiondate\"].dt.quarter\n", "    df.drop([\"transactiondate\"], inplace=True, axis=1)\n", "    return df"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "1c24a39df7a2e4d27afe429b883976b31438444d", "collapsed": true, "_cell_guid": "49c98a71-c4eb-4adc-ba9e-3257ad3112a6"}, "cell_type": "code"}, {"source": ["VAL_SPLIT_DATE = '2016-09-15'   # Cutoff date for validation split\n", "select_qtr4 = train_df[\"transactiondate\"] >= VAL_SPLIT_DATE\n", "valid_df = train_df[select_qtr4]\n", "train_df = train_df[~select_qtr4]\n", "train_df = add_date_features(train_df)\n", "valid_df = add_date_features(valid_df)\n", "train_df = train_df.merge(properties, how='left', on='parcelid')\n", "valid_df = valid_df.merge(properties, how='left', on='parcelid')\n", "test_df = test_df.merge(properties, how='left', on='parcelid')\n", "print(\"Train: \", train_df.shape)\n", "print(\"Test: \", test_df.shape)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "eb9217247052ecd75669acf3adb641949b2e797e", "_cell_guid": "11a44166-e6f7-49bb-80e9-4ff8891dfc1b"}, "cell_type": "code"}, {"source": ["# 0.a) Remove missing data fields"], "metadata": {"_uuid": "c9c7096a81696525317c5b9bd2de4c456463daff", "_cell_guid": "316cb2e8-417b-4923-9ac3-6f22a36bd66c"}, "cell_type": "markdown"}, {"source": ["missing_perc_thresh = 0.98\n", "exclude_missing = []\n", "num_rows = train_df.shape[0]\n", "for c in train_df.columns:\n", "    num_missing = train_df[c].isnull().sum()\n", "    if num_missing == 0:\n", "        continue\n", "    missing_frac = num_missing / float(num_rows)\n", "    if missing_frac > missing_perc_thresh:\n", "        exclude_missing.append(c)\n", "print(\"We exclude: %s\" % exclude_missing)\n", "print(len(exclude_missing))"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "753ffed925fa400f6b8532346660c5a05882c58c", "_cell_guid": "8352977d-4c2f-49ac-8a52-eaf571434924"}, "cell_type": "code"}, {"source": ["# 0.b) Remove data that is always the same"], "metadata": {"_uuid": "94278c1a63bfff8a44c2be043a269344cc4de89a", "_cell_guid": "895a7825-d2bb-418a-8f86-c58dd59d5c30"}, "cell_type": "markdown"}, {"source": ["# exclude where we only have one unique value :D\n", "exclude_unique = []\n", "for c in train_df.columns:\n", "    num_uniques = len(train_df[c].unique())\n", "    if train_df[c].isnull().sum() != 0:\n", "        num_uniques -= 1\n", "    if num_uniques == 1:\n", "        exclude_unique.append(c)\n", "print(\"We exclude: %s\" % exclude_unique)\n", "print(len(exclude_unique))"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "8b127a67bdc436702bc454ea959a73dd38d43324", "_cell_guid": "6f2efae0-2ebb-41b1-8f11-c60a44f69a11"}, "cell_type": "code"}, {"source": ["# 1.a) Define training features"], "metadata": {"_uuid": "e3eb67c9f6fe44195579bdb9f58b833f853940c4", "_cell_guid": "b645ccb4-a935-413f-808f-ceabce6f757d"}, "cell_type": "markdown"}, {"source": ["exclude_other = ['parcelid', 'logerror']  # for indexing/training only\n", "# do not know what this is LARS, 'SHCG' 'COR2YY' 'LNR2RPD-R3' ?!?\n", "exclude_other.append('propertyzoningdesc')\n", "train_features = []\n", "for c in train_df.columns:\n", "    if c not in exclude_missing \\\n", "       and c not in exclude_other and c not in exclude_unique:\n", "        train_features.append(c)\n", "print(\"We use these for training: %s\" % train_features)\n", "print(len(train_features))"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "288ad567e88fc7a64b0c87e6d705da4b63461163", "_cell_guid": "ab746fa0-7d15-433c-a5aa-9daabe0bbef3"}, "cell_type": "code"}, {"source": ["# 1.b) Define which of these training features are categorical"], "metadata": {"_uuid": "2ac2cbffda8aa023eba4cc2d1d2ad750a432d23f", "_cell_guid": "ae0ef91f-7ee6-49fc-bd44-affbdfa128d9"}, "cell_type": "markdown"}, {"source": ["cat_feature_inds = []\n", "cat_unique_thresh = 1000\n", "for i, c in enumerate(train_features):\n", "    num_uniques = len(train_df[c].unique())\n", "    if num_uniques < cat_unique_thresh \\\n", "       and not 'sqft' in c \\\n", "       and not 'cnt' in c \\\n", "       and not 'nbr' in c \\\n", "       and not 'number' in c:\n", "        cat_feature_inds.append(i)\n", "        \n", "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "d3678ded0e754c733101b7097a737e731ae01119", "_cell_guid": "9c9b7f5d-a31f-4996-9c7e-bccb8cda8654"}, "cell_type": "code"}, {"source": ["# 1.c) Fill missing values"], "metadata": {"_uuid": "32696c6609af571797ee8dd10516e50b6e2dfe4a", "_cell_guid": "fd7a9887-61c7-4742-94a4-2982d8be41b0"}, "cell_type": "markdown"}, {"source": ["# some out of range int is a good choice\n", "train_df.fillna(-999, inplace=True)\n", "valid_df.fillna(-999, inplace=True)\n", "test_df.fillna(-999, inplace=True)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "c199ecded662aba626f0e75240bb8d5a738cbb92", "collapsed": true, "_cell_guid": "26a1755f-88a9-4f7c-b477-4a30465a72fa"}, "cell_type": "code"}, {"source": ["# 2.a) Training time!"], "metadata": {"_uuid": "68d52b1e117a55dc0a5bb4d98bbcf5f2c57c1339", "_cell_guid": "be9d099e-28c9-404e-a37f-4c812ca9b405"}, "cell_type": "markdown"}, {"source": ["X_train = train_df[train_features]\n", "y_train = train_df.logerror\n", "X_valid = valid_df[train_features]\n", "y_valid = valid_df.logerror\n", "print(X_train.shape, y_train.shape)\n", "print(X_valid.shape, y_valid.shape)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "d22163b09dda59e33e88d50b076795b630ddc2c3", "_cell_guid": "f53d5b61-2803-4066-9ead-cd5714838e86"}, "cell_type": "code"}, {"source": ["test_df['transactiondate'] = pd.Timestamp('2016-12-01')  # Dummy\n", "test_df = add_date_features(test_df)\n", "X_test = test_df[train_features]\n", "print(X_test.shape)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "207c5a53b094ff42ca418ea7a3152c31bb5f441e", "_cell_guid": "5c4ad2db-33dd-4035-9045-70fa88e0d948"}, "cell_type": "code"}, {"source": ["num_ensembles = 2\n", "tree_counts = []\n", "MAEs = []\n", "for i in range(num_ensembles):\n", "    # TODO(you): Use CV, tune hyperparameters\n", "    model = CatBoostRegressor(\n", "        iterations=4000, learning_rate=0.004,\n", "        depth=6, l2_leaf_reg=15,\n", "        bagging_temperature=8,\n", "        loss_function='MAE',\n", "        eval_metric='MAE',\n", "        random_seed=i)\n", "    model.fit(\n", "        X_train, y_train,\n", "        eval_set=[X_valid, y_valid],\n", "        cat_features=cat_feature_inds,\n", "#        verbose=True,\n", "        use_best_model=True\n", "        )\n", "    tree_counts.append( model.tree_count_ )\n", "    MAEs.append( mean_absolute_error(y_valid, model.predict(X_valid)) )   \n"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "101f5048a7fd0874cb7d0b7a49d4f5e27c01bb20", "_cell_guid": "7dc0215e-f2f1-41a3-bf68-d3b7e4ca0cd0"}, "cell_type": "code"}, {"source": ["print( tree_counts )\n", "print( MAEs )"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "814c5484b9973bd6d806a738c23a1b47d97405b5", "_cell_guid": "574da1d8-2f4e-4168-8ab6-3fb1688cf1ad"}, "cell_type": "code"}, {"source": ["The MAE's from the verbose validation (\"bestTest\") don't match those from the prediction (\"MAEs\").  Not sure why.  Maybe there is randomness in the prediction process? Maybe sklearn calculates it differently than Catboost? Maybe I'm misundertanding something? Maybe something else?"], "metadata": {"_uuid": "243c5896978e758af45c169291401a35e8ac56b7", "_cell_guid": "e5cf209b-8007-4b4c-ac25-438f4fb1b94c"}, "cell_type": "markdown"}, {"source": [], "outputs": [], "execution_count": null, "metadata": {"_uuid": "0c70b3a2356fa51acff74d8d37956e2e1fbd91c9", "collapsed": true, "_cell_guid": "d48c86fd-7430-4bed-bda6-69afca15400a"}, "cell_type": "code"}], "nbformat": 4}