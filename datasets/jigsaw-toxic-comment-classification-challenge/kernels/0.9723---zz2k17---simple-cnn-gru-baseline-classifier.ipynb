{"cells":[{"metadata":{"_uuid":"94005793e143431ce667ffba15d321a5419d5e8b"},"cell_type":"markdown","source":"# CNN-GRU Model\nBuilding a simple baseline model on the toxic comments dataset to explore multi-label classification.\n\nThis model is an application of theory learned from fchollet and johkhron excellent work in deep learning and has inspired me to advance my knowledge in various deep learning architectures and testing them on various datasets. \n\nThe baseline model is developed to show the potential on this particular dataset and set the foundations for improvement as well as motivation for more complex architectures. \n\nThe initial model developed is a 1D CNN with a single GRU layer trained using the GloVe 100D word embeddings, with a modest accuracy of ~96% training and validation.  This model takes the same architecture and applies the 200D word embeddings (as it is easily located on Kaggle) and shows the modest performance improvement using simply larger word vector embeddings. \n\nIn this notebook we will discuss data preprocessing, model training as well applying techniques in the future such as Dropout and Batch Normalization to further improve model generalization.\n\nAs I develop my own knowledge I aim to apply them in this notebook as a journey to track progress as well as inspire comments and discussions from other Kaggle members.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c12a06c04d4ff30130e77fc0f4f55e69692c448e"},"cell_type":"code","source":"# verify GPU acceleration\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7ad20146e779b572ca82e4470556d3308281375"},"cell_type":"markdown","source":"## Custom helper functions\nFunctions for reading in embedding data and plotting model accuracy"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"13a20bbbbe68e06a5c9d6b2efda865f28c215d85"},"cell_type":"code","source":"# functions for reading in embedding data and\n# tokenizing and processing sequences with padding and\n# function for plotting model accuracy and loss\n\nimport os\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# vectorizer and sequence function\n# takes in raw text and labels\n# params for max sequence length and max words\n# default arg for Shuffle=True to randomise data\n# returns tokenizer object. x_train,y_train, x_val,y_val\ndef tokenize_and_sequence(full_data_set, texts, labels, max_len, max_words, validation_samples, shuffle=True):\n    #initialise tokenizer with num_words param\n    tokenizer = Tokenizer(num_words=max_words)\n    tokenizer.fit_on_texts(full_data_set)\n    # convert texts to sequences\n    sequences = tokenizer.texts_to_sequences(texts)\n    # generate work index\n    word_index = tokenizer.word_index\n    # print top words count\n    print('{} of unique tokens found'.format(len(word_index)))\n    # pad sequences using max_len param\n    data = pad_sequences(sequences, maxlen=max_len)\n    # convert list of labels into numpy array\n    labels = np.asarray(labels)\n    # print shape of text and label tensors\n    print('data tensor shape: {}\\nlabel tensor shape:{}'.format(data.shape, labels.shape))\n\n    # shuffle data=True as labels are ordered\n    # randomise data to vary class distribution\n    if shuffle:\n        # get length of data sequence and create array\n        indices = np.arange(data.shape[0])\n        np.random.shuffle(indices)\n        # shuffle data and labels\n        data = data[indices]\n        labels = labels[indices]\n    else:\n        pass\n\n    # split training data into training and validation splits\n    # split using validation length\n    # validation split\n    x_val = data[:validation_samples]\n    y_val = labels[:validation_samples]\n    # training split\n    x_train = data[validation_samples:]\n    y_train = labels[validation_samples:]\n\n    # return tokenizer, word_index, training and validation data\n    return tokenizer, word_index, x_train, y_train, x_val, y_val\n\n\n# function to lpad pretrained glove embeddings\n# takes in embedding dim for variable embedding sizes\n# and base directory as well as txt file\n# embedding dim should match the file name dimension\n# and max words and word_index for embedding features\ndef load_glove(base_directory, f_name, max_words, word_index, embedding_dim=None):\n    # check file name ends in .txt\n    # read file name embedding value if not specified\n    if f_name[-4:] == '.txt':\n        # check embedding value\n        if embedding_dim is not None:\n            dim = f_name[-8:-5]\n            dim = int(dim)\n            embedding_dim = dim\n        else:\n            # assuming dimension is not none for manual input\n            pass\n        # continue\n\n        # create embedding dictionary\n        embeddings_index = {}\n        # open embeddings file\n        try:\n            f = open(os.path.join(base_directory, f_name))\n            # iterate over lines and split on individual words\n            # split coefficient of word values\n            # map words and coefficients to embeddings dictionary\n            for line in f:\n                values = line.split() # returns list of [word, coeff]\n                word = values[0] # gets first list element\n                coeff = np.asarray(values[1:], dtype='float32')  # slice coefficiennt value array from remainder of list\n                # assign mapping to dictionary\n                embeddings_index[word] = coeff\n            f.close()\n        except IOError:\n            print('cannot read file. check file paths')\n\n        # prepare glove word-embedding matrix\n        # create empty embedding tensor\n        embedding_matrix = np.zeros((max_words,embedding_dim ))\n        # map the top words of the data into the glove embedding matrix\n        # words not found from the data in glove will be zeroed\n        for word, i in word_index.items():\n            if i < max_words:\n                embedding_vector = embeddings_index.get(word)\n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n\n        # return embedding matrix\n        return embedding_matrix\n\n\n# function to visualise keras model history metrics\n# function takes in acc, val_acc, loss, val_loss for model params\n# range is defined by epochs in range len(acc)\n\nimport matplotlib.pyplot as plt\n\ndef plot_training_and_validation(acc, val_acc, loss, val_loss):\n    epochs = range(1, len(acc) + 1)\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()\n\n# end","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f963137c66d87c3add9adec5f478da25c81f66c"},"cell_type":"markdown","source":"# 1 - Baseline CNN-GRU Model\n"},{"metadata":{"_uuid":"193da66be580622c5ba89aa503860674f0b2308f"},"cell_type":"markdown","source":"# 1.1 - Loading Data\nIn this section we load the raw data into a Pandas DataFrame which will help in isolating the comments and labels into lists of arrays, ready for vectorization in 1.2"},{"metadata":{"trusted":true,"_uuid":"3168501aa25cf9abaf3d3e59174a445e4cbf4bc4"},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"trusted":true,"_uuid":"4980c232ff0dcbfa985185613259287be19eb21f"},"cell_type":"code","source":"# set file paths for training and test files\n# base directory\ntoxic_base = '../input/jigsaw-toxic-comment-classification-challenge/'\ntoxic_train = toxic_base + 'train.csv'\ntoxic_test = toxic_base + 'test.csv'\n# load train and test data into DataFrames\ntrain_df = pd.read_csv(toxic_train)\ntest_df = pd.read_csv(toxic_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1739ac3e7f9bc9f66bb030534b3a3dab74c5fac6"},"cell_type":"code","source":"# verify and inspect data\n# training data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a051ffdf36cd8b61bb5e5fbfcaed8b6da22e3397"},"cell_type":"code","source":"print(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22dbff15698e3b68da28cc8857d8df678d865850"},"cell_type":"code","source":"# test data\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ec832adef3df42984454c8a921b936c03fbb5df"},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffa1053b0d76b9c0b6abc906c6e10e505ee189c5"},"cell_type":"code","source":"# extract label columns\ncols = list(train_df.columns)\nprint(type(cols))\nprint(list(cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c90f831b78cf33a6ce76588f584a264c62ad42b"},"cell_type":"code","source":"# remove id and comment_text columns\ncols = cols[2:]\nprint(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f52ecbb4a15d104ce49046d7ff08e7117405b1c1"},"cell_type":"code","source":"# describe data and check for any null or missing values, and the spread of labels\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb99c2770e383516555b13852349794e40664cbe"},"cell_type":"code","source":"# check for null values in labels in training data\ntrain_df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e883fd2d1b1f128ea290206415340a81b9fb23f"},"cell_type":"code","source":"# check for null values in test data\ntest_df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c00669dbb3207faa6384d7248992b9a242b844d2"},"cell_type":"code","source":"# view spread of classess across the training data\nfor label in cols:\n    print('label: {}'.format(label))\n    for x in [0, 1]:\n        print('value: {}, total:{}'.format(x, train_df[label].eq(x).sum()))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18dd96d63e7038243c142f22aae7c264ae8716e9"},"cell_type":"markdown","source":"# 1.2 Split data into text and labels\nSplit training data into text and label arrays as well as test text"},{"metadata":{"trusted":true,"_uuid":"c78e53a58621f3b0e87a03a9393af9f312266231"},"cell_type":"code","source":"# split class labels into a y array\ny_train = train_df[cols].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e39c0ce7e25d9c72bae3b721aa7468fe2c2c90d"},"cell_type":"code","source":"# verify labels are arrays of 6 values\ny_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"928e8beb85b753eb25f4e9090b311a6a9ee017e2"},"cell_type":"code","source":"# convert to numpy array\ny_train = np.asarray(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ce37da4cdce284b70294aa36fd626d6ec7d154c"},"cell_type":"code","source":"# verify matrix shape\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95dea6ad72e8e4150c4d3141ce55e92748cc6287"},"cell_type":"code","source":"# split comment_text from training and test data\ncomment = 'comment_text'\n# extract Series object from DataFrames for train and test \nsentences_train = train_df[comment]\nsentences_test = test_df[comment]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cda6eeaf75a20308b271bcdd4fe86585634b7424"},"cell_type":"code","source":"# transform Series into list of text values\nx_train = list(sentences_train)\nx_test = list(sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08bc51f22ac63acf2f5b2c51a7a5666f9ea437cf","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# verify train and test text\nprint(x_train[0], '\\n')\nprint(x_test[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b760b71d859152d7e05d252cb61813018570c7b9"},"cell_type":"markdown","source":"# 1.3 Vectorize training and test data\nUsing the custom functions to load in raw text and label data and vectorize them based on a set of parameters.\n\nParameters such as max_len, max_words and validation_samples for the max sequence length of each text, total number of words for the tokenizer and finally, the split for validation data.\n\nThe baseline model will use a max_len of 100, max_words of 10,000 and a validation split of 10% of the training data"},{"metadata":{"trusted":true,"_uuid":"2cf0ca35d77c4c0cee0d056d08793d7a9eb98f4a"},"cell_type":"code","source":"# check training data sample size\nprint(len(x_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bff2f9ff419b8dc1349dfb99d3374d2fc9a327cb"},"cell_type":"code","source":"# get the value of 10% of the traning data\nprint(len(x_train) // 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"744086812f56638be9392e059b544c23e7cdf1e2"},"cell_type":"code","source":"validation_samples = int((len(x_train) // 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"477a33052dc97c2b8e398c430c305438280387ed"},"cell_type":"code","source":"validation_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"500b220d49327073075932930778afa4e4b32e37"},"cell_type":"code","source":"# define max sequence length and total dictionary words\nmax_len = 100\nmax_words = 10000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0eee0ce53c4be2d5bed1aa16993e5720e1483f4e"},"cell_type":"code","source":"# Vectorize training data and return tokenizer and word_index as well as validation splits\ntokenizer, word_index, X_train, Y_train, x_val, y_val = tokenize_and_sequence(\n    x_train, y_train, max_len=max_len, max_words=max_words, validation_samples=validation_samples, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aea906196d23efdea4d28ce162f306720f374f14"},"cell_type":"code","source":"# verify train and validation text and labels\nprint('training:',X_train.shape, Y_train.shape, '\\nvalidation:', x_val.shape, y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"240e50b6e915532429d64e809912c1774696ae10"},"cell_type":"markdown","source":"# 1.4 Load pre-trainged GloVe word embeddings\nLoading in the 200D word embeddings to establish a baseline model"},{"metadata":{"trusted":true,"_uuid":"db25a7b2e2e163b8f2538b1569842fea4ac015c3"},"cell_type":"code","source":"# define directory paths for glove embeddings\nglove_dir = '../input/glove6b200d/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aa0e2b073fdbfa919729aca1d4d676026abe3bc"},"cell_type":"code","source":"# glove file name\nglove_file = 'glove.6B.200d.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"148d0b8b4239d5cebba07398e73cad921c3ee366"},"cell_type":"code","source":"# define embedding dimension value to match the glove200d file\nembedding_dim = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ce4434c9f424782788f8652488590ececd53d58","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# load in glove embedding using custom function from earlier\n# function takes as input the raw file, word_index returned from the tokenizer and max_words\nglove_embedding_200d = load_glove(\n    glove_dir, glove_file, max_words=max_words, word_index=word_index, embedding_dim=embedding_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4f646b92c170ef41c4f4413d0309eb6f50a4fcb"},"cell_type":"code","source":"# verify embeddings loaded correctly\nglove_embedding_200d.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8b58c5505b45798e614bbbf41c17197aa901295"},"cell_type":"markdown","source":"# 2. Model Architectures\n"},{"metadata":{"trusted":true,"_uuid":"930bf6e1916c1e7d0327b243da5267f49ab085ff"},"cell_type":"markdown","source":"# 2.1 CNN-GRU Baseline\nDesign a simple model using a 1D convolution layer followed by a GRU layer to establish a benchmark performance score for further improvements.\n\nUsing the pre-trained embeddings with weights frozen to prevent re-training of word vectors during model training.\n\nAs this is a multi-label multi-class problem, loss is calculated using binary crossentropy with an adam optimizer. Further experiments with hyperparameter turning will explore various optimizer peformance on this dataset.\n\nTraining for 5 epochs on batch sizes of 32\n\n## Design\nA sequential model with the following layers:\n- Embedding(dimension=200)\n- Conv1D(64, 3, 'relu') *64 convolutions with a kernel size of 3, can be extended up to 7*\n- MaxPooling1D(4) *standard practive following convolutions*\n- GRU(64, dropout=0.1, recurrent_dropout=0.5) *using a layer dropout of 10% and a recurent unit dropout of 50%, as seen in research to return good performance*\n- Dense(6, activation='sigmoid') *dense classifier layer of six outputs*\n\n"},{"metadata":{"trusted":true,"_uuid":"86e25c20f80b9a28d5ce7295ed4d61c24fd8f5c4"},"cell_type":"code","source":"# import layers\nfrom keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D\nfrom keras.layers import Dropout, Dense, Activation, Flatten,Conv1D, SpatialDropout1D\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bbb7f21228b7b3fb02310589eee50f1ac585413"},"cell_type":"code","source":"# import AUC ROC metrics from sklearn\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"739e90ba0669500138d7d2257064b89e3c6db883"},"cell_type":"code","source":"# define model architecture\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len))\nmodel.add(Conv1D(64, 3, activation='relu'))\nmodel.add(MaxPooling1D(4))\nmodel.add(GRU(64, dropout=0.1, recurrent_dropout=0.5)) # defaults inclide tanh activation\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f12f98e8762ec5553c002a4b8d665cedf65d0882"},"cell_type":"code","source":"# load pre-trained Glove embeddings in the first layer\nmodel.layers[0].set_weights([glove_embedding_200d])\n# freeze embedding layer weights\nmodel.layers[0].trainable = False\n# compile model with adam optimizer\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b626daff1b748f82e8b6821957b38e926934e4b"},"cell_type":"code","source":"# fit model and train on training data and validate on validation samples\n# train for 5 epochs to establish baseline overfitting model\n# saves results to histroy object\nhistory = model.fit(X_train, Y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f9acae0d1e6404f2cc9ec77e4b29093a8111db1"},"cell_type":"code","source":"# save model\nmodel.save('cnn_gru_200d.h5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"068ee1922d8a24ba4f80c6073f17c961f29a4be4"},"cell_type":"markdown","source":"## 2.1.1 Model Results\nPlot model training and validation performance\n"},{"metadata":{"trusted":true,"_uuid":"7d0614389adbf9bd74a5aa4ee7ca3cf17703e332"},"cell_type":"code","source":"# define plotting metrics\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n# plot model training and validation accuracy and loss\nplot_training_and_validation(acc, val_acc, loss, val_loss)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"936032144863ad89402667eec57e80c4c90a4fd5"},"cell_type":"markdown","source":"Let's use the AUC ROC score metrics for a slightly better understanding of model performance.\n\nThe training and validation performance remains consistent but let's review it on the validation data and plot the AUC graph for a better representation of model generalization"},{"metadata":{"trusted":true,"_uuid":"08aaa495e4497ca38580680e24c1e201ecfda90d"},"cell_type":"code","source":"y_hat = model.predict(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eeb8f6a663207f95393ff852b861fdfefb65831c"},"cell_type":"code","source":"# print auc roc score\n\"{:0.2f}\".format(roc_auc_score(y_val, y_hat)*100.0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d310f61c0a1ce015f9e343edf49e03ff1cc6b02"},"cell_type":"markdown","source":"# Conclusion\nWith an auc roc score of a modest 96.19%, we have established a strong baseline model that uses a 1D convolution and a GRU layer. \n\nAs a first experiment to test this type of architecture on a multi-label classification task on natural language data, it shows the effeciveness of using a convolution base with larger (over 100d) pre-trained word embeddings along with a less compute intensive recurrent layer such as the GRU. \n\nWith the leader achieving an accuracy of 98.85%, further enhacements to this model such as dropout, batch normalisation, increased layers and units, as well as hyper-parameter tuning could lead to performance close to or matching those scores.\n\nAs this is my first kernel on Kaggle, I hope my code and notebook is a good introduction to approaching text based classification problems.\n"},{"metadata":{"trusted":true,"_uuid":"e30a70cc11bc6254cf3b03c47cac7426906f75a4"},"cell_type":"markdown","source":"# 2.2 Baseline Improvement"},{"metadata":{"_uuid":"7f7fe828ad2d919a86daa32429b08eecab21f75d"},"cell_type":"markdown","source":"## Experiment with model architecture, model and data complexity \nHaving established a reasonable baseline model using a simple CNN-GRU architecture, we take the learning from our previous experiment and add more complex features as well as modify input data. \n\nThe main improvements we will add are:\n- Larger vectors\n- Shorter sequences\n- Vectorize on both training and test data\n- Callbacks: Early stopping, Reduce LR, ROC per epoch\n- Larger convolution kernels\n- Dropout\n- Batch Normalization\n\nExtended modifications we can test:\n- Larger units\n- More Layers\n- Stacked recurrent layers\n- Bidirectional recurrent layer\n\n"},{"metadata":{"_uuid":"61d35ecfb529f84c4d6b0d4f2f12e79e800fab7d"},"cell_type":"markdown","source":"## 2.2.1 Vectorize input data\nIn this experiment we fit the Tokenizer on both the training and test data, to capture as many relevant words as possible before padding the sequences and extracting their relevant word embeddings.\n\nThe process is identical to Section 1.3"},{"metadata":{"trusted":true,"_uuid":"eaa0f0fdda5755e23efd21af2ae4f027a031f776"},"cell_type":"code","source":"# verify length of training and test data\nprint(len(x_train), len(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9937672b0928d61f68b0610759a10740153b1bb5"},"cell_type":"code","source":"# create validation split of 10%\nvalidation_split = len(x_train) // 10\nprint(validation_split)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4155b8c7003b6d25c1bf67520d3fcc5754e30657"},"cell_type":"markdown","source":"### Define sequence length and max number of words\nHere we reduce the sequence length to 50 as our EDA showed most of our samples are less than 60 sequences in length.\n\nWe also increase the total number of words to 20,000 for the same intended effect, for larger coverage.\n"},{"metadata":{"trusted":true,"_uuid":"acfbd90cc97d048beb5df6b805cd22624a2b8bdc"},"cell_type":"code","source":"# define max_len and max_words\nmax_len = 50\nmax_words = 20000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4654620b91d4738ae874f3a8aea4dc590a0c2957"},"cell_type":"code","source":"full_tokenized = x_train + x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a23a291e75d6d8405dcd0f29f7fc244da8c29ea8"},"cell_type":"code","source":"# repeat vectorization process using our custom function\n# Vectorize training data and return tokenizer and word_index as well as validation splits\n# tokenize on x_train AND x_test using new parameter to capture as many words as possible\ntokenizer, word_index, X_train, Y_train, x_val, y_val = tokenize_and_sequence(full_tokenized,\n    x_train, y_train, max_len=max_len, max_words=max_words, \n    validation_samples=validation_split, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"290eb3f55b91465e488b8a776fe2893315d5694b"},"cell_type":"markdown","source":"## 2.2.2 Load pre-trained GloVe embeddings"},{"metadata":{"trusted":true,"_uuid":"47311f156b15dd5485c47291157e85fad5765990"},"cell_type":"code","source":"# define directory paths for glove embeddings\nglove_dir = '../input/glove6b200d/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f1d4707d4ce5a6696ff38f61d649c4cbdabc3d5"},"cell_type":"code","source":"# glove file name\nglove_file = 'glove.6B.200d.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8efc12348bec259b49cdc93c94f30b4379dccef1"},"cell_type":"code","source":"# define embedding dimension value to match the glove200d file\nembedding_dim = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb4dd7e173a9d5b8b25fbda91ccd3c6b53b589d3"},"cell_type":"code","source":"# load in glove embedding using custom function from earlier\n# function takes as input the raw file, word_index returned from the tokenizer and max_words\nglove_embedding_200d = load_glove(\n    glove_dir, glove_file, max_words=max_words, word_index=word_index, embedding_dim=embedding_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9e73de9618f586517717214be6283ad87bc133f"},"cell_type":"code","source":"# verify embeddings loaded correctly\nglove_embedding_200d.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6cb909cd2356f41b1768abc8048d1ca60314240"},"cell_type":"markdown","source":"## 2.2.3 Baseline++ Model\nWe take the same core design of the baseline model of a CNN-GRU but add features such as Dropout, Batch normalization. \n\nWe optimise the model with callbacks and monitor progress to compare if the modifications in input sequence length and size improves our results.\n"},{"metadata":{"trusted":true,"_uuid":"3d8a5c067b6c53af03e78c22a4100d8c7c8d324e"},"cell_type":"code","source":"# import AUC ROC metrics from sklearn\nfrom sklearn.metrics import roc_auc_score\n\n# define class for ROC AUC callback with simple name modifications\n# credit to https://www.kaggle.com/yekenot\nclass roc_auc_validation(keras.callbacks.Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.x_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.x_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5ddd3db0da60f7ddc22d679ef076dc677360164"},"cell_type":"code","source":"# import keras layers \nfrom keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import Dropout, Dense, Activation, Flatten,Conv1D, Bidirectional, SpatialDropout1D, BatchNormalization\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95fd6e6ae2ad7983a8ecb40a6aca1db36c64cca0"},"cell_type":"code","source":"# define model architecture\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len))\nmodel.add(SpatialDropout1D(0.2)) # add spatial dropout\nmodel.add(Conv1D(64, 3, activation='relu')) # increase kernel size to 5 # change to 3 to test\nmodel.add(MaxPooling1D(4))\nmodel.add(BatchNormalization()) # add batch normalization\nmodel.add(Dropout(0.1))\n# modify to CuDNNGRU\n#model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5)) # defaults inclide tanh activation\nmodel.add(CuDNNGRU(64)) # does not have a dropout or recurrent dropout param\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d35ab1d963526b4881f983eb9809795cccac26d9"},"cell_type":"markdown","source":"# 2.2.3.1 Increasing Baseline++ complexity\nThe performance of our Baseline++ remains constant at around 98% for 20 epochs without overfitting, suggesting there is still room for improvement. \n\nIn this subsection we increase the units of both the CNN and GRU layers and test the model once again. We also change callback parameters to stop early by monitoring val_loss\n\nResults - Early stopping at epoch 7 with accuracy again at 98%.\n\nExperiement 2 \nAdd a second convolution layer and change kernels to 7 , 5 in each\nResults = Early stopping at epoch 5 with accuracy at 97%\n\nExperiement 3\nReturn to ++ model with a Bidirectional GRU layer instead of a single one\nResults = Epoch 18/20\n143614/143614 [==============================] - 21s 144us/step - loss: 0.0403 - acc: 0.9849 - val_loss: 0.0733 - val_acc: 0.9767\n\n ROC-AUC - epoch: 18 - score: 0.949673\nEpoch 19/20\n143614/143614 [==============================] - 21s 143us/step - loss: 0.0396 - acc: 0.9849 - val_loss: 0.0730 - val_acc: 0.9767\n\nExperiment 4 - Test this model using rmsprop optimizer\nResults = highest training accuracy of 98.6% but early stopping at epoch 4, with ROC of 95%\n\nExperiment 5 - Test again using 128 batch size\nResults = Early stopping at epoch 2\nConclusion = adam optimiser is better for this type of problem\n\nExperiment 6 - Add a second convolution layer, 128, 64, same parameters\nResults = more compute, weaker ROC at 93, training at 97\n\nExperiment 7 - Return to baseline++ of 64,64 cnn,gru, reduce filter size to 3 as this is a small sequential data set\nResults = Highest ROC from epoch 1 at 95, and 97 with a peak of 98% in epoch 10. Training accuracy is constant around 98% as is validation accuracy up to 98%. This is the best result I have observed so far. A shorter convolution kernel of 4, with 64 filters and a 64 unit GRU is best suited for this data set, which makes sense considering we are using a sequence size of 50 as the text sequences themselves are relatively short. This also shows that a simpler network architecture is better rather than adding more layers of branches.\n"},{"metadata":{"trusted":true,"_uuid":"dbc00dc541db66c1d28c23a0b86f420abd6a731c","scrolled":true},"cell_type":"code","source":"# Baseline++ with more units\n# this model performs the best with the highest training accuracy of loss: 0.0378 - acc: 0.9857 \n# - val_loss: 0.0763 - val_acc: 0.9759 # ROC-AUC - epoch: 20 - score: 0.951627\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len))\nmodel.add(SpatialDropout1D(0.2)) # add spatial dropout\nmodel.add(Conv1D(128, 5, activation='relu')) # increase kernel size to 5\nmodel.add(MaxPooling1D(4))\nmodel.add(BatchNormalization()) # add batch normalization\nmodel.add(Dropout(0.1))\n\n# modify to CuDNNGRU and double units to 128\n#model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5)) # defaults inclide tanh activation\nmodel.add(Bidirectional(CuDNNGRU(128))) # does not have a dropout or recurrent dropout param\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca90f54f4c935b89a030c61d0dfe3e38628d0c64"},"cell_type":"code","source":"# Baseline++ with 2 CNN layers\n# less performant compared to baseline++\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len))\nmodel.add(SpatialDropout1D(0.2)) # add spatial dropout\n# CNN layers\nmodel.add(Conv1D(256, 5, activation='relu')) # increase kernel size to 5\nmodel.add(MaxPooling1D(4))\nmodel.add(BatchNormalization()) # add batch normalization\nmodel.add(Dropout(0.1))\n# second CNN \nmodel.add(Conv1D(128, 7, activation='relu')) # increase kernel size to 5\nmodel.add(MaxPooling1D(4))\nmodel.add(BatchNormalization()) # add batch normalization\nmodel.add(Dropout(0.1))\n# modify to CuDNNGRU and double units to 128\n#model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5)) # defaults inclide tanh activation\nmodel.add(Bidirectional(CuDNNGRU(64))) # does not have a dropout or recurrent dropout param\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c73b0ca2bff0ffeb805d358b3496a5c25bc84ab5"},"cell_type":"code","source":"# highest performing architecture\n# baseline++ with shorter convolution kernel of 3\n# define model architecture\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len))\nmodel.add(SpatialDropout1D(0.2)) # add spatial dropout\nmodel.add(Conv1D(64, 3, activation='relu')) # increase kernel size to 5 # change to 3 to test\nmodel.add(MaxPooling1D(4))\nmodel.add(BatchNormalization()) # add batch normalization\nmodel.add(Dropout(0.1))\n# modify to CuDNNGRU\n#model.add(GRU(64, dropout=0.1, recurrent_dropout=0.5)) # defaults inclide tanh activation\nmodel.add(CuDNNGRU(64)) # does not have a dropout or recurrent dropout param\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35d5939389b597ec1db584fbfeb38e975f25f27b"},"cell_type":"markdown","source":"# best performing model training results\nTrain on 143614 samples, validate on 15957 samples\nEpoch 1/20\n143614/143614 [==============================] - 10s 72us/step - loss: 0.2060 - acc: 0.9235 - val_loss: 0.0596 - val_acc: 0.9798\n\n ROC-AUC - epoch: 1 - score: 0.959088\nEpoch 2/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0643 - acc: 0.9785 - val_loss: 0.0536 - val_acc: 0.9810\n\n ROC-AUC - epoch: 2 - score: 0.972622\nEpoch 3/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0590 - acc: 0.9795 - val_loss: 0.0524 - val_acc: 0.9814\n\n ROC-AUC - epoch: 3 - score: 0.974925\nEpoch 4/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0554 - acc: 0.9806 - val_loss: 0.0517 - val_acc: 0.9813\n\n ROC-AUC - epoch: 4 - score: 0.976720\nEpoch 5/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0535 - acc: 0.9810 - val_loss: 0.0522 - val_acc: 0.9808\n\n ROC-AUC - epoch: 5 - score: 0.976915\nEpoch 6/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0516 - acc: 0.9815 - val_loss: 0.0506 - val_acc: 0.9817\n\n ROC-AUC - epoch: 6 - score: 0.978048\nEpoch 7/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0504 - acc: 0.9818 - val_loss: 0.0502 - val_acc: 0.9814\n\n ROC-AUC - epoch: 7 - score: 0.979499\nEpoch 8/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0490 - acc: 0.9821 - val_loss: 0.0502 - val_acc: 0.9816\n\n ROC-AUC - epoch: 8 - score: 0.979762\nEpoch 9/20\n143614/143614 [==============================] - 8s 57us/step - loss: 0.0481 - acc: 0.9825 - val_loss: 0.0495 - val_acc: 0.9817\n\n ROC-AUC - epoch: 9 - score: 0.979418\nEpoch 10/20\n143614/143614 [==============================] - 8s 57us/step - loss: 0.0472 - acc: 0.9828 - val_loss: 0.0500 - val_acc: 0.9816\n\n ROC-AUC - epoch: 10 - score: 0.979449\nEpoch 11/20\n143614/143614 [==============================] - 8s 57us/step - loss: 0.0463 - acc: 0.9829 - val_loss: 0.0497 - val_acc: 0.9818\n\n ROC-AUC - epoch: 11 - score: 0.980230\nEpoch 12/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0456 - acc: 0.9831 - val_loss: 0.0504 - val_acc: 0.9813\n\n ROC-AUC - epoch: 12 - score: 0.979920\nEpoch 13/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0451 - acc: 0.9833 - val_loss: 0.0504 - val_acc: 0.9810\n\n ROC-AUC - epoch: 13 - score: 0.980413\nEpoch 14/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0446 - acc: 0.9835 - val_loss: 0.0502 - val_acc: 0.9810\n\n ROC-AUC - epoch: 14 - score: 0.980023\nEpoch 15/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0440 - acc: 0.9836 - val_loss: 0.0504 - val_acc: 0.9813\n\n ROC-AUC - epoch: 15 - score: 0.979886\nEpoch 16/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0435 - acc: 0.9838 - val_loss: 0.0514 - val_acc: 0.9811\n\n ROC-AUC - epoch: 16 - score: 0.979540\nEpoch 17/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0429 - acc: 0.9839 - val_loss: 0.0508 - val_acc: 0.9817\n\n ROC-AUC - epoch: 17 - score: 0.979724\nEpoch 18/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0427 - acc: 0.9839 - val_loss: 0.0509 - val_acc: 0.9815\n\n ROC-AUC - epoch: 18 - score: 0.979505\nEpoch 19/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0423 - acc: 0.9841 - val_loss: 0.0510 - val_acc: 0.9810\n\n ROC-AUC - epoch: 19 - score: 0.979367\nEpoch 20/20\n143614/143614 [==============================] - 8s 56us/step - loss: 0.0420 - acc: 0.9842 - val_loss: 0.0511 - val_acc: 0.9809\n\n ROC-AUC - epoch: 20 - score: 0.979686\n"},{"metadata":{"trusted":true,"_uuid":"c5f49d67feffcfec0dd33ef4546849c5cc36562b"},"cell_type":"code","source":"# define callbacks\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0e98c582052af5945cfdaaaf5825ce20ed9711b"},"cell_type":"code","source":"# initialise customer roc callback\nroc_callback = roc_auc_validation(validation_data=(x_val, y_val), interval=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5ba9f1462710263a2e3501981628e185cba313d"},"cell_type":"code","source":"# define early stopping and reduce lr callbacks\ncallback_list = [keras.callbacks.EarlyStopping(monitor='acc', patience=1),\n                 keras.callbacks.ModelCheckpoint(filepath='baseline_plus_complex.h5', monitor='val_loss',\n                                                 save_best_only=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f509338d21f4fcca1c0655efdbb4c285af220794"},"cell_type":"code","source":"# add roc to callbacks list\ncallback_list.append(roc_callback)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b971a7717b33ba4a619910c18314669345e3144"},"cell_type":"code","source":"callback_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d564f21118b3e875bbf815f9daea7dc490b41a87"},"cell_type":"code","source":"# load pre-trained Glove embeddings in the first layer\nmodel.layers[0].set_weights([glove_embedding_200d])\n# freeze embedding layer weights\nmodel.layers[0].trainable = False\n# compile model with adam optimizer\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56713fe383cd9148818737a31de576da985173f1"},"cell_type":"markdown","source":"Train model with increased batch size and epoch range to let early stopping terminate training"},{"metadata":{"trusted":true,"_uuid":"dec5f6716cf0e83b9e9b64a04be1937c583f8738"},"cell_type":"code","source":"# fit model and train on training data and validate on validation samples\n# train for 5 epochs to establish baseline overfitting model\n# saves results to histroy object\nhistory = model.fit(X_train, Y_train, epochs=20, batch_size=256, callbacks=callback_list,validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e81826306b612ab0f6e7ac816a7543c0e61444dd"},"cell_type":"code","source":"# tokenize and pad test data\nX_test = tokenizer.texts_to_sequences(x_test)\nX_test = pad_sequences(X_test, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"361cedd1380c7e0da517aa8b3924c5f035cd429d"},"cell_type":"code","source":"# verify test data shape\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c2cbc86d578915e0142cc372463bdd7296c5a21"},"cell_type":"code","source":"# test model on submission as reading in test_labels fails\ny_hat = model.predict(X_test, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6e7d226cfb3691b2a069d6e3d8eff309d13c9be"},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"292a2d74d4b94de099d869fa0bec9265e3ffc77e"},"cell_type":"code","source":"submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_hat\nsubmission.to_csv('submission_baseline_plus_best_k3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6b7085fc43899012b11b5fbcd6920b58758df8f"},"cell_type":"markdown","source":"## 2.2.4 Conclusion\nWith a maximum training accuracy of 98.49%, we have reached the peak of the sequential model as it is still essentially a data distillation process. \n\nIn the next experiment we will createa a multi-headed model taking the learnings of our experiments so far.\n\nWhat we have learnt from this notebook is that a simpler model with a CNN and GRU of 64 units each with dropout and batch normalisation returns a test score of 96%, while a more complex model actually suffers by 3% on the test set. For this particular data set we maybe benefit from model ensembling to gain those last couple of accuracy points but it is clear that simpler models capture the features of this dataset better than more complex sequential models. There may also be greater performance to be gained by using larger dimensional word embeddings as well as some very limited data preprocessing such as removing numbers and punctuation.\n\nIn conclusion, this has been a good start to experimenting with multi-label multi-class classification and shows that a simple but well thought out model architecture paired with pre-trained word embeddings can lead to very good results, and sets the foundation for similar text classification problems.\n\nEdit:\n\nHaving run more experiments, the best perfoming network is the baseline++ with a kernel size of 3, achieving the highest ROC accuracy consistently of 97/98% up from 95 in previous tests. Training and validation accuracy also remain constant around 98% which shows that modifying the architecture to be simpler and suit the nature of the input data yields better training and confidence. \n\n"},{"metadata":{"trusted":true,"_uuid":"831de6d3f356bac19954e03b8a04385086b9ffee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}