{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["We'll take the ideas introduced in Rachael Tatman's [Beginner Tutorial: Python](https://www.kaggle.com/rtatman/beginner-s-tutorial-python), scale them up into a complete machine learning pipeline, and add some basic feature engineering.\n", "\n", "First, we need to make quite a few  imports. It's a long list but everything important is a component of either [pandas](http://pandas.pydata.org/pandas-docs/stable/) for data manipulation, [spaCy](https://spacy.io/docs/usage/lightning-tour) for text processing, or [scikit-learn](http://scikit-learn.org/stable/) for machine learning. I'll assume you have general familiarity with both pandas and scikit-learn."], "metadata": {"_cell_guid": "2f1136b3-8764-4b6a-9fde-3c669ede4285", "_uuid": "a3a679cfb7f8295756314d13a14698f9d927d00c"}}, {"outputs": [], "cell_type": "code", "source": ["import pandas as pd\n", "import spacy\n", "\n", "from multiprocessing import cpu_count\n", "from sklearn.base import TransformerMixin\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn import metrics\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.pipeline import Pipeline\n", "from spacy import attrs\n", "from spacy.symbols import VERB, NOUN, ADV, ADJ"], "execution_count": null, "metadata": {"_cell_guid": "b543c40e-d743-4c88-a82a-e626a2f01c17", "collapsed": true, "_uuid": "f00d3262d20e016bbaa6deed03eee0fb431e3035"}}, {"cell_type": "markdown", "source": ["Next we'll declare important constants, per the python style guide, [PEP8](https://www.python.org/dev/peps/pep-0008). This isn't strictly necessary, but makes for cleaner code."], "metadata": {"_cell_guid": "c860b662-61ea-40fc-a0d1-4c9c2d6b11c9", "_uuid": "c3a50f9642caf5d9c503bdd2ad4946af40600e13"}}, {"outputs": [], "cell_type": "code", "source": ["TEXT_COLUMN = 'text'\n", "Y_COLUMN = 'author'"], "execution_count": null, "metadata": {"_cell_guid": "e0f481b7-474e-4b07-9a3f-3f94fc4aeba5", "collapsed": true, "_uuid": "5cd425c1fa1893e7122b744107ff7d214d905532"}}, {"cell_type": "markdown", "source": ["We're going to run a couple of different models with different sets of features, so it's worth taking a moment to set up our model evaluation process as its own function.\n", "\n", "For evaulation, we need to do several things:\n", "1. Split the input dataframe into the a feature dataframe and a label dataframe (X and Y).\n", "- Conduct  feature engineering.\n", "- Train the model.\n", "- Perform cross validation.\n", "- Report the relevant score. In this case, we'll use log loss to match the competition's evaluation.\n", "\n", "Integrating [Scikit-learn pipelines](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) into our evaluation makes this straightforward to repeat with different models and features."], "metadata": {"_cell_guid": "7f905549-1615-4882-b86f-174055d74786", "_uuid": "4aa67efeb67b573e07da69a8016ca69a97d61eda"}}, {"outputs": [], "cell_type": "code", "source": ["def test_pipeline(df, nlp_pipeline, pipeline_name=''):\n", "    y = df[Y_COLUMN].copy()\n", "    X = pd.Series(df[TEXT_COLUMN])\n", "    # If you've done EDA, you may have noticed that the author classes aren't quite balanced.\n", "    # We'll use stratified splits just to be on the safe side.\n", "    rskf = StratifiedKFold(n_splits=5, random_state=1)\n", "    losses = []\n", "    for train_index, test_index in rskf.split(X, y):\n", "        X_train, X_test = X[train_index], X[test_index]\n", "        y_train, y_test = y[train_index], y[test_index]\n", "        nlp_pipeline.fit(X_train, y_train)\n", "        losses.append(metrics.log_loss(y_test, nlp_pipeline.predict_proba(X_test)))\n", "    print(f'{pipeline_name} kfolds log losses: {str([str(round(x, 3)) for x in sorted(losses)])}')\n", "    print(f'{pipeline_name} mean log loss: {round(pd.np.mean(losses), 3)}')"], "execution_count": null, "metadata": {"_cell_guid": "444f8ae6-c5dc-4130-bde5-419914b61d10", "collapsed": true, "_uuid": "1294fb927703857c0eecd4ac17daa1ca72960012"}}, {"cell_type": "markdown", "source": ["We're ready to load the data and run our first model. We'll start with the exact same model,\n", "a naive bayes classifer on unigram probabilities, as in Rachael's tutorial. Using sklearn instead of implementing everything ourselves will make this both easier to code up and faster to run.\n", "\n", "The `Id` column doesn't actually help us (or if it does, isn't really in the spirit of an NLP competition), so we'll skip over it."], "metadata": {"_cell_guid": "e7f8ed86-8731-47fc-97ab-d3552e56f288", "_uuid": "f23757cd9420ea698cb46c2d7dac3af26faa2e36"}}, {"outputs": [], "cell_type": "code", "source": ["train_df = pd.read_csv(\"../input/train.csv\", usecols=[TEXT_COLUMN, Y_COLUMN])"], "execution_count": null, "metadata": {"_cell_guid": "05ad7b6c-155e-4c02-b83d-69a9ca7f6c52", "collapsed": true, "_uuid": "dcd97ba69f69a203f2843e65a749bdc0dad417bf"}}, {"outputs": [], "cell_type": "code", "source": ["unigram_pipe = Pipeline([\n", "    ('cv', CountVectorizer()),\n", "    ('mnb', MultinomialNB())\n", "                        ])\n", "test_pipeline(train_df, unigram_pipe, \"Unigrams only\")"], "execution_count": null, "metadata": {"_cell_guid": "a7422664-b524-44cb-92d5-9811370a9131", "_uuid": "300ee98403c0a8c1f0846751365b93f5daf77564"}}, {"cell_type": "markdown", "source": ["Since we want to turn this into a nice clean pipeline, we'll do all of our feature engineering using custom transformers.  This first transformer takes the unigram pipeline that we built above and returns the predicted probabilities as features. We could use the raw CountVectorizer output and let our final model deal with the unigram features directly, but that would create two issues:\n", "\n", "- CountVectorizer returns [a sparse format](https://docs.scipy.org/doc/scipy-0.19.1/reference/generated/scipy.sparse.csr_matrix.html) that is a pain to integrate with the rest of our pipeline. \n", "- Using CountVectorizer and MultinomialNB allows us to skip converting the word counts to probabilities, and to skip ensuring that probabilities are never exactly zero. See the `alpha` parameter in the [MultinomialNB documentation?](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) As long as we use the default input of one, the model will peform this task (a [Laplace transform](https://en.wikipedia.org/wiki/Additive_smoothing)) for us."], "metadata": {"_cell_guid": "a853aa5a-9551-495e-b126-9e0bd3aff33c", "_uuid": "1c3219753e00229e3bbfca2f629f868db40c4088"}}, {"outputs": [], "cell_type": "code", "source": ["class UnigramPredictions(TransformerMixin):\n", "    def __init__(self):\n", "        self.unigram_mnb = Pipeline([('text', CountVectorizer()), ('mnb', MultinomialNB())])\n", "\n", "    def fit(self, x, y=None):\n", "        # Every custom transformer requires a fit method. In this case, we want to train\n", "        # the naive bayes model.\n", "        self.unigram_mnb.fit(x, y)\n", "        return self\n", "    \n", "    def add_unigram_predictions(self, text_series):\n", "        # Resetting the index ensures the indexes equal the row numbers.\n", "        # This guarantees nothing will be misaligned when we merge the dataframes further down.\n", "        df = pd.DataFrame(text_series.reset_index(drop=True))\n", "        # Make unigram predicted probabilities and label them with the prediction class, aka \n", "        # the author.\n", "        unigram_predictions = pd.DataFrame(\n", "            self.unigram_mnb.predict_proba(text_series),\n", "            columns=['naive_bayes_pred_' + x for x in self.unigram_mnb.classes_]\n", "                                           )\n", "        # We only need 2 out of 3 columns, as the last is always one minus the \n", "        # sum of the other two. In some cases, that colinearity can actually be problematic.\n", "        del unigram_predictions[unigram_predictions.columns[0]]\n", "        df = df.merge(unigram_predictions, left_index=True, right_index=True)\n", "        return df\n", "\n", "    def transform(self, text_series):\n", "        # Every custom transformer also requires a transform method. This time we just want to \n", "        # provide the unigram predictions.\n", "        return self.add_unigram_predictions(text_series)"], "execution_count": null, "metadata": {"_cell_guid": "19460f9f-b192-444d-a146-048f47d79830", "collapsed": true, "_uuid": "ae5c7b391174692dcb9e59f38bb5f01075cebdbe"}}, {"cell_type": "markdown", "source": ["It's time to start adding new features with spaCy. We'll flag the main parts of speech used in each sentence, average word length, and overall sentence length.\n", "\n", "The single slowest step of working with spaCy is often loading the model in the first place, so we'll ensure this step is only done once. By default, spaCy will tag each word, build a dependency model, and perform entity recognition. We only need the part of speech tags, so we'll restrict the pipeline accordingly. In tests on my local machine, this sped up the parse by 5-10x."], "metadata": {"_cell_guid": "a3380ef9-e9fc-4eed-b32a-2a3f7b7ba5fe", "_uuid": "dd7e54289fc8cf448abc2fbfc2e892394ea25335"}}, {"outputs": [], "cell_type": "code", "source": ["NLP = spacy.load('en', disable=['parser', 'ner'])"], "execution_count": null, "metadata": {"_cell_guid": "475f7f11-b6f9-47de-b3c8-52391e7951eb", "collapsed": true, "_uuid": "aa737f257555e300edf7dc77371d0a5860c6e849"}}, {"outputs": [], "cell_type": "code", "source": ["class PartOfSpeechFeatures(TransformerMixin):\n", "    def __init__(self):\n", "        self.NLP = NLP\n", "        # Store the number of cpus available for when we do multithreading later on\n", "        self.num_cores = cpu_count()\n", "\n", "    def part_of_speechiness(self, pos_counts, part_of_speech):\n", "        if eval(part_of_speech) in pos_counts:\n", "            return pos_counts[eval(part_of_speech).numerator]\n", "        return 0\n", "\n", "    def add_pos_features(self, df):\n", "        text_series = df[TEXT_COLUMN]\n", "        \"\"\"\n", "        Parse each sentence with part of speech tags. \n", "        Using spaCy's pipe method gives us multi-threading 'for free'. \n", "        This is important as this is by far the single slowest step in the pipeline.\n", "        If you want to test this for yourself, you can use:\n", "            from time import time \n", "            start_time = time()\n", "            (some code)\n", "            print(f'Code took {time() - start_time} seconds')\n", "        For faster functions the timeit module would be standard... but that's\n", "        meant for situations where you can wait for the function to be called 1,000 times.\n", "        \"\"\"\n", "        df['doc'] = [i for i in self.NLP.pipe(text_series.values, n_threads=self.num_cores)]\n", "        df['pos_counts'] = df['doc'].apply(lambda x: x.count_by(attrs.POS))\n", "        # We get a very minor speed boost here by using pandas built in string methods\n", "        # instead of df['doc'].apply(len). String processing is generally slow in python,\n", "        # use the pandas string methods directly where possible.\n", "        df['sentence_length'] = df['doc'].str.len()\n", "        # This next step generates the fraction of each sentence that is composed of a \n", "        # specific part of speech.\n", "        # There's admittedly some voodoo in this step. Math can be more highly optimized in python\n", "        # than string processing, so spaCy really stores the parts of speech as numbers. If you\n", "        # try >>> VERB in the console you'll get 98 as the result.\n", "        # The monkey business with eval() here allows us to generate several named columns\n", "        # without specifying in advance that {'VERB': 98}.\n", "        for part_of_speech in ['NOUN', 'VERB', 'ADJ', 'ADV']:\n", "            df[f'{part_of_speech.lower()}iness'] = df['pos_counts'].apply(\n", "                lambda x: self.part_of_speechiness(x, part_of_speech))\n", "            df[f'{part_of_speech.lower()}iness'] /= df['sentence_length']\n", "        df['avg_word_length'] = (df['doc'].apply(\n", "            lambda x: sum([len(word) for word in x])) / df['sentence_length'])\n", "        return df\n", "\n", "    def fit(self, x, y=None):\n", "        # since this transformer doesn't train a model, we don't actually need to do anything here.\n", "        return self\n", "\n", "    def transform(self, df):\n", "        return self.add_pos_features(df.copy())"], "execution_count": null, "metadata": {"_cell_guid": "5e846a49-23c6-4ae5-9f3a-d3ef5014c5f0", "collapsed": true, "_uuid": "fcda4ffa05c25b75aeb1969852fcf77972745d6b"}}, {"cell_type": "markdown", "source": ["Finally, sklearn models generally don't accept strings as inputs, so we'll need to drop all string columns. This includes the original\n", "'text' column that we read from the csv!"], "metadata": {"_cell_guid": "94441886-5b06-4f88-809f-4806cd2157d4", "_uuid": "0c43b46ab6714df1c7d3e30ed5781cec759a66f6"}}, {"outputs": [], "cell_type": "code", "source": ["class DropStringColumns(TransformerMixin):\n", "    # You may have noticed something odd about this class: there's no __init__!\n", "    # It's actually inherited from TransformerMixin, so it doesn't need to be declared again.\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, df):\n", "        for col, dtype in zip(df.columns, df.dtypes):\n", "            if dtype == object:\n", "                del df[col]\n", "        return df"], "execution_count": null, "metadata": {"_cell_guid": "2e0bbfb7-8fb3-495a-8a9c-c8bc1f2d4747", "collapsed": true, "_uuid": "597da86e08a5e6f8fe8e2067c1bbc53e1274f4c0"}}, {"cell_type": "markdown", "source": ["If you want to experiment with different combinations of features, try writing your own transformers and adding them to the pipeline.\n", "\n", "If you're running this at home, expect this next step to take ~30 seconds or so as we're retraining the model several times during the cross validation."], "metadata": {"_cell_guid": "1a9519fa-9a5e-4ca6-95f5-b154cabdd2d6", "_uuid": "68ad04eafddc62f0d99cd77a21856671a829d295"}}, {"outputs": [], "cell_type": "code", "source": ["logit_all_features_pipe = Pipeline([\n", "        ('uni', UnigramPredictions()),\n", "        ('nlp', PartOfSpeechFeatures()),\n", "        ('clean', DropStringColumns()), \n", "        ('clf', LogisticRegression())\n", "                                     ])\n", "test_pipeline(train_df, logit_all_features_pipe)"], "execution_count": null, "metadata": {"_cell_guid": "df01bca5-dd70-4fb7-860f-f265bc995b89", "_uuid": "05dd8488d587ef4d63f6b17dae7841bd5691a2f8"}}, {"cell_type": "markdown", "source": ["This pipeline is better... but only just barely. I'll leave it as an exercise for you to add better features and more powerful models. However, if we did want to submit this, we'd just feed `logit_all_features_pipe` into the `generate_submission_df` function."], "metadata": {"_cell_guid": "755e453b-2898-4ada-b39b-8110a8636f02", "_uuid": "c7696b34bcd633ac41bf323f6edd0e14c6a65a57"}}, {"outputs": [], "cell_type": "code", "source": ["def generate_submission_df(trained_prediction_pipeline, test_df):\n", "    predictions = pd.DataFrame(\n", "        trained_prediction_pipeline.predict_proba(test_df.text),\n", "        columns=trained_prediction_pipeline.classes_\n", "                               )\n", "    predictions['id'] = test_df['id']\n", "    predictions.to_csv(\"submission.csv\", index=False)\n", "    return predictions"], "execution_count": null, "metadata": {"_cell_guid": "f290f858-c6bd-4c52-bd0b-0a7c66102837", "collapsed": true, "_uuid": "07ed25cc4f970b4b74b4cc8083923621086871ec"}}, {"cell_type": "markdown", "source": ["Exercises:\n", "1. Update the `PartOfSpeechFeatures` transformer to record all parts of speech, not just the original four.\n", "- Can you generate a useful feature with spaCy's dependency parser? Fair warning,I haven't tried yet so the answer may well be no!\n", "- More challenging: Kevin Schiroo figured out that [sentences for MWS are missing exclamation marks](https://www.kaggle.com/c/spooky-author-identification/discussion/42135). A simple regex based on capital letters like `re.sub(r'\\b (?=[A-Z])', '! ', sentence)` would insert too many exclamation points by treating names as ends of sentences. Can you use spaCy's entity recognition model to clean those up?"], "metadata": {"_cell_guid": "84d7b027-8de2-4dad-8984-bcd065f059a6", "_uuid": "72431b2964e9d01439061870fe640b20a0bac34d"}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"_cell_guid": "d3a3a143-92bc-439d-af61-864115cbb503", "collapsed": true, "_uuid": "a23266878433d247035fc6105143888154d7d115"}}], "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python"}}}