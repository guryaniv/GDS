{"cells":[{"metadata":{"_uuid":"eddf927a8278f5dfac97bf5974292e6449adcda4"},"cell_type":"markdown","source":"This is a relatively simple model submission using the same UNet neural network provided in the [\"UNet with Depth\"](https://www.kaggle.com/bguberfain/unet-with-depth) kernel.\n\n## Preprocessing\n\nThe cells that follow define a sequence of image transformations, most of them taken from [an excellent post by Heng](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63974).\n\n* `ImageDatasetBuilder` &mdash; transforms the image reference matrix into a featurized dataset.\n* `CropBoth` &mdash; Crops one of (in the test case) or both of (in the train case) inputted `X` and `y` feature matrices. Cropping is useful for two different reasons. First of all, reducing some of the area of the image can reduce the vulnerability of the model to artifacts on the edge of the image, something that has been noted to be an issue in other kernels in this competition. Second, cropping an image allows us to create multiple images (and therefore, multiple records to train on) out of a single distinct one. Of course, this only works if the amount of information lost by reducing the size of the images is less than the amount of regularization gained by increasing the size of the training set. In this example kernel I omitted cropping more than once.\n* `ScaleBoth` &mdash; Scales the image. UNet works on images that are multiples of 36.\n* `RefractBoth` &mdash; Symmetrically pads the image, reflecting the edges outwards. This has been shown to improve model accuracy with sufficiently advanced models (it's a bit of a stretch to say it helps much here, but whatever) because it helps with artifacts on the edges of the image.\n* `FeaturizeDepth` creates a feature vector for depth, taken from the `depths.csv` file provided by the competition.\n* `Melt` reshapes the output into a four-dimensional record-number, x-value, y-value, channel matrix, as required by the `Conv2D` layer in `keras`.\n\nAll of our transforms follow the standard `sklearn` transform pattern."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom matplotlib.image import imread\n\nclass ImageDatasetBuilder:\n    \"\"\"\n    Given a DataFrame whose index is a set of image IDs (as with {train, test}.csv), returns featurized images.\n    \"\"\"\n    def __init__(self, x_dim=100, y_dim=100, source='../data/train/images/', mask=False):\n        \"\"\"\n        Builds the featurized image transform.\n        \n        x_dim: int\n            The X dimension to crop the images to.\n        y_dim: int\n            The Y dimension to crop the images to.\n        source: str\n            Path to the folder containing the image files.\n        mask: booleon\n            If true, the underlying data is a mask. If false, the underlying data is RGB. If the data is RGB,\n            we take just the R component and skip the GB, because the images are grayscale anyway.\n        \"\"\"\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n        self.source = source\n        self.mask = mask\n        \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        r = np.asarray(\n            list(\n                map(lambda img_id: np.ravel(\n                    imread(f'{self.source}/{img_id}.png')[:self.x_dim,:self.y_dim]\n                ), X.index.values)\n            )\n        )\n        return r if self.mask else r[:,::3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f313979be99f149221c65ae83545a060c50cc0af"},"cell_type":"code","source":"class CropBoth:\n    \"\"\"\n    Crops featurized images.\n    \"\"\"\n    \n    def __init__(self, ratio=0.9, cardinality=1):\n        \"\"\"\n        ratio: float\n            The percentage of the image to retain, expressed as a float ratio. Images are cropped in squares.\n        cardinality: int\n            The number of images to output. Each image will be a random crop of the original.\n            \n            If more than 1, this will result in multiplicatively more training cases created from the original \n            images.\n        \"\"\"\n        self.ratio, self.cardinality = ratio, cardinality\n        \n    def fit(self, X, y=None):\n        import math\n        \n        self.n_samples, self.n_pixels = X.shape[0], X.shape[1]\n        self.img_dimension = int(self.n_pixels**(1/2))\n        self.max_offset = math.floor((1 - self.ratio) * self.img_dimension)\n        return self\n    \n    def transform(self, X, y=None):\n        out_X = []\n        out_y = []\n\n        selection_size = int(self.ratio * self.img_dimension)\n        \n        for n in range(self.n_samples):\n            for _ in range(self.cardinality):\n                x_offset = np.random.randint(1, self.max_offset)\n                y_offset = np.random.randint(1, self.max_offset)\n\n                img_data = X[n]\n                selection = np.ravel(img_data.reshape([self.img_dimension]*2)[x_offset:x_offset + selection_size,\n                                                                              y_offset:y_offset + selection_size])\n                out_X.append(selection)\n                \n                if y is not None:\n                    img_data = y[n]\n                    selection = np.ravel(img_data.reshape([self.img_dimension]*2)[x_offset:x_offset + selection_size,\n                                                                                  y_offset:y_offset + selection_size])\n                    out_y.append(selection)\n        \n        if y is not None:\n            return np.asarray(out_X), np.asarray(out_y)\n        else:\n            return np.asarray(out_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8d75debbf38a69b8c848797a64f0c2b22b70cb3"},"cell_type":"code","source":"class ScaleBoth:\n    \"\"\"\n    Scales featurized images.\n    \"\"\"\n    \n    def __init__(self, shape=(128, 128)):\n        self.shape = shape\n        \n    def fit(self, X, y=None):\n        self.img_dimension = int(X.shape[1]**(1/2))\n        return self\n    \n    def transform(self, X, y=None):\n        from skimage.transform import rescale\n        \n        in_shape = [self.img_dimension]*2\n        dim_mult = np.asarray(self.shape) / self.img_dimension\n        \n        X_out = np.asarray([np.ravel(rescale(x.reshape(in_shape), dim_mult)) for x in X])\n        if y is not None:\n            y_out = np.round(np.asarray([np.ravel(rescale(_y.reshape(in_shape), dim_mult)) for _y in y]))\n        \n        if y is not None:\n            return X_out, y_out\n        else:\n            return X_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04dce5666d649c421c61fec67721b4d59843c416"},"cell_type":"code","source":"class RefractBoth:\n    \"\"\"\n    Refracts the edges of a featurized image.\n    \"\"\"\n    \n    def __init__(self, ratio=0.95):\n        self.ratio = ratio\n        \n    def fit(self, X, y=None):\n        self.img_dimension = int(X.shape[1]**(1/2))\n        return self\n    \n    def transform(self, X, y=None):\n        in_shape  = [self.img_dimension] * 2\n        out_shape = [int(self.img_dimension * (1 + (1 - self.ratio) * 2))] * 2\n        \n        # Code from: https://stackoverflow.com/a/52472602/1993206\n        def transformOne(x):\n            x = x.reshape(in_shape)\n            outy = outx = out_shape[0]\n            \n            iny, inx, *_ = x.shape\n            iny -= 1; inx -= 1\n            yoffs, xoffs = (outy - iny) // 2, (outx - inx) // 2\n\n            Y, X = np.ogrid[:outy, :outx]\n            out = x[np.abs((Y - yoffs + iny) % (2*iny) - iny), np.abs((X - xoffs + inx) % (2*inx) - inx)]\n            return out\n        \n        X_out = np.asarray([np.ravel(transformOne(x)) for x in X])\n        if y is not None:\n            y_out = np.asarray([np.ravel(transformOne(_y)) for _y in y])\n        \n        if y is not None:\n            return X_out, y_out\n        else:\n            return X_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60b43c11b6a2e20768d921134483f5fcc8047e42"},"cell_type":"code","source":"class FeaturizeDepth:\n    \"\"\"\n    Extracts the depth feature (from depths.csv), which is later passed to the model.\n    \"\"\"\n    \n    def __init__(self):\n        pass\n    \n    def transform(self, X):\n        X_out = X.loc[:, 'z'].values[:, np.newaxis]\n        from sklearn.preprocessing import StandardScaler\n        return StandardScaler().fit(X_out.astype(float)).transform(X_out.astype(float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fa647c9037dfb0792446dd0b932f515096326a9"},"cell_type":"code","source":"class Melt:\n    \"\"\"\n    Transforms a flattened image matrix back into an image grid representation.\n    \n    This is necessary because the Conv2D layer in keras expects input in the form of a list of images with a certain number of \n    channels (RGB is three, this data is grayscale and thus only one). That is, it wants a four-dimensional matrix, whose axes \n    are record, x-value, y-value, channel. This transform mutates our data representation to match this format.\n    \"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X):        \n        self.img_dimension = int(X.shape[1]**(1/2))\n        return self\n\n    def transform(self, X):\n        return X.reshape((X.shape[0], self.img_dimension, self.img_dimension))[:,:,:,np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1443cf496c6e2706633379c8d2a3a57fd5645e3"},"cell_type":"markdown","source":"Now that we've defined all this machinery, here we apply it."},{"metadata":{"trusted":true,"_uuid":"53a8c1c2aaebfa22934fbf5226776546274ab033"},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n_depths = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain = train.join(_depths)\ntest = _depths[~_depths.index.isin(train.index)]\ndel _depths\n\n\nX = ImageDatasetBuilder(source='../input/train/images/').transform(train)\ny = ImageDatasetBuilder(source='../input/train/masks/', mask=True).transform(train)\nX_trans, y_trans = CropBoth(ratio=0.9, cardinality=1).fit(X, y).transform(X, y)\nX_trans, y_trans = RefractBoth(ratio=0.9).fit(X_trans, y_trans).transform(X_trans, y_trans)\nX_trans, y_trans = ScaleBoth(shape=(128, 128)).fit(X_trans, y_trans).transform(X_trans, y_trans)\n\n\nX_trans = Melt().fit(X_trans).transform(X_trans)\ny_trans = Melt().fit(y_trans).transform(y_trans)\n\n\nX_feat = FeaturizeDepth().transform(train)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, X_feat_train, X_feat_test, y_train, y_test = train_test_split(X_trans, X_feat, y_trans, \n                                                                               test_size=0.15, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf0e525d391e6d9238d65480d5bada6e194928c9"},"cell_type":"markdown","source":"## Modeling\n\nNow we define the neural network. This is a UNet, as noted previous taken from the [\"UNet with depth\"](https://www.kaggle.com/bguberfain/unet-with-depth) kernel."},{"metadata":{"trusted":true,"_uuid":"d4128368976b0d1122c7d36740163b0c0b1aa50c","scrolled":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers.core import Lambda, RepeatVector, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\n\ninput_img = Input((X_trans.shape[1], X_trans.shape[2], 1), name='img')\ninput_features = Input((1,), name='feat')\n\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same', name='conv2d_1') (input_img)\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same', name='conv2d_2') (c1)\np1 = MaxPooling2D((2, 2), name='max_pooling2d_1') (c1)\n\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same', name='conv2d_3') (p1)\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same', name='conv2d_4') (c2)\np2 = MaxPooling2D((2, 2), name='max_pooling2d_2') (c2)\n\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv2d_5') (p2)\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv2d_6') (c3)\np3 = MaxPooling2D((2, 2), name='max_pooling2d_3') (c3)\n\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2d_7') (p3)\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2d_8') (c4)\np4 = MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_4') (c4)\n\n# Join features information in the depthest layer\nf_repeat = RepeatVector(8*8, name='repeat_vector_1')(input_features)\nf_conv = Reshape((8, 8, 1), name='reshape_1')(f_repeat)\np4_feat = concatenate([p4, f_conv], -1, name='concatenate_1')\n\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2d_9') (p4_feat)\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2d_10') (c5)\n\nu6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', name='conv2d_transpose_1') (c5)\nu6 = concatenate([u6, c4], name='concatenate_2')\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2d_11') (u6)\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2d_12') (c6)\n\nu7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same', name='conv2d_transpose_2') (c6)\nu7 = concatenate([u7, c3], name='concatenate_3')\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv2d_13') (u7)\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv2d_14') (c7)\n\nu8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same', name='conv2d_transpose_3') (c7)\nu8 = concatenate([u8, c2], name='concatenate_4')\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same', name='conv2d_15') (u8)\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same', name='conv2d_16') (c8)\n\nu9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same', name='conv2d_transpose_4') (c8)\nu9 = concatenate([u9, c1], axis=3, name='concatenate_5')\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same', name='conv2d_17') (u9)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same', name='conv2d_18') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid', name='conv2d_out') (c9)\n\nclf = Model(inputs=[input_img, input_features], outputs=[outputs])\nclf.compile(optimizer='adam', loss='binary_crossentropy')\nclf.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a3267a11cd6e31b08c5424cee19d27378e1dd60"},"cell_type":"markdown","source":"That was our model description. Now we need to train the model. Note the collection of `keras` callbacks we use here to modify the learning behavior:\n\n* `EarlyStopping` &mdash; neural networks that are left \"on\" for too long will tend to overfit the data, reducing training loss but raising validation loss. This callback will stop and back the model off if it finds that, after an epoch is done, the model performance doesn't improve by enough.\n* `ReduceLROnPlateau` &mdash; neural networks can get stuck at local plateaus in the cost surface they are optimizing. This can happen if the model learning rate is too large to escape the plateau. This callback reduces the learning rate when the model doesn't look like it's going anywhere, in order to hopefully escape local minima.\n* `ModelCheckpoint` &mdash; this callback saves the model to an `h5` file at the end of each epoch.\n\nNote that all of these callbacks were inherited from the prior kernel, [\"UNet with depth\"](https://www.kaggle.com/bguberfain/unet-with-depth)."},{"metadata":{"trusted":true,"_uuid":"3ebca9ac1d858af7dd6908b633e0ce01f57bdcfe"},"cell_type":"code","source":"callbacks = [\n    EarlyStopping(patience=5, verbose=1),\n    ReduceLROnPlateau(patience=3, verbose=1),\n    ModelCheckpoint('model-tgs-salt-1.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]\n\nresults = clf.fit({'img': X_train, 'feat': X_feat_train}, y_train, batch_size=32, epochs=5,\n                   callbacks=callbacks,\n                   validation_data=({'img': X_test, 'feat': X_feat_test}, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5c530208c87debf26167ecda334e58416609196"},"cell_type":"markdown","source":"## Evaluation\n\nLet's take a quick look at what we got."},{"metadata":{"trusted":true,"_uuid":"0865111612cdefa1d0fbf23955d752117767a75c"},"cell_type":"code","source":"pd.DataFrame(results.history).loc[:, ['val_loss', 'loss']].plot.line(figsize=(12, 6), fontsize=16, linewidth=5)\nimport seaborn as sns; sns.despine()\nimport matplotlib.pyplot as plt\nplt.title(\"Validation and Training Loss Per Epoch\", fontsize=24)\nplt.ylabel(\"Epoch\", fontsize=18)\nplt.xlabel(\"Loss (Binary Cross-Entropy)\", fontsize=18)\npass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7db5d3c27b44eaf20e370f45f59dfce6354df72c"},"cell_type":"code","source":"np.random.seed(42)\nr = np.random.randint(X_train.shape[0], size=10)\nX_check = X_train[r]\ny_check = y_train[r]\nX_feat_check = X_feat[r]\ny_predict = clf.predict({'img': X_check, 'feat': X_feat_check})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6688ae293d076aa51a43c8487c341467c97a2616"},"cell_type":"markdown","source":"In the following plot the first element is the raw image, the second element is the ground truth, the third element is the prediction, and the fourth image is the difference between the prediction and the ground truth: green where we added false positives, and red where we added false negatives. I'm using this as a quick sense check the the model is indeed learning useful things about the dataset.\n\nNotice here that we're using `np.round` on the output. By default, the model will output what is roughly interpretable as confidence scores for each pixel in the image, but the competition submission format expects us to provide prediction classes, not prediction confidences. So we round our values to 0 or 1."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"387eb0df7b6e6f20f481a6d493400d4153691a5a"},"cell_type":"code","source":"fig, axarr = plt.subplots(4, r.shape[0], figsize=(24, 9))\n\nfor n in range(r.shape[0]):\n    axarr[0][n].imshow(X_check[n][:,:,0], cmap='gray')\n    axarr[1][n].imshow(y_check[n][:,:,0], cmap='gray')\n    axarr[2][n].imshow(np.round(y_predict[n][:,:,0]), cmap='gray')\n    axarr[3][n].imshow(np.round(y_predict[n][:,:,0]) - y_check[n][:,:,0], cmap='RdYlGn', vmin=-1, vmax=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55f20556041865b0714c6336fbf25dade9771a6b"},"cell_type":"markdown","source":"## Prediction\n\nLet's go ahead and predict the test values that we'll submit."},{"metadata":{"trusted":true,"_uuid":"8fcd6c4398cbbeac975ffe269b2592d1e4e6c6ea"},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n_depths = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain = train.join(_depths)\ntest = _depths[~_depths.index.isin(train.index)]\ndel _depths\n\n\nX_out = ImageDatasetBuilder(source='../input/test/images/').transform(test)\nX_out_trans = ScaleBoth(shape=(128, 128)).fit(X_out).transform(X_out)\nX_out_trans = Melt().fit(X_out_trans).transform(X_out_trans)\n\nX_out_feat = FeaturizeDepth().transform(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"384c1141ee1090c9b6508aecd5600e0260a41858"},"cell_type":"markdown","source":"I clean up a bunch of no longer necessary leftover variables to free up RAM. The training data is reasonably large, and if we don't get rid of a few things it may overflow RAM and crash the kernel."},{"metadata":{"trusted":true,"_uuid":"1101dcebaee7289df5bb1307dce30d7eec04c359"},"cell_type":"code","source":"# Free up RAM. Use %whos to see memory utilization.\ndel train\ndel test\ndel X_out\ndel X_trans\ndel y_trans\ndel X\ndel y\ndel X_test\ndel y_test\ndel X_train\ndel y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ddefbe9ef783fe20767a445272f53f497ac1d5c0"},"cell_type":"code","source":"%time y_out_pred = np.round(clf.predict({'img': X_out_trans, 'feat': X_out_feat}))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72e635832807c7b16c343bd57a4cb6616fdc1e9c"},"cell_type":"markdown","source":"The competition expects us to submit results in a run-length encoded format, but we generate our results as matrices of prediction values. To make the output legal for submission to the competition, we have to perform this encoding. The follow algorithm, `RLenc`, came from the early kernel [\"How to geophysics kernel\"](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics). Given an input image matrix, it outputs the encoded string."},{"metadata":{"trusted":true,"_uuid":"d5db2ae9d7e2e31b207691c5903854cb10ea9437"},"cell_type":"code","source":"def RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    _bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in _bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31a30911fd94b1e23869e560bf3d99ace1cbc134"},"cell_type":"markdown","source":"## Submission\n\nFinally we'll build the submission and save it to disc.\n\nNote that the model expects input in the form of, and predicts outputs shaped like, arrays whose dimensions are multiples of 32. But the competition images are 100 by 100. So we have to scale the images up, score them with the model, then scale them back down (using our `ScaleBoth` transform from earlier)."},{"metadata":{"trusted":true,"_uuid":"de60048b85d7f073d0d200dfd3ddf191065d7f79"},"cell_type":"code","source":"y_out_pred = y_out_pred[:,:,:,0].reshape((y_out_pred.shape[0], 128**2))\ny_out_pred = ScaleBoth(shape=(100, 100)).fit(y_out_pred).transform(y_out_pred)\ny_out_pred = y_out_pred.reshape((y_out_pred.shape[0], 100, 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4d2d2b89b3ef29161fa317a5c5a4849cb7a3a98"},"cell_type":"code","source":"results = []\nfrom tqdm import tqdm_notebook\nfor subarr in tqdm_notebook(y_out_pred):\n    img = subarr\n    result = RLenc(np.round(subarr))\n    results.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"485d06dd3541322963f888cc18491e1235c02e76"},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n_depths = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain = train.join(_depths)\ntest = _depths[~_depths.index.isin(train.index)]\ndel _depths\ndel train\n\nout = pd.DataFrame(results)\nout = out.assign(id=test.index).rename(columns={0: 'rle_mask'})[['id', 'rle_mask']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1826c01aedcf14d22c162f6a0595b821b5ad599a"},"cell_type":"markdown","source":"Here's what our output looks like. Note the use of a run-length encoded string in the output column."},{"metadata":{"trusted":true,"_uuid":"ed52808c902987eeff552c63ab585e348e2d4e0f"},"cell_type":"code","source":"out.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eea4e7b2ba768d198c401094be1584cb6cb66f4d"},"cell_type":"code","source":"out.set_index(\"id\").to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0ee1ec5c9a783a64738f85c9460555d726f3b67"},"cell_type":"markdown","source":"To submit this model prediction to the competition, navigate to the Data tab on the kernel (after you've finished running it) and click on the \"Submit to competition\" button.\n\n## Further improvements\n\nThis is a very unoptimized kernel. It implements and runs a handful of useful transformations, uses a pretty standard model, and then immediately submits the result. I don't claim this is a good model, but I do think it's a decent starting point. =)\n\nWe can improve all three of the steps: preprocessing, modeling, and postprocessing. Here are some ideas (this is not original research; these were taken mostly from the forums):\n\n* Preprocessing\n  * Experiment with the preprocessing steps to determine e.g. how much to crop the image, how much refraction to use, how many crops to use for training, etctera.\n  * Experiment with other image transforms. In particular, [time test augmentation](https://towardsdatascience.com/augmentation-for-image-classification-24ffcbc38833) is a technique which it has been pointed out results in big improvements in the model accuracy.\n* Modeling\n  * Try building a correction model which determines whether there is *any* salt in the image or not. Only bother feeding images with salt to the prediction nueral model.\n  * Try preprocessing the data using k-means to count the number of clusters of salt, and embed that data as a feature into the model.\n  * Try using a different error metric. Hinge loss has been mentioned in the discussions as a good one.\n  * Try optimizing the hyperparameters, like the batch size, optimizer algorithm, and especially the number of epochs.\n  * Try a more sophisticated model.\n* Postprocessing\n  * Try treating images which just a little bit of predicted salt differently.\n  * Try using an [IoU](https://www.kaggle.com/aglotero/another-iou-metric) metric to mask pixel confidence predictions more optimally, instead of just taking pixels with >50% confidence, as we do here.\n  \n  Good luck!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}