{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c21e08ac-8968-e25e-8514-420043feca4e"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "import scipy\n",
        "import xgboost as xgb\n",
        "import difflib\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.metrics import jaccard_distance\n",
        "\n",
        "\n",
        "#Reading and processing of data\n",
        "train=pd.read_csv('../input/train.csv')[:10000].fillna(\"\")\n",
        "#train=pd.read_csv('../input/train.csv').dropna()\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "y=train['is_duplicate']\n",
        "train=train.drop(['id', 'qid1', 'is_duplicate','qid2'], axis=1)\n",
        "\n",
        "#Cleaning up the data\n",
        "#Removing ? mark and non ASCII characters\n",
        "def cleanup(data):\n",
        "    data['question1'] = data['question1'].apply(lambda x: x.rstrip('?'))\n",
        "    data['question2'] = data['question2'].apply(lambda x: x.rstrip('?'))\n",
        "    # Removing non ASCII chars\n",
        "    data['question1']=data['question1'].apply(lambda x: x.replace(r'[^\\x00-\\x7f]',r' '))\n",
        "    data['question2']=data['question2'].apply(lambda x: x.replace(r'[^\\x00-\\x7f]',r' ')) \n",
        "    # Pad punctuation with spaces on both sides\n",
        "    '''\n",
        "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
        "        x = x.replace(char, ' ' + char + ' ')\n",
        "    '''\n",
        "    contractions = {\n",
        "      \"ain't\": \"am not\",  \"aren't\": \"are not\",  \"can't\": \"cannot\",  \"can't've\": \"cannot have\",  \"'cause\": \"because\",  \"could've\": \"could have\",  \"couldn't\": \"could not\",\n",
        "      \"couldn't've\": \"could not have\",  \"didn't\": \"did not\",  \"doesn't\": \"does not\",  \"don't\": \"do not\",  \"hadn't\": \"had not\",  \"hadn't've\": \"had not have\",\"hasn't\": \"has not\",  \"haven't\": \"have not\",  \"he'd\": \"he would\",  \"he'd've\": \"he would have\",  \"he'll\": \"he will\",  \"he'll've\": \"he will have\",  \"he's\": \"he is\",  \"how'd\": \"how did\",\n",
        "      \"how'd'y\": \"how do you\",  \"how'll\": \"how will\",  \"how's\": \"how is\",  \"I'd\": \"I would\",  \"I'd've\": \"I would have\",  \"I'll\": \"I will\",  \"I'll've\": \"I will have\",  \"I'm\": \"I am\",\n",
        "      \"I've\": \"I have\",  \"isn't\": \"is not\",  \"it'd\": \"it had\",  \"it'd've\": \"it would have\",  \"it'll\": \"it will\",  \"it'll've\": \"it will have\",  \"it's\": \"it is\",  \"let's\": \"let us\",\"ma'am\": \"madam\",  \"mayn't\": \"may not\",  \"might've\": \"might have\",  \"mightn't\": \"might not\",  \"mightn't've\": \"might not have\",  \"must've\": \"must have\",  \"mustn't\": \"must not\",\n",
        "      \"mustn't've\": \"must not have\",  \"needn't\": \"need not\",  \"needn't've\": \"need not have\",  \"o'clock\": \"of the clock\",  \"oughtn't\": \"ought not\",  \"oughtn't've\": \"ought not have\",      \"shan't\": \"shall not\",  \"sha'n't\": \"shall not\",  \"shan't've\": \"shall not have\",  \"she'd\": \"she would\",  \"she'd've\": \"she would have\",  \"she'll\": \"she will\",  \"she'll've\": \"she will have\",\n",
        "      \"she's\": \"she is\",  \"should've\": \"should have\",  \"shouldn't\": \"should not\",  \"shouldn't've\": \"should not have\",  \"so've\": \"so have\",  \"so's\": \"so is\",  \"that'd\": \"that would\",  \"that'd've\": \"that would have\",\n",
        "      \"that's\": \"that is\",  \"there'd\": \"there had\",  \"there'd've\": \"there would have\",  \"there's\": \"there is\",  \"they'd\": \"they would\",  \"they'd've\": \"they would have\",      \"they'll\": \"they will\",  \"they'll've\": \"they will have\",  \"they're\": \"they are\",  \"they've\": \"they have\",  \"to've\": \"to have\",  \"wasn't\": \"was not\",  \"we'd\": \"we had\",  \"we'd've\": \"we would have\",\n",
        "      \"we'll\": \"we will\",  \"we'll've\": \"we will have\",  \"we're\": \"we are\",  \"we've\": \"we have\",  \"weren't\": \"were not\",  \"what'll\": \"what will\",  \"what'll've\": \"what will have\",      \"what're\": \"what are\",  \"what's\": \"what is\",  \"what've\": \"what have\",  \"when's\": \"when is\",  \"when've\": \"when have\",  \"where'd\": \"where did\",  \"where's\": \"where is\",  \"where've\": \"where have\",\n",
        "      \"who'll\": \"who will\",  \"who'll've\": \"who will have\",  \"who's\": \"who is\",  \"who've\": \"who have\",  \"why's\": \"why is\",  \"why've\": \"why have\",  \"will've\": \"will have\",  \"won't\": \"will not\",      \"won't've\": \"will not have\",  \"would've\": \"would have\",  \"wouldn't\": \"would not\",  \"wouldn't've\": \"would not have\",  \"y'all\": \"you all\",  \"y'alls\": \"you alls\",  \"y'all'd\": \"you all would\",  \"y'all'd've\": \"you all would have\",\n",
        "      \"y'all're\": \"you all are\",  \"y'all've\": \"you all have\",  \"you'd\": \"you had\",  \"you'd've\": \"you would have\",  \"you'll\": \"you you will\",  \"you'll've\": \"you you will have\",  \"you're\": \"you are\",  \"you've\": \"you have\"\n",
        "      }\n",
        "    contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
        "    def expand_contractions(s, contractions_dict=contractions):\n",
        "        def replace(match):\n",
        "            return contractions_dict[match.group(0)]\n",
        "        return contractions_re.sub(replace, s)     \n",
        "    data['question1']=data['question1'].apply(lambda x: expand_contractions(x.lower()) )\n",
        "    data['question2']=data['question2'].apply(lambda x: expand_contractions(x.lower()) )\n",
        "    return data\n",
        "train=cleanup(train)\n",
        "\n",
        "def build_dict(sentences):\n",
        "    #Dictionary of train words --> word index: word freq\n",
        "    print('Building dictionary using train words..')\n",
        "    wordcount = dict()\n",
        "    #For each worn in each sentence, cummulate frequency\n",
        "    for ss in sentences:\n",
        "        for w in ss:\n",
        "            if w not in wordcount:\n",
        "                wordcount[w] = 1\n",
        "            else:\n",
        "                wordcount[w] += 1    \n",
        "    worddict = dict()\n",
        "    for idx, w in enumerate(sorted(wordcount.items(), key = lambda x: x[1], reverse=True)):\n",
        "        worddict[w[0]] = idx+2  # leave 0 and 1 (UNK)\n",
        "    return worddict, wordcount\n",
        "\n",
        "def generate_sequence(sentences, dictionary):\n",
        "    seqs = [None] * len(sentences)\n",
        "    for idx, ss in enumerate(sentences):\n",
        "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
        "    return seqs\n",
        "\n",
        "def tokenize(x):\n",
        "    return x.lower().split()\n",
        "\n",
        "questions = train['question1'].tolist() + train['question2'].tolist()\n",
        "tok_questions = [tokenize(s) for s in questions]\n",
        "worddict, wordcount = build_dict(tok_questions)\n",
        "print(np.sum(list(wordcount.values())), ' total words ', len(worddict), ' unique words')\n",
        "\n",
        "#Metrics for sentence comparison\n",
        "def jc(x):\n",
        "    return jaccard_distance(set(x['Q1seq']),set(x['Q2seq']))\n",
        "\n",
        "def cosine_d(x):\n",
        "    a = set(x['Q1seq'])\n",
        "    b = set(x['Q2seq'])\n",
        "    d = len(a)*len(b)\n",
        "    if (d == 0):\n",
        "        return 0\n",
        "    else: \n",
        "        return len(a.intersection(b))/d\n",
        "    \n",
        "def diff_ratios(st1, st2):\n",
        "    seq = difflib.SequenceMatcher()\n",
        "    seq.set_seqs(str(st1).lower(), str(st2).lower())\n",
        "    return seq.quick_ratio()\n",
        "\n",
        "def word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).lower().split():\n",
        "        if word not in stops:\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).lower().split():\n",
        "        if word not in stops:\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
        "    return R\n",
        "\n",
        "def ngrams_split(lst, n):\n",
        "    counts = dict()\n",
        "    grams = [''.join(lst[i:i+n]) for i in range(len(lst)-n)]\n",
        "    return grams\n",
        "\n",
        "def intersect3(x,y):    \n",
        "    return set(x).intersection(y)\n",
        "\n",
        "def edit_distance(s1, s2):\n",
        "    m=len(s1)+1\n",
        "    n=len(s2)+1\n",
        "\n",
        "    tbl = {}\n",
        "    for i in range(m): tbl[i,0]=i\n",
        "    for j in range(n): tbl[0,j]=j\n",
        "    for i in range(1, m):\n",
        "        for j in range(1, n):\n",
        "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
        "            tbl[i,j] = min(tbl[i, j-1]+1, tbl[i-1, j]+1, tbl[i-1, j-1]+cost)\n",
        "\n",
        "    return tbl[i,j]\n",
        "\n",
        "def leve3(string_1, string_2):\n",
        "    len_1 = len(ngrams_split(string_1,3)) + 1\n",
        "    len_2 = len(ngrams_split(string_2,3)) + 1\n",
        "    d=[0]\n",
        "    if len_1>3 and len_2>3:\n",
        "        d = [0] * (len_1 * len_2)\n",
        "\n",
        "        for i in range(len_1):\n",
        "            d[i] = i\n",
        "        for j in range(len_2):\n",
        "            d[j * len_1] = j\n",
        "\n",
        "        for j in range(1, len_2):\n",
        "            for i in range(1, len_1):\n",
        "                if string_1[i - 3] == string_2[j - 3]:\n",
        "                    d[i + j * len_1] = d[i - 1 + (j - 1) * len_1]\n",
        "                else:\n",
        "                    d[i + j * len_1] = min(\n",
        "                       d[i - 1 + j * len_1] + 1,        # deletion\n",
        "                       d[i + (j - 1) * len_1] + 1,      # insertion\n",
        "                       d[i - 1 + (j - 1) * len_1] + 1,  # substitution\n",
        "                    )\n",
        "\n",
        "    return d[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ba8a9983-f012-0c33-1d79-b00c13becf16"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances,laplacian_kernel,sigmoid_kernel,polynomial_kernel,rbf_kernel\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def intersecting(a, b):\n",
        "    return ' '.join(list(set(a.split()) & set(b.split())))\n",
        "\n",
        "def differencing(a, b):\n",
        "    return ' '.join(list(set(a.split()) ^ set(b.split())))\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "tfidf.fit_transform(questions)\n",
        "\n",
        "def get_features(df_features):    \n",
        "    #Question length\n",
        "    print('question lengths....')\n",
        "    df_features['Qlen1'] = df_features.question1.map(lambda x: len(str(x)))\n",
        "    df_features['Qlen2'] = df_features.question2.map(lambda x: len(str(x)))\n",
        "    df_features['diffQlen'] = df_features['Qlen1']-df_features['Qlen2']   \n",
        "    print('question dist....')\n",
        "    df_features['editdis'] = df_features[['question1','question2']].apply(lambda x: edit_distance(*x), axis=1)\n",
        "    df_features['levens'] = df_features[['question1','question2']].apply(lambda x: leve3(*x), axis=1)\n",
        "    #Question number of words\n",
        "    df_features['Qwords1'] = df_features.question1.map(lambda x: len(str(x).split()))\n",
        "    df_features['Qwords2'] = df_features.question2.map(lambda x: len(str(x).split()))\n",
        "    df_features['diffQword'] = df_features['Qwords1'] -df_features['Qwords2']\n",
        "    df_features['isdup'] = df_features.diffQword.map(lambda x: 1-len(str(x)))\n",
        "    print('jaccard...')\n",
        "    df_features['Q1seq'] = generate_sequence(df_features['question1'].apply(tokenize),worddict)\n",
        "    df_features['Q2seq'] = generate_sequence(df_features['question2'].apply(tokenize),worddict)\n",
        "    df_features['jaccard'] = df_features.apply(jc,axis = 1)    \n",
        "    print('cosine....')\n",
        "    df_features['cosine'] = df_features.apply(cosine_d,axis = 1)        \n",
        "    #matching the sequences\n",
        "    print('difflib...')\n",
        "    df_features['SeqMatchRatio'] = df_features.apply(lambda r: diff_ratios(r.question1, r.question2), axis=1)  #takes long\n",
        "    #percentage of common words in both questions\n",
        "    print('word match...')    \n",
        "    df_features['WordMatch'] = df_features.apply(word_match_share, axis=1, raw=True)    \n",
        "\n",
        "    df_features['interseq'] = df_features[['question1','question2']].apply(lambda x: intersecting(*x), axis=1)\n",
        "    df_features['diffseq'] = df_features[['question1','question2']].apply(lambda x: differencing(*x), axis=1)    \n",
        "    print('tfidf...')      \n",
        "    question1_tfidf = tfidf.transform(df_features.question1.tolist())  #print(question1_tfidf)  sparse matrix \n",
        "    question2_tfidf = tfidf.transform(df_features.question2.tolist())    \n",
        "    questionI_tfidf = tfidf.transform(df_features.interseq.tolist())    \n",
        "    questionD_tfidf = tfidf.transform(df_features.diffseq.tolist()) \n",
        "    print('svd...')\n",
        "    svd = TruncatedSVD(n_components=50, n_iter=20, random_state=42)\n",
        "    df_features=df_features.join(pd.DataFrame(svd.fit_transform(questionI_tfidf)),how='inner')  \n",
        "    \n",
        "    svd = TruncatedSVD(n_components=30, n_iter=20, random_state=42)\n",
        "    temp=pd.DataFrame(svd.fit_transform(questionD_tfidf))\n",
        "    temp.rename(columns=lambda x: str(x)+'_d', inplace=True) #nog eens zoeken omcolumns te renamen\n",
        "    df_features=df_features.join(temp,how='inner')    \n",
        "    svd = TruncatedSVD(n_components=20, n_iter=20, random_state=42)\n",
        "    temp=pd.DataFrame(svd.fit_transform(question1_tfidf))\n",
        "    temp.rename(columns=lambda x: str(x)+'_q1', inplace=True) #nog eens zoeken omcolumns te renamen\n",
        "    df_features=df_features.join(temp,how='inner') \n",
        "    svd = TruncatedSVD(n_components=20, n_iter=20, random_state=42)\n",
        "    temp=pd.DataFrame(svd.fit_transform(question2_tfidf))\n",
        "    temp.rename(columns=lambda x: str(x)+'_q2', inplace=True) #nog eens zoeken omcolumns te renamen\n",
        "    df_features=df_features.join(temp,how='inner') \n",
        "    \n",
        "    df_features['tfidfCo_Si'] = cosine_similarity(question1_tfidf,question2_tfidf).diagonal().T\n",
        "    print('Eucl...')    \n",
        "    df_features['tfidfEu_Di'] = euclidean_distances(question1_tfidf,question2_tfidf).diagonal().T  \n",
        "    print('Sig...')      \n",
        "    df_features['tfidfSi_Ke'] = 1-sigmoid_kernel(question1_tfidf,question2_tfidf).diagonal().T  \n",
        "    print('rbf...')      \n",
        "    df_features['tfidfrbf_Ke'] = rbf_kernel(question1_tfidf,question2_tfidf).diagonal().T*1000-1000       \n",
        "    print('poly...')      \n",
        "    df_features['tfidfpol_Ke'] = polynomial_kernel(question1_tfidf,question2_tfidf).diagonal().T*1000-1000           \n",
        "    #Exactly same questions\n",
        "    df_features['exactly_same'] = (df_features['question1'] == df_features['question2']).astype(int)\n",
        "    return df_features.fillna(0.0)\n",
        "\n",
        "df_train = get_features(train)\n",
        "feats = df_train.columns.values.tolist()\n",
        "feats=[x for x in feats if x not in ['question1','question2','Q1seq','Q2seq','interseq', 'diffseq','id','qid1','qid2','is_duplicate']]\n",
        "print(\"features\",feats)\n",
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad3edd8d-0449-3f50-8e1f-f63f6d506b80"
      },
      "outputs": [],
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(df_train[feats], y, test_size=0.3, random_state=0)\n",
        "#XGBoost model\n",
        "params = {\"objective\":\"binary:logistic\",'eval_metric':'logloss',\"eta\": 0.11,\n",
        "          \"subsample\":0.7,\"min_child_weight\":1,\"colsample_bytree\": 0.7,\n",
        "          \"max_depth\":5,\"silent\":1,\"seed\":2017}\n",
        "\n",
        "d_train = xgb.DMatrix(x_train, label=y_train)\n",
        "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
        "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
        "bst = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=200,verbose_eval=25) #change to higher #s\n",
        "print('training done')\n",
        "\n",
        "print(\"log loss for training data set\",log_loss(y, bst.predict(xgb.DMatrix(df_train[feats]))))\n",
        "#Predicting for test data set\n",
        "sub = pd.DataFrame() # Submission data frame\n",
        "sub['test_id'] = []\n",
        "sub['is_duplicate'] = []\n",
        "header=['test_id','question1','question2','id','qid1','qid2','is_duplicate']\n",
        "test=pd.read_csv('../input/test.csv')[:20000].fillna(\"\")\n",
        "print(\"cleaning test\")\n",
        "df_test=cleanup(test)\n",
        "print(\"feature engineering for test\")\n",
        "df_test = get_features(df_test)\n",
        "sub=pd.DataFrame({'test_id':df_test['test_id'], 'is_duplicate':bst.predict(xgb.DMatrix(df_test[feats]))})\n",
        "sub.to_csv('quora_submission_xgb_11.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}