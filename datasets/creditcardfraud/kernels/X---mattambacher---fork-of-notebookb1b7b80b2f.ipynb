{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aad0adf3-43a6-0464-8516-e74e7509a0cc"
      },
      "source": [
        "***Under development***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e3b67800-51a1-5db3-ea07-063819ad7da8"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_curve, auc, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "80fb96c5-126b-5221-6537-8e0bb33d7fa5"
      },
      "source": [
        "<h2> Introduction </h2>\n",
        "The given data set contains information from ~250,000 credit card transactions that took place over a certain time period in Europe. Each of the transactions is labeled 0 for a legitimate transaction and 1 for a fraudulent transaction. The data set is skewed, containing many more legitimate transactions compared to fraudulent transactions.\n",
        "\n",
        "Other features given include the timestamp (since the 1st transaction in the data set), amount of the purchase (in USD), as well as the first 28 principle components generated from other data collected for each transaction. For privacy purposes, no more information is given about the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "372587ce-4381-5a74-1e0b-536d3af15ddc"
      },
      "source": [
        "<h2> Loading and Visualizing the Data </h2> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2fc4d0cb-e803-63d9-7a7e-3069f73c3025"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../input/creditcard.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1c818143-32c5-ee06-aeda-15b91e9aa466"
      },
      "source": [
        "Our goal is to be able to predict whether a transaction is fraudulent  given this data. The timestamp, though not unique, serves essentially as a unique ID for each transaction, and is ultimately unimportant in determining if a transaction is legit or not. So we'll go ahead and remove that, as well as split the data into the fraudulent and legitimate transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a742149-a73b-53d9-46d4-4219b3f1babc"
      },
      "outputs": [],
      "source": [
        "df = df.drop('Time', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ce0e8c17-86d5-39a0-5550-66b3df957fd8"
      },
      "outputs": [],
      "source": [
        "fraud = df[df['Class'] == 1]\n",
        "legit = df[df['Class'] == 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f101847a-7cb9-ea0d-e3d8-590342f24c68"
      },
      "source": [
        "Let's also take a look at how many transactions are actually fraudulent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "931a42ba-f733-f51c-2f20-3a160b2fa82c"
      },
      "outputs": [],
      "source": [
        "len(legit)/(len(fraud) + len(legit))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6233f12e-248d-32f0-f45f-db28760efa25"
      },
      "source": [
        "So, around 0.17% are actually fraudulent. We keep this in mind when choosing a performance evaluation. Naively choosing classification accuracy would not necessarily accurately describe how well our learning algorithm is doing. For example, by blindly classifying every transaction as legitimate, we would be right over 99.8% of the time. More on this later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "41ec200e-205f-8534-0e28-3c3d13dfd8c0"
      },
      "outputs": [],
      "source": [
        "fraud_ax = pd.DataFrame.hist(fraud, column='Amount')\n",
        "plt.xlabel('Purchase Amount (USD)')\n",
        "plt.ylabel('Occurances')\n",
        "plt.title('Fraudulent Transactions')\n",
        "\n",
        "legit_ax = pd.DataFrame.hist(legit, column='Amount')\n",
        "plt.xlabel('Purchase Amount (USD)')\n",
        "plt.ylabel('Occurances')\n",
        "plt.title('Legitimate Transactions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "509aef0e-f731-df18-893e-500cb84e2574"
      },
      "source": [
        "Just from the autoscaling, Most of the legitimate purchases are under $5000, but it looks like there's a relatively small amount of legitimate purchases that reach as high as $25,000. The fraudulent purchases interestingly are all rather small. Most fall under $500, but there are some that reach the neighborhood of $2000.\n",
        "\n",
        "Let's take a look at exactly how many outliers we're dealing with in each case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "67172ecc-9cf6-8409-2523-2ec5f33b0a42"
      },
      "outputs": [],
      "source": [
        "sns.violinplot(x='Class', y='Amount', data=df, inner='quartile')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0904c3ce-692b-3dbc-7978-e5b181fd195a"
      },
      "source": [
        "Since we have a lot of outliers, and there's no <i>a priori</i> reason to ignore them, we want to be careful with choosing a learning algorithm. Learning algorithms using regression can be significantly affected by these outliers. \n",
        "\n",
        "Here, I'll choose to use an ensemble method, namely a Random Forest Classifier. A RFC is not sensitive to these outliers since it's based on splitting data into two groups at each node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9df63aea-10c4-88b5-b554-29c2d6764244"
      },
      "outputs": [],
      "source": [
        "X = df\n",
        "y = X.pop('Class')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c932663a-a93c-c6c1-b1b7-9e9d789e40a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "RANDOM_STATE = 123\n",
        "clf = RandomForestClassifier(n_estimators=10, warm_start=True, max_features='auto',\n",
        "                             random_state=RANDOM_STATE)\n",
        "clf.fit(X_train, y_train)\n",
        "prediction = clf.predict(X_test)\n",
        "fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "plt.plot(fpr,tpr, 'b-', [0,1], [0,1], 'k--')\n",
        "\n",
        "prec = precision_score(y_test, prediction)\n",
        "recall = recall_score(y_test, prediction)\n",
        "print(\"Number of actual fraud cases in test set: {0}\".format(sum(y_test)))\n",
        "print(\"Number of predicted fraud cases: {0}\".format(sum(prediction)))\n",
        "print(\"AUC score: {0}\".format(roc_auc))\n",
        "print(\"F1 score: {0}\".format(2*prec*recall/(prec + recall)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8bf6b1e3-571d-c76a-55ea-d27740d2bed4"
      },
      "source": [
        "Ok, so we've got an AUC of around 0.924 with an F1 score of 0.835 using 10 trees and sqrt(29) features. Not bad, but the parameter choices for this were rather arbitrary. We can try tuning some parameters to get a better result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1008df60-ba47-baf4-c883-bc3243f58018"
      },
      "source": [
        "<h2> Parameter Tuning </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff3bceef-3ad7-9875-fa40-9c13f0460370"
      },
      "outputs": [],
      "source": [
        "f1_scores = [2*prec*recall/(prec + recall)]\n",
        "auc_scores = [roc_auc]\n",
        "\n",
        "for i in range(10):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "    clf.n_estimators += 10\n",
        "    clf.fit(X_train, y_train)\n",
        "    prediction = clf.predict(X_test)\n",
        "    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    prec = precision_score(y_test, prediction)\n",
        "    recall = recall_score(y_test, prediction)\n",
        "    F1 = 2*prec*recall/(prec+recall)\n",
        "    f1_scores.append(F1)\n",
        "    auc_scores.append(roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eedf0953-abd7-b1fc-2690-a9c752a17fb8"
      },
      "outputs": [],
      "source": [
        "plt.plot(auc_scores, 'bo-')\n",
        "plt.plot(f1_scores, 'ro-')"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}