{"cells":[
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "import pandas as pd \r\nimport numpy as np \r\nimport xgboost as xgb\r\nfrom scipy.optimize import fmin_powell\r\nfrom ml_metrics import quadratic_weighted_kappa\r\n\r\ndef eval_wrapper(yhat, y):  \r\n    y = np.array(y)\r\n    y = y.astype(int)\r\n    yhat = np.array(yhat)\r\n    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \r\n    return quadratic_weighted_kappa(yhat, y)\r\n    \r\ndef get_params():\r\n    \r\n    params = {}\r\n    params[\"objective\"] = \"reg:linear\"     \r\n    params[\"eta\"] = 0.05\r\n    params[\"min_child_weight\"] = 55\r\n    params[\"subsample\"] = 0.8\r\n    params[\"colsample_bytree\"] = 0.30\r\n    params[\"silent\"] = 1\r\n    params[\"max_depth\"] = 8\r\n    plst = list(params.items())\r\n\r\n    return plst\r\n    \r\ndef apply_offset(data, bin_offset, sv, scorer=eval_wrapper):\r\n    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\r\n    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\r\n    score = scorer(data[1], data[2])\r\n    return score\r\n\r\n# global variables\r\ncolumns_to_drop = ['Id', 'Response']\r\nxgb_num_rounds = 500\r\nnum_classes = 8\r\n\r\nprint(\"Load the data using pandas\")\r\ntrain = pd.read_csv(\"../input/train.csv\")\r\ntest = pd.read_csv(\"../input/test.csv\")\r\n\r\n# combine train and test\r\nall_data = train.append(test)\r\n\r\n# factorize categorical variables    \r\nall_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\r\n\r\nprint('Eliminate missing values')    \r\n# Use -1 for any others\r\nall_data.fillna(-1, inplace=True)\r\n\r\n# fix the dtype on the label column\r\nall_data['Response'] = all_data['Response'].astype(int)\r\n\r\n# Provide split column\r\nall_data['Split'] = np.random.randint(5, size=all_data.shape[0])\r\n\r\n# split train and test\r\ntrain = all_data[all_data['Response']>0].copy()\r\ntest = all_data[all_data['Response']<1].copy()\r\n\r\n# convert data to xgb data structure\r\nxgtrain = xgb.DMatrix(train.drop(columns_to_drop, axis=1), train['Response'].values)\r\nxgtest = xgb.DMatrix(test.drop(columns_to_drop, axis=1), label=test['Response'].values)    \r\n\r\n# get the parameters for xgboost\r\nplst = get_params()\r\nprint(plst)      \r\n\r\n# train model\r\nmodel = xgb.train(plst, xgtrain, xgb_num_rounds) \r\n\r\n# get preds\r\ntrain_preds = model.predict(xgtrain, ntree_limit=model.best_iteration)\r\nprint('Train score is:', eval_wrapper(train_preds, train['Response'])) \r\ntest_preds = model.predict(xgtest, ntree_limit=model.best_iteration)\r\ntrain_preds = np.clip(train_preds, -0.99, 8.99)\r\ntest_preds = np.clip(test_preds, -0.99, 8.99)\r\n\r\n# train offsets \r\noffsets = np.ones(num_classes) * -0.5\r\noffset_train_preds = np.vstack((train_preds, train_preds, train['Response'].values))\r\nfor j in range(num_classes):\r\n    train_offset = lambda x: -apply_offset(offset_train_preds, x, j)\r\n    offsets[j] = fmin_powell(train_offset, offsets[j])  \r\n\r\n# apply offsets to test\r\ndata = np.vstack((test_preds, test_preds, test['Response'].values))\r\nfor j in range(num_classes):\r\n    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \r\n\r\nfinal_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\r\n\r\npreds_out = pd.DataFrame({\"Id\": test['Id'].values, \"Response\": final_test_preds})\r\npreds_out = preds_out.set_index('Id')\r\npreds_out.to_csv('xgb_offset_submission0501.csv')\r\n "
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}