{"cells": [{"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "6c3c4146-8473-41e2-a662-413efb27665a", "_uuid": "637cff8e2103e323ef609bd2c350cb29aa05e099"}, "source": ["# to be imported\n", "from keras.preprocessing.text import text_to_word_sequence\n", "import pandas as pd\n", "from keras.preprocessing.text import Tokenizer\n", "import numpy as np\n", "from __future__ import print_function\n", "\n", "from keras.preprocessing import sequence\n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Activation\n", "from keras.layers import Embedding\n", "from keras.layers import Conv1D, GlobalMaxPooling1D"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "1da17257-6c48-42fe-b761-15148be8a56a", "_uuid": "ddb47654b861d29d92251edeed58352f05490e21"}, "source": ["\n", "# Read the input dataset \n", "d = pd.read_csv(\"../input/consumer_complaints.csv\", \n", "                usecols=('product','consumer_complaint_narrative'),\n", "                dtype={'consumer_complaint_narrative': object})\n", "# Only interested in data with consumer complaints\n", "d=d[d['consumer_complaint_narrative'].notnull()]\n", "\n", "d=d[d['product'].notnull()]\n", "d.reset_index(drop=True,inplace=True)\n", "x = d.iloc[:, 1].values\n", "y = d.iloc[:, 0].values\n", "print(y)\n", "\n", "#there are 11 unique classes for classification\n", "print(np.unique(y, return_counts=True))\n", "\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "432b95a1-0703-4279-a5e2-3f4797ba9854", "_uuid": "077100d4dd6995a3f78f183057055969e2808927"}, "source": [" # encode the text with word sequences - Preprocessing step 1\n", "tk = Tokenizer(num_words= 200, filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True, split=\" \")\n", "tk.fit_on_texts(x)\n", "x = tk.texts_to_sequences(x)\n", "x = sequence.pad_sequences(x, maxlen=200)\n", "\n", "print(x)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "9316d3ac-cb03-421e-8d81-225238d13978", "_uuid": "80b7c94682a711491f8bdf574c382c8f156024dc"}, "source": [" # Label Encoding categorical data for the classification category\n", "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n", "labelencoder_Y = LabelEncoder()\n", "y = labelencoder_Y.fit_transform(y)\n", "print(y)\n", "print(np.unique(y, return_counts=True))"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "_cell_guid": "8a79ea2e-8033-4919-b376-9ace5c02afab", "_uuid": "f9c775c1d7793986af517cf922df277e9414a221"}, "source": ["# Perform one hot encoding \n", "from keras import utils as np_utils\n", "y = np_utils.to_categorical(y, num_classes= 11)\n", "\n", "print(y)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "3a817ac7-7adf-404b-becf-5f2a0a9d6d05", "_uuid": "af2812f73524c6fe28fa276bc5c67b0db5110ee1"}, "source": ["# Seeding\n", "np.random.seed(200)\n", "indices = np.arange(len(x))\n", "np.random.shuffle(indices)\n", "x = x[indices]\n", "y = y[indices]"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "source": ["index_from=3\n", "start_char = 1\n", "if start_char is not None:\n", "        x = [[start_char] + [w + index_from for w in x1] for x1 in x]\n", "elif index_from:\n", "        x = [[w + index_from for w in x1] for x1 in x]"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["\n", "\n", "num_words = None\n", "if not num_words:\n", "        num_words = max([max(x1) for x1 in x])\n", "        \n", "oov_char = 2\n", "skip_top = 0\n", "# by convention, use 2 as OOV word\n", "# reserve 'index_from' (=3 by default) characters:\n", "# 0 (padding), 1 (start), 2 (OOV)\n", "if oov_char is not None:\n", "        x = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in x]\n", "else:\n", "        x = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in x]\n", "        \n", "# split test and train data\n", "test_split = 0.2\n", "idx = int(len(x) * (1 - test_split))\n", "x_train, y_train = np.array(x[:idx]), np.array(y[:idx])\n", "x_test, y_test = np.array(x[idx:]), np.array(y[idx:])\n", "\n", "print(x_train.shape)\n", "print(y_train.shape)\n", "print(x_test.shape)\n", "print(y_test.shape)\n", "print(y)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["x_train = sequence.pad_sequences(x_train, maxlen=201)\n", "x_test = sequence.pad_sequences(x_test, maxlen=201)\n", "print('x_train shape:', x_train.shape)\n", "print('x_test shape:', x_test.shape)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["\n", "max_features = 1000\n", "maxlen = 201\n", "embedding_dims = 50\n", "filters = 250\n", "kernel_size = 3\n", "hidden_dims = 250\n", "\n", "\n", "# CNN with max pooling imeplementation \n", "print('Build model...')\n", "model = Sequential()\n", "# we start off with an efficient embedding layer which maps\n", "# our vocab indices into embedding_dims dimensions\n", "model.add(Embedding(max_features,\n", "                    embedding_dims,\n", "                    input_length=maxlen))\n", "model.add(Dropout(0.2))\n", "\n", "# we add a Convolution1D, which will learn filters\n", "# word group filters of size filter_length:\n", "model.add(Conv1D(filters,\n", "                 kernel_size,\n", "                 padding='valid',\n", "                 activation='relu',\n", "                 strides=1))\n", "# we use max pooling:\n", "model.add(GlobalMaxPooling1D())\n", "\n", "# We add a vanilla hidden layer:\n", "model.add(Dense(hidden_dims))\n", "model.add(Dropout(0.2))\n", "model.add(Activation('relu'))\n", "\n", "# We project onto a single unit output layer, and squash it with a sigmoid:\n", "model.add(Dense(11))\n", "model.add(Activation('softmax'))\n", "\n", "model.compile(loss='categorical_crossentropy',\n", "              optimizer='rmsprop',\n", "              metrics=['accuracy'])\n", "\n", "model.fit(x_train, y_train,\n", "          batch_size=32,\n", "          epochs=50,\n", "          validation_data=(x_test, y_test))"]}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "pygments_lexer": "ipython3", "version": "3.6.3", "name": "python"}}}