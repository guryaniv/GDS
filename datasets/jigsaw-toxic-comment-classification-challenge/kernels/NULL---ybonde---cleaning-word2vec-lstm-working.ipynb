{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "version": "3.6.3", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["### WARNING: Contains Obscene Words"], "metadata": {"_cell_guid": "2e45162d-76fb-43e5-b520-bf6370d4b44c", "_uuid": "2380f0c4758bcdb348c25d39c7a1c69034e5445f"}, "cell_type": "markdown"}, {"source": ["# Loading and Visualization of Data\n", "We first load the data using pandas and load it into data frame."], "metadata": {"_cell_guid": "d65110f0-304f-4711-83b4-cd020316f9de", "_uuid": "b2653e003dfb148d9eb2846c5d05173b68f046c9"}, "cell_type": "markdown"}, {"source": ["# importing the dependencies\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from tqdm import tqdm # progress bar\n", "import copy # perform deep copyong rather than referencing in python\n", "import multiprocessing # for threading of word2vec model process\n", "\n", "# importing classes helpfull for text processing\n", "import nltk # general NLP\n", "import re # regular expressions\n", "import gensim.models.word2vec as w2v # word2vec model\n", "\n", "import matplotlib.pyplot as plt # data visualization\n", "%matplotlib inline\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_cell_guid": "b3bb84bb-0f88-4caa-86e6-42eec378bc79", "_uuid": "027f28ffdbd1240f800e671dba94c9523bd386dc", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["data_train = pd.read_csv('../input/train.csv')\n", "data_test = pd.read_csv('../input/test.csv')"], "metadata": {"_cell_guid": "85b3211e-0158-4d9f-b383-a91b988b81a8", "_uuid": "c2e99a7f50903fd367980490aaa1f988e4be3b91", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["data_train.head(10)"], "metadata": {"_cell_guid": "cfc9e1d5-6fdb-4aef-b40d-05cc6b90442d", "_uuid": "8904d3990a9278ec58e0d641e67e681a922e995d"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["data_test.head(10)"], "metadata": {"_cell_guid": "bdee578e-4edd-4a0f-bbf9-2a4b670b93b9", "_uuid": "45433411a2a93660bc5caaea11b182d02a58bdab"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["col_names = data_train.columns.values[2:]\n", "col_names = col_names.tolist()\n", "col_names.append('None')\n", "x = [sum(data_train[y]) for y in data_train.columns.values[2:]]\n", "x.append(len(data_train) - sum(x))"], "metadata": {"_cell_guid": "8e76e968-8b66-4867-91d4-addd84064021", "_uuid": "2f507718a7ac0a927d00dbe8e17c1d6e4a9c4f20", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["### Occurrence Plot\n", "We see how often the different catagories occur, thus giving us insight on how to pick up the data for training. As we can see that None of the catagories occur the most and thus we will sample ~10,000 sentences from it."], "metadata": {"_cell_guid": "e3885614-e8c2-47ec-bf8a-06a85f888327", "_uuid": "66b28d29a4cc213e800bccbf84ca14397101427a"}, "cell_type": "markdown"}, {"source": ["plt.figure(figsize = (10, 10))\n", "plt.bar(np.arange(len(x)),x)\n", "plt.xticks(np.arange(len(x)), col_names)\n", "plt.xlabel('Catagories')\n", "plt.ylabel('Occurrence')"], "metadata": {"_cell_guid": "95f74e2c-1420-4576-9c16-825a754d3725", "_uuid": "a7042b345b4439274f9629a554a3d8fdedeba76f"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Making Vocabulary and Text Conversion\n", "We now need to make vocabulary and convert the text into usable information vectors.\n", "\n", "For cleaning of sentences we first convert the sentences into a string that has all the sentences merged together. Then we convert the huge string into a sentence with each tokenized. After that we pass it through a regex filter which then returns each sentence converted into a list of tokens, and we append them to make a huge list of tokenized sentences."], "metadata": {"_cell_guid": "1242c381-c9c4-4a63-a3a6-3fd6e0d6bcc4", "_uuid": "6aeaee42011c6a80b8947846474bfa504cc6dcd8", "collapsed": true}, "cell_type": "markdown"}, {"source": ["train_sentences = data_train['comment_text'].values.tolist()\n", "test_sentences = data_test['comment_text'].values.tolist()\n", "# making a list of total sentences\n", "total_ = copy.deepcopy(train_sentences)\n", "total_.extend(test_sentences)\n", "print('[*]Training Sentences:', len(train_sentences))\n", "print('[*]Test Sentences:', len(test_sentences))\n", "print('[*]Total Sentences:', len(total_))\n", "\n", "# converting the text to lower\n", "for i in tqdm(range(len(total_))):\n", "    total_[i] = str(total_[i]).lower()"], "metadata": {"_cell_guid": "9615784d-1e7e-4eab-8dcf-6b8199b5a8bf", "_uuid": "fe318fafb006476da53c8825439df664dad47dbe"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["The code in comments below wa previously used but was later discarded for poor performance, you can still go through it for learning more about the methods"], "metadata": {}, "cell_type": "markdown"}, {"source": ["'''\n", "Won't be performing by this method rather will be using the crude way to do it\n", "#initialize rawunicode, we'll add all text to this one big string\n", "corpus_raw = u\"\"\n", "#for each sentence, read it, convert in utf 8 format, add it to the raw corpus\n", "for i in tqdm(range(len(total_))):\n", "    corpus_raw += str(total_[i])\n", "print('[*]Corpus is now', len(corpus_raw), 'characters long')\n", "\n", "# converting everything to small letter removing all the non caps\n", "corpus_lower = corpus_raw.lower()\n", "\n", "# we do no need a seperate tokenizer, sentence_to_wordlist function does it for us\n", "# tokenization process will result in words\n", "def tokenizer(sentences):\n", "    temp = []\n", "    for i in tqdm(range(len(sentences))):\n", "        temp.append(sentences.split())\n", "    return temp\n", "\n", "# NLTKs tokenizer was giving me a lot of difficulties, so dropping it for now\n", "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n", "'''"], "metadata": {"_cell_guid": "4838768a-3cf7-477f-a69b-14932431fa38", "_uuid": "7c8debf83b00a127ca576ed2447697e16ea28fb6", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# convert into list of words remove unecessary characters, split into words,\n", "# no hyphens and other special characters, split into words\n", "def sentence_to_wordlist(raw):\n", "    clean = re.sub(\"[^a-zA-Z0-9]\",\" \", raw)\n", "    words = clean.split()\n", "    return words"], "metadata": {"_cell_guid": "4cd130ea-c3be-4894-92d6-7fe4c25f050a", "_uuid": "eacab8908c63ae04b10fe821a80aaceecf30b963", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["***Trying something else***\n", "\n", "What if we take only the words that are lower and convert capitalised words to lower, how many tokens to we get then. This gives us ~1000 less tokens compared to when we were also using caps, which is not a difference of lot but will still help us."], "metadata": {"_cell_guid": "7c6e01fb-c84c-4a08-bbe8-d999430c56d7", "_uuid": "6815029530c8ff3e650d9296773349fcfda82e84"}, "cell_type": "markdown"}, {"source": ["# tokenising the lowered corpus\n", "clean_lower = []\n", "for i in tqdm(range(len(total_))):\n", "    clean_lower.append(sentence_to_wordlist(total_[i]))"], "metadata": {"_cell_guid": "1a7eb602-630e-41f3-86f0-cdd29e591d9c", "_uuid": "0422d93256b2daf0cf25066ebb006189ee1b877a"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["Thus we see we have 322849 sentences in total, now we see the lengths of each of them, to decide our designing of the model. So some of the features of the data are as follows:"], "metadata": {}, "cell_type": "markdown"}, {"source": ["# tokens and its count\n", "total_tokens_l = []\n", "for s in clean_lower:\n", "    total_tokens_l.extend(s)\n", "unk_tokens_l = list(set(total_tokens_l))\n", "print(\"[!]Total number of tokens:\", len(total_tokens_l))\n", "print(\"[!]Total number of unique tokens:\", len(unk_tokens_l))"], "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# while we convert each sentence into it's feature matrix, we need to have a consistency\n", "# of size, else we will not be able to train the model efficiently. For that we need the\n", "# length of largest sentence, and that of the smallest\n", "maxlen = max([len(s) for s in clean_lower])\n", "minlen = min([len(s) for s in clean_lower])\n", "print(maxlen)\n", "print(minlen)"], "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# finding index where length is zero\n", "index = [int(i) for i,s in enumerate(clean_lower) if len(s) == 0]\n", "print(\"[*]No. of entries with 0 length:\", len(index))"], "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# as we can see that all those sentences exist in test data set\n", "print('[*]Minimum index with length 0:',min(index))\n", "print('[*]Length of training dataset:', len(train_sentences))\n", "\n", "# so reducing the values of index by length of train_sentences\n", "index_test = [i-len(train_sentences) for i in index]\n", "# looking at those sentences with 0 length\n", "# print(len(train_sentences) < index[0])\n", "print(test_sentences[index_test[0]])\n", "print(test_sentences[index_test[12]])\n", "print(test_sentences[index_test[34]])"], "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# we remove these indexes and in submission classify them as 0.5 for all catagories\n", "clean_ = [c for i,c in enumerate(clean_lower) if i not in index]"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["print(clean_[10])"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Word-to-Vector\n", "For converting them into features we train a word2vec model which converts each word into it's corresponding vector. Word2Vec models are extremely efficient in finding temporal relations as they themselves are shallow neural networks.\n", "\n", "We train the word2vec model on the entire corpus so that it learns the similarities in the text and can give us vectors for all the words, not just those that occur in training dataset.\n", "\n", "You can perform the following operations and learn about word2vec model."], "metadata": {"_cell_guid": "c598155c-fd8f-4bbe-b9f1-e5e7c4b045e5", "_uuid": "8728d7b4aa5b8ebe2ccc080ca7fb3e8d01f8a6de"}, "cell_type": "markdown"}, {"source": ["# hyper parameters of the word2vec model\n", "num_features = 200 # dimensions of each word embedding\n", "min_word_count = 1 # this is not advisable but since we need to extract\n", "# feature vector for each word we need to do this\n", "num_workers = multiprocessing.cpu_count() # number of threads running in parallel\n", "context_size = 7 # context window length\n", "downsampling = 1e-3 # downsampling for very frequent words\n", "seed = 1 # seed for random number generator to make results reproducible"], "metadata": {"_cell_guid": "79e83c47-bb02-4b70-bd64-b916e300c69c", "_uuid": "559d05083dcf986d8184a962123b123966297ba1", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["word2vec_ = thrones2vec = w2v.Word2Vec(\n", "    sg = 1, seed = seed,\n", "    workers = num_workers,\n", "    size = num_features,\n", "    min_count = min_word_count,\n", "    window = context_size,\n", "    sample = downsampling\n", ")"], "metadata": {"_cell_guid": "6baf4fd9-158e-4d99-8bb2-33fc2e081572", "_uuid": "44e9ad872449d7042050db81a60a694b7e24a950", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# first we need to built the vocab\n", "word2vec_.build_vocab(clean_)"], "metadata": {"_cell_guid": "aebe1a54-0a31-4772-9a7f-e84118150d43", "_uuid": "fc9e7ee6b91f6e9723b9f4cc541bf1909a1acf76", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# now we need to train the model\n", "word2vec_.train(clean_, total_examples = word2vec_.corpus_count, epochs = word2vec_.iter)"], "metadata": {"_cell_guid": "f8403f50-4c1e-43e9-b71e-39326e2c35a6", "_uuid": "8de9eff563fea4c7de639891a6731db2488e28b0", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["word2vec_.wv.most_similar('male')"], "metadata": {"_cell_guid": "cdf0cd57-041f-4c72-98d1-94b90995a631", "_uuid": "a4263aaa767ae0730d0f8094138e153c84c88cf5", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["word2vec_.wv.most_similar('gay')"], "metadata": {"_cell_guid": "f25afca8-d131-4a26-9140-c55e02c169f2", "_uuid": "b55f0080f1b0606cf904e5c521fefab1f06e8196", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["word2vec_.wv.most_similar('dick')"], "metadata": {"_cell_guid": "01aae3a2-0e8f-435f-a6a5-10fcec1098c2", "_uuid": "ff40b6e839c13592131f5fbcfee5c23cecf2d02e", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# how to get vector for each word\n", "vec_ = word2vec_['male']\n", "print('[*]Shape of vec_:', vec_.shape)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["The best practice is to save the model once the training is done, I cannot perform these on the Kaggle kernel, but following is the code on how to save it, following it is the code to load it.\n"], "metadata": {}, "cell_type": "markdown"}, {"source": ["'''\n", " if not os.exists('trained'):\n", "    os.makedirs('trained')\n", "    \n", "w2vector_.save(os.path.join('trained', 'w2vector_.w2v'))\n", "\n", "w2vector_ = word2vec.Word2Vec.load(os.path.join('trained', 'w2vector_.w2v'))\n", "'''"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Making Feature Matrices\n", "Now that we have our word2vec model trained we can convert each word into it's vector and make our numerical data.\n", "\n", "We need to pad the sequences to make them of consistent length if we have to use Keras fixed LSTM network. for this we can use \"PAD\" and add it to the start of all the sequences which are less than maxlen. We also define the vector for PAD which will be the numerical information.\n", "\n", "*As we can see this is a very slow process, might have to play around a bit to optimize the padding*"], "metadata": {"_cell_guid": "b4a38506-6a96-41bc-84f3-2162a84c9205", "_uuid": "94275dabc041e03a1a180b2e887ab5555c39f032"}, "cell_type": "markdown"}, {"source": ["# adding 'PAD' to each sequence\n", "print('[!]Adding \\'PAD\\' to each sequence...')\n", "for i in tqdm(range(len(clean_))):\n", "    sentence = clean_[i][::-1]\n", "    for _ in range(maxlen - len(sentence)):\n", "        sentence.append('PAD')\n", "    clean_[i] = sentence[::-1]\n", "print()\n", "\n", "# defining 'PAD'\n", "PAD = np.zeros(word2vec_['guy'].shape)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# first we make the training set\n", "train_features = []\n", "print('[!]Making training features...')\n", "for i in tqdm(range(len(data_training))):\n", "    sentence = clean_[i]\n", "    temp = []\n", "    for token in sentence:\n", "        temp.append(word2vec_[token].tolist())\n", "    train_features.append(temp)\n", "\n", "# perform on local machine no need to waste kaggle resources\n", "# train_data = np.array(train_features)\n", "print()"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# now we make the testing set\n", "test_features = []\n", "print('[!]Making training features...')\n", "for i in tqdm(range(len(data_testing))):\n", "    sentence = clean_[i+len(data_training)]\n", "    temp = []\n", "    for token in sentence:\n", "        temp.append(word2vec_[token].tolist())\n", "    test_features.append(temp)\n", "    \n", "# perform on local machine no need to waste kaggle resources\n", "# test_data = np.array(testing_features)\n", "print()"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# saving the numpy arrays\n", "print('[!]Saving training data file at:', PATH_SAVE_DATA_TRAIN, ' ...')\n", "np.save(PATH_SAVE_DATA_TRAIN , train_data)\n", "\n", "print('[!]Saving testing data file at:', PATH_SAVE_DATA_TEST, ' ...')\n", "np.save(PATH_SAVE_DATA_TEST , test_data)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Making the labels\n", "Now we have one last job to do, make the labels file and dump it so we can use it later"], "metadata": {}, "cell_type": "markdown"}, {"source": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}]}