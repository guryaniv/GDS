{"cells":[{"metadata":{"trusted":true,"_uuid":"2bb4e49c02fcc8ca00b543b035ed9935736c294f"},"cell_type":"code","source":"\"\"\"\nthis kernel uses code and data from these kernels:\nhttps://www.kaggle.com/dfrumkin/a-simple-way-to-use-giba-s-features/notebook\nhttps://www.kaggle.com/johnfarrell/giba-s-property-extended-result\nhttps://www.kaggle.com/titericz/the-property-by-giba\n\"\"\"\nimport numpy as np\nimport pandas as pd\n\nimport os\nimport datetime\n\nimport gc\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nfrom math import sqrt\nimport math\n\nimport lightgbm as lgb\n\nfrom tqdm import tqdm_notebook\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2613f001b5f6532a3422722b324a7047f1ea158"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0c90cabd7be7fc9271d94a2308a88a1f7adebcb9"},"cell_type":"markdown","source":"## This section finds all the ordered feature sets. \n\n## If you want more then the 100 40 length sets you can change the length filter in the code after you have the first 100 and run this some more. It gets a bit iffy below 30 length. I did not find any improvement after 110 extra feature sets."},{"metadata":{"trusted":true,"_uuid":"2ca6b8857e99df44d7495866acdaa3f8379b9a4f"},"cell_type":"code","source":"#This code is borrowed from a kernel. Not sure\nall_features = [c for c in test.columns if c not in ['ID']]\ndef has_ugly(row):\n    for v in row.values[row.values > 0]:\n        if str(v)[::-1].find('.') > 2:\n            return True\n    return False\ntest['has_ugly'] = test[all_features].apply(has_ugly, axis=1)\ntest_og = test[['ID']].copy()\ntest_og['nonzero_mean'] = test[[c for c in test.columns if c not in ['ID', 'has_ugly']]].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\ntest = test[test.has_ugly == False]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"085ce2ba99f1265d2a6b228141e557358b9da7b6"},"cell_type":"code","source":"\ntrain_t = train.drop(['target'], axis = 1, inplace=False)\ntrain_t.set_index('ID', inplace=True)\ntrain_t = train_t.T\ntest_t = test.set_index('ID', inplace=False)\ntest_t = test_t.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bff3c959fb65db41fe110f665a05be3acb6d5c9"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"704d799304db1af1edacbde41b8eec83d784cb1d"},"cell_type":"code","source":"\nfeatures = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n        '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n        'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b',\n        '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212', '66ace2992',\n        'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd',\n        '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n        '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2', '0572565c2',\n        '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74cab0ba7cad2e9db8b879edb35816080929d064"},"cell_type":"code","source":"extra_features = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d22f34ebde894800dcb25a0e1b8a63384c2fd319"},"cell_type":"code","source":"#run this iteratively until you have no more links. Then prune\ndef chain_pairs(ordered_items):\n    ordered_chains = []\n    links_found = 0\n    for i_1, op_chain in enumerate(ordered_items.copy()[:]):\n        if op_chain[0] != op_chain[1]:\n            end_chain = op_chain[-1]\n            for i_2, op in enumerate(ordered_items.copy()[:]):\n                if (end_chain == op[0]):\n                    links_found += 1\n                    op_chain.extend(op[1:])\n                    end_chain = op_chain[-1]\n\n            ordered_chains.append(op_chain)\n    return links_found, ordered_chains\n\ndef prune_chain(ordered_chain):\n    \n    ordered_chain = sorted(ordered_chain, key=len, reverse=True)\n    new_chain = []\n    id_lookup = {}\n    for oc in ordered_chain:\n        id_already_in_chain = False\n        for idd in oc:\n            if idd in id_lookup:\n                id_already_in_chain = True\n            id_lookup[idd] = idd\n\n        if not id_already_in_chain:\n            new_chain.append(oc)\n    return sorted(new_chain, key=len, reverse=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d72f92792fc8c764904c8ec059cd98fa9c0d98bb"},"cell_type":"code","source":"def find_new_ordered_features(ordered_ids, data_t):\n    data = data_t.copy()\n    \n    f1 = ordered_ids[0][:-1]\n    f2 = ordered_ids[0][1:]\n    for ef in ordered_ids[1:]:\n        f1 += ef[:-1]\n        f2 += ef[1:]\n            \n    d1 = data[f1].apply(tuple, axis=1).apply(hash).to_frame().rename(columns={0: 'key'})\n    d1['ID'] = data.index\n    gc.collect()\n    d2 = data[f2].apply(tuple, axis=1).apply(hash).to_frame().rename(columns={0: 'key'})\n    d2['ID'] = data.index\n    gc.collect()\n    d3 = d2[~d2.duplicated(['key'], keep=False)]\n    d4 = d1[~d1.duplicated(['key'], keep=False)]\n    d5 = d4.merge(d3, how='inner', on='key')\n\n    d_feat = d1.merge(d5, how='left', on='key')\n    d_feat.fillna(0, inplace=True)\n\n    ordered_features = list(d_feat[['ID_x', 'ID_y']][d_feat.ID_x != 0].apply(list, axis=1))\n    del d1,d2,d3,d4,d5,d_feat\n    gc.collect()\n    \n    links_found = 1\n    #print(ordered_features)\n    while links_found > 0:\n        links_found, ordered_features = chain_pairs(ordered_features)\n        #print(links_found)\n    \n    ordered_features = prune_chain(ordered_features)\n    \n    #make lookup of all features found so far\n    found = {}\n    for ef in extra_features:\n        found[ef[0]] = ef\n        #print (ef[0])\n    found [features[0]] = features\n\n    #make list of sets of 40 that have not been found yet\n    new_feature_sets = []\n    for of in ordered_features:\n        if len(of) >= 40:\n            if of[0] not in found:\n                new_feature_sets.append(of)\n                \n    return new_feature_sets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09674a44baf766daa096506909de3586d4e2ab21"},"cell_type":"code","source":"\ndef add_new_feature_sets(data, data_t):\n    \n    print ('\\nData Shape:', data.shape)\n    f1 = features[:-1]\n    f2 = features[1:]\n\n    for ef in extra_features:\n        f1 += ef[:-1]\n        f2 += ef[1:]\n\n    d1 = data[f1].apply(tuple, axis=1).apply(hash).to_frame().rename(columns={0: 'key'})\n    d1['ID'] = data['ID']    \n    gc.collect()\n    d2 = data[f2].apply(tuple, axis=1).apply(hash).to_frame().rename(columns={0: 'key'})\n    d2['ID'] = data['ID']\n    gc.collect()\n    #print('here')\n    d3 = d2[~d2.duplicated(['key'], keep=False)]\n    del d2\n    d4 = d1[~d1.duplicated(['key'], keep=False)]\n    #print('here')\n    d5 = d4.merge(d3, how='inner', on='key')\n    del d4\n    d = d1.merge(d5, how='left', on='key')\n    d.fillna(0, inplace=True)\n    #print('here')\n    ordered_ids = list(d[['ID_x', 'ID_y']][d.ID_x != 0].apply(list, axis=1))\n    del d1,d3,d5,d\n    gc.collect()\n\n    links_found = 1\n    while links_found > 0:\n        links_found, ordered_ids = chain_pairs(ordered_ids)\n        #print(links_found)\n\n    print ('OrderedIds:', len(ordered_ids))\n    #Make distinct ordered id chains\n    ordered_ids = prune_chain(ordered_ids)\n    print ('OrderedIds Pruned:', len(ordered_ids))\n\n    #look for ordered features with new ordered id chains\n    new_feature_sets = find_new_ordered_features(ordered_ids, data_t)    \n\n    extra_features.extend(new_feature_sets)\n    print('New Feature Count:', len(new_feature_sets))\n    print('Extra Feature Count:', len(extra_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5472651784c152f47b86c4ee6456dc145b116d67"},"cell_type":"code","source":"%%time\n\nadd_new_feature_sets(train,train_t)\nadd_new_feature_sets(test,test_t)\nadd_new_feature_sets(train,train_t)\nadd_new_feature_sets(test,test_t)\nadd_new_feature_sets(train,train_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f70c8e23b44df08c09100d18d85d2a34527abe4"},"cell_type":"code","source":"with open(\"extra_features_{}.txt\".format(len(extra_features)), \"w\") as text_file:\n    for ef in extra_features:\n        text_file.write(','.join(ef) + '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62a096dd7e5b4918a327d70b5d9d90e30ee65a82"},"cell_type":"code","source":"del train_t, test_t, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7aaca7f3526d9ab4e6c932997294ad8065557ca"},"cell_type":"code","source":"#now that memory is cleared we can get back full test\ntest = pd.read_csv('../input/test.csv')\ntest['has_ugly'] = test[all_features].apply(has_ugly, axis=1)\ntest[test.has_ugly == True] = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60a96a890e6a1d16b402a780b1cbe841e7a6a605"},"cell_type":"markdown","source":"## This section uses the feature sets to exploit the leak and make a leak baseline to be used for ML training and submissions.\n\n## All of my submissions where based off setting end_offset to 40 instead of 39. This is optimal in train and public LB. But not in private LB. It is kind of intuitive that 40 is too far. I wish I had done more full ML pipeline testing with this solution. Maybe there was some information that could have saved me from this mistake."},{"metadata":{"trusted":true,"_uuid":"14cb31918a12d991f2cc82f9b601c8ab516838b9"},"cell_type":"code","source":"\ndef get_log_pred(data, feats, extra_feats, offset = 2):\n    f1 = feats[:(offset * -1)]\n    f2 = feats[offset:]\n    for ef in extra_feats:\n        f1 += ef[:(offset * -1)]\n        f2 += ef[offset:]\n        \n    d1 = data[f1].apply(tuple, axis=1).apply(hash).to_frame().rename(columns={0: 'key'})\n    d2 = data[f2].apply(tuple, axis=1).apply(hash).to_frame().rename(columns={0: 'key'})\n    d2['pred'] = data[feats[offset-2]]\n    d2 = d2[d2['pred'] != 0] # Keep?\n    d3 = d2[~d2.duplicated(['key'], keep=False)]\n    d4 = d1[~d1.duplicated(['key'], keep=False)]\n    d5 = d4.merge(d3, how='inner', on='key')\n        \n    d = d1.merge(d5, how='left', on='key')\n    return np.log1p(d.pred).fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6d1b6310d78d9439314fc78b41c36fcc63cc60f"},"cell_type":"code","source":"end_offset = 39\npred_test = []\npred_train = []\nefs = extra_features\nfor o in tqdm_notebook(list(range(2, end_offset))):\n    print('Offset:', o)\n\n    log_pred = get_log_pred(train, features, extra_features, o)\n    pred_train.append(np.expm1(log_pred))\n    have_data = log_pred != 0\n    train_count = have_data.sum()\n    score = sqrt(mean_squared_error(np.log1p(train.target[have_data]), log_pred[have_data]))\n    print(f'Score = {score} on {have_data.sum()} out of {train.shape[0]} training samples')\n\n\n    log_pred_test = get_log_pred(test, features, efs, o)\n    pred_test.append(np.expm1(log_pred_test))\n    have_data = log_pred_test != 0\n    test_count = have_data.sum()\n    print(f'Have predictions for {have_data.sum()} out of {test.shape[0]} test samples')\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"234f927c86330623ee4cfef7c1c839dcf8835337"},"cell_type":"code","source":"pred_train_final = pred_train[0].copy()\nfor r in range(1,len(pred_train)):\n    pred_train_final[pred_train_final == 0] = pred_train[r][pred_train_final == 0]\n\ntrain_leak_match_count = (pred_train_final!=0).sum();\nno_match_count = (pred_train_final==0).sum();\nprint (\"Train leak count: \", train_leak_match_count, \"Train no leak count: \",  no_match_count)\n\npred_train_temp = pred_train_final.copy()\ntrain[\"nonzero_mean\"] = train[[f for f in train.columns if f not in [\"ID\", \"target\",\"nonzero_mean\"]]].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\npred_train_temp[pred_train_temp==0] = train['nonzero_mean'][pred_train_temp==0]\nprint(f'Baseline Train Score = {sqrt(mean_squared_error(np.log1p(train.target), np.log1p(pred_train_temp)))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d510ca1f506680b6092a710ec96486acce1ce00"},"cell_type":"code","source":"pred_test_final = pred_test[0].copy()\nfor r in range(1,len(pred_test)):\n    pred_test_final[pred_test_final == 0] = pred_test[r][pred_test_final == 0]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce2c681f6536754bf87e33bde1b33d25d3355144"},"cell_type":"code","source":"##https://www.kaggle.com/rsakata/21st-place-solution-bug-fixed-private-0-52785\npred_test_final[(4e+07 < pred_test_final)] = 4e+07\npred_test_final[((pred_test_final < 29000) & (pred_test_final > 0))] = 30000\n##https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/63931\npred_test_final[test.ID == 'd72fad286'] = 1560000\npred_test_final[test.ID == 'a304cde42'] = 320000.0\n\ntest_leak_match_count = (pred_test_final!=0).sum();\nno_match_count = (pred_test_final==0).sum();\nprint (\"Test leak count: \", test_leak_match_count, \"Test no leak count: \",  no_match_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"664e495cf6496fdc3500fb31d8dccad920cb150a"},"cell_type":"code","source":"##Make Leak Baseline\npred_test_temp = pred_test_final.copy()\ntest_og[\"nonzero_mean\"] = test_og[[f for f in test_og.columns if f not in [\"ID\", \"target\", \"nonzero_mean\", \"has_ugly\"]]].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\npred_test_temp[pred_test_temp==0] = test_og['nonzero_mean'][pred_test_temp==0]\ntest_og['target']=pred_test_temp\ntest_og[['ID', 'target']].to_csv('leak_baseline_{}.csv'.format(test_leak_match_count), index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"057c3b0f4ab8e525a5fd0e4936742d7af3ac5e27"},"cell_type":"code","source":"\ntest_leaks = pd.read_csv('../input/sample_submission.csv')\ndel test_leaks['target']\ntest_leaks['target']=pred_test_final\ntest_leaks.to_csv('leak_only_{}.csv'.format(test_leak_match_count), index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b9091a832509d58aa6429fa48e8a0d3712e4c52"},"cell_type":"markdown","source":"## This section makes aggregate features from the 40 length feature sets\n"},{"metadata":{"trusted":true,"_uuid":"33e3655640badab1adacc694b53f6a2160482246"},"cell_type":"code","source":"extra_features_list = []\n\nfor ef in extra_features:\n    extra_features_list.extend(ef)\n\nextra_features_list.extend(features)\nlen(extra_features_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da8869f64feb87e331fa012c22784dbfb6967b16"},"cell_type":"code","source":"#This makes the 100 40 length feature groups into 40 100 length feature groups. \n#These 100 size vectors is what I would have liked to feed into an LSTM\\CNN but I never got a chance to try this\nfeats = pd.DataFrame(extra_features) \ntime_features = []\nfor c in feats.columns[:]:    \n    time_features.append([f for f in feats[c].values if f is not None])\n    \n\n#Make a bunch of different feature groups to build aggregates from\nagg_features = []\nall_cols = train.columns.drop(['ID', 'target', 'nonzero_mean'])\nagg_features.append(all_cols)\nagg_features.append([c for c in all_cols if c not in extra_features_list])\nagg_features.append(extra_features_list)\nagg_features.extend(time_features)\nagg_features.extend(extra_features)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d74cc9cc7a592d01d07d5d3b4437c06ac20df33"},"cell_type":"code","source":"#I made more aggregate feature to select from in model\\feature selection. \n#See this thread for some more aggregate ideas\n#https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/62446\n\ndef add_new_features(source, dest, feats):\n    #dest['high_{}_{}'.format(feats[0], len(feats))] = source[feats].max(axis=1)\n    #dest['mean_{}_{}'.format(feats[0], len(feats))] = source[feats].replace(0, np.nan).mean(axis=1)\n    #dest['low_{}_{}'.format(feats[0], len(feats))] = source[feats].replace(0, np.nan).min(axis=1)\n    #dest['median_{}_{}'.format(feats[0], len(feats))] = source[feats].replace(0, np.nan).median(axis=1)\n    #dest['sum_{}_{}'.format(feats[0], len(feats))] = source[feats].sum(axis=1)\n    #dest['stddev_{}_{}'.format(feats[0], len(feats))] = source[feats].std(axis=1)\n    \n    dest['mean_log_{}_{}'.format(feats[0], len(feats))] = np.log1p(source[feats].replace(0, np.nan).mean(axis=1))    \n    dest['first_nonZero_{}_{}'.format(feats[0], len(feats))] = np.log1p(source[feats].replace(0, np.nan).bfill(1).iloc[:, 0])\n    dest['last_nonZero_{}_{}'.format(feats[0], len(feats))] = np.log1p(source[feats[::-1]].replace(0, np.nan).bfill(1).iloc[:, 0])    \n    \n    #dest['nb_nans_{}_{}'.format(feats[0], len(feats))] =  source[feats].replace(0, np.nan).isnull().sum(axis=1)\n    #dest['unique_{}_{}'.format(feats[0], len(feats))] = source[feats].nunique(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4214a246a3297bc36c66120f1e3a6f08a3d77d7e"},"cell_type":"code","source":"#now that leak is done we should get back ugly data for feature engineering. This might not be necessary.\ndel test\ngc.collect\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47292d99be739080dfe92acd54a11cd52ab1ad99"},"cell_type":"code","source":"train_feats = pd.DataFrame()\ntest_feats =pd.DataFrame()\n\nfor i, ef in tqdm_notebook(list(enumerate(agg_features))):        \n    add_new_features(train, train_feats, ef)\n    add_new_features(test, test_feats, ef)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdb397f5ad1367196e7c79b51a51bf741b01e544"},"cell_type":"markdown","source":"## This section runs a single LGB model on all aggregate features. \n\n## My final score was based on blended feature selection from aggregates and raw features only.\n"},{"metadata":{"trusted":true,"_uuid":"f8425c7c8e6eeca5961bd063f8cd99a2c7ac0078"},"cell_type":"code","source":"# I made a general model runner but in this kernel it is hard coded to just the LGB class. \n# I left some of the generalness in the kernel incase it is of interest\n\nclass MyModel():\n    def __init__(self, X_tr, y_tr, X_val, y_val, X_test):\n        self.X_tr = X_tr\n        self.y_tr = y_tr\n        self.X_val = X_val\n        self.y_val = y_val\n        self.X_test = X_test\n        self.params = {}\n    def predict_val(self):\n        return self.model.predict(self.X_val)\n    def predict_test(self):\n        return self.model.predict(self.X_test)\n    \nclass LgbBoostModel(MyModel):\n    def train(self):          \n        \n        \n        self.params = { 'objective': 'regression', 'metric': 'rmse', 'boosting': 'gbdt', 'seed':seed, 'is_training_metric': True\n                  ,'max_bin': 350 #,'max_bin': 150\n                  ,'learning_rate': .005\n                  ,'max_depth': -1                  \n                  ,'num_leaves': 48\n                  ,'feature_fraction': 0.1\n                  ,'reg_alpha': 0\n                  ,'reg_lambda': 0.2\n                  ,'min_child_weight': 10}\n        \n        self.model = lgb.train(self.params, lgb.Dataset(self.X_tr, label=self.y_tr), 30000, \n                            [lgb.Dataset(self.X_tr, label=self.y_tr), lgb.Dataset(self.X_val, label=self.y_val)], \n                               verbose_eval=200, early_stopping_rounds=200)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"438bf6e52400fedefe6137a12b812524f5d8a6f1"},"cell_type":"code","source":"#Make training data from the original training plus the leak. This is key to getting a good score.\ncols = train_feats.columns\ntrain_feat_final = pd.concat([train_feats[cols], test_feats[cols][test_leaks.target != 0]], axis = 0)\ntrain_feat_id = pd.concat([train['ID'], test['ID'][test_leaks.target != 0]], axis = 0)\ntest_feat_final = test_feats[cols]    \ny = np.array(list(np.log1p(train.target.values)) + list(np.log1p(test_leaks['target'][test_leaks.target != 0])))\n\nX = train_feat_final.values\nX_test = test_feat_final.values\n\nprint(X.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"347c7c329afa02c9b5e71120d979d7f6570c7407"},"cell_type":"code","source":"n_splits = 5\nseed = 42\n\nkf = KFold(n_splits=n_splits, random_state=seed, shuffle=True)\n\nrmse_scores = {}\noof_preds = {}\nsub_preds = {}\nmodel_params = {}\n\nmodel_types = ['lgb']\n\nfor model_type in model_types:\n    rmse_scores[model_type] = list()\n    oof_preds[model_type] = np.zeros((X.shape[0],))\n    sub_preds[model_type] = np.zeros((X_test.shape[0],))\n\nprint('{} fold..'.format(n_splits))\n\nfor fold, (train_index, test_index) in tqdm_notebook(list(enumerate(list(kf.split(y))[:]))):\n\n    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_tr, X_val = X[train_index], X[test_index]\n    y_tr, y_val = y[train_index], y[test_index]\n\n    for model_type in model_types:\n        print ('\\n*** ' + model_type)\n        #model = get_model_class(model_type,  X_tr, y_tr, X_val, y_val, X_test)\n        model = LgbBoostModel(X_tr, y_tr, X_val, y_val, X_test)\n\n        model.train()\n\n        oof_preds[model_type][test_index] = model.predict_val()\n        sub_preds[model_type] += model.predict_test() / n_splits        \n        rmse = mean_squared_error(y_val, model.predict_val())**0.5\n        rmse_scores[model_type].append(rmse)\n\n        model.params['cv'] = n_splits\n        #model.params['fold_by_target'] = fold_by_target\n        model.params['seed'] = seed            \n        model_params[model_type] = model.params\n\n        print('Fold %d: %s Mean Squared Error %f'%(fold, model_type, rmse))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2ebd487ae4ea393ebc2938d168ca380f356440d"},"cell_type":"code","source":"def mean(values):\n    return float(sum(values)) / max(len(values), 1)\n\ndef sum_of_square_deviation(values, mean):\n    return float(1/len(values) * sum((x - mean)** 2 for x in values))    \n\ndef export_results(model_type):\n    subm = pd.read_csv('../input/sample_submission.csv')\n    subm['target'] = np.expm1(sub_preds[model_type])\n    \n    oof = pd.DataFrame(train_feat_id.copy())\n    oof['target'] = np.expm1(y)\n    oof['prediction'] = np.expm1(oof_preds[model_type])\n    mean_rmse = mean(rmse_scores[model_type])\n    standard_deviation_rmse = math.sqrt(sum_of_square_deviation(rmse_scores[model_type],mean_rmse))\n    \n    #key = '{}_{}_{}'.format(model_type, int(mean_rmse * 10000), int(standard_deviation_rmse * 10000))\n    key = '{}'.format(model_type)\n    print( '{} Mean Squared Error {}'.format(model_type ,mean_rmse))\n    print( '{} Stdev Squared Error {}'.format(model_type, standard_deviation_rmse))\n    \n    file_name = 'subm_{}_ml_base.csv'.format(key)                                 \n    subm.to_csv(file_name, index=False, float_format=\"%.8f\")\n    \n    #file_name = 'subm_{}_with_leaks.csv'.format(key)    \n    file_name = 'submission.csv'.format(key)    \n    subm['target'][test_leaks.target != 0] = test_leaks['target'][test_leaks.target != 0]\n    subm.to_csv(file_name, index=False, float_format=\"%.8f\")\n    \n    \n    file_name = 'subm_{}_oof.csv'.format(key)    \n    oof.to_csv(file_name, index=False, float_format=\"%.8f\")\n    model_params[model_type]['cv_score'] = int(mean_rmse * 10000)\n    model_params[model_type]['cv_stddev'] = int(standard_deviation_rmse * 10000)\n    model_params[model_type]['train_row_count'] = X.shape[0]\n    model_params[model_type]['train_feature_count'] = X.shape[1]\n    model_params[model_type]['test_leak_count'] = (test_leaks.target != 0).sum()\n    with open('subm_{}_params.txt'.format(key) , \"w\") as text_file:\n        params = str(model_params[model_type])\n        print(f\"{params}\", file=text_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3af683d675a8aa8fb2e56c77227c0f441d58ff61"},"cell_type":"code","source":"\nfor model_type in model_types:\n    export_results(model_type)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca80c8edbb34fa52feb57aa142e2d7744610f541"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"685bff17ed48d7bcc558d0001e34c5031b1cb113"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}