{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py"}}, "cells": [{"metadata": {"_cell_guid": "d6673536-6ee8-4f11-92ce-1c8e12202926", "_uuid": "44de0de7fe83e40bf36f5d67681a789485f84ba7"}, "cell_type": "code", "execution_count": null, "source": ["import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from nltk.corpus import stopwords\n", "import string\n", "from gensim.models import Word2Vec\n", "from gensim import corpora\n", "from gensim import models\n", "\n", "from gensim.matutils import jaccard, cossim\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.cross_validation import train_test_split\n", "import xgboost as xgb\n", "from sklearn.cross_validation import train_test_split\n", "import os.path\n", "\n", "\n", "import logging\n", "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n", "\n", "\n", "stop = stopwords.words('english')\n", "\n", "def preprocess(data):\n", "    return data.lower().split()\n", "\n", "train = pd.read_csv(\"../input/train.csv\", keep_default_na='')\n", "test = pd.read_csv(\"../input/test.csv\", keep_default_na='')\n", "\n", "combine = [train, test]\n", "\n", "print(\"Preprocessing questions...\")    \n", "for dataset in combine:\n", "    dataset['processed_q1'] = dataset['question1'].apply(preprocess)\n", "    dataset['processed_q2'] = dataset['question2'].apply(preprocess)\n", "\n", "\n", "all_questions = train['processed_q1'] + train['processed_q1'] + test['processed_q1'] + test['processed_q2']\n", "all_questions = all_questions[all_questions.notnull()]\n", "\n", "dict_file = 'quora.dict'\n", "corpus_file = 'corpus.mm'\n", "word_vec_file = 'word2vec.model'\n", "\n", "\n", "if os.path.isfile(dict_file):\n", "    print(\"Loading dictionary file\")\n", "    dictionary = corpora.Dictionary.load(dict_file)\n", "else:    \n", "    print(\"Generating dictionary file\")\n", "    dictionary = corpora.Dictionary(all_questions)\n", "    dictionary.save(dict_file)\n", "\n", "if os.path.isfile(corpus_file):\n", "    print(\"Loading corpus file\")\n", "    corpus = corpora.MmCorpus(corpus_file)\n", "else:    \n", "    print(\"Generating corpus file\")\n", "    corpus = [dictionary.doc2bow(question) for question in all_questions]\n", "    corpora.MmCorpus.serialize(corpus_file, corpus) \n", "\n", "if os.path.isfile(word_vec_file):\n", "    print(\"Loading word 2 vect file\")\n", "    w2v_model = Word2Vec.load(word_vec_file)\n", "else:\n", "    print(\"Generating word 2 vect file\")\n", "    w2v_model = Word2Vec(all_questions, min_count=10)\n", "    w2v_model.save(word_vec_file)\n", "    \n", "def to_bow(doc):\n", "    return dictionary.doc2bow(doc)\n", "\n", "def filter_in_vacob(doc):\n", "    return list(filter(lambda x: x in w2v_model, doc))\n", "\n", "print(\"Computing similarity metrics\")\n", "for dataset in combine:\n", "    dataset['jaccard'] = dataset.apply(lambda d: jaccard(to_bow(d['processed_q1']), to_bow(d['processed_q2'])), axis=1)\n", "    dataset['cosine'] = dataset.apply(lambda d: cossim(to_bow(d['processed_q1']), to_bow(d['processed_q2'])), axis=1)\n", "    dataset['wv_doc_1'] = dataset['processed_q1'].apply(lambda d:filter_in_vacob(d))\n", "    dataset['wv_doc_2'] = dataset['processed_q2'].apply(lambda d:filter_in_vacob(d))\n", "    dataset['wv_sim'] = dataset.apply(lambda d: w2v_model.wv.n_similarity(d['wv_doc_1'], d['wv_doc_2']) if d['wv_doc_1'] and d['wv_doc_2'] else -1, axis=1)\n", "\n", "    \n", "x_train = pd.DataFrame()\n", "x_test = pd.DataFrame()\n", "\n", "x_train['jaccard'] = train['jaccard']\n", "x_train['cosine'] = train['cosine']\n", "x_train['wv_sim'] = train['wv_sim']\n", "\n", "x_test['jaccard'] = test['jaccard']\n", "x_test['cosine'] = test['cosine']\n", "x_test['wv_sim'] = test['wv_sim']\n", "\n", "y_train = train['is_duplicate']\n", "\n", "\n", "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n", "    "], "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["params = {}\n", "params['objective'] = 'binary:logistic'\n", "params['eval_metric'] = 'logloss'\n", "params['eta'] = 0.02\n", "params['max_depth'] = 4\n", "\n", "d_train = xgb.DMatrix(x_train, label=y_train)\n", "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n", "\n", "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "\n", "bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, verbose_eval=10)\n", "d_test = xgb.DMatrix(x_test)\n", "p_test = bst.predict(d_test)\n", "\n", "sub = pd.DataFrame()\n", "sub['test_id'] = test['test_id']\n", "sub['is_duplicate'] = p_test\n", "sub.to_csv('simple_xgb.csv', index=False)"], "outputs": []}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["from sklearn.ensemble import RandomForestClassifier\n", "random_forest = RandomForestClassifier(n_estimators=100)\n", "random_forest.fit(x_train, y_train)\n", "y_pred = random_forest.predict_proba(x_test)\n", "print(random_forest.score(x_train, y_train))\n", "\n", "y_pred_df = pd.DataFrame({'0':y_pred[:,0],'1':y_pred[:,1]})\n", "sub = pd.DataFrame()\n", "sub['test_id'] = test['test_id']\n", "sub['is_duplicate'] = y_pred_df['1']\n", "sub.to_csv('random_forest_probability.csv', index=False)"], "outputs": []}], "nbformat": 4, "nbformat_minor": 1}