{"cells":[{"metadata":{"_uuid":"8b6a599dc516a7f9eb31ca005982c844477efbf9"},"cell_type":"markdown","source":"### PyTorch - Clean baseline\n\nThis kernel has been inspired by notebook [https://www.kaggle.com/dremovd/goto-pytorch-baseline](GoTo - PyTorch - Baseline), from which model definition and training function was taken.\n\nThis should serve as an introduction to Pytorch, with workflow example starting with data loading and processing, it's preparation for training, model definition, model optimization and submission generation from test set predictions. \n\nPytorch is a bit different than Keras and requires a bit more low-level approach. This comes with both advantages and disadvantages. While model definition and training/prediction itself require more work, Pytorch is usually faster and has a very useful `Dataset` definition, which easily enables changing between working with in-memory data to loading from disk. Plus the way of data processing is kept in one place, so it's usually easier to spot mistakes :)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import gc\nimport glob\nimport os\nfrom pathlib import Path\n\nimport cv2\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nimport tqdm\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import data\nfrom torchvision import models\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (14, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eed69f27d93b30a0a809e1cac5de58cec7dbcc2b"},"cell_type":"markdown","source":"### For current torch version: >0.4\n\nWhen preparing model definition, if some of the blocks will be used multiple times, they can be wrapped as a function or class and then the those itself will be called. This approach saves time and space. \n\nIf a basic 2D Convolution with kernel size (3x3) will serve as a building block of bigger blocks, it can be elegantly wrapped as `conv3x3`. \n\nIn a similar way, if ReLU will be default activation used after Convolutions, another block can be created - `ConvRelu`. UNet models are based on encoder and decoder architecture, where a chosen network architecture (like VGG, Inception or ResNet) acts as an encoder, compressing representation and extracting features from the input and the other part, decoder, upsamples the encoder output trying to reconstruct the original representation. This is why in `DecoderBlock` ConvTranspose2d is used, which is a transposed convolution, causing the dimensionality of output to grow.\n\nUNets contain skip connections, where features of the same dimensionality are concatenated from the encoder and decoder architecture part, improving learned representation. [Original UNet paper](https://arxiv.org/pdf/1505.04597.pdf)."},{"metadata":{"_uuid":"eb0ca26b30e98a6d73c479ec7b15693d10b83c31"},"cell_type":"markdown","source":"### Model definition:"},{"metadata":{"trusted":true,"_uuid":"aeeac431142114aef0e4aec1dbff8cd1dfb7cfbe","collapsed":true},"cell_type":"code","source":"# Basic 2D Convolution with 3x3 kernel\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\n\n# conv3x3 with ReLU activation afterwards\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super().__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\n\n# Decoder block containing 2D transposed Convolution upsampling the features\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            ConvRelu(in_channels, middle_channels),\n            nn.ConvTranspose2d(middle_channels, out_channels,\n                               kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\n# UNet architecture based on VGG11\nclass UNet11(nn.Module):\n    def __init__(self, num_filters=32):\n        \"\"\"\n        :param num_filters:\n        \"\"\"\n        super().__init__()\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # VGG11 encoder\n        self.encoder = models.vgg11().features\n\n        # Encoder part\n        self.relu = self.encoder[1]\n\n        self.conv1 = self.encoder[0]\n        self.conv2 = self.encoder[3]\n        self.conv3s = self.encoder[6]\n        self.conv3 = self.encoder[8]\n        self.conv4s = self.encoder[11]\n        self.conv4 = self.encoder[13]\n        self.conv5s = self.encoder[16]\n        self.conv5 = self.encoder[18]\n\n        # Decoder part\n        self.center = DecoderBlock(\n            num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n        \n        self.dec5 = DecoderBlock(\n            num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n        self.dec4 = DecoderBlock(\n            num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n        self.dec3 = DecoderBlock(\n            num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n        self.dec2 = DecoderBlock(\n            num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n\n        # Output layer\n        self.final = nn.Conv2d(num_filters, 1, kernel_size=1)\n\n    def forward(self, x):\n        conv1 = self.relu(self.conv1(x))\n        conv2 = self.relu(self.conv2(self.pool(conv1)))\n        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n        conv3 = self.relu(self.conv3(conv3s))\n        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n        conv4 = self.relu(self.conv4(conv4s))\n        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n        conv5 = self.relu(self.conv5(conv5s))\n\n        center = self.center(self.pool(conv5))\n\n        # Deconvolutions with copies of VGG11 layers of corresponding size\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n        \n        # Sigmoid over final convolution map is needed for Binary Crossentropy loss\n        output = F.sigmoid(self.final(dec1))\n        \n        return output\n\n\ndef get_model(params):\n    model = UNet11(**params)\n    model.train()  # set model for training\n    return model.to(device)  # put model on selected device, CPU ('cpu') or GPU ('cuda')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e70afccc1f03b6b8c5aa9feb5fd92542cda717ce"},"cell_type":"markdown","source":"### Dataset definition:\n\nTo change method of data loading or processing, it is enough to change the main` __get_item__()` function and the helper function `__load_image()` (this one is not obligatory, although it helps to keep the main` __get_item__()` clean. "},{"metadata":{"trusted":true,"_uuid":"a7a5110a26151b9a4f789437fbf14e29fb472cba","collapsed":true},"cell_type":"code","source":"class TGSSaltDataset(data.Dataset):\n\n    def __init__(self,\n                 root_path,\n                 file_list,\n                 is_test=False,\n                 divide=False,\n                 image_size=(128, 128)):\n\n        self.root_path = root_path\n        self.file_list = file_list\n        self.is_test = is_test\n\n        self.divide = divide\n        self.image_size = image_size\n\n        self.orig_image_size = (101, 101)\n        self.padding_pixels = None\n        \n        \"\"\"\n        root_path: folder specifying files location\n        file_list: list of images IDs\n        is_test: whether train or test data is used (contains masks or not)\n        \n        divide: whether to divide by 255\n        image_size: output image size, should be divisible by 32\n        \n        orig_image_size: original images size\n        padding_pixels: placeholder for list of padding dimensions\n        \"\"\"\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        if index not in range(0, len(self.file_list)):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n\n        file_id = self.file_list[index]\n\n        # Get image path\n        image_folder = os.path.join(self.root_path, 'images')\n        image_path = os.path.join(image_folder, file_id + '.png')\n    \n        # Get mask path\n        mask_folder = os.path.join(self.root_path, 'masks')\n        mask_path = os.path.join(mask_folder, file_id + '.png')\n\n        # Load image\n        image = self.__load_image(image_path)\n        if not self.is_test:\n            # Load mask for training or evaluation\n            mask = self.__load_image(mask_path, mask=True)\n            if self.divide:\n                image = image / 255.\n                mask = mask / 255.\n            # Transform into torch float Tensors of shape (CxHxW).\n            image = torch.from_numpy(\n                image).float().permute([2, 0, 1])\n            mask = torch.from_numpy(\n                np.expand_dims(mask, axis=-1)).float().permute([2, 0, 1])\n            return image, mask\n\n        if self.is_test:\n            if self.divide:\n                image = image / 255.\n            image = torch.from_numpy(image).float().permute([2, 0, 1])\n            return (image,)\n\n    def set_padding(self):\n\n        \"\"\"\n        Compute padding borders for images based on original and specified image size.\n        \"\"\"\n        \n        pad_floor = np.floor(\n            (np.asarray(self.image_size) - np.asarray(self.orig_image_size)) / 2)\n        pad_ceil = np.ceil((np.asarray(self.image_size) -\n                            np.asarray(self.orig_image_size)) / 2)\n\n        self.padding_pixels = np.asarray(\n            (pad_floor[0], pad_ceil[0], pad_floor[1], pad_ceil[1])).astype(np.int32)\n\n        return\n\n    def __pad_image(self, img):\n        \n        \"\"\"\n        Pad images according to border set in set_padding.\n        Original image is centered.\n        \"\"\"\n\n        y_min_pad, y_max_pad, x_min_pad, x_max_pad = self.padding_pixels[\n            0], self.padding_pixels[1], self.padding_pixels[2], self.padding_pixels[3]\n\n        img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad,\n                                 x_min_pad, x_max_pad,\n                                 cv2.BORDER_REPLICATE)\n\n        assert img.shape[:2] == self.image_size, '\\\n        Image after padding must have the same shape as input image.'\n\n        return img\n\n    def __load_image(self, path, mask=False):\n        \n        \"\"\"\n        Helper function for loading image.\n        If mask is loaded, it is loaded in grayscale (, 0) parameter.\n        \"\"\"\n\n        if mask:\n            img = cv2.imread(str(path), 0)\n        else:\n            img = cv2.imread(str(path))\n\n        height, width = img.shape[0], img.shape[1]\n\n        img = self.__pad_image(img)\n\n        return img\n\n    def return_padding_borders(self):\n        \"\"\"\n        Return padding borders to easily crop the images.\n        \"\"\"\n        return self.padding_pixels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df2f73c9fb0f66d74c9c36f25465dbb9a0b5a9d8"},"cell_type":"markdown","source":"## 1. Parameters:"},{"metadata":{"trusted":true,"_uuid":"1c49501d1595de527af31321f3cd9c20f8cd0550","collapsed":true},"cell_type":"code","source":"device = 'cuda:0'\ndata_src = '../input/'\n\nquick_try = False\ngrayscale = False\n\norig_image_size = (101, 101)\nimage_size = (128, 128)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"846615b33f335a2cf46bf06133af228e171cf0e2"},"cell_type":"markdown","source":"## 2. Initialize train and test DataFrames to access IDs and depth information."},{"metadata":{"trusted":true,"_uuid":"e6cbee784f2af28d5b1a750ebc46f535ace902a4","collapsed":true},"cell_type":"code","source":"print('Initialize.')\n\ntrain_df = pd.read_csv('{}train.csv'.format(data_src),\n                       usecols=[0], index_col='id')\ndepths_df = pd.read_csv('{}depths.csv'.format(data_src),\n                        index_col='id')\n\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2c7276e4fc7013e6f07efcf00f1a0c2e2998a01"},"cell_type":"markdown","source":"### Helper: Load training images and masks and compute coverage based on them.\n\nMasks are needed for computing the coverage."},{"metadata":{"trusted":true,"_uuid":"020c405d8b9b58462cb861b6649e825ca309d060","collapsed":true},"cell_type":"code","source":"X_train = []\ny_train = []\n\nprint('Loading training set.')\nfor i in tqdm.tqdm(train_df.index):\n    img_src = '{}train/images/{}.png'.format(data_src, i)\n    mask_src = '{}train/masks/{}.png'.format(data_src, i)\n    if grayscale:\n        img_temp = cv2.imread(img_src, 0)\n    else:\n        img_temp = cv2.imread(img_src)\n    mask_temp = cv2.imread(mask_src, 0)\n    if orig_image_size != image_size:\n        img_temp = cv2.resize(img_temp, image_size)\n        mask_temp = cv2.resize(mask_temp, image_size)\n    X_train.append(img_temp)\n    y_train.append(mask_temp)\n\nX_train = np.asarray(X_train)\ny_train = np.asarray(y_train)\nif grayscale:\n    X_train = np.expand_dims(X_train, -1)\ny_train = np.expand_dims(y_train, -1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25334b22987512c5a65bab079bc77c1474dc669c"},"cell_type":"markdown","source":"### Compute coverage:"},{"metadata":{"trusted":true,"_uuid":"1f9d4f26ae2c144724e6ead29dd3f22e0b3b5200","collapsed":true},"cell_type":"code","source":"print('Compute mask coverage for each observation.')\n\ndef cov_to_class(val):\n    for i in range(0, 11):\n        if val * 10 <= i:\n            return i\n\n# Percent of area covered by mask.\ntrain_df['coverage'] = np.mean(y_train / 255., axis=(1, 2))\ntrain_df['coverage_class'] = train_df.coverage.map(\n    cov_to_class)\n\n\n# del X_train, y_train\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"344dbb56b68662c8df42913433848c0070c9d6c3","collapsed":true},"cell_type":"code","source":"plt.imshow(y_train[-2, :, :, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7b7ac275689d446822306c3b29b73ff7b078e25","collapsed":true},"cell_type":"code","source":"train_df.coverage_class","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e62887267c02c0bc5b0edbc5235db9806ac3d24"},"cell_type":"markdown","source":"## 3. Loading the data:"},{"metadata":{"_uuid":"ba82018034c27fede08c2aa98916e9648a0d2893"},"cell_type":"markdown","source":"### 3.1 Set data loading parameters:"},{"metadata":{"trusted":true,"_uuid":"91b462e4b84ff3c08d3e969457ed14af36ca2d3f","collapsed":true},"cell_type":"code","source":"train_path = data_src + 'train'\ntest_path = data_src\n\ntrain_ids = train_df.index.values\ntest_ids = test_df.index.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ab5c784c8a59d4fa80ca87e7992448443986b30"},"cell_type":"markdown","source":"### 3.2 Perform stratified train/valid split based on coverage class:"},{"metadata":{"trusted":true,"_uuid":"403f9a0f9f92453a75b3e92b6f60693e80581452","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntr_ids, valid_ids, tr_coverage, valid_coverage = train_test_split(\n    train_ids,\n    train_df.coverage.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state= 1234)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2e9b4869414fec13c108009160ddc263120ded8"},"cell_type":"markdown","source":"### 3.3 Define data loading:"},{"metadata":{"trusted":true,"_uuid":"597323c14ad0e3f30ec685ea70970df317e4cdf9","collapsed":true},"cell_type":"code","source":"# Training dataset:\ndataset_train = TGSSaltDataset(train_path, tr_ids, divide=True)\ndataset_train.set_padding()\ny_min_pad, y_max_pad, x_min_pad, x_max_pad = dataset_train.return_padding_borders()\n        \n# Validation dataset:\ndataset_val = TGSSaltDataset(train_path, valid_ids, divide=True)\ndataset_val.set_padding()\n\n# Test dataset:\ndataset_test = TGSSaltDataset(test_path, test_ids, is_test=True, divide=True)\ndataset_test.set_padding()\n\n\n# Data loaders:\n# Use multiple workers to optimize data loading speed.\n# Pin memory for quicker GPU processing.\ntrain_loader = data.DataLoader(\n    dataset_train,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True)\n\n# Do not shuffle for validation and test.\nvalid_loader = data.DataLoader(\n    dataset_val,\n    batch_size=32,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True)\n\ntest_loader = data.DataLoader(\n    dataset_test,\n    batch_size=32,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2be03674c702134bc0682d61d5aab5f457237198"},"cell_type":"markdown","source":"## 4. Training:"},{"metadata":{"trusted":true,"_uuid":"6bd960ee82b2aeaa6d8dc92b8fd364e1730af851","collapsed":true},"cell_type":"code","source":"# Get defined UNet model.\nmodel = get_model({'num_filters': 32})\n# Set Binary Crossentropy as loss function.\nloss_fn = torch.nn.BCELoss()\n\n# Set optimizer.\nlearning_rate = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n# Train for n epochs\nn = 2\nfor e in range(n):\n\n    # Training:\n    train_loss = []\n    for image, mask in tqdm.tqdm(train_loader):\n\n        # Put image on chosen device\n        image = image.type(torch.float).to(device)\n        # Predict with model:\n        y_pred = model(image)\n        # Compute loss between true and predicted values\n        loss = loss_fn(y_pred, mask.to(device))\n\n        # Set model gradients to zero.\n        optimizer.zero_grad()\n        # Backpropagate the loss.\n        loss.backward()\n\n        # Perform single optimization step - parameter update\n        optimizer.step()\n        \n        # Append training loss\n        train_loss.append(loss.item())\n\n    # Validation:\n    val_loss = []\n    val_iou = []\n    for image, mask in valid_loader:\n        \n        image = image.to(device)\n        y_pred = model(image)\n        \n        loss = loss_fn(y_pred, mask.to(device))\n        val_loss.append(loss.item())\n\n    print(\"Epoch: %d, Train: %.3f, Val: %.3f\" %\n          (e, np.mean(train_loss), np.mean(val_loss)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db2ff1ecb902074d862866cf10a38efaa5bfdc57"},"cell_type":"markdown","source":"## 5. Validation prediction:"},{"metadata":{"trusted":true,"_uuid":"2df10508d5ab4a15e3c71f7faa6cf68d87fe3050","collapsed":true},"cell_type":"code","source":"val_predictions = []\nval_masks = []\n\nfor image, mask in tqdm.tqdm(valid_loader):\n    image = image.type(torch.float).to(device)\n    # Put prediction on CPU, detach it and transform to a numpy array.\n    y_pred = model(image).cpu().detach().numpy()\n    val_predictions.append(y_pred)\n    val_masks.append(mask)\n\n\n# Stack all masks and predictions along first axis.\n# Output of valid_loader is of shape (NxBxCxHxW), where N is number of batches and B is batch size.\nval_predictions_stacked = np.vstack(val_predictions)[:, 0, :, :]\nval_masks_stacked = np.vstack(val_masks)[:, 0, :, :]\n\n\n# Cut off padded parts of images.\nval_predictions_stacked = val_predictions_stacked[\n    :, y_min_pad:-y_max_pad, x_min_pad:-x_max_pad]\n\nval_masks_stacked = val_masks_stacked[\n    :, y_min_pad:-y_max_pad, x_min_pad:-x_max_pad]\n\nprint(val_masks_stacked.shape, val_predictions_stacked.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"331c71f1372285935f175f3d8a70d5fbc501602c"},"cell_type":"markdown","source":"### Perform check on randomly chosen mask and prediction:"},{"metadata":{"trusted":true,"_uuid":"22fa8123a40841b5eccc9a2c92e745f27698721f","collapsed":true},"cell_type":"code","source":"random_index = np.random.randint(0, val_masks_stacked.shape[0])\nprint('Validation Index: {}'.format(random_index))\n\nfig, ax = plt.subplots(2, 1)\nax[0].imshow(val_masks_stacked[random_index], cmap='seismic')\nax[1].imshow(val_predictions_stacked[random_index] > 0.5, cmap='seismic')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c74eb0faa4f7eabc76e692741f874006de3681de"},"cell_type":"markdown","source":"## 6. Test prediction:"},{"metadata":{"trusted":true,"_uuid":"8640caeed5892ce492f0af5822c9c2c10892ab15","collapsed":true},"cell_type":"code","source":"test_predictions = []\n\nfor image in tqdm.tqdm(test_loader):\n    image = image[0].type(torch.float).to(device)\n    y_pred = model(image).cpu().detach().numpy()\n    test_predictions.append(y_pred)\n\n    \ntest_predictions_stacked = np.vstack(test_predictions)[:, 0, :, :]\ntest_predictions_stacked = test_predictions_stacked[:, y_min_pad:-y_max_pad, x_min_pad:-x_max_pad]\n\nprint(test_predictions_stacked.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab7f7f09c8772c186d4742d7325af80da52deaea"},"cell_type":"markdown","source":"## 7. Submission preparation:"},{"metadata":{"_uuid":"ea4a6a46fdb28ed0dbc8b76867ddf9958f98d6c5"},"cell_type":"markdown","source":"### 7.1 Run length encoding to reduce predictions size\n\n[Description on Kaggle](https://www.kaggle.com/c/tgs-salt-identification-challenge#evaluation)"},{"metadata":{"trusted":true,"_uuid":"ba7feff28e00ba711f03a4f22440308b20a1c2d5","collapsed":true},"cell_type":"code","source":"def rle_encode(im):\n    pixels = im.flatten(order='F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\n# To perform RLE, predictions must be in binary integer (0/1) format.\nbinary_prediction = (test_predictions_stacked > 0.5).astype(int)\n\n# RLE encoding.\nall_masks = {idx:rle_encode(binary_prediction[i])\n                           for i, idx in enumerate(\n                               tqdm.tqdm(test_ids))}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01b13810cf95e185436b0fc62201bcc5976aa002"},"cell_type":"markdown","source":"### 7.2 Submission output:"},{"metadata":{"trusted":true,"_uuid":"bd6125d9a38a983cb22fecc023eac14530b48901","collapsed":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict(all_masks, orient='index')\nsubmission.index.names = ['id']\nsubmission.columns = ['rle_mask']\nsubmission.to_csv('submission.csv')  # 12 epochs score 0.673","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"870be6856c9ffa5cea11de0f24e22da15fedfa84"},"cell_type":"markdown","source":"## Further ideas for improvement:\n\n- Model architecture optimization\n- Training parameters optimization\n- Confidence threshold optimization\n- KFold split\n- Data augmentation"},{"metadata":{"trusted":true,"_uuid":"b4f511286f36fe7f13501dee93ed5884deeb2173","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}