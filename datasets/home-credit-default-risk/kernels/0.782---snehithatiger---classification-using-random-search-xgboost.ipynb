{"cells":[{"metadata":{"_uuid":"c02a14c73390dc52f601eabb60ba8042257036d2","trusted":true},"cell_type":"code","source":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Modeling\nimport lightgbm as lgb\n\n# Splitting data\nfrom sklearn.model_selection import train_test_split\n\nN_FOLDS = 5\nMAX_EVALS = 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a42d50c0cf134a1147c35e1aea539b12053d24c3","trusted":true},"cell_type":"code","source":"features = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\n\n# Sample 17000 rows (10000 for training, 7000 for testing)\nfeatures = features.sample(n = 17000, random_state = 42)\n\n# Only numeric features\nfeatures = features.select_dtypes('number')\n\n# Extract the labels\nlabels = np.array(features['TARGET'].astype(np.int32)).reshape((-1, ))\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n\n# Split into training and testing data\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 7000, random_state = 37)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecadc412aea2eba03da9349e07973f1a872ae3a0","trusted":true},"cell_type":"code","source":"print(\"Training features shape: \", train_features.shape)\nprint(\"Testing features shape: \", test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"130ac2f4bf4f58817f6c25ff88a5c19d60bbe712","trusted":true},"cell_type":"code","source":"# Create a training and testing dataset\ntrain_set = lgb.Dataset(data = train_features, label = train_labels)\ntest_set = lgb.Dataset(data = test_features, label = test_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"704dce8d25515252ff5510f319eae1bb021703be","trusted":true},"cell_type":"code","source":"# Get default hyperparameters\nmodel = lgb.LGBMClassifier()\ndefault_params = model.get_params()\n\n# Remove the number of estimators because we set this to 10000 in the cv call\ndel default_params['n_estimators']\n\n# Cross validation with early stopping\ncv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = N_FOLDS, seed = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4f547977aa72be4bd3d8f364aea10e0eff987d4","trusted":true},"cell_type":"code","source":"print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\nprint('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d59ad26529d2f5dbd1377fd79741678cd2cf6a4","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"553e5514ef458b0db032a21a2bf89e965bf7214c","trusted":true},"cell_type":"code","source":"# Optimal number of esimators found in cv\nmodel.n_estimators = len(cv_results['auc-mean'])\n\n# Train and make predicions with model\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\n\nprint('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d904f4e3ffb10d2a9b1748fb0c236101486ede0","trusted":true},"cell_type":"code","source":"def objective(hyperparameters, iteration):\n    \"\"\"Objective function for grid and random search. Returns\n       the cross validation score from a set of hyperparameters.\"\"\"\n    \n    # Number of estimators will be found using early stopping\n    if 'n_estimators' in hyperparameters.keys():\n        del hyperparameters['n_estimators']\n    \n     # Perform n_folds cross validation\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n    \n    # results to retun\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators \n    \n    return [score, hyperparameters, iteration]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1944462565245ef7399a4905f5319b0341ada833","trusted":true},"cell_type":"code","source":"\nscore, params, iteration = objective(default_params, 1)\n\nprint('The cross-validation ROC AUC was {:.5f}.'.format(score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02a083d4b41697c47dcb21c48bc543c05cb4a4e8","trusted":true},"cell_type":"code","source":"# Create a default model\nmodel = lgb.LGBMModel()\nmodel.get_params()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b91a772cb5b4dba4bc383d396826c97ed99b3943","trusted":true},"cell_type":"code","source":"# Hyperparameter grid\nparam_grid = {\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(20, 150)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n    'is_unbalance': [True, False]\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f1839c3d16fb4b7a93fd732954ccd78058f8062","trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(50)\n\n# Randomly sample a boosting type\nboosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n\n# Set subsample depending on boosting type\nsubsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n\nprint('Boosting type: ', boosting_type)\nprint('Subsample ratio: ', subsample)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a54d3b3af9f0635f26ebc310134706e3e0b25b25","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Learning rate histogram\nplt.hist(param_grid['learning_rate'], bins = 20, color = 'r', edgecolor = 'k');\nplt.xlabel('Learning Rate', size = 14); plt.ylabel('Count', size = 14); plt.title('Learning Rate Distribution', size = 18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e15c0314dc15ddcb5376dac391c31108cef56f76"},"cell_type":"code","source":"a = 0\nb = 0\n\n# Check number of values in each category\nfor x in param_grid['learning_rate']:\n    # Check values\n    if x >= 0.005 and x < 0.05:\n        a += 1\n    elif x >= 0.05 and x < 0.5:\n        b += 1\n\nprint('There are {} values between 0.005 and 0.05'.format(a))\nprint('There are {} values between 0.05 and 0.5'.format(b))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a24c021c4770b44a88f371a6348a54b9fc4668ba"},"cell_type":"markdown","source":"As an example of a simple domain, the `num_leaves` is a uniform distribution. This means values are evenly spaced on a linear scale."},{"metadata":{"_uuid":"7be71b814abbfd25e4514e3de127b29a1f20b070","trusted":true},"cell_type":"code","source":"# number of leaves domain\nplt.hist(param_grid['num_leaves'], color = 'm', edgecolor = 'k')\nplt.xlabel('Learning Number of Leaves', size = 14); plt.ylabel('Count', size = 14); plt.title('Number of Leaves Distribution', size = 18);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb3d66b0d87aff8da433329682988d531415b5ee","trusted":true},"cell_type":"code","source":"# Dataframes for random and grid search\nrandom_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n\ngrid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59bfa6c52a24f259a75cfc616f1b6d1f360ecbbd","trusted":true},"cell_type":"code","source":"com = 1\nfor x in param_grid.values():\n    com *= len(x)\nprint('There are {} combinations'.format(com))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e3faab45df397ba9470f961d238e71e396d64ec","trusted":true},"cell_type":"code","source":"print('This would take {:.0f} years to finish.'.format((100 * com) / (60 * 60 * 24 * 365)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc053e847d75e461d52557aea11d062cedfddb61","trusted":true},"cell_type":"code","source":"import itertools\n\ndef grid_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n    \n    # Dataframe to store results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    \n    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n    keys, values = zip(*param_grid.items())\n    \n    i = 0\n    \n    # Iterate through every possible combination of hyperparameters\n    for v in itertools.product(*values):\n        \n        # Create a hyperparameter dictionary\n        hyperparameters = dict(zip(keys, v))\n        \n        # Set the subsample ratio accounting for boosting type\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        \n        # Evalute the hyperparameters\n        eval_results = objective(hyperparameters, i)\n        \n        results.loc[i, :] = eval_results\n        \n        i += 1\n        \n        # Normally would not limit iterations\n        if i > MAX_EVALS:\n            break\n       \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    \n    return results    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6672b3dd8c4047bc16a1bca70a3217bf3b761aa5","trusted":true},"cell_type":"code","source":"grid_results = grid_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(grid_results.loc[0, 'params'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bff132b94401f3bf83428b9b1af57c39e32486e4","trusted":true},"cell_type":"code","source":"# Get the best parameters\ngrid_search_params = grid_results.loc[0, 'params']\n\n# Create, train, test model\nmodel = lgb.LGBMClassifier(**grid_search_params, random_state=42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7ebf4e12d2c8359271fa185ba5d7236ded4ae08","trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth = 1000\ngrid_results['params'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5679964bab15104f395235d95fa0259baa01d488","trusted":true},"cell_type":"code","source":"random.seed(50)\n\n# Randomly sample from dictionary\nrandom_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n# Deal with subsample ratio\nrandom_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n\nrandom_params","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92287f56a80bd35a9cd8e31a91e417fcc344a97f","trusted":true},"cell_type":"code","source":"def random_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Random search for hyperparameter optimization\"\"\"\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                  index = list(range(MAX_EVALS)))\n    \n    # Keep searching until reach max evaluations\n    for i in range(MAX_EVALS):\n        \n        # Choose random hyperparameters\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n\n        # Evaluate randomly selected hyperparameters\n        eval_results = objective(hyperparameters, i)\n        \n        results.loc[i, :] = eval_results\n    \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    return results ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04a714aac0572eb2a7b718cfc518c3f199d3b368","trusted":true},"cell_type":"code","source":"random_results = random_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(random_results.loc[0, 'params'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41b7a4dc042776f7f28c0c8d371f927290e18cd0","trusted":true},"cell_type":"code","source":"# Get the best parameters\nrandom_search_params = random_results.loc[0, 'params']\n\n# Create, train, test model\nmodel = lgb.LGBMClassifier(**random_search_params, random_state = 42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3a337e380aea5c07c59663d170f4fced1ee29a2","trusted":true},"cell_type":"code","source":"random_results['params']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5c7d3bd7e7bb7383eace282fb0c3025112076c2","trusted":true},"cell_type":"code","source":"import csv\n\n# Create file and open connection\nout_file = 'random_search_trials.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write column names\nheaders = ['score', 'hyperparameters', 'iteration']\nwriter.writerow(headers)\nof_connection.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ef7f51ecb7f0e884b867d1b45daeb5bd74ac3ae","trusted":true},"cell_type":"code","source":"def random_search(param_grid, out_file, max_evals = MAX_EVALS):\n    \"\"\"Random search for hyperparameter optimization. \n       Writes result of search to csv file every search iteration.\"\"\"\n    \n    \n    # Dataframe for results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                  index = list(range(MAX_EVALS)))\n    for i in range(MAX_EVALS):\n        \n        # Choose random hyperparameters\n        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n\n        # Evaluate randomly selected hyperparameters\n        eval_results = objective(random_params, i)\n        results.loc[i, :] = eval_results\n\n        # open connection (append option) and write results\n        of_connection = open(out_file, 'a')\n        writer = csv.writer(of_connection)\n        writer.writerow(eval_results)\n        \n        # make sure to close connection\n        of_connection.close()\n        \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n\n    return results ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3f63599e98af370afc24976679e10c19e3f97b3","trusted":true},"cell_type":"code","source":"def grid_search(param_grid, out_file, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\n       Writes result of search to csv file every search iteration.\"\"\"\n    \n    # Dataframe to store results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    \n    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n    keys, values = zip(*param_grid.items())\n    \n    i = 0\n    \n    # Iterate through every possible combination of hyperparameters\n    for v in itertools.product(*values):\n        # Select the hyperparameters\n        parameters = dict(zip(keys, v))\n        \n        # Set the subsample ratio accounting for boosting type\n        parameters['subsample'] = 1.0 if parameters['boosting_type'] == 'goss' else parameters['subsample']\n        \n        # Evalute the hyperparameters\n        eval_results = objective(parameters, i)\n        \n        results.loc[i, :] = eval_results\n        \n        i += 1\n        \n        # open connection (append option) and write results\n        of_connection = open(out_file, 'a')\n        writer = csv.writer(of_connection)\n        writer.writerow(eval_results)\n        \n        # make sure to close connection\n        of_connection.close()\n        \n        # Normally would not limit iterations\n        if i > MAX_EVALS:\n            break\n       \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    \n    return results    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6df200465e7263ce209d7a6fc34f55efb6abde4","trusted":true},"cell_type":"code","source":"# MAX_EVALS = 1000\n\n# # Create file and open connection\n# out_file = 'grid_search_trials_1000.csv'\n# of_connection = open(out_file, 'w')\n# writer = csv.writer(of_connection)\n\n# # Write column names\n# headers = ['score', 'hyperparameters', 'iteration']\n# writer.writerow(headers)\n# of_connection.close()\n\n# grid_results = grid_search(param_grid, out_file)\n\n\n# # Create file and open connection\n# out_file = 'random_search_trials_1000.csv'\n# of_connection = open(out_file, 'w')\n# writer = csv.writer(of_connection)\n\n# # Write column names\n# headers = ['score', 'hyperparameters', 'iteration']\n# writer.writerow(headers)\n# of_connection.close()\n\n# random_results = random_search(param_grid, out_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ae7ab9dfbb5ec7b8c1f005566cadac5fa09b588"},"cell_type":"code","source":"random_results = pd.read_csv('../input/home-credit-model-tuning/random_search_trials_1000.csv')\ngrid_results = pd.read_csv('../input/home-credit-model-tuning/grid_search_trials_1000.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec8b37cd0bb2d4016e8b9bdb3d26de536d7feb69"},"cell_type":"code","source":"import ast\n\n# Convert strings to dictionaries\ngrid_results['hyperparameters'] = grid_results['hyperparameters'].map(ast.literal_eval)\nrandom_results['hyperparameters'] = random_results['hyperparameters'].map(ast.literal_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6bbc8cd8673c1642300cdc747ce794478e55c73"},"cell_type":"code","source":"def evaluate(results, name):\n    \"\"\"Evaluate model on test data using hyperparameters in results\n       Return dataframe of hyperparameters\"\"\"\n        \n    # Sort with best values on top\n    results = results.sort_values('score', ascending = False).reset_index(drop = True)\n    \n    # Print out cross validation high score\n    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, results.loc[0, 'score'], results.loc[0, 'iteration']))\n    \n    # Use best hyperparameters to create a model\n    hyperparameters = results.loc[0, 'hyperparameters']\n    model = lgb.LGBMClassifier(**hyperparameters)\n    \n    # Train and make predictions\n    model.fit(train_features, train_labels)\n    preds = model.predict_proba(test_features)[:, 1]\n    \n    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(test_labels, preds)))\n    \n    # Create dataframe of hyperparameters\n    hyp_df = pd.DataFrame(columns = list(results.loc[0, 'hyperparameters'].keys()))\n\n    # Iterate through each set of hyperparameters that were evaluated\n    for i, hyp in enumerate(results['hyperparameters']):\n        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n                               ignore_index = True)\n        \n    # Put the iteration and score in the hyperparameter dataframe\n    hyp_df['iteration'] = results['iteration']\n    hyp_df['score'] = results['score']\n    \n    return hyp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cad2145564ee9bc0377a86e4fd61201e260c3c5"},"cell_type":"code","source":"grid_hyp = evaluate(grid_results, name = 'grid search')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c2e7adafa94eff6b13faba1959bd65eb9ec22c1"},"cell_type":"code","source":"random_hyp = evaluate(random_results, name = 'random search')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"929e96f0606896748689d6033e8e7068b4d55af0"},"cell_type":"code","source":"import altair as alt\n\nalt.renderers.enable('notebook')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8c355ad0471e6eec3809d72318d71c1f43790a9"},"cell_type":"code","source":"# Combine results into one dataframe\nrandom_hyp['search'] = 'random'\ngrid_hyp['search'] = 'grid'\n\nhyp = random_hyp.append(grid_hyp)\nhyp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b61c29d8f996d8e1a8420c58b6b96cada63d6be"},"cell_type":"code","source":"max_random = random_hyp['score'].max()\nmax_grid = grid_hyp['score'].max()\n\nc = alt.Chart(hyp, width = 400, height = 400).mark_circle(size = 150).encode(alt.Y('score', scale = alt.Scale(domain = [0.65, 0.76])),\nx = 'iteration', color = 'search')\n\nc.title = 'Score vs Iteration'\nc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74e4f3aa0eb0ee75b406fca230469ff46a836d23"},"cell_type":"code","source":"best_grid_hyp = grid_hyp.iloc[grid_hyp['score'].idxmax()].copy()\nbest_random_hyp = random_hyp.iloc[random_hyp['score'].idxmax()].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef48f6c3c16b4c118d49c3fe78b202f4c32991b9"},"cell_type":"code","source":"\nhyp.sort_values('search', inplace = True)\n\n# Plot of scores over the course of searching\nsns.lmplot('iteration', 'score', hue = 'search', data = hyp, size = 8);\nplt.scatter(best_grid_hyp['iteration'], best_grid_hyp['score'], marker = '*', s = 400, c = 'blue', edgecolor = 'k')\nplt.scatter(best_random_hyp['iteration'], best_random_hyp['score'], marker = '*', s = 400, c = 'orange', edgecolor = 'k')\nplt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"Validation ROC AUC versus Iteration\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cd9f14d24770911771b88864bd9834aad07a9a0"},"cell_type":"code","source":"print('Average validation score of grid search =   {:.5f}.'.format(np.mean(grid_hyp['score'])))\nprint('Average validation score of random search = {:.5f}.'.format(np.mean(random_hyp['score'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34f71e7dafd823bb2b449330556e6f7b0b88503a"},"cell_type":"code","source":"# Create bar chart\nbars = alt.Chart(random_hyp, width = 400).mark_bar().encode(x = 'boosting_type', y = alt.Y('count()', scale = alt.Scale(domain = [0, 400])))\n\nbars.title = 'Boosting Type for Random Search'\n\n# Add text for labels\ntext = bars.mark_text(align = 'center', baseline = 'bottom', size = 20).encode(text = 'count()')\n\n# Display\nbars + text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"206337faff87bfbd5338ba05da49a9988ad5dd07"},"cell_type":"code","source":"# Bar plots of boosting type\nrandom_hyp['boosting_type'].value_counts().plot.bar(figsize = (14, 6), color = 'blue', title = 'Random Search Boosting Type');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4353cdf9e87d0870316d5cf09a6b9496c279d80"},"cell_type":"code","source":"random_hyp['score'] = random_hyp['score'].astype(float)\nbest_random_hyp = random_hyp.loc[0, :].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bf0271173d8cadf2ece3e339d8c1579a48a3d09"},"cell_type":"code","source":"plt.figure(figsize = (20, 8))\nplt.rcParams['font.size'] = 18\n\n# Density plots of the learning rate distributions \nsns.kdeplot(param_grid['learning_rate'], label = 'Sampling Distribution', linewidth = 4)\nsns.kdeplot(random_hyp['learning_rate'], label = 'Random Search', linewidth = 4)\nplt.vlines([best_random_hyp['learning_rate']],\n           ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['orange'])\nplt.legend()\nplt.xlabel('Learning Rate'); plt.ylabel('Density'); plt.title('Learning Rate Distribution');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"981d7c01b50672959da7c472f8b6755900acc4d6"},"cell_type":"code","source":"# Iterate through each hyperparameter\nfor i, hyper in enumerate(random_hyp.columns):\n    if hyper not in ['boosting_type', 'iteration', 'subsample', 'score', 'learning_rate', 'is_unbalance', 'metric', 'verbose', 'iteration', 'n_estimators', 'search']:\n        plt.figure(figsize = (14, 6))\n        \n        # Plot the random search distribution and the sampling distribution\n        if hyper != 'loss':\n            sns.kdeplot(param_grid[hyper], label = 'Sampling Distribution', linewidth = 4)\n        sns.kdeplot(random_hyp[hyper], label = 'Random Search', linewidth = 4)\n        plt.vlines([best_random_hyp[hyper]],\n                     ymin = 0.0, ymax = 10.0, linestyles = '--', linewidth = 4, colors = ['orange'])\n        plt.legend(loc = 1)\n        plt.title('{} Distribution'.format(hyper))\n        plt.xlabel('{}'.format(hyper)); plt.ylabel('Density');\n        plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ae5b010b38a2ae7cb0891ea719e2be1b93bb46c"},"cell_type":"code","source":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Plot of four hyperparameters\nfor i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot('iteration', hyper, data = random_hyp, ax = axs[i])\n        axs[i].scatter(best_random_hyp['iteration'], best_random_hyp[hyper], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d359498249f98ad9a3bc7f7c5839f87906948c34"},"cell_type":"code","source":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Scatterplot of next four hyperparameters\nfor i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin', 'subsample']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        sns.regplot('iteration', hyper, data = random_hyp, ax = axs[i])\n        axs[i].scatter(best_random_hyp['iteration'], best_random_hyp[hyper], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3004fc088380f24ad0226defaf12d43faf3a0fcf"},"cell_type":"markdown","source":"## Score versus Hyperparameters"},{"metadata":{"trusted":true,"_uuid":"97ff38aaa830262afed599a0325276d1c90826ca"},"cell_type":"code","source":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Plot of four hyperparameters\nfor i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot(hyper, 'score', data = random_hyp, ax = axs[i])\n        axs[i].scatter(best_random_hyp[hyper], best_random_hyp['score'], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = '{}'.format(hyper), ylabel = 'Score', title = 'Score vs {}'.format(hyper));\n\nplt.tight_layout()\n\nfig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Scatterplot of next four hyperparameters\nfor i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin', 'subsample']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        sns.regplot(hyper, 'score', data = random_hyp, ax = axs[i])\n        axs[i].scatter(best_random_hyp[hyper], best_random_hyp['score'], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = '{}'.format(hyper), ylabel = 'score', title = 'Score vs {}'.format(hyper));\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0728c505165d5cd7ca752e357551f1a05a65e772"},"cell_type":"code","source":"# Read in full dataset\ntrain = pd.read_csv('../input/home-credit-simple-featuers/simple_features_train.csv')\ntest = pd.read_csv('../input/home-credit-simple-featuers/simple_features_test.csv')\n\n# Extract the test ids and train labels\ntest_ids = test['SK_ID_CURR']\ntrain_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n\ntrain = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\ntest = test.drop(columns = ['SK_ID_CURR'])\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5818bc2522484932cf49eda5efce7e3e8745598"},"cell_type":"code","source":"train_set = lgb.Dataset(train, label = train_labels)\n\nhyperparameters = dict(**random_results.loc[0, 'hyperparameters'])\ndel hyperparameters['n_estimators']\n\n# Cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set,\n                    num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = N_FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e209bb9e815adb2cfa4b622d3f705bfd29e91280"},"cell_type":"code","source":"print('The cross validation score on the full dataset = {:.5f} with std: {:.5f}.'.format(\n    cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\nprint('Number of estimators = {}.'.format(len(cv_results['auc-mean'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1f025794814044457e8aaf7939be4d2ca097b00"},"cell_type":"code","source":"# Train the model with the optimal number of estimators from early stopping\nmodel = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(train, train_labels)\n                        \n# Predictions on the test data\npreds = model.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70565041ab49bf36cab2ca85dc399bd6df7d6e01"},"cell_type":"code","source":"submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\nsubmission.to_csv('submission_simple_features_random.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"882076013f32fe56da77755345595a5269030bc9"},"cell_type":"code","source":"import xgboost as xgb\n\nclf_xgBoost = xgb.XGBClassifier(\n    learning_rate =0.01, n_estimators=1000, max_depth=4, min_child_weight=4, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', \n            nthread=4, scale_pos_weight=2, seed=27)\n# Fit the models\nclf_xgBoost.fit(train,train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3754a31fe77728dd8062108e68ba696f471fa40f"},"cell_type":"code","source":"pred = clf_xgBoost.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69fda83ed0e58cf2e5a3fc5177696461f833b8a9"},"cell_type":"code","source":"submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': pred})\nsubmission.to_csv('submission_xgboost.csv', index = False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}