{"cells":[{"metadata":{"_uuid":"4c70242ab186a02a442f427b4de193e9a212100f"},"cell_type":"markdown","source":"Hey, I am new to kaggle and I am struggle to make model which make some decent predictions, but as geophysicist I would like to share some of my findings about the input data.\n\nDuring revision of the images I was thinking a lot about process of data preparation. As they said in the competiotion overview and data description we are dealing here with 3d seismic data, which means that images could have been cutted out from x, y and z planes or In-lines, Cross-lines and time-slices using the geophysical vocabulary. \n\n\n[0](https://drive.google.com/file/d/1m9Cu6bAvOzWkAaUbVl_ooSP-w-Jrp9Wx/view?usp=sharing)\n\nIn this case we are probably not dealing with time slices so two planes remains to cut out the images. As i saw a lot of images which looked similiar I decided to try to recombine them at least partially. The competition rules says that labeling by-hand is forbidden but there is nothing about image reconstruction.\n\nFirst step: reading the images and depth info\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\n\n\n\nf = open(\"../input/depths.csv\")\ndata=f.readlines()\nf.close()\n\nfils=[]\ndep=[]\n\nfor i in data[1:]:\n    fils.append(i.split(',')[0])\n    dep.append(float(i.split(',')[1]))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cf6c8a58c775d5f2b60f4067a45e9f20588a2b0"},"cell_type":"markdown","source":"As a second step we need to make lists of images, masks, depths and lists of the leftmost and rightmost seismic traces (left and right column of the image).\nI also generated numpy zeros array with 255 in the topmost row to put them as a test masks. \nThis makes it easier to plot later and allows to clearly differentiate between test and train images."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dir_='../input/'\n\nfnames=[]\nimag=[]\nr=[]\nl=[]\nu=[]\nd=[]\ndept=[]\nmask=[]\n\n#No mask generation\nema=np.zeros((101,101))\nema[0,:]=255\n\nfor filename in os.listdir(dir_+'train/images'):\n    fnames.append(filename)\n    fimg=cv2.imread(dir_+'train/images/'+filename,0)\n    mask.append(cv2.imread(dir_+'/train/masks/'+filename,0))\n    imag.append(fimg)\n    r.append(fimg[:,-1])\n    l.append(fimg[:,0])\n    dept.append(dep[fils.index(filename.split('.')[0])])\n\nfor filename in os.listdir(dir_+'test/images'):\n    fnames.append(filename)\n    fimg=cv2.imread(dir_+'test/images/'+filename,0)\n    mask.append(ema)\n    imag.append(fimg)\n    r.append(fimg[:,-1])\n    l.append(fimg[:,0])\n    dept.append(dep[fils.index(filename.split('.')[0])])\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8dc6bf3cc4e111430a10c0754d349728acb3cdf"},"cell_type":"markdown","source":"The next step is finding the correlation coefficients between the rightmost column of given image and the leftmost columns of every other image. \nIndex of maximum correlation coeff gives us the most probable neighbouring image to the right. I find that in general correlation coeffs of >0.9 % gives true neighbours. \n\nYou can choose random image to look for neighbour."},{"metadata":{"trusted":true,"_uuid":"acb9deac0da1ab3f4574de79fb701b1603773316"},"cell_type":"code","source":"m=np.random.randint(22000,size=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2586bb0cbc1f1dd3d34faec4c2647886568d318f"},"cell_type":"markdown","source":"Here are some of the interesting indexes to check: \n20449,4439, 18152, 6156, 16461,3535,8944,15799,21609,453"},{"metadata":{"trusted":true,"_uuid":"550d30434a03b670fb4c40ea447e375fa36f1e9c"},"cell_type":"code","source":"m=[20449,4439, 18152, 6156, 16461,3535,8944,15799,21609,453]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"407e639f06e00dc7a0743354541cd2335583d2be"},"cell_type":"code","source":"for x in m:\n\n    corr_r=[]\n    corr_l=[]\n\n    for i in range(0,len(l)):\n        if i==x:\n            corr_r.append(0)\n        else:\n            corr_r.append(np.corrcoef(r[x],l[i])[0,1])\n\n    # Finding index of max correlation\n    ri=corr_r.index(max(corr_r))\n\n    # Normalization of images\n    a=imag[x]-np.mean(imag[x])\n    b=imag[ri]-np.mean(imag[ri])\n\n    mina=min([np.amin(a),np.amin(b)])\n    maxa=min([np.amax(a),np.amax(b)])\n\n    plt.figure(figsize=(10,7))\n    plt.subplot(121)\n    plt.imshow(a,cmap='gray',vmin=mina,vmax=maxa)\n    plt.imshow(mask[x],cmap='Greens',vmin=0,vmax=255,alpha=0.2)\n    plt.axis('off')\n    plt.text(50, 100, dept[x], fontsize=12)\n    plt.subplot(122)\n    plt.imshow(b,cmap='gray',vmin=mina,vmax=maxa)\n    plt.imshow(mask[ri],cmap='Greens',vmin=0,vmax=255,alpha=0.2)\n    plt.axis('off')\n    plt.text(0, 10, str(max(corr_r))[:5], fontsize=12)\n    plt.text(50, 100, dept[ri], fontsize=12)\n\n    # Tight layout of images\n    plt.suptitle(str(x))\n    plt.subplots_adjust(wspace=0, hspace=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2b2b627b10dc97513dcca186eeb917a1951b020"},"cell_type":"markdown","source":"Now you can compute correlations for all of the images and make patches of highly correlated images.\nHere are some examples. The values at the bottom of every picture is the depth.\n\n[1](https://drive.google.com/file/d/1kyKBrk6GzZk8nMeIg1IRP6WUQ5SYj-G1/view?usp=sharing)\n[2](https://drive.google.com/file/d/1fIcIuVis4lqB3HW2QPIM_mhqsAVZ97f-/view?usp=sharing)\n[3](https://drive.google.com/file/d/1N_NxGyemLoiEKTnsK4ImFXrHTX97JyzW/view?usp=sharing)\n[4](https://drive.google.com/file/d/1QzlZVgBUSlWjJ-Lu95Yh4LKW08Dbc421/view?usp=sharing)\n[5](https://drive.google.com/file/d/1mVGERNTAdKCfCINyhzfnGajMQ1Hekb4P/view?usp=sharing)\n[6](https://drive.google.com/file/d/1QMFwpP8-uUpmD4lIAe4ub3oL6FDT0O54/view?usp=sharing)\n[7](https://drive.google.com/file/d/1UiXPBh_qxnkYyzGjqtx4cCUbfCgJ0XWQ/view?usp=sharing)\n\nHere are some of the conclusions which I draw from analysing them:\n\n1) The depths are incorrect or biased so much that they carry only the qulitative information.\nOn the figures above you can see that images which fit perfectly can have depths different by hundreds of feet.\nOn the other hands pictures with only zeros pixels which refers to the area over the sea bottom which is shallow falls in the depths from 0 to 300 ft which seems to be reasonable.\n(explained here: https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/61725#380962)\n\n2) The images are most probably not in the same scale.\nThickness of the reflactions can vary with different frequency spectrum of the seismic data after processing.\nFrequency spectrum depend on the quality of the data and is also decreasing with depth which can lower the resolution of image.\nHowever this can not explein the differences between 4th and 6th images. The first one is of local scale and the second is of regional scale.\nthis mean that some images shows the zoomed( better sampled data) and some shows outzoomed(low sampled data)\n\n3) There are some images coming from the neghbouring inlines which can act like doubling some of the pictures.\nI make additional analysis where i checked the correlation between only rightmost columns of the images and i found those (right image is always flipped left-right):\n\n[a](https://drive.google.com/file/d/16NmT22SFL1HB-TXC_TZfnunO-SELL6SE/view?usp=sharing)\n[b](https://drive.google.com/file/d/1FAHD57BMLEKpouYCDnQdAqUdIKy52JqB/view?usp=sharing)\n\nAs you can see images are very similiar, but not the same. They are probably from neighbouring inlines (if that need to be explained let me know in the comments).\nthis results in effect close to augmentation or doubling the images.\n\n4) If you can find neighbouring train images then you can get different augmentation, by getting middle part of connected images like in the one below\n"},{"metadata":{"trusted":true,"_uuid":"bf1d5e59db3ae7d062e2c14c44c61df45302cfb2"},"cell_type":"code","source":"m=[3535]\nfor x in m:\n\n    corr_r=[]\n    corr_l=[]\n\n    for i in range(0,len(l)):\n        if i==x:\n            corr_r.append(0)\n        else:\n            corr_r.append(np.corrcoef(r[x],l[i])[0,1])\n\n    # Finding index of max correlation\n    ri=corr_r.index(max(corr_r))\n\n    # Normalization of images\n    a=imag[x]-np.mean(imag[x])\n    b=imag[ri]-np.mean(imag[ri])\n\n    mina=min([np.amin(a),np.amin(b)])\n    maxa=min([np.amax(a),np.amax(b)])\n\n    plt.figure(figsize=(10,7))\n    plt.subplot(121)\n    plt.imshow(a,cmap='gray',vmin=mina,vmax=maxa)\n    plt.imshow(mask[x],cmap='Greens',vmin=0,vmax=255,alpha=0.2)\n    plt.axis('off')\n    plt.text(50, 100, dept[x], fontsize=12)\n    plt.subplot(122)\n    plt.imshow(b,cmap='gray',vmin=mina,vmax=maxa)\n    plt.imshow(mask[ri],cmap='Greens',vmin=0,vmax=255,alpha=0.2)\n    plt.axis('off')\n    plt.text(0, 10, str(max(corr_r))[:5], fontsize=12)\n    plt.text(50, 100, dept[ri], fontsize=12)\n\n    # Tight layout of images\n    plt.suptitle(str(x))\n    plt.subplots_adjust(wspace=0, hspace=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf4138d55edb57c35a72c117a329ecab5b83ce4b"},"cell_type":"markdown","source":"\nI hope that you find this kernel intersting and useful. If there will be interest I could also public a kernel about depth analysis."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}