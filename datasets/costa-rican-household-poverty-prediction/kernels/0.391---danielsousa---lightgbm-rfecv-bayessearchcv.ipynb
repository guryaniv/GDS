{"cells":[{"metadata":{"_uuid":"2216d1ffefff017bae8bc7bf123f43d2f23988b3"},"cell_type":"markdown","source":"<center> \n<strong>LightGBM+RFECV+BayesSearchCV</strong><br />\n<img src=\"https://panampost.com/wp-content/uploads/pobreza-costa-rica-560x276.jpg\">\nHelping the Inter-American Development Bank with income qualification of the world's poorest families.\n\n</center>"},{"metadata":{"_uuid":"2f6b98b77ac14c9fd4b5978b3dc5ab6d34211b14"},"cell_type":"markdown","source":"# Brief Introduction\nAccording to the annual report on Costa Rica’s “State of Nation”, from 2017, 20 percent of households were in a situation of poverty and exclusion. Despite some improvements, such as the fall of the percentage of households poverty situations between 2015 and 2016, last year 31.5 percent of Costa Rican households suffered from some kind of poverty - monetary, multidimensional, and other types. Also, 'The State of the Nation Report 2017' states that Costa Rica has failed to name some of the structural problems underlying poverty. These facts lead to a necessary action from Costa Rican authorities to fight these structural problems. And, to do so, some institutions like The Inter-American Development Bank are asking skilled people to help them deal with such issue.\n"},{"metadata":{"_uuid":"e081b2402cf3ecdbdfa21e984e1a1a1b3cbc607a"},"cell_type":"markdown","source":"# First steps\nIn this notebook, we will approach this problem using a LightGBM , recursive feature elemination technice and BayesSearchCV to get the best hyper-parameters."},{"metadata":{"trusted":true,"_uuid":"1270281c769aa0437333c66acd1e6ee268a7b97b"},"cell_type":"code","source":"import matplotlib\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom skopt import BayesSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\n\ndf = pd.read_csv('../input/train.csv')\n\ntest_data = pd.read_csv('../input/test.csv')\n\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6749986c967809a29e5332e35330a1ecfcbfd379"},"cell_type":"markdown","source":"# Data treatment"},{"metadata":{"trusted":true,"_uuid":"37ca07812de3cf4be3ae30d461e5f98cf3ce925b"},"cell_type":"code","source":"print(df.shape) # Shape of the data\ndf.head(10) # See the first 3 values of the df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"567c41353cdc39428f55f81694b8dea4657af172"},"cell_type":"markdown","source":"### Next, we have built some functions in order to simplify our dataset:\n1.  First, we checked that different persons with the same 'idhogar' had different targets, we corrected this fact by constructing a function which makes all targets equal to the target of the head of the household;\n2. We also treated the 'dependecy' column by recalculating all values; \n3. Next we treated the columns 'edjefe' and 'edjefa' wich had some \"no\" and \"yes\" answers;\n4. We then noted that the \"v2a1\" had some NaN values we tried to figure out what was the reason for this;\n5. \"v18q1\" had some inconsisties too and we took care of them;\n6. In the last step we treated 'rez_esc' and 'meaneduc' wich both had some zeros;\n5. Checking the Kernel [\"Start Here: A Complete Walkthrough\"](https://www.kaggle.com/willkoehrsen/start-here-a-complete-walkthrough) may help you understand this dataset more deply."},{"metadata":{"_uuid":"b22c1791f45863a9ae8f0ae69b41cc60a7e37eaa"},"cell_type":"markdown","source":"### Checking wich data types we have in our dataset"},{"metadata":{"trusted":true,"_uuid":"89b99c6d1f00af6a58fb9237c49c5102139fb97e"},"cell_type":"code","source":"print('Dtypes count:' + '\\n', df.dtypes.value_counts())\ncolumns_object = df.columns[df.dtypes == object]\nprint('Columns wich could have a problem :', \\\n       columns_object) # Columns wich need treatment beacause they are object type","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0905018572fa5ada266d1bdc8a5616f5291feaec"},"cell_type":"markdown","source":"The columns 'Id' and 'idhogar' don't need treatment because they don't represent a direct feature in the meaning that they represent a simple identification for each row and a identification for the household."},{"metadata":{"_uuid":"473900bab9f83a9a8ad2920808d3f9843619612a"},"cell_type":"markdown","source":"### Treating people with the same 'idhogar' and different targets"},{"metadata":{"trusted":true,"_uuid":"a1564c8b8a4f2b62f98a921bbf8707a20c7a7016"},"cell_type":"code","source":"def correct_targets(df):\n    # Making groups by household\n    all_equal_groups_1 = df.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n    \n    # Selection of households were targets are not equal for all members\n    all_not_equal_groups_1 = all_equal_groups_1[all_equal_groups_1 != True]\n\n    for household in all_not_equal_groups_1.index:\n        # We assumed that the correct label is the label of the head of the household, this is one possible approach\n        true_target = int(df[(df['idhogar'] == household) & (df['parentesco1'] == 1.0)]['Target'])\n        # Setting the correct tag for every member of the household\n        df.loc[df['idhogar'] == household, 'Target'] = true_target\n    \n    all_equal_groups_2 = df.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n    all_not_equal_groups_2 = all_equal_groups_2[all_equal_groups_2 != True]\n    \n    n_corrected = len(all_not_equal_groups_1) - len(all_not_equal_groups_2)\n    print(\"Number of targets corrected :\", n_corrected)\n    \n    return df\n    \ndf = correct_targets(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11421c257b48db00dce609f10b6723a77544f960"},"cell_type":"markdown","source":"### Treating 'dependency'"},{"metadata":{"trusted":true,"_uuid":"d88fc7f3c45086cfd2fe453e5ca15cb89cd0632d"},"cell_type":"code","source":"df[columns_object[2]].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc67b76074be66b58e37eccee94e3daa47e77b4d"},"cell_type":"markdown","source":"As you can see this column has some 'yes' and 'no' answers when the anwser should be a float or int.\nSince the 'dependecy' is calculated as:  \ndependency = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)  \nThen we can calculate it manually by suming the values of the  columns 'hogar_nin' + 'hogar_mayor' and dividing it by the column 'hogar_adul'."},{"metadata":{"trusted":true,"_uuid":"6939e563508467a36b55c82aa2d8abd4007ab85c"},"cell_type":"code","source":"(df['hogar_nin'] + df['hogar_adul'] == df['hogar_total']).all()\n# Testing if the 'total' is the sum of 'min' +'adult'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3fdb9ed4eb8606e1776002f438a3adf5e4efeb9"},"cell_type":"markdown","source":"This means that the number of members of the household wich are youger than 19 and older than 65 is given by:"},{"metadata":{"trusted":true,"_uuid":"74e0e31ff85f36eaf827893ca6af6f835e07ea6c"},"cell_type":"code","source":"inf_19_sup_65 = df['hogar_nin'] + df['hogar_mayor']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab05671c51fe77560201c7e21694cfc50ce42324"},"cell_type":"markdown","source":"As you can see the column 'hogar_adul' inclues the number of people older than 19 years old.\nThe column 'hogar_mayor' has the number of people older then 65.\nThis means that the number of members of the household wich are older than 19 and younger than 65 is given by:"},{"metadata":{"trusted":true,"_uuid":"9f07c8a886a268076fa86191ad03048b77810726"},"cell_type":"code","source":"sup_19_inf_65 = df['hogar_adul'] - df['hogar_mayor']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed836abf8228ad5283f67455955ef9d596df9814"},"cell_type":"code","source":"dependecy = inf_19_sup_65*1.0 / sup_19_inf_65 # Recalculates the dependecy\ndependecy.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"373f596b772b5602b2fa8d224c04cddbfd6849de"},"cell_type":"code","source":"dependecy = dependecy.replace([np.inf, -np.inf], np.nan) # Replaces all inf with NaN\ndependecy.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74fbbb749a0fc0ff838e1cd1708e378584877239"},"cell_type":"code","source":"dependecy.nlargest() # Gives the heighest dependecy rate of all families without considering inf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1156fcabd55fd7ea4c533e1d161850ae52f2db45"},"cell_type":"markdown","source":"We will assume that the families which have no one who is older than 19 and younger than 65 have a dependecy rate of 8, this was the initial dependecy rate before any treatment."},{"metadata":{"trusted":true,"_uuid":"e997b9f010808eafb31216ff84e9b293723fe8d4"},"cell_type":"code","source":"dependecy = dependecy.fillna(dependecy.nlargest().iloc[0]) \n# Finds the largest value and fills de NaN with it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc03eb91e3fc645e375c28842464359ee1270a0"},"cell_type":"code","source":"df['dependency'].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef1ba4b69296be8f35cc5694d4626efb85d40545"},"cell_type":"code","source":"df['dependency'] = dependecy.values\ndf['dependency'].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1786149381349ceb3403815a61133ec5d1ab380e"},"cell_type":"markdown","source":"#### Defining a function to treat 'dependency'"},{"metadata":{"trusted":true,"_uuid":"6d9fd5072e3ca3542a88f0574ff7241e1cf04839"},"cell_type":"code","source":"def correct_dependency(df):\n    inf_19_sup_65 = df['hogar_nin'] + df['hogar_mayor']\n    sup_19_inf_65 = df['hogar_adul'] - df['hogar_mayor']\n    dependecy = inf_19_sup_65*1.0 / sup_19_inf_65 # Recalculates the dependecy\n    dependecy = dependecy.replace([np.inf, -np.inf], np.nan) # Replaces all inf with NaN\n    dependecy = dependecy.fillna(dependecy.nlargest().iloc[0]) # Finds the largest value and fills de NaN with it\n    df['dependency'] = dependecy.values\n    return df    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c640589e160cf8138816be8df4000b05d2857f43"},"cell_type":"markdown","source":"### Treating 'edjefe' and 'edjefa'"},{"metadata":{"_uuid":"5f06697e5599bebc1fa3c33c78e0079ce2769e1c"},"cell_type":"markdown","source":"Based on the introduction for each feature we found out that we can assing 'yes' to be 1 and 'no' to be 0."},{"metadata":{"trusted":true,"_uuid":"c289acb617e553f428552df1e13c7d17cc5814fc"},"cell_type":"code","source":"df[columns_object[3]].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c311887a516dff4ebc7e68e3c2c033653dbc5ff4"},"cell_type":"code","source":"df[columns_object[3]] = df[columns_object[3]].replace({'no': 0, 'yes':1}).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84acee5fd368e4e002513139a60f9fc1fbb48e4c"},"cell_type":"code","source":"df[columns_object[4]].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c013ac1fd8a47d345ae22833fe16d54026156710"},"cell_type":"code","source":"df[columns_object[4]] = df[columns_object[4]].replace({'no': 0, 'yes':1}).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eee6ac0c32c1d3c57693957374900ed9d219e66"},"cell_type":"markdown","source":"#### Defining a function to treat 'edjefe' and 'edjefa'"},{"metadata":{"trusted":true,"_uuid":"608e014f3a3753a533d3af373d873bc7169eb6b6"},"cell_type":"code","source":"def correct_edjefe_edjefa(df):\n    df[columns_object[3]] = df[columns_object[3]].replace({'no': 0, 'yes':1}).astype(float)\n    df[columns_object[4]] = df[columns_object[4]].replace({'no': 0, 'yes':1}).astype(float)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"840531a816c16977d3ac0f9beb98366606853933"},"cell_type":"markdown","source":"## Columns with NaN\nIn this part we checked wich columns had NaN values."},{"metadata":{"trusted":true,"_uuid":"346c38bda75afa4574e8237deb7b7efeecc80114"},"cell_type":"code","source":"list_na = df.columns[df.isnull().any()].tolist() #It's a list of all columns that have NAN\nfor column in list_na:\n    series = df[column]\n    n_null = series.isnull().sum()\n    print('The column ' + column + ' has ' + str(n_null) + ' null values.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2265aa8d8d7492edd31e8a0fc003091575508a8"},"cell_type":"markdown","source":"### Treating v2a1"},{"metadata":{"_uuid":"f58bf8a67d927c0fdb72a47b105ad44a7137e751"},"cell_type":"markdown","source":"After checking the documentation we saw that:\n* v2a1, Monthly rent payment;  \n* tipovivi1, =1 own and fully paid house;  \n* tipovivi4, =1 precarious;  \n* tipovivi5, \"=1 other(assigned,  borrowed)\"\n\nTherefore there must be a correlation between the 4 variables above. Every time that tipovivi1 = 1 or tipovivi4 = 1 or tipovivi5 = 1 our v2a1 must be NaN."},{"metadata":{"trusted":true,"_uuid":"d7a262fc3b08df3418602026e49f2267d4855d12"},"cell_type":"code","source":"df[['v2a1', 'tipovivi1', 'tipovivi4', 'tipovivi5']].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c782c4dc17c10b1b6b079eb3b646c016ac8d110"},"cell_type":"code","source":"count = 0\nn_ret = 0\nfor index, row in df.iterrows():\n    if np.isnan(row['v2a1']) == True and row['tipovivi1'] == 1 or row['tipovivi4'] == 1 or row['tipovivi5'] == 1:\n        df.loc[index,'v2a1'] = 0\n        n_ret += 1\n        \nprint('The amount of v2a1 changed was :', n_ret)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"387015a4e9419ae2dbf6fb922ba427f20edb7e29"},"cell_type":"markdown","source":"#### Defining a function to treat 'v2a1'"},{"metadata":{"trusted":true,"_uuid":"309a6f48232344f00a09ec507cf95931b126ce0c"},"cell_type":"code","source":"def correct_v2a1(df):\n    count = 0\n    n_ret = 0\n    for index, row in df.iterrows():\n        if np.isnan(row['v2a1']) == True and row['tipovivi1'] == 1 or row['tipovivi4'] == 1\\\n        or row['tipovivi5'] == 1:\n            df.loc[index,'v2a1'] = 0\n            n_ret += 1\n    print('The amount of v2a1 changed was :', n_ret)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3763f4e5178de95d844adb81182a30b3091d669f"},"cell_type":"markdown","source":"### Treating v18q1"},{"metadata":{"trusted":true,"_uuid":"20cd60c0c70fac3fee875683c22d242a81f3fee2"},"cell_type":"code","source":"df[['v18q', 'v18q1']].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a81a91a37d02adbf5cd766855872c072f26f2407"},"cell_type":"markdown","source":"In the documentation it is stated that:  \n* 'v18q' = owns a tablet;  \n* 'v18q1' = number of tablets household owns.  \nTherefore if the 'v18q' = 0 then 'v18q1' should also be zero."},{"metadata":{"trusted":true,"_uuid":"4e7b1f03d3ce98f7e11f7c6574a6e15f5475e382"},"cell_type":"code","source":"count = 0\nn_ret = 0\nfor index, row in df.iterrows():\n    if row['v18q'] == 0 and np.isnan(row['v18q1']) == True:\n        df.loc[index,'v18q1'] = 0\n        n_ret += 1\n        \nprint('The amount of v18q1 changed was :', n_ret)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a13f3c505790a5e36b002fa1af4868b9a6882d27"},"cell_type":"code","source":"def correct_v18q1(df):\n    count = 0\n    n_ret = 0\n    for index, row in df.iterrows():\n        if row['v18q'] == 0 and np.isnan(row['v18q1']) == True:\n            df.loc[index,'v18q1'] = 0\n            n_ret += 1\n    print('The amount of v18q1 changed was :', n_ret)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"281af6fdb7473cfd5fd8a10827c1b905f9c18833"},"cell_type":"markdown","source":"### Treating rez_esc"},{"metadata":{"trusted":true,"_uuid":"e047060640bb7e5e4319221992f4d42234366475"},"cell_type":"code","source":"df[['rez_esc', 'escolari']].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dca6202f75ade44892b8008d9bfa3b235ee4f66c"},"cell_type":"markdown","source":"After seeing the documentation is stated that 'rez_esc' is equivalent to the 'Years behind in school', we assumed than that all the NaN values should be equal to 0. We assumed that the people who never failed a year in school didn't fill this field leading to NaN."},{"metadata":{"trusted":true,"_uuid":"b8daf4b51866e962af98b3cbf1adef160da8ff4a"},"cell_type":"code","source":"df['rez_esc'] = df['rez_esc'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3aec85c2376e19091733eff4b861ace20570b0a"},"cell_type":"markdown","source":"### Treating meaneduc"},{"metadata":{"_uuid":"4de7589c7b2bd826d6e465c7194f31f848bed915"},"cell_type":"markdown","source":"We filled the 'meaneduc' and 'SQBmeaned' columns with zeros. We did this because we assumed that the reason why people that had 'meaneduc'= NaN was because they didn't attend school therefore their 'meaneduc' = 'SQBmeaned' = 0."},{"metadata":{"trusted":true,"_uuid":"6701a6453b83ad34bc22bfaa934e4fdf837811f5"},"cell_type":"code","source":"df['meaneduc'] = df['meaneduc'].fillna(0)\ndf['SQBmeaned'] = df['SQBmeaned'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d159cf01f4444bf8ef2d4c7c554636ef328771d"},"cell_type":"markdown","source":"#### Treating 'meaneduc', 'rez_esc' and 'SQBmeaned'"},{"metadata":{"trusted":true,"_uuid":"094eb17ec8e2273f4d21ae29bde4fc143142434e"},"cell_type":"code","source":"def correct_meaneduc_rez_esc(df):\n    df['rez_esc'] = df['rez_esc'].fillna(0)\n    df['meaneduc'] = df['meaneduc'].fillna(0)\n    df['SQBmeaned'] = df['SQBmeaned'].fillna(0)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b5d257cd667236b94d6016f3f6c65e04d621a93"},"cell_type":"markdown","source":"# Feature importance\nWe discarted all columns that had a standard deviation of zero, and also we eleminated the columns 'idhogar' and 'Id'."},{"metadata":{"trusted":true,"_uuid":"43edd9d50c355d0ee93161f4bac165cf464199eb"},"cell_type":"code","source":"# We choose to drop all Ids\nneedless_col_prov = ['idhogar','Id']\ndf = df.drop(needless_col_prov, axis = 1)\n\nneedless_col = needless_col_prov\n\n# We assumed that all columns with a std inferior to 0.05 should also be droped\nneedless_col_prov = []\nfor col in df.columns:\n    if df[col].std() == 0: \n        needless_col_prov.append(col)\nprint('The following columns have zero std so they will be discarted :', needless_col_prov)\ndf = df.drop(needless_col_prov, axis = 1)\n\n\nneedless_col = needless_col + needless_col_prov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57119e4db4638b74767e33928c2aa1025454d0ea"},"cell_type":"code","source":"print('In total there where', len(needless_col), 'columns eleminated.')\nprint('The eleminated columns where the following ones :', needless_col)\nprint('We are now considering', len(df.columns.tolist())-1, 'features.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14ee3b5b2be8318c47bead20eb195b663e11a931"},"cell_type":"markdown","source":"# Features and target"},{"metadata":{"trusted":true,"_uuid":"f540a38a0ba2de51a77ba6575166f33b2bf71917"},"cell_type":"code","source":"X = df.drop('Target', axis = 1)\ny = df[['Target']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"019e27c0131926dcdcffe97a79f8b1d95f45ff70"},"cell_type":"markdown","source":"# LGBM hyper-parameters auto-optimizer\nWe used a Bayesian optimization to optimize our hyper-parameters.  \nFrom here on all the code that is comment out we only executed it on our computer, we didn't executed it on the Kaggle enviroment because we assumed that it would take to long to process."},{"metadata":{"trusted":true,"_uuid":"8fccd020f72eccabd0373c83e21ff727222b62cf"},"cell_type":"code","source":"'''\nbayes_cv_tuner = BayesSearchCV( estimator = lgb.LGBMClassifier(boosting_type='gbdt', n_jobs=-1, verbose=2),\n        search_spaces = {\n        'learning_rate': (0.01, 1.0, 'log-uniform'),\n        'num_leaves': (2, 500),\n        'max_depth': (0, 500),\n        'min_child_samples': (0, 200),\n        'max_bin': (100, 100000),\n        'subsample': (0.01, 1.0, 'uniform'),\n        'subsample_freq': (0, 10),\n        'colsample_bytree': (0.01, 1.0, 'uniform'),\n        'min_child_weight': (0, 10),\n        'subsample_for_bin': (100000, 500000),\n        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n        'scale_pos_weight': (1e-6, 500, 'log-uniform'),\n        'n_estimators': (10, 10000),\n        },\n        scoring = 'f1_macro', cv = StratifiedKFold(n_splits=2), n_iter = 30, verbose = 1, refit = True)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"850d3c9dd33ec536bf4d9a127e3388a1f9c5540d"},"cell_type":"markdown","source":"# Recursive Feature elemination\nWe defined the function rfecv_opt wich was responsible for doing a recursive feature elemination with F1 Macro as the metrics for calculating the score."},{"metadata":{"trusted":true,"_uuid":"f42095d2cb48ae11aaa939c983711886cbab33a9"},"cell_type":"code","source":"'''\ndef rfecv_opt(model, n_jobs, X, y, cv = StratifiedKFold(2)):\n    rfecv = RFECV(estimator = model, step = 1, cv = cv,\n                    n_jobs = n_jobs, scoring = 'f1_macro', verbose = 1)\n    rfecv.fit(X.values, y.values.ravel())\n    print('Optimal number of features : %d', rfecv.n_features_)\n    print('Max score with current model :', round(np.max(rfecv.grid_scores_), 3))\n    # Plot number of features VS. cross-validation scores\n    plt.figure()\n    plt.xlabel('Number of features selected')\n    plt.ylabel('Cross validation score (f1_macro)')\n    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n    plt.show()\n    important_columns = []\n    n = 0\n    for i in rfecv.support_:\n        if i == True:\n            important_columns.append(X.columns[n])\n        n +=1\n    return important_columns, np.max(rfecv.grid_scores_), rfecv\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1638af265f659546f2de8a6fa37926c13d9c359"},"cell_type":"markdown","source":"# Routine\nNext we did a function to optmize our model and do a RFECV on every step."},{"metadata":{"trusted":true,"_uuid":"af40301c200805b6cd10a38d57fe019bf4cf7305"},"cell_type":"code","source":"'''\ndef routine(X, y, n_iter_max, n_jobs):\n    list_models = []\n    list_scores_max = []\n    list_features = []\n    list_f1_score = []\n    for i in range(n_iter_max):\n        print('Currently on iteration', i+1, 'of', n_iter_max, '.')\n        if i == 0:\n            model = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                            silent = True, metric = 'None', n_jobs = n_jobs,\n                            n_estimators = 8000, class_weight = 'balanced')\n        else:\n            print('Adjusting model.')\n            X_provi = X[imp_columns]\n            # Get current parameters and the best parameters    \n            result = bayes_cv_tuner.fit(X_provi.values, y.values.ravel())\n            best_params = pd.Series(result.best_params_)\n            param_dict=pd.Series.to_dict(best_params)\n            model = lgb.LGBMClassifier(colsample_bytree = param_dict['colsample_bytree'],\n                          learning_rate = param_dict['learning_rate'],\n                          max_bin = int(param_dict['max_bin']),\n                          max_depth = int(param_dict['max_depth']),\n                          min_child_samples = int(param_dict['min_child_samples']),\n                          min_child_weight = param_dict['min_child_weight'],\n                          n_estimators = int(param_dict['n_estimators']),\n                          num_leaves = int(param_dict['num_leaves']),\n                          reg_alpha = param_dict['reg_alpha'],\n                          reg_lambda = param_dict['reg_lambda'],\n                          scale_pos_weight = param_dict['scale_pos_weight'],\n                          subsample = param_dict['subsample'],\n                          subsample_for_bin = int(param_dict['subsample_for_bin']),\n                          subsample_freq = int(param_dict['subsample_freq']),\n                          n_jobs = n_jobs,\n                          class_weight='balanced',\n                          objective='multiclass'\n                          )\n        imp_columns, max_score, rfecv = rfecv_opt(model, n_jobs, X, y)\n        list_models.append(model)\n        list_scores_max.append(max_score)\n        list_features.append(imp_columns)\n        \n    return list_models, list_scores_max, list_features\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3128eb13573dcb4e268dc6bb0b575301f83532b2"},"cell_type":"code","source":"'''\nlist_models, list_scores_max, list_features = routine(X, y, 15, 4)\n\nindex_max = list_scores_max.index(max(list_scores_max))\nfeatures = list_features[index_max]\nmodel = list_models[index_max]\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9f2fa420a1dfd6bd849279d32a0101cddf03fe0"},"cell_type":"markdown","source":"# Results\nAfter running it on our computer we got the following results."},{"metadata":{"trusted":true,"_uuid":"ea912397db3d22f1775a167929e604256f8a4b6c"},"cell_type":"code","source":"model = lgb.LGBMClassifier(boosting_type='gbdt', class_weight='balanced',\n        colsample_bytree=0.364429092365, learning_rate=0.11718910536,\n        max_bin=75490, max_depth=312, min_child_samples=21,\n        min_child_weight=7.0, min_split_gain=0.0, n_estimators=5392,\n        n_jobs=15, num_leaves=249, objective='multiclass',\n        random_state=None, reg_alpha=2.51960359296e-05,\n        reg_lambda=10.9020792516, scale_pos_weight=0.0247756521295,\n        silent=True, subsample=0.195224406679, subsample_for_bin=126252,\n        subsample_freq=3)\n\nfeatures = ['v2a1', 'rooms', 'r4h2', 'r4h3', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3', 'tamhog', 'tamviv',\\\n            'escolari', 'energcocinar3', 'hogar_nin', 'hogar_adul', 'dependency', 'edjefe', 'edjefa',\\\n            'meaneduc', 'bedrooms', 'overcrowding', 'qmobilephone', 'lugar1', 'age', 'SQBescolari', 'SQBage',\\\n            'SQBedjefe', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0e39d85a1dec727779cd5eba9a157b9945d75b0"},"cell_type":"markdown","source":"# Testing our model on trainset\nWe divides our trainset into train and test to check the accuracy of our model on our trainset."},{"metadata":{"trusted":true,"_uuid":"54ce88a3f324744432fc9b8b520c6863749ed078"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nX_train = X_train[features]\nX_test = X_test[features]\ny_train = y_train.values.ravel()\ny_test = y_test.values.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe41ac4ed7affce4619ff73f5eb34ae9bc68f960"},"cell_type":"code","source":"test_model = model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=500, verbose=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78d717ad0e83e42ec77309f8f94291a72f78bd96"},"cell_type":"code","source":"predictions = test_model.predict(X_test)\n\nprint('F1-macro score on train = ', f1_score(y_test, predictions, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05b985bb9c4ff11ee17cc2c0b8e22ec7395e8a46"},"cell_type":"markdown","source":"# Treating testset\nThe treatment for the testset is similar to what we have done for the trainset."},{"metadata":{"trusted":true,"_uuid":"99a77781671c0085998f9160457a1fa7fcf27a41"},"cell_type":"code","source":"print('Dtypes count:' + '\\n', test_data.dtypes.value_counts())\ncolumns_object = test_data.columns[test_data.dtypes == object]\nprint('Columns wich could have a problem :', \\\n      columns_object) # Columns wich need treatment beacause they are object type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bce1c1203462fa8aa66e08a6adc431330f41c9a"},"cell_type":"code","source":"test_data = correct_dependency(test_data) # Correcting the 'dependency' column problem\ntest_data = correct_edjefe_edjefa(test_data) # Correcting the 'edjefa' and 'edjefe' problem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7219b68bfad74c681b378a929170954214fc9764"},"cell_type":"code","source":"list_na = test_data.columns[test_data.isnull().any()].tolist() \n#It's a list of all columns that have NAN\nfor column in list_na:\n    series = test_data[column]\n    n_null = series.isnull().sum()\n    print('The column ' + column + ' has ' + str(n_null) + ' null values.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d2d77864c0744d8c4d14858a68c2a6ab677da11"},"cell_type":"code","source":"test_data = correct_v2a1(test_data)\ntest_data = correct_v18q1(test_data)\ntest_data = correct_meaneduc_rez_esc(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2efd9a411c50ac5b186875a2eeda590c8399cf4"},"cell_type":"code","source":"if not test_data.columns[test_data.isnull().any()].tolist(): \n    #It's a list of all columns that have NAN\n    print('There are no columns with NaN values on the testset')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0f3d01efcb5ddde4d5abed2ad023aa2839414ea"},"cell_type":"markdown","source":"# Training our model to make predictions"},{"metadata":{"trusted":true,"_uuid":"8ecb49919493b7bd6c85949c9d05204edb0f6809"},"cell_type":"code","source":"X = X[features]\ny = y.values.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"455397baa8b8b495fe76fc0e24f9cf562450050f"},"cell_type":"code","source":"prediction_model = model.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5345064ff0d6fd7452eb888d1b8fe32070c755c"},"cell_type":"code","source":"id_column = test_data.Id\n\ny_pred_final = prediction_model.predict(test_data[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a188cc59f2f7e6026ae84a9624be441a3238eaf"},"cell_type":"code","source":"file_to_submit = pd.DataFrame({'Id':id_column, 'Target':y_pred_final})\nfile_to_submit.to_csv('prediction.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ff644fc6ec13454e3f644b0426459d9eafb36d4"},"cell_type":"markdown","source":"# Conclusions\nIn this kernel, we fitted a Light and got a score of 0.97 meaning that this model can work a get good predictions overall.  \nOur next steps will be set with the goal of proving our dataset treatment, as well as finding possible correlations between features improving, possibly, our score. We will try to do a plot of number of features vs. score using recursive feature elimination and also improve our visualization sets, giving us a better look at our challenge."},{"metadata":{"trusted":true,"_uuid":"ccd0a2692ec9977bf8a229c439b522eadf0540c4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}