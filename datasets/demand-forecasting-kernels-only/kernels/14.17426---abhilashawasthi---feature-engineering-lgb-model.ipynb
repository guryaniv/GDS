{"cells":[{"metadata":{"_uuid":"454d802d65270b418e4d05e9cfd3093edba6cd25","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d519f87e986a4caff405792bdf5f88af064dd4b8"},"cell_type":"markdown","source":"## Loading data"},{"metadata":{"trusted":true,"_uuid":"40c4ad4e03ff3701bacf112e0cfb79321f0456e5"},"cell_type":"code","source":"# Loading the data\ntrain = pd.read_csv('../input/train.csv', parse_dates=['date'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['date'])\nsample_sub = pd.read_csv('../input/sample_submission.csv')\nprint('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03f82077e80bba187f3ff0d0cf58ba973c13c7ef"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"49f8496f37589f16e61d1f2aa965412a14ec7069"},"cell_type":"code","source":"# Concatenating train & test\ntrain['train_or_test'] = 'train'\ntest['train_or_test'] = 'test'\ndf = pd.concat([train,test], sort=False)\nprint('Combined df shape:{}'.format(df.shape))\ndel train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2c63fcf7e65e09b015f0d5aced7a2e0e009daf2"},"cell_type":"markdown","source":"### Date Features"},{"metadata":{"trusted":true,"_uuid":"424bf991b29ea359b007bcd8c2a8cdf054c1cc1f"},"cell_type":"code","source":"# Extracting date features\ndf['dayofmonth'] = df.date.dt.day\ndf['dayofyear'] = df.date.dt.dayofyear\ndf['dayofweek'] = df.date.dt.dayofweek\ndf['month'] = df.date.dt.month\ndf['year'] = df.date.dt.year\ndf['weekofyear'] = df.date.dt.weekofyear\ndf['is_month_start'] = (df.date.dt.is_month_start).astype(int)\ndf['is_month_end'] = (df.date.dt.is_month_end).astype(int)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"328bae42ea8aa58ebc512be340c55d3f05918b94"},"cell_type":"code","source":"# Sorting the dataframe by store then item then date\n#df.sort_values(by=['store','item','month','dayofweek'], axis=0, inplace=True)\ndf.sort_values(by=['store','item','date'], axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"019db2cf8e367672bb2339330767de1b70fae4cd"},"cell_type":"markdown","source":"### Monthwise aggregated sales values"},{"metadata":{"trusted":true,"_uuid":"28475aa2a93911d3149c2c38f369b22c12ab58b3"},"cell_type":"code","source":"def create_sales_agg_monthwise_features(df, gpby_cols, target_col, agg_funcs):\n    '''\n    Creates various sales agg features with given agg functions  \n    '''\n    gpby = df.groupby(gpby_cols)\n    newdf = df[gpby_cols].drop_duplicates().reset_index(drop=True)\n    for agg_name, agg_func in agg_funcs.items():\n        aggdf = gpby[target_col].agg(agg_func).reset_index()\n        aggdf.rename(columns={target_col:target_col+'_'+agg_name}, inplace=True)\n        newdf = newdf.merge(aggdf, on=gpby_cols, how='left')\n    return newdf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3caa2627c7c33bc8c273daf3ba87fada453a5ca"},"cell_type":"markdown","source":"### Features constructed from previous sales values"},{"metadata":{"trusted":true,"_uuid":"79559cff98ede8c16cff2f7f7d575436112488ae"},"cell_type":"code","source":"# Creating sales lag features\ndef create_sales_lag_feats(df, gpby_cols, target_col, lags):\n    gpby = df.groupby(gpby_cols)\n    for i in lags:\n        df['_'.join([target_col, 'lag', str(i)])] = \\\n                gpby[target_col].shift(i).values + np.random.normal(scale=1.6, size=(len(df),))\n    return df\n\n# Creating sales rolling mean features\ndef create_sales_rmean_feats(df, gpby_cols, target_col, windows, min_periods=2, \n                             shift=1, win_type=None):\n    gpby = df.groupby(gpby_cols)\n    for w in windows:\n        df['_'.join([target_col, 'rmean', str(w)])] = \\\n            gpby[target_col].shift(shift).rolling(window=w, \n                                                  min_periods=min_periods,\n                                                  win_type=win_type).mean().values +\\\n            np.random.normal(scale=1.6, size=(len(df),))\n    return df\n\n# Creating sales rolling median features\ndef create_sales_rmed_feats(df, gpby_cols, target_col, windows, min_periods=2, \n                            shift=1, win_type=None):\n    gpby = df.groupby(gpby_cols)\n    for w in windows:\n        df['_'.join([target_col, 'rmed', str(w)])] = \\\n            gpby[target_col].shift(shift).rolling(window=w, \n                                                  min_periods=min_periods,\n                                                  win_type=win_type).median().values +\\\n            np.random.normal(scale=1.6, size=(len(df),))\n    return df\n\n# Creating sales exponentially weighted mean features\ndef create_sales_ewm_feats(df, gpby_cols, target_col, alpha=[0.9], shift=[1]):\n    gpby = df.groupby(gpby_cols)\n    for a in alpha:\n        for s in shift:\n            df['_'.join([target_col, 'lag', str(s), 'ewm', str(a)])] = \\\n                gpby[target_col].shift(s).ewm(alpha=a).mean().values\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1abfbefd0f70daebfdce943d4a292d8a8d7f71be"},"cell_type":"markdown","source":"### OHE of categorical features"},{"metadata":{"trusted":true,"_uuid":"ee278af552f8180d1d087979104995d89f5736ff"},"cell_type":"code","source":"def one_hot_encoder(df, ohe_cols=['store','item','dayofmonth','dayofweek','month','weekofyear']):\n    '''\n    One-Hot Encoder function\n    '''\n    print('Creating OHE features..\\nOld df shape:{}'.format(df.shape))\n    df = pd.get_dummies(df, columns=ohe_cols)\n    print('New df shape:{}'.format(df.shape))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69d6af2b8e69c797cc27b887c0e87050ec18870d"},"cell_type":"markdown","source":"### Log Sales "},{"metadata":{"trusted":true,"_uuid":"9075af279047a4b90ef6d2bb96c268058621452b"},"cell_type":"code","source":"# Converting sales to log(1+sales)\ndf['sales'] = np.log1p(df.sales.values)\ndf.sample(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06b09956138bd3ef34009606aba6cf6f5065ae02"},"cell_type":"markdown","source":"## Time-based Validation set"},{"metadata":{"trusted":true,"_uuid":"e4d32958c67c166e1fe59b73bbe5b674c7335c52"},"cell_type":"code","source":"# For validation \n# We can choose last 3 months of training period(Oct, Nov, Dec 2017) as our validation set to gauge the performance of the model.\n# OR to keep months also identical to test set we can choose period (Jan, Feb, Mar 2017) as the validation set.\n# Here we will go with the latter choice.\nmasked_series = (df.year==2017) & (df.month.isin([1,2,3]))\nmasked_series2 = (df.year==2017) & (~(df.month.isin([1,2,3])))\ndf.loc[(masked_series), 'train_or_test'] = 'val'\ndf.loc[(masked_series2), 'train_or_test'] = 'no_train'\nprint('Train shape: {}'.format(df.loc[df.train_or_test=='train',:].shape))\nprint('Validation shape: {}'.format(df.loc[df.train_or_test=='val',:].shape))\nprint('No train shape: {}'.format(df.loc[df.train_or_test=='no_train',:].shape))\nprint('Test shape: {}'.format(df.loc[df.train_or_test=='test',:].shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69c7c13414eb865e98745489ab36bfb3d0b98a07"},"cell_type":"markdown","source":"## Model Validation"},{"metadata":{"trusted":true,"_uuid":"3e5de9c3d49c9a95887a8e7e9d110e2e72851594"},"cell_type":"code","source":"# Converting sales of validation period to nan so as to resemble test period\ntrain = df.loc[df.train_or_test.isin(['train','val']), :]\nY_val = train.loc[train.train_or_test=='val', 'sales'].values.reshape((-1))\nY_train = train.loc[train.train_or_test=='train', 'sales'].values.reshape((-1))\ntrain.loc[train.train_or_test=='val', 'sales'] = np.nan\n\n# # Creating sales lag, rolling mean, rolling median, ohe features of the above train set\ntrain = create_sales_lag_feats(train, gpby_cols=['store','item'], target_col='sales', \n                               lags=[91,98,105,112,119,126,182,364,546,728])\n\ntrain = create_sales_rmean_feats(train, gpby_cols=['store','item'], \n                                 target_col='sales', windows=[364,546], \n                                 min_periods=10, win_type='triang') #98,119,91,182,\n\n# # train = create_sales_rmed_feats(train, gpby_cols=['store','item'], \n# #                                 target_col='sales', windows=[364,546], \n# #                                 min_periods=10, win_type=None) #98,119,91,182,\n\ntrain = create_sales_ewm_feats(train, gpby_cols=['store','item'], \n                               target_col='sales', \n                               alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5], \n                               shift=[91,98,105,112,119,126,182,364,546,728])\n\n# # Creating sales monthwise aggregated values\n# agg_df = create_sales_agg_monthwise_features(df.loc[df.train_or_test=='train', :], \n#                                              gpby_cols=['store','item','month'], \n#                                              target_col='sales', \n#                                              agg_funcs={'mean':np.mean, \n#                                              'median':np.median, 'max':np.max, \n#                                              'min':np.min, 'std':np.std})\n\n# # Joining agg_df with train\n# train = train.merge(agg_df, on=['store','item','month'], how='left')\n\n# One-Hot Encoding \ntrain = one_hot_encoder(train, ohe_cols=['store','item','dayofweek','month']) \n#,'dayofmonth','weekofyear'\n\n# Final train and val datasets\nval = train.loc[train.train_or_test=='val', :]\ntrain = train.loc[train.train_or_test=='train', :]\nprint('Train shape:{}, Val shape:{}'.format(train.shape, val.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7732144f23f0a99b5846ca8315455b84f1ca46e"},"cell_type":"markdown","source":"## LightGBM Model"},{"metadata":{"_uuid":"17982e472879d213fd8e771c3156c89cdd954388"},"cell_type":"markdown","source":"### Training features"},{"metadata":{"trusted":true,"_uuid":"3c33f479d52628e200d2905e7c78ba43238be2a7","_kg_hide-output":true},"cell_type":"code","source":"avoid_cols = ['date', 'sales', 'train_or_test', 'id', 'year']\ncols = [col for col in train.columns if col not in avoid_cols]\nprint('No of training features: {} \\nAnd they are:{}'.format(len(cols), cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de085b7a91c27ff28248735e6c1387f432131644"},"cell_type":"code","source":"def smape(preds, target):\n    '''\n    Function to calculate SMAPE\n    '''\n    n = len(preds)\n    masked_arr = ~((preds==0)&(target==0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds-target)\n    denom = np.abs(preds)+np.abs(target)\n    smape_val = (200*np.sum(num/denom))/n\n    return smape_val\n\ndef lgbm_smape(preds, train_data):\n    '''\n    Custom Evaluation Function for LGBM\n    '''\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bebd7beef960cb6bcdf39a66ce1f5bb09ec23e04"},"cell_type":"code","source":"# LightGBM parameters\nlgb_params = {'task':'train', 'boosting_type':'gbdt', 'objective':'regression', \n              'metric': {'mae'}, 'num_leaves': 10, 'learning_rate': 0.02, \n              'feature_fraction': 0.8, 'max_depth': 5, 'verbose': 0, \n              'num_boost_round':15000, 'early_stopping_rounds':200, 'nthread':-1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f6c0b39b78177a733641da24c42abe18c2a33d9"},"cell_type":"code","source":"# Creating lgbtrain & lgbval\nlgbtrain = lgb.Dataset(data=train.loc[:,cols].values, label=Y_train, \n                       feature_name=cols)\nlgbval = lgb.Dataset(data=val.loc[:,cols].values, label=Y_val, \n                     reference=lgbtrain, feature_name=cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bf8adfe7d434402acaaa0b03399937f24dcd007"},"cell_type":"code","source":"def lgb_validation(params, lgbtrain, lgbval, X_val, Y_val, verbose_eval):\n    t0 = time.time()\n    evals_result = {}\n    model = lgb.train(params, lgbtrain, num_boost_round=params['num_boost_round'], \n                      valid_sets=[lgbtrain, lgbval], feval=lgbm_smape, \n                      early_stopping_rounds=params['early_stopping_rounds'], \n                      evals_result=evals_result, verbose_eval=verbose_eval)\n    print(model.best_iteration)\n    print('Total time taken to build the model: ', (time.time()-t0)/60, 'minutes!!')\n    pred_Y_val = model.predict(X_val, num_iteration=model.best_iteration)\n    pred_Y_val = np.expm1(pred_Y_val)\n    Y_val = np.expm1(Y_val)\n    val_df = pd.DataFrame(columns=['true_Y_val','pred_Y_val'])\n    val_df['pred_Y_val'] = pred_Y_val\n    val_df['true_Y_val'] = Y_val\n    print(val_df.shape)\n    print(val_df.sample(5))\n    print('SMAPE for validation data is:{}'.format(smape(pred_Y_val, Y_val)))\n    return model, val_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21eb1034ea7d6b6523d21aceb98849c9a1f29ce9"},"cell_type":"code","source":"# Training lightgbm model and validating\nmodel, val_df = lgb_validation(lgb_params, lgbtrain, lgbval, val.loc[:,cols].values, \n                               Y_val, verbose_eval=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9f60a5bce135813adf2ad8524ff72a67f72fb7b"},"cell_type":"code","source":"# Let's see top 25 features as identified by the lightgbm model.\nprint(\"Features importance...\")\ngain = model.feature_importance('gain')\nfeat_imp = pd.DataFrame({'feature':model.feature_name(), \n                         'split':model.feature_importance('split'), \n                         'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\nprint('Top 25 features:\\n', feat_imp.head(25))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70a1432509985f18bdb6dd1535838db297a22658"},"cell_type":"markdown","source":"## Final Model"},{"metadata":{"trusted":true,"_uuid":"a8781fd8f0fb9e98559a2c4f99519fc91c633e50"},"cell_type":"code","source":"# Creating sales lag, rolling mean, rolling median, ohe features of the above train set\ndf_whole = create_sales_lag_feats(df, gpby_cols=['store','item'], target_col='sales', \n                                  lags=[91,98,105,112,119,126,182,364,546,728])\ndf_whole = create_sales_rmean_feats(df_whole, gpby_cols=['store','item'], \n                                    target_col='sales', windows=[364,546], \n                                    min_periods=10, win_type='triang')\n# df = create_sales_rmed_feats(df, gpby_cols=['store','item'], target_col='sales', \n#                              windows=[364,546], min_periods=2) #98,119,\ndf_whole = create_sales_ewm_feats(df_whole, gpby_cols=['store','item'], target_col='sales', \n                                  alpha=[0.95, 0.9, 0.8, 0.7, 0.6, 0.5], \n                                  shift=[91,98,105,112,119,126,182,364,546,728])\n\n# # Creating sales monthwise aggregated values\n# agg_df = create_sales_agg_monthwise_features(df.loc[~(df.train_or_test=='test'), :], \n#                                              gpby_cols=['store','item','month'], \n#                                              target_col='sales', \n#                                              agg_funcs={'mean':np.mean, \n#                                              'median':np.median, 'max':np.max, \n#                                              'min':np.min, 'std':np.std})\n\n# # Joining agg_df with df\n# df = df.merge(agg_df, on=['store','item','month'], how='left')\n\n# One-Hot Encoding\ndf_whole = one_hot_encoder(df_whole, ohe_cols=['store','item','dayofweek','month']) \n#'dayofmonth',,'weekofyear'\n\n# Final train and test datasets\ntest = df_whole.loc[df_whole.train_or_test=='test', :]\ntrain = df_whole.loc[~(df_whole.train_or_test=='test'), :]\nprint('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d93caa77885762a3e6729341414f54c1c1d0675"},"cell_type":"code","source":"# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=train.loc[:,cols].values, \n                           label=train.loc[:,'sales'].values.reshape((-1,)), \n                           feature_name=cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"051cf13823f8edf3810e88016c90db5c14054ef2"},"cell_type":"code","source":"def lgb_train(params, lgbtrain_all, X_test, num_round):\n    t0 = time.time()\n    model = lgb.train(params, lgbtrain_all, num_boost_round=num_round, feval=lgbm_smape)\n    test_preds = model.predict(X_test, num_iteration=num_round)\n    print('Total time taken in model training: ', (time.time()-t0)/60, 'minutes!')\n    return model, test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4192fdca974d3c4fade051fac2e87723f12983b7"},"cell_type":"code","source":"# Training lgb model on whole data(train+val)\nlgb_model, test_preds = lgb_train(lgb_params, lgbtrain_all, test.loc[:,cols].values, model.best_iteration)\nprint('test_preds shape:{}'.format(test_preds.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23056dbbe715566d1cba5d1b6dafc3d9d972d128"},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1aba1607a8f47480213035f7af77f9ac6f9c30c7"},"cell_type":"code","source":"# Create submission\nsub = test.loc[:,['id','sales']]\nsub['sales'] = np.expm1(test_preds)\nsub['id'] = sub.id.astype(int)\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35eb0e52bff6541e1f948f6d43dab6a105818d67"},"cell_type":"markdown","source":"## WaveNet Model "},{"metadata":{"trusted":true,"_uuid":"bb5af7ea2e8f0cb18f6fc30061ede9422634262b"},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fe1b567de6709709f6310a760e284395510a5db"},"cell_type":"code","source":"df.date.min(), df.date.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae6542095a3558301661f2ccb6317094f14c7e17"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}