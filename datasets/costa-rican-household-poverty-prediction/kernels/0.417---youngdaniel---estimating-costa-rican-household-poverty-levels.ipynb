{"cells":[{"metadata":{"_uuid":"9f21d726a22dd2080614e3a8b98cdcbe3d2966d6"},"cell_type":"markdown","source":"This is my first kernel on Kaggle. This is also one of the first times applying machine learning to a real world problem (as opposed to an exercise from a class). I've used quite a lot of material from Will Koehrsen's walkthrough for this competition, with a few twists of m own added. His kernel can be viewed [here](https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough). "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Setup\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Any results you write to the current directory are saved as output.\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d30e725622408e095f4d75cd2a7d402a11888224"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntrain = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e9ad1ca2e7f2811e25e340016b54e3ed262d3d8"},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e9ffbf775608532aa9c2764bdb790f408175dad"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c47b5d9c8e7a1fca4957092f8ab75d91d9be577"},"cell_type":"markdown","source":"Something interesting to note here: the test data set has almost 24 thousand entries and the training set has less than 10 thousand entries. \n\nNote also that a lot of the columns in the 140+ column dataset are binary/boolean values. We can determine precisely which ones by looking at the descriptions, or we can count the number of unique values for each column and count the number of columns with only two unique values. "},{"metadata":{"trusted":true,"_uuid":"9d2a11541668020fa9bcf9ce2b05fe2623110b9a"},"cell_type":"code","source":"train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue')\nplt.xlabel('Number of Unique Values')\nplt.ylabel('Count')\nplt.title('Count of Unique Values in Columns of Type Int64')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1e76d1ca41cb02febab2d4bb625a10b638ffa28"},"cell_type":"markdown","source":"So we see that there are about 101 columns with boolean entries. (I say about because its possible that there is an entry with only two unique values which do not represent true and false but very unlikely.) Quite a few of these are actually categorical variables which have been encoded. For example `lugar1` to `lugar6` are simply columns indicating the region to which the household belongs. This is a purely cateogrical variable so aside from possibly removing `lugar6`, we will leave these columns as is. There are other columns which represent ordinal variables, such as say `eviv1`, `eviv2`, and `eviv3`, which tells us whether the floor is \"bad\", \"regular\", or \"good\", in that order. We may merge these three columns into one, with entries 0, 1, or 2, indicating whether the house has `eviv1 = 0` and so on.\n\nOf the remaining columns, 8 are floats. These include `v2a1`, `v18q1`, `rez_esc`, `mean_educ`, `overcrowding`, and a few columns containing squared data. We will ignore the columns with squared data, since they're only really needed witih linear models. Of the five float columns we are focusing on, `v2a1`, `mean_educ`, and `overcrowding` are the only ones for which a float makes sense. `v18q1` and `rez_esc` are both counts of discrete objects (tablets and years behind in schooling respectively) and only take on integer values. "},{"metadata":{"_uuid":"f3e26f79b63edc90bc9cddbb0888ad494ad87e2c"},"cell_type":"markdown","source":"The data we have is organized at the individual level, with certain columns which are pertinent to the individual in particular and certain others which are for the entire household. Since our task is only to classify at the household level, we will distinguish the individuals who are the heads of their households. We will also separate the household level columns and the columns which give features at the individual level. The latter will need to be aggregated before we proceed. "},{"metadata":{"trusted":true,"_uuid":"6194c9c37b7d58122d4fb4db11112ade1d4644e1"},"cell_type":"code","source":"# Quality of life changes:\ntest['Target'] = np.nan\ndata = train.append(test, ignore_index = False)\n\nheads = data.loc[data['parentesco1'] == 1, :]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0d5309b6f035199b6b9196b4b439404893dc7c1"},"cell_type":"markdown","source":"**Correcting Errors in the Data**\n\nWe know from the discussion that there are some households with incosistent Target data, that is, with members whose Target is different from that of the head of their household. We know that the correct labels for these individuals is the label that has been assigned to the head of their household. We turn now to correcting such errors for the sake of completeness. "},{"metadata":{"trusted":true,"_uuid":"fa677d721c3cf4c4b3f275af2355a2a9b4be0394"},"cell_type":"code","source":"consistent = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\ninconsistent = consistent[consistent != True]\nfor household in inconsistent.index:\n    actual = int(train[(train['idhogar'] == household) & (train['parentesco1'] == 1)]['Target'])\n    train.loc[train['idhogar'] == household, 'Target'] = actual\n    \n# let's check again for inconsistencies\nconsistent = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\ninconsistent = consistent[consistent != True]\nprint('There are {} households with inconsistent Target labels.'.format(len(inconsistent)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e9b9d7b28ae69dd5c3c9ebdd3b8d622aaf51583"},"cell_type":"markdown","source":"**Exploring Missing Values in the Data**"},{"metadata":{"trusted":true,"_uuid":"881f00bc413126bc219bdce5d1d09eba4ddb4239"},"cell_type":"code","source":"missing = train.isnull().sum().sort_values(ascending = False)\nmissing = missing[missing > 0]\nmissing = (missing/len(train)) # express missing counts as ratio of whole\nmissing.round(3).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9a10195279ea149e1c771be853c654349182a75"},"cell_type":"markdown","source":"Three columns have a significant number of missing values. The other two, `meaneduc` and `SQBmeaned` can actually be derived from the other values in the same row.  The column `rez_esc` represents the number of years that the individual represented is behind in their schooling. \n\n>This [data] is only collected for people between 7 and 19 years of age and [is] the difference between the years of education a person should have and the years of education he/she has. [It] is capped at 5. \n\nThe second column, `v18q1`, counts the number of tablets the household owns. The third column, `v2a1`,  records the household's monthly rent payment. `v18q` indicates whether the household owns a tablet or not. It is possible that `v18q1` lists `NaN` for the households for whom `v18q` is 0. "},{"metadata":{"trusted":true,"_uuid":"04320b012940a41bb522c61e92ed8b6eebf8a3a7"},"cell_type":"code","source":"heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())\n# This counts the number of null entries in v18q1, grouped by v18q. \n\n# Attribute error raised for .isnull().sum(). the above seems to be the right way to do that. Why is this?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc75878cee6d273a5368976c41cc94d8834fb3b4"},"cell_type":"markdown","source":"It seems that the `v18q1` column is `NaN` whenever `v18q` is 0. Thus, we may simply replace the `NaN` entries in `v18q1` with zeroes and possibly drop the `v18q` column altogether. "},{"metadata":{"trusted":true,"_uuid":"f0344be53a3575c0bd575570f356ed8312e45e8d"},"cell_type":"code","source":"data = data.fillna({'v18q1':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a0730abf30d22a352a85ed56a9d656dc9eeac16"},"cell_type":"code","source":"# and now we want to graph\ndf = heads.loc[:, ['idhogar', 'v18q1', 'Target']]\nrelative = df.groupby('Target')['v18q1'].value_counts()\nrelative = relative/relative.groupby('Target').sum()\nrelative = relative.rename('counts').reset_index()\ng = sns.catplot(data = relative, col = 'Target', kind = 'bar', y= 'counts', x = 'v18q1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d73f5f387b248e14cc21fc80b8632096c1b069f0"},"cell_type":"markdown","source":"Could the same be true of `v2a1`? That is, could it be that the households who do not have a rent payment listed simply do not pay rent, say, for example, because they own the house already? To find out, we take a look at the `tipovivi` columns:\n\n```\ntipovivi1, =1 own and fully paid house\ntipovivi2, =1 own, paying in installments\ntipovivi3, =1 rented\ntipovivi4, =1 precarious\ntipovivi5, =1 other(assigned, borrowed)\n```"},{"metadata":{"trusted":true,"_uuid":"528d11bf6f63587bebf732e19f150f63663b9b31"},"cell_type":"code","source":"house_vars = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5']\nnulls = train.loc[train.v2a1.isnull(), house_vars].sum()\nnotnulls = train.loc[train.v2a1.notnull(), house_vars].sum()\npd.DataFrame(data = {'isnull': nulls, 'notnull': notnulls})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07f4679c9375cf69f32aefe9061b7470f7d64fcb"},"cell_type":"markdown","source":"So we see that the households with entries in `v2a1` are households which either own the house and are paying it off in installments or are renting the house and the households with null entries in `v2a1` are households which either own the house, are classified as \"precarious\", or belong in the \"other\" category. In the case of households that own the house, we may simply replace the null entries with 0.  For the other cases, we will have to impute these values, but we should add a column indicating that they did not provide rent data."},{"metadata":{"_uuid":"17916281ccd2a7f17b493b28443d70acc11d4ebc","trusted":true},"cell_type":"code","source":"data.loc[data.tipovivi1 == 1, 'v2a1'] = 0 # if the family owns the house set rent to 0. \n\ndata['v2a1-missing'] = data['v2a1'].isnull()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bc7a6d68b0537703a1d7e87c120aa8084f2c785"},"cell_type":"markdown","source":"Finally we have `rez_esc`.  From the description, we know that only persons aged 7 to 19 will have values collected for this variable. Thus we may as well set the value for those outside of this age range to 0 and then add a column indicating which entries had to be imputed, as with `v2a1`."},{"metadata":{"trusted":true,"_uuid":"eb24be5b910b3f207b97846306d9551361da4f81"},"cell_type":"code","source":"data.loc[((data['age'] < 7) | (data['age'] > 19)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n\ndata['rez_esc-missing'] = data['rez_esc'].isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd01f8c63e1caaad99d5edc5423e3bb2e7174947"},"cell_type":"code","source":"heads = data.loc[data.parentesco1 == 1, :]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cfb4cba0c2b2d73d57912b49bfb820498881f3c"},"cell_type":"markdown","source":"We now proceed to separate the data as needed into their different categories. For example, we will distinguish between boolean colums at the household level, boolean columns at the individual level, categorical columns at the household level, and so on. We will also discard the squared variables, which are only really necessary when building a linear model. "},{"metadata":{"trusted":true,"_uuid":"7fa2ada286b777ac99046de17a2d024c20bdbd84"},"cell_type":"code","source":"id_ = ['Id', 'idhogar', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01f9b47de6c75575fe911ae4e84f5baf51b0180a"},"cell_type":"code","source":"ind_bool = ['dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['rez_esc', 'escolari', 'age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47313b34385ea1b6f4359a15c087909128cdbb12"},"cell_type":"code","source":"hh_bool = ['v18q', 'hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d87a68042359a953edaa8316f0cc990644274c0"},"cell_type":"markdown","source":"**Household Level Variables**"},{"metadata":{"trusted":true,"_uuid":"8ba2268c2c8af5b893203ee165283bff2e5cca81"},"cell_type":"code","source":"heads = heads[id_ + hh_bool + hh_cont + hh_ordered]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33d9ab183097a45841018c4a70c60bc2bd6439d2"},"cell_type":"markdown","source":"A quick note here.  `dependency`, `edjefe`, and `edjefa` are listed here as continuous variables, but the column data is of dtype object. This is primarily because some of the entries are \"no\", and some are \"yes\".  From the variable description, we know that \"yes\" represents 1 and \"no\" represents 0. Presumably, a \"no\" in `edjefe`means that the household leader is not male. To simplify matters, we will combine the data in `edjefe` and `edjefa` and then add a column that indicates whether the household leader is male or female. "},{"metadata":{"trusted":true,"_uuid":"40ca7d075cc1a0c1ac01fbd8e4a7f23c95e1fcc8"},"cell_type":"code","source":"maps = {'yes': 1, 'no': 0}\n\nheads['dependency'] = heads.dependency.replace(maps).astype(np.float64)\nheads['edjef'] = heads.edjefe.replace(maps).astype(np.float64) + heads.edjefa.replace(maps).astype(np.float64)\nheads['leader_male'] = heads.edjefe.map(lambda x: x != 0)\nheads = heads.drop(columns = ['edjefe', 'edjefa'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5377358474e6d352da99ab0c43abc860d694b5b8"},"cell_type":"markdown","source":"Note now that we have several variables which seem to overlap:\n\n* r4t3, Total persons in the household\n* tamhog, size of the household\n* tamviv, number of persons living in the household\n* hhsize, household size\n* hogar_total, number of total individuals in the household\n\nLet us examine the correlation matrix of these five features."},{"metadata":{"trusted":true,"_uuid":"80a41f4c5f928d9f47eb7865db2e8b052cac3ec1"},"cell_type":"code","source":"sns.heatmap(heads[['r4t3', 'tamhog', 'tamviv', 'hhsize', 'hogar_total']].corr(), annot = True, fmt = '.3f')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34a96d60516b855da45490f5ad6cc57a074013e2"},"cell_type":"markdown","source":"We see that `r4t3`, `tamhog`, `hhsize`, and `hogar_total` hold essentially the same data, so we will choose one (`hhsize`) and discard the rest. On the other hand, notice that `tamviv`, although highly correlated, differs slightly from `hhsize`. `tamviv` counts the number of people living in the household, the organizers have indicated this sometimes includes domestic workers/househelp, while `hhsize` counts the number of people who belong to the household, not all of whom may live in the household. "},{"metadata":{"trusted":true,"_uuid":"e92fe71a06ee50d3c284bc45f0c103298b3d639a"},"cell_type":"code","source":"heads.plot.scatter(x = 'tamviv', y = 'hhsize')\n\nsns.jointplot(x = 'tamviv', y = 'hhsize', data = heads, kind = 'hex', gridsize = 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1965ad3b112bdababfa5d5ca47d853b5c1729a52"},"cell_type":"markdown","source":"There aren't any households for which `tamviv` is less than `hhsize`. There are some households with larger `tamviv` than `hhsize`, but as we can see from the hexplot, they are a small minority. Nevertheless, this may come into play, so we will create a new feature showing the difference between `tamviv` and `hhsize`. "},{"metadata":{"trusted":true,"_uuid":"2bd03f806303667810f4a79f8d9e67ddc21c6b4c"},"cell_type":"code","source":"heads['hhsize-diff'] = heads.hhsize - heads.tamviv\nheads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd02e557a8712b91a1e669d8736f3c8330e46360"},"cell_type":"markdown","source":"There are a number of other categorical variables which have already been encoded in the dataset. For example:\n\n* paredblolad, =1 if predominant material on the outside wall is block or brick\n* paredzocalo, =1 if predominant material on the outside wall is socket (wood,  zinc or asbestos)\n* paredpreb, =1 if predominant material on the outside wall is prefabricated or cement\n* pareddes, =1 if predominant material on the outside wall is waste material\n* paredmad, =1 if predominant material on the outside wall is wood\n* paredzinc, =1 if predominant material on the outside wall is zink\n* paredfibras, =1 if predominant material on the outside wall is natural fibers\n* paredother, =1 if predominant material on the outside wall is other\n\nWe can delete the `paredother` column in this case. Similarly, we can delete `pisoother`, `techootro`,  `abastaguafuera`,  `coopele`, `sanitario6`, `energcocinar4`, `elimbasu6`, `tipovivi5`, `lugar6`, `area2`. \n\nWe would also like to create oridnal variables out of the boolean variables like `epared1`, `epared2`, `epared3`, which are binary variables indicating whether the walls are \"bad\", \"regular\", or \"good\" respectively. "},{"metadata":{"trusted":true,"_uuid":"f8e9f8609cf5dbe548623f2a44b28465188a0b6e"},"cell_type":"code","source":"heads = heads.drop(columns = ['paredother', 'pisoother', 'techootro', 'abastaguafuera', \n                      'coopele', 'sanitario6', 'energcocinar4', 'elimbasu6', \n                      'tipovivi5', 'lugar6', 'area2'])\n# delete the dummy variable trap columns\n\nheads['wall'] = np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]), axis = 1)\nheads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]), axis = 1)\nheads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]), axis = 1)\nheads = heads.drop(columns = ['epared1', 'epared2', 'epared3', \n                      'eviv1', 'eviv2', 'eviv3', \n                      'etecho1', 'etecho2', 'etecho3'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"931d0ef3bdea772f8c54f1fd8a2555c4ac568c39"},"cell_type":"markdown","source":"**Some Additional Features**"},{"metadata":{"trusted":true,"_uuid":"8b3359d5031da47046ec285ea381a9f9fcbb23d0"},"cell_type":"code","source":"heads['phones-per-capita'] = heads['qmobilephone']/heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1']/heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms']/heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1']/heads['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f2769c29cc6c4c94f214bd3fdf13efc14a2dfc8"},"cell_type":"markdown","source":"**Individual Level Variables**"},{"metadata":{"trusted":true,"_uuid":"84abdc1d3ed1f856ba4aff51b0aaf63bc5d1ec09"},"cell_type":"code","source":"ind = data[id_ + ind_bool + ind_ordered]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28f14fa3a1cedfaaf4fb0c2d8df976f0dd14c2d2"},"cell_type":"markdown","source":"As before, we have some boolean columns which are categorical variables that have been encoded. In particular, we can look at the `instlevel_` variables. We also have `male` and `female`, one of which we can remove. "},{"metadata":{"trusted":true,"_uuid":"0057c32940fe85d812241c57b182b3e03dcb7bed"},"cell_type":"code","source":"ind['inst'] = np.argmax(np.array(ind[['instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5',\n                                      'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']]), axis = 1)\nind = ind.drop(columns = [c for c in ind if c.startswith('instlevel')])\nind = ind.drop(columns = 'female')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08dbbfb0432999c3860e44fac0e35feab5927997"},"cell_type":"markdown","source":"Now we have to deal with aggregation. For the categorical variables such as `dis`, which specifies whether the individual is disabled or not, and `male`, we will take the mean over the entire household. This will tell us what porportion of the whole household is disabled or male. For the variables which indicate the civil status of the individuals, we will take the sum.  "},{"metadata":{"trusted":true,"_uuid":"919eb4609d7bd5384f75b0ca930ee2c6b18eaef0"},"cell_type":"code","source":"ind = ind.drop(columns = 'Target') # We don't need Target data from the individual level\nind_agg_ordered = ind[['age', 'escolari', 'rez_esc', 'inst', 'idhogar']].groupby('idhogar').agg(['min', 'max'])\n# rename the columns\nnew_col = []\nfor c in ind_agg_ordered.columns.levels[0]:\n    for stat in ind_agg_ordered.columns.levels[1]:\n        new_col.append(f'{c}-{stat}')\nind_agg_ordered.columns = new_col\n\nind_agg = ind.groupby('idhogar').agg('mean')\n# rename the columns\nnew_col = []\nfor c in ind_agg:\n    new_col.append(f'{c}-mean')\nind_agg.columns = new_col\n\n#concatenate the dataframes\nind_agg = pd.concat([ind_agg, ind_agg_ordered], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc68a25fe7b63a0754a497e8e8175e350df17104"},"cell_type":"markdown","source":"Are any of these variables we've created correlated with each other?"},{"metadata":{"trusted":true,"_uuid":"fa823d65496d47190656a6f3710bc1751ffcad74"},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = ind_agg.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f84158e4d4fe0e6f6d57e7b48ca8266bfe3cb548"},"cell_type":"markdown","source":"We will drop these three columns and then create the final data."},{"metadata":{"trusted":true,"_uuid":"188692d80e8ced24db2d3c9e2cf3f2e9da34ded5"},"cell_type":"code","source":"ind_agg = ind_agg.drop(columns = to_drop)\nfinal = heads.merge(ind_agg, on = 'idhogar', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d6a907bc387644a1914e15b4d049e2ca5d7b9f1"},"cell_type":"markdown","source":"The fourth class (non-vulnerable) is more easily distinguished from the first three. We will split our classification into two portions. First, we wish to separate the fourth class from the first three. Once we have accomplished this, we will try to train the machine to distinguish between the three vulnerable classes. "},{"metadata":{"trusted":true,"_uuid":"5c6f44d10cb83394dd8f01ae77bd5ff5ecd29a38"},"cell_type":"code","source":"from sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# custom scorer with macro f1\nscorer = make_scorer(f1_score, greater_is_better = True, average = 'macro')\n\n# this is where we will make our predictions when submitting\nsubmissions_base = test[['Id', 'idhogar']].copy()\n\ntrain_set = final[final.Target.notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = final[final.Target.isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntrain_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\ntest_ids = list(final.loc[final.Target.isnull(), 'idhogar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"568205b51c759b2b538952d3854297ca73f41d52"},"cell_type":"code","source":"def split_model(model, train_set, train_labels, test_set, test_ids):\n    \"\"\" \n    Main learning module, two phases\n    data is household level DataFrame\n    \"\"\"\n    \n    # Phase 1: Distinguish class 4 from classes 1-3\n    train0_labels = np.array([x < 4 for x in train_labels])\n    model.fit(train_set, train0_labels)\n    test0_labels = model.predict(test_set)\n    \n    # Filter out the non-vulnerable households\n    test1_set = test_set[test0_labels]\n    test1_ids = [test_ids[i] for i in range(len(test_ids)) if test0_labels[i]]\n    train1_set = train_set[train0_labels]\n    train1_labels = train_labels[train0_labels]\n    \n    # Phase 2: Distinguish between classes 1-3\n    model.fit(train1_set, train1_labels)\n    labels = model.predict(test1_set)\n    labels = pd.DataFrame({'idhogar': test1_ids, 'Target': labels})\n    \n    #Everything that hasnt been given a label by Phase 2 either has no parentesco1==1 or belongs to class 4\n    submission = submissions_base.merge(labels, how = 'left', on = 'idhogar').drop(columns = 'idhogar')\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    return submission\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"280c1fa4d068ad811c9055378f48d6d5de5f3da6"},"cell_type":"code","source":"#features = list(train_set.columns)\npipeline = Pipeline([('imputer', Imputer(strategy = 'median')), ('scaler', MinMaxScaler())])\n\n# Impute missing values as well as scale data\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dad215cdd3ff385231f415167fcc7c9b2268b1c"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\ncv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\ncv_score.mean(), cv_score.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30fd938430d4b8518ce16a4e7cb3f1019c88a50b"},"cell_type":"code","source":"RF_submission = split_model(model, train_set, train_labels, test_set, test_ids)\nRF_submission.to_csv('RF_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"3034f827c44abdb7f8d26bdcae8c773fa877c3f1"},"cell_type":"markdown","source":"Submitting this result gives us a surprisingly high score of 0.414. I say surprising because the random forest classifier run without phases yields a cross valuation score as seen above. Would this score improve with some parameter tuning or perhaps a more powerful gradient boost model? "},{"metadata":{"trusted":true,"_uuid":"cfdc243e6f8ffd6ae0b5b8a05810bc8528f412b9"},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier(objective = 'multi:softprob', num_class = 4, learning_rate = 0.006)\n#cv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\n#cv_score.mean(), cv_score.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"593f371c51e4d91e514a0b5bcab6295be6d18f33"},"cell_type":"code","source":"# Phase 1: Distinguish class 4 from classes 1-3\nmodel = XGBClassifier(learning_rate = 0.05)\ntrain0_labels = np.array([x < 4 for x in train_labels])\nmodel.fit(train_set, train0_labels)\ntest0_labels = model.predict(test_set)\n\n# Filter out the non-vulnerable households\ntest1_set = test_set[test0_labels]\ntest1_ids = [test_ids[i] for i in range(len(test_ids)) if test0_labels[i]]\ntrain1_set = train_set[train0_labels]\ntrain1_labels = train_labels[train0_labels]\n\n# Phase 2: Distinguish between classes 1-3\nmodel = XGBClassifier(objective = 'multi:softprob', num_class = 4, learning_rate = 0.05)\nmodel.fit(train1_set, train1_labels)\nlabels = model.predict(test1_set)\nlabels = pd.DataFrame({'idhogar': test1_ids, 'Target': labels})\n\n#Everything that hasnt been given a label by Phase 2 either has no parentesco1==1 or belongs to class 4\nsubmission = submissions_base.merge(labels, how = 'left', on = 'idhogar').drop(columns = 'idhogar')\nsubmission['Target'] = submission['Target'].fillna(4).astype(np.int8)\nsubmission.to_csv('xgb_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37d13b61413143e9f4f0d563e516415c841ce98e"},"cell_type":"code","source":"import lightgbm as lgb\n\nmodel = lgb.LGBMClassifier(boosting_type = 'dart', colsample_bytree = 0.88, learning_rate = 0.028,\n                          min_child_samples = 10, num_leaves = 36, reg_alpha = 0.76, reg_lambda = 0.43,\n                          subsample_for_bin = 40000, subsample = 0.54, class_weight = 'balanced',\n                          objective = 'multiclass', n_estimators = 100, random_state = 10)\ncv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\ncv_score.mean(), cv_score.std()\n\nmodel = lgb.LGBMClassifier(boosting_type = 'dart', colsample_bytree = 0.88, learning_rate = 0.028,\n                          min_child_samples = 10, num_leaves = 36, reg_alpha = 0.76, reg_lambda = 0.43,\n                          subsample_for_bin = 40000, subsample = 0.54, class_weight = 'balanced',\n                          objective = 'binary', n_estimators = 100, random_state = 10)\ntrain0_labels = np.array([x < 4 for x in train_labels])\nmodel.fit(train_set, train0_labels)\ntest0_labels = model.predict(test_set)\n\n# Filter out the non-vulnerable households\ntest1_set = test_set[test0_labels]\ntest1_ids = [test_ids[i] for i in range(len(test_ids)) if test0_labels[i]]\ntrain1_set = train_set[train0_labels]\ntrain1_labels = train_labels[train0_labels]\n\n# Phase 2: Distinguish between classes 1-3\nmodel = lgb.LGBMClassifier(boosting_type = 'dart', colsample_bytree = 0.88, learning_rate = 0.028,\n                          min_child_samples = 10, num_leaves = 36, reg_alpha = 0.76, reg_lambda = 0.43,\n                          subsample_for_bin = 40000, subsample = 0.54, class_weight = 'balanced',\n                          objective = 'multiclass', n_estimators = 100, random_state = 10)\nmodel.fit(train1_set, train1_labels)\nlabels = model.predict(test1_set)\nlabels = pd.DataFrame({'idhogar': test1_ids, 'Target': labels})\n\n#Everything that hasnt been given a label by Phase 2 either has no parentesco1==1 or belongs to class 4\nsubmission = submissions_base.merge(labels, how = 'left', on = 'idhogar').drop(columns = 'idhogar')\nsubmission['Target'] = submission['Target'].fillna(4).astype(np.int8)\nsubmission.to_csv('lgb_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a64218edfe38e4bee52d9d3af65053f9bad19ca9"},"cell_type":"code","source":"\n\n\n\n\n\n\n\n\n\n\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}