{"cells":[{"metadata":{"_cell_guid":"587dae31-5a55-449e-9eac-1af88bdcca59","_uuid":"012d8e3d0d3714e49ee77614f3573fdaed68585a"},"cell_type":"markdown","source":"Approach in this kernel can be divided into following steps:\n- extracting features: speciall care is taken by creating dummy (categorical) features from string features Ticket, Name, Cabin\n- selecting significant features using logistic regression with L1 (lasso) regularization. Significant features are those, that have non-zero coefficients\n- taking 4 ML Algorithms: SVC, RandomForest, KNeighbors and Logistic regression and for each\n    - searching optimal value of regularization parameter in L1 logistic regression used in feature selction (grid search)\n    - determing if interaction terms help (grid search)\n- training a soft voting classifier to make prediction\n\nPublic score of this kernel is 0.78-0.80"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import Imputer, LabelBinarizer, StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline, FeatureUnion","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"65f84595-9a51-441f-8aea-f5a16d86a21c","_uuid":"d37abdb8b763903d305c7346855307bcf0dd8644","collapsed":true,"trusted":true},"cell_type":"code","source":"rawdata=pd.read_csv(\"../input/train.csv\") #reading data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"loading CategoricalEncoder from future scikit learn ( http://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.CategoricalEncoder.html ): "},{"metadata":{"_cell_guid":"8697f40d-8f2f-4d27-bb5b-940757acf0c3","_uuid":"4b9cb7b90fa262f80a6ad07bd358726c6f9d6ae9","collapsed":true,"trusted":true},"cell_type":"code","source":"# Definition of the CategoricalEncoder class, copied from PR #9151.\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n        return self\n\n    def transform(self, X):\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f3ed82dc-31fc-4625-99e9-0a1bfe31155a","_uuid":"cbf533d1fd3c849584ac5420466efc4a0a05b9a9"},"cell_type":"markdown","source":"Defining some help function for parsing string data:"},{"metadata":{"_cell_guid":"096a4caa-e4cb-4613-88d0-7a5ec93667ec","_uuid":"bdef00444e52df7b4258345b63035b7c7f3030f4","collapsed":true,"trusted":true},"cell_type":"code","source":"def first_digit(x):\n    if str.isdigit(x):\n        return x[0]\n    else:\n        return x\n    \ndef first_letter(x):\n    if isinstance(x,str):\n        return x[0]\n    else:\n        return '0'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"786e20a8-688f-4803-a5f2-d7f817872c2a","_uuid":"2e555ee2d8ac1f48145663d50b5a915d6ce88ed6"},"cell_type":"markdown","source":"Defining new transformators:"},{"metadata":{"_cell_guid":"738814f4-fc6c-4786-b250-9215965fa3d6","_uuid":"2770710e2b53935d97bd79f39e047c03c28f4f9f","collapsed":true,"trusted":true},"cell_type":"code","source":"# class for droping data with some incomplete features:\nclass NaDroper(BaseEstimator, TransformerMixin):  \n    def fit(self,X,y=None):\n        return self\n    def transform(self, X):\n        features_to_check=[\"Pclass\", \"Sex\", \"Embarked\", \"SibSp\", \"Parch\"]\n        return X.dropna(subset=features_to_check)\n    \n# class for selecting numerical features:\nclass NumSelector(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.features=[\"Age\", \"Fare\", \"SibSp\", \"Parch\"]  \n    def fit(self,X,y=None):\n        return self\n    def transform(self, X):\n        XX=X.copy()\n        return XX[self.features]\n    \n# class for selecting/creating categorical features:    \nclass CatSelector(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.features=[\"Pclass\", \"Sex\", \"Embarked\", \"CabinCat\",\n                       \"Title\", \"TicketCat\", \"SibSpBin\", \"ParchBin\", \"AgeBin\"]       \n    def fit(self,X,y=None):\n        return self\n    def transform(self, X):\n        XX=X.copy()\n        #extracting Title:\n        XX['Title']=XX['Name'].str.split(',').map(lambda x: x[1]).str.split('.').map(lambda x: x[0])\n        #extracting the string in front of Ticket number OR first digit of Ticket number\n        XX['TicketCat']=XX['Ticket'].str.split(' ').map(lambda x: x[0]).str.split('/').map(lambda x: x[0])\\\n        .str.replace('.','').map(lambda x: first_digit(x))\n        XX[\"SibSpBin\"]=X[\"SibSp\"]>0\n        XX[\"ParchBin\"]=X[\"Parch\"]>0\n        #extracting first letter of Cabin\n        XX[\"CabinCat\"]=X[\"Cabin\"].map(lambda x: first_letter(x))\n        XX[\"AgeBin\"]=np.isnan(X[\"Age\"])\n        return XX[self.features]  \n\n#class for selecting important features\nclass LogL1Selector(BaseEstimator, TransformerMixin):\n    def __init__(self, C=1):\n        self.C=C\n    def fit(self,X,y):\n        self.lgr=LogisticRegression(C=self.C, penalty=\"l1\", fit_intercept=True, tol=1e-6)\n        self.lgr.fit(X,y)\n        self.features=np.where(self.lgr.coef_!=0)[1]\n        return self\n    def transform(self, X):\n        return X[:,self.features]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5a9655c7-cd1f-4d26-aa56-b4e75b42ede2","_uuid":"2757d4d7f1f01b3ff568b4c26614482b07e10276"},"cell_type":"markdown","source":"Creating NaDroper instance and pipelines:"},{"metadata":{"_cell_guid":"72195438-5107-456b-ac17-c1bb8bc08863","_uuid":"cad85786f9633d76f1c78b2ab2817528e8033b85","collapsed":true,"trusted":true},"cell_type":"code","source":"# instance of NaDroper class for droping the data that are incomplete in categorical features.\nnadroper=NaDroper()\n\n# pipeline to preprocess numerical data\nnum_pipeline=Pipeline([('numselector',NumSelector()),\n                      ('imputer',Imputer(strategy='median')),\n                      ('standardscaler',StandardScaler())\n                      ])\n# pipeline to preprocess categorical data\ncat_pipeline=Pipeline([('catselector',CatSelector()),\n                       ('categoricalencoder',CategoricalEncoder(encoding='onehot-dense', \n                                                               handle_unknown='ignore')),\n                      ])\n# joint pipeline for preprocessing both numerical and categorical data:\npreprocess_pipeline0=FeatureUnion(transformer_list=[('numpip',num_pipeline),\n                                                    ('catpip',cat_pipeline)])\n\n# pipeline for possible inclusion of interactions and for feature selction\npreprocess_pipeline1=Pipeline([('ppip0', preprocess_pipeline0),\n                               ('polyf', PolynomialFeatures(degree=1)),\n                               ('logl1selector',LogL1Selector(C=1))])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8aaefa42-5fdb-4a2e-87d5-bd4cfdea6f65","_uuid":"4881e7c3264ace898c15f6eef9e33eaf78a356ae"},"cell_type":"markdown","source":"Full Pipelines with 4 different ML algorithms:"},{"metadata":{"_cell_guid":"cf45ec98-bfad-4b4e-8731-cdc76092f96f","_uuid":"2ae2969bfd6c312c02d53acd9f95cb5f9a27efa4","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfull_pipeline=[]\nfull_pipeline.append(Pipeline([('ppip1', preprocess_pipeline1),\n                        ('algo', SVC(kernel='rbf',probability=True))]))\n\nfull_pipeline.append(Pipeline([('ppip1', preprocess_pipeline1),\n                        ('algo', RandomForestClassifier(n_estimators=1000, max_depth=3))]))\n\nfull_pipeline.append(Pipeline([('ppip1', preprocess_pipeline1),\n                        ('algo', KNeighborsClassifier())]))\n\nfull_pipeline.append(Pipeline([('ppip1', preprocess_pipeline1),\n                        ('algo', LogisticRegression(tol=1e-6))]))\n\n# parametric grid we will search in\nparam_grid ={'ppip1__polyf__degree':[ 1, 2],\n         'ppip1__polyf__interaction_only':[True],\n         'ppip1__logl1selector__C':[0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,2.5,5,7.5,10,15],\n        }","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5ea77669-9514-4d6a-87ab-3bca1f500e04","_uuid":"0fb9524780d42ef90ce93028b6fa8201d5ba75c5"},"cell_type":"markdown","source":"Searching for best input data for each algorithm"},{"metadata":{"_cell_guid":"9572bd38-9176-408d-83cf-f402f4afd20f","_uuid":"e3a4a953ca6b02783ddb1a1b8f1d27ef10954f14","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n\ndata_cleaned=nadroper.fit_transform(rawdata)\ny=data_cleaned['Survived']\n\ngscv=[None] * len(full_pipeline)\ntop_estim=[None] * len(full_pipeline)\nfor k in range(len(full_pipeline)):\n    gscv[k]=GridSearchCV(full_pipeline[k], param_grid, cv=3, scoring='accuracy',n_jobs=-1,\n                                   verbose=1)\n    gscv[k].fit(data_cleaned,data_cleaned[\"Survived\"])\n    print(gscv[k].best_params_)\n    top_estim[k]=gscv[k].best_estimator_\n    print(cross_val_score(top_estim[k], data_cleaned,y))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d3966e6-b6f7-41c1-9270-b3eb22206c87","_uuid":"5f881b69bd7b0481cfa3d95175791cd8a29dcf64"},"cell_type":"markdown","source":"List of best estimators:"},{"metadata":{"_cell_guid":"8124d7b7-c3bf-4a9a-86c3-e2d288d71aaf","_uuid":"c94ddc86ca8c68fc57deb6778c614ff7ffc97e53","collapsed":true,"trusted":true},"cell_type":"code","source":"estimators=[('est'+str(k),top_estim[k]) for k in range(len(full_pipeline))]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c2471f7-a0ec-4ff7-ba8b-908f66018779","_uuid":"5130ebe52d9ec9b0768fe0aab6dccdcf46dd19b6"},"cell_type":"markdown","source":"Voting Classifier using best estimators: "},{"metadata":{"_cell_guid":"775e6e20-95dd-4ba6-ab07-1cd7e8a370f9","_uuid":"d1e126774e0f892ae7255c3d3dd33a7bf1c1c433","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nvotc=VotingClassifier(estimators=estimators, n_jobs=-1, voting=\"soft\")\ncross_val_score(votc, data_cleaned,data_cleaned[\"Survived\"])\nvotc.fit(data_cleaned,data_cleaned[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81aa2e83-bbd4-4b14-9501-40223b01c34e","_uuid":"e7afa107e4a9af5594a66aeed3ecd6d874ed0d67"},"cell_type":"markdown","source":"Using the voting classifier to predict survivors from test data:"},{"metadata":{"_cell_guid":"1829fb51-e7ec-4818-b694-103d20fd85fa","_uuid":"e7272a1a1ec31ddbbc52542d92be6b71391e7c75","collapsed":true,"trusted":true},"cell_type":"code","source":"testdata=pd.read_csv(\"../input/test.csv\") \ny_test_prediction=votc.predict(testdata)\nsubmission = pd.DataFrame({\n        \"PassengerId\": testdata[\"PassengerId\"],\n        \"Survived\": y_test_prediction\n    })\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f7af900-3a0f-4ce2-8bdb-64fcd016e20f","_uuid":"0e7be28531eeda5ada94c8d535fa915b6f280dbb","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}