{"cells":[{"metadata":{"_uuid":"610abc30723ab01d8a021cdc109a3726f4f180d3"},"cell_type":"markdown","source":"# Santander Value Prediction\n\nIn this notebook I will show you an approach in how to handle a dataset with several features. I will implement two techniques which are:\n\n* PCA\n* Selection of K best\n* Random Forest With Cross Validation\n\nUsing both techniques I'll try to show a way in how to extract important features from this dataset. I use Extreme Gradient Boost for better results.\n\n### Important!\n##### For future work I will try to apply permutation for each feature column and show you how the MAE is improved or worsen"},{"metadata":{"_uuid":"66d3097737563746b79c4939456b0d3b838f3969"},"cell_type":"markdown","source":"# 1. Loading libraries"},{"metadata":{"trusted":true,"_uuid":"0f815b648c6d8f9d2be54faaf417ae7988491d1a"},"cell_type":"code","source":"\"\"\"Handle data\"\"\"\nimport numpy as np\nimport pandas as pd\n\n\"\"\"Metrics\"\"\"\nfrom sklearn.metrics import mean_absolute_error\n\n\"\"\"Feature Selection\"\"\"\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import RFECV\n\n\"\"\"Regressors\"\"\"\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e15182282d0fb21f9f1e2eb03ba754f4cdf0902"},"cell_type":"markdown","source":"## 1.1 Reading csv and dropping garbage"},{"metadata":{"trusted":true,"_uuid":"15a25fee1a2753367171bbcfb2a41db464b87265"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\n#test = pd.read_csv('test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4ad55aca70c6cd7fdc6d6e89144e5fe92b649f1"},"cell_type":"code","source":"#test_ID = test['ID']\ny_train = train['target']\n#y_train = np.log1p(y_train)\ntrain.drop(\"ID\", axis = 1, inplace = True)\ntrain.drop(\"target\", axis = 1, inplace = True)\n#test.drop(\"ID\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a4d34317448ce644802946e78a7c8d405240ec4"},"cell_type":"markdown","source":"Obtaining name of columns with only one value, which are going to be dropped from train and test datasets"},{"metadata":{"trusted":true,"_uuid":"9f1d8285205f6a9dd600c420f44bdd282c195ec3"},"cell_type":"code","source":"columns_one_value = [element for element, ele in train.items() \n                     if (pd.Series(train[element], name=element)).nunique() == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67beb5150023cc9fd956eb889c3459f7a2917303"},"cell_type":"code","source":"train = train.drop(columns_one_value, axis=1)\n#test = test.drop(columns_one_value, axis=1)\ntrain = train.round(16)\n#test = test.round(16)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4815d95dfe8ddeee9f8f3f39b273c170d7007848"},"cell_type":"markdown","source":"I proceed removing columns with only one value."},{"metadata":{"trusted":true,"_uuid":"6999e0187c3ae5d3d669b56e526508cb6406838e"},"cell_type":"code","source":"colsToRemove = []\ncolumns = train.columns\nfor i in range(len(columns)-1):\n    v = train[columns[i]].values\n    dupCols = []\n    for j in range(i + 1,len(columns)):\n        if np.array_equal(v, train[columns[j]].values):\n            colsToRemove.append(columns[j])\n            \ntrain.drop(colsToRemove, axis=1, inplace=True) \n#test.drop(colsToRemove, axis=1, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f39aaef7ba02e4ca5431cd56d5010f063dc9e62f"},"cell_type":"code","source":"print(\"Shape train: \", train.shape)\n#print(\"Shape test: \", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"348d3101bda4c4be220253540a8b235be2f30a09"},"cell_type":"markdown","source":"# 2. Feature Extraction"},{"metadata":{"_uuid":"e993b746ff64b35cf65d8b86a647d517b42dc806"},"cell_type":"markdown","source":"This is one of the most <b>important</b> things we should priorize. There are many ways in how to extract the most important features. In this case I will proceed with this methodology:\n\n* Using PCA I will look at what is the best number of features\n* Having the \"best number of features\" I will proceed to extract the features with the method \"selection of K best\" tested with a random forest.\n* After that, I will apply Random Forest with Croos Validation based on XGBoost"},{"metadata":{"_uuid":"2fc99d31a5c083373ab9ce064f48782829943b32"},"cell_type":"markdown","source":"## 2.1 Dimensionality Reduction"},{"metadata":{"trusted":true,"_uuid":"ca7c88723b8c51bd171499584a9921dc6f36802b"},"cell_type":"code","source":"pca = PCA()\npca.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eec89057446c524c1f298141d559db874882b245"},"cell_type":"code","source":"# Plotting to visualize the best number of elements\nplt.figure(1, figsize=(9, 8))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('Number of Feautres')\nplt.ylabel('Variance Ratio')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f809565241817f2d479547d3e273113aa8d914d2"},"cell_type":"markdown","source":"So as we can see, the best number of features is in the range between [50-100] aprox. I will proceed to choose the best 100 features."},{"metadata":{"collapsed":true,"_uuid":"2bfcc3524d515628ddac06f317b0b96de2b77d83"},"cell_type":"markdown","source":"## 2.2 Selecting the K best Features"},{"metadata":{"trusted":true,"_uuid":"0f90bd345dff9749acc4f94a03fdb988d605643f"},"cell_type":"code","source":"ytrain = np.array(y_train)\nytrain = ytrain.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8f516ddfb4c6c292fe56829d85082f258696f01"},"cell_type":"code","source":"# Initialize SelectKBest function\nX = SelectKBest(chi2, k=100).fit_transform(train, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6e284bb9d69a1e02e9252269418fdc7d9d2fa7c"},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c238fd0fad0557b1ba5955704fdc20df8c8421c"},"cell_type":"markdown","source":"## 2.3 Testing with Random Forest"},{"metadata":{"_uuid":"f792cadd85fa91acf6df57245f03f902e46613f9"},"cell_type":"markdown","source":"In this part I try to test how is the performance with just 100 features using Random Forest"},{"metadata":{"trusted":true,"_uuid":"303535f9c7517928b627d7251c53fb70ca97d555"},"cell_type":"code","source":"RandForest_K_best = RandomForestRegressor()      \nRandForest_K_best.fit(X, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e00915966438fcc87693651c5c2bf29db6af30ee"},"cell_type":"code","source":"ypred = RandForest_K_best.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa02628eb8358ce9f19290b8decfad0171d6dd68"},"cell_type":"code","source":"mae = mean_absolute_error(ytrain, ypred)\nprint(\"MAE with 100 features: \", mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4c87912d87a51ec46c6e3f715d7875acff0ce7e"},"cell_type":"markdown","source":"## 2.4 Applying Random Forest with Cross Validation based on XGBoost"},{"metadata":{"_uuid":"9514a8c6a4b2481be5491460c6b24742771de835"},"cell_type":"markdown","source":"Having a dataset with 100 features, I will try to extract the most important features from this, so I will apply a more sophisticated method based on applying Croos Validation through a regressor, in this case I will use XGBoost."},{"metadata":{"trusted":true,"_uuid":"d715189e35e050aaae898543531db10a55fe77ec"},"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(objective ='reg:linear', learning_rate=0.1, max_depth=3, n_estimators=300) \n# Initialize the RFECV function setting 3-fold cross validation\nrfecv = RFECV(estimator=xg_reg, step=1, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"817250bdf41339e52d90f780d8a42a98fb2dec76"},"cell_type":"code","source":"rfecv = rfecv.fit(X, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"773829fa5db98720d474a98903ead2d2da2b8a36"},"cell_type":"code","source":"print('Best number of features :', rfecv.n_features_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2102a7a599bc6b2263df77b5dc462b0909201287"},"cell_type":"code","source":"# Plotting the best features with respect to the Cross Validation Score\nplt.figure()\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score of Selected Features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0085d076037ed271fbcf37b97b380ec6d1407067"},"cell_type":"markdown","source":"As we can see, the best score is obtained for 57 features. So lets modify our dataset and apply a regressor."},{"metadata":{"trusted":true,"_uuid":"cff70d82502942395ce9bd99298ecf687ce3794f"},"cell_type":"code","source":"Xnew = X[:,rfecv.support_]\nprint(Xnew.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4e210b356b138bceb4b336e1f1b83e4f70b8b90"},"cell_type":"markdown","source":"## 2.5 Apply XGBoost"},{"metadata":{"trusted":true,"_uuid":"5caf5e6116b0965ac83dd7e1ee2d26173f2482b0"},"cell_type":"code","source":"xg_reg.fit(Xnew ,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41c927b53e8ae0a41a0a25c7f87905854bb716d3"},"cell_type":"code","source":"ypred = xg_reg.predict(Xnew)\nmae = mean_absolute_error(ytrain, ypred)\nprint(\"MAE with 57 features: \", mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"165c37651a9087708f17e8ae2f0a4d6d8782b8a3"},"cell_type":"markdown","source":"## 3. Conclusion\n\nWe can observe the follows results:\n\n* <b>MAE - RandomForest</b> with 100 features: 3658356.481798614\n* <b>MAE - XGBoost</b> with 57 features: 4853294.47253908\n\nFurther we could apply:\n* <b>Permutation</b> to see how could be the model behavior for every feature. \n* <b>Kolmogorov Smirnov</b> to test the null hypothesis for every distribution of each feature."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"fc02c88500e85e6e4de8ebf3be3f6f3fb80791dc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}