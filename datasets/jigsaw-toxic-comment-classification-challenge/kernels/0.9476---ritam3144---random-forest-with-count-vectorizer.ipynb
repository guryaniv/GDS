{"cells":[{"metadata":{"_cell_guid":"f7ec128e-f331-4a13-aa80-025345d91544","_uuid":"3aa94f0ad3bfa190cf30f29578c64633de6dfd33","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"_cell_guid":"d09269ca-a5f5-4ba7-aa06-6975daabee4e","collapsed":true,"_uuid":"117ec2e532f950ef6b3e79f8795c3029adabe243","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom scipy.special import logit, expit","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"aba57ae3-bb93-426e-a38e-d176a6de9ce0","_uuid":"f55940f915a8c559a4440572b32db7111af0f5b0"},"cell_type":"markdown","source":"# Read the data\n\nThere are six classes in the data and also some NA values"},{"metadata":{"_cell_guid":"d919447e-deec-4e3b-aea8-27a4d84ab66f","collapsed":true,"_uuid":"f1c4b2e9f0e732b66b73c37d6faf3ee76d688791","trusted":true},"cell_type":"code","source":"class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ntrain = pd.read_csv('../input/train.csv').fillna(' ')\ntest = pd.read_csv('../input/test.csv').fillna(' ')\n\ntrain_text = train['comment_text']\ntest_text = test['comment_text']","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"7716dd78-a25b-4c67-95b3-d7c68803d815","_uuid":"99cd605602c5844ade65e6552d758009c27006a1"},"cell_type":"markdown","source":"# Combine the comments from both train and test class for creating vocabulary\n\nThere are many words in the test set which differs from the train data. So combining the data into one frame to get all the words and then creating the vocabulary to get feature vector helps to improve the accuracy.\n\nthe main code is [here](https://www.kaggle.com/thousandvoices/logistic-regression-with-words-and-char-n-grams)\n"},{"metadata":{"_cell_guid":"92ab0619-df78-4206-afcb-f230824deb00","collapsed":true,"_uuid":"71476ccae975e46d7a86a4fe41c7cc76eff3e5aa","trusted":true},"cell_type":"code","source":"all_text = pd.concat([train_text, test_text])","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"bd1a0d85-04eb-4d99-9a52-dad2842d1931","_uuid":"aa78a0fbdba5f4ee8d5ead445d0b832e0a98299d"},"cell_type":"markdown","source":"# Apply count vectorizer on word level\n\nFirstly as there are six classes then we dont know if only the word level features will be enough so for better accuracy we are going to take the char level features too.\n"},{"metadata":{"_cell_guid":"d249cd05-19c6-4279-b68f-b23cf580840e","collapsed":true,"_uuid":"e93cc3f5261e715e9747ce5bdcf67cda64c6024f","trusted":true},"cell_type":"code","source":"word_vectorizer = CountVectorizer(stop_words = 'english',analyzer='word')\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"0d52e7d0-2c16-4df3-965c-bb160a329eef","_uuid":"f96e6bc8550fc6077b9e82c2d10b5b30d75aa2ba"},"cell_type":"markdown","source":"# and on char level\n\nTaking the char level features.\n"},{"metadata":{"_cell_guid":"59c3db67-ae83-4d55-9775-02af0842725b","collapsed":true,"_uuid":"bb9faef1cafdf522cce02a85b1bc59c57b0e6388","trusted":true},"cell_type":"code","source":"char_vectorizer = CountVectorizer(stop_words = 'english',analyzer='char')\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"f99b7b07-1ac4-4223-9763-aa7d7bbbdeb5","_uuid":"23e0b6d90ce1c32e888e8b59eaec88c4fc865c6d"},"cell_type":"markdown","source":"# Combine word and char features"},{"metadata":{"_cell_guid":"fcbe3d43-cd75-4e66-88b3-15ee3aabadab","collapsed":true,"_uuid":"6478c6b9cc67cad2e6e50675f57fc1ca301f8381","trusted":true},"cell_type":"code","source":"train_features = hstack([train_char_features, train_word_features])\ntest_features = hstack([test_char_features, test_word_features])","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"7a57721f-cd77-4972-9df0-cf22f3f64a72","_uuid":"8034150321b0e94ef9be8506bca02afce493c731"},"cell_type":"markdown","source":"# Train the classifier and get the CV-Score"},{"metadata":{"_cell_guid":"0898447e-41f9-406e-875c-1f4d1931ad84","_uuid":"6ac6d5878bf2a4a1d4f48350e85f5146aac09960","trusted":true},"cell_type":"code","source":"losses = []\npredictions = {'id': test['id']}\nfor class_name in class_names:\n    train_target = train[class_name]\n    classifier = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=100, max_features=1000, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=3, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\n    cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='f1_micro'))\n    losses.append(cv_loss)\n    print('CV score for class {} is {}'.format(class_name, cv_loss))\n\n    classifier.fit(train_features, train_target)\n    predictions[class_name] = expit(logit(classifier.predict_proba(test_features)[:, 1]))\n\nprint('Total CV score is {}'.format(np.mean(losses)))","execution_count":7,"outputs":[{"output_type":"stream","text":"CV score for class toxic is 0.9147840004158873\nCV score for class severe_toxic is 0.9900420504011899\nCV score for class obscene is 0.9504170475580883\nCV score for class threat is 0.9970044683087528\nCV score for class insult is 0.9525603042593308\nCV score for class identity_hate is 0.9911951420522724\nTotal CV score is 0.9660005021659203\n","name":"stdout"}]},{"metadata":{"_cell_guid":"c2fd9c68-56c1-4805-b6ca-a830c5ee7ed1","_uuid":"46d9056249acd7b8f78fbed0877e48eec81dd5d1"},"cell_type":"markdown","source":"# Write predictions in submission.csv"},{"metadata":{"_cell_guid":"748ab1c0-0015-444f-b304-3448802b2dcf","collapsed":true,"_uuid":"889b571fbd3555eb44005cbf88d2a0fbc0d6f1a6","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict(predictions)\nsubmission.to_csv('submission.csv', index=False)","execution_count":8,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}