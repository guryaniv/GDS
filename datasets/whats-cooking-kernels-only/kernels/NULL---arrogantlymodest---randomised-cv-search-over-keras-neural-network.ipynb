{"cells":[{"metadata":{"_uuid":"4b12c0f26dfe4101a886d82bae66196c2b4dcfd0"},"cell_type":"markdown","source":"In this notebook I create a code structure which will allow the user to build a keras neural networks for this challenge\n\nThe main feature of this notebook is the functions which allow you to perform a gridsearch over the neural network hyperparameters and architecture, including learning rates, activation functions, layers and neurons per node."},{"metadata":{"trusted":true,"_uuid":"8b7520718f94dd64fbbc40cdb791e8bba92950de"},"cell_type":"code","source":"import sklearn\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nimport os\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport numpy\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.metrics import make_scorer\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.optimizers import SGD\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9ae005aaf2380d273b29fd6c2e20f7dbe51f06e"},"cell_type":"markdown","source":"> ## Load in data"},{"metadata":{"trusted":true,"_uuid":"050e28794359792157dcb0f12a24d688fbbda468"},"cell_type":"code","source":"# load in data\ndf = pd.read_json('../input/train.json')\ny = pd.get_dummies(df['cuisine'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebeb3abd92f8637801a31ec2f3ce971644fdfe7a"},"cell_type":"markdown","source":"## Cleaning\n\nHere we do some basic cleaning using NLTK. The cleaning functions will take the list of strings, the ingredients list for each row, process these and concatenate them together. \n\nI have left in the bigrams option in cleaning which I was using for a simpler, non-neural net approach, but I do not believe it will particularly beneficial for the neural net. \n\nI believe that the neural net should be able to find meaningful bigrams using the ingredients list in vectorised form anyway"},{"metadata":{"trusted":true,"_uuid":"0fe9df81f0eb9195de8db8940563181824e51d8a"},"cell_type":"code","source":"# prepare cleaning products\nmy_tokenizer = RegexpTokenizer(r'\\w+')\nmy_stopwords = nltk.corpus.stopwords.words('english')\nword_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n#from nltk.corpus import stopwords\n\ndef clean_string(x, bigrams=True):\n    tokens = my_tokenizer.tokenize(x)\n    tokens = [t.lower() for t in tokens]\n    tokens = [word_rooter(t) for t in tokens if t not in my_stopwords]\n    if bigrams:\n        tokens = tokens + [tokens[i]+'_'+tokens[i+1] for i in range(len(tokens)-1)]\n    return ' '.join(tokens)\n\ndef clean_ingredients_list(x, bigrams=True):\n    return ' '.join([clean_string(s, bigrams=bigrams) for s in x])\n    \n    from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"28df36dffcdb009836f55a029c4c79e802859470"},"cell_type":"code","source":"# clean data for neural network\ntf = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1, 1), max_df=1.0, min_df=50)\nX = tf.fit_transform(df.loc[:,'ingredients'].apply(clean_ingredients_list, bigrams=False))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9d8c1bc2204ec1f9b2c9600af09177e24c22c54"},"cell_type":"markdown","source":"## Keras -- sklearn-randomised-search compatibility"},{"metadata":{"trusted":true,"_uuid":"05ac5094c2d5fb0a36af657bc2613b808d97c5e4"},"cell_type":"code","source":"def create_model( nl1=1, nl2=1,  nl3=1, \n                 nn1=1000, nn2=500, nn3 = 200, lr=0.01, decay=0., l1=0.01, l2=0.01,\n                act = 'relu', dropout=0, input_shape=1000, output_shape=20):\n    '''This is a model generating function so that we can search over neural net \n    parameters and architecture'''\n    \n    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999,  decay=decay)\n    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n                                                     \n    model = Sequential()\n    \n    # for the firt layer we need to specify the input dimensions\n    first=True\n    \n    for i in range(nl1):\n        if first:\n            model.add(Dense(nn1, input_dim=input_shape, activation=act, kernel_regularizer=reg))\n            first=False\n        else: \n            model.add(Dense(nn1, activation=act, kernel_regularizer=reg))\n        if dropout!=0:\n            model.add(Dropout(dropout))\n            \n    for i in range(nl2):\n        if first:\n            model.add(Dense(nn2, input_dim=input_shape, activation=act, kernel_regularizer=reg))\n            first=False\n        else: \n            model.add(Dense(nn2, activation=act, kernel_regularizer=reg))\n        if dropout!=0:\n            model.add(Dropout(dropout))\n            \n    for i in range(nl3):\n        if first:\n            model.add(Dense(nn3, input_dim=input_shape, activation=act, kernel_regularizer=reg))\n            first=False\n        else: \n            model.add(Dense(nn3, activation=act, kernel_regularizer=reg))\n        if dropout!=0:\n            model.add(Dropout(dropout))\n            \n    model.add(Dense(output_shape, activation='sigmoid'))\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'],)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1a1816ad9c8756fa2e52c201506cb3dbc43bc4cf"},"cell_type":"code","source":"# model class to use in the scikit random search CV \nmodel = KerasClassifier(build_fn=create_model, epochs=6, batch_size=20, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1477a6754dcd7d9f4e4c65e336374290df9e00e4"},"cell_type":"markdown","source":"### Parameters and network structure to search over"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0c8afa7d930e7c07ec1e28cc4414f962ca52c3c3"},"cell_type":"code","source":"# learning algorithm parameters\nlr=[1e-2, 1e-3, 1e-4]\ndecay=[1e-6,1e-9,0]\n\n# activation\nactivation=['relu', 'sigmoid']\n\n# numbers of layers\nnl1 = [0,1,2,3]\nnl2 = [0,1,2,3]\nnl3 = [0,1,2,3]\n\n# neurons in each layer\nnn1=[300,700,1400, 2100,]\nnn2=[100,400,800]\nnn3=[50,150,300]\n\n# dropout and regularisation\ndropout = [0, 0.1, 0.2, 0.3]\nl1 = [0, 0.01, 0.003, 0.001,0.0001]\nl2 = [0, 0.01, 0.003, 0.001,0.0001]\n\n# dictionary summary\nparam_grid = dict(\n                    nl1=nl1, nl2=nl2, nl3=nl3, nn1=nn1, nn2=nn2, nn3=nn3,\n                    act=activation, l1=l1, l2=l2, lr=lr, decay=decay, dropout=dropout, \n                    input_shape=[X.shape[1]], output_shape = [y.shape[1]],\n                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b420801ca93966c64e1766f58194c367bd78cadb"},"cell_type":"code","source":"grid = RandomizedSearchCV(estimator=model, cv=KFold(3), param_distributions=param_grid, \n                          verbose=20,  n_iter=10, n_jobs=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d49bd12f589a9cbf049fc1ab523f9668e11e675"},"cell_type":"code","source":"grid_result = grid.fit(X.toarray(), y)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"adf390d539fd03d30e1892ecfa5d724c0c49d8df"},"cell_type":"code","source":"cv_results_df = pd.DataFrame(grid_result.cv_results_)\ncv_results_df.to_csv('gridsearch.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"021e1332218ba9d10c3b0334f220a70ef8b53587"},"cell_type":"markdown","source":"From the best set of model parameters and structure found you can rebuild either here or in another purpose built script"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0be38d3becad4eecf7a6ca43f672ac8b4d275d45"},"cell_type":"code","source":"cv_results_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2cb38f62d91a2281982e23120f78fe568f7139c2"},"cell_type":"code","source":"print(grid_result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"14ed83cce4392507d0f5899242de93f11191645e"},"cell_type":"code","source":"best_model = grid_result.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51495052adeb81ddfe7258e482339086de8a2d61"},"cell_type":"markdown","source":"## Predict and submit"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8a693f6bece814838684be8ec8fda37f5540a98c"},"cell_type":"code","source":"df_holdout = pd.read_json('../input/test.json')\ndf_submission = df_holdout[['id']]\nX_holdout = tf.transform(df_holdout.loc[:,'ingredients'].apply(clean_ingredients_list, bigrams=False))\n\n# Predictions \ny_pred_holdout_proba = best_model.predict_proba(X_holdout)\ny_pred_holdout = (y_pred_holdout_proba == y_pred_holdout_proba.max(axis=1)[:,np.newaxis])*1\ny_pred_string = np.sum(y_pred_holdout*y.columns.values, axis=1)\n\ndf_submission['cuisine']=y_pred_string\ndf_submission.to_csv('neural_net_base_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}