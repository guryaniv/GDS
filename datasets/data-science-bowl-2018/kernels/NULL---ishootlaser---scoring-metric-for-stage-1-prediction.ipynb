{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/data-science-bowl-2018\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"This is my first CV competition on Kaggle and I want to figure out how to implement the scoring matrix for LB. Since stage 1 solution is out, I can use my stage 1 submission to verify this implementation.<br>\nI used a U-Net with dropout = 0.3, batchnorm for encoder layers. I added a few extra layers at the end of decoder output (64 filters -> 32 -> 16 -> 8 -> output) because it seemed to improve the performance by 0.02~0.04 on LB.<br>\nMy best LB = 0.377 for U-Net output with cutoff threshold at 0.55. This LB score will be the baseline to check whether my implementation is correct. <br>\nI developed my model based loosely on the [U-Net starter kernel](https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277?scriptVersionId=2164855). I based my scoring metric implementation on [this kernel](https://www.kaggle.com/aglotero/another-iou-metric/notebook). Stephen's [kernel](https://www.kaggle.com/stkbailey/step-by-step-explanation-of-scoring-metric) also gave me a lot of insight on the scoring metric. Thanks to the authors of these kernel, who really helped me a lot for my first image competition ever."},{"metadata":{"_cell_guid":"2f740dd2-1fdc-4661-9081-db7d4d7beb48","_uuid":"73e12cd6e648f472bddf0ff78669d8f4dbeb0040","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport warnings\nimport h5py\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n\ntrain_id = pd.read_csv('../input/stage-1-id/train_id.csv').values.tolist()\ntest_id = pd.read_csv('../input/stage-1-id/test_id.csv').values.tolist()\n\n#rle decoder taken from the discussion forum\ndef rle_decode(rle_str, mask_shape, mask_dtype):\n    s = rle_str.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    mask = np.zeros(np.prod(mask_shape), dtype=mask_dtype)\n    for lo, hi in zip(starts, ends):\n        mask[lo:hi] = 1\n    return mask.reshape(mask_shape[::-1]).T","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"9a39a677-d14e-40b4-83dc-da0610cf54d8","_uuid":"3c2fe1defca57405a848332f4cea985d6fa6badf"},"cell_type":"markdown","source":"I added my submission as private dataset. I hope that uploading submission which already has a published solution does not violate any kind of Kaggle honor code. Please let me know if I am in trouble."},{"metadata":{"_cell_guid":"959b132d-eb7e-4261-bd6e-973b6b1beec1","_uuid":"126f4269e825e4d3150d832f90c32dfa688bb6d1","trusted":false,"collapsed":true},"cell_type":"code","source":"sol = pd.read_csv('../input/stage-1-solution/stage1_solution.csv')\nsol_masks = []\nsize = []\nfor id in test_id:\n    dummy = sol[sol['ImageId'] == id[0]]\n    h, w = dummy[['Height', 'Width']].values[0]\n    size.append((h, w))\n    masks = np.zeros((len(dummy.index), h, w))\n    for n, info in enumerate(dummy[['EncodedPixels', 'Height', 'Width']].values):\n        mask_rle = rle_decode(info[0], (info[1], info[2]), np.uint8)\n        masks[n] = mask_rle\n    masks = np.max(masks, axis = 0)\n    sol_masks.append(masks)\n\n# Load my prediction mask\nh5f = h5py.File('../input/stage-1-submission/stage_1_pred.h5', 'r')\ny_pred_ = h5f['pred'][:]\nh5f.close()","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"38283cfb-9b10-433f-9f08-b6ef84bf3e50","_uuid":"2cc23a0ccda4b8c76f301ba7011c899efcdd7b06"},"cell_type":"markdown","source":"Let's visualize some predicted mask vs ground truth mask."},{"metadata":{"_cell_guid":"b4210924-d469-4de0-a651-07fe835d9655","_uuid":"dd13e7dfaa94b73060bd9874dc7329b740d23c47","trusted":false},"cell_type":"code","source":"y_pred = []\nfor n, img in enumerate(sol_masks):\n    y_pred.append((resize(np.squeeze(y_pred_[n]), sol_masks[n].shape, mode = 'constant', preserve_range = True)))\n    \nix = np.random.randint(0,len(y_pred_))\nf, ax = plt.subplots(1,2)\nax[0].imshow(np.squeeze(y_pred[ix]))\nax[1].imshow(np.squeeze(sol_masks[ix]))\nax[0].axis('off')\nax[1].axis('off')\nax[0].set_title('predicted mask')\nax[1].set_title('ground truth')\nplt.show()","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"1595e19c-0db4-475e-bf02-536c86e62fba","_uuid":"4f6c1c3f8323a2f8ba044d087340744e1d482e57"},"cell_type":"markdown","source":"Now let's define mIOU implementation as described by [this kernel](https://www.kaggle.com/aglotero/another-iou-metric/notebook)."},{"metadata":{"_cell_guid":"6bf4bb92-68bc-4e60-8da4-19190e80a8b4","_uuid":"4e7b2f960c13ba7adfebe7c3ed6dc18f812f6101","collapsed":true,"trusted":false},"cell_type":"code","source":"def iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = label(y_true_in > 0.5)\n    y_pred = label(y_pred_in > 0.55)\n    \n    true_objects = len(np.unique(labels))\n    pred_objects = len(np.unique(y_pred))\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = len(y_true_in)\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n#     return np.array(np.mean(metric), dtype=np.float32)\n    return metric","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"52f36759-2221-40f2-a9b2-beb3638681d7","_uuid":"260014232930cc139a6f83706d0f8862d2f8d0e8","trusted":false},"cell_type":"code","source":"mIOU = np.array(iou_metric_batch(sol_masks, y_pred))\nprint('The mean IOU is {}'.format(np.mean(mIOU)))\nlowconf = np.where(mIOU<0.3)[0]","execution_count":38,"outputs":[]},{"metadata":{"_cell_guid":"590565af-0d6c-44d3-8d28-21d60d0e240c","_uuid":"e7c440e625cf92fa2c500dfe041391e23a433319","collapsed":true},"cell_type":"markdown","source":"Here we can check to see which prediction masks perform poorly. Let's use 0.3 as threshold and look at some random low confidence prediction. <br>These are worthy looking at if you want to implement some specialist. I noticed that color images are much harder to predict, and the predictor often misclassify tissues as a large block of nuclei. There are definately room for improvement."},{"metadata":{"trusted":false,"_uuid":"87558d1ce6cde34e74acab4e1981e4f371a96f82"},"cell_type":"code","source":"ix = np.random.choice(lowconf, 3)\nf, ax = plt.subplots(3, 2, figsize = (10, 20))\nfor n, i in enumerate(ix):\n    ax[n, 0].imshow(y_pred[i])\n    ax[n, 1].imshow(sol_masks[i])\n    ax[n, 0].axis('off')\n    ax[n, 1].axis('off')\n    ax[n, 0].set_title('Prediction')\n    ax[n, 1].set_title('Mask')\n    \n","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"42d500e085941c6878267d3545042922e0bcf999"},"cell_type":"markdown","source":"But my baseline score is 0.377, lower than the above mIOU (0.49). The problem with this implementation is that it uses skimage's label() method to identify how many individual masks there are. Since many human labels overlap with each other, this could cause the label() method to malfunction. <br>\nLet's quickly fix it. First let's examine the number of masks we got from using label(). Compare that with the number of masks from solution"},{"metadata":{"trusted":false,"_uuid":"87aa65921657505e5383e40abc542a77f11ed0fd"},"cell_type":"code","source":"sol_masks_ct = []\nfor id in test_id:\n    dummy = sol[sol['ImageId'] == id[0]]\n    sol_masks_ct.append(len(dummy.index))\n    \ndef mask_ct(y_true_in):\n    ct = []\n    for mask in y_true_in:\n        labels = label(mask > 0.5)\n        ct.append(len(np.unique(labels)))\n    return ct\n\nsol_cv_ct = mask_ct(sol_masks)\nmask_ct_diff = np.where((np.array(sol_masks_ct) == np.array(sol_cv_ct)) == False)\nprint('Number of mask count discrepancy = {}.'.format(mask_ct_diff[0].shape[0]))\nprint('Difference between true mask count and false mask count: {}'.format(np.array(sol_masks_ct) - np.array(sol_cv_ct)))\n\nplt.rcParams['figure.figsize'] = [10, 10]\nix = np.random.choice(mask_ct_diff[0], 3)\nf, ax = plt.subplots(1,3)\nfor n, i in enumerate(ix):\n    print('Image {}: True count = {}, false count = {}'.format(i, sol_masks_ct[i], sol_cv_ct[i]))\n    ax[n].imshow(sol_masks[i])\n    ax[n].axis('off')\n    ax[n].set_title('Image {}'.format(i))\nplt.show()","execution_count":50,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ecab78990faa226c2d8a3ef09949ab6ea84c06fc"},"cell_type":"code","source":"#Rewriting the mIOU function to account for correct number of ground truth mask\ndef iou_metric_new(sol_df, sol_masks, batch, y_pred_in, print_table=False):\n    labels = label(sol_masks > 0)\n    y_pred = label(y_pred_in > 0.55)\n    \n    true_objects = len(sol_df[sol_df['ImageId'] == test_id[batch][0]].index)\n    pred_objects = len(np.unique(y_pred))\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(sol_df, sol_masks, y_pred_in):\n    batch_size = len(sol_masks)\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric_new(sol_df, sol_masks[batch], batch, y_pred_in[batch])\n        metric.append(value)\n#     return np.array(np.mean(metric), dtype=np.float32)\n    return metric","execution_count":57,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"241b730d70c8de513df9c20de99585943f0acfa2"},"cell_type":"code","source":"mIOU_new = iou_metric_batch(sol, sol_masks, y_pred)\nprint('The mean IoU score for LB = {}'.format(np.mean(mIOU_new)))","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"cec434543d0a74c580c6c0ffc0a61340cc33c4f6"},"cell_type":"markdown","source":"Great! This should approximate the LB score from stage 1. Notice that the mIOU is 0.365, about 0.01 below the LB score. I think this has to do with an [implementation detail mentioned by Allen Goodman](https://www.kaggle.com/c/data-science-bowl-2018/discussion/47756#270559) (\"...we match the prediction object to the ground truth object by using the greatest IOU score\").<br> I have not figure out this part yet but I am happy with the metric so far. Since the deadline is approaching I am going to skip this implementation detail and use the method above to measure /fine-tune my submission.\nThis is my first kernel on Kaggle so please let me know if there is anything wrong. Thank you."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8e15c4f9dc3f28f455c4a631d4f0757de4b21261"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}