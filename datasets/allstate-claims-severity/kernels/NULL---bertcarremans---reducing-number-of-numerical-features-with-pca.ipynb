{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c481ecb2-3a73-01a2-332a-df8ee82d5cd6"
      },
      "source": [
        "From this [great forum post][1] by Achal we noticed that there is quite some correlation between the numerical features. Therefore I wanted to explore to how many components we could reduce the feature subspace without losing too much of the explained variance.\n",
        "\n",
        "\n",
        "  [1]: https://www.kaggle.com/achalshah/allstate-claims-severity/allstate-feature-analysis-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7aaf62a2-4cc9-5c00-860c-2b7c6e38a69f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a3224761-6012-e4b7-a846-ebbe9f2202b3"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('../input/train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "399de919-911a-39e6-55c4-45b4942b5cf5"
      },
      "source": [
        "**Checking how many numercial and categorical features + putting the colnames in a list per data type**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c24570a7-14f2-fb5e-eae5-96eab7ce49a7"
      },
      "outputs": [],
      "source": [
        "numFeatures = []\n",
        "catFeatures = []\n",
        "\n",
        "for col, val in train.iloc[0,:].iteritems():\n",
        "    if type(val) is not str:\n",
        "        numFeatures.append(col)\n",
        "    elif type(val) is str:\n",
        "        catFeatures.append(col)\n",
        "        \n",
        "# Remove id and loss from the numFeatures\n",
        "numFeatures.remove('id')\n",
        "numFeatures.remove('loss')\n",
        "        \n",
        "print(len(numFeatures), 'Numerical Features:', numFeatures, \"\\n\")\n",
        "print(len(catFeatures), 'Categorical Features:', catFeatures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4d5dfe05-e83e-99fb-c18f-100dd8b781aa"
      },
      "source": [
        "**Standardizing the numerical features before performing PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "405591e5-9771-459e-5e9e-aeccf11b91c2"
      },
      "outputs": [],
      "source": [
        "sc = StandardScaler()\n",
        "train_nums_std = sc.fit_transform(train[numFeatures])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a9dae0ed-04c8-caa1-0553-2f067fe6aa60"
      },
      "source": [
        "**PCA**<br>\n",
        "Set n_components to None to keep all principal components and their explained variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18f4573a-6eca-7c11-1e74-35366dfb60db"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=None)\n",
        "train_nums_pca = pca.fit_transform(train_nums_std)\n",
        "varExp = pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b531e383-b64b-2736-1214-d8bad7a70952"
      },
      "source": [
        "**Plot the cumulative explained variance as a function of the number of components**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fa3fec90-c2fc-6d43-3ff3-de7d5a231daf"
      },
      "outputs": [],
      "source": [
        "cumVarExplained = []\n",
        "nb_components = []\n",
        "counter = 1\n",
        "for i in varExp:\n",
        "    cumVarExplained.append(varExp[0:counter].sum())\n",
        "    nb_components.append(counter)\n",
        "    counter += 1\n",
        "\n",
        "plt.subplots(figsize=(8, 6))\n",
        "plt.plot(nb_components, cumVarExplained, 'bo-')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylim([0.0, 1.1])\n",
        "plt.xticks(np.arange(1, len(nb_components), 1.0))\n",
        "plt.yticks(np.arange(0.0, 1.1, 0.10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "13c1861d-84eb-37ae-c4c9-d43565e0fc05"
      },
      "source": [
        "With 7 components we already explain more than 90% of all variance in the features. So we could reduce the number of features to half of the original numerical features."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}