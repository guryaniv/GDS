{"cells":[{"metadata":{"_uuid":"db930183cc19fa051bb41a7b35b38370eb144e48"},"cell_type":"markdown","source":"[TO DO]\n* Comment the code with better explanation\n* Study the baseline model (linear regression) thorougly\n* Improve the model (grid search)\n* Explore new models"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\n# display all outputs in jupyter notebook\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# we take a subsample of the data to avoid too much computation\ntrain_data = pd.read_csv(r'../input/train.csv', nrows=50000)\ntrain_data.head()\ntrain_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"053f24a3f899de3bd291e121331171b2418dd1ea"},"cell_type":"markdown","source":"# Data exploration\n"},{"metadata":{"_uuid":"f0ec66a038a56feab56f391b931ea9ddebf4e5f0"},"cell_type":"markdown","source":"## Fare exploration"},{"metadata":{"trusted":true,"_uuid":"be39d483f90fda650daf4c8ff166141047a53eb7"},"cell_type":"code","source":"bins = np.linspace(0,max(train_data['fare_amount']),100)\nfare_hist = plt.hist(train_data['fare_amount'], bins=bins)\nplt.show()\ntrain_data['fare_amount'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6bbea11f2199bef88b9f681712ce6e2a381138c","scrolled":true},"cell_type":"code","source":"# we notice that the min price is negative : -5\n\n# how many negative fares are negatives ?\n(train_data['fare_amount']<0).sum()\n\n# let's see them in details\nnegative_fares = train_data[train_data['fare_amount']<0]\nnegative_fares\n\n# let's see these rides on a map\ncolors = ['red', 'blue', 'green', 'purple', 'orange', 'pink']\nm = folium.Map(location=[40.790112, -74.000031], tiles='Stamen Toner')\nfor i in range(len(negative_fares)):\n    pickup_lat = negative_fares.iloc[i]['pickup_latitude']\n    pickup_long = negative_fares.iloc[i]['pickup_longitude']\n    a = folium.Circle(\n        radius=100,\n        location=[pickup_lat,pickup_long],\n        popup='The Waterfront',\n        color=colors[i],\n        fill=False,\n    ).add_to(m)\n    dropoff_lat = negative_fares.iloc[i]['dropoff_latitude']\n    dropoff_long = negative_fares.iloc[i]['dropoff_longitude']\n    b = folium.Circle(\n        radius=100,\n        location=[dropoff_lat, dropoff_long],\n        popup='The Waterfront',\n        color=colors[i],\n        fill=False\n    ).add_to(m)\nm\n\n# we see that the taxi rides with negative fare amounts are either\n# - small rides\n# - absurd (purple dot at 0 0)\n# we decide to get rid of these rows because we consider taxi fares \n# cannot be negatives\ntrain_data = train_data[train_data['fare_amount'] >= 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4435a1c52552712b0addc9fb412681decf8c3dfd","scrolled":false},"cell_type":"code","source":"columns = train_data.columns\nnum_missing = train_data.isnull().sum()\nnum_zero = pd.Series()\nfor col in columns:\n    num_zero.at[col] = (train_data[col]==0).sum()\nmissing_value_df = pd.DataFrame({\n                                 'num_missing': num_missing,\n                                 'num_zero':  num_zero\n                                })\nmissing_value_df\n\ntrain_data = train_data.drop(train_data[train_data.pickup_longitude==0].index)\ntrain_data = train_data.drop(train_data[train_data.pickup_latitude==0].index)\ntrain_data = train_data.drop(train_data[train_data.dropoff_longitude==0].index)\ntrain_data = train_data.drop(train_data[train_data.dropoff_latitude==0].index)\ntrain_data = train_data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d71b6f5bdaa93a1fb4cc6d9ea8baf9d2aca81f4"},"cell_type":"markdown","source":"## Geo exploration"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f6024a7982587eec6361cdcad4b226cd853eeef5"},"cell_type":"code","source":"import folium\n\nmean_pickup_latitude = np.mean(train_data['pickup_latitude'])\nmean_pickup_longitude = np.mean(train_data['pickup_longitude'])\n\nm = folium.Map(location=[mean_pickup_latitude, mean_pickup_longitude], tiles='Stamen Toner')\n# we only display 1000\nfor i in range(len(train_data.sample(frac=0.02))):\n    pickup_lat = train_data.at[i,'pickup_latitude']\n    pickup_long = train_data.at[i,'pickup_longitude']\n    a = folium.Circle(\n        radius=10,\n        location=[pickup_lat,pickup_long],\n        popup='The Waterfront',\n        color='crimson',\n        fill=False,\n    ).add_to(m)\nm\n# we notice some dots in the water ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"431cb7dc2b3e7a084a6270c3596c9c3503940dcf","scrolled":false},"cell_type":"code","source":"from folium import plugins\nfrom folium.plugins import HeatMap\n\nheat_data = []\nfor i in range (len(train_data.sample(frac=0.02))):\n    heat_data.append([train_data.at[i,'pickup_latitude'], train_data.at[i,'pickup_longitude']])\n\nm = folium.Map(location=[mean_pickup_latitude, mean_pickup_longitude])\nHeatMap(heat_data).add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a44535a6adc0dd4e94e8f00754889fcc65018e3"},"cell_type":"code","source":"for i in range(len(train_data)):\n    train_data.at[i,'pickup_datetime'] = train_data.at[i,'pickup_datetime'].split(' UTC')[0]\ntrain_data['pickup_datetime'] = pd.to_datetime(train_data['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\ntrain_data.at[0,'pickup_datetime'].year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31a3857d6c51a215ffad454e6bdf5ef7e5285ee5"},"cell_type":"code","source":"# evolution of the traffic over the years \nfrom folium.plugins import HeatMapWithTime\ntrain_data.dtypes\nmax(train_data['pickup_datetime'].dt.year)\nmin(train_data['pickup_datetime'].dt.year)\nheat_data_year = [[[row['pickup_latitude'], row['pickup_longitude']] \\\n                  for index, row in train_data[train_data['pickup_datetime'].dt.year == i].iterrows()] \\\n                 for i in range(2009, 2016)]\nm = folium.Map(location=[mean_pickup_latitude, mean_pickup_longitude])\nHeatMapWithTime(heat_data_year).add_to(m)\nm\n\n# taxi work areas seem to have shrunk over the years\n# two hubs external of the city can be noticed : the LaGuardia and JFK airports","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"7e514ddb2a90643bd3cb9b3f4d85beafd11ea2f7"},"cell_type":"code","source":"# evolution of the traffic over the months for 2015\nheat_data_month = [[[row['pickup_latitude'], row['pickup_longitude']] \\\n                    for index, row in train_data[(train_data['pickup_datetime'].dt.year == 2013) & (train_data['pickup_datetime'].dt.month == i)].iterrows()] \\\n                   for i in range(1,13)]\nm = folium.Map(location=[mean_pickup_latitude, mean_pickup_longitude])\nHeatMapWithTime(heat_data_month, auto_play=True).add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"847fc01b2de5ded30206edfbb2868c72c1d5ca1f"},"cell_type":"code","source":"# evolution of the traffic on a day \nheat_map_hour = [[[row['pickup_latitude'], row['pickup_longitude']] \\\n                 for index, row in train_data[(train_data['pickup_datetime'].dt.year == 2015) & \\\n                                             (train_data['pickup_datetime'].dt.month == 5) & \\\n                  (train_data['pickup_datetime'].dt.day == 19) & (train_data['pickup_datetime'].dt.hour == i)].iterrows()] for i in range(0,24)]\nHeatMapWithTime(heat_map_hour, auto_play=True).add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa7ed3995674fcd7aaed78e55c4cfeaabfdc4b6"},"cell_type":"markdown","source":"# Distance exploration"},{"metadata":{"_uuid":"41a3407437949b551491f2f50a494a8258360d82"},"cell_type":"markdown","source":"## Haversine distance"},{"metadata":{"trusted":true,"_uuid":"e717d57254d66f80eba6ed75c2142bbc3e060e74","scrolled":true},"cell_type":"code","source":"# Let's compute the distance between the pickup and dropoff places \n# First we need to compute the Haversine distance as we use the latitude and longitude\n# We use the formula given : https://www.movable-type.co.uk/scripts/latlong.html\n\ndef haversine_distance(lat1, lat2, long1, long2):\n    # R is the Earth radius\n    R = 6371\n    delta_lat = (lat1 - lat2)*np.pi/180.\n    delta_long = (long1 - long2)*np.pi/180\n    a = np.sin(delta_lat / 2.)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(delta_long / 2.)**2\n    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    d = R * c\n    return d\n\ntrain_data['h_dist'] = haversine_distance(train_data['pickup_latitude'], train_data['dropoff_latitude'], \\\n                                          train_data['pickup_longitude'], train_data['dropoff_longitude'])\n\ntrain_data['h_dist'].describe()\ntrain_data['h_dist'].median()\n\n# We can notice extemely long distances (max at near 20000 kms) which is very suspicious\n# especially when considering the price\n# We drop too big values\ntrain_data = train_data.drop((train_data[train_data['h_dist']>200]).index)\n\n\nfig, ax = plt.subplots(1,2, figsize=[10,3])\nbins = np.linspace(min(train_data['h_dist']), max(train_data['h_dist']), 100)\nbins_focus = np.linspace(min(train_data['h_dist']), 0.1*max(train_data['h_dist']), 100)\na = ax[0].hist(train_data['h_dist'], bins)\nb = ax[1].hist(train_data['h_dist'], bins_focus)\n\ntrain_data[train_data['h_dist']==max(train_data['h_dist'])]\n\n# We notice that there are a lot of distances at zero\n# which correspond to mistakes\n# We drop these values \ntrain_data = train_data.drop(train_data[train_data['h_dist']==0].index)\n\n# We will check the fare according to the distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d2e282b198e138f75f4f8532d717f8c179a4e5f"},"cell_type":"code","source":"# Now we plot the fare according to the distance \n\nfig, ax = plt.subplots(2,2, figsize=(20,20))\na = ax[0,0].set_xlabel('distance')\na = ax[0,0].set_ylabel('prix')\na = ax[0,0].scatter(train_data['h_dist'],train_data['fare_amount'])\n\n\n# Zoom In\nidx_zoom = (train_data['h_dist']<train_data['h_dist'].quantile(0.75)) & \\\n(train_data['fare_amount'] < 100)\na = ax[0,1].set_title('Zoom in')\na = ax[0,1].set_xlabel('distance')\na = ax[0,1].set_ylabel('prix')\na = ax[0,1].scatter(train_data['h_dist'][idx_zoom],train_data['fare_amount'][idx_zoom],\\\n                  c = np.clip(train_data['fare_amount'][idx_zoom], 0, 100), cmap='viridis')\n\n\n# distance between 0 and 50 kms\n\na = ax[1,0].set_xlabel('distance')\na = ax[1,0].set_ylabel('prix')\na = ax[1,0].scatter(train_data['h_dist'][train_data['h_dist']<50],train_data['fare_amount'][train_data['h_dist']<50])\n\n\nidx_zoom2 = (train_data['h_dist']<30)\na = ax[1,1].set_title('Zoom in')\na = ax[1,1].set_xlabel('distance')\na = ax[1,1].set_ylabel('prix')\na = ax[1,1].scatter(train_data['h_dist'][idx_zoom2],train_data['fare_amount'][idx_zoom2],\\\n                  c = np.clip(train_data['fare_amount'][idx_zoom2], 0, 100), cmap='viridis')\n\n# We assume that a linear regression could be a good estimate\n# though there are outliers with big prices for short distances\n# and there is the same price for different distances around 50$","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf24adcb7b83a0fc843b69359683ba08e92f52db"},"cell_type":"markdown","source":"## Manhattan distance"},{"metadata":{"trusted":true,"_uuid":"f7c8486ca1f0a7311495466ee1f1b3bf5a352cc0"},"cell_type":"code","source":"# We should also test the Manhattan distance \n# https://stackoverflow.com/questions/32923363/manhattan-distance-for-two-geolocations\n# Indeed Manhattan distance computes a distance based on a grid between two points\n# and not just the direct distance like with the Euclidean distance\n\ndef manhattan_distance(lat1, lat2, long1, long2):\n    # To compute the Manhattan distance we take the latitude distance (considering the longitude distance zero)\n    # and the same for the longitude distance\n    lat_dist = haversine_distance(lat1, lat2, 0, 0)\n    long_dist = haversine_distance(0, 0, long1, long2)\n    return lat_dist + long_dist\n\ntrain_data['m_dist'] = manhattan_distance(train_data['pickup_latitude'], train_data['dropoff_latitude'], \\\n                                          train_data['pickup_longitude'], train_data['dropoff_longitude'])\ntrain_data['m_dist'].describe()\n\n# We notice that the max distance is very high\n\nfig, ax = plt.subplots(1,2,figsize=[10,3])\nbins = np.linspace(min(train_data['m_dist']), max(train_data['m_dist']),100)\nbins_zoom =  np.linspace(min(train_data['m_dist']), 0.05*max(train_data['m_dist']),100)\na = ax[0].hist(train_data['m_dist'], bins)\nb = ax[1].hist(train_data['m_dist'], bins_zoom)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb16deec37165784acf07e2c1a64e4c0de3c66e2"},"cell_type":"code","source":"# Now we plot the fare according to the distance \n\ndef plot_fare_distance(dist):\n \n    fig, ax = plt.subplots(2,2, figsize=(20,20))\n    a = ax[0,0].set_xlabel('distance')\n    a = ax[0,0].set_ylabel('prix')\n    a = ax[0,0].scatter(train_data[dist],train_data['fare_amount'])\n\n\n    # Zoom In\n    idx_zoom = (train_data[dist]<train_data[dist].quantile(0.75)) & \\\n    (train_data['fare_amount'] < 100)\n    a = ax[0,1].set_title('Zoom in')\n    a = ax[0,1].set_xlabel('distance')\n    a = ax[0,1].set_ylabel('prix')\n    a = ax[0,1].scatter(train_data[dist][idx_zoom],train_data['fare_amount'][idx_zoom],\\\n                      c = np.clip(train_data['fare_amount'][idx_zoom], 0, 100), cmap='viridis')\n\n\n    # distance between 0 and 50 kms\n\n    a = ax[1,0].set_xlabel(dist)\n    a = ax[1,0].set_ylabel('prix')\n    a = ax[1,0].scatter(train_data[dist][train_data[dist]<50],train_data['fare_amount'][train_data[dist]<50])\n\n\n    idx_zoom2 = (train_data[dist]<30)\n    a = ax[1,1].set_title('Zoom in')\n    a = ax[1,1].set_xlabel('distance')\n    a = ax[1,1].set_ylabel('prix')\n    a = ax[1,1].scatter(train_data[dist][idx_zoom2],train_data['fare_amount'][idx_zoom2],\\\n                      c = np.clip(train_data['fare_amount'][idx_zoom2], 0, 100), cmap='viridis')\n\n    # We assume that a linear regression could be a good estimate\n    # though there are outliers with big prices for short distances\n    # and there is the same price for different distances around 50$\n    \nplot_fare_distance('h_dist')\nplot_fare_distance('m_dist')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a92b20b2f0bac156c68fc55cc12d8b52a110411"},"cell_type":"markdown","source":"# Prediction model"},{"metadata":{"_uuid":"242252ba83777bfe9286688a218167c29424be5e"},"cell_type":"markdown","source":"## Load the dataset"},{"metadata":{"trusted":true,"_uuid":"ce670812353b3af9ab719c6e266a9b22225f015b"},"cell_type":"code","source":"train_df = pd.read_csv(r'../input/train.csv', nrows=5000000)\ntest_df = pd.read_csv(r'../input/test.csv')\n\ntrain_df.head()\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc5093ec5576d93185333bef175b393599d2cc80"},"cell_type":"markdown","source":"## Drop non coherent values"},{"metadata":{"trusted":true,"_uuid":"8f26a338fada9f2b2064a32e33c2fb7e808b8420"},"cell_type":"code","source":"train_df = train_df[train_df['fare_amount']>=0]\n\ntrain_df = train_df.drop(train_df[train_df.pickup_longitude==0].index)\ntrain_df = train_df.drop(train_df[train_df.pickup_latitude==0].index)\ntrain_df = train_df.drop(train_df[train_df.dropoff_longitude==0].index)\ntrain_df = train_df.drop(train_df[train_df.dropoff_latitude==0].index)\ntrain_df = train_df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4bfad3b9a4f51f300fb73159ef68ea1d5c36a46"},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"trusted":true,"_uuid":"215b810203a4bffbd99b3ae3249a391d2d642040"},"cell_type":"code","source":"train_df['m_dist'] = manhattan_distance(train_df['pickup_latitude'], train_df['dropoff_latitude'], \\\n                                        train_df['pickup_longitude'], train_df['dropoff_longitude'])\ntest_df['m_dist'] = manhattan_distance(test_df['pickup_latitude'], test_df['dropoff_latitude'], \\\n                                        test_df['pickup_longitude'], test_df['dropoff_longitude'])\n\nfor i in range(len(train_df)):\n    train_df.at[i,'pickup_datetime'] = train_df.at[i,'pickup_datetime'].split(' UTC')[0]\ntrain_df['pickup_datetime'] = pd.to_datetime(train_df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\ntrain_df['month'] = train_df['pickup_datetime'].dt.month\ntrain_df['day'] = train_df['pickup_datetime'].dt.day\ntrain_df['hour'] = train_df['pickup_datetime'].dt.hour\n\nfor i in range(len(test_df)):\n    test_df.at[i,'pickup_datetime'] = test_df.at[i,'pickup_datetime'].split(' UTC')[0]\ntest_df['pickup_datetime'] = pd.to_datetime(test_df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\ntest_df['month'] = test_df['pickup_datetime'].dt.month\ntest_df['day'] = test_df['pickup_datetime'].dt.day\ntest_df['hour'] = test_df['pickup_datetime'].dt.hour\n\ntrain_df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b563b8ee8fae9b3ae49868683fcc71450228fa6"},"cell_type":"code","source":"features = ['m_dist']\n\nX = train_df[features]\nY = train_df['fare_amount']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"292168d6344c3c59b1b20949bc59591058b7463e"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y)\nX_train.shape\nY_train.shape\nX_test.shape\nY_test.shape\n\nnp.any(np.isnan(X_train))\nnp.all(np.isfinite(X_train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d85839fc09abbefa8ccf9c01ff338d22733c30e6"},"cell_type":"markdown","source":"## Establishing a baseline"},{"metadata":{"trusted":true,"_uuid":"f850933d7ad747ebbddd3f82516aeb6e7d130049"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6dd747076b0a7d5eec5e057a4f90515beeba92c","scrolled":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nlin = LinearRegression()\nlin.fit(X_train, Y_train)\nY_test_pred = lin.predict(X_test)\n\nprint(Y_test_pred)\n\nprint('RMSE of the baseline is :')\nnp.sqrt(mean_squared_error(Y_test, Y_test_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c68626dc13294b0a840eb538c661eee63e4e3b71"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true,"_uuid":"cf5b58b47c3783b0623a412108abe9f7e5173648"},"cell_type":"code","source":"features = ['m_dist', 'passenger_count', 'month', 'day', 'hour']\n#features = ['m_dist', 'day']\n\n\nX = train_df[features]\nY = train_df['fare_amount']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96e9b5f546f02607b216c96b411e477f2f4a8378"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"475583b337f89df94b76235abcaef2387b1e3c0a"},"cell_type":"code","source":"clf = RandomForestRegressor()\nclf.fit(X_train, Y_train)\nprint(clf.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d71d6cea1112989886cc0025d53c73343a19905b"},"cell_type":"code","source":"Y_test_pred = clf.predict(X_test)\nprint(Y_test_pred)\nprint('RMSE of the the Random Forest Regressor is :')\nnp.sqrt(mean_squared_error(Y_test, Y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"178634a5bc73d7f50a59210be9cb6c0b0ffc1c6a"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"9c4ee7eae1d130b9ce37146bb90312a51020444f"},"cell_type":"code","source":"x_test = test_df[features]\ny_test_pred = clf.predict(x_test)\nsubmission = pd.DataFrame(\n    {'key': test_df.key, 'fare_amount': y_test_pred},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b4387fe34839861d6c73797ca5197d4e3b3c498"},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecf265f51fdf6ad8101408850f0502dbfa3fa069"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}