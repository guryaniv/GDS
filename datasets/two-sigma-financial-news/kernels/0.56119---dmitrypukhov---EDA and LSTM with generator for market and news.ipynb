{
  "cells": [
    {
      "metadata": {
        "_uuid": "7d991c36c4724376252558b4e494858192a5dc1d"
      },
      "cell_type": "markdown",
      "source": "# 1. Introduction\n\nI used ideas and sometimes copy pasted the code from kernels:\n\nEDA, outliers: https://www.kaggle.com/artgor/eda-feature-engineering-and-everything\n\nNN: https://www.kaggle.com/christofhenkel/market-data-nn-baseline#\n\nLSTM: https://www.kaggle.com/pablocastilla, https://www.kaggle.com/sergeykalutsky/lstm-model-on-market-data#,  https://www.kaggle.com/ashkaan/lstm-baseline# \n\nNews processing: https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data# \n\n**Disclaimer:** currently the model's performance is not perfect.\n\n**ToDo:**\n \n1. Experiment with news resampling on different periods - 10 days. Now many market rows have** empty news** joined\n1. Train on random time windows instead of sampling single records.\n1. Try technical indicators on market data: MACD, RSI etc. \n1. Work with residuals instead of raw data\n"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#####################################\n# Libraries\n#####################################\n# Common libs\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport os.path\nimport random\nfrom pathlib import Path\nfrom time import time\nfrom itertools import chain\n\n# Image processing\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n#from skimage.transform import rescale, resize, downscale_local_mean\n\n# Charts\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\n\n\n# ML\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n#from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n#from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n#from sklearn.preprocessing import OneHotEncoder\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, LSTM, Embedding\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\n#####################################\n# Settings\n#####################################\nplt.style.use('seaborn')\n# Set random seed to make results reproducable\nnp.random.seed(42)\ntensorflow.set_random_seed(42)\nos.environ['PYTHONHASHSEED'] = '42'\n# Improve printed df readability\npd.options.display.float_format = '{:,.4f}'.format\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 200)\n\nprint(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e4c6d73d0bd15aa0a90ba67e81cac4f3884815c"
      },
      "cell_type": "code",
      "source": "# This competition settings\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdb8de212049feaee5ae7d97251e673167317a51"
      },
      "cell_type": "code",
      "source": "# Read the data\n# Read market and news data\n(market, news) = env.get_training_data()\n\n# Set time index at market data\nmarket.time = market.time.astype('datetime64[D, UTC]')\n#market.set_index(['time', 'assetCode'], inplace=True, drop=False)\nnews.time = news.time.astype('datetime64[D, UTC]')\n#news.set_index(['time', 'assetCode'],inplace=True, drop=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3cbafb2d81af7efc279b3e82c12db5560c11de42"
      },
      "cell_type": "markdown",
      "source": "# 2. Market data EDA\n\n"
    },
    {
      "metadata": {
        "_uuid": "08ee61bcfbe325cd164cb482e76dc79d4cd136c6"
      },
      "cell_type": "markdown",
      "source": "## General view of market data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88b43981558d36cfc7ad8424cbab2876739a4035"
      },
      "cell_type": "code",
      "source": "market.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "d45d4122252b45c730e1b1329d74e17f28aed2f6"
      },
      "cell_type": "code",
      "source": "# Look at column types\nmarket.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "beb3dfd4d3bfb51440aa8a7f74e5a448b677320b"
      },
      "cell_type": "code",
      "source": "# Look at min-max, quantiles\nmarket.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5dc1a2d927875f06b513ba9997ffdbe803c9918d"
      },
      "cell_type": "code",
      "source": "# How many total records and assets are in the data\nnassets=len(market.assetName.unique().categories)\nnrows=market.close.count()\nprint(\"Total count: %d records of %d assets\" % (nrows, nassets))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0e706e6de7be9968c23fe9d2e03a6e0cf97cefef"
      },
      "cell_type": "markdown",
      "source": "## Look at label values\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bf0b433b2b62fc7fecbb73f2047845bcab9421e"
      },
      "cell_type": "code",
      "source": "# Plot label column\nmarket.returnsOpenNextMktres10.plot(figsize=(12,5))\nplt.title('Label values: returnsOpenNextMktres10')\nplt.ylabel('returnsOpenNextMktres10')\nplt.xlabel('Observation no')\nplt.show()\n\n# Look at quantiles\nmarket.returnsOpenNextMktres10.describe(percentiles=[0.01, 0.99])\n#market.returnsOpenNextMktres10.describe()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d9d7ec6e1010b5fff31de23e1dd5c1ed0c067ece"
      },
      "cell_type": "markdown",
      "source": "As we can see, the most of labels lay between -0.2 and 0.2 and there are otliers. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4687e251755a199c0321a20706b4b514a6d2482"
      },
      "cell_type": "code",
      "source": "sns.distplot(market.returnsOpenNextMktres10.clip(-1,1))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ecb013e4cef2cfec05b4bf61e72fa50e1d78462"
      },
      "cell_type": "markdown",
      "source": "## Price and volume chart of random asset."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "429383be22444b0e8ab691025d1a5f5655f4f532"
      },
      "cell_type": "code",
      "source": "def plot_random_asset(market):\n    \"\"\"\n    Get random asset, show price, volatility and volume\n    \"\"\"\n    # Get any asset\n    ass = market.assetCode.sample(1, random_state=24).iloc[0]\n    ass_market = market[market['assetCode'] == ass]\n    ass_market.index = ass_market.time\n\n    # Plotting\n    f, axs = plt.subplots(3,1, sharex=True, figsize=(12,8))\n    # Close price \n    ass_market.close.plot(ax=axs[0])\n    axs[0].set_ylabel(\"Price\")\n\n    # Volatility (close-open)\n    volat_df = (ass_market.close - ass_market.open)\n    (ass_market.close - ass_market.open).plot(color='green', ax = axs[1])\n    axs[1].set_ylabel(\"Volatility\")\n\n    # Volume\n    ass_market.volume.plot(ax=axs[2], color='darkred')\n    axs[2].set_ylabel(\"Volume\")\n\n    # Show the plot\n    f.suptitle(\"Asset: %s\" % ass, fontsize=22)\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.93)\n    plt.show()\n\nplot_random_asset(market)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6c4f895acc2ee587015e5a048e6b29c672987cb7"
      },
      "cell_type": "markdown",
      "source": "# 3. News data EDA"
    },
    {
      "metadata": {
        "_uuid": "74af99712fedd3c3444d11dc56cc6f4c37783f13"
      },
      "cell_type": "markdown",
      "source": "## General look"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f929789f26f86022c9717166fdd9065af86aebd"
      },
      "cell_type": "code",
      "source": "news.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "218f014ba0bb68ce59c4e60a838cf58777c848ed",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# See column types\nnews.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dda568f22f29d229dec984c5bc9c5ab522df9253"
      },
      "cell_type": "code",
      "source": "nnews = news.size\nnassets = len(news.assetName.cat.categories)\nprint(\"Total %d news about %d assets\" % (nnews, nassets))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ad1660ff27ae184bda5253b204ab5edcee511934"
      },
      "cell_type": "markdown",
      "source": "## Positivity and negativity\nLet's see which attitude prevails in news."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "144e5d4a831086e37c87ee038f8ee7578a5b740a"
      },
      "cell_type": "code",
      "source": "    # Barplot on negative, neutral and positive columns.\n    news[['sentimentNegative', 'sentimentNeutral','sentimentPositive']].mean().plot(kind='bar')\n    plt.title(\"News positivity chart\")\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "37dad8f7614beb8e9af710199a61b798a510be6f"
      },
      "cell_type": "markdown",
      "source": "Neutral and positive a little bit higher. So according to the news the market is something like flat whith a little grow tendency."
    },
    {
      "metadata": {
        "_uuid": "6f5555947cb4d6d0e197ceba77a4c970b7387618"
      },
      "cell_type": "markdown",
      "source": "# 4.  Preprocess the data\nWe are going to use data generator to feed the model. Generator yields data to the model batch by batch. For each batch we are doing following steps:\nPreprocess news and market separately. Then join them and yield, so it will come to the **model.fit_generate** method."
    },
    {
      "metadata": {
        "_uuid": "6baf8670d3ee8e0bad58992cf76c6454157aefcf"
      },
      "cell_type": "markdown",
      "source": "## Split to train, validation and test\n\nWe are using indices with time only. Full features and labels will be prepared in generator per batch to save memory."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5f7720c368c90479c2b9bfc857372d820328771"
      },
      "cell_type": "code",
      "source": "toy = True\n\ndef train_test_val_split(market):\n    \"\"\"\n    Get sample of assets but each asset has full market data after 2009\n    Split to time sorted train, validation and test.\n    @return: train, validation, test df. Short variant - time and asset columns only\n    \"\"\"\n    # Work with data after 2009\n    market_idx = market[market.time > '2009'][['time', 'assetCode']]\n    if toy: market_idx = market_idx.sample(100000)\n    else: market_idx = market_idx.sample(1000000)\n    # Split to train, validation and test\n    market_idx = market_idx.sort_values(by=['time'])\n    market_train_idx, market_test_idx = train_test_split(market_idx, shuffle=False, random_state=24)\n    market_train_idx, market_val_idx = train_test_split(market_train_idx, test_size=0.1, shuffle=False, random_state=24)\n    return(market_train_idx, market_val_idx, market_test_idx)\n\n# Split\nmarket_train_idx, market_val_idx, market_test_idx = train_test_val_split(market)\n\n# Plot train/val/test size\nsns.barplot(['Train', 'Validation', 'Test'],[market_train_idx.index.size,market_val_idx.index.size,market_test_idx.index.size])\nplt.title('Train, validation, test split.')\nplt.ylabel('Count')\nplt.show()\n\nmarket_train_idx.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eafcd6a52d8e6375dd4d54fce2c41029d180454d"
      },
      "cell_type": "markdown",
      "source": "## Market preprocessor\nPrepare market batch for generator - scale numeric columns, encode categorical etc."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "85edd087f2e51454ba31d540d624004af2f9e288"
      },
      "cell_type": "code",
      "source": "class MarketPrepro:\n    \"\"\"\n    Standard way to generate batches for model.fit_generator(generator, ...)\n    Should be fit on train data and used on all train, validation, test\n    \"\"\"\n    # Features\n    assetcode_encoded = []\n    assetcode_train_count = 0\n    time_cols=['year', 'week', 'day', 'dayofweek']\n    numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n    feature_cols = ['assetCode_encoded']  + time_cols + numeric_cols\n    \n    # Labels\n    label_cols = ['returnsOpenNextMktres10']   \n    \n    def __init__(self):\n        self.cats={}\n    \n    def fit(self, market_train_df):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df.\n        Store given indices to generate batches_from.\n        @param market_train_df: train data to fit on\n        \"\"\"\n        df = market_train_df.copy()\n        # Clean bad data. We fit on train dataset and it's ok to remove bad data\n        market_train_df = self.fix_train(market_train_df)\n        \n        # Extract day, week, year from time\n        market_train_df = self.prepare_time_cols(market_train_df)\n        \n        # Fit for numeric and time\n        self.numeric_scaler = StandardScaler()\n        self.numeric_scaler.fit(market_train_df[self.numeric_cols + self.time_cols].astype(float))\n        \n        # Fit asset encoding\n        market_train_df = self.encode_asset(market_train_df, True)\n        \n    def fix_train(self, train_df):\n        \"\"\"\n        Remove bad data. For train dataset only\n        \"\"\"\n        # Remove strange cases with close/open ratio > 2\n        max_ratio  = 2\n        train_df = train_df[(train_df['close'] / train_df['open']).abs() <= max_ratio].loc[:]\n        # Fix outliers etc like for test set\n        train_df = self.safe_fix(train_df)\n        return(train_df)\n\n    def safe_fix(self, df):\n        \"\"\"\n        Fill na, fix outliers. Safe for test dataset, no rows removed.\n        \"\"\"\n        # Fill nans\n        df[self.numeric_cols] = df[ ['assetCode'] + self.numeric_cols].groupby('assetCode').transform(lambda g: g.fillna(method='bfill'))\n        \n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        # Fix outliers\n        df[self.numeric_cols] = df[self.numeric_cols].clip(df[self.numeric_cols].quantile(0.01), df[self.numeric_cols].quantile(0.99), axis=1)\n        return(df)\n    \n    def get_X(self,df):\n        \"\"\"\n        Preprocess and return X without y\n        \"\"\"\n        df = df.copy()\n        # Fix bad data without removing rows\n        df = self.safe_fix(df)\n\n        # Add day, week, year\n        df = self.prepare_time_cols(df)\n        # Encode assetCode\n        df = self.encode_asset(df, is_train=False)\n        # Scale numeric features and labels\n        df[self.numeric_cols+self.time_cols] = self.numeric_scaler.transform(df[self.numeric_cols+self.time_cols].astype(float))\n\n        # Return X\n        return df[self.feature_cols]\n    \n    def get_y(self, df):\n        y=(df[self.label_cols] >=0).astype(float)\n        return y\n\n    def encode_asset(self, df, is_train):\n        def encode(assetcode):\n            \"\"\"\n            Encode categorical features to numbers\n            \"\"\"\n            try:\n                # Transform to index of name in stored names list\n                index_value = self.assetcode_encoded.index(assetcode) +1\n            except ValueError:\n                # If new value, add it to the list and return new index\n                self.assetcode_encoded.append(assetcode)\n                index_value = len(self.assetcode_encoded)\n\n            #index_value = 1.0/(index_value)\n            index_value = index_value / (self.assetcode_train_count + 1)\n            return(index_value)       \n        \n        if is_train:\n            self.assetcode_train_count = len(df['assetCode'].unique())+1\n        df['assetCode_encoded'] = df['assetCode'].apply(lambda assetcode: encode(assetcode))\n        return(df)\n        \n    def prepare_time_cols(self, df):\n        \"\"\" \n        Extract time parts, they are important for time series \n        \"\"\"\n        df = df.copy()\n        df['year'] = df['time'].dt.year\n        # Maybe remove month because week of year can handle the same info\n        df['day'] = df['time'].dt.day\n        # Week of year\n        df['week'] = df['time'].dt.week\n        df['dayofweek'] = df['time'].dt.dayofweek\n        return(df)\n\n# Create instance for global usage    \nmarket_prepro = MarketPrepro()\nprint('market_prepro created')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d8ea2a3998e93f5a2f8d2c4a6c7aef7de737bc75"
      },
      "cell_type": "markdown",
      "source": "### News preprocessor\nPrepare news batch for generator.\nAsset can have many news per day, so group them by asset, day and aggregate. Then normalize numerical values. News aggregation part is based on this kernel: https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data#"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "725ee92df319ffdba9d5db27ac68e08860f67f4a"
      },
      "cell_type": "code",
      "source": "class NewsPrepro:\n    \"\"\"\n    Aggregate news by day and asset. Normalize numeric values.\n    \"\"\"\n    news_cols_agg = {\n        'urgency': ['min', 'count'],\n        'takeSequence': ['max'],\n        'bodySize': ['min', 'max', 'mean', 'std'],\n        'wordCount': ['min', 'max', 'mean', 'std'],\n        'sentenceCount': ['min', 'max', 'mean', 'std'],\n        'companyCount': ['min', 'max', 'mean', 'std'],\n        'marketCommentary': ['min', 'max', 'mean', 'std'],\n        'relevance': ['min', 'max', 'mean', 'std'],\n        'sentimentNegative': ['min', 'max', 'mean', 'std'],\n        'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n        'sentimentPositive': ['min', 'max', 'mean', 'std'],\n        'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n        'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n        'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n        'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n        'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n        'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n        'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n        'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n        'volumeCounts7D': ['min', 'max', 'mean', 'std']\n            }\n    news_cols_numeric = set(news_cols_agg.keys()) - set(['assetCode', 'time'])\n        \n    def fit(self, news_train_df):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df.\n        @param news_train_df: train data to fit on\n        \"\"\"\n        # Fill na with previous value for the asset\n        news_train_df = news_train_df.copy()\n        #news_train_df = news_train_df.groupby('assetCode').transform(lambda g: g.fillna(method='bfill'))\n        \n        # Aggregation\n        news_train_df_agg = self.aggregate_news(news_train_df)\n        news_train_df_agg.fillna(0, inplace=True)\n        #news_train_df_agg = news_train_df_agg._get_numeric_data().astype(float)\n        \n        #Fit scaler\n        self.numeric_scaler = StandardScaler()\n        self.numeric_scaler.fit(news_train_df_agg)\n        # Save news feature cols\n        self.feature_cols = list(news_train_df_agg.columns.values)\n\n    def get_X(self, df):\n        news_df = df.copy()\n        # Fill na with previous value for the asset\n        #news_df = df.groupby('assetCode').transform(lambda g: g.fillna(method='bfill'))\n        \n        # Aggregate by time, asset code\n        news_df = self.aggregate_news(df)\n        # Normalize, fillna etc. Don't remove rows.\n        news_df.fillna(0, inplace=True)\n        if not news_df.empty:\n            news_df_numeric = news_df._get_numeric_data().astype(float)\n            news_df[news_df_numeric.columns] = self.numeric_scaler.transform(news_df_numeric)\n        return(news_df)\n        \n    def aggregate_news(self, df):\n        # Fix asset codes (str -> list)\n        df['assetCodes'] = df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n\n        # Leave only days in time\n        if not df.empty: df.time = df.time.astype('datetime64[D, UTC]') #.tail()\n        \n        #Expand assetCodes\n        assetCodes_expanded = list(chain(*df['assetCodes']))\n        \n        if(not df.empty): assetCodes_index = df.index.repeat(df['assetCodes'].apply(len)) \n        else: assetCodes_index = df.index\n        assert len(assetCodes_index) == len(assetCodes_expanded)\n        df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n        # Create expanded news (will repeat every assetCodes' row)\n        news_cols = ['time', 'assetCodes'] + sorted(list(self.news_cols_agg.keys()))\n        df_expanded = pd.merge(df_assetCodes, df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n        # Aggregate numerical news features\n        df_aggregated = df_expanded.groupby(['time', 'assetCode']).agg(self.news_cols_agg)\n\n        # Convert to float32 to save memory\n        #df_aggregated = df_aggregated.apply(np.float32)\n\n        # Flat columns\n        df_aggregated.columns = ['_'.join(col).strip() for col in df_aggregated.columns.values]\n\n        return df_aggregated    \n        \n# Create instance for global usage\nnews_prepro = NewsPrepro()\nprint('news_prepro created')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a3fa368ce3db72952421561b00ce1bbc000f3d5"
      },
      "cell_type": "markdown",
      "source": "## Join market and news\nGenerator, tests and submission will call this facade to request joined market&news data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b84e6f57dba7ed270b14ec28888d9d537c0806a"
      },
      "cell_type": "code",
      "source": "class JoinedPreprocessor:\n    def __init__(self, market_prepro, news_prepro):\n        self.market_prepro = market_prepro\n        self.news_prepro = news_prepro\n        \n    def fit(self, market_train_idx, market, news):\n        # market has index [time, assetCode]\n        market_train_df = market.loc[market_train_idx.index]\n        self.market_prepro.fit(market_train_df)\n        # We select news in train time interval\n        news_train_df = news.merge(market_train_idx, on=['time'])\n        self.news_prepro.fit(news_train_df)\n    \n    def get_X(self, market_df, news_df):\n        # Market should already has index (time, assetCode)\n        # Preprocess market X\n        market_X = market_prepro.get_X(market_df)\n        market_X['time'] = market_df['time']\n        market_X['assetCode'] = market_df['assetCode']\n        \n        #news_X will have index [time, assetCode]\n        news_X = news_prepro.get_X(news_df)\n        # Join by index, which is time, assetCode. Some assets have no news at all, so left join and 0 nans\n        X = market_X.merge(news_X, how='left', left_on=['time', 'assetCode'], right_on=['time','assetCode'],  right_index=True)\n        \n        # Some market data can be without news, fill nans\n        X.fillna(0, inplace=True)\n        # Return features market + news from joined df\n        features = X[market_prepro.feature_cols + news_prepro.feature_cols]\n        return(features)\n\n    def get_y(self, market_df): \n        return(self.market_prepro.get_y(market_df))\n    \n    def get_Xy(self, market_df, news_df):\n        return(self.get_X(market_df, news_df), self.get_y(market_df))\n    \n    def fix_train(self, market_df, news_df):\n        \"\"\"\n        Clean train data. Here we can remove bad rows\n        \"\"\"\n        return(market_prepro.fix_train(market_df), news_df)\n\n    \n# Market and news preprocessor instance\nprepro = JoinedPreprocessor(market_prepro, news_prepro)\nprepro.fit(market.loc[market_train_idx.index], market, news)\nprint('Preprocessor created, it is fit')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58187e775042c68fada7f699d809a3a78bb3041e"
      },
      "cell_type": "markdown",
      "source": "### Look at market and news X, y\nWe can preprocess a sample and check scaled X, y.  At this point there is no look back window for LSTM, it will be calculated in generator later."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "513c6ad0abfb0f8efc145fd36ab5b590883c932a"
      },
      "cell_type": "code",
      "source": "def get_merged_Xy(idx):\n    \"\"\"\n    Show min/max and quantiles for given sample\n    \"\"\"\n    market_df = market.loc[idx.index]\n    # Select subset of news for future merge by assetCode and time. \n    news_df = news.merge(idx, on=['time'])\n    X, y = prepro.get_Xy(market_df, news_df)\n    return pd.concat([X,y], axis=1)\n\n# Look at statistics of preprocessed sample\nget_merged_Xy(market_test_idx.sample(10000)).describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ba0bad0c479fb66301530eb12f288d926b10c02"
      },
      "cell_type": "markdown",
      "source": "## Data generator\nKeras standard approach to generate batches for **model.fit_generator()**. \nGet market and news data together here.\n\n**Opened question:** how to better organize the data?\nIdeally we could have one trained model per asset. But there are more than 3K assets - no resources for such a big train. There also are new unseen assets in future data. Still have no clear idea how to handle this.\n\n**Current implementation: ** sort sample data by assetCode, time, like human trader looks at candlestick charts one by one.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe9e4d1c83f80d45cb7ef9baa4e3890ddf31f7e8"
      },
      "cell_type": "code",
      "source": "class JoinedGenerator:\n    \"\"\"\n    Keras standard approach to generage batches for model.fit_generator() call.\n    \"\"\"\n    def __init__(self, prepro, market, news, index_df):\n        \"\"\"\n        @param preprocessor: market and news join preprocessor\n        @param market: full loaded market df\n        @param news: full loaded news df\n        @param index_df: df with assetCode and time of train or validation market data. Batches will be taken from them.\n        \"\"\"\n        self.market = market\n        self.prepro = prepro\n        self.news = news\n        self.index_df = index_df\n\n    def flow_lstm(self, batch_size, is_train, look_back, look_back_step):\n        \"\"\"\n        Generate batch data for LSTM NN\n        Each cycle in a loop we yield a batch for one training step in epoch. \n        \"\"\"\n        while True:\n            # Get market indices of random assets, sorted by assetCode, time.\n            batch_index_df = self.get_random_assets_idx(batch_size)\n\n            # Get X, y data for this batch, containing market and news, but without look back yet\n            X, y = self.get_batch(batch_index_df, is_train)\n            # Add look back data to X, y\n            X, y = self.with_look_back(X,y,look_back,look_back_step)\n            yield X,y\n    \n    def get_random_assets_idx(self, batch_size):\n        \"\"\"\n        Get random asset and it's last market data indices.\n        Repeat for next asset until we reach batch_size.\n        \"\"\"\n        asset_codes = self.index_df['assetCode'].unique().tolist()\n\n        # Insert first asset\n        asset = np.random.choice(asset_codes)\n        asset_codes.remove(asset)\n        batch_index_df = self.index_df[self.index_df.assetCode == asset].tail(batch_size)\n        # Repeat until reach batch_size records\n        while (batch_index_df.index.size < batch_size) and (len(asset_codes) > 0):\n            asset = np.random.choice(asset_codes)\n            asset_codes.remove(asset)\n            asset_index_df = self.index_df[self.index_df.assetCode == asset].tail(batch_size - batch_index_df.index.size)\n            batch_index_df = pd.concat([batch_index_df, asset_index_df])\n        \n        return batch_index_df.sort_values(by=['assetCode', 'time'])\n            \n    def get_batch(self, batch_idx, is_train):\n        \"\"\"\n        Get batch of market-news data withoutlook back yet.\n        \"\"\"\n        market_df = self.market.loc[batch_idx.index]\n        # Select subset of news for future merge by assetCode and time. \n        news_df = news.merge(batch_idx, on=['time'])\n        # Remove bad rows, clean the data. It's ok for train.\n        if is_train: \n            market_df, news_df = prepro.fix_train(market_df, news_df)\n        # Join market and news using preprocessor       \n        X = self.prepro.get_X(market_df, news_df)\n        y = self.prepro.get_y(market_df)\n        return(X, y)\n    \n    # convert an array of values into a dataset matrix\n    def with_look_back(self, X, y, look_back, look_back_step):\n        \"\"\"\n        Add look back window values to prepare dataset for LSTM\n        \"\"\"\n        X_processed, y_processed = [], []\n        # Fix last window in batch, can be not full\n        if look_back > len(X): \n            look_back = len(X)\n            look_back_step = min(look_back_step, look_back)\n            \n        for i in range(0,len(X)-look_back+1):\n            # Add lookback to X\n            x_window = X.values[i:(i+look_back):look_back_step, :]\n            X_processed.append(x_window)\n            # If input is X only, we'll not output y\n            if y is None: continue\n            # Add lookback to y\n            y_window = y.values[i+look_back-1, :]\n            y_processed.append(y_window)\n        # Return Xy for train/test or X for prediction\n        if(y is not None): return np.array(X_processed), np.array(y_processed)\n        else: return np.array(X_processed)\n\n    \n# Train data generator instance\njoin_generator = JoinedGenerator(prepro, market, news, market_train_idx)\n\n# Validation data generator instance\nval_generator = JoinedGenerator(prepro, market, news, market_val_idx)\nprint('Generators created')\n\n# X,y=next(join_generator.flow_lstm(20,True,10,2))\n# print(X.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a3b7fafdd69852eac0ad94501adfe87ef4a48e7b"
      },
      "cell_type": "markdown",
      "source": "# 5. Base LSTM model for market and news\n"
    },
    {
      "metadata": {
        "_uuid": "9ae61753db796dbbbcbbce536ade22008c7e33e4"
      },
      "cell_type": "markdown",
      "source": "## Define the model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7eae329a820e1614958e031fccc9e47c065d4abd",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "class ModelFactory:\n    \"\"\"\n    Generate different models. Actually only one of them is used in the kernel, \n    this factory is for experiments when debugging.\n    \"\"\"\n    # LSTM look back window size\n    look_back=90\n    # In windows size look back each look_back_step days\n    look_back_step=10\n\n    def lstm_128():\n        model = Sequential()\n        # Add an input layer market + news\n        input_size = len(market_prepro.feature_cols) + len(news_prepro.feature_cols)\n        # input_shape=(timesteps, input features)\n        model.add(LSTM(units=128, return_sequences=True, input_shape=(None,input_size)))\n        model.add(LSTM(units=64, return_sequences=True ))\n        model.add(LSTM(units=32, return_sequences=False))\n        \n        # Add an output layer \n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n        \n        return(model)        \n\nmodel = ModelFactory.lstm_128()\nmodel.summary()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a06ff01bdad074a86dce8a3ef685f15eac017f4c"
      },
      "cell_type": "markdown",
      "source": "## Train market and news model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4e5814f7f8cbd1591c99e7580d16072ebf93961"
      },
      "cell_type": "code",
      "source": "weights_file='best_weights.h5'\n\n# We'll stop training if no improvement after some epochs\nearlystopper = EarlyStopping(patience=5, verbose=1)\n\n# Low, avg and high scor training will be saved here\n# Save the best model during the traning\ncheckpointer = ModelCheckpoint(weights_file\n    #,monitor='val_acc'\n    ,verbose=1\n    ,save_best_only=True\n    ,save_weights_only=True)\n\n#reduce_lr = ReduceLROnPlateau(factor=0.2, patience=3, min_lr=0.001)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.001)\n\n# Set fit parameters\n# Rule of thumb: steps_per_epoch = TotalTrainingSamples / TrainingBatchSize\n#                validation_steps = TotalvalidationSamples / ValidationBatchSize\nif toy:\n    batch_size=1000\n    validation_batch_size=1000\n    steps_per_epoch=5\n    validation_steps=2\n    epochs=5\n    ModelFactory.look_back=30\n    ModelFactory.look_back_step=5\nelse:\n    batch_size=1000\n    validation_batch_size=1000\n    steps_per_epoch=20\n    validation_steps=5\n    epochs=20\n\nprint(f'Toy:{toy}, epochs:{epochs}, steps per epoch: {steps_per_epoch}, validation steps:{validation_steps}')\nprint(f'Batch_size:{batch_size}, validation batch size:{validation_batch_size}')\n\n# Fit\ntraining = model.fit_generator(join_generator.flow_lstm(batch_size=batch_size \n            , is_train=True \n            , look_back=ModelFactory.look_back \n            , look_back_step=ModelFactory.look_back_step) \n        , epochs=epochs \n        , validation_data=val_generator.flow_lstm(batch_size=validation_batch_size\n            , is_train=False\n            , look_back=ModelFactory.look_back\n            , look_back_step=ModelFactory.look_back_step) \n        , steps_per_epoch=steps_per_epoch \n        , validation_steps=validation_steps \n        , callbacks=[earlystopper, checkpointer, reduce_lr])\n# Load best weights saved\nmodel.load_weights(weights_file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8364d16fd63e4ef983573c9f4237e02ad0d004e9"
      },
      "cell_type": "markdown",
      "source": "## Evaluate market model\n\n### Loss function by epoch "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cc30491137c941043e2bd82f739c4147e5c1569f"
      },
      "cell_type": "code",
      "source": "# # Plotting\n# f, axs = plt.subplots(3,1, sharex=True, figsize=(12,8))\n# # Close price \n# ass_market.close.plot(ax=axs[0])\n# axs[0].set_ylabel(\"Price\")\n\nplt.figure(1, figsize=(8,3))\nplt.subplot(121)\nplt.plot(training.history['loss'])\nplt.plot(training.history['val_loss'])\nplt.title(\"Loss and validation loss\")\nplt.legend([\"Loss\", \"Validation loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.subplot(122)\nplt.plot(training.history['acc'])\nplt.plot(training.history['val_acc'])\nplt.title(\"Acc and validation acc\")\nplt.legend([\"Acc\", \"Validation acc\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.suptitle('Training history', fontsize=16)\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "386c1f3461d9b9b3df18856eceb53dd2ee495579"
      },
      "cell_type": "markdown",
      "source": "### Predict on test data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f843bc7b234b61837de390722af3f191737c483"
      },
      "cell_type": "code",
      "source": "def predict_on_test():\n    # Predict on last test data\n    pred_size=100\n    pred_idx = market_test_idx.tail(pred_size + ModelFactory.look_back)\n    market_df = market.loc[pred_idx.index]\n    news_df = news.merge(pred_idx, on=['time'])\n    # Get preprocessed X, y\n    X_test, y_test = prepro.get_Xy(market_df, news_df)\n    # Add there look back rows for LSTM\n    X_test, y_test = join_generator.with_look_back(X_test, y_test, look_back = ModelFactory.look_back, look_back_step=ModelFactory.look_back_step)\n    \n    # Predict\n    y_pred = model.predict(X_test)*2-1\n\n    # Plot\n    ax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n    ax1.plot(market_df['returnsOpenNextMktres10'].values, linestyle='none', marker='.', color='darkblue')\n    ax1.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    ax1.legend([\"Ground truth\",\"Predicted\"])\n    ax1.set_title(\"Both\")\n    ax1.set_xlabel(\"Epoch\")\n    ax2 = plt.subplot2grid((2, 2), (0, 1), colspan=1,rowspan=1)\n    ax2.plot(market_df['returnsOpenNextMktres10'].values, linestyle='none', marker='.', color='darkblue')\n    ax2.set_title(\"Ground truth\")\n    ax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1,rowspan=1)\n    ax3.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    ax3.set_title(\"Predicted\")\n    plt.tight_layout()\n    plt.show()\n\npredict_on_test()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e34e3ee9448fcbc131830b3277ee830fa5e6ec1b"
      },
      "cell_type": "markdown",
      "source": "### Predict on random asset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "180acea47aa0580448e8790bf55b64d8e202013b"
      },
      "cell_type": "code",
      "source": "def predict_random_asset():\n    \"\"\"\n    Get random asset from test set, predict on it, plot ground truth and predicted value\n    \"\"\"\n    # Get any asset\n    asset = market_test_idx['assetCode'].sample(1, random_state=24).values[0]\n    #asset_idx = market_test_idx[market_test_idx.assetCode == asset]\n    market_df = market.loc[market.assetCode == asset].copy().set_index(['time'], drop=False)\n    news_df = news.merge(market_df, on=['time'])\n    \n    # Preprocess market and news\n    X,y = prepro.get_Xy(market_df, news_df)\n    X,y = join_generator.with_look_back(X, y, look_back=ModelFactory.look_back, look_back_step=ModelFactory.look_back_step)\n    \n    # Prediction\n    y_pred = model.predict(X)*2-1\n    # Set time index from market df for predicted values\n    y_pred = pd.DataFrame(y_pred, index = market_df.iloc[ModelFactory.look_back-1:]['time'].dt.date)\n\n    # Plot\n    plt.plot( market_df['returnsOpenNextMktres10'], linestyle='none', marker='.', color='darkblue')\n    plt.plot(y_pred, linestyle='none', marker='.', color='darkorange')\n    plt.xticks(rotation=45)\n    plt.title(asset)\n    plt.legend([\"Ground truth\", \"predicted\"])\n    plt.show()\n    \npredict_random_asset()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46dd65f9bf556765fc0447e7f57abbbc629d1deb"
      },
      "cell_type": "code",
      "source": "# def get_score():\n#     \"\"\"\n#     Calculation of actual metric that is used to calculate final score\n#     @param r: returnsOpenNextMktres10\n#     @param u: universe\n#     where rti is the 10-day market-adjusted leading return for day t for instrument i, and uti is a 0/1 universe variable (see the data description for details) that controls whether a particular asset is included in scoring on a particular day.    \n#     \"\"\"\n#     # Get test sample to calculate score on\n#     idx = market_test_idx.sample(1000)\n#     market_df = market.loc[idx.index]\n#     news_df = news.merge(idx, on=['time'])\n    \n#     # Prepare X, y\n#     X_test, y_test = prepro.get_Xy(market_df, news_df)\n#     X_test, y_test = join_generator.with_look_back(X_test, y_test, ModelFactory.look_back, ModelFactory.look_back_step)\n#     y_test = pd.DataFrame(y_test)\n\n#     # Accuracy metric\n#     confidence = model.predict(X_test)*2-1   \n#     look_back=ModelFactory.look_back\n#     r=market_df['returnsOpenNextMktres10']#.values[look_back:]\n#     u=market_df['universe']#.values[look_back:]\n# #     print(df.size)\n#     print(len(confidence))\n# #     print(r.size())\n# #     print(u.size())\n#     #print('df: %s, confidence: %s, r: %s, u: %s ' % (df.count(), confidence.count(), r.count(), u.count()))\n#     # calculation of actual metric that is used to calculate final score\n#     r = r.clip(-1,1) # get rid of outliers. Where do they come from??\n#     x_t_i = confidence.values * r * u\n#     print(x_t_i.iloc[0])\n#     d = (market_df['time'].dt.day).values[look_back:]\n# #     print('d.count() = %s, x_t_i count = %s' % (d.count(), x_t_i.count()))\n#     data = {'day' : d, 'x_t_i' : x_t_i}\n#     df = pd.DataFrame(data)\n#     x_t = df.groupby('day').sum().values.flatten()\n#     mean = np.mean(x_t)\n#     std = np.std(x_t)\n#     score = mean / std\n#     print(score)\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46dd65f9bf556765fc0447e7f57abbbc629d1deb",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "def calc_acc():\n    # Get X_test, y_test with look back for LSTM\n    market_df = market.loc[market_test_idx.index].sample(1000)\n    news_df = news.merge(market_test_idx, on=['time'])\n    X_test, y_test = prepro.get_Xy(market_df, news_df)\n    X_test, y_test = join_generator.with_look_back(X_test, y_test, look_back=ModelFactory.look_back, look_back_step=ModelFactory.look_back_step)\n    y_test = pd.DataFrame(y_test)\n    \n    # True labels\n    labels = market_df.returnsOpenNextMktres10.iloc[ModelFactory.look_back-1:]\n    \n    # Accuracy metric\n    y_pred = pd.DataFrame(model.predict(X_test))*2-1\n\n    #y_pred = pd.DataFrame(market_prepro.y_scaler.inverse_transform(model.predict(X_test)))\n    print(\"Accuracy: %f\" % accuracy_score(labels >= 0, y_pred >= 0))\n    #score = get_score(market_df, confidence, market_df.returnsOpenNextMktres10, market_df.universe)\n    print('Predictions size: ', len(y_pred.values))\n    print('y_test size:', len(y_test.values))\n     # Show distribution of confidence that will be used as submission\n    plt.hist(labels.values, bins='auto', alpha=0.3)\n    plt.hist(y_pred.values, bins='auto', alpha=0.3, color='darkorange')\n    plt.legend(['Ground truth', 'Predicted'])\n    plt.xlabel(\"Confidence\")\n    plt.ylabel(\"Count\")\n    plt.title(\"predicted confidence\")\n    plt.show()\n\n# Call accuracy calculation and plot    \ncalc_acc()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a1089c3423417fe6d568afa4a9ca6d57dd5f449d"
      },
      "cell_type": "markdown",
      "source": "# Submission"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43a3295c06c520b525542183b1049d0a593e0d26"
      },
      "cell_type": "code",
      "source": "def make_predictions(market_obs_df, news_obs_df, predictions_template_df):\n    \"\"\"\n    Predict confidence for one day and update predictions_template_df['confidenceValue']\n    @param market_obs_df: market_obs_df returned from env\n    @param predictions_template_df: predictions_template_df returned from env.\n    @return: None. prediction_template_df updated instead. \n    \"\"\"\n    # Preprocess the data\n    X = prepro.get_X(market_obs_df, news_obs_df)\n    # Add look back window for LSTM, passing X only - we don't know y, we are predicting them\n    X = join_generator.with_look_back(X, None, look_back=ModelFactory.look_back, look_back_step=ModelFactory.look_back_step)\n    # Predict\n    y_pred = model.predict(X)\n    confidence_df=pd.DataFrame(y_pred*2-1, columns=['confidence'])\n\n    # Merge predicted confidence to predictions template\n    pred_df = pd.concat([predictions_template_df, confidence_df], axis=1).fillna(0)\n    predictions_template_df.confidenceValue = pred_df.confidence",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79ed08e77aa14dfb7bd811d2187b0d2059a95052"
      },
      "cell_type": "code",
      "source": "##########################\n# Submission code\n\n# Save data here for later debugging on it\ndays_saved_data = []\n\n# Store execution info for plotting later\npredicted_days=[]\npredicted_times=[]\nlast_predictions_template_df = None\n\n# Predict day by day\ndays = env.get_prediction_days()\nlast_year=None\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    # Store the data for later debugging on it\n    days_saved_data.append((market_obs_df, news_obs_df, predictions_template_df))\n    # For later plotting\n    predicted_days.append(market_obs_df.iloc[0].time.strftime('%Y-%m-%d'))\n    time_start = time()\n    # For logging\n    cur_year = market_obs_df.iloc[0].time.strftime('%Y')\n    if cur_year != last_year:\n        print(f'Predicting {cur_year}...')\n        last_year = cur_year\n\n    # Call prediction func\n    make_predictions(market_obs_df, news_obs_df, predictions_template_df)\n    #!!!\n    env.predict(predictions_template_df)\n    \n    # For later plotting\n    last_predictions_template_df = predictions_template_df\n    predicted_times.append(time()-time_start)\n    #print(\"Prediction completed for \", predicted_days[-1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38ce296e50b8c15c4dde4c4626e489c474b39179"
      },
      "cell_type": "code",
      "source": "# Plot execution time \nsns.barplot(np.array(predicted_days), np.array(predicted_times))\nplt.title(\"Execution time per day\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Execution time, seconds\")\nplt.show()\n\n# Plot predicted confidence for last day\nlast_predictions_template_df.plot(linestyle='none', marker='.', color='darkorange')\nplt.title(\"Predicted confidence for last observed day: %s\" % predicted_days[-1])\nplt.xlabel(\"Observation No.\")\nplt.ylabel(\"Confidence\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bcbe9c35e83e45f890d2450942fca4a5819b40d5"
      },
      "cell_type": "code",
      "source": "# We've got a submission file!\n# !!! Write submission after all days are predicted\nenv.write_submission_file()\nprint([filename for filename in os.listdir('.') if '.csv' in filename])",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}