{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Credit card fraud detection\nThis notebook will test different methods on skewed data. The idea is to compare if preprocessing techniques work better when there is an overwhelming majority class that can disrupt the efficiency of our predictive model.\nYou will also be able to see how to apply cross validation for hyperparameter tuning on different classification models. My intention is to create models using:\n\n1.Logistic Regression\n\n2.SVMs\n\n3.Decision trees"},{"metadata":{"trusted":true,"_uuid":"8533682c3bf22632fc3729dee910136b0fcba357"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb3a5e57b3bfb1a89abe45fa86328b6815805f39"},"cell_type":"code","source":"data = pd.read_csv('../input/creditcard.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7f5ac003d471a4f5dac0266a90da29d1862ddd8"},"cell_type":"markdown","source":"From above data , we can say that there are 28 anonamised variables and 2 named variables - time & amount"},{"metadata":{"trusted":true,"_uuid":"b43421ff8cc183a9a0f90f9d7ade939f646a614b"},"cell_type":"code","source":"class_count = pd.value_counts(data['Class']).sort_index()\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.title('Fraud class histogram')\nclass_count.plot(kind=\"bar\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"161d11aa0024566cb0f8b8d8e8e1dffd3a9fe59d"},"cell_type":"markdown","source":"Clearly the data is totally unbalanced!!\nThis is a clear example where using a typical accuracy score to evaluate our classification algorithm\nThere are several ways to approach this classification problem taking into consideration this imbalance.\n\n* Collect more data? Nice strategy but not applicable in this case\n* Resampling the dataset\nEssentially this is a method that will process the data to have an approximate 50-50 ratio.\nOne way to achieve this is by OVER-sampling, which is adding copies of the under-represented class (better when you have little data)\nAnother is UNDER-sampling, which deletes instances from the over-represented class (better when he have lot's of data)\n "},{"metadata":{"_uuid":"4693b3150237ba982f51549e72447b733958d66d"},"cell_type":"markdown","source":"**Setting our input and target variables + resampling**\n1. Normalising the amount column. The amount column is not in line with the anonimised features"},{"metadata":{"trusted":true,"_uuid":"f6ad7258e0806a202856b50addf9bf2d3ece085a"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ndata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"645dc0d9d5bee5cfa4d03f56d95c454d1b050a6f"},"cell_type":"code","source":"data['normAmount'].head()\ndata = data.drop(['Time', 'Amount'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1d1e94d97b0f74f7d9465263a9d707cc627ca0f"},"cell_type":"code","source":"data.columns.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ff190dea5301a2dc582014c87e61e8ab0be8eac"},"cell_type":"markdown","source":"2. Resampling.\nAs we mentioned earlier, there are several ways to resample skewed data. We will be using traditional UNDER-sampling technique.\nThe way we will under sample the dataset will be by creating a 50/50 ratio. This will be done by randomly selecting \"x\" amount of sample from the majority class, being \"x\" the total number of records with the minority class."},{"metadata":{"trusted":true,"_uuid":"907c10528d3ce2a5e480ff4152c2d252cb3627f9"},"cell_type":"code","source":"#No of data points in minority class\nno_of_frauds = len(data[data['Class']==1])\nprint(no_of_frauds)\n# Picking the indices of the fraud classes\nfraud_indices = np.array(data[data['Class']==1].index)\n# Picking the indices of the normal classes\nnormal_indices = np.array(data[data['Class']==0].index)\n# Out of the indices we picked, randomly select \"x\" number (no_of_frauds)\nnormal_random_indices = np.random.choice(normal_indices, no_of_frauds, replace=False)\nnormal_random_indices = np.array(normal_random_indices)\n\n#Append 2 indices\nunder_sample_indices  = np.concatenate([fraud_indices, normal_random_indices])\nprint('Appended 2 indices')\n#Select rows with indices present in under_sample_indices\nunder_sample_data = data.iloc[under_sample_indices,:]\nprint('Select rows with indices present in under_sample_indices')\nunder_sample_data.head()\n#Percentage of normal transactions\nnormal_trans_per = len(under_sample_data[under_sample_data['Class']==0])/ len(under_sample_data)*100\nprint('Normal Transaction Percentage', normal_trans_per)\n\n#Percentage of fraud transactions\nfraud_trans_per =  len(under_sample_data[under_sample_data['Class']==0])/ len(under_sample_data)*100\nprint('Normal Transaction Percentage', fraud_trans_per)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b184bd9710ecc134e40de9bc42c62d52b7347155"},"cell_type":"code","source":"x_under_sample_data = under_sample_data\nx_under_sample_data = x_under_sample_data.drop('Class', axis=1)\ny_under_sample_data = under_sample_data['Class']\nprint(x_under_sample_data.sample())\ny_under_sample_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2efd51012ce3896c21494dc552e14aa020ecc21"},"cell_type":"code","source":"x = data\nx = x.drop('Class', axis=1)\ny=data['Class']\nprint(len(x))\nprint(len(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06b8f13ba8c5485c2eb9d2dc064c4c7de1b93289"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#whole dataset\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\nprint('Train data length',len(x_train))\nprint('Test data length', len(x_test))\nprint('Total Length', len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"440e19cb66e6d3c2c2c1bd8c5508f1f66546910e"},"cell_type":"code","source":"#Undersampled dataset\n\nunder_sampled_x_train, under_sampled_x_test, under_sampled_y_train, under_sampled_y_test = train_test_split(x_under_sample_data, y_under_sample_data, test_size= 0.3, random_state = 0)\nprint('Under_sampled Train data length', len(under_sampled_x_train))\nprint('Under_sampled Test data length', len(under_sampled_x_test))\nprint('Under_sampled Train data length', len(under_sampled_y_train))\nprint('Under_sampled Test data length', len(under_sampled_y_test))\nprint('Total Length', len(x_under_sample_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6910db2552b3d644843b511267503d969002d452"},"cell_type":"markdown","source":"**Logistic regression classifier - Undersampled data**\nWe are very interested in the recall score, because that is the metric that will help us try to capture the most fraudulent transactions. If you think how Accuracy, Precision and Recall work for a confusion matrix, recall would be the most interesting:\n\nAccuracy = (TP+TN)/total\nPrecision = TP/(TP+FP)\nRecall = TP/(TP+FN)\n\nAs we know, due to the imbalacing of the data, many observations could be predicted as False Negatives, being, that we predict a normal transaction, but it is in fact a fraudulent one. Recall captures this."},{"metadata":{"trusted":true,"_uuid":"2f0140532a249479c78539cfd78147fbabe0b715"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, auc,roc_auc_score, roc_curve, recall_score,classification_report \n\nc_param_range = [0.01, 0.1, 1, 10, 100]\n\nfor i in c_param_range:\n    lr = LogisticRegression(C=i, penalty ='l1')\n    lr.fit(x_train, y_train)\n    pred = lr.predict(x_test)\n    recall_acc = recall_score(y_test, pred)\n    print('Recall Score of ', recall_acc, ' for C = ', i)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52c9bc5847978702a3e9d7dfbdabebe3986f088a"},"cell_type":"code","source":"print(under_sampled_x_train.shape)\nprint(under_sampled_y_train.shape)\nprint(under_sampled_x_test.shape)\nprint(under_sampled_y_test.shape)\n\nprint(type(under_sampled_x_train))\nprint(type(under_sampled_y_train))\nprint(type(under_sampled_x_test))\nprint(type(under_sampled_y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da41b7eafcb53049af1ee4bb9ac7c00be6ce52a5"},"cell_type":"code","source":"under_sampled_y_train = under_sampled_y_train.values.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e102454eb226cea147a51441de98fe7cc158d217"},"cell_type":"code","source":"under_sampled_y_test = under_sampled_y_test.values.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12d920d551231c656bd055ba9ebf1feedcf174a5"},"cell_type":"code","source":"\nc_param_range = [0.01, 0.1, 1, 10, 100]\n\nfor i in c_param_range:\n    lr = LogisticRegression(C=i, penalty ='l1')\n    lr.fit(under_sampled_x_train, under_sampled_y_train)\n    under_sampled_pred = lr.predict(under_sampled_x_test)\n    under_sampled_recall_acc = recall_score(under_sampled_y_test, under_sampled_pred)\n    print('Shape' ,under_sampled_recall_acc.shape)\n    print('Recall Score of ', under_sampled_recall_acc, ' for C = ', i)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}