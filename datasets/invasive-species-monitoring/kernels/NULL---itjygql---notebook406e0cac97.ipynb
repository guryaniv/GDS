{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "01cd3aa6-9225-9f23-bd87-f8fdc09cfd55"
      },
      "source": [
        "cnn code with tensorflow , 96.1% accuracy , python2.7, tensorflow1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1e4306f5-d7c8-b4ca-a3b8-167f8fa9f7b5"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from skimage import io, transform\n",
        "import os, gc, sys, glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "def read_img(img_path):\n",
        "    img = io.imread(img_path)\n",
        "    img = transform.resize(img, (128, 128))\n",
        "    return img\n",
        "train_set = pd.read_csv('images/train_labels.csv/train_labels.csv')\n",
        "train_img, test_img = [], []\n",
        "for img_path in tqdm(train_set['name'].iloc[: ]):\n",
        "    train_img.append(read_img('images/train/' + str(img_path) + '.jpg'))\n",
        "train_img = np.array(train_img, np.float32)\n",
        "train_label = np.array(train_set['invasive'].iloc[: ])\n",
        "train_label = train_label.reshape([-1,1])\n",
        "val_split_num = int(round(0.2*len(train_label)))\n",
        "x_train = train_img[val_split_num:]\n",
        "y_train = train_label[val_split_num:]\n",
        "x_test = train_img[:val_split_num]\n",
        "y_test = train_label[:val_split_num]\n",
        "evaluation_data = x_test, y_test\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "            # featurewise_center = True,\n",
        "            #rotation_range = 30,\n",
        "            width_shift_range = 0.2,\n",
        "            height_shift_range = 0.2,\n",
        "            # zca_whitening = True,\n",
        "            shear_range = 0.2,\n",
        "            zoom_range = 0.2,\n",
        "            horizontal_flip = True,\n",
        "            vertical_flip = True,\n",
        "            fill_mode = 'nearest')\n",
        "datagen.fit(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "881a138d-e3a5-9dc6-6185-2a5fb5c04419"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "class network(object):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.learning_rate = 0.001\n",
        "        self.batch_size = 64\n",
        "        self.nclasses = 1\n",
        "        self.n_input = 128*128\n",
        "        self.displaytime = 10\n",
        "        self.epoches = 1000\n",
        "        self.regularation_param = 1e-6\n",
        "        self.weights = {\n",
        "            'wc1': tf.Variable(initial_value=tf.random_uniform(shape=[3, 3, 3, 16], minval=-0.05, maxval=0.05)),\n",
        "            'wc2': tf.Variable(initial_value=tf.random_uniform(shape=[3, 3, 16, 32], minval=-0.05, maxval=0.05)),\n",
        "            'wc3': tf.Variable(initial_value=tf.random_uniform(shape=[3, 3, 32, 64], minval=-0.05, maxval=0.05)),\n",
        "            'wc4': tf.Variable(initial_value=tf.random_uniform(shape=[3, 3, 64, 128], minval=-0.05, maxval=0.05)),\n",
        "            'wd1': tf.Variable(initial_value=tf.random_uniform(shape=[12*12*128, 1024], minval=-0.05, maxval=0.05)),\n",
        "            'wd2': tf.Variable(initial_value=tf.random_uniform(shape=[1024, 512], minval=-0.05, maxval=0.05)),\n",
        "            # OOM\n",
        "            'out': tf.Variable(initial_value=tf.random_normal(shape=[512, self.nclasses]))\n",
        "\n",
        "        }\n",
        "        self.biases = {\n",
        "            'b1': tf.Variable(initial_value=tf.zeros([16])),\n",
        "            'b2': tf.Variable(initial_value=tf.zeros([32])),\n",
        "            'b3': tf.Variable(initial_value=tf.zeros([64])),\n",
        "            'b4': tf.Variable(initial_value=tf.zeros([128])),\n",
        "            'bfc1': tf.Variable(initial_value=tf.zeros([1024])),\n",
        "            'bfc2': tf.Variable(initial_value=tf.zeros([512])),\n",
        "            'out': tf.Variable(initial_value=tf.zeros([self.nclasses]))\n",
        "        }\n",
        "        self.x = tf.placeholder(tf.float32, [None, 128, 128, 3])\n",
        "        self.y = tf.placeholder(tf.float32, [None, self.nclasses])\n",
        "        self.keep_prob_1 = tf.placeholder(tf.float32)  # dropout (keep probability)\n",
        "        self.keep_prob_2 = tf.placeholder(tf.float32)  # dropout (keep probability)\n",
        "\n",
        "\n",
        "    def conv2d(self, x, W, b, strides=1):\n",
        "        output = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='VALID')\n",
        "        output = tf.nn.bias_add(output, b)\n",
        "        return tf.nn.relu(output)\n",
        "\n",
        "    def maxpool2d(self, x, k=2, strides=2):\n",
        "        output = tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, strides, strides, 1], padding='VALID')\n",
        "        return output\n",
        "\n",
        "    def convNet(self, x):\n",
        "        ## reshape x for conv layer\n",
        "        #x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "        conv1 = self.conv2d(x, self.weights['wc1'], self.biases['b1'])\n",
        "        pool1 = self.maxpool2d(conv1)\n",
        "        conv2 = self.conv2d(pool1, self.weights['wc2'], self.biases['b2'])\n",
        "        pool2 = self.maxpool2d(conv2)\n",
        "        conv3 = self.conv2d(pool2, self.weights['wc3'], self.biases['b3'])\n",
        "        pool3 = self.maxpool2d(conv3)\n",
        "        conv4 = self.conv2d(pool3, self.weights['wc4'], self.biases['b4'])\n",
        "        ## reshape output for fully-connected layer\n",
        "        fc1 = tf.reshape(conv4, [-1, self.weights['wd1'].get_shape().as_list()[0]])\n",
        "        fc1 = tf.add(tf.matmul(fc1, self.weights['wd1']), self.biases['bfc1'])\n",
        "        fc1 =  tf.nn.relu(fc1)\n",
        "        fc1 = tf.nn.dropout(fc1, self.keep_prob_1)\n",
        "\n",
        "        fc2 = tf.add(tf.matmul(fc1, self.weights['wd2']), self.biases['bfc2'])\n",
        "        fc2 =  tf.nn.relu(fc2)\n",
        "        fc2 = tf.nn.dropout(fc2, self.keep_prob_2)\n",
        "        \n",
        "        out = tf.add(tf.matmul(fc2, self.weights['out']), self.biases['out'])\n",
        "        #out = tf.reshape([-1,])\n",
        "        return out\n",
        "\n",
        "\n",
        "    def cost(self, logits, labels):\n",
        "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)) \\\n",
        "     + self.regularation_param * (tf.nn.l2_loss(self.weights['wc1']) + tf.nn.l2_loss(self.weights['wc2']) + \\\n",
        "                                 tf.nn.l2_loss(self.weights['wc3']) + tf.nn.l2_loss(self.weights['wc4']) + \\\n",
        "                                 tf.nn.l2_loss(self.weights['wd1']) + tf.nn.l2_loss(self.weights['out']))\n",
        "\n",
        "    def evaluation(self, pred, labels):\n",
        "        \n",
        "        correct_pred = tf.equal(tf.round(tf.reduce_max(tf.sigmoid(pred), 1)), tf.reduce_max(labels, 1))\n",
        "        return tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "        \n",
        "    def train(self, datagen, x_train, y_train, evaluation_data):\n",
        "        pred = self.convNet(self.x)\n",
        "        cost = self.cost(pred, labels=self.y)\n",
        "        acc = self.evaluation(pred, self.y)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "        optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.8,use_nesterov=True).minimize(cost)\n",
        "        init = tf.global_variables_initializer()\n",
        "        merged = tf.summary.merge_all()\n",
        "        with tf.Session() as sess:\n",
        "            summary_writer = tf.summary.FileWriter('/home/gql/Documents/logs/', sess.graph)\n",
        "            sess.run(init)\n",
        "            for epoch in range(self.epoches):\n",
        "                print \"epoch:\", epoch\n",
        "                step = 0\n",
        "                n = len(x_train)\n",
        "                for batch_x, batch_y in datagen.flow(x_train, y_train, self.batch_size):\n",
        "                    \n",
        "                    # update mini_bacth\n",
        "                    ## bp for mini_batch and update w,b\n",
        "                    summary, _= sess.run([merged ,optimizer], feed_dict={self.x: batch_x, self.y: batch_y, self.keep_prob_1: 0.65, self.keep_prob_2: 0.55})\n",
        "                    step += 1\n",
        "                    summary_writer.add_summary(summary, step)\n",
        "                    if step % self.displaytime == 0:\n",
        "                        loss, accuracy = sess.run([cost, acc], feed_dict={self.x: batch_x, self.y: batch_y, self.keep_prob_1: 1., self.keep_prob_2: 1.})\n",
        "                        print \"Iter \" + str(step * self.batch_size) + \", Minibatch Loss= \" + \\\n",
        "                            \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                            \"{:.5f}\".format(accuracy) + \",Testing Accuracy= \",\\\n",
        "                            sess.run(acc, feed_dict={self.x: evaluation_data[0][:256],\n",
        "                                              self.y: evaluation_data[1][:256],\n",
        "                                              self.keep_prob_1: 1., self.keep_prob_2: 1.})\n",
        "                    if step >= n / self.batch_size:\n",
        "                        break\n",
        "               \n",
        "            print 'optimization finished!'\n",
        "            print \"Testing Accuracy:\", \\\n",
        "                sess.run(acc, feed_dict={self.x: evaluation_data[0],\n",
        "                                              self.y: evaluation_data[1],\n",
        "                                              self.keep_prob_1: 1., self.keep_prob_2: 1.})\n",
        "            print x_train.shape\n",
        "            print \"training Accuracy:\", \\\n",
        "                sess.run(acc, feed_dict={self.x: x_train[:512],\n",
        "                                              self.y: y_train[:512],\n",
        "                                              self.keep_prob_1: 1., self.keep_prob_2: 1.})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da585cbb-a227-ff2f-f0e1-804409067bc4"
      },
      "outputs": [],
      "source": [
        "net = network()\n",
        "net.train(datagen, x_train, y_train, evaluation_data)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}