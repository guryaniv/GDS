{"cells":[{"metadata":{"_uuid":"0b917ce195a96b76423e768c77a52d583b2c466d"},"cell_type":"markdown","source":"Some features taken from: https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro by Misha Lisovyi\n\nSecond LGB model taken from: https://www.kaggle.com/opanichev/lgb-as-always by Oleg Panichev"},{"metadata":{"_uuid":"091212d4ad7253c4d09cb47c50dfc14ece335312","trusted":false,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport datetime\n\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, RepeatedKFold, GroupKFold\n\nimport lightgbm as lgb\nimport gc\n\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import ADASYN\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55ff131fc91be63e1392b646804ee1dba058a32f","trusted":false,"collapsed":true},"cell_type":"code","source":"def feature_importance(forest, X_train):\n    ranked_list = []\n    \n    importances = forest.feature_importances_\n\n    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n    indices = np.argsort(importances)[::-1]\n\n    # Print the feature ranking\n    print(\"Feature ranking:\")\n\n    for f in range(X_train.shape[1]):\n        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n        ranked_list.append(X_train.columns[indices[f]])\n    \n    return ranked_list\n\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f61eb7f147f66be193a72e66780c0a3c0116a4f0","trusted":false,"collapsed":true},"cell_type":"code","source":"data_path = \"../input\"\ntrain = pd.read_csv(os.path.join(data_path, \"train.csv\"))\ntest = pd.read_csv(os.path.join(data_path, \"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52a9f3e0a2c33b84d24eb1c0ee93ccd57b007eb7"},"cell_type":"markdown","source":"## Clean Up Data"},{"metadata":{"_uuid":"a17cd0bee94b1379db78ff124487756c5a664768","trusted":false,"collapsed":true},"cell_type":"code","source":"# some dependencies are Na, fill those with the square root of the square\ntrain['dependency'] = np.sqrt(train['SQBdependency'])\ntest['dependency'] = np.sqrt(test['SQBdependency'])\n\n# change education to a number instead of a string, combine the two fields and drop the originals\ntrain.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\ntrain.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 1\ntrain['edjefa'] = train['edjefa'].astype(\"int\")\n\ntrain.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\ntrain.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 1\ntrain['edjefe'] = train['edjefe'].astype(\"int\")\n\ntest.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\ntest.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\ntest['edjefa'] = test['edjefa'].astype(\"int\")\n\ntest.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\ntest.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\ntest['edjefe'] = test['edjefe'].astype(\"int\")\n\ntrain['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\ntest['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n\n# let's keep these features for now\n# train.drop([\"edjefe\", \"edjefa\"], axis=1, inplace=True)\n# test.drop([\"edjefe\", \"edjefa\"], axis=1, inplace=True)\n\n# fill some nas\ntrain['v2a1']=train['v2a1'].fillna(0)\ntest['v2a1']=test['v2a1'].fillna(0)\n\ntest['v18q1']=test['v18q1'].fillna(0)\ntrain['v18q1']=train['v18q1'].fillna(0)\n\ntrain['rez_esc']=train['rez_esc'].fillna(0)\ntest['rez_esc']=test['rez_esc'].fillna(0)\n\ntrain.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\ntrain.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\ntest.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\ntest.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\n# some rows indicate both that the household does and does not have a toilet, if there is no water we'll assume they do not\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e3d2739b13f7a29c9b4f18617eb7e9f4393fdef","trusted":false,"collapsed":true},"cell_type":"code","source":"# this came from another kernel, some households have different targets for the same household,\n# we should make the target for each household be the target for the head of that household\nd={}\nweird=[]\nfor row in train.iterrows():\n    idhogar=row[1]['idhogar']\n    target=row[1]['Target']\n    if idhogar in d:\n        if d[idhogar]!=target:\n            weird.append(idhogar)\n    else:\n        d[idhogar]=target\n        \nfor i in set(weird):\n    hhold=train[train['idhogar']==i][['idhogar', 'parentesco1', 'Target']]\n    target=hhold[hhold['parentesco1']==1]['Target'].tolist()[0]\n    for row in hhold.iterrows():\n        idx=row[0]\n        if row[1]['parentesco1']!=1:\n            train.at[idx, 'Target']=target        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d415735372415899860e022b8da2838570c0b2c"},"cell_type":"markdown","source":"## Some EDA"},{"metadata":{"_uuid":"c5089b3071aa8da8aa775b83749f03ccbf744318","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Unique Households:\", train.idhogar.nunique())\nprint(\"Total Rows:\", len(train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ae2c197feeb86ee05ba69f882298323b0eaf84a","trusted":false,"collapsed":true},"cell_type":"code","source":"train[['Id','idhogar', 'v2a1', 'hacdor', 'rooms', 'hacapo', 'v14a', 'refrig',\n       'v18q', 'v18q1', 'r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3',\n       'r4t1', 'r4t2', 'r4t3', 'tamhog', 'tamviv', 'escolari', 'rez_esc',\n       'hhsize', 'paredblolad', 'paredzocalo', 'paredpreb', 'pareddes',\n       'paredmad', 'paredzinc', 'paredfibras', 'paredother', 'pisomoscer',\n       'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene',\n       'pisomadera', 'techozinc', 'techoentrepiso', 'techocane',\n       'techootro', 'cielorazo', 'abastaguadentro', 'abastaguafuera',\n       'abastaguano', 'public', 'planpri', 'noelec', 'coopele',\n       'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5',\n       'sanitario6', 'energcocinar1', 'energcocinar2', 'energcocinar3',\n       'energcocinar4', 'elimbasu1', 'elimbasu2', 'elimbasu3',\n       'elimbasu4', 'elimbasu5', 'elimbasu6', 'epared1', 'epared2',\n       'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2',\n       'eviv3', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2',\n       'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6',\n       'estadocivil7', 'parentesco1', 'parentesco2', 'parentesco3',\n       'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7',\n       'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11',\n       'parentesco12', 'hogar_nin', 'hogar_adul',\n       'hogar_mayor', 'hogar_total', 'dependency', \n       'meaneduc', 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4',\n       'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8',\n       'instlevel9', 'bedrooms', 'overcrowding', 'tipovivi1', 'tipovivi2',\n       'tipovivi3', 'tipovivi4', 'tipovivi5', 'computer', 'television',\n       'mobilephone', 'qmobilephone', 'lugar1', 'lugar2', 'lugar3',\n       'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'age',\n       'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe',\n       'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned',\n       'agesq', 'Target']].sort_values(\"idhogar\").head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a803c4d26b24d9d40bb87386f3d7695041923e0"},"cell_type":"markdown","source":"The only features which vary within a household are related to the individuals, we will consolidate these and add some household level features."},{"metadata":{"_uuid":"03d5f8a64b427a45e54682898df174479a13afc1"},"cell_type":"markdown","source":"### Add Some Features"},{"metadata":{"_uuid":"117aa610423d88ac4ee85da3d78a44da2aec75f6","trusted":false,"collapsed":true},"cell_type":"code","source":"# min/max education by household\ntrain['max_esc'] = train.groupby('idhogar')['escolari'].transform('max')\ntrain['min_esc'] = train.groupby('idhogar')['escolari'].transform('min')\n\ntest['max_esc'] = test.groupby('idhogar')['escolari'].transform('max')\ntest['min_esc'] = test.groupby('idhogar')['escolari'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec1b8295dad1ec739c77cb01aa57e8deb194d50b","trusted":false,"collapsed":true},"cell_type":"code","source":"# min/max/mean behind in education by household\ntrain['max_rez'] = train.groupby('idhogar')['rez_esc'].transform('max')\ntrain['min_rez'] = train.groupby('idhogar')['rez_esc'].transform('min')\ntrain['mean_rez'] = train.groupby('idhogar')['rez_esc'].transform('mean')\n\ntest['max_rez'] = test.groupby('idhogar')['rez_esc'].transform('max')\ntest['min_rez'] = test.groupby('idhogar')['rez_esc'].transform('min')\ntest['mean_rez'] = test.groupby('idhogar')['rez_esc'].transform('mean')\n\n# these features are added by a function, so there is no need to add them twice\n# train['max_age'] = train.groupby('idhogar')['age'].transform('max')\n# train['min_age'] = train.groupby('idhogar')['age'].transform('min')\n\n# test['max_age'] = test.groupby('idhogar')['age'].transform('max')\n# test['min_age'] = test.groupby('idhogar')['age'].transform('min')\n\n# we'll init the feature to 0, then count the people over 18 in the household and use the max of that\ntrain['num_over_18'] = 0\ntrain['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\ntrain['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntrain['num_over_18'] = train['num_over_18'].fillna(0)\n\ntest['num_over_18'] = 0\ntest['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\ntest['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntest['num_over_18'] = test['num_over_18'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90fdb0a92e74a2bb8fba5cbafad8d81485d05dab","trusted":false,"collapsed":true},"cell_type":"code","source":"def extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n    # some households have no one over 18, use the total rent for those\n    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n    df['dependency_yes'] = df['dependency'].apply(lambda x: 1 if x == 'yes' else 0)\n    df['dependency_no'] = df['dependency'].apply(lambda x: 1 if x == 'no' else 0)\n    \ndef do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n\n    return df    \n\ndef convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df\n\ndef process_df(df_):\n    # fix categorical features\n#     encode_data(df_)\n    #fill in missing values based on https://www.kaggle.com/mlisovyi/missing-values-in-the-data\n    for f_ in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n        df_[f_] = df_[f_].fillna(0)\n    df_['rez_esc'] = df_['rez_esc'].fillna(-1)\n    # do feature engineering and drop useless columns\n    return do_features(df_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43a2a412d068852fc1d8425213b26258aa5a481b","trusted":false,"collapsed":true},"cell_type":"code","source":"extract_features(train)\nextract_features(test)\n\ntrain = process_df(train)\ntest = process_df(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0ea3cb0c2452a39c2f67940dc4d32f66392a8808"},"cell_type":"code","source":"cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n              'pared_LE']\ncols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\ndef convert_geo2aggs(df_):\n    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n                        pd.get_dummies(df_[cols_2_ohe], \n                                       columns=cols_2_ohe)],axis=1)\n    #print(pd.get_dummies(train[cols_2_ohe], \n    #                                   columns=cols_2_ohe).head())\n    #print(tmp_df.head())\n    #print(tmp_df.groupby(['lugar_LE','idhogar']).mean().head())\n    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n    \n    #print(gb.T)\n    del tmp_df\n    return df_.join(geo_agg, how='left', on='lugar_LE')\n\n#train, test = train_test_apply_func(train, test, convert_geo2aggs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ab3efe5a98cc90dd46ca46f879e409003e7ddc3e"},"cell_type":"code","source":"def train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9b52e7dad45ae606a453b54c67e448b7b690d756"},"cell_type":"code","source":"train, test = train_test_apply_func(train, test, convert_OHE2LE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"abf87fb47aebbd14177d6887cd2985efd9a0b025"},"cell_type":"code","source":"print(\"train:\", train.shape)\nprint(\"test:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4610e7f02bbf346a72f289813da71728250b28bf"},"cell_type":"markdown","source":"### Split Data\n\nWe'll split the data by household to avoid leakage."},{"metadata":{"_uuid":"e31ca28aa2fd6574814552c61589a3528b5239e9","trusted":false,"collapsed":true},"cell_type":"code","source":"# only train on head of household records since apparently only they are scored?\n# train2 = train.query('parentesco1==1')\n\ntrain2 = train.copy()\ntarget = train2.Target\ntrain2 = train2.drop(\"Target\", axis=1)\n\ntrain_hhs = train2.idhogar\n\nhouseholds = train2.idhogar.unique()\ncv_hhs = np.random.choice(households, size=int(len(households) * 0.25), replace=False)\n\ncv_idx = np.isin(train2.idhogar, cv_hhs)\n\n# train2 = train2.dropna(axis=1, how='any')\n# test = test.dropna(axis=1, how='any')\n\nX_cv = train2[cv_idx]\ny_cv = target[cv_idx]\n\nX_tr = train2[~cv_idx]\ny_tr = target[~cv_idx]\n\n# train on entire dataset\nX_tr = train2\ny_tr = target","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7676ecedb0f21b467c5c20138f45d273988da2b0"},"cell_type":"code","source":"drop_cols = train.columns[train.isnull().any()]\n\nX_tr.drop(drop_cols, axis=1, inplace=True)\nX_cv.drop(drop_cols, axis=1, inplace=True)\ntest.drop(drop_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a3d24d47650553a3e857081b7e68320cdec26f7","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"X_tr:\", X_tr.shape)\nprint(\"X_cv:\", X_cv.shape)\nprint(\"test:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0d9701419cd0f4b6fc263c77faf476edc2a5394"},"cell_type":"markdown","source":"### First Model with ExtraTrees"},{"metadata":{"_uuid":"ccf5d144373e663424b7db5c4c6aeffab4d9bb49","trusted":false,"collapsed":true},"cell_type":"code","source":"extra_drop_cols = []","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26f723ed77374617c4b34d22db16b945e9d6992d","trusted":false,"collapsed":true},"cell_type":"code","source":"# we got this list of columns by training on all columns and looking at the importances\nextra_drop_cols = ['fe_people_weird_stat',\n 'min_rez',\n 'dependency_no',\n 'rez_esc',\n 'parentesco1',\n 'parentesco_LE',\n 'dependency_yes']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc82e75a00925e6e0d823caacf0457ef955ecda6","trusted":false,"collapsed":true},"cell_type":"code","source":"et_drop_cols = ['Id', 'idhogar'] + extra_drop_cols\n\net = ExtraTreesClassifier(n_estimators=250, max_depth=24, min_impurity_decrease=1e-10, min_samples_leaf=1, min_samples_split=2, \n            min_weight_fraction_leaf=0.0, n_jobs=-1, random_state=1, verbose=1)\net.fit(X_tr.drop(et_drop_cols, axis=1), y_tr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"134b495975b505e87ced8da02ae9f0a66fd6b102","trusted":false,"collapsed":true},"cell_type":"code","source":"et_cv_preds = et.predict(X_cv.drop(et_drop_cols, axis=1))\net_cv_probs = et.predict_proba(X_cv.drop(et_drop_cols, axis=1)) * 2\n\nprint(\"Accuracy:\", et.score(X_cv.drop(et_drop_cols, axis=1), y_cv))\nprint(\"F1:\", f1_score(y_cv, et_cv_preds, average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"319a1f719d8ad2cc88eebbb5e75b4a3c9ab4a003","trusted":false,"collapsed":true},"cell_type":"code","source":"# depth 24\nprint(\"Accuracy:\", et.score(X_cv.drop(et_drop_cols, axis=1), y_cv))\nprint(\"F1:\", f1_score(y_cv, et_cv_preds, average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"799a86d4754fad45f6df613b6c5de316bd79b695","trusted":false,"collapsed":true},"cell_type":"code","source":"et_preds = et.predict(test.drop(et_drop_cols, axis=1))\net_probs = et.predict_proba(test.drop(et_drop_cols, axis=1)) * 2\nmodel_count = 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8bcc266c05b1af365fd7e8b21e6eb6192f99fbf","trusted":false,"collapsed":true},"cell_type":"code","source":"cv_submission = pd.DataFrame()\ncv_submission['Id'] = X_cv.Id\ncv_submission['Target_et'] = et_cv_preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86f87f6d8458de41030328ca82ce31e1bb54e188","trusted":false,"collapsed":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = test.Id\nsubmission['Target_et'] = et_preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7406f0da4bd5ec553e01a099332406b7cd38523e","trusted":false,"collapsed":true},"cell_type":"code","source":"submission[[\"Id\", \"Target_et\"]].to_csv(\"20180725_etc_1.csv\", header=[\"Id\", \"Target\"], index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95cc224372ddc826f9ef354c6a00b630a46e2f2c","trusted":false,"collapsed":true},"cell_type":"code","source":"ranked_features = feature_importance(et, X_tr.drop(et_drop_cols, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bb6284bab1e5d3b9e9e71b7acd25f941f358f3a","trusted":false,"collapsed":true},"cell_type":"code","source":"extra_drop_cols = ranked_features[95:]\nextra_drop_cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75d6b8b07420a4a24fb4cd07526662794e247b52"},"cell_type":"markdown","source":"### LGB"},{"metadata":{"_uuid":"451a837790a576eda653ccf32145d0d0e345f4e8","trusted":false,"collapsed":true},"cell_type":"code","source":"extra_lgb_drop_cols = ['techo_LE',\n 'abastagua_LE',\n 'hogar_total',\n 'v18q',\n 'mobilephone',\n 'hacdor',\n 'tamhog',\n 'hacapo',\n 'parentesco1'\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e44605b2ca4eb9c310f6b03565ac8805a54f41fd","trusted":false,"collapsed":true},"cell_type":"code","source":"lgb_drop_cols = ['Id', 'idhogar'] + extra_lgb_drop_cols\n# use the full training set for our cv, and then train only on training set so we can validate\n# train_all = lgb.Dataset(train[feature_names],signal)\ntrain_data = lgb.Dataset(X_tr.drop(lgb_drop_cols, axis=1), y_tr-1)\n\nmax_depth = 15\n\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'max_depth': max_depth,\n    'num_leaves': 2**max_depth-1,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.85,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'num_threads': 4,\n    'lambda_l2': 0.85,\n    'min_gain_to_split': 0,\n    'num_class': len(np.unique(y_tr)),\n}\nmodel = lgb.train(params, train_data, num_boost_round=500)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dedcd00015f99e345407bcabd926e7fcee14745a","trusted":false,"collapsed":true},"cell_type":"code","source":"lgb_cv_probs = model.predict(X_cv.drop(lgb_drop_cols, axis=1))\nlgb_cv_preds = np.argmax(lgb_cv_probs, axis=1) + 1\n\nprint(\"Accuracy:\", accuracy_score(y_cv, lgb_cv_preds))\nprint(\"F1:\", f1_score(y_cv, lgb_cv_preds, average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5717d985b5d9d658ebfdb45277493308306ff91d","trusted":false,"collapsed":true},"cell_type":"code","source":"# lambda 0.85\n# lr 0.05\n# ff 0.75\n# bf 0.85\n# depth 15\nprint(\"Accuracy:\", accuracy_score(y_cv, lgb_cv_preds))\nprint(\"F1:\", f1_score(y_cv, lgb_cv_preds, average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea6235486f053d7e16d1f29e4eb3d1079444338b","trusted":false,"collapsed":true},"cell_type":"code","source":"lgb_probs = model.predict(test.drop(lgb_drop_cols, axis=1))\nlgb_preds = np.argmax(lgb_probs, axis=1) + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13c0430bc49928ece52a5f006d0a0a7f7850e61d","trusted":false,"collapsed":true},"cell_type":"code","source":"# give this model a lot of weight\ncv_probs = et_cv_probs + lgb_cv_probs + lgb_cv_probs + lgb_cv_probs\ntest_probs = et_probs + lgb_probs + lgb_probs + lgb_probs\nmodel_count += 3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75cfd2754340e6a7083e950016b57becdcbb29de","trusted":false,"collapsed":true},"cell_type":"code","source":"# since this is the best model we'll give it more weight in the vote by adding the predictions twice\nsubmission['Target_lgb'] = lgb_preds\nsubmission['Target_lgb4'] = lgb_preds\nsubmission[[\"Id\", \"Target_lgb\"]].to_csv(\"20180725_lgb_1.csv\", header=[\"Id\", \"Target\"], index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d53f4d2642ca7cf36fe30ce6c03335ed6357e77a","trusted":false,"collapsed":true},"cell_type":"code","source":"# since this is the best model we'll give it more weight in the vote by adding the predictions twice\ncv_submission['Target_lgb'] = lgb_cv_preds\ncv_submission['Target_lgb4'] = lgb_cv_preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0a1305ef4f74415cddba52b4875f22d7cc96770","trusted":false,"collapsed":true},"cell_type":"code","source":"importance = model.feature_importance()\nmodel_fnames = model.feature_name()\ntuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\ntuples = [x for x in tuples if x[1] > 0]\nranked_features = []\n\nprint('Important features:')\nfor i in range(len(model_fnames)):\n    if i < len(tuples):\n        print(i, tuples[i])\n        ranked_features.append(tuples[i][0])\n    else:\n        break\n\ndel importance, model_fnames, tuples","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"522e7dd56cd151deb1d7bbbd1ef1a2bbb865b8f0","trusted":false,"collapsed":true},"cell_type":"code","source":"extra_lgb_drop_cols = ranked_features[85:]\nextra_lgb_drop_cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4816e34c53fed1e800f58a135b35f5b8c37716a8"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e0350263d94dcca301c712f5609531b133ecb2d6"},"cell_type":"code","source":"extra_rf_drop_cols = []","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00f0e15131393e7e9d2b6ed046f5eb14a3b20c46","trusted":false,"collapsed":true},"cell_type":"code","source":"extra_rf_drop_cols = ['r4t3_to_tamhog', 'min_rez', 'dependency_no', 'dependency_yes']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44bdef32e776b20a63358a52e2a1bca72e9952dc","trusted":false,"collapsed":true},"cell_type":"code","source":"rf_drop_cols = ['Id', 'idhogar', 'parentesco1'] + extra_rf_drop_cols\n\nrf = RandomForestClassifier(n_estimators=350, max_depth=31, verbose=1, min_impurity_decrease=1e-6, n_jobs=-1)\nrf.fit(X_tr.drop(rf_drop_cols, axis=1), y_tr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb17ba50f11722d2e40b44bba45d120e26e9bb34","trusted":false,"collapsed":true},"cell_type":"code","source":"rf_cv_preds = rf.predict(X_cv.drop(rf_drop_cols, axis=1))\nrf_cv_probs = rf.predict_proba(X_cv.drop(rf_drop_cols, axis=1))\n\nprint(\"Accuracy:\", accuracy_score(y_cv, rf_cv_preds))\nprint(\"F1:\", f1_score(y_cv, rf_cv_preds, average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bfc8adaede6a8e4caef2810feee991c6dd2b029","trusted":false,"collapsed":true},"cell_type":"code","source":"# 95 cols\nprint(\"Accuracy:\", accuracy_score(y_cv, rf_cv_preds))\nprint(\"F1:\", f1_score(y_cv, rf_cv_preds, average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e77898bcaadc6f7760aa525dc34540814c305244","trusted":false,"collapsed":true},"cell_type":"code","source":"rf_preds = rf.predict(test.drop(rf_drop_cols, axis=1))\nrf_probs = rf.predict_proba(test.drop(rf_drop_cols, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13fa04cc5c083e08a4fae559fa8d08988b7851f9","trusted":false,"collapsed":true},"cell_type":"code","source":"# weight this model more\ncv_probs = cv_probs + rf_cv_probs\ntest_probs = test_probs + rf_probs\nmodel_count += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa8f2878f8da933c568ffcd56699f8bb8d66bd56","trusted":false,"collapsed":true},"cell_type":"code","source":"submission['Target_rf'] = rf_preds\nsubmission[[\"Id\", \"Target_rf\"]].to_csv(\"20180725_rf_1.csv\", header=[\"Id\", \"Target\"], index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11413a175ae50790c16dfafee73042b925d6e7d5","trusted":false,"collapsed":true},"cell_type":"code","source":"cv_submission['Target_rf'] = rf_cv_preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9e720079ee91c95f3d0bb27d5097cc5226bc5bf","trusted":false,"collapsed":true},"cell_type":"code","source":"rf_features = feature_importance(rf, X_tr.drop(rf_drop_cols, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82465905a46a3a5f2ddc50a6dfd3ea2e202bf923","trusted":false,"collapsed":true},"cell_type":"code","source":"extra_rf_drop_cols = rf_features[96:]\nextra_rf_drop_cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83b434e272842eaa6fac2cb8dbcb863b86beb5a6"},"cell_type":"markdown","source":"### LGB 2\n\nOriginally from https://www.kaggle.com/opanichev/lgb-as-always by Oleg Panichev"},{"metadata":{"_uuid":"6f5fcb892b272d86288758703c5bc562c5e4d2ea","trusted":false,"collapsed":true},"cell_type":"code","source":"def dprint(*args, **kwargs):\n    print(\"[{}] \".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")) + \\\n        \" \".join(map(str,args)), **kwargs)\n\nid_name = 'Id'\ntarget_name = 'Target'\n\ndf_all = pd.concat([train, test], axis=0)\ncols = [f_ for f_ in df_all.columns if df_all[f_].dtype == 'object' and f_ != id_name]\n\nfor c in cols:\n    le = LabelEncoder()\n    le.fit(df_all[c].astype(str))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))\n\ngc.collect()\nprint(\"Done.\")\n\n# Build the model\ncnt = 0\np_buf = []\nn_splits = 5\nn_repeats = 1\nkf = RepeatedKFold(\n    n_splits=n_splits, \n    n_repeats=n_repeats, \n    random_state=0)\nerr_buf = []   \n\ncols_to_drop = [\n    id_name, \n    target_name,\n    'idhogar',\n]\n\nX = train2.drop(cols_to_drop, axis=1, errors='ignore')\nfeature_names = list(X.columns)\n\nX = X.fillna(0)\nX = X.values\ny = target\n\nclasses = np.unique(y)\ndprint('Number of classes: {}'.format(len(classes)))\nc2i = {}\ni2c = {}\nfor i, c in enumerate(classes):\n    c2i[c] = i\n    i2c[i] = c\n\ny_le = np.array([c2i[c] for c in y])\n\nX_test = test.drop(cols_to_drop, axis=1, errors='ignore')\nX_test = X_test.fillna(0)\nX_test = X_test.values\nid_test = test[id_name].values\n\ndprint(X.shape, y.shape)\ndprint(X_test.shape)\n\nn_features = X.shape[1]\n\nmax_depth = 12\n\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'max_depth': max_depth,\n    'num_leaves': (2**max_depth)-1,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.75,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'num_threads': 4,\n    'lambda_l2': 0.85,\n    'min_gain_to_split': 0,\n    'num_class': len(np.unique(y)),\n}\n\nfor i in range(5):\n    cv_hhs = np.random.choice(households, size=int(len(households) * 0.25), replace=False)\n\n    valid_index = np.isin(train_hhs, cv_hhs)\n    train_index = ~np.isin(train_hhs, cv_hhs)\n    \n    print('Fold {}/{}*{}'.format(cnt + 1, n_splits, n_repeats))\n    params = lgb_params.copy() \n\n    sampler = ADASYN(random_state=0)\n    X_train, y_train = sampler.fit_sample(X[train_index], y_le[train_index])\n\n    lgb_train = lgb.Dataset(\n        X_train, \n        y_train, \n        feature_name=feature_names,\n        )\n    lgb_train.raw_data = None\n\n    lgb_valid = lgb.Dataset(\n        X[valid_index], \n        y_le[valid_index],\n        feature_name=feature_names,\n        )\n    lgb_valid.raw_data = None\n\n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=99999,\n        valid_sets=[lgb_train, lgb_valid],\n        early_stopping_rounds=200, \n        verbose_eval=100, \n    )\n\n    if cnt == 0:\n        importance = model.feature_importance()\n        model_fnames = model.feature_name()\n        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n        tuples = [x for x in tuples if x[1] > 0]\n        print('Important features:')\n        for i in range(10):\n            if i < len(tuples):\n                print(i, tuples[i])\n            else:\n                break\n\n        del importance, model_fnames, tuples\n\n    p = model.predict(X[valid_index], num_iteration=model.best_iteration)\n    \n    err = f1_score(y_le[valid_index], np.argmax(p, axis=1), average='micro')\n\n    dprint('{} F1: {}'.format(cnt + 1, err))\n\n    p = model.predict(X_test, num_iteration=model.best_iteration)\n    \n    if len(p_buf) == 0:\n        p_buf = np.array(p, dtype=np.float16)\n    else:\n        p_buf += np.array(p, dtype=np.float16)\n    err_buf.append(err)\n\n    cnt += 1\n\n    del model, lgb_train, lgb_valid, p\n    gc.collect()\n\n\nerr_mean = np.mean(err_buf)\nerr_std = np.std(err_buf)\nprint('F1 = {:.6f} +/- {:.6f}'.format(err_mean, err_std))\n\npreds_lgb2 = p_buf/cnt\n\n# Prepare probas\nsubm = pd.DataFrame()\nsubm[id_name] = id_test\nfor i in range(preds_lgb2.shape[1]):\n    subm[i2c[i]] = preds_lgb2[:, i]\n\n# Prepare submission\nsubmission[\"Target_lgb2\"] = [i2c[np.argmax(p)] for p in preds_lgb2]\nsubmission[[\"Id\", \"Target_lgb2\"]].to_csv(\"20180725_lgb2_preds.csv\", header=[\"Id\", \"Target\"], index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b75ac340e7306caee61af3aa4c57df879a3953f","trusted":false,"collapsed":true},"cell_type":"code","source":"test_probs = test_probs + preds_lgb2\nmodel_count += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c31f91b0a0e97d1cffc05d250743268ba812ca9"},"cell_type":"markdown","source":"### One More LGB\n\nTaken from https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro by Misha Lisovyi, with some slight modifications."},{"metadata":{"_uuid":"a0c02c1435dbb0495b6f9d0962b281d76465d00f","trusted":false,"collapsed":true},"cell_type":"code","source":"lgb_drop_cols = ['Id', 'idhogar']\nopt_parameters = {'colsample_bytree': 0.93, 'min_child_samples': 56, 'num_leaves': 19, 'subsample': 0.84}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f36a01c3476eca58c49a71b4c1b92e34e62b356c"},"cell_type":"code","source":"def evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) \n\nfit_params={\"early_stopping_rounds\":300, \n            \"eval_metric\" : evaluate_macroF1_lgb, \n            \"eval_set\" : [(X_tr.drop(lgb_drop_cols, axis=1),y_tr-1), (X_cv.drop(lgb_drop_cols, axis=1),y_cv-1)],\n            'eval_names': ['train', 'valid'],\n            'verbose': False,\n            'categorical_feature': 'auto'}\n\ndef learning_rate_power_0997(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return max(lr, min_learning_rate)\n\nfit_params['verbose'] = 200\nfit_params['callbacks'] = [lgb.reset_parameter(learning_rate=learning_rate_power_0997)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"41b93e7c18ee4bfcd889cd433e5afd09ffec8354"},"cell_type":"code","source":"from sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.base import clone\nfrom sklearn.ensemble import VotingClassifier\n\ndef _parallel_fit_estimator(estimator, X, y, sample_weight=None, **fit_params):\n    \"\"\"Private function used to fit an estimator within a job.\"\"\"\n    if sample_weight is not None:\n        estimator.fit(X, y, sample_weight=sample_weight, **fit_params)\n    else:\n        estimator.fit(X, y, **fit_params)\n    return estimator\n\nclass VotingClassifierLGBM(VotingClassifier):\n    '''\n    This implements the fit method of the VotingClassifier propagating fit_params\n    '''\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError('Multilabel and multi-output'\n                                      ' classification is not supported.')\n\n        if self.voting not in ('soft', 'hard'):\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n                             % self.voting)\n\n        if self.estimators is None or len(self.estimators) == 0:\n            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n                                 ' should be a list of (string, estimator)'\n                                 ' tuples')\n\n        if (self.weights is not None and\n                len(self.weights) != len(self.estimators)):\n            raise ValueError('Number of classifiers and weights must be equal'\n                             '; got %d weights, %d estimators'\n                             % (len(self.weights), len(self.estimators)))\n\n        if sample_weight is not None:\n            for name, step in self.estimators:\n                if not has_fit_parameter(step, 'sample_weight'):\n                    raise ValueError('Underlying estimator \\'%s\\' does not'\n                                     ' support sample weights.' % name)\n        names, clfs = zip(*self.estimators)\n        self._validate_names(names)\n\n        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n        if n_isnone == len(self.estimators):\n            raise ValueError('All estimators are None. At least one is '\n                             'required to be a classifier!')\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        self.estimators_ = []\n\n        transformed_y = self.le_.transform(y)\n\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n                                                 sample_weight=sample_weight, **fit_params)\n                for clf in clfs if clf is not None)\n\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"017637f02e748f84d4d32e9e20158145de2c18d8"},"cell_type":"code","source":"clfs = []\nfor i in range(10):\n    clf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=314+i, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced')\n    clf.set_params(**opt_parameters)\n    clfs.append(('lgbm{}'.format(i), clf))\n    \nvc = VotingClassifierLGBM(clfs, voting='soft')\ndel clfs\n#Train the final model with learning rate decay\n_ = vc.fit(X_tr.drop(lgb_drop_cols, axis=1), y_tr-1, **fit_params)\n\nclf_final = vc.estimators_[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8f8dd57ca7ef973ca693b8c665294e546f5d6adc"},"cell_type":"code","source":"global_score = f1_score(y_cv-1, clf_final.predict(X_cv.drop(lgb_drop_cols, axis=1)), average='macro')\nvc.voting = 'soft'\nglobal_score_soft = f1_score(y_cv-1, vc.predict(X_cv.drop(lgb_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\nprint('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"17b5157a7575fbefa63fe4d8420c7066cba8ba0b"},"cell_type":"code","source":"lgb_cv_probs2 = clf_final.predict_proba(X_cv.drop(lgb_drop_cols, axis=1))\n\nlgb_probs2 = clf_final.predict_proba(test.drop(lgb_drop_cols, axis=1))\nlgb_preds2 = np.argmax(lgb_probs2, axis=1) + 1\nsubmission['Target_lgb3'] = lgb_preds2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2332b7deb9d5f206a61eeda4d242f14ba3435e4e"},"cell_type":"code","source":"# weight this model more\ncv_probs = cv_probs + lgb_cv_probs2 + lgb_cv_probs2\ntest_probs = test_probs + lgb_probs2 + lgb_probs2\nmodel_count += 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"505401b2192a8f13c4582200a89c368be4ec0024"},"cell_type":"code","source":"submission[[\"Id\", \"Target_lgb3\"]].to_csv(\"20180725_lgb3_preds.csv\", header=[\"Id\", \"Target\"], index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d1b647daf3902796f04276f0186bcfe3c3f978f"},"cell_type":"markdown","source":"### Average Probabilities and Make Predictions"},{"metadata":{"_uuid":"413d7669b97c2d1c1d1bb684ed8e1df09f9a17e6","trusted":false,"collapsed":true},"cell_type":"code","source":"cv_probs = cv_probs / model_count\ntest_probs = test_probs / model_count","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c744c75fc8ca766fb6ff01732c43df1b3bc8c77b","trusted":false,"collapsed":true},"cell_type":"code","source":"cv_predictions = np.argmax(cv_probs, axis=1) + 1\ntest_predictions = np.argmax(test_probs, axis=1) + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ede7e61cd97d749aaf9d2784e767d748fb8bf64","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(y_cv, cv_predictions))\nprint(\"F1:\", f1_score(y_cv, cv_predictions, average=\"micro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54de9aa2de0890c3afa3221a824d8144af8d8790","trusted":false,"collapsed":true},"cell_type":"code","source":"submission['Avg_probs'] = test_predictions\nsubmission[[\"Id\", \"Avg_probs\"]].to_csv(\"20180725_avg_preds.csv\", header=[\"Id\", \"Target\"], index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c1864da4caf03bc04a5680c244ec11c530a4cc67"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}