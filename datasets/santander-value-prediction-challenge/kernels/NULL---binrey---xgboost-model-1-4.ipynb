{"cells":[{"metadata":{"_uuid":"7754192c569ea955f62a51b69e096662bab4c5b6"},"cell_type":"markdown","source":"# SANTANDER\n## XGBoost + data engeneering"},{"metadata":{"_uuid":"0953a4b4f5f707b1ead19040e23a1b76af748823"},"cell_type":"markdown","source":"Regession XGBoost-model with cleaning low important features and creating new statistical representations. The idea comes from:\n> https://www.kaggle.com/alexpengxiao/preprocessing-model-averaging-by-xgb-lgb-1-39"},{"metadata":{"_uuid":"cff5cd1c6c039b569a7f4dad71a97fcbf7636a78"},"cell_type":"markdown","source":"## Prepair data, feature preprocessing"},{"metadata":{"trusted":true,"_uuid":"b199284bdc6481791b88dac5d6ff655d3609452c"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%pylab inline","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d07dfa0710750c71e5c0249beb273e58a47a601b"},"cell_type":"code","source":"# Import DATA\n\nx_train = pd.read_csv('../input/train.csv')\nx_test = pd.read_csv('../input/test.csv')\n\nprint('x_train: {0}, x_test: {1}'.format(x_train.shape, x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"697024aee4a99f5163f08cfc9ef33caa4aea7848"},"cell_type":"code","source":"# Create target array y\n\ntest_ID = x_test['ID']\ny_train = x_train['target']\ny_train = np.log1p(y_train)\nx_train.drop(\"ID\", axis = 1, inplace = True)\nx_train.drop(\"target\", axis = 1, inplace = True)\nx_test.drop(\"ID\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f41df84871738201d5f04af4c315d2fdc8c62f82"},"cell_type":"code","source":"# Drop features with only one value\n\ncols_with_onlyone_val = x_train.columns[x_train.nunique() == 1]\nx_train.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\nx_test.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\nprint('remove n columns: {0}'.format(len(cols_with_onlyone_val)))\nprint('x_train: {0}, x_test: {1}'.format(x_train.shape, x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96b8c797449e00bd4808fbc0de3d3079ed8cecd7"},"cell_type":"code","source":"# Check for duplicated columns\n\ncolsToRemove = []\ncolumns = x_train.columns\nfor i in range(len(columns)-1):\n   v = x_train[columns[i]].values\n   dupCols = []\n   for j in range(i + 1,len(columns)):\n      if np.array_equal(v, x_train[columns[j]].values):\n          colsToRemove.append(columns[j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6ecea720c536b6e8d40e260bbf1a773c488697a"},"cell_type":"code","source":"x_train.drop(colsToRemove, axis=1, inplace=True)\nx_test.drop(colsToRemove, axis=1, inplace=True)\nprint('removed columns: {0}'.format(colsToRemove))\nprint('x_train: {0}, x_test: {1}'.format(x_train.shape, x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc8ac18d2e82268c88ad6f744099950d708ecf5b"},"cell_type":"code","source":"# Select top NUM_OF_FEATURES informative features\n\nfrom sklearn import model_selection as ms\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef rmsle(y, pred):\n    return np.sqrt(np.mean(np.power(y - pred, 2)))\n\nx1, x2, y1, y2 = ms.train_test_split(\n    x_train, y_train.values, test_size=0.10)\nmodel = RandomForestRegressor(n_estimators=20, n_jobs=-1, random_state=0)\nmodel.fit(x1, y1)\nprint('rmsle: {0}'.format(rmsle(y2, model.predict(x2))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94ef17259faecb019b6e3b57cd87154853ccb5f8"},"cell_type":"code","source":"# Delete features (from low to tom importance) until sum importance will be less 0.95\n\nimp_cols = pd.DataFrame({'importance': model.feature_importances_, \n                    'feature': x_train.columns}).sort_values(by=['importance'], \n                    ascending=[True]).reset_index()\n\nfor i in range(imp_cols.shape[0]):\n    imp_cols.drop(i, inplace=True)\n    if imp_cols.importance.sum() <= 0.95:\n        break\n\nx_train = x_train[imp_cols.feature.values]\nx_test = x_test[imp_cols.feature.values]\nprint('x_train: {0}, x_test: {1}'.format(x_train.shape, x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b8a8e59181a1ff02b4b0a3354e1c50dbc3ef99b"},"cell_type":"code","source":"# Plot distribution of feature importances\n\nfig, ax = subplots(figsize=(12,6))\nax.set_yscale('log')\nplt.xlabel('feature number')\nplt.ylabel('log(importance)')\nplt.grid()\nplt.plot(imp_cols.index, imp_cols.importance*10000);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed4e4d9407f0ac98bbd26dc4a127aac479eb1a76"},"cell_type":"code","source":"# Create new features\n\nntrain = len(x_train)\nntest = len(x_test)\ntmp = pd.concat([x_train, x_test])\nweight = ((x_train != 0).sum()/len(x_train)).values # Sum of non zero elements of every feature\ntmp_train = x_train[x_train!=0]\ntmp_test = x_test[x_test!=0]\n\nx_train[\"weight_count\"] = (tmp_train*weight).sum(axis=1)\nx_test[\"weight_count\"] = (tmp_test*weight).sum(axis=1)\nx_train[\"count_not0\"] = (x_train != 0).sum(axis=1)\nx_test[\"count_not0\"] = (x_test != 0).sum(axis=1)\nx_train[\"sum\"] = x_train.sum(axis=1)\nx_test[\"sum\"] = x_test.sum(axis=1)\nx_train[\"var\"] = tmp_train.var(axis=1)\nx_test[\"var\"] = tmp_test.var(axis=1)\nx_train[\"median\"] = tmp_train.median(axis=1)\nx_test[\"median\"] = tmp_test.median(axis=1)\nx_train[\"mean\"] = tmp_train.mean(axis=1)\nx_test[\"mean\"] = tmp_test.mean(axis=1)\nx_train[\"std\"] = tmp_train.std(axis=1)\nx_test[\"std\"] = tmp_test.std(axis=1)\nx_train[\"max\"] = tmp_train.max(axis=1)\nx_test[\"max\"] = tmp_test.max(axis=1)\nx_train[\"min\"] = tmp_train.min(axis=1)\nx_test[\"min\"] = tmp_test.min(axis=1)\nx_train[\"skew\"] = tmp_train.skew(axis=1)\nx_test[\"skew\"] = tmp_test.skew(axis=1)\nx_train[\"kurtosis\"] = tmp_train.kurtosis(axis=1)\nx_test[\"kurtosis\"] = tmp_test.kurtosis(axis=1)\n\ndel(tmp_train)\ndel(tmp_test)\nprint('x_train: {0}, x_test: {1}'.format(x_train.shape, x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd3977f291cc79a3dbd4d9187cc729b5fa502ffc"},"cell_type":"markdown","source":"## Develop and train the model"},{"metadata":{"trusted":true,"_uuid":"132182be856cb3602b94c0316396cf29b95a2a22"},"cell_type":"code","source":"# Train XGBoost model and check results of cross-validation\n\nimport xgboost as xg\nfrom sklearn import metrics\n\nclf = xg.XGBRegressor(n_estimators=242, max_depth=5, learning_rate=0.02, min_child_weight=40)#211\ncvs_res = -ms.cross_val_score(clf, x_train, y_train, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\nprint('rmsle = ', round(cvs_res.mean()**0.5,3), '+/-', round(cvs_res.std(),3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81ac0a6b52eeeb73c5504ad021be930fa58bf4fb"},"cell_type":"code","source":"# Function to find best model\n\nfrom scipy import optimize as opt\ndef f(x):\n    n = int(x[0])\n    md = int(x[1])\n    lr = 0.1\n    mchw = 20\n    clf = xg.XGBRegressor(n_estimators=n, max_depth=md, learning_rate=lr, min_child_weight=mchw, reg_alpha=0.1)\n    res = -ms.cross_val_score(clf, x_train, ravel(y_train), scoring='neg_mean_squared_error', cv=3, n_jobs=-1).mean()\n    print('-> {0} | {1} = {2}'.format(n, md, round(res**0.5,3)))\n    return res\n\n#opt_res = opt.differential_evolution(f,[(20,300), (3,6)], maxiter=200, disp=True)\n#print opt_res","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f509bd97732819c0e83ac2a691880155d8d60245"},"cell_type":"markdown","source":"## Final score 1.4"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}