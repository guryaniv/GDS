{"cells":[{"metadata":{"_uuid":"0998028489bf46341c2b5511a022d36b196defcb"},"cell_type":"markdown","source":"@christofhenkel kernel on image feature extraction with pretrained models in Keras (https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn) inspired me to do the same thing with PyTorch. In this kernel I demonstrate how to extract features with pretrained Inception_v3 model in PyTorch. Previously, @pvlima posted a great kernel on image classification with pretrained models in PyTorch (https://www.kaggle.com/pvlima/use-pretrained-pytorch-models). But, unlike in Keras, pretrained models in PyTorch contain the last FC layer which, in the case of feature extraction, is unnecessary. Also, the trick with Inception_v3 is that it has two outputs: there's an auxiliary branch in the network (https://arxiv.org/abs/1409.4842, https://arxiv.org/abs/1512.00567) which helps with the classification task, but useless for our goal. So, in order to get rid of all unnecessary parts, I inherited Inception_v3 class and overrode the forward method. Also, I added extra 1D average pooling layer to reduce the number of features (originally, it's a 2048D vector). "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import time\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torchvision.models.inception import *\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.optim import lr_scheduler\n\nimport random\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c5808b9f53159f2f7cc8d2cb1842a5b303e13e4"},"cell_type":"code","source":"img_size = 299 # for Incerption V3\nBATCH_SIZE = 128\nN_EPOCHS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9442781af94f289d9b6fa0a39775a030aeca5061"},"cell_type":"code","source":"train_path = \"../input/petfinder-adoption-prediction/train_images/\"\ntest_path = \"../input/petfinder-adoption-prediction/test_images/\"\ntrain_df = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4207c4bf1c69c02b1d337180692f2a495020186"},"cell_type":"code","source":"train_df.Name = train_df.Name.fillna('GOTNONAME')\ntest_df.Name = test_df.Name.fillna('GOTNONAME')\n\ntrain_df.Description = train_df.Description.fillna('GOTNODESC')\ntest_df.Description = test_df.Description.fillna('GOTNODESC')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b20c3d5481cdcd531442202b64d88048cd2c2d7"},"cell_type":"markdown","source":"### Copy pretrained models weights into ~/.torch - default directory for model weights"},{"metadata":{"trusted":true,"_uuid":"83461456c21dfa2426e6b93983ab928d03623ee3"},"cell_type":"code","source":"# https://www.kaggle.com/pvlima/use-pretrained-pytorch-models\n\ncache_dir = expanduser(join('~', '.torch'))\nif not exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    os.makedirs(models_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cd9767c687bd9e7d69d67129b3ae60512817e60"},"cell_type":"code","source":"!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2244ca0ba126aae7b507f89c36b0e0fce5ddc678"},"cell_type":"code","source":"!cp ../input/pytorch-pretrained-models/* ~/.torch/models/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0bcaa3b6c17854870fc6f96f080518e4261919b"},"cell_type":"code","source":"!ls ~/.torch/models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81929a3d0b3e9fa2e358312bccbfeb260645cd1b"},"cell_type":"markdown","source":"### Seed everything for deterministic result"},{"metadata":{"trusted":true,"_uuid":"d1502f3600cf8a7c9faeeacc753f3e0e5cf43b75"},"cell_type":"code","source":"# https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abdaab4c0188cd69c3aec31340d21a55d3f79bb2"},"cell_type":"markdown","source":"### Image preprocessing"},{"metadata":{"trusted":true,"_uuid":"18987c2d130e6d25de4082cb19d1d2a152c9749e"},"cell_type":"code","source":"# https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn\ndef resize_to_square(im):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n    #new_image = preprocess_input(new_image)\n    return new_image","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"897d659d1701468573f25257c1e0b91fb60f13fd"},"cell_type":"markdown","source":"### Define DataLoader that uploads and preprocess images"},{"metadata":{"trusted":true,"_uuid":"2d77a4972b3b9c5e0bbef43173a8f2d6e71049ec"},"cell_type":"code","source":"class ImageLoader(Dataset):\n\n    def __init__(self, list_IDs, labels=None, dir_name=None, transform=None, return_id=False):\n        'Initialization'\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.dir_name = dir_name\n        self.transform = transform\n        self.return_id = return_id\n    \n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.list_IDs)\n    \n    def __getitem__(self, index):\n        'Generates one sample of data'\n        ID = self.list_IDs[index]\n\n        try: \n            X = load_image(self.dir_name, ID)\n        except:\n            X = np.zeros((img_size, img_size, 3))\n            \n        if self.transform:\n            X = self.transform(X)\n        \n        if self.labels and self.return_id:\n            return X, y, ID\n        elif self.labels:\n            return X, y\n        elif self.return_id:\n            return X, ID\n        else:\n            return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a13bba239b3b725fdc482c730e2bfa104d6b4e8"},"cell_type":"code","source":"normalize = transforms.Normalize(\n   mean=[0.485, 0.456, 0.406],\n   std=[0.229, 0.224, 0.225]\n)\nds_trans = transforms.Compose([\n                               #transforms.Scale(224),\n                               #transforms.CenterCrop(224),\n                               transforms.ToTensor(),\n                               normalize])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a38aebe4ded4e837228f7292423980176c5cc18a"},"cell_type":"markdown","source":"We retain Inception constructor as is, so that we initiate all the layers in the network, even those that we won't use later. We add an extra field *final_pooling* that equals to the kernel size of the last 1d average pooling layer"},{"metadata":{"trusted":true,"_uuid":"7addce2b771665eda0b176508c756b87e8ece0aa"},"cell_type":"code","source":"class CustomInception3(Inception3):\n    def __init__(self, num_classes=1000, aux_logits=False, transform_input=False, final_pooling=None):\n        self.final_pooling = final_pooling\n        super(CustomInception3, self).__init__()\n        \n    def forward(self, x):\n        if self.transform_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(x)\n        # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x)\n        # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x)\n        # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x)\n        # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x)\n        # 71 x 71 x 192\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 35 x 35 x 192\n        x = self.Mixed_5b(x)\n        # 35 x 35 x 256\n        x = self.Mixed_5c(x)\n        # 35 x 35 x 288\n        x = self.Mixed_5d(x)\n        # 35 x 35 x 288\n        x = self.Mixed_6a(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6b(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6c(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6d(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6e(x)\n        # 17 x 17 x 768\n        \n        ## we turn off auxiliary\n        #if self.training and self.aux_logits:\n        #    aux = self.AuxLogits(x)\n        \n        # 17 x 17 x 768\n        x = self.Mixed_7a(x)\n        # 8 x 8 x 1280\n        x = self.Mixed_7b(x)\n        # 8 x 8 x 2048\n        x = self.Mixed_7c(x)\n        # 8 x 8 x 2048\n        x = F.avg_pool2d(x, kernel_size=8) # size (batch_size, 2048, 1, 1)\n        # 1 x 1 x 2048\n        \n        ## We'll save average pooling over the last conv output, but turn off the last FC layer\n       \n        #x = F.dropout(x, training=self.training)\n        # 1 x 1 x 2048\n        #x = x.view(x.size(0), -1)\n        # 2048\n        #x = self.fc(x)\n        \n        ## turn off aux output\n        # 1000 (num_classes)\n        #if self.training and self.aux_logits:\n        #    return x, aux\n        \n        if self.final_pooling:\n            x = F.avg_pool1d(x.view(x.size(0), 2048, 1).permute(0, 2, 1), kernel_size=self.final_pooling)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3e4d254cf4c7b9151fb7dcc465719384b920d0a"},"cell_type":"markdown","source":"### Create Custom Inception instance and load weights"},{"metadata":{"trusted":true,"_uuid":"c87ceed61eff52017fc7e08672856edd82e2bab8"},"cell_type":"code","source":"inception_weights = \"/tmp/.torch/models/inception_v3_google-1a9a5a14.pth\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9938c8c89d64ddb167b278f72e847d48af058335"},"cell_type":"markdown","source":"We apply an extra 1D average pooling to the final 1 x 1 x 2048 activations. <br>\nHere we use kernel of size 8, so the final output will have 256 features."},{"metadata":{"trusted":true,"_uuid":"1d7e52a2fdb66fd1f94f0a11a6823b82351e2c6f"},"cell_type":"code","source":"InceptionModel = CustomInception3(final_pooling=8)\n\nInceptionModel.load_state_dict(torch.load(inception_weights))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d41cd1c762e91961eb4012fb107c34f216b212c1"},"cell_type":"markdown","source":"### Extract features with CustomInception and concat them to the original dataframe"},{"metadata":{"trusted":true,"_uuid":"4709aa75b9fdba15046f6a2e49ad30bb4e8764c1"},"cell_type":"code","source":"def find_ids_w_images(df, image_folder):\n    pet_ids = [s.split(\"-1.jpg\")[0] for s in os.listdir(image_folder) if s.endswith(\"-1.jpg\")]\n    #df_img = df[train_df.PetID.isin(pet_ids)]\n    #pet_ids = df_img.PetID.values\n    return pet_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5aa114e13cba397e21ab4ac6fa57744fd9d0029"},"cell_type":"code","source":"def extract_features(df, image_folder, model):\n    model.eval() \n    model = model.cuda()\n    img_ids = find_ids_w_images(df, image_folder)\n    \n    data_ds = ImageLoader(img_ids, dir_name=image_folder, transform=ds_trans)\n    data_dl = DataLoader(data_ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n    with torch.no_grad():\n        features = None\n        for x in tqdm(data_dl, disable=True):\n            x = x.cuda()\n            output = model(x)\n\n            if features is not None:\n                features = torch.cat((features, output), 0)\n            else:\n                features = output\n        \n        features = features.view(features.size(0), -1)\n        feat_df = pd.DataFrame(features.cpu().numpy(), columns=[f'img_{n}' for n in range(features.size(-1))])\n        feat_df = pd.concat((pd.DataFrame({'PetID': img_ids}), feat_df), axis=1)\n        \n        feat_df = df.merge(feat_df, on='PetID', how='outer')\n        \n        feat_df = feat_df.fillna(0)\n    \n    return feat_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8e17e20b9304468cb20f637af3888c5d34d32d5b"},"cell_type":"code","source":"train_df = extract_features(train_df, train_path, InceptionModel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5d6157b20b374630b976bd9304d6ffd8101bc69"},"cell_type":"code","source":"test_df = extract_features(train_df, train_path, InceptionModel)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}