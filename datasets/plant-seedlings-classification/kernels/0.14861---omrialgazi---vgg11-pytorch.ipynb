{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nfrom math import sqrt, ceil, trunc\nfrom random import shuffle\nimport random\nimport cv2\n\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\nfrom skimage.transform import resize\n\nimport torch\nfrom torchvision import transforms\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nfrom torch.autograd import Variable\n\nimport time\n\nroot_path = \"../input/train/\"\nSEED = 448\nrandom.seed(SEED)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc7307ea85690805f8d2dc3feb98cf88567a0f03","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"427aa1a345cbb1277809f1f47816806e7e08675c"},"cell_type":"code","source":"dtype = torch.cuda.FloatTensor\n\nlr = 0.0002\nresize_target = 200\nbatch_size = 32\nlimit_dataset = 1\nsplit_train_val = 0.8\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e86d8e2236b332ee9dc308fe6a377f1fd3441bc3","collapsed":true},"cell_type":"code","source":"def conv_calc(in_size, model, pool_f = 2, pool_s = 2, conv_f = 3, conv_s = 1, conv_p = 1):\n    out = in_size\n    for layer in model:\n        if layer =='M':\n            out = trunc(((out - pool_f)/ pool_s) + 1)\n        else:\n            out = ((out - conv_f + (2 * conv_p))/conv_s) + 1\n            channels = layer\n    output = out * out * channels\n    print (f'Image size after conv is {out}*{out}, channels:{channels}, final vector size:{output}')\n    return  int(output)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e04cca0aa5c3285cff59df04f101315fb43add50"},"cell_type":"markdown","source":"Classes for our dataset"},{"metadata":{"trusted":true,"_uuid":"526ddcd9029159cc40285c93f5d7012f229bc48f","collapsed":true},"cell_type":"code","source":"class FeatExtract:\n    \"\"\"Grabs file locations, splits to test&val\n    Also shuffles\"\"\"\n    def __init__(self, path, limit = None, shuffled = False, split = 0.8):\n        self.path = path\n        self.limit = limit\n        self.shuffled = shuffled\n        self.mapping = {}\n        self.all_files =[]\n        self.list_all_files()\n        if self.shuffled:\n            shuffle(self.all_files)\n        \n        \n        #Split to train and validate\n        self.split = int(len(self.all_files) * split)\n        self.train = self.all_files[:self.split]\n        self.val = self.all_files[self.split:]\n         \n    def list_all_files(self):\n        for label, directory in enumerate(os.listdir(self.path)):\n            self.mapping[label] = directory\n            tmp_list = [[self.path + directory+'/'+file,label] for file in os.listdir(self.path + directory)]\n            if self.shuffled:\n                shuffle(tmp_list)\n            if self.limit:\n                tmp_list=tmp_list[:int(len(tmp_list)*self.limit)]\n            self.all_files.extend(tmp_list)\n        self.n_features = label + 1\n        \n    \n\nclass Seeds(nn.Module):\n    \"\"\"Reads through a DB one by one, perform transforms\"\"\"\n    def __init__(self, file_list, segment = False, transform = None):\n        self.file_list = file_list\n        self.transform = transform \n        self.segment = segment\n        \n    def __len__(self):\n        return len(self.file_list)\n        \n    def __getitem__(self, idx):\n        item = self.file_list[idx][0]\n        label = self.file_list[idx][1]\n        img = Image.open(item).convert('RGB')\n        if self.segment:\n            img = segment_plant(img)\n        img = adjust_colors(img)\n        if self.transform:\n            img = self.transform (img)\n        else:\n            img =  transforms.functional.resize(img, (resize_target,resize_target))\n            img =  transforms.functional.to_tensor(img)\n        return img, label\n\n\nclass AccLossPlotter:\n    \"\"\"Perform fwd pass on the loaded set to get loss and accuracy\n    Also - plots!\"\"\"\n    def __init__(self, loader):\n        self.train_loss = []\n        self.val_loss = []\n        self.val_acc = []\n        self.loader = loader\n        \n    def add_epoch(self, loss):\n        self.train_loss.append(loss)\n        \n    def evaluate(self, model):\n        acc, loss = check_accuracy(model, self.loader)\n        self.val_acc.append(acc)\n        self.val_loss.append(loss)\n        \n    def plot(self):\n        curcurr=range(len(self.val_loss))\n        plt.plot(curcurr,self.train_loss,'.r',label='Train loss')\n        plt.plot(curcurr,self.val_loss,'+b',label='Validation loss')\n        plt.plot(curcurr,self.val_acc,'*g',label='Validation accuracy')\n        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n        plt.xlabel('Epochs')\n        plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5625a34f8907ced5ae50920eb81e753e1ba9d9b2"},"cell_type":"markdown","source":"\nImage manipulation area ! "},{"metadata":{"trusted":true,"_uuid":"8b09c6e2a2c7d4035b7c7fa7407e9836136b9b92"},"cell_type":"code","source":"data_transform = transforms.Compose([\n        transforms.transforms.Resize((resize_target,resize_target)),\n        transforms.transforms.ColorJitter(brightness=1, contrast=0.5, saturation=0.5, hue=0),\n        transforms.transforms.RandomHorizontalFlip(),\n        transforms.transforms.RandomRotation(180),\n        transforms.transforms.RandomVerticalFlip(),\n        transforms.ToTensor()\n    ])\n\n\ndef adjust_colors(img):\n    img = transforms.functional.adjust_brightness(img, 2)\n    img = transforms.functional.adjust_contrast(img, 1.1)\n    img = transforms.functional.adjust_saturation(img, 1.1)\n    return img\n\n\ndef create_mask_for_plant(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n\n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return mask\n\n\ndef segment_plant(image):\n    image = np.array(image)\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    output = transforms.functional.to_pil_image (output)\n    return output\n\n\n\nclass RandomCrop:\n    def __init__(self, image, label, n_crops, crop_size):  \n        mask = create_mask_for_plant(image)\n        if image.shape[0] > 300:\n            image = resize(image, (300,300))\n            mask = resize(mask,(300,300))\n        self.label = label\n        self.image = image\n        self.mask = mask\n        self.n_crops = n_crops\n        self.crop_size = crop_size\n        self.crops = []\n    \n    def create_crops(self):\n        #Divide image to a grid where each square is the size of crop_size\n        if int(self.image.shape[0]) < 1 + self.crop_size * 2:\n            self.crops = [[self.image, self.label] for im in range(self.n_crops)]\n        else:\n            for row in np.linspace(0,self.image.shape[0] - self.crop_size, int(self.image.shape[0]/self.crop_size), dtype = int):\n                for col in  np.linspace(0,self.image.shape[1] - self.crop_size, int(self.image.shape[1]/self.crop_size), dtype = int):\n\n                    #Check for each square crop, the intensity of relevant pixels, and add them to crops list\n                    green_pixels_in_crop = sum(sum(self.mask[row:row+self.crop_size,col:col+self.crop_size]/255))\n                    self.crops.append((self.image[row:row+self.crop_size, col:col+self.crop_size], green_pixels_in_crop ))\n\n                    #Sort the list by most relevant and take top n_crops\n                    self.crops = sorted(self.crops, key=lambda x:x[1], reverse = True)\n                    self.crops = [[x[0], self.label] for x in self.crops[:self.n_crops]] #Returns images and label\n                    if len(self.crops) < self.n_crops:\n                        pass\n                        #print ('problem with crops, shape image is', self.image.shape)\n                        #print ('we got %s crops but %s requested'%(len(self.crops),self.n_crops))\n\n\n    def get_crop(self):\n        for i in self.crops:\n            yield i\n\n\n\n'''\n#for testing\n\nall_images = FeatExtract(root_path, limit = 1, shuffled = True, split = 1)\ndisp_images = Seeds(all_images.train)\nim = disp_images[0][0]\nlabel = disp_images[0][1]\nprint (im.shape)\ntim1=time.time()\nim2 = random_crop(im, n_crops = 4, crop_size = 51)\nprint(time.time()-tim1)\nprint (len(im2))\nplt.imshow(im2[0]) \n'''\n#show_images_range(0,4,disp_images)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4138f5ed3cebe8dbdb34aa5f947e8c408d5160da"},"cell_type":"markdown","source":"Functions to actually run the model"},{"metadata":{"trusted":true,"_uuid":"b793d1f8892e0574a5e5cd6ff2e1de747cfe6e07","collapsed":true},"cell_type":"code","source":"def reset(m):\n    if hasattr(m, 'reset_parameters'):\n        m.reset_parameters()\n\n\ndef check_accuracy(model, loader):\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n    for x, y in loader:\n        x_var = Variable(x.type(dtype), volatile=True).cuda()\n        y = Variable(y, volatile =True).cuda()\n        \n        scores = model(x_var)\n        scores = scores.float()\n        loss = loss_fn(scores, y)\n        \n        _, preds = scores.data.cpu().max(1)\n        preds = preds.cuda()\n        \n        y = y.data.cuda()\n        num_correct += (preds == y).sum()\n        num_samples += preds.size(0)\n    acc = float(num_correct) / num_samples\n    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n    return acc, loss.data[0]\n\n\n\ndef train(model, loss_fn, optimizer, flex_lr_optim, num_epochs = 1, print_every = 10):\n    for epoch in range(num_epochs):\n        start_epoch = time.time()\n        print('Starting epoch %d / %d' % (epoch + 1 , num_epochs))\n        model.train()\n        for t, (x, y) in enumerate(loader_train):\n            x_var = Variable(x.type(dtype))\n            y_var = Variable(y).cuda()\n            scores = model(x_var)\n            loss = loss_fn(scores, y_var)\n            if (t + 1) % print_every == 0:\n                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        print ('Epoch time in seconds: %.2f' %(time.time() - start_epoch))\n        plt.imshow(np.array(x[0].cpu()).transpose(1,2,0))\n        valoss_plotter.add_epoch(loss.data[0])\n        valoss_plotter.evaluate(model)\n        flex_lr_optim.step(valoss_plotter.val_loss[-1])\n    valoss_plotter.plot()\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"55d702b6f8ec88ec1d669c10c5def8a25683bfed"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f882a01f9aba1f7be49f2c31555b6edd044c4325"},"cell_type":"markdown","source":"Our hyper parameter area!"},{"metadata":{"trusted":true,"_uuid":"cb43051dd351019d810c90cb4d209b21877832cc","scrolled":false,"collapsed":true},"cell_type":"code","source":"trainer = FeatExtract(root_path, limit = False, shuffled = True, split = split_train_val)\n\nloader_train = DataLoader(Seeds(trainer.train, segment = False, transform = data_transform), batch_size=batch_size)\n\nloader_val = DataLoader(Seeds(trainer.val, segment = False), batch_size=batch_size)\n\n#Our loss function!\nloss_fn=nn.CrossEntropyLoss()\n#loss_fn = nn.MSELoss()\n\n#model = models.vgg11_bn(num_classes = 12)\nmodel = models.resnet101(num_classes = 12)\n#Adjust our model to work with our own image size\n'''model.classifier._modules['0'] = nn.Linear(in_features=conv_calc(\n    resize_target,[64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']),\n                                           out_features=4096, bias=True)\n'''\nmodel = model.cuda()\n\nvaloss_plotter = AccLossPlotter(loader_val)\n\n#optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n\noptimizer = torch.optim.Adam(model.parameters(),lr=lr)\n\nflex_lr_optimizer = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5, verbose = True)\n\ntrain(model, loss_fn, optimizer, flex_lr_optimizer , num_epochs = 40)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49fed32c0944b30677690c43aebd15d15967235e","scrolled":true,"collapsed":true},"cell_type":"code","source":"valoss_plotter.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad25d007713605f358918158eb0ca5f0a2c62843","scrolled":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10fc8c8aa2c59101f4a89824f2d911f2850dacc8","scrolled":true,"collapsed":true},"cell_type":"markdown","source":"Builiding the test set scores\n"},{"metadata":{"trusted":true,"_uuid":"0cf8bd5c93ff3d429709599b3b7de47e2b33bd3a","collapsed":true},"cell_type":"code","source":"test_path = '../input/test/'\ntest_files = [test_path + filename for filename in os.listdir(test_path)]\ntest_predictions =[]\nmodel.eval()\nfor path in test_files:\n    img = Image.open(path).convert('RGB')    \n    #img = segment_plant(img)\n    img = adjust_colors(img)\n    img = transforms.functional.resize(img, (resize_target,resize_target))\n    img = np.array(img).transpose(2,0,1)\n    img = np.expand_dims(img, axis = 0)\n    img = torch.from_numpy(img)\n\n\n    x_var = Variable(img.type(dtype), volatile=True).cuda()\n    \n    scores = model(x_var)\n\n    _, preds = scores.data.cpu().max(1)\n    test_predictions.append(preds)\n\ntest_predictions = np.array(test_predictions)\ncsv_results = pd.DataFrame([trainer.mapping[pred] for pred in test_predictions], os.listdir(test_path))[0]\ncsv_results = csv_results.reset_index(level = 0)\ncsv_results.columns = ['file', 'species']\ncsv_results.to_csv('preds1.csv', header = True, index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"934885d2a63da565583ccee464eb3cde6d203e1f","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"930bec1e1db0d542629f831037fbd8d7d9a2f00f","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3e923e6ccd3eb0205a371f14305964f67ba6398","collapsed":true},"cell_type":"code","source":"#Finding optimal learning rate\n'''\nmodel.train()\nresults = []\nfor lr in np.geomspace(0.00001,0.0004,6):\n    reset(model)\n    print (lr)\n    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n    loss = train(model, loss_fn, optimizer, num_epochs = 2)\n    acc = check_accuracy(model, loader_val)\n    results.append((loss, acc))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7aed81ba506ed0317b93b20cef9c6fb48da519a9","collapsed":true},"cell_type":"code","source":"\"\"\"\n#Getting some stats on the image sizes\nall_images = FeatExtract(root_path, limit = 1, shuffled = True, split = 1)\ndisp_images = Seeds(all_images.train)\n\n\nsizes = np.array([])\nlabel = np.array([])\nfor num, image in enumerate(disp_images):\n    sizes = np.append(sizes,image[0].shape[1])\n    label = np.append(label, image[1])\n\nplt.subplot (2, 1 ,1 )\nplt.hist(label, bins = 25)\nplt.title ('Image label distribution')\nplt.subplot (2, 1, 2)\nplt.hist(sizes, range=(40,1000), bins = 25)\nplt.title ('All images')\nplt.tight_layout()\nplt.show()\n\nsizes = None\nlabel = None\n\"\"\"\n\n\"\"\"\n#Getting a visualization of image processing \n\nall_images = FeatExtract(root_path, limit = 1, shuffled = True, split = 1)\ndisp_images = Seeds(all_images.train)\nsample_images = [disp_images[img] for img in range(20)]\nimage = sample_images[0]\nimage_mask = create_mask_for_plant(image[0])\nimage_segmented = segment_plant(image[0])\nimage_sharpen = sharpen_image(image_segmented)\n\nplt.imshow(image[0])\n\nfig = plt.figure(figsize = (25,25))\nfig.add_subplot(3,1,1)\nplt.imshow(image_mask)\nfig.add_subplot(3,1,2)\nplt.imshow(image_segmented)\nfig.add_subplot(3,1,3)\nplt.imshow(image_sharpen)\n\n\n\n\n\ndef show_images_range(start,end,input_x, fig_size = 25):\n    #displays images from ^start till end$ in input_x - list of images\n    all_pics = end - start\n    \n    subplot_size = ceil(sqrt(all_pics))\n    \n    input_x = [input_x[x] for x in range(start, end+1)]\n\n    fig=plt.figure(figsize=(fig_size, fig_size))\n    \n    plt.axis('off')\n\n    for image in range(1, all_pics + 1):\n        fig.add_subplot(subplot_size, subplot_size, image)\n        plt.imshow(input_x[image][0])\n    plt.tight_layout()\n\n    \n\"\"\"\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2145b7109c04c54bc5c651cff269beabc776b944","collapsed":true},"cell_type":"code","source":"\ndef show_images_range(start,input_x, fig_size = 25):\n    #displays images from ^start till end$ in input_x - list of images\n    img_arr = []\n    print((input_x))\n    subplot_size = ceil(sqrt(start))\n    for i, x in enumerate(input_x):\n        img_arr.append(x)\n        if i>start:\n            break\n    \n    fig=plt.figure(figsize=(fig_size, fig_size))\n    plt.axis('off')\n\n    for image in range(1, len(img_arr)):\n        fig.add_subplot(subplot_size, subplot_size, image)\n        plt.imshow(img_arr[image])\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7810fe823d3707defdf1f533df3ac5e12a0c4853","collapsed":true},"cell_type":"code","source":"show_images_range(25,loader_train)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}