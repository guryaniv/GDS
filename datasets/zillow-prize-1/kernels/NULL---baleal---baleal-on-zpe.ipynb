{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.1", "name": "python"}}, "nbformat_minor": 0, "cells": [{"metadata": {"_uuid": "13d217817290becd8dd0f41a5108db8b96dd166d", "_execution_state": "idle", "collapsed": false, "_cell_guid": "65c07bb7-7f8d-46a6-abb5-2bd109c4c339"}, "source": "This notebook will be my first attempt on the Zillow Prize competition\nThe objective: Build a model to improve the Zestimate residual error.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "c996571ac7a13da5b959404a50dc5534207da622", "_execution_state": "idle", "collapsed": false, "_cell_guid": "fa56e7a6-b5ed-4e40-a0a8-e619ef9cbbf5"}, "source": " ## 1. Data Exploration ##", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"trusted": false, "_execution_state": "idle", "_uuid": "98f5000bd8d7362ecdf73b8a67ed4a848f46120a", "_cell_guid": "33ca57a7-8e1f-4957-b838-165086883c7a"}, "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "2b9a97dd7076e9980ff97b4fc97d55ebeb42595e", "_execution_state": "idle", "collapsed": false, "_cell_guid": "04290e0d-9c3f-41ac-9311-78df7a2d7924"}, "source": "1.1 Train_2016_v2\n-----------------\n\nI used https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-zillow-prize as a nice introduction to the data exploration\n\n1. Parcel Id: 90554 Id's are unique/ 127 *2 /1 * 3 \n2. logerror: normally distributed with heavy outliners\n3. Transaciont date: the number of transactions is way smaler for the months nov - dec ( from the datapage: *The train data has all the transactions before October 15, 2016, plus some of the transactions after October 15, 2016*) . When checking for weekdays: the number of transactions rise during to week, maxing on friday. Almost no transactions occur during weekends.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "da16b62dae417f846c30fa881b91308bedec2bde", "_execution_state": "idle", "collapsed": false, "_cell_guid": "91d862f5-7df6-42d1-9f87-09ea28a626c9", "trusted": false}, "source": "train_df = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates = [\"transactiondate\"])\ntrain_df.shape", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "f77a65b67890b6de9e3d613bd5fca0ac7f7c6d5a", "_execution_state": "idle", "collapsed": false, "_cell_guid": "25587b0a-facc-4063-8d2d-01285882c82c", "trusted": false}, "source": "train_df['transaction_day'] = train_df['transactiondate'].dt.weekday\n\ncnt_srs = train_df['transaction_day'].value_counts()\nplt.figure(figsize=(10,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\nplt.xticks(rotation='vertical')\nplt.xlabel('Day of transaction', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.show()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "19a84ee1f38b517472c707304a004cb543a5e212", "_execution_state": "idle", "collapsed": false, "_cell_guid": "fb0064e9-c570-410f-bb6a-6e9802035cfe"}, "source": "\n\n1.2 Properties_2016\n-----------------\n\n", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "c3b0743dfb51de1d648db583c119b93383ce8ea1", "_execution_state": "idle", "collapsed": false, "_cell_guid": "816a486d-b204-44f0-9235-fa0cadbdb0f3", "trusted": false}, "source": "prop_df = pd.read_csv(\"../input/properties_2016.csv\")\nprop_df.shape", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "eff40e4311a0b8b4d6b098209f185f36a60cd362", "_execution_state": "idle", "collapsed": false, "_cell_guid": "98d050c3-3331-4f97-92f8-1843f9c97f28"}, "source": "Not all the data in the properties is linked to an errorlog (90275 vs 2985217)", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "4e8680bdb71acf2ab550c053eb849f76fc7d60b6", "_execution_state": "idle", "collapsed": false, "_cell_guid": "8612185d-ea5c-4fce-9d1e-f5c350f04df8", "trusted": false}, "source": "if train_df.shape[1] == 3: \n    train_df = pd.merge(train_df, prop_df, on='parcelid', how='left' )\ntrain_df.shape\n", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "5d536390178092f8925599a3386ab1a929d1a3ff", "_execution_state": "idle", "collapsed": false, "_cell_guid": "e9eb30e0-4c96-4fb5-9282-d07efef26b93"}, "source": "Lets check for the Nans in th train_df dataset:\n10 variables have over 99% of Nans", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "654ad9ae74aed078f057c62318e9c72a2387c8d0", "_execution_state": "idle", "collapsed": false, "_cell_guid": "cdc6b9b4-55f0-4931-aba8-613276b086b3", "trusted": false}, "source": "def NanPercent(daf):\n    var, c_nan, p_nan = [], [], [];\n    for i in range (0,len(list(daf))):\n            count_nan = daf.shape[0] - daf[list(daf)[i]].count()\n            percent_nan = (count_nan / daf.shape[0]) * 100\n            var.append(list(daf)[i]),c_nan.append(count_nan), p_nan.append(percent_nan)\n    \n    Nanpercent_df = pd.DataFrame(\n        {'Variable': var,\n         'Nr of Nans': c_nan,\n         '% of Nans': p_nan\n        })\n    return Nanpercent_df.sort_values(['% of Nans'])\nNanPercent(train_df)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "43a630392e6c674ea14038c8649b079c72bf1a6a", "_execution_state": "idle", "collapsed": false, "_cell_guid": "d2254c4a-c95c-41c0-a81c-8666b0f538d4", "trusted": false}, "source": "#Data PreProcessing\n#propertyzoningdesc => string\ntrain_df['propertyzoningdesc'] = train_df['propertyzoningdesc'].astype(str)\n#hashottuborspa => TRUE ==> 1\ntrain_df.hashottuborspa.replace('True',1, inplace=True)\ntrain_df['hashottuborspa'] = train_df['hashottuborspa'].astype('float64')\n#propertycountylandusecode ==> string\ntrain_df['propertycountylandusecode'] = train_df['propertycountylandusecode'].astype(str)\n#fireplaceflag ==> TRUE ==> 1\ntrain_df.fireplaceflag.replace('True',1, inplace=True)\ntrain_df['fireplaceflag'] = train_df['fireplaceflag'].astype('float64')\n#taxdelinquencyflag ==> y\ntrain_df.taxdelinquencyflag.replace('Y',1, inplace=True)\ntrain_df['taxdelinquencyflag'] = train_df['taxdelinquencyflag'].astype('float64')\n\ntrain_df.dtypes", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_uuid": "9f1c12589e94958efdb11a12f0673a2f57e04ffa", "_cell_guid": "c3329c89-ec15-40b1-909b-0b1cdd4fe66a", "trusted": false}, "source": "sns.set(context=\"paper\", font=\"monospace\")\ncorrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmat,vmax=1, square=True)\nf.tight_layout()", "outputs": [], "execution_count": null, "cell_type": "code"}], "nbformat": 4}