{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b2d5887a-9824-b1e9-c975-28b23a20fda3"
      },
      "source": [
        "The goal for this analysis is to predict credit card default based on transactional data. \n",
        "We will be using Tensorflow to build the predictive model, \n",
        "and t-SNE to visualize the dataset in two dimensions.\n",
        "\n",
        "Dataset: Default of Credit Card Clients Dataset\n",
        "link: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset\n",
        "\n",
        "The sections of this analysis include:\n",
        "\n",
        " 1. Visualizing the Data with t-SNE.\n",
        " 2. Exploring the Data\n",
        " 3. Create and train the Neural Network\n",
        "\n",
        "The achieved prediction accuracy is 82%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c2550ae3-c669-553b-8916-f7117dcb0285"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import sklearn as skl\n",
        "from sklearn.cross_validation import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8295b1e6-d352-9fd5-f729-f5075dce1538"
      },
      "source": [
        "1) Visualizing the Data with t-SNE\n",
        "==================================\n",
        "\n",
        "t-SNE is a technique for dimensionality reduction that is particularly well suited for the visualisation of high-dimensional datasets. The technique has become widespread in the field of machine learning, since it gives the opportunity for a compelling two-dimensional \u201cmap\u201d of a dataset. In our case we are looking for a qualitative first look on that map that will set some expectations for the prediction accuracy that we are targeting. In simple words, if our dataset 'looks' mixed, i.e. with many overlaps, we will not be disappointed if our neural network achieves an accuracy of 60-70%. Let's see!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e423dd8-2bc8-ba11-b04c-0c8e5f244a28"
      },
      "outputs": [],
      "source": [
        "###### 1) Visualizing the Data with t-SNE\n",
        "# Load the dataset\n",
        "tsne_data = pd.read_csv(\"../input/UCI_Credit_Card.csv\")\n",
        "tsne_data.rename(columns = {'default.payment.next.month':'default'}, inplace=True)\n",
        "\n",
        "\n",
        "#Set df4 equal to a set of a sample of 1000 deafault and 1000 non-default observations.\n",
        "df2 = tsne_data[tsne_data.default == 0].sample(n = 1000)\n",
        "df3 = tsne_data[tsne_data.default == 1].sample(n = 1000)\n",
        "df4 = pd.concat([df2, df3], axis = 0)\n",
        "\n",
        "#Scale features to improve the training ability of TSNE.\n",
        "standard_scaler = StandardScaler()\n",
        "df4_std = standard_scaler.fit_transform(df4)\n",
        "\n",
        "#Set y equal to the target values.\n",
        "y = df4.ix[:,-1].values\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "x_test_2d = tsne.fit_transform(df4_std)\n",
        "\n",
        "#Build the scatter plot with the two types of transactions.\n",
        "color_map = {0:'red', 1:'blue'}\n",
        "plt.figure()\n",
        "for idx, cl in enumerate(np.unique(y)):\n",
        "    plt.scatter(x = x_test_2d[y==cl,0], \n",
        "                y = x_test_2d[y==cl,1], \n",
        "                c = color_map[idx], \n",
        "                label = cl)\n",
        "plt.xlabel('X in t-SNE')\n",
        "plt.ylabel('Y in t-SNE')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('t-SNE visualization of test data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fc722200-83c1-fd3d-9c53-ce73133e924e"
      },
      "source": [
        "*The visual reveals a rather mixed up dataset which means that we shall not expect to end up with an extremely accurate model. We shall also expect that our neural network will learn fairly fast due to the fact that the dataset looks balanced; there is a considerable amount of observed defaults in our dataset.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "63123f00-b5a9-d112-d499-96809195123f"
      },
      "source": [
        "2) Exploring the data\n",
        "====================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8d51c327-2209-c8ae-4d55-92b1ce489731"
      },
      "outputs": [],
      "source": [
        "###### 2) Exploring the Data\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"../input/UCI_Credit_Card.csv\")\n",
        "df.rename(columns = {'default.payment.next.month':'default'}, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7ff88c9a-6ab8-89f3-c714-7d250551cd86"
      },
      "source": [
        "Let's see if there are empty values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7289b1d4-ac1f-aa3b-233b-72091ab26544"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1955a66c-3251-1def-44b5-0fa42f26af4d"
      },
      "source": [
        "No missing values! That makes our life easier! \n",
        "\n",
        "----------\n",
        "Let's see how AGE compares across default and non-default observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "97788d33-d641-5fe6-a234-910e601c3c61"
      },
      "outputs": [],
      "source": [
        "print (\"Default :\")\n",
        "print (df.AGE[df.default == 1].describe())\n",
        "print ()\n",
        "print (\"NO default :\")\n",
        "print (df.AGE[df.default == 0].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e236c108-d25a-5f45-790e-c965cdd2e4f4"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
        "\n",
        "bins = 20\n",
        "\n",
        "ax1.hist(df.AGE[df.default == 1], bins = bins)\n",
        "ax1.set_title('Default')\n",
        "\n",
        "ax2.hist(df.AGE[df.default == 0], bins = bins)\n",
        "ax2.set_title('No Default')\n",
        "\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Number of Observations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b9def3f3-a4b2-68a2-d2f0-ee4c0e18d386"
      },
      "source": [
        "There is no evidence that age has a significant role on whether the \n",
        "credit card will default or not. There are more defaults observed \n",
        "between the age of 25 and 35 but this has to do with the fact that\n",
        "there are more observations in this range, i.e. credit cards are \n",
        "primarily used/issued by people in this age range.\n",
        "\n",
        "\n",
        "----------\n",
        "Now let's see if we can infer a relationship between default and education"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c30eb4d5-1a9d-b509-b558-12f14d19dc42"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
        "\n",
        "bins = 6 # as many as the education types for simplicity \n",
        "\n",
        "ax1.hist(df.EDUCATION[df.default == 1], bins = bins)\n",
        "ax1.set_title('Default')\n",
        "\n",
        "ax2.hist(df.EDUCATION[df.default == 0], bins = bins)\n",
        "ax2.set_title('No Default')\n",
        "\n",
        "plt.xlabel('Education')\n",
        "plt.ylabel('Number of Observations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d7de6870-2d6a-4b51-24da-66b3ae802553"
      },
      "source": [
        "Nothing notable here as well! \n",
        "\n",
        "\n",
        "----------\n",
        "\n",
        "3.a) Prepare the Training, Testing and Validation datasets\n",
        "=========================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "577e1ad1-4b96-9f6a-508c-1b59005b80cb"
      },
      "outputs": [],
      "source": [
        "#Create a new Class for Non Default observations.\n",
        "df.loc[df.default == 0, 'nonDefault'] = 1\n",
        "df.loc[df.default == 1, 'nonDefault'] = 0\n",
        "\n",
        "print(df.default.value_counts())\n",
        "print()\n",
        "print(df.nonDefault.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e504454f-9938-511d-f01f-bac360913031"
      },
      "source": [
        "A fairly balanced dataset; a default has been observed on the 22% of our total observations (6636/(23364+6636)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e56615f6-ee21-d974-c549-5b76c225e57e"
      },
      "outputs": [],
      "source": [
        "#Create dataframes of only default and nonDefault observations.\n",
        "Default = df[df.default == 1]\n",
        "NonDefault = df[df.nonDefault == 1]\n",
        "\n",
        "# Set X_train equal to 80% of the observations that defaulted.\n",
        "X_train = Default.sample(frac=0.8)\n",
        "count_Defaults = len(X_train)\n",
        "\n",
        "# Add 80% of the not-defaulted observations to X_train.\n",
        "X_train = pd.concat([X_train, NonDefault.sample(frac = 0.8)], axis = 0)\n",
        "\n",
        "# X_test contains all the observations not in X_train.\n",
        "X_test = df.loc[~df.index.isin(X_train.index)]\n",
        "\n",
        "#Shuffle the dataframes so that the training is done in a random order.\n",
        "X_train = shuffle(X_train)\n",
        "X_test = shuffle(X_test)\n",
        "\n",
        "#Add our target classes to y_train and y_test.\n",
        "y_train = X_train.default\n",
        "y_train = pd.concat([y_train, X_train.nonDefault], axis=1)\n",
        "\n",
        "y_test = X_test.default\n",
        "y_test = pd.concat([y_test, X_test.nonDefault], axis=1)\n",
        "\n",
        "#Drop target classes from X_train and X_test.\n",
        "X_train = X_train.drop(['default','nonDefault'], axis = 1)\n",
        "X_test = X_test.drop(['default','nonDefault'], axis = 1)\n",
        "\n",
        "#Check to ensure all of the training/testing dataframes are of the correct length\n",
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "print(len(X_test))\n",
        "print(len(y_test))\n",
        "\n",
        "# CHECKED !\n",
        "\n",
        "#Names of all of the features in X_train.\n",
        "features = X_train.columns.values\n",
        "\n",
        "#Transform each feature in features so that it has a mean of 0 and standard deviation of 1; \n",
        "#this helps with training the neural network.\n",
        "for feature in features:\n",
        "    mean, std = df[feature].mean(), df[feature].std()\n",
        "    X_train.loc[:, feature] = (X_train[feature] - mean) / std\n",
        "    X_test.loc[:, feature] = (X_test[feature] - mean) / std\n",
        "    \n",
        "# Split the testing data into validation and testing sets\n",
        "split = int(len(y_test)/2)\n",
        "\n",
        "inputX = X_train.as_matrix()\n",
        "inputY = y_train.as_matrix()\n",
        "inputX_valid = X_test.as_matrix()[:split]\n",
        "inputY_valid = y_test.as_matrix()[:split]\n",
        "inputX_test = X_test.as_matrix()[split:]\n",
        "inputY_test = y_test.as_matrix()[split:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "10ebc384-1aa5-de92-d464-f2d691253e44"
      },
      "source": [
        "3.b) Build and Train the model\n",
        "=============================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "87d611cc-5bbc-60fa-a1b9-97a20ea635c3"
      },
      "outputs": [],
      "source": [
        "# Number of input nodes.\n",
        "input_nodes = 24\n",
        "\n",
        "# Multiplier maintains a fixed ratio of nodes between each layer.\n",
        "mulitplier = 3 \n",
        "\n",
        "# Number of nodes in each hidden layer\n",
        "hidden_nodes1 = 24\n",
        "hidden_nodes2 = round(hidden_nodes1 * mulitplier)\n",
        "hidden_nodes3 = round(hidden_nodes2 * mulitplier)\n",
        "\n",
        "# Percent of nodes to keep during dropout.\n",
        "pkeep = tf.placeholder(tf.float32)\n",
        "\n",
        "# input\n",
        "x = tf.placeholder(tf.float32, [None, input_nodes])\n",
        "\n",
        "# layer 1\n",
        "W1 = tf.Variable(tf.truncated_normal([input_nodes, hidden_nodes1], stddev = 0.15))\n",
        "b1 = tf.Variable(tf.zeros([hidden_nodes1]))\n",
        "y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
        "\n",
        "# layer 2\n",
        "W2 = tf.Variable(tf.truncated_normal([hidden_nodes1, hidden_nodes2], stddev = 0.15))\n",
        "b2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
        "y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2)\n",
        "\n",
        "# layer 3\n",
        "W3 = tf.Variable(tf.truncated_normal([hidden_nodes2, hidden_nodes3], stddev = 0.15)) \n",
        "b3 = tf.Variable(tf.zeros([hidden_nodes3]))\n",
        "y3 = tf.nn.sigmoid(tf.matmul(y2, W3) + b3)\n",
        "y3 = tf.nn.dropout(y3, pkeep)\n",
        "\n",
        "# layer 4\n",
        "W4 = tf.Variable(tf.truncated_normal([hidden_nodes3, 2], stddev = 0.15)) \n",
        "b4 = tf.Variable(tf.zeros([2]))\n",
        "y4 = tf.nn.softmax(tf.matmul(y3, W4) + b4)\n",
        "\n",
        "# output\n",
        "y = y4\n",
        "y_ = tf.placeholder(tf.float32, [None, 2])\n",
        "\n",
        "# Parameters\n",
        "training_epochs = 20 # These proved to be enough to let the network learn\n",
        "training_dropout = 0.9\n",
        "display_step = 1 # 10 \n",
        "n_samples = y_train.shape[0]\n",
        "batch_size = 2048\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Cost function: Cross Entropy\n",
        "cost = -tf.reduce_sum(y_ * tf.log(y))\n",
        "\n",
        "# We will optimize our model via AdamOptimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# Correct prediction if the most likely value (default or non Default) from softmax equals the target value.\n",
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "###### Train the network\n",
        "accuracy_summary = [] # Record accuracy values for plot\n",
        "cost_summary = [] # Record cost values for plot\n",
        "valid_accuracy_summary = [] \n",
        "valid_cost_summary = [] \n",
        "stop_early = 0 # To keep track of the number of epochs before early stopping\n",
        "\n",
        "# Initialize variables and tensorflow session\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for epoch in range(training_epochs): \n",
        "        for batch in range(int(n_samples/batch_size)):\n",
        "            batch_x = inputX[batch*batch_size : (1+batch)*batch_size]\n",
        "            batch_y = inputY[batch*batch_size : (1+batch)*batch_size]\n",
        "\n",
        "            sess.run([optimizer], feed_dict={x: batch_x, \n",
        "                                             y_: batch_y,\n",
        "                                             pkeep: training_dropout})\n",
        "\n",
        "        # Display logs after every 10 epochs\n",
        "        if (epoch) % display_step == 0:\n",
        "            train_accuracy, newCost = sess.run([accuracy, cost], feed_dict={x: inputX, \n",
        "                                                                            y_: inputY,\n",
        "                                                                            pkeep: training_dropout})\n",
        "\n",
        "            valid_accuracy, valid_newCost = sess.run([accuracy, cost], feed_dict={x: inputX_valid, \n",
        "                                                                                  y_: inputY_valid,\n",
        "                                                                                  pkeep: 1})\n",
        "\n",
        "            print (\"Epoch:\", epoch,\n",
        "                   \"Acc =\", \"{:.5f}\".format(train_accuracy), \n",
        "                   \"Cost =\", \"{:.5f}\".format(newCost),\n",
        "                   \"Valid_Acc =\", \"{:.5f}\".format(valid_accuracy), \n",
        "                   \"Valid_Cost = \", \"{:.5f}\".format(valid_newCost))\n",
        "            \n",
        "            # Record the results of the model\n",
        "            accuracy_summary.append(train_accuracy)\n",
        "            cost_summary.append(newCost)\n",
        "            valid_accuracy_summary.append(valid_accuracy)\n",
        "            valid_cost_summary.append(valid_newCost)\n",
        "            \n",
        "            # If the model does not improve after 15 logs, stop the training.\n",
        "            if valid_accuracy < max(valid_accuracy_summary) and epoch > 100:\n",
        "                stop_early += 1\n",
        "                if stop_early == 15:\n",
        "                    break\n",
        "            else:\n",
        "                stop_early = 0\n",
        "            \n",
        "    print()\n",
        "    print(\"Optimization Finished!\")\n",
        "    print()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "83da402d-5e6c-46a7-145e-53b5a91ba0d1"
      },
      "outputs": [],
      "source": [
        "# Plot the accuracy and cost summaries \n",
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,4))\n",
        "\n",
        "ax1.plot(accuracy_summary) # blue\n",
        "ax1.plot(valid_accuracy_summary) # green\n",
        "ax1.set_title('Accuracy')\n",
        "\n",
        "ax2.plot(cost_summary)\n",
        "ax2.plot(valid_cost_summary)\n",
        "ax2.set_title('Cost')\n",
        "\n",
        "plt.xlabel('Epochs (x10)')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}