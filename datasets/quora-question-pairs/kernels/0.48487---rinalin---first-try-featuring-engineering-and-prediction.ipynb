{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e3b13065-7a58-17da-e783-dd3304d43d7f"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import nltk # natural language processing\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "#from subprocess import check_output\n",
        "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "54a1af60-8f05-f9e7-0700-8ef4f0512c24"
      },
      "outputs": [],
      "source": [
        "def strip_punctuation(s):\n",
        "    # input str, output str, strip out punctuations\n",
        "    return ''.join(c for c in s if c not in punctuation)\n",
        "\n",
        "def data_transformation(s):\n",
        "    return strip_punctuation(str.lower(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "640e7262-08d4-adc2-408d-1e4260206156"
      },
      "outputs": [],
      "source": [
        "def to_nltk_text(text):\n",
        "    #input dataframe, output nltk text object. \n",
        "    token = nltk.word_tokenize(text)\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(t) for t in token if t not in stop]\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c4dc3b17-82ff-7e3c-72d0-e665f72122b3"
      },
      "outputs": [],
      "source": [
        "def shared_differed(s1,s2):\n",
        "    s1_text = to_nltk_text(s1)\n",
        "    s2_text = to_nltk_text(s2)\n",
        "    return list(set(s1_text).difference(s2_text))\n",
        "\n",
        "def check_synonym(word, word2):\n",
        "    \"\"\"checks to see if word and word2 are synonyms\"\"\"\n",
        "    l_syns = list()\n",
        "    synsets = wn.synsets(word)\n",
        "    for synset in synsets:\n",
        "        if word2 in synset.lemma_names():\n",
        "            return True\n",
        "            break\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1722a4d0-b284-3eee-d67c-59f78d67fce5"
      },
      "outputs": [],
      "source": [
        "def syn_dist(word_list1,word_list2):\n",
        "    try:\n",
        "        share = 1/float(len(word_list1)+len(word_list2))\n",
        "    except :\n",
        "        return 0         \n",
        "    result = 1.0\n",
        "    for word1 in word_list2:\n",
        "        for word2 in word_list2:\n",
        "            if check_synonym(word1,word2):\n",
        "                result = result - share\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "949ea519-0acd-cadd-21cd-7b1ec0eddc23"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "01905179-3426-67dc-4a15-b9e31d4742e8"
      },
      "outputs": [],
      "source": [
        "train['source'] = 1.0\n",
        "test['source'] = 0.0\n",
        "alls = pd.concat([train,test],ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f449647-4b40-495f-c280-b00d575d3af6"
      },
      "outputs": [],
      "source": [
        "alls['question1'] = alls['question1'].fillna(\"\").apply(lambda x: data_transformation(x))\n",
        "alls['question2'] = alls['question2'].fillna(\"\").apply(lambda x: data_transformation(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c83ef52f-b81a-6415-8284-f5153436782e"
      },
      "outputs": [],
      "source": [
        "mydoclist = alls['question1'].tolist() + alls['question2'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c865e0e4-b60f-38f1-9600-490d7fa558b8"
      },
      "outputs": [],
      "source": [
        "n = len(alls['question1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b9fd18d2-2d10-8138-4016-c8a77e3c2d0a"
      },
      "outputs": [],
      "source": [
        "#Compute Tf-Idf\n",
        "count_vectorizer = CountVectorizer(min_df=1)\n",
        "term_freq_matrix = count_vectorizer.fit_transform(mydoclist)\n",
        "tfidf = TfidfTransformer(norm=\"l2\")\n",
        "tfidf.fit(term_freq_matrix)\n",
        "tf_idf_matrix = tfidf.transform(term_freq_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "afa68ce5-ce9c-dab7-6ebb-bf187a44153c"
      },
      "outputs": [],
      "source": [
        "tf_idf_matrix_q1  = tf_idf_matrix[0:n,]\n",
        "tf_idf_matrix_q2  = tf_idf_matrix[n:2*n,]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7ff1713b-b28d-b44e-5a3b-9d7cf62bdcb9"
      },
      "outputs": [],
      "source": [
        "dot_dist = tf_idf_matrix_q1.multiply(tf_idf_matrix_q2).sum(axis=1).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3fd41923-4375-477a-bb58-85db4586485e"
      },
      "outputs": [],
      "source": [
        "dot_dist = [dot for li in dot_dist for dot in li]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "145f78ae-922c-ea56-2fe1-66fcd439c70e"
      },
      "outputs": [],
      "source": [
        "alls['dot_dist'] = dot_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7a9d4873-9100-b66c-f4f8-ad7a30b1b84d"
      },
      "outputs": [],
      "source": [
        "alls['word_count_q1'] = alls['question1'].apply(len)\n",
        "alls['word_count_q2'] = alls['question2'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6ad57a49-67a6-9722-557f-5b1af433ee20"
      },
      "outputs": [],
      "source": [
        "alls['differed_word_q1'] = alls.apply(lambda x: len(shared_differed(x['question1'],x['question2'])),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8b0977bd-474f-9b64-7816-06a410f47ff6"
      },
      "outputs": [],
      "source": [
        "alls['differed_word_q2'] = alls.apply(lambda x: len(shared_differed(x['question2'],x['question1'])),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "25ffc317-055f-7bef-b5f4-f8537d0e0aba"
      },
      "outputs": [],
      "source": [
        "#alls['syn_dist'] = alls.apply(lambda x: syn_dist(x['differed_word_q1'],x['differed_word_q2']),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "763c81e3-2eb8-4f2a-7f64-a88cf7c6b60c"
      },
      "outputs": [],
      "source": [
        "#alls['diff_ct'] = alls.apply(lambda x: len(x['differed_word_q1'])+len(x['differed_word_q2']),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0a412a0b-53dd-0205-c12f-275e5893dc60"
      },
      "outputs": [],
      "source": [
        "train = alls[alls['source'] == 1.0]\n",
        "test = alls[alls['source'] == 0.0]\n",
        "train.drop('source',axis=1,inplace=True)\n",
        "test.drop('source',axis=1,inplace=True)\n",
        "test.drop('is_duplicate',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0e08405-33ad-10d4-07da-585511a33aeb"
      },
      "outputs": [],
      "source": [
        "x = train[['dot_dist','word_count_q1','word_count_q2','differed_word_q1','differed_word_q2']]\n",
        "x_test = test[['dot_dist','word_count_q1','word_count_q2','differed_word_q1','differed_word_q2']]\n",
        "Y = train['is_duplicate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f3a38fe4-1650-ae91-2c83-81c2bf6cdc62"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf = GradientBoostingClassifier()\n",
        "clf.fit(x,Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "52c68d9a-390d-cd04-fde3-47cfec621e62"
      },
      "outputs": [],
      "source": [
        "#Training Score\n",
        "Y_pred = clf.predict_proba(x)\n",
        "from sklearn.metrics import log_loss\n",
        "print(log_loss(Y,Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c81aac8f-5dc2-46f5-a989-000aa9126e07"
      },
      "outputs": [],
      "source": [
        "#Prediction\n",
        "Y_Test_Pred = clf.predict_proba(x_test)\n",
        "test['is_duplicate'] =Y_Test_Pred[:,1]\n",
        "submission = test[['test_id','is_duplicate']]\n",
        "submission['test_id'] = submission['test_id'].apply(int)\n",
        "submission.to_csv('submission.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}