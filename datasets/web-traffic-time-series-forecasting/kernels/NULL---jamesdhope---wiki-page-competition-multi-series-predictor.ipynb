{"cells": [{"outputs": [], "metadata": {"_uuid": "f5ff82653244ab9649c6c005d5bb17ae3ee970c4", "collapsed": true, "trusted": false, "_cell_guid": "9ed3f1fb-1b2b-4a70-9942-ff3d7d0abdd8"}, "cell_type": "markdown", "source": "An introduction to RNN's (Recurrent Neural Networks) for time series prediction. \n\nThe Wikipedia page data for the Kaggle hosting competition is an ideal candidate for this type of model.\n\nThe RNN below takes multiple time series across a number of Wiki pages (time series). You can play around with the model hyperparameters: number of inputs/outputs, time steps, layers, batch size and see what effect this has on the prediction. You can also play around with the number of epochs (whole cycles through the training data) and batch size.\n\nThe model is cycled forward 90 time steps; the results are plotted on cartesian axes.\n\nAny thoughts, comments or suggestions are welcome. Twitter DM me @jamesdhope.\n\nPlease note this Notebook is superseeded here: https://www.kaggle.com/jamesdhope/wiki-page-forecasting-grucell-improved", "execution_count": null}, {"execution_count": null, "metadata": {"_uuid": "3bc111fb75c93f0451462835a757cdad69afe17c", "collapsed": true, "trusted": false, "_cell_guid": "94131758-220a-4a5e-b6a1-5b587b8ac050"}, "cell_type": "code", "source": "import tensorflow as tf\nimport numpy.random as rnd\nimport numpy as np", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "9b7e7779c5bc14e1e81624e802ad2e90c951917a", "collapsed": true, "trusted": false, "_cell_guid": "7fb68ec0-b26c-4d79-9656-36f68dbd682c"}, "cell_type": "code", "source": "# To plot pretty figures\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "2e8be448926bfc77714487a3dfa7e15ddbf9ad08", "collapsed": true, "trusted": false, "_cell_guid": "76da9861-e62f-48a4-8a64-a01f9cf408bc"}, "cell_type": "code", "source": "# to make this notebook's output stable across runs\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "beebb228f228325a08f47847d4c0a00a645016a1", "collapsed": true, "trusted": false, "_cell_guid": "903b8e19-c749-4d57-b194-99e2dfa593ef"}, "cell_type": "code", "source": "reset_graph()\n\n#CONSTRUCTION PHASE\nn_steps = 21\nn_inputs = 10 #same as rows\nn_neurons = 200\nn_outputs = 10 #same as rows\nn_layers = 6\n\nlearning_rate = 0.001\n\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n\n#y has the same shape\ny = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n\n#create the cells and the layers of the network\nlayers = [tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),output_size=n_outputs) for layer in range(n_layers)]\nmulti_layer_cell = tf.contrib.rnn.MultiRNNCell(layers, state_is_tuple=True)\noutputs, states = tf.nn.dynamic_rnn(multi_layer_cell,X,dtype=tf.float32)\n\nprint(outputs.shape)\nprint(y.shape)\n\n#define the cost function and optimization\nloss = tf.reduce_mean(tf.square(outputs - y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntraining_op = optimizer.minimize(loss)\n\n#Adds an op to initialize all variables in the model\ninit = tf.global_variables_initializer()", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "f3ffd4b2fb19f6e0005b286974845c52d74fce3a", "collapsed": true, "trusted": false, "_cell_guid": "e0e30472-cf6f-456e-b2da-c90830acac5d"}, "cell_type": "code", "source": "#Import training data\nimport csv\nfrom numpy import genfromtxt\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport random\nfrom sklearn.preprocessing import Imputer\n\n#read the csv into a dataframe\ndf=pd.read_csv('../input/train_1.csv', sep=',', error_bad_lines=False, warn_bad_lines=True)\n\n#clean the data\ndf.drop(['Page'], axis=1, inplace = True)\ndf = df.replace(np.nan, 0, regex=True)\n\n#grab the relevant rows\nX_test = df.values.astype(int)\nrows = print(len(X_test))\n\n#number of rows to read \nrows = 10\n\n#fetch the rows\nX_test = df.values.astype(int)\nX_test = X_test[:rows,0:]\n\n#set the labelled data as the time step + 1\nY_test = X_test[:,1:]\n\n#strip n numbers off the rows for reshape\nX_test = X_test[:,:-4]\nY_test = Y_test[:,:-3]\n\n#print(\"unshaped X data\", X_test)\n#print(\"unshaped X data shape\", X_test.shape)\n\n#print(\"unshaped Y data\", Y_test)\n#print(\"unshaped Y data shape\", Y_test.shape)\n\nfor iteration in range(len(X_test)):\n    plt.plot(X_test[iteration,:], label=\"page\"+str(iteration+1))\n\nplt.xlabel(\"Time\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.show()    \n    \nY_test = Y_test.reshape((-1, n_steps, n_inputs))\nX_test = X_test.reshape((-1, n_steps, n_inputs))\n\n#print(\"transformed X data\", X_test[0])\n#print(\"transformed X data\", X_test[1])\n#print(\"transformed Y data\", Y_test[0])", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "31350ad53b0336101c0f0a6b8ceecad4f68d6ac2", "collapsed": true, "trusted": false, "_cell_guid": "df108a2a-d567-4853-a8c2-3746f5a34175"}, "cell_type": "code", "source": "#TRAINING PHASE\n\nimport numpy\nn_epochs = 1000\nbatch_size = 10\ninstances = X_test.shape[0]\nsaver = tf.train.Saver()\n\nprint(\"RNN construction\", \"n_layers\", n_layers, \"n_neurons\", n_neurons, \"n_inputs\", n_inputs, \"n_outputs\", n_outputs)\nprint(\"training dataset\", \"shape\", X_test.shape, \"instances\", instances, \"batch_size\", batch_size)\nprint(\"training with\", \"n_epochs\", n_epochs, \"iterations (instances // batch_size)\", (instances//batch_size))\n\n#open a TensorFlow Session\nwith tf.Session() as sess:\n    init.run()\n    \n    #Epoch is a single pass through the entire training set, followed by testing of the verification set.\n    for epoch in range(n_epochs):\n    \n        #print(\"X_test\",X_test)\n        idxs = numpy.random.permutation(instances) #shuffled ordering\n        #print(idxs)\n        #print(idxs.shape)\n        X_random = X_test[idxs]\n        #print('X_random', X_random)\n        Y_random = Y_test[idxs]\n        #print('Y_random', Y_random)\n    \n        #Number of batches, here we exhaust the training set\n        for iteration in range(instances // batch_size):   \n\n            #get the next batch - we permute the examples in each batch\n            #X_batch, y_batch = next_batch(batch_size, n_steps)\n            X_batch = X_random[iteration * batch_size:(iteration+1) * batch_size]\n            y_batch = Y_random[(iteration * batch_size):((iteration+1) * batch_size)]\n            \n            #print(\"iteration\", iteration, \"X_batch\", X_batch)\n            #print(\"iteration\", iteration, \"y_batch\", y_batch)\n            \n            X_batch = X_batch.reshape((-1, n_steps, rows))\n            y_batch = y_batch.reshape((-1, n_steps, rows))\n        \n            #feed in the batch\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        \n            if (epoch % 100 == 0 and iteration == 0):\n                #check contents of first example in each batch\n                #print(\"iteration\", iteration, \"X_batch\", X_batch[0])\n                #print(\"iteration\", iteration, \"y_batch\", y_batch[0])\n                mse = loss.eval(feed_dict={X:X_batch, y:y_batch})\n                print(\"epoch\", epoch, \"iteration\", iteration, \"\\tMSE:\", mse)\n            \n            #print(epoch)\n    \n    saver.save(sess, \"./my_model\") ", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "5b65935a1852902888234194cb2bfd37632106fb", "collapsed": true, "trusted": false, "_cell_guid": "40a2ec0b-5a2a-4f28-baaa-f96ee8f1b7ea"}, "cell_type": "code", "source": "#Sequence generation using the model\n\nmax_iterations = 90 #number of time steps to look forward\n\nwith tf.Session() as sess:                        \n    saver.restore(sess, \"./my_model\") \n\n    sequence = [[0] * n_steps] * rows\n    for iteration in range(max_iterations):\n        #print(\"iteration\", iteration)\n        \n        X_batch = np.array(sequence[-n_steps*rows:]).reshape(-1,n_steps,rows)\n        #print(X_batch)\n        \n        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n        #print(\"Y\", y_pred)\n        #print(\"numbers to be added\", np.array(y_pred[0,n_steps-1]))\n            \n        sequence = np.append(sequence, y_pred[0,n_steps-1])      \n        #print(\"sequence so far...\", np.array(sequence))\n        \n    #print(\"end sequence\", np.array(sequence))\n    sequence[numpy.where(sequence<0)] = 0\n    #print(sequence)\n    sequence = np.array(sequence).reshape(-1,rows)\n    #print(sequence)", "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "dbf28477fc899e687ed35180a307a0e59dc07b2c", "collapsed": true, "trusted": false, "_cell_guid": "c99c59ed-0a2f-48cd-9b54-d30a1f40487a"}, "cell_type": "code", "source": "plt.figure(figsize=(8,4))\n\n#print(sequence[:,0])\n\nfor iteration in range(rows):\n    plt.plot(sequence[:,iteration], label = \"page\"+str(iteration+1))\n\nplt.xlabel(\"Time\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.show()", "outputs": []}], "nbformat": 4, "metadata": {"language_info": {"version": "3.6.1", "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1}