{"cells":[{"metadata":{"_uuid":"6b36442a2628d51a0ae2bed1193ff971b95788d3"},"cell_type":"markdown","source":"# Load Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = 10, 7\n\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read train and test files\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fcf67bdaafe97630e8e4ddd1ab1c2624564c2a2"},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43ec46ef122e866722adc2d917de306c356efe01"},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"491e9cf23e44c996f67cbc01d7dc404b9f212ea2"},"cell_type":"markdown","source":"We can notice a difference of columns types between train and test dataframes."},{"metadata":{"_uuid":"b1f9006e6970ae3fe5d22299d3bad914e7c0fa22"},"cell_type":"markdown","source":"# 1) Exploration"},{"metadata":{"_uuid":"2dd57477c67fb9300ba48e47e60548a3b36b9d01"},"cell_type":"markdown","source":"## Missing Values"},{"metadata":{"trusted":true,"_uuid":"e44ad3f43127c1955a6025f080066a0904de2f24"},"cell_type":"code","source":"train_missing_columns = train_df.isnull().any().sum()\ntest_missing_columns = test_df.isnull().any().sum()\nprint(\"Train : Number of features with missing values {}\".format(train_missing_columns))\nprint(\"Test : Number of features with missing values {}\".format(test_missing_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"56edd6f35f668e379dd07365b58bf2a9bc579e43"},"cell_type":"markdown","source":"## Distribution of the Target Variable"},{"metadata":{"trusted":true,"_uuid":"caa6a21b795bff018eca3317d9c7997a9b5abebd"},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.distplot(train_df[\"target\"],kde=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9841ef05f76d190a73d5b6695a290b4b48c74afb"},"cell_type":"markdown","source":"The distribution is positively skewed, we need to perform a data Transformation on the target variable with a log transform, the distribution should be more normal."},{"metadata":{"trusted":true,"_uuid":"f2f4dd4b37bba740d025a003749991370eba1571"},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.distplot(np.log(train_df[\"target\"]),kde=True)\nplt.title(\"Train set\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e59a0b48d3b1cd262cdecd7971ae5ff1f2dc7ea2"},"cell_type":"code","source":"train_df[\"target\"] = np.log(train_df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f34a1c59f8b48d0fba9f95d0a2c50fa391c60b9b"},"cell_type":"markdown","source":"# 3) Remove Outliers"},{"metadata":{"trusted":true,"_uuid":"bc9a2f3dfb48f02f2aa62f6dec8223a9380ac54e"},"cell_type":"code","source":"# TRAIN\nraw_lines = train_df.shape[0]\nprint(\"Before filtering : \" + str(raw_lines))\n# Compute IQR\nQ1 = train_df['target'].quantile(0.25)\nQ3 = train_df['target'].quantile(0.75)\nIQR = Q3 - Q1\n\ntrain = train_df[(train_df['target'] < (Q3 + 1.5 * IQR)) & (train_df['target'] > (Q1 - 1.5 * IQR))]\nclean_lines = train.shape[0]\nprint(\"After filtering : \" + str(clean_lines))\nprint(\"We lost {} lines with the IQR filter\".format(raw_lines - clean_lines))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc16db297b3cb058f5e54b8e10b950705caf19df"},"cell_type":"markdown","source":"# 4) Explore Dimensionality Reduction with ACP"},{"metadata":{"trusted":true,"_uuid":"765b6b25055c146e3bbbad862512a9f1f50d599c"},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\npca = PCA(n_components=None, svd_solver=\"full\")\n# ID column is a string, and remove the target feature\ncols_pca = [col for col in train.columns if col not in ['ID', 'target']]\npca.fit(StandardScaler().fit_transform(train.loc[:, cols_pca]))\ncum_var_exp = np.cumsum(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6d42cdd84c7e34095c0c6bf23c71840c74e060b"},"cell_type":"code","source":"n_components = 20\n\nplt.figure(figsize=(12, 6))\nplt.bar(np.arange(n_components), list(pca.explained_variance_ratio_[:n_components]), align=\"center\",\n        color='red', label=\"Individual explained variance\")\nplt.step(np.arange(n_components), cum_var_exp[:n_components], where=\"mid\", label=\"Cumulative explained variance\")\nplt.xticks(np.arange(n_components))\nplt.legend(loc=\"best\")\nplt.xlabel(\"Principal component index\", {\"fontsize\": 14})\nplt.ylabel(\"Explained variance ratio\", {\"fontsize\": 14})\nplt.title(\"PCA on training data\", {\"fontsize\": 16})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c71851f8fc58d2ee2415d8ffb806a83cf4d4c73"},"cell_type":"markdown","source":"## Choosing the number of components \nFirst components don't explain a lot of varirance. To select the best number of components we are going to use the percentage of variance explained"},{"metadata":{"trusted":true,"_uuid":"e94a4986d9c657d3736add6ad74499de0d1a0f32"},"cell_type":"code","source":"for explained_variance in np.arange(0.80, 1.0, 0.05):\n    pca = PCA(explained_variance, svd_solver=\"full\")\n    # ID column is a string, and remove the target feature\n    cols_pca = [col for col in train.columns if col not in ['ID', 'target']]\n    pca.fit(StandardScaler().fit_transform(train.loc[:, cols_pca]))\n    cum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n    print(pca.n_components_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7349dec15ff7a7fb6678fd4df3ecc219d6b5733"},"cell_type":"markdown","source":"## PCA computation with the best number of Components"},{"metadata":{"trusted":true,"_uuid":"925c8b9930f6b2ebf0f8d3adf848dc67cbdcea49"},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\npca = PCA(0.15, svd_solver=\"full\")\n# ID column is a string, and remove the target feature\ncols_pca = [col for col in train.columns if col not in ['ID', 'target']]\nX_train = pca.fit_transform(StandardScaler().fit_transform(train.loc[:, cols_pca]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b10f80d3586728a9f1c530ad7b141084e45226f"},"cell_type":"markdown","source":"# 5) Split Train and Test"},{"metadata":{"trusted":true,"_uuid":"df14520609d5749f7e30584feb189ba9acffd643"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = train[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9947801e86e6cf769b8bae4093249bb30379441a"},"cell_type":"markdown","source":"# 6) Modeling"},{"metadata":{"_uuid":"8a96ee4f7f88e66b8e7119a64cd998053e3efffa"},"cell_type":"markdown","source":"## 1) Linear Regression"},{"metadata":{"trusted":true,"_uuid":"9e9bdb3667030d3ef0ff826d875902e9475db915"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\n#Transform the Prediction to the correct form : we reverse Log() with Exp() \n#final_prediction = np.exp(prediction)\n\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint('Linear Regression Performance on the test set: {}'.format(rmse))\n#rmsle = np.sqrt(mean_squared_log_error(y_test, predictions))\n#print('Linear Regression Performance on the test set: {}'.format(rmsle))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8a89312f64873d37ce4dbe89ac0089f3d2876b7a"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true,"_uuid":"133d6a35490f31fecac9d2dfcec8522cc13b2e4f"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n\ndef rmsle(h, y): \n    \"\"\"\n    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n    Args:\n        h - numpy array containing predictions with shape (n_samples, n_targets)\n        y - numpy array containing targets with shape (n_samples, n_targets)\n    \"\"\"\n    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())\n\n\ndef runRF(x_train, y_train,x_test, y_test,test):\n    model=RandomForestRegressor(bootstrap=True, max_features=0.75, min_samples_leaf=11, min_samples_split=13, n_estimators=100)\n    model.fit(x_train, y_train)\n    y_pred_train=model.predict(x_test)\n    mse=rmsle(np.exp(y_pred_train)-1,np.exp(y_test)-1)\n    y_pred_test=model.predict(test)\n    return y_pred_train,mse,y_pred_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0048b0d12c4bf88353dfff901c1f6345f176cd47"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}