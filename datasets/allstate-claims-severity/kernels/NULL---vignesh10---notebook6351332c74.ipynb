{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0decc98a-62b5-38ad-92ad-c88fe3dbef70"
      },
      "source": [
        "EDA Python.\n",
        "\n",
        "This is my first kernel on Kaggle. Please share your views, to help me improve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "998e5766-f27c-541b-3c2c-2d8fcf0a7b4b"
      },
      "outputs": [],
      "source": [
        "#Please go through the comments in each cell.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c64996ed-8108-7356-a097-9e7612fce78a"
      },
      "outputs": [],
      "source": [
        "#Fetching the training data as a pandas dataframe for visualization\n",
        "train_df = pd.read_csv(\"../input/train.csv\")\n",
        "#deleting the column 'id' from the dataframe as it is a unique and does not have any effect on the algorithm\n",
        "del train_df['id']\n",
        "\"../input/train.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d906098a-e1e2-a525-ff37-3d2297f5350f"
      },
      "outputs": [],
      "source": [
        "#creating 2 seperate dataframes for categorical and continuous features.\n",
        "train_df_cat = pd.DataFrame()#training data frame with categorical features\n",
        "train_df_cont = pd.DataFrame() #training data with continuous features\n",
        "cat_list = []#list of categorical features\n",
        "cont_list = []#list of continuous features\n",
        "\n",
        "#populating the created data frames for categorical and continuous features\n",
        "for each_column in train_df.columns:\n",
        "    if train_df[each_column].dtype == 'float':\n",
        "        cont_list.append(each_column)\n",
        "cont_list.remove('loss')\n",
        "for each_column in train_df.columns:\n",
        "    if train_df[each_column].dtype == 'object':\n",
        "        cat_list.append(each_column)\n",
        "\n",
        "for i in range(0,len(cat_list)):\n",
        "    train_df_cat[i] = train_df[cat_list[i] ]\n",
        "train_df_cat.columns =cat_list\n",
        "for i in range(0,len(cont_list)):\n",
        "    train_df_cont[i] = train_df[cont_list[i] ]\n",
        "train_df_cont.columns =cont_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3879141d-b58b-41cf-a40d-4f5af2d7f134"
      },
      "outputs": [],
      "source": [
        "\n",
        "#checking for missing values:\n",
        "train_df.isnull().sum().sum()\n",
        "#sum is 0 which indicates there are no misisng values in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bd468408-11ed-8cee-f054-e2bb3a32ce3d"
      },
      "outputs": [],
      "source": [
        "#Univariate analysis:\n",
        "#It is method of analysis where each variable can be analysed independently.\n",
        "#For analysing the distributions of each feature, I have used  4 methods:\n",
        "    #1.Histogram\n",
        "    #2.Boxplot\n",
        "    #3.stats.skew to measure the skewness in each continuous feature\n",
        "#For the first three methods, I have used the plot() function with just changing the arguements for respective plots\n",
        "#ex: df.plot(kind = 'hist') for histogram. I have customized the plot using other arguements to the plot function such\n",
        " #as (subplot = True) for plotting each column (each features) independently.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# change outlier point symbols\n",
        "hist_dist = train_df_cont.plot(kind='hist', subplots=True,layout = (5,3),figsize = (15,15),sharex = False)\n",
        "\n",
        "#On analysing the histogram of all the continuous features, the features cont1,cont5,cont7,cont8,cont9,cont13 seems to be \n",
        "#skewed. Let us confirm, if that is the case with the remaining methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "01f55ade-31c3-924a-0c17-1b97d6ba7d32"
      },
      "outputs": [],
      "source": [
        "#Method 2: Density plots\n",
        "density_plot_cont = train_df_cont.plot(kind='density', subplots=True,layout = (5,3),figsize = (15,15),sharex = False)\n",
        "\n",
        "#On analysing the density plots, it almost presents a similar picture as the histogram. The same set of features \n",
        " #cont1,cont5,cont7,cont8,cont9,cont13 seems to be skewed, while the other features seems to be fairly symmetrical.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6abd5030-9dee-a048-128f-1ac080673930"
      },
      "outputs": [],
      "source": [
        "#Method 3: Box plots\n",
        "\n",
        "color = dict(boxes='DarkGreen', whiskers='DarkOrange',medians='DarkBlue', caps='Gray')\n",
        "box_plot_cont = train_df_cont.plot(kind='box', subplots=True,layout = (5,3),color=color, sym='r+',figsize = (15,15),\n",
        "                                   sharex = False,showfliers=True)\n",
        "\n",
        "#After all we are learning machine learning and avoiding redundancy is a key component hence would end the analysis\n",
        "#with just stating similar results. But one key point to be highlighted in this case, is the presence of outliers \n",
        "#in the features cont 7, cont 9, cont 10.  eliminating outliers is a one of the important pre processing step before \n",
        "#applying the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d8afae7e-05e7-4591-4e27-f1acf5a71de8"
      },
      "outputs": [],
      "source": [
        "#Method 4: stats.skew\n",
        "#Using the general thumb rule\n",
        "#If the skewness is between -0.5 and 0.5, the data are fairly symmetrical\n",
        "#If the skewness is between -1 and \u2013 0.5 or between 0.5 and 1, the data are moderately skewed\n",
        "#Hence fetching the list of features with skewness above and below the specified limit.\n",
        "\n",
        "import scipy.stats as stats\n",
        "skew_list = []\n",
        "for each_column in train_df_cont.columns:\n",
        "    skew_list.append(round(scipy.stats.skew(train_df_cont[each_column],bias = False),2))\n",
        "skew_dict = dict(zip(train_df_cont.columns,skew_list))\n",
        "print (\"The skewness in each continuous features are:\",skew_dict)\n",
        "#Fetching the list of continuous features to be normalized\n",
        "to_be_normalized = []\n",
        "for keys,value in skew_dict.items():\n",
        "    if  not((-0.5 < value < 0.5)):\n",
        "        to_be_normalized.append(keys)\n",
        "\n",
        "print ('\\t')\n",
        "print (\"The continuous features to be normalized are\",to_be_normalized)\n",
        "\n",
        "print (train_df_cont.head())\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "df7a4080-c159-f608-24c7-1a31808026a1"
      },
      "outputs": [],
      "source": [
        "#Feature pre processing of continuous features.\n",
        "#Now that we have an idea of continuous features, let us get into preprocessing\n",
        "\n",
        "#We are going to try Log transformation on the continuous features to reduce skewness.\n",
        "\n",
        "for each_column in to_be_normalized:\n",
        "    print (\"The skew in\", each_column ,\"before applying the transformation:\",train_df_cont[each_column].skew())\n",
        " \n",
        "    train_df_cont[each_column] = np.log1p(train_df_cont[each_column])\n",
        "    print (\"The skew in\", each_column ,\"after applying the transformation:\",train_df_cont[each_column].skew())\n",
        "    print ('\\t')\n",
        "    \n",
        "#You can see the reduction is skewness, but still the features 'cont5','cont7','cont9' have high skewness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "24717e9c-ef30-74c6-5c1b-732ef2f8b92e"
      },
      "outputs": [],
      "source": [
        "#Multivariate Data Analysis:\n",
        "#lets analyze and intrepret a key factor in the training data set, correlation between the features.\n",
        "#Stating the general intitution, for better performance of a machine learning algorithm, the features has to be highly\n",
        "#correalted with the class variable and ideally uncorrealted with the other features in the feature set.\n",
        "#Lets perform the correaltion analysis to check if the above mentioned condition holds for our training data.\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#creating a dataframe for storing the correaltion values between each continuous features. Could use any other data \n",
        "#structure as well, using pandas the code for analysis could be concise.\n",
        "train_df_cont_corr = pd.DataFrame()\n",
        "#populating the created dataframe\n",
        "train_df_cont_corr = train_df_cont.corr()\n",
        "plt.subplots(figsize=(21, 11))\n",
        "sns.heatmap(train_df_cont_corr,annot=True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#Now that we have a visual image of the correaltion between the continuous features, lets get the list of columns \n",
        "#which has correaltion above the specified limit of 0.6 in the heatmap.\n",
        "\n",
        "for index,rows in train_df_cont_corr.iterrows():\n",
        "    for each_column in train_df_cont_corr.columns:\n",
        "        if rows[each_column] > 0.6 and rows[each_column] != 1:\n",
        "            print (\"The correaltion of continous features:\", index, \"and\",each_column,\"is\", rows[each_column])\n",
        "\n",
        "#Now we have the list of features which has inter feature correaltion above 0.6. I have added the condition '!= 1'\n",
        "#beacuse it is clear from the heat map that a feature has a correlation of 1 only with itself, hence removing the \n",
        "#redundant information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "21c90d00-3c59-07c5-2666-a224145dc0ac"
      },
      "outputs": [],
      "source": [
        "#If a feature's value are all same, it cannot give us extra information. Hence finding the vatriance in data for \n",
        "#all the continuous features\n",
        "vt = VarianceThreshold()\n",
        "xt = vt.fit_transform(train_df_cont)\n",
        "variance_dict = dict(zip(train_df_cont.columns,vt.variances_))\n",
        "print (variance_dict)\n",
        "print (min(variance_dict, key = variance_dict.get))\n",
        "\n",
        "#Result: Found that the feature 'cont7' has the least variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3218c514-05ee-250b-327b-8f523b25ade3"
      },
      "outputs": [],
      "source": [
        "#Let us analyse the feature importance of the continuous features.\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "train_df_cont['loss'] = train_df['loss']\n",
        "array = train_df_cont.values\n",
        "\n",
        "\n",
        "X = array[:,0:14]\n",
        "Y = array[:,14].astype(int)\n",
        "\n",
        "test = SelectKBest(score_func=f_regression, k=4)\n",
        "fit = test.fit(X, Y)\n",
        "print (\"The feature importance for the various continuous features are:\")\n",
        "d = dict(zip(train_df_cont.columns,fit.scores_))\n",
        "sorted(d.items(), key=lambda x: (-x[1], x[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d16083a-0396-8d7b-529e-c7676f22b0d0"
      },
      "outputs": [],
      "source": [
        "#Now that, we have pre processed the continuous features lets analyze the categorical data.\n",
        "#Intially we will see the number of categories in each categorical feature\n",
        "\n",
        "category_list = []\n",
        "for each_column in train_df_cat.columns:\n",
        "    print (\"The number of categories in the feature\",each_column,\"are:\",len(train_df_cat[each_column].unique()))\n",
        "\n",
        "#Now we know the number of categories in each categorical feature, for better performance of the algorithm \n",
        "#we will encode categories with numerical value"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}