{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom time import time\nimport seaborn as sns\n\ninputTrain = pd.read_csv('../input/train.csv')\ninputTest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ae474557bb0cc449e8bf7f0ac507fd53e1ca911"},"cell_type":"markdown","source":"# Preparing Data"},{"metadata":{"trusted":true,"_uuid":"43bed2c39d87bc88297950b1c922a947dc349624"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\ndef oversampling(train):\n    target = train['target']\n    train = train.drop(['target'], axis=1)\n    train['target'] = target\n    features, target = SMOTE(n_jobs=-1, random_state=42).fit_sample(train.drop(['target'], axis=1), train['target'])\n    target[target >= 0.5] = 1\n    target[target < 0.5] = 0\n    finalArray = np.column_stack((features, target))\n    columns = train.columns.copy()\n    train = pd.DataFrame(finalArray, columns=columns).reset_index(drop=True)\n    return train\n\ndef replacingMissingValues(dataset):\n    for i in dataset.columns:\n        if dataset[i].isnull().sum() > 0:\n            print(i,\" => \",dataset[i].isnull().sum())\n            dataset[i].fillna(np.mean(dataset[i]), inplace=True)\n    return dataset \ndef dropColumns(data):\n    dropColumns = []\n    if 'ID_code' in data.columns:\n        dropColumns.append('ID_code')\n    if 'target' in data.columns:\n        dropColumns.append('target')\n    print('droped columns ',dropColumns)\n    features = data.drop(dropColumns, axis = 1)   \n    return features\ndef getCategories(features):\n    categories = []\n    for category in categories: \n        features[category] = features[category].astype(\"category\").cat.codes\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cad3a12a29c213aabd1b901b104c3a209eb0831b"},"cell_type":"code","source":"def getTarget(data):\n    return data['target']\ndef getFeatures(data):\n    features = dropColumns(data)\n    features  = getCategories(features)\n    replacingMissingValues(features)\n    return features\ndef prepareData(data):\n    target = getTarget(data)\n    features = getFeatures(data)\n    print(features.shape)\n    return features,target","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d47c460c72b81a408954ba579a51cbe48db8756b"},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"997fadf7b206e524e6c6f4b2a2ee0db7ab66424d"},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import f1_score\n   \ndef train_predict(clf, X_train, y_train, X_test, y_test):\n    print(\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(features)\n    return clf.score(features, y_pred)     \ndef split_data(features):\n    n_splits = 15\n    folds = KFold(n_splits=n_splits, random_state=42)\n    return enumerate(folds.split(features))\ndef training(clf, features, target):\n    score = 0\n    start_fold = time()\n    for fold, (train_idx, test_idx) in split_data(features):\n        print(\"\\nFold \", fold)\n        X_train = features.iloc[train_idx]\n        y_train = target.iloc[train_idx]\n        X_test = features.iloc[test_idx]\n        y_test = target.iloc[test_idx]            \n        score = train_predict(clf, X_train, y_train, X_test, y_test)\n        print(score)        \n    end_fold = time()\n    print('Training folds in {:.4f}'.format(end_fold - start_fold))\n    return score \ndef training_lgm( features, target):\n    param = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.38,\n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.045,\n        'learning_rate': 0.01,\n        'max_depth': -1,  \n        'metric':'auc',\n        'min_data_in_leaf': 80,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 13,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }\n    start_fold = time()\n    for fold, (train_idx, test_idx) in split_data(features):\n        print(\"\\nFold \", fold)\n        X_train = features.iloc[train_idx]\n        y_train = target.iloc[train_idx]\n        X_test = features.iloc[test_idx]\n        y_test = target.iloc[test_idx]            \n        train_data = lightgbm.Dataset(X_train, label=y_train)\n        test_data = lightgbm.Dataset(X_test, label=y_test)\n        model = lightgbm.train(param,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=5000,\n                       early_stopping_rounds=100)        \n    end_fold = time()\n    print('Training folds in {:.4f}'.format(end_fold - start_fold))        \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"730b4912247295d36e5765fd539504123547c9f2"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"d206a94b465f87409ce164cbc2fa88d9772d873c"},"cell_type":"code","source":"def submissionFile(clf):\n    print('Creating submission file')\n    columnId = 'ID_code'\n    columnTarget = 'target'\n    sub = pd.DataFrame(inputTest[columnId], columns=[columnId,columnTarget])\n    features=getFeatures(inputTest)\n    pred = clf.predict(features) \n    sub[columnTarget]=pred\n    print('submit_{}.csv'.format(clf.__class__.__name__))\n    sub.to_csv('submit_{}.csv'.format(clf.__class__.__name__), index=False)\ndef submission2(predictions):\n    submission = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n    submission[\"target\"] = predictions\n    submission.to_csv(\"submission_lightgbm.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7d3cbf6112aaa6db3c17ca61a16c4643206a009"},"cell_type":"markdown","source":"# Main"},{"metadata":{"trusted":true,"_uuid":"5c433592914a5de073515b29721974a3acd073ae"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom xgboost import XGBClassifier\nimport lightgbm\n\nclfs = [\n    lightgbm\n#    LogisticRegression(),\n#    XGBClassifier(silent=False,scale_pos_weight=1,learning_rate=0.01,colsample_bytree = 0.4,subsample = 0.8,objective='binary:logistic',n_estimators=1000,reg_alpha = 0.3,max_depth=4,gamma=10)\n]\nprint(\"Starting\")\nstart_init = time()\n\ntrain = inputTrain.drop('ID_code', axis=1)\nprint(\"Starting oversampling\")\ntrain2 = oversampling(train)\nend_over = time()\nprint(\"Ending oversampling in {}\",(end_over - start_init))\n\nfeatures, target = prepareData(train2)\nfor clf in clfs:\n   model=training_lgm(features, target)\n   submissionFile(model)\n   #clf=tuning(clf,features, target, score)    \n   #submissionFile(clf)\nend_init = time()\nprint(\"Finished in {:.4f} seconds\".format(end_init - start_init))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}