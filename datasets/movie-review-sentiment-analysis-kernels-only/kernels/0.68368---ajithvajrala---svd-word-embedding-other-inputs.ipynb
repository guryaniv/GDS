{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nimport re\nimport string\nfrom sklearn.preprocessing import OneHotEncoder\nimport textblob\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import *\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\", sep=\"\\t\")\ntest_df = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\", sep=\"\\t\")\npos_df = pd.read_csv(\"../input/pos-neg-files/positive words.txt\", sep=\"\\n\", header=None)\nneg_df = pd.read_csv(\"../input/pos-neg-files/Negative words.txt\", sep=\"\\n\", header=None, encoding = \"ISO-8859-1\")\npos_df.columns = ['words']\nneg_df.columns = ['words']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dc822a5fb1d5a44e62d6825f50524c5e6018f54"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a07b394557d9238636a5721b28599a8f504eb15d"},"cell_type":"code","source":"print(train_df['Sentiment'].value_counts())\nsns.countplot(train_df['Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b19ce526206c4c2664700ec6d4739dccc453729"},"cell_type":"code","source":"train_df['Phrase'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2aefeb8420b2356949f198b286f4461a1b1050ff"},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\nfor df in [train_df, test_df]:\n    df['words_length'] = df['Phrase'].apply(lambda x: len(x))\n    df['sent_length'] = df['Phrase'].apply(lambda x: len(word_tokenize(x)))\n    df['no_stops'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w in stop_words]))\n    df['no_non_stops'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w not in stop_words]))\n    df['no_punctuations'] = df['Phrase'].apply(lambda x: \n                                               len([w for w in word_tokenize(x.lower()) if w in string.punctuation if w not in \".\" \n                                                   if w not in \",\"]))\n    \n    df['pos_words'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w in pos_df.words.values]))\n    df['neg_words'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w in neg_df.words.values]))\n    df['neutral_words'] = df['Phrase'].apply(lambda x: len([w for w in word_tokenize(x.lower()) if w not in neg_df.words.values\n                                                           if w not in pos_df.words.values]))\n    df[\"Phrase\"] = df[\"Phrase\"].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6f20e0df06edd6b54e4838fc246909dc923b78a"},"cell_type":"code","source":"train_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39fa24557ed95049d4b2e7a54546c4556a98f5cc"},"cell_type":"code","source":"dense_features = [\"words_length\", \"sent_length\", \"no_stops\", \"no_non_stops\", \"no_punctuations\", \"pos_words\", \"neg_words\", \"neutral_words\"]\ntrain_df.groupby(\"Sentiment\")[dense_features].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd22c71e796cf96d3dceb8875583628707138582"},"cell_type":"code","source":"tvec = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\ntvec.fit(np.append(train_df[\"Phrase\"].values, test_df[\"Phrase\"].values))\n\nx_train_tfidf = tvec.transform(train_df[\"Phrase\"].values)\nx_test_tfidf = tvec.transform(test_df[\"Phrase\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9073a597116be88bca0bd3ce248c2b7a2a6bbff4"},"cell_type":"code","source":"print(x_train_tfidf.shape)\nprint(x_test_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6f9893296974d28cd46e37a7fc6136a5151c9c1"},"cell_type":"code","source":"svd = TruncatedSVD(n_components = 20)\nx_train_svd = svd.fit_transform(x_train_tfidf)\nx_test_svd = svd.fit_transform(x_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3f2b1273275616f0e1890f24d73cc12578b0dd5"},"cell_type":"code","source":"print(x_train_svd.shape)\nprint(x_test_svd.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bc52fae4b38956c1aaabac7d1a0d5e40b64da10"},"cell_type":"code","source":"enc = OneHotEncoder(sparse=False)\nenc.fit(train_df[\"Sentiment\"].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71d667673c1461ce83bd9462ccdad59b8c775080"},"cell_type":"code","source":"print(\"train set: {0:.2f}\".format(len(set(train_df[\"Phrase\"]).intersection(set(test_df[\"Phrase\"])))/test_df.shape[0]))\ntest_new = pd.merge(test_df, train_df[[\"Phrase\", \"Sentiment\"]], on=\"Phrase\", how=\"left\")\ncv1 = CountVectorizer()\ncv1.fit(train_df[\"Phrase\"])\ncv2 = CountVectorizer()\ncv2.fit(test_df[\"Phrase\"])\nprint(\"Train Set Vocabulary Size:\", len(cv1.vocabulary_))\nprint(\"Test Set Vocabulary Size:\", len(cv2.vocabulary_))\nprint(\"Number of Words that occur in both:\", \n      len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys()))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9f029cbb344f2868663fce6f97d1827a0e0ee99"},"cell_type":"code","source":"print(\"total length of test data: %d\"%(len(test_df)))\nprint(\"No of Phrases in test which are in train: %d\"%(len(test_new) - (test_new['Sentiment'].isna().sum())))\nprint(\"No of Phrases in test which are not in train: %d\"%(test_new['Sentiment'].isna().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db4889924e701d016b1775d632d724eb310818aa"},"cell_type":"code","source":"NUM_FOLDS = 5\ntrain_df[\"fold_id\"] = train_df[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)\nEMBEDDING_FILE = \"../input/glove6b100dtxt//glove.6B.100d.txt\"\nEMBEDDING_DIM = 100\nall_words = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f589afc61120b579ec50e58643620334fcdecebe"},"cell_type":"code","source":"def get_embedding():\n    embeddings_index = {}\n    f = open(EMBEDDING_FILE)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n            coefs = np.asarray(values[1:], dtype=\"float32\")\n            embeddings_index[word] = coefs\n    f.close()\n    return embeddings_index\n\nembeddings_index = get_embedding()\nprint(\"Number of words that don't exist in GLOVE:\", len(all_words - set(embeddings_index)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f64b460ed151f8f5219fce78b653b5b9407571c8"},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 60\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(np.append(train_df[\"Phrase\"].values, test_new[\"Phrase\"].values))\nword_index = tokenizer.word_index\n\nnb_words = len(word_index) + 1\nembedding_matrix = np.random.rand(nb_words, EMBEDDING_DIM + 2)\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    sent = textblob.TextBlob(word).sentiment\n    if embedding_vector is not None:\n        embedding_matrix[i] = np.append(embedding_vector, [sent.polarity, sent.subjectivity])\n    else:\n        embedding_matrix[i, -2:] = [sent.polarity, sent.subjectivity]\n        \nold_seq = pad_sequences(tokenizer.texts_to_sequences(train_df[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\ntest_seq = pad_sequences(tokenizer.texts_to_sequences(test_new[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db31c24c00d2965671659d2a533a1258ab584952"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"829e4b48ebc5368b1a658e4d32931e425d38e047"},"cell_type":"code","source":"def build_model():\n    embedding_layer = Embedding(nb_words,\n                                EMBEDDING_DIM + 2,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True)\n    dropout = SpatialDropout1D(0.2)\n    mask_layer = Masking()\n    lstm_layer = LSTM(100)\n    \n    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n    dense_input = Input(shape=(len(dense_features),))\n    svd_input = Input(shape=(20,))\n    \n    dense_vector = BatchNormalization()(dense_input)\n    svd_vector = BatchNormalization()(svd_input)\n    \n    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n    \n    feature_vector = concatenate([phrase_vector, dense_vector, svd_vector ])\n    feature_vector = Dense(128, activation=\"relu\")(feature_vector)\n    feature_vector = Dense(128, activation=\"relu\")(feature_vector)\n    feature_vector = Dense(64, activation=\"relu\")(feature_vector)\n    \n    output = Dense(5, activation=\"softmax\")(feature_vector)\n    \n    model = Model(inputs=[seq_input, dense_input, svd_input], outputs=output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b5d6c5809cc2b5ba820fc96541422347e4c122f"},"cell_type":"code","source":"test_preds = np.zeros((test_new.shape[0], 5))\n\nfor i in range(NUM_FOLDS):\n    print(\"FOLD\", i+1)\n    \n    print(\"Splitting the data into train and validation...\")\n    train_seq, val_seq = old_seq[train_df[\"fold_id\"] != i], old_seq[train_df[\"fold_id\"] == i]\n    train_dense, val_dense = train_df[train_df[\"fold_id\"] != i][dense_features], train_df[train_df[\"fold_id\"] == i][dense_features]\n    train_svd, val_svd = x_train_svd[train_df[\"fold_id\"] != i], x_train_svd[train_df[\"fold_id\"] == i]\n    \n    y_train = enc.transform(train_df[train_df[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n    y_val = enc.transform(train_df[train_df[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))\n    \n    print(\"Building the model\")\n    model = build_model()\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n    \n    print(\"Training the model\")\n    model.fit([train_seq, train_dense, train_svd], y_train, validation_data=([val_seq, val_dense, val_svd], y_val),epochs=15,\n              batch_size=1024, shuffle=True, callbacks=[early_stopping], verbose=1)\n    \n    print(\"Predicting...\")\n    test_preds += model.predict([test_seq, test_new[dense_features], x_test_svd], batch_size=1024, verbose=5)\n    print()\n    \ntest_preds /= NUM_FOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"055dc6001806379b34c03c409658e2c87607e62a"},"cell_type":"code","source":"test_new[\"pred\"] = test_preds.argmax(axis=1)\ntest_new.loc[test_new[\"Sentiment\"].isnull(), \"Sentiment\"] = test_new.loc[test_new[\"Sentiment\"].isnull(), \"pred\"]\ntest_new[\"Sentiment\"] = test_new[\"Sentiment\"].astype(int)\ntest_new[[\"PhraseId\", \"Sentiment\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bea111767c10d9b868e1055c6cbe10e5201cab5"},"cell_type":"markdown","source":"reference: https://www.kaggle.com/shubhammank/movie-sentiment"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}