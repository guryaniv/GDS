{"cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "cac40ef8-33e8-489d-aa25-be5570103252", "_uuid": "22719b2ecfce733d4bf99d3fb44c0f51c2eb106c"}, "source": ["This kernel started as a demonstration of the impact of rounding the prediction to integers using some realistic examples (in line with the opinions stated in the Discussion (https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/38727). In this version I used this cross-validation framework to explore the popular ideas (such as the use of the median of medians (MM) by Ehsan and the use of weekly seasonality by Clustifier (WK). The example as published here - a simple combination of both ideas - leads to LB 44.5 and with one small unpublished modification to 44.0 (top 3% as of yesterday). Both MM and WK are reasonably robust on all sets where I tested it  though I would not be suprised to see the score deteriorate to 46 in the future as it happened on some previous 60 days sets.\n", "    \n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a8684c22-abe0-40e3-9666-886b2bcadd2d", "_uuid": "8da7dcf84b6159626b61fdca27b30605414f3d35"}, "source": ["The idea is to split the 551 days that were given to us into 60-day segments and then use each 60-day segment as a validation for prediction trained on the previous sets. For example use the last 60 days for testing and previous 490 days for traing, next use the last 120 to 60 days for testing and the previous 430 for traing etc. Furthermore we can also remove any smaller number of days from the end of the dataset (eg the last 10 and then repeat the exercise)\n", "\n", "The botom line: a) rounding matters, b) median of medians is beter than just medians, c) weekly seasonality matters\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "c2394042-9b4f-4e23-a199-d944e5220851", "_uuid": "cc95911b3903cf4de5c9fd7b62d17ccabd7caa90"}, "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import gc\n", "\n", "train = pd.read_csv('../input/train_1.csv', usecols=[0]+list(range(1,551)) ) # select a subset \n", "train.fillna(0, inplace=True)\n", "train_flattened60 = pd.melt(train[list(train.columns[-60:])+['Page']], id_vars='Page', var_name='D60', value_name='V60')\n", "train_flattened120 = pd.melt(train[list(train.columns[-120:-60])+['Page']], id_vars='Page', var_name='D120', value_name='V120')\n", "train_flattened180 = pd.melt(train[list(train.columns[-180:-120])+['Page']], id_vars='Page', var_name='D180', value_name='V180')\n", "train_flattened120.drop(['Page'],axis=1,inplace=True)\n", "train_flattened180.drop(['Page'],axis=1,inplace=True)\n", "test = pd.concat([train_flattened60, train_flattened120, train_flattened180], axis=1, join_axes=[train_flattened60.index])\n", "del train_flattened60, train_flattened120, train_flattened180\n", "test.head(2)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5b3ee332-5451-4fd0-b844-47cf929e725b", "_uuid": "09eb7006737081862f190640f537ce30f6af3a42"}, "source": ["# create some means (m), medians (M), median of medians (MofM), means of medians (mofM) etc \n", "# predictions (with different time windows):\n", "\n", "Windows= [7,14,28,35,42,49,56]\n", "tmp=train['Page'].to_frame()\n", "for i in Windows: tmp['M'+str(i)]=train.iloc[:,-i-60:-60].median(axis=1)\n", "tmp['MofM']=tmp.iloc[:,1:].median(axis=1) \n", "tmp['mofM']=tmp.iloc[:,1:-1].mean(axis=1) \n", "for i in [7,14]: tmp['m'+str(i)]=train.iloc[:,-i-60:-60].mean(axis=1)\n", "test=test.merge(tmp, on = 'Page')\n", "del tmp\n", "test.head(2)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "cb7bd967-d48e-4d49-ba51-fca4d1301122", "collapsed": true, "_uuid": "bfc05338365c3e870df5849f64207ed836ccff84"}, "source": ["def smape(true,pred):\n", "    return 200.* (  (true-pred).abs()/(pred.abs()+true.abs()).replace({0:1}) ).mean()"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "73715c38-0299-4923-9d6a-2705ff32a046", "_uuid": "3dc40e43338f6b3b3ade9d80fc742de67653a704"}, "source": ["# Create sample predictions first as floating mumbers and again \n", "# as rounded to integers and then with ceiling  and floor\n", "# to be compared with the \"true\" set of the last 60 days\n", "\n", "predictions=['pred1','M14', 'M49', 'MofM',  'm7','m14','mofM']         # float\n", "true=test['V60'].astype(int)\n", "test['weight']=np.random.uniform(0,1,len(test))\n", "test['pred1']=test['V120']*test['weight']+test['V180']*(1-test['weight'])\n", "\n", "plt.clf()\n", "s=pd.DataFrame(0,index=predictions,columns=['float','round','ceil','floor'])\n", "for j in predictions:\n", "    Float = smape(true,test[j])\n", "    Round = smape(true,test[j].round().astype(int))\n", "    Ceil = smape(true,np.ceil(test[j]))\n", "    Floor = smape(true,np.floor(test[j]))\n", "    s.loc[j,['float','round', 'ceil', 'floor']] = (Float,Round,Ceil,Floor)\n", "    print('{:>7} is {:.2f} round {:.2f} ceil {:.2f} floor {:.2f} '.\n", "          format(str(j),Float,Round,Ceil,Floor))\n", "\n", "s.plot(kind='bar',ylim=(42,62), figsize=(10,6))\n", "plt.show()\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "dd963cbc-cfdf-4d2f-9512-a5f73324fbc0", "_uuid": "b3429c3e34c35fa2a503e722b645ce9ba87c3963"}, "source": ["Float = smape(true,test['mofM'])\n", "Round1 = smape(true,test['mofM'].round())\n", "Round2 = smape(true,test['mofM'].where(test['mofM']>=0.5,0))\n", "Round3 = smape(true,test['mofM'].where(test['mofM']>1,0))\n", "print(' float {:.2f} round {:.2f}  1/2: {:.2f}  1: {:.2f}'.format(Float, Round1,Round2,Round3))"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "f37dcaf4-c02b-40ab-8a8e-0fe13a51b530", "_uuid": "62a734f109d4de321f4e0f8fb1dcb5e33b956b65"}, "source": ["We see that rounding is usually better than taking the ceiling or floor and also that most of the gain is from replacing values between 0 and 1/2 by 0 since by definition smape is 0 for 0/0 (even though rounding still matters for numbers greater than 1 as well).\n", "\n", "Next we look at the weekly seasonality:"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5a405ac4-ee72-4e43-b9a8-7c60da747d7d", "collapsed": true, "_uuid": "0769a94a335f43214e09d429328ab034070b5c6c"}, "source": ["for i in ['D60','D120','D180']: test[i] = test[i].astype('datetime64[ns]')\n", "test['wk_60']= test.D60.dt.dayofweek >=5  \n", "test['wk_120']= test.D120.dt.dayofweek >=5  \n", "test['wk_180']= test.D180.dt.dayofweek >=5  "]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "dfd45ebe-43ff-4ab5-80e7-775697fb459d", "_uuid": "6d24e66bf4271a820ebc35f4459bc9e89e987f13"}, "source": ["colm=test.columns[7:7+len(Windows)]    # save the names of the median columns\n", "\n", "cv=60    # chose 60 or 120 or 180 \n", "if cv !=60:   \n", "    tmp=train['Page'].to_frame()\n", "    test1=test['Page'].to_frame()\n", "    for i in Windows: tmp['M'+str(i)]=train.iloc[:,-i-cv:-cv].median(axis=1)\n", "    test1=test1.merge(tmp, on = 'Page')\n", "    for i in Windows: test['M'+str(i)]=test1['M'+str(i)]\n", "    del tmp, test1\n", "\n", "colmw=[]\n", "for i in Windows:\n", "    print(cv,i, end=' ')\n", "    val='MW'+str(i)\n", "    colmw=colmw+[val]\n", "    tmp = pd.melt(train[list(train.columns[-i-cv:-cv])+['Page']], \n", "                  id_vars='Page', var_name='D', value_name=val)\n", "    tmp['D'] = tmp['D'].astype('datetime64[ns]')\n", "    tmp['wk_'+str(cv)]= tmp.D.dt.dayofweek  >= 5\n", "    tmp1 = tmp.groupby(['Page','wk_'+str(cv)]).median().reset_index()\n", "    test = test.merge(tmp1, how='left')\n", "\n", "print(test.shape)\n", "del tmp,tmp1\n", "gc.collect()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "06c90d62-25a9-43cf-be04-a62567de834a", "_uuid": "2c2e999d6e8e3cef7ff70cf49de4ccc67b79b6f5"}, "source": ["Let us plot the score for the median prediction with (MW) and without (M) grouping by weekends as well as the median of medians of the medians (MMW) that take into account the seasonality. The last one starts with taking just two medians and ends with taking all. The conclusion is that taking the median of the full set of medians with weekly seasonality is better. \n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0a99e81a-7799-4364-85d0-d8c00354e557", "_uuid": "3a8902c2e8e66f637965f1cc4ad51cad10db88be"}, "source": ["plt.clf()\n", "true=test['V'+str(cv)]\n", "smape_all=pd.DataFrame(0,index=colm,columns=[ 'M','MW','MMW'])\n", "for i in range(0,len(colm)) :   \n", "    smape_all.loc[colm[i],'M'] = smape(true,test[colm[i]])\n", "    smape_all.loc[colm[i],'MW'] = smape(true,test[colmw[i]])\n", "    smape_all.loc[colm[i],'MMW'] = smape(true,test.loc[:,colmw[0:i]].median(axis=1).round())        \n", "    print(' M smape from {:>7} is {:.2f} with wk {:.2f} and MM {:.2f}'.format(\n", "        colm[i], smape_all.loc[colm[i],'M'],smape_all.loc[colm[i],'MW']\n", "         ,smape_all.loc[colm[i],'MMW']))\n", "\n", "test['MMW']=test.loc[:,colmw].median(axis=1).round()\n", "smape_all.plot(ylim=(43,50), figsize=(10,6))\n", "plt.show()    \n", "gc.collect()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "7d1b1f7e-b723-49c8-9835-a428ad61744d", "_uuid": "afea20ed825f56eaee427be073a42ff1fd2bd12a"}, "source": ["And finally:"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "a0c0347d-e190-4a92-808c-26549c6a3b59", "collapsed": true, "_uuid": "cf838a9185e5c92510bd18631d2d2700bbdf9eec"}, "source": ["#this scores 44.5 on LB  (and with one small modification 44.0, ie top 3% as of yesterday)\n", "\n", "# define the Windows according to Ehsan's kernel\n", "r = 1.61803398875  \n", "Windows = np.round(r**np.arange(0,9) * 7).astype(int)\n", "\n", "test = pd.read_csv('../input/key_1.csv')\n", "test['Date'] = test.Page.apply(lambda x: x[-10:])\n", "test['Page'] = test.Page.apply(lambda x: x[:-11])\n", "test['Date'] = test['Date'].astype('datetime64[ns]')\n", "test['wk']= test.Date.dt.dayofweek >=5\n", "\n", "for i in Windows:\n", "    print(i,end= ' ')\n", "    val='MW'+str(i)\n", "    tmp = pd.melt(train[list(train.columns[-i:])+['Page']], \n", "                  id_vars='Page', var_name='D', value_name=val)\n", "    tmp['D'] = tmp['D'].astype('datetime64[ns]')\n", "    tmp['wk']= tmp.D.dt.dayofweek  >=5           \n", "    tmp1 = tmp.groupby(['Page','wk']).median().reset_index()\n", "    test = test.merge(tmp1, how='left')\n", "    \n", "test['Visits']=test.iloc[:,4:].median(axis=1).round().astype(int)\n", "test[['Id','Visits']].to_csv('sub.csv', index=False)\n", "gc.collect()"]}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.1", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}}