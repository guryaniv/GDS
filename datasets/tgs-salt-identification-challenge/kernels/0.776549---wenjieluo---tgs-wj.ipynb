{"cells":[{"metadata":{"_uuid":"20601468dd1d852564a67afc24dc6e1cce330604"},"cell_type":"markdown","source":"all 3*3-> 78.34% <br/>\npart 5*5-> 77.5%<br/>\nall 5*5 -> 75.8%<br/>\nbottleneck aplied -> 38%<br/>\nenlarge validation set with 3*3 filter -> 79.56%<br/>\nlovasz-hinge loss:  \nXception Encoder:\nDepth Information:(Dont know how"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization,Activation,Add\n\nfrom tqdm import tqdm_notebook\nfrom keras import backend as K\nfrom keras import optimizers\nimport time\nt_start = time.time()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ec7d794b351ba8ab710db3701fb1762a084c980"},"cell_type":"markdown","source":"Resize images from 101*101 to 128*128 and back"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"279dd7e2a743a00872b897050d3a6242084423f6"},"cell_type":"markdown","source":"Get Traning data"},{"metadata":{"_uuid":"f2c074402bc22ebd86b1210b435139819d7f5494","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\nprint(train_df.shape)\nprint(train_df.head)\ntrain_df = train_df.join(depths_df)\nprint(train_df.shape)\nprint(train_df.head)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\nprint(\"test_df\")\nprint(test_df.head)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95ce41b89ce1054e95035e1224c82b34051e5fac","trusted":true},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx),grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ea2be7b42539d9e49ed0f05e6a6319c683bf690","trusted":true},"cell_type":"code","source":"train_df[\"images\"][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9152ab66c7248d860c7232cf9ddd14a1cc174d6"},"cell_type":"code","source":"train_df[\"images\"].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6e5ed242ab437f5d6913f2e8066f8acb71abedf","trusted":true},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73b9ebfad4d830effd46854eb4751cfc3de4bc00"},"cell_type":"code","source":"#train_df[\"masks\"]=[np.array([i,i,i]) for i in train_df[\"masks\"]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f6b0b64ebb8fbc3f3f09eaa3b96676d02a688c2","trusted":true},"cell_type":"code","source":"print((train_df[\"masks\"][0][0][0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4db77f6b503a0550a4bf4cba2762c2ed3572a787","trusted":true},"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / 3/pow(img_size_ori, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5919c2b18d8f26912340164919931fa51a03fd09"},"cell_type":"code","source":"train_df[\"coverage\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"419f8a2a8670fb5c5760c7536a80ef2fa06f0465"},"cell_type":"markdown","source":"Divide data into different classes.(Used to divide data sets)"},{"metadata":{"_uuid":"c3b9e75ee6c94703edd1624953323e0cbb1354b0","trusted":true},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5399f3d7ce173d52a118c085e1e1e182f293eb1"},"cell_type":"markdown","source":"Split dataset into training data and validation data"},{"metadata":{"_uuid":"99aaa43dfeff63beed3cbf08b9b5c055c1296ddd","trusted":true},"cell_type":"code","source":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"628fc12463cfb184c9bd53c7d66a0c381078d5d5","trusted":true},"cell_type":"code","source":"x_train[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1290bc55f21e9bba560b0a0262b16580146954ec","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f88bd80e1f08957718c3ccf7a190190a50018704"},"cell_type":"markdown","source":"Simple Block for Resnet with batchnorm layer"},{"metadata":{"_uuid":"407f4f5ab644c7211f76dc9c743d68c2fd81dffa","trusted":true},"cell_type":"code","source":"def BatchActivate(x):\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = BatchActivate(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16, batch_activate = False):\n    x = BatchActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3))\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    if batch_activate:\n        x = BatchActivate(x)\n    return x\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64796ff082199979f7528a301d291852206dfba0"},"cell_type":"markdown","source":"Define the Structure of model"},{"metadata":{"_uuid":"0338008f2b057512fb5144d4173ee5720c28f62e","trusted":true},"cell_type":"code","source":"def build_model(input_layer, start_neurons):\n    # 128 -> 64\n    conv1 = Conv2D(start_neurons * 1 , (3,3), activation=None, padding=\"same\")(input_layer)\n    conv1 = residual_block(conv1,start_neurons * 1)\n    conv1 = residual_block(conv1,start_neurons * 1,True)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n\n    # 64 -> 32\n    conv2 = Conv2D(start_neurons * 2 , (3,3), activation=None, padding=\"same\")(pool1)\n    conv2 = residual_block(conv2,start_neurons * 2)\n    conv2 = residual_block(conv2,start_neurons * 2,True)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    # 32 -> 16\n    conv3 = Conv2D(start_neurons * 4 , (3,3), activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = residual_block(conv3,start_neurons * 4)\n    conv3 = residual_block(conv3,start_neurons * 4,True)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n\n    # 16 -> 8\n    conv4 = Conv2D(start_neurons * 8 , (3,3), activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = residual_block(conv4,start_neurons * 8)\n    conv4 = residual_block(conv4,start_neurons * 8,True)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16 , (3,3), activation=\"relu\", padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 16)\n    convm = residual_block(convm,start_neurons * 16,True)\n    #convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n\n    # 8 -> 16\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3,3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 8)\n    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n\n    # 16 -> 32\n    deconv3 = Conv2DTranspose(start_neurons * 4 , (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])\n    uconv3 = Dropout(0.5)(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3,3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 4)\n    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n\n\n    # 32 -> 64\n    deconv2 = Conv2DTranspose(start_neurons * 2 , (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n    uconv2 = Dropout(0.5)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3,3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 2)\n    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n\n    # 64 -> 128\n    deconv1 = Conv2DTranspose(start_neurons * 1 , (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    uconv1 = Dropout(0.5)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1 *4, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(start_neurons * 1 *4, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n\n    #uconv1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer\n\ninput_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc42378cd4ad8f82951aa8125e13e19495412226"},"cell_type":"code","source":"model = Model(input_layer, output_layer)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94ca21f67761afef724f65b9f316c0717ec44d8c"},"cell_type":"markdown","source":"Network Sturcture Visualization"},{"metadata":{"_uuid":"35aee80386fd1028b0f650b13f2876758d235853","trusted":true},"cell_type":"code","source":"from IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nplot_model(model, to_file='model.png',show_shapes=True)\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cab241d1efb5ffaca040def197a380e7004dbc8"},"cell_type":"markdown","source":"Data Argumentation and visualization"},{"metadata":{"_uuid":"6f00c817f26e1e6a24f9471907934120d7852f93","trusted":true},"cell_type":"code","source":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\nx_train = np.append(x_train, [np.flipud(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.flipud(x) for x in y_train], axis=0)\nx_valid = np.append(x_valid, [np.flipud(x) for x in x_valid], axis=0)\ny_valid = np.append(y_valid, [np.flipud(x) for x in y_valid], axis=0)\nx_valid = np.append(x_valid, [np.flipud(x) for x in x_valid], axis=0)\ny_valid = np.append(y_valid, [np.flipud(x) for x in y_valid], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc9f93eb52365dece6210407b407420e83bf9fc1","trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d3ed79270d0a9787bae10c5bca582adb5a3401d"},"cell_type":"markdown","source":"Train the model"},{"metadata":{"_uuid":"8fbdc2231a7bfca61080e7349171a82c41369390","trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\nepochs = 200\nbatch_size = 32\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36d24eac6502758d51d5167b12b0523b0fd49643","trusted":true},"cell_type":"code","source":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\nmodel = load_model(\"./keras.model\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2454517a7e3d64c07738256b2f14cd849562c19"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a665ca9005afbd6fba31afa2423daf14b808b857","trusted":true},"cell_type":"code","source":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daaf64803c4b83f4e5fe9de823759dfa7e00730a"},"cell_type":"markdown","source":"metric function used to adjust the model"},{"metadata":{"_uuid":"0ef99912d2f5f656c9c6305f05e7238ff991595a","trusted":true},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef61d622bb41c6d7c60c9c8864d9a45b7c544839"},"cell_type":"markdown","source":"Find best threshold based on IOU"},{"metadata":{"_uuid":"946463565a203a3322ee196086624aea4f830561","trusted":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73db638e89b222106647dd45ac60b481418e2d40","trusted":true},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e73816d15101e39e4b231c8814569d6e099e9a7d","trusted":true},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c8647bd9db52ca208caefa6c2db198ddf2b7b6f"},"cell_type":"markdown","source":"Submit to competition"},{"metadata":{"_uuid":"bed3ce8919d1c62bbcf5068129b2434efa59839f","trusted":true},"cell_type":"code","source":"# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c97b3e3b05a19ee18713ffc293eeb0a9b665fb7","trusted":true},"cell_type":"code","source":"x_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx),grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c2b6adbb0b2ff01cd22f1274f2599cf2b0132fa","trusted":true},"cell_type":"code","source":"preds_test = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56600a6e0661b48c7dbaa9c869a40c4f6d1d4d51","trusted":true},"cell_type":"code","source":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f8be95889e2e2c869f852fe8856eec464f3e7d0","trusted":true},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"747041addd36be06296781c4a4155832f3e0f048","trusted":true},"cell_type":"code","source":"t_finish = time.time()\nprint(f\"Kernel run time = {(t_finish-t_start)/3600} hours\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19c8f23f0b566f0038df4cadcc63f2aefa30db61","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c34774c1364dc5af96d10bc4fbaba117197100ce","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}