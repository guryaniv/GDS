{"cells":[{"metadata":{"_cell_guid":"c6cdd1a0-7a58-44d5-9bfc-26fca842dc5c","_uuid":"9a4c25138f04a9e158f62c858a99bb413d7f8f9c"},"cell_type":"markdown","source":"# Exhaustive Weather EDA/File Overview\n\n### Table of Contents\n* [Why Should You Trust Modified Store Info Files?](#Why-trust-modified-store-info)\n* [Files: air/hpg_store_info_with_nearest_active_station](#Store-info-with-nearest-active-station)\n* [Files: air/hpg_station_distances](#Station-distances)\n    * [Helper: select_stations()](#Helper-select-stations)\n* [File: feature_manifest](#Feature-manifest)\n    * [Weather Feature Coverage - Visualization](#Weather-feature-coverage)\n    * [Using feature_manifest with air/hpg_station_distances](#Feature-manifest-with-station-distances)\n    * [Helpers to Analyze Coverage](#Helpers-to-analyze-coverage)\n* [It's Gettin' Hot in Here (Station Coverage Heatmaps)](#Station-coverage-heatmaps)\n* [Filling in Missing Data](#Fill-in-missing-data)\n* [Combined Coverages by Prefecture](#Combined-coverages-by-prefecture)\n* [Closest Station Proximity - Visualization](#Closest-station-proximity)\n* [Distances Between Identical Stores](#Identical-stores-distances)\n* [Files: weather_stations, nearby_active_stations](#Nearby-active-weather-stations)\n    * [Mapping All Active Stations](#Mapping-all-active-stations)\n* [Weather Station Background](#Weather-station-background)\n\n# Introduction\nI'll be adding to this kernel continually, as it is absolutely still a work in progress.\n<br>\nMy aim in this kernel is to be as detailed as possible. If you'd like a quicker overview, take a look at [my other kernel.](https://www.kaggle.com/huntermcgushion/weather-station-location-eda)\n<br>\nIf you have any suggestions or catch me screwing up anywhere, I'd truly love to hear about it! Thanks for your time!"},{"metadata":{"_cell_guid":"af7e8576-07b3-4389-9126-78bf6b36e4fd","_uuid":"bf0ceb6949c66fb1be021b206b4f902ae49ad515","collapsed":true,"trusted":false},"cell_type":"code","source":"% matplotlib inline\n\n###############################################\n# Import Miscellaneous Assets\n###############################################\nimport numpy as np\nimport pandas as pd\nimport json\nfrom datetime import datetime\nfrom dateutil.parser import parse as date_parse\nfrom collections import Counter\nfrom pprint import pprint as pp\nfrom tqdm import tqdm, tqdm_notebook\nfrom IPython.display import display\nimport warnings\nimport os\nimport sys\nfrom geopy.distance import vincenty, great_circle\n\n###############################################\n# Import Plotting Assets\n###############################################\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.ticker as ticker\nfrom matplotlib.pyplot import subplot, figure\nimport seaborn as sns\nimport folium\nfrom folium import plugins as folium_plugins\nfrom folium import features\n\n###############################################\n# Declare Global Variables\n###############################################\nplt.interactive(False)\nsns.set_style('whitegrid')\npd.set_option('display.expand_frame_repr', False)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nweather_set_dir = '../input/rrv-weather-data'\nweather_data_dir = '{}/1-1-16_5-31-17_Weather/1-1-16_5-31-17_Weather'.format(weather_set_dir)\noriginal_data_dir = '../input/recruit-restaurant-visitor-forecasting'\n\nweather_columns = [\n    'avg_temperature', 'high_temperature', 'low_temperature', 'precipitation',\n    'hours_sunlight', 'solar_radiation', 'deepest_snowfall', 'total_snowfall', 'avg_wind_speed',\n    'avg_vapor_pressure', 'avg_local_pressure', 'avg_humidity', 'avg_sea_pressure', \n    'cloud_cover'\n]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa00f884-f96e-4676-a259-7de43b92398c","_uuid":"2a504c7b0b02b6102bf65f033aaa7f843c948a22","collapsed":true,"trusted":false},"cell_type":"code","source":"air_store_info = pd.read_csv('{}/air_store_info_with_nearest_active_station.csv'.format(weather_set_dir))\nhpg_store_info = pd.read_csv('{}/hpg_store_info_with_nearest_active_station.csv'.format(weather_set_dir))\n\nair_station_distances = pd.read_csv('{}/air_station_distances.csv'.format(weather_set_dir))\nhpg_station_distances = pd.read_csv('{}/hpg_station_distances.csv'.format(weather_set_dir))\n\nweather_stations = pd.read_csv('{}/weather_stations.csv'.format(weather_set_dir))\nnearby_active_stations = pd.read_csv('{}/nearby_active_stations.csv'.format(weather_set_dir))\nfeature_manifest = pd.read_csv('{}/feature_manifest.csv'.format(weather_set_dir))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b37780f5-1295-475a-8278-af0369b86842","_uuid":"c1ce9c630ad3b986dcc0a016fbebafc34a7d69dd"},"cell_type":"markdown","source":"# Why Should You Trust Modified Store Info Files?\n<a id=\"Why-trust-modified-store-info\"></a>\n\nBefore now, you probably shouldn't. After all, I'm asking you to use my versions of the air_store_info and hpg_store_info files.\n<br>\nThat sounds pretty scary, but let me show you why you can trust these files."},{"metadata":{"_cell_guid":"5a456f03-c246-4590-9322-6e8b2963427b","_uuid":"248e4e26b7220bd78c3908ca1645e015f165e6db","collapsed":true,"trusted":false},"cell_type":"code","source":"original_as_info = pd.read_csv('{}/air_store_info.csv'.format(original_data_dir))\noriginal_hs_info = pd.read_csv('{}/hpg_store_info.csv'.format(original_data_dir))\n\ndisplay(original_as_info.head(5))\ndisplay(original_hs_info.head(5))\n\nprint('Air Equal: {}'.format(original_as_info.equals(air_store_info[original_as_info.columns])))\nprint('HPG Equal: {}'.format(original_hs_info.equals(hpg_store_info[original_hs_info.columns])))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"969faedb-6700-4be9-bf9d-530d9c10a87e","_uuid":"fc38b65ab5b15befc8775bca652626b6b9e56eae"},"cell_type":"markdown","source":"Now, we're sure that the original store_info files are equal to the appropriate columns in the modified files\n<br>\nThe new files are just adding a few columns to the existing files.\n\n# About air/hpg_store_info_with_nearest_active_station?\n<a id=\"Store-info-with-nearest-active-station\"></a>\n\nWe'll just look at hpg_store_info because all the same stuff gets added to air_store_info."},{"metadata":{"_cell_guid":"2e0971b2-8aaa-40a5-9ea5-ca0b137fe0fc","_uuid":"87dfbc4ff4ef7184f0c5a3e9b44d80428bf96fa4","collapsed":true,"trusted":false},"cell_type":"code","source":"display(hpg_store_info.head())\nprint('These columns are added:')\npp([_ for _ in hpg_store_info.columns if _ not in original_hs_info.columns])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"000e386a-6931-4148-86ea-7ff7dcc80a1e","_uuid":"177700ecd949ac34ae635ddec148c56154dc7f76"},"cell_type":"markdown","source":"The 'latitude_str' and 'longitude_str' columns are just string versions of 'latitude' and 'longitude', but we need them to preserve the precision of those values so we can use the ' &lt;air/hpg&gt;station_distances' files.\n\n# About air_station_distances and hpg_station_distances\n<a id=\"Station-distances\"></a>\nAgain, we'll just look at hpg_station_distances, as with hpg_store_info above."},{"metadata":{"_cell_guid":"f75e0980-3106-4eb2-93eb-718f63186b81","_uuid":"b9c78ce5cf4912b7203d7ab7b23332ff3de910e6","collapsed":true,"trusted":false},"cell_type":"code","source":"print(hpg_station_distances.shape)\ndisplay(hpg_station_distances.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ae3e489-d1f7-4ebb-934f-853348d02f48","_uuid":"bca21627d2d18ac8d3152a3bb810811c7974aeec"},"cell_type":"markdown","source":"We can see that there are 1,663 rows, one for every weather file in the '1-1-16_5-31-17_Weather' directory.\n<br>\nFor each row, we have the 'station_id', which is also the name of the corresponding weather file.\n<br>\nWe have 'station_latitude' and 'station_longitude', and the remaining 129 columns are all of the unique coordinates in 'hpg_store_info'. \n<br>\nThese 129 columns are what the 'latitude_str' and 'longitude_str' [columns in this section are for.](#Store-info-with-nearest-active-station)\n\nLet's look at the first store in 'hpg_store_info' (hpg_6622b62385aec8bf), which is also represented by the above column in 'hpg_station_distances' immediately after 'station_longitude' (\"(35.6436746642265, 139.668220854814)\")."},{"metadata":{"_cell_guid":"f18b8fa1-515b-4dcc-8aba-52707b67c993","_uuid":"7cf504e6337772ff3964bfeae45e265f0e7b146b","collapsed":true,"trusted":false},"cell_type":"code","source":"store = hpg_store_info.iloc[0]\nlat_str, lon_str = store['latitude_str'], store['longitude_str']\nlookup_coords = '({}, {})'.format(lat_str, lon_str).replace('\"', '')\nprint(lookup_coords)\n\ndistances = hpg_station_distances[lookup_coords]\nprint(distances.values[:5])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f087cc5-818b-425f-ac74-f9c855e1c9ff","_uuid":"132f843980ee572c2ad792c119dfc04ea8e819e5"},"cell_type":"markdown","source":"As expected, these first five values mirror the first five values of hpg_station_distances['(35.6436746642265, 139.668220854814)'].\n<br>\nLet's keep going."},{"metadata":{"_cell_guid":"bffb49f7-fd98-4c0c-81c5-a0bebeb16037","_uuid":"81d94f6125ed62f529e91803b3f61e32c1832014","collapsed":true,"trusted":false},"cell_type":"code","source":"closest_station_distance = distances.min()\nprint('Distance to Closest Station: {} km'.format(closest_station_distance))\n\nids = hpg_station_distances['station_id'].values\nclosest_station_id = ids[distances.tolist().index(closest_station_distance)]\nprint('Closest Station ID: {}'.format(closest_station_id))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c11cd411-21e7-44d0-aa4d-5634e129ba24","_uuid":"a2903cfd3db8e4d4ffdd5dc271551a96147f924c"},"cell_type":"markdown","source":"These are the same values we see in the first row of 'hpg_store_info' for 'station_id' and 'station_vincenty' [in this section.](#Store-info-with-nearest-active-station)\n<br>\nBut what if we wanted weather data from all stations in a 17 kilometer radius around this store?\n<br>\nNote: I'll go into why 17 km might be a useful distance further below.\n\n### Helper: select_stations()\n<a id=\"Helper-select-stations\"></a>"},{"metadata":{"code_folding":[],"_cell_guid":"4505d6dd-4df9-4998-b379-f0ff06796cc1","_uuid":"3876f6c2b14278213637e1fc3b4b3f99463c0373","collapsed":true,"trusted":false},"cell_type":"code","source":"stations_in_range = [(ids[_], distances[_]) for _ in range(len(distances)) if distances[_] <= 17]\nstations_in_range = sorted(stations_in_range, key=lambda _: _[1], reverse=False)\npp(stations_in_range)\n\ndef select_stations(latitude_str, longitude_str, distance_df, effective_range=17.0, date_floor=None, top_n=None):\n    \"\"\"\n    Filters stations based on proximity to coordinates, and termination status\n    Note: if longitude_str is None, the first argument is assumed to be a properly formatted coordinate string\n    :param latitude_str: latitude_str from air/hpg_store_info_with_nearest_active_station\n    :param longitude_str: longitude_str from air/hpg_store_info_with_nearest_active_station\n    :param distance_df: one of the following DFs: air_station_distances, hpg_station_distances\n    :param effective_range: float in kilometers specifying the max distance a station can be from the store\n    :param date_floor: if datetime, remove stations terminated before date_floor. If None, ignore termination\n    :param top_n: if int, return at most top_n many stations. If None, all stations will be returned\n    :returns: a list of tuples of (station_id, distance) that meet the given specifications - sorted by distance\n    \"\"\"\n    if longitude_str is not None:\n        _lookup_coords = '({}, {})'.format(latitude_str, longitude_str).replace('\"', '')\n    else:\n        _lookup_coords = latitude_str\n        \n    _ids, _distances = distance_df['station_id'].values, distance_df[_lookup_coords]\n    _result = [(_ids[_], _distances[_]) for _ in range(len(_ids)) if _distances[_] <= effective_range]\n    \n    if date_floor is not None and isinstance(date_floor, datetime):\n        _result = [_ for _ in _result if '____' not in _[0] or date_parse(_[0].split('____')[1]) > date_floor]\n\n    return sorted(_result, key=lambda _: _[1])[:top_n]\n\n_test_0 = select_stations(lat_str, lon_str, hpg_station_distances)\n_test_1 = select_stations(lat_str, lon_str, hpg_station_distances, date_floor=date_parse('2017-5-31'))\n_test_2 = select_stations(lat_str, lon_str, hpg_station_distances, date_floor=date_parse('2017-5-31'), top_n=2)\n_test_3 = select_stations(lat_str, lon_str, hpg_station_distances, date_floor=date_parse('1975-12-13'))\n\nassert(_test_0 == stations_in_range)\nassert(_test_1 == stations_in_range[:4] + [stations_in_range[-1]])\nassert(_test_2 == stations_in_range[:2])\nassert(_test_3 == stations_in_range)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed85fecb-2fc7-4387-9732-f2aeb5880aee","_uuid":"5ff30e9b95f9a860d73a54c8d6dcfc14f2ffe7ee"},"cell_type":"markdown","source":"Suddenly, we have 6 different stations that we can use data from for a single store.\n<br>\nWell actually its 5, because 'tokyo\\_\\_chofu-kana\\_\\_NONE\\_\\_\\_\\_1976-12-13' was terminated way back in 1976, which is before 2016.\n\nNow that we know what's going on behind the scenes, we'll make the helper function ```select_stations``` for us to use later.\n<br>\nTo quickly test ```select_stations``` and provide some examples, we'll set up a few simple assertion tests. \n<br>\nWe don't get any AssertionErrors, so it seems to be working.\n\nNow, let's move on to how to use 'feature_manifest'.\n\n# About feature_manifest\n<a id=\"Feature-manifest\"></a>\n\nThis file is very useful if you're concerned about a station having too many null values, or the fact that it only records precipitation.\n<br>\nAnd we should all be worried about that..."},{"metadata":{"_cell_guid":"4812d4b7-8060-4791-a224-de7882cfaab0","_uuid":"9897c721dd77c2f3f5b183c1a7f399816dd77e0b","scrolled":false,"collapsed":true,"trusted":false},"cell_type":"code","source":"print('feature_manifest.shape: {}'.format(feature_manifest.shape))\nactive_feature_manifest = feature_manifest.loc[~feature_manifest['id'].str.contains('____')]\nprint('active_feature_manifest.shape: {}'.format(active_feature_manifest.shape))\ndisplay(active_feature_manifest.sample(10))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16ead046-883e-4a7e-af00-e72de26d6086","_uuid":"fec8d72b7e4b5be8cbb973ef4b446e607ce7f280"},"cell_type":"markdown","source":"We got rid of 360 terminated stations in 'active_feature_manifest', and now we're looking at a sample of its rows.\n<br>\nThe first column is 'id', which is the id of a weather station, and its the filename you're gonna look for if you want that station's data.\n<br>\nThe remaining columns are each of the different weather features, and they all have values between 0.0 and 1.0 (inclusive).\n\nIn the interest of full disclosure and to make its contents abundantly clear, here is the exact code I used to make the feature manifest for each weather file...\n\n<pre><code class=\"python\">num_rows = float(df.shape[0])\nnull_counts = df.isnull().sum()\ncoverage_manifest = [((num_rows - float(null_counts[_])) / num_rows) for _ in weather_columns]\n</code></pre>\n\nNote: In the above code block, ```df``` is a DataFrame read directly from a weather file, and ```weather_columns``` is a list of the names of the 14 weather features.\n\nNow, we want to figure out how many stations are supposed to record each feature, and then how often they actually do.\n<br>\nSo here's a clustermap showing just that! A heatmap will work, as well, but the clustermap looks better for now.\n\n# Weather Feature Coverage\n<a id=\"Weather-feature-coverage\"></a>\n"},{"metadata":{"_cell_guid":"364452e9-3f94-4421-97a3-de47bb137065","_uuid":"e78279788ac7813bba7c12eb2e6e89359e39c739","scrolled":false,"collapsed":true,"trusted":false},"cell_type":"code","source":"ax = sns.clustermap(active_feature_manifest.drop(['id'], axis=1), figsize=(8, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1dbbeb40-ebe5-4365-a2ec-62a7ef04e94d","_uuid":"4713d32fdf71a9fb9305981cfcb894b7f620e569"},"cell_type":"markdown","source":"At first glance, this might not look very promising, but keep reading and it'll get better.\n\nStarting on the left, we see that very few stations have data on snowfall; although, that is to be expected.\n<br>\nThe Japan Meteorological Agency states that snow depth is recorded only in areas of heavy snowfall, which is only ~280 stations.\n\nNext, we come to the various pressure measurements, and humidity, which are only recorded at a few hundred stations. \n\nEven fewer stations record solar radiation and cloud cover.\n\nThe light at the end of the tunnel: precipitation, which is recorded at all but 3 of the active stations.\n<br>\nIn fact, at 910 stations, we have precipitation data for every single day.\n\nThe remaining features (sunlight, wind, and temperature) all have relatively high coverages, as well.\n\n### Interpreting the Colors Above\nIt may be worth mentioning that a coverage of 0 *probably almost always* means that station was never meant to record the feature.\n<br>\nWhat this means for the above clustermap is that the pale/white (or whatever) sections are at or near full coverage.\n<br>\nSections with a red/purple/orange color are meant to record that feature, but either failed to part of the time, or the results seemed questionable to the JMA.\n<br>\nBlack sections indicate that 1) the station was never intended to record this feature, so it didn't, or 2) the station failed to record the feature, or produced questionable results for every single day in this 517-day span.\n\nNote: I'm pretty sure 0 coverage is always due to the former of the above two cases, but I mention the latter because I suppose it's possible, no matter how unlikely it may seem."},{"metadata":{"_cell_guid":"2b1da06c-65f4-4ab6-9a32-f067d294c5a8","_uuid":"4af1da6f66ea37dd0a61376d2753aeaa86d1bc14"},"cell_type":"markdown","source":"# Using feature_manifest with air/hpg_station_distances\n<a id=\"Feature-manifest-with-station-distances\"></a>\n\nThe clustermap above is great, but it might be missing the point. After all, we don't really care about all of the weather stations.\n\nWe only care about the weather stations that are close to our stores. So let's look at those."},{"metadata":{"_cell_guid":"64fc82b2-67fc-4a93-a2f5-f1d70f530a8d","_uuid":"9b072a229d074d85c09a1db0868a3c045d0a3b82","scrolled":true,"collapsed":true,"trusted":false},"cell_type":"code","source":"all_ranges, d_floor = ['10', '17', '20'], date_parse('2017-5-31')\nseparate_results, combined_results = {_: [] for _ in all_ranges}, {_: [] for _ in all_ranges}\n\nfor sys_distances in [air_station_distances, hpg_station_distances]:\n    for coords in tqdm(sys_distances.columns.values[3:], leave=False):\n        for e_range in all_ranges:\n            res = select_stations(coords, None, sys_distances, effective_range=int(e_range), date_floor=d_floor)\n            \n            separate_results[e_range].extend([_[0] for _ in res if _[0] not in separate_results[e_range]])\n            combined_results[e_range].append([_[0] for _ in res])\n\nprint('#' * 30 + ' separate_results ' + '#' * 30)\nfor _r in all_ranges:\n    print('{}     {}'.format(_r, len(separate_results[str(_r)])))\nprint('#' * 30 + ' combined_results ' + '#' * 30)\nfor _r in all_ranges:\n    print('{}     {}'.format(_r, len(combined_results[str(_r)])))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ccdfa9ee-00b5-4c23-b416-06a34d9dd459","_uuid":"f683741503e09f1cb7d56b266c2539b56a252f33"},"cell_type":"markdown","source":"### What Just Happened?\n\nIn line 1, we declare ```all_ranges``` as the ranges we're interested in, and ```d_floor``` as the final date in the competition.\n\nIn line 2, we declare two variables: ```separate_results, combined_results```. Notice that their initial values are exactly the same: a dict, whose keys are the elements of ```all_ranges```, pointing to empty lists. We'll see soon why we have both of them.\n\nIn line 4, we look at ```air_station_distances``` and ```hpg_station_distances``` [from this section](#Station-distances) via ```sys_distances```. \n<br>\nIn line 5, ```coords``` loops through columns in ```sys_distances``` that are coordinate strings, which is all columns starting at index 3.\n<br>\nIn line 6, we loop through ```all_ranges``` from line 1, via ```e_range```.\n\nHere's the good stuff.\n<br>\nIn line 7, we call ```select_stations```, [the helper function we made earlier](#Helper-select-stations), and we pass it:\n- ```coords``` from line 5,\n- ```None``` (see the ```select_stations``` docstring if you don't understand this),\n- ```sys_distances``` DataFrame from line 4, and\n- the keyword arguments ```effective_range=int(e_range), date_floor=d_floor```\n\nAnd we receive a list of (station_id, distance) tuple pairs, as ```res```\n\nIn line 9, we **extend** ```separate_results[e_range]``` by all the new station_ids in ```res```.\n<br>\nIn line 10, we **append** all of the station_ids to ```combined_results[e_range]```.\n<br>\nThat means ```separate_results[e_range]``` is a list of unique station_ids; whereas, ```combined_results[e_range]``` is a list of lists, whose contents are all the station_ids from each ```res```.\n\nFinally, we print out the lengths of the lists at each range in ```separate_results``` and ```combined_results```.\n\nNotice that the lengths in ```separate_results``` increase because they correspond to the number of unique weather stations that are within the given range of any store. So when we're looking everywhere within a 10 km radius around our stores, we find 65 different stations. But when the radius is increased to 20 km, we find 151 stations.\n\nOn the other hand, the lengths in ```combined_results``` are 237 at all ranges because in line 10, we appended the list of station_ids to each range in ```combined_results```, meaning the lengths correspond to the number of coordinate group strings in ```air_station_distances``` and ```hpg_station_distances``` combined (108 and 129, respectively), which adds up to 237.\n\nRemember that the length 237 is the number of lists of station_ids in these elements; the lists inside actually contain far fewer ids.\n\n### Helpers to Analyze Coverage\n<a id=\"Helpers-to-analyze-coverage\"></a>\n\nLet's set up a few helper functions to simplify our visualization of weather station coverages."},{"metadata":{"_cell_guid":"b75f9b5a-e27f-4292-9b7a-b8c869633559","_uuid":"65586ee20b0b14ecc7c2fb1bbde5543a48e9933f","scrolled":true,"collapsed":true,"trusted":false},"cell_type":"code","source":"def build_coordinate_count_map(store_info, station_distances):\n    info_by_coordinates = {_: 0 for _ in station_distances.columns.values[3:]}\n\n    for i, row in store_info.iterrows():\n        coordinate_str = '({}, {})'.format(row['latitude_str'], row['longitude_str']).replace('\"', '')\n        info_by_coordinates[coordinate_str] += 1\n\n    return pd.DataFrame(\n        columns=['coordinates', 'coordinate_count'],\n        data=[[_k, info_by_coordinates[_k]] for _k in station_distances.columns.values[3:]]\n    )\n\ndef filter_coverage(manifest, target_vals, do_isin=False, target_col='id', reindex=True, drop_cols=['id']):\n    if do_isin is False:\n        _res = pd.DataFrame(\n            columns=manifest.columns.values,\n            data=[manifest.loc[manifest[target_col] == _, :].values[0] for _ in target_vals]\n        )\n    else:\n        _res = manifest.loc[manifest[target_col].isin(target_vals)]\n    \n    _res = _res.reset_index(drop=True) if reindex is True else _res\n    _res = _res.drop(drop_cols, axis=1) if drop_cols is not None else _res\n        \n    return _res\n\nair_coord_counts = build_coordinate_count_map(air_store_info, air_station_distances)\nhpg_coord_counts = build_coordinate_count_map(hpg_store_info, hpg_station_distances)\ncoordinate_map = pd.concat((air_coord_counts, hpg_coord_counts)).reset_index(drop=True)\nall_coords = air_station_distances.columns.values[3:].tolist() + hpg_station_distances.columns.values[3:].tolist()\n\n# station_ids must be a list of length == len(all_coords)\n# station_ids must contain a station_id for each coordinate group in all_coords\n# station_ids must also be in the same order as all_coords\n# i.e. station_ids[i] is the station for the stores at all_coords[i]\ndef coverage_by_store(station_ids, drop_cols=['id']):\n    all_nearby_station_ids = []\n    for i, coords in enumerate(all_coords):\n        coord_count = coordinate_map.loc[coordinate_map['coordinates'] == coords, 'coordinate_count'].values[0]\n        all_nearby_station_ids.extend([station_ids[i]] * coord_count)\n    \n    return filter_coverage(feature_manifest, all_nearby_station_ids, drop_cols=drop_cols)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"14e768fd-33c0-4397-bee9-c70f02ed3814","_uuid":"1b9cec5ba04b80976e5b27a49c5b05fc8754441d"},"cell_type":"markdown","source":"# It's Gettin' Hot in Here (Station Coverage Heatmaps)\n<a id=\"Station-coverage-heatmaps\"></a>\n\nOur first clustermap of station coverages [(here)](#Weather-feature-coverage) was only the tip of the proverbial iceberg.\n\nFirst, we'll look at three heatmaps side-by-side.\n<br>\n\\#1 (leftmost) will look familiar because it is the heatmap version of the clustermap we saw earlier, shown for comparison.\n<br>\n\\#2 will show coverages for every **coordinate group's** nearest station. Despite some overlap, it only shows the essential stations.\n<br>\n\\#3 will show coverages for every **store's** nearest station. We'll go over the differences between the plot \\#2 and this one later.\n\nHere we go..."},{"metadata":{"_cell_guid":"6164a9a9-d6fb-4201-bb29-e001306c2aa6","_uuid":"73c45e3336adaeb8240299eaaa0b728404f9f59f","scrolled":false,"collapsed":true,"trusted":false},"cell_type":"code","source":"###############################################\n# Prepare the Data\n###############################################\n# Coverage for the nearest station for every store coordinate group\nnearest_by_coord_group = filter_coverage(feature_manifest, [_[0] for _ in combined_results['20'] if len(_) > 0])\n# Coverage for the nearest station for every store (same values as above, but applied to every store)\nnearest_by_store = coverage_by_store([_[0] for _ in combined_results['20']], drop_cols=None)\n\n###############################################\n# Plot the Data\n###############################################\nfig = figure(figsize=(12, 9))\ngs = gridspec.GridSpec(13, 3)\n\ncbar_ax = subplot(gs[0, :])\nax1, ax2, ax3 = subplot(gs[1:, 0]), subplot(gs[1:, 1]), subplot(gs[1:, 2])\n\ncbar_kwargs = dict(cbar=True, cbar_ax=cbar_ax, cbar_kws=dict(orientation='horizontal'))\nsns.heatmap(active_feature_manifest.drop(['id'], axis=1), ax=ax1, yticklabels=False, cbar=False)\nsns.heatmap(nearest_by_coord_group, ax=ax2, yticklabels=False, cbar=False)\nsns.heatmap(nearest_by_store.drop(['id'], axis=1), ax=ax3, yticklabels=False, **cbar_kwargs)\n\nax1.set_title('1) All Stations {}'.format(active_feature_manifest.drop(['id'], axis=1).shape))\nax2.set_title('2) Coord Groups {}'.format(nearest_by_coord_group.shape))\nax3.set_title('3) Stores {}'.format(nearest_by_store.drop(['id'], axis=1).shape))\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"567285c5-9458-4e55-a680-15bc7925879e","_uuid":"b12ec87d6b3d284a06aadc2e389684f794e307b6"},"cell_type":"markdown","source":"### What are We Looking At?\n\n**Left**\n<br>\nAs mentioned earlier, this plot shows the same data as [this one](#Weather-feature-coverage). It's just shown as a heatmap here, but its as unappealing as ever.\n<br>\nIt shows the coverage for every weather station that is active during our target timeframe.\n<br>\nWe don't want to work with that... And we don't have to.\n\n**Center**\n<br>\nAlright this one doesn't look as bad as the last one. It's not fantastic, but at least it doesn't make me want to throw my computer.\n<br>\nPlot \\#2 shows the station closest to each unique store coordinate pair, hence 237 rows (108 unique air coordinates, plus 129 for hpg).\n<br>\nBut wait, there's more.\n\n**Right**\n<br>\nThings are looking up. This shows the closest station to every store, of which there are 5,519 (829 from air, plus 4,690 from hpg).\n<br>\nBasically, we're looking at the same rows from heatmap \\#2, multiplied by how many stores are located at each set of coordinates.\n<br>\nYes, there's a lot of duplication, but the bottom line is that this is the data we're working with if we want every store's nearest station.\n\nBut we're not done yet. Hold on to your overfitting models!\n\n# Filling in Missing Data\n<a id=\"Fill-in-missing-data\"></a>\n\nEven though we were able to show that the missing data problem wasn't as big as we thought, it's still the obvious problem.\n\nOne way to mitigate this issue is to combine the data from different stations that are near each other.\n<br>\nLet's see how our coverage is affected by combining multiple stations' data. We'll use ```combined_results``` [from this earlier section](#Feature-manifest-with-station-distances).\n\n Let's start with some helper functions. We'll also read in all of the weather station data we'll need at this point."},{"metadata":{"_cell_guid":"81f0651f-abe1-4938-9f1d-8d46d5a3072f","_uuid":"d23c6296650cab4727d87059418d028caba78d25","collapsed":true,"trusted":false},"cell_type":"code","source":"weather_data = {_: pd.read_csv('{}/{}.csv'.format(weather_data_dir, _)) for _ in separate_results['20']}\n\ndef calculate_coverage(df):\n    num_rows = float(df.shape[0])\n    null_counts = df.isnull().sum()\n    return [((num_rows - float(null_counts[_])) / num_rows) for _ in weather_columns]\n\ndef build_daily_combined_coverage(stations):\n    summed_df = None\n    if len(stations) == 0:\n        return np.zeros(14).tolist()\n\n    for station in stations:\n        station_df = weather_data[station].drop(['calendar_date'], axis=1)\n        summed_df = station_df if summed_df is None else summed_df.add(station_df, fill_value=0)\n\n    return calculate_coverage(summed_df)\n\ncombined_coverages = {_: [] for _ in all_ranges}\n\nfor i, row in coordinate_map.iterrows():\n    for e_range in all_ranges:\n        _current_coverage = build_daily_combined_coverage(combined_results[e_range][i])\n        combined_coverages[e_range].extend([[row['coordinates']] + _current_coverage] * row['coordinate_count'])\n\ncombined_coverage_dfs = {_k: pd.DataFrame(\n    columns=['coordinates'] + weather_columns, data=_v\n) for _k, _v in combined_coverages.items()}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d56f1f47-c316-4771-a55f-d6fee4369f1c","_uuid":"122d0127c9b4655d0ea1cb6ce0bf3c127099d509","collapsed":true,"trusted":false},"cell_type":"code","source":"fig = figure(figsize=(12, 15))\ngs = gridspec.GridSpec(19, 2)\n\ncbar_ax = subplot(gs[0, :])\nax1, ax2 = subplot(gs[1:10, 0]), subplot(gs[1:10, 1])\nax3, ax4 = subplot(gs[10:, 0]), subplot(gs[10:, 1])\n\ncbar_kwargs = dict(cbar=True, cbar_ax=cbar_ax, cbar_kws=dict(orientation='horizontal'))\nsns.heatmap(nearest_by_store.drop(['id'], axis=1), ax=ax1, yticklabels=False, xticklabels=False, cbar=False)\nsns.heatmap(combined_coverage_dfs['10'].drop(['coordinates'], axis=1), ax=ax2, yticklabels=False, xticklabels=False, cbar=False)\nsns.heatmap(combined_coverage_dfs['17'].drop(['coordinates'], axis=1), ax=ax3, yticklabels=False, cbar=False)\nsns.heatmap(combined_coverage_dfs['20'].drop(['coordinates'], axis=1), ax=ax4, yticklabels=False, **cbar_kwargs)\n\nax1.set_title('1) Where We Left Off - No Combinations {}'.format(nearest_by_store.drop(['id'], axis=1).shape))\nax2.set_title('2) Combined - 10 km {}'.format(combined_coverage_dfs['10'].drop(['coordinates'], axis=1).shape))\nax3.set_title('3) Combined - 17 km {}'.format(combined_coverage_dfs['17'].drop(['coordinates'], axis=1).shape))\nax4.set_title('4) Combined - 20 km {}'.format(combined_coverage_dfs['20'].drop(['coordinates'], axis=1).shape))\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"46695f30-d130-459b-8e8f-d4f8ee9dfb1e","_uuid":"45ac9b7b75b670bf69d98268bde194eb48f02749","scrolled":true},"cell_type":"markdown","source":"### What's Going On?\n\nThe above heatmaps are numbered in the order we'll be looking at them.\n\n**1)** The upper-left heatmap is (as its title suggests) the last heatmap from [this section](#Station-coverage-heatmaps), as a benchmark for the next three heatmaps.\n\n**2)** This heatmap shows the combined coverage for all stores, when stores can rely on all weather stations in a 10 km radius.\n<br>\nSome of the null values are already filling in, but also notice the black stripes in ```precipitation```, where there were none before.\n<br>\nThis is because plot \\#1 takes the nearest station for every store regardless of its distance; whereas, plot \\#2 looks strictly within 10 km.\n<br>\nYou could easily make a hybrid of these two to ensure every store has at least one station, but that's what's going on there.\n\n**3)** In the lower-left heatmap, we start to see significant improvements after increasing the range to 17 km (as you probably expected).\n<br>\nJust like plot \\#2, by looking at ```precipitation```, we can see that there are still a few stores without any weather station in range.\n<br>\nThese are actually 14 hpg stores that are 17.18 km away from any station. So close! We'll look at them later on.\n\n**4)** When we increase the range to 20 km, of course we see that all stores now have at least one station within range.\n<br>\nAdditionally, it looks like we get nearly full coverage in the following features: ```avg_temperature, high_temperature, low_temperature, precipitation, hours_sunlight, avg_wind_speed```. We also have respectable (in my opinion) coverages in most non-snow-related features.\n\nJudge for yourself whether the improvement from plot \\#1 to plot \\#4 is noteworthy. I think it's significant progress.\n<br>\nAnd we're not stopping there.\n\n# Combined Coverages by Prefecture\n<a id=\"Combined-coverages-by-prefecture\"></a>\n\nIf you're thinking there's something weird about the distribution of our null spots, you're not alone. Let's get to the bottom of that.\n\nWe'll add a ```coordinates``` column to our store_info DataFrames to simplify things.\n<br>\nThen, we'll make a copy of plot \\#4's DataFrame for us to use, and we'll add the ```prefecture``` and ```area``` for each store. "},{"metadata":{"_cell_guid":"094c9222-f296-49aa-8294-b74e300739c0","_uuid":"87f9ab7684f1f4462f979f21d911bd74e7926bfe","scrolled":false,"collapsed":true,"trusted":false},"cell_type":"code","source":"def coord_format(row):\n    return '({}, {})'.format(row['latitude_str'], row['longitude_str']).replace('\"', '')\n\nair_store_info['coordinates'] = air_store_info.apply(coord_format, axis=1)\nhpg_store_info['coordinates'] = hpg_store_info.apply(coord_format, axis=1)\n\ndef clean_string(a_string):\n    if sys.version_info < (3, 0):\n        a_string = a_string.decode('utf-8').encode('unicode-escape')\n    return a_string.replace('\\u014d', 'o').replace('\\u014c', 'O')\n\ndef find_location(row, prefecture=True):\n    for (col, store_info) in [('air_area_name', air_store_info), ('hpg_area_name', hpg_store_info)]:\n        if row['coordinates'] in store_info['coordinates'].values:\n            area = clean_string(store_info.loc[store_info['coordinates'] == row['coordinates']][col].values[0])\n            return area.split(' ')[0].split('-')[0] if prefecture is True else area\n            \ncombined_df_20 = combined_coverage_dfs['20'].copy()\ncombined_df_20['prefecture'] = combined_df_20.apply(find_location, axis=1)\ncombined_df_20['area'] = combined_df_20.apply(lambda _: find_location(_, prefecture=False), axis=1)\n\ndisplay(combined_df_20.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d999ea8-893d-48dd-9f8b-42de63e6ca07","_uuid":"c219eb62afa9079e3414945873a5b06c1b40fcfe"},"cell_type":"markdown","source":"To recap, what we now have in ```combined_df_20``` is a DataFrame with rows corresponding to each store, showing the combined coverage for all stations within 20 km of that store, along with that store's ```coordinates```, ```prefecture```, and ```area```.\n\nWhat we want to see is how the coverage heatmap looks when we sort our stores by location.\n<br>\nIf you're wondering why we should care, I don't blame you.\n\nHere's what's going on: \n<br>\nBesides the coverage heatmap above just looking a bit odd, we also know that the JMA selectively records features in certain areas.\n<br>\nAdmittedly, we only know this for certain with regards to the snow-related features.\n<br>\nSpecifically, snow data is only recorded in areas known to experience heavy snowfall.\n\nSo, there's a good chance we'll see some patterns emerge in the coverage of our snow features at least.\n\n### Sorting by Area\n\nIn the cell below, we're copying our last DataFrame, and sorting its rows by ```prefecture``` and ```area```. Then we reset its indices.\n\nThat should be enough to try to find the patterns we're looking for, but we can do better.\n<br>\n```find_changes``` takes in a sorted DataFrame, a column name, and an optional ```handler``` that is called on ```target_col``` values.\n<br>\nFinally, we call ```find_changes``` on ```sorted_df_20``` three times, each time with different ```target_col``` and ```handler``` values.\n\nWhat we end up with are lists of the indexes at which the value of ```target_col``` changes, along with the corresponding values.\n\nWe'll see very soon why these will be useful."},{"metadata":{"_cell_guid":"57975eb4-8c35-4dc5-882b-4356e7d7e875","_uuid":"f4a5ab381f5df2931261261f42e8ef1b2708c84a","scrolled":false,"collapsed":true,"trusted":false},"cell_type":"code","source":"sorted_df_20 = combined_df_20.copy()\nsorted_df_20.sort_values(by=['prefecture', 'area'], axis=0, inplace=True)\nsorted_df_20.reset_index(drop=False, inplace=True)\n\ndef find_changes(df, target_col, handler=None):\n    indexes, values, previous_value = [], [], None\n    for i, row in df.iterrows():\n        current_value = row[target_col] if handler is None else handler(row[target_col])\n        if previous_value != current_value:\n            previous_value = current_value\n            indexes.append(i)\n            values.append(current_value)\n                \n    return indexes, values\n\n(prefecture_indexes, prefecture_values) = find_changes(sorted_df_20, 'prefecture')\n(city_indexes, city_values) = find_changes(sorted_df_20, 'area', handler=lambda _: ' '.join(_.split(' ')[:2]))\n(area_indexes, area_values) = find_changes(sorted_df_20, 'area')\n\nprint('Number of prefecture changes: {}'.format(len(prefecture_indexes)))\nprint('Number of city changes:       {}'.format(len(city_indexes)))\nprint('Number of area changes:       {}'.format(len(area_indexes)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de46506f-84a6-4d78-96ee-dd6f40cbea60","_uuid":"19fc99f9d3de136615b9d21ad6380261c2824d68","collapsed":true,"trusted":false},"cell_type":"code","source":"previous_df = combined_coverage_dfs['20'].drop(['coordinates'], axis=1)\ndisplay_df = sorted_df_20.drop(['index', 'coordinates', 'prefecture', 'area'], axis=1)\n\nfig = figure(figsize=(12, 15))\ngs = gridspec.GridSpec(19, 2)\n\ncbar_ax = subplot(gs[0, :])\nax1, ax2 = subplot(gs[1:10, 0]), subplot(gs[1:10, 1])\nax3, ax4 = subplot(gs[10:, 0]), subplot(gs[10:, 1])\n\ncbar_kwargs = dict(cbar=True, cbar_ax=cbar_ax, cbar_kws=dict(orientation='horizontal'))\nsns.heatmap(previous_df, ax=ax1, xticklabels=False, yticklabels=False, cbar=False)\nsns.heatmap(display_df, ax=ax2, xticklabels=False, yticklabels=False, cbar=False)\nsns.heatmap(display_df, ax=ax3, yticklabels=False, cbar=False)\nsns.heatmap(display_df, ax=ax4, yticklabels=False, **cbar_kwargs)\n\n_ = ax1.set_title('1) Where We Left Off {}'.format(previous_df.shape))\n_ = ax2.set_title('2) Sorted - No Lines {}'.format(display_df.shape))\n_ = ax3.set_title('3) Sorted - City Lines {}'.format(display_df.shape))\n_ = ax4.set_title('4) Sorted - Prefecture Lines {}'.format(display_df.shape))\n\nax3.hlines(city_indexes, *ax3.get_xlim(), colors='#00e600')\nax4.hlines(prefecture_indexes, *ax4.get_xlim(), colors='#00e600')\n\nfor i, index_update in enumerate(prefecture_indexes):\n    ax4.text(\n        x=ax4.get_xlim()[1] + 0.5,\n        y=index_update - 15 if i % 2 == 1 else index_update + 15,\n        s='{} - {}'.format(index_update, prefecture_values[i]),\n        ha='left',\n        va='top' if i % 2 == 1 else 'bottom',\n        fontsize=8\n    )\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"349577ce-c48e-40a8-96e7-e6fcb2084276","_uuid":"12b836b79c7e0542d74111438d9e6d3b27d1bd55"},"cell_type":"markdown","source":"### Why Aren't The Plots as Colorful as Before?\n\n**1)** As you've probably come to expect, our upper-left plot is our final plot from [this earlier section](#Fill-in-missing-data) as a helpful comparison.\n<br>\nRemember, this DataFrame's rows represent all our stores and their combined coverages within 20 km (like the next three plots).\n<br>\nThe only thing that's different about the data in plot \\#1 is that it hasnt been sorted by location.\n\n**2)** This plot shows the same exact content as plot \\#1, but after its rows have been sorted alphabetically by ```area```.\n<br>\nWe can see that the sections with non-null snow features have been reorganized from lots of small strips into two larger groups.\n<br>\nBut we don't really know what those two groups are, and that's why we collected the indexes at which our locations changed earlier.\n\n**3)** The lower-left plot's heatmap is identical to plot \\#2, but it is overlaid by bright green lines showing where the row's city changes.\n<br>\nThis one is cool, but its too crowded to see anything really useful.\n\n**4)** In the last plot, we are again looking at a heatmap identical to plot \\#2, but with a line overlay whenever the prefecture changes.\n<br>\nThis is just enough to see that at index 784, we start looking at the Hokkaido prefecture and we suddenly have snowfall data.\n<br>\nWhen we change prefectures, the snow features become null again, until we reach the Niigata prefecture at index 1,733.\n\nWhat this shows us is that basically outiside of Hokkaido and Niigata, we don't even need to think about snow.\n<br>\nIt's now much easier for our models to leverage snow data, and it seems like snow would have a significant effect on how many visitors a restaurant has.\n\n\n# Let it Snow!\n\nJust like that, we see that ```deepest_snowfall``` and ```total_snowfall``` are only recorded in the Hokkaido and Niigata prefectures!\n<br>\nNot only that, but it also looks like those two prefectures have nearly full coverage across the board!\n\nOn the question of what to do with this information, my first thought is using separate ensembles to handle the weather data:\n- One when looking at weather in these two prefectures \n- And at least one more to handle other prefectures\n\nI'm sure you all have other ideas, and I would love to hear them in the comments if you'd like to share!\n\n# Correlations Between Combined Stations\n## COMING SOON\n"},{"metadata":{"_cell_guid":"77ca7f45-c8b3-4004-b5a0-9a1e9b3c6ca5","_uuid":"738579b66f9617c9bdbe0dbb587d0a414db6a0b7"},"cell_type":"markdown","source":"# Closest Station Proximity\n<a id=\"Closest-station-proximity\"></a>\n\nIn other words, how close are our closest stations?\n<br>\nWe know that our air stores comprise 108 distinct coordinate groups, and our hpg stores are in 129 different coordinate groups.\n<br>\nIt's important to understand how much distance is between each of these groups and their nearest stations in order to assess the reliability of the weather data.\n\nPerhaps it's asking too much to hope for all of the groups to be very close to a station, so ideally, those groups that contain the greatest number of stores will be closer to a station.\n<br>\nIt's less important (although important nonetheless) that some group containing only a single store is very close to a station."},{"metadata":{"code_folding":[],"_cell_guid":"2693dc61-5780-411d-8c22-712753c28bca","_uuid":"4470d70778fc41de14e0f31ed3263ea4aec4c5b3","collapsed":true,"trusted":false},"cell_type":"code","source":"air_info_copy, hpg_info_copy = air_store_info.copy(), hpg_store_info.copy()\n\nair_info_copy['coordinate_count'] = air_info_copy.groupby(\n    ['latitude', 'longitude']\n).latitude.transform('count').astype(int)\n\nhpg_info_copy['coordinate_count'] = hpg_info_copy.groupby(\n    ['latitude', 'longitude']\n).latitude.transform('count').astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f80a10f-2efc-4df4-b5ae-052e7322e5b5","_uuid":"14fdfea3ac48e48171c23b42594590e510e02d25","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.distplot(air_info_copy['station_vincenty'], rug=True, kde=True)\nplt.title('AIR - Distances (km) to Stations Distribution')\nplt.show()\n\nsns.distplot(hpg_info_copy['station_vincenty'], rug=True, kde=True)\nplt.title('HPG - Distances (km) to Stations Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b99f840-5e83-466e-a602-a62db812cb8d","_uuid":"bc49f1962a6d89f452032befc45732189c3f9854"},"cell_type":"markdown","source":"The above distribution plots show us that most of our stores are within 10 kilometers of a station; although, there are a few outliers.\n<br>\nIf the plots above aren't quite clear enough, the joint plots below might help."},{"metadata":{"_cell_guid":"4a14d526-7d79-44ae-89de-11622693f7fc","_uuid":"a4572fe7fb58a3b6dcaa0459617663d5cd727baa","collapsed":true,"trusted":false},"cell_type":"code","source":"p = sns.jointplot(x='station_vincenty', y='coordinate_count', data=air_info_copy, kind='kde')\nplt.title('AIR KDE Joint Plot', loc='left')\np.plot_joint(plt.scatter, c='r', s=30, linewidth=1, marker='x')\nplt.show()\n\np = sns.jointplot(x='station_vincenty', y='coordinate_count', data=hpg_info_copy, kind='kde')\nplt.title('HPG KDE Joint Plot', loc='left')\np.plot_joint(plt.scatter, c='r', s=30, linewidth=1, marker='x')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6d7f0062-1362-4a72-9998-80f6d1e0116d","_uuid":"3bfaa99f5f4a65b22aa51a2cb8217e54d433bc87"},"cell_type":"markdown","source":"The joint plots show most of the closest weather stations are within about 8 km of the store we are looking at.\n\nThe air outliers are 3 groups whose nearest stations are between 10 and 12 km away.\n<br>\nThe coordinate_count values seem low, so there are probably under 20 air stores that are more than 10 km away from a station.\n\nThe hpg outliers are more numerous and confusing, largely because there are 4,690 HPG stores, compared to air's 829.\n<br>\nThey seem to be: a little over 100 stores (4 unique coordinates) that are between 12 and 15 km from their nearest station, along with about 10 more that are a whopping 17.5 km away.\n\nBut let's not just trust the plots. Let's check the data..."},{"metadata":{"code_folding":[],"_cell_guid":"39f30a85-301d-48bb-a817-eb806b7a24a1","_uuid":"3e0c60a14e62ba5d3bfeb0c7878b62facb7d27e7","collapsed":true,"trusted":false},"cell_type":"code","source":"def view_distances(df, distance, cols=['station_vincenty', 'coordinate_count']):\n    return df[cols].groupby(cols).filter(\n         lambda _: _[cols[0]].mean() > distance\n    ).drop_duplicates().sort_values(by=cols).reset_index(drop=True)\n\ndisplay(view_distances(air_info_copy, 8))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bd34082c-fe7b-46f8-9b79-939255f41d6e","_uuid":"1dc80381f88e822d598ceb842dd1338fd10ecbc8"},"cell_type":"markdown","source":"For the air stores, there are 18 stores (4 unique coordinates) that are between 8 and 9 km away from their closest station.\n<br>\nThere are also 15 stores (4 unique coordinates) that are more than 10 km away from a station, with the furthest being 12.21 km away. \n\nNow for the hpg stores..."},{"metadata":{"_cell_guid":"0cc38387-a7a1-4673-bad2-e86f322c356c","_uuid":"af99d95980814496aaf7315f038777de9154477c","collapsed":true,"trusted":false},"cell_type":"code","source":"display(view_distances(hpg_info_copy, 10))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c76b221a-3d36-4e7a-bda6-caa50c5615fc","_uuid":"53065d521e553bad3ddf7dda22b2966fc05e5c9f"},"cell_type":"markdown","source":"For the hpg stores, we see 147 stores (8 unique coordinates) between 10 and 14 km away from a station.\n<br>\nThen, there are 16 stores (2 unique coordinate pairs) located more than 14 km from a station, with the furthest being 17.18 km away.\n\nThe good news is that all of our store are within about 17 km of a weather station.\n<br>\nAdditionally, those coordinate groups that are further from a weather station tend to have fewer stores in them.\n\nThe bad news is that I have very little to base my 17 km range on. I have no idea what the effective range of each weather station is.\n<br>\nI'm just assuming that the JMA chose the 17 km spread between stations for a reason, and I'm assuming the reason is that a station's effective range is 17 km.\n\n# About weather_stations and nearby_active_stations\n<a id=\"Nearby-active-weather-stations\"></a>\n\n# Mapping All Active Stations\n<a id=\"Mapping-all-active-stations\"></a>\n\nNow we're going to take a look at all of the active weather stations that we have data for, alongside all of our unique store coordinates. \n\nTo start, we'll set up some helper functions and convert our stations' ```date_terminated``` columns to Pandas date objects.\n\nThe ```calculate_cumulative_opacity``` function is my attempt to lighten the load on Folium when it's plotting many hundreds of duplicated store points.\n<br>\nI noticed that it was struggling to actually plot them all with all of the interactivity I wanted, but perhaps someone wiser than myself can tell me whether doing this is actually a good idea. I welcome all feedback.\n\nAlso, you may notice we're doing a little bit of back-tracking here by not making use of some files available to use. \n<br>\nI wanted to do this, once again in the interest of full-disclosure, so you know how I arrived at the data I'm giving you."},{"metadata":{"_cell_guid":"3990485c-e96e-4d0f-93ae-8e29d63e5030","_uuid":"68e87679a845e2758e736c648964a91c8ec73f0b","collapsed":true,"trusted":false},"cell_type":"code","source":"weather_stations['date_terminated'] = pd.to_datetime(weather_stations['date_terminated'], format='%Y-%m-%d').dt.date\nactive_stations = weather_stations.loc[pd.isnull(weather_stations['date_terminated'])]\n\ndef calculate_cumulative_opacity(base_opacity, num_stacks):\n    cumulative_opacity = base_opacity\n    for i in range(num_stacks - 1):\n        cumulative_opacity = cumulative_opacity + (1 - cumulative_opacity) * base_opacity\n    return cumulative_opacity\n\n\ndef coord_groups(df):\n    unique_groups = df.groupby(['latitude', 'longitude']).groups\n    unique_coords = unique_groups.keys()\n    unique_vals = [unique_groups[_] for _ in unique_coords]\n    return (unique_groups, unique_coords, unique_vals)\n\n(unique_air_groups, unique_air_coords, unique_air_vals) = coord_groups(air_store_info)\n(unique_hpg_groups, unique_hpg_coords, unique_hpg_vals) = coord_groups(hpg_store_info)\n(unique_active_groups, unique_active_coords, unique_active_vals) = coord_groups(active_stations)\n\nbase_opacity = 0.3\nair_opacity_vals = [calculate_cumulative_opacity(base_opacity, len(_)) for _ in unique_air_vals]\nhpg_opacity_vals = [calculate_cumulative_opacity(base_opacity, len(_)) for _ in unique_hpg_vals]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e96aee1c-1044-4447-991f-a703159e550c","_uuid":"5a5e641b2e644c9230c7abbb0b3f1151211a0c61"},"cell_type":"markdown","source":"Now that we have the data we want to plot, along with a guide for the markers' opacity, we're ready to start plotting.\n<br>\nAs mentioned earlier, we're using Folium. It's fantastic for plotting geospatial data, and I only know about it thanks to a recommendation from @Heads or Tails. \n<br>\nSo you have @Heads or Tails to thank for the improvements over [the maps in my previous kernel](https://www.kaggle.com/huntermcgushion/weather-station-location-eda).\n<br>\nUnless you liked the old way more. In that case, it's my fault. Sorry, I'll try better next time. "},{"metadata":{"_cell_guid":"d4f423f6-94fc-420b-b114-d87eeed3b494","_uuid":"646163fb55157ef620ebd2e7d461190dc7f68a32","collapsed":true,"trusted":false},"cell_type":"code","source":"###############################################\n# Initialize Map\n###############################################\nf_map = folium.Map(location=[38, 137], zoom_start=5, tiles='Cartodb Positron')\n\n###############################################\n# Create Active Weather Station MarkerCluster\n###############################################\nmarker_cluster = folium_plugins.MarkerCluster(name='marker_cluster', control=True, overlay=True)\n\nfor i, coords in enumerate(unique_active_coords):\n    _station = active_stations.loc[unique_active_vals[i][0]]\n\n    marker = folium.Marker(\n        location=coords, \n        popup=folium.Popup('ID:  {}<br>Coords:  {}'.format(\n            _station['id'],\n            [float('{:.4f}'.format(_)) for _ in coords]\n        ))\n    )\n\n    marker_cluster.add_child(marker)\n\n###############################################\n# Create AIR Stores FeatureGroup\n###############################################\nair_stores_group = folium.FeatureGroup('air_stores')\n\nfor i, coords in enumerate(unique_air_coords):\n    _store_group = air_store_info.loc[unique_air_vals[i][0]]\n\n    air_stores_group.add_child(folium.RegularPolygonMarker(\n        location=coords,\n        popup='Coords:  {}<br>Store Count:  {}<br>Station:  {}<br>Vincenty:  {:.5f}'.format(\n            coords,\n            len(unique_air_vals[i]),\n            _store_group['station_id'],\n            _store_group['station_vincenty']\n        ),\n        fill_opacity=air_opacity_vals[i],\n        fill_color='red',\n        number_of_sides=100, weight=0, radius=10\n    ))\n\n###############################################\n# Create HPG Stores FeatureGroup\n###############################################\nhpg_stores_group = folium.FeatureGroup('hpg_stores')\n\nfor i, coords in enumerate(unique_hpg_coords):\n    _store_group = hpg_store_info.loc[unique_hpg_vals[i][0]]\n\n    hpg_stores_group.add_child(folium.RegularPolygonMarker(\n        location=coords,\n        popup='Coords:  {}<br>Store Count:  {}<br>Station:  {}<br>Vincenty:  {:.5f}'.format(\n            coords,\n            len(unique_hpg_vals[i]),\n            _store_group['station_id'],\n            _store_group['station_vincenty']\n        ),\n        fill_opacity=hpg_opacity_vals[i],\n        fill_color='green',\n        number_of_sides=100, weight=0, radius=10\n    ))\n\n###############################################\n# Add Active Stations FeatureGroup\n# Add HPG and AIR Stores FeatureGroups\n###############################################\nactive_stations_group = folium.FeatureGroup(name='active_stations')\nactive_stations_group.add_child(marker_cluster)\nf_map.add_child(active_stations_group)\n\nf_map.add_child(hpg_stores_group)\nf_map.add_child(air_stores_group)\n\n###############################################\n# Add Map Extras and Display Map\n###############################################\nf_map.add_child(folium.map.LayerControl(collapsed=False))\n# f_map.add_child(features.LatLngPopup())\nf_map.add_child(folium_plugins.MeasureControl(\n    position='bottomleft', primary_length_unit='kilometers', secondary_length_unit='miles'\n))\n\ndisplay(f_map)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c00f7d5-892c-4db8-bc73-cda42c283d18","_uuid":"36673d1e55ae82b377d2d9482d83a262286f1407"},"cell_type":"markdown","source":"### What is This?\n\nAbove, we're looking at a map of Japan, with the following elements:\n1. Dark green \"circle\" markers are stores in the hpg system\n2. Red \"circle\" markers are air stores\n3. Weather stations are shown using marker clusters. Zooming into a cluster reveals blue icons representing active weather stations\n\nClicking on a store marker will bring up a popup, showing the following:\n- The latitude/longitude coordinates of that store group,\n- The number of stores that exist at these coordinates, which should be reflected in the marker's opacity\n- The id of the closest active weather station\n- The vincenty distance (in kilometers) from this store group to the above weather station\n\nClicking on a blue weather station icon will display a popup, showing the following:\n- The id of this weather station, which corresponds to the third items in a store's popup\n- The coordinates of this weather station\n\nYou can click any of the square checkboxes in the upper-right corner to show/hide the corresponding elements from the map.\n<br>\nI recommend looking at the map with different boxes selected to see how clustered together our stores are, relative to the even distribution of weather stations.\n\nYou can also click the square, ruler-looking thingy in the lower-left corner to draw lines on the map and see the distances they span.\n<br>\nThis tool can be useful if: you don't trust my calculations, you prefer miles to km, or you're interested in the distance between a store and a different station."},{"metadata":{"_cell_guid":"2ffa5424-5fac-4c27-b643-2ebfcf9c9200","_uuid":"4eb20e439b9d5e743d0749f86d1be048d987eba8"},"cell_type":"markdown","source":"# Distances Between Identical Stores\n<a id=\"Identical-stores-distances\"></a>\n\nYes, I know that sounds strange, but there are 63 stores in 'store_id_relation.csv' that have coordinates in both air and hpg systems.\n<br>\nBy calculating the distances between these \"identical\" stores, we can get a better idea of the extent to which locations were made noisy.\n\nAdditionally, this can help determine how much wiggle room we have when considering the distance between a store and a weather station."},{"metadata":{"_cell_guid":"76fef1cb-f2c3-49ca-a44e-8eca16edfe10","_uuid":"5a84fb2ea21a47830aca392a2e96292b85a3be67","scrolled":true,"collapsed":true,"trusted":false},"cell_type":"code","source":"store_id_relation = pd.read_csv('{}/store_id_relation.csv'.format(original_data_dir))\n\ntemp_air_store_info = air_store_info.loc[:, ['air_store_id', 'latitude', 'longitude']]\ntemp_air_store_info.columns = ['air_store_id', 'air_latitude', 'air_longitude']\n\ntemp_hpg_store_info = hpg_store_info.loc[:, ['hpg_store_id', 'latitude', 'longitude']]\ntemp_hpg_store_info.columns = ['hpg_store_id', 'hpg_latitude', 'hpg_longitude']\n\nidentical_stores_df = pd.merge(store_id_relation, temp_air_store_info, on=['air_store_id'], how='inner')\nidentical_stores_df = pd.merge(identical_stores_df, temp_hpg_store_info, on=['hpg_store_id'], how='inner')\n\nidentical_stores_df['vincenty'] = identical_stores_df.apply(\n    lambda _: vincenty((_['air_latitude'], _['air_longitude']), (_['hpg_latitude'], _['hpg_longitude'])).km,\n    axis=1\n)\nprint(identical_stores_df.shape)\ndisplay(identical_stores_df.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d7e774f-d533-4f04-8ff7-8453b93e7b96","_uuid":"bcdc69360894d4a6678b279a4b27002d7fffcf40","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.distplot(identical_stores_df['vincenty'].values, rug=True, kde=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66e3c335-e895-4e38-b454-c07705287a31","_uuid":"df3e976a4b5c5253e76ac9135a33e22905fa7c1f"},"cell_type":"markdown","source":"It looks like the majority of the identical stores are less than a couple kilometers from each other.\n<br>\nThere are 11 stores whose coordinates are more than two kilometers away from each other.\n<br>\nAdditionally, there is one singular store that has coordinates in both store_info systems, whose air coordinates place it an eye-watering 20.69 kilometers away from its hpg coordinates...\n\nIts existence proves that its acceptable to use stations in a 20 km range from a store.\n<br>\nI'm joking, of course, this store is clearly an outlier, but its interesting nonetheless.\n\nIn the end, it looks like our coordinates have about a kilometer, maybe two, of wiggle room."},{"metadata":{"_cell_guid":"44a63ddc-a865-40a6-ab0a-9278076be093","_uuid":"eaa5ae69ccddf68c42ef8bfee16c4319a55517f3"},"cell_type":"markdown","source":"# Weather Station Background\n<a id=\"Weather-station-background\"></a>\n\nThe Japan Meteorological Agency utilizes AMeDAS stations, which stands for \"Automated Meteorological Data Acquisition System\".\n<br>\nThey began operations on November 1, 1974.\n<br>\nObservations are made every 10 minutes, then transmitted on a real-time basis to the AMeDAS Center in Tokyo via dedicated phone lines.\n\nThere are approximately 1,300 currently active systems around Japan, with about 17 kilometers of separation between each.\n<br>\nOf the ~1,300 active stations, over 1,100 are unmanned\n<br>\nFor about 280 stations located in areas of heavy snowfall, snow depth is measured.\n<br>\nAbout 700 of the unmanned stations observe precipitation, temperature, wind, and sunlight.\n<br>\nThe others only observe precipitation."},{"metadata":{"_cell_guid":"2ae5148e-c956-48d7-b5d9-6e1ef9ddf844","_uuid":"5ef7be1b63e73ec70bdd8d81c60c2bb2c93606f5","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"varInspector":{"window_display":false,"cols":{"lenType":16,"lenName":16,"lenVar":40},"kernels_config":{"python":{"library":"var_list.py","delete_cmd_prefix":"del ","delete_cmd_postfix":"","varRefreshCmd":"print(var_dic_list())"},"r":{"library":"var_list.r","delete_cmd_prefix":"rm(","delete_cmd_postfix":") ","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"position":{"right":"20px","width":"444px","left":"1601.46px","height":"356px","top":"113.984px"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}