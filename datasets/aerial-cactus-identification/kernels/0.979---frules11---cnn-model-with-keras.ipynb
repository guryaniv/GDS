{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom tqdm import tqdm\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57f222f2294c6cf75566a9856626a03c89c220b5"},"cell_type":"code","source":"class DataLoader:\n    def __init__(self, npy_file: str = \"npy_data\"):\n        self.npy_file = npy_file\n        self.csv_name = \"../input/train.csv\"\n        self.df = self.read_csv()\n        self.n_classes = 2\n\n        os.makedirs(self.npy_file, exist_ok=True)\n\n    def read_csv(self):\n        df = pd.read_csv(self.csv_name)\n\n        return df\n\n    def read_data(self, load_from_npy: bool = True, size2resize: tuple = (75, 75), make_gray: bool = True,\n                  save: bool = True, categorical: bool = False, n_classes: int = 2):\n\n        x_data = []\n        y_data = []\n\n        if load_from_npy:\n            try:\n                x_data = np.load(fr\"{self.npy_file}/x_data.npy\")\n                y_data = np.load(fr\"{self.npy_file}/y_data.npy\")\n            except FileNotFoundError:\n                load_from_npy = False\n                print(\"NPY files not found!\")\n                pass\n\n        if not load_from_npy:\n            x_data = []\n            y_data = []\n\n            for dir_label in tqdm(self.df.values):\n                img = cv2.imread(os.path.join(\"../input\", \"train/train\", dir_label[0]))\n\n                if make_gray:\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n                img = cv2.resize(img, size2resize)\n\n                x_data.append(img)\n                y_data.append(int(dir_label[1]))\n\n                del img\n\n            x_data = np.array(x_data)\n            y_data = np.array(y_data)\n\n            if save:\n                np.save(fr\"{self.npy_file}/x_data.npy\", x_data)\n                np.save(fr\"{self.npy_file}/y_data.npy\", y_data)\n\n        if categorical:\n            y_data = tf.keras.utils.to_categorical(y_data, num_classes=n_classes)\n\n        if not categorical:\n            y_data = y_data.reshape(-1, 1)\n\n        if load_from_npy and make_gray:\n            try:\n                x_data_2 = [cv2.cvtColor(n, cv2.COLOR_BGR2GRAY) for n in x_data]\n                x_data = x_data_2\n            except cv2.error:\n                pass\n\n        if make_gray:\n            x_data = np.expand_dims(x_data, axis=-1)\n\n        return x_data, y_data\n\n    def read_test_data(self, load_from_npy: bool = True, size2resize: tuple = (75, 75), make_gray: bool = True,\n                  save: bool = True, categorical: bool = False, n_classes: int = 2):\n\n        test_df = pd.read_csv(\"../input/sample_submission.csv\")\n\n        x_data = []\n        y_data = []\n\n        if load_from_npy:\n            try:\n                x_data = np.load(fr\"{self.npy_file}/x_data_test.npy\")\n                y_data = np.load(fr\"{self.npy_file}/y_data_test.npy\")\n            except FileNotFoundError:\n                load_from_npy = False\n                print(\"NPY files not found!\")\n                pass\n\n        if not load_from_npy:\n            x_data = []\n            y_data = []\n\n            for dir_label in tqdm(test_df.values):\n                img = cv2.imread(os.path.join(\"../input\", \"test/test\", dir_label[0]))\n\n                if make_gray:\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n                img = cv2.resize(img, size2resize)\n\n                x_data.append(img)\n                y_data.append(int(dir_label[1]))\n\n                del img\n\n            x_data = np.array(x_data)\n            y_data = np.array(y_data)\n\n            if save:\n                np.save(fr\"{self.npy_file}/x_data_test.npy\", x_data)\n                np.save(fr\"{self.npy_file}/y_data_test.npy\", y_data)\n\n        if categorical:\n            y_data = tf.keras.utils.to_categorical(y_data, num_classes=n_classes)\n\n        if not categorical:\n            y_data = y_data.reshape(-1, 1)\n\n        if load_from_npy and make_gray:\n            try:\n                x_data_2 = [cv2.cvtColor(n, cv2.COLOR_BGR2GRAY) for n in x_data]\n                x_data = x_data_2\n            except cv2.error:\n                pass\n\n        if make_gray:\n            x_data = np.expand_dims(x_data, axis=-1)\n\n        return x_data, y_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e80d70d4e54bee75e0fcb295dc0eb834e8432d8"},"cell_type":"code","source":"class TrainWithKeras:\n    def __init__(self, x_data, y_data, lr: float = 0.001, epochs: int = 10, batch_size: int = 32,\n                 loss: str = \"categorical_crossentropy\", model_path: str = \"model.h5\"):\n        self.x_data = x_data\n        self.y_data = y_data\n        self.model_path = model_path\n\n        self.epochs = epochs\n        self.batch_size = batch_size\n\n        self.optimizer = Adam(lr=lr)\n        self.loss = loss\n\n    def make_model(self, summarize: bool = True):\n        model = Sequential()\n\n        model.add(Conv2D(64, (3, 3), strides=1, activation=\"relu\",\n                         input_shape=(self.x_data.shape[1], self.x_data.shape[2], self.x_data.shape[3])))\n        model.add(MaxPooling2D())\n        model.add(Conv2D(128, (3, 3), strides=1, activation=\"relu\"))\n        model.add(Dropout(0.3))\n        model.add(BatchNormalization())\n\n        model.add(Conv2D(256, (3, 3), strides=1, activation=\"relu\"))\n        model.add(MaxPooling2D())\n        model.add(Conv2D(512, (3, 3), strides=1, activation=\"relu\"))\n        model.add(Dropout(0.3))\n\n        model.add(Conv2D(1024, (3, 3), strides=1, activation=\"relu\"))\n        \n        model.add(Flatten())\n\n        model.add(Dense(1024, activation=\"relu\"))\n        model.add(Dropout(0.3))\n        model.add(Dense(2, activation=\"softmax\"))\n\n        if summarize:\n            model.summary()\n\n        return model\n\n    def compile(self, kmodel: Sequential):\n        kmodel.compile(loss=self.loss, optimizer=self.optimizer, metrics=[\"acc\"])\n\n        return kmodel\n\n    def train(self, kmodel: Sequential, save: bool = True):\n        history = kmodel.fit(self.x_data, self.y_data, batch_size=self.batch_size, epochs=self.epochs,\n                             validation_split=0.0)\n\n        if save:\n            kmodel.save(self.model_path)\n\n        return history, kmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4eae8a50d2e45b69109f4e53161bf662ba2c1ca9"},"cell_type":"code","source":"class MakeSubmission:\n    def __init__(self, x_test: np.array, model_path: str, csv_path: str):\n        self.x_test = x_test\n        self.model_path = model_path\n        self.csv_path = csv_path\n\n        self.model = tf.keras.models.load_model(self.model_path)\n        self.df = pd.read_csv(self.csv_path)\n\n        preds = self.make_predictions()\n\n        submission = pd.DataFrame({'id': self.df['id'], 'has_cactus': preds})\n        submission.to_csv(\"sample_submission.csv\", index=False)\n\n    def make_predictions(self, make_it_ready: bool = True):\n        preds = self.model.predict(self.x_test)\n\n        if make_it_ready:\n            preds = [np.argmax(n) for n in preds]\n\n        return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b843b05f60fb5d46b31a50ff7e060fa2f0084b1d"},"cell_type":"code","source":"os.makedirs(\"models\", exist_ok=True)\n\ndl = DataLoader()\nX_data, Y_data = dl.read_data(True, (32, 32), False, True, True, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a3eda25e23077359f4fa02b24cc901be789d452"},"cell_type":"code","source":"trainer = TrainWithKeras(X_data, Y_data, model_path=\"models/model.h5\", epochs=50, batch_size=1024, lr=0.0002)\nmodel = trainer.make_model()\nmodel = trainer.compile(model)\n\nhistroy = trainer.train(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56626391863dd2213f2567d83e970b857fad4de3"},"cell_type":"code","source":"X_data_test, Y_data_test = dl.read_test_data(True, (32, 32), False, True, False)\nms = MakeSubmission(X_data_test, \"models/model.h5\", \"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8537d778127423f0cecb87dcc72f873fe4e2131"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}