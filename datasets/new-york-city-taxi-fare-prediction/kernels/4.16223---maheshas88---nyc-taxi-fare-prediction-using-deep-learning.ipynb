{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom math import radians, cos, sin, asin, sqrt\nfrom haversine import haversine\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nimport random\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow import set_random_seed\n\n#set random seed\nrandom.seed(123)\nnp.random.seed(123)\nset_random_seed(123)\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#load train and test data\ntrain =  pd.read_csv('../input/train.csv', nrows = 6_000_000)\ntest =  pd.read_csv('../input/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380b7c901c36cf17b06af9d00c22e8b9744ccd48"},"cell_type":"code","source":"#no of null values in train data. There are null vlaue sin dropoff attributes.\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27fa6e3cce9b2f2d26bc6e19c50ce8397c89efc7"},"cell_type":"code","source":"#no of null values in test data. There are no null values which was expected.\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea5f64fdb841a48a9ee54db30177a442f2100dcf"},"cell_type":"code","source":"#the train data\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a12f13fce8dac93eace7f9c05ba8a1462190bea"},"cell_type":"code","source":"#datatypes of the train data\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"508b163d21289b306174ff151d01a2f068b3485d"},"cell_type":"code","source":"#since there are lots training data, the no. of null value records is negligible. So we are dropping\n#the null value records.\ntrain.dropna(subset=['dropoff_latitude','dropoff_longitude'], inplace=True)\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b41d92cb801381dea91e3141534e0fbca3e241e"},"cell_type":"code","source":"#with little bit of googling, we can find the exact lattitude and longitude values for NYC. So we can \n#filter out only those records which are within these bounds. We filter both pickup and \n#dropoff atributes.\ndef clean_lats_long(full_data):    \n    full_data = full_data[(-76 <= full_data['pickup_longitude']) & (full_data['pickup_longitude'] <= -72)]\n    full_data = full_data[(-76 <= full_data['dropoff_longitude']) & (full_data['dropoff_longitude'] <= -72)]\n    full_data = full_data[(38 <= full_data['pickup_latitude']) & (full_data['pickup_latitude'] <= 42)]\n    full_data = full_data[(38 <= full_data['dropoff_latitude']) & (full_data['dropoff_latitude'] <= 42)]\n    return full_data\n\ntrain= clean_lats_long(train)\nprint(train.shape)\n#test = clean_lats_long(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3659a1e50aae92d03bccfd2e3baf7acb2ec6e8e"},"cell_type":"code","source":"#the passenger count has s´few records with count more than 10 and less than 1. Usually even a SUV kind\n#of taxi woud take max of 10 people. Definitley counts that are less than 1 are wrong. So we drop\n#these records\nprint(train.passenger_count[train.passenger_count > 10].count())\nprint(train.passenger_count[train.passenger_count < 10].count())\nprint(train.passenger_count[train.passenger_count == 0].count())\ntrain = train[(train.passenger_count < 10)&(train.passenger_count > 0)]\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c41da152d5ab8a7c1ba7d7429bbc09299d253331"},"cell_type":"code","source":"#usually taxis start off with certain amount of fixed cost. We may have to pay some money just to get into\n#a taxi. So there can be no trips tha cost less than 1$. So we drop them as well.\nprint(train.fare_amount[train.fare_amount < 1].count())\ntrain = train[train.fare_amount >= 1]\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50784d9fb8ca924e33d4d398188c811dbb931797"},"cell_type":"code","source":"#haversine distance is the distance between two set of lattitude and longitude points. Taxi fare is\n#directly influenced by the time travelled and the distance. SO distance as a feature is a good addition.\ndef drop_latlong_column(df):\n    df.drop(columns=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude'\n                     ,'dropoff_longitude'],inplace=True)\n    return df\n\ndef haversine_lat_long(df):\n    df['dist_travel']=df.apply(lambda x:haversine((x['pickup_latitude'],\n                                                                 x['pickup_longitude']),\n                                                                 (x['dropoff_latitude'],\n                                                                 x['dropoff_longitude']),\n                                                   unit='mi'),axis=1)\n    return drop_latlong_column(df)\n\ntrain = haversine_lat_long(train)\ntest = haversine_lat_long(test)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e1476337250fe5fe347339faad4dc8ddfeb80a0"},"cell_type":"code","source":"#once dist is calculated, we can now plot the fare amount and dist and make some exploratory analysis.\n#we just use he last 10000 records to save memory and time.\ntrain[:10000].plot.scatter(x=\"fare_amount\", y=\"dist_travel\")\nprint(train.fare_amount.corr(train.dist_travel))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e89da54458cdf2e51c3acc8e05a4ef06d9dd578a"},"cell_type":"code","source":"#form the above plot, it is clear that some fare amounts and dist travelled dont add up. THere are\n#some points which have high fare with very little or no dist travelled and vice versa. But the plot \n#is only for 10000 records, so in general we could say fare amount less than 10$ with dist travelled\n#more than 50 as well fare amount more than 150$ and dis less than 5 can be dropped.\ntrain.drop(train[(train.fare_amount < 10) & (train.dist_travel>50)].index, inplace=True)\ntrain.drop(train[(train.fare_amount >150) & (train.dist_travel<5)].index, inplace=True)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5661047c7d00354bf89d4874a11049e00adfbe8"},"cell_type":"code","source":"#to process data and time feature. We convert it to type datatime first and extract all the values \n#separately like, hour, day, etc. This helpful tonascertain when the ride was taken. For example, during\n#peak hours the fare can be higher than usual. Secondly, we convert hours to bins(evening, morning,afternoon,\n#late night). THis again determines adn tells us when the ride as taken. ALso makes sense to group similar\n#hours to a certain bin so that they are treted similarly. \ndef process_datetime(df):\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n    df['year'] = df.pickup_datetime.dt.year\n    df['day'] = df.pickup_datetime.dt.day\n    df['hour'] = df.pickup_datetime.dt.hour\n    df['month'] = df.pickup_datetime.dt.month\n    df['weekday'] = df.pickup_datetime.dt.weekday\n    df['hour_bin'] = pd.cut(df.hour,bins=4, labels=['LN','MO','AN','EV'])\n    df.drop(columns=['pickup_datetime','hour'],inplace=True)\n    return df\n\ntrain = process_datetime(train)\ntest = process_datetime(test)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74f7facf26d7b604b967ec5e40f6d367a110cafc"},"cell_type":"code","source":"#drop the columns key in both datasets\ntrain.drop(columns=['key'],inplace=True)\ntest.drop(columns=['key'],inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f860e86d337ce3e25027a19ab04d4d9457adeb7d"},"cell_type":"code","source":"#create dummies for hour_bin feature\ntrain= pd.get_dummies(train, columns=['hour_bin'],drop_first=True)\ntest= pd.get_dummies(test, columns=['hour_bin'],drop_first=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"005da05bf89ae0a3827eac9ed6f318cf6c3fbb4d"},"cell_type":"code","source":"#separate the target variable and drop it from train set\ntarget = train.fare_amount\ntrain.drop(columns=['fare_amount'],inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6bc080b1a2cda4c634207c6e464fb79f46fa413"},"cell_type":"code","source":"#convert to np array and do scaling. We use standard scaling so that data is centered and also has a \n#standard deviation of 1. scaling is important for gradient based learning algorithms. \ntrain = np.array(train)\ntest = np.array(test)\ntarget = np.array(target)\nscaler = StandardScaler(copy=False)\nscaler.fit(train)\nscaler.transform(train)\nscaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f6a82cb0fe69317f1234c2b8096ad7b98714111","scrolled":true},"cell_type":"code","source":"#split to train and validation sets.\nx_train, x_val, y_train, y_val = train_test_split(train, target, test_size=0.3)\nprint(len(x_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b8259703afdb7b7f03c5dd9a446fcb14cb73e15"},"cell_type":"code","source":"#Train the model using keras deep learning.\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', min_delta=1e-3, patience=3)\ncallback=[early_stop]\nadam = Adam(lr=0.0001)\n\nmodel = Sequential() \nmodel.add(Dense(100, activation='relu', input_shape=(train.shape[1],)))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(80, activation='relu'))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(40, activation='relu'))\nmodel.add(Dropout(0.6))\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss='mse', optimizer=adam, metrics=['mae'])\nhistory = model.fit(x_train,y_train,batch_size=256, epochs=25, verbose=1, callbacks=callback,\n         validation_data=(x_val, y_val), shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be12f31e3e3834cd114132733a9f00d8c82fdef7"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# summarize history for loss using learning curve\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee533f7a66d53017f186e17bceb2c69945d7e165"},"cell_type":"code","source":"#predict the rmse for validation set. \nfrom sklearn.metrics import mean_squared_error\ny_pred = np.array(model.predict(x_val))\nprint(sqrt(mean_squared_error(y_val, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b1977b5e3160b6b9acdbaf3c113553d4c5f0897"},"cell_type":"code","source":"#submit the submission results. The results could be improved a lot with hyperparameter tunig using\n#gridsearch or using an ensmble model with final result based on outputs of different regressors.\n#Other improvements like using important spots in NYC like airports,etc can be added to features.\nsub =  pd.read_csv('../input/sample_submission.csv')\ny_sub = model.predict(test)\nsub.fare_amount = y_sub\nsub.to_csv('Submission1.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}