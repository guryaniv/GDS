{"cells":[{"metadata":{"_uuid":"4467efa2d51d8c7b88151003de74aafef3707002"},"cell_type":"markdown","source":"# Background\nAccording to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception.\n\nThe digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner… and often before they´ve even realized they need the service. In their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.\n\nIn this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale."},{"metadata":{"_uuid":"1f1731843a4c6da5f5297900bc02a0f585a50737"},"cell_type":"markdown","source":"# Load Libraries & Load Data\n## Load Libraries"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e18d82883337d35f09886c06a5741568dcaf2cff"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\n# import catboost as cbt\nimport lightgbm as lgb\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d907d9f680cdef584e0fe585016aaa8dd1524f98"},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b92a3d6bf80b9caaa0214deed7efd6d580e3790e"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest =  pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a19a6ac6bc1943e8b06af810253a4869e1ef7892"},"cell_type":"markdown","source":"# Data preliminary analysis\n## Analysis of Train\n1. There are 1845 float features, 3147 integal  features, 1 object feature\n2. There is no null element in train data set\n3. There are 256 columns have only one number, which is useless for many models since it brings no information.\n4. There are 9 columns provide repeated information"},{"metadata":{"trusted":true,"_uuid":"4e0be083b75847b226f3a1f1dcfa07639a3d7ff7","collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a04be17c0012d9a68e8dc81282e0b3c346a4923b"},"cell_type":"markdown","source":"- There are 1845 float features, 3147 integal features, 1 object feature"},{"metadata":{"trusted":true,"_uuid":"f35dd58990ed4cc7f02871b425250d51d6290b4a","collapsed":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba8edf151367bb9a091077c35ecea47b0b7901c6"},"cell_type":"markdown","source":"- There is no null element in train data set"},{"metadata":{"trusted":true,"_uuid":"ace073fe0baf50d7ef117dd1cca1fbba63ed51aa","collapsed":true},"cell_type":"code","source":"null_num =  train.isnull().sum().sum()\nprint('There are {} null elements in train data'.format(null_num))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81b681cebe62481a854d0d4686ebcd51481c49a5"},"cell_type":"markdown","source":"- There are 256 columns have only one number, which is useless for many models since it brings no information.(Delete this columns)\n\nWe see many people choose to merge train and test data and then delete columns with only one value, but I think only do this for train is ok since we only need to train our model use training data."},{"metadata":{"trusted":true,"_uuid":"3cdfb4e27d3471d699187cfae918bc3f8e0f6582","collapsed":true},"cell_type":"code","source":"cols_with_onlyone_val = train.columns[train.nunique() == 1]\ncols_with_onlyone_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5020ffd05619a869da496660498acbe2834ad994","collapsed":true},"cell_type":"code","source":"len(cols_with_onlyone_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"80ed5102f253c8645027828ed528655642550151"},"cell_type":"code","source":"cols = [x for x in train.columns if x not in cols_with_onlyone_val]\ntrain_clean = train[cols].copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a2d2fccfebf883fcc34fc4b538e3cbfba57a9d0"},"cell_type":"markdown","source":"- Drop those duplicate features and keeps only one feature.\n\nHere, I provide accelarate this process in two ways, the first is by heuristic method, we only calculate the statistical infomation of all columns, and we think if two columns are with the same statistical infomation, they are the same.  The second way is use numpy operations to accelarate .\n\n"},{"metadata":{"trusted":true,"_uuid":"a01a804b0be74fe06e1a07aab58b7ef20dc331c6","collapsed":true},"cell_type":"code","source":"train_clean_info = train_clean.describe()\ntrain_clean_info","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f283098b362476932c3292017c1b14653065ca7d","collapsed":true},"cell_type":"code","source":"columns = train_clean_info.columns\ncols_del = []  # del those duplicated columns \ndup_dict = {} \n\nfor i in range(len(columns)-1):\n    if columns[i] in cols_del:\n        continue \n    if i % 10 ==0:\n        print(i / len(columns))\n    first = train_clean_info[columns[i]].values  \n    res = train_clean_info.iloc[:,i+1:] - np.tile([first],[train_clean_info.shape[1]-i -1,1]).T  \n    cols_del.extend(res.columns[np.sum(res) == 0]) \n    if np.sum(np.sum(res) == 0) > 0: \n        dup_dict[columns[i]] = res.columns[np.sum(res) == 0] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4165a49539385f2f55e7fba24e99d1192c9d0444"},"cell_type":"markdown","source":"- There are 9 columns provide repeated information, we can drop them."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"fa9c4ac5d9b2d62c3a229c290110cedc5074f37a","collapsed":true},"cell_type":"code","source":"cols_del","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93aa14cf124cd8e92db077b023b85cdfb4a9dfd7","collapsed":true},"cell_type":"code","source":"dup_dict","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9397de3399ca65a44bc759fe9087e1bb401b8ee"},"cell_type":"markdown","source":"Let's take a look! we take three pairs and we see yes, that's what we want to find."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"59f19fb1c6ca3f72388a172b508421cdcefe5e72","collapsed":true},"cell_type":"code","source":"train_clean[['168b3e5bc','f8d75792f','34ceb0081','d60ddde1b','70f3a87ec','66f57f2e5']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"96d17504afb0e7b0e636a8abfc48138ee9375b79"},"cell_type":"code","source":"cols = [x for x in train_clean.columns if x not in cols_del]\ntrain_clean = train_clean[cols].copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c817f01c72e9fc6d235cc76852870e6e682750b1"},"cell_type":"markdown","source":"## Analysis of Test\n1. There are 4991 float features\n2. There is no null element in test data set\n3. No columns with only one value\n4. no columns are duplicated"},{"metadata":{"trusted":true,"_uuid":"4a7e26c2c23b41b543fe1f041da0477bb897fe81","collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"706b4a96cd247bd94b21082ad0ff29afd60653e7"},"cell_type":"markdown","source":"- There are 4991 float features, which is interesting, since we have many integals in train dataset."},{"metadata":{"trusted":true,"_uuid":"30b01dde3c17afd39b097ee0103dd5d3ed093087","collapsed":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2891be267fd8769872c0a67aeb4cda2afd7083f6"},"cell_type":"markdown","source":"There is no need to do extra work for those columns that we have done analysis."},{"metadata":{"scrolled":true,"trusted":true,"collapsed":true,"_uuid":"de3a683cd7b7f2e5fbd10425be7ca4ac290026d4"},"cell_type":"code","source":"cols = [x for x in train_clean.columns if x!='target']\ntest_clean = test[cols].copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed624420f67f78d732eaacc802f69d9ff3ccf1c"},"cell_type":"markdown","source":"- Nice work, no columns with only one value here!"},{"metadata":{"trusted":true,"_uuid":"c64706139d60c7a5794b19b17791fa91fa2e4c2a","collapsed":true},"cell_type":"code","source":"cols_with_onlyone_val = test_clean.columns[test_clean.nunique() == 1]\ncols_with_onlyone_val","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cffd750cb9ee048fcd8bfc67b2c0c04d09adcc7"},"cell_type":"markdown","source":"- Nice work, no columns are duplicated!"},{"metadata":{"trusted":true,"_uuid":"ac7759e0e0cd49fbbae20b63a464e4d7144c070e","collapsed":true},"cell_type":"code","source":"test_clean_info = test_clean.describe()\ntest_clean_info","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"44bd542f8fad57e34c1ee810f372600587cc7480","collapsed":true},"cell_type":"code","source":"columns = test_clean_info.columns\ncols_del = []  # del those duplicated columns \ndup_dict = {} \n\nfor i in range(len(columns)-1):\n    if columns[i] in cols_del:\n        continue \n    if i % 10 ==0:\n        print(i / len(columns))\n    first = test_clean_info[columns[i]].values  \n    res = test_clean_info.iloc[:,i+1:] - np.tile([first],[test_clean_info.shape[1]-i -1,1]).T  \n    cols_del.extend(res.columns[np.sum(res) == 0]) \n    if np.sum(np.sum(res) == 0) > 0: \n        dup_dict[columns[i]] = res.columns[np.sum(res) == 0] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ba6a31198ec793a7229749157475e5b8d4076d6","collapsed":true},"cell_type":"code","source":"cols_del","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfffb6bc3130c7c7ebea1e8dab3cf48536a5676f"},"cell_type":"markdown","source":"## Label\nFor regression problem, there are many tricks. But before that, we need to take a look at our label.\n- There are unusual values\n- All labels are bigger than 0\n- Our target is skewed"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"cd4abdfc7754b2bdc65bf26a170cffb953e27035","collapsed":true},"cell_type":"code","source":"train['target']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"64e976eccaec4a680624b3c4f18553a8f4cc34ce","collapsed":true},"cell_type":"code","source":"plt.scatter(x = range(train.shape[0]), y = train['target'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdd98b069a2d3bc190094c8d33f131ab9d47a582","collapsed":true},"cell_type":"code","source":"train['target'].plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c93d3a202669cab8e28c1bd665cbeeb8de8f7b22"},"cell_type":"markdown","source":"- Our target is skewed!"},{"metadata":{"trusted":true,"_uuid":"2d03eac2c2481de52e322652ccfd899f9dd7f425","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(train_clean[\"target\"].values, bins=50, kde=False)\nplt.xlabel('Target', fontsize=12)\nplt.title(\"Target Histogram\", fontsize=14) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7e0303f0d5d87bc4c1b68d82bd78e46541a2b48"},"cell_type":"markdown","source":"- There are unusual values, which I mean (value - mean) / std > 2"},{"metadata":{"trusted":true,"_uuid":"489aa568b44876f50c25cac55e2639849398ba70","collapsed":true},"cell_type":"code","source":"train['target'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba00754a4890329e15142bf9229e58dc06ef9d0d","collapsed":true},"cell_type":"code","source":"(train['target'].describe().loc['max'] - train['target'].describe().loc['mean']) / train['target'].describe().loc['std']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a489e51753429f711e294c7441f70439ea22b2d3"},"cell_type":"markdown","source":"# Baseline Model\nOf course, there are still many things we need to do, but since this is only an anonymous game, so, after simple preprocessing, now, we need a baseline to see how far to go."},{"metadata":{"_uuid":"2ad2494eeb5d166021e2f8a206886c530d78f131"},"cell_type":"markdown","source":"## Label transformation\nIf we want to get a nice score, the best way is to find a loss function that can be easily optimized, like mse, so here, we need to do some transformations."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6d778bea68a4e151e8d21f03424effb6c83e2815"},"cell_type":"code","source":"train_clean[\"target\"] = train_clean[\"target\"].apply(np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e957ee7deeede0bcade6a75efccd4f948abea12"},"cell_type":"markdown","source":"## Validation\n\nHere, to reduce randomness, we take three random seed, and see if the validation results are similar. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"127ec9a4e56cad2bf37eccaa1b11d376f51c2ef8"},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"08be9fb083df61a81d4b5fbaa23ff391fb778cb1"},"cell_type":"code","source":"def run_lgb_val(train_X, train_y, val_X, val_y):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 64,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.85,\n        \"feature_fraction\" : 0.85,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 100,\n        \"verbosity\" : -1,\n        \"seed\": 921212\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=50, \n                      evals_result=evals_result)  \n    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"853863cdef7f4f75b3fe158eb52dffc74a600389","collapsed":true},"cell_type":"code","source":"feature_columns = [x for x in train_clean.columns if x not in ['ID','target']]\nfor rnd in range(3):\n    print('*' * 50)\n    print(rnd)\n    print('*' * 50)\n    train_X, val_X, train_y, val_y = train_test_split(train_clean[feature_columns], train_clean['target'], test_size = 0.3, random_state = rnd)\n    run_lgb_val(train_X, train_y, val_X, val_y) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85799f253a51beda489336341e227e69adf81c47"},"cell_type":"markdown","source":"## Submit(The Final Result is 1.47 online, which is not a huge gap between underline & online)\n\nFor robustness, here, we choose to use simple ensemble methods to submit our predition to see if it is similar to our underline result.\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0ea8848e760aa168c8128bdabb823a0793f7ae5a"},"cell_type":"code","source":"def run_lgb_test(train_X, train_y, val_X, val_y):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 64,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.85,\n        \"feature_fraction\" : 0.85,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 100,\n        \"verbosity\" : -1,\n        \"seed\": 921212\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=50, \n                      evals_result=evals_result)  \n    return model","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"b0bee110876ef08e591cf90912a681c2179326d9","collapsed":true},"cell_type":"code","source":"feature_columns = [x for x in train_clean.columns if x not in ['ID','target']]\nres = []\nfor rnd in range(3):\n    print('*' * 50)\n    print(rnd)\n    print('*' * 50)\n    train_X, val_X, train_y, val_y = train_test_split(train_clean[feature_columns], train_clean['target'], test_size = 0.3, random_state = rnd)\n    model = run_lgb_test(train_X, train_y, val_X, val_y) \n    pred = model.predict(test_clean[feature_columns])\n    res.append(pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0b176b2dcd424bb9bf39258421202eb076f94c11"},"cell_type":"code","source":"test['target'] = np.expm1(np.mean(res,axis=0))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"88c8c91ce76aeca4a53218840a1a65bb52c4e0bc","collapsed":true},"cell_type":"code","source":"test[['ID','target']].head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"78504590ae1d8d77a78c8fcd4496ec2917b5ff39"},"cell_type":"code","source":"test[['ID','target']].to_csv('Baseline.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3d92f135608f315a0a177383fb9ad48720c23c2b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}