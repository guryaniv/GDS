{"cells":[{"metadata":{"trusted":true,"_uuid":"4d3e2cdb14be07a4889f7d803f4299228afbd376","scrolled":true},"cell_type":"code","source":"import gc\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt  \n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a449d475899b6c3a91ba3744e676bf5ee2d026f9"},"cell_type":"markdown","source":"# Load train data, and reduce memory"},{"metadata":{"trusted":true,"_uuid":"f71a4e26f21acfb6045f7b350085ee0f79648154"},"cell_type":"code","source":"INPUT_DIR = \"../input/\"\nLABEL = 'winPlacePerc'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b05df50e2e7c00321ffb71c528cb1ce1c86b9563"},"cell_type":"code","source":"df_train = pd.read_csv(INPUT_DIR+'train_V2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"636dbd9cf6f714c233ee3a32d280e1ecd8db2286"},"cell_type":"code","source":"# credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"020641cb7c6f88e5a886d11caae53a69b971fba3"},"cell_type":"code","source":"df_train = reduce_mem_usage(df_train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c11a878a415797f7bf2b7d7189fbdcd39cbc034"},"cell_type":"markdown","source":"# EDA"},{"metadata":{"_uuid":"d324424f8e1de25062a092e6ddf078eb7e7178fa"},"cell_type":"markdown","source":"## Data type, and bad value "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c513f9ce9827705197b021618af01f69690625b3"},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8eebf50eb891f21c7bcbac36aeef6917dfa91afe"},"cell_type":"code","source":"df_train.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ae273b08b15b3b38969c29775cb8a2c4351fbc1"},"cell_type":"markdown","source":"There are missing values. "},{"metadata":{"trusted":true,"_uuid":"b9f7f7e981eba2622eb6533748a2e14f0fc014dc"},"cell_type":"code","source":"df_train = df_train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daef97617e693469515cd4cddd39b04ba38a276b"},"cell_type":"code","source":"# there are matches with no players or just one player, which is abnormal\ndf_train = df_train[df_train['maxPlace'] > 1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"751aec9d3ac9fdcd9b8610550c7b808d95427c37"},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"_uuid":"92564b6218658089346318f9492ff9c18ec86efa"},"cell_type":"markdown","source":"### The final placement depends on the performance of the group and the match dynamics, not individuals.  So first, get all the group and match features.\n\nfrom group:\n- group size. More people, more chance to rank high\n- the summary of group performace: the max,mean,min of each variant, except of id, groupId, matchId, numGroup, match duration, martch type, maxPlace\n\nfrom match:\n- **mean, max, and min of group features except group size**\n- numGroup. The smaller, the better\n- match duration\n- match type\n- maxPlace"},{"metadata":{"trusted":true,"_uuid":"cb8b194997dc54523e807eb129f70ddbc3b3cd1f"},"cell_type":"code","source":"def FE(df,train=True):\n    LABEL = 'winPlacePerc'\n    \n    \n    # get label data\n    if train:\n        df_y = df.groupby(['matchId','groupId'])[LABEL].agg('mean')\n        ## now we can delete label and 'id' column to save GPU\n        df = df.drop([LABEL],axis=1)\n    else:\n        df_ids = df[['Id','matchId','groupId']]\n        \n    df = df.drop('Id',axis=1)\n    \n    # define group and match features\n    MATCH_FEATURE_part = ['numGroups','matchDuration','matchType','maxPlace']\n    \n    GROUP_FEATURE = df.columns.tolist()\n    GROUP_FEATURE.remove('groupId')\n    GROUP_FEATURE.remove('matchId')\n    for fe in MATCH_FEATURE_part:\n        GROUP_FEATURE.remove(fe)\n    # ATTENTION: here group feature doesn't include 'groupSize'\n    MATCH_FEATURE = MATCH_FEATURE_part + GROUP_FEATURE\n    MATCH_FEATURE.remove('matchType') \n    \n    # get group features\n    ## group size\n    dm = df.groupby(['matchId','groupId'])\n    df_X = dm.size().to_frame(name='groupSize') #df_X has indices: matchid, groupid\n    \n    ## other group features\n    gp = dm[GROUP_FEATURE].agg('max')\n    gp_rank = gp.reset_index().groupby(['matchId'])[GROUP_FEATURE].rank(pct=True).set_index(df_X.index)\n    df_X = df_X.join(gp).join(gp_rank,rsuffix='_max_rank') # join by indices\n    \n    gp = dm[GROUP_FEATURE].agg('min')\n    gp_rank = gp.reset_index().groupby(['matchId'])[GROUP_FEATURE].rank(pct=True).set_index(df_X.index)\n    df_X = df_X.join(gp,rsuffix='_min').join(gp_rank,rsuffix='_min_rank') # join by indices\n    \n    gp = dm[GROUP_FEATURE].agg('mean')\n    gp_rank = gp.reset_index().groupby(['matchId'])[GROUP_FEATURE].rank(pct=True).set_index(df_X.index)\n    df_X = df_X.join(gp,rsuffix='_mean').join(gp_rank,rsuffix='_mean_rank') # join by indices\n\n    #a variable called killPlace, it's already the rank in the match, so the _rank are all duplicates, so need to delete\n    df_X = df_X.drop(columns = ['killPlace_min_rank','killPlace_max_rank','killPlace_mean_rank'])\n    \n    # get match features except mactchType\n    dm = df.groupby(['matchId'])\n    df_X = df_X.join(dm[MATCH_FEATURE].agg('mean'),lsuffix='_max',rsuffix='_mean_match')\n    \n    # get matchType\n    df_X = df_X.join(pd.concat([pd.get_dummies(df.matchType),df[['matchId','groupId']]],axis=1).groupby(['matchId','groupId']).agg('min'))\n    \n    # prepare for output\n    if not train:\n        df_X_index = df_X.reset_index()[['matchId','groupId']]\n    \n    lst_features = list(df_X.columns)\n    \n    del df,dm,gp,gp_rank\n    gc.collect()\n    \n    if train:\n        return df_X,df_y,lst_features\n    else:\n        return df_X, df_X_index, df_ids\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"326ac12913a0ee5d2e82d96fd4ea4e7357927f94"},"cell_type":"code","source":"X_train,y_train,lst_features = FE(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"325c3b1ad4d69272a2c4bdffbcdb215f18734467"},"cell_type":"code","source":"list(X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"023dd563fd2171a2659632ed432be58731a9888b"},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0800164635d612f0c4a5ebdf29a40aa476dce4c"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"048ddb5672f7629d54ffb2a29d147f817533cb59"},"cell_type":"markdown","source":"# NN using Keras"},{"metadata":{"_uuid":"8e1d18e325c176709d1b5df0443ac285d68cbd53"},"cell_type":"markdown","source":"### Split train data to train and validation"},{"metadata":{"trusted":true,"_uuid":"de56ed9e5f26b23d143da49b4e1bc4c742d74f31"},"cell_type":"code","source":"# X_train, X_vali, y_train, y_vali = train_test_split(X_train, y_train, test_size = 0.2, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df86f91e1f837f474f7d69b8790494a3f971c708"},"cell_type":"markdown","source":"### Feature-wise normalization"},{"metadata":{"trusted":true,"_uuid":"2194132caba3f1ca6f9e325a18e11b3b433805a7"},"cell_type":"code","source":"# mean = X_train.mean(axis=0)\n# X_train -= mean\n# std = X_train.std(axis=0)\n# X_train /= std\n\n# X_vali -= mean\n# X_vali /= std","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e01a53d67554fa0973c1619a3dcc3fbd7b67580"},"cell_type":"markdown","source":"### Build the model"},{"metadata":{"trusted":true,"_uuid":"b0d1c739e7cba60bb6db4dcafeedbf9c0b0bc5fc"},"cell_type":"code","source":"# nn_model = Sequential()\n# nn_model.add(Dense(512,input_dim= X_train.shape[1], activation='relu'))\n# nn_model.add(Dropout(0.1))\n# nn_model.add(Dense(256, activation='relu'))\n# nn_model.add(Dropout(0.1))\n# nn_model.add(Dense(128, activation='relu'))\n# nn_model.add(Dropout(0.1))\n# nn_model.add(Dense(1,activation='linear')) \n\n# nn_model.compile(optimizer=optimizers.Adam(), loss='mse', metrics=['mae'])\n# nn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d3f4699825115051c37c09de3d4cdc669d04349","scrolled":true},"cell_type":"code","source":"# history = nn_model.fit(X_train, y_train, \n#                  validation_data=(X_vali,y_vali),\n#                  epochs=10,\n#                  batch_size=10000,\n#                  verbose=1)\n# del X_train, y_train\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a842f06165e2395b726a7751de59fe1400c7984"},"cell_type":"code","source":"# from keras.models import load_model\n\n# nn_model.save('NN_model.h5')  # creates a HDF5 file","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba372ba726819c8b9c76506cf2f09c5271355968"},"cell_type":"markdown","source":"\n### Visualization"},{"metadata":{"trusted":true,"_uuid":"5471319f2021bedefb081db0ec94591621bddee8"},"cell_type":"code","source":"# # Plot training & validation loss values\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('Model loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Test'], loc='upper left')\n# plt.show()\n\n# # Plot training & validation mae values\n# plt.plot(history.history['mean_absolute_error'])\n# plt.plot(history.history['val_mean_absolute_error'])\n# plt.title('Mean Abosulte Error')\n# plt.ylabel('Mean absolute error')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Test'], loc='upper left')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65cb335ca132689c5bfaafbd01ce7973239bc417"},"cell_type":"markdown","source":"### Make prediction Using NN"},{"metadata":{"trusted":true,"_uuid":"56921494ab07bb1398c6359dddc215cb27600feb"},"cell_type":"code","source":"# df_test = pd.read_csv(INPUT_DIR+'test_V2.csv')\n# df_test = reduce_mem_usage(df_test)\n# gc.collect()\n# X_test, X_test_index,df_test_ids = FE(df_test,train=False)\n# X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dc2c7eb326468361b897db7e2b8b6b866bdfeb9"},"cell_type":"code","source":"# del df_test\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92882ab808cad88ad184a6131a63e723cb4d6078"},"cell_type":"code","source":"# # need to normailize for NN\n# X_test -= mean\n# X_test /= std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c1e4cf386c7611bdf5d985a1568b07574d70d87"},"cell_type":"code","source":"# pred = nn_model.predict(X_test)\n# pred = np.clip(pred, a_min=0, a_max=1)\n# df_pred = X_test_index.assign(winPlacePerc=pred)\n# result = pd.merge(df_test_ids, df_pred, how='left', on=['matchId', 'groupId'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e3ed4d3c679a5e6af5ad3e08242ab302ed3ac8d"},"cell_type":"code","source":"# submission = result[['Id', LABEL]]\n# submission.to_csv('submission.csv', index=False)\n# submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebac2258964bda98a8afa9bf8768321f6d5bee0b"},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"_uuid":"8d602154cba4b62b309eba449e6a0085f82491bf"},"cell_type":"markdown","source":"### prep"},{"metadata":{"trusted":true,"_uuid":"658072f0174c498b2cfcb484cfcfaf52e147c748"},"cell_type":"code","source":"# lgb_model = lgb.Booster(model_file='lgb_model.txt')  #init model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05e90eadd86dbb53959f042574ab3162268e07b7"},"cell_type":"code","source":"del df_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c1f9879935fec03bd9fde13ab0a15166716548f"},"cell_type":"code","source":"df_test = pd.read_csv(INPUT_DIR+'test_V2.csv')\ndf_test = reduce_mem_usage(df_test)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"110a85fe9f9e3bbb87f82950ae939a169de7a418"},"cell_type":"code","source":"X_test, X_test_index,df_test_ids = FE(df_test,train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6590db562cbd5fa29af15a398b10fba7febd90fe"},"cell_type":"code","source":"# prep for interatoion on every fold; initialize the value\nfolds = KFold(n_splits=3,random_state=3)\nvali_pred = np.zeros(X_train.shape[0])\npred = np.zeros(X_test.shape[0]) #final pred\ndf_feature_importance = pd.DataFrame()\nvalid_score = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5bf5c179fe907a11a0e44226eccab0ff1459d98"},"cell_type":"markdown","source":"### define model"},{"metadata":{"trusted":true,"_uuid":"f2f013c39d26a4be77ef4bd93768988ec86cb0ff"},"cell_type":"code","source":"params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':20000, 'early_stopping_rounds':100,\n          \"num_leaves\" : 25, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.9, \"feature_fraction\":0.7,\n           \"bagging_seed\" : 0, \"num_threads\" : 4\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f27b64f750f3448f78a93f7bbb9ae9343fc2799f"},"cell_type":"code","source":"for n_fold, (train_idx, vali_idx) in enumerate(folds.split(X_train, y_train)): # for each fold\n    # split train data\n    X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train[train_idx]\n    X_vali, y_vali = X_train.iloc[vali_idx], y_train[vali_idx]    \n    \n    #build model, fit\n    train_data = lgb.Dataset(data=X_train_fold, label=y_train_fold)\n    valid_data = lgb.Dataset(data=X_vali, label=y_vali)   \n    \n    lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=1000) \n    \n    #predict\n    pred_fold = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration) \n    pred_fold[pred_fold>1] = 1 \n    pred_fold[pred_fold<0] = 0 \n    pred += pred_fold/ folds.n_splits\n    \n    #evaluate 1: check on the validation data\n    vali_pred[vali_idx] = lgb_model.predict(X_vali, num_iteration=lgb_model.best_iteration)\n    vali_pred[vali_pred>1] = 1\n    vali_pred[vali_pred<0] = 0\n    print('Fold %2d MAE : %.6f' % (n_fold + 1, mean_absolute_error(y_vali, vali_pred[vali_idx])))\n\n    #evaluation 2: check the importance of each feature\n    df_fold_importance = pd.DataFrame()\n    df_fold_importance = df_fold_importance.assign(feature= lst_features)\n    df_fold_importance = df_fold_importance.assign(importance = lgb_model.feature_importance())\n    df_fold_importance = df_fold_importance.assign(fold = n_fold + 1)\n    df_feature_importance = pd.concat([df_feature_importance, df_fold_importance])\n    \n    gc.collect()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f891a5c99dcee7452974bfa4395cf05c5bea70d7"},"cell_type":"code","source":"print('Full mae score %.6f' % mean_absolute_error(y_train, vali_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c1d1c7b35811f90bba80c608566fba821c434c3"},"cell_type":"code","source":"lgb_model.save_model('lgb_model.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14ea95bb498dd6845d13378c3e33eed728134a5e"},"cell_type":"code","source":"top_features = df_feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].reset_index()\n\nplt.figure(figsize=(14,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=top_features)\nplt.title('LightGBM_Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('LightGBM_Importances.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed00aad1079db5081a79290b853cf01ff1236ea3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b1360541c54faf4d849248b9f5f2aea9b079628"},"cell_type":"markdown","source":"### Make Prediction"},{"metadata":{"trusted":true,"_uuid":"6c1e4cf386c7611bdf5d985a1568b07574d70d87"},"cell_type":"code","source":"pred = np.clip(pred, a_min=0, a_max=1)\ndf_pred = X_test_index.assign(winPlacePerc=pred)\nresult = pd.merge(df_test_ids, df_pred, how='left', on=['matchId', 'groupId'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e3ed4d3c679a5e6af5ad3e08242ab302ed3ac8d"},"cell_type":"code","source":"submission = result[['Id', 'winPlacePerc']]\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5562388f7bb1c3b47342923e1fb7bb0995dce9fc"},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20c696ff9bfb75f042e6f57c107ed4d61e0f8caf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78bdd8ff6c52c159b3d0f7776cb19a2a79259030"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba1625d92113e7fc6821f024025697c2cfc1eb0c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}