{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"import os, cv2, random\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n%matplotlib inline \n\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import np_utils","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a75fa1bb344f613d4389807310717771eb0d9c4","_cell_guid":"723b260e-ea43-4ff5-870a-e04bbe24ddc8","trusted":true,"scrolled":false,"collapsed":true},"cell_type":"code","source":"TRAIN_DIR = '../input/dogs-vs-cats-redux-kernels-edition/train/'\nTEST_DIR = '../input/dogs-vs-cats-redux-kernels-edition/test/'\n\nROWS = 128\nCOLS = 128\nCHANNELS = 3\nprint(os.listdir(TRAIN_DIR)[:5])\ntrain_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\ntrain_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\ntrain_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n\ntest_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n\n\n# slice datasets for memory efficiency on Kaggle Kernels, delete if using full dataset\ntrain_images = train_dogs[:] + train_cats[:]\nrandom.shuffle(train_images)\ntest_images =  test_images[:]\n\ndef read_image(file_path):\n    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n\n\ndef prep_data(images):\n    count = len(images)\n    data = np.ndarray((count, CHANNELS, ROWS, COLS), dtype=np.uint8)\n\n    for i, image_file in enumerate(images):\n        image = read_image(image_file)\n        data[i] = image.T\n        if i%250 == 0: print('Processed {} of {}'.format(i, count))\n    \n    return data\n\ntrain = prep_data(train_images)\ntest = prep_data(test_images)\n\nprint(\"Train shape: {}\".format(train.shape))\nprint(\"Test shape: {}\".format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"747d8040ea3ee07e384cf165c0be7a170236da5c","_cell_guid":"b0a175cc-7d69-45df-81b2-c2ce1f5fb024","trusted":true,"collapsed":true},"cell_type":"code","source":"labels = []\nfor i in train_images:\n    if 'dog' in i.split('/')[-1]:\n        labels.append(1)\n    else:\n        labels.append(0)\n\nsns.countplot(labels).set_title('Cats and Dogs')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a43f4e8f2ae53117bd4ebcb40f2f3c5d167b5dfa","_cell_guid":"c2ae9c97-c9b1-42ee-a956-adac243927c0","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"def show_cats_and_dogs(idx):\n    cat = read_image(train_cats[idx])\n    dog = read_image(train_dogs[idx])\n    pair = np.concatenate((cat, dog), axis=1)\n    print(cat.shape)\n    plt.figure(figsize=(10,5))\n    plt.imshow(pair)\n    plt.show()\n    \nfor idx in range(0,5):\n    show_cats_and_dogs(idx)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"collapsed":true},"cell_type":"code","source":"dog_avg = np.array([dog[0].T for i, dog in enumerate(train) if labels[i]==1]).mean(axis=0)\nplt.imshow(dog_avg)\nplt.title('Your Average Dog')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0de5ec101e72ab974ce1d6ee07b684a10bed5ff9","collapsed":true},"cell_type":"code","source":"cat_avg = np.array([cat[0].T for i, cat in enumerate(train) if labels[i]==0]).mean(axis=0)\nplt.imshow(cat_avg)\nplt.title('Your Average Cat')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6a8748e65956abf4d6973e5e0945061135b42403"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"526b61430ad18fc95770d2337785b008e475ef06","_kg_hide-input":false,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"from keras.layers import ZeroPadding2D\nfrom keras.layers import BatchNormalization\n\noptimizer = RMSprop(lr=1e-4)\nobjective = 'binary_crossentropy'\n\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', input_shape=(3, ROWS, COLS), activation='relu'))\nmodel.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_first'))\n\nmodel.add(Conv2D(64, kernel_size = (3, 3), padding = 'same', activation='relu'))  \nmodel.add(Conv2D(64, kernel_size = (3, 3), padding = 'same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_first'))\n\nmodel.add(Conv2D(128, kernel_size = (3, 3), padding = 'same', activation='relu'))\nmodel.add(Conv2D(128, kernel_size = (3, 3), padding = 'same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_first'))\n\nmodel.add(Conv2D(256, kernel_size = (3, 3), padding = 'same', activation='relu'))\nmodel.add(Conv2D(256, kernel_size = (3, 3), padding = 'same', activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_first'))\nmodel.add(Conv2D(256, kernel_size = (3, 3), padding = 'same', activation='relu'))\nmodel.add(Conv2D(256, kernel_size = (3, 3), padding = 'same', activation='relu'))\nmodel.add(Conv2D(256, kernel_size = (3, 3), padding = 'same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format='channels_first'))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss=objective, optimizer='sgd', metrics=['accuracy'])\n\nnb_epoch = 15\nbatch_size = 16\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')  \n\nmodel.fit(train, labels, batch_size=batch_size, epochs=nb_epoch,\n          validation_split=0.25, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9a2e11bda023c2a191c54745447e05109a1b9d5","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d9ca1f3a640e76e72e6440cb8b870b09ec2e4f7d"},"cell_type":"code","source":"predictions = model.predict(test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d481d1854a8168b3bb412548b2c318be7aca4bb8","scrolled":false,"collapsed":true},"cell_type":"code","source":"'''nb_epoch = 15\nbatch_size = 16\n\n## Callback for loss logging per epoch\nclass LossHistory(Callback):\n    \n    \n    def on_train_begin(self, logs={}):\n        self.x = 0\n        self.losses = []\n        self.val_losses = []\n        print('Epoch ' + str(self.x) + '/' + str(nb_epoch))\n        print(logs)\n        \n    def on_epoch_end(self, batch, logs={}):\n        self.x+=1\n        print('Epoch ' + str(self.x) + '/' + str(nb_epoch))\n        print(logs)\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        \nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')        \n        \ndef run_catdog():\n    \n    history = LossHistory()\n    model.fit(train, labels, batch_size=batch_size, epochs=nb_epoch,\n              validation_split=0.25, verbose=0, shuffle=True, callbacks=[history, early_stopping])\n    \n\n    predictions = model.predict(test, verbose=0)\n    return predictions, history\n\npredictions, history = run_catdog()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f5cfbb73a48c5687ffa6e188028a8cd0f5516d2b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f290186303ca4a383ba566984580b3e855d8890f","collapsed":true},"cell_type":"code","source":"'''loss = history.losses\nval_loss = history.val_losses\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('VGG16 Loss Trend')\nplt.plot(loss, 'blue', label='Training Loss')\nplt.plot(val_loss, 'green', label='Validation Loss')\nplt.xticks(range(0,nb_epoch)[0::2])\nplt.legend()\nplt.show()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aa57c3dc1c52003790a17d8b6bd42d166146c40","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6a8ce7d8fe8be395f08d7002277dff60e4a9213","scrolled":false,"collapsed":true},"cell_type":"code","source":"# Modify 'test1.jpg' and 'test2.jpg' to the images you want to predict on\n\nfrom keras.models import load_model\nfrom keras.preprocessing import image\nimport pandas as pd\nimport numpy as np\n\n# dimensions of our images\n#img_width, img_height = 224, 224\n\n# load the model we saved\n\n#model.load_weights('../input/model/my_model.h5')\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# predicting images\nids = []\nlabels = []\ndf = []\n#for i in range(len(test_images)):\nfor i in range(len(test_images)):    \n    #fig = read_image(test_images[i])\n    img = test[i]\n    #img = image.load_img('test', target_size=(img_width, img_height))\n    x = np.expand_dims(img, axis=0)\n    '''\n    images = np.vstack([x])\n    classes = model.predict_classes(images, batch_size=10)\n    '''\n    \n    \n    # pass the list of multiple images np.vstack()\n    e = {'id':test_images[i].split('/')[-1].split('.')[0],\n    'label':predictions[i][0]}\n    if i%250==0:\n        print('{} out of {} images labeled.'.format(i, len(test_images)))\n    df.append(e)\n    # print the classes, the images belong to\n    '''\n    print (classes)\n    plt.figure(figsize=(10,5))\n    plt.imshow(fig)\n    plt.show()\n    '''\n    \ndf = pd.DataFrame(df)\nprint(df.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16147bc790d1eb87b786cf2ce0183269163eb525","collapsed":true},"cell_type":"code","source":"df = df.apply(pd.to_numeric, errors='ignore').sort_values('id')\n\ndf.index = np.arange(1, len(df)+1)\ndf = df.drop(columns = 'id')\ndf.index.name = 'id'\ndf.to_csv('submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4db5af32f4dd88d8a0130e63aabd7e2efbede10c","collapsed":true},"cell_type":"code","source":"\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6de303da90c810360a0dbf3923487f574f832e0","scrolled":false,"collapsed":true},"cell_type":"code","source":"for i in range(20,40):\n    if predictions[i, 0] >= 0.5: \n        print('I am {:.2%} sure this is a Dog'.format(predictions[i][0]))\n    else: \n        print('I am {:.2%} sure this is a Cat'.format(1-predictions[i][0]))\n        \n    plt.imshow(test[i].T)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96ccdc5cd5e0cebd33dbeeb3bd8d6fed3725a43e","collapsed":true},"cell_type":"code","source":"print(predictions[:20])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}