{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"**Want to find out how to do the feature enigeering step automatically in a concise and compact tutorial, applying it on a binary classification problem using lightGBM?---look no further **\n\nAfter finding out that data scientists created a tool that \"replaces\" data-scientists I had to try it out. Thank you [https://docs.featuretools.com/](http://)! Feature-engineering is tiresome, and takes the biggest amount of time do it. What if we can make it a one liner. Well now it seems we can. Even more appropriately we will be working on Home Credit Default Risk. A set of datasets where all of them are in a relationship with one-another and from all of them some information should be extracted. Featuretools makes it easy! Our goal in the end is simple. Predict whether the customer will default or not."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import necessary modules\n\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nimport featuretools as ft\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2c49c31ed5606432614445c5f1784bef36ebfa2"},"cell_type":"markdown","source":"Load in the data, NOTE: datasets are huge, working on them will be computationally costly. In order to avoid it we can introduce some limited sample size."},{"metadata":{"trusted":true,"_uuid":"b84eafe53ef747c4157c7efa146e71c17181112f"},"cell_type":"code","source":"# Please note that you could have read it with simple read_csv, without using os (operating system commands...)\nsample_size = 30000 \n\"\"\" Load and process inputs \"\"\"\ninput_dir = os.path.join(os.pardir, 'input')\nprint('Input files:\\n{}'.format(os.listdir(input_dir)))\nprint('Loading data sets...')\napp_train_df = pd.read_csv(os.path.join(input_dir,'application_train.csv'), nrows=sample_size)\napp_test_df = pd.read_csv(os.path.join(input_dir,'application_test.csv'))\nprev_app_df = pd.read_csv(os.path.join(input_dir,'previous_application.csv'), nrows=sample_size)\nbureau_df = pd.read_csv(os.path.join(input_dir,'bureau.csv'), nrows=sample_size)\nbureau_balance_df = pd.read_csv(os.path.join(input_dir,'bureau_balance.csv'), nrows=sample_size)\ninstallments_df = pd.read_csv(os.path.join(input_dir,'installments_payments.csv'), nrows=sample_size)\ncc_balance_df = pd.read_csv(os.path.join(input_dir,'credit_card_balance.csv'), nrows=sample_size)\npos_balance_df = pd.read_csv(os.path.join(input_dir,'POS_CASH_balance.csv'), nrows=sample_size)\n\nprint('Data loaded.\\nMain application training data set shape = {}'.format(app_train_df.shape))\nprint('Main application test data set shape = {}'.format(app_test_df.shape))\nprint('Positive target proportion = {:.2f}'.format(app_train_df['TARGET'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcf665362a616d9eaad5b224c0dba18d795235e6"},"cell_type":"markdown","source":"If we merge datasets now we can perofrm neccesary operations and seperate them later."},{"metadata":{"trusted":true,"_uuid":"3b026acae488f2879d56d06bf96ebac72c798412"},"cell_type":"code","source":"# Merge the datasets into a single one for training\napp_both = pd.concat([app_train_df, app_test_df])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f52a1a64c1045e69fae9877f1bf932631c930624"},"cell_type":"markdown","source":"**NOTE** This NaN handling is just for the sake of it. It is by no-means complete and there are lot of them underneath (function is built that shows us percentage). But there is a specific way that GBM (light and xBGM) handle missing values. So even tough it would be better we want to focus on algortihm and automatic feature engineering!"},{"metadata":{"trusted":true,"_uuid":"20a4767fc44d4746646fea35c89fd761b3a5cddd"},"cell_type":"code","source":"# A lot of the continuous days variables have integers as missing value indicators.\nprev_app_df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fce5138ce46705393f5a4f87ebc2d8a95e73878"},"cell_type":"markdown","source":"**NOTE** Even tough it is automatic, we can incorporate some manual features. IF we know some domain specific information."},{"metadata":{"trusted":true,"_uuid":"e8d9039def06a47a23335265636aad43f0989ef2"},"cell_type":"code","source":"#Add new features\n# Amount loaned relative to salary\napp_both['LOAN_INCOME_RATIO'] = app_both['AMT_CREDIT'] / app_both['AMT_INCOME_TOTAL']\napp_both['ANNUITY_INCOME_RATIO'] = app_both['AMT_ANNUITY'] / app_both['AMT_INCOME_TOTAL']\n    \n# Number of overall payments (I think!)\napp_both['ANNUITY LENGTH'] = app_both['AMT_CREDIT'] / app_both['AMT_ANNUITY']\n    \n# Social features\napp_both['WORKING_LIFE_RATIO'] = app_both['DAYS_EMPLOYED'] / app_both['DAYS_BIRTH']\napp_both['INCOME_PER_FAM'] = app_both['AMT_INCOME_TOTAL'] / app_both['CNT_FAM_MEMBERS']\napp_both['CHILDREN_RATIO'] = app_both['CNT_CHILDREN'] / app_both['CNT_FAM_MEMBERS']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7f591515abb2f47b6b30e13a7802f479f92fcca"},"cell_type":"markdown","source":"***Featuretools*** is an open-source Python library for automatically creating features out of a set of related tables using a technique called deep feature synthesis. Automated feature engineering, like many topics in machine learning, is a complex subject built upon a foundation of simpler ideas. By going through these ideas one at a time, we can build up our understanding of how featuretools which will later allow for us to get the most out of it.\n\nThere are a few concepts that we will cover along the way:\n\n1.  Entities and EntitySets\n2. Relationships between tables\n3. Feature primitives: aggregations and transformations\n4. Deep feature synthesis"},{"metadata":{"trusted":true,"_uuid":"fdfb769a276145282016871696464a80075c3594"},"cell_type":"code","source":"# Create new entityset\nes = ft.EntitySet(id='home_credit_default_risk')\n\n\n# Create an entity from the applications (app_both) dataframe\n# This dataframe already has an index\nes = es.entity_from_dataframe(entity_id='applications',\n                              \n                              dataframe=app_both, index='SK_ID_CURR')\n\n\n# Create an entity from the bureau dataframe\n# This dataframe already has an index\nes = es.entity_from_dataframe(entity_id='bureau', \n                            \n                              dataframe=bureau_df, index='SK_ID_BUREAU')\n\n# Create an entity from the bureau balance dataframe\nes = es.entity_from_dataframe(entity_id='bureau_balance', \n                             \n                              make_index = True,\n                              dataframe=bureau_balance_df, index='bureau_balance_id')\n\n# Create an entity from the installments dataframe\nes = es.entity_from_dataframe(entity_id='installments',\n                              make_index = True,\n                              dataframe=installments_df, index='installment_id')\n\n\n\n# Create an entity from the previous applications dataframe\nes = es.entity_from_dataframe(entity_id='previous_application',\n                             \n                              make_index = True,\n                              dataframe=prev_app_df, index='prev_app_id')\n\n# Create an entity from the credit card balance dataframe\nes = es.entity_from_dataframe(entity_id='cc_balance',\n                         \n                              make_index = True,\n                              dataframe=cc_balance_df, index='cc_balance_id')\n\n# Create an entity from the POS Cash balance dataframe\nes = es.entity_from_dataframe(entity_id='pos_balance',\n\n                              make_index = True,\n                              dataframe=pos_balance_df, index='pos_balance_id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6863e0a4a331b8dba833f64c8ed4718452d66509"},"cell_type":"markdown","source":"**2. Relationships betweeen the sets**"},{"metadata":{"trusted":true,"_uuid":"397f819b10bd2a7929c95351524c6ad01b9d81a7"},"cell_type":"code","source":"# Relationship between applications and credits bureau\nr_applications_bureau = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['bureau']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_bureau)\n\n# Relationship between applications and credits bureau\nr_applications_installment = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['installments']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_installment)\n\n# Relationship between applications and credits bureau\nr_bureau_bureaubalance = ft.Relationship(es['bureau']['SK_ID_BUREAU'],\n                                    es['bureau_balance']['SK_ID_BUREAU'])\nes = es.add_relationship(r_bureau_bureaubalance)\n\n# Relationship between applications and previous applications\nr_applications_prev_apps = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['previous_application']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_prev_apps)\n\n# Relationship between applications and credit card balance\nr_applications_cc_balance = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['cc_balance']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_cc_balance)\n\n# Relationship between applications and POS cash balance\nr_applications_pos_balance = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['pos_balance']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_pos_balance)\n\nprint(es)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"568cc4ed19f086f09144c2c0d5c55c3accf703f0"},"cell_type":"markdown","source":"**Feature primitives** Basically which functions are we going to use to create features. Since we did not specify it we will be using standard ones (check doc) There is a option to define own ones or to just select some of the standards."},{"metadata":{"trusted":true,"_uuid":"9c4d0d94520b40d00bdd31bc5d100be560de2ff7"},"cell_type":"code","source":"\"\"\"\nDeep Feature Synthesis (DFS) is an automated method for performing feature engineering on relational and transactional data.\nhttps://docs.featuretools.com/automated_feature_engineering/afe.html\n\"\"\"\n# Create new features using specified primitives\nfeature_matrix, feature_defs = ft.dfs(entityset = es, target_entity = 'applications',\n                                      drop_contains=['SK_ID_PREV'], max_depth=2, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f2c8903de18c743240d7405b45ef72a53f3beac"},"cell_type":"code","source":"feature_matrix.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c40e3d2104dbad644ef21146555c5834bea9eb1d"},"cell_type":"markdown","source":"**Label encoding** Making it machine readable"},{"metadata":{"trusted":true,"_uuid":"696fb367b86558aaf1f69d36fefdcb1717f37705"},"cell_type":"code","source":"def process_dataframe(input_df, encoder_dict=None):\n    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n\n    # Label encode categoricals\n    print('Label encoding categorical features...')\n    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n    for feat in categorical_feats:\n        encoder = LabelEncoder()\n        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n    print('Label encoding complete.')\n\n    return input_df, categorical_feats.tolist(), encoder_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b13bcdf47c539865fa87b836ca8ee3450e33049"},"cell_type":"code","source":"feature_matrix_enc, categorical_feats, encoder_dict = process_dataframe(input_df=feature_matrix)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c6bc1a740ab808e6f7d35a12b1ddfa920dce0aa"},"cell_type":"markdown","source":"**NaN imputation** will be skipped in this tutorial."},{"metadata":{"trusted":true,"_uuid":"b8d1c150e5e296e916ccef90f21f56c1f54abb2a"},"cell_type":"code","source":"all_data_na = (feature_matrix_enc.isnull().sum() / len(feature_matrix_enc)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54a68df24e8c458acccfd4c614bd0fa0df167999"},"cell_type":"markdown","source":"Let us split the variables one more time."},{"metadata":{"trusted":true,"_uuid":"ab7d41aa07fc7307604debc6dcfdf432759d9dbc"},"cell_type":"code","source":"# Separate into train and test\ntrain_df = feature_matrix_enc[feature_matrix_enc['TARGET'].notnull()].copy()\n\ntest_df = feature_matrix_enc[feature_matrix_enc['TARGET'].isnull()].copy()\ntest_df.drop(['TARGET'], axis=1, inplace=True)\n\ndel feature_matrix, feature_defs, feature_matrix_enc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bac3276d52e668921d1d6f46e6415a04f500c124"},"cell_type":"markdown","source":"**Train** the model, predict, etc."},{"metadata":{"trusted":true,"_uuid":"60c57ed000ac073b1d1076401dc9555e5f1ae8e0"},"cell_type":"code","source":"\"\"\" Train the model \"\"\"\ntarget = train_df.pop('TARGET')\n\nlgbm_train = lgbm.Dataset(data=train_df,\n                          label=target,\n                          categorical_feature=categorical_feats,\n                          free_raw_data=False)\nlgbm_params = {\n    'boosting': 'dart',\n    'application': 'binary',\n    'learning_rate': 0.1,\n    'min_data_in_leaf': 30,\n    'num_leaves': 31,\n    'max_depth': -1,\n    'feature_fraction': 0.5,\n    'scale_pos_weight': 2,\n    'drop_rate': 0.02\n}\n\ncv_results = lgbm.cv(train_set=lgbm_train,\n                     params=lgbm_params,\n                     nfold=5,\n                     num_boost_round=600,\n                     early_stopping_rounds=50,\n                     verbose_eval=20,\n                     metrics=['auc'])\n\noptimum_boost_rounds = np.argmax(cv_results['auc-mean'])\nprint('Optimum boost rounds = {}'.format(optimum_boost_rounds))\nprint('Best CV result = {}'.format(np.max(cv_results['auc-mean'])))\n\nclf = lgbm.train(train_set=lgbm_train,\n                 params=lgbm_params,\n                 num_boost_round=optimum_boost_rounds)\n\n\"\"\" Predict on test set and create submission \"\"\"\ny_pred = clf.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe1817f7977a21200b195467fd798bb318ba44b1"},"cell_type":"code","source":"out_df = pd.DataFrame({'SK_ID_CURR': test_df.index, 'TARGET': y_pred})\nout_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"859d7c583dde5c88f146206df233d15146bd1840"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}