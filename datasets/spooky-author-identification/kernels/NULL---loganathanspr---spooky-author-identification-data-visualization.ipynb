{"cells": [{"outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "3beea829-117b-404f-a5ba-b704c0723831", "_uuid": "0bc2753ac8c66f5e17d43e2e1f271a6f0a2d4b98"}}, {"source": ["# Imports\n", "\n", "\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "cff64719-3f4a-428c-90aa-73ff8824217c", "_uuid": "4a2dbe6454aa21f564bea98b3fc244a2f2bc233c"}}, {"outputs": [], "source": ["from os import path\n", "import string\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "from plotly import tools\n", "from plotly import __version__\n", "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n", "import plotly.graph_objs as go\n", "print(__version__)\n", "init_notebook_mode(connected=True)\n", "\n", "import nltk\n", "from nltk import word_tokenize\n", "from nltk.stem import WordNetLemmatizer\n", "from nltk.stem.snowball import SnowballStemmer\n", "from nltk.corpus import stopwords\n", "\n", "# Spacy\n", "import spacy\n", "\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n", "\n", "from sklearn.metrics.pairwise import cosine_distances\n", "from sklearn.manifold import MDS\n", "from sklearn.manifold import TSNE\n", "\n", "from PIL import Image\n", "from wordcloud import WordCloud, STOPWORDS\n", "import urllib\n", "from io import BytesIO"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "4b4f0f32-66e5-4dfb-8233-d4327a2e553a", "_kg_hide-input": false, "_uuid": "fb4e017c6ef80660e8c8a8955a28a6c11c4d96b1"}}, {"source": ["# Initialization"], "cell_type": "markdown", "metadata": {"_cell_guid": "c5e79d89-b32a-48a2-a8a1-32c9f7093bab", "_uuid": "3361d03c8b1ab0477debe5944fca9b9c4898f089"}}, {"outputs": [], "source": ["# NLTK \n", "wnl = WordNetLemmatizer()\n", "sb_stemmer = SnowballStemmer(\"english\")\n", "\n", "# Spacy\n", "nlp = spacy.load('en')\n", "\n", "# Plotly\n", "author_images = [\n", "    \"https://upload.wikimedia.org/wikipedia/commons/8/84/Edgar_Allan_Poe_daguerreotype_crop.png\",\n", "    \"https://upload.wikimedia.org/wikipedia/commons/6/65/RothwellMaryShelley.jpg\",\n", "    \"https://upload.wikimedia.org/wikipedia/commons/1/10/H._P._Lovecraft%2C_June_1934.jpg\"\n", "]\n", "\n", "paper_bgcolor=\"rgb(240,240,240)\"\n", "plot_bgcolor=\"rgb(240,240,240)\"\n", "\n", "# color definitions (from colorlover module)\n", "# import colorlover as cl\n", "# cl.scales[\"3\"][\"div\"][\"RdBu\"]\n", "rd_bu = ['rgb(239,138,98)', 'rgb(247,247,247)', 'rgb(103,169,207)']"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "f29820ec-9a79-46c2-8288-5778f53f4e5b", "_kg_hide-input": false, "_uuid": "c46802a01c05f1cb176827e52e090bd7412e18f7"}}, {"source": ["## Load data"], "cell_type": "markdown", "metadata": {"_cell_guid": "4e73fc3f-b0dc-4beb-89b4-d8d5adbcd5d7", "_uuid": "92b052bf6b6045d508dfc65dcb5e4a7ea73020ca"}}, {"outputs": [], "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "40d3b364-394e-4534-b0d9-a62fd9f70bd7", "_kg_hide-input": false, "_uuid": "ef06ebfe8a1115ae1af8f9c09e0c37496c2be073"}}, {"source": ["# Preprocessing\n", "* Reduce the number of instances per class to 1/20 th th to make it easy for visualizing through T-SNE.\n", "* Before preprocessing, if class A has 220 examples, and class B has 401 examples, after preprocessing, there will be 11 examples for class A and 21 examples for class B, respectively.\n", "\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "b8801cd9-c982-4444-a43c-506d363bfca9", "_uuid": "6670003b649f879da0c13a87e9b96e6633858f3f"}}, {"outputs": [], "source": ["def shrink_train_data_size_by_factor(factor, text_arr):\n", "    text_arr_reduced = []\n", "    for i in range(len(text_arr) // factor):\n", "        start = i * factor\n", "        end = (start + factor) - 1\n", "        text_arr_reduced.append(\"\\n\".join(text_arr[start:end+1]))\n", "    if len(text_arr) % factor != 0:\n", "        rem_elments = len(text_arr) % factor\n", "        start = (len(text_arr) // factor) * factor\n", "        end = (start + rem_elments) - 1\n", "        text_arr_reduced.append(\"\\n\".join(text_arr[start:end+1]))\n", "    return text_arr_reduced"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "f34854f0-a5d7-427c-a2f8-f81888430680", "_kg_hide-input": false, "_uuid": "6e6e1eb01af16ca692dc340d74e091f873e59c92"}}, {"outputs": [], "source": ["counts_by_author = train[\"author\"].value_counts()\n", "author_names = list(counts_by_author.index)\n", "instances_per_author = list(counts_by_author)\n", "\n", "# make copy of the original training data set\n", "counts_by_author_orig = list(counts_by_author)\n", "author_names_orig = list(author_names)\n", "instances_per_author_orig = list(instances_per_author)\n", "train_orig = train.copy()\n", "\n", "# factor to which the train data should be reduced\n", "factor = 20\n", "\n", "# temporary variables to hold the reduced data\n", "shrinked_train = []\n", "shrinked_labels = []\n", "\n", "for i in range(len(author_names)):\n", "    instances = train[train[\"author\"] == author_names[i]][\"text\"].as_matrix()\n", "    instances_red = shrink_train_data_size_by_factor(factor, instances)\n", "    labels_red = [author_names[i]] * len(instances_red)\n", "    shrinked_train += instances_red\n", "    shrinked_labels += labels_red\n", "\n", "# training data (shrinked)\n", "train = pd.DataFrame({\"text\": shrinked_train, \"author\": shrinked_labels})"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "e8390e07-c795-46c6-8b1a-bc85e5d757c8", "_kg_hide-input": false, "_uuid": "373dc2bf350580e457c6badd0b8905fa153a91fd"}}, {"outputs": [], "source": ["counts_by_author = train[\"author\"].value_counts()\n", "author_names = list(counts_by_author.index)\n", "instances_per_author = list(counts_by_author)\n", "print(\"Original training data stats\")\n", "print(\"----------------------------\")\n", "print(\"Training data size: \", train_orig.shape)\n", "print(\"Author names:\", author_names_orig)\n", "print(\"Instances per author:\", instances_per_author_orig)\n", "\n", "print(\"\\nAfter shrinking the training data by factor: {}\".format(factor))\n", "print(\"------------------------------------------------\")\n", "print(\"Training data size: \", train.shape)\n", "print(\"Author names:\", author_names)\n", "print(\"Instances per author:\", instances_per_author)\n", "train.head()"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "42695101-d7dd-4c70-9539-dd9958da7d6e", "_kg_hide-input": false, "_uuid": "68bf171d314758eb29f6b262ef236e35fe918783"}}, {"outputs": [], "source": ["# concatenate texts belonging to same author\n", "combined_texts = []\n", "for author in author_names: \n", "    texts_of_author = train[train[\"author\"] == author][\"text\"]\n", "    texts_np_array = texts_of_author.as_matrix()\n", "    text_together = \" \".join(texts_np_array)\n", "    combined_texts.append(text_together)\n", "combined_texts = np.asarray(combined_texts)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "9513ad07-9755-4b2b-a027-0b32e69d54f5", "_kg_hide-input": false, "_uuid": "5ebd061dd8c4ccfc1c07d5a2432edead55f8a2e5"}}, {"source": ["# EDA\n", "\n", "1. General summary: number of instances per class\n", "1. Corpus level insights"], "cell_type": "markdown", "metadata": {"_cell_guid": "1e9915f9-7ab9-429a-9cf6-e7099d845137", "_uuid": "31316714d1ee243fc99001770bb555ffe11c644c"}}, {"outputs": [], "source": ["layout_images = []\n", "for i in range(len(author_images)):\n", "    layout_image = dict( source = author_images[i], \n", "                        xref=\"paper\", \n", "                        yref=\"paper\", \n", "                        x= (instances_per_author_orig[i] / max(instances_per_author_orig)) - 0.15, \n", "                        y = (i / len(author_images))+ 0.05, \n", "                        sizex=0.4,\n", "                        sizey=0.2,\n", "                        xanchor=\"left\", \n", "                        yanchor=\"bottom\")\n", "    layout_images.append(layout_image)\n", "        \n", "bar_data = [go.Bar(\n", "            x=instances_per_author_orig,\n", "            y=author_names_orig,\n", "            orientation = 'h'\n", ")]\n", "layout = go.Layout(title = \"Training data distribution (original data)\", images=layout_images,\n", "    xaxis=dict(title=\"# of training instances\"),\n", "    yaxis=dict(title=\"Author\"))\n", "fig = dict(data=bar_data, layout=layout)\n", "iplot(fig)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "4fef2cdd-1d7c-4e77-85bb-65a84c055813", "scrolled": false, "_kg_hide-input": false, "_uuid": "ff2dda3ccb23e99372f1ca9f6ff0befe4c9bcbb0"}}, {"outputs": [], "source": ["layout_images = []\n", "for i in range(len(author_images)):\n", "    layout_image = dict( source = author_images[i], \n", "                        xref=\"paper\", \n", "                        yref=\"paper\", \n", "                        x= (instances_per_author[i] / max(instances_per_author)) - 0.15, \n", "                        y = (i / len(author_images))+ 0.05, \n", "                        sizex=0.4,\n", "                        sizey=0.2,\n", "                        xanchor=\"left\", \n", "                        yanchor=\"bottom\")\n", "    layout_images.append(layout_image)\n", "        \n", "bar_data = [go.Bar(\n", "            x=instances_per_author,\n", "            y=author_names,\n", "            orientation = 'h'\n", ")]\n", "layout = go.Layout(title = \"Training data distribution (after shrinking)\", images=layout_images,\n", "    xaxis=dict(title=\"# of training instances\"),\n", "    yaxis=dict(title=\"Author\"))\n", "fig = dict(data=bar_data, layout=layout)\n", "iplot(fig)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "780abe3f-e6a4-44f2-aea0-6318199bfabe", "_kg_hide-input": false, "_uuid": "942b8e34e8493a098441cd1f45be45294313ce85"}}, {"source": ["## Corpus level insights\n", "1. Heatmap of sentence lengths of training corpus for each author."], "cell_type": "markdown", "metadata": {"_cell_guid": "2bc880a9-93ce-43a9-8aa0-b569de7800fb", "_uuid": "5c2bbd48c92341bfd85c82e4a413cdebd827000c"}}, {"outputs": [], "source": ["stop_words_en = set(stopwords.words('english'))"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "bd917657-4175-4678-98d7-ccaf1e7fe845", "_kg_hide-input": false, "_uuid": "1d4091d92bd6b3141b374d845add40a9d32ab6d5"}}, {"outputs": [], "source": ["def avg_sentence_length_for_text(text):\n", "    \"\"\"Given a text containing one more sentences in the form of \n", "    paragraph, the function returns the average sentence length\n", "    for the entire paragraph.\n", "    \"\"\"\n", "    sentences = nltk.sent_tokenize(text)\n", "    num_tokens_in_corpus = 0\n", "    for s in sentences:\n", "        num_tokens_in_corpus += len(nltk.word_tokenize(s))\n", "    return float(num_tokens_in_corpus) / len(sentences)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "2692825b-f0ff-4795-86bc-88c09a1bf582", "_kg_hide-input": false, "_uuid": "140c4a8dc9f027f0cad46fd6039b570e3c1964b3"}}, {"outputs": [], "source": ["def avg_stop_words_per_sentence(text):\n", "    \"\"\"Given a text containing one more sentences in the form of \n", "    paragraph, the function returns the average number of function words\n", "    per sentence in the given text\n", "    \"\"\"    \n", "    sentences = nltk.sent_tokenize(text)\n", "    num_stop_words_in_corpus = 0\n", "    for s in sentences:\n", "        s_tokens =  nltk.word_tokenize(s.lower())\n", "        for w in s_tokens:\n", "            if w in stop_words_en: \n", "                num_stop_words_in_corpus += 1\n", "    return float(num_stop_words_in_corpus) / len(sentences)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "d9434db8-3ba7-41aa-baf9-e839214df8d1", "_kg_hide-input": false, "_uuid": "1aaa7f69a276a3a761c3a152abe7a2ba8160b19d"}}, {"outputs": [], "source": ["def make_array_size_divisible_by_factor(oned_nparray, fc):\n", "    \"\"\"Given an 1d array of an arbitrary size, make the shape of the \n", "    1d array divisible by 100 by appending np.nan values. For ex: If the array \n", "    dimension is (114, ) then the np.nan 1d array of shape 86 will \n", "    be appended to the original 1d array to become array size 200. \n", "    \"\"\"\n", "    remainder = oned_nparray.shape[0] % fc\n", "    if remainder > 0:\n", "        cells_to_fill = fc - remainder\n", "        nan_array = np.full(cells_to_fill, np.nan)\n", "        oned_nparray = np.append(oned_nparray, nan_array)\n", "    num_cols_heatmap = int(oned_nparray.shape[0] / fc)\n", "    num_rows_heatmap = fc\n", "    return oned_nparray.reshape((num_rows_heatmap, num_cols_heatmap))"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "cbcb8558-a736-4f85-a8cc-2328cc05302e", "_kg_hide-input": false, "_uuid": "8ef8cba5f702062c20c2e68721c92870c33126df"}}, {"outputs": [], "source": ["def get_reshaped_sen_len_trace_for_author(author_name):\n", "    sen_lengths_for_author = train_copy[train_copy[\"author\"] == author_name]\n", "    sen_lengths_nparray = sen_lengths_for_author[\"sen_len\"].as_matrix()\n", "    sen_lengths_nparray_reshaped = make_array_size_divisible_by_factor(sen_lengths_nparray, 25)    \n", "    return sen_lengths_nparray_reshaped"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "f6e95eb3-1dd2-4c4d-b6fa-5d340df42bc9", "_kg_hide-input": false, "_uuid": "1d43f02292a568ec37e56342c908e90bda49f674"}}, {"outputs": [], "source": ["def get_reshaped_stop_words_trace_for_author(author_name):\n", "    stop_words_for_author = train_copy[train_copy[\"author\"] == author_name]\n", "    stop_words_counts_nparray = stop_words_for_author[\"stop_words\"].as_matrix()\n", "    stop_words_counts_nparray_reshaped = make_array_size_divisible_by_factor(stop_words_counts_nparray, 25)    \n", "    return stop_words_counts_nparray_reshaped"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "69e352ca-1a47-4a9a-b82f-728367f7e4d0", "_kg_hide-input": false, "_uuid": "b1ab9f0068343c1f2fb1d2ed755bde85be420f46"}}, {"outputs": [], "source": ["train_copy = train.copy()\n", "train_copy[\"sen_len\"] = train_copy[\"text\"].apply(avg_sentence_length_for_text)\n", "train_copy[\"stop_words\"] = train_copy[\"text\"].apply(avg_stop_words_per_sentence)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "b67b856e-501a-4c89-9876-c0a0ea5e5e2e", "_kg_hide-input": false, "_uuid": "db123544601794970fef802b1037235c5dc62db3"}}, {"outputs": [], "source": ["avg_sen_len_used_by_authors = []\n", "for author in author_names:\n", "    temp = train_copy[train_copy[\"author\"] == author]\n", "    num_tokens = temp[\"sen_len\"].sum()\n", "    avg_sen_len = float(num_tokens) / temp.shape[0]\n", "    avg_sen_len_used_by_authors.append(avg_sen_len)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "eca8011c-b36d-470b-9418-b2af08b96c01", "_kg_hide-input": false, "_uuid": "b9dee2e679a540e02f4b5929c4082d93dd57d796"}}, {"outputs": [], "source": ["fig_coords = [(1,1), (1,2), (1,3)]\n", "axes_names = [(\"x1\", \"y1\"), (\"x2\", \"y2\"), (\"x3\", \"y3\")]\n", "axes_lo_names = [(\"xaxis1\", \"yaxis1\"), (\"xaxis2\", \"yaxis2\"), (\"xaxis3\", \"yaxis3\")]\n", "fig = tools.make_subplots(rows=1, cols=3, subplot_titles=(author_names[0], author_names[1], author_names[2]))\n", "for author, fig_coord, ax in zip(author_names, fig_coords, axes_names):\n", "    reshaped_sen_lengths = get_reshaped_sen_len_trace_for_author(author)\n", "    trace = go.Heatmap(z=reshaped_sen_lengths, colorscale = 'Portland', zmin=0, zmax=80, xaxis=ax[0], yaxis=ax[1])\n", "    fig.append_trace(trace, fig_coord[0], fig_coord[1])\n", "fig[\"layout\"].update(title = \"Average sentence lengths (# of words)\")\n", "for ax_name in axes_lo_names:\n", "    fig[\"layout\"][ax_name[0]].update(showgrid=False, showline=False, zeroline=False, ticks='', showticklabels=False)\n", "    fig[\"layout\"][ax_name[1]].update(showgrid=False, showline=False, zeroline=False, ticks='', showticklabels=False)    \n", "iplot(fig)\n", "\n", "fig_coords = [(1,1), (1,2), (1,3)]\n", "axes_names = [(\"x1\", \"y1\"), (\"x2\", \"y2\"), (\"x3\", \"y3\")]\n", "axes_lo_names = [(\"xaxis1\", \"yaxis1\"), (\"xaxis2\", \"yaxis2\"), (\"xaxis3\", \"yaxis3\")]\n", "fig = tools.make_subplots(rows=1, cols=3, subplot_titles=(author_names[0], author_names[1], author_names[2]))\n", "for author, fig_coord, ax in zip(author_names, fig_coords, axes_names):\n", "    reshaped_stop_word_counts = get_reshaped_stop_words_trace_for_author(author)\n", "    trace = go.Heatmap(z=reshaped_stop_word_counts, colorscale = 'Portland', zmin=0, zmax=35, xaxis=ax[0], yaxis=ax[1])\n", "    fig.append_trace(trace, fig_coord[0], fig_coord[1])\n", "fig[\"layout\"].update(title = \"Average number of stop words per sentence\")\n", "for ax_name in axes_lo_names:\n", "    fig[\"layout\"][ax_name[0]].update(showgrid=False, showline=False, zeroline=False, ticks='', showticklabels=False)\n", "    fig[\"layout\"][ax_name[1]].update(showgrid=False, showline=False, zeroline=False, ticks='', showticklabels=False)\n", "iplot(fig)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "3470213a-d372-44ca-a7a2-3de5a8c09c17", "scrolled": false, "_kg_hide-input": false, "_uuid": "b7189c9e8d917d7796eac95aa6131beb893e9a3c"}}, {"source": ["# How close authors are to each other?"], "cell_type": "markdown", "metadata": {"_cell_guid": "72e5b16c-3d1b-42eb-9f3a-4f2c8e4eb40f", "_uuid": "1b034348528dc7fecb3f11b09fb50287b0954744"}}, {"outputs": [], "source": ["counts_vectorizer = CountVectorizer(stop_words=\"english\", min_df=3)\n", "counts_comb = counts_vectorizer.fit_transform(combined_texts)\n", "\n", "tfidf_comb = TfidfTransformer().fit_transform(counts_comb)\n", "\n", "cosine_dist = cosine_distances(tfidf_comb)\n", "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n", "pos = mds.fit_transform(cosine_dist)\n", "xs, ys = pos[:,0], pos[:, 1]\n", "data = []\n", "for i in range(len(author_names)): \n", "    trace = go.Scatter(x=[xs[i]], y=[ys[i]], mode=\"markers\",\n", "                       marker= dict(size= 20, line= dict(width=1), color= rd_bu[i]),\n", "                       name= author_names[i])\n", "    data.append(trace)\n", "layout = go.Layout(title=\"How close authors are to each other?\")\n", "fig = go.Figure(data=data, layout=layout)\n", "iplot(fig)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "34d8a9e8-6373-47e3-989f-ea9bd7369597", "_kg_hide-input": false, "_uuid": "801383203bd762212cef6c5849b8c664968b1b05"}}, {"source": ["# Topic modeling"], "cell_type": "markdown", "metadata": {"collapsed": true, "_cell_guid": "4fcf96b9-cbd7-4909-bf9d-d88200945d88", "_uuid": "b4f3ab82b88ae6460fc5541a5cf797a7d4551219"}}, {"outputs": [], "source": ["def is_valid_token(tok):\n", "    \"\"\"The function returns false, if \n", "    1. tok length is < 3\n", "    2. tok contains non-alphabetic characters\n", "    \"\"\"\n", "    if not tok.isalpha(): \n", "        return False \n", "    if len(tok) < 4: \n", "        return False\n", "    return True"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "861d4a4b-2200-422c-9335-b295883340d8", "_kg_hide-input": false, "_uuid": "adac8bd300015f70d8143d35cbf3365346c6778a"}}, {"outputs": [], "source": ["def lemmatize(input_str):\n", "    tokens = word_tokenize(input_str)\n", "    tokens = [t for t in tokens if is_valid_token(t) is True]\n", "    lemmatized = [wnl.lemmatize(t) for t in tokens]\n", "    return lemmatized"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "56531189-a1fa-4fb3-896a-c5c740ffcc15", "_kg_hide-input": false, "_uuid": "be5c7410278349403d2d577b55d9dc7cd01addc1"}}, {"outputs": [], "source": ["num_topics = 20\n", "\n", "vectorizer_ind = CountVectorizer(stop_words=\"english\", min_df=5, tokenizer=lemmatize)\n", "counts_ind = vectorizer_ind.fit_transform(train[\"text\"])\n", "vocab_ind = np.array(vectorizer_ind.get_feature_names())\n", "tfidf_ind = TfidfTransformer().fit_transform(counts_ind)\n", "\n", "clf = NMF(n_components=num_topics)\n", "doctopic = clf.fit_transform(tfidf_ind)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "8ed56076-0ed5-4638-8943-1d0c79db14cb", "scrolled": false, "_kg_hide-input": false, "_uuid": "b9ac1ff675beba99da12953adbf35f8ba6b1523c"}}, {"outputs": [], "source": ["# words associated with topics\n", "topic_strings = []\n", "for topic in clf.components_:\n", "    word_idx = np.argsort(topic)[::-1][0:10]\n", "    topic_words = [vocab_ind[i] for i in word_idx]\n", "    topic_strings.append(\" \".join(topic_words))\n", "    #topic_words.append([vocab_ind[i] for i in word_idx])\n", "\n", "authors_of_train_data = train[\"author\"].values\n", "doctopic_one_per_author = np.zeros((len(author_names), num_topics))\n", "for i in range(len(author_names)): \n", "    doctopic_one_per_author[i, :] = np.mean(doctopic[authors_of_train_data == author_names[i], :], axis=0)\n", "\n", "doctopic_one_per_author = doctopic_one_per_author / np.sum(doctopic_one_per_author, axis=1, keepdims=True)\n", "\n", "for t in range(len(topic_strings)):\n", "    print(\"Topic {}: {}\".format(t+1, topic_strings[t]))"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "4d128b57-d04e-451e-8849-3718d355ec22", "_kg_hide-input": false, "_uuid": "ca6ff68c4ba44c9b307658bca77f4ef1e52f02e9"}}, {"source": ["# Main themes of authors in their texts "], "cell_type": "markdown", "metadata": {"collapsed": true, "_cell_guid": "5256cc2b-ff69-4b92-9ee3-ed9eb3e16a46", "_uuid": "8836d72de676c6e4db82948e31d51888d43590de"}}, {"source": ["## Heatmap of topics"], "cell_type": "markdown", "metadata": {"collapsed": true, "_cell_guid": "53efbcbb-e08c-4ce8-bc78-3d64b7c534f7", "_uuid": "21785cb5d76adb31412623c43ecfb8587af698c5"}}, {"outputs": [], "source": ["topic_headers = [ \"topic-\"+str(i) for i in range(1, num_topics+1)]\n", "trace = go.Heatmap(z=doctopic_one_per_author.T, x = author_names, y=topic_strings, colorscale='Portland')\n", "layout = go.Layout(height=600, \n", "                   width=900,\n", "                   margin=go.Margin(l=400, t=50, r=150, b=200), title=\"Heatmap of topics\")\n", "data = [trace]\n", "fig = go.Figure(data=data, layout=layout)\n", "iplot(fig)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "9229919e-8bf4-48a7-bf0d-db6d0e8a2a07", "scrolled": false, "_kg_hide-input": false, "_uuid": "df90293ee76cf77289e02be1f10d0bc5a698b292"}}, {"source": ["## Visual clustering with SVD, TSNE"], "cell_type": "markdown", "metadata": {"collapsed": true, "_cell_guid": "03859585-4fac-4bf8-ab62-247ea9c91578", "_uuid": "8e248b453e2765051854ce51e830ae596c6e39dd"}}, {"outputs": [], "source": ["tsne_doctopic_model = TSNE(n_components=2)\n", "tsne_doctopic_output = tsne_doctopic_model.fit_transform(doctopic)\n", "\n", "svd_doctopic_model = TruncatedSVD(n_components=2)\n", "svd_doctopic_output = svd_doctopic_model.fit_transform(doctopic)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "b4a04097-e7a7-4c48-8bbd-fb63b90525c3", "_kg_hide-input": false, "_uuid": "9287f143898d429ece3b928923113792c14192a4"}}, {"outputs": [], "source": ["fig = tools.make_subplots(rows=1, cols=2, subplot_titles=(\"SVD\", \"TSNE\"))\n", "fig_coords = [(1,1), (1,2)]\n", "for i in range(len(author_names)):\n", "    x_points = svd_doctopic_output[authors_of_train_data == author_names[i], 0]\n", "    y_points = svd_doctopic_output[authors_of_train_data == author_names[i], 1]\n", "    trace = go.Scatter(x=x_points, y=y_points, mode=\"markers\",\n", "                           marker= dict(size= 10, line= dict(width=0.5), color= rd_bu[i]),\n", "                           name= author_names[i])\n", "    fig.append_trace(trace, 1, 1)\n", "for i in range(len(author_names)):\n", "    x_points = tsne_doctopic_output[authors_of_train_data == author_names[i], 0]\n", "    y_points = tsne_doctopic_output[authors_of_train_data == author_names[i], 1]\n", "    trace = go.Scatter(x=x_points, y=y_points, mode=\"markers\",\n", "                           marker= dict(size= 10, line= dict(width=0.5), color= rd_bu[i]),\n", "                           name= author_names[i], showlegend=False)\n", "    fig.append_trace(trace, 1, 2)\n", "fig['layout'].update(height=500, width=800, title=\"Visualizing training instances in reduced dimension\")\n", "iplot(fig)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "a3d7edca-49c4-408a-b985-e74a58705125", "_kg_hide-input": false, "_uuid": "393174c896729d4a2ff79e758c9e1e380bc3c89b"}}, {"source": ["# References\n", "\n", "1. Text Analysis with Topic Models for the Humanities and Social Sciences - https://de.dariah.eu/tatom/\n", "2. Literature Fingerprinting: A New Method for Visual Literary Analysis - http://ieeexplore.ieee.org/document/4389004/\n", "3. Overview of Text Visualization Techniques - Springer"], "cell_type": "markdown", "metadata": {"collapsed": true, "_cell_guid": "908a71f5-1abb-49ca-b1a9-2ea239e64d89", "_uuid": "4cacc3a13da3aa1bf0bbaf9ebfcb8a30e346dbcc"}}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "pygments_lexer": "ipython3"}}}