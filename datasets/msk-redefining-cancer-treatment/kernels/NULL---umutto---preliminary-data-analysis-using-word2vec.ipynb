{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"pygments_lexer": "ipython3", "version": "3.6.1", "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}}, "anaconda-cloud": {}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "735da42a-f22f-4593-8064-e84aeed4ad8a", "_uuid": "6db1914e33b0978c91f82ba1f39763c03e8b6bb2"}, "cell_type": "markdown", "source": ["# A simple anaysis of the dataset using nltk and Word2Vec\n", "This notebook goes over the dataset in the following order:\n", "- Read the data into a dataframe using pandas library.\n", "- Cleaning unnecessary data (unique or null columns).\n", "- Analyzing data distributions.\n", "- Analyzing text data via keywords and summarization.\n", "- Tokenizing (Lemmatization and stopwording) for further analysis.\n", "- Analyzing word distributions for any surface correlations.\n", "- Creating a word cloud of the whole text.\n", "- Using Word2Vec to check the correlation between text and the classes.\n", "  \n", "------  \n", "**Disclaimer:** I couldn't find a way to upload the trained word2vec weight vectors to kaggle kernel, so I just attached the results as markdown. More thorough version can be found on my [github](https://github.com/umutto/Kaggle-Personalized-Medicine/blob/master/data_analysis.ipynb).  \n", "  \n", "*This kernel has been tested with python 3.6 (x64) on Windows.*"]}, {"metadata": {"collapsed": true, "_cell_guid": "68bc6008-0c54-4242-88e8-ece657edafbf", "_uuid": "2743be3dcb624cc8038a878530d95b96aadb74e9"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["%matplotlib inline\n", "\n", "# Data wrapper libraries\n", "import pandas as pd\n", "import numpy as np\n", "\n", "# Visualization Libraries\n", "import matplotlib.pyplot as plt\n", "from matplotlib.patches import Patch\n", "from matplotlib.markers import MarkerStyle\n", "import seaborn as sns\n", "\n", "# Text analysis helper libraries\n", "from gensim.summarization import summarize\n", "from gensim.summarization import keywords\n", "\n", "# Text analysis helper libraries for word frequency etc..\n", "from nltk.tokenize import word_tokenize\n", "from nltk.stem import WordNetLemmatizer\n", "from nltk.corpus import stopwords\n", "from string import punctuation\n", "\n", "# Word cloud visualization libraries\n", "from scipy.misc import imresize\n", "from PIL import Image\n", "from wordcloud import WordCloud, ImageColorGenerator\n", "from collections import Counter\n", "\n", "# Word2Vec related libraries\n", "from gensim.models import KeyedVectors\n", "\n", "# Dimensionaly reduction libraries\n", "from sklearn.decomposition import PCA\n", "\n", "# Clustering library\n", "from sklearn.cluster import KMeans\n", "\n", "# Set figure size a bit bigger than default so everything is easily red\n", "plt.rcParams[\"figure.figsize\"] = (11, 7)"]}, {"metadata": {"_cell_guid": "9f1cac1b-973c-459e-b019-8f51942804e0", "_uuid": "5639b952184820abd153255842cfbff9106d35b1"}, "cell_type": "markdown", "source": ["Let's take a casual look at the *variants* data."]}, {"metadata": {"collapsed": true, "_cell_guid": "a10c73a9-ef65-439f-8633-bebe9fe3bd99", "_uuid": "ee1e3624e86beaa3eeb142e56e82f95c2e348e1b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_variants = pd.read_csv('../input/training_variants').set_index('ID')\n", "df_variants.head()"]}, {"metadata": {"_cell_guid": "499507a5-bd67-40bd-9e8c-93b482959eed", "_uuid": "95dd47fc041bdd22a01f38de37b414eb3ba8d8f2"}, "cell_type": "markdown", "source": ["Let's take a look at the *text* data. Data is still small enough for memory so read to memory using pandas."]}, {"metadata": {"collapsed": true, "_cell_guid": "a7bb0422-5cc7-4c5b-91ec-f1d99d702670", "_uuid": "88af716db0de30be05863325eca3e58cb135a1f9"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_text = pd.read_csv('../input/training_text', sep='\\|\\|', engine='python', \n", "                      skiprows=1, names=['ID', 'Text']).set_index('ID')\n", "df_text.head()"]}, {"metadata": {"_cell_guid": "68ba24b9-33b2-413f-b0aa-970cda386534", "_uuid": "cd7b9286af6f8cf58b80825654987084ebc1969e"}, "cell_type": "markdown", "source": ["Join two dataframes on index"]}, {"metadata": {"collapsed": true, "_cell_guid": "6555e149-7319-491e-8ae3-a282c87386fa", "_uuid": "374274aec5a4ff115c3e7fbf68d152c5fde634be"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df = pd.concat([df_variants, df_text], axis=1)\n", "df.head()"]}, {"metadata": {"_cell_guid": "106c6353-c822-4a2d-a4de-6cd95b63de12", "_uuid": "b40c4a376c687694bcc0ea7b170b26760f9190c1"}, "cell_type": "markdown", "source": ["*Variation* column is mostly consists of independant unique values. So its not very helpfull for our predictions. So we will drop it."]}, {"metadata": {"collapsed": true, "_cell_guid": "ab3dece5-13dd-4fc4-a039-cf39eabc15dd", "_uuid": "5bcfcd99c12f59df3dfc52d389fd4ebc4eec1294"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df['Variation'].describe()"]}, {"metadata": {"_cell_guid": "946b3375-7bdf-40c2-a75a-5a9e8388f551", "_uuid": "1bf47b2c689ed94f6eb4635f56dfa2e24dfd9520"}, "cell_type": "markdown", "source": ["*Gene* column is a bit more complicated, values seems to be heavly skewed.  \n", "Data can still be valuable if normalized and balanced by weights.  "]}, {"metadata": {"collapsed": true, "_cell_guid": "33d8f4c4-37a1-41e0-a3d4-eab554f4ba33", "_uuid": "ee676b737ba8b178b7e18db4c7814994012b73eb"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["plt.figure()\n", "ax = df['Gene'].value_counts().plot(kind='area')\n", "\n", "ax.get_xaxis().set_ticks([])\n", "ax.set_title('Gene Frequency Plot')\n", "ax.set_xlabel('Gene')\n", "ax.set_ylabel('Frequency')\n", "\n", "plt.tight_layout()\n", "plt.show()"]}, {"metadata": {"_cell_guid": "5a045f1c-0eb5-43f2-bd44-f0e17dc6bb22", "_uuid": "1e942cbeaa4c5e63f6b71177596ab6d1b3fb2078"}, "cell_type": "markdown", "source": ["Even with domination of some gene's, it still gives a nice insight from their class distributions.  \n", "But not to over complicate things for this kernel, we'll skip that and drop it as well."]}, {"metadata": {"collapsed": true, "_cell_guid": "0e990288-e4f7-4ce4-bfac-e4a09fd0278b", "_uuid": "5bdba79cf75f4a1d8552fb03e86dd75db88428f4"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["fig, axes = plt.subplots(nrows=3, ncols=3, sharey=True, figsize=(11,11))\n", "\n", "# Normalize value counts for better comparison\n", "def normalize_group(x):\n", "    label, repetition = x.index, x\n", "    t = sum(repetition)\n", "    r = [n/t for n in repetition]\n", "    return label, r\n", "\n", "for idx, g in enumerate(df.groupby('Class')):\n", "    label, val = normalize_group(g[1][\"Gene\"].value_counts())\n", "    ax = axes.flat[idx]\n", "    ax.bar(np.arange(5), val[:5],\n", "           tick_label=label[:5]) \n", "    ax.set_title(\"Class {}\".format(g[0]))\n", "    \n", "fig.text(0.5, 0.97, '(Top 5) Gene Frequency per Class', ha='center', fontsize=14, fontweight='bold')\n", "fig.text(0.5, 0, 'Gene', ha='center', fontweight='bold')\n", "fig.text(0, 0.5, 'Frequency', va='center', rotation='vertical', fontweight='bold')\n", "fig.tight_layout(rect=[0.03, 0.03, 0.95, 0.95])"]}, {"metadata": {"_cell_guid": "aea60649-f512-4717-8f68-3686001ea50a", "_uuid": "2a3a67be45d9c7dfa98963f38668041a856ee0fc"}, "cell_type": "markdown", "source": ["And finally lets look at the class distribution."]}, {"metadata": {"collapsed": true, "_cell_guid": "fa9fed90-0725-4a2c-9814-15ef946c90e5", "_uuid": "a42e7661c3824cb73e3631fd4650656e232c4371"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["plt.figure()\n", "ax = df['Class'].value_counts().plot(kind='bar')\n", "\n", "ax.set_title('Class Distribution Over Entries')\n", "ax.set_xlabel('Class')\n", "ax.set_ylabel('Frequency')\n", "\n", "plt.tight_layout()\n", "plt.show()"]}, {"metadata": {"_cell_guid": "c7cf14fb-cc6f-43a8-95d6-9c178810bee1", "_uuid": "1c61e9d05a5beeeee35314782f1fa640f2af8f7a"}, "cell_type": "markdown", "source": ["Distribution looks skewed towards some classes, there are not enough examples for classes 8 and 9. During training, this can be solved using bias weights, careful sampling in batches or simply removing some of the dominant data to equalize the field.  \n", "  \n", "  ----\n", "Finally, lets drop the columns we don't need and be done with the initial cleaning."]}, {"metadata": {"collapsed": true, "_cell_guid": "7945dde4-ea1c-4192-9211-1470c474b30b", "_uuid": "98e3dc99311f96bd90e0aabd50b66c8026d07254"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df.drop(['Gene', 'Variation'], axis=1, inplace=True)\n", "\n", "# Additionaly we will drop the null labeled texts too\n", "df = df[df['Text'] != 'null']"]}, {"metadata": {"_cell_guid": "e184bf23-f789-495f-8901-453c27f173c3", "_uuid": "569ff28d633782e0abfaa6b9e5c10ef33ebb309e"}, "cell_type": "markdown", "source": ["Now let's look at the remaining data in more detail.  \n", "Text is too long and detailed and technical, so I've decided to summarize it using gensim's TextRank algorithm.  \n", "Still didn't understand anything :/"]}, {"metadata": {"collapsed": true, "_cell_guid": "38580240-7231-4ea2-8248-a10efe88e074", "_uuid": "37f88c2d93d9a5b8d4221ef243594af3d64fd73b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["t_id = 0\n", "text = df.loc[t_id, 'Text']\n", "\n", "word_scores = keywords(text, words=5, scores=True, split=True, lemmatize=True)\n", "word_scores = ', '.join(['{}-{:.2f}'.format(k, s[0]) for k, s in word_scores])\n", "summary = summarize(text, word_count=100)\n", "\n", "print('ID [{}]\\nKeywords: [{}]\\nSummary: [{}]'.format(t_id, word_scores, summary))"]}, {"metadata": {"_cell_guid": "fbdc05b8-1db5-4c8d-b4fb-b5435282be78", "_uuid": "92bef461c126b63add7ac87a17860196b5b89003"}, "cell_type": "markdown", "source": ["Text is tokenized, cleaned of stopwords and lemmatized for word frequency analysis.  \n", "\n", "Tokenization obviously takes a lot of time on a corpus like this. So bear that in mind.  \n", "May skip this, use a simpler tokenizer like `ToktokTokenizer` or just use `str.split()` instead."]}, {"metadata": {"collapsed": true, "_cell_guid": "477b75f6-5e86-4013-8d79-fa6e45481377", "_uuid": "1959056b14f476f60d7e92494aa3917653d4140f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["custom_words = [\"fig\", \"figure\", \"et\", \"al\", \"al.\", \"also\",\n", "                \"data\", \"analyze\", \"study\", \"table\", \"using\",\n", "                \"method\", \"result\", \"conclusion\", \"author\", \n", "                \"find\", \"found\", \"show\", '\"', \"\u2019\", \"\u201c\", \"\u201d\"]\n", "\n", "stop_words = set(stopwords.words('english') + list(punctuation) + custom_words)\n", "wordnet_lemmatizer = WordNetLemmatizer()\n", "\n", "class_corpus = df.groupby('Class').apply(lambda x: x['Text'].str.cat())\n", "class_corpus = class_corpus.apply(lambda x: Counter(\n", "    [wordnet_lemmatizer.lemmatize(w) \n", "     for w in word_tokenize(x) \n", "     if w.lower() not in stop_words and not w.isdigit()]\n", "))"]}, {"metadata": {"_cell_guid": "77f8ebf2-d600-4232-92f7-11cc8f909d37", "_uuid": "5c399a507dec0036c7d60d32cc25355b4b5380ea"}, "cell_type": "markdown", "source": ["Lets look at the dominant words in classes. And see if we can find any correlation."]}, {"metadata": {"collapsed": true, "_cell_guid": "d173959f-1c85-40ec-8aa1-d6aa6a5ea24d", "_uuid": "7fae6828866caa4766ab8a1e45b48bb9e8ac529d"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["class_freq = class_corpus.apply(lambda x: x.most_common(5))\n", "class_freq = pd.DataFrame.from_records(class_freq.values.tolist()).set_index(class_freq.index)\n", "\n", "def normalize_row(x):\n", "    label, repetition = zip(*x)\n", "    t = sum(repetition)\n", "    r = [n/t for n in repetition]\n", "    return list(zip(label,r))\n", "\n", "class_freq = class_freq.apply(lambda x: normalize_row(x), axis=1)\n", "\n", "# set unique colors for each word so it's easier to read\n", "all_labels = [x for x in class_freq.sum().sum() if isinstance(x,str)]\n", "unique_labels = set(all_labels)\n", "cm = plt.get_cmap('Blues_r', len(all_labels))\n", "colors = {k:cm(all_labels.index(k)/len(all_labels)) for k in all_labels}\n", "\n", "fig, ax = plt.subplots()\n", "\n", "offset = np.zeros(9)\n", "for r in class_freq.iteritems():\n", "    label, repetition = zip(*r[1])\n", "    ax.barh(range(len(class_freq)), repetition, left=offset, color=[colors[l] for l in label])\n", "    offset += repetition\n", "    \n", "ax.set_yticks(np.arange(len(class_freq)))\n", "ax.set_yticklabels(class_freq.index)\n", "ax.invert_yaxis()\n", "\n", "# annotate words\n", "offset_x = np.zeros(9) \n", "for idx, a in enumerate(ax.patches):\n", "    fc = 'k' if sum(a.get_fc()) > 2.5 else 'w'\n", "    ax.text(offset_x[idx%9] + a.get_width()/2, a.get_y() + a.get_height()/2, \n", "            '{}\\n{:.2%}'.format(all_labels[idx], a.get_width()), \n", "            ha='center', va='center', color=fc, fontsize=14, family='monospace')\n", "    offset_x[idx%9] += a.get_width()\n", "    \n", "ax.set_title('Most common words in each class')\n", "ax.set_xlabel('Word Frequency')\n", "ax.set_ylabel('Classes')\n", "\n", "plt.tight_layout()\n", "plt.show()"]}, {"metadata": {"_cell_guid": "4e716a9f-ee3f-4d2e-b392-31ee5a3d33c3", "_uuid": "2befd23a2138413de89da78427ce703bfddda446"}, "cell_type": "markdown", "source": ["**Mutation** and **cell** seems to be commonly dominating in all classes, not very informative. But the graph is still helpful. And would give more insight if we were to ignore most common words.  \n", "Let's plot how many times 25 most common words appear in the whole corpus."]}, {"metadata": {"collapsed": true, "_cell_guid": "ca31e217-7354-4138-9247-c7c9f75f75e2", "_uuid": "6e9ccd0be0d8775cb9b6368b124f18d28293c943"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["whole_text_freq = class_corpus.sum()\n", "\n", "fig, ax = plt.subplots()\n", "\n", "label, repetition = zip(*whole_text_freq.most_common(25))\n", "\n", "ax.barh(range(len(label)), repetition, align='center')\n", "ax.set_yticks(np.arange(len(label)))\n", "ax.set_yticklabels(label)\n", "ax.invert_yaxis()\n", "\n", "ax.set_title('Word Distribution Over Whole Text')\n", "ax.set_xlabel('# of repetitions')\n", "ax.set_ylabel('Word')\n", "\n", "plt.tight_layout()\n", "plt.show()"]}, {"metadata": {"_cell_guid": "0fddd5bc-953f-4107-9c97-f2a744f6389e", "_uuid": "3a3a31d03094f2b5f4d8b6d08d36f44a1071a008"}, "cell_type": "markdown", "source": ["-----\n", "## Sadly code below is copied and pasted as markdown (couldn't upload the files I need to run into Kaggle)  \n", "  \n", "More thorough version can be found on my [github](https://github.com/umutto/Kaggle-Personalized-Medicine/blob/master/data_analysis.ipynb).    \n", "\n", "-----"]}, {"metadata": {"_cell_guid": "049e10ea-3ece-4f30-8b0e-c409da9dfb6c", "_uuid": "a17cf8c79c1044fe53dcb523a0dd1c11be757f8f"}, "cell_type": "markdown", "source": ["Words are plotted to a word cloud using the beautiful [word_cloud](https://github.com/amueller/word_cloud) library.  \n", "This part is unnecessary for analysis but pretty =)."]}, {"metadata": {"collapsed": true, "_cell_guid": "d5a13ffb-5f01-42cb-89d0-d8c4e36d09aa", "_uuid": "00c9cdc80160412a7daff2f4c5375d1971d114db"}, "cell_type": "markdown", "source": ["\n", "```python\n", "def resize_image(np_img, new_size):\n", "    old_size = np_img.shape\n", "    ratio = min(new_size[0]/old_size[0], new_size[1]/old_size[1])\n", "    \n", "    return imresize(np_img, (round(old_size[0]*ratio), round(old_size[1]*ratio)))\n", "\n", "mask_image = np.array(Image.open('tmp/dna_stencil.png').convert('L'))\n", "mask_image = resize_image(mask_image, (4000, 2000))\n", "\n", "wc = WordCloud(max_font_size=140,\n", "               min_font_size=8,\n", "               max_words=1000,\n", "               width=mask_image.shape[1], \n", "               height=mask_image.shape[0],\n", "               prefer_horizontal=.9,\n", "               relative_scaling=.52,\n", "               background_color=None,\n", "               mask=mask_image,\n", "               mode=\"RGBA\").generate_from_frequencies(freq)\n", "\n", "plt.figure()\n", "plt.axis(\"off\")\n", "plt.tight_layout()\n", "plt.imshow(wc, interpolation=\"bilinear\")\n", "```"]}, {"metadata": {"_cell_guid": "23bfe63a-04db-4a55-a8a2-466d15b61173", "_uuid": "df09aba16cc624410608972716b0b7b198c1dc7f"}, "cell_type": "markdown", "source": ["![](http://i.imgur.com/oRQptjx.png?1)\n", "  \n", "We can also use the text data and visualize the relationships between words using Word2Vec. Even average the word vectors of a sentence and visualize the relationship between sentences.  \n", "(Doc2Vec could give much better results, for simplicity averaging word vectors are sufficient for this kernel)  \n", "  \n", "We'll use gensim's word2vec algorithm with Google's (huge) pretrained word2vec tokens."]}, {"metadata": {"_cell_guid": "ea4cb51e-62cb-4a6f-a0f0-283d933dba02", "_uuid": "d8b44273593359926562761e526a9e66ebc1d841"}, "cell_type": "markdown", "source": ["```python\n", "vector_path = r\"word_vectors\\GoogleNews-vectors-negative300.bin\"\n", "\n", "model = KeyedVectors.load_word2vec_format (vector_path, binary=True)\n", "model.wv.similar_by_word('mutation')\n", "```"]}, {"metadata": {"_cell_guid": "c19f95a8-bcde-491b-8196-da0722525daa", "_uuid": "cb775987ccc9d4648f7807b6d4f7db074456ce1d"}, "cell_type": "markdown", "source": ["```\n", "[('mutations', 0.8541924953460693),  \n", " ('genetic_mutation', 0.8245046138763428),  \n", " ('mutated_gene', 0.7879971861839294),  \n", " ('gene_mutation', 0.7823827266693115),  \n", " ('genetic_mutations', 0.7393667697906494),  \n", " ('gene', 0.7343351244926453),  \n", " ('gene_mutations', 0.7275242209434509),  \n", " ('genetic_variant', 0.7182294726371765),  \n", " ('alleles', 0.7164379358291626),  \n", " ('mutant_gene', 0.7144376039505005)] \n", " ```\n", "\n", "The results of word2vec looks really promising.  \n", "  \n", "----\n", "Now that we can somewhat understand the relationship between words, we'll use that to understand the relationship between sentences and documents. I'll be simply averaging the word vectors over a sentence, but better ways exist like using idf weighted averages or training a paragraph2vec model from scratch over the corpus."]}, {"metadata": {"_cell_guid": "5f22bd25-a44c-4897-82f4-b73ccc91f107", "_uuid": "f6eadf5e8ac5f408aba6ef6422f7ff56f422f501"}, "cell_type": "markdown", "source": ["```python\n", "def get_average_vector(text):\n", "    tokens = [w.lower() for w in word_tokenize(text) if w.lower() not in stop_words]\n", "    return np.mean(np.array([model.wv[w] for w in tokens if w in model]), axis=0)\n", "\n", "model.wv.similar_by_vector(get_average_vector(df.loc[0, 'Text']))\n", "```"]}, {"metadata": {"_cell_guid": "2f043df4-f885-4a9d-b015-b4105b590df5", "_uuid": "8105d55a5b99a39abd2fa224bb91bf315ba9c92b"}, "cell_type": "markdown", "source": ["```\n", "[('cyclic_AMP_cAMP', 0.7930851578712463),\n", " ('mRNA_transcripts', 0.7838510274887085),\n", " ('oncogenic_transformation', 0.7836254239082336),\n", " ('MT1_MMP', 0.7755827307701111),\n", " ('microRNA_molecule', 0.773587703704834),\n", " ('tumorigenicity', 0.7722263932228088),\n", " ('coexpression', 0.7706621885299683),\n", " ('transgenic_mice_expressing', 0.7698256969451904),\n", " ('pleiotropic', 0.7698150873184204),\n", " ('cyclin_B1', 0.7696200013160706)]\n", "```\n", "  \n", "And finally we can visualize the relationships between sentences by averaging the vector representations of each word in a sentence and reducing the vector dimensions to 2D (Google's Word2Vec embeddings come as [,300] vectors).  \n", "I will use PCA for dimensionality reduction because it usually is faster (and/or uses less memory) but t-sne could give better results."]}, {"metadata": {"_cell_guid": "b76c710d-7e6c-483a-a588-e49afb45dbcc", "_uuid": "fa21c1bfe9a8ca1f8a8a5e2aff7ad062f3a09f55"}, "cell_type": "markdown", "source": ["```python\n", "text_vecs = df.apply(lambda x: (x['Class'], get_average_vector(x['Text'])), axis=1)\n", "classes, vecs = list(zip(*text_vecs.values))\n", "\n", "pca = PCA(n_components=2)\n", "reduced_vecs = pca.fit_transform(vecs)\n", "\n", "fig, ax = plt.subplots()\n", "\n", "cm = plt.get_cmap('jet', 9)\n", "colors = [cm(i/9) for i in range(9)]\n", "ax.scatter(reduced_vecs[:,0], reduced_vecs[:,1], c=[colors[c-1] for c in classes], cmap='jet', s=8)\n", "\n", "\n", "plt.legend(handles=[Patch(color=colors[i], label='Class {}'.format(i+1)) for i in range(9)])\n", "\n", "plt.show()\n", "```"]}, {"metadata": {"_cell_guid": "9b685559-a110-4a2b-8f92-cf3315885fd3", "_uuid": "18c11dc03496714f5b00370dbda502e83fdb0417"}, "cell_type": "markdown", "source": ["![](http://i.imgur.com/hT6GIAK.png)\n", "  \n", "No imminent correlation can be seen based on this analysis.  \n", "This may be due to:\n", "- Dimensional Reduction (we may not be seeing the correlation in 2D).\n", "- Averaging word vectors are not effective solutions to infer sentence/paragraph vectors.\n", "- There is no obvious correlation between texts.\n", "  \n", "In any case let's see the difference with a simple k-means clustering."]}, {"metadata": {"_cell_guid": "a13d5c53-5372-4d08-ba14-8da6f3b8513a", "_uuid": "b3597d2537eb04ee864e804b5a8b34d077ba89eb"}, "cell_type": "markdown", "source": ["```python\n", "kmeans = KMeans(n_clusters=9).fit(vecs)\n", "c_labels = kmeans.labels_\n", "\n", "fig, ax = plt.subplots()\n", "\n", "cm = plt.get_cmap('jet', 9)\n", "colors = [cm(i/9) for i in range(9)]\n", "ax.scatter(reduced_vecs[:,0], reduced_vecs[:,1], c=[colors[c-1] for c in c_labels], cmap='jet', s=8)\n", "\n", "plt.legend(handles=[Patch(color=colors[i], label='Class {}'.format(i+1)) for i in range(9)])\n", "\n", "plt.show()\n", "```"]}, {"metadata": {"_cell_guid": "972d9632-0d7e-48a0-a874-93e6acfb6227", "_uuid": "959de5583cb023177f8a26c2622ae218bad76240"}, "cell_type": "markdown", "source": ["![](http://i.imgur.com/IYjRzd0.png)"]}]}