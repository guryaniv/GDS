{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom glob import glob\nfrom random import shuffle\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Convolution1D, concatenate, SpatialDropout1D, GlobalMaxPool1D, GlobalAvgPool1D, Embedding, \\\n    Conv2D, SeparableConv1D, Add, BatchNormalization, Activation, GlobalAveragePooling2D, LeakyReLU, Flatten\nfrom keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\nfrom keras.layers.pooling import _GlobalPooling1D\nfrom keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\nfrom keras.models import Model\nfrom keras.applications.nasnet import NASNetMobile, NASNetLarge, preprocess_input\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom imgaug import augmenters as iaa\nimport imgaug as ia\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train_labels.csv\")\nid_label_map = {k:v for k,v in zip(df_train.id.values, df_train.label.values)}\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c1169ec9a84704cff822b6e8ba90729d0ee383e"},"cell_type":"code","source":"def get_id_from_file_path(file_path):\n    return file_path.split(os.path.sep)[-1].replace('.tif', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4839c33e47619dfaf0f63d29bcf01ba50a96dfec"},"cell_type":"code","source":"labeled_files = glob('../input/train/*.tif')\ntest_files = glob('../input/test/*.tif')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d3168a806b716023cef1d798b3ede847f7546ee"},"cell_type":"code","source":"print(\"labeled_files size :\", len(labeled_files))\nprint(\"test_files size :\", len(test_files))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af19d597affefcbb1a14e5dd0d591bb3294efb61"},"cell_type":"code","source":"train, val = train_test_split(labeled_files, test_size=0.2, random_state=101010)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd67e144f327114229b410e0af43b71f45ce0d72"},"cell_type":"code","source":"def chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\ndef get_seq():\n    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n    seq = iaa.Sequential(\n        [\n            # apply the following augmenters to most images\n            iaa.Fliplr(0.5), # horizontally flip 50% of all images\n            iaa.Flipud(0.2), # vertically flip 20% of all images\n            sometimes(iaa.Affine(\n                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, # scale images to 80-120% of their size, individually per axis\n                translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, # translate by -20 to +20 percent (per axis)\n                rotate=(-10, 10), # rotate by -45 to +45 degrees\n                shear=(-5, 5), # shear by -16 to +16 degrees\n                order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n                cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n                mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n            )),\n            # execute 0 to 5 of the following (less important) augmenters per image\n            # don't execute all of them, as that would often be way too strong\n            iaa.SomeOf((0, 5),\n                [\n                    sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n                    iaa.OneOf([\n                        iaa.GaussianBlur((0, 1.0)), # blur images with a sigma between 0 and 3.0\n                        iaa.AverageBlur(k=(3, 5)), # blur image using local means with kernel sizes between 2 and 7\n                        iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 2 and 7\n                    ]),\n                    iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), # sharpen images\n                    iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n                    # search either for all edges or for directed edges,\n                    # blend the result with the original image using a blobby mask\n                    iaa.SimplexNoiseAlpha(iaa.OneOf([\n                        iaa.EdgeDetect(alpha=(0.5, 1.0)),\n                        iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n                    ])),\n                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images\n                    iaa.OneOf([\n                        iaa.Dropout((0.01, 0.05), per_channel=0.5), # randomly remove up to 10% of the pixels\n                        iaa.CoarseDropout((0.01, 0.03), size_percent=(0.01, 0.02), per_channel=0.2),\n                    ]),\n                    iaa.Invert(0.01, per_channel=True), # invert color channels\n                    iaa.Add((-2, 2), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n                    iaa.AddToHueAndSaturation((-1, 1)), # change hue and saturation\n                    # either change the brightness of the whole image (sometimes\n                    # per channel) or change the brightness of subareas\n                    iaa.OneOf([\n                        iaa.Multiply((0.9, 1.1), per_channel=0.5),\n                        iaa.FrequencyNoiseAlpha(\n                            exponent=(-1, 0),\n                            first=iaa.Multiply((0.9, 1.1), per_channel=True),\n                            second=iaa.ContrastNormalization((0.9, 1.1))\n                        )\n                    ]),\n                    sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n                    sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n                    sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n                ],\n                random_order=True\n            )\n        ],\n        random_order=True\n    )\n    return seq\n\ndef data_gen(list_files, id_label_map, batch_size, augment=False):\n    seq = get_seq()\n    while True:\n        shuffle(list_files)\n        for batch in chunker(list_files, batch_size):\n            X = [cv2.imread(x) for x in batch]\n            Y = [id_label_map[get_id_from_file_path(x)] for x in batch]\n            if augment:\n                X = seq.augment_images(X)\n            X = [preprocess_input(x) for x in X]\n                \n            yield np.array(X), np.array(Y)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86d0af5cefa44cb4f4f75d63096f2aa5db258ab5"},"cell_type":"code","source":"def plot_model_history(model_history, acc='acc', val_acc='val_acc'):\n    fig, axs = plt.subplots(1,2,figsize=(15,5))\n    axs[0].plot(range(1,len(model_history.history[acc])+1),model_history.history[acc])\n    axs[0].plot(range(1,len(model_history.history[val_acc])+1),model_history.history[val_acc])\n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy')\n    axs[0].set_xlabel('Epoch')\n    axs[0].set_xticks(np.arange(1,len(model_history.history[acc])+1),len(model_history.history[acc])/10)\n    axs[0].legend(['train', 'val'], loc='best')\n    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss')\n    axs[1].set_xlabel('Epoch')\n    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n    axs[1].legend(['train', 'val'], loc='best')\n    plt.show()\n    \ndef precision(y_true, y_pred):\n    \"\"\"Precision metric.\n\n    Only computes a batch-wise average of precision.\n\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef recall(y_true, y_pred):\n    \"\"\"Recall metric.\n\n    Only computes a batch-wise average of recall.\n\n    Computes the recall, a metric for multi-label classification of\n    how many relevant items are selected.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc04f6578df4e440addd49fc18f9d30ba01d82c7"},"cell_type":"code","source":"# Select optimizer\nfrom keras import optimizers\nfrom keras.optimizers import *\n\nsgd = SGD(lr=1e-4, momentum=0.9)\nrms_prop = optimizers.RMSprop(lr=1e-3)\nadam = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \nadam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\nadamax = Adamax(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\nadadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n\nearly_stop = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=3,\n                              verbose=1,\n                              restore_best_weights=True)\n\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.2, \n                                            min_delta=0.0001)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, \n                                            min_lr=0.000001, cooldown=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c6f749617fd9ffef03980c8edd76843a364edce"},"cell_type":"code","source":"from keras.applications import VGG16\nfrom keras import layers\n\npre_trained_model = VGG16(input_shape=(96, 96, 3) , include_top=False, weights=\"imagenet\")\n    \nfor layer in pre_trained_model.layers[:15]:\n    layer.trainable = False\n\nfor layer in pre_trained_model.layers[15:]:\n    layer.trainable = True\n    \nlast_layer = pre_trained_model.get_layer('block5_pool')\nlast_output = last_layer.output\n    \n# Flatten the output layer to 1 dimension\nx = layers.GlobalMaxPooling2D()(last_output)\n# Add a fully connected layer with 512 hidden units and ReLU activation\nx = layers.Dense(512, activation='relu')(x)\n# Add a dropout rate of 0.5\nx = layers.Dropout(0.5)(x)\n# Add a final sigmoid layer for classification\nx = layers.Dense(1, activation='sigmoid')(x)\n\n# Configure and compile the model\n\nmodel = Model(pre_trained_model.input, x)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=adam,\n              metrics=['binary_accuracy',recall,precision])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56e51e7dd5a1452acbefd7a4e5ab6b3991af1aac"},"cell_type":"code","source":"batch_size=64\n\nhistory = model.fit_generator(\n    data_gen(train, id_label_map, batch_size, augment=True),\n    validation_data=data_gen(val, id_label_map, batch_size),\n    epochs=10, verbose=1,\n    callbacks=[learning_rate_reduction],\n    steps_per_epoch=len(train) // batch_size,\n    validation_steps=len(val) // batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"961c40bc47c8464770ca3b20bb9836c6e1b86774"},"cell_type":"code","source":"loss, accuracy, recall,precision= model.evaluate_generator(data_gen(val, id_label_map, batch_size), len(val) // batch_size)\nprint(\"Test: accuracy = %f  ;  loss = %f ; recall = %f ; precision = %f \" % (accuracy, loss,recall,precision))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40e2f142a09eca38763a34fa0f731376490e9ce7"},"cell_type":"code","source":"plot_model_history(history,'binary_accuracy','val_binary_accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d399c51b099a414ef83e9b45a8bafab7206b1c24"},"cell_type":"code","source":"preds = []\nids = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"787cec513417ebb2c703d48b0ba97e1c4344c8d7"},"cell_type":"code","source":"for batch in chunker(test_files, batch_size):\n    X = [preprocess_input(cv2.imread(x)) for x in batch]\n    ids_batch = [get_id_from_file_path(x) for x in batch]\n    X = np.array(X)\n    preds_batch = ((model.predict(X).ravel()*model.predict(X[:, ::-1, :, :]).ravel()*model.predict(X[:, ::-1, ::-1, :]).ravel()*model.predict(X[:, :, ::-1, :]).ravel())**0.25).tolist()\n    preds += preds_batch\n    ids += ids_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b531bfe1d28a7bae83b74d9f857ae0f7029fdd2"},"cell_type":"code","source":"df = pd.DataFrame({'id':ids, 'label':preds})\ndf.to_csv(\"vgg16_1901_1100.csv\", index=False)\ndf.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}