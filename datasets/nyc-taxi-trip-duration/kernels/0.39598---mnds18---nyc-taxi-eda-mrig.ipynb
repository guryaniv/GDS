{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import timedelta\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"beluga = pd.DataFrame({'since_start_hour': [8.0, 8.0, 20.0, 28.0, 37.0, 45.0, 85.0, 85.0, 100.0, 134.0, 134.0, 143.0, 148.0],\n                       'Score': [ 0.400,  0.398,  0.393,  0.390,  0.383,  0.380,  0.379,  0.377,  0.376,  0.376,  0.375,  0.371,  0.368]})\nothers = pd.DataFrame({'since_start_hour': [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,68,69,70,71,72,73,74,75,76,77,78,79,80,81,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148],\n                        'BestScore': [.452,.452,.452,.452,.452,.452,.421,.421,.421,.406,.398,.398,.398,.398,.394,.394,.394,.394,.391,.391,.390,.390,.390,.390,.389,.389,.389,.389,.389,.389,.389,.385,.385,.385,.385,.385,.385,.385,.385,.385,.385,.385,.385,.383,.383,.383,.383,.383,.383,.383,.383,.383,.383,.383,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.378,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.377,.373,.373,.373]}) \nwith plt.xkcd():\n    # This figure will be in XKCD-style\n    fig, ax = plt.subplots()\n    ax.plot(others.since_start_hour.values, others.BestScore.values, 'b', alpha=0.8, lw=5, label='others')\n    ax.plot(beluga.since_start_hour.values, beluga.Score.values, 'k', alpha=0.8, lw=5, label='Mrig')\n    ax.set_ylim(0.36, 0.4)\n    ax.legend(loc=0)\n    ax.set_xlabel('hours since start')\n    ax.set_ylabel('RMSLE')\n    ax.set_title('Race for the top')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98fbfbfef10065576edf2fac9581819164a73165"},"cell_type":"code","source":"np.random.seed(1987)\nN = 100000 # number of sample rows in plots\nt0 = dt.datetime.now()\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsample_submission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d250a178b0c40c5083b3bb080f3f6dedbed7dcb"},"cell_type":"code","source":"print('We have {} training rows and {} test rows.'.format(train.shape[0], test.shape[0]))\nprint('We have {} training columns and {} test columns.'.format(train.shape[1], test.shape[1]))\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27a0e5eaf223fb7b856fe8a3a565ea48770d4a93"},"cell_type":"code","source":"print('Id is unique.') if train.id.nunique() == train.shape[0] else print('oops')\nprint('Train and test sets are distinct.') if len(np.intersect1d(train.id.values, test.id.values))== 0 else print('oops')\nprint('We do not need to worry about missing values.') if train.count().min() == train.shape[0] and test.count().min() == test.shape[0] else print('oops')\nprint('The store_and_fwd_flag has only two values {}.'.format(str(set(train.store_and_fwd_flag.unique()) | set(test.store_and_fwd_flag.unique()))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b31a5e27cd3c06bfdf7e9cf4ccf55663debb243b"},"cell_type":"code","source":"train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\ntest['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\ntrain.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\ntest.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date\ntrain['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\ntrain['store_and_fwd_flag'] = 1 * (train.store_and_fwd_flag.values == 'Y')\ntest['store_and_fwd_flag'] = 1 * (test.store_and_fwd_flag.values == 'Y')\ntrain['check_trip_duration'] = (train['dropoff_datetime'] - train['pickup_datetime']).map(lambda x: x.total_seconds())\nduration_difference = train[np.abs(train['check_trip_duration'].values  - train['trip_duration'].values) > 1]\nprint('Trip_duration and datetimes are ok.') if len(duration_difference[['pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration']]) == 0 else print('Ooops.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"865bc94cfde4222f545cca4449b5b8c1d831e66e"},"cell_type":"code","source":"train['trip_duration'].max() // 3600","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1ad2fcb9a1e0485360f07b7c0c004fcee726f21"},"cell_type":"code","source":"train['log_trip_duration'] = np.log(train['trip_duration'].values + 1)\nplt.hist(train['log_trip_duration'].values, bins=100)\nplt.xlabel('log(trip_duration)')\nplt.ylabel('number of train records')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71710847b61c5d5f646bb2d7d4227ef5d86ad166"},"cell_type":"code","source":"plt.plot(train.groupby('pickup_date').count()[['id']], 'o-', label='train')\nplt.plot(test.groupby('pickup_date').count()[['id']], 'o-', label='test')\nplt.title('Train and test period complete overlap.')\nplt.legend(loc=0)\nplt.ylabel('number of records')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df336b1a35d6e006cc20f877a5d8e682ab0a77e3"},"cell_type":"code","source":"city_long_border = (-74.03, -73.75)\ncity_lat_border = (40.63, 40.85)\nfig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\nax[0].scatter(train['pickup_longitude'].values[:N], train['pickup_latitude'].values[:N],\n              color='blue', s=1, label='train', alpha=0.1)\nax[1].scatter(test['pickup_longitude'].values[:N], test['pickup_latitude'].values[:N],\n              color='green', s=1, label='test', alpha=0.1)\nfig.suptitle('Train and test area complete overlap.')\nax[0].legend(loc=0)\nax[0].set_ylabel('latitude')\nax[0].set_xlabel('longitude')\nax[1].set_xlabel('longitude')\nax[1].legend(loc=0)\nplt.ylim(city_lat_border)\nplt.xlim(city_long_border)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb8e7253daec9736ab05d96fa314d6057be419de"},"cell_type":"code","source":"coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n                    test[['pickup_latitude', 'pickup_longitude']].values,\n                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n\npca = PCA().fit(coords)\ntrain['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntrain['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntrain['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntrain['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\ntest['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntest['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntest['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntest['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65692b77da19a71262f83a9d4cc6bbb738d30245"},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2)\nax[0].scatter(train['pickup_longitude'].values[:N], train['pickup_latitude'].values[:N],\n              color='blue', s=1, alpha=0.1)\nax[1].scatter(train['pickup_pca0'].values[:N], train['pickup_pca1'].values[:N],\n              color='green', s=1, alpha=0.1)\nfig.suptitle('Pickup lat long coords and PCA transformed coords.')\nax[0].set_ylabel('latitude')\nax[0].set_xlabel('longitude')\nax[1].set_xlabel('pca0')\nax[1].set_ylabel('pca1')\nax[0].set_xlim(city_long_border)\nax[0].set_ylim(city_lat_border)\npca_borders = pca.transform([[x, y] for x in city_lat_border for y in city_long_border])\nax[1].set_xlim(pca_borders[:, 0].min(), pca_borders[:, 0].max())\nax[1].set_ylim(pca_borders[:, 1].min(), pca_borders[:, 1].max())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b13c06738c21c0da0862643979ba9fbb5ef872fa"},"cell_type":"code","source":"def haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n    a = haversine_array(lat1, lng1, lat1, lng2)\n    b = haversine_array(lat1, lng1, lat2, lng1)\n    return a + b\n\ndef bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\ntrain.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])\n\ntest.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])\n\ntrain.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2\ntrain.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2\ntest.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2\ntest.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7adfbb3b606877525b3c199ad46fc0fbfd1dcdf"},"cell_type":"code","source":"train.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday\ntrain.loc[:, 'pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear\ntrain.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\ntrain.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\ntrain.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntrain.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n\ntest.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\ntest.loc[:, 'pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\ntest.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\ntest.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\ntest.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntest.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28c7570d20767bac53a0ac4df722afabd4a1eda4"},"cell_type":"code","source":"train.loc[:, 'avg_speed_h'] = 1000 * train['distance_haversine'] / train['trip_duration']\ntrain.loc[:, 'avg_speed_m'] = 1000 * train['distance_dummy_manhattan'] / train['trip_duration']\nfig, ax = plt.subplots(ncols=3, sharey=True)\nax[0].plot(train.groupby('pickup_hour').mean()['avg_speed_h'], 'bo-', lw=2, alpha=0.7)\nax[1].plot(train.groupby('pickup_weekday').mean()['avg_speed_h'], 'go-', lw=2, alpha=0.7)\nax[2].plot(train.groupby('pickup_week_hour').mean()['avg_speed_h'], 'ro-', lw=2, alpha=0.7)\nax[0].set_xlabel('hour')\nax[1].set_xlabel('weekday')\nax[2].set_xlabel('weekhour')\nax[0].set_ylabel('average speed')\nfig.suptitle('Rush hour average traffic speed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c65fdf4224ebc34c6035d1b65bc65677ed26c34"},"cell_type":"code","source":"train.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 3)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 3)\n# Average speed for regions\ngby_cols = ['pickup_lat_bin', 'pickup_long_bin']\ncoord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\ncoord_count = train.groupby(gby_cols).count()[['id']].reset_index()\ncoord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\ncoord_stats = coord_stats[coord_stats['id'] > 100]\nfig, ax = plt.subplots(ncols=1, nrows=1)\nax.scatter(train.pickup_longitude.values[:N], train.pickup_latitude.values[:N],\n           color='black', s=1, alpha=0.5)\nax.scatter(coord_stats.pickup_long_bin.values, coord_stats.pickup_lat_bin.values,\n           c=coord_stats.avg_speed_h.values,\n           cmap='RdYlGn', s=20, alpha=0.5, vmin=1, vmax=8)\nax.set_xlim(city_long_border)\nax.set_ylim(city_lat_border)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nplt.title('Average speed')\nplt.show()\n\ntrain.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 2)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 2)\ntrain.loc[:, 'center_lat_bin'] = np.round(train['center_latitude'], 2)\ntrain.loc[:, 'center_long_bin'] = np.round(train['center_longitude'], 2)\ntrain.loc[:, 'pickup_dt_bin'] = (train['pickup_dt'] // (3 * 3600))\ntest.loc[:, 'pickup_lat_bin'] = np.round(test['pickup_latitude'], 2)\ntest.loc[:, 'pickup_long_bin'] = np.round(test['pickup_longitude'], 2)\ntest.loc[:, 'center_lat_bin'] = np.round(test['center_latitude'], 2)\ntest.loc[:, 'center_long_bin'] = np.round(test['center_longitude'], 2)\ntest.loc[:, 'pickup_dt_bin'] = (test['pickup_dt'] // (3 * 3600))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b82123433c2b97c4aaa00e655c6d550c8039b63b"},"cell_type":"code","source":"sample_ind = np.random.permutation(len(coords))[:500000]\nkmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0b652c9d54f2d5c01228428503022d827b16e91"},"cell_type":"code","source":"train.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])\ntrain.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])\ntest.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\ntest.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])\nt1 = dt.datetime.now()\nprint('Time till clustering: %i seconds' % (t1 - t0).seconds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad75204d35cf1d69a317e9c275aa48b86c46d16c"},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=1, nrows=1)\nax.scatter(train.pickup_longitude.values[:N], train.pickup_latitude.values[:N], s=10, lw=0,\n           c=train.pickup_cluster[:N].values, cmap='tab20', alpha=0.2)\nax.set_xlim(city_long_border)\nax.set_ylim(city_lat_border)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ee6641b06ac4a5d0ebf8726565956a389c8d34a"},"cell_type":"code","source":"for gby_col in ['pickup_hour', 'pickup_date', 'pickup_dt_bin',\n               'pickup_week_hour', 'pickup_cluster', 'dropoff_cluster']:\n    gby = train.groupby(gby_col).mean()[['avg_speed_h', 'avg_speed_m', 'log_trip_duration']]\n    gby.columns = ['%s_gby_%s' % (col, gby_col) for col in gby.columns]\n    train = pd.merge(train, gby, how='left', left_on=gby_col, right_index=True)\n    test = pd.merge(test, gby, how='left', left_on=gby_col, right_index=True)\n\nfor gby_cols in [['center_lat_bin', 'center_long_bin'],\n                 ['pickup_hour', 'center_lat_bin', 'center_long_bin'],\n                 ['pickup_hour', 'pickup_cluster'],  ['pickup_hour', 'dropoff_cluster'],\n                 ['pickup_cluster', 'dropoff_cluster']]:\n    coord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\n    coord_count = train.groupby(gby_cols).count()[['id']].reset_index()\n    coord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\n    coord_stats = coord_stats[coord_stats['id'] > 100]\n    coord_stats.columns = gby_cols + ['avg_speed_h_%s' % '_'.join(gby_cols), 'cnt_%s' %  '_'.join(gby_cols)]\n    train = pd.merge(train, coord_stats, how='left', on=gby_cols)\n    test = pd.merge(test, coord_stats, how='left', on=gby_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2701b1c17e12ba23c1dc7f7053dfe4425862c590"},"cell_type":"code","source":"group_freq = '60min'\ndf_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\ntrain.loc[:, 'pickup_datetime_group'] = train['pickup_datetime'].dt.round(group_freq)\ntest.loc[:, 'pickup_datetime_group'] = test['pickup_datetime'].dt.round(group_freq)\n\n# Count trips over 60min\ndf_counts = df_all.set_index('pickup_datetime')[['id']].sort_index()\ndf_counts['count_60min'] = df_counts.isnull().rolling(group_freq).count()['id']\ntrain = train.merge(df_counts, on='id', how='left')\ntest = test.merge(df_counts, on='id', how='left')\n\n# Count how many trips are going to each cluster over time\ndropoff_counts = df_all \\\n    .set_index('pickup_datetime') \\\n    .groupby([pd.TimeGrouper(group_freq), 'dropoff_cluster']) \\\n    .agg({'id': 'count'}) \\\n    .reset_index().set_index('pickup_datetime') \\\n    .groupby('dropoff_cluster').rolling('240min').mean() \\\n    .drop('dropoff_cluster', axis=1) \\\n    .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n    .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'dropoff_cluster_count'})\n\ntrain['dropoff_cluster_count'] = train[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\ntest['dropoff_cluster_count'] = test[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47b5e31cecbf242a754594193fca7802aa5d419d"},"cell_type":"code","source":"# Count how many trips are going from each cluster over time\ndf_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\npickup_counts = df_all \\\n    .set_index('pickup_datetime') \\\n    .groupby([pd.TimeGrouper(group_freq), 'pickup_cluster']) \\\n    .agg({'id': 'count'}) \\\n    .reset_index().set_index('pickup_datetime') \\\n    .groupby('pickup_cluster').rolling('240min').mean() \\\n    .drop('pickup_cluster', axis=1) \\\n    .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n    .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'pickup_cluster_count'})\n\ntrain['pickup_cluster_count'] = train[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)\ntest['pickup_cluster_count'] = test[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0afb28ed20f6be717dcb885bbd544aaa6d2e5944"},"cell_type":"code","source":"# fr1 = pd.read_csv('../input/fastest_routes_train_part_1.csv',\n#                   usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps'])\n# fr2 = pd.read_csv('../input/fastest_routes_train_part_2.csv',\n#                   usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n# test_street_info = pd.read_csv('../input/fastest_routes_test.csv',\n#                                usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n# train_street_info = pd.concat((fr1, fr2))\n# train = train.merge(train_street_info, how='left', on='id')\n# test = test.merge(test_street_info, how='left', on='id')\n# train_street_info.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ab112ce55842192b8bb9ea3d28130123ac43583"},"cell_type":"code","source":"feature_names = list(train.columns)\nprint(np.setdiff1d(train.columns, test.columns))\ndo_not_use_for_training = ['id', 'log_trip_duration', 'pickup_datetime', 'dropoff_datetime',\n                           'trip_duration', 'check_trip_duration',\n                           'pickup_date', 'avg_speed_h', 'avg_speed_m',\n                           'pickup_lat_bin', 'pickup_long_bin',\n                           'center_lat_bin', 'center_long_bin',\n                           'pickup_dt_bin', 'pickup_datetime_group']\nfeature_names = [f for f in train.columns if f not in do_not_use_for_training]\n# print(feature_names)\nprint('We have %i features.' % len(feature_names))\ntrain[feature_names].count()\ny = np.log(train['trip_duration'].values + 1)\n\nt1 = dt.datetime.now()\nprint('Feature extraction time: %i seconds' % (t1 - t0).seconds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d39da1f35878ac1ee77ed3e23f5a4d9077d3260d"},"cell_type":"code","source":"feature_stats = pd.DataFrame({'feature': feature_names})\nfeature_stats.loc[:, 'train_mean'] = np.nanmean(train[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'test_mean'] = np.nanmean(test[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'train_std'] = np.nanstd(train[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'test_std'] = np.nanstd(test[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'train_nan'] = np.mean(np.isnan(train[feature_names].values), axis=0).round(3)\nfeature_stats.loc[:, 'test_nan'] = np.mean(np.isnan(test[feature_names].values), axis=0).round(3)\nfeature_stats.loc[:, 'train_test_mean_diff'] = np.abs(feature_stats['train_mean'] - feature_stats['test_mean']) / np.abs(feature_stats['train_std'] + feature_stats['test_std'])  * 2\nfeature_stats.loc[:, 'train_test_nan_diff'] = np.abs(feature_stats['train_nan'] - feature_stats['test_nan'])\nfeature_stats = feature_stats.sort_values(by='train_test_mean_diff')\nfeature_stats[['feature', 'train_test_mean_diff']].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20124008a1d2b5fde35a48ec47b4d3a16cf8cf3a"},"cell_type":"code","source":"feature_stats = feature_stats.sort_values(by='train_test_nan_diff')\nfeature_stats[['feature', 'train_nan', 'test_nan', 'train_test_nan_diff']].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1991fbd2a08e77eb66f2f08f72596781b73cccf2"},"cell_type":"code","source":"Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=1987)\ndtrain = xgb.DMatrix(Xtr, label=ytr)\ndvalid = xgb.DMatrix(Xv, label=yv)\ndtest = xgb.DMatrix(test[feature_names].values)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Try different parameters! My favorite is random search :)\nxgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n            'subsample': 0.8, 'lambda': 1., 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bceedb6b55c9557ce01e5ea101b40a240249c07f"},"cell_type":"code","source":"# You could try to train with more epoch\nmodel = xgb.train(xgb_pars, dtrain, 60, watchlist, early_stopping_rounds=50,\n                  maximize=False, verbose_eval=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98052a8d04eec6207e4f29d01311728c3bc3bced"},"cell_type":"code","source":"print('Modeling RMSLE %.5f' % model.best_score)\nt1 = dt.datetime.now()\nprint('Training time: %i seconds' % (t1 - t0).seconds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34e0fa995be131113b526462ec70e69013c9fc92"},"cell_type":"code","source":"# rmse_wo_feature = [0.39224, 0.38816, 0.38726, 0.38780, 0.38773, 0.38792, 0.38753, 0.38745, 0.38710, 0.38767, 0.38738, 0.38750, 0.38678, 0.39359, 0.38672, 0.38794, 0.38694, 0.38750, 0.38742, 0.38673, 0.38754, 0.38705, 0.38736, 0.38741, 0.38764, 0.38730, 0.38676, 0.38696, 0.38750, 0.38705, 0.38746, 0.38727, 0.38750, 0.38771, 0.38747, 0.38907, 0.38719, 0.38756, 0.38701, 0.38734, 0.38782, 0.38673, 0.38797, 0.38720, 0.38709, 0.38704, 0.38809, 0.38768, 0.38798, 0.38849, 0.38690, 0.38753, 0.38721, 0.38807, 0.38830, 0.38750, np.nan, np.nan, np.nan]\n# feature_importance_dict = model.get_fscore()\n# fs = ['f%i' % i for i in range(len(feature_names))]\n# f1 = pd.DataFrame({'f': list(feature_importance_dict.keys()),\n#                    'importance': list(feature_importance_dict.values())})\n# f2 = pd.DataFrame({'f': fs, 'feature_name': feature_names, 'rmse_wo_feature': rmse_wo_feature})\n# feature_importance = pd.merge(f1, f2, how='right', on='f')\n# feature_importance = feature_importance.fillna(0)\n\n# feature_importance[['feature_name', 'importance', 'rmse_wo_feature']].sort_values(by='importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc6d85b940e3abc8ffe9393fed01f0bf890be128"},"cell_type":"code","source":"# feature_importance = feature_importance.sort_values(by='rmse_wo_feature', ascending=False)\n# feature_importance = feature_importance[feature_importance['rmse_wo_feature'] > 0]\n# with sns.axes_style(\"whitegrid\"):\n#     fig, ax = plt.subplots(figsize=(10, 10))\n#     ax.scatter(feature_importance['importance'].values, feature_importance['rmse_wo_feature'].values,\n#                c=feature_importance['rmse_wo_feature'].values, s=500, cmap='RdYlGn_r', alpha=0.7)\n#     for _, row in feature_importance.head(5).iterrows():\n#         ax.text(row['importance'], row['rmse_wo_feature'], row['feature_name'],\n#                 verticalalignment='center', horizontalalignment='center')\n#     ax.set_xlabel('xgb feature importance')\n#     ax.set_ylabel('rmse without feature')\n#     ax.set_ylim(np.min(feature_importance['rmse_wo_feature']) - 0.001,\n#                 np.max(feature_importance['rmse_wo_feature']) + 0.001)\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b760c5f76fb1b519a3bccf9c9370b2006162e342"},"cell_type":"code","source":"ypred = model.predict(dvalid)\nfig,ax = plt.subplots(ncols=2)\nax[0].scatter(ypred, yv, s=0.1, alpha=0.1)\nax[0].set_xlabel('log(prediction)')\nax[0].set_ylabel('log(ground truth)')\nax[1].scatter(np.exp(ypred), np.exp(yv), s=0.1, alpha=0.1)\nax[1].set_xlabel('prediction')\nax[1].set_ylabel('ground truth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18890751ba6a33fbb704ef15f38e6e054cc3a4c1"},"cell_type":"code","source":"ytest = model.predict(dtest)\nprint('Test shape OK.') if test.shape[0] == ytest.shape[0] else print('Oops')\ntest['trip_duration'] = np.exp(ytest) - 1\ntest[['id', 'trip_duration']].to_csv('mrig_xgb_submission.csv', index=False)\n\nprint('Valid prediction mean: %.3f' % ypred.mean())\nprint('Test prediction mean: %.3f' % ytest.mean())\n\nfig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\nsns.distplot(ypred, ax=ax[0], color='blue', label='validation prediction')\nsns.distplot(ytest, ax=ax[1], color='green', label='test prediction')\nax[0].legend(loc=0)\nax[1].legend(loc=0)\nplt.show()\n\nt1 = dt.datetime.now()\nprint('Total time: %i seconds' % (t1 - t0).seconds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e78cb64ec016c6f546a2a5906965333e85dd1a61"},"cell_type":"code","source":"FOREVER_COMPUTING_FLAG = False\nxgb_pars = []\nfor MCW in [10, 20, 50, 75, 100]:\n    for ETA in [0.05, 0.1, 0.15]:\n        for CS in [0.3, 0.4, 0.5]:\n            for MD in [6, 8, 10, 12, 15]:\n                for SS in [0.5, 0.6, 0.7, 0.8, 0.9]:\n                    for LAMBDA in [0.5, 1., 1.5,  2., 3.]:\n                        xgb_pars.append({'min_child_weight': MCW, 'eta': ETA, \n                                         'colsample_bytree': CS, 'max_depth': MD,\n                                         'subsample': SS, 'lambda': LAMBDA, \n                                         'nthread': -1, 'booster' : 'gbtree', 'eval_metric': 'rmse',\n                                         'silent': 1, 'objective': 'reg:linear'})\n\nwhile FOREVER_COMPUTING_FLAG:\n    xgb_par = np.random.choice(xgb_pars, 1)[0]\n    print(xgb_par)\n    model = xgb.train(xgb_par, dtrain, 2000, watchlist, early_stopping_rounds=50,\n                      maximize=False, verbose_eval=100)\n    print('Modeling RMSLE %.5f' % model.best_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cce3e40638670914e4d52340b8baa0c25b62435b"},"cell_type":"code","source":"# paropt = pd.DataFrame({'lambda':[1.5,1.0,1.0,1.5,1.5,1.0,1.5,1.0,1.5,2.0,0.5,1.0,0.5,1.5,1.5,0.5,1.0,1.5,0.5,2.0,1.0,2.0,2.0,1.5,1.5,2.0,1.5,2.0,1.5,0.5,1.0,1.0,2.0,1.5,1.0,1.0,0.5,2.0,1.0,0.5,0.5,2.0,1.0,1.0,0.5,0.5,1.5,0.5,1.5,2.0,2.0,2.0,2.0,0.5,1.5,1.0,1.5,2.0,2.0,0.5,1.5,1.0,0.5,1.0,1.5,2.0,1.0,1.0,2.0,2.0,1.0,0.5,0.5,1.0,1.5,2.0,0.5,1.0,1.5,1.0,1.0,1.5,1.5,1.5,0.5,1.5,1.0,1.5,2.0,2.0,2.0,1.0,2.0,0.5,2.0,0.5,1.5,0.5,2.0,0.5,1.0,1.5,1.5,1.5,2.0,0.5,0.5,1.0,2.0],\n#                        'eta':[.1,.1,.05,.05,.05,.15,.15,.1,.1,.05,.15,.15,.15,.1,.1,.1,.1,.05,.15,.05,.05,.05,.15,.15,.05,.05,.05,.05,.15,.15,.15,.15,.1,.05,.05,.1,.1,.1,.1,.1,.05,.15,.15,.15,.1,.1,.05,.05,.15,.15,.15,.1,.1,.05,.05,.05,.05,.05,.15,.1,.1,.15,.1,.1,.05,.15,.15,.15,.1,.05,.05,.05,.05,.15,.1,.1,.1,.1,.05,.05,.05,.15,.15,.1,.1,.1,.1,.05,.15,.15,.1,.1,.1,.05,.05,.1,.1,.1,.1,.1,.05,.15,.15,.15,.15,.05,.05,.15,.15],\n#                        'min_child_weight': [50,50,20,100,10,50,100,100,75,10,10,50,50,100,75,100,50,10,20,10,75,20,50,75,100,100,10,20,75,75,75,20,10,75,10,100,100,10,20,20,50,50,100,20,50,100,100,75,20,75,20,50,20,10,20,20,20,75,20,75,100,10,10,20,10,20,100,75,75,10,100,50,100,100,50,10,75,75,50,10,75,75,50,75,20,100,100,50,20,20,50,50,75,20,50,100,75,75,100,75,10,10,20,20,10,10,75,50,20],\n#                        'subsample':[.8,.9,.8,.6,.6,.6,.9,.6,.5,.9,.8,.9,.7,.5,.5,.9,.7,.7,.5,.8,.5,.9,.6,.6,.8,.8,.8,.7,.5,.5,.9,.9,.5,.6,.7,.8,.8,.6,.9,.7,.8,.6,.6,.9,.7,.7,.8,.6,.6,.5,.9,.8,.7,.6,.6,.6,.5,.9,.8,.5,.7,.6,.8,.6,.8,.8,.6,.7,.9,.5,.7,.5,.9,.7,.8,.9,.9,.7,.8,.5,.7,.8,.6,.8,.8,.5,.9,.5,.5,.7,.8,.6,.6,.8,.7,.6,.6,.6,.7,.7,.8,.6,.5,.9,.7,.6,.9,.5,.5],\n#                        'rmse': [.380,.380,.377,.378,.378,.386,.382,.382,.383,.374,.386,.381,.385,.383,.383,.379,.381,.376,.389,.375,.381,.374,.385,.385,.378,.377,.375,.376,.385,.386,.382,.384,.384,.379,.376,.380,.380,.382,.380,.382,.378,.385,.384,.383,.383,.383,.379,.381,.386,.387,.381,.380,.380,.377,.377,.377,.379,.376,.382,.385,.382,.386,.380,.382,.375,.383,.385,.384,.379,.378,.380,.381,.378,.384,.380,.377,.379,.383,.380,.380,.380,.383,.385,.381,.379,.386,.380,.383,.387,.383,.382,.384,.385,.377,.380,.383,.383,.383,.382,.382,.377,.386,.388,.382,.384,.379,.378,.387,.388]\n#                        })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"912b5bf3dfbf4bb18e3760cb692ab65534efa194"},"cell_type":"code","source":"# for i, par in enumerate(['lambda', 'min_child_weight', 'subsample', 'eta']):\n#     fig, ax = plt.subplots()\n#     ax = sns.boxplot(x=par, y=\"rmse\", data=paropt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bd54d5949ec84ef313654c6588214c37a640592"},"cell_type":"code","source":"# with sns.axes_style(\"whitegrid\"):\n#     fig, axs = plt.subplots(ncols=4, sharey=True, figsize=(12, 3))\n#     for i, par in enumerate(['lambda', 'min_child_weight', 'subsample', 'eta']):\n#         mean_rmse = paropt.groupby(par).mean()[['rmse']].reset_index()\n#         axs[i].scatter(mean_rmse[par].values, mean_rmse['rmse'].values, c=mean_rmse['rmse'].values,\n#                        s=300, cmap='viridis_r', vmin=.377, vmax=.385, )\n#         axs[i].set_xlabel(par)\n#         axs[i].set_xticks(mean_rmse[par].values)\n#         axs[i].set_ylim(paropt.rmse.min(), paropt.rmse.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adf72c2c5d819a10aadb40320410f8750a69a1f3"},"cell_type":"code","source":"# cv_lb = pd.DataFrame({'cv': [0.3604,0.36056,0.3614,0.3618,0.3623,0.3626,0.3646,0.3696,0.3702,0.3706,0.372,0.3738,0.37477,0.37691,0.3824,0.3868,0.3904],\n#                       'lb': [0.367,0.367,0.368,0.368,0.368,0.368,0.371,0.375,0.376,0.376,0.377,0.377,0.379,0.381,0.387,0.39,0.393]})\n# ax = sns.regplot(x=\"cv\", y=\"lb\", data=cv_lb, scatter_kws={'s': 200})\n# ax.set_xlabel('Local validation (RMSLE)')\n# ax.set_ylabel('Leaderboard (RMSLE)')\n# ax.set_title('Local validation and Leaderboard consistency')\n# print('CV - LB Diff: %.3f' % np.mean(cv_lb['lb'] - cv_lb['cv']))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}