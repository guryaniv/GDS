{"cells":[{"metadata":{"_uuid":"045c17c9b0212a76c13c84015f83b7ff69663926"},"cell_type":"markdown","source":"I am trying to analyze data thru diff views to get a clue which features may be impacting target. My intenetion is to keep things simple and easily comprehendable. I myself get lost sometimes in good kernels which are bit low on structure part.  I have tried to keep it structured and scalable for new features and models. "},{"metadata":{"_uuid":"a1ac7e184fb43ff589e96361b88e2eeeaef12a08"},"cell_type":"markdown","source":"-  [Import and Read](#LibLink)\n-  [Basic EDA](#EDALink)\n-  [Functions](#FuncLink)\n-  [Plotting](#PlotLink)\n-  [Corr and Bin](#CorLink)\n-  [Features](#FeatLink)\n-  [Model](#ModLink)"},{"metadata":{"_uuid":"20dc019a068ecac80acbf9e73a6b7db23351a7f3"},"cell_type":"markdown","source":"<div id=\"LibLink\">\n**Import libraries**\n</div>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import rankdata\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport itertools\nfrom sklearn import metrics\nfrom scipy.stats import norm, rankdata\n\npd.set_option('display.max_columns', 200)\n# below is to have multiple outputs from same Jupyter cells\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ee018ed8963c8f89080775233e577c00097bb66"},"cell_type":"markdown","source":"Read files "},{"metadata":{"trusted":true,"_uuid":"c31a7a77a67dc8fc1a31ecded7604644eba9c191","_kg_hide-input":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53d8c7a7be2b176e9a3d787bd033d01ad3365153"},"cell_type":"markdown","source":"<div id=\"EDALink\">\n **Basic EDA**\n </div>"},{"metadata":{"_uuid":"e5b9dbbf245c752967f3c54e71cd7445cc1d693e"},"cell_type":"markdown","source":"Number of rows and columns in Dataset"},{"metadata":{"trusted":true,"_uuid":"cb4b634debc40b073a0111b82e5c98384b059b32","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(\"Train Shape\\n\")\ntrain_df.shape\nprint(\"Test Shape\\n\")\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48ddf52efdf7079e7152df8d1d6b92cf1396c3ab"},"cell_type":"markdown","source":"Basic statistics for datasets"},{"metadata":{"trusted":true,"_uuid":"9a4ce748d907ff3dc4c95c3edfc42545038a6b73","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(\"Train Describe\\n\")\ntrain_df.describe() \nprint(\"Test Describe\\n\")\ntest_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83090337f70e68708e47bee88a3872b628fce67d"},"cell_type":"markdown","source":"Distribution of target in training dataset. This shows its a imbalance data set, with 90% of data being 0 and 10% as 1."},{"metadata":{"trusted":true,"_uuid":"b7c82e3e72b2be025166f0419ce802e19819b9e5","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df[\"target\"].value_counts()/train_df.shape[0]*100\nfig,ax= plt.subplots()\nsns.countplot(data=train_df,x=\"target\",ax=ax)\nax.set(xlabel=\"Target\",\n       ylabel=\"Count\", \n       Title = \"Target Distribution\"\n       )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6c5c1d194bca8fd8ae5f74dad231ce59157d3c2"},"cell_type":"markdown","source":"Looking at output of describe for both df, data seems to be similar in both the datasets (test and train).  Another point is test is of same size as train. we need to find a way to extract some info from test data."},{"metadata":{"_uuid":"e21fbc5df4c5f168562906c60d878ff26292e6a1"},"cell_type":"markdown","source":"### Missing values"},{"metadata":{"_uuid":"fc56f602f4ff55a327dcbb4af51ae6052f3096e4"},"cell_type":"markdown","source":"None of dataset has any missing values. "},{"metadata":{"trusted":true,"_uuid":"b9c2610042de4c2e31e689003ca2917dd3fd62e3","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df.isnull().sum().sum()\ntest_df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c856c7871e285eb6c473b6b7784e28100a51aa7"},"cell_type":"markdown","source":"<div id=\"FuncLink\">\n** Utility Functions for EDA and Feature Engineering **\n    </div>"},{"metadata":{"_uuid":"c64b5059a1512621727341669618b0647a8f4a63"},"cell_type":"markdown","source":"To plot distributions features of two datasets  **plot_feature_distribution**"},{"metadata":{"trusted":true,"_uuid":"b35042d04d478d00964be01da3d5a63e20bfdc9e","_kg_hide-input":true},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad43773a0231bd5562529ad89c08624bbbc0f110"},"cell_type":"markdown","source":"To plot boxplot of features of two datasets, along with class split  **plot_feature_boxplot**"},{"metadata":{"trusted":true,"_uuid":"f560708977ce27991e066efff9a6d10835335ddc","_kg_hide-input":true},"cell_type":"code","source":"def plot_feature_boxplot(df1,df2,label1,label2,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(100,2,figsize=(10,180))\n\n    for feature in features:\n        i += 1\n        plt.subplot(100,2,i)\n        sns.boxplot(y=df1[feature], x=target, showfliers=False)\n        plt.title(feature+'_train', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n        i += 1\n        plt.subplot(100,2,i)\n        sns.boxplot(df2[feature],orient='v',color='r')\n        plt.title(feature+'_test', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n\n        #locs, labels = plt.xticks()\n        #plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        #plt.tick_params(axis='y', which='major', labelsize=6)\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc348fe279443864f77f9e8cfcfc5f8e5ce58895"},"cell_type":"markdown","source":"To plot violinplot of features of two datasets, along with class split  **plot_feature_violinplot**"},{"metadata":{"trusted":true,"_uuid":"ff1eef6b1cb7df1ea2ab7e2f7011ebff11cc90ef","_kg_hide-input":true},"cell_type":"code","source":"def plot_feature_violinplot(df1,df2,label1,label2,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(100,2,figsize=(10,180))\n\n    for feature in features:\n        i += 1\n        plt.subplot(100,2,i)\n        sns.violinplot(y=df1[feature], x=target, showfliers=False)\n        plt.title(feature+'_train', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n        i += 1\n        plt.subplot(100,2,i)\n        sns.violinplot(df2[feature],orient='v',color='r')\n        plt.title(feature+'_test', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n\n        #locs, labels = plt.xticks()\n        #plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        #plt.tick_params(axis='y', which='major', labelsize=6)\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51ce65e4db9056894eb80316cf18a473e718408a"},"cell_type":"markdown","source":"To plot violinplot of binned features for training along with class split  **plot_binned_feature_target_violinplot**"},{"metadata":{"trusted":true,"_uuid":"8eb06c6dfe19a3831ed18366a3b025305796dcdf","_kg_hide-input":true},"cell_type":"code","source":"def plot_binned_feature_target_violinplot(df,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(200,1,figsize=(8,380))\n\n    for feature in features:\n        bins = np.nanpercentile(df[feature], range(0,101,10))\n        df[feature+\"_binned\"] = pd.cut(df[feature],bins=bins)\n        i += 1\n        plt.subplot(200,1,i)\n        sns.violinplot(y=df[feature+\"_binned\"], x=target, showfliers=False)\n        plt.title(feature+'_binned & Target', fontsize=12)\n        plt.ylabel('')\n        plt.xlabel('')\n       \n        locs, labels = plt.xticks()\n        plt.xticks([0.0,1.0])\n        plt.tick_params(axis='y', which='major', labelsize=8)\n        #ax.set_xticks([0.15, 0.68, 0.97])\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64bf6241ffe7401d3f42b713a06560f5ed16a0c5"},"cell_type":"markdown","source":"To add new features row wise  **add_new_feature_row**"},{"metadata":{"trusted":true,"_uuid":"9edf44dad8b57bbe0f8cba463a46494ca43d7f5c","_kg_hide-input":true},"cell_type":"code","source":"def add_new_feature_row(df,features):\n    for feature in features:\n        df[feature+\"_pct\"] = df[feature].pct_change()\n        df[feature+\"_diff\"] = df[feature].diff()\n        df.drop(feature,axis=1)\n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acfc7046d668f5837b9580242596ddb6bc1f6a52"},"cell_type":"markdown","source":"To normailize features using combined dataset **normalize_df**"},{"metadata":{"trusted":true,"_uuid":"e9fd60e00e06680009907a79a1b81b68ee673632","_kg_hide-input":true},"cell_type":"code","source":"def normalize_df(df,features):\n    for feature in features:\n        #normalize\n        df[feature+'_norm'] = (df[feature] - df[feature].mean())/df[feature].std()\n        #percentage change row wise\n        #df[feature+\"_pct\"] = df[feature].pct_change() # didnt give boost\n        #diff change row wise\n        #df[feature+\"_diff\"] = df[feature].diff() # didnt give boost\n        # Square\n        #df[feature+'^2'] = df[feature] * df[feature]\n        # Cube\n        #df[feature+'^3'] = df[feature] * df[feature] * df[feature]\n        # 4th power\n        #df[feature+'^4'] = df[feature] * df[feature] * df[feature] * df[feature]\n        # Cumulative percentile (not normalized)\n        #df[feature+'_cp'] = rankdata(df[feature]).astype('float32')\n        # Cumulative normal percentile\n        #df[feature+'_cnp'] = norm.cdf(df[feature]).astype('float32')\n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea721d668737ea900169937387134d4626294de8"},"cell_type":"markdown","source":"Lets separate the dataset for positive and negative class and check if feature distributions give us some signal.Lift and shift from [Gabriel's](https://www.kaggle.com/gpreda/santander-eda-and-prediction) kernel."},{"metadata":{"_uuid":"23a745b575245d371d181c7a393b325fd34334d3"},"cell_type":"markdown","source":"<div id=\"PlotLink\">\n**Plotting**\n    </div>"},{"metadata":{"_uuid":"4a53c3fa744894da6e4f1d5f4fc2d9c818bb325f"},"cell_type":"markdown","source":"Distplot for 1-100 features"},{"metadata":{"trusted":true,"_uuid":"e4fbbca22a51adf62ffa9564116f319477f285a9","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0] # segregate in two datasets correseponding to target\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[2:102] # run time errors forced this step to split into 100 sets\nplot_feature_distribution(t0, t1, '0', '1', features)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"437774d68238d81871dbb4dfc212054306fd5c99"},"cell_type":"markdown","source":"Distplot for 100-200 features"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e3ab785da538dc4a1191997a63926473018809a9","_kg_hide-output":true},"cell_type":"code","source":"features = train_df.columns.values[102:200] \nplot_feature_distribution(t0, t1, '0', '1', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"125b8af7fabe1e99dfc7a345d1fff10434a00526"},"cell_type":"markdown","source":"Lets see how the train and test features affects target. It may give some signal if some feature is more important for target prediction."},{"metadata":{"_uuid":"1338fdc375f3791f321dcb20e7d5dbbbb5cfd3b8"},"cell_type":"markdown","source":"Boxplot 1-100 features"},{"metadata":{"trusted":true,"_uuid":"bac6dac1e8cf9a0f884c9af88a99c4be0d2b1b3e","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"target   = train_df[\"target\"]\nfeatures = train_df.columns.values[2:102]\nplot_feature_boxplot(train_df, test_df, 'train', 'test', features, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebbdea24bfb2ec1823a3754df40d0fea5006628c"},"cell_type":"markdown","source":"Boxplot 100-200 features"},{"metadata":{"trusted":true,"_uuid":"1232f74d7bcdfa0d9684b17cb18062730bab6ee6","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"features = train_df.columns.values[102:200]\nplot_feature_boxplot(train_df, test_df, 'train', 'test', features, target )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08faefcf793d92dce1aac000a65b11bae8e1b9f6"},"cell_type":"markdown","source":"Violin Plot 1-100 features"},{"metadata":{"trusted":true,"_uuid":"9420c28fe3649de34e62f9ebc3675b6fb3a71dae","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"features = train_df.columns.values[2:102]\nplot_feature_violinplot(train_df, test_df, 'train', 'test', features, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d8dde0ef1921cdcd1ca3b2139b49f26a15d8e76"},"cell_type":"markdown","source":"Violin Plot 100-200 features"},{"metadata":{"trusted":true,"_uuid":"ab75857b28232e3579ad7f9c38d804ede31ae82f","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"features = train_df.columns.values[102:200]\nplot_feature_violinplot(train_df, test_df, 'train', 'test', features, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6d4aaaa8b30f142a0c4302b5315c3a56a72ec42"},"cell_type":"markdown","source":"All these plots above shows data between train and test is very much similar and mostly normally distributed. We may be able to use test dataset for extracting some info assuming its homogeneous with train. "},{"metadata":{"_uuid":"104543b85f97354744d3bd88bb4da6e21fe17d03"},"cell_type":"markdown","source":"<div id=CorLink>\n** Correlation and Binning **\n</div>"},{"metadata":{"_uuid":"b411b853a7ca5292eafa9bf60d36b8ca81c3f3c1"},"cell_type":"markdown","source":"Lets find out corr between features, we may be able to drop couple of features if highly correlated. "},{"metadata":{"trusted":true,"_uuid":"508f2e7cfb23b2f8b80fb9b7d98d4f44d9dc2a98","_kg_hide-input":true},"cell_type":"code","source":"correlations = train_df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index() #\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']] # remove corr between same cols\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"565058985a7018f19f05eb5f3cd976e511b35830"},"cell_type":"markdown","source":"Highest correlation between top 10 features is as follows"},{"metadata":{"trusted":true,"_uuid":"615d0c89c275687c12bc2e24585e0440df22f474","_kg_hide-input":true},"cell_type":"code","source":"#highest correlated features are\ncorrelations.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f062a6b91ef3ea4399b62c84f791efc5cfd1d7c","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"correlations = correlations.iloc[-25:,]\nplt.figure()\nfig, ax = plt.subplots(figsize=(10,12))\nsns.heatmap(correlations.pivot_table(index='level_0',columns='level_1'))\nplt.xlabel(\"\")\nplt.ylabel(\"\")\n#plt.xticks([], [])\n#plt.yticks([], [])\nplt.xticks(rotation=70)\nplt.title(\"Corr plot between top 25 vars\",fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2715630f49dba95b7c9546e99c77e810c9eb079"},"cell_type":"markdown","source":"Correlation plot also shows not relation between features, looks to be pretty independent of each other."},{"metadata":{"_uuid":"ca0dc1702a521a7079ff896003ca3be48fa307c6"},"cell_type":"markdown","source":"### Binning"},{"metadata":{"_uuid":"f27004dbc2dbdbaa72e967bed03966c115dbe8d3"},"cell_type":"markdown","source":"lets try to find if binning of the features shows some trend for predicting target. We will use consistent pattern of using aa utility function and calling with 100 features in one call. "},{"metadata":{"trusted":true,"_uuid":"8868f610c484bf723a72eb8556de61267263bc38","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#features = train_df.columns.values[2:102]\n#plot_binned_feature_target_violinplot(train_df,features,target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ebe7ef051b2caba5e15f2d13b859027ee087233","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#features = train_df.columns.values[102:200]\n#plot_binned_feature_target_violinplot(train_df,features,target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca5840e18fdaef1823bbf1812d3e7aeec8e6b2bb"},"cell_type":"markdown","source":"Binning plots also does not show any different story. "},{"metadata":{"trusted":true,"_uuid":"d34774824dc25f89aa8e14ae79ff12de9e3e3ee9","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0f97c79fafccbc795d71f394e9ab73dcdc27364"},"cell_type":"markdown","source":"<div id = FeatLink>\n** Feature Engineering **\n    </div>"},{"metadata":{"trusted":true,"_uuid":"6129ea24500d025bc35582afb5fc176b26a71489"},"cell_type":"markdown","source":"Lets try to find if data is some sort of time series data. as one of the post was doubting. We will try to add features which will be row wise, like percentage increase from one row to next, difference from one row to next, ratio etc."},{"metadata":{"_uuid":"e965fc7c61a64f5b2a0f894137e08f098901f236"},"cell_type":"markdown","source":"Next we are going to combine is two data sets and try to extract some info from test dataset into train features. This idea is from [William's](https://www.kaggle.com/blackblitz/gaussian-naive-bayes) kernel. Wel will create a ratio /pct_change/diff as new features to factor for time series hypothesis."},{"metadata":{"trusted":true,"_uuid":"7d657777097d7b7c2a5d128bcb8e5bde25becf77","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"test_df['target']= np.nan\ncombine_df = train_df.append(test_df,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dab867b443e9b811bb170ecdecc59c530f69fd1e"},"cell_type":"code","source":"features = train_df.columns.values[2:]\ncombine_df = normalize_df(combine_df,features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f70b83874df6001fa8d3e43ea7dfdc7900f411b8"},"cell_type":"markdown","source":"Separate out train and test. Append new features created to training dataset."},{"metadata":{"trusted":true,"_uuid":"70e5e4e99e3f9c3e0d5c3fd6f4d47921cfbfc7b4","_kg_hide-input":true},"cell_type":"code","source":"train_df = combine_df[combine_df['target'].notnull()].reset_index(drop=True)\ntest_df = combine_df[combine_df['target'].isnull()].reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f6fc555dc9db851676f9f67ac90465b216f7f0e"},"cell_type":"markdown","source":"Plot selectively if new any new features give some insight for target prediction."},{"metadata":{"trusted":true,"_uuid":"579128f0893db55eec000e60a6927ecd121ab56f","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#features = train_df.columns.values[201:]\n#plot_binned_feature_target_violinplot(train_df,features,target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f78b966da8865bb2fa0c54c3a7f87671d15743fb"},"cell_type":"markdown","source":"<div id = ModLink>\n** Modeling **\n    </div>"},{"metadata":{"_uuid":"b1cfdf207e434c21a34727c39adb0714cb614bfa"},"cell_type":"markdown","source":"This is model lifted and shifted from [Fayaz's](https://www.kaggle.com/fayzur/lightgbm-customer-transaction-prediction) kernel."},{"metadata":{"trusted":true,"_uuid":"19087a7d6d9c3f1d85fcb01f3ec1736b5b6902aa","_kg_hide-input":true},"cell_type":"code","source":"#test_df = test_df.drop(\"target\",axis=1)\npredictors = train_df.columns.values.tolist()[2:]\nnfold = 10\ntarget = 'target'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"118e7f6d0b3247ce06fd3c371ddd4c2cafb709e6","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"param = {\n     'num_leaves': 18,\n     'max_bin': 63,\n     'min_data_in_leaf': 5,\n     'learning_rate': 0.010614430970330217,\n     'min_sum_hessian_in_leaf': 0.0093586657313989123,\n     'feature_fraction': 0.056701788569420042,\n     'lambda_l1': 0.060222413158420585,\n     'lambda_l2': 4.6580550589317573,\n     'min_gain_to_split': 0.29588543202055562,\n     'max_depth': 49,\n     'save_binary': True,\n     'seed': 1337,\n     'feature_fraction_seed': 1337,\n     'bagging_seed': 1337,\n     'drop_seed': 1337,\n     'data_random_seed': 1337,\n     'objective': 'binary',\n     'boosting_type': 'gbdt',\n     'verbose': 1,\n     'metric': 'auc',\n     'is_unbalance': True,\n     'boost_from_average': False\n}\n\n\nnfold = 10\n\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    #print(\"after lgb train\")\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n    #print(\"after lgb test\")\n    nround = 8523\n    clf = lgb.train(param, \n                    xg_train, \n                    nround, \n                    valid_sets = [xg_valid], \n                    early_stopping_rounds=250,\n                    verbose_eval=250)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=nround) \n    #print(\"after lgb fit\")\n    predictions += clf.predict(test_df[predictors], num_iteration=nround) / nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.4f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f278c7024610bf96fd6e8481d0ad7fbbefcace3"},"cell_type":"markdown","source":"Feature Importance as per model"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"44a4fa07451a6ee51b23de052f48690d3ccada00"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(clf, max_num_features=100, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"206bc988ebf49ece7fcc0e6b3731316c94d46cef"},"cell_type":"markdown","source":"Submission file"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"ab96b4788012a836de70b91402fc95cc03cc94f7"},"cell_type":"code","source":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"sant_lgb.csv\", index=False)\nsub_df[:10]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}