{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"datatypes = {'key': 'str', \n              'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\ntrain_df = pd.read_csv('../input/train.csv', nrows=5000000, dtype=datatypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c273afc07a82469a02c80fd119c20f9f3f4df150"},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a80805b212ac2358142848b1f271a120e60a8ba"},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv', dtype=datatypes)\ntest_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10505206bede4f33af3f6a60deb6c23ef528c357"},"cell_type":"code","source":"def manhattan_distance(lat1, long1, lat2, long2):\n    diff_lat = abs(lat1 - lat2)\n    diff_long = abs(long1 - long2)\n    return (diff_lat + diff_long)\n\ndef distance_between_points(df):\n    df['diff_lat'] = abs(df['dropoff_latitude'] - df['pickup_latitude'])\n    df['diff_long'] = abs(df['dropoff_longitude'] - df['pickup_longitude'])\n    df['manhattan_dist'] = df['diff_lat'] + df['diff_long']\n    \n    jfk = [40.6413, -73.7781]\n    lga = [40.7769, -73.8740]\n    ewr = [40.6895, -74.1745]\n    # how far was this ride from the 3 nearby airports?\n    df['jfk_dist_pickup'] = manhattan_distance(df['pickup_latitude'], df['pickup_longitude'], jfk[0], jfk[1])\n    df['jfk_dist_dropoff'] = manhattan_distance(df['dropoff_latitude'], df['dropoff_longitude'], jfk[0], jfk[1])\n    df['lga_dist_pickup'] = manhattan_distance(df['pickup_latitude'], df['pickup_longitude'], lga[0], lga[1])\n    df['lga_dist_dropoff'] = manhattan_distance(df['dropoff_latitude'], df['dropoff_longitude'], lga[0], lga[1])\n    df['ewr_dist_pickup'] = manhattan_distance(df['pickup_latitude'], df['pickup_longitude'], ewr[0], ewr[1])\n    df['ewr_dist_dropoff'] = manhattan_distance(df['dropoff_latitude'], df['dropoff_longitude'], ewr[0], ewr[1])\n    \ndistance_between_points(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fc73096b8f634e139d9d5dbc8261f967f4770d1"},"cell_type":"code","source":"def extract_date_details(df):\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC')\n    df['year'] = df['pickup_datetime'].apply(lambda date: date.year)\n    df['month'] = df['pickup_datetime'].apply(lambda date: date.month)\n    df['day'] = df['pickup_datetime'].apply(lambda date: date.weekday())\n    df['hour'] = df['pickup_datetime'].apply(lambda date: date.hour)\n    \nextract_date_details(train_df)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19ece663f679bdfdfe4caa499f7191a99cbfcb72"},"cell_type":"code","source":"def remove_outliers(df):\n    # remove nulls\n    df = df.dropna()\n    \n    # remove any lat/long changes that are too big or too small\n    df = df[(df['diff_lat'] < 5.0) & (df['diff_long'] < 5.0)]\n    df = df[(df['diff_lat'] > .001) & (df['diff_long'] > .001)]\n    \n    # remove any pickups/dropoffs not within nyc bounds\n    df = df[(df['pickup_longitude'] < -72) & (df['pickup_longitude'] > -75)]\n    df = df[(df['pickup_latitude'] < 42) & (df['pickup_latitude'] > 39)]\n    df = df[(df['dropoff_longitude'] < -72) & (df['dropoff_longitude'] > -75)]\n    df = df[(df['dropoff_latitude'] < 42) & (df['dropoff_latitude'] > 39)]\n\n    # remove invalid fare or passenger count\n    df = df[(df['fare_amount'] > 2.50) & (df['fare_amount'] < 200) & (df['passenger_count'] <= 6) & (df['passenger_count'] > 0)] \n    return df\n    \ntrain_df = remove_outliers(train_df)\nlen(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf9c4740426c623786c0759ff990a879e9c6f57"},"cell_type":"code","source":"plt.scatter(train_df[:10000]['manhattan_dist'], train_df[:10000]['fare_amount'])\nplt.xlabel('manhattan distance')\nplt.ylabel('fare')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cae445fb43ef90e62f3ede5a6158bba4323cf4a2"},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70b526279023bdf77f07f746fde6398768051f0c"},"cell_type":"code","source":"def convert_to_one_hot (column, num_buckets, df, starting_index = 0):\n    df_size = df.shape[0]\n    one_hots = np.zeros((df_size, num_buckets), dtype='byte')\n    one_hots[np.arange(df_size), df[column].values - starting_index] = 1\n    return one_hots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43c3c6c731df19c8ee97824600529207617ee170"},"cell_type":"code","source":"year = convert_to_one_hot('year', 7, train_df, 2009)\nhour = convert_to_one_hot('hour', 24, train_df, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b93fc8a1438b743298971a9b11897791d9862d6"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f83d6e254e10a593567f6b66e6ad11baf3708b3"},"cell_type":"code","source":"def bucketize_feature(df,column):\n    # split rides into 10 bins where 10% of rides were\n    # use the quantile splits from train_df data\n    buckets = train_df[column].quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9]).values\n    bins = np.array(df[column].values)\n    \n    # set bin number\n    lower_bound = -100000\n    for i in range(buckets.shape[0]):\n        upper_bound = buckets[i]\n        bins[(bins >= lower_bound) & (bins < upper_bound)] = i\n        lower_bound = upper_bound\n    bins[(bins < 0) | (bins > 8)] = 9\n    bins = np.array(bins, dtype='byte')\n\n    return bins\np_long = bucketize_feature(train_df, 'pickup_longitude')\np_lat = bucketize_feature(train_df, 'pickup_latitude')\nd_long = bucketize_feature(train_df, 'dropoff_longitude')\nd_lat = bucketize_feature(train_df, 'dropoff_latitude')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc11ea19a52c2d339df01a2a5e52ec9d1a300d7b"},"cell_type":"code","source":"print(p_long)\nprint(p_lat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72ee2d4c61e82726c9cd0944276bf7684c6ee0ee"},"cell_type":"code","source":"def feature_cross(a1, a2):\n    rows = a1.shape[0]\n    # 10 buckets for each, means 10*10 columns in feature cross\n    cols = 100\n    cross = np.zeros((rows, cols), dtype='byte')\n    cross[np.arange(rows), (a1 * 10) + a2] = 1\n    return cross\n\n# cross latitudes and longitudes to get 1-hot vector representing grid of nyc\np_lat_x_long = feature_cross(p_lat, p_long)\nd_lat_x_long = feature_cross(d_lat, d_long)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab965f9fc6beb56595fc32b27a1033656cb28cb3"},"cell_type":"code","source":"unique, counts = np.unique(p_long, return_counts=True)\nprint (np.asarray((unique, counts)).T)\nunique, counts = np.unique(p_lat, return_counts=True)\nprint (np.asarray((unique, counts)).T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e5bbf37afab3c27fe1dd583522a717018c8ec65"},"cell_type":"code","source":"print (p_lat_x_long.shape)\nprint (d_lat_x_long.shape)\nprint (year.shape)\nprint (hour.shape)\nprint (train_df['manhattan_dist'].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a0453b2bd020e6ab8eb6ad8406a7ab3b421acf6"},"cell_type":"code","source":"# combine engineered features to create input layer\nmanhattan = train_df['manhattan_dist'].values.reshape(len(train_df), 1)\njfk_p = train_df['jfk_dist_pickup'].values.reshape(len(train_df), 1)\njfk_d = train_df['jfk_dist_dropoff'].values.reshape(len(train_df), 1)\nlga_p = train_df['lga_dist_pickup'].values.reshape(len(train_df), 1)\nlga_d = train_df['lga_dist_dropoff'].values.reshape(len(train_df), 1)\newr_p = train_df['ewr_dist_pickup'].values.reshape(len(train_df), 1)\newr_d = train_df['ewr_dist_dropoff'].values.reshape(len(train_df), 1)\n\ntrain_X = np.concatenate((p_lat_x_long, d_lat_x_long, year, hour, manhattan, jfk_p, jfk_d, lga_p, lga_d, ewr_p, ewr_d), axis=1)\ntrain_y = train_df['fare_amount'].values\nprint(train_X.shape)\nprint(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"210af669596f7eb095d3160f390cfe66975a4bf9"},"cell_type":"code","source":"validate_df = pd.read_csv('../input/train.csv', skiprows=range(1,10000001), nrows=10000, dtype=datatypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5174084b54476540da14def050465566dee12ff4"},"cell_type":"code","source":"distance_between_points(validate_df)\nvalidate_df = remove_outliers(validate_df)\n\ndef extract_features(df):\n    #preprocess data, extract features we care about\n    extract_date_details(df)\n    p_lo = bucketize_feature(df, 'pickup_longitude')\n    p_la = bucketize_feature(df, 'pickup_latitude')\n    d_lo = bucketize_feature(df, 'dropoff_longitude')\n    d_la = bucketize_feature(df, 'dropoff_latitude')\n    p_la_x_lo = feature_cross(p_la, p_lo)\n    d_la_x_lo = feature_cross(d_la, d_lo)\n    yr = convert_to_one_hot('year', 7, df, 2009)\n    hr = convert_to_one_hot('hour', 24, df, 0)\n    manhattan = df['manhattan_dist'].values.reshape(len(df), 1)\n    jfk_p = df['jfk_dist_pickup'].values.reshape(len(df), 1)\n    jfk_d = df['jfk_dist_dropoff'].values.reshape(len(df), 1)\n    lga_p = df['lga_dist_pickup'].values.reshape(len(df), 1)\n    lga_d = df['lga_dist_dropoff'].values.reshape(len(df), 1)\n    ewr_p = df['ewr_dist_pickup'].values.reshape(len(df), 1)\n    ewr_d = df['ewr_dist_dropoff'].values.reshape(len(df), 1)\n\n    print (p_la_x_lo.shape)\n    print (d_la_x_lo.shape)\n    print (yr.shape)\n    print (hr.shape)\n    print (manhattan.shape)\n\n    X = np.concatenate((p_la_x_lo, d_la_x_lo, yr, hr, manhattan, jfk_p, jfk_d, lga_p, lga_d, ewr_p, ewr_d), axis=1)\n    return X\n\nX = extract_features(validate_df)\ntrue_y = validate_df['fare_amount'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14004b1eb0c2c180e076ec6112a227cd46ce0af5"},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\nmodel = tf.keras.Sequential()\nmodel.add(layers.Dense(128, activation='relu', input_dim=238))\n# model.add(layers.BatchNormalization())\nmodel.add(layers.Dense(64, activation='relu'))\n# model.add(layers.BatchNormalization())\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer='adam',\n              loss='mse',       # mean squared error\n              metrics=['mae'])  # mean absolute error\nmodel.fit(train_X, train_y, epochs=15, batch_size=256,validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c5c9629e950f6bdf82a130228ddef902cab7258"},"cell_type":"code","source":"result = model.predict(X).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d8bf803931e0d06cec4f9e1c23a1b793318bc56"},"cell_type":"code","source":"mean_y = np.mean(train_df['fare_amount'].values)\n# result[result > 100] = mean_y\ndiff = true_y - result\nmse = np.sum(diff ** 2) / len(diff)\nrmse = np.sqrt(mse)\nprint (rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b95950f62aaa1f4e02d19eeefa59a05757628620"},"cell_type":"code","source":"distance_between_points(test_df)\nX_test = extract_features(test_df)\npred_y_test = model.predict(X_test).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccb99b6c919fedb16601f08e9f2d1058f5407dba"},"cell_type":"code","source":"print (max(pred_y_test))\nprint (min(pred_y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de5f1bdd30961a333951bf95e455710a6c67aec1"},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c3946bfa05be2557f624225d2c622c2d9e1d485"},"cell_type":"code","source":"sample_submission['fare_amount'] = pd.Series(pred_y_test)\nsample_submission.to_csv('nn_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4de48c64c74f2dbfe2712e3b93bd3af82222b4bb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}