{"cells":[{"metadata":{"_uuid":"479991d059e7b4474563b01c8629e776b7100a8d"},"cell_type":"markdown","source":"This kernel is based on the [olivier](https://www.kaggle.com/ogrellier) adversial validation [kernel](https://www.kaggle.com/ogrellier/adversarial-validation-and-lb-shakeup). I have only added how to find a validation set using adversial validation because I didn't know how to make one.\nIf my approach of making an adversarial validation set is wrong then please correct me.\n<br>Thanks"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"Adversarial validation is a mean to check if train and test datasets have significant differences. The idea is to use the dataset features to try and separate train and test samples.\n\nSo you would create a binary target that would be 1 for train samples and 0 for test samples and fit a classifier on the features to predict if a given sample is in train or test datasets!\n\nHere we will use a LogisticRegression and a TF-IDF vectorizer to check if text features distributions are different and see if we can separate the samples. \n\nThe best kernel on this is certainly [here](https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms) by [Konrad Banachewicz](https://www.kaggle.com/konradb)\n\nOther resources can be found [on fastML](http://fastml.com/adversarial-validation-part-one/)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntrn = pd.read_csv(\"../input/train.csv\", encoding=\"utf-8\")\nsub = pd.read_csv(\"../input/test.csv\", encoding=\"utf-8\")","execution_count":106,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a7850afb48af422d046f73641fb6fe3da0be690e"},"cell_type":"code","source":"#assign target if set is test set or not\ntrn['is_test'] = 0\nsub['is_test'] = 1","execution_count":107,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"19f9620aa26bd54937dfc11baa60ed963f87dd78"},"cell_type":"code","source":"orginal_train = trn.copy()","execution_count":108,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"db49fcda7f9cd7a66743c7864b366dd76c7d3159"},"cell_type":"code","source":"train = pd.concat([trn, sub], axis=0)","execution_count":109,"outputs":[]},{"metadata":{"_cell_guid":"356ea9e9-126b-4cf3-abe5-9eb4f9a0f5af","_uuid":"a9d76507e3a7805b16b2bf6e902223ee48ff1d8b","trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport regex\nvectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    tokenizer=lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 1), \n    max_features=20000\n)\ntrn_idf = vectorizer.fit_transform(trn.comment_text)\ntrn_vocab = vectorizer.vocabulary_\nsub_idf = vectorizer.fit_transform(sub.comment_text)\nsub_vocab = vectorizer.vocabulary_\nall_idf = vectorizer.fit_transform(train.comment_text.values)\nall_vocab = vectorizer.vocabulary_","execution_count":110,"outputs":[]},{"metadata":{"_cell_guid":"61f32cac-cd19-445c-bd04-3ffd9ccdcc1d","_uuid":"de3f629a93be69f4079ad1de465482ec9de4ba05"},"cell_type":"markdown","source":"Convert vocab dictionnaries to list of words"},{"metadata":{"_cell_guid":"c8539cd9-b7b2-47fa-9cdd-0de5498ac0c6","_uuid":"578f39976d38377dd72adc9062bf200d7e715e77","collapsed":true,"trusted":true},"cell_type":"code","source":"trn_words = [word for word in trn_vocab.keys()]\nsub_words = [word for word in sub_vocab.keys()]\nall_words = [word for word in all_vocab.keys()]","execution_count":111,"outputs":[]},{"metadata":{"_cell_guid":"c1d48036-0873-48ea-b712-db51b607684f","_uuid":"2681c121a8366fdda44d681c6708e809e8e1da2f"},"cell_type":"markdown","source":"Check a few figures on words not in train or test"},{"metadata":{"_cell_guid":"8dc68da6-c0ef-44ab-9ce8-4aa6a4425574","_kg_hide-input":true,"_uuid":"acc03c89faa3578c8b7c6a249a384f4b443ce4d7","trusted":true},"cell_type":"code","source":"common_words = set(trn_words).intersection(set(sub_words)) \nprint(\"number of words in both train and test : %d \"\n      % len(common_words))\nprint(\"number of words in all_words not in train : %d \"\n      % (len(trn_words) - len(set(trn_words).intersection(set(all_words)))))\nprint(\"number of words in all_words not in test : %d \"\n      % (len(sub_words) - len(set(sub_words).intersection(set(all_words)))))","execution_count":112,"outputs":[]},{"metadata":{"_cell_guid":"819aee86-98a6-4f24-82cb-4133a8d0dd13","_uuid":"5d17d63b9c61e83516b77c74a0a208171ec38848"},"cell_type":"markdown","source":"This means there are substantial differences between train and test vocabularies or term frequencies\n\nLet's check if a LinearRegression can make a difference between train and test using this.\n\nWe would take the output of the TF-IDF vectorizer fitted on train + test."},{"metadata":{"_cell_guid":"92863c3d-4600-4fe5-8287-e2777b693f70","_uuid":"94884ef5129ad259ecc0034fa8bc328da14aa47c","trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n#predictions to save each fold predictions results\npredictions = np.zeros(train.shape[0])\n\n# Create target where all train samples are ones and all test samples are zeros\ntarget = train.is_test.values\n# Shuffle samples to mix zeros and ones\nidx = np.arange(all_idf.shape[0])\nnp.random.seed(1)\nnp.random.shuffle(idx)\nall_idf = all_idf[idx]\ntarget = target[idx]\n# Train a Logistic Regression\nfolds = StratifiedKFold(5, True, 1)\nfor trn_idx, val_idx in folds.split(all_idf, target):\n    lr = LogisticRegression()\n    lr.fit(all_idf[trn_idx], target[trn_idx])\n    print(roc_auc_score(target[val_idx], lr.predict_proba(all_idf[val_idx])[:, 1]))\n    predictions[val_idx] = lr.predict_proba(all_idf[val_idx])[:, 1]","execution_count":113,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"068598c13277553a61a7b82944caeb17c6e9cc26"},"cell_type":"code","source":"\n#seperate train rows which have been misclassified as test and use them as validation\ntrain[\"predictions\"] = predictions\npredictions_argsort = predictions.argsort()\ntrain_sorted = train.iloc[predictions_argsort]\n\n#select only trains set because we need to find train rows which have been misclassified as test set and use them for validation\ntrain_sorted = train_sorted.loc[train_sorted.is_test == 0]\n\n#Why did I chose 0.7 as thereshold? just a hunch, but you should try different thresholds i.e 0.6, 0.8 and see the difference in validation score and please report back. :) \ntrain_as_test = train_sorted.loc[train_sorted.predictions > 0.7]\n#save the indices of the misclassified train rows to use as validation set\nadversarial_set_ids = train_as_test.index.values\nadversarial_set = pd.DataFrame(adversarial_set_ids, columns=['adversial_set_ids'])\n#save adversarial set index\nadversarial_set.to_csv('adversarial_set_ids.csv', index=False)","execution_count":115,"outputs":[]},{"metadata":{"_uuid":"18a373f0d7c436b2b5f5eea31b13267706d4f79a"},"cell_type":"markdown","source":"We can now use the ids to seperate an adversarial validation set from the train set and validate our models on adversarial set because traditional Kfold might not work in this competition as found by [olivier](https://www.kaggle.com/ogrellier/adversarial-validation-and-lb-shakeup) and [Konrad Banachewicz](https://www.kaggle.com/konradb/adversarial-validation).\n\n"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c4856858b6218a5b04c5a35ec6b5f9afe82ef129"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}