{"cells":[{"metadata":{"_uuid":"d6d416591c00598f04bfef280548e47217fe9456"},"cell_type":"markdown","source":"**Fraud Detection with Neural Network**\n\n"},{"metadata":{"_uuid":"287b79323023b6a2759534741357a7bb3ad6eff7"},"cell_type":"markdown","source":"# Neural Network for Fraud detection with SMOTE\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIn this notebook for handle the imbalanced data I have used the undersample technique. \n\nI have also writed another notebook where handle this situation using [SMOTE](https://www.kaggle.com/davidevegliante/nn-for-fraud-detection-with-smote) technique.\n\n**These are my first two notebooks, I hope to receive comment and advice for improve the understanding of the tools I used. **\n\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\n\n# read the dataset and print five rows\noriginal_dataset = pd.read_csv('../input/creditcard.csv')\n\ndataset = original_dataset.copy()\nprint(dataset.head(5))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52378368d3e73e4934641a70614520403b21a4ca"},"cell_type":"markdown","source":"Let's see how many example in our dataset we have. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# count how many entry there are for every class\nclasses_count = pd.value_counts(dataset['Class'])\n\nprint(\"{} Non-fraud example\\n{} Fraud examples\".format(classes_count[0], classes_count[1]))\n\n# classes_count is a Series. \nclasses_count.plot(kind = 'bar')\nplt.xlabel('Classes')\nplt.ylabel('Frequencies')\nplt.title('Fraud Class Hist')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc01e42af1b5496883736f07fa289d203ea9e974"},"cell_type":"markdown","source":"The classes of the dataset are not represented equally. "},{"metadata":{"trusted":true,"_uuid":"76f0cbb25c13d9fce563e985a70df6fd632ed224"},"cell_type":"code","source":"# scale the amount feature\nfrom sklearn.preprocessing import StandardScaler\namount_scaler = StandardScaler().fit(dataset[['Amount']])\ndataset['AmountScaled'] = amount_scaler.transform(dataset[['Amount']])\n\n# remove the old Amount Feature\ndataset.drop(['Time', 'Amount'], axis = 1, inplace = True)\n\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c9da2de181f0f0437433e5c037d1516e7104404"},"cell_type":"markdown","source":"**  Undersampling with ratio 1**"},{"metadata":{"trusted":true,"_uuid":"a03ba047bd2ce7e44ee5d4847960803393cb5c0b"},"cell_type":"code","source":"X = dataset.loc[:, dataset.columns != 'Class' ]\ny = dataset.loc[:, dataset.columns == 'Class' ]\n\nfrom imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state = 0, sampling_strategy = 1.0)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.20, stratify = y_resampled)\n\nassert len(y_train[y_train == 1]) + len(y_test[y_test == 1]) == len(dataset[dataset.Class == 1])\nprint(\"train_set size: {} - Class0: {}, Class1: {}\".format( len(y_train), len(y_train[y_train == 0]), len(y_train[y_train == 1]) ))\nprint(\"test_set size: {} - Class0: {}, Class1: {}\".format( len(y_test), len(y_test[y_test == 0]), len(y_test[y_test == 1]) ))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a12bb26240b3169a8283fe4ae59f3c295e12a55"},"cell_type":"markdown","source":"**NN Structure**\n"},{"metadata":{"trusted":true,"_uuid":"e7c877e99d06779c9da4adf454fd457d424ac7c3"},"cell_type":"code","source":"import tensorflow as tf\nimport keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import regularizers\n\n# init ann\nclassifier = Sequential()\n\n# in this case we used a rectifier activation function for the hidden layers\n# and a sigmoid function for the output layer\nclassifier.add(Dense(\n    input_dim = len(X.columns), # input neurons\n    units = 15, # first hidden layer\n    kernel_initializer = 'he_normal', \n    bias_initializer = 'zeros',\n    activation = 'relu', # activation function (rectifier)\n    kernel_regularizer=regularizers.l2(0.006)\n))\n# add a new hidden layer with the same number of neurons\nclassifier.add(Dense(\n    units = 6, # second hidden layer\n    kernel_initializer = 'he_normal',\n    bias_initializer = 'zeros',\n    activation = 'relu', # activation function (rectifier)\n    kernel_regularizer=regularizers.l2(0.006)\n))\n# add the output layer with one neuron\nclassifier.add(Dense(\n    units = 1, # output layer\n    kernel_initializer = 'random_uniform',\n    bias_initializer = 'zeros',\n    activation = 'sigmoid', # activation function (sigmoid)\n    kernel_regularizer=regularizers.l2(0.006)\n))\n\n# Compiling the ANN\nclassifier.compile(\n    optimizer = 'adam', \n    loss = 'binary_crossentropy', # cost function\n    metrics = ['accuracy']\n)\n\n# fit the ann to the Training set\nhistory = classifier.fit(\n    X_train, y_train,  # training set\n    validation_data = (X_test, y_test),\n    batch_size = 40,\n    epochs = 70,\n    verbose = False\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"834979aec6948463781095d581d4d79d93149085"},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea6a94b895724239172956e405e298224dbd4422"},"cell_type":"code","source":"\nfrom sklearn.metrics import confusion_matrix\ny_test_pred = classifier.predict(X_test) > 0.5\ncm = confusion_matrix(y_test, y_test_pred)\n\nprint('Train Accuracy: {}\\nTest Accuracy:{}'.format(history.history['acc'][-1], history.history['val_acc'][-1]))\nprint(cm)\n\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.copper)\nclassNames = ['Negative','Positive']\nplt.title('Fraud or Not Fraud Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), color = 'white')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1143084affe794305fcfa8e93bfffa9422bbe56e"},"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1 = f1_score(y_test, y_test_pred)\n\nprint(\"F1 Score: {}\".format(f1))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}