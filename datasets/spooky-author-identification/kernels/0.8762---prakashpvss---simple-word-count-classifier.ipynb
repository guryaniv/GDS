{"metadata": {"language_info": {"mimetype": "text/x-python", "version": "3.6.3", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "name": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "cells": [{"outputs": [], "metadata": {"_uuid": "6f6fe84deebeef8fd2540637d31307b38c1f9609", "_cell_guid": "13b4a25f-258c-4e43-8e25-50da7ad63a0f", "collapsed": true}, "cell_type": "code", "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "sample = pd.read_csv('../input/sample_submission.csv')"], "execution_count": 1}, {"outputs": [], "metadata": {"_uuid": "0cbb47de1292eda9b57935246b92ecd7556e79b8", "_cell_guid": "43771189-6157-472e-ab72-d98b3ed84b75", "collapsed": true}, "cell_type": "code", "source": ["import nltk"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "ba95f3d5ffb08c144ead9a7dfca0c46278719c73", "_cell_guid": "88eec9b5-4c7a-4120-a1ce-020efd77d9a6", "collapsed": true}, "cell_type": "code", "source": ["from nltk.stem import WordNetLemmatizer\n", "wordnet_lemmatizer = WordNetLemmatizer()\n", "\n", "from nltk.corpus import stopwords\n", "stop_words = set(stopwords.words('english'))\n", "\n", "def myTokenizer(s):\n", "    s=s.lower()\n", "    tokens = nltk.tokenize.word_tokenize(s)\n", "    tokens = [t for t in tokens if len(t) > 2]\n", "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n", "    tokens = [t for t in tokens if t not in stop_words]\n", "    return tokens\n", "    "], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "d379d554ae0d0cfbf8c385d2f47e631f853da431", "_cell_guid": "831983ce-4e00-4921-9066-34b50b17238d", "collapsed": true}, "cell_type": "code", "source": ["word_index_map={}\n", "curr_index = 0\n", "for index,row in train.iterrows():\n", "    tokens = myTokenizer(row['text'])\n", "    for t in tokens:\n", "        if t not in word_index_map:\n", "            word_index_map[t]=curr_index\n", "            curr_index +=1 \n", "print(len(word_index_map))\n"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "9e9258608f3a66964c767428cb3cf4ffb247d13e", "_cell_guid": "0a484233-050c-4631-9a0f-0ba9a7be1e37", "collapsed": true}, "cell_type": "code", "source": ["N = len(train)\n", "data = np.zeros((N,len(word_index_map)+1)) # +1 for Label\n"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "063c4ac435836f1249849ed0fedbe2f2c9759833", "_cell_guid": "6b47e1d9-c172-46f0-9f50-246c835dc4cb", "collapsed": true}, "cell_type": "code", "source": ["from sklearn import preprocessing"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "21a8200658d69aa5f93fb6904e3ee90ee18cc0de", "_cell_guid": "fd278350-1a78-4a21-91e1-db0bffab2681", "collapsed": true}, "cell_type": "code", "source": ["lbl_enc = preprocessing.LabelEncoder()\n", "y = lbl_enc.fit_transform(train.author.values)"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "dad79bd41c0c08de2c69496ae06e2b410c31877e", "_cell_guid": "0a999815-033f-4f85-bcad-91f9f7832f3f", "collapsed": true}, "cell_type": "code", "source": ["def tokens_to_vectors(tokens,label):\n", "    if(len(tokens) == 0):\n", "        print('No Tokens Available')\n", "    x = np.zeros(len(word_index_map)+1) # +1 is for label\n", "    for t in tokens:\n", "        if t in word_index_map:\n", "            x[word_index_map[t]] +=1\n", "    if x.sum() != 0:\n", "        x = x/x.sum()\n", "    x[-1] = label\n", "    return x\n", "for idx, row in train.iterrows():\n", "    tokens = myTokenizer(row['text'])\n", "    if len(tokens) == 0:\n", "        print(row['text'])\n", "    data[idx,:] = tokens_to_vectors(tokens,y[idx])\n", "    "], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "35cbcda3424e0acd21dd6eda229104aa79318af4", "_cell_guid": "5e928d6e-d867-4164-840a-d0abda65db17", "collapsed": true}, "cell_type": "code", "source": ["from sklearn.linear_model import LogisticRegression"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "77a791fc1f66352b2e161d480db4beb5700d5b80", "_cell_guid": "47eee6ba-db37-470e-a098-15e1dcddd3fb", "collapsed": true}, "cell_type": "code", "source": ["model = LogisticRegression()\n", "model.fit(data[:,:-1],data[:,-1])"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "14f1d2dfb124a9625656da2cf3692347eacd8d4a", "_cell_guid": "d48affaf-7280-4fb5-898c-af0d064ea4ba", "collapsed": true}, "cell_type": "code", "source": ["model.score(data[:,:-1],data[:,-1])"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "ed7a5a8e1818d3f675d729d13df3c2200a0194a7", "_cell_guid": "1962a6e0-f936-4081-99b9-a3795be54651", "collapsed": true}, "cell_type": "code", "source": ["N=len(test)\n", "td = np.zeros((N,len(word_index_map)+1))\n", "for idx,row in test.iterrows():\n", "    tokens = myTokenizer(row['text'])\n", "    td[idx,:] = tokens_to_vectors(tokens,0)\n", "td = td[:,:-1]\n", "    \n", "p = model.predict_proba(td)# 0 is dummy\n", "print(p.shape)\n", "    "], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "68b03a54cd5d039401c2a2a30d5f2d62765855e8", "_cell_guid": "eb15990c-346a-4a16-879c-113ef18f951a", "collapsed": true}, "cell_type": "code", "source": ["result = pd.DataFrame()\n", "result['id'] = test['id']\n", "result['EAP'] = p[:,0]\n", "result['HPL'] = p[:,1]\n", "result['MWS'] = p[:,2]\n"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "6041413d65e79c0ec5272de42cdb2c2be809c720", "_cell_guid": "a3c44972-42db-4149-9458-2628a8b0fed5", "collapsed": true}, "cell_type": "code", "source": ["result.head()\n", "result.to_csv(\"result.csv\", index=False)"], "execution_count": null}, {"outputs": [], "metadata": {"_uuid": "4642db9dbb832b186dd847278f85725225671c13", "_cell_guid": "de003339-c508-412c-85e1-083008b939b5", "collapsed": true}, "cell_type": "code", "source": [], "execution_count": null}], "nbformat": 4, "nbformat_minor": 1}