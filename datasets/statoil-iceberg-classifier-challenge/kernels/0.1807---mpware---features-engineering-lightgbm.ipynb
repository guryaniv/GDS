{"cells": [{"source": ["The idea of this notebook is to run LightGBM classifier with the following simple features:\n", "\n", "**Regular aggregations per band considered as a signal:**\n", "* Minimum\n", "* Maximum\n", "* Mean\n", "* Standard deviation\n", "* Kurtosis\n", "* Skew\n", "\n", "**Other aggregrations per band considered as image:**\n", "* Standard deviation after Sobel filtering on x\n", "* Standard deviation after Sobel filtering on y\n", "* Standard deviation after Laplace filtering\n", "\n", "**Combined bands :**\n", "* Pearson correlation coefficient\n", "* Standard deviation of sqrt(band1 x band1 + band2 x band2)\n", "\n", "One feature extracted from color composite image: Volume of shape inspired from this [notebook](https://www.kaggle.com/submarineering/submarineering-what-about-volume). And finally incidence_angle (all NaN dropped).\n", "\n", "LightGBM model is trained with Cross-Validation over 10 stratified folds without any normalization.\n", "\n", "Results: \n", "* Public LB: 0.1807\n", "* Private LB: 0.2088\n", "\n", "Not so bad for a simple model without CNN. Plotting features importance shows that angle of incidence is important.\n"], "cell_type": "markdown", "metadata": {"_uuid": "48b295c654df13513ab42cef309cc897b33e69c6", "_cell_guid": "369ace3b-5604-4eae-9aba-83782a8ba67f"}}, {"source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import os, math, shutil\n", "from scipy.ndimage import gaussian_filter\n", "from scipy.stats import kurtosis, skew\n", "from scipy.ndimage import laplace, sobel\n", "from skimage import img_as_float\n", "from sklearn.externals import joblib\n", "from skimage.morphology import reconstruction\n", "from sklearn.model_selection import KFold #for K-fold cross validation\n", "from sklearn.model_selection import StratifiedKFold #for K-fold cross validation\n", "from sklearn.model_selection import train_test_split #training and testing data split\n", "from sklearn.metrics import log_loss\n", "from sklearn.metrics import accuracy_score\n", "from sklearn import metrics #accuracy measure\n", "from sklearn.metrics import confusion_matrix #for confusion matrix\n", "import lightgbm as lgb\n", "import matplotlib.pyplot as plt\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "ANGLE = \"inc_angle\"\n", "ICEBERG = \"is_iceberg\"\n", "BAND1 = \"band_1\"\n", "BAND2 = \"band_2\"\n", "ID = \"id\"\n", "initial_model_path = \"lgbm\"\n", "#from subprocess import check_output\n", "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "7e58061f0e6bf53c0862edfb7514d13833b026a7", "_cell_guid": "b30ac1f2-ce8a-465b-b7e1-a8bdd143b684", "collapsed": true}}, {"source": ["# Load data\n", "train = pd.read_json('../input/train.json')\n", "print(\"rows, cols = \" + str(train.shape))\n", "train.head()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "24eedf826dd599e3e83f3736b97b1c7571a83d7e", "_cell_guid": "b89b77a0-9e9a-48ea-bf4d-239f6bc36c7c", "collapsed": true}}, {"source": ["Check for missing data. 133 angles not available, too bad."], "cell_type": "markdown", "metadata": {"_uuid": "502391a3f443045a9fe0a660a3fe1f7a0d6795e5", "_cell_guid": "12a3fbca-1175-4ffa-98ef-d09fc5129a36"}}, {"source": ["# Any missing data?\n", "train[ANGLE] = pd.to_numeric(train[ANGLE],errors='coerce')\n", "nullPD = pd.DataFrame(train.isnull().sum(), columns=['TotalNull'])\n", "nullPD"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0d879160abda2721f738c9faa0b862c70afdd7e5", "_cell_guid": "346ad158-f217-4c64-9f21-0b3497747dc6", "collapsed": true}}, {"source": ["Plot distribution of angle of incidence. It looks we already have good information with it!"], "cell_type": "markdown", "metadata": {"_uuid": "e2228c2fb47563edadc4f8ec4679f052f8771117", "_cell_guid": "cfe4b0be-c9a5-4f2e-8182-c58b64854518"}}, {"source": ["f, ax = plt.subplots(1,2,figsize=(16,5))\n", "r = train.plot(kind=\"kde\", y = ANGLE, ax=ax[0], label=\"Any\", grid=True, title=\"Angle of incidence KDE\")\n", "r = train[train[ICEBERG] == 1].plot(kind=\"kde\", label=\"Iceberg\", grid=True, y = ANGLE, ax=ax[1], title=\"Angle of incidence KDE\")\n", "r = train[train[ICEBERG] == 0].plot(kind=\"kde\", label=\"Not Iceberg\", grid=True, y = ANGLE, ax=r)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "98a5d4376a931bc3ab1c1fef60b3a5f1538afaef", "_cell_guid": "5cd6de2f-be9d-4fbc-8776-af0ebffbb06e", "collapsed": true}}, {"source": ["Check for balanced data. Looks good (not imbalanced)!"], "cell_type": "markdown", "metadata": {"_uuid": "664b8283824f460526ecc6d62c5bbf16379a2ebc", "_cell_guid": "15e84da0-2626-4338-9e30-c0e04f180845"}}, {"source": ["P = train.groupby(ICEBERG)[ID].count().reset_index()\n", "P['Percentage'] = 100*P[ID]/P[ID].sum()\n", "P"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "3bf5625a0bcda448dadf3db7d47cb237200d3e20", "_cell_guid": "d3a31589-fc3a-4abf-9e95-0af3b1dc6d31", "collapsed": true}}, {"source": ["Now the 22 features."], "cell_type": "markdown", "metadata": {"_uuid": "a2ed00f665d48822ec20a8b033637c03af05ed03", "_cell_guid": "a68efed1-4e6a-426e-a1df-a2b9d93f3762"}}, {"source": ["# Features\n", "MIN=\"min\"\n", "MAX=\"max\"\n", "MEAN=\"mean\"\n", "STD=\"std\"\n", "LAPLACE=\"laplacestd\"\n", "SOBEL0 = \"sobelstd_x\"\n", "SOBEL1 = \"sobelstd_y\"\n", "KURTOSIS = \"kurtosis\"\n", "SKEW = \"skew\"\n", "CORR = \"pearson\"\n", "HYP = \"hypstd\"\n", "\n", "AGG_COLS = [\n", "    \"%s_%s\"%(BAND1,MAX), \"%s_%s\"%(BAND2,MAX),\n", "    \"%s_%s\"%(BAND1,MIN), \"%s_%s\"%(BAND2,MIN),\n", "    \"%s_%s\"%(BAND1,MEAN), \"%s_%s\"%(BAND2,MEAN),\n", "    \"%s_%s\"%(BAND1,STD), \"%s_%s\"%(BAND2,STD),\n", "    \"%s_%s\"%(BAND1,KURTOSIS), \"%s_%s\"%(BAND2,KURTOSIS),\n", "    \"%s_%s\"%(BAND1,SKEW), \"%s_%s\"%(BAND2,SKEW),\n", "    \"%s_%s\"%(BAND1,SOBEL0), \"%s_%s\"%(BAND2,SOBEL0),\n", "    \"%s_%s\"%(BAND1,SOBEL1), \"%s_%s\"%(BAND2,SOBEL1),\n", "    \"%s_%s\"%(BAND1,LAPLACE), \"%s_%s\"%(BAND2,LAPLACE),\n", "    CORR,\n", "    HYP,\n", "]\n", "\n", "# Volume\n", "ISO = \"iso\"\n", "ISO_COLS = [\n", "        \"%s_%s\"%(BAND1,ISO), \"%s_%s\"%(BAND2,ISO)\n", "]\n", "VOLUME = \"vol\"\n", "\n", "# Final features\n", "FEATURES = [ANGLE, VOLUME] + AGG_COLS\n", "\n", "# Isolation function.\n", "def iso(arr):\n", "    image = img_as_float(np.reshape(np.array(arr), [75,75]))\n", "    image = gaussian_filter(image,2.5)\n", "    seed = np.copy(image)\n", "    seed[1:-1, 1:-1] = image.min()\n", "    mask = image \n", "    dilated = reconstruction(seed, mask, method='dilation')\n", "    return image-dilated\n", "\n", "# Standard deviation for sobel filter\n", "def sobelstd(arr, axis=0):\n", "    image = img_as_float(np.reshape(np.array(arr), [75,75]))\n", "    sobelstd = sobel(image, axis=axis, mode='reflect', cval=0.0).ravel()\n", "    return [sobelstd.std(), sobelstd.max(), sobelstd.mean()]\n", "\n", "# Standard deviation for laplace filter\n", "def lapacestd(arr):\n", "    image = img_as_float(np.reshape(np.array(arr), [75,75]))\n", "    lapacestd = laplace(image, mode='reflect', cval=0.0).ravel()\n", "    return [lapacestd.std(), lapacestd.max(), lapacestd.mean()]\n", "\n", "def volume(arr):\n", "    return np.sum(arr)\n", "\n", "def hypot(arr1, arr2):\n", "    hyp = np.hypot(arr1, arr2)\n", "    return [np.std(hyp), np.max(hyp), np.median(hyp)]\n", "\n", "def computeAdditionalFeatures(df):\n", "    # Aggregation on raw signal\n", "    df[\"%s_%s\"%(BAND1,MAX)] = df[BAND1].apply(lambda x: np.max(x))\n", "    df[\"%s_%s\"%(BAND2,MAX)] = df[BAND2].apply(lambda x: np.max(x))\n", "    df[\"%s_%s\"%(BAND1,MIN)] = df[BAND1].apply(lambda x: np.min(x))\n", "    df[\"%s_%s\"%(BAND2,MIN)] = df[BAND2].apply(lambda x: np.min(x))\n", "    df[\"%s_%s\"%(BAND1,MEAN)] = df[BAND1].apply(lambda x: np.mean(x))\n", "    df[\"%s_%s\"%(BAND2,MEAN)] = df[BAND2].apply(lambda x: np.mean(x))\n", "    df[\"%s_%s\"%(BAND1,STD)] = df[BAND1].apply(lambda x: np.std(x))\n", "    df[\"%s_%s\"%(BAND2,STD)] = df[BAND2].apply(lambda x: np.std(x))\n", "    df[\"%s_%s\"%(BAND1,KURTOSIS)] = df[BAND1].apply(lambda x: kurtosis(x))\n", "    df[\"%s_%s\"%(BAND2,KURTOSIS)] = df[BAND2].apply(lambda x: kurtosis(x))    \n", "    df[\"%s_%s\"%(BAND1,SKEW)] = df[BAND1].apply(lambda x: skew(x))\n", "    df[\"%s_%s\"%(BAND2,SKEW)] = df[BAND2].apply(lambda x: skew(x))     \n", "    df[\"%s_%s\"%(BAND1,SOBEL0)] = df[BAND1].apply(lambda x: sobelstd(x, axis=0)[0])\n", "    df[\"%s_%s\"%(BAND1,SOBEL1)] = df[BAND1].apply(lambda x: sobelstd(x, axis=1)[0])    \n", "    df[\"%s_%s\"%(BAND2,SOBEL0)] = df[BAND2].apply(lambda x: sobelstd(x, axis=0)[0])\n", "    df[\"%s_%s\"%(BAND2,SOBEL1)] = df[BAND2].apply(lambda x: sobelstd(x, axis=1)[0])   \n", "    df[\"%s_%s\"%(BAND1,LAPLACE)] = df[BAND1].apply(lambda x: lapacestd(x)[0])\n", "    df[\"%s_%s\"%(BAND2,LAPLACE)] = df[BAND2].apply(lambda x: lapacestd(x)[0])    \n", "    df[CORR] = df.apply(lambda row: np.corrcoef(x=row[BAND1], y=row[BAND2])[1,0], axis=1)\n", "    df[HYP] = df.apply(lambda row: hypot(row[BAND1], row[BAND2])[0], axis=1)\n", "    \n", "    # Volume\n", "    df[\"%s_%s\"%(BAND1,ISO)] = df[BAND1].apply(lambda x: iso(x))\n", "    df[\"%s_%s\"%(BAND2,ISO)] = df[BAND2].apply(lambda x: iso(x))\n", "    df[VOLUME] = (df[\"%s_%s\"%(BAND1,ISO)] + df[\"%s_%s\"%(BAND2,ISO)]).apply(volume)\n", "\n", "    cleanDF = df.dropna()\n", "    ret = cleanDF[FEATURES].copy(deep=True)\n", "    ret_labels = None\n", "    if ICEBERG in cleanDF.columns:\n", "        ret_labels = cleanDF[[ICEBERG]]\n", "    ret_ids = cleanDF[[ID]]\n", "    ret_cols = ret.columns\n", "            \n", "    return ret, ret_labels, ret_ids"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "fca4a081edf5ff734eb2bc57438b6dbcb9950d70", "_cell_guid": "73df076f-847c-4232-b3db-65d5ba08e9de", "collapsed": true}}, {"source": ["def read_and_normalize_train_data(train_df):\n", "    featuresDF, labelsDF, idsDF = computeAdditionalFeatures(train_df.copy(deep=True))\n", "    train_features = featuresDF.as_matrix()\n", "    train_target = labelsDF.as_matrix()\n", "    train_id = idsDF.as_matrix()\n", "    print(\"Features size: %s/%s\"%(str(train_features.shape), str(train_target.shape)))\n", "    return train_features, train_target, train_id, featuresDF[FEATURES].columns"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "412b3d747073e5e3f5944b62aa041663c7eb4914", "_cell_guid": "7333638d-ed40-4287-91c7-4ad5763cc80b", "collapsed": true}}, {"source": ["def fit_evaluate_model_lgbm(X_train, Y_train, X_valid, Y_valid, train_data_columns, model_path, num_fold, importance=False):\n", "    X_trainDF = pd.DataFrame(X_train, columns=train_data_columns)\n", "    X_validDF = pd.DataFrame(X_valid, columns=train_data_columns)\n", "    train_dataset = lgb.Dataset(X_trainDF, Y_train.reshape(Y_train.shape[0]))\n", "    test_dataset = lgb.Dataset(X_validDF, Y_valid.reshape(Y_valid.shape[0]))\n", "    # Fit\n", "    evals_result = {}\n", "    params = {\n", "            'objective': 'binary',\n", "            'metric': 'binary_logloss',\n", "            'boosting': 'gbdt',\n", "            'learning_rate': 0.1,\n", "            'num_rounds': 150,\n", "            'early_stopping_rounds': 100\n", "    }\n", "    gbm = lgb.train(params, train_dataset, \n", "                    valid_sets=test_dataset, \n", "                    evals_result=evals_result,\n", "                    verbose_eval=50)\n", "    # Evaluate\n", "    predict_y_proba_gbm = gbm.predict(X_valid, num_iteration=gbm.best_iteration) # Proba of class 1\n", "    predict_y_gbm = np.where(predict_y_proba_gbm.reshape((predict_y_proba_gbm.shape[0])) > 0.5, 1, 0)\n", "\n", "    score_ll = metrics.log_loss(Y_valid, predict_y_proba_gbm)\n", "    score_ac = metrics.accuracy_score(Y_valid, predict_y_gbm)\n", "    score_pr = metrics.precision_score(Y_valid, predict_y_gbm)\n", "    score_re = metrics.recall_score(Y_valid, predict_y_gbm)\n", "    score = [score_ll, score_ac, score_pr, score_re]\n", "    \n", "    if (importance == True):\n", "        ax = lgb.plot_importance(gbm, max_num_features=20, figsize=(16, 5))\n", "        plt.show()\n", "    \n", "    gbmDF = pd.DataFrame([tuple(gbm.feature_importance())], columns= gbm.feature_name())\n", "    gbmDF.sort_index(axis=1, inplace=True)\n", "\n", "    return score, gbmDF, gbm"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "498d88997690cf4c20b0fbf63add624fe0f4f602", "_cell_guid": "f870a77c-3a01-428c-b386-7edbcd7172e4", "collapsed": true}}, {"source": ["# With KFolder stratified CV.\n", "def run_cross_validation_create_models(train_df, model_path, nfolds=4, break_fold=-1, importance=False):\n", "    train_data, train_target, train_id, train_data_columns = read_and_normalize_train_data(train_df)\n", "    train_target = train_target.ravel()\n", "    kf = StratifiedKFold(n_splits=nfolds, random_state=None, shuffle=True)\n", "    num_fold = 0\n", "    sum_score_ll = 0\n", "    sum_score_ac = 0\n", "    sum_score_pr = 0\n", "    sum_score_re = 0\n", "    scores = []\n", "    models = []\n", "    importanceDF = pd.DataFrame()\n", "    for train_index, test_index in kf.split(train_data, train_target):\n", "        num_fold += 1\n", "        print('\\n==> Start KFold number {} from {}'.format(num_fold, nfolds))\n", "        X_train = train_data[train_index]\n", "        Y_train = train_target[train_index]\n", "        X_valid = train_data[test_index]\n", "        Y_valid = train_target[test_index]\n", "        print(\"Train size: %s/%s\"%(str(X_train.shape), str(Y_train.shape)))\n", "        print(\"Valid size: %s/%s\"%(str(X_valid.shape), str(Y_valid.shape)))\n", "        score, impDF, m = fit_evaluate_model_lgbm(X_train,Y_train, X_valid, Y_valid, train_data_columns, \n", "                                   model_path, num_fold, importance=False)\n", "        models.append(m)\n", "        if len(importanceDF) == 0:\n", "            importanceDF = impDF\n", "        else:\n", "            importanceDF = pd.concat([importanceDF, impDF])\n", "        print('Test loss:', score[0])\n", "        print('Test accuracy:', score[1])\n", "        sum_score_ll += score[0]*len(test_index)\n", "        sum_score_ac += score[1]*len(test_index)\n", "        sum_score_pr += score[2]*len(test_index)\n", "        sum_score_re += score[3]*len(test_index)\n", "        scores.append(score)\n", "        # Break KFold loop\n", "        if (break_fold > 0) & (break_fold == num_fold):\n", "            break\n", "    score_ll = sum_score_ll/len(train_data)\n", "    score_ac = sum_score_ac/len(train_data)\n", "    score_pr = sum_score_pr/len(train_data)\n", "    score_re = sum_score_re/len(train_data)\n", "    print('\\nCV average scores:')\n", "    print('log_loss: %s\\naccuracy: %s\\nprecision: %s\\nrecall: %s\\n'%(score_ll, score_ac, score_pr, score_re))\n", "    return scores, importanceDF, models"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "471ddca17624f9bef38a66d9f6d32b2c519cd605", "_cell_guid": "caeca89e-d2cb-4c92-a9b8-e3f7b3d388dc", "collapsed": true}}, {"source": ["# Training\n", "NFOLDS = 10\n", "seed = 1337\n", "np.random.seed(seed)\n", "print(\"-----------   Seed %d   ----------------\"%seed)\n", "model_path = \"%s.cv%d.%d\"%(initial_model_path, NFOLDS, seed)\n", "os.makedirs(model_path, exist_ok=True)\n", "\n", "scores, importanceDF, models = run_cross_validation_create_models(train, model_path, nfolds=NFOLDS, break_fold=-1, importance = True)\n", "scores = np.array(scores)\n", "scores_loss = scores[:,0]\n", "scores_other = scores[:,1:4]\n", "\n", "box_loss = pd.DataFrame(scores_loss, columns=[\"log loss\"])\n", "box_other = pd.DataFrame(scores_other*100.0, columns=[\"accuracy\", \"precision\", \"recall\"])\n", "f, ax = plt.subplots(1, 2, figsize=(16,3))\n", "box_other.boxplot(ax=ax[0], showmeans=True)\n", "ax[0].set_title(\"Accuracy, Precision, Recall\")\n", "box_loss.boxplot(ax=ax[1], showmeans=True)\n", "ax[1].set_title(\"Log Loss\")\n", "plt.show()\n", "print(\"CV val Log loss: %s\"%(np.mean(scores_loss)))\n", "\n", "importanceDF.head()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "99a47e603618716f29745310501fa4b6bb7cb8cc", "_cell_guid": "99ac7b9c-00d4-454f-9dd3-231356c1c017", "collapsed": true, "scrolled": false}}, {"source": ["mgbmDF = pd.DataFrame(importanceDF.mean(), columns=[\"Importance\"])\n", "mgbmDF = mgbmDF.apply(lambda x: 100.0 * x / float(x.sum()))\n", "mgbmDF.sort_values(by=[\"Importance\"], ascending=[True]).plot(kind=\"barh\", legend=False, grid=True, figsize=(16,8))\n", "a = plt.title(\"Features Importance CV mean\")"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "8c4fc0a2b79e1c58809d2e24469fbeabe65628dd", "_cell_guid": "e9c9114b-089f-455c-9eeb-bb0751bd8eae", "collapsed": true}}, {"source": ["# Testing\n", "test = pd.read_json('../input/test.json')\n", "print(\"rows, cols = \" + str(test.shape))"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "41fb13b682290b268bdfda61ff337fcc251ddabc", "_cell_guid": "0bf42e56-7fa9-4b74-88d9-0345f454412a", "collapsed": true}}, {"source": ["def read_and_normalize_test_data(test_df):\n", "    featuresDF, labelsDF, idsDF = computeAdditionalFeatures(test_df.copy(deep=True))\n", "    test_features = featuresDF.as_matrix()\n", "    test_id = idsDF.as_matrix()\n", "    print(\"Features size: %s/%s\"%(str(test_features.shape), str(test_id.shape)))\n", "    return test_features, test_id, featuresDF[FEATURES].columns"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "8c06619c2d4aefbf879dbaa125b4d52d74572bd4", "_cell_guid": "36b3ce70-9eee-45ce-a48a-f27148c9185b", "collapsed": true}}, {"source": ["X_test, X_test_id, X_test_columns = read_and_normalize_test_data(test)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "c035096bc935568f9cefaffb0c3cd52c5c93788f", "_cell_guid": "a6d3b27f-e2c5-4022-8f63-37ca54eeff59", "collapsed": true}}, {"source": ["yfull_proba_train = []\n", "yfull_proba_test = []\n", "yfull_label_test = []\n", "num_fold = 0\n", "# Run predictions on each fold\n", "for model in models:\n", "    num_fold += 1\n", "    print('==> Start KFold number {} from {}'.format(num_fold, NFOLDS))\n", "    # Testing\n", "    predicted_test = model.predict(X_test, num_iteration=model.best_iteration) # Proba of class 1\n", "    predicted_test_label = np.where(predicted_test.reshape((predicted_test.shape[0])) > 0.5, 1, 0)\n", "    yfull_proba_test.append(predicted_test)\n", "    yfull_label_test.append(predicted_test_label)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "a553c171b883a4eb5c8a207121f4e61e944de961", "_cell_guid": "be3a967d-2c36-45e2-8498-ac80ed34cf84", "collapsed": true}}, {"source": ["def merge_several_folds_mean(data, nfolds):\n", "    a = np.array(data[0])\n", "    aPD = pd.DataFrame(a, columns = [\"%s_%d\"%(ICEBERG, 1)])\n", "    for i in range(1, nfolds):\n", "        a = a + np.array(data[i])\n", "        aPD[\"%s_%d\"%(ICEBERG, i+1)] = np.array(data[i])\n", "    a = a / nfolds*1.0\n", "    return a, aPD"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "4390fdfb5b71f57ddc162d1e7954e828b75c1279", "_cell_guid": "5bb29905-c111-4e9b-8bbf-d3e529b3b779", "collapsed": true}}, {"source": ["kfold_cols = [\"%s_%d\"%(ICEBERG, i) for i in range(1, NFOLDS + 1) ]\n", "predicted_test_mean, predicted_test_pd = merge_several_folds_mean(yfull_proba_test, NFOLDS)\n", "predicted_test_pd.head()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "ba55fab5b1ed8ad0e4cdcb9b3a28fd9a049c0947", "_cell_guid": "e5dcce8b-2889-4bf6-af94-bc6bc4ae3116", "collapsed": true}}, {"source": ["# Submission file\n", "submission = pd.DataFrame()\n", "submission[ID]=test[ID]\n", "submission[ICEBERG]=predicted_test_mean\n", "submission.to_csv(\"submissionlgbmv1.csv\", index=False, sep=\",\", decimal=\".\")"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "efdf26f08473c7cf6a61c5c4b9656e47b9765890", "_cell_guid": "a1c52c79-785c-4312-aba8-0121f8a516bd", "_kg_hide-output": false, "collapsed": true}}, {"source": [], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "f52786da0d5a8f60bddde80f5f05effac47997dd", "_cell_guid": "6c5b4c9b-3a7d-4b6d-adf0-5a01e73caaaa", "collapsed": true}}], "nbformat": 4, "metadata": {"language_info": {"version": "3.6.4", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1}