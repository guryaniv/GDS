{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "90d450a2-8a64-1a07-b44e-ced103ff4c17"
      },
      "source": [
        "In this notebook I will discuss how we can use logistic regression for fraud prediction. We will see that the dataset is highly imbalanced, and discuss how this can be dealt with. Please skip the first block, where I only define some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6096dda-2cb0-b928-b9dd-6bf0451f6d1e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "#    else:\n",
        "#        print('Confusion matrix, without normalization')\n",
        "\n",
        "#    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "def show_data(cm, print_res = 0):\n",
        "    tp = cm[1,1]\n",
        "    fn = cm[1,0]\n",
        "    fp = cm[0,1]\n",
        "    tn = cm[0,0]\n",
        "    if print_res == 1:\n",
        "        print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n",
        "        print('Recall (TPR) =  {:.3f}'.format(tp/(tp+fn)))\n",
        "        print('Fallout (FPR) = {:.3e}'.format(fp/(fp+tn)))\n",
        "    return tp/(tp+fp), tp/(tp+fn), fp/(fp+tn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8398bfe9-af40-70f5-9eeb-8a405c0b444e"
      },
      "source": [
        "Problem Analysis\n",
        "===\n",
        "We load and prepare the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "093c8124-2b89-4db0-7966-edff708a05ec"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../input/creditcard.csv\")\n",
        "print(df.head(3))\n",
        "y = np.array(df.Class.tolist())     #classes: 1..fraud, 0..no fraud\n",
        "df = df.drop('Class', 1)\n",
        "df = df.drop('Time', 1)     # optional\n",
        "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))    #optionally rescale non-normalized column\n",
        "X = np.array(df.as_matrix())   # features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "183161f8-48ca-c6d3-5e61-3a514e10538b"
      },
      "source": [
        "A class of 0 means that the transaction was in order, and a class of 1 means that the transaction was fraudulent. From personal experience we expect frauds to make up only a tiny fraction of all transactions. Indeed, in this dataset, for every fraud there are almost 600 non-fraudulent transactions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "019b0781-994b-0391-e517-7a3dd58a56f8"
      },
      "outputs": [],
      "source": [
        "print(\"Fraction of frauds: {:.5f}\".format(np.sum(y)/len(y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "540e9a85-abda-f9d6-7465-efe12e84a924"
      },
      "source": [
        "**Since the classes are highly imbalanced, we need to consider an appropriate measure for the quality of a fraud detection method.** If we would naively count the fraction of classes we got right (i.e. correctly predicted 0 or 1), already the simple model that *always* predicts 0 (non-fraud) would achieve an \"accuracy\" of 99.827%, since only frauds are not recognized correctly, and they are obviously rare. However, such a model is nonsense, since it cannot tell us whether a given transaction is a fraud or not. And *the extremely high accuracy is misleading*, and does not tell us anything about the actual quality of the prediction. Hence, we need a more sophisticated approach to evaluate a prediction. Let us illustrate this with a simple Logistic Regression (without parameter tuning, ..., yet)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1be1f11a-6634-47c7-6b9f-9e8cd2c9b7d8"
      },
      "source": [
        "We split the data into a training set and a test set. StratifiedKFold takes the ratio of fraud/non-fraud into account, which is important for such an imbalanced set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ef73f8c8-7506-6579-b5ff-c0af9a466d95"
      },
      "outputs": [],
      "source": [
        "lrn = LogisticRegression()\n",
        "\n",
        "skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, y_train = X[train_index], y[train_index]\n",
        "    X_test, y_test = X[test_index], y[test_index]\n",
        "    break\n",
        "\n",
        "lrn.fit(X_train, y_train)\n",
        "y_pred = lrn.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "if lrn.classes_[0] == 1:\n",
        "    cm = np.array([[cm[1,1], cm[1,0]], [cm[0,1], cm[0,0]]])\n",
        "\n",
        "plot_confusion_matrix(cm, ['0', '1'], )\n",
        "pr, tpr, fpr = show_data(cm, print_res = 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ecb04a75-f9e1-ba02-0863-308ea6942873"
      },
      "source": [
        "We have split the data into a training and a testing set at a ratio of 4:1, trained a Logistic Regression on the training set, and predicted the outcome on the testing set. The result of this prediction is depicted in the confusion matrix. We can see that almost all non-fraudulent transactions are also recognized as such. About 2/3 of all frauds are detected, but quite many are not recognized. The confusion matrix is a convenient way of illustrating the behavior of a classifier.\n",
        "\n",
        "- The **Precision** denotes the probability that a transaction that is classified as fraud is truly a fraud.\n",
        "- The **Recall** (aka. True Positive Rate) is the probability that a true fraud is recognized by the classifier.\n",
        "- The **Fallout** (aka. False Positive Rate) is the probability that a non-fraud is wrongly classified as a fraud.\n",
        "\n",
        "These numbers provide a more tangible characterization of a classifier. When tuning the parameters for a classifier, we need to ask ourselves what we expect of a \"good\" classifier.\n",
        "\n",
        "- The Precision should be large (close to 1). A precision close to 0 means that a fraud alert will turn out as a mistake in the majority of the cases. This can be annoying for customers.\n",
        "- The Recall should be close to 1. We want to detect frauds with a high probability. A recall of ~60% like for the classifier above is certainly not good enough - many frauds will go undetected. We definitly aim for something higher, 80% would be quite good! (Note that since frauds are often non-singular events on an account, it can be good enough to detect at least one out of two frauds - with a Recall of 80% this means that the probability that two consecutive frauds go undetected is as low as 4% already!)\n",
        "- The Fallout should be very low. Customers would not want to receive a fraud alert every week. I speculate that a Fallout less than 0.1% would be adequate - for an average daily number of 3 transactions this would result in 1 wrong alert per year. Here we will actually aim for the even better fallout <3e-4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "642465c7-c196-cdb0-d0d7-6776e4dd4f66"
      },
      "source": [
        "Parameter Tuning\n",
        "===\n",
        "The parameters I want to optimize are *C* in the logistic regression, the class weight, and the probability threshold: The logistic regression returns *probabilities* for the predicted classes, and the prediction is usually based on whether a probability is above a certain threshold or not. By changing the threshold, we can make our model more sensitive (but also increase wrong detections), or less strict (and miss more frauds).\n",
        "\n",
        "A canonical approach for our problem would be the ROC-curve, which plots the Recall vs. the Fallout for varying thresholds. Recall and Fallout are precisely the quantities we are interested of. It will not tell us which threshold is suitable, but it will help us find the optimal *C*. \n",
        "\n",
        "We will determine the ROC-curve from repeating the StratifiedKFold splitting several times, and taking the mean, in order to eliminate noise. The logistic regression can take imbalanced classes into account, using the option *class_weight*. The parameter *r* gives a weight to the underrepresented class 1: *r=1* gives class 1 a weight inversely proportional to its occurence, which is actually recommended. The bigger *r* is chosen, the less weight is given to class 1 (see the specification in *dic_weight* below).\n",
        "\n",
        "The following function creates the data for the ROC-curve through repeating StratifiedKFold:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "497ebc72-9d31-ec88-d5bb-7d751b3f442f"
      },
      "outputs": [],
      "source": [
        "def ROC(X, y, c, r):\n",
        "#makes cross_validation for given parameters c,r. Returns FPR, TPR (averaged)\n",
        "    dic_weight = {1:len(y)/(r*np.sum(y)), 0:len(y)/(len(y)-r*np.sum(y))} \n",
        "    lrn = LogisticRegression(penalty = 'l2', C = c, class_weight = dic_weight)\n",
        "    \n",
        "    N = 5      #how much k-fold\n",
        "    N_iter = 3    #repeat how often (taking the mean)\n",
        "    mean_tpr = 0.0\n",
        "    mean_thresh = 0.0\n",
        "    mean_fpr = np.linspace(0, 1, 50000)\n",
        "    \n",
        "\n",
        "    for it in range(N_iter):\n",
        "        skf = StratifiedKFold(n_splits = N, shuffle = True)\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            X_train, y_train = X[train_index], y[train_index]\n",
        "            X_test, y_test = X[test_index], y[test_index]\n",
        "         \n",
        "            lrn.fit(X_train, y_train)\n",
        "            y_prob = lrn.predict_proba(X_test)[:,lrn.classes_[1]]\n",
        "            \n",
        "            fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "            mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "            mean_thresh += np.interp(mean_fpr, fpr, thresholds)\n",
        "            mean_tpr[0] = 0.0\n",
        "\n",
        "    mean_tpr /= (N*N_iter)\n",
        "    mean_thresh /= (N*N_iter)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    return mean_fpr, mean_tpr, roc_auc_score(y_test, y_prob), mean_thresh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ddb36a64-161f-7713-7f16-3dd0e387d5f7"
      },
      "source": [
        "The following plots the ROC-curve for different parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6e58301a-c980-d637-6e3e-fb1cefaab037"
      },
      "outputs": [],
      "source": [
        "def plot_roc(X,y, list_par_1, par_1 = 'C', par_2 = 1):\n",
        "\n",
        "    f = plt.figure(figsize = (12,8));\n",
        "    for p in list_par_1:\n",
        "        if par_1 == 'C':\n",
        "            c = p\n",
        "            r = par_2\n",
        "        else:\n",
        "            r = p\n",
        "            c = par_2\n",
        "        list_FP, list_TP, AUC, mean_thresh = ROC(X, y, c, r)      \n",
        "        plt.plot(list_FP, list_TP, label = 'C = {}, r = {}, TPR(3e-4) = {:.4f}, AUC = {:.4f}'.format(c,r,list_TP[10],AUC));\n",
        "    plt.legend(title = 'values', loc='lower right')\n",
        "    plt.xlim(0, 0.001)   #we are only interested in small values of FPR\n",
        "    plt.ylim(0.5, 0.9)\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.title('ROC detail')\n",
        "    plt.axvline(3e-4, color='b', linestyle='dashed', linewidth=2)\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ec973915-71c7-1193-8e56-a46e5f5a7321"
      },
      "source": [
        "We will now perform a grid search, in order to find quasi-optimal parameters *C* and *r*. Say that we want a Fallout < 3e-4. Our goal is to maximize TPR for FPR = 3e-4 (see the dotted vertical line). Let us first look at the influence of the class weight parameter *r* (since only *r* = 1 correctly represents the class weights, we somehow expect to find *r* = 1 confirmed):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "28d5c64b-cafe-db7a-71c2-fd0942cbf85e"
      },
      "outputs": [],
      "source": [
        "plot_roc(X,y, [1,3,10,70], 'r', 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9b49e58d-222f-b092-8d17-50fb1a42c694"
      },
      "source": [
        "From the plots we find that for a Fallout less than 4e-4 and *C = 1* the model works best for small values of *r*. Since *r* = 1 is the natural choice, we will fix this value for now. Note that the AUC-score is not necessarily maximized: We are interested in steep ROC-curves for small FPR. Less steep curves might still produce higher AUC-scores.\n",
        "\n",
        "*Note: Due to the runtime limit I have taken a smaller N_iter and less values for the grid search. A better version can be found on my Github page.*\n",
        "\n",
        "We proceed now with *r* = 1, and search for a good choice of *C*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5b9afcc9-12d9-b199-b936-ba20a7851ac3"
      },
      "outputs": [],
      "source": [
        "plot_roc(X,y, [0.001, 0.01, 0.1, 1, 100], 'C', 1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3b0844ae-ebaa-9d65-48b5-40ffb9dbd226"
      },
      "source": [
        "Especially the very small *C* = 0.001 stands out with significantly poorer performance. For all larger values the model performs similarly well, and we pick *C* = 1 for our final model.\n",
        "\n",
        "Validation of the final model\n",
        "===\n",
        "\n",
        "As we will see soon, it is a probability threshold very close to 1 yields the desired result. I will now repeat the StratifiedKFold for the final model, and plot TPR and FPR as functions of the threshold, so that we can choose the right threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03612425-4ba0-4829-5eff-1f361629c36b"
      },
      "outputs": [],
      "source": [
        "N = np.arange(10,80,2)     # will define threshold\n",
        "cm = {}     #will store the confusion matrix for different thresholds\n",
        "for n in N:\n",
        "    cm[n] = 0.0\n",
        "lrn = LogisticRegression(penalty = 'l2', C = 1, class_weight = 'balanced')   #'balanced' corresponds to the case r=1\n",
        "N_Kfold = 5      #how much k-fold\n",
        "N_iter = 3   #repeat how often (taking the mean)\n",
        "for it in range(N_iter):\n",
        "    skf = StratifiedKFold(n_splits = N_Kfold, shuffle = True)\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, y_train = X[train_index], y[train_index]\n",
        "        X_test, y_test = X[test_index], y[test_index]\n",
        "        lrn.fit(X_train, y_train)\n",
        "        y_prob = lrn.predict_proba(X_test)[:,lrn.classes_[1]]\n",
        "        \n",
        "        for n in N:\n",
        "            thresh = 1 - np.power(10.,-(n/10))  #we want thresholds very close to 1\n",
        "            # generate the prediction from the probabilities y_prob:\n",
        "            y_pred = np.zeros(len(y_prob))\n",
        "            for j in range(len(y_prob)):\n",
        "                if y_prob[j] > thresh:\n",
        "                    y_pred[j] = 1\n",
        "    \n",
        "            B = confusion_matrix(y_test, y_pred)\n",
        "            #if the classes are mixed up, remedy that:\n",
        "            if lrn.classes_[0] == 1:\n",
        "                B = np.array([[B[1,1], B[1,0]], [B[0,1], B[0,0]]])\n",
        "            cm[n]+=B\n",
        "#finally, normalize the confusion matrices:\n",
        "for n in N:\n",
        "    cm[n] = cm[n]//(N_Kfold*N_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "32b7c2dc-e579-59c1-8b22-89e4a09a2152"
      },
      "source": [
        "We have now averaged confusion matrices for different thresholds, stored in *cm*. We extract TPR and FPR from the matrices, and plot them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d653510f-e15d-562b-10f9-42f0bc991a96"
      },
      "outputs": [],
      "source": [
        "PR = []      #precision\n",
        "TPR = []\n",
        "FPR = []\n",
        "THRESH = N\n",
        "for n in N:\n",
        "    pr, tpr, fpr = show_data(cm[n])\n",
        "    PR.append(pr)\n",
        "    TPR.append(tpr)\n",
        "    FPR.append(-np.log(fpr)/10)\n",
        "\n",
        "g  = plt.figure(figsize = (12,8))   \n",
        "plt.plot(THRESH, PR, label = 'Precision')\n",
        "plt.plot(THRESH, TPR, label = 'Recall (TPR)')\n",
        "plt.plot(THRESH, FPR, label = '-log(FPR)/10')\n",
        "plt.axhline(-np.log(3e-4)/10, color='b', linestyle='dashed', linewidth=2)\n",
        "plt.title('Evaluation of the classifier')\n",
        "plt.legend( loc='lower right')\n",
        "plt.xlabel('-log(1-thresh)/log(10)')\n",
        "plt.ylim(0.55,0.9)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3d409513-526b-5ae5-147b-63aaa9a29c1d"
      },
      "source": [
        "If we want a Fallout < 3e-4 we are only interested in data right of the intersection of the dotted line and the FPR-curve. We compute Recall, Fallout and Precision for the corresponding threshold:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e4395e6e-fbd7-4382-f95a-a4875d630edc"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "while FPR[i] < -np.log(3e-4)/10:\n",
        "    i+=1\n",
        "A = cm[THRESH[i]].astype(int)\n",
        "plot_confusion_matrix(A, ['0', '1'])\n",
        "show_data(A, print_res = 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a64529f2-c770-8951-fc38-629112018823"
      },
      "source": [
        "To some extent we can use this model for different thresholds, or FPR, respectively. Only for FPR > 4e-4 the above cross-validation would need to be redone. Let us check what happens when we relax the FPR-requirement:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eef5d2fd-27e2-a7df-afb0-bc1685f2da88"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "while FPR[i] < -np.log(4.1e-4)/10:\n",
        "    i+=1\n",
        "A = cm[THRESH[i]].astype(int)\n",
        "plot_confusion_matrix(A, ['0', '1'])\n",
        "show_data(A, print_res = 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e511dd29-b441-8135-972b-77f7326679d3"
      },
      "source": [
        "This is not really an improvement - in particular the Recall did not improve significantly, only the Fallout got worse. This is not desirable. Let us also look at a smaller FPR of 2e-4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "06dd01ef-ad93-847e-dcc8-8bbf484b0e20"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "while FPR[i] < -np.log(2e-4)/10:\n",
        "    i+=1\n",
        "A = cm[THRESH[i]].astype(int)\n",
        "plot_confusion_matrix(A, ['0', '1'])\n",
        "show_data(A, print_res = 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "842e7840-486d-9397-a0e1-719f32345d3f"
      },
      "source": [
        "In the end I think that that a fallout between 2e-4 and 3e-4 is reasonabe. The expected recall then lies between 75% and 80%. The final choice might depend on practical considerations, based on data not available here (e.g. what are the costs for the bank of a false alert vs a not detected fraud). Similarly to the approach taken here, one could also fix a Recall and then minimize the Fallout. However, the above analysis indicates that the ROC-curve flattens out significantly for FPR > 4e-4, and a Recall over 80% is extremely hard to achieve - and then only in combination with a drastic increase of the Fallout, for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "064d547b-c29a-4cad-0fbf-76ef4004675a"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "while FPR[i] < -np.log(2e-3)/10:\n",
        "    i+=1\n",
        "A = cm[THRESH[i]].astype(int)\n",
        "plot_confusion_matrix(A, ['0', '1'])\n",
        "show_data(A, print_res = 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4345bd65-a285-f072-c612-60922d58aef3"
      },
      "source": [
        "Conclusion\n",
        "===\n",
        "We have found that for a Fallout of 0.03% we can obtain a Recall of roughly 80%. Any significant improvement of the Recall comes with very high cost for the Fallout. Similarly, any substantial improvement of the Fallout will drastically reduce the Fallout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aca46e06-aa50-3359-28df-6aee5fd06e59"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "while FPR[i] < -np.log(3e-4)/10:\n",
        "    i+=1\n",
        "A = cm[THRESH[i]].astype(int)\n",
        "plot_confusion_matrix(A, ['0', '1'])\n",
        "show_data(A, print_res = 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c29201d1-c13d-f020-1c61-d7eb01bc88b3"
      },
      "source": [
        "In my next notebook I want to compare these results with others obtained with XGBoost."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}