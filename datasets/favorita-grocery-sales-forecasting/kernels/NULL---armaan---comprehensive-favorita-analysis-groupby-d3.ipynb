{"cells": [{"metadata": {"_uuid": "b36e6423a34c79b224df04d497a36850c24ee3a2", "_cell_guid": "759c2537-ec1f-4f5a-96d5-7292a3ca168c"}, "source": ["# Introduction\n", "\n", "This competition is hosted by a large grocery company in Ecuador called \"Corporacion Favorita\" where the aim of the game is to accurately predict and forecast the unit sales for items sold at various Favorita supermarket chains across Ecuador. Apart from the usual training and test data files provided, there are also quite a handful of other supplementary data files (5 extra files to be exact) provided to us. \n", "\n", "This notebook aims to take a deep-dive analysis into each of the files provided in this competition and to investigate what types of insights or observations can be derived from each. The structure of this analysis is as follows:\n", "\n", "1. Data loading and inspection \n", "2. Supplementary Data exploration \n", "3. Training data exploration \n", "4. Feature ranking with learning models"], "cell_type": "markdown"}, {"metadata": {"_uuid": "a5c67cb421d1deace8e5528e19deff588d3eff7b", "_cell_guid": "5c52ebc6-3d76-4daa-8f30-22a5e5dae4fa"}, "outputs": [], "source": ["# Importing the relevant libraries\n", "import pandas as pd\n", "import seaborn as sns\n", "%matplotlib inline\n", "import missingno as msno\n", "import plotly.offline as py\n", "py.init_notebook_mode(connected=True)\n", "import plotly.graph_objs as go\n", "import plotly.tools as tls\n", "import numpy as np\n", "from scipy.fftpack import fft\n", "from matplotlib import pyplot as plt"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "19f6a670b6b44aefdb6882a550a45c439d23d41e", "_cell_guid": "8c8e2fae-bfb5-4447-b6f0-5e70e5c92c9a"}, "source": ["# 1. Data loading and inspection checks\n", "\n", "To start off with, let us load in the various supplementary comma-separated value files with the Pandas package via the the read_csv function as follows. Borrowing from Inversion's very helpful kernel in creating a dataframe with all [Date-Store_Item combinations](https://www.kaggle.com/inversion/dataframe-with-all-date-store-item-combinations) (please do check it out ), I will load in the training data using some of his methods."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "8f5e396190d05ba9a2cd021c480b2b63d89185a8", "_cell_guid": "8d831841-c0a1-4b1d-9fa5-3b309d852957"}, "outputs": [], "source": ["items = pd.read_csv(\"../input/items.csv\")\n", "holiday_events = pd.read_csv(\"../input/holidays_events.csv\")\n", "stores = pd.read_csv(\"../input/stores.csv\")\n", "oil = pd.read_csv(\"../input/oil.csv\")\n", "transactions = pd.read_csv(\"../input/transactions.csv\",parse_dates=['date'])\n", "# I read in the full training data just to get prior information and here is the output:\n", "# Output: \"125,497,040 rows | 6 columns\"\n", "train = pd.read_csv(\"../input/train.csv\", nrows=6000000)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "87d82898e50b545f48f0e807686cc6d317428857", "_cell_guid": "136c24ba-a573-4c17-ad4b-8122ad98d66c"}, "source": ["With regards to the training data, it contains a whooping 125,497,040 rows (and 6 columns). For  Therefore I will just load in 6 million rows of the training data (approx 5% of the data) just to get a rough idea of what is in store for us.\n", "Let us take a quick peek into the it to see what kind of datatypes and columns are there."], "cell_type": "markdown"}, {"metadata": {"_uuid": "24f0ede2d7060bc6f19adc993efee1a7b0cafbd7", "_cell_guid": "15a0cb65-fcf4-4381-ad5f-4a42a28a9507"}, "outputs": [], "source": ["train.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "7ac3181b2087464ee187f39c4d71b8c614cfaeb1", "_cell_guid": "ed029374-989b-4069-b772-5f19d5790074"}, "source": ["Now, note that the training data only consists of 6 rather measly columns and therefore coupled with the fact that we have approx 125 million rows, therefore there does seem to be a discrepancy in the number of features that we are going to provide our learning model to train on. However that's where the other supplementary files comes in to play as we will most definitely have to join the fields \"store_nbr\" (store number) and \"item_nbr\" as well as dates ( bring in daily oil prices). So there is actually quite a lot of potential and avenues for feature enhancement and engineering."], "cell_type": "markdown"}, {"metadata": {"_uuid": "78641a33978cbfee6b4835b6b44f1944d31e5576", "_cell_guid": "a3f35557-47e6-45a1-9502-b7a3fb3979de"}, "source": ["### NULL or missing values check\n", "\n", "One standard check I like to carry out is to simply inspect all our data for any Null or missing values. If there are any, then we might have to think of strategies to handle them (eg. Imputation, removal of nulls etc). A good library to conveniently visualise missing values is via the \"missingno\" package as an aside. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "d1aea94631f9fe7393bec92a100b82954ea951af", "_cell_guid": "c9dfc8fa-55e6-4b2e-8420-ef6fe25b8c6b"}, "outputs": [], "source": ["print(\"Nulls in Oil columns: {0} => {1}\".format(oil.columns.values,oil.isnull().any().values))\n", "print(\"=\"*70)\n", "print(\"Nulls in holiday_events columns: {0} => {1}\".format(holiday_events.columns.values,holiday_events.isnull().any().values))\n", "print(\"=\"*70)\n", "print(\"Nulls in stores columns: {0} => {1}\".format(stores.columns.values,stores.isnull().any().values))\n", "print(\"=\"*70)\n", "print(\"Nulls in transactions columns: {0} => {1}\".format(transactions.columns.values,transactions.isnull().any().values))"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "45893f31594adcdfb68b4858cf3ebee7613f2988", "_cell_guid": "0d1b39c1-f02d-40ce-941f-9a3395a47cf5"}, "source": ["As we can see,  the only missing data occurs in the oil data file, which provides the historical daily price for oil. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "99e917d88a435cefffb96c342d5ab1c0683bb9a8", "_cell_guid": "eedc6081-ce78-45a3-a959-d22771cdf50f"}, "source": ["# 2. Supplementary Data Exploration"], "cell_type": "markdown"}, {"metadata": {"_uuid": "42233b22af155c7ee92e385ef3793f76a56ac0c0", "_cell_guid": "9a41b9ff-cec6-405c-a642-5321136fa198"}, "source": ["## 2a. Oil data\n", "\n", "First up we can take a look at the \"oil.csv\" provided to us. As alluded to in section 1, this file contains daily oil prices within a time range that covers both the train and test data timeframe so this is something to note should one's learning model take into account the trend in these oil prices. This supplementary oil data seems to be a very simple two column table with one column being the date and the other the daily oil price \"dcoilwtico\" which seems to be the abbreviation for [Crude oil prices: West Texas Intermediate - Cushing, Oklahoma](https://fred.stlouisfed.org/series/DCOILWTICO). \n", "\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "4c9c78274b22324a8b7bf6fd6d206f8cea2c8f3f", "_cell_guid": "35975467-e5f9-4129-abbe-08fa8fc87cd6"}, "outputs": [], "source": ["oil.isnull().any(axis=0).values"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "bea836a2ba4002d3d5d07a9dc0c6155c7c72ff7f", "_cell_guid": "b90972a5-40eb-40bd-bad6-43f5f07f44af"}, "source": ["**Interactive Visualisations with Plotly**\n", "\n", "Let us take a look at the underlying data by plotting the daily oil prices in a time series plot via the interactive Python visualisation library Plot.ly as follows. Here we invoke the Plot.ly scatter plot function by calling \"Scatter\" and it is a simple matter of providing the date range in the x-axis and the corresponding daily oil prices in the y-axis. Here I have also simultaneously dropped nulls by calling dropna( ) in the oil dataframe."], "cell_type": "markdown"}, {"metadata": {"_uuid": "b08346770a50e556c799c392ef603a0b3108aa6c", "_cell_guid": "9c1de4fa-3ee4-4d8d-92cd-32468b84eb54"}, "outputs": [], "source": ["trace = go.Scatter(\n", "    name='Oil prices',\n", "    x=oil['date'],\n", "    y=oil['dcoilwtico'].dropna(),\n", "    mode='lines',\n", "    line=dict(color='rgb(220, 150, 0, 0.8)'),\n", "    #fillcolor='rgba(68, 68, 68, 0.3)',\n", "    fillcolor='rgba(230, 200, 6, 0.3)',\n", "    fill='tonexty' )\n", "\n", "data = [trace]\n", "\n", "layout = go.Layout(\n", "    yaxis=dict(title='Daily Oil price'),\n", "    title='Daily oil prices from Jan 2013 till July 2017',\n", "    showlegend = False)\n", "fig = go.Figure(data=data, layout=layout)\n", "\n", "py.iplot(fig, filename='pandas-time-series-error-bars')"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "fba0982a687749b7558101d3a10aad0c44d3bed6", "_cell_guid": "84f6d13c-bbb2-41df-ab19-0d16840fdb24"}, "source": ["#### *[THE ABOVE PLOT IS INTERACTIVE SO YOU CAN DRAG AND ZOOM ON IT. DOUBLE-CLICK TO GET BACK TO THE ORIGINAL SIZE]*"], "cell_type": "markdown"}, {"metadata": {"_uuid": "91de04350832765db069a1b7db60d9ddf90f8ef5", "_cell_guid": "082fb224-e62a-4561-b6b8-f2b6ffcd4a05"}, "source": ["**Takeaway from the plots**\n", "\n", "This plot shows that the daily oil price is on a general downward trend from Jan 2013 till July 2017. Where the price of oil started out 2013 by increasing and even busting the 100 dollar mark for a good few months in 2013 and 2014, it reached the middle of 2014 where there was a drastic drop in the price of oil. Via some quick open-source research (i.e Googling), this trend checks out as it seems oil prices were kept fairly stable from 2010 till mid-2014 after which it drastically fell ( due to a confluence of reasons such as weak demand due to poor economic growth and surging alternative sources of crude oil from shale/tar sands).\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "4089fa8e74806befd03ee90257c9a76e32d23c7f", "_cell_guid": "d32bc964-fd62-4a4e-b78c-cc977b696c49"}, "source": ["## 2b. Stores data\n", "\n", "With regards to the \"stores.csv\" file, the data dictionary on the Kaggle competition simply states that it contains metadata on the city, state, the store type and a column termed \"cluster\". Now this cluster column is a grouping to stores that are similar to each other and as we can see from the latter analysis, there are a total of 17 distinct clusters. With regards to the number of stores, there are a total of 54 stores (based off a unique list of store_nbr) and therefore I presume that all the unit sales and transactions are generated off the data collected from these 54 stores."], "cell_type": "markdown"}, {"metadata": {"_uuid": "0e8a36efec8344a8dfe445dc43ac0f7a42ab5a57", "_cell_guid": "d5695ece-8ddc-4aba-8742-de5680816579"}, "outputs": [], "source": ["stores.head(3)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "0192875aea6d21a9d020bd9fe8f108f660e626b8", "_cell_guid": "0bb70686-5039-4f55-89a2-c1a769d7d5d5"}, "source": ["**Inspecting the allocation of clusters to store numbers**\n", "\n", "The first plot that we are going to generate will be that of our store numbers ordered against their respective store clusters so that we can observe if there are any apparent trends or relationships in the data. To do so, I will take our stores Python dataframe and group it based on the columns \"store_nbr\" and \"cluster\" via the **groupby** statement. After which, I will unstack the grouping which means that I will pivot on the level of store_nbr index labels, returning a DataFrame having a new level of columns which are the store clusters whose inner-most level relate to the pivoted store_nbr index labels. This technique is commonly used for producing stacked barplots in Python but since we only have unique store_nbr numbers, therefore we will simply get barplots of store numbers ordered by their relevant clusters."], "cell_type": "markdown"}, {"metadata": {"_kg_hide-output": true, "collapsed": true, "_uuid": "9642ff7a392d7166b9fc592559d7d98a66f4c0ca", "_kg_hide-input": true, "_cell_guid": "805de539-02f9-47ec-8ba9-9fb9ddfaf642"}, "outputs": [], "source": ["# Unhide to see the sorted zip order\n", "neworder = [23, 24, 26, 36, 41, 15, 29, 31, 32, 34, 39, 53, 4, 37, 40, 43, 8, 10, 19, 20, 33, 38, 13, 21, 2, 6, 7, 3, 22, 25, 27, 28, 30, 35, 42, 44, 48, 51, 16, 0, 1, 5, 52, 45, 46, 47, 49, 9, 11, 12, 14, 18, 17, 50]"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "cf8a64ac6ab140ce63e183b7707e7362d8f6b504", "_cell_guid": "a374b77d-208a-4d8b-9fab-068cb5be3d70"}, "outputs": [], "source": ["nbr_cluster = stores.groupby(['store_nbr','cluster']).size()\n", "nbr_cluster.unstack().iloc[neworder].plot(kind='bar',stacked=True, colormap= 'tab20', figsize=(13,11),  grid=False)\n", "plt.title('Store numbers and the clusters they are assigned to')\n", "plt.ylabel('')\n", "plt.xlabel('Store number')\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "6c3febc4c4941f72ec81540c31596da89ca97129", "_cell_guid": "63a2e275-57cd-458b-8e9e-f116d06f801d"}, "source": ["**Takeaways from thse plot**\n", "\n", "From visualising the store numbers side-by-side based on the clustering, we can identify certain patterns. For example clusters 3, 6, 10 and 15 are the most common store clusters based off the fact that there are more store_nbrs attributed to them then the others. We can also identify outlier stores based on their assignmen \n", "\n", "**Stacked Barplots of Types against clusters**\n", "\n", "Here it might be informative to look at the distribution of clusters based on the store type to see if we can identify any apparent relationship between types and the way the company has decided to cluster the particular store. Again we apply the groupby operation but this time on type and on cluster. This time when we pivot based off this grouped operation, we are able to get counts of each distinct cluster distributed and stacked on top of other clusters per store type as follows:"], "cell_type": "markdown"}, {"metadata": {"_uuid": "576078b42faffa4c5126ae749b026004bd923f2d", "_cell_guid": "f188388c-d818-4934-afb2-220a5ae9a145"}, "outputs": [], "source": ["type_cluster = stores.groupby(['type','cluster']).size()\n", "type_cluster.unstack().plot(kind='bar',stacked=True, colormap= 'viridis_r', figsize=(13,11),  grid=False)\n", "plt.title('Stacked Barplot of Store types and their cluster distribution')\n", "plt.ylabel('Count of clusters in a particular store type')\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "47c816d230159469b67e948d6d06a25e55436019", "_cell_guid": "c0c61627-81aa-4ebe-9efe-734a279d7a5d"}, "source": ["**Takeaway from the plots**\n", "\n", "Most of the store types seem to contain a mix of the clusters from both"], "cell_type": "markdown"}, {"metadata": {"_uuid": "ee5413119e88d368ea8b9638b66932f9ab353afe", "_cell_guid": "5aeaa9cb-6937-44f6-9424-2a936eec90fd"}, "source": ["**Stacked barplot of types of stores across the different cities**\n", "\n", "Another interesting distribution to observe would be the types of stores that Corporacion Favorita has decided to open in each city in Ecuador. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "dec0629f469dde60d4e01437d0d284c8d6fd0dbe", "_cell_guid": "85191c9e-44cb-43f0-a841-157aa3f00f06"}, "outputs": [], "source": ["city_cluster = stores.groupby(['city','type']).size()\n", "city_cluster.unstack().plot(kind='bar',stacked=True, colormap= 'PuBu', figsize=(13,11),  grid=False)\n", "plt.title('Stacked Barplot of Store types distributed across cities')\n", "plt.ylabel('Count of stores in a particular city')\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "5a4421f1162f5e3228aef2dc36206396f55e1830", "_cell_guid": "10406822-b05c-4859-9dc3-55e47b3b3ff4"}, "source": ["**Takeaways from the plot**: \n", "\n", "As observed from the stacked barplots, there are two cities that standout in terms of the variety and store types that they offer - Guayaquil and Quito. These should come as no surprise as [Quito](https://en.wikipedia.org/wiki/Quito) is the capital city of Ecuador while [Guayaquil](https://en.wikipedia.org/wiki/Guayaquil) is the largest and most populous city. Therefore one would think it logical to expect Corporacion Favorita to target the major cities with the most diverse store types as well as setting up more stores evinced from the numerous store_nbrs attributed to those two cities."], "cell_type": "markdown"}, {"metadata": {"_uuid": "de3edff9d78f6ab0288447420facff18cb4c3b0b", "_cell_guid": "89fe7bab-068b-400a-9bc9-38ce324bf111"}, "source": ["## 2c. Holiday Events data\n", "\n", "Trudging on, we can inspect the \"holiday_events.csv\" "], "cell_type": "markdown"}, {"metadata": {"_uuid": "f3e8d2711c40e33fcb17e00b33fef1cfad73c5e4", "_cell_guid": "fbcb9370-f682-43fc-a0ac-624b6de630b1"}, "outputs": [], "source": ["holiday_events.head(3)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "58b91bcdf53c45bbbb0e61b6fad3cce7459aa3c9", "_cell_guid": "e03a2932-cdd2-4eea-a1b0-0d46e93cccbb"}, "outputs": [], "source": ["holiday_local_type = holiday_events.groupby(['locale_name', 'type']).size()\n", "holiday_local_type.unstack().plot(kind='bar',stacked=True, colormap= 'inferno', figsize=(12,10),  grid=False)\n", "plt.title('Stacked Barplot of locale name against event type')\n", "plt.ylabel('Count of entries')\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "410644cd98ec8007481a84c894e7952d4e038268", "_cell_guid": "419467fc-5ee4-4cc0-88fb-371321366104"}, "outputs": [], "source": ["x = holiday_events.groupby(['type', 'description']).size()\n", "x.unstack().plot(kind='bar',stacked=True, colormap= 'inferno', figsize=(12,10),  grid=False)\n", "plt.title('Stacked Barplot of locale name against event type')\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "1ac9c30bb9c2824871f3f58125e3c9f32f92c04f", "_cell_guid": "3874bf7e-23f5-4d9b-9041-c2057ea1fc1c"}, "source": ["## 2d. Transactions data\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "0d4857a7715e4b5ab9d3f113b6d4ebfe90823382", "_cell_guid": "8e60cf1f-1d2b-477c-aa13-2ee0cee17897"}, "source": ["\n", "**End-of-Year PERIODICITY IN TRANSACTION PATTERN**"], "cell_type": "markdown"}, {"metadata": {"_uuid": "55407ca5bfee024a9fc66a82fb4e6ec789e4ea7a", "_cell_guid": "d81126d5-53a9-4e06-9009-3876d11562f2"}, "outputs": [], "source": ["print(transactions.head(3))\n", "print(\"=\"*60)\n", "print(transactions.shape)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "88e916f4a58cc4c28ce56d6ddd2ce0406c32c970", "_cell_guid": "b8b228bb-13e5-491e-89fb-12ddffbc46ef"}, "outputs": [], "source": ["transactions.iloc[33700]"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "0140db838832b5c6ac5c43cec5e9ae195ad8483e", "_cell_guid": "357aea86-5f53-4055-a4fc-daca14e509be"}, "outputs": [], "source": ["plt.figure(figsize=(13,11))\n", "plt.plot(transactions.date.values, transactions.transactions.values)\n", "plt.axvline(x='2015-12-23',color='red',alpha=0.2)\n", "plt.axvline(x='2016-12-23',color='red',alpha=0.2)\n", "plt.axvline(x='2014-12-23',color='red',alpha=0.2)\n", "plt.axvline(x='2013-12-23',color='red',alpha=0.2)\n", "plt.ylim(-50, 10000)\n", "plt.ylabel('transactions per day')\n", "plt.xlabel('Date')\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "9964dd0a8935f87670c60298e310bb769385e92b", "_cell_guid": "d90286a5-acfd-4c11-9753-b539149ea745"}, "outputs": [], "source": ["transactions.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "228d48b321425cde9803c66f28016bc9a895b928", "_cell_guid": "9bdb1afd-c4b8-4aa9-bd38-72f3c2371ca2"}, "source": ["## 2e. Items data"], "cell_type": "markdown"}, {"metadata": {"_uuid": "cbe341a609ce30874c347001dade421872f9fa9d", "_cell_guid": "10acf2f4-56a8-496a-ae6e-1c21e372c78d"}, "outputs": [], "source": ["items.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "aa4196277ec16070ad7be3cfe5bf3ab8c12debc0", "_cell_guid": "88efa69d-9120-4a9c-8e84-5b01575033ad"}, "source": ["# 3. Training Data exploration"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "c3e206e37456d3eef90c7010b143c4ff423e649a", "_cell_guid": "f23c42bd-5552-43c5-ae4a-18c97b88e96b"}, "outputs": [], "source": ["import sklearn\n", "from sklearn import linear_model\n", "from sklearn import model_selection\n", "ridge = linear_model.Ridge()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "c7bf37336c49340b5e39a4d78b96301242c686af", "_cell_guid": "ce237876-d491-4c20-b0b8-57b0031a1e6b"}, "outputs": [], "source": ["data = train\n", "(train, test) = model_selection.train_test_split(data, train_size=0.75)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "b863c9ae1597b69a9c444086d51a5757221f5804", "_cell_guid": "a8166524-d023-4f05-aeae-d7094034a3dc"}, "outputs": [], "source": ["ridge.fit(train[['store_nbr','item_nbr']], train['unit_sales'])"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "8cbee68cad8a5dda2304ab069c6fcccffaa5b1d2", "_cell_guid": "dbeb04de-54e2-4635-a8fb-dbf551875000"}, "outputs": [], "source": ["print(ridge.score(train[['store_nbr','item_nbr']], train['unit_sales']))\n", "print(ridge.score(test[['store_nbr','item_nbr']], test['unit_sales']))"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "8b5df3addd0938c7ffe7c4a1972e2150be782b48", "_cell_guid": "ccfab081-0c05-4b6e-a0a5-86a18baed86c"}, "outputs": [], "source": ["test_data = pd.read_csv('../input/test.csv')\n", "test_data.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "3a82c77bbc88bff7badfa14f47a763fc72cc6417", "_cell_guid": "2a8e4ea4-b654-4af8-bc60-a955c2586f43"}, "outputs": [], "source": ["sample_submission = pd.read_csv('../input/sample_submission.csv')\n", "sample_submission.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "0fd8afe16c327844779160bf862e035820ebd46a", "_cell_guid": "f6ae028d-9dae-4e08-94ba-4b2f51c0c5c9"}, "outputs": [], "source": ["predictions = ridge.predict(test_data[['store_nbr','item_nbr']])\n", "print(predictions)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "402b07755169173b19942b689d410cd7c7061ce3", "_cell_guid": "81432dac-204f-4366-85ca-ca69f0a16712"}, "outputs": [], "source": ["sample_submission['unit_sales'] = predictions"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "ec01a7edd18f969dfd28788cfff59132d4e1b879", "_cell_guid": "4edc75ca-5321-416a-8b34-c2289b65791b"}, "outputs": [], "source": ["sample_submission.to_csv('submission11.csv', index=False)"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["sub = pd.read_csv('../output/sumbmission1.csv')\n", "sub.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["%ls"], "execution_count": null, "cell_type": "code"}], "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3", "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1, "nbformat": 4}