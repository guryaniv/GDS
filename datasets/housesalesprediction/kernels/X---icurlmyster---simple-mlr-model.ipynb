{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2515b9e6-7a3e-507f-52d6-5758a04bc982"
      },
      "source": [
        "In this notebook we will be exploring the King County, USA Housing data set and seeing how well we can get a simple multiple linear regression model to predict the housing prices.  To tackle this problem we will be using the tensorflow, pandas, and numpy libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "646c07e7-732e-2b49-85dc-81b3334454ac"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "74034aee-7f85-2d66-f06a-c92fe0d68eaa"
      },
      "source": [
        "Next we set our seeds and read in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "52e6c2cf-7ab2-9e28-f2f6-0f2f34d8a6ec"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7)\n",
        "tf.set_random_seed(7)\n",
        "init_data = pd.read_csv(\"../input/kc_house_data.csv\")\n",
        "print(\"Col: {0}\".format(list(init_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "be073e4e-17a1-7ebc-7f9f-b86fc26b9e89"
      },
      "source": [
        "Lets look at the data info to see what data types each of these columns are and see if there is any data missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3d178061-f4b0-987d-3223-daae95c3970a"
      },
      "outputs": [],
      "source": [
        "print(init_data.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d50908c8-d877-b5e3-474e-700f173854cd"
      },
      "source": [
        "We can see that all of the data types are numeric except for \"date\". We can also tell that each column has 21613 rows and none of the columns have missing data. So now we need to get rid of \"id\", because this is useless, and \"date\", because we just want to deal with numeric values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77bdf333-83f2-aaa6-e253-97c4755dd71d"
      },
      "outputs": [],
      "source": [
        "init_data = init_data.drop(\"id\", axis=1)\n",
        "init_data = init_data.drop(\"date\", axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "24a27e09-b074-c47e-39d5-a9b29be2f3bc"
      },
      "source": [
        "Now lets take a look at the how the rest of the data correlates with the \"price\" column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "45dab002-3908-288c-8cd1-9309a10f06f6"
      },
      "outputs": [],
      "source": [
        "matrix_corr = init_data.corr()\n",
        "print(matrix_corr[\"price\"].sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c72957d3-8c33-ea42-2151-6ee90532f2ba"
      },
      "source": [
        "As we can see we have quite a few strong correlated features, with \"zipcode\" being the only negative correlating feature. We might want to experiment with seeing which features are the best to remove but for right now we will just remove \"zipcode\" and test the rest of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7113ec34-5cdb-dc0b-d4b7-148889982aa6"
      },
      "outputs": [],
      "source": [
        "init_data = init_data.drop(\"zipcode\", axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9a1c38dd-6bdc-4384-dda1-f725808d7ccd"
      },
      "source": [
        "Lets graphically compare \"price\" to the top four correlated features to see if we can spot any trends or anomalies with the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "17da478d-a296-849f-4289-782c1979baeb"
      },
      "outputs": [],
      "source": [
        "from pandas.tools.plotting import scatter_matrix\n",
        "%matplotlib inline\n",
        "## attributes to look at\n",
        "attr = [\"price\", \"sqft_living\", \"grade\", \"sqft_above\", \"sqft_living15\"]\n",
        "scatter_matrix(init_data[attr], figsize=(20,8) );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9b8d8b95-bee7-a25d-48ba-205b76f3475a"
      },
      "source": [
        "As we can see, it seems there are some possible outliers to worry about. So what we will do is standardize the data when we create our model so that we won't have to worry about this possible problem as much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "affcb6cc-0625-fdb0-53ca-a6839d04a0fa"
      },
      "source": [
        "Now lets define a split function to permutate our data and split it into a training and test set that we can work with. We will have 80% as training data and 20% as test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d96c9964-8e25-375c-cab9-a428b194d8a2"
      },
      "outputs": [],
      "source": [
        "def split_data(data, ratio):\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data) * ratio)\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    train_indices = shuffled_indices[test_set_size:]\n",
        "    return data.iloc[train_indices], data.iloc[test_indices]\n",
        "\n",
        "train_set, test_set = split_data(init_data, 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4320fc78-c440-fa35-3c1a-2e6a1e9861a8"
      },
      "source": [
        "We need to grab the mean and standard deviation of the training set to standardize our input data for the model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69679bcb-73c9-2241-e8fd-b04c692e26f9"
      },
      "outputs": [],
      "source": [
        "mean_offset = train_set.mean()\n",
        "stddev_offset = train_set.std()\n",
        "mean_xOffset = (mean_offset.drop(\"price\")).values\n",
        "stddev_xOffset = (stddev_offset.drop(\"price\")).values\n",
        "mean_yOffset = np.array([mean_offset[\"price\"].copy()])\n",
        "stddev_yOffset = np.array([stddev_offset[\"price\"].copy()])\n",
        "\n",
        "mean_xOffset = mean_xOffset.reshape(1, mean_xOffset.shape[0])\n",
        "stddev_xOffset = stddev_xOffset.reshape(1, stddev_xOffset.shape[0])\n",
        "mean_yOffset = mean_yOffset.reshape(1, mean_yOffset.shape[0])\n",
        "stddev_yOffset = stddev_yOffset.reshape(1, stddev_yOffset.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "71d4926f-df4b-4a69-b0fb-e8807d617f7d"
      },
      "source": [
        "We need to split our training set into the data and its labels for our model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c1ead08-0a27-42fc-2a68-5a872d40a210"
      },
      "outputs": [],
      "source": [
        "data = (train_set.drop(\"price\", axis=1)).values\n",
        "data_labels = (train_set[\"price\"].copy()).values\n",
        "data_labels = data_labels.reshape([len(data_labels),1]) # forcing a [None, 1] shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ad3109ef-9a50-fa2c-31b2-b66f5c75740f"
      },
      "source": [
        "Next lets pull out some info we need to help set up our model, like the number of features we are using and the number of samples that we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "59718d6d-31b1-bff0-d1be-c8f3caac7b5e"
      },
      "outputs": [],
      "source": [
        "num_features = data.shape[1]\n",
        "n_samples = data.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9061b83a-6570-b726-0d1c-e840101da76a"
      },
      "source": [
        "Now its time to implement our model. We will define our inputs along with the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "57833380-09bf-b026-3206-f54b594ceed4"
      },
      "outputs": [],
      "source": [
        "X_init = tf.placeholder(tf.float32, [None, num_features])\n",
        "Y_init = tf.placeholder(tf.float32, [None, 1])\n",
        "W = tf.Variable(tf.random_normal([num_features,1]))\n",
        "b = tf.Variable(tf.random_normal([1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f5e2e166-a904-c89d-6837-278901a787bf"
      },
      "source": [
        "Next what we are going to do is standardize our input data so that any possible outliers in the data won't be as big of a problem. Plus this makes the size of the values smaller and easier to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "939b686e-605f-c5c7-8ed9-26da8c73ca12"
      },
      "outputs": [],
      "source": [
        "##  Grab the mean and stddev values we took from the training set earlier\n",
        "x_mean = tf.constant(mean_xOffset, dtype=tf.float32)\n",
        "y_mean = tf.constant(mean_yOffset, dtype=tf.float32)\n",
        "\n",
        "x_stddev = tf.constant(stddev_xOffset, dtype=tf.float32)\n",
        "y_stddev = tf.constant(stddev_yOffset, dtype=tf.float32)\n",
        "\n",
        "## Making the input have a mean of 0 and a stddev of 1\n",
        "X = tf.div(tf.subtract(X_init, x_mean), x_stddev)\n",
        "Y = tf.div(tf.subtract(Y_init, y_mean), y_stddev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ebc117d6-1dc2-962d-eac2-388dffaf247d"
      },
      "source": [
        "With all of that taken care of we can define the prediction function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6ec4bb7-4f4b-c1ad-2855-c8236778008b"
      },
      "outputs": [],
      "source": [
        "pred = tf.add(tf.matmul(X,W), b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e3fb1ffd-c0e5-b95c-5e31-97e0c39c1ae0"
      },
      "source": [
        "To calculate the loss we are going the use the Mean Squared Error(MSE) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c29b8d24-4a60-dba0-5cb3-4c129abfdcae"
      },
      "outputs": [],
      "source": [
        "pow_val = tf.pow(tf.subtract(pred, Y),2)\n",
        "cost = tf.reduce_mean(pow_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2f0d6a3f-15b2-7c79-dbc0-58957ec92f73"
      },
      "source": [
        "Normally we would use the gradient descent optimizer to minimize our cost function but the Adam optimizer shows better results. We will also set its learning rate to 0.1, which is kind of high but in this case it converges decently in a short amount of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b27978d7-b36f-e367-e49c-3a04e02229eb"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.train.AdamOptimizer(1e-1).minimize(cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "16550abd-2651-e074-430f-2b66bb860f99"
      },
      "source": [
        "Now lets initalize our variables and start our tensorflow session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b27ebfcd-ca27-da50-a1da-52e0bb509dd2"
      },
      "outputs": [],
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1286d0a3-7f9b-af49-8d38-0c547de220e3"
      },
      "source": [
        "We are going to define our for loop to iterate through 1000 epochs. We will also store our cost results to graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5733bd8c-86f6-ec9c-fa9c-0da8f35c2ef5"
      },
      "outputs": [],
      "source": [
        "cost_values = []\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs): \n",
        "    _, c = sess.run([optimizer, cost], feed_dict={X_init:data, Y_init:data_labels})\n",
        "    cost_values.append(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "02f6cd38-92a3-edbc-b845-bf4b33cdca87"
      },
      "source": [
        "Just to see how our model looks lets print out the final cost, weights, and bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fe668c71-63df-f611-dc9f-b9d76d6021b8"
      },
      "outputs": [],
      "source": [
        "training = sess.run(cost, feed_dict={X_init:data, Y_init:data_labels})\n",
        "print(\"Final cost: {0} final weights: {1} final biases: {2}\".format(training, sess.run(W), sess.run(b)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c6e82dd5-b24c-e053-5e22-98a471ac3faa"
      },
      "source": [
        "Lets also look at our cost function values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "001bc6fa-c144-8030-255c-0e4a69520ba1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(1)\n",
        "plt.title(\"Cost values\")\n",
        "plt.plot(cost_values);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "14b45d31-57da-977d-bf7d-0df6a450f89a"
      },
      "source": [
        "To see how well we did, we need to compute the R^2 to see how well our model explains the data, the Root Mean Squared Error(RMSE) to tell us standard deviation of our predicted values vs. the actual values, and the Adjusted R^2 function to make sure the regular R^2 function is not being influenced by the high number of features we have in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a10168a1-0eb3-ba61-7c27-7bfa3819e0a2"
      },
      "outputs": [],
      "source": [
        "## Defining R^2\n",
        "ss_e = tf.reduce_sum(tf.pow(tf.subtract(Y, pred), 2))\n",
        "ss_t = tf.reduce_sum(tf.pow(tf.subtract(Y, 0), 2))\n",
        "r2 = tf.subtract(1.0, tf.div(ss_e, ss_t))\n",
        "\n",
        "## Defining Adjusted R^2\n",
        "adjusted_r2 = tf.subtract(1.0, tf.div(tf.div(ss_e, (n_samples - 1.0)), tf.div(ss_t, (n_samples - num_features - 1)) ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b7d85d4d-a01b-7f9b-f7a0-630346e8a0b3"
      },
      "source": [
        "Now we grab all of the predictions and all of the standardized Y values. Then we compute the R^2, Adjusted R^2, and RMSE on our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e9a666f8-56ff-28f1-a262-ecfda8accd9d"
      },
      "outputs": [],
      "source": [
        "pred_data = sess.run(pred, feed_dict={X_init:data, Y_init:data_labels})\n",
        "std_y_data = sess.run(Y, feed_dict={Y_init:data_labels})\n",
        "## computing rmse\n",
        "rmse = np.sqrt(np.mean(np.power(np.subtract(pred_data, std_y_data), 2)))\n",
        "print(\"rmse of pred_data and std_y_data is: {0}\".format(rmse))\n",
        "print(\"R^2 value: {0}\".format(sess.run(r2,feed_dict={X_init:data, Y_init:data_labels})) )\n",
        "print(\"Adjusted R^2 value: {0}\".format(sess.run(adjusted_r2, feed_dict={X_init:data, Y_init:data_labels})))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "687f8157-e56d-b35c-0447-ef1d4059584a"
      },
      "source": [
        "As we can see we have a RMSE of ~0.55, $R^2$ of ~0.70, and Adjusted $R^2$ of ~0.70 with our simple multiple linear regression model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "263e25c5-c90f-dc42-181a-06f1aa4e4c36"
      },
      "source": [
        "Now lets see how well our model can predict our test data by comparing RMSE values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "51d52eee-e00f-3a3a-8c99-c5eb145a6fd0"
      },
      "outputs": [],
      "source": [
        "rmse_train = np.sqrt(np.mean(np.power(np.subtract(pred_data, std_y_data), 2)))\n",
        "print(\"RMSE for Training data is: {0}\".format(rmse_train))\n",
        "\n",
        "## run test set through model\n",
        "test_data = (test_set.drop(\"price\", axis=1)).values\n",
        "test_data_labels = (test_set[\"price\"].copy()).values\n",
        "test_data_labels = test_data_labels.reshape([len(test_data_labels), 1])\n",
        "test_pred = sess.run(pred, feed_dict={X_init:test_data, Y_init:test_data_labels})\n",
        "test_data_labels = sess.run(Y, feed_dict={Y_init:test_data_labels})\n",
        "rmse_test = np.sqrt(np.mean(np.power(np.subtract(test_pred, test_data_labels), 2)))\n",
        "print(\"RMSE for Test data is: {0}\".format(rmse_test))\n",
        "print(\"RMSE difference: {0}\".format(rmse_test - rmse_train))\n",
        "sess.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12281efa-8aa3-510e-a5e9-6e36c15c8f88"
      },
      "source": [
        "It seems that our model has a similar outcome for our test data as our training data with only a difference of ~0.01 with the RMSE values. So for a simple model this seems to be a good start."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}