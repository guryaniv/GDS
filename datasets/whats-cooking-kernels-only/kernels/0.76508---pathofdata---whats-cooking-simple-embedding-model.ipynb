{"cells":[{"metadata":{"_uuid":"75c0b25989cabef125c0ec01e338178a938d93b0"},"cell_type":"markdown","source":"Import libraries"},{"metadata":{"trusted":true,"_uuid":"9036d7a11ddcf38f21b05c85b8ab2acd54015cd4"},"cell_type":"code","source":"import collections\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st\nfrom sklearn import metrics\nfrom matplotlib import cm\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\nsns.set(color_codes=True)\nsns.set(style=\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a160e75aec14d93cc910e9ca422472b71eb8ce9"},"cell_type":"markdown","source":"Let's load the data and get some basic information."},{"metadata":{"trusted":true,"_uuid":"bd114ae94970d140130fb08107de0287685163c3"},"cell_type":"code","source":"raw_data = pd.read_json('../input/train.json')\nraw_ingredients = [item for sublist in list(raw_data['ingredients']) for item in sublist]\nraw_data['seq_length'] = [len(item) for item in raw_data['ingredients']]\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7724dc4fd81d70a64c6eaa8a09ac23551da6e040"},"cell_type":"markdown","source":"Ingredients column appears to be a list of lists of variable size, let's find out the distribution of the ingredient lengths."},{"metadata":{"trusted":true,"_uuid":"8c536e1bdf8a4da462c5a39a5d317ba293032c23"},"cell_type":"code","source":"sns.distplot(raw_data['seq_length'],axlabel='Number of ingredients per dish', color=\"m\")\nplt.title('Distribution of number of ingredients per dish')\nplt.ylabel('(%)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d36b448c5de3a962321d20a1e4eb37842aedbec"},"cell_type":"code","source":"def distribution_fit(data, distribution):\n    y, x = np.histogram(data, bins=200, density=True)\n    x = (x + np.roll(x, -1))[:-1] / 2.0\n    params = distribution.fit(data)\n    arg = params[:-2]\n    loc = params[-2]\n    scale = params[-1]\n    pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n    sse = np.sum(np.power(y - pdf, 2.0))\n    return pdf,sse\npdf, sse = distribution_fit(raw_data['seq_length'], st.norm)\nprint('Sum of squared errors: {}' .format(sse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf713db14cf80327ce1ec28d9294324e69c6f22e"},"cell_type":"markdown","source":"Distribution resembles normal distribution with a sum of squared errors of 0.45. A common practice would be to cut the long tail right of the distribution. This clears some outlier cases and will reduce the number of padding later on."},{"metadata":{"trusted":true,"_uuid":"e4e5bbfc3dae633ca21fa32c7e4da28fe594543a"},"cell_type":"code","source":"max_length = 35\nnew_data = pd.DataFrame()\nnew_data = raw_data.loc[raw_data['seq_length'] < max_length].sample(frac=1).reset_index(drop=True).copy()\n\n\nsns.distplot(new_data['seq_length'],axlabel='Number of ingredients per dish', color=\"m\")\nplt.title('Distribution of number of ingredients per dish')\nplt.ylabel('(%)')\nplt.show()\n\npdf, sse = distribution_fit(new_data['seq_length'], st.norm)\nprint('Sum of squared errors: {}' .format(sse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"095d9fa411a21cc32079b60828569ba2121c3dc3"},"cell_type":"markdown","source":"Notice that the new distribution has a sum of squared errors of 2, higher than before. Let's see the distribution of labels in the dataset."},{"metadata":{"trusted":true,"_uuid":"947c914d358c30b44e51159a5b2c8e8acacd3689"},"cell_type":"code","source":"def get_label_representation(dataframe, labels_name):\n    num_labels = sorted(set((dataframe[labels_name].values)))\n    label_count = {}\n    for i,x in enumerate(num_labels):\n        label_count[x] = len(dataframe.loc[dataframe[labels_name] == x])\n    return label_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0083840fc72b76475881e4f5811c1b470369a09"},"cell_type":"code","source":"dist = get_label_representation(new_data, 'cuisine')\nplt.figure(figsize=(20,3))\nsns.barplot(list(dist.keys()), list(dist.values()), color='m')\nplt.title('Distribution of dishes')\nplt.ylabel('Number')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df2d564d0e656af7cd2561750573fa864c6be164"},"cell_type":"markdown","source":"Our dataset is far from balanced, with the majority of dishes being italian,mexican and southern american. In order for our model to not adopt a bias towards the more commonly occuring dishes, a common practice is to balance the dataframe. Here in order to not throw away too much data we experiment with a balanced dataframe as well as an almost balanced dataframe, only reducing the representation of the three most prominent labels."},{"metadata":{"trusted":true,"_uuid":"a21150d3b2417c7016a363f3317197e111d348d7"},"cell_type":"code","source":"def balance_dataframe(df,labels_n, hard_balance=False):\n    representation = get_label_representation(df, labels_n)\n    label_keys = list(representation.keys())\n    label_values = list(representation.values())\n    soft_value = 3000\n    min_value = min(label_values)\n    cols = list(df.columns)\n    balanced_df = pd.DataFrame()\n    if hard_balance:\n        thresh = min_value\n    else:\n        thresh = soft_value\n    for i,x in enumerate(label_keys):\n        label_slice = df.loc[df[labels_n] == x].sample(min(thresh, label_values[i]))\n        balanced_df = balanced_df.append(label_slice)\n    balanced_df = balanced_df.sample(frac=1).reset_index(drop=True)\n    return balanced_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65f99356717aed7e6291dc701ca32386d73b4920"},"cell_type":"code","source":"balanced_df = balance_dataframe(new_data, 'cuisine', hard_balance=True)\nalmost_balanced_df = balance_dataframe(new_data, 'cuisine', hard_balance=False)\nunbalanced_df = new_data\n\nfor dataframe in [unbalanced_df, almost_balanced_df, balanced_df]:\n    print('\\n\\n')\n    print('Number of dishes: {}' .format(len(dataframe)))\n    dist = get_label_representation(dataframe, 'cuisine')\n    plt.figure(figsize=(20,3))\n    sns.barplot(list(dist.keys()), list(dist.values()), color='m')\n    plt.title('Distribution of dishes')\n    plt.ylabel('Number')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c26ddff0477bd609ab5c41ceea9f4966e01fb46f"},"cell_type":"markdown","source":"Notice that the balanced dataframe contains only 23% of the original data and the almost balanced dataframe contains about 76%. Next we tokenize the ingredients. We choose the 'UNK' token for rare ingredients, namely the ones appearing only once. Similarly we convert string labels to numerical categories."},{"metadata":{"trusted":true,"_uuid":"b4f40bf7be473ffa5633c735d9a818463364babc"},"cell_type":"code","source":"def tokenize_df(data_, cutoff = 1):\n    data = data_.copy()\n    raw_ingredients = [item for sublist in list(data['ingredients']) for item in sublist]\n    ingredient_frequencies = collections.Counter(raw_ingredients)\n    num_ingredients = len(set(raw_ingredients))\n    print('Number of total ingredients: {}' .format(num_ingredients))\n    sorted_ingredients_by_prc = ingredient_frequencies.most_common()\n    vocabulary_items = [item[0] for item in sorted_ingredients_by_prc if item[1] > cutoff]\n    print('Number of common ingredients (appearing more than once): {}' .format(len(vocabulary_items)))\n    \n    # Create the vocabulary that maps ingredients to the numerical values. \n    # 0 value is reserved for padding while the last value of the vocabulary \n    # is reserved for the UNK token.\n    vocabulary = sorted(vocabulary_items)\n    unk_index = len(vocabulary) + 1\n    voc2idx = {}\n    voc2idx.update({'PAD' : 0})\n    ind = 1\n    for item in sorted(set(raw_ingredients)):\n        if item in vocabulary:\n            voc2idx.update({item : ind})\n            ind += 1\n        else:\n            voc2idx[item] = unk_index\n    idx2voc = dict(zip(voc2idx.values(), voc2idx.keys()))\n    idx2voc[unk_index] = 'UNK'\n    print('Vocabulary length : {}' .format(len(idx2voc)))\n    \n    \n    # Update the dataframe with tokenized data.\n    data['idx_ingredients'] = [[voc2idx[item] for item in sublist]for sublist in data['ingredients']]\n    cuisines = sorted(set(list(data['cuisine'])))\n    idx2cuisine = dict(enumerate(cuisines))\n    cuisine2idx = dict(zip(idx2cuisine.values(), idx2cuisine.keys()))\n    data['idx_cuisine'] = [cuisine2idx[item] for item in data['cuisine']]\n    data = data.sample(frac=1).reset_index(drop=True)\n    data.head()\n    return data ,voc2idx, idx2voc, idx2cuisine, cuisine2idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30d864bab54f2bc725438f76aa5e96c3fba9605c"},"cell_type":"code","source":"tokenized_df = tokenize_df(balanced_df)\ndf = tokenized_df[0]\nvoc_len = len(tokenized_df[2])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a39519b08e78dd41121784a804d147f502de968"},"cell_type":"markdown","source":"Split the dataframe into training and validation set."},{"metadata":{"trusted":true,"_uuid":"433139707277927462112e02989353209d1fb4b7"},"cell_type":"code","source":"train_len, valid_len = np.floor(len(df)*0.8), np.floor(len(df)*0.2)\ntrain_df = df.loc[:train_len-1].sample(frac=1).reset_index(drop=True).copy()\nvalid_df = df.loc[train_len:(train_len + valid_len)].sample(frac=1).reset_index(drop=True).copy()\nprint('Training set length: {}\\nValidation set length: {}' .format(train_len,valid_len))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62d8c70fcf09ddb9c395be914d760c0ac1c54adf"},"cell_type":"markdown","source":"Create a simple iterator that pads each input sequence within a batch with zeros."},{"metadata":{"trusted":true,"_uuid":"31c8c6677688f548f5f1a11ccea8e4b74eb2a5fc"},"cell_type":"code","source":"class dish_iterator():\n    \n    def __init__(self, df):\n        self.size = len(df)\n        self.dfs = df\n        self.cursor = 0\n        self.epochs = 0\n        self.shuffle()\n\n    def shuffle(self):\n        self.dfs = self.dfs.sample(frac=1).reset_index(drop=True)\n        self.cursor = 0\n    \n    def next_batch(self, n):\n        res = self.dfs.loc[self.cursor:self.cursor+n-1]\n        self.cursor += n\n        maxlen = max(res['seq_length'])\n        x = np.zeros([n, maxlen], dtype=np.int32)\n        for i, x_i in enumerate(x):\n            x_i[:res['seq_length'].values[i]] = res['idx_ingredients'].values[i]\n        \n        if self.cursor+n+1 > self.size:\n            self.epochs += 1\n            self.shuffle()\n        return x, res['idx_cuisine']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83364cf3f4ab2e4aedfedc55cfebd84f7aeec723"},"cell_type":"markdown","source":"Build a graph with an embedding layer followed by a global average pooling op to reduce batches of sequences to batches of ints. Values higher than 20 for the output dimension of the embedding layer work fine with this model. A dropout layer is added after pooling to reduce overfitting."},{"metadata":{"trusted":true,"_uuid":"150a3a0c4ecb7165ebaef96026b886c70ce803dd"},"cell_type":"code","source":"def reset_graph():\n    if 'sess' in globals() and sess:\n        sess.close()\n    tf.reset_default_graph()\n\ndef build_graph(\n    vocab_size,\n    output_dims = 50,\n    batch_size = 32,\n    num_classes = 20,\n    learning_rate = 1e-3):\n\n    reset_graph()\n\n    # Placeholders\n    x = tf.placeholder(tf.int32, [batch_size, None], name='input_tensor')\n    y = tf.placeholder(tf.int32, [batch_size], name='labels_tensor')\n    keep_prob = tf.placeholder_with_default(1.0,[])\n\n    # Embedding layer\n    embeddings = tf.get_variable('embeddings', [vocab_size, output_dims])\n    model_inputs = tf.nn.embedding_lookup(embeddings, x)\n    \n    # Global Average Pooling to reduce sequences\n    pooling_output = tf.reduce_mean(model_inputs, axis = 1)\n    pooling_output_d = tf.nn.dropout(pooling_output, keep_prob)\n\n    # Softmax layer\n    with tf.variable_scope('softmax'):\n        W = tf.get_variable('W', [output_dims, num_classes])\n        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n    logits = tf.matmul(pooling_output_d, W) + b\n    preds = tf.nn.softmax(logits)\n    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = y))\n    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n    \n    print('\\nGraph summary:')\n    print('Embedding layer size: {}' .format(model_inputs.get_shape()))\n    print('Global Average Pooling size: {}' .format(pooling_output.get_shape()))\n    print('Dropout layer size: {}' .format(pooling_output_d.get_shape()))\n    print('Softmax layer size: {}' .format(logits.get_shape()))\n    trainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n    print('Trainable parameters: {}' .format(trainable_params))\n    \n    return {\n        'x': x,\n        'y': y,\n        'dropout': keep_prob,\n        'loss': loss,\n        'ts': train_step,\n        'preds': preds,\n        'accuracy': accuracy, \n    }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da7538d4b6a34ad77cb76c1cf057793a10f5dd7f"},"cell_type":"markdown","source":"Train the model on the data and compute validation accuracy after each epoch. Early stopping with a tolerance of 2 prevents the model from overfitting."},{"metadata":{"trusted":true,"_uuid":"3558ba9e882fc3d03ee8896eea78191bf8ab177e"},"cell_type":"code","source":"def train_graph(graph,\n                train_dataset,\n                validation_dataset,\n                batch_size = 32, \n                num_epochs = 50, \n                iterator = dish_iterator, \n                savepath = False):\n    saver = tf.train.Saver()\n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        tr = iterator(train_dataset)\n        vd = iterator(validation_dataset)\n\n        step, accuracy = 0, 0\n        tr_acc, vd_acc = [], []\n        current_epoch = 0\n        early_stopping= 0\n        tolerance_flag= False\n        while current_epoch < num_epochs:\n            step += 1\n            batch = tr.next_batch(batch_size)\n            feed = {graph['x']: batch[0], graph['y']: batch[1], graph['dropout']: 0.5}\n            accuracy_, _ = sess.run([graph['accuracy'], graph['ts']], feed_dict=feed)\n            accuracy += accuracy_\n            if tr.epochs > current_epoch:\n                current_epoch += 1\n                tr_acc.append(accuracy / step)\n                step, accuracy = 0, 0\n                vd_epoch = vd.epochs\n                while vd.epochs == vd_epoch:\n                    step += 1\n                    batch = vd.next_batch(batch_size)\n                    feed = {graph['x']: batch[0], graph['y']: batch[1]}\n                    accuracy_ = sess.run([graph['accuracy']], feed_dict=feed)[0]\n                    accuracy += accuracy_\n                \n                vd_acc.append(accuracy / step)\n                step, accuracy = 0,0\n                \n                if (vd_acc[-1] - early_stopping < 3e-3):\n                    if tolerance_flag:\n                        print('Early stopping at epoch {}: '\n                              'Train accuracy {} - Validation accuracy {}' .format(\n                            current_epoch, tr_acc[-1], vd_acc[-1]))\n                        break\n                    tolerance_flag = True\n#                     early_stopping = vd_acc[-1]\n                else:\n                    early_stopping = vd_acc[-1]\n                    tolerance_flag = False\n        if savepath:\n            save_path = saver.save(sess, savepath)\n            print(\"Model saved in path: %s\" % save_path)\n\n    return tr_acc, vd_acc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f0e541a85c16246ab46c4a8a87e0b20eb589eef"},"cell_type":"markdown","source":"Simple iterator to feed data for prediction"},{"metadata":{"trusted":true,"_uuid":"9a393b3277d7f489deb3fe36b6b5b2215230c0f0"},"cell_type":"code","source":"class inference_iterator():\n    def __init__(self, df):\n        self.dfs = df.reset_index(drop=True)\n        self.size = len(self.dfs)\n        self.epochs = 0\n        self.cursor = 0\n\n    def next_batch(self): \n        res = self.dfs.loc[self.cursor]\n        x = np.array(res['idx_ingredients'])\n        self.cursor += 1\n        if self.cursor  + 1 > self.size:\n            self.epochs += 1\n        return [x]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90b41e9e76bdacf42e77aab0a2f42299869def81"},"cell_type":"markdown","source":"A function that uses the model for predictions"},{"metadata":{"trusted":true,"_uuid":"018c4dda7d2c22127a39331526b97d93a329a475"},"cell_type":"code","source":"def predict_dish(graph, checkpoint, inference_data, iterator= inference_iterator):\n    saver = tf.train.Saver()\n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        saver.restore(sess, checkpoint) \n        \n        predictions = []\n        tinf = iterator(inference_data)\n        while tinf.epochs == 0:\n            item = tinf.next_batch()\n            feed = {graph['x']: item}\n            predictions_ = sess.run([graph['preds']], feed_dict=feed)[0]\n            predictions.append(predictions_)\n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38d2a427ea8795e4d54a6b9d1718446978955ad3"},"cell_type":"markdown","source":"Function to plot a confusion matrix given a list of labels and predictions"},{"metadata":{"trusted":true,"_uuid":"d45b440d625777de4181917bb56497a3fd5e0f50"},"cell_type":"code","source":"def create_confusion_matrix(labels, predictions):\n    cm = metrics.confusion_matrix(labels, predictions)\n    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n    ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n    ax.set_aspect(1)\n    plt.title(\"Confusion matrix\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3688310381e4d658253a2b6a7f5e986b25cff53a"},"cell_type":"markdown","source":"Function to quickly plot the distribution of labels"},{"metadata":{"trusted":true,"_uuid":"e2b46f0356bb9fb3d89481d3994f2a1c301842c8"},"cell_type":"code","source":"def plot_distribution(dataframe, label_title):\n    dist = get_label_representation(dataframe, label_title)\n    sns.barplot(list(dist.keys()), list(dist.values()), color='m')\n    plt.title('Distribution of dishes')\n    plt.ylabel('Number')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70cb9e06da0975f74935579870a2410852c862d5"},"cell_type":"markdown","source":"Next we wrap everything up in one function that will help training multiple models and plot the accuracy, confusion matrix and the lebel distribution on each dataframe."},{"metadata":{"trusted":true,"_uuid":"b2289e9745c3a80d2776d571b21ff13ab14fd45e"},"cell_type":"code","source":"def session_wrapper(dataframe, cutoff_number, save):\n    tokenized_df = tokenize_df(dataframe, cutoff = cutoff_number)\n    df = tokenized_df[0]\n    voc_len = len(tokenized_df[2])\n    train_len, valid_len = np.floor(len(df)*0.8), np.floor(len(df)*0.2)\n    train_df, valid_df = df.loc[:train_len-1], df.loc[train_len:(train_len + valid_len)]\n    print('Training set length: {}\\nValidation set length: {}' .format(train_len,valid_len))\n\n    g = build_graph(vocab_size=voc_len)\n    tr_acc, vd_acc = train_graph(g, train_df, valid_df, savepath= save)\n\n    plt.plot(tr_acc,label='Training accuracy')\n    plt.plot(vd_acc, label='Validation accuracy')\n    plt.grid(True)\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy %')\n    plt.legend()\n    plt.show()\n\n    inf_data = valid_df\n    g = build_graph(vocab_size=voc_len, batch_size=1)\n    preds = predict_dish(g, save, inf_data)\n    final_preds = [np.argmax(item) for item in preds]\n    create_confusion_matrix(valid_df['idx_cuisine'].values, final_preds)\n    plot_distribution(train_df, 'idx_cuisine')\n    return tokenized_df","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"83a87a5394c1a35ebf6172a6d5cf11d046541e72"},"cell_type":"code","source":"_ = session_wrapper(balanced_df, 3, 'saves/balanced')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e652de0eae9b81389a9cc657afed50360ff0286"},"cell_type":"markdown","source":"Accuracy is low. We will keep this image to compare it with the results from the other two cases."},{"metadata":{"trusted":true,"_uuid":"73a59f355dbb39dff9f26e5193bb4599626bbb61"},"cell_type":"code","source":"_ = session_wrapper(unbalanced_df, 3, 'saves/unbalanced')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d055d3c6a27376f2650be15184d71fd611ec9e98"},"cell_type":"markdown","source":"There is a noticable improvement in performance, enforcing the idea of balanced dataframe containing too few unique data to train on. The model though has developed a bias towards the more represented labels. "},{"metadata":{"trusted":true,"_uuid":"6d8b112c0c783314ba98fc5bd86c2e0281a1ce1e"},"cell_type":"code","source":"_ = session_wrapper(almost_balanced_df, 3, 'saves/semibalanced')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35b0d948a6704c80554d0f9eddf763e2a8fd36a3"},"cell_type":"markdown","source":"A bit less accuracy. Bias is still there but less noticable than before. Next up we'll extract the vocabulary only from the training set. This ensures that the model trains on all words in the vocabulary, as opposed to the previous case were some words may be appearing only in the validation set and not as 'UNK\" token."},{"metadata":{"trusted":true,"_uuid":"5dd0c485cdeca8a6f8ddbffd56eeff895a64bf0b"},"cell_type":"code","source":"def alt_session_wrapper(dataframe_, cutoff_number, save):\n    pre_split_df = dataframe_.copy()\n    train_len, valid_len = np.floor(len(pre_split_df)*0.8), np.floor(len(pre_split_df)*0.2)\n    train_df = pre_split_df.loc[:train_len-1].sample(frac=1).reset_index(drop=True).copy()\n    valid_df = pre_split_df.loc[train_len:(train_len + valid_len)].sample(frac=1).reset_index(drop=True).copy()\n    print('Training set length: {}\\nValidation set length: {}' .format(train_len,valid_len))\n    tokenized_df = tokenize_df(train_df, cutoff = cutoff_number)\n    df = tokenized_df[0]\n    voc2idx = tokenized_df[1]\n    idx2voc = tokenized_df[2]\n    idx2cuisine = tokenized_df[3]\n    cuisine2idx = tokenized_df[4]\n    unk = len(tokenized_df[2])-1\n    valid_df['idx_ingredients'] = [[voc2idx[item] if item in list(voc2idx.keys()) else unk for item in sublist]for sublist in valid_df['ingredients']]\n    valid_df['idx_cuisine'] = [cuisine2idx[item] for item in valid_df['cuisine']]\n    valid_df = valid_df.sample(frac=1).reset_index(drop=True)\n    voc_len = len(tokenized_df[2])\n    \n    g = build_graph(vocab_size=voc_len)\n    tr_acc, vd_acc = train_graph(g, df, valid_df, savepath= save)\n\n    plt.plot(tr_acc,label='Training accuracy')\n    plt.plot(vd_acc, label='Validation accuracy')\n    plt.grid(True)\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy %')\n    plt.legend()\n    plt.show()\n\n    inf_data = valid_df\n    g = build_graph(vocab_size=voc_len, batch_size=1)\n    preds = predict_dish(g, save, inf_data)\n    final_preds = [np.argmax(item) for item in preds]\n    create_confusion_matrix(valid_df['idx_cuisine'].values, final_preds)\n    plot_distribution(df, 'idx_cuisine')\n    return tokenized_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4383a5c091cd814eb8391a276ba4499524c3d859"},"cell_type":"code","source":"_ = alt_session_wrapper(balanced_df, 3, 'saves/alt_balanced')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31b2b6e38f693e4fa1616b24004fe39d322dcaa9"},"cell_type":"code","source":"_ = alt_session_wrapper(unbalanced_df, 3, 'saves/alt_unbalanced')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c52155f9efcaf1ef6b8bf2862b30a1745b91d20"},"cell_type":"code","source":"tokenized_data = alt_session_wrapper(almost_balanced_df, 3, 'saves/alt_semi_balanced')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"199e2292c4c36bf26b7fba81ff39487fc8689bc4"},"cell_type":"markdown","source":"Next we'll be balancing the dataframe by trimming the three big popular labels but also duplicating the unpopular ones. By duplicating part of  the data there is a higher risk of overfitting."},{"metadata":{"trusted":true,"_uuid":"4b8d8a9416d8229c3bd658fe83ca0d1db0f6a94a"},"cell_type":"code","source":"def alt_balance_dataframe(df,labels_n):\n    representation = get_label_representation(df, labels_n)\n    label_keys = list(representation.keys())\n    label_values = list(representation.values())\n    soft_value = 3000\n    min_value = min(label_values)\n    cols = list(df.columns)\n    balanced_df = pd.DataFrame()\n    for i,x in enumerate(label_keys):\n        label_slice = df.loc[df[labels_n] == x].sample(soft_value, replace=True)\n        balanced_df = balanced_df.append(label_slice)\n    balanced_df = balanced_df.sample(frac=1).reset_index(drop=True)\n    return balanced_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f53a8ff75087fcbb34167a04c6c5e06cae88812e"},"cell_type":"code","source":"def alt_session_wrapper(dataframe_, cutoff_number, save):\n    pre_split_df = dataframe_.copy()\n    train_len, valid_len = np.floor(len(pre_split_df)*0.8), np.floor(len(pre_split_df)*0.2)\n    train_df_ = pre_split_df.loc[:train_len-1].sample(frac=1).reset_index(drop=True).copy()\n    valid_df = pre_split_df.loc[train_len:(train_len + valid_len)].sample(frac=1).reset_index(drop=True).copy()\n    print('Training set length: {}\\nValidation set length: {}' .format(train_len,valid_len))\n    train_df = alt_balance_dataframe(train_df_, 'cuisine')\n    tokenized_df = tokenize_df(train_df, cutoff = cutoff_number)\n    df = tokenized_df[0]\n    voc2idx = tokenized_df[1]\n    idx2voc = tokenized_df[2]\n    idx2cuisine = tokenized_df[3]\n    cuisine2idx = tokenized_df[4]\n    unk = len(tokenized_df[2])-1\n    valid_df['idx_ingredients'] = [[voc2idx[item] if item in list(voc2idx.keys()) else unk for item in sublist]for sublist in valid_df['ingredients']]\n    valid_df['idx_cuisine'] = [cuisine2idx[item] for item in valid_df['cuisine']]\n    valid_df = valid_df.sample(frac=1).reset_index(drop=True)\n    voc_len = len(tokenized_df[2])\n    \n    g = build_graph(vocab_size=voc_len)\n    tr_acc, vd_acc = train_graph(g, df, valid_df, savepath= save)\n\n    plt.plot(tr_acc,label='Training accuracy')\n    plt.plot(vd_acc, label='Validation accuracy')\n    plt.grid(True)\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy %')\n    plt.legend()\n    plt.show()\n\n    inf_data = valid_df\n    g = build_graph(vocab_size=voc_len, batch_size=1)\n    preds = predict_dish(g, save, inf_data)\n    final_preds = [np.argmax(item) for item in preds]\n    create_confusion_matrix(valid_df['idx_cuisine'].values, final_preds)\n    plot_distribution(df, 'idx_cuisine')\n    return tokenized_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2adbfe9d75bd05ddc7a644c5f8b442da759eca9e"},"cell_type":"code","source":"_ = alt_session_wrapper(unbalanced_df, 3, 'saves/alt_balanced_embeddings_trim')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"825e672cf16156137eb181affb566259967304e1"},"cell_type":"markdown","source":"While the model has no visible bias towards a label, overfitting is a big issue here. Increasing dropout alleviates things a bit, but accuray is lower than before. We also silently introduced a new hyper parameter to the model, that of number of samples in each label. It was intuitively set to 3000 with the mindset of not introducing too much duplicate data, but tuning this parameter could improve performance. Moving on testing on the test data."},{"metadata":{"trusted":true,"_uuid":"1ae616a89a188042a3a8076ff4a445d58a742581"},"cell_type":"code","source":"test_data = pd.read_json('../input/test.json')\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae7bffe99b2cfd45421e2c9dcc0a8503ed849392"},"cell_type":"code","source":"voc2idx = tokenized_data[1]\nidx2voc = tokenized_data[2]\nidx2cuisine = tokenized_data[3]\nunk = len(tokenized_data[2])-1\nunk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcdd87038c173d58506a86f2aeb0c26643d3ea69"},"cell_type":"code","source":"test_data['idx_ingredients'] = [[voc2idx[item] if item in list(voc2idx.keys()) else unk for item in sublist]for sublist in test_data['ingredients']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc2041f10b97327217ccfd86a7abaab6be4c0ac0"},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16e3404f7ce67c16850dc1f3c90bdf0c57f62891"},"cell_type":"code","source":"voc_len = len(tokenized_data[2])\ng = build_graph(vocab_size=voc_len, batch_size= 1)\npreds = predict_dish(g, 'saves/alt_semi_balanced', test_data)\nfinal_preds = [np.argmax(item) for item in preds]\npredictions = [idx2cuisine[item] for item in final_preds]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5fff067a0bfa1cf41de97aba0b5042e0b8e776a"},"cell_type":"code","source":"out = pd.DataFrame()\nout['id']= test_data['id'].values\nout['predictions'] = predictions\nout.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5acec58efc1877ac0abd9121b6f251f2e6b59caa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}},"nbformat":4,"nbformat_minor":1}