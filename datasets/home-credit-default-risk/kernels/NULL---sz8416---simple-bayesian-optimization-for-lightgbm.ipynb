{"cells":[{"metadata":{"_uuid":"0ae131807ff23796210d5bdc10e6dcb038bea652"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport time\nwarnings.filterwarnings(\"ignore\")\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"application_train = pd.read_csv('../input/application_train.csv')\nfrom sklearn.preprocessing import LabelEncoder\ndef label_encoder(input_df, encoder_dict=None):\n    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n    # Label encode categoricals\n    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n    for feat in categorical_feats:\n        encoder = LabelEncoder()\n        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n    return input_df, categorical_feats.tolist(), encoder_dict\napplication_train, categorical_feats, encoder_dict = label_encoder(application_train)\nX = application_train.drop('TARGET', axis=1)\ny = application_train.TARGET","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d32f1f813d7eeb86125c8ff526f8925c422a5096"},"cell_type":"markdown","source":"### Step 1: parameters to be tuned\n**Note**: values for parameters should make sense, e.g.: 'num_leaves' needs to be a integer and 'feature_fraction' should between 0 and 1"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"14c0f91c2e16816d6b68a033b31539db910fb27a"},"cell_type":"code","source":"def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n    params = {'application':'binary','num_iterations':4000, 'learning_rate':0.05, 'early_stopping_round':100, 'metric':'auc'}\n    params[\"num_leaves\"] = round(num_leaves)\n    params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n    params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n    params['max_depth'] = round(max_depth)\n    params['lambda_l1'] = max(lambda_l1, 0)\n    params['lambda_l2'] = max(lambda_l2, 0)\n    params['min_split_gain'] = min_split_gain\n    params['min_child_weight'] = min_child_weight\n    cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n    return max(cv_result['auc-mean'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c12b26e61112fee99f74337460fec971e8bad34"},"cell_type":"markdown","source":"### Step 2: Set the range for each parameter\n**Gentle reminder**: try to make the range as narrow as possible"},{"metadata":{"trusted":true,"_uuid":"a0962e7ab187fc72b1e9292df0ed69180254276f","collapsed":true},"cell_type":"code","source":"lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n                                        'feature_fraction': (0.1, 0.9),\n                                        'bagging_fraction': (0.8, 1),\n                                        'max_depth': (5, 8.99),\n                                        'lambda_l1': (0, 5),\n                                        'lambda_l2': (0, 3),\n                                        'min_split_gain': (0.001, 0.1),\n                                        'min_child_weight': (5, 50)}, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b683fc24ec1527392b173ac7035873de0a501dc"},"cell_type":"markdown","source":"### Step 3: Bayesian Optimization: Maximize"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5ec2289350b5495bbbe26b004d117fa472dcfe62"},"cell_type":"code","source":"# lgbBO.maximize(init_points=init_round, n_iter=opt_round)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da31d0363b7dadae7a7d9b83e6fd4739bd21b9c9"},"cell_type":"markdown","source":"### Step 4: Get the parameters"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"27b4de927f17d02ccc9069882a32a52c7cc09738"},"cell_type":"code","source":"# lgbBO.res['max']['max_params']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6259359813067b5117714076b26b6331ff7ade7"},"cell_type":"markdown","source":"### Put all together\n**Note**: It is just a demo. To get a better result, you should increase initial rounds, optimization rounds and n_estimators"},{"metadata":{"trusted":true,"_uuid":"3523303f8f6ff9a9dff3fca9849240e6284feda1","collapsed":true},"cell_type":"code","source":"X = application_train.drop('TARGET', axis=1)\ny = application_train.TARGET\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.05, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, categorical_feature = categorical_feats, free_raw_data=False)\n    # parameters\n    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n        params = {'application':'binary','num_iterations': n_estimators, 'learning_rate':learning_rate, 'early_stopping_round':100, 'metric':'auc'}\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n    # range \n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n                                            'feature_fraction': (0.1, 0.9),\n                                            'bagging_fraction': (0.8, 1),\n                                            'max_depth': (5, 8.99),\n                                            'lambda_l1': (0, 5),\n                                            'lambda_l2': (0, 3),\n                                            'min_split_gain': (0.001, 0.1),\n                                            'min_child_weight': (5, 50)}, random_state=0)\n    # optimize\n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    # output optimization process\n    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n    \n    # return best parameters\n    return lgbBO.res['max']['max_params']\n\nopt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=10, n_folds=3, random_seed=6, n_estimators=100, learning_rate=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"73b0d3a7f9a21cc11f2958a51b54bb29a9e9e978"},"cell_type":"code","source":"print(opt_params)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}