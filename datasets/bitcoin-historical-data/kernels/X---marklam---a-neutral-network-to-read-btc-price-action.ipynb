{"nbformat_minor": 2, "metadata": {"language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "cells": [{"outputs": [], "cell_type": "markdown", "source": "**Training a neural network to read price action like floor trader**", "metadata": {"_cell_guid": "3dd5a5a5-f3a8-4449-b68a-e99df9f8e416", "_uuid": "c618bdaebc478d8c738e46c365a2fdc20ff859cb"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "First of all, a thank you for uploading the trading data and therefore making it available for the following analysis and discussion.\n\n**1. Introduction**\n\nThe data is presented in OHLC format in 1min resolution with additional volumetric and valuation data points like volume traded (VolB) and value traded (VolC). Using these additional data points, a volume weighted average price (WgtPx) is derived. (For completeness, WgtPx is included in the supplied data).\n\nTraditional time series analysis relies on a typical relationship like Y[t] = f( Y[t-1], Y[t-2]\u2026) where Y = price at time with a laundry list of over-arching assumptions. Using the data in the such format, technical analysis/indicators like SMA(15) are constructed. This following discussion will deviate from such norms and focuses on price action of the WgtPx i.e. Given the relative positions of OHLC in each snapshot, what is the relative movement of WgtPx in the next snapshot?\n\nIt should be noted the definition of WgtPx implies that \u2019market clearing\u2019 benchmark over the said time horizon and therefore the relative movements of this said benchmark presents a view of the underlying valuation (V) of the financial instrument/asset. This is the discussion\u2019s hypothesis.\n\nThe above approach also assumes that the change in price is a \u2018not-so-random\u2019 walk (with drift?) and therefore the underlying price action is a leading indicator for V. Price action is opined to be readily observed in markets where liquidity and price volatility are highest, but anything that is bought or sold freely in a market will per se demonstrate price action (Source: Wikipedia). In this case, the Bitcoin market fits the former description.", "metadata": {"_cell_guid": "954aed6a-1be6-44b2-877f-19c8c44fab47", "_uuid": "fa75708fbc95c952593d79d759948ed6ffc08404"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "**2. Translating the above into Python**\n\nO: Price at the start of snapshot which is fixed\n\nC: Price at the close of snapshot which may equal O\n\nH: Highest price recorded during the snapshot which may equal C and or O\n\nL: Lowest price recorded during the snapshot which may equal C and or O\n\nWgtPx, W: A derived price based on the ratio of value traded to volume traded\n\nHence:\n\nChange in WgtPX or V[t] \u2013 V[t-1]= f(HO[t], LO[t], CO[t], WO[t]), \n\nwhere HO, LO, CO and WO are the relative distance of H, L, C and W from a fixed datum O[t]", "metadata": {"_cell_guid": "4427f85c-fc51-4b9f-8edf-fb8154433f27", "_uuid": "354dabe226d0ae2463a26e79adc0ced5365b57d8"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "```\nINPUT_FILE_1 = os.path.join(os.getcwd(),sub_dir_in, filename_1) \n#filename_1 is a cleaned up pickle file based on \n#'coinbaseUSD_1-min_data_2014-12-01_to_2017-05-31.csv'\n\nwith open(INPUT_FILE_1, 'rb') as file:\n    df= pickle.load(file)\n\ndf = df[['O', 'H', 'L', 'C', 'VolB', 'VolC','WgtPx']]\ndf.index = pd.date_range('2015-02-28 23:59:00', periods=len(df), freq='1T')\ndf.index.name ='time_UTC'\n\n#Resample data\n\ndf['V'] = df.WgtPx - df.WgtPx.shift(1)\ndf['V+1'] = df['V'].shift(-1)\ndf['WC'] = df.WgtPx - df.C\n\ndf['HO'] = df.H - df.O\ndf['LO'] = df.L - df.O\ndf['CO'] = df.C - df.O\ndf['WO'] = df.WgtPx - df.O\ndf_Xt = df.iloc[:,-4:]\ntrain_Xt_array = df_Xt.values\n```", "metadata": {"_cell_guid": "43178e75-9043-4d56-9a4f-b6be3fc76d91", "_uuid": "0028a17a3981767e35c0b0570181cc5706d1f10a"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "*Making this into a supervised learning problem*\n```\nUP = np.logical_and(df['V+1'].round(decimals=2)>0, \n                    np.logical_and(df['CO']>0, df['WO']>0)).astype(int)\nDN = np.logical_and(df['V+1'].round(decimals=2)<0, \n                    np.logical_and(df['CO']<0, df['WO']<0)).astype(int)\nFLAT = np.logical_and(UP==0, DN==0).astype(int)\ndf_Yt = pd.concat([UP, DN, FLAT], join = 'outer', axis =1)\ndf_Yt.columns = ['UP', 'DN', 'FLAT']\ntrain_y_array = df_Yt.values\n```\n\nOne can check-sum the above by ensuring df_Yt.mean().values.sum() == 1", "metadata": {"_cell_guid": "3968c3ad-55cb-4e3a-9b0c-7fd9d954aa0a", "_uuid": "faf16cb1d48c039f7676a6410ff56f929c6510da"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "**3. Neutral Network setup**\n\nThe proposed neutral network is a straight vanilla ANN as set up below. This also assumes an efficient market hypothesis where prices fully reflect all available information. The model is structured as a classfication model simply because it is easier to relate to the model's effectiveness (accuracy, recall, f1-score) as opposed to the model being a regression variant (RMS).\n\n```\nmodel = Sequential()\nmodel.add(Dense(32, activation = 'tanh', input_dim = features))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation = 'tanh'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(32, activation = 'tanh'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(3, activation = 'softmax')) \n# out shaped on df_Yt.shape[1]\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', \n                  metrics=['accuracy'])\n```\n\nA plausible alternative is a LSTM (which is not discussed here) therefore assumes there are some memory in the price actions. \n\nLastly, the structure (# of hidden layers and neutrons) of the NN is somewhat arbitrary and solely for the purpose of this discussion.", "metadata": {"_cell_guid": "16838ba5-8649-41a2-a001-bdda684f9ea5", "_uuid": "1c7b785577c275e2e08259317ea4a5528a94101c"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "*Fitting the Neutral Network*\n\nThe models are trained using a cloud GPU and with the following hyperparameters:\n```\nbatch_size = 60*24 # Total 'blocks/snapshot' in a day\nepochs = 1000\n```\nModel output/prediction is put thru sci-kit learn's classification report\n```\nSeries_pred = np.argmax(model.predict(train_Xt_array, \n                                      batch_size=batch_size, \n                                      verbose = 0),axis = 1)\nSeries_actual = np.argmax(train_y_array, axis = 1)\nclassreport= classification_report(Series_actual, Series_pred, \n                                   target_names = df_Yt.columns,\n                                   digits = 4)\nprint(classreport)                                  \n```", "metadata": {"_cell_guid": "9a13ae54-a6e9-46d2-8570-d72ff91f28c1", "_uuid": "456f85d093b70c301e9ef0d511afc57b39cc361d"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "**4. Reviewing the trained models**\n\nThe model is trained using trading data for calendar year 2016. \n\nNote: Consider lowering epoch to 100 if one wishes to train the model on a local machine (with basic specs). The following results are based on 100 epochs.\n\n```\n             precision    recall  f1-score   support\n\n         UP     0.6139    0.8120    0.6992    106311\n         DN     0.6245    0.4227    0.5042     70077\n       FLAT     0.8217    0.7943    0.8078    350652\n\navg / total     0.7536    0.7485    0.7455    527040\n\n```\nIt is worth nothing the Keras' accuracy metric differs from the above as the Keras uses a straight average method whilst ski-learn computes on a weighted average basis", "metadata": {"_cell_guid": "e9e8884f-36e9-4ecf-a336-71124d2ef37f", "_uuid": "9383d08a13f12200f213b5bbbf16ecee9fc6910c"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "*Walking forward the model*\n\nValidating the model with trading data from 1 Jan 2017 to 30 May 2017\n\n```\n             precision    recall  f1-score   support\n\n         UP     0.6756    0.8454    0.7510     50578\n         DN     0.6520    0.5271    0.5829     33741\n       FLAT     0.8104    0.7720    0.7907    131681\n\navg / total     0.7541    0.7509    0.7490    216000\n```", "metadata": {"_cell_guid": "eba38566-f874-4a24-aa31-30e24f69e83d", "_uuid": "b7a32ad3c290920ff9f8398cad95420fd1f36f06"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "**5.Developing the model further**\n\nA potential nextstep is to upsample the data. Since 60 is divisible by 2, 3, 4, 5 and 10, a function can be written to resample the OHLC data in those resolution. Furthermore, up-sampling can smooth out the 'noise' at the lower resolution and possibly allowing the underlying pattern to come thru.\n\nIt also worth noting that price action can be reasonably assummed to be is the principal drive of valuation within a short time frame. Such assumption may be tenuous in higher resolution snapshots. ", "metadata": {"_cell_guid": "17145403-4b0f-4509-8a2a-4e28ff66046f", "_uuid": "d868d1c93369cadf6b8e5121fa2600f1a70d511e"}, "execution_count": null}]}