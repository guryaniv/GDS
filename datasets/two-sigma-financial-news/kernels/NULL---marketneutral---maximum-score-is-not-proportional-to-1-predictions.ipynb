{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# What is the optimal weighting policy to maxmize the score?\nBy @marketneutral\n\nThis kernel is a companion to the discussion [here](https://www.kaggle.com/c/two-sigma-financial-news/discussion/71996). In short, **given a model forecast of the target in this competition, what is the optimal policy to choose $\\hat{y}_{ti}$ to maximize the score?** \n\nThe scoring function is defined as\n\n$$x_t = \\sum_i \\hat{y}_{ti}  r_{ti}  u_{ti}$$\n$$\\text{score} = \\frac{\\bar{x}_t}{\\sigma(x_t)}$$\n\nWhere  $\\hat{y}_{ti}$ is the submission per asset per day,  $r_{ti}$ is the (unknown at time time t) forward realization of the 10-day market residualized return, and $u_{ti}$ is an indicator variable, $\\{0,1\\}$, to indiciate if the return for that asset on that day matters in the score or not.\n\nIn the leak-that-wasn't-a-leak, some kernels (like [this one](https://www.kaggle.com/pennacchio/env-var07)) showed that to maxmize the score, **if you know $r_{ti}$ for certainty for all i,t, you set $\\hat{y}_{ti}$ proportional to $1/r_{ti}$.** The apparent paradox here is that stocks with **larger** returns get **lower** \"portfolio\" weights. The confusing thing with this result is that the score here is like the Information Ratio of a theoretical portfolio with weights $\\propto \\hat{y}_{ti}$ and it seems odd that one would construct a portfolio where, all other things being equal, an asset with a higher return would get a lower weight.\n\n**In making model submissions for this competition therefore, should one choose  $\\hat{y}_{ti}$ proportional to the inverse of the model predicted confidence?** \n\nIn this kernel, I argue that the policy of setting the weights proportional to $1/r$ maximizes the *ex-post* Information Ratio, but setting the weight proportional to $1/\\hat{r}$, where $\\hat{r}$ is your model prediction at time t, is significantly sub-optimal to maximize the *ex-ante* Information Ratio. In other words, **NO**, don't do this!\n\nWhy is this?\n\n# Bias, Variance, and Noise\n\nFollowing the nice write-up [here](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), we assume a data generating process, $y = f(x) + \\epsilon$ where $y$ are the labels we have in training and what we wish to predict. When we model, we propose a model $\\hat{f}(x)$ to minimize some evaluation metric on the distance between $f(x)$ and $\\hat{f}(x)$. To do this we could minimize the MSE between these two. In out-of-sample prediction, we can decompose the **expected error** as:\n\n$$\\mathbb{E}[(y -  \\hat{f}(x))^2] = (\\text{Bias}[\\hat{f}(x)])^2 + \\text{Var}[\\hat{f}(x)] + \\sigma^{2}_{\\epsilon}$$\n\n\nImagine you have a “perfect” model. What though does \"perfect\" mean? **Perfect in the ML sense means you have a model with zero bias and zero variance and are left with just error due to irreducible noise.** It does not mean zero prediciton error. **There is always irreducible noise.**\n\n# The Two-Asset Case\n\nIn that context, imagine the two asset case. \n\nThe forecast is 0.025 and 0.075 and this is the ground truth but there is irreducible (say Gaussian) noise around each of 0.05 (and *wlog* the errors are not correlated).\n\n# The 1/r strategy\nLet's try the 1/r stategy for 20-days.\n"},{"metadata":{"trusted":true,"_uuid":"7d1ece2ad5b691fbe21a4087663de27a0c41ecf7"},"cell_type":"code","source":"import numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\nnp.random.seed(seed=100)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14ab2ecf64985cc9f5b0827e23baf43d7a524f6d"},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10, 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"313db23796842769d9bdd3121e5f822f53871604"},"cell_type":"code","source":"n_days = 20\nr_mean = np.array([0.025, 0.075])\nepsilon = np.array([0.05, 0.05])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8ef1aa2853bdf754edc93115024ea291e7c65c6"},"cell_type":"markdown","source":"We have a **perfect model** which knows `r_mean` precisely (and `epsilon`). \n\nTo test the initial policy, we set our weights == `1/r` for each day."},{"metadata":{"trusted":true,"_uuid":"8c330f461fba378dbd788049af709e8edcb6652a"},"cell_type":"code","source":"w_1 = np.repeat(1/r_mean[0], n_days)\nw_2 = np.repeat(1/r_mean[1], n_days)\n\nprint(w_1)\nprint(w_2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69811f14883904dc0c95ac197c28a6d23a4cdb91"},"cell_type":"markdown","source":"Now the universe unfolds and the realizations are made."},{"metadata":{"trusted":true,"_uuid":"e8bf50118f2acab59b7f4d133ba74902e99cc753"},"cell_type":"code","source":"r_1 = np.random.normal(loc=r_mean[0], scale=epsilon[0], size=n_days)\nr_2 = np.random.normal(loc=r_mean[1], scale=epsilon[1], size=n_days)\n\nplt.plot(r_1);\nplt.plot(r_2);\nplt.title('Single Realization of Generating Process');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ab1c941cd99dd91b9608018f4b35d8529a6d3b4"},"cell_type":"markdown","source":"The score as defined in the competition:"},{"metadata":{"trusted":true,"_uuid":"0ad4db7a9b27d3b92b2485503436ab2ee2184bf7"},"cell_type":"code","source":"def score(w1, w2, r1, r2):\n    x = w1*r1 + w2*r2\n    return np.mean(x)/np.std(x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c635b6d1ec7d148ab1770979a90f38801208b592"},"cell_type":"markdown","source":"**So what is the score of the `1/r` policy?**"},{"metadata":{"trusted":true,"_uuid":"771284ce6d22452de9891e48281d827e25806779"},"cell_type":"code","source":"score(w_1, w_2, r_1, r_2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f4b013a254ef85affe4640d1fe69750554bcd4a"},"cell_type":"markdown","source":"# Alternative: Single Period Optimization\n\nNow lets' try an alternative policy:\n\nGiven our perfect model knowledge of `r_mean` and `epsilon`, we will seek to explicitly maximize $\\mathbb{E} [ \\text{score}]$:\n$$\n\\begin{align}\n\\max_w \\quad& \\frac{w^\\prime \\hat{r}}{ \\sqrt{ w^\\prime \\Sigma w}}\\\\\n\\text{subject to} \\quad&-1 \\leq  w  \\leq 1 \\\\\n\\end{align}\n$$\n\nWhere $\\Sigma$ is the covariance matrix (in this case, just `np.diag(epsilon*epsilon)` since we assume zero correlation)."},{"metadata":{"trusted":true,"_uuid":"78a6fe30e8e7f55430502f9ddc3e0c08cbd78c40"},"cell_type":"code","source":"# the one-period expected two asset information ratio given weights, predictions, and noise\n# I make it negative because we are going to optimize to find the maximum, but scipy will only find the minimum,\n#  so we find the minimum of the negative to get the maximum\ndef information_ratio_2(w, y_hat, epsilon):\n    r = w[0]*y_hat[0] + w[1]*y_hat[1]\n    s = np.sqrt(w[0]*w[0]*epsilon[0]*epsilon[0] + w[1]*w[1]*epsilon[1]*epsilon[1])\n    return -r/s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aab10865378d857aa52d09ebc576c5a8c092f91c"},"cell_type":"code","source":"bounds = ((-1,1), (-1,1))\nres = minimize(\n    information_ratio_2,\n    np.array([1.0, 1.0]),\n    args=(r_mean, epsilon),\n    bounds=bounds, \n    method='SLSQP'\n)\nprint(res.x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"169553f53f0e1bd3f53fb667f29179bb6696eaa6"},"cell_type":"code","source":"w_optimal_1 = np.repeat(res.x[0], n_days)\nw_optimal_2 = np.repeat(res.x[1], n_days)\n\nprint(w_optimal_1)\nprint(w_optimal_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf7a1bd94e40a3e01ba655a5c12b1acadbb06e05"},"cell_type":"code","source":"score(w_optimal_1, w_optimal_2, r_1, r_2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"markdown","source":"The score of the `1/r` policy is 1.28 and the score of the alternative policy is 1.69. \nThis alternative policy beats `1/r`.  This is a contrived example, but the key is that in the optimal strategy, the **weights are directly proportional to the forecasts, not inversely proportional.**"},{"metadata":{"trusted":true,"_uuid":"e9124d42110a670363817cdbcfad67e6a2587015"},"cell_type":"markdown","source":"# The Expectation of the Optimal Policy\nSince we are dealing with random variables, perhaps I just set the seed to work out this way :-)... So let's see across many draws how things look."},{"metadata":{"trusted":true,"_uuid":"c3f5ad421b3ea82db8ed140cf648bf362e8ad8ba"},"cell_type":"code","source":"one_over_r = []\nalt = []\nex_post = []\nn_sims = 10000\n    \n# set weights for the 1/r policy\nw_1 = np.repeat(1/r_mean[0], n_days)\nw_2 = np.repeat(1/r_mean[1], n_days)\n\n# set weights for the alternative policy\nbounds = ((-1,1), (-1,1))\nres = minimize(\n    information_ratio_2,\n    np.array([1.0, 1.0]),\n    args=(r_mean, epsilon),\n    bounds=bounds, \n    method='SLSQP'\n)\nw_optimal_1 = np.repeat(res.x[0], n_days)\nw_optimal_2 = np.repeat(res.x[1], n_days)\n\nfor i in range(n_sims):\n    r_1 = np.random.normal(loc=r_mean[0], scale=epsilon[0], size=n_days)\n    r_2 = np.random.normal(loc=r_mean[1], scale=epsilon[1], size=n_days)\n\n    # run the 1/r weights\n    trial_score = score(w_1, w_2, r_1, r_2)\n    one_over_r.append(trial_score)\n    \n    # run the one-period optimal weights\n    trial_score = score(w_optimal_1, w_optimal_2, r_1, r_2)\n    alt.append(trial_score)\n    \n    # run the \"leak scenario\"; we know the actual realizations\n    w_1_expost = 1/r_1\n    w_2_expost = 1/r_2\n    trial_score = score(w_1_expost, w_2_expost, r_1, r_2)\n    ex_post.append(trial_score)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fee9d7bd6fb97b1d213cfe589e321c99e7d67bd1"},"cell_type":"code","source":"plt.hist(one_over_r, alpha=0.5);\nplt.hist(alt, alpha=0.5);\nplt.legend(['1/r', 'alternative']);\nplt.title('Comparison to the 1/r policy and Optimal Policy for %d Simulations' % n_sims);\nplt.xlabel('Score');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb76d3a2a93aaa2433de3b26efb17036372a9fd4"},"cell_type":"markdown","source":"So, **don't set your `confidenceValue` proportional to 1/predictions**. It is not optimal for ex-ante predictions (i.e., predictions in reality)."},{"metadata":{"_uuid":"807e0584353bc47dcfbf7e91fe724e3abdfa68ae"},"cell_type":"markdown","source":"Nowt if you knew *in hindsight the realizations*, then `1/r` is optimal..."},{"metadata":{"trusted":true,"_uuid":"b9fffea1f0aa3bc97407a8daa519efd0602c3cf3"},"cell_type":"code","source":"np.mean(ex_post)  # :-)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49295ab4b6fd591bceed93cb895caf3e41b7f537"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}