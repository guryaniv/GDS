{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,AveragePooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\nfrom keras.optimizers import Adam # I believe this is better optimizer for our case\nfrom keras.preprocessing.image import ImageDataGenerator # to augmenting our images for increasing accuracy\nfrom keras.utils.vis_utils import plot_model\nimport scipy\nfrom sklearn.model_selection import train_test_split # to split our train data into train and validation sets\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nnp.random.seed(13) # My lucky number","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"030b1ebf761aa450ce18b253813433ca5293c2fc","trusted":true},"cell_type":"code","source":"num_classes = 10 # We have 10 digits to identify\nbatch_size = 128 # Handle 128 pictures at each round\nepochs = 700 \nimg_rows, img_cols = 28, 28 # Image dimensions 28 pixels in height&width\ninput_shape = (img_rows, img_cols,1) # We'll use this while building layers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc20a3a5e6d491e98be7d0ff3aaa6eec7bdc123b","trusted":true},"cell_type":"code","source":"# Load some date to rock'n roll\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23e65adaf0ef381576c5597aebb6ce5dcc466174","trusted":true},"cell_type":"code","source":"# Drop the label from the data and move it to real label part\ny_train = train[\"label\"]\nx_train = train.drop(labels = [\"label\"],axis = 1 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc3b21df164c213db201b317ba453c8914dd2200","trusted":true},"cell_type":"code","source":"# Normalize both sets\nx_train /= 255\ntest /= 255","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"922cb2c73388a6ec4b368b032f133b2c4c9783c6","trusted":true},"cell_type":"code","source":"print(x_train.shape[0], 'train samples')\nprint(test.shape[0], 'test samples')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ac625840bb33a39580405985955d9076c003d3c","trusted":true},"cell_type":"code","source":"# Images should be in shape of height,width and color channel so it will be 28x28x1\nx_train = x_train.values.reshape(-1,img_rows,img_cols,1).astype('float32')\ntest = test.values.reshape(-1,img_rows,img_cols,1).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"057065fd2bf1e11bdb9885b746c27452ee21505c","trusted":true},"cell_type":"code","source":"# Class vectors needs to be binary so we use \"to_catogorical\" function of keras utilities for one-hot-encoding\ny_train = keras.utils.to_categorical(y_train, num_classes = num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"412ce7ad208189b6f5ff03837f22358b683971c8"},"cell_type":"code","source":"# Lets split our train set into train and validation test sets with my lucky number 13 :)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e271e0d37535c7c401a873578ad3dd19b51b133","trusted":true},"cell_type":"code","source":"def model_cnn(input_shape=input_shape, num_classes=num_classes):   \n    model = Sequential()\n\n    # Add convolutional layer consisting of 32 filters and shape of 3x3 with ReLU activation\n    # We want to preserve more information for following layers so we use padding\n    # 'Same' padding tries to pad evenly left and right, \n    # but if the amount of columns to be added is odd, it will add the extra column to the right\n    model.add(Conv2D(32, kernel_size = (3,3), activation='relu', input_shape = input_shape))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = (3,3), activation='relu'))\n    model.add(BatchNormalization())\n\n    # Add convolutional layer consisting of 32 filters and shape of 5x5 with ReLU activation\n    # We give strides=2 for space between each sample on the pixel grid\n    model.add(Conv2D(32, kernel_size = (5,5), strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    # Dropping %40 of neurons\n    model.add(Dropout(0.4))\n    \n    model.add(Conv2D(64, kernel_size = (3,3), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = (3,3), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = (5,5), strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model.add(BatchNormalization())\n    # To be able to merge into fully connected layer we have to flatten\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    # Lets add softmax activated neurons as much as number of classes\n    model.add(Dense(num_classes, activation = \"softmax\"))\n    # Compile the model with loss and metrics\n    model.compile(optimizer =  Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3b33a030f086dcc7469773c0482b1fee0ceccd8"},"cell_type":"code","source":"def LeNet5(input_shape=input_shape,num_classes=num_classes):\n    model = Sequential()\n    model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape, padding=\"same\"))\n    model.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n    model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n    model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n    model.add(Flatten())\n    model.add(Dense(84, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer =  Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02093afb55f3706cd0cee60ae934b68ccd5bd1f9"},"cell_type":"code","source":"print(\"My Custom CNN Network:\")\nplot_model(model_cnn(), to_file='custom-cnn.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b697658f251681da1aac55d9261b118be7d301b6"},"cell_type":"markdown","source":"<img src=\"custom-cnn.png\">"},{"metadata":{"trusted":true,"_uuid":"02093afb55f3706cd0cee60ae934b68ccd5bd1f9"},"cell_type":"code","source":"print(\"Master Yann LeCun's LeNet-5 Network:\")\nplot_model(LeNet5(), to_file='lenet-5.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58ae6f476c947df6e94bdce5ef35ba65b72e278a"},"cell_type":"markdown","source":"<img src=\"lenet-5.png\">"},{"metadata":{"trusted":true,"_uuid":"f9a562361c9a42940bb22e9e41f3889c011f61f2"},"cell_type":"code","source":"model = []\nmodel.append(model_cnn())\nmodel.append(LeNet5())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b5468b524407441d9731b87e97bb010b7cded69","trusted":true},"cell_type":"code","source":"# Generate batches of tensor image data with real-time data augmentation more detail: https://keras.io/preprocessing/image/\ndatagen = ImageDataGenerator(rotation_range=10, zoom_range = 0.1, width_shift_range=0.1, height_shift_range=0.1)\ndatagen.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"a6a4c747aac8cb06afa5f8be4f2c344600b12c46"},"cell_type":"code","source":"# Start multiple model training with the batch size\nmodels = []\nfor i in range(len(model)):\n    model[i].fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                                        epochs = epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n                                        validation_data = (x_test,y_test), \n                                        callbacks=[ReduceLROnPlateau(monitor='loss', patience=3, factor=0.1)], \n                                        verbose=2)\n    models.append(model[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b6fb41e9cc272eecab5b9a2c3394ef1aeb3ad09"},"cell_type":"code","source":"# Predict labels with models\nlabels = []\nfor m in models:\n    predicts = np.argmax(m.predict(test), axis=1)\n    labels.append(predicts)\n    \n# Ensemble with voting\nlabels = np.array(labels)\nlabels = np.transpose(labels, (1, 0))\nlabels = scipy.stats.mode(labels, axis=-1)[0]\nlabels = np.squeeze(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3505412d1bd1dc69df81edf60314e7bbbc266f39"},"cell_type":"code","source":"# Dump predictions into submission file\npd.DataFrame({'ImageId' : np.arange(1, predicts.shape[0] + 1), 'Label' : labels }).to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}