{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Utilities from kaggle kernels\n# Instead of data = pd.read_csv(\"../input/train_V2.csv\")\n# We use : data = read_fast(\"../input/train_V2.csv\")\nimport random\nimport time\n\ndef reduce_mem_usage_func(df):\n    \"\"\" Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n        iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef get_sampled_data(filename, sample_percent):\n    n = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)\n    print('total records in dataset: %s' % n)\n    sample_size = int((sample_percent * n) / 100)\n    print('will select %s percent (%s) sample records randomly' % (sample_percent, sample_size))\n    skip = sorted(random.sample(range(1,n+1),n-sample_size)) #the 0-indexed header will not be included in the skip list\n    df = pd.read_csv(filename, skiprows=skip)\n    return df\n\n\ndef read_fast(filename, sample=True, sample_percent=20, reduce_mem_usage=True):\n    start_time = time.time()\n    df = get_sampled_data(filename, sample_percent) if sample else pd.read_csv(filename)\n    new_df = reduce_mem_usage_func(df) if reduce_mem_usage else df\n    elapsed_time = int(time.time() - start_time)\n    print('Time to get data frame: {:02d}:{:02d}:{:02d}'.format(\n               elapsed_time // 3600,\n               (elapsed_time % 3600 // 60),\n               elapsed_time % 60))\n    return new_df\n\n\ndef get_datasets(input_path, num, sample_percent=20, sample=True):\n    datasets = []\n    if sample:\n        for i in range(1, num + 1):\n            print('generating sampled dataset for num : %s' % i)\n            datasets.append(read_fast(input_path, sample_percent=sample_percent))\n    else:\n        datasets.append(read_fast(input_path, sample = False))\n    return datasets\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bc905ada90af4e0d799da697bfda7366b5dc025"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n% matplotlib inline\nimport  plotly.plotly as py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"941d430c2a375d5b528763ab9fb942c7bd43a09c"},"cell_type":"code","source":"data = read_fast(\"../input/train_V2.csv\",sample_percent=30 )\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ccd5ca3ef836d7eabe48b1cffd91ad15432c76b"},"cell_type":"code","source":"test_data = read_fast(\"../input/test_V2.csv\", sample = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ab26e5daa8118d609cd97ac62af56f3b6540e6b"},"cell_type":"markdown","source":"Feature Engineering "},{"metadata":{"trusted":true,"_uuid":"c668fc0d11c280809e0c170fd8eba3494dff24af"},"cell_type":"code","source":"def dostuff(data):\n    data['headshotrate'] = data['kills']/data['headshotKills']\n    data['killStreakrate'] = data['killStreaks']/data['kills']\n    data['healthitems'] = data['heals'] + data['boosts']\n    data['totalDistance'] = data['rideDistance'] + data[\"walkDistance\"] + data[\"swimDistance\"]\n    data['killPlace_over_maxPlace'] = data['killPlace'] / data['maxPlace']\n    data['headshotKills_over_kills'] = data['headshotKills'] / data['kills']\n    data['distance_over_weapons'] = data['totalDistance'] / data['weaponsAcquired']\n    data['walkDistance_over_heals'] = data['walkDistance'] / data['heals']\n    data['walkDistance_over_kills'] = data['walkDistance'] / data['kills']\n    data['killsPerWalkDistance'] = data['kills'] / data['walkDistance']\n    data[\"kill_skill\"] = data[\"headshotKills\"] + data[\"roadKills\"]\n    data['killsWithoutMoving'] = ((data['kills'] > 0) & (data['totalDistance'] == 0))\n    return data\n\ndata= dostuff(data)\ntest_data = dostuff(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9dd82d518c201aaa7c64734e66375bacf6b05e3"},"cell_type":"code","source":"#test_data['headshotrate'] = test_data['kills']/test_data['headshotKills']\n#test_data['killStreakrate'] = test_data['killStreaks']/test_data['kills']\n#test_data['healthitems'] = test_data['heals'] + test_data['boosts']\n#test_data['totalDistance'] = test_data['rideDistance'] + test_data[\"walkDistance\"] + test_data[\"swimDistance\"]\n#test_data['killPlace_over_maxPlace'] = test_data['killPlace'] / test_data['maxPlace']\n#test_data['headshotKills_over_kills'] = test_data['headshotKills'] / test_data['kills']\n#test_data['distance_over_weapons'] = test_data['totalDistance'] / test_data['weaponsAcquired']\n#test_data['walkDistance_over_heals'] = test_data['walkDistance'] / test_data['heals']\n#test_data['walkDistance_over_kills'] = test_data['walkDistance'] / test_data['kills']\n#test_data['killsPerWalkDistance'] = test_data['kills'] / test_data['walkDistance']\n#test_data[\"kill_skill\"] = test_data[\"headshotKills\"] + test_data[\"roadKills\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b0968d3778ea0b7d3b60b2eb087c9cd54457cc9"},"cell_type":"markdown","source":"**Killing without moving**\nchecking if people are getting kills without moving.\n"},{"metadata":{"trusted":true,"_uuid":"299c132e7ad004cb39a49da7aa67ff18bfafeaec"},"cell_type":"code","source":"\n#test_data['killsWithoutMoving'] = ((test_data['kills'] > 0) & (test_data['totalDistance'] == 0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33fd624b8a9dda0d0e5537bc2ccb1743871789ed"},"cell_type":"markdown","source":"**Outlier Detection **\n"},{"metadata":{"trusted":true,"_uuid":"2f1ef8232f2f6c67422fe56d42509c3114e0aea4"},"cell_type":"code","source":"# Remove outliers\ndata.drop(data[data['killsWithoutMoving'] == True].index, inplace=True)\n#data2.drop(data2[data2['killsWithoutMoving'] == True].index, inplace=True)\n# Players who got more than 10 roadKills\ndata.drop(data[data['roadKills'] > 10].index, inplace=True)\n#data2.drop(data2[data2['roadKills'] > 10].index, inplace=True)\n\ndata.drop(data[data['kills'] > 30].index, inplace=True)\n#data2.drop(data2[data2['kills'] > 30].index, inplace=True)\n\ndata.drop(data[data['longestKill'] >= 1000].index, inplace=True)\n#data2.drop(data2[data2['longestKill'] >= 1000].index, inplace=True)\n\ndata.drop(data[data['heals'] >= 40].index, inplace=True)\n#data2.drop(data2[data2['heals'] >= 40].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42b3c81736e9de1daa9b70b399757eb913af2eb4"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,15)) \nsns.heatmap(data1.corr(), cmap ='RdBu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf60256fcf6075d3a08b1b8e22bae00f96b31b83"},"cell_type":"code","source":"correlations = data.corr().abs()\ncorrelations = correlations[\"winPlacePerc\"].sort_values(ascending=False)\nfeatures = correlations.index[1:6]\ncorrelations.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e79e3ed61953e4d4c1ac6c6988cd32a7d67056e"},"cell_type":"code","source":"#'winPlacePerc' CorrelationMatrix\ncorrmat = data1.corr().abs()\nk = 10\ncols = corrmat.nlargest(k , 'winPlacePerc')['winPlacePerc'].index\ncm = np.corrcoef(data[cols].values.T)\nfig, ax = plt.subplots(figsize=(10,10))  \nhm = sns.heatmap(cm ,annot=True, cmap = \"RdBu\",cbar = True,square = True,\n                 yticklabels = cols.values, xticklabels = cols.values, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28fdc21ed26c00b7cb54997b6d0b175bf9788c61"},"cell_type":"markdown","source":"**POSITIVE CORRELATION:**\nIf an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\n**NEGATIVE CORRELATION: **\nIf an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation."},{"metadata":{"_uuid":"b0d5c353ea26698def542c020d975b8b56961a3d"},"cell_type":"markdown","source":"Working with numeric features"},{"metadata":{"trusted":true,"_uuid":"6984055c3121f9799d78553381b03c10169b7949"},"cell_type":"code","source":"numeric_features = data.select_dtypes(include=[np.number])\nnumeric_features.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da19fe3e7c608fa0039f7d79dd8d1174b683a899"},"cell_type":"code","source":"corr = numeric_features.corr()\n\nprint (corr['winPlacePerc'].sort_values(ascending=False)[:5], '\\n')\nprint (corr['winPlacePerc'].sort_values(ascending=False)[-5:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20222a5dda1417b13b1f3632a8c0385b687f6027"},"cell_type":"code","source":"# Missing Data\ntotal = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total,percent], axis = 1, keys = ['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16c3fb1846d61eff5707ee8c11437e9319c81329"},"cell_type":"code","source":"categoricals = data.select_dtypes(exclude=[np.number])\ncategoricals.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3ab61c97e9bdb498f9ae17f357f023a621e02c3"},"cell_type":"code","source":"#data1.drop(2744604, inplace =True)\n#data2.drop(2744604, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86ac2857ad6d5f495f59e73b859d3888bc97df6a"},"cell_type":"code","source":"#data1 = data1.fillna(data1.mean())\ndata= data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3b1ae08169237fabe6b9c6cf57d149700fd71ec"},"cell_type":"code","source":"#data = data.fillna(data.mean())\n#data2 = data2.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ef4398d8fa657930c5cf3a249c0e5cd227f5ee3"},"cell_type":"code","source":"#data = pd.concat([data1, data2], ignore_index=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"428bede642596b75f756effa8d977c0eed936331"},"cell_type":"code","source":"data = data.drop(columns=['groupId','matchId' ,'matchType'], axis = 1)\ntest_data = test_data.drop(columns=['groupId','matchId' ,'matchType'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9857d53b62c74aeeabd51719b75a818c4343e083"},"cell_type":"code","source":"y = data.winPlacePerc\nX = data.drop(['winPlacePerc', 'Id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c083498e37eec462d4963d29e0e6b19f5cc8e2c1"},"cell_type":"code","source":"y[y < 0] = 0\ny[y >1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0908382c3a265001b7f97f8d103af6b3251c974c"},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbf544b96e0bc2674784650b48efa2a594d9bf7c"},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\ndef identify_zero_importance_features(X, y, iterations = 2):\n    \"\"\"\n    Identify zero importance features in a training dataset based on the \n    feature importances from a gradient boosting model. \n    \n    Parameters\n    --------\n    train : dataframe\n        Training features\n        \n    train_labels : np.array\n        Labels for training data\n        \n    iterations : integer, default = 2\n        Number of cross validation splits to use for determining feature importances\n    \"\"\"\n    \n    # Initialize an empty array to hold feature importances\n    feature_importances = np.zeros(X.shape[1])\n\n    # Create the model with several hyperparameters\n    model = lgb.LGBMRegressor(objective='regression', boosting_type = 'goss', \n                               n_estimators =6000, class_weight = 'balanced')\n    \n    # Fit the model multiple times to avoid overfitting\n    for i in range(iterations):\n\n        # Split into training and validation set\n        train_features, valid_features, train_y, valid_y = train_test_split(X, y, \n                                                                            test_size = 0.25, \n                                                                            random_state = i)\n\n        # Train using early stopping\n        model.fit(train_features, train_y, early_stopping_rounds=100, \n                  eval_set = [(valid_features, valid_y)])\n\n        # Record the feature importances\n        feature_importances += model.feature_importances_ / iterations\n    \n    feature_importances = pd.DataFrame({'feature': list(X.columns), \n                            'importance': feature_importances}).sort_values('importance', \n                                                                            ascending = False)\n    \n    # Find the features with zero importance\n    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n    \n    return zero_features, feature_importances\n\nzero_features, feature_importances = identify_zero_importance_features(X, y, iterations = 2)\nprint('zero_features:',zero_features)\nprint('feature_importances : ', feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"353f8ae081af30c2953649c624550acff1a004ed"},"cell_type":"code","source":"feature_importances.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a023e865154c0895aafe18b972909bea835ff5f5"},"cell_type":"code","source":"pp =np.percentile(feature_importances['importance'], 25) \nprint(pp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ea5adaa6effae507ffc6a3524f1b183a7fdd025"},"cell_type":"code","source":"to_drop = feature_importances[feature_importances['importance'] <= pp]['feature']\nX = X.drop(columns = to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f873431204a062b79ef181fac7ea910f8fce2f6"},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be7b5fcc47dc734ec922a62d1df83b1de711ac66"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"630007e36bdb0ea379cace71489088fa5a0e596f"},"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_squared_error\nfrom lightgbm import LGBMRegressor\n\ngbm = LGBMRegressor(objective='regression',\n                              num_leaves=40,\n                              learning_rate=0.05, \n                              n_estimators=20000,\n                              max_bin=55, \n                              bagging_fraction=0.7,\n                              bagging_freq=9, \n                              feature_fraction=0.7,\n                              feature_fraction_seed=9, \n                              bagging_seed=10,\n                              min_data_in_leaf=7, \n                              min_sum_hessian_in_leaf=5)\ngbm.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric='rmsle',\n        early_stopping_rounds=100)\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\nprint('The accuracy of the lgbm Regressor is',r2_score(y_test,y_pred))\nprint ('RMSE is: \\n', mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3e3aafe8951f30f101bb6b1d761aeb6a481900d"},"cell_type":"code","source":"feats = test_data.drop(['Id'], axis=1)\n\nfeats = feats[X_train.columns]\nfinal_preds = gbm.predict(feats,num_iteration=gbm.best_iteration_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0054ff70ef367f679cc85ad7a6e6d8c56eeb94ec"},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = test_data.Id\nsubmission['winPlacePerc'] = final_preds \nsubmission.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2d2a909c145318ac0828a0b5b1247e7ba76a999"},"cell_type":"code","source":"submission.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e411d3c4e7c7e2b2a25e77844f133f0ec4bf3a0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}