{"cells":[{"metadata":{"_uuid":"a1a770b241f839f29c62fba25b1327ff399fd48f"},"cell_type":"markdown","source":"# A cursory attempt to compare Logistic Regression vs Neural Network #\n\nBelow we will compare the train, validation and test errors using logistic regression vs neural network. As we will see, logistic regression produces an unbiased AUC estimate of ~75% while the neural network comes in around 80%. The NN has significantly lower cost, but only slightly improved accuracy. Overfitting?"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport math\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.framework import ops\nfrom sklearn import metrics\nimport os\nimport random\n\nnp.random.seed(1)\n\n# Input data files are available in the \"../input/\" directory.\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cef95530557ebb79e5f87c35fcac6d203b45d6a"},"cell_type":"markdown","source":"# Exploratory Data Analysis #\n\nLet's first take a look at the data and normalize it as needed.\n\nWe can also add new features to the existing dataset. Notice that 1s only occur ~10% of the time. Anomaly detection with an appropriate distribution may be worth a try."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ec800ee71df14f4af40ea2cf7cb039e0527cc2d"},"cell_type":"code","source":"print(f\"Train shape:{train.shape}\")\n\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68aaf9f293ec5b54e0334d6051097af43bd5cf42"},"cell_type":"code","source":"train = train.drop(columns=['ID_code'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9224598cb17040d26dda70c8b23943fde5a5b4fd"},"cell_type":"code","source":"#add new feature: squared-values\nfor col in train.columns:\n    if col != 'target':\n        train[col + '_sq'] = train[col]**2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33c49c9befb0398ca195e063c845876af83e22f8"},"cell_type":"code","source":"def normalize_inputs(inputs, everything=False):\n    if everything:\n        X = inputs\n    else:\n        X = inputs[:, 1:]\n        \n    mu = np.mean(X, axis=0).reshape(1,X.shape[1])\n    sigma = np.std(X, axis=0).reshape(1,X.shape[1])\n    \n    X = (X-mu)/sigma\n    \n    if everything:\n        return X\n    else:\n        return np.concatenate((inputs[:, 0].reshape(inputs.shape[0],1), X), axis=1)\n\ndef split_dataset(dataset, pct_train=0.6, pct_dev=0.2, pct_test=0.2):\n    (m, n) = dataset.shape\n    np.random.shuffle(dataset)\n    \n    if (pct_train + pct_dev + pct_test) != 1.0:\n        pass #need to sum to 1\n    elif pct_train <= 0.0:\n        pass # cannot have 0% percent to train\n    elif pct_dev <= 0.0 or pct_test <= 0.0:       #if pct_test or pct_dev is 0, call the other dataset \"test\" and return both train & test\n        train = dataset[:int(m*pct_train+1),:]    #if both pct_test and pct_dev is 0, return the empty dataset \"test\"\n        test = dataset[int(m*pct_train+1):,:]\n\n        X_train = train[:, 1:]\n        Y_train = train[:, 0].reshape(train.shape[0], 1)\n\n        X_test = test[:, 1:]\n        Y_test = test[:, 0].reshape(test.shape[0], 1)\n    \n        return X_train, Y_train, X_test, Y_test\n    else:                                         #if neither pct_test nor pct_dev is 0, return 3 datasets\n        train = dataset[:int(m*pct_train+1),:]\n        dev = dataset[int(m*pct_train+1):int(m*(pct_train+pct_dev)+1),:]\n        test = dataset[int(m*(pct_train+pct_dev)+1):,:]\n        \n        X_train = train[:, 1:]\n        Y_train = train[:, 0].reshape(train.shape[0], 1)\n        \n        X_dev = dev[:, 1:]\n        Y_dev = dev[:, 0].reshape(dev.shape[0], 1)\n        \n        X_test = test[:, 1:]\n        Y_test = test[:, 0].reshape(test.shape[0], 1)\n        \n        return X_train, Y_train, X_dev, Y_dev, X_test, Y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1499dae2fee9189c390bc6334df54b7d6ac9a45"},"cell_type":"code","source":"train = train.values\ntrain = normalize_inputs(train)\n(X_train, Y_train, X_dev, Y_dev, X_test, Y_test) = split_dataset(train, pct_train=0.7, pct_dev=0.15, pct_test=0.15)\n(X_train, Y_train, X_dev, Y_dev, X_test, Y_test) = (X_train.T, Y_train.T, X_dev.T, Y_dev.T, X_test.T, Y_test.T) #X is (n,m) matrix; Y is (1,m) matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1eabb6ba07f65804afa05a4e37e317380ead178"},"cell_type":"markdown","source":"# Logistic Regression #\n\nLet's first try to run a simple logistic regression to see how a simple model performs"},{"metadata":{"trusted":true,"_uuid":"4421417d9b4095cdf1bbdcf899faa0bc3f1b20f6"},"cell_type":"code","source":"def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef lr_cost(theta, X,  Y):\n    (n,m) = X_train.shape\n    h = sigmoid(np.dot(theta.T, X)) #h is (1,m) vector\n    cost = -1/m * (np.dot(Y, np.log(h).T) + np.dot((1-Y), np.log(1-h).T)) #scalar\n    grad = 1/m * np.dot(X_train, (h - Y_train).T)\n    \n    return cost, grad\n\ndef lr_predict(theta, X):\n    p = sigmoid(np.dot(theta.T, X))\n    \n    return np.round(p)\n    \ndef lr_model(X_train, Y_train, X_test, Y_test, learning_rate=1, num_iter=20):\n    (n,m) = X_train.shape\n    theta = np.zeros((n,1)) # theta is (n,1) matrix\n    costs = []   \n\n    for i in range(num_iter): # gradient descent\n        cost, grad = lr_cost(theta, X_train, Y_train)\n        theta = theta - learning_rate * grad\n        costs.append(cost)\n        if i % 10 == 0:\n            print(f\"Cost after {i} iterations: {np.squeeze(cost)}\")\n\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n\n    prediction = lr_predict(theta, X_train)\n    accuracy = metrics.roc_auc_score(Y_train.T, prediction.T)\n    print(f\"Train accuracy: {accuracy}\")\n\n    prediction = lr_predict(theta, X_test)\n    accuracy = metrics.roc_auc_score(Y_test.T, prediction.T)\n    print(f\"Test accuracy: {accuracy}\")\n    \n    return theta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efc0dd57d59700098b439fccd379b718ae81c625"},"cell_type":"code","source":"weights = lr_model(X_train, Y_train, X_dev, Y_dev)\npredictions = lr_predict(weights, X_test)\nunbiased_auc = metrics.roc_auc_score(Y_test.T, predictions.T)\nprint(f\"Unbiased AUC estimate for LR: {unbiased_auc}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17de7039931fb96f28d7e31ba0f6a2d1a29f54a1"},"cell_type":"markdown","source":"# Neural Network #\n\nLet's try a 4-layer neural network with ReLU hidden activations and final Sigmoid activation."},{"metadata":{"trusted":true,"_uuid":"ddb6486be34027a4466ca2442bd24689fc146a52"},"cell_type":"code","source":"def random_mini_batches(X, Y, mini_batch_size = 32, seed=1):\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # randomly mix (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    # partition (shuffled_X, shuffled_Y), minus the end case\n    num_complete_minibatches = math.floor(m/mini_batch_size) \n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\ndef create_placeholders(n):\n    X = tf.placeholder(tf.float32, shape=[n, None], name='X')\n    Y = tf.placeholder(tf.float32, shape=[1, None], name='Y')\n    \n    return X, Y\n\ndef initialize_parameters(n):    \n    tf.set_random_seed(1)\n    \n    l1 = 256                     # Hyperparameter: no of hidden units for each layer\n    l2 = 128\n    l3 = 64\n    \n    W1 = tf.get_variable(\"W1\", [l1,n], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b1 = tf.get_variable(\"b1\", [l1,1], initializer = tf.zeros_initializer())\n    W2 = tf.get_variable(\"W2\", [l2,l1], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b2 = tf.get_variable(\"b2\", [l2,1], initializer = tf.zeros_initializer())\n    W3 = tf.get_variable(\"W3\", [l3,l2], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b3 = tf.get_variable(\"b3\", [l3,1], initializer = tf.zeros_initializer())\n    W4 = tf.get_variable(\"W4\", [1,l3], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b4 = tf.get_variable(\"b4\", [1,1], initializer = tf.zeros_initializer())\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W3\": W3,\n                  \"b3\": b3,\n                  \"W4\": W4,\n                  \"b4\": b4}\n    \n    return parameters\n\ndef forward_propagation(X, parameters, dropout_rate=0):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    W4 = parameters['W4']\n    b4 = parameters['b4']\n    \n    Z1 = tf.add(tf.matmul(W1, X), b1)\n    A1 = tf.nn.relu(Z1)\n    A1_dropout = tf.nn.dropout(A1, keep_prob=(1-dropout_rate))      # inverted dropout\n    \n    Z2 = tf.add(tf.matmul(W2, A1_dropout), b2)\n    A2 = tf.nn.relu(Z2)\n    A2_dropout = tf.nn.dropout(A2, keep_prob=(1-dropout_rate))\n    \n    Z3 = tf.add(tf.matmul(W3, A2_dropout), b3)\n    A3 = tf.nn.relu(Z3)\n    A3_dropout = tf.nn.dropout(A3, keep_prob=(1-dropout_rate))\n    \n    Z4 = tf.add(tf.matmul(W4, A3_dropout), b4)\n        \n    return Z4\n\ndef compute_cost(Z4, Y, parameters, lambd=0.01):\n    logits = tf.transpose(Z4) # inputs of tf.nn.sigmoid_cross_entropy need to be shape (m, 1)\n    labels = tf.transpose(Y)\n    \n    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n    reg_cost = tf.nn.l2_loss(parameters['W1']) + tf.nn.l2_loss(parameters['W2']) + tf.nn.l2_loss(parameters['W3']) + tf.nn.l2_loss(parameters['W4'])\n    cost = tf.reduce_mean(cost + lambd * reg_cost)\n    \n    return cost\n    \ndef model(X_train, Y_train, X_test, Y_test, learning_rate = 0.01, lambd = 0.01, dropout_rate = 0, num_epochs = 10, minibatch_size = 32, to_print = True):\n    tf.set_random_seed(1)\n    seed = 3\n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    (n, m) = X_train.shape\n    costs = []\n    train_aucs = []\n    test_aucs = []\n    \n    X,Y = create_placeholders(n)\n    parameters = initialize_parameters(n)\n    Z4 = forward_propagation(X, parameters, dropout_rate)\n    cost = compute_cost(Z4, Y, parameters, lambd)\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    init = tf.global_variables_initializer()\n    \n    # Compute AUC for evaluation\n    prediction = tf.cast(tf.nn.sigmoid(Z4) > 0.5, \"float\") # Prediction decision boundary value\n    auc = tf.metrics.auc(Y, prediction)[1]\n\n    with tf.Session() as sess:\n        sess.run(init)       \n\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.\n            seed += 1\n            num_minibatches = int(m / minibatch_size) \n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n                (minibatch_X, minibatch_Y) = minibatch\n                \n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                \n                epoch_cost += minibatch_cost / num_minibatches\n\n            costs.append(epoch_cost) #track costs for each epoch\n            sess.run(tf.local_variables_initializer()) #need local variables for auc\n            train_aucs.append(sess.run(auc, feed_dict={X: X_train, Y: Y_train}))\n            test_aucs.append(sess.run(auc, feed_dict={X: X_test, Y: Y_test}))\n                \n            if to_print == True and epoch % 10 == 0:\n                print (f\"Cost after epoch {epoch}: {epoch_cost}\")\n                \n        # at this point, model is done training; run variables to return\n        sess.run(tf.local_variables_initializer()) #need local variables for auc        \n        parameters = sess.run(parameters)\n        train_auc = sess.run(auc, feed_dict={X: X_train, Y: Y_train})\n        test_auc = sess.run(auc, feed_dict={X: X_test, Y: Y_test})\n        aucs = (train_auc, test_auc)\n        \n        if to_print == True:\n            # plot the cost\n            plt.plot(np.squeeze(costs))\n            plt.ylabel('cost')\n            plt.xlabel('iterations')\n            plt.title(\"Learning rate =\" + str(learning_rate))\n            plt.show()\n        \n            print (\"Parameters have been trained!\")\n        \n            # plot the accuracy\n            plt.plot(train_aucs, linestyle='dashed') #train accuracy is dashed\n            plt.plot(test_aucs)\n            plt.xlabel('iterations')\n            plt.title('Train (dashed) vs. Test AUC')\n            plt.show()       \n        \n            print(f\"Train AUC: {train_auc}\")\n            print(f\"Test AUC: {test_auc}\")\n                \n        return parameters, aucs\n    \ndef predict(X, parameters):\n    \n    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n    W4 = tf.convert_to_tensor(parameters[\"W4\"])\n    b4 = tf.convert_to_tensor(parameters[\"b4\"])\n    \n    params = {\"W1\": W1,\n              \"b1\": b1,\n              \"W2\": W2,\n              \"b2\": b2,\n              \"W3\": W3,\n              \"b3\": b3,\n              \"W4\": W4,\n              \"b4\": b4}\n    \n    n = X.shape[0]\n    x = tf.placeholder(tf.float32, shape=[n, None], name='X')\n    \n    z4 = forward_propagation(x, params)\n    p = tf.nn.sigmoid(z4)\n    \n    sess = tf.Session()\n    prediction = sess.run(p, feed_dict = {x: X})\n        \n    return prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2854d37470ee1a51425e08add770de625c79053f"},"cell_type":"code","source":"parameters, auc = model(X_train, Y_train, X_dev, Y_dev, learning_rate=0.00015, lambd=0, dropout_rate=0, num_epochs=20, minibatch_size=128)\n# compute AUC for test dataset\npredictions = predict(X_test, parameters)\nunbiased_auc = metrics.roc_auc_score(Y_test.T, predictions.T)\nprint(f\"Unbiased AUC estimate for NN: {unbiased_auc}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"667e76beda7be2f6cc11d3c90a117f389732d457"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}