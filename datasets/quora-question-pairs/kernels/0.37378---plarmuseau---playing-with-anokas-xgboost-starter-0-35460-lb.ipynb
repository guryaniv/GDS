{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3c6f97d6-bab0-0096-c643-b39af050e548"
      },
      "source": [
        "Fork our little genius Anokas\n",
        "---\n",
        "throwing away overhead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d9f5b5bf-b8a8-a88a-a16c-7666be39bd7e"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "from textblob import TextBlob as tb\n",
        "import time\n",
        "start = time.clock()\n",
        "\n",
        "print('# File sizes')\n",
        "for f in os.listdir('../input'):\n",
        "    if 'zip' not in f:\n",
        "        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cba92fbf-43e0-16a3-695d-603fa08bcf50"
      },
      "source": [
        "adding ,encoding='utf8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f2fcbec-8b24-2bec-2ab7-e0491168cf55"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d6729ec-11d8-1248-462c-575b13e1b984"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0ac4cab6-214b-957b-bb2a-01f7e8d5ed2b"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_train = pd.read_csv('../input/train.csv',encoding='utf8')[:10000]\n",
        "df_train = df_train.fillna('leeg')\n",
        "df_test = pd.read_csv('../input/test.csv',encoding='utf8')[:50000]\n",
        "df_test = df_test.fillna('leeg')\n",
        "df_train.head(2)\n",
        "df_test.head(2)\n",
        "end = time.clock()\n",
        "print('open:',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d0c734ff-8bd9-eb9b-8148-59670a9638b3"
      },
      "outputs": [],
      "source": [
        "import rake\n",
        "import operator\n",
        "stop = set(stopwords.words('english'))\n",
        "rake_object = rake.Rake(stop, 3, 3, 1)\n",
        " \n",
        "text = \"Natural language processing (NLP) deals with the application of computational models to text or speech data. Application areas within NLP include automatic (machine) translation between languages; dialogue systems, which allow a human to interact with a machine using natural language; and information extraction, where the goal is to transform unstructured text into structured (database) representations that can be searched and browsed in flexible ways. NLP technologies are having a dramatic impact on the way people interact with computers, on the way people interact with each other through the use of language, and on the way people access the vast amount of linguistic data now in electronic form. From a scientific viewpoint, NLP involves fundamental questions of how to structure formal models (for example statistical models) of natural language phenomena, and of how to design algorithms that implement these models. In this course you will study mathematical and computational models of language, and the application of these models to key problems in natural language processing. The course has a focus on machine learning methods, which are widely used in modern NLP systems: we will cover formalisms such as hidden Markov models, probabilistic context-free grammars, log-linear models, and statistical models for machine translation. The curriculum closely follows a course currently taught by Professor Collins at Columbia University, and previously taught at MIT.\"\n",
        " \n",
        "keywords = rake_object.run(text)\n",
        "print( \"keywords: \", keywords )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ef13480a-2522-2d1f-cf8a-56670e0856ab"
      },
      "outputs": [],
      "source": [
        "def cleantxt(x):    \n",
        "    x = str(x)\n",
        "    #x = x.replace(r'[^\\x00-\\x7f]',r' ') \n",
        "    # Pad punctuation with spaces on both sides\n",
        "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
        "        #x = x.replace(char, ' '+char+' ')\n",
        "        x = x.replace(char, ' ')\n",
        "    return x\n",
        "\n",
        "def cleantxtsplit(x):\n",
        "    x=cleantxt(x)\n",
        "    return x.split()\n",
        "\n",
        "def word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in cleantxtsplit(row['question1']):\n",
        "            q1words[word] = 1\n",
        "    for word in cleantxtsplit(row['question2']):\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    nonshared_words_in_q1 = [w for w in q1words.keys() if w not in q2words]\n",
        "    nonshared_words_in_q2 = [w for w in q2words.keys() if w not in q1words]\n",
        "    \n",
        "    #X = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
        "    #Y = (len(nonshared_words_in_q1) + len(nonshared_words_in_q2))/(len(q1words) + len(q2words))\n",
        "    X1 = (len(shared_words_in_q1))/(len(q1words) )\n",
        "    Y1 = (len(nonshared_words_in_q1))/(len(q1words) )\n",
        "    X2 = (len(shared_words_in_q2))/(len(q2words))\n",
        "    Y2 = (len(nonshared_words_in_q2))/(len(q2words))\n",
        "    #R1= math.atan(X1/(Y1+0.0001))\n",
        "    #R2= math.atan(X2/(Y2+0.0001))\n",
        "    R3 = (Y1+Y2)/2\n",
        "    #R= math.atan(X/(Y+0.0001))\n",
        "    return R3 #R1-R2\n",
        "\n",
        "train_word_match = df_train.apply(word_match_share, axis=1, raw=True)\n",
        "train_qs = pd.Series(df_train['question1'].map(cleantxt).tolist() + df_train['question2'].map(cleantxt).tolist()).astype(str)\n",
        "test_qs = pd.Series(df_test['question1'].map(cleantxt).tolist() + df_test['question2'].map(cleantxt).tolist()).astype(str)\n",
        "\n",
        "end = time.clock()\n",
        "print('clean:',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "99d8e8e1-e2fd-25c6-137d-4492b850e1b5"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# If a word appears only once, we ignore it completely (likely a typo)\n",
        "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
        "def get_weight(count, eps=10000, min_count=2):\n",
        "    if count < min_count:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1 / (count + eps)\n",
        "\n",
        "eps = 5000 \n",
        "words = (\" \".join(train_qs)).split()\n",
        "counts = Counter(words)\n",
        "weights = {word: get_weight(count) for word, count in counts.items()}\n",
        "end = time.clock()\n",
        "print('def:',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9ba1077f-19e7-0cfa-a6b1-bbf050c86921"
      },
      "outputs": [],
      "source": [
        "# remove start words, so make all startwords stopwords\n",
        "format = lambda x: x.split(' ', 1)[0]\n",
        "merknamen=df_train['question1'].map(format)\n",
        "merknamen=list(set(merknamen))\n",
        "\n",
        "txt1=df_train['question1']\n",
        "txt2=df_train['question2']\n",
        "txt=txt1.append(pd.DataFrame(list(txt2)))\n",
        "txt.columns=['q']\n",
        "print(txt.head())\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(4,4),min_df=1,stop_words=merknamen)\n",
        "tfidf.fit(txt.q)\n",
        "tfidf_m = tfidf.transform(txt.q) #print('list of words', tfidf.vocabulary_.keys()) #print('list of nummers', tfidf.vocabulary_.values())\n",
        "words = pd.DataFrame(pd.Series(list(tfidf.vocabulary_.keys()), index=tfidf.vocabulary_.values()),columns=['woord']) # #print(words)  #woordenboek !\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4af5d7d3-33e6-2efa-c713-ec5cce45d27d"
      },
      "outputs": [],
      "source": [
        "def sort_coo(m):\n",
        "    tuples = izip(m.row, m.col, m.data)\n",
        "    return sorted(tuples, key=lambda x: (x[2]))\n",
        "\n",
        "txt['core']=''\n",
        "for xi in range(0,len(txt)):\n",
        "    rij1=tfidf_m[xi][0:]\n",
        "    print(sort_coo(coo_matrix(rij1)) )\n",
        "\n",
        "    rij1['woord']=words['woord']\n",
        "    toprij=rij1[rij1['plaats']>0]\n",
        "    formathw = lambda x: len(x)\n",
        "    toprij['lengte']=toprij['woord'].map(formathw)\n",
        "    toprij=toprij[toprij['lengte']==toprij['lengte'].max()]['woord']\n",
        "    if len(toprij)>0:\n",
        "        print(list(toprij)[0])\n",
        "        txt.set_value(xi, 'klas', list(toprij)[0])\n",
        "    \n",
        "\n",
        "print(txt)\n",
        "end = time.clock()\n",
        "print('tf:',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d791c6b1-9860-7c3b-2b96-0767e607d6dc"
      },
      "outputs": [],
      "source": [
        "print('Most common words and weights: \\n')\n",
        "print(sorted(weights.items(), key=lambda x: x[1] if x[1] > 0 else 9999)[:10])\n",
        "print('\\nLeast common words and weights: ')\n",
        "(sorted(weights.items(), key=lambda x: x[1], reverse=True)[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2c8f2e86-f96b-5d36-e2f1-5f4c4c903e26"
      },
      "outputs": [],
      "source": [
        "def tf(word,blob):\n",
        "    return blob.words.count(word)/len(blob.words)\n",
        "def n_containing(word,bloblist):\n",
        "    return blob.words.count(word)/len(blob.words)\n",
        "def idf(word,bloblist):\n",
        "    return sum(1 for blob in bloblist if word in blob)\n",
        "def tfidf(word,blob,bloblist):\n",
        "    return tf(word,blob)*idf(word,bloblist)\n",
        "\n",
        "def tfidf_word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).split():\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).split():\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
        "        return 0\n",
        "    \n",
        "    shared_weights_q1=[weights.get(w, 0) for w in q1words.keys() if w in q2words] \n",
        "    shared_weights_q2=[weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
        "    nonshared_weights_q1 = [weights.get(w, 0) for w in q1words.keys() if w not in q2words]\n",
        "    nonshared_weights_q2 =  [weights.get(w, 0) for w in q2words.keys() if w not in q1words]\n",
        "    total_weights_q1 = [weights.get(w, 0) for w in q1words] \n",
        "    total_weights_q2 =[weights.get(w, 0) for w in q2words]\n",
        "    X1 = np.sum(shared_weights_q1) / np.sum(total_weights_q1)\n",
        "    Y1 = np.sum(nonshared_weights_q1) / np.sum(total_weights_q1)\n",
        "    X2 = np.sum(shared_weights_q2) / np.sum(total_weights_q2)\n",
        "    Y2 = np.sum(nonshared_weights_q2) / np.sum(total_weights_q2)\n",
        "    #R1= math.atan(X1/(Y1+0.0001))\n",
        "    #R2= math.atan(X2/(Y2+0.0001))\n",
        "    R3 = (Y1+Y2)/2\n",
        "    #X = np.sum(shared_weights) / np.sum(total_weights)\n",
        "    #Y = np.sum(nonshared_weights) / np.sum(shared_weights)\n",
        "    #R= math.atan(X/(Y+0.0001))\n",
        "    return R3 #R1-R2\n",
        "\n",
        "end = time.clock()\n",
        "print('wordmatch:',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "157fd891-0b63-22df-4129-e4267cb55ff0"
      },
      "outputs": [],
      "source": [
        "tfidf_train_word_match = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\n",
        "\n",
        "end = time.clock()\n",
        "print('tfidf:',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f1814b89-b895-f442-0ba2-b4e2fc6b6288"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "print('Original AUC:', roc_auc_score(df_train['is_duplicate'], train_word_match))\n",
        "print('   TFIDF AUC:', roc_auc_score(df_train['is_duplicate'], tfidf_train_word_match.fillna(0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0589da36-d092-c951-f3cd-68df3fd91c7a"
      },
      "outputs": [],
      "source": [
        "# First we create our training and testing data\n",
        "x_train = pd.DataFrame()\n",
        "x_test = pd.DataFrame()\n",
        "x_train['word_match'] = train_word_match\n",
        "x_train['tfidf_word_match'] = tfidf_train_word_match\n",
        "x_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\n",
        "x_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n",
        "\n",
        "y_train = df_train['is_duplicate'].values\n",
        "end = time.clock()\n",
        "print('createtestdata:',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5ccae7db-46e1-cf74-fdd8-4d4758bf460f"
      },
      "outputs": [],
      "source": [
        "pos_train = x_train[y_train == 1]\n",
        "neg_train = x_train[y_train == 0]\n",
        "\n",
        "# Now we oversample the negative class\n",
        "# There is likely a much more elegant way to do this...\n",
        "p = 0.165\n",
        "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
        "while scale > 1:\n",
        "    neg_train = pd.concat([neg_train, neg_train])\n",
        "    scale -=1\n",
        "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
        "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
        "\n",
        "x_train = pd.concat([pos_train, neg_train])\n",
        "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
        "del pos_train, neg_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "20684b12-fd83-bad6-b00b-77c51dd14c32"
      },
      "outputs": [],
      "source": [
        "# Finally, we split some of the data off for validation\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9df47145-da6a-ad51-7bfb-1b588c4bab06"
      },
      "source": [
        "## XGBoost\n",
        "\n",
        "Now we can finally run XGBoost on our data, in order to see the score on the leaderboard!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a5f9f7fc-5ec4-fc9f-7fc0-608f5f25e7c6"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Set our parameters for xgboost\n",
        "params = {}\n",
        "params['objective'] = 'binary:logistic'\n",
        "params['eval_metric'] = 'logloss'\n",
        "#params['eta'] = 0.12\n",
        "#params['max_depth'] = 5\n",
        "\n",
        "d_train = xgb.DMatrix(x_train, label=y_train)\n",
        "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
        "\n",
        "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
        "\n",
        "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n",
        "#bst = xgb.train(params, d_train, watchlist, early_stopping_rounds=50, verbose_eval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "11616998-a057-45e0-ac3c-d129e5cfec36"
      },
      "outputs": [],
      "source": [
        "d_test = xgb.DMatrix(x_test)\n",
        "p_test = bst.predict(d_test)\n",
        "\n",
        "sub = pd.DataFrame()\n",
        "sub['test_id'] = df_test['test_id']\n",
        "sub['is_duplicate'] = p_test\n",
        "sub.to_csv('simple_xgb.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4b74fd4a-5e89-27e1-5ddd-f170bc370697"
      },
      "source": [
        "**0.37** on the leaderboard - a good first score! splitting better the ? makes it worse..."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}