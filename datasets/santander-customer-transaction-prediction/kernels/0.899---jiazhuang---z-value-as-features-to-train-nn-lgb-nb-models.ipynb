{"cells":[{"metadata":{"_uuid":"872d0eab1e8adb228e302a92002ca188f9772a0b"},"cell_type":"markdown","source":"In the kernel [A proof of synthetic data](https://www.kaggle.com/jiazhuang/a-proof-of-synthetic-data), I have tried to calculate Z-value for each sample, which I think is also a good way to standerize data. So in this kernel, I will use the Z-value as features to train a few models, including lightgbm, naive bayes, neural network, then stacking them together using logistic regression."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential, layers, callbacks\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b704c2175d28128a5d3a59414dbbb70b1efcbff3"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', index_col=0)\ntest = pd.read_csv('../input/test.csv', index_col=0)\n\ntarget = train.target.values\ntrain.drop('target', axis=1, inplace=True)\ntrain.shape, target.shape, test.shape, ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"231c5c6d21a19c4379039afde3ebab8e26ed85e2"},"cell_type":"markdown","source":"### Calculate mean/sd of positive and negative samples for each feature"},{"metadata":{"trusted":true,"_uuid":"41d4c7c1f34f5750542c04f0ecf3d29fcb9d4aea","scrolled":true},"cell_type":"code","source":"pos_idx = (target == 1)\nneg_idx = (target == 0)\nstats = []\nfor col in train.columns:\n    stats.append([\n        train.loc[pos_idx, col].mean(),\n        train.loc[pos_idx, col].std(),\n        train.loc[neg_idx, col].mean(),\n        train.loc[neg_idx, col].std()\n    ])\n    \nstats_df = pd.DataFrame(stats, columns=['pos_mean', 'pos_sd', 'neg_mean', 'neg_sd'])\nstats_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b14ba2e4f4e10b3efe26679b36d982e0c93e363"},"cell_type":"markdown","source":"### Standerize train/test data"},{"metadata":{"trusted":true,"_uuid":"d647f8993b781df34e6b89201da2d23b23ee6435"},"cell_type":"code","source":"zval1 = (train.values - stats_df.neg_mean.values) / stats_df.neg_sd.values\nzval2 = (train.values - stats_df.pos_mean.values) / stats_df.pos_sd.values\ntr_zval = np.column_stack([zval1, zval2])\ntr_zval.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d647f8993b781df34e6b89201da2d23b23ee6435"},"cell_type":"code","source":"zval1 = (test.values - stats_df.neg_mean.values) / stats_df.neg_sd.values\nzval2 = (test.values - stats_df.pos_mean.values) / stats_df.pos_sd.values\nte_zval = np.column_stack([zval1, zval2])\nte_zval.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b6a43b7e1d13b06833c174c62b9f73ec874df81"},"cell_type":"markdown","source":"### Data augment"},{"metadata":{"_uuid":"f1919b8c97d6e6bf5394be60e61cc18456317ee6"},"cell_type":"markdown","source":"Based on [@Branden Murray](https://www.kaggle.com/brandenkmurray)'s hypothesis **For each feature they had a distribution for target==0 and a distribution for target==1 and they randomly sampled from each and then put it together**, we can up-sample positive samples to get a balanced dataset. Since the positive/negative ratio is 1/9, I'm going to augment the positive sample 8 times to make them balance.\n\nWe only apply up-sample to train fold, not valid fold."},{"metadata":{"trusted":true,"_uuid":"fd97afe268baacdb93dfa1ecb817c1035bc31274"},"cell_type":"code","source":"def augment(X, y, times=8):\n    # up-sample positive samples\n    pos_idx = (y == 1)\n    nsample = times * pos_idx.sum()\n    \n    X_sample = []\n    for i in range(X.shape[1]):\n        X_sample.append(np.random.choice(X[pos_idx, i], size=nsample))\n        \n    X_sample = np.column_stack(X_sample)\n    \n    # shuffle\n    idx = np.arange(X.shape[0] + nsample)\n    np.random.shuffle(idx)\n    \n    return np.vstack([X, X_sample])[idx], np.hstack([y, np.ones(nsample)])[idx]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"718e5a828d4a47a85df69210de751abc2e9021e2"},"cell_type":"markdown","source":"### Cross validation for NN/LGB/NaiveBayes models"},{"metadata":{"trusted":true,"_uuid":"a14c794d0a412cd101c534a3ffcf4ed9a988c329"},"cell_type":"code","source":"nfold = 5\nkfold = StratifiedKFold(n_splits=nfold, shuffle=True)\n\nnn_oof_tr, nn_oof_te = np.zeros(tr_zval.shape[0]), 0\nlgb_oof_tr, lgb_oof_te = np.zeros(tr_zval.shape[0]), 0\nnb_oof_tr, nb_oof_te = np.zeros(tr_zval.shape[0]), 0\n\nfor tr_idx, va_idx in kfold.split(tr_zval, target):\n    X_tr, y_tr = tr_zval[tr_idx], target[tr_idx] \n    X_va, y_va = tr_zval[va_idx], target[va_idx]\n    # data augment\n    X_tr, y_tr = augment(X_tr, y_tr)\n    \n    # NN model\n    model = Sequential([\n        layers.Dense(16, input_shape=(400, 1), activation='relu'),\n        layers.Flatten(),\n        layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_crossentropy'])\n    earlystop = callbacks.EarlyStopping(patience=10)\n    # reshape data for nn input\n    X_tr_re = X_tr[:, :, np.newaxis]\n    X_va_re = X_va[:, :, np.newaxis]\n    model.fit(X_tr_re, y_tr, validation_data=(X_va_re, y_va), epochs=100, verbose=2, batch_size=256, callbacks=[earlystop])\n    # get oof\n    nn_oof_tr[va_idx] = model.predict(X_va_re).flatten()\n    X_te_re = te_zval[:, :, np.newaxis]\n    nn_oof_te += model.predict(X_te_re).flatten() / nfold\n    \n    # LGB model\n    param = {\n        'objective': 'binary',\n        'boost': 'gbdt',\n        'metric': 'auc',\n        'learning_rate': 0.01,\n        'num_leaves': 13,\n        'max_depth': -1,\n        'feature_fraction': 0.05,\n        'bagging_freq': 5,\n        'bagging_fraction': 0.4,\n        'min_data_in_leaf': 80,\n        'min_sum_hessian_in_leaf': 10,\n        'num_threads': 4\n    }\n    trn_data = lgb.Dataset(X_tr, y_tr)\n    val_data = lgb.Dataset(X_va, y_va)\n    clf = lgb.train(param, trn_data, 100000, valid_sets=(val_data), early_stopping_rounds=600, verbose_eval=600)\n    # get oof\n    lgb_oof_tr[va_idx] = clf.predict(X_va)\n    lgb_oof_te += clf.predict(te_zval) / nfold\n    \n    # Naive Bayes model\n    bayes = GaussianNB()\n    bayes.fit(X_tr, y_tr)\n    nb_oof_tr[va_idx] = bayes.predict_proba(X_va)[:, 1]\n    nb_oof_te += bayes.predict_proba(te_zval)[:, 1] / nfold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fca49319f1c848e945dd871234cc143cb0853135"},"cell_type":"markdown","source":"### Stacking"},{"metadata":{"trusted":true,"_uuid":"aedfad6ad405fc149db52399159e31164535ecf0"},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(np.column_stack([nn_oof_tr, lgb_oof_tr, nb_oof_tr]), target)\npred = lr.predict_proba(np.column_stack([nn_oof_te, lgb_oof_te, nb_oof_te]))[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62a07bf27da14d826eb346c097bdba1dd5de25de"},"cell_type":"code","source":"pd.DataFrame({\n    'ID_code': test.index,\n    'target': pred\n}).to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}