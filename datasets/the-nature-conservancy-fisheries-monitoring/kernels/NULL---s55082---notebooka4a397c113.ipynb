{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b35aca4-9fe8-b58f-0a17-01ecc37fa0aa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "from sklearn.cross_validation import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
        "from keras.optimizers import SGD, Adagrad\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from keras.constraints import maxnorm\n",
        "from sklearn.metrics import log_loss\n",
        "from keras import __version__ as keras_version\n",
        "import tensorflow as tf\n",
        "print('\u5b8c\u6210\u8bfb\u53d6\u76f8\u5e94\u5305')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7900a874-a987-55ef-9700-0a2c30f73eb5"
      },
      "outputs": [],
      "source": [
        "#img = cv2.imread('../input/train/ALB/img_00967.jpg')\n",
        "#resized = cv2.resize(img, (128, 128), cv2.INTER_LINEAR)\n",
        "#Gray = resized[:,:,1]*0.299 + resized[:,:,1]*0.587 + resized[:,:,1]*0.114"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c5c5a92f-5b45-c6bc-b404-75adf536a72f"
      },
      "outputs": [],
      "source": [
        "def get_im_cv2(path):\n",
        "    img = cv2.imread(path)\n",
        "    resized = cv2.resize(img, (128, 128), cv2.INTER_LINEAR)\n",
        "    resized = resized[:,:,1]*0.299 + resized[:,:,2]*0.587 + resized[:,:,3]*0.114\n",
        "    return resized\n",
        "\n",
        "def load_train():\n",
        "    X_train = []\n",
        "    X_train_id = []\n",
        "    y_train = []\n",
        "\n",
        "    print('Read train images')\n",
        "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
        "    for fld in folders:\n",
        "        index = folders.index(fld)\n",
        "        print('Load folder {} (Index: {})'.format(fld, index))\n",
        "        path = os.path.join('..', 'input', 'train', fld, '*.jpg')\n",
        "        files = glob.glob(path)\n",
        "        for fl in files:\n",
        "            flbase = os.path.basename(fl)\n",
        "            img = get_im_cv2(fl)\n",
        "            X_train.append(img)\n",
        "            X_train_id.append(flbase)\n",
        "            y_train.append(index)\n",
        "\n",
        "    return X_train, y_train, X_train_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "04cc2dd0-34a7-fdfa-79ef-a14f42763bef"
      },
      "outputs": [],
      "source": [
        "def load_test():\n",
        "    path = os.path.join('..', 'input', 'test_stg1', '*.jpg')\n",
        "    files = sorted(glob.glob(path))\n",
        "\n",
        "    X_test = []\n",
        "    X_test_id = []\n",
        "    for fl in files:\n",
        "        flbase = os.path.basename(fl)\n",
        "        img = get_im_cv2(fl)\n",
        "        X_test.append(img)\n",
        "        X_test_id.append(flbase)\n",
        "\n",
        "    return X_test, X_test_id\n",
        "\n",
        "def create_submission(predictions, test_id, info):\n",
        "    result1 = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n",
        "    result1.loc[:, 'image'] = pd.Series(test_id, index=result1.index)\n",
        "    sub_file = 'submission1.csv'\n",
        "    result1.to_csv(sub_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58ded619-074d-0a58-a9e6-95aa3eb57720"
      },
      "outputs": [],
      "source": [
        "#\u8bfb\u53d6\u6570\u636e\n",
        "train_data, train_target, train_id = load_train()\n",
        "#\u8f6c\u6362\u4e3aarray\n",
        "train_data = np.array(train_data, dtype=np.uint8)\n",
        "train_target = np.array(train_target, dtype=np.uint8)\n",
        "#\u8f6c\u6362\u77e9\u9635\u987a\u5e8f\n",
        "#train_data = train_data.transpose((0, 3, 1, 2))\n",
        "#\u6570\u636e\u5f52\u4e00\u5316\u52300~1\u4e4b\u95f4\n",
        "train_data = train_data.astype('float32')\n",
        "train_data = train_data / 255\n",
        "#label\u8f6c\u6362\u4e3a\u54d1\u53d8\u91cf\n",
        "train_target = np_utils.to_categorical(train_target, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69cbe1c7-aecb-5b5d-b35b-f288fc8d8c56"
      },
      "outputs": [],
      "source": [
        "#\u8bfb\u53d6\u6d4b\u8bd5\u6570\u636e\n",
        "test_data, test_id = load_test()\n",
        "test_data = np.array(test_data, dtype=np.uint8)\n",
        "test_data = test_data.transpose((0, 3, 1, 2))\n",
        "test_data = test_data.astype('float32')\n",
        "test_data = test_data / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cb2f73f8-ee84-7fe6-797d-476c828ba1ef"
      },
      "outputs": [],
      "source": [
        "def dict_to_list(d):\n",
        "    ret = []\n",
        "    for i in d.items():\n",
        "        ret.append(i[1])\n",
        "    return ret\n",
        "\n",
        "\n",
        "def merge_several_folds_mean(data, nfolds):\n",
        "    a = np.array(data[0])\n",
        "    for i in range(1, nfolds):\n",
        "        a += np.array(data[i])\n",
        "    a /= nfolds\n",
        "    return a.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60db4ffe-2816-8253-b9d1-0e2801db0de2"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "display_step = 10\n",
        "\n",
        "# Network Parameters\n",
        "n_input = 16384 #data input (img shape: 128*128)\n",
        "n_classes = 8 # total classes (0-7 digits)\n",
        "dropout = 0.75 # Dropout, probability to keep units\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
        "\n",
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 128, 128, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([16, 16, 1, 32])),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([16, 16, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([32*32*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9528c7fd-ef3b-e4eb-7045-3a5d6244ae6f"
      },
      "outputs": [],
      "source": [
        "pred = conv_net(x, weights, biases, keep_prob)\n",
        "# Define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "# Initializing the variables\n",
        "init = tf.initialize_all_variables()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9fd11823-ad19-1b77-13a0-03387afbfa35"
      },
      "outputs": [],
      "source": [
        "train_target.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "39ed4358-5931-cdcf-99c0-a58569334f9d"
      },
      "outputs": [],
      "source": [
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "for i in range(training_iters):\n",
        "    feed={x:train_data, y:train_target,keep_prob: dropout}\n",
        "    sess.run(optimizer, feed_dict=feed)\n",
        "    if i % 1000 == 0 or i == ITERATIONS-1:\n",
        "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y,keep_prob: 1.})\n",
        "        print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.5f}\".format(acc))\n",
        "print(\"Optimization Finished!\")        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7a8f072-cf47-653c-3c55-a475e2b875bb"
      },
      "outputs": [],
      "source": [
        "predicted = sess.run(y, feed_dict={x:test_features})\n",
        "\n",
        "\n",
        "sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
        "                                      y: mnist.test.labels[:256],\n",
        "                                      keep_prob: 1.}))"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}