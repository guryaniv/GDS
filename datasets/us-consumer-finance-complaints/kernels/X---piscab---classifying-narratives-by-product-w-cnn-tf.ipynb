{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3e06f606-838c-cf6f-6da4-cfb786ea912c"
      },
      "source": [
        "\n",
        "In this notebook, I try to classify complaints by product from their narratives. \n",
        "\n",
        "I use - among others - Tensorflow (TF) and a Convolutional Neural Network (CNN) with an embedding layer, followed by convolutional, max-pooling and softmax layers.\n",
        "\n",
        "In the following, there is a little modified version of a well known model: for the nearest \"deep\" explanation, please see: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/ \n",
        "\n",
        "I use a training set of 40.000 narratives and a validation set of 13.000. To shrink the idle time, I set the number of epochs to 1 ( i.e, 827 training steps with batch size 64 ). \n",
        "\n",
        "Results seem promising. Here, the accuracy for a completely new test set of 13.000 is about 77%, but setting the number of epochs to 8, for the same test set the accuracy could result greater than 85% (not shown)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "21845a8b-5eaf-8e86-b9ec-e10e2f7b933c"
      },
      "source": [
        "----\n",
        "### To be imported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9e54decf-b258-5444-fc00-64ed7e7e05d5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "import string\n",
        "from io import BytesIO\n",
        "from tensorflow.contrib import learn\n",
        "from collections import Counter\n",
        "from time import time\n",
        "import datetime\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "238987bd-e2cd-70e6-18e3-30cc18ea0efa"
      },
      "outputs": [],
      "source": [
        "# Read the input dataset \n",
        "d = pd.read_csv(\"../input/consumer_complaints.csv\", \n",
        "                usecols=('product','consumer_complaint_narrative'),\n",
        "                dtype={'consumer_complaint_narrative': object})\n",
        "# Only interested in data with consumer complaints\n",
        "d=d[d['consumer_complaint_narrative'].notnull()]\n",
        "d=d[d['product'].notnull()]\n",
        "d.reset_index(drop=True,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "04a3727a-5bfb-f650-1160-0444aa671eb6"
      },
      "outputs": [],
      "source": [
        "# Let's see what's in the data \n",
        "print (\"Data dimensions:\", d.shape)\n",
        "print (d.head())\n",
        "\n",
        "# Let's see a table of how many examples we have of each product\n",
        "print (\"\\nList of Products       Occurrences\\n\")\n",
        "print (d[\"product\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b14de947-cbe6-2514-716f-c84d5b96ae72"
      },
      "source": [
        "### Data helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0ad6e5b6-b665-22e1-9555-4a53db6eb350"
      },
      "outputs": [],
      "source": [
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning (partially modified)\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9()!?\\'\\`%$]\", \" \", string) # keep also %$ but removed comma\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" ( \", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\$\", \" $ \", string) #yes, isolate $\n",
        "    string = re.sub(r\"\\%\", \" % \", string) #yes, isolate %\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    \n",
        "    # fixing XXX and xxx like as word\n",
        "    string = re.sub(r'\\S*(x{2,}|X{2,})\\S*',\"xxx\",string)\n",
        "    # removing non ascii\n",
        "    string = re.sub(r'[^\\x00-\\x7F]+', \"\", string) \n",
        "    \n",
        "    return string.strip().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a3cbb5b-11f5-4231-b65c-1b8966cbf8af"
      },
      "outputs": [],
      "source": [
        "word_data=[]\n",
        "t0 = time()\n",
        "\n",
        "for message in d['consumer_complaint_narrative']:\n",
        "    word_data.append(clean_str(message))\n",
        "\n",
        "# With a MacBook Pro (Late 2011)\n",
        "# 2.4 GHz Intel Core i5, 4 GB 1333 MHz DDR3\n",
        "print (\"\\nCleaning time: mine = 41.8 s, here =\", round(time()-t0, 1), \"s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c5b402ce-e99b-d918-7b86-1c826ff2493b"
      },
      "outputs": [],
      "source": [
        "# Have a look before and after cleaning texts\n",
        "an_example = 38\n",
        "print (\"Note: the reference product is\",d ['product'][an_example])\n",
        "print (\"\\n** Before cleaning ** \\n\")\n",
        "print (d['consumer_complaint_narrative'][an_example])\n",
        "print (\"** After cleaning ** \\n\")\n",
        "print (word_data [an_example])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2da74428-5c25-ae45-48ed-b2bd84ce2c9b"
      },
      "source": [
        "### Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9dc2268c-e85c-df19-e9a9-36535661b038"
      },
      "outputs": [],
      "source": [
        "max_document_length = max([len(x.split(\" \")) for x in word_data])\n",
        "print (\"Max_document_length:\",max_document_length)\n",
        "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "num_data = np.array(list(vocab_processor.fit_transform(word_data)))\n",
        "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c926c3d-9271-293d-cbf0-a288cbe78af4"
      },
      "outputs": [],
      "source": [
        "# Check data \"lengths\"\n",
        "print (\"Check my variables:\")\n",
        "print (\"\\n* word_data length:\", len(word_data))\n",
        "print (\"* num_data length: \", len(num_data)) # once words are numbers\n",
        "\n",
        "#Create the list of products\n",
        "product_labels = list(set(d['product']))\n",
        "print (\"\\nProducts:\")\n",
        "print (\"* data length: \",len(product_labels))\n",
        "print (\"* labels:\\n\",product_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "55a388b4-99ae-4aac-3059-b3f71505c8f0"
      },
      "source": [
        "### Randomly shuffle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f6a6c1f4-e0a0-0def-8208-499603a5fac5"
      },
      "outputs": [],
      "source": [
        "np.random.seed(57)\n",
        "shuffle_indices = np.random.permutation(np.arange(len(num_data)))\n",
        "x_shuffled = num_data[shuffle_indices]\n",
        "y_shuffled = d['product'][shuffle_indices]\n",
        "print (\"* x shuffled:\", x_shuffled.shape)\n",
        "print (\"* y shuffled:\", y_shuffled.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d6333317-5f11-9e9b-cbbc-57dffcf9f0cd"
      },
      "source": [
        "### Create Train, Validation and Test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6152d37-aa04-51dc-c169-b445a626fb72"
      },
      "outputs": [],
      "source": [
        "features_dummy, x_test, labels_dummy, test_labels = model_selection.train_test_split(x_shuffled, y_shuffled, test_size=0.20, random_state= 23)\n",
        "x_train, x_valid, train_labels, valid_labels = model_selection.train_test_split(features_dummy, labels_dummy, test_size=0.25, random_state= 34)\n",
        "\n",
        "print('Training set  ',   x_train.shape, train_labels.shape)\n",
        "print('Validation set',   x_valid.shape, valid_labels.shape)\n",
        "print('Test set      ',    x_test.shape,  test_labels.shape)\n",
        "\n",
        "# free some memory\n",
        "del num_data, d \n",
        "del x_shuffled, y_shuffled, labels_dummy, features_dummy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b2aa1439-2b84-3b11-ed65-7ed2e28fc716"
      },
      "source": [
        "### Selecting batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "17d1092c-8122-86fc-eb67-353c0a28514d"
      },
      "outputs": [],
      "source": [
        "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates a batch iterator for a dataset.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aa353e6f-1733-5ba1-6ac6-8e0df1e17f06"
      },
      "source": [
        "### CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "22761851-4a09-3ea2-fe25-dfe5849f0373"
      },
      "outputs": [],
      "source": [
        "class TextCNN(object):\n",
        "    \"\"\"\n",
        "    A CNN for text classification.\n",
        "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "      self, sequence_length, num_classes, vocab_size,\n",
        "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
        "\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "        # Keeping track of l2 regularization loss (optional)\n",
        "        l2_loss = tf.constant(0.0)\n",
        "\n",
        "        # Embedding layer\n",
        "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "            W = tf.Variable(\n",
        "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
        "                name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "        # Create a convolution + maxpool layer for each filter size\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.embedded_chars_expanded,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1], \n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(3, pooled_outputs)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "            l2_loss += tf.nn.l2_loss(W)\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\") \n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "        # CalculateMean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            print (self.scores)\n",
        "            print (self.input_y)\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f838751e-34cb-5f19-5553-96a78f07765b"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4bfe972e-ee4a-c655-3a7f-4fa076c2f0c5"
      },
      "outputs": [],
      "source": [
        "# Model Hyperparameters\n",
        "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
        "\n",
        "# WAS: tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
        "tf.flags.DEFINE_string(\"filter_sizes\", \"2,3,4\", \"Comma-separated filter sizes (default: '2,3,4')\")\n",
        "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
        "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
        "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda (default: 0.0)\")\n",
        "\n",
        "# Training parameters\n",
        "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
        "tf.flags.DEFINE_integer(\"num_epochs\", 1, \"Number of training epochs (best: 8)\") # was 200\n",
        "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
        "# Misc Parameters\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "FLAGS._parse_flags()\n",
        "print(\"\\nParameters:\")\n",
        "for attr, value in sorted(FLAGS.__flags.items()):\n",
        "    print(\"{}={}\".format(attr.upper(), value))\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ddc3cd3f-88c7-0a62-e2fb-829a3a481d50"
      },
      "source": [
        "### OneHot for more than two features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "83aeb12e-8287-783c-e690-ab5edbd92d94"
      },
      "outputs": [],
      "source": [
        "def oneHot(dummy_labels):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    enc = OneHotEncoder()\n",
        "    \n",
        "    le.fit (dummy_labels)\n",
        "    y_dummy = le.fit_transform(dummy_labels)\n",
        "    y_dummy = y_dummy.reshape(-1, 1)\n",
        "    enc.fit(y_dummy)\n",
        "    y_dummy = enc.transform(y_dummy).toarray()\n",
        "    y_dummy = y_dummy.astype('float32')\n",
        "    print (\"\\n * OneHot example\")\n",
        "    print (y_dummy)\n",
        "    return (y_dummy)\n",
        "        \n",
        "y_train = oneHot(train_labels)\n",
        "y_valid = oneHot(valid_labels)\n",
        "y_test  = oneHot( test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cce092d9-381f-ef45-9cf8-330b950bbe1e"
      },
      "source": [
        "### Model estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7bcf7222-5a9e-ba4a-3f44-913b9b07f3d5"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "# ==================================================\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    session_conf = tf.ConfigProto(\n",
        "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
        "      log_device_placement=FLAGS.log_device_placement)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    with sess.as_default():\n",
        "        cnn = TextCNN(\n",
        "            sequence_length=x_train.shape[1],\n",
        "            num_classes=len(product_labels),\n",
        "            vocab_size=len(vocab_processor.vocabulary_),\n",
        "            embedding_size=FLAGS.embedding_dim,\n",
        "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
        "            num_filters=FLAGS.num_filters,\n",
        "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "\n",
        "        # Define Training procedure\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Keep track of gradient values and sparsity (optional)\n",
        "        grad_summaries = []\n",
        "        for g, v in grads_and_vars:\n",
        "            if g is not None:\n",
        "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
        "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                grad_summaries.append(grad_hist_summary)\n",
        "                grad_summaries.append(sparsity_summary)\n",
        "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
        "\n",
        "        # Output directory for models and summaries (if needed)\n",
        "        \n",
        "        #timestamp = str(int(time()))\n",
        "        #out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        #print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
        "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
        "        #train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        #train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
        "        #dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        #dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory (if needed)\n",
        "        # Tensorflow assumes this directory already exists so we need to create it\n",
        "        \n",
        "        #checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        #checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        #if not os.path.exists(checkpoint_dir):\n",
        "        #    os.makedirs(checkpoint_dir)\n",
        "        #saver = tf.train.Saver(tf.all_variables())\n",
        "\n",
        "        # Write vocabulary (if needed)\n",
        "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "\n",
        "        def train_step(x_batch, y_batch):\n",
        "            \"\"\"\n",
        "            A single training step\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "              cnn.input_x: x_batch,\n",
        "              cnn.input_y: y_batch,\n",
        "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
        "            }\n",
        "            _, step, summaries, loss, accuracy = sess.run(\n",
        "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            \n",
        "            # Uncomment next print if interested in batch results \n",
        "            #print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            \n",
        "            #train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "        def dev_step(x_batch, y_batch, writer=None):\n",
        "            \"\"\"\n",
        "            Evaluates model on a dev set\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "              cnn.input_x: x_batch,\n",
        "              cnn.input_y: y_batch,\n",
        "              cnn.dropout_keep_prob: 1.0\n",
        "            }\n",
        "            step, summaries, loss, accuracy = sess.run(\n",
        "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            return loss, accuracy, summaries\n",
        "\n",
        "        # Generate batches\n",
        "        batches = batch_iter(\n",
        "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
        "        \n",
        "        # Training loop. For each batch...\n",
        "        for batch in batches:\n",
        "            x_batch, y_batch = zip(*batch)\n",
        "            train_step(x_batch, y_batch)\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            \n",
        "# Validating\n",
        "# ==================================================\n",
        "            if current_step % FLAGS.evaluate_every == 0:\n",
        "                #print(\"\\nEvaluation:\")\n",
        "                \n",
        "                # Generate batches\n",
        "                batches_valid = batch_iter(\n",
        "                    list(zip(x_valid, y_valid)), FLAGS.batch_size, 1)\n",
        "                \n",
        "                loss_valid = 0.\n",
        "                acc_valid = 0.\n",
        "                len_batches = 0.\n",
        "                \n",
        "                for batch_valid in batches_valid:  \n",
        "                    \n",
        "                    x_batch_valid, y_batch_valid = zip(*batch_valid)\n",
        "                    #aLoss, anAcc, aSummary = dev_step(x_batch_valid, y_batch_valid, writer=dev_summary_writer)\n",
        "                    aLoss, anAcc, aSummary = dev_step(x_batch_valid, y_batch_valid)\n",
        "                    loss_valid += aLoss \n",
        "                    acc_valid  += anAcc\n",
        "                    len_batches += 1.\n",
        "                \n",
        "                loss_valid = loss_valid / len_batches\n",
        "                acc_valid  = acc_valid  / len_batches \n",
        "                time_str = datetime.datetime.now().isoformat()\n",
        "                print(\"Validation set: {}, step {}, loss {:g}, acc {:g}\".format(time_str, current_step, loss_valid, acc_valid))\n",
        "                #dev_summary_writer.add_summary(aSummary, current_step)\n",
        "                #print(\"\")\n",
        "                \n",
        "            #if current_step % FLAGS.checkpoint_every == 0:\n",
        "            #    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "            #    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "    \n",
        "        \n",
        "# Testing\n",
        "# ==================================================\n",
        "        if True:\n",
        "            print(\"\\n\\nTest set:\")\n",
        "            \n",
        "            # Generate batches\n",
        "            batches_test = batch_iter(\n",
        "                list(zip(x_test, y_test)), FLAGS.batch_size, 1)\n",
        "        \n",
        "            loss_test = 0.\n",
        "            acc_test  = 0.\n",
        "            len_batches = 0.\n",
        "            \n",
        "            for batch_test in batches_test:  \n",
        "                    \n",
        "                    x_batch_test, y_batch_test = zip(*batch_test)\n",
        "                    #aLoss, anAcc, aSummary = dev_step(x_batch_test, y_batch_test, writer=dev_summary_writer)\n",
        "                    aLoss, anAcc, aSummary = dev_step(x_batch_test, y_batch_test)\n",
        "                    loss_test += aLoss \n",
        "                    acc_test  += anAcc\n",
        "                    len_batches += 1.\n",
        "                \n",
        "            loss_test = loss_test / len_batches\n",
        "            acc_test  = acc_test  / len_batches \n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, current_step, loss_test, acc_test))\n",
        "            #dev_summary_writer.add_summary(aSummary, current_step)\n",
        "            print(\"\")"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}