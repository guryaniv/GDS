{"nbformat_minor": 1, "cells": [{"execution_count": null, "outputs": [], "metadata": {"_kg_hide-output": false, "_kg_hide-input": true, "_uuid": "031635f673b13d03c71be09e0b125a2b243977d9", "_cell_guid": "528b1e4a-7eaf-42f9-9df1-c3a2bf60936e", "collapsed": true}, "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n", "from sklearn.linear_model import SGDClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from imblearn.pipeline import make_pipeline\n"]}, {"execution_count": null, "outputs": [], "metadata": {"_uuid": "b5b15d11e02443d3dab478a4f5fd2b5d44966c72", "_cell_guid": "60913ce8-81f6-480f-b3b1-51b792e66799", "collapsed": true}, "cell_type": "code", "source": ["pd.concat([pd.read_csv(\"../input/sample_submission.csv\")['id'],pd.DataFrame(make_pipeline(CountVectorizer(), TfidfTransformer(), SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=10, random_state=42)).fit(*np.split(pd.read_csv(\"../input/train.csv\")[['text','author']].T.values.flatten(), 2)).predict_proba(pd.read_csv(\"../input/test.csv\")['text']), columns=['EAP','HPL','MWS'])], axis=1).to_csv('submission.csv', sep=',',index=False)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3a733c21d576a4be25616fc64611d8424f421b9f", "_cell_guid": "e54039bb-b836-4d6d-9440-705bb01df1f4"}, "source": ["**\\*record scratch\\* **\n", "\n", "**\\*freeze frame\\***\n", "\n", "*Yup, that's my code. You're probably wondering how I ended up in this situation.*\n", "\n", "\n", "### So, let's break it down:\n", "\n", "*(ok, so I lied a bit in the title: if you also count the imports, it's not really a one-line solution. Sorry about that!*\n", "\n", "First, we read in the train data (we are only interested in the **text** and **author** columns):"]}, {"execution_count": null, "outputs": [], "metadata": {"_uuid": "a0d2da098096ea99b65089042ba83559c0c2c309", "_cell_guid": "36edc993-3b6b-4f37-b4c2-086bad2a9a55", "collapsed": true}, "cell_type": "code", "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "train = train[['text','author']]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "a968bae03fbf5e299695f77e3a17d8593326ca84", "_cell_guid": "8ac3629d-6f76-41b6-b221-599f568a1379"}, "source": ["To classify all the given texts based on their authors, we use the **sklearn** library.\n", "We define a *pipeline* that does three things:\n", "* preprocess, tokenize and filter stopwords, basically transforming texts into feature vectors (**CountVectorizer**)\n", "* compute the *Term Frequency times Inverse Document Frequency* or tf-idf (**TfidfTransformer**)\n", "* train a linear classifier with stochastic gradient descent (SGD) learning (**SGDClassifier**)"]}, {"execution_count": null, "outputs": [], "metadata": {"_uuid": "2dd0b24423c7e69edf53ee48fb192a8091b112cf", "_cell_guid": "82e45e99-2070-45db-b5db-09c9f8ccc035", "collapsed": true}, "cell_type": "code", "source": ["classifier_pipeline = make_pipeline(\n", "    CountVectorizer(), \n", "    TfidfTransformer(), \n", "    SGDClassifier(loss='log', penalty='l2', alpha=1e-3, max_iter=10, random_state=42)\n", ")"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f2685fc0b3d01feea55e8fc67a25d6d46d5d3b57", "_cell_guid": "de69e7e5-1c22-4170-a97c-bc775c573cc9"}, "source": ["Our pipeline needs to be trained using a pair of example inputs and outputs. These are our two columns in the *train* data frame. So we do a little trick to pass the two pandas columns as parameters to our *fit* function. \n", "We flatten the two columns so that the first half of the resulting array contains the *text* samples and the second the *author* ones.\n"]}, {"execution_count": null, "outputs": [], "metadata": {"_cell_guid": "cde201b8-a906-44a0-b879-815c54cf4ae5", "_uuid": "b82bc16f3297e3ca734853b602a1a7233d70912d", "_kg_hide-output": true}, "cell_type": "code", "source": ["flattened = train.T.values.flatten()\n", "x,y = np.split(flattened, 2) # x is text, y is authors\n", "classifier_pipeline.fit(x, y)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "df613d77c00da0eaa0c32186d2850426839c6230", "_cell_guid": "183d2674-8119-4974-b7d0-fe0cb318a74e"}, "source": ["Once our classifier is trained, we read the test data and do our predictions"]}, {"execution_count": null, "outputs": [], "metadata": {"_uuid": "8b5926b9dd1fd375f7c0d85435cda71822792bf6", "_cell_guid": "c8f889d2-8fbf-4238-932a-e7a2610beaf2", "collapsed": true}, "cell_type": "code", "source": ["test = pd.read_csv(\"../input/test.csv\")\n", "prediction = classifier_pipeline.predict_proba(test['text'])"]}, {"cell_type": "markdown", "metadata": {"_uuid": "a47b9d70c26b155f5a0e9c89aef6e43ed81dfa0c", "_cell_guid": "33774407-4702-4a5a-a36c-ef6098a8fda5"}, "source": ["For each text given as input, the prediction contains an array with the probabilities for that text belonging to each of the three authors.\n", "We then use the sample submission (we re-use the *id* column) and overwrite the three authors columns with our prediction probabilities"]}, {"execution_count": null, "outputs": [], "metadata": {"_uuid": "46028d4ddd4a47dabcd6ffe9dd13f3dadc57d2ab", "_cell_guid": "0ac90003-74f2-476a-a0b9-52cb3e1f3bf2", "collapsed": true}, "cell_type": "code", "source": ["sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n", "id_column = sample_submission['id']\n", "\n", "authors = pd.DataFrame(prediction, columns=['EAP','HPL','MWS'])\n", "\n", "submission = pd.concat([id_column, authors], axis=1)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f0ee156b67b16db3db94715ac8761d24ee3f28de", "_cell_guid": "abd5a030-bfb5-434f-a8dc-3633a122b365"}, "source": ["Note the *axis=1* above. This means we add columns instead of trying to append rows to our data frame. \n", "We are now ready to save our data to a .csv file and submit it."]}, {"execution_count": null, "outputs": [], "metadata": {"_uuid": "bf52d909cbbfb57a729646e24e82fa94aaf0fce8", "_cell_guid": "5097ed51-763d-417b-adbf-5f049136da85", "collapsed": true}, "cell_type": "code", "source": ["submission.to_csv('submission_long.csv', sep=',', index=False)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "676a14e7b13f4433c8da407678750dbf80ea73d6", "_cell_guid": "580622ed-f36b-491a-af05-0caa5c251a04", "collapsed": true}, "source": ["The two files generated should be identical, and they score a 0.89 on the leaderboard. Not an impressive score, but a good start nonetheless.\n", "And given that you now have a pipeline set up, you can start experimenting with hyper-parameter tuning, cross-validation, and all sorts of other pre-processors and classifiers.\n", "\n", "Happy coding!"]}], "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "pygments_lexer": "ipython3"}}}