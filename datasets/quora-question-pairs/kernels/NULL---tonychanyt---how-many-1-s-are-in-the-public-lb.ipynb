{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "96811c29-4d24-a823-7a3e-c36933b18dd3"
      },
      "source": [
        "From the training set, we know that 36.9% of the pairs are duplicates. However, in the public test set, the percentage is different. Prove it.\n",
        "\n",
        "When submitting a constant probability of 0.369 on the public test set, the log loss score was 0.554."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c909f80d-aece-6913-3f17-f2d756b3fc90"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import log_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b9832a8f-4ef0-76cd-5d89-ae38463749b8"
      },
      "outputs": [],
      "source": [
        "l = []\n",
        "p = [0.369] * 1000\n",
        "print(p)\n",
        "for r in range(1, 1000):\n",
        "    y = [1]*r + [0]*(1000-r)\n",
        "    #print(y)\n",
        "    t=log_loss(y, p)\n",
        "    #print(t)\n",
        "    l.append(t)\n",
        "    #print(l)\n",
        "l = np.array(l)\n",
        "x = np.arange(0.1, 100, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d13c2828-a339-0083-99b6-97efb845b89b"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, l, '_')\n",
        "plt.title('Log Loss vs. Pct. Positve with Constant Prediction of 0.37')\n",
        "plt.xlabel('% Positve in LB')\n",
        "plt.ylabel('Log Loss for Constant Prediction 0.37')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "10d57ca5-6633-9c46-2ec6-36d42ae6802b"
      },
      "source": [
        "From the graph, the log loss of 0.55 that Anokas got looks like it occurs at around 0.165 or 0.17. In fact, we can compute it directly. Using the log loss formula from [here][1], and using the fact that this is a constant prediction, we get:    \n",
        "\n",
        "$$r = \\frac{logloss + log(1-p)}{log\\big( \\frac{1-p}{p}\\big)}$$\n",
        "\n",
        "where r is the fraction of positives. In that expression, p and the logloss are known for Anokas' constant prediction of p=0.369, which gave loss of 0.554. That yields r of 0.174, about the same as the graph.\n",
        "\n",
        "  [1]: https://www.kaggle.com/wiki/LogarithmicLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6c7efcaa-4f1b-d8e4-d42e-2616ab28dd8f"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('../input/test.csv')\n",
        "sub = test[['test_id']].copy()\n",
        "sub['is_duplicate'] = 0.174\n",
        "sub.to_csv('constant_sub.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "99a55440-4b9e-c59a-489a-f37d547e3d8a"
      },
      "source": [
        "So that get's about 0.463 on the LB. Now the bigger question is: How many 1's are in the private LB? Is it just a fluke that the public LB has so many fewer 1's than train?"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}