{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5cc08b2d-6f7c-898c-379e-8ad31baa0836"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aa50b20d-2c02-01dc-22fe-aac92aa317d4"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da9b1e09-b1c4-fe65-81dc-5193c1e26eb7"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection, preprocessing, ensemble\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3353c35a-3e8c-0afc-2ae2-f12af8cf275d"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    data['num_photos'] = data['photos'].apply(len)\n",
        "    data['num_features'] = data['features'].apply(len)\n",
        "\n",
        "    def fill_empty(row):\n",
        "        if row.num_features == 0:\n",
        "            return ['null']\n",
        "        return row.features\n",
        "\n",
        "    data['features_filled'] = data.apply(lambda row: fill_empty(row), axis = 1)\n",
        "    \n",
        "    data['features_word'] = data['features_filled'].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
        "    tfidf = CountVectorizer(stop_words = 'english', max_features = 200)\n",
        "    data_sparse = tfidf.fit_transform(data['features_word'])\n",
        "    \n",
        "    data['created_year'] = pd.to_datetime(data['created']).dt.year\n",
        "    data['created_month'] = pd.to_datetime(data['created']).dt.month\n",
        "    data['created_day'] = pd.to_datetime(data['created']).dt.day\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "70e83eb7-d96d-57bc-2640-021b5123f898"
      },
      "outputs": [],
      "source": [
        "train = pd.read_json('../input/train.json')\n",
        "label_num_map = { 'high': 0, 'medium': 1, 'low': 2 }\n",
        "train['label'] = train['interest_level'].apply(lambda x: label_num_map[x])\n",
        "\n",
        "interest_level = pd.get_dummies(train['interest_level'])\n",
        "interest_level\n",
        "train = pd.concat([train, interest_level], axis = 1)\n",
        "\n",
        "train = preprocess(train)\n",
        "use_features = ['bedrooms', 'bathrooms', 'price', 'num_features', 'num_photos']\n",
        "train_X = train[use_features]\n",
        "train_y = train['label']\n",
        "gradientB_model = GradientBoostingClassifier(n_estimators=20, learning_rate=1.0, max_depth=1, random_state=0).fit(train_X, train_y)\n",
        "gradientB_model = gradientB_model.fit(train_X, train_y)\n",
        "\n",
        "test = pd.read_json('../input/test.json')\n",
        "listing_id = test['listing_id']\n",
        "test = preprocess(test)[use_features]\n",
        "result = gradientB_model.predict(test)\n",
        "\n",
        "result_df = pd.DataFrame({ 'level': result })\n",
        "il = pd.get_dummies(result_df['level'])\n",
        "result_df = pd.concat([result_df, il], axis = 1)\n",
        "result_df = result_df[[0, 1, 2]]\n",
        "result_df.index = test.index\n",
        "result_df.rename(columns = {0: 'high', 1: 'medium', 2: 'low'}, inplace = True)\n",
        "result_df = pd.concat([result_df, listing_id], axis = 1)\n",
        "result_df.set_index('listing_id', inplace = True)\n",
        "result_df.to_csv('result.csv')"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}