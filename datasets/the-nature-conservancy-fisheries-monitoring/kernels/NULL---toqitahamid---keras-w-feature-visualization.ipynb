{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0e96ae8a-8871-4511-4fc3-8f247ee0af58"
      },
      "source": [
        "## Fish Species Classification With Keras Convolutional Neural Network\u00b6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5fe135a2-3814-2cc9-df48-ab0bf720a91a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "import h5py\n",
        "import csv\n",
        "\n",
        "from scipy.misc import imresize, imsave\n",
        "\n",
        "from sklearn.cross_validation import KFold, train_test_split\n",
        "from sklearn.metrics import log_loss, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from PIL import Image, ImageChops, ImageOps\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, Callback\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Activation, Dropout, Flatten, Dense\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6b5da7cd-d0e7-0735-d6a1-70439acbe29c"
      },
      "source": [
        "## Configuration and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aa5096f3-b930-daab-99f6-33dae9f76b6a"
      },
      "outputs": [],
      "source": [
        "### paths to training and testing data\n",
        "train_path = '../data_aug/train' # I'm using an augmented dataset locally so results will be different\n",
        "test_path = '../data_aug/test'\n",
        "\n",
        "### path for preloaded vgg16 weights and bottleneck model (once trained)\n",
        "weights_path = '../vgg16_weights.h5'\n",
        "bottleneck_model_weights_path = '../bottleneck_model_300_aug.h5'  # these need to be in local directory\n",
        "\n",
        "### settings for keras early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=1, mode='auto')\n",
        "\n",
        "### other hyperparameters\n",
        "n_folds = 5\n",
        "batch_size = 8\n",
        "nb_epoch = 2\n",
        "bottleneck_epoch = 3  # used when training bottleneck model\n",
        "val_split = .15  # if not using kfold cv\n",
        "classes = [\"ALB\", \"BET\", \"DOL\", \"LAG\", \"NoF\", \"OTHER\", \"SHARK\", \"YFT\"]\n",
        "num_classes = len(classes)\n",
        "\n",
        "### image dimensions\n",
        "img_width, img_height = 300, 300\n",
        "num_channels = 3\n",
        "\n",
        "### class weights\n",
        "# target_num = 472.125\n",
        "# class_weight = {0: target_num/1719., 1: target_num/200., 2: target_num/117., 3: target_num/67.,\n",
        "#                 4: target_num/465., 5: target_num/299., 6: target_num/176., 7: target_num/734.}\n",
        "class_weight = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "25ee1563-6800-9558-07a4-ea7404bd2e31"
      },
      "source": [
        "## Helper Functions For Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1cd89dd6-0ef4-eaaa-aca0-dc5fa0d80855"
      },
      "outputs": [],
      "source": [
        "def load_images(path):\n",
        "    img = cv2.imread(path)\n",
        "    resized = cv2.resize(img, (img_width, img_height), cv2.INTER_LINEAR)\n",
        "    return resized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "75d260e7-2b9d-21c8-95ed-64fcfaef22b1"
      },
      "outputs": [],
      "source": [
        "def load_train():\n",
        "    X_train = []\n",
        "    X_train_id = []\n",
        "    y_train = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    print('Loading training images...')\n",
        "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
        "    for fld in folders:\n",
        "        index = folders.index(fld)\n",
        "        print('Loading {} files (Index: {})'.format(fld, index))\n",
        "        path = os.path.join(train_path, fld, '*g')\n",
        "        files = glob.glob(path)\n",
        "        for fl in files:\n",
        "            flbase = os.path.basename(fl)\n",
        "            img = load_images(fl)\n",
        "            X_train.append(img)\n",
        "            X_train_id.append(flbase)\n",
        "            y_train.append(index)\n",
        "\n",
        "    print('Training data load time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
        "    return X_train, y_train, X_train_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c66fc235-b311-3762-432c-614e19062ab9"
      },
      "outputs": [],
      "source": [
        "def load_test():\n",
        "    path = os.path.join(test_path, 'test_stg1', '*.jpg')\n",
        "    files = sorted(glob.glob(path))\n",
        "\n",
        "    X_test = []\n",
        "    X_test_id = []\n",
        "    for fl in files:\n",
        "        flbase = os.path.basename(fl)\n",
        "        img = load_images(fl)\n",
        "        X_test.append(img)\n",
        "        X_test_id.append(flbase)\n",
        "\n",
        "    return X_test, X_test_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f063380-fdbd-0ecf-d7f5-618dba32c55a"
      },
      "outputs": [],
      "source": [
        "def normalize_train_data():\n",
        "    train_data, train_target, train_id = load_train()\n",
        "\n",
        "    train_data = np.array(train_data, dtype=np.uint8)\n",
        "    train_target = np.array(train_target, dtype=np.uint8)\n",
        "\n",
        "    train_data = train_data.transpose((0, 3, 1, 2))\n",
        "\n",
        "    train_data = train_data.astype('float32')\n",
        "    train_data = train_data / 255\n",
        "    train_target = np_utils.to_categorical(train_target, 8)\n",
        "\n",
        "    print('Shape of training data:', train_data.shape)\n",
        "    return train_data, train_target, train_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "19208c2f-343a-c369-784b-95074c38293d"
      },
      "outputs": [],
      "source": [
        "def normalize_test_data():\n",
        "    start_time = time.time()\n",
        "    test_data, test_id = load_test()\n",
        "\n",
        "    test_data = np.array(test_data, dtype=np.uint8)\n",
        "    test_data = test_data.transpose((0, 3, 1, 2))\n",
        "\n",
        "    test_data = test_data.astype('float32')\n",
        "    test_data = test_data / 255\n",
        "\n",
        "    print('Shape of testing data:', test_data.shape)\n",
        "    return test_data, test_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a16a53c6-f638-6d6a-f627-c22a940618e8"
      },
      "outputs": [],
      "source": [
        "train_data, train_target, train_id = normalize_train_data()\n",
        "train_data, train_target, train_id = shuffle(train_data, train_target, train_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2eb7cb8c-e54e-7de3-de7c-78420d743610"
      },
      "source": [
        "## Helper Function For Plotting Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bff5f407-9e4f-e3bc-709b-be42baed2234"
      },
      "source": [
        "Function used to plot 9 images in a 3x3 grid (or fewer, depending on how many images are passed), and writing the true and predicted classes below each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c83f2af-0828-fe6f-49cb-f80a73d8a72e"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, cls_true, cls_pred=None):\n",
        "    \n",
        "    if len(images) == 0:\n",
        "        print(\"no images to show\")\n",
        "        return \n",
        "    else:\n",
        "        random_indices = random.sample(range(len(images)), min(len(images), 9))\n",
        "            \n",
        "    images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n",
        "    \n",
        "    # Create figure with 3x3 sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3)\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Plot image.\n",
        "        image = images[i].transpose((1, 2, 0))\n",
        "        ax.imshow(image)\n",
        "\n",
        "        # Show true and predicted classes.\n",
        "        if cls_pred is None:\n",
        "            xlabel = \"True: {0}\".format(cls_true[i])\n",
        "        else:\n",
        "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
        "\n",
        "        # Show the classes as the label on the x-axis.\n",
        "        ax.set_xlabel(xlabel)\n",
        "        \n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ed067645-88d0-0305-185a-8b05452f4624"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "51cf3ce9-aae6-4266-0ba9-b6dd205a3453"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
        "\n",
        "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    \n",
        "    # load the weights of the VGG16 networks\n",
        "    f = h5py.File(weights_path)\n",
        "    for k in range(f.attrs['nb_layers']):\n",
        "        if k >= len(model.layers):\n",
        "            # we don't look at the last (fully-connected) layers in the savefile\n",
        "            break\n",
        "        g = f['layer_{}'.format(k)]\n",
        "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
        "        model.layers[k].set_weights(weights)\n",
        "    f.close()\n",
        "    \n",
        "    # build a classifier model to put on top of the convolutional model\n",
        "    bottleneck_model = Sequential()\n",
        "    bottleneck_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
        "    bottleneck_model.add(Dense(256, activation='relu'))\n",
        "    bottleneck_model.add(Dropout(0.5))\n",
        "    bottleneck_model.add(Dense(8, activation='softmax'))\n",
        "    \n",
        "    # load weights from bottleneck model\n",
        "    bottleneck_model.load_weights(bottleneck_model_weights_path)\n",
        "\n",
        "    # add the model on top of the convolutional base\n",
        "    model.add(bottleneck_model)\n",
        "\n",
        "    # set the first 25 layers (up to the last conv block)\n",
        "    # to non-trainable (weights will not be updated)\n",
        "    for layer in model.layers[:25]:\n",
        "        layer.trainable = False\n",
        "        \n",
        "    # compile the model with a SGD/momentum optimizer\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.9))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "548a4e2d-48ec-a345-c886-c30edd97c9c6"
      },
      "source": [
        "Before we start training, we use the bottleneck method to extract features from the images in our dataset. We save them as .npy files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff969955-449f-b429-9394-c57bb136c2f4"
      },
      "outputs": [],
      "source": [
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # build the VGG16 network\n",
        "    model = Sequential()\n",
        "    model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
        "\n",
        "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    # load the weights of the VGG16 networks\n",
        "    f = h5py.File(weights_path)\n",
        "    for k in range(f.attrs['nb_layers']):\n",
        "        if k >= len(model.layers):\n",
        "            # we don't look at the last (fully-connected) layers in the savefile\n",
        "            break\n",
        "        g = f['layer_{}'.format(k)]\n",
        "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
        "        model.layers[k].set_weights(weights)\n",
        "    f.close()\n",
        "    print('Model loaded.')\n",
        "    \n",
        "    # create validation split\n",
        "    train_data, train_target, _ = normalize_train_data()\n",
        "    X_train, X_valid, Y_train, Y_valid = train_test_split(train_data, train_target, test_size=val_split)\n",
        "\n",
        "    # create generator for train data\n",
        "    generator = datagen.flow(\n",
        "            X_train,\n",
        "            Y_train,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False)\n",
        "    \n",
        "    # save train features to .npy file\n",
        "    bottleneck_features_train = model.predict_generator(generator, X_train.shape[0])\n",
        "    np.save(open('bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
        "\n",
        "    # create generator for validation data\n",
        "    generator = datagen.flow(\n",
        "            X_valid,\n",
        "            Y_valid,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False)\n",
        "    \n",
        "    # save validation features to .npy file\n",
        "    bottleneck_features_validation = model.predict_generator(generator, X_valid.shape[0])\n",
        "    np.save(open('bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)\n",
        "    return Y_train, Y_valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3da78229-123b-244a-d4f9-4ae97ccb39c5"
      },
      "source": [
        "Then we train a base model on these features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "22cda7cc-9220-31e2-7ffd-c5ab34f122c0"
      },
      "outputs": [],
      "source": [
        "def train_bottleneck_model():\n",
        "    train_labels, validation_labels = save_bottleneck_features()\n",
        "\n",
        "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
        "    validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "    model.fit(train_data,\n",
        "              train_labels,\n",
        "              nb_epoch=bottleneck_epoch,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels),\n",
        "              callbacks=[early_stopping],\n",
        "              class_weight=class_weight,\n",
        "              verbose=2)\n",
        "    \n",
        "    model.save_weights(bottleneck_model_weights_path)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b7c1f6eb-b6e2-2fc9-c174-0b1e7632c656"
      },
      "outputs": [],
      "source": [
        "# train_bottleneck_model()  # leave this commented out once it's been done once -- takes a while to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cfcd7d0c-9e6f-9de5-673a-43bdf3718fab"
      },
      "outputs": [],
      "source": [
        "## Main Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "97c1e4a3-d028-a2a8-2eea-4ec82c0771fe"
      },
      "outputs": [],
      "source": [
        "def run_train(n_folds=n_folds):\n",
        "    num_fold = 0\n",
        "    sum_score = 0\n",
        "    models = []   \n",
        "    callbacks = [\n",
        "        early_stopping\n",
        "    ]\n",
        "    \n",
        "    ### if we just want to train a single model without cross-validation, set n_folds to 0 or None\n",
        "    if not n_folds:\n",
        "        model = build_model()\n",
        "        \n",
        "        X_train, X_valid, Y_train, Y_valid = train_test_split(train_data, train_target, test_size=val_split)\n",
        "        print('Training...')\n",
        "        print('Size of train split: ', len(X_train), len(Y_train))\n",
        "        print('Size of validation split: ', len(X_valid), len(Y_valid))\n",
        "              \n",
        "        model.fit(X_train,\n",
        "          Y_train,\n",
        "          batch_size=batch_size,\n",
        "          nb_epoch=nb_epoch,\n",
        "          shuffle=True,\n",
        "          verbose=1,\n",
        "          validation_data=(X_valid, Y_valid),\n",
        "          callbacks=callbacks,\n",
        "          class_weight=class_weight)\n",
        "\n",
        "        predictions_valid = model.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=2)\n",
        "        score = log_loss(Y_valid, predictions_valid)\n",
        "        print('Loss: ', score)\n",
        "        sum_score += score\n",
        "        models.append(model)\n",
        "                     \n",
        "    else:\n",
        "        kf = KFold(len(train_id), n_folds=n_folds, shuffle=True, random_state=7)\n",
        "\n",
        "        for train_index, test_index in kf:\n",
        "            model = build_model()\n",
        "            X_train = train_data[train_index]\n",
        "            Y_train = train_target[train_index]\n",
        "            X_valid = train_data[test_index]\n",
        "            Y_valid = train_target[test_index]\n",
        "\n",
        "            num_fold += 1\n",
        "            print('Training on fold {} of {}...'.format(num_fold, n_folds))\n",
        "            print('Size of train split: ', len(X_train), len(Y_train))\n",
        "            print('Size of validation split: ', len(X_valid), len(Y_valid))\n",
        "\n",
        "            model.fit(X_train,\n",
        "                      Y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      nb_epoch=nb_epoch,\n",
        "                      shuffle=True,\n",
        "                      verbose=1,\n",
        "                      validation_data=(X_valid, Y_valid),\n",
        "                      callbacks=callbacks,\n",
        "                      class_weight=class_weight)\n",
        "\n",
        "            predictions_valid = model.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=2)\n",
        "            score = log_loss(Y_valid, predictions_valid)\n",
        "            print('Loss for fold {0}: '.format(num_fold), score)\n",
        "            sum_score += score*len(test_index)\n",
        "            models.append(model)\n",
        "        score = sum_score/len(train_data)\n",
        "        \n",
        "    print(\"Average loss across folds: \", score)\n",
        "    \n",
        "    info_string = \"loss-{0:.2f}_{1}fold_{2}x{3}_{4}epoch_patience_vgg16\".format(score, n_folds, img_width, img_height, nb_epoch)\n",
        "    return info_string, models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7ef3e8c9-9004-323a-73ed-f5c8b3d19084"
      },
      "source": [
        "## Helper Functions For Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "441e8a1d-6385-9ef3-a325-cb85a711d9cd"
      },
      "outputs": [],
      "source": [
        "def create_submission(predictions, test_id, info):\n",
        "    result = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n",
        "    result.loc[:, 'image'] = pd.Series(test_id, index=result.index)\n",
        "    now = datetime.datetime.now()\n",
        "    sub_file = info + '.csv'\n",
        "    result.to_csv(sub_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f7e54456-e557-e141-9f0e-2862f78b0c0b"
      },
      "outputs": [],
      "source": [
        "def merge_several_folds_mean(data, n_folds):\n",
        "    a = np.array(data[0])\n",
        "    for i in range(1, n_folds):\n",
        "        a += np.array(data[i])\n",
        "    a /= n_folds\n",
        "    return a.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e26068e-8526-3ccb-c936-63c2de28eb90"
      },
      "outputs": [],
      "source": [
        "def ensemble_predict(info_string, models):\n",
        "    num_fold = 0\n",
        "    yfull_test = []\n",
        "    test_id = []\n",
        "    n_folds = len(models)\n",
        "\n",
        "    for i in range(n_folds):\n",
        "        model = models[i]\n",
        "        num_fold += 1\n",
        "        print('Predicting on fold {} of {}'.format(num_fold, n_folds))\n",
        "        test_data, test_id = normalize_test_data()\n",
        "        test_prediction = model.predict(test_data, batch_size=batch_size, verbose=2)\n",
        "        yfull_test.append(test_prediction)\n",
        "\n",
        "    preds = merge_several_folds_mean(yfull_test, n_folds)\n",
        "    create_submission(preds, test_id, info_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "69bf2a30-eef3-dc8c-7717-6ab96fb6f0f9"
      },
      "source": [
        "Run the training and prediction code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58200372-a161-f069-6dfc-52c1abc4e617"
      },
      "outputs": [],
      "source": [
        "info_string, models = run_train()\n",
        "ensemble_predict(info_string, models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "18a5a7bd-f1d2-6469-8435-5140920f0987"
      },
      "source": [
        "## Performance & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "caac2768-704e-2ba3-68ca-6d4431d9e2b0"
      },
      "outputs": [],
      "source": [
        "model = random.choice(models)  # choose a model for visualization\n",
        "\n",
        "### or choose one manually...\n",
        "\n",
        "# model = models[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e48d070-b698-2f4e-01f7-ec2573dbbda9"
      },
      "outputs": [],
      "source": [
        "perm = np.arange(int(val_split*len(train_target)))\n",
        "np.random.shuffle(perm)\n",
        "sample_valid = train_data[perm]\n",
        "labels_valid = train_target[perm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e5b47a3b-3d64-0316-c073-6323ab370698"
      },
      "outputs": [],
      "source": [
        "def plot_example_errors(cls_pred, correct):\n",
        "    # This function is called from print_validation_accuracy() below.\n",
        "\n",
        "    # cls_pred is an array of the predicted class-number for\n",
        "    # all images in the validation set.\n",
        "\n",
        "    # correct is a boolean array whether the predicted class\n",
        "    # is equal to the true class for each image in the validation set.\n",
        "\n",
        "    # Negate the boolean array.\n",
        "    incorrect = (correct == False)\n",
        "    \n",
        "    # Get the images from the validation set that have been\n",
        "    # incorrectly classified.\n",
        "    images = sample_valid[incorrect]\n",
        "    \n",
        "    # Get the predicted classes for those images.\n",
        "    cls_pred = cls_pred[incorrect]\n",
        "\n",
        "    # Get the true classes for those images.\n",
        "    labels = np.array([classes[np.argmax(x)] for x in labels_valid])\n",
        "    cls_true = labels[incorrect]\n",
        "    \n",
        "    # Plot the first 9 images.\n",
        "    plot_images(images=images[0:9],\n",
        "                cls_true=cls_true[0:9],\n",
        "                cls_pred=cls_pred[0:9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "75b78fd4-8a86-bfde-e68f-ef3f4cda6ff7"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cls_pred):\n",
        "    # This is called from print_validation_accuracy() below.\n",
        "\n",
        "    # cls_pred is an array of the predicted class-number for\n",
        "    # all images in the validation set.\n",
        "\n",
        "    # Get the true classifications for the test-set.\n",
        "    cls_true = [classes[np.argmax(x)] for x in labels_valid]\n",
        "    \n",
        "    # Get the confusion matrix using sklearn.\n",
        "    cm = confusion_matrix(y_true=cls_true,\n",
        "                          y_pred=cls_pred,\n",
        "                          labels=classes)\n",
        "\n",
        "    # Print the confusion matrix as text.\n",
        "    print(cm)\n",
        "\n",
        "    # Plot the confusion matrix as an image.\n",
        "    plt.matshow(cm)\n",
        "\n",
        "    # Make various adjustments to the plot.\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(num_classes)\n",
        "    plt.xticks(tick_marks, classes)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "98d4023c-d7d2-4474-548b-ebb276d000e2"
      },
      "outputs": [],
      "source": [
        "def print_validation_accuracy(show_example_errors=False,\n",
        "                              show_confusion_matrix=False):\n",
        "    \n",
        "    test_batch_size = 4\n",
        "    \n",
        "    # Number of images in the validation set.\n",
        "    num_test = len(labels_valid)\n",
        "    \n",
        "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
        "    \n",
        "    i = 0\n",
        "    # iterate through batches and create list of predictions\n",
        "    while i < num_test:\n",
        "        # The ending index for the next batch is denoted j.\n",
        "        j = min(i + test_batch_size, num_test)\n",
        "\n",
        "        # Get the images from the test-set between index i and j.\n",
        "        images = sample_valid[i:j, :]\n",
        "\n",
        "        # Calculate the predicted class using TensorFlow.\n",
        "        cls_pred[i:j] = [np.argmax(x) for x in model.predict(images)]\n",
        "\n",
        "        # Set the start-index for the next batch to the\n",
        "        # end-index of the current batch.\n",
        "        i = j\n",
        "    \n",
        "    # Convenience variable for the true class-numbers of the validation set.\n",
        "    cls_pred = np.array([classes[x] for x in cls_pred])\n",
        "    cls_true = np.array([classes[np.argmax(x)] for x in labels_valid])\n",
        "\n",
        "    # Create a boolean array whether each image is correctly classified.\n",
        "    correct = (cls_true == cls_pred)\n",
        "\n",
        "    # Calculate the number of correctly classified images.\n",
        "    # When summing a boolean array, False means 0 and True means 1.\n",
        "    correct_sum = correct.sum()\n",
        "\n",
        "    # Classification accuracy is the number of correctly classified\n",
        "    # images divided by the total number of images in the test-set.\n",
        "    acc = float(correct_sum) / num_test\n",
        "\n",
        "    # Print the accuracy.\n",
        "    msg = \"Accuracy on validation set: {0:.1%} ({1} / {2})\"\n",
        "    print(msg.format(acc, correct_sum, num_test))\n",
        "\n",
        "    # Plot some examples of mis-classifications, if desired.\n",
        "    if show_example_errors:\n",
        "        print(\"Example errors:\")\n",
        "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
        "\n",
        "    # Plot the confusion matrix, if desired.\n",
        "    if show_confusion_matrix:\n",
        "        print(\"Confusion Matrix:\")\n",
        "        plot_confusion_matrix(cls_pred=cls_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4cd4904b-8948-af33-a268-90200a24c8f3"
      },
      "outputs": [],
      "source": [
        "print_validation_accuracy(show_example_errors=False, show_confusion_matrix=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "415e0cbe-b9fe-fef5-2fd4-6f439e278fab"
      },
      "outputs": [],
      "source": [
        "layer_name = 'conv5_3'\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    if K.image_dim_ordering() == 'th':\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "\n",
        "# this is the placeholder for the input images\n",
        "input_img = model.input\n",
        "\n",
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "kept_filters = []\n",
        "for filter_index in range(0, 512):\n",
        "    if filter_index % 64 == 0:\n",
        "        print('Processing filter %d' % filter_index)\n",
        "    start_time = time.time()\n",
        "\n",
        "    # we build a loss function that maximizes the activation\n",
        "    # of the nth filter of the layer considered\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    if K.image_dim_ordering() == 'th':\n",
        "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
        "    else:\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "    # we compute the gradient of the input picture wrt this loss\n",
        "    grads = K.gradients(loss, input_img)[0]\n",
        "\n",
        "    # normalization trick: we normalize the gradient\n",
        "    grads = normalize(grads)\n",
        "\n",
        "    # this function returns the loss and grads given the input picture\n",
        "    iterate = K.function([input_img], [loss, grads])\n",
        "\n",
        "    # step size for gradient ascent\n",
        "    step = 1.\n",
        "\n",
        "    # we start from a gray image with some random noise\n",
        "    if K.image_dim_ordering() == 'th':\n",
        "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
        "    else:\n",
        "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
        "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
        "\n",
        "    # we run gradient ascent for 20 steps\n",
        "    for i in range(20):\n",
        "        loss_value, grads_value = iterate([input_img_data])\n",
        "        input_img_data += grads_value * step\n",
        "\n",
        "        if loss_value <= 0.:\n",
        "            # some filters get stuck to 0, we can skip them\n",
        "            break\n",
        "\n",
        "    # decode the resulting input image\n",
        "    if loss_value > 0:\n",
        "        img = deprocess_image(input_img_data[0])\n",
        "        kept_filters.append((img, loss_value))\n",
        "    end_time = time.time()\n",
        "\n",
        "# we will stich the best n**2 filters on a n x n grid.\n",
        "n = 5\n",
        "\n",
        "# the filters that have the highest loss are assumed to be better-looking.\n",
        "# we will only keep the top n**2 filters.\n",
        "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
        "kept_filters = kept_filters[:n * n]\n",
        "\n",
        "# build a black picture with enough space for\n",
        "# our n x n filters of size with a 5px margin in between\n",
        "margin = 5\n",
        "width = n * img_width + (n - 1) * margin\n",
        "height = n * img_height + (n - 1) * margin\n",
        "stitched_filters = np.zeros((width, height, 3))\n",
        "\n",
        "# fill the picture with our saved filters\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        img, loss = kept_filters[i * n + j]\n",
        "        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
        "                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
        "\n",
        "# save image and display\n",
        "imsave('feats.jpg', stitched_filters)\n",
        "plt.imshow(stitched_filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1652c90a-5d3a-734d-3240-60158298bfab"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7ce3c0c-304f-e45d-7d7c-542460b4aeee"
      },
      "outputs": [],
      "source": [
        "### if we like this model, save the weights\n",
        "\n",
        "#model.save_weights(\"favorite_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "35c09403-80f4-e76c-8870-e05d886c16c3"
      },
      "source": [
        "## MISC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "601edf8d-8aec-6f3c-d3d1-4b78bd41b311"
      },
      "source": [
        "Script for adding augmented images to dataset using keras ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2e836ecd-305e-f6d9-8d9d-cddc768dd9d1"
      },
      "outputs": [],
      "source": [
        "### augmentation script\n",
        "\n",
        "# train_path = '../data_aug/train/YFT/'\n",
        "\n",
        "# ## define data preparation\n",
        "# datagen = ImageDataGenerator(\n",
        "#                              width_shift_range=.1,\n",
        "#                              )\n",
        "\n",
        "# ## fit parameters from data\n",
        "# generator = datagen.flow_from_directory(\n",
        "#                            train_path,\n",
        "#                            target_size=(512, 512),\n",
        "#                            class_mode=None,\n",
        "#                            batch_size=335,\n",
        "#                            shuffle=True,\n",
        "#                            save_to_dir=train_path,\n",
        "#                            save_prefix=\"aug_\"\n",
        "#                            )\n",
        "\n",
        "\n",
        "# for X_batch, y_batch in generator:\n",
        "#     break"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}