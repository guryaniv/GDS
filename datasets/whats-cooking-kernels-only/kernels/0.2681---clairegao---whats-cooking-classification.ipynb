{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n# Data Cleanning \nimport json\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n\n# Model \nfrom scipy import stats\nfrom math import sqrt\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33ba6c343c3051e718c1a69ff85a1b7a9736e7ec"},"cell_type":"markdown","source":"## Read Data"},{"metadata":{"trusted":true,"_uuid":"eb6a599c7eaeadbc57caac67ac797fde0a08c216"},"cell_type":"code","source":"Train_data = pd.read_json('../input/train.json')\nTest_data = pd.read_json('../input/test.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b106cc9764248de73198f2131a0e243cdd270fbf"},"cell_type":"code","source":"# Transfer list of dictionaries to Dataframe\nTrain_Raw = pd.DataFrame.from_dict(Train_data)\nTrain_Raw.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"899046e792b01dd280e691fecf83086b047dea19"},"cell_type":"markdown","source":"## Data Cleanning"},{"metadata":{"trusted":true,"_uuid":"b0eaab2800218c21082b46c2be3f67f19a14066d"},"cell_type":"code","source":"def Targetencoding(df):\n    df[\"cuisine\"].replace([\"italian\"], 1, inplace=True)\n    df[\"cuisine\"].replace([\"mexican\"], 2, inplace=True)\n    df[\"cuisine\"].replace([\"southern_us\"], 3, inplace=True)\n    df[\"cuisine\"].replace([\"indian\"], 4, inplace=True)\n    df[\"cuisine\"].replace([\"chinese\"], 5, inplace=True)\n    df[\"cuisine\"].replace([\"french\"], 6, inplace=True)\n    df[\"cuisine\"].replace([\"cajun_creole\"], 7, inplace=True)\n    df[\"cuisine\"].replace([\"thai\"], 8, inplace=True)\n    df[\"cuisine\"].replace([\"japanese\"], 9, inplace=True)\n    df[\"cuisine\"].replace([\"greek\"], 10, inplace=True)\n    \n    df[\"cuisine\"].replace([\"spanish\"], 11, inplace=True)\n    df[\"cuisine\"].replace([\"korean\"], 12, inplace=True)\n    df[\"cuisine\"].replace([\"vietnamese\"], 13, inplace=True)\n    df[\"cuisine\"].replace([\"moroccan\"], 14, inplace=True)\n    df[\"cuisine\"].replace([\"british\"], 15, inplace=True)\n    df[\"cuisine\"].replace([\"filipino\"], 16, inplace=True)\n    df[\"cuisine\"].replace([\"irish\"], 17, inplace=True)\n    df[\"cuisine\"].replace([\"jamaican\"], 18, inplace=True)\n    df[\"cuisine\"].replace([\"russian\"], 19, inplace=True)\n    df[\"cuisine\"].replace([\"brazilian\"], 20, inplace=True)\n    \n    return df\n\nTrain_Raw = Targetencoding(Train_Raw)\nTrain_Raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bca4c809a78af3dd9deec3db49d3766f21e89ffd"},"cell_type":"code","source":"Unique_Wordlist = sorted(list(set([element.lower().split(\" \")[-1].replace('(','').replace(')','') for element in np.unique(np.hstack(Train_Raw.ingredients)).tolist()])))[1:]\nUnique_Wordlist[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b83677f2a06258f188549023371bd791d932beb"},"cell_type":"code","source":"def dataPreprocessor(df,k):\n    # create stop word dictionary\n    stop = ['sauce','mix','powder','paste']\n    \n    counter = Counter()\n    counter.update([word.lower().split(\" \")[-1] for word in np.hstack(df.ingredients).tolist() if word.lower().split(\" \")[-1] not in stop])\n\n    topk = counter.most_common(k)\n    test = []\n    \n    for i in range(len(df)):\n        tempCounter = Counter([word.lower().split(\" \")[-1] for word in df.ingredients[i] if word.lower().split(\" \")[-1] not in stop])\n        topkinDoc = [tempCounter[word] if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n        \n        test.append([df.id[i]]+[df.cuisine[i]]+topkinDoc)    # [df.id[i]]+\n    \n    data = pd.DataFrame(test)\n    dfName = []\n    for c in topk:\n        dfName.append(c[0])\n        \n    data.columns = ['id','target'] + dfName     # 'id',\n    return topk, data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b3231f604027f0350d7e25ab34762fe95b7d539"},"cell_type":"code","source":"# Data cleanning - transfer list of text in the \"ingredients\" column to boolean representation \n\nTrain_word,clean_traindata = dataPreprocessor(Train_Raw,k=160)\nclean_traindata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b67b19016a5f6fcde3ea3ae25f8a05cf3fc1d49"},"cell_type":"code","source":"clean_traindata.groupby('target')['id'].nunique().sort_values(ascending=False)\n\n# Check, 20 types of dishes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a1ca3fcbc178f4a7dd1d2c3b0ef396c9bb4c1a3"},"cell_type":"markdown","source":"## Test Data Cleanning"},{"metadata":{"trusted":true,"_uuid":"be6417d8e509613ab854dbdb884f76062fc97d78"},"cell_type":"code","source":"Test_Raw = pd.DataFrame.from_dict(Test_data)\nTest_Raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62c1f10aa1777dc4369608f41abc148cf9f758a6"},"cell_type":"code","source":"def dataPreprocessor2(df,k):\n    # create stop word dictionary\n    stop = ['sauce','mix','powder','paste']\n    \n    counter = Counter()\n    counter.update([word.lower().split(\" \")[-1] for word in np.hstack(df.ingredients).tolist() if word.lower().split(\" \")[-1] not in stop])\n\n    topk = counter.most_common(k)\n    test = []\n    \n    for i in range(len(df)):\n        tempCounter = Counter([word.lower().split(\" \")[-1] for word in df.ingredients[i] if word.lower().split(\" \")[-1] not in stop])\n\n        topkinDoc = [tempCounter[word] if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n        test.append([df.id[i]]+topkinDoc)\n    \n    data = pd.DataFrame(test)\n    dfName = []\n    for c in topk:\n        dfName.append(c[0])\n        \n    data.columns = ['id'] + dfName     \n    return topk, data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"618920d97e22dadf917295624e909eed1c015e5f"},"cell_type":"code","source":"Test_word,clean_testdata = dataPreprocessor2(Test_Raw,k=160)\nclean_testdata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2643d0a22291b18d27edd990b200286942269ad6"},"cell_type":"markdown","source":"## Test on Feature Size Selection"},{"metadata":{"trusted":true,"_uuid":"7b4c44f0c757da8ca94c4ee39f0c7a95c09da304"},"cell_type":"code","source":"# #Confidence Interval Function\n\n# def mean_confidence_interval(data, confidence=0.95):\n#     a = 1.0*np.array(data)\n#     n = len(a)\n#     mu,sd = np.mean(a),np.std(a)\n#     z = stats.t.ppf(confidence, n)\n#     h=z*sd/sqrt(n)\n#     return mu, h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5f64280428644c92fbb5ba8c3c4c3f917f963d5"},"cell_type":"code","source":"# def featureSizeAC(data, num_run, **params):\n\n#     feature_precentage = np.linspace(0.1, 1, 10, endpoint=True)\n    \n#     columnsize = len(data.columns)-2\n#     train_scores = []\n#     test_scores = []\n#     train_mean_fs = []\n#     train_ci_fs = []\n#     test_mean_fs = []\n#     test_ci_fs = []\n    \n#     classifier = KNeighborsClassifier(n_neighbors=int(len(data)/20))\n        \n#     for i in range(len(feature_precentage)):\n#         sliceindex = int(columnsize*feature_precentage[i])\n#         features_df = data.iloc[:,2:sliceindex]\n#         features = features_df.as_matrix()\n#         target_df = data['target']\n#         target = target_df.as_matrix()\n\n#         for j in range(num_run):\n#             features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.25, stratify = target)\n            \n#             clfModel = classifier.fit(features_train, target_train)\n#             train_target_pred = clfModel.predict(features_train)\n#             test_target_pred = clfModel.predict(features_test)\n\n#             train_scores.append(metrics.accuracy_score(target_train, train_target_pred))\n#             test_scores.append(metrics.accuracy_score(target_test, test_target_pred))      \n    \n#         train_mean,train_ci = mean_confidence_interval(train_scores)\n#         test_mean,test_ci = mean_confidence_interval(test_scores) \n#         train_mean_fs.append(train_mean)\n#         train_ci_fs.append(train_ci)\n#         test_mean_fs.append(test_mean)\n#         test_ci_fs.append(test_ci)\n\n#     return train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a5fccc10470a9bdf77cf670114ce797968dfa6f"},"cell_type":"code","source":"# train_mean_fs, train_ci_fs, test_mean_fs, test_ci_fs = featureSizeAC(clean_traindata, 1, c=1.0)\n\n# print(\"Train\\\n#     \\nAverage Accuracy: {0} \\\n#     \\nConfidence Interval: {1}\\n\".format(train_mean_fs, train_ci_fs)\n#      )\n\n# print(\"Test\\\n#     \\nAverage Accuracy: {0} \\\n#     \\nConfidence Interval: {1}\".format(test_mean_fs, test_ci_fs)\n#      )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cf894f836ecb72c06c1ce867df5e43d37ce5094"},"cell_type":"markdown","source":"## Model Testing"},{"metadata":{"trusted":true,"_uuid":"1b442f5b93ec33b09da0773d2d19ec5cfe753c47"},"cell_type":"code","source":"## Tune Model \n\n# model = KNeighborsClassifier()\n\n# features_df = clean_traindata.iloc[:,2:]\n# features = features_df.as_matrix()\n# target_df = clean_traindata['target']\n# target = target_df.as_matrix()\n# features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.25, stratify = target)\n\n# params = {'n_neighbors':[int(len(clean_traindata)/20),int(len(clean_traindata)/40)]}\n\n# model1 = GridSearchCV(model, param_grid=params, n_jobs=1)\n\n# model1.fit(features_train,target_train)\n\n# print(\"Best Hyper Parameters:\\n\",model1.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"821382911394819bf655bc957ca5a86ff889ba17"},"cell_type":"code","source":"# classifier = KNeighborsClassifier(n_neighbors= int(len(clean_traindata)/20))\n\n# test_features = clean_testdata.iloc[:,1:]\n\n# clfModel = classifier.fit(features, target)\n# test_target_pred = clfModel.predict(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2a4202625e7da5370aa7c894017a4a7893c8fa0","scrolled":true},"cell_type":"code","source":"# Without tuning\n\nclassifier = KNeighborsClassifier(n_neighbors= int(len(clean_traindata)/20))\n        \nfeatures_df = clean_traindata.iloc[:,2:]\nfeatures = features_df.as_matrix()\ntarget_df = clean_traindata['target']\ntarget = target_df.as_matrix()\n\ntest_features = clean_testdata.iloc[:,1:]\n\nclfModel = classifier.fit(features, target)\ntest_target_pred = clfModel.predict(test_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fbde6b27af217712b7e7cf07222c3fff5228af5"},"cell_type":"code","source":"output = pd.DataFrame(test_target_pred, columns = ['cuisine'])\noutput['id'] = clean_testdata['id'] \noutput = output[['id','cuisine']]\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6486a011578ff5456faa2524c76e3a387f76c363"},"cell_type":"markdown","source":"## Results Cleanning"},{"metadata":{"trusted":true,"_uuid":"69fb89fac13ca7afb2cc6571069125550d67591d"},"cell_type":"code","source":"def Targetencoding_back(df):\n    df[\"cuisine\"].replace(1, \"italian\", inplace=True)\n    df[\"cuisine\"].replace(2, \"mexican\", inplace=True)\n    df[\"cuisine\"].replace(3, \"southern_us\", inplace=True)\n    df[\"cuisine\"].replace(4, \"indian\", inplace=True)\n    df[\"cuisine\"].replace(5, \"chinese\", inplace=True)\n    df[\"cuisine\"].replace(6, \"french\", inplace=True)\n    df[\"cuisine\"].replace(7, \"cajun_creole\", inplace=True)\n    df[\"cuisine\"].replace(8, \"thai\", inplace=True)\n    df[\"cuisine\"].replace(9, \"japanese\", inplace=True)\n    df[\"cuisine\"].replace(10, \"greek\", inplace=True)\n    \n    df[\"cuisine\"].replace(11, \"spanish\", inplace=True)\n    df[\"cuisine\"].replace(12, \"korean\", inplace=True)\n    df[\"cuisine\"].replace(13, \"vietnamese\", inplace=True)\n    df[\"cuisine\"].replace(14, \"moroccan\", inplace=True)\n    df[\"cuisine\"].replace(15, \"british\", inplace=True)\n    df[\"cuisine\"].replace(16, \"filipino\", inplace=True)\n    df[\"cuisine\"].replace(17, \"irish\", inplace=True)\n    df[\"cuisine\"].replace(18, \"jamaican\", inplace=True)\n    df[\"cuisine\"].replace(19, \"russian\", inplace=True)\n    df[\"cuisine\"].replace(20, \"brazilian\", inplace=True)    \n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7ea6ecd206a0090c397aa3ffa54c3172e58d829"},"cell_type":"code","source":"results = Targetencoding_back(output)\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b948b7a927a22a21a529c331d85750b4d11ce49"},"cell_type":"code","source":"results.groupby('cuisine')['id'].nunique().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9b12047511c8c10913f7a0ea710ac2e2de2be99"},"cell_type":"code","source":"results.to_csv('results.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"377d1059d5972bc8c984bf54979cd1fb0ce6b2ab"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}