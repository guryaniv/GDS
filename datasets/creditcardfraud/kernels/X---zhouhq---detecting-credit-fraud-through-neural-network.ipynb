{"nbformat_minor": 1, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "name": "python", "version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python"}}, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": ["**Hongqiang Zhou**\n", "\n", "Silver Spring, MD\n", "\n", "This notebook continues my work on the credit card fraud detection. In a previous notebook (https://www.kaggle.com/zhouhq/credit-fraud-detection-the-power-of-ensemble), we have practiced algorithms of logistic regression, classification tree, random forest, and AdaBoost. Instead of expanding that notebook, we start this new one. \n", "\n", "In this notebook, we build a neural network with the help of TensorFlow. The model initialize parameters through Adam optimizer, computes the loss function through gradient descent on mini-batches, and optimizes hyper-parameters thorugh L2-regularization. A two-layer network is then constructed and tuned on the training data. \n", "\n", "The data is highly skewed. In the previous notebook, we have boosted the minority class through SMOTE technique on training data. In this notebook, we try a different way to balance classes in training data: under-sampling. This approach removes most instances of the major class from training data set. The advantage of this approach is that it balances the classes without introducing hypothetical data. The disadvantage is that we lose a great amount of data. To compensate for the lack of data, cross-validation is employed in model training.\n", "\n", "First, let us import modules to be employed in this project."], "metadata": {"_cell_guid": "c878ae5e-fd23-48d8-a9f6-4f812d8f97ea", "_uuid": "34edea5b0ce6c0078a8e50dd3d82fcb4804bf861"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "1aabe489-83bd-4f02-8aff-ccd5e2e25cae", "_uuid": "4e3af560484d30f604758d5a9ff1ecc337bf05c3"}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "from tensorflow.python.framework import ops\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n", "from itertools import cycle\n", "\n", "plt.rcParams['figure.figsize'] = (7, 4)\n", "plt.rcParams['image.interpolation'] = 'nearest'\n", "plt.rcParams['image.cmap'] = 'gray'\n", "%matplotlib inline"]}, {"cell_type": "markdown", "source": ["**Construct a neural network through TensorFlow**\n", "\n", "The following functions compose a general neural network model."], "metadata": {"_cell_guid": "5b0f654c-1faa-4bfc-a3c2-0f46f9167c41", "_uuid": "1eb244081542a7e5571c0146d82f1128099e0794"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "4afc76c5-db09-40a3-ab60-09cff2620083", "_uuid": "b73443b68427ff2ddc4cc70dfc6e9a40979af019"}, "outputs": [], "source": ["def create_placeholders(n_x, n_y):\n", "    X = tf.placeholder(dtype = tf.float32, shape = (n_x, None), name = 'X')\n", "    Y = tf.placeholder(dtype = tf.float32, shape = (n_y, None), name = 'Y')\n", "    return X, Y"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "04bf3a4b-01bb-4974-8113-161a290ef69e", "_uuid": "e5a5f4c12253ba2ddd97a382e59e4c5d1cbf5769"}, "outputs": [], "source": ["def initialize_parameters(layers_dims):\n", "    num_layers = len(layers_dims) - 1\n", "    parameters = {}\n", "    for l in range(1, num_layers + 1):\n", "        parameters['W' + str(l)] = tf.get_variable('W' + str(l), [layers_dims[l], layers_dims[l - 1]],\\\n", "                            initializer = tf.contrib.layers.xavier_initializer(seed = next(seeds)))\n", "        parameters['b' + str(l)] = tf.get_variable('b' + str(l), [layers_dims[l], 1], \\\n", "                                                   initializer = tf.zeros_initializer())\n", "    \n", "    return parameters   "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "86f45352-6243-4786-ba9f-8c5910117a0d", "_uuid": "d59182f1655c77b9af3a16e084e5cf18d1bcdf40"}, "outputs": [], "source": ["def forward_propagation(X, parameters):\n", "    L = len(parameters) // 2\n", "    A = X\n", "    for l in range(1, L):\n", "        Z = tf.add(tf.matmul(parameters['W' + str(l)], A), parameters['b' + str(l)])\n", "        A = tf.nn.relu(Z)\n", "    ZL = tf.add(tf.matmul(parameters['W' + str(L)], A), parameters['b' + str(L)])\n", "    \n", "    return ZL"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "e1ce88e4-ed71-40c9-8b47-434fc16a079e", "_uuid": "2dfa9e27b5c45e1ffcd487cb02019c1afaa5bf15"}, "outputs": [], "source": ["def compute_l2_regularization_cost(parameters, l2):\n", "    L = len(parameters) // 2\n", "    cost = 0.0\n", "    for l in range(1, L + 1):\n", "        cost += tf.reduce_sum(tf.nn.l2_loss(parameters['W' + str(l)]))\n", "    l2_regularization_cost = cost * l2\n", "    \n", "    return l2_regularization_cost"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "8e5ba5b1-3cbb-4a65-a14f-b4e8d9dd104d", "_uuid": "522a633a82fd4945f754354f17a8c08fe5382cba"}, "outputs": [], "source": ["def compute_cross_entropy_cost(ZL, Y):\n", "    cross_entropy_cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits = ZL, \\\n", "                                                                               labels = Y))\n", "    \n", "    return cross_entropy_cost                                   "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "9360cfe3-82a9-41fc-8935-3249a788efed", "_uuid": "1b0157ccf61d8077f2a3c4e5ede0807a33d29814"}, "outputs": [], "source": ["def random_mini_batches(X, Y, minibatch_size = 64):\n", "    m = X.shape[1]\n", "    minibatches = []\n", "    \n", "    np.random.seed(next(seeds))\n", "    permutation = list(np.random.permutation(m))\n", "    shuffled_X = X[:, permutation]\n", "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0], m))\n", "    \n", "    num_complete_minibatches = m // minibatch_size\n", "    for k in range(0, num_complete_minibatches):\n", "        minibatch_X = shuffled_X[:, k * minibatch_size : (k + 1) * minibatch_size]\n", "        minibatch_Y = shuffled_Y[:, k * minibatch_size : (k + 1) * minibatch_size]\n", "        minibatch = (minibatch_X, minibatch_Y)\n", "        minibatches.append(minibatch)\n", "    \n", "    if m % minibatch_size != 0:\n", "        minibatch_X = shuffled_X[:, num_complete_minibatches * minibatch_size :]\n", "        minibatch_Y = shuffled_Y[:, num_complete_minibatches * minibatch_size :]\n", "        minibatch = (minibatch_X, minibatch_Y)\n", "        minibatches.append(minibatch)\n", "        \n", "    return minibatches"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "c2a9a740-f344-47ca-aca9-fc1063188090", "_uuid": "385d6aa141c97197a338349a5cfbcdda626a9b20"}, "outputs": [], "source": ["def model(X_train, Y_train, layers_dims, l2 = 1e-6, learning_rate = 0.0001, \n", "          num_epochs = 1500, minibatch_size = 64, print_cost = True):\n", "    ops.reset_default_graph()\n", "    #tf.set_random_seed(seed)\n", "    (n_x, m) = X_train.shape\n", "    n_y = Y_train.shape[0]\n", "    costs = []\n", "    \n", "    X, Y = create_placeholders(n_x, n_y)\n", "    parameters = initialize_parameters(layers_dims)\n", "    ZL = forward_propagation(X, parameters)\n", "    cross_entropy_cost = compute_cross_entropy_cost(ZL, Y)\n", "    l2_regularization_cost = compute_l2_regularization_cost(parameters, l2)\n", "    cost = cross_entropy_cost + l2_regularization_cost \n", "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n", "    init = tf.global_variables_initializer()\n", "    \n", "    with tf.Session() as sess:\n", "        sess.run(init)\n", "        \n", "        for epoch in range(num_epochs):\n", "            epoch_cost = 0.0\n", "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n", "            num_minibatches = len(minibatches)\n", "            \n", "            for minibatch in minibatches:\n", "                (minibatch_X, minibatch_Y) = minibatch\n", "                _, minibatch_cost = sess.run([optimizer, cost], feed_dict = {X: minibatch_X, \n", "                                                                             Y: minibatch_Y})\n", "                epoch_cost += minibatch_cost\n", "                \n", "            epoch_cost = epoch_cost / m    \n", "            costs.append(epoch_cost)    \n", "            \n", "            if print_cost and epoch % 100 == 0:\n", "                print('Cost after epoch {}: {}'.format(epoch, np.float(epoch_cost)))\n", "        else:\n", "            if print_cost:\n", "                print('Cost after epoch {}: {}'.format(epoch, np.float(epoch_cost)))\n", "                \n", "        parameters = sess.run(parameters)\n", "        return parameters, costs"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "b42c9448-8bc5-41f7-b30a-0d55f3cfebd0", "_uuid": "f3400f346ffecc06d7669b6622444bb9447d736b"}, "outputs": [], "source": ["def predict(parameters, X):\n", "    nx = X.shape[0]\n", "    params = {}\n", "    L = len(parameters) // 2\n", "    for l in range(1, L+1):\n", "        params['W' + str(l)] = tf.convert_to_tensor(parameters['W' + str(l)])\n", "        params['b' + str(l)] = tf.convert_to_tensor(parameters['b' + str(l)])\n", "    \n", "    x = tf.placeholder(dtype = tf.float32, shape = (nx, None))\n", "    z = forward_propagation(x, params) \n", "    a = tf.sigmoid(z)\n", "    \n", "    with tf.Session() as sess:\n", "        proba = sess.run(a, feed_dict = {x: X})\n", "        \n", "    return proba"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "23637b9f-3117-4761-a1fe-5fb3e0dcb4a0", "_uuid": "d2644913850dc519e04b294f63821401b15abec6"}, "outputs": [], "source": ["def model_evaluation(parameters, feature_matrix, target):\n", "    probs = predict(parameters, feature_matrix)\n", "    (fpr, tpr, thresholds) = roc_curve(y_true = target.ravel(), y_score = probs.ravel())\n", "    auc_score = auc(x = fpr, y = tpr)\n", "    fig, ax = plt.subplots()\n", "    ax.plot(fpr, tpr, 'r-', linewidth = 2)\n", "    ax.plot([0, 1], [0, 1], 'k--', linewidth = 1)\n", "    plt.title('ROC curve with AUC = {0:.3f}'.format(auc_score))\n", "    plt.xlabel('fpr')\n", "    plt.ylabel('tpr')\n", "    plt.axis([-0.01, 1.01, -0.01, 1.01])\n", "    plt.tight_layout()\n", "    \n", "    return {'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds, 'auc': auc_score}"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "744a316c-e63e-46ca-95da-474f40e4368c", "_uuid": "8a8a64f53ee7919d4f6e7537cc079b9d3a5d15a4"}, "outputs": [], "source": ["def k_fold_cross_validation(train_data, k, n_h, l2):\n", "    layers_dims = [29, n_h, 1]\n", "    fold_size = train_data.shape[0] // k\n", "    np.random.seed(next(seeds))\n", "    permutation = list(np.random.permutation(train_data.shape[0]))\n", "    shuffled_data = train_data.values[permutation, :]\n", "    shuffled_data = shuffled_data.T\n", " \n", "    error = 0.0\n", "    for i in range(k):\n", "        val_X = shuffled_data[:-1, i * fold_size : (i + 1) * fold_size]\n", "        val_y = shuffled_data[-1, i * fold_size : (i + 1) * fold_size].reshape(1, -1)\n", "        \n", "        train_X = np.concatenate([shuffled_data[:-1, 0 : i * fold_size],  \\\n", "                                  shuffled_data[:-1, (i + 1) * fold_size :]], axis = 1)\n", "        train_y = np.concatenate([shuffled_data[-1, 0 : i * fold_size], \\\n", "                                 shuffled_data[-1, (i + 1) * fold_size :]])\n", "        train_y = train_y.reshape(1, -1)\n", "        \n", "        parameters, _ = model(train_X, train_y, layers_dims, l2, learning_rate = 1e-4, \\\n", "                                   num_epochs = 1500, minibatch_size = 16, print_cost = False)\n", "        \n", "        probs = predict(parameters, val_X)\n", "        preds = np.where(probs > 0.5, 1, 0)\n", "        error += np.sum(preds != val_y)\n", "        \n", "    accuracy = 1.0 - error / train_data.shape[0]\n", "    return accuracy "]}, {"cell_type": "markdown", "source": ["The function in below cell will be employed to tune the hyper-parametrs: number of neurons in hidden layers and L2-penalty coefficient. "], "metadata": {"_cell_guid": "42b06314-9a2e-423d-8a1e-5b67bcd93565", "_uuid": "0765c5f6a210748d94b435fe01267ff1a1e2a66d"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "502992cf-20ca-4d65-afab-83cdc3d44e34", "_uuid": "8705c13c38273f02824f171603b20be08cb3adb5"}, "outputs": [], "source": ["def tune_hparams(train_data, hparams):\n", "    n_h = hparams['n_h'] \n", "    l2 = hparams['l2'] \n", "    accuracies = []\n", "    for n in range(len(n_h)):\n", "        accuracy = k_fold_cross_validation(train_data, 5, n_h[n], l2[n])\n", "        accuracies.append(accuracy)\n", "        print('Trial = {}, n_h = {}, l2 = {}, accuracy = {}'.format(n, n_h[n], l2[n], accuracy))\n", "    \n", "    return accuracies "]}, {"cell_type": "markdown", "source": ["**Prepare the data**"], "metadata": {"collapsed": true, "_cell_guid": "5d0799a3-202c-45f7-8276-c1b9b57699b1", "_uuid": "3a5bc89f2fa599f9fd590f4b60fc0a906b4b5f9c"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "c4d1964a-5ab6-4186-8af6-f491ef77284e", "_uuid": "9c9478c7fa54c4f2e455c23ab1e2cc8e49ffd69f"}, "outputs": [], "source": ["data = pd.read_csv('../input/creditcard.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "31ca6eb5-73a2-4fd4-8b35-e441e6494b9c", "_uuid": "3446db48264fcf7feaf804ecb255de94fa56b5cc"}, "outputs": [], "source": ["features = data.columns\n", "features = [str(s) for s in features]\n", "label = features[-1]\n", "features = features[1 : -1]\n", "data = data[features + [label]]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "f9ef080d-875f-4e18-9948-da4d85132d63", "_uuid": "c1993051fa614b5bbe7559734de3db9814e229ae"}, "outputs": [], "source": ["scaler = StandardScaler().fit(data[features])\n", "scaler_mean = scaler.mean_\n", "scaler_scale = scaler.scale_\n", "data[features] = scaler.transform(data[features])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "af6102e7-c1f3-4102-85b1-5b6c8fef482a", "_uuid": "70a1861d1c45f50ad69a3226200dd1c154999dfa"}, "outputs": [], "source": ["train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "e52dee6a-cd50-4bef-924d-e23736407a0c", "_uuid": "0d3a49d01dc69b295d02e8ff9998f0193882ad09"}, "outputs": [], "source": ["np.random.seed(1)\n", "train_positive = train_data[train_data[label] == 1]\n", "train_negative = train_data[train_data[label] == 0]\n", "indices = np.random.choice(a = train_negative.index, size = train_positive.shape[0], replace = False)\n", "sample_negative = train_negative.loc[indices, :]\n", "sample = pd.concat([train_positive, train_negative.loc[indices, :]], axis = 0)"]}, {"cell_type": "markdown", "source": ["**Train the model**"], "metadata": {"_cell_guid": "5e41681c-353f-4ac6-b319-9ed3d718292d", "_uuid": "b4d55cbd1661b4dbde646df477cabd81b01af88d"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "ec318ba5-8d8e-48d1-ac84-1e5d9a2a22b8", "_uuid": "ac9d9625c1d38282a4afddc7e8eea57db837ad6e"}, "outputs": [], "source": ["np.random.seed(1)\n", "seeds = np.random.randint(0, 10000, 10000)\n", "seeds = cycle(seeds)"]}, {"cell_type": "markdown", "source": ["In below two cells, I demonstrate that the present setup of minibath size, learning rate, and number of epochs is good enough to minimize the cost."], "metadata": {"_cell_guid": "04dcfeb4-ef4e-407a-8e98-8cac12bc1d08", "_uuid": "bb560eaa21814abf93f93f101727228e73cfb352"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "b0fa641c-f1d3-4b6f-a7b8-3d3773c78cb7", "_uuid": "54663ebb70077bba528a2c8b659fcf79fd87a912"}, "outputs": [], "source": ["parameters, costs = model(sample[features].values.T, sample[label].values.reshape(1, -1), [29, 10, 1], \n", "                          l2 = 0.001, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 16, \n", "                          print_cost = True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "87ad471d-6113-470b-9e01-7239a9d437f7", "_uuid": "3a76341a04289e544c25e433d88aba6a7489db23"}, "outputs": [], "source": ["plt.plot(costs)\n", "plt.xlabel('epoch')\n", "plt.ylabel('cost')\n", "plt.title('Cost function minimizes during a few epochs.')\n", "plt.tight_layout()"]}, {"cell_type": "markdown", "source": ["In below cell, we tune a two-layer network. This process has taken a very long time on my PC. My experiment shows employing 16 neurons is likely to yield the best performance. In fact, model performance is not very sensative to this parameter. While keeping all other facts unchanged, varying this parameter can change the model accuracy within a range of less than 5%."], "metadata": {"_cell_guid": "498d9619-4fb6-4348-aa12-862637b68074", "_uuid": "c154f18c793f57466379a41481aafada04588fb0"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "593dacd4-5403-44e2-9d2d-a38ca674694c", "_uuid": "21813d558a18efbf686834d29c17f26749a6001c"}, "outputs": [], "source": ["#n_h = np.arange(4, 22, 2)\n", "#l2 = 10 ** np.linspace(-3, 3, 7)\n", "#n_h, l2 = np.meshgrid(n_h, l2)\n", "#hparams = {'n_h': n_h.ravel(), 'l2': l2.ravel()}\n", "#accuracies = tune_hparams(sample, hparams)                "]}, {"cell_type": "markdown", "source": ["In below cell, we tune the L2-penalty coefficient for a two-layer network with 16 neurons in hidden layer. Once again, I comment off the code as it can take a very too long time to execute. My experiment indicates the best value of this coefficient is around 0.25."], "metadata": {"_cell_guid": "8324e88e-ee40-475c-87a7-f54a390f4e11", "_uuid": "e5872671463eb80185580c4406e2decf410414d6"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "7f32a886-ba07-4df0-856e-5fb751dcce3f", "_uuid": "e020c77fd909446b655256f4af143f1aedfb7647"}, "outputs": [], "source": ["#n_h = np.ones(21) * 16\n", "#l2 = 10 ** np.linspace(-2, 2, 21)\n", "#hparams = {'n_h': n_h, 'l2': l2}\n", "#accuracies = tune_hparams(sample, hparams)     "]}, {"cell_type": "markdown", "source": ["Finally, we train the model with optimized hyper parameters, and evaluate model performance on test data."], "metadata": {"_cell_guid": "8f6eb833-6cca-4ecb-b933-4b22ce497c41", "_uuid": "c29e4b94a5d31763694054b38c4f05b3a50469fd"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "645d7f38-3403-465a-86ef-4c1aa9165ebc", "_uuid": "ee6c672455597cb65e2a14359171507b70db5777"}, "outputs": [], "source": ["parameters, costs = model(sample[features].values.T, sample[label].values.reshape(1, -1), [29, 16, 1], \n", "                          l2 = 0.25, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 16, \n", "                          print_cost = True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "4142a743-2f27-4468-92e1-9fd8493dd8b0", "_uuid": "f913b3dd9f456d02c5034529c76f2cf3d79abf7d"}, "outputs": [], "source": ["metrics = model_evaluation(parameters, test_data[features].T, test_data[label])"]}, {"cell_type": "markdown", "source": ["We also investigate the variation of TPR and FPR as a function of threshold. "], "metadata": {"_cell_guid": "ce18dfab-27c8-42c0-ba12-0f54c4706908", "_uuid": "8f97d2260356618d9cbb2cb23eb1ffc0465b7596"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "8c531c7e-7bbc-416d-9224-a67f950d0c75", "_uuid": "4b39e74044607552a33d22b73017d2cd774b880c"}, "outputs": [], "source": ["plt.plot(metrics['thresholds'], metrics['tpr'], 'r-', linewidth = 2, label = 'tpr')\n", "plt.plot(metrics['thresholds'], metrics['fpr'], 'b-', linewidth = 2, label = 'fpr')\n", "plt.legend(loc = 'best')\n", "plt.axis([0, 1, 0, 1])\n", "plt.xlabel('threshold')\n", "plt.tight_layout()"]}, {"cell_type": "markdown", "source": ["If we set our threshold to 0.9. The metrics of model performance is computed as follows."], "metadata": {"_cell_guid": "7e38f4cb-dafb-4218-b1cd-05fe36e22419", "_uuid": "495ff50aaccc5c0fcad5e7a6f1dff8d4a2e969b6"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "d2f50a01-0905-4bb2-9e40-b2ad447dc9af", "_uuid": "2434d01b61090c3b3e4bcb72c8744514b4e40852"}, "outputs": [], "source": ["probs = predict(parameters, test_data[features].T)\n", "preds = np.where(probs > 0.9, 1, 0)\n", "tn, fp, fn, tp = confusion_matrix(y_true = test_data[label].values.ravel(), \\\n", "                                  y_pred = preds.ravel()).ravel()\n", "print ('(tn, fp, fn, tp) = ({}, {}, {}, {})'.format(tn, fp, fn, tp))\n", "print ('precision = {}'.format(tp / (tp + fp)))\n", "print ('recall = {}'.format(tp / (tp + fn)))\n", "print ('accuracy = {}'.format((tp + tn) / float(len(test_data))))"]}, {"cell_type": "markdown", "source": ["Among the 87 fraudulent transactions, the model correctly identifies 72 and misses 15, at the cost of misclassifying 189 genuine transactions as positive. \n", "\n", "**A brief summary**\n", "\n", "In this notebook, we construct a general neural network model by using the TensorFlow library. A two-layer model is then trained on the under-sampled training data, and evaluted on the test data set. In general, the present model shows some improvement over linear regression and ensembled tree classifiers. Neural network is a powerful tool for problems with a large number of variables. The present problem has only 29 parameters. It may not be a good application for neural network."], "metadata": {"collapsed": true, "_cell_guid": "44882571-a7af-4447-9efe-d8fc3c31ff70", "_uuid": "eff1e25866605b25e1aed070ade8ba1fc9b9df75"}}, {"cell_type": "markdown", "source": [" *Copyright reserved to Hongqiang Zhou (hongqiang.zhou@hotmail.com)*\n", " \n", "*Last updated 25 Oct. 2017* "], "metadata": {"collapsed": true, "_cell_guid": "11201b69-c080-48ef-bd06-67ab9df26046", "_uuid": "b51726816639ba57d6760c3ee99d2d9fce464153"}}]}