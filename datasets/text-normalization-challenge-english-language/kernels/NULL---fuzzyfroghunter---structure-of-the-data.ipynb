{"cells": [{"source": ["# Structure of the data"], "metadata": {"_uuid": "cad812227f41777390854e7d4df5299121ff856c", "_cell_guid": "aadf5ef7-278e-4398-aaac-f6409883b769"}, "cell_type": "markdown"}, {"source": ["!ls ../input"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6c0295b2dceb5b9f5da82b4f73c96c0dceecb58f", "collapsed": true, "_cell_guid": "a4ecc983-91d6-4abb-bb8c-c179e5f78b2c"}, "outputs": []}, {"source": ["TRAIN_DATA = '../input/en_train.csv'\n", "TEST_DATA = '../input/en_test.csv'"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "fbad12ed1745ab3dc51e8eef16e29488f7dcce40", "collapsed": true, "_cell_guid": "6a876e94-acee-4b3a-b42f-eb2d2dcf86b0"}, "outputs": []}, {"source": ["## Columns"], "metadata": {"_uuid": "5f0c91dbb905ccebe84828c5ee5e208fdbbacd5b", "_cell_guid": "55c26ee0-a582-44bb-b163-3463db13f231"}, "cell_type": "markdown"}, {"source": ["import pandas as pd"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ab5c453d746785f8df76d637f369c92ca40ff789", "collapsed": true, "_cell_guid": "d551ea5a-e942-4354-ac0f-2337ad46666e"}, "outputs": []}, {"source": ["train_data = pd.read_csv(TRAIN_DATA)\n", "test_data = pd.read_csv(TEST_DATA)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "4549a7f13a48cb4a662f28bcbd5840325fdea616", "collapsed": true, "_cell_guid": "4776d8a6-d4d2-4d82-a67d-ffede2a93702"}, "outputs": []}, {"source": ["### Columns in training data\n", "\n", "1. `sentence_id` - identifies groups of tokens occuring together in sentences\n", "\n", "2. `token_id` - marks the position of the token in the sentence corresponding to the given `sentence_id`\n", "\n", "3. `class` - semantic type of the token\n", "\n", "4. `before` - the token itself\n", "\n", "5. `after` - the normalized form of the token"], "metadata": {"_uuid": "d770861677a957773d509d5b9be46237fc49f79c", "_cell_guid": "8f5168cc-cab0-4261-9cbe-115ca50aa886"}, "cell_type": "markdown"}, {"source": ["train_data.iloc[:5]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "868050009c6896b38f536d7867599375cdcb3a7e", "collapsed": true, "_cell_guid": "54c0ff4b-04dd-42db-bcd9-c6e7a318728a"}, "outputs": []}, {"source": ["train_data.sample(5)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d31ebb2866269b21c4256617619ffb7a838ece2c", "collapsed": true, "_cell_guid": "adf1e075-db77-4cf1-9728-d48b43dd5827"}, "outputs": []}, {"source": ["### Columns in test data\n", "\n", "1. `sentence_id`\n", "\n", "2. `token_id`\n", "\n", "3. `before`\n", "\n", "These columns form a subset of the ones present in the training data. We certainly do not get the labels, but we also don't get the semantic hints in the form of `class`."], "metadata": {"_uuid": "4c8ebc32840449d15a0f1e6e2a990bfa4a11893e", "_cell_guid": "0c140e49-4d78-4d06-87ef-db60569b2a04"}, "cell_type": "markdown"}, {"source": ["test_data.iloc[:5]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "975d85c5b3ec6c695a11995e9a069cf8ec790856", "collapsed": true, "_cell_guid": "b3331ab0-69b9-4ddc-a560-59d233e55028"}, "outputs": []}, {"source": ["test_data.sample(5)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d38b545615c9029c021af0924dad7e7b5aa83c92", "collapsed": true, "_cell_guid": "7fc0ad6e-9a60-4922-a386-8b77202ad957"}, "outputs": []}, {"source": ["## Questions\n", "\n", "The samples above naturally suggest some questions:\n", "\n", "1. What proportion of tokens in the training set are their own normalizations? How would the trivial normalization perform?\n", "\n", "2. What are the different semantic classes represented in the training data?\n", "\n", "3. How does the above statistic distribute over the different classes?\n", "\n", "4. What is the distribution of classes over the training data?\n", "\n", "5. If classes contain information about whether or not to normalize, can semantic class be inferred from things like character distributions, bigram distributions, etc.?"], "metadata": {"_uuid": "2f2d91546902102735e0161d159d7f532c99766f", "_cell_guid": "adc17e68-ddcf-49fe-8129-4b37faf9921f"}, "cell_type": "markdown"}, {"source": ["## Answers"], "metadata": {"_uuid": "6128fdb6ad85e1b0a2a2a895f745ea019437f9d9", "_cell_guid": "efcda61d-9f11-4b2e-80ec-bde4f12f77cd"}, "cell_type": "markdown"}, {"source": ["### Lazy baseline\n", "\n", "Let us start by identifying the rows of the training data in which some non-trivial normalization is required:"], "metadata": {"_uuid": "f10b3dede4657747b5af074e73099b2237e2e7ed", "_cell_guid": "fdda95af-55f6-4895-a096-7112fffac783"}, "cell_type": "markdown"}, {"source": ["nontrivial_train_data = train_data[train_data.before != train_data.after]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a038f3cda489b1b3fe92b17528b87f7335db0e8e", "collapsed": true, "_cell_guid": "6c3c048f-78a8-4d8c-98eb-bf7c66308faa"}, "outputs": []}, {"source": ["The proportion of rows which require normalization is:"], "metadata": {"_uuid": "5fedee45301f0de79d1dde0dc6247e6700bc6bbc", "_cell_guid": "0faa92b9-eacd-46f8-822f-ae0fae894a28"}, "cell_type": "markdown"}, {"source": ["proportion_nontrivial = nontrivial_train_data.shape[0]/train_data.shape[0]\n", "print(proportion_nontrivial)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9548341347f336dfbeef300cf9da3db7ed4824bf", "collapsed": true, "_cell_guid": "15349c79-2cfd-4693-906e-bbce03cfaf36"}, "outputs": []}, {"source": ["Based on this training data, if we simply \"normalized\" every token to itself, we should expect an accuracy of:"], "metadata": {"_uuid": "a9d99ff2c588762957beb21760adad291c3cb19e", "_cell_guid": "9b020925-5175-4ab3-8c2e-0dacd61d1f78"}, "cell_type": "markdown"}, {"source": ["1 - proportion_nontrivial"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8f63d0f897bceb7b76f65b0b624e3b85b28c8e52", "collapsed": true, "_cell_guid": "ab585f1f-b055-4630-a8de-211051a53c16"}, "outputs": []}, {"source": ["This statistic will serve as a sanity check going forward. If we're not doing at least this well, there is something seriously wrong with the approach!"], "metadata": {"_uuid": "a2457d790ec95791169e24e1309d679e3b6ac8b5", "_cell_guid": "4ebcb348-5997-4526-a6be-aac336fed5dd"}, "cell_type": "markdown"}, {"source": ["### Semantic classes"], "metadata": {"_uuid": "06459bdee6573c5acc6bee6067e8ef6bbb4f4894", "_cell_guid": "8a28bdf0-32bd-4e5f-b6c5-82d9fa7e7b29"}, "cell_type": "markdown"}, {"source": ["A lot of our analysis in this notebook will focus on the semantic classes of tokens in the training data.\n", "\n", "Semantic classes are very important because they provide us invaluable contextual information that we *have* to use in our normalization.\n", "\n", "A very simple example is the way alphabetical characters are processed in class `PLAIN` and in class `LETTERS`.\n", "\n", "Consider the following examples:"], "metadata": {"_uuid": "7cea1d555effcc4f4eb55277f84d40819b8f95a3", "_cell_guid": "71fcee94-8977-4527-a5ed-44f075965171"}, "cell_type": "markdown"}, {"source": ["train_data[train_data['class'] == 'PLAIN'].sample(1)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9a88e027d43fcf2abd816438015fb59be08854d7", "collapsed": true, "_cell_guid": "9fb5f935-eff2-4017-8f0a-8dc73bfb9ab9"}, "outputs": []}, {"source": ["train_data[(train_data['class'] == 'LETTERS')].sample(1)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9db377e63d96aa7bed13588574f34a40495f7aed", "collapsed": true, "_cell_guid": "6de75313-ef96-4755-9f15-1599b5d83a9c"}, "outputs": []}, {"source": ["Without the context provided by semantic class, it would be very hard to tell which type of normalization to apply."], "metadata": {"_uuid": "d1b180b509cee7dc15f935e3f8e3d4f824eeb3c5", "_cell_guid": "799d5515-198d-443c-9702-60e480616bd2"}, "cell_type": "markdown"}, {"source": ["#### Distinct classes\n", "\n", "We can simply ask `pandas` to tell us the unique elements of the `classes` column of the `train_data` dataframe:"], "metadata": {"_uuid": "6f02a204e25721398515bf2cb88adfcef8b8e935", "_cell_guid": "49413335-9b10-4312-b0dd-22acd22017a7"}, "cell_type": "markdown"}, {"source": ["CLASSES = sorted(list(train_data['class'].unique()))\n", "print(CLASSES)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "734184a5364d300255044e55ebfd238399ef8035", "collapsed": true, "_cell_guid": "14c049ec-e92a-4d73-b970-10ad05c6d676"}, "outputs": []}, {"source": ["len(CLASSES)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0f9face99f96c387f3189c1bb6c515a45ebbd83a", "collapsed": true, "_cell_guid": "b6099702-db98-4c54-834b-c24da0754ecf"}, "outputs": []}, {"source": ["#### Nontriviality"], "metadata": {"_uuid": "0f69733d55d1c014490bdfc64f0b7094233eaa56", "_cell_guid": "a7fef9a3-e004-4550-8008-139e76cc6ddb"}, "cell_type": "markdown"}, {"source": ["grouped_by_class = train_data.groupby('class')"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "375b857722ccd354fbf17f0bcb2de229c956a32a", "collapsed": true, "_cell_guid": "1568bb34-cdfd-4f47-8d6d-30fb18b7bd62"}, "outputs": []}, {"source": ["def proportion_nontrivial(df):\n", "    \"\"\"\n", "    Args:\n", "    1. df - Dataframe with 'before' and 'after' columns\n", "    \n", "    Returns:\n", "    Proportion of rows in dataframe for which 'before' is not equal to 'after'\n", "    \"\"\"\n", "    nontrivial = df[df.before != df.after]\n", "    return nontrivial.shape[0]/df.shape[0]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f8284e8435de68f773f5803551dbb15f10571e2c", "collapsed": true, "_cell_guid": "9caf23f6-694c-490b-88e2-647a52dc9ad8"}, "outputs": []}, {"source": ["class_nontriviality = [(key, proportion_nontrivial(group)) for key, group in grouped_by_class]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "58c231e93391007e832a5830a71b7380740e0900", "collapsed": true, "_cell_guid": "2d3532cf-bc53-4fbf-8182-61b70ff28a29"}, "outputs": []}, {"source": ["print('Proportion of rows for which nontrivial normalization is required (by class):\\n')\n", "for key, s in class_nontriviality:\n", "    print('{} - {}'.format(key, s))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "88fe22391090cdbb27c60b2e563caa6084388368", "collapsed": true, "_cell_guid": "984cae28-295d-4c45-98ca-d99acbdfd5a8"}, "outputs": []}, {"source": ["#### Distribution of classes over training data\n", "\n", "The previous section suggests that there is a heavily skewed distribution of classes over the training data. Let us verify."], "metadata": {"_uuid": "5843d9fec958f1eb13f61ef9ff359be401ed2522", "_cell_guid": "fd389c13-2c19-4294-a408-9a959ed7974b"}, "cell_type": "markdown"}, {"source": ["class_weights = [(key, group.shape[0]/train_data.shape[0]) for key, group in grouped_by_class]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d564401c190fcb6890ed4b9fe2fa719dac310e40", "collapsed": true, "_cell_guid": "6ee6d5a4-94d6-419a-b724-f3d65859dcb8"}, "outputs": []}, {"source": ["sorted_class_weights = sorted(class_weights, key=lambda p: -p[1])"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "fd835a1d9cb3a77c5c513d9f19bc1af19e5fc6f5", "collapsed": true, "_cell_guid": "226d86c6-f4e0-441e-9de5-9f6c33f908ac"}, "outputs": []}, {"source": ["print('Proportion of the training data made up by each class (sorted in descending order of weight):\\n')\n", "for key, weight in sorted_class_weights:\n", "    print('{} - {}'.format(key, weight))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6212988adf03483535f8f4bcefc8ed10264d12bd", "collapsed": true, "scrolled": true, "_cell_guid": "af3086e4-7841-4c5d-b606-a3067fb6d531"}, "outputs": []}, {"source": ["Let us verify as a sanity check that this is consistent with the nontriviality of the entire training set:"], "metadata": {"_uuid": "55a0c7631093b3ac18c6874f91f249bb08a50fe7", "_cell_guid": "cf54e816-3285-416f-a1e4-e17b60feb51b"}, "cell_type": "markdown"}, {"source": ["nontriviality_dict = dict(class_nontriviality)\n", "weight_dict = dict(class_weights)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "073de63fe7231605d2b6a2299ef736b5094787be", "collapsed": true, "_cell_guid": "4b4a7659-0d05-422d-a1ec-93a66063f902"}, "outputs": []}, {"source": ["total_nontriviality = sum([nontriviality_dict[k]*weight_dict[k] for k in nontriviality_dict])"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9b4905724e49aabf32754f2655ae1e3e6dfd5bd4", "collapsed": true, "_cell_guid": "a5c124ae-821d-4914-80aa-872027ec9f61"}, "outputs": []}, {"source": ["total_nontriviality"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "29f035484460715336c5e05fa3d93b522154bbd9", "collapsed": true, "_cell_guid": "2b2ed442-df86-4058-85c6-a72ca1c07d0f"}, "outputs": []}, {"source": ["total_nontriviality == proportion_nontrivial(train_data)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1aa05debf936c92b1e4ea73a6b9cbbb3d7c03b5a", "collapsed": true, "_cell_guid": "5cf85fc5-f2fc-4fda-8d0d-d19051f57f46"}, "outputs": []}, {"source": ["We are still sane!"], "metadata": {"_uuid": "0799ecbcea1e69750c29bb445a216b3a862675d9", "_cell_guid": "c13c8afb-a86a-4ad0-8da6-350ba2d30409"}, "cell_type": "markdown"}, {"source": ["### Characters"], "metadata": {"_uuid": "d4dfbdbdc5c72b88175c3a65f116a0dc292fab9a", "_cell_guid": "e80f1db6-db4b-472d-849c-fce1789c0fce"}, "cell_type": "markdown"}, {"source": ["Before looking at how things work over the semantic classes, let us do something very simple.\n", "\n", "Let us identify all characters appearing in the dataset."], "metadata": {"_uuid": "3f2f9f24f1225e305cd08d99450fd11aadd844e7", "_cell_guid": "b5edc23c-8bef-4582-99e3-9d0d35842d5b"}, "cell_type": "markdown"}, {"source": ["from collections import Counter"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8da27bd23a576f0ba8af16fe37a02cde6446d572", "collapsed": true, "_cell_guid": "98c9e799-bf69-4139-8c06-5068ec9d422f"}, "outputs": []}, {"source": ["def generate_recorder_fn(counter):\n", "    def recorder_fn(iterable):\n", "        for c in iterable:\n", "            counter[c] += 1\n", "    return recorder_fn"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "19f0ce207c56bb9c7f8cae16448db36e51557091", "collapsed": true, "_cell_guid": "5bb427d0-b59d-4b54-b3ed-c738a4ad8cd3"}, "outputs": []}, {"source": ["char_counter = Counter()\n", "\n", "_ = train_data['before'].astype(str).apply(generate_recorder_fn(char_counter))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b0b6bf72b5969f97e4d8a0d1dacf2df98d5fac67", "collapsed": true, "_cell_guid": "65c5eafd-50df-4f2a-b16c-8dd2f07eca01"}, "outputs": []}, {"source": ["len(char_counter)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c2ddd2bc37c4665c9e821c5a92f976c424b196db", "collapsed": true, "_cell_guid": "d69d3bbb-3d6a-431a-b025-26a5c1d77816"}, "outputs": []}, {"source": ["char_counter.most_common(20)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "fe347dc40458d4643b5f2bd218fe26f9b8af889c", "collapsed": true, "_cell_guid": "e6d2d80a-75c5-4aa5-8c71-8dc821af9eb4"}, "outputs": []}, {"source": ["import matplotlib.pyplot as plt\n", "import numpy as np"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d6165182da47940ce4e6707831c40f6ec8c6f965", "collapsed": true, "_cell_guid": "612baf63-703d-4883-bf7f-62f5956835a7"}, "outputs": []}, {"source": ["To get a sense of this distribution, let us plot the frequency of occurency of the 50 most common characters.\n", "\n", "(Note: Plotting code shamelessly stolen from [here](https://stackoverflow.com/a/19199002).)"], "metadata": {"_uuid": "e761168d028eee4d4ece54e19e95c80eed31b824", "_cell_guid": "d69ecec2-0cd9-45dc-8778-ee2f5a4cc12b"}, "cell_type": "markdown"}, {"source": ["labels, values = zip(*char_counter.most_common(50))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "087cefd765d02e971c7396fd7a57eed058b33bef", "collapsed": true, "_cell_guid": "a7a55735-43af-4bc1-bfb9-415a5779114a"}, "outputs": []}, {"source": ["indexes = np.arange(len(labels))\n", "width = 0.2\n", "\n", "plt.bar(indexes, values, width)\n", "plt.xticks(indexes + width * 0.5, labels)\n", "plt.xlabel('characters')\n", "plt.ylabel('counts')\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "44902c707761dc693fe955360939ad055f914f0c", "collapsed": true, "_cell_guid": "0362dcea-d0ab-4878-8b40-b719c5feb8bd"}, "outputs": []}, {"source": ["Let's wrap up this counting functionality inside an abstraction:"], "metadata": {"_uuid": "e05c8cfe34ee2debc96b84751dc85784754ab648", "_cell_guid": "3a21ede5-6171-43fb-ad59-952753a756ad"}, "cell_type": "markdown"}, {"source": ["def n_grams(string, n):\n", "    return zip(*(string[k:] for k in range(n)))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1fc9d138b97f9da5e880674958d26cf6d7262a9a", "collapsed": true, "_cell_guid": "36cf0926-38d0-4842-8781-a59b0820ad9d"}, "outputs": []}, {"source": ["def n_gram_frequency(strings, n=1):\n", "    counter = Counter()\n", "    record = generate_recorder_fn(counter)\n", "    for string in strings:\n", "        record(n_grams(string, n))\n", "    return counter"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "48b62e7c76589242388b83f185db380fe2f72ba7", "collapsed": true, "_cell_guid": "d3159370-c943-463e-93a8-9ef91343ff1a"}, "outputs": []}, {"source": ["class_char_counters = {k:n_gram_frequency(df['before'].astype(str), 1) for k,df in grouped_by_class}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f14d61cf6bb4ea51dd5c13024e4f9bd7a5b37837", "collapsed": true, "_cell_guid": "5d6bc3ad-4eff-41ec-8bcd-db4ecdb2b5e9"}, "outputs": []}, {"source": ["##### Sanity check\n", "\n", "The sum of the number of occurences of the character `e` in each class should be equal to the number of its occurences over the entire corpus"], "metadata": {"_uuid": "91730d8e92d3f9945e496f0a3f46bc06bd0ad24a", "_cell_guid": "af838b4b-40d0-4b1a-9fd9-493b6631bc0c"}, "cell_type": "markdown"}, {"source": ["sum(class_char_counters[k][('e',)] for k in class_char_counters)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d9969f86e99d5e9aa747d2f4cd90f68c3d353fa0", "collapsed": true, "_cell_guid": "c2f3ffb9-fa5a-4f05-84f1-0778a051bd7f"}, "outputs": []}, {"source": ["sum(class_char_counters[k][('e',)] for k in class_char_counters) == char_counter['e']"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a400fea28be48b97bde9ac0eec12a647f6f9a3bd", "collapsed": true, "_cell_guid": "45dba14c-b2a6-44e3-a2d0-61ab090f0ad0"}, "outputs": []}, {"source": ["#### Bigrams\n", "\n", "Let us also create bigram frequency counters:"], "metadata": {"_uuid": "147fd61ecf4e8254d1a2d3896a8f59bd221e5af1", "_cell_guid": "263f116e-aff0-4cb5-b802-a9be6104e8a7"}, "cell_type": "markdown"}, {"source": ["bigram_counter = n_gram_frequency(train_data['before'].astype(str), 2)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e89adea46ab63031344d4f157495e16d759f2c80", "collapsed": true, "_cell_guid": "e222636c-97e6-4f12-ae9f-a2e935d48906"}, "outputs": []}, {"source": ["len(bigram_counter)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "4187d5aacc6e4e51e7e3f9a317928247ec861c00", "collapsed": true, "_cell_guid": "098f142c-623d-4b4b-868a-0e3fa69f9fd3"}, "outputs": []}, {"source": ["bigram_counter.most_common(20)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "3100c78cc05b14388bcfa11fc2fbb78c02413d52", "collapsed": true, "_cell_guid": "5d3d9073-c050-4726-944e-e91f1c5414a0"}, "outputs": []}, {"source": ["def plot_frequencies(counter, n, width=0.5, font_size=5):\n", "    labels, values = zip(*counter.most_common(n))\n", "    indexes = np.arange(len(labels))\n", "    width = 0.5\n", "    \n", "    plt.bar(indexes, values, width)\n", "    plt.xticks(indexes + width * 0.5, labels, rotation='vertical', fontsize=font_size)\n", "    plt.xlabel('grams')\n", "    plt.ylabel('counts')\n", "    plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "449c2ae5f2697136d83f09935952d4dd0266e113", "collapsed": true, "_cell_guid": "933c68f1-89ad-4cd4-baf0-d33444584d0f"}, "outputs": []}, {"source": ["plot_frequencies(bigram_counter, 80)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f34e22b83e4b3d90d74e5f550f85324d890b458b", "collapsed": true, "scrolled": true, "_cell_guid": "960f3455-e921-4135-b1e3-05b8288e38e4"}, "outputs": []}, {"source": ["class_bigram_counters = {k:n_gram_frequency(df['before'].astype(str), 2) for k,df in grouped_by_class}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "06cce7ae31884c9cbc835226ee35f892bdc0b001", "collapsed": true, "_cell_guid": "e9fc7012-d5c7-4ee8-9047-3087ffbea9b3"}, "outputs": []}, {"source": ["##### Sanity check\n", "\n", "Sum of occurences of the bigram `('a','l')` within each of the classes should be equal to the number of occurences over the entire corpus."], "metadata": {"_uuid": "8b1abb5ff53901a2baabdc266f2e66d06e8ca022", "_cell_guid": "4e82801d-6c5a-45f4-bf21-6d0422aa0049"}, "cell_type": "markdown"}, {"source": ["sum(class_bigram_counters[k][('a','l')] for k in class_bigram_counters)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1f2e1e0c8858b80388b62d1f6cfa241d2206d577", "collapsed": true, "_cell_guid": "0ab8fde0-f52e-4847-a4c4-5bdf12063bda"}, "outputs": []}, {"source": ["sum(class_bigram_counters[k][('a','l')] for k in class_bigram_counters) == bigram_counter[('a','l')]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e4a03ff25e855a2867baa3709ec6ed7b65ba4db6", "collapsed": true, "_cell_guid": "c84e86eb-996d-4672-8ee5-0970ff651b7e"}, "outputs": []}, {"source": ["#### How different are these frequencies over each class?\n", "\n", "We will answer this question by deriving within-class character and bigram probability distributions from these frequency counts and comparing those distributions to each other."], "metadata": {"_uuid": "3cbf051e190098b9a4b79d905983d7b64aa15b55", "_cell_guid": "fbc2bf82-9d5e-46d8-a12a-0556af689988"}, "cell_type": "markdown"}, {"source": ["##### Character distributions"], "metadata": {"_uuid": "075bea656ce43eca0b64b269b6f253dccff478da", "_cell_guid": "70cec7c6-d2e5-48d1-8074-a4b6d05f01c0"}, "cell_type": "markdown"}, {"source": ["def freq_to_dist(counter):\n", "    total = sum(counter[k] for k in counter)\n", "    return {k:counter[k]/total for k in counter}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "7edd10c2baeae37ed9232fdf4bab53ee3fd83291", "collapsed": true, "_cell_guid": "5a39a7a1-4c31-49d3-b52e-7a6b35aa9211"}, "outputs": []}, {"source": ["class_char_dists = {c:freq_to_dist(class_char_counters[c]) for c in CLASSES}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e8b236e8b2572904f3cb946e2fb384c2e1a5b37e", "collapsed": true, "_cell_guid": "a3f56781-e4fc-4d06-a58e-642722c6ccd5"}, "outputs": []}, {"source": ["##### Sanity check\n", "\n", "The values in each item of `class_char_dists` should add up to (roughly) 1."], "metadata": {"_uuid": "41836037a4ba6b8cdc2efb92e0dc29051a34b507", "_cell_guid": "6602c60a-27b6-42a2-8900-0bd9122de3c9"}, "cell_type": "markdown"}, {"source": ["class_char_dists_sums = {c:sum(class_char_dists[c][k] for k in class_char_dists[c]) for c in CLASSES}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "05007ae8935bbe98d42454d8ef53fa0735db3915", "collapsed": true, "_cell_guid": "c7460493-0f5b-4274-ac71-13f4b90da1d5"}, "outputs": []}, {"source": ["sum(abs(class_char_dists_sums[c] - 1) < 0.000001 for c in CLASSES) == len(CLASSES)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "54fd59e6826ecb384f569ca0c1981806473f8b55", "collapsed": true, "_cell_guid": "5fb3d906-234a-4af0-8dc8-a05464f59508"}, "outputs": []}, {"source": ["##### Bigram distributions"], "metadata": {"_uuid": "143d3cc7d3ef293344ae1382bf1cc92481358c64", "_cell_guid": "faabebd0-e72c-4e63-98ba-59691e0741f1"}, "cell_type": "markdown"}, {"source": ["class_bigram_dists = {c:freq_to_dist(class_bigram_counters[c]) for c in CLASSES}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "517273fd656ae93911f225b3298689a69479c83c", "collapsed": true, "_cell_guid": "11ddaac0-493f-4148-915b-fd9940fd7715"}, "outputs": []}, {"source": ["#### Pairwise $L^1$ distances\n", "\n", "To begin with, let us calculate the pairwise $L^1$ distances between the character and bigram distributions for each of the classes."], "metadata": {"_uuid": "d6ce6c2fa23e9738b6586a99d514bac04203a201", "_cell_guid": "6cbd3127-c3cd-4f17-bad8-b92b150bf435"}, "cell_type": "markdown"}, {"source": ["def l1_distance(dist1, dist2):\n", "    \"\"\"\n", "    Args:\n", "    1. dist1 - a probability distribution represented as a Python dictionary\n", "    2. dist2 - a probability distribution represented as a Python dictionary\n", "    (Note: Dictionary representation of probability distributions is as {value:probability for value in universe})\n", "    \n", "    Returns:\n", "    L^1 distance between dist1 and dist2\n", "    \"\"\"\n", "    keys = set(dist1) | set(dist2)\n", "    return sum(abs(dist1.get(k,0) - dist2.get(k, 0)) for k in keys)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "32c6940164c3c0ce9f834005a3e7cd9f60f168d8", "collapsed": true, "_cell_guid": "b5f3786c-4728-4f35-9dc7-938f16a15eb2"}, "outputs": []}, {"source": ["##### Sanity check\n", "\n", "The $L^1$ distance between any distribution and itself should be 0."], "metadata": {"_uuid": "eb3d667855d030bdbd53d7ab9d3f6c0cff66a94f", "_cell_guid": "d3d68fd4-62e7-41a0-87d5-4f47637241b2"}, "cell_type": "markdown"}, {"source": ["distribution = class_char_dists['PLAIN']"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "4f990a66c340e7f75123e6f42adc66985bea0fbb", "collapsed": true, "_cell_guid": "e4fa8ae4-574e-419a-ad7b-f766dc78a469"}, "outputs": []}, {"source": ["l1_distance(distribution, distribution) == 0"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f89692505c8e36c06bc5d69b496ed3b935e2b9d8", "collapsed": true, "_cell_guid": "5dde8650-b488-41d5-9cf5-c32dc24b3781"}, "outputs": []}, {"source": ["The $L^1$ distance should be (roughly) symmetric."], "metadata": {"_uuid": "af1630c4eba038be5ed8d1b6570286ea9f1a6e91", "_cell_guid": "bbf7b4f6-4ab8-4a29-95dc-4a59fd4231f7"}, "cell_type": "markdown"}, {"source": ["dist1 = class_char_dists['PLAIN']"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "4ced5d993aa20336a3a7387f3d2f19a4fe6d867e", "collapsed": true, "_cell_guid": "dc54fb88-63d1-4f33-a473-733ed3815618"}, "outputs": []}, {"source": ["dist2 = class_char_dists['VERBATIM']"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "3c66a498450bab29a48932d608ce143559ac72a2", "collapsed": true, "_cell_guid": "cf2a6ff1-b637-4625-9f35-75afc57afb9d"}, "outputs": []}, {"source": ["abs(l1_distance(dist1, dist2) - l1_distance(dist2, dist1)) < 0.000001"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "12d9dc96095ce984ac02d4b9ec529728d826f971", "collapsed": true, "_cell_guid": "bae770fb-2b1f-41e2-af0d-7c8e15d50613"}, "outputs": []}, {"source": ["##### Tables of pairwise distances"], "metadata": {"_uuid": "f12e2cc6fdd15f08b38c63db41f1d27ffe0c0e7e", "_cell_guid": "12fae522-6bae-4d02-bfc7-b2e601954bce"}, "cell_type": "markdown"}, {"source": ["char_dist_distances_dict = {c1:{c2:l1_distance(class_char_dists[c1], class_char_dists[c2]) for\n", "                           c2 in CLASSES} for\n", "                       c1 in CLASSES}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "13e7aefac9726c3f9e8920373f42c807b6eec497", "collapsed": true, "_cell_guid": "63b13477-a8dc-4805-9cf7-43409faadf26"}, "outputs": []}, {"source": ["char_dist_distances = pd.DataFrame.from_dict(char_dist_distances_dict)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1a8215e3e2498a9cdce5330ecc77911e7b738b67", "collapsed": true, "_cell_guid": "4650faa4-6cc2-4a87-ad7c-839341848512"}, "outputs": []}, {"source": ["bigram_dist_distances_dict = {c1:{c2:l1_distance(class_bigram_dists[c1], class_bigram_dists[c2]) for\n", "                             c2 in CLASSES} for\n", "                         c1 in CLASSES}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "65a31cb9c92616e78df1247c8f3adc631b2b074c", "collapsed": true, "_cell_guid": "e32d94cf-4356-4ea1-9e4e-e6940baf4d17"}, "outputs": []}, {"source": ["bigram_dist_distances = pd.DataFrame.from_dict(bigram_dist_distances_dict)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "daccf7bc72d2db0dac453ef1346d083a78a55aca", "collapsed": true, "_cell_guid": "e503115a-bc2e-4eff-bfdd-93721cfb7586"}, "outputs": []}, {"source": ["import seaborn as sns"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "53cc626e40846f68f4b962af636711da9afcba4b", "collapsed": true, "_cell_guid": "768037eb-82ec-459b-a4be-94850c95f1c4"}, "outputs": []}, {"source": ["sns.heatmap(char_dist_distances)\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "44ebb123d1e6060be2a4806f1aed5c6032ee5467", "collapsed": true, "_cell_guid": "f1a6f6bb-5fd0-46be-85ca-3272f639b9e7"}, "outputs": []}, {"source": ["sns.heatmap(bigram_dist_distances)\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c91325b6ab34438d06a66792bc67641733d78cc4", "collapsed": true, "_cell_guid": "71ba0bdc-0b5b-4745-89cf-225371cbdf46"}, "outputs": []}, {"source": ["These heatmaps are quite instructive, as they suggest classes that can be treated similarly to each other for normalization purposes.\n", "\n", "For example:\n", "\n", "+ The `CARDINAL`, `DIGIT`, and `TELEPHONE` classes are very similar in both character and bigram distribution, and so there will be a substantial degree of overlap in how we normalize those.\n", "\n", "+ `PUNCT` (punctuation) is very, very different from every other class.\n", "\n", "+ There are similarities in character distribution between `FRACTION` and `CARDINAL`/`DIGIT`/`TELEPHONE`, but these similarities get de-emphasized by considering the bigram distribution.\n", "\n", "+ There are similarities in both distribution types between `PLAIN` and `ELECTRONIC`.\n", "\n", "We can use this information is to define compound semantic classes."], "metadata": {"_uuid": "ff3a68268557b1189d41b0d43970e2100db4a2c5", "_cell_guid": "a8a0522e-a75b-42cb-842e-b771e494562f"}, "cell_type": "markdown"}, {"source": ["##### ELECTRONIC\n", "\n", "It's not clear to me what the `ELECTRONIC` class contains, so let's have a look:"], "metadata": {"_uuid": "238a775c9775d6f7062c9c73ee33c24bedd822f9", "_cell_guid": "0b922a61-d90a-4482-a9ae-bc23e01509d6"}, "cell_type": "markdown"}, {"source": ["electronic = [df for k,df in grouped_by_class if k == 'ELECTRONIC'][0]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "194e77fe10523184d4e3cba9b225b4fca61e6244", "collapsed": true, "_cell_guid": "8709e20e-8446-4ffd-ae25-bdc772dfeaf8"}, "outputs": []}, {"source": ["electronic.sample(5)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6f14d9ddf849bf2fa1819af88f65b914614cf7ca", "collapsed": true, "_cell_guid": "3211ca89-8656-4262-b5a6-54d2af8d7264"}, "outputs": []}, {"source": ["This class seems to denote eletronic identifiers like URLs.\n", "\n", "Note that, although the character and bigram distributions between `ELECTRONIC` and `PLAIN` are similar for the actual raw content, they are unlikely to be so for the normalized content (given how spaces are inserted to normalize `ELECTRONIC`).\n", "\n", "Our analysis in this section concerns the semantics of the raw tokens, but not of the normalization (by analyzing the normalized tokens)."], "metadata": {"_uuid": "b5057af027c46e42607c04b0f52367d3e5ba1360", "_cell_guid": "942d36d5-ab20-415b-b218-0692d56dd21d"}, "cell_type": "markdown"}, {"source": ["##### Inferring semantic class\n", "\n", "Let us explore the possibility of using a nearest-neighbor scheme to identify the semantic class of a raw token.\n", "\n", "The idea is that, for a given token, we can associate it with the semantic class whose character and/or bigram probability distribution is closest (in terms of the $L^1$ distance) to that of the token itself.\n", "\n", "Note that this is unlikely to work well. Not because the idea is bad, but because the length of each token is very small relative to the number of characters (and so each token also contains very few bigrams compared to the total number of possible bigrams represented in the training data). These size considerations may take us down the road of applying a syntactic classification to the characters comprising each token before doing the analysis that we have done thus far in this notebook.\n", "\n", "Regardless, let us see where the idea takes us with the unmodified tokens. We can investigate the approach mentioned above in a later notebook if it seems promising based on the experiment that follows."], "metadata": {"_uuid": "49382f81ddc4bbcba7d4e2357f8be56cd6a23d27", "_cell_guid": "467bcd09-3649-4197-9aa1-20bcfb76bd05"}, "cell_type": "markdown"}, {"source": ["We will begin by sampling a number of tokens from the corpus. For each token:\n", "\n", "1. We will calculate the character/bigram distribution.\n", "\n", "2. We will identify the within-class character/bigram distribution that is closest to the corresponding distribution for the given token.\n", "\n", "3. We will record the class corresponding to the distribution identified in step 2.\n", "\n", "We will then calculate the accuracy of the assignments over all sampled tokens."], "metadata": {"_uuid": "2693acc20e3a9faa5a1a24a6dac32fa2cd19ef09", "_cell_guid": "03464427-f96d-4bee-8aa4-802c3794a758"}, "cell_type": "markdown"}, {"source": ["SAMPLE_SIZE = 1000"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "dac6659d97d6438497ced2da63d6daa20f562903", "collapsed": true, "_cell_guid": "97749bf1-a899-4bd3-baad-78ebb325b1d9"}, "outputs": []}, {"source": ["sample_df = train_data.sample(SAMPLE_SIZE)[['before','class']]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "33c4db4c19cf6a9f065efb987e1978ba06e1ad00", "collapsed": true, "_cell_guid": "d2f6541d-2fe6-4234-8d59-d7465add1f62"}, "outputs": []}, {"source": ["sample = list(sample_df['before'])"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "931937aa1a29184e6d5c81ea9b4bd52102a95278", "collapsed": true, "_cell_guid": "b5fab335-6580-48ac-a476-4aeb32bb175a"}, "outputs": []}, {"source": ["labels = list(sample_df['class'])"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "2d60445d9e59280e95aeeab251408043c181b0cd", "collapsed": true, "_cell_guid": "d50af61b-5a8a-401c-99eb-0396599360be"}, "outputs": []}, {"source": ["def char_inferred_class(token):\n", "    token_dist = freq_to_dist(n_gram_frequency([token], 1))\n", "    distances = [(c, l1_distance(token_dist, class_char_dists[c])) for c in CLASSES]\n", "    return min(distances, key=lambda p: p[1])[0]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a60abe92883f2cfdbc48981d4a1760d6b873e636", "collapsed": true, "_cell_guid": "de0b2ae8-ed54-4466-808a-82c385ca3a18"}, "outputs": []}, {"source": ["inferences = [char_inferred_class(token) for token in sample]"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "24544c67ff814893c4358098e5e776f8db528ed1", "collapsed": true, "_cell_guid": "21d2c60c-3273-4926-b941-5fdb24c8c866"}, "outputs": []}, {"source": ["accuracy = sum(inferred == actual for inferred, actual in zip(inferences, labels))/len(labels)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ca54f5a46b77be82681a08337eb37c9afb2ec9a7", "collapsed": true, "_cell_guid": "a7dd736b-68ab-46c5-975b-7b3134ec451e"}, "outputs": []}, {"source": ["accuracy"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "21504e5ede7835b35af4dd2bd74c2b5cf6357ba7", "collapsed": true, "_cell_guid": "28b831a9-943f-4bd5-a037-9ad28f27b9e6"}, "outputs": []}, {"source": ["Of course, in this case, the `PLAIN` class is over-represented given that it dwarfs all the other classes in terms of representation in the training dataset.\n", "\n", "It is more meaningful for us to consider the accuracy of this inference within each class."], "metadata": {"_uuid": "957bcb39dab481fdf2b96e411943e285fb6e9b08", "_cell_guid": "e4147544-a9e0-444d-982b-a39c52122378"}, "cell_type": "markdown"}, {"source": ["class_samples = {c:list(train_data[train_data['class'] == c].sample(SAMPLE_SIZE, replace=True)['before'].astype(str)) for\n", "                 c in CLASSES}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a4e46b435778d891b0866acf158621134152930c", "collapsed": true, "_cell_guid": "788131e8-14a6-452b-aa37-9107848cb55c"}, "outputs": []}, {"source": ["class_inferences = {c:[char_inferred_class(token) for token in class_samples[c]] for c in CLASSES}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e56db189104ddeee6a2e03e51a1a2cf5f44b399b", "collapsed": true, "_cell_guid": "5a92692a-831f-433b-9f70-cacf3715d6cb"}, "outputs": []}, {"source": ["class_accuracies = {c:sum(i==c for i in class_inferences[c])/SAMPLE_SIZE for c in CLASSES}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "99981af9fb29f0fcdd5592dc7cc95131feba4f68", "collapsed": true, "_cell_guid": "a7ec3ebf-50a9-4494-bd61-35f5ab1e43f3"}, "outputs": []}, {"source": ["class_accuracies"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0898881be2dedde5dd5cd4e958b03389ebdc5310", "collapsed": true, "_cell_guid": "85598d83-2f4a-4e82-b32d-9476604b8ef3"}, "outputs": []}, {"source": ["The above statistics estimate the accuracy conditioned on the true labels. Even *more* meaningful than that are the estimates for accuracy conditioned on the *inferred* labels:"], "metadata": {"_uuid": "5cccbc3788fd9429f6f7434a134f55e4ef714374", "_cell_guid": "9b767f00-68e2-423c-a13d-762247cea70e"}, "cell_type": "markdown"}, {"source": ["inferred_class_confusion = {c:[] for c in CLASSES}\n", "\n", "for c in CLASSES:\n", "    for i in class_inferences[c]:\n", "        inferred_class_confusion[i].append(c)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "dccf0b9f502ccbecf58793c0d37b42e3c79ac2c9", "collapsed": true, "_cell_guid": "9d875549-d2c6-4339-8501-b423b4745050"}, "outputs": []}, {"source": ["inferred_class_accuracies = {c:(sum(l==c for l in inferred_class_confusion[c])/len(inferred_class_confusion[c]),\n", "                                len(inferred_class_confusion[c])) for\n", "                             c in inferred_class_confusion if len(inferred_class_confusion[c]) > 0}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "fa47bff9b78025e74aa553e5bfe66a091cc15a7a", "collapsed": true, "_cell_guid": "e19302c5-8b2f-4021-bc11-37d1ae775a59"}, "outputs": []}, {"source": ["inferred_class_accuracies"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "65f13b6066c62b56dd081a72a34875e4e8fbc3e6", "collapsed": true, "_cell_guid": "42ca6539-4001-4b3f-9339-c10b15f9a6f1"}, "outputs": []}, {"source": ["Actually, a better way to visualize this information is as follows:"], "metadata": {"_uuid": "e3f8c477886a73605b4c8456093ec5832d13d1e0", "_cell_guid": "298add9a-70af-47b8-be39-20140b0eea43"}, "cell_type": "markdown"}, {"source": ["confusion = {c:Counter(class_inferences[c]) for c in CLASSES}"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6eccb561f2fc5b29a400e48f40a022def61ae211", "collapsed": true, "_cell_guid": "52afca61-0c8b-4170-851a-d91abedbafcd"}, "outputs": []}, {"source": ["for c in confusion:\n", "    for k in CLASSES:\n", "        if not k in confusion[c]:\n", "            confusion[c][k] = 0"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "81e5d97a7f5baad273ff5018dbf6b64fb852925c", "collapsed": true, "_cell_guid": "a7c562cf-657d-4ff6-b93b-b9fe8cf7c4fe"}, "outputs": []}, {"source": ["sns.heatmap(pd.DataFrame.from_dict(confusion))\n", "plt.xlabel('Inferred labels')\n", "plt.ylabel('True labels')\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "27bc9d861bb0cb7e4924b3740906a255fd2d0230", "collapsed": true, "_cell_guid": "891ee9d3-802a-4b59-b9ca-af03fd5b60a0"}, "outputs": []}, {"source": ["Any way you slice it, there is promise in this approach to identifying classes."], "metadata": {"_uuid": "dbfbe227b224ce1ce30b69a147ea106d7b44de6f", "_cell_guid": "8568f3b5-6c4d-469a-ab3a-a22a09f35d4b"}, "cell_type": "markdown"}], "metadata": {"language_info": {"mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 1}