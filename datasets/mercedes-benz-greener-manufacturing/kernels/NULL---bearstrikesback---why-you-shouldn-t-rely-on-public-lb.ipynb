{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9e6aab86-5e24-06da-c414-4bdcccdbed65"
      },
      "source": [
        "This notebook is created as an attempt to explain mismatch between local CV, Public LB and (likely) Private LB. Any advices, suggestions are welcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0388f1df-f721-afa1-97c9-05051ca3e904"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "35005eef-5d02-726c-e59f-b5f30f3b8f90"
      },
      "outputs": [],
      "source": [
        "X_test = pd.read_csv('../input/test.csv')\n",
        "X_test = X_test.drop([\"ID\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "684673b3-28d8-0db0-3fca-3078d2923191"
      },
      "outputs": [],
      "source": [
        "print ('Number of duplicates among test set: {}, {}'\n",
        "       .format(X_test.duplicated().sum(), X_test.duplicated(keep=False).sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0cff3d0c-83bc-f8a1-340e-fd3f2c7e9ff6"
      },
      "outputs": [],
      "source": [
        "print ('Percentage of duplicates among test set:')\n",
        "print ('{:.2%}'.format(X_test.duplicated().sum()/len(X_test)))\n",
        "print ('{:.2%}'.format(X_test.duplicated(keep=False).sum()/len(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4059b03d-481b-cc84-16d9-80f1d1a324bb"
      },
      "source": [
        "It is clear that there are duplicates among test set. Number of duplicates (with and without **keep=False**) indicates that each duplicated row has not only one copy, but maybe two, three or even more.\n",
        "\n",
        "Let's consider some distribution with the same fraction of duplicates and two models which approximate this distribution pretty well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "058db53b-29e4-ce09-f64e-ad453abd26ed"
      },
      "outputs": [],
      "source": [
        "valid = [i for i in range (100,136)] + [136, 136, 137, 137, 137]\n",
        "model_1 = [i for i in range (101,137)] + [136.7,136.7,137.5,137.5,137.5]\n",
        "model_2 = [i+0.5 for i in range (100,136)] + [138.5, 138.5, 139, 139, 139]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d361349e-256c-07f0-8485-68f3cda375cd"
      },
      "outputs": [],
      "source": [
        "print ('Percentage of duplicates among valid set:')\n",
        "print ('{:.2%}'.format((3/len(valid))))\n",
        "print ('{:.2%}'.format((5/len(valid))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f96b3b21-5773-335f-5697-efd93a97a2bd"
      },
      "outputs": [],
      "source": [
        "plt.plot(valid, 'b.')\n",
        "plt.plot(model_1, 'r.')\n",
        "plt.plot(model_2, 'g.')\n",
        "plt.ylim(ymin=95, ymax=145)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0dedc906-a672-a45c-1776-fc0522467882"
      },
      "source": [
        "It is obvious that both models approximate our \"truth\" distribution well. However, **model_1** is likely to have larger error considering non-duplicated values, whereas **model_2** will have larger errors considering duplicated one's. \n",
        "\n",
        "Let's find out how our models perform on **whole valid set**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1bbef1b6-2281-e871-86b6-f6486475b650"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b56e94de-ab44-25e6-655d-136c9041c07e"
      },
      "outputs": [],
      "source": [
        "print ('Calculating r2 score for both models:')\n",
        "print ('Model_1: {:.2%}'.format(r2_score(valid, model_1)))\n",
        "print ('Model_2: {:.2%}'.format(r2_score(valid, model_2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e47bede4-301d-3228-4fdf-2b772fbd20b2"
      },
      "source": [
        "Both models show high and almost the same accuracy for the whole set. Let's move further and look at how models will perform if we slice our data according to contest rules.\n",
        "\n",
        "First of all, let's consider two extreme cases: when all duplicated values will be either within Public LB subset or Private LB subset. This is unlikely to occur, but anyway. Here LB refers to **Public LB score**, PB refers to **Private LB score**. \n",
        "\n",
        "I used only eight values to simulate the same split we have in this contest. (i.e. 8/41 ~ 19.5%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1731d2f0-d70e-024e-e83a-2638d442305e"
      },
      "outputs": [],
      "source": [
        "valid_LB1 = valid[0:8]\n",
        "model_1_LB1 = model_1[0:8]\n",
        "model_2_LB1 = model_2[0:8]\n",
        "\n",
        "valid_PB1 = valid[8:]\n",
        "model_1_PB1 = model_1[8:]\n",
        "model_2_PB1 = model_2[8:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8033301d-c7e1-6986-dcf6-72aa58b208a0"
      },
      "outputs": [],
      "source": [
        "print ('Public LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_LB1, model_1_LB1), r2_score(valid_LB1, model_2_LB1)))\n",
        "print ('Private LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_PB1, model_1_PB1), r2_score(valid_PB1, model_2_PB1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c8376cd4-7fe0-799f-a78c-f52cf0773c02"
      },
      "source": [
        "Here we get first interesting result. If there will be no duplicated values in Public LB, there is heavy mismatch between private and public LB. Would you choose **model_1** based on PublicLB? I bet you won't."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "46cfd322-a3d4-3726-e8e2-3af088931a1f"
      },
      "outputs": [],
      "source": [
        "valid_LB2 = valid[-8:]\n",
        "model_1_LB2 = model_1[-8:]\n",
        "model_2_LB2 = model_2[-8:]\n",
        "\n",
        "valid_PB2 = valid[:-8]\n",
        "model_1_PB2 = model_1[:-8]\n",
        "model_2_PB2 = model_2[:-8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "32967c61-7609-b98b-d966-887f725d26eb"
      },
      "outputs": [],
      "source": [
        "print ('Public LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_LB2, model_1_LB2), r2_score(valid_LB2, model_2_LB2)))\n",
        "print ('Private LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_PB2, model_1_PB2), r2_score(valid_PB2, model_2_PB2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0fbc2dea-212c-5d44-c439-52ab36fa8f15"
      },
      "source": [
        "Another interesting result. It is better not to estimate our set using **model_2** at all! It performs worse than simple average value. However, once Private LB is revealed we might be very sad for not choosing it.\n",
        "\n",
        "These cases are unlikely to happen, since it's hard \"randomly\" to split testing set in these presented ways. Now let's look at more realistic splits of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6bf3a95a-8a52-a830-697f-8c66a2db48dd"
      },
      "outputs": [],
      "source": [
        "valid_LB3 = valid[:5] + valid[-3:]\n",
        "model_1_LB3 = model_1[:5] + model_1[-3:]\n",
        "model_2_LB3 = model_2[:5] + model_2[-3:]\n",
        "\n",
        "valid_PB3 = valid[5:] + valid[:-3]\n",
        "model_1_PB3 = model_1[5:] + model_1[:-3]\n",
        "model_2_PB3 = model_2[5:] + model_2[:-3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "31ea7d02-4f73-e870-8c7e-017c9c6fd674"
      },
      "outputs": [],
      "source": [
        "print ('Public LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_LB3, model_1_LB3), r2_score(valid_LB3, model_2_LB3)))\n",
        "print ('Private LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_PB3, model_1_PB3), r2_score(valid_PB3, model_2_PB3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "86986b62-ec2c-3c70-bdca-d32fa05cbe1e"
      },
      "source": [
        "In this case three duplicated values are in Public LB subset. They all have the same ground truth (i.e. all three row are the same). Other five rows are from non-duplicated part. And again mismatch happens to us. If we rely on Public score, we will choose the worse model.\n",
        "\n",
        "Now let's consider another split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6ebe761-44fb-7281-6a1f-08021b203dd3"
      },
      "outputs": [],
      "source": [
        "valid_LB4 = valid[:5] + valid[-5:-2]\n",
        "model_1_LB4 = model_1[:5] + model_1[-5:-2]\n",
        "model_2_LB4 = model_2[:5] + model_2[-5:-2]\n",
        "\n",
        "valid_PB4 = valid[5:-5] + valid[-2:]\n",
        "model_1_PB4 = model_1[5:-5] + model_1[-2:]\n",
        "model_2_PB4 = model_2[5:-5] + model_2[-2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "165b1b9b-831a-37ef-e9be-b7142a6865a6"
      },
      "outputs": [],
      "source": [
        "print ('Public LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_LB4, model_1_LB4), r2_score(valid_LB4, model_2_LB4)))\n",
        "print ('Private LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_PB4, model_1_PB4), r2_score(valid_PB4, model_2_PB4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "383fbfdb-2f40-1ea6-ca31-30c2e0263751"
      },
      "source": [
        "Again three duplicates are in Public LB subset. This time 2 duplicates have one truth behind them (i.e. they are simular) and third value has it's pairs only in Private LB subset. We again might see a mismatch between Public LB and Private LB scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f2137d9-9338-6ddf-99a8-7e936bb246e5"
      },
      "outputs": [],
      "source": [
        "valid_LB5 = valid[:5] + valid[-5:-4] + valid[-3:-1]\n",
        "model_1_LB5 = model_1[:5] + model_1[-5:-4] + model_1[-3:-1]\n",
        "model_2_LB5 = model_2[:5] + model_2[-5:-4] + model_2[-3:-1]\n",
        "\n",
        "valid_PB5 = valid[5:-5] + valid[-4:-3] + valid[-1:]\n",
        "model_1_PB5 = model_1[5:-5] + model_1[-4:-3] + model_1[-1:]\n",
        "model_2_PB5 = model_2[5:-5] + model_2[-4:-3] + model_2[-1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "158b936e-3976-8bb7-84e8-9436767b4320"
      },
      "outputs": [],
      "source": [
        "print ('Public LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_LB5, model_1_LB5), r2_score(valid_LB5, model_2_LB5)))\n",
        "print ('Private LB scores. Model_1: {:.2%}, Model_2: {:.2%}'.format(r2_score(valid_PB5, model_1_PB5), r2_score(valid_PB5, model_2_PB5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1a3035c7-d105-48de-e91f-2af2d12d4f71"
      },
      "source": [
        "Finally, we managed to consider last example, when duplicates are distributed among subsets like simple stratification. (e.i. all duplicated pairs presented both in Public and Private subsets). And here again some mismatch occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "43bb05ff-8cd4-bd8f-2b45-bc168d98f7ff"
      },
      "source": [
        "Conclusion\n",
        "----------\n",
        "In this notebook I tried to explain why I believe that this contest is more like a lottery, since we don't know actual distribution of duplicated pairs (or some other observations with relatively high errors) between Public and Private LB subsets of testing data. That's why some random model might be a winning one.\n",
        "\n",
        "I know that ideas presented here are quite naive. It is unlikely to have constant errors for all non-duplicated values. Or to have strictly higher of lower constant errors for each duplicated pair of values. In the real dataset relations between values will be much more complicated. However, I believe that this example is enough to explain the missmatch many participants have between their Local CV and Public LB as well as possible shuffle in the Private LB.\n",
        "\n",
        "P.S. Stay tuned. Probably (not sure yet) I will present ideas why you shouldn't rely much on your Local CV either."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}