{"cells": [{"outputs": [], "cell_type": "markdown", "source": "# Tensorflow with Logistic Regression", "metadata": {"_cell_guid": "dcfca4fc-5889-cabd-c24a-f3d7373fd065", "_uuid": "b54a9f1b4bf5227fb710f45b7b047569dfaaff0a"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "The main agenda for this analysis is to predict credit crad fraud in the trasaction data.I will be using tensorflow to build the predictive model. To learn more about dataset,visit: https://www.kaggle.com/dalpozz/creditcardfraud.", "metadata": {"_cell_guid": "ddfe368a-ae7a-9de4-7cad-09fddcb3839e", "_uuid": "0c7ee2a83868bac08777d97fca06c4a913e63b91"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split", "metadata": {"trusted": false, "_cell_guid": "2099d993-46df-ce0b-d032-249535061195", "collapsed": true, "_uuid": "647359f4c5e74c185616f65e9dfb41f7c80739fe"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "df = pd.read_csv('../input/creditcard.csv')", "metadata": {"trusted": false, "_cell_guid": "006e178b-78ee-d4af-964c-8ddc6eb9607d", "_uuid": "052c154b5d8dc51e0d212e43cc93f90319c4fff4"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "\n\n# Exploring the data", "metadata": {"_cell_guid": "0217c14d-45fb-74f7-8a29-f0c40a22e92a", "_uuid": "a549f77e7eddeb8150d3aaede1807ae2709dad48"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "df.head()", "metadata": {"trusted": false, "_cell_guid": "af5aece1-4a07-46d4-eacf-4e23ed876c32", "_uuid": "20daba1c1f78b8759ab3b059ff91f6d516adef11"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "df.isnull().sum()", "metadata": {"trusted": false, "_cell_guid": "7c7acfb9-541b-71ab-c376-f1d1b77f6344", "_uuid": "ffbc26b621b802358ee2db6ce4aba407a0d2e6a3"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "No missing value, that makes things a bit easier", "metadata": {"_cell_guid": "5ce86230-5274-e23d-7232-7bd9382ae96b", "_uuid": "aa0cbc5470998260aed7097932117f9ba02d4661"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "After alot of heeding and trying different differential graphs for analyzing dataset, i took a step up to create statistical graphs using 'seaborn' module. ", "metadata": {"_cell_guid": "3daff603-fbda-fb67-04cf-f242daf4bf73", "_uuid": "36239efcfa2cf6a303eccc5a72b3cbb4302e9661"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#Select only the anonymized features.\nv_features = df.ix[:,1:29].columns", "metadata": {"trusted": false, "_cell_guid": "93e055b1-802f-b49c-da99-7139a8190756", "collapsed": true, "_uuid": "7adc88d2dd46334dc492dd900b55f81df4345988"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "plt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[cn][df.Class == 1], bins=50)\n    sns.distplot(df[cn][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show() ", "metadata": {"trusted": false, "_cell_guid": "aa1230f6-6100-5b85-f595-9c3ea6a09edd", "_uuid": "568151d676974c11e8bec4788bcbe972f5963b84"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#Drop all of the features that have very similar distributions between the two types of transactions.\ndf = df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)", "metadata": {"trusted": false, "_cell_guid": "ef9ad368-596c-29be-9e4e-444404a044eb", "_uuid": "eef25d4a192eddc383d4774cebaaf89a5f05cb1e"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "# create new fearures for distribution\ndf.loc[df.Class == 0, 'Normal'] = 1\ndf.loc[df.Class == 1, 'Normal'] = 0", "metadata": {"trusted": false, "_cell_guid": "85fe11f7-626b-5823-9a84-bd2f82a1de44", "_uuid": "5371be95e504012ab9834393e4ae909255dc2642"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#Rename 'Class' to 'Fraud'.\ndf = df.rename(columns={'Class': 'Fraud'})", "metadata": {"trusted": false, "_cell_guid": "44bcdd96-8a9d-0cce-d3a0-b4d28f3e7903", "_uuid": "df51ce52a5403eaae9c47b1fd30dca46b534af47"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#create Fraud and normal feature distribution\nFraud = df[df.Fraud == 1]\nNormal = df[df.Normal == 1]", "metadata": {"trusted": false, "_cell_guid": "b88053ca-2f8d-6e9a-d92c-4d0b122c0ccd", "_uuid": "ff762ce17b3d687e5c93eb206e15da6ec1760725"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "# create X_train by taking 80% of fraud transactions and 80% of normal transactions\nX_train = Fraud.sample(frac=0.8)\ncount_Frauds = len(X_train)\nX_train = pd.concat([X_train, Normal.sample(frac = 0.8)], axis = 0)\nX_test = df.loc[~df.index.isin(X_train.index)]", "metadata": {"trusted": false, "_cell_guid": "66db88ad-6fab-7bc5-7fc1-b4d3228cb6f0", "_uuid": "be3d222e9aa576f0710757fd94d0c9dfb1333d85"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "# create Y_train by taking 80% of fraud transactions and 80% of normal transactions\ny_train = X_train.Fraud\ny_train = pd.concat([y_train, X_train.Normal], axis=1)\ny_test = X_test.Fraud\ny_test = pd.concat([y_test, X_test.Normal], axis=1)", "metadata": {"trusted": false, "_cell_guid": "a2df6e44-8bf6-ba81-acbf-dbef5f154b3c", "_uuid": "4297d127a370c997015eb67f849f93464c5b60f3"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "# drop the guest features\nX_train = X_train.drop(['Fraud','Normal'], axis = 1)\nX_test = X_test.drop(['Fraud','Normal'], axis = 1)", "metadata": {"trusted": false, "_cell_guid": "394e7b8b-5eba-321d-060e-dfab37149217", "_uuid": "5510827da3546f23d6cf88fe832675973f0ad543"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "# scale values of features\nfeatures = X_train.columns.values\nfor feature in features:\n    mean, std = df[feature].mean(), df[feature].std()\n    X_train.loc[:, feature] = (X_train[feature] - mean) / std\n    X_test.loc[:, feature] = (X_test[feature] - mean) / std", "metadata": {"trusted": false, "_cell_guid": "28f7dee3-2e78-ce22-c92f-45ab07a3b3c5", "_uuid": "a07ae6c8cc4849e7a73fae1954fa736954bf93a9"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "\n# Train the graph", "metadata": {"_cell_guid": "957bfc24-519b-ca42-71b8-eeb1d9a4de65", "_uuid": "5cf34fc453917ac8554e9402e2f8462d013cf887"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#split the dataset for train,test & validation\nsplit = int(len(y_test)/2)\n\ninputX = X_train.as_matrix()\ninputY = y_train.as_matrix()\ninputX_valid = X_test.as_matrix()[:split]\ninputY_valid = y_test.as_matrix()[:split]\ninputX_test = X_test.as_matrix()[split:]\ninputY_test = y_test.as_matrix()[split:]", "metadata": {"trusted": false, "_cell_guid": "3b4dcec6-6bc7-13a8-7779-4ef12559a45f", "_uuid": "6785e95088472f384f10007bd1b4bff982e79dbc"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#parameters\nlearning_rate = 0.005\ntraining_epoch = 10\nbatch_size = 2048\ndisplay_step = 1", "metadata": {"trusted": false, "_cell_guid": "f1cfddab-b866-87ab-b1d3-3711715df240", "_uuid": "47960b73af10929517180e5e8ba9b6ba8561686a"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#tf graph input\nx = tf.placeholder(tf.float32,[None,19])\ny = tf.placeholder(tf.float32,[None,2])", "metadata": {"trusted": false, "_cell_guid": "8ff1c8f9-0801-f64a-686e-6196e6a4f38f", "_uuid": "36d840f476fa5ff0fa5d931e6f07f95f67c2b23d"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#set model weights\nw = tf.Variable(tf.zeros([19,2]))\nb = tf.Variable(tf.zeros([2]))", "metadata": {"trusted": false, "_cell_guid": "f88df848-fa2a-1139-060d-8961d38f9e8d", "_uuid": "e991345ba178af8bb4075c495edfb7c79c9238a8"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#construct model using softmax activation\npred = tf.nn.softmax(tf.matmul(x,w) + b) ", "metadata": {"trusted": false, "_cell_guid": "f2138ba6-6c24-7d2c-ce49-d6a00cd08786", "_uuid": "b4737b73026fee0050b8a1867a314ba2e0a5e710"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#minimize error using cross entropy\ncost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred)))", "metadata": {"trusted": false, "_cell_guid": "0d1475da-53c8-de88-4f9b-76490ceb310d", "_uuid": "b504c531129f51d7e2f984c5018295f3d8fd517c"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#Gradient descent\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)", "metadata": {"trusted": false, "_cell_guid": "ee0dfc52-4a81-b582-5089-0cbd8164c265", "_uuid": "a3baedb5d4b13963a3119be1ebc96a535c151d49"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#initializing variables\ninit = tf.global_variables_initializer()", "metadata": {"trusted": false, "_cell_guid": "d90cbe9d-7c9d-fa70-23d9-a4425d4e7e39", "_uuid": "52e3a4c4883ff6980348458d6bd840373fb6f6a3"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "#launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    final_output_array = []\n    #training cycle\n    for epoch in range(training_epoch):\n        total_batch = len(inputX)/batch_size\n        avg_cost = 0\n        #loop over all the batches\n        for batch in range(int(total_batch)):\n            batch_xs = inputX[(batch)*batch_size:(batch+1) *batch_size]\n            batch_ys = inputY[(batch)*batch_size:(batch+1) *batch_size]\n\n            # run optimizer and cost operation\n            _,c= sess.run([optimizer,cost],feed_dict={x:batch_xs,y:batch_ys})\n            avg_cost += c/total_batch\n\n        correct_prediction = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\n        #disply log per epoch step\n        if (epoch+1) % display_step == 0:\n            train_accuracy, newCost = sess.run([accuracy, cost], feed_dict={x: inputX_test,y: inputY_test})\n            print (\"epoch:\",epoch+1,\"train_accuracy\",train_accuracy,\"cost\",newCost,\"valid_accuracy\",sess.run([accuracy],feed_dict={x:inputX_valid,y:inputY_valid}))\n            print (\"\")\n\n    print ('optimization finished.')", "metadata": {"trusted": false, "_cell_guid": "163be27f-af01-5bd0-5dcd-8a2afd29e455", "_uuid": "5c898dd30bf97808e2b964c59db51dfd0b3c7cbd"}, "execution_count": null}], "metadata": {"language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3"}, "_is_fork": false, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "_change_revision": 0}, "nbformat": 4, "nbformat_minor": 0}