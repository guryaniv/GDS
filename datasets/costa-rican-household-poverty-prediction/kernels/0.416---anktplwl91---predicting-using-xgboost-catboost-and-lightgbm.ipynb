{"cells":[{"metadata":{"trusted":true,"_uuid":"b8bf2221de06da90c44cd7e480480a99be5700e1","collapsed":true},"cell_type":"code","source":"# Importing important libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import chi2, mutual_info_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50ad8878bd303fa00a53a7d62a1620c821882575","collapsed":true},"cell_type":"code","source":"sc = StandardScaler()\nle = LabelEncoder()\nonehot = OneHotEncoder(sparse=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fad2d54cd7839d5e9831feae4f736b5ca93733c9"},"cell_type":"code","source":"# Reading train.csv\ntrain_df = pd.read_csv('../input/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26b46aacc368e3012a06126c0fced673788debcd","collapsed":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2a2e4fb4403fd86c17d06639ad0db618df89900","collapsed":true},"cell_type":"code","source":"# Getting the description of train.csv file\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"074563ccc777dc1df41312e1a035e9fabc49817d","collapsed":true},"cell_type":"code","source":"# Distribution of target classes. As, we can see the target classes are highly imbalanced, espeicially class 4 taking majority of samples\nsns.countplot(train_df['Target'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebb1cac7bf7e7c4f866d25fb1895fbda65bf3ca2"},"cell_type":"code","source":"# Reading test.csv file\ntest_df = pd.read_csv('../input/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96c25992252d28d161c04f312c6785a73eca78fa","collapsed":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f4dacff7ea459b0dc69548ea019798697989a07"},"cell_type":"code","source":"# Combining both train and test file into one. This will help us preprocessing both files simultaneously, and after we are done with that, we can seperate both.\nall_df = train_df.append(test_df, sort=False)\nall_df.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4b9568ffe1adaa944688bcae34bda6d2bf2f2867","collapsed":true},"cell_type":"code","source":"# Checking fraction of null values in each feature column (ignore the Target variable as its null for the test.csv file)\nmissing_vals = (all_df.isnull().sum() / len(all_df)).sort_values(ascending=False)\nmissing_vals = missing_vals[missing_vals > 0]\nmissing_vals = missing_vals.to_frame()\nmissing_vals.columns = ['count']\nmissing_vals.index.names = ['Name']\nmissing_vals['Name'] = missing_vals.index\n\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.barplot(x = 'Name', y = 'count', data=missing_vals)\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cb0014c34f28aaa7d2f40015acf568f2f927e2e","collapsed":true},"cell_type":"code","source":"# Dropping columns having too much null values. Filling null values by interpolating in such feature columns can lead to misleading data, so its better to drop them.\n# Filling feature columns with too little null values with their median values, as the Target classes are imbalanced, its a good idea to replace null values with\n# median values rather than mean values\nall_df.drop(['rez_esc', 'v18q1', 'v2a1'], axis=1, inplace=True)\nall_df.fillna({'SQBmeaned': all_df['SQBmeaned'].median(), 'meaneduc': all_df['meaneduc'].median()}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac93292dbf376ac58f8a583fb58cd33541786797","collapsed":true},"cell_type":"code","source":"# dividing feature columns according to their dtypes, so that we can visualize them further\nfloat_cols = [col for col in all_df.columns if all_df[col].dtype=='float64']\nint_cols = [col for col in all_df.columns if all_df[col].dtype=='int64']\nobject_cols = [col for col in all_df.columns if all_df[col].dtype=='object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dbab61aa3b5a5c8efb31270538da497f51c4973"},"cell_type":"code","source":"# Removing Target feature from list and visualizing float feature columns\ndel(float_cols[-1])\nfloat_flat = pd.melt(all_df, value_vars=float_cols)\ng = sns.FacetGrid(float_flat, col='variable', col_wrap=5, sharex=False, sharey=False)\ng = g.map(sns.distplot, 'value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2b0c686d3584bb61a10ddaec046d7aaa31be28c"},"cell_type":"code","source":"log_meaneduc = np.log1p(all_df['meaneduc'])\nlog_overcrowding = np.log1p(all_df['overcrowding'])\nlog_SQBovercrowding = np.log1p(all_df['SQBovercrowding'])\nlog_SQBdependency = np.log1p(all_df['SQBdependency'])\nlog_SQBmeaned = np.log1p(all_df['SQBmeaned'])\n\ntemp_df = pd.DataFrame({'log_meaneduc': log_meaneduc, 'log_overcrowding': log_overcrowding, 'log_SQBovercrowding': log_SQBovercrowding, 'log_SQBdependency': log_SQBdependency, 'log_SQBmeaned': log_SQBmeaned})\ntemp_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32da166535869a0bd73be942867230bb3861f027"},"cell_type":"code","source":"temp_flat = pd.melt(temp_df)\ng = sns.FacetGrid(temp_flat, col='variable', col_wrap=5, sharex=False, sharey=False)\ng = g.map(sns.distplot, 'value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"202ce22f4740d36097bb3d9140c8722d4af79e3a","collapsed":true},"cell_type":"code","source":"#temp_df['log_meaneduc'] = pd.cut(temp_df['log_meaneduc'], [0.0, 1.945910, 2.268684, 2.525729, 3.637586], labels=[1, 2, 3, 4], include_lowest=True)\n#temp_df['log_overcrowding'] = pd.cut(temp_df['log_overcrowding'], [0.133531, 0.693147, 0.916291, 1.098612, 2.639057], labels=[1, 2, 3, 4], include_lowest=True)\n#temp_df['log_SQBovercrowding'] = pd.cut(temp_df['log_SQBovercrowding'], [0.020203, 0.693147, 1.178655, 1.609438, 5.135798], labels=[1, 2, 3, 4], include_lowest=True)\n#temp_df['log_SQBdependency'] = pd.cut(temp_df['log_SQBdependency'], [0.0, 0.105361, 0.367725, 1.021651, 4.174387], labels=[1, 2, 3, 4], include_lowest=True)\n#temp_df['log_SQBmeaned'] = pd.cut(temp_df['log_SQBmeaned'], [0.0, 3.610918, 4.332194, 4.892227, 7.222566], labels=[1, 2, 3, 4], include_lowest=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd8dafd887ae740f3e0c821c7bed0657edb11186","collapsed":true},"cell_type":"code","source":"#temp_df.fillna({'log_meaneduc': 2, 'log_overcrowding': 2, 'log_SQBovercrowding': 2, 'log_SQBdependency': 1, 'log_SQBmeaned': 4}, inplace=True)\nall_df[['meaneduc', 'overcrowding', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned']] = temp_df[['log_meaneduc', 'log_overcrowding', 'log_SQBovercrowding', 'log_SQBdependency', 'log_SQBmeaned']]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"1981b146b2c492b55e32b4c4f1c0e254e57b1407"},"cell_type":"code","source":"# Visualizing integer feature columns\nint_flat = pd.melt(all_df, value_vars=int_cols)\ng = sns.FacetGrid(int_flat, col='variable', col_wrap=6, sharex=False, sharey=False)\ng = g.map(sns.countplot, 'value')\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e9f3049d8cd1d9d6b22d9cce359ac13d890209f"},"cell_type":"code","source":"# Removing Id feature from list and visualizing object feature columns\ndel(object_cols[0])\nobject_flat = pd.melt(all_df, value_vars=object_cols)\ng = sns.FacetGrid(object_flat, col='variable', col_wrap=4, sharex=False, sharey=False)\ng = g.map(sns.countplot, 'value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"08f5cbff46096aabf245ef1f763b4261ec20c909"},"cell_type":"code","source":"# Encoding feature columns of 'object' dtype\nle = LabelEncoder()\nfor col in object_cols:\n    all_df[col] = le.fit_transform(all_df[col].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6316c31769ca531593baf0adcb556e590617dbd"},"cell_type":"code","source":"dup_cols = [col for col in all_df.columns if col[:3] == 'SQB']\nall_df.drop(dup_cols, axis=1, inplace=True)\nall_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c0af562184cbbeac8f9d30e962aebf00731991a"},"cell_type":"code","source":"all_df.drop('agesq', axis=1, inplace=True)\nall_df['edjefe'] = (all_df['edjefe'] == all_df['edjefe'].max()) * 1\nall_df['edjefa'] = (all_df['edjefa'] == all_df['edjefa'].max()) * 1\nall_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5ecfa189c6ce832769cd4c5911b4871612699a3"},"cell_type":"code","source":"all_df['age'] = sc.fit_transform(all_df['age'].values.reshape((-1, 1)))\nall_df['idhogar'] = sc.fit_transform(all_df['idhogar'].values.reshape((-1, 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"55be71f3124a272d08a07d6b23a5d131e4b5f253"},"cell_type":"code","source":"cat_cols = [col for col in int_cols if col not in [col for col in int_cols if col[:3]=='SQB']]\ncat_cols.remove('agesq')\ncat_cols.remove('age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1da2ff9dffa97e159649f9d30a29d1eb8bca33ba"},"cell_type":"code","source":"onehot_cols = [col for col in cat_cols if len(all_df[col].unique()) > 2]\nonehot_arr = onehot.fit_transform(all_df[onehot_cols].values)\nonehot_arr.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69da4d58db2827334aca3558652a5c3b3edb6f16"},"cell_type":"code","source":"all_df.drop(onehot_cols, axis=1, inplace=True)\nall_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17ab674034e5504683d34c57166e9403bea34055"},"cell_type":"code","source":"# Dividing the whole dataframe into train and test dataframes\ntrain_df = all_df[all_df['Target'].notnull()]\ntest_df = all_df[all_df['Target'].isnull()]\nprint (train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee1de456bf575c64639327b9c50507ff41383431"},"cell_type":"code","source":"# We have to reduce the target value of each class by 1, otherwise XgBoost thinks its training on 5 classes, since highest class is 4. We will undo this change\n# after prediction\ntrain_df['Target'] = train_df['Target'].apply(lambda x: x-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca6ef0ca541420fc1bb18574ff26db5232583349","collapsed":true},"cell_type":"code","source":"# Splitting train dataframe into train and validation dataset\ntr_cols = [col for col in train_df.columns if col not in ['Id', 'Target']]\nX_train = train_df[tr_cols].values\nX_train = np.concatenate((X_train, onehot_arr[:9557, :]), axis=1)\ny_train = train_df['Target'].values\nskf = StratifiedKFold(n_splits=5, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4e3e2e870743fa9dd0059af7cd5ca558017f9da","collapsed":true},"cell_type":"code","source":"# Declaring class weights as the 4 classes are imbalanced, all some common constant parameters across three models\nclass_weights = compute_class_weight('balanced', np.sort(train_df['Target'].unique()), train_df['Target'].values)\nn_rounds = 1000\nlearning_rate = 0.2\nmax_depth = 5\nl2_reg = 2\nn_classes = 4\nearly_stopping_rounds = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9415af4c57d6ec992f281fcd901e63064e6a6711"},"cell_type":"code","source":"test_arr = test_df[[col for col in test_df.columns if col not in ['Id', 'Target']]].values\ntest_arr = np.concatenate((test_arr, onehot_arr[9557:, :]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"8d24848cdebcc96a7125930f0a4b9c2f0d9c1f9b"},"cell_type":"code","source":"# Defining XgBoost parameters and training the model. I have used EarlyStopping using the validation error\nxgb_params = [('eta', learning_rate), ('max_depth', max_depth), ('colsample_bytree', 0.8), ('lambda', l2_reg), ('objective', 'multi:softprob'), ('num_class', n_classes), ('eval_metric', 'mlogloss'), ('silent', 1)]\n\nfor tr_idx, val_idx in skf.split(X_train, y_train):\n    X_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n    X_val, y_val = X_train[val_idx], y_train[val_idx]\n    dtrain = xgb.DMatrix(X_tr, y_tr, weight=[class_weights[int(y_tr[i])] for i in range(y_tr.shape[0])])\n    dval = xgb.DMatrix(X_val, y_val, weight=[class_weights[int(y_val[i])] for i in range(y_val.shape[0])])\n    eval_list = [(dtrain, 'train'), (dval, 'validation')]\n    bst = xgb.train(xgb_params, dtrain, n_rounds, eval_list, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"339c879db79822f2c453b88ff0c9e75d532ba386"},"cell_type":"code","source":"# Predicting the class labels on test dataset\ndtest = xgb.DMatrix(test_arr, weight=class_weights)\nxgb_preds = bst.predict(dtest)\nxgb_sample_subm = pd.read_csv('../input/sample_submission.csv')\nxgb_sample_subm['Target'] = np.argmax(xgb_preds, axis=1).astype(int)\nxgb_sample_subm['Target'] = xgb_sample_subm['Target'].apply(lambda x: x+1)\nxgb_sample_subm.to_csv('xgb_preds.csv', index=False)\nsns.countplot(xgb_sample_subm['Target'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdefdba57f3e0a38dc2fa73b60180bd114405989","collapsed":true},"cell_type":"code","source":"# Defining LightGBM parameters and training the model, using earlystopping on validation error\nlgb_params = {'objective': 'multiclass', 'num_class': n_classes, 'learning_rate': learning_rate, 'num_leaves': 31, 'num_thread': 4, 'max_depth': max_depth, 'feature_fraction': 0.8, 'lambda_l2': l2_reg}\n\nfor tr_idx, val_idx in skf.split(X_train, y_train):\n    X_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n    X_val, y_val = X_train[val_idx], y_train[val_idx]\n    dtrain = lgb.Dataset(X_tr, y_tr, weight=[class_weights[int(y_tr[i])] for i in range(y_tr.shape[0])])\n    dval = lgb.Dataset(X_val, y_val, weight=[class_weights[int(y_val[i])] for i in range(y_val.shape[0])])\n    bst = lgb.train(lgb_params, dtrain, n_rounds, valid_sets = [dval, dtrain], valid_names = ['validation', 'train'], early_stopping_rounds = early_stopping_rounds, verbose_eval=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cd5569ac2d2f6a543debe9fb8e9d4b3a12261cb","collapsed":true},"cell_type":"code","source":"# Predicting on test set using LightGBM model\nlgb_preds = bst.predict(test_arr)\nlgb_sample_subm = pd.read_csv('../input/sample_submission.csv')\nlgb_sample_subm['Target'] = np.argmax(lgb_preds, axis=1).astype(int)\nlgb_sample_subm['Target'] = lgb_sample_subm['Target'].apply(lambda x: x+1)\nlgb_sample_subm.to_csv('lgb_preds.csv', index=False)\nsns.countplot(lgb_sample_subm['Target'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ded20ab3e064af112dc00188991c9d8e9be587e","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Defining CatBoost parameters and training the model, using earlystopping on validation error\ncat_model = CatBoostClassifier(iterations=n_rounds, learning_rate=learning_rate, depth=max_depth, loss_function='MultiClass', classes_count=n_classes, logging_level='Silent', l2_leaf_reg=l2_reg, thread_count=4, class_weights=class_weights)\nfor tr_idx, val_idx in skf.split(X_train, y_train):\n    X_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n    X_val, y_val = X_train[val_idx], y_train[val_idx]\n    cat_model.fit(X_tr, y_tr, use_best_model=True, eval_set=(X_val, y_val), early_stopping_rounds=early_stopping_rounds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15fb7ae51b599420ee3dd805c216f6177c68fd9e","collapsed":true},"cell_type":"code","source":"# Predicting on test set using CatBoost model\ncat_preds = cat_model.predict_proba(test_arr)\ncat_sample_subm = pd.read_csv('../input/sample_submission.csv')\ncat_sample_subm['Target'] = np.argmax(cat_preds, axis=1).astype(int)\ncat_sample_subm['Target'] = cat_sample_subm['Target'].apply(lambda x: x+1)\ncat_sample_subm.to_csv('cat_preds.csv', index=False)\nsns.countplot(cat_sample_subm['Target'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d03dc2bbc2ef96071e67aa913648281bd0dd335a"},"cell_type":"code","source":"# Averaging all three predictions using arithmetic mean and weighing models according to their individual performances\navg_preds = (xgb_preds + cat_preds + lgb_preds) / 3.0\navg_sample_subm = pd.read_csv('../input/sample_submission.csv')\navg_sample_subm['Target'] = np.argmax(avg_preds, axis=1).astype(int)\navg_sample_subm['Target'] = avg_sample_subm['Target'].apply(lambda x: x+1)\navg_sample_subm.to_csv('avg_preds.csv', index=False)\nsns.countplot(avg_sample_subm['Target'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cf3b97235727b3108a10aeeb4fbba18713da377b"},"cell_type":"code","source":"# Geometric weighing three predictions and weighing models according to their individual performances\n#geo_preds = (xgb_preds**0.15) * (cat_preds**0.6) * (lgb_preds**0.25)\n#geo_sample_subm = pd.read_csv('../input/sample_submission.csv')\n#geo_sample_subm['Target'] = np.argmax(geo_preds, axis=1).astype(int)\n#geo_sample_subm['Target'] = geo_sample_subm['Target'].apply(lambda x: x+1)\n#sns.countplot(geo_sample_subm['Target'])\n#plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}