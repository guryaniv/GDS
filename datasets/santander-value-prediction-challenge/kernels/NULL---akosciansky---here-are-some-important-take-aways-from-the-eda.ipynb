{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"**Introduction**\nIn this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.\n\nAnd as part of any good data analysis or machine learning project we need to do some exploratory data analysis first. One thing that makes things a little more tricky than in other competitions is that the features have no actual names as they are annonymised and there is no data dictionary. \n\nSo, without further ado, let's get started."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import libraries\n\nimport pandas as pd\nimport numpy as np\n\n# Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep') \nimport matplotlib.style as style\nstyle.use('fivethirtyeight')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b698c52587e2402cd18c91eedbebe92db6f3c7b","collapsed":true},"cell_type":"code","source":"# Import the data\n\ntrain = pd.read_csv('../input/train.csv', index_col='ID')\n#test = pd.read_csv('../input/test.csv', index_col='ID')","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"85ac5f29ce791d1717d126b5cf89c0a55b41cf44"},"cell_type":"markdown","source":"## Data Summary"},{"metadata":{"trusted":true,"_uuid":"249a6264b6e447b56120d1fe8cd2e52265855518","collapsed":true},"cell_type":"code","source":"print('train shape:', train.shape)\nprint('test shape:', test.shape)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8f160e5c3672e5cfa13b2547b3a0f49e7ff82b7","collapsed":true},"cell_type":"code","source":"train.head()","execution_count":49,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcb12c8050892c6417638e2a5e69493feb227bdb","collapsed":true},"cell_type":"code","source":"test.head()","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"c696b3c8f02c479b2fa4dbd6fd7bc3a0d0fc6276"},"cell_type":"markdown","source":"## Missing Data"},{"metadata":{"_uuid":"5238957b091d1fef291e04e70a5a01a10fff9638"},"cell_type":"markdown","source":"After having had a look at the initial data, one of the first questions you should be asking is whether there anay features (columns) with missing data. This happens quite often but this time there are no missing data (NaNs) in either training or test set. The graphs below would have shown which fields contain Nans and how many. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ac9cf4f0fe47c1ad5bccf20bc9ddf71ec2026015","collapsed":true},"cell_type":"code","source":"# Capture the necessary data\nvariables = train.columns\n\ncount = []\n\nfor variable in variables:\n    length = train[variable].count()\n    count.append(length)\n    \ncount_pct = np.round(100 * pd.Series(count) / len(train), 2)\ncount = pd.Series(count)\n\nmissing = pd.DataFrame()\nmissing['variables'] = variables\nmissing['count'] = len(train) - count\nmissing['count_pct'] = 100 - count_pct\nmissing = missing[missing['count_pct'] > 0]\nmissing.sort_values(by=['count_pct'], inplace=True)\nmissing_train = np.array(missing['variables'])\n\n#Plot number of available data per variable\nplt.subplots(figsize=(15,6))\n\n# Plots missing data in percentage\nplt.subplot(1,2,1)\nplt.barh(missing['variables'], missing['count_pct'])\nplt.title('Count of missing training data in percent', fontsize=15)\n\n# Plots total row number of missing data\nplt.subplot(1,2,2)\nplt.barh(missing['variables'], missing['count'])\nplt.title('Count of missing training data as total records', fontsize=15)\n\nplt.show()","execution_count":19,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f5c6e4e81d11e0573a47c47e9394cca12459b87f","collapsed":true},"cell_type":"code","source":"# Capture the necessary data\nvariables = test.columns\n\ncount = []\n\nfor variable in variables:\n    length = test[variable].count()\n    count.append(length)\n    \ncount_pct = np.round(100 * pd.Series(count) / len(test), 2)\ncount = pd.Series(count)\n\nmissing = pd.DataFrame()\nmissing['variables'] = variables\nmissing['count'] = len(test) - count\nmissing['count_pct'] = 100 - count_pct\nmissing = missing[missing['count_pct'] > 0]\nmissing.sort_values(by=['count_pct'], inplace=True)\nmissing_test = np.array(missing['variables'])\n\n#Plot number of available data per variable\nplt.subplots(figsize=(15,6))\n\n# Plots missing data in percentage\nplt.subplot(1,2,1)\nplt.barh(missing['variables'], missing['count_pct'])\nplt.title('Count of missing test data in percent', fontsize=15)\n\n# Plots total row number of missing data\nplt.subplot(1,2,2)\nplt.barh(missing['variables'], missing['count'])\nplt.title('Count of missing test data as total records', fontsize=15)\n\nplt.show()","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"731dd2c32ae2f0b9f97766fdef26870ac268bb14"},"cell_type":"markdown","source":"The challenge with this dataset is that it is very sparse and contains many zeroes. In fact there 256 features in the training set that have only one value - zero. Let's delete those from the training set as they contain no information."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1f382c9cfb0dc944fe6a6eed3282eefc74e94b6e"},"cell_type":"code","source":"unique_df = train.nunique().reset_index()\nunique_df.columns = [\"col_name\", \"unique_count\"]\nconstant_df = unique_df[unique_df[\"unique_count\"]==1]\nconstant_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c9960d11228c802f66dee3dcae66b5480525adc1"},"cell_type":"code","source":"# Delete columns with constant values\n\ntrain = train.drop(constant_df.col_name.tolist(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1f285c041acc2a0b24fe7638090a8f0dd141129"},"cell_type":"markdown","source":"## Data Types of Features"},{"metadata":{"_uuid":"6c91ee45fdd7bd87271722b355d474eb24d731d1"},"cell_type":"markdown","source":"Let's check whether the data types of all features are correct. From what I have seen so far I would expect everything to be floats. However, there are lots of integers. Could these be categorical features?"},{"metadata":{"trusted":true,"_uuid":"71085712184b2c7ef1f592f0f212cd8785e320f3","collapsed":true},"cell_type":"code","source":"dtype = train.dtypes.reset_index()\ndtype.columns = [\"Count\", \"Column Type\"]\ndtype.groupby(\"Column Type\").aggregate('count').reset_index()","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"71ac967981d9354ec99611b82446a3ac01f31731"},"cell_type":"markdown","source":"## Target Variable"},{"metadata":{"_uuid":"55e6e41d3210c13557cf4c2cf14db4555e8cc7c5"},"cell_type":"markdown","source":"Next one up is the target value. What does the distribution look like and do we have to transform it?\n\nFrom the plots below, we can clearly see that the distribution is skewed and we will have to transform it, so that it approaches a more 'normal' distributions as this will help when training the data. Below you can see the log transformation of the traget varaible, which looks more 'normal'. Also notice that I added 1 to every value before taking the log. This gets around the problem of having 0s as you can't take the log of 0."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bd11156fe3e08e94e04a03f57d55fa5c3b9fa229","collapsed":true},"cell_type":"code","source":"sns.set()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n#plt.figure(figsize=(10,6))\nax = axes[0]\nsns.distplot(train['target'], ax=ax)\nax.set_title('Histogram of Target')\n\nax = axes[1]\nsns.boxplot(data=train, x='target', ax=ax)\nax.set_title('Boxplot of Target')\n\nplt.show()","execution_count":21,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0384f3591edf79c53ddbd5f180663d547a842131","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(14,5))\nsns.distplot(np.log1p(train['target']))\nplt.title('Histogram of Log of Target')\nplt.show()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"36eafdd450b86cb0c1a6006e4a7f04dacb870105"},"cell_type":"markdown","source":"## Features / Independent Variables"},{"metadata":{"_uuid":"a15b97ac8161d3dcaac25a9ff2c9f980e7266180"},"cell_type":"markdown","source":"We can't write much about the features or how we think they will affect the target as we don't have any names or any other prior information. What we can do however is to get an overview of  their statistical properties.\n\nLet's start with a correlation matrix of the features with the highest absolute correlations with target - otherwise this would be a huge matrix. We have over 4000 features after all. The first thing we need to do though, is scale the data. Otherwise the correlation calculations can lead to incorrect results when the features are on different scales. "},{"metadata":{"trusted":true,"_uuid":"7e0ff2e92239c8051d600d72b782800ad947ac5e","collapsed":true},"cell_type":"code","source":"# Scale the data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train)\ntrain_scaled = pd.DataFrame(train_scaled, columns=train.columns)","execution_count":58,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2aacb165c50bb4b89396ef30e5ab14896ea6776b","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"labels = []\nvalues = []\nfor col in train_scaled.columns:\n    if col != 'target':\n        labels.append(col)\n        values.append(np.corrcoef(train_scaled[col].values, train_scaled['target'].values)[0,1])\ncorr = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr = corr.sort_values(by='corr_values')\n\ncols_to_use = corr[(corr['corr_values']>0.25) | (corr['corr_values']<-0.25)].col_labels.tolist()\n\ntemp_df = train_scaled[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corrmat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(14, 14))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corrmat, mask=mask, \n            square=True, linewidths=.5, annot=False, cmap=cmap)\nplt.yticks(rotation=0)\nplt.title(\"Correlation Matrix of Most Correlated Features\", fontsize=15)\nplt.show()","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"60acce35932616a22aacafd4254598110896e255"},"cell_type":"markdown","source":"## Final Points"},{"metadata":{"_uuid":"b366e37a073e868d40ca9657d9490f8dc6505bff"},"cell_type":"markdown","source":"I will add more over the coming days. One interesting point [Wesam Elshamy](https://www.kaggle.com/wesamelshamy/beyond-eda-santander-transaction-value) is making though, is that the distributions of the features of the training and test set are quite different. This is very important as it will impact you prediction score. Ideally, they would have the same distributions."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}