{"nbformat_minor": 1, "metadata": {"language_info": {"file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "mimetype": "text/x-python", "version": "3.6.1", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "4ba984e341740e3083dad687525ed9c021c81fd3", "_cell_guid": "1fe71a5f-5470-4c61-aded-2cc489012df2"}, "source": ["End-to-end neural network model without statistic  features like N-gram or TF-IDF.  \n", "Gene and Variation info is processed by char-level CNN, and Text info is processed by word-level CNN,\n", "then 3 features are combined and then processed by MLP.  \n", "Main nonlinearlity is GLU.   \n", "Validation logloss is about 1.09 (sorry, I didn't CV yet).  \n", "This model is not so good and worse than MLP with statistic features."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "6d859f2ebaa1b8b00b63e3934f3fe22b7983ac5d", "_cell_guid": "e32c9ade-246a-44d1-9fe1-0ed5090f0ead"}, "source": ["import numpy as np\n", "import pandas as pd"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "1678cbd653e5ace6cd9272ebe8bc1324ecef2aad", "_cell_guid": "e317cfaf-4b1b-4a4f-903c-f1b2a9b8ec72"}, "source": ["### Step 1: load data\n", "train = pd.read_csv('../input/training_variants')\n", "test1 = pd.read_csv('../input/test_variants')\n", "test2 = pd.read_csv('../input/stage2_test_variants.csv')\n", "\n", "trainx = pd.read_csv('../input/training_text', sep=\"\\|\\|\", engine='python', \n", "                     header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "test1x = pd.read_csv('../input/stage2_test_text.csv', sep=\"\\|\\|\", engine='python', \n", "                    header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "testx2 = pd.read_csv('../input/stage2_test_text.csv', sep=\"\\|\\|\", engine='python', \n", "                     header=None, skiprows=1, names=[\"ID\",\"Text\"], encoding='utf-8')\n", "\n", "test_solution = pd.read_csv('../input/stage1_solution_filtered.csv')"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "3061a7c00f0bc579e9d0e49dec5fcc746ee3f8d1", "_cell_guid": "617db739-9d77-4158-ba48-015d82fab3b8"}, "source": ["# merge test 1 and test 1 solution\n", "test_idx = sorted(list(test_solution['ID'].unique()))\n", "test_filterd = test1.loc[test_idx]\n", "\n", "test_y = test_solution.iloc[:,1:].as_matrix()\n", "test_y = np.argmax(test_y, axis=1)\n", "test_y = test_y+1\n", "test_filterd['Class'] = test_y\n", "\n", "testx_filterd = test1x.loc[test_idx]"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "7507362881b196309f8768e05051ec6c6cd13f8f", "_cell_guid": "eb496d04-eed3-41ff-a450-7f1a16854de6"}, "source": ["# merge variants and text\n", "train = pd.merge(train, trainx, how='left', on='ID').fillna('')\n", "test_filterd = pd.merge(test_filterd, testx_filterd, how='left', on='ID').fillna('')\n", "test2 = pd.merge(test2, testx2, how='left', on='ID').fillna('')\n", "\n", "pid = test2['ID'].values"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "1f42ddacc8a4af7359b797cf1db60604ae00b9b3", "_cell_guid": "2afbe393-f5e6-4a91-bc56-d7de0d1074a7"}, "source": ["# merge training and test data\n", "train_test1 = pd.concat((train, test_filterd), axis=0, ignore_index=True)\n", "y = train_test1['Class'].values # y\u3092\u5206\u96e2\n", "train_test1 = train_test1.drop(['Class'], axis=1)\n", "\n", "df_all = pd.concat((train_test1, test2), axis=0, ignore_index=True)\n", "df_all.shape # should be (4675, 4)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "fb3b437829f51973635594f7566564ef6300ce7c", "_cell_guid": "81e257d4-631f-4bb6-b796-32a28622aa0a"}, "source": ["### Step 2: Text Tokenize\n", "from keras.preprocessing.text import Tokenizer\n", "\n", "# tokenize gene in char level\n", "gene_tokenizer = Tokenizer(char_level=True)\n", "print(\"tokenizer learning...\")\n", "gene_tokenizer.fit_on_texts(texts=df_all['Gene'])\n", "print(\"word count\", len(gene_tokenizer.word_counts)) # 37"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "b2141d7f42be3de5be35140d21cdeb17f7f39055", "_cell_guid": "633080cd-ee91-41cd-a3f5-30c30ebf52c4"}, "source": ["gene_token_list = gene_tokenizer.texts_to_sequences(df_all['Gene'])\n", "gene_token = np.zeros([len(gene_token_list), 9], dtype=np.uint8)\n", "for k, v in enumerate(gene_token_list):\n", "    gene_token[k,:len(v)] = np.array(v)\n", "for i in range(5):\n", "    print(df_all['Gene'][i], gene_token[i])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d8ffd8d7e43ecdffc30b1c04f006a7fbd83663bd", "_cell_guid": "34e41c8a-c29f-4d87-977d-7e975e070d5f"}, "source": ["# variation tokenize in char level\n", "vari_tokenizer = Tokenizer(char_level=True)\n", "print(\"tokenizer learning...\")\n", "vari_tokenizer.fit_on_texts(texts=df_all['Variation'])\n", "print(\"word count\", len(vari_tokenizer.word_counts)) # 65"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d7a8d25a56eb12a6a2b27b90b55d6af09dd2ae39", "_cell_guid": "786a02bd-afc9-4b58-8280-e8e6c86909aa"}, "source": ["vari_token_list = vari_tokenizer.texts_to_sequences(df_all['Variation'])\n", "vari_token = np.zeros([len(vari_token_list), 55], dtype=np.uint8)\n", "for k, v in enumerate(vari_token_list):\n", "    vari_token[k,:len(v)] = np.array(v)\n", "for i in range(5):\n", "    print(df_all['Variation'][i], vari_token[i])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "af7ebfc16bc79c5ea48aae4f1d5ba9a44905f3e9", "_cell_guid": "94423a19-eee9-4252-9d4a-63c1e8286480"}, "source": ["# text tokenize in word level. this process spends a few minutes.\n", "text_tokenizer = Tokenizer()\n", "print(\"tokenizer learning...\")\n", "text_tokenizer.fit_on_texts(texts=df_all['Text'])\n", "print(\"word count\", len(text_tokenizer.word_counts)) # 196704"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "4c57dc29f580ba1370d083e3272e3baffc058b08", "_kg_hide-output": true, "_cell_guid": "05646e10-d703-4a74-b9f8-37d8ceb2d74f"}, "source": ["text_token_list = text_tokenizer.texts_to_sequences(df_all['Text']) #this process spends a few minutes.\n", "print(text_token_list[0])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "fbf8a29522ed2aa74e0292da9d685180350c8fb0", "_cell_guid": "e0560aa5-9520-4427-a97e-0b84838160d8"}, "source": ["# split data into training and test\n", "gene_token_train = gene_token[:train_test1.shape[0]]\n", "gene_token_test = gene_token[train_test1.shape[0]:]\n", "\n", "vari_token_train = vari_token[:train_test1.shape[0]]\n", "vari_token_test = vari_token[train_test1.shape[0]:]\n", "\n", "text_token_train = text_token_list[:train_test1.shape[0]]\n", "text_token_test = text_token_list[train_test1.shape[0]:]"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "ad32ceb1dae10ed9645a683e5d49efc41a45901e", "_cell_guid": "b62bb397-3cd4-4c74-b251-9fa3bef980a6"}, "source": ["# make y into one-hot\n", "y = y -1\n", "encoded_y = np.eye(9)[y]"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "8297b3a7cf3db8680e8c0dc349e6641b85b3492d", "_cell_guid": "b5a9177d-8415-4f44-981a-ce788ca953da"}, "source": ["### Step 3: training\n", "# build model\n", "from keras.models import Model\n", "from keras.layers import Input, Embedding, Dense, Activation, Dropout, Reshape, Flatten\n", "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Concatenate, Add, Multiply\n", "from keras.optimizers import Adam, SGD\n", "\n", "def ConvGlu_block(input_tensor, nb_filter, kernel_size=7, strides=2):\n", "    x = Conv1D(nb_filter, kernel_size, padding='same', strides=strides)(input_tensor)\n", "    x = BatchNormalization()(x)\n", "    gate = Conv1D(nb_filter, kernel_size, padding='same', strides=strides)(input_tensor)\n", "    gate = BatchNormalization()(gate)\n", "    gate = Activation('sigmoid')(gate)\n", "    x = Multiply()([x, gate])\n", "    x = Dropout(0.5)(x)\n", "    shortcut = Conv1D(nb_filter, 1, padding='same', strides=strides)(input_tensor)\n", "    shortcut = BatchNormalization()(shortcut)\n", "\n", "    x = Add()([x, shortcut])\n", "\n", "    return x\n", "\n", "def CNN(k=9,\n", "        embed_size=128,\n", "        length=[9,55, 2048],\n", "        boc=196704,\n", "        ):\n", "\n", "    input_gene = Input(shape=(length[0],))\n", "    x = Embedding(37+1, embed_size)(input_gene)\n", "    x = Reshape((length[0], embed_size))(x)\n", "    x = ConvGlu_block(input_tensor=x, nb_filter=128, kernel_size=7, strides=1)\n", "    x = MaxPooling1D(pool_size=9)(x)\n", "    feature_gene = Flatten()(x)\n", "\n", "    input_vari = Input(shape=(length[1],))\n", "    x = Embedding(65+1, embed_size)(input_vari)\n", "    x = Reshape((length[1], embed_size))(x)\n", "    x = ConvGlu_block(input_tensor=x, nb_filter=128, kernel_size=7, strides=1)\n", "    x = MaxPooling1D(pool_size=55)(x)\n", "    feature_vari = Flatten()(x)\n", "\n", "    input_text = Input(shape=(length[2],))\n", "    x = Embedding(boc, embed_size)(input_text)\n", "    x = Reshape((length[2], embed_size))(x)\n", "    x = ConvGlu_block(input_tensor=x, nb_filter=128, kernel_size=7, strides=2)\n", "    x = ConvGlu_block(input_tensor=x, nb_filter=256, kernel_size=7, strides=2)\n", "    x = ConvGlu_block(input_tensor=x, nb_filter=512, kernel_size=7, strides=2)\n", "    x = ConvGlu_block(input_tensor=x, nb_filter=512, kernel_size=7, strides=2)\n", "    x = ConvGlu_block(input_tensor=x, nb_filter=512, kernel_size=7, strides=2)\n", "\n", "    x = MaxPooling1D(pool_size=length[2]//2**5)(x)\n", "\n", "    feature_text = Flatten()(x)\n", "\n", "    gate = Dense(256)(feature_text)\n", "    gate = BatchNormalization()(gate)\n", "    gate = Activation('sigmoid')(gate)\n", "\n", "    linear = Concatenate()([feature_gene, feature_vari])\n", "    gated = Multiply()([linear, gate])\n", "    x = Dense(1024)(gated)\n", "    x = Activation('relu')(x)\n", "    x = Dropout(0.5)(x)\n", "    x = Dense(1024)(x)\n", "    x = Activation('relu')(x)\n", "    x = Dropout(0.5)(x)\n", "    x = Dense(1024)(x)\n", "    x = Activation('relu')(x)\n", "    x = Dropout(0.2)(x)\n", "    y = Dense(k, activation='softmax')(x)\n", "\n", "    model = Model(inputs=[input_gene,\n", "                          input_vari,\n", "                          input_text\n", "                          ],\n", "                  outputs=y)\n", "    opt = SGD(decay=1e-6, momentum=0.1, nesterov=False)\n", "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n", "    return model\n", "\n", "#commented for Kaggle Limits\n", "len_text = 256  #Change to 2048, 256 for Kaggle Limits. \n", "boc = 1000 # Change to 196704, 1000 for Kaggle Limits. this is number of words to use.\n", "model = CNN(length=[9,55,len_text], boc=boc)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "a1092abf649e8ad39385faf2df5bce6a61783367", "_cell_guid": "52bbdb50-f67f-4dd5-a90d-c088bf510581"}, "source": ["# define batch generator\n", "def batch_generator(gene,vari, text, y, batch_size, shuffle=True, len_text=2048, boc=1000):\n", "    batch_index = 0\n", "    n = y.shape[0]\n", "    while 1:\n", "        if batch_index == 0:\n", "            index_array = np.arange(n)\n", "            if shuffle:\n", "                index_array = np.random.permutation(n)\n", "\n", "        current_index = (batch_index * batch_size) % n\n", "        if n >= current_index + batch_size:\n", "            current_batch_size = batch_size\n", "            batch_index += 1\n", "        else:\n", "            current_batch_size = n - current_index\n", "            batch_index = 0\n", "\n", "        batch_text = np.zeros([current_batch_size, len_text], np.uint32)\n", "        index_array_batch = index_array[current_index: current_index + current_batch_size]\n", "        for i in range(current_batch_size):\n", "            text_i = text[index_array_batch[i]]\n", "            text_i = np.array(text_i, dtype=np.uint32)\n", "            text_i = text_i[text_i<boc]\n", "            if text_i.shape[0] <= len_text:\n", "                batch_text[i,:text_i.shape[0]] = text_i\n", "            else:\n", "                if shuffle:\n", "                    start = np.random.randint(0, text_i.shape[0] - len_text)\n", "                else:\n", "                    start = 0\n", "                text_crop = text_i[start:start+len_text]\n", "                batch_text[i] = text_crop\n", "\n", "        batch_gene = gene[index_array[current_index: current_index + current_batch_size]]\n", "        batch_vari = vari[index_array[current_index: current_index + current_batch_size]]\n", "        batch_y = y[index_array[current_index: current_index + current_batch_size]]\n", "\n", "        yield [batch_gene, batch_vari, batch_text], batch_y"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "9cb4e147946d97252ac8369dcabb174c0ba74f18", "_cell_guid": "c9552c76-e7fd-48ed-b076-6bf36065f06c"}, "source": ["import math\n", "# parameters\n", "num_epoch = 5 #Change to 100, 5 for Kaggle Limits. \n", "batch_size = 16\n", "learning_rate = 0.001\n", "nb_val = 256\n", "\n", "nb_sample = y.shape[0]\n", "nb_train = nb_sample - nb_val\n", "nb_train_step = math.ceil(nb_train / batch_size)\n", "nb_val_step = math.ceil(nb_val / batch_size)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "e1ccc6bca6b4b9bb980a1d70b571ca743cb3b0c4", "_cell_guid": "46eb6d37-831e-4934-9cb6-51f73a6fe63d"}, "source": ["# split data into training and validation\n", "np.random.seed(42)\n", "perm = np.arange(nb_sample)\n", "np.random.shuffle(perm)\n", "idx_val, idx_train = perm[:nb_val], perm[nb_val:]\n", "text_train = []\n", "for i in idx_train:\n", "    text_train.append(text_token_train[i])\n", "text_val = []\n", "for i in idx_val:\n", "    text_val.append(text_token_train[i])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "8f64b9295be4e72ce6d31ffdcf1b50cc97b313fa", "_cell_guid": "9cdfd236-b102-40de-bd0c-235be637fec3"}, "source": ["# build batch generator\n", "gen = batch_generator(gene=gene_token_train[idx_train],\n", "                      vari=vari_token_train[idx_train,],\n", "                      text=text_train,\n", "                      y=encoded_y[idx_train],\n", "                      batch_size=batch_size,\n", "                      shuffle=True,\n", "                      len_text=len_text,\n", "                      boc=boc)\n", "\n", "gen_val = batch_generator(gene=gene_token_train[idx_val],\n", "                          vari=vari_token_train[idx_val],\n", "                          text=text_val,\n", "                          y=encoded_y[idx_val],\n", "                          batch_size=batch_size,\n", "                          shuffle=False,\n", "                          len_text=len_text,\n", "                          boc=boc)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e082463695125224cbd9a0a09655dbf552e54f3f", "_cell_guid": "ae44df23-e407-493d-a86b-0b42b2751641"}, "source": ["# training\n", "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n", "\n", "save_checkpoint = ModelCheckpoint(filepath='best_weight.hdf5', monitor='val_loss', save_best_only=True)\n", "lerning_rate_schedular = ReduceLROnPlateau(patience=8, min_lr=learning_rate * 0.00001)\n", "early_stopping = EarlyStopping(monitor='val_loss',patience=16, verbose=1, min_delta=1e-4, mode='min')\n", "Callbacks = [save_checkpoint, lerning_rate_schedular, early_stopping]\n", "\n", "model.fit_generator(gen,\n", "                    steps_per_epoch=nb_train_step,\n", "                    epochs=num_epoch,\n", "                    validation_data=gen_val,\n", "                    validation_steps=nb_val_step,\n", "                    callbacks=Callbacks)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "647af4ca1ec5583b361957540533f8e2c2739082", "_cell_guid": "8db625b6-6911-4f32-963c-ffb54abb9939"}, "source": ["### Step 4: prediction\n", "# build batch generator\n", "def test_batch_generator(gene, vari, text, batch_size, shuffle=True, len_text=2048, boc=1000):\n", "    batch_index = 0\n", "    n = gene.shape[0]\n", "    while 1:\n", "        if batch_index == 0:\n", "            index_array = np.arange(n)\n", "            if shuffle:\n", "                index_array = np.random.permutation(n)\n", "\n", "        current_index = (batch_index * batch_size) % n\n", "        if n >= current_index + batch_size:\n", "            current_batch_size = batch_size\n", "            batch_index += 1\n", "        else:\n", "            current_batch_size = n - current_index\n", "            batch_index = 0\n", "\n", "        batch_text = np.zeros([current_batch_size, len_text], np.uint32)\n", "        index_array_batch = index_array[current_index: current_index + current_batch_size]\n", "        for i in range(current_batch_size):\n", "            text_i = text[index_array_batch[i]]\n", "            text_i = np.array(text_i, dtype=np.uint32)\n", "            text_i = text_i[text_i<boc]\n", "            if text_i.shape[0] <= len_text:\n", "                batch_text[i,:text_i.shape[0]] = text_i\n", "            else:\n", "                if shuffle:\n", "                    start = np.random.randint(0, text_i.shape[0] - len_text)\n", "                else:\n", "                    start = 0\n", "                text_crop = text_i[start:start+len_text]\n", "                batch_text[i] = text_crop\n", "\n", "        batch_gene = gene[index_array[current_index: current_index + current_batch_size]]\n", "        batch_vari = vari[index_array[current_index: current_index + current_batch_size]]\n", "\n", "        yield [batch_gene, batch_vari, batch_text]\n", "        \n", "        \n", "gen_test = test_batch_generator(gene=gene_token_test,\n", "                                vari=vari_token_test,\n", "                                text=text_token_test,\n", "                                batch_size=batch_size,\n", "                                shuffle=False,\n", "                                len_text=len_text,\n", "                                boc=boc,\n", "                                )"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "0194386671e0bc195a2cdb6f66622be4954e119f", "_cell_guid": "16463d2f-54da-4466-a7d3-44a78861e22b"}, "source": ["# predict\n", "model.load_weights('best_weight.hdf5')\n", "nb_test = gene_token_test.shape[0]\n", "nb_test_step = math.ceil(nb_test / batch_size)\n", "y_pred = np.empty([nb_test, 9], dtype=np.float32)\n", "print('predicting...')\n", "for i in range(nb_test_step):\n", "    batch = next(gen_test)\n", "    predict = model.predict(batch)\n", "    if i != nb_test_step - 1:\n", "        y_pred[i * batch_size:(i + 1) * batch_size] = predict\n", "    else:\n", "        y_pred[i * batch_size:] = predict\n", "print('done.')"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "4729832426f9316ab9b53e7ef46244b7b85d88a4", "_cell_guid": "e4c80630-3c0a-43f1-903a-bf86a4d1ef02"}, "source": ["# make submission\n", "submission = pd.DataFrame(y_pred, columns=\n", "                          ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9'])\n", "submission['ID'] = np.arange(y_pred.shape[0]) + 1\n", "submission.to_csv(\"submission_CNN.csv\", index=False)"], "outputs": []}]}