{"cells":[{"metadata":{"_uuid":"b50052fc578d352468fd1f5dc42ab13f8439a94e"},"cell_type":"markdown","source":"# <a name=\"content_list\">Home work I (Part I)</a>\n   1. [Importing modules](#importing_modules)\n   2. [Simple features analysing](#features_analysing)\n       1. [Age](#age)\n       2. [Distance](#distance)\n           1. [Haversine function](#haversine_function)\n       3. [Average square footage](#average_sqft)\n       4. [Square footage of all basements](#all_basement_sqft)\n       5. [Date](#date)\n       6. [Renovation square footage difference](#sqft_15_diff)\n   3. [Clustering](#clustering)\n       1. [Districts](#districts)\n           1. [Districts clustering by price](#d_clustering_by_price)\n           2. [Districts clustering by location](#d_clustering_by_location)\n       2. [Neighborhoods](#neighborhoods)\n           1. [Neigborhoods clustering by price](#n_clustering_by_price)\n           2. [Neigborhoods clustering by location](#n_clustering_by_price)\n   4. [Linear regression](#linear_regression)\n       1. [Features usefulness analys](#features_usefulness_analys)\n           1. [Useless feature](#useless_feature)\n           2. [Usefull feature](#usefull_feature)\n       2. [Linear regression result](#lr_prediction_result)\n   5. [Decision tree](#decision_tree)\n       1. [Decision tree result](#dt_result)\n   6. [Summary](#summary)\n       \n![image.png](https://upload.wikimedia.org/wikipedia/commons/f/fb/Seattle_Columbia_Pano2.jpg)\n\nCourse: Introduction in machine learning\n\nLecturer: Taras Lehinevych\n\nAuthor: Kyryll Vasylenko"},{"metadata":{"_uuid":"5cf99890f3ca7a9b199ff2efa42ff4b229075625"},"cell_type":"markdown","source":"# <a name=\"importing_modules\">Importing modules, dataset initializing</a>\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"80874f9ab7c1705467d863f766c81dc9d4348eef"},"cell_type":"code","source":"# Importing\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport mpl_toolkits as mpl_toolkits\nfrom math import radians, cos, sin, asin, sqrt, log\nfrom datetime import datetime\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n#for decision tree\nfrom sklearn.tree import DecisionTreeRegressor\n#for clustering\\graphics\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\n\n# Dataset reading\nframe = pd.read_csv('../input/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ce6268b17b8f5db2295043d3c5dd378f6562718"},"cell_type":"markdown","source":"# <a name=\"features_analysing\">Features analysing</a>\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"3e01960c15d5d933ec953ce16c762a6fef8e1bcb"},"cell_type":"code","source":"frame.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13126b5e729ce091db7e68be89036e77e53e1178"},"cell_type":"markdown","source":"#### <a name=\"age\">Age</a>\n\nAge of house starting from renovation date or if house wasn't renovated - staring from year of building\n\nAs we can see, yr_renovated has 0 values, so, we can replase them with yr_build values and with formula below we will have house age\n\nage = yr_renovated - current_year"},{"metadata":{"trusted":false,"_uuid":"d150025111ee3a4a6f815441e408c9520d7dd9c6"},"cell_type":"code","source":"current_year = datetime.now().year\nfor i, x in frame[['yr_built']].itertuples():\n    if(frame.at[i, 'yr_renovated'] == 0):\n        frame.at[i, 'yr_renovated'] = x\n    frame.at[i, 'age'] = current_year - frame.at[i, 'yr_renovated']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4cccea5f055604607bc9f75781914f6c668cd5d"},"cell_type":"markdown","source":"#### <a name=\"distance\">Distance</a>\n\nLet's get from lot and lang coordinate and count distance from center of Seattle!\n\nMy conclusion - than house farther from the center, than house price lower."},{"metadata":{"_uuid":"64081d22cae7bf3072a210a6a196a03dacde8b95"},"cell_type":"markdown","source":"##### <a name=\"haversine_function\">Haversine auxiliary function</a>\n*Calculates the great circle distance between two points \non the earth (specified in decimal degrees)*\n\n$long$ - Longitude\n\n$lat$ - Latitude\n\n$r = 6371$ - Earth radius in kilometers\n\n$\\Delta lat = {lat_2 - lat_1} $\n\n$\\Delta long = {long_2 - long_1} $\n\n**haversine** = $2r\\arcsin{(\\sqrt{\\sin^2{(\\frac{\\Delta lat}{2})} + \\cos{(lat_1)} \\cos{(lat_2)} \\sin^2{(\\frac{\\Delta long}{2})}})}$"},{"metadata":{"trusted":false,"_uuid":"c11bb8fd901083d88c176b4e8f3525101d81c3a9"},"cell_type":"code","source":"def haversine(lon2, lat2):\n    # Seattle center coordinates\n    centerLon = -122.352033\n    centerLat = 47.622451\n\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [centerLon, centerLat, lon2, lat2])\n\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a)) \n    r = 6371 # Radius of earth in kilometers, 3956 for miles\n    return c * r","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d5fdce7217f4889128503fcbe4cf31c8109ff99f"},"cell_type":"code","source":"for i, lat, long in frame[['lat','long']].itertuples():\n    frame.at[i,'distance'] = haversine(long, lat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a652b2cddc83705b286ac740cf4a2f113a34fef"},"cell_type":"markdown","source":"#### <a name=\"average_sqft\">Average square footage</a>\n\nIf house has average square footage by room or by floor n, this house can be worst than house with average square footage m, where m > n"},{"metadata":{"trusted":false,"_uuid":"c744659c91462c92ce0eadaf5ad3d0490865f6a3"},"cell_type":"code","source":"for i, f, a, b, c, d in frame[['floors','bedrooms','bathrooms','sqft_above','sqft_basement']].itertuples():\n    frame.at[i,'average_sqft_by_room'] = ((c+d)/((a+b)+1))#+1 because house can be without bedrooms or bathrooms\n    frame.at[i,'average_sqft_by_floor'] = ((c+d)/f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8688f173224920cc1c258c029c628b92311f2c76"},"cell_type":"markdown","source":"#### <a name=\"all_basement_sqft\">All basements square footage</a>\n\nWe have sqft_basement field - information about house basement, we also have house floors count, we can multiply this parameters and get house sqft for all floors\n\nSuggestion: Than more floors, than biggest price of house"},{"metadata":{"trusted":false,"_uuid":"569778613813c946cc955edbf693590eff83b11a"},"cell_type":"code","source":"for i, f, sqft in frame[['floors','sqft_basement']].itertuples():\n    frame.at[i,'sqft_floors_mult_basement'] = f*sqft","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98392c8800d65f1198b467d3a5f3e3561d4b6024"},"cell_type":"markdown","source":"#### <a name=\"date\">Date</a>\n\n- We can't create regression by date object\n- Sales starts in 2014 year\n- Not effective convert date to millis or seconds or days, because generally house prices can't change each second or each day\n- Each year - too large\n- Each month - sounds good"},{"metadata":{"trusted":false,"_uuid":"76a31e6ef095b6185dedd544889326f41423fcd4"},"cell_type":"code","source":"frame['date'] = pd.to_datetime(frame['date'])\nfor i, d in frame[['date']].itertuples():\n    frame.at[i, 'mnths'] = (d.year - 2014)*12 + d.month\n\n#Delete useless field\ndel frame['date']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d9506c0d23d53e2633041432a769a0f5efe762e"},"cell_type":"markdown","source":"#### <a name=\"sqft_15_diff\">Square footage difference</a>\n\nCreated difference between sqft in 15 year and sqft living.\n\nWe have some result of reonvation for sque."},{"metadata":{"trusted":false,"_uuid":"c4249c6a971d92900b36875a76cf2fe18e61efc2"},"cell_type":"code","source":"#Convert difference between sqft_living15 and sqft_living to ren_living_diff - sqft difference after renovation \nfor i, l, lo, l15, lo15 in frame[['sqft_living','sqft_lot','sqft_living15','sqft_lot15']].itertuples():\n    frame.at[i,'ren_living_diff'] = l15-l\n    frame.at[i,'ren_lot_diff'] = lo15-lo","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58c568ca28ff1d07c4ac6e2a9ab268f788a5953c"},"cell_type":"markdown","source":"# <a name=\"clustering\">Clustering</a>\n\n[Back to content list](#content_list)"},{"metadata":{"_uuid":"8c4e2347a3467abea531ddd622870dd92260f9b8"},"cell_type":"markdown","source":"## <a name=\"districts\">Districts</a>\n\n- Seattle has 7 districts\n- So, I will split lots into seven clusters by coordinates\n    - Firstly I will split into clasters by location\n    - And I will split by price and distance, I think, can be price correlation by ditance from center\n    \n![image.png](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Seattle_City_Council_District_map.png/1280px-Seattle_City_Council_District_map.png)\n\n[Back to content list](#content_list)"},{"metadata":{"_uuid":"eb7235c6e0531c8baaeea9e1c5dfd84ae095616c"},"cell_type":"markdown","source":"# <a name=\"d_clustering_by_price\">Districts clustering by price</a>\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"2bd3614f01e204000fff882d72a7a10efed9b975"},"cell_type":"code","source":"# Simple points visualization by x and y coordinates\ndef draw(x, y):\n    data = frame[[x,y]].values\n    plt.scatter(data[:, 0], data[:, 1], s=5, alpha=.4)\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.show()\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e26fb36212108656070f4dd1f58bf40d4374a284"},"cell_type":"code","source":"# Distance by price visualization\ndist_price = draw('price','distance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8116f55c9eb43779ebe5acef3a74938ee98c6c8f"},"cell_type":"code","source":"# Train and centers initializing\nk_means_district = KMeans(n_clusters=7)\nk_means_district.fit(dist_price)\ncenters = k_means_district.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6af978f7e9f7b48b79801850e87e8259aec2430d"},"cell_type":"code","source":"# Districts clustering by price visualization\nplt.xlabel(\"price\")\nplt.ylabel(\"distance\")\nplt.scatter(dist_price[:, 0], dist_price[:, 1], c=k_means_district.predict(dist_price), s=10, alpha=1)\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=30, alpha=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"75e2d35d9aa709a73a521996ba9304f25169d562"},"cell_type":"code","source":"# Splitting lots into districts\ndistance_clusters = k_means_district.predict(dist_price)\nfor i, d in frame[['distance']].itertuples():\n    frame.at[i,'price_district'] = distance_clusters[i]\n    \npd.DataFrame({\n    'price': frame['price'],\n    'price_district': frame['price_district']\n    }).groupby('price_district').sum().plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d34ea1671045532a4c4e3d8b5e8c520b0a6b52a2"},"cell_type":"markdown","source":"# <a name=\"d_clustering_by_location\">Districts clustering by location</a>\n\n[Back to content list](#content_list)"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"dc75e68c9e3d9b7432e010030c27453d24b94b2f"},"cell_type":"code","source":"# Coordinates visualization\ncoordinates = draw('long','lat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dc69f2f9cb5bbc07323de932f8124b03b27745cb"},"cell_type":"code","source":"# Train and centers initialization \nk_means_district.fit(coordinates)\ncenters = k_means_district.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4769a6c78f223ef2615ef20269c4986c254f7104"},"cell_type":"code","source":"plt.xlabel(\"long\")\nplt.ylabel(\"lat\")\nplt.scatter(coordinates[:, 0], coordinates[:, 1], c=k_means_district.predict(coordinates), s=10, alpha=1)\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=10, alpha=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1ad7af68bda19e7c972133d3243e44d32ddd9a93"},"cell_type":"code","source":"# Splitting lots into districts\nloc_clusters = k_means_district.predict(coordinates)\nfor i, lat, long in frame[['lat','long']].itertuples():\n    frame.at[i,'loc_district'] = loc_clusters[i]\npd.DataFrame({\n    'price': frame['price'],\n    'loc_district': frame['loc_district']\n    }).groupby('loc_district').sum().plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2983768cc3d20aa7aa258f6d83c5127bd96b172"},"cell_type":"markdown","source":"# <a name=\"neighborhoods\">Neighborhoods</a>\n\n- And also Seattle has 127 neighborhoods\n- So, I will split coordinates into 127 clusters, because clustering by neighborhoods can give us more presicion than clustering by districts\n    - Split into clasters by location\n    - Split by price\n    \n[Back to content list](#content_list)"},{"metadata":{"_uuid":"6328b87df9f2c89b42ae32584b6ca575c92c56b6"},"cell_type":"markdown","source":"## <a name=\"n_clustering_by_price\">Neigborhoods clustering by price</a>\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"c422a407b304738e7e6c6b0828905584eca8ff80"},"cell_type":"code","source":"# Distance by price visualization\ndist_price = draw('price','distance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0ff6d74082fc70003f4c55036c9517813d31a311"},"cell_type":"code","source":"# Train and centers initializing\nk_means_neighborhoods = KMeans(n_clusters=127)\nk_means_neighborhoods.fit(dist_price)\ncenters = k_means_neighborhoods.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4a6f2d642fe83c6d49eae9ba0263bb263e2351ab"},"cell_type":"code","source":"# Neigborhoods clustering by price visualization\nplt.xlabel(\"price\")\nplt.ylabel(\"distance\")\nplt.scatter(dist_price[:, 0], dist_price[:, 1], c=k_means_neighborhoods.predict(dist_price), s=10, alpha=1)\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=10, alpha=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c724e36ab0c60686e0e3d984408a2a8b6ca98987"},"cell_type":"code","source":"# Splitting lots into neigborhoods\ndistance_clusters = k_means_neighborhoods.predict(dist_price)\nfor i, d in frame[['distance']].itertuples():\n    frame.at[i,'price_neigborhood'] = distance_clusters[i]\n\n#Visualisation\npd.DataFrame({\n    'price': frame['price'],\n    'price_neigborhood': frame['price_neigborhood']\n    }).groupby('price_neigborhood').sum().plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88383b61ab518cc45d6c2c92e4a988633fbdf628"},"cell_type":"markdown","source":"## <a name=\"n_clustering_by_price\">Neigborhoods clustering by location</a>\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"c852c28f54c9f3c4a5257dfcfe6faf616204a380"},"cell_type":"code","source":"# Train and centers initialization\nk_means_neighborhoods.fit(coordinates)\ncenters = k_means_neighborhoods.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"13ad7fc59eb7406a2ab857b197a45401f70ccae5"},"cell_type":"code","source":"plt.xlabel(\"long\")\nplt.ylabel(\"lat\")\nplt.scatter(coordinates[:, 0], coordinates[:, 1], c=k_means_neighborhoods.predict(coordinates), s=10, alpha=1)\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=10, alpha=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"430e57fbb8743521e40608c02ee1b7106aebcfd8"},"cell_type":"code","source":"# Splitting lots into neighborhoods\nloc_clusters = k_means_neighborhoods.predict(coordinates)\nfor i, lat, long in frame[['lat','long']].itertuples():\n    frame.at[i,'loc_district'] = loc_clusters[i]\npd.DataFrame({\n    'price': frame['price'],\n    'loc_district': frame['loc_district']\n    }).groupby('loc_district').sum().plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d0c58a503b5938a31461d2c9ca4595d4523bca3"},"cell_type":"markdown","source":"# <a name=\"linear_regression\">Linear Regression</a>\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"727ec280271553edde1b395d33776dbe29822324"},"cell_type":"code","source":"# Create linear regression object\nregr = linear_model.LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a99b88fff65a3417eaf577b3c35e9e509e554397"},"cell_type":"code","source":"def split(frame, features_columns, target_column):\n    # Split the frame into training/testing sets\n    train_frame, test_frame = train_test_split(frame, test_size = 0.2, random_state=49)\n    \n    train_features = train_frame.iloc[:,features_columns]\n    test_features = test_frame.iloc[:,features_columns]\n    \n    train_target = train_frame[target_column]\n    test_target = test_frame[target_column]\n    return train_features, train_target, test_features, test_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aff1d9f26f1b0d59e741a373526d3ef527ba8591"},"cell_type":"code","source":"def fit_and_test(train_features, train_target, test_features, test_target):\n    regr.fit(train_features, train_target)\n    train_prediction = regr.predict(train_features)\n    test_prediction = regr.predict(test_features)\n    \n    print(\":::Train:::\")\n    mse = mean_squared_error(train_target, train_prediction)\n    r2s = r2_score(train_target, train_prediction)\n    print(\"Mean squared error: %.2f\" % mse)\n    print('Variance score: %.2f' % r2s)\n    \n    mse = mean_squared_error(test_target, test_prediction)\n    r2s = r2_score(test_target, test_prediction)\n    print(\":::Test:::\")\n    print(\"Mean squared error: %.2f\" % mse)\n    print('Variance score: %.2f' % r2s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b20f047fba0b0580a68cf02f5cbc30c96fdfadb8"},"cell_type":"code","source":"#Fit and visualisation model by single feature\ndef fit_and_test_visualisation(frm, feature_column, target):\n    train_features, train_target, test_features, test_target = split(frame, feature_column, target)\n    \n    regr.fit(train_features, train_target)\n    train_prediction = regr.predict(train_features)\n    test_prediction = regr.predict(test_features)\n    \n    print('Feature # {} : {}'.format(feature_column[0], frm.columns[feature_column]))\n    \n    print(\":::Train:::\")\n    mse = mean_squared_error(train_target, train_prediction)\n    r2s = r2_score(train_target, train_prediction)\n    print(\"Mean squared error: %.2f\" % mse)\n    print('Variance score: %.2f' % r2s)\n    \n    mse = mean_squared_error(test_target, test_prediction)\n    r2s = r2_score(test_target, test_prediction)\n    print(\":::Test:::\")\n    print(\"Mean squared error: %.2f\" % mse)\n    print('Variance score: %.2f' % r2s)\n    \n    \n    plt.scatter(test_features, test_target,  color='black')\n    plt.plot(test_features, test_prediction, color='blue', linewidth=3)\n    plt.xticks(())\n    plt.yticks(())\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c5629c78df607c3927afc290b43697e00a04835"},"cell_type":"markdown","source":"# <a name=\"features_usefulness_analys\">Features usefulness analys</a>\n\nLet's build for each feature linear regression model and watch at the results\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"7d513875a1fdf5ebc40fda3dc5aed2d4f8e8e6e4"},"cell_type":"code","source":"for x in [*range(frame.columns.size)]:\n    fit_and_test_visualisation(frame, [x], 'price')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"598c3d06a7e2109a860be5cd6727b1f26d904d89"},"cell_type":"markdown","source":"## <a name=\"useless_feature\">Useless feature</a>\n\nWe built linear regression for target y by single x feature\n\nSuggestion:\n\nWhen variance score of linear regression by **x** feature ~0.00 and mean squared error very high* - feature useless and can't really increase variance of prediction.\n\nvery high value - for each dataset or for each target it can be difference value, but in my case big mean squared error > 140000000000. \n\nFor exmaple:\n\nFeature # 0 (id):\n- mean squared error = 141215184926.76\n- variance score = 0\n\n**Model results with using id for features:**\n- Train: Mean squared error: 35703385195.60; Variance score: 0.73;\n- Test: Mean squared error: 32429380744.27; Variance score: 0.77;\n\n**Model results without using id:**\n- Train: Mean squared error: 35841752226.07; Variance score: 0.73;\n- Test: Mean squared error: 32803207742.06; Variance score: 0.77;\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"86ecc199cf4f2fa454fbae64d0e2f9138733483f"},"cell_type":"code","source":"# Useless fields removing\nif 'id' in frame:\n    del frame['id']\nif 'zipcode' in frame:\n    del frame['zipcode']\nif 'sqft_lot' in frame:\n    del frame['sqft_lot']\nif 'yr_renovated' in frame:\n    del frame['yr_renovated']\nif 'sqft_living15' in frame:\n    del frame['sqft_living15']\nif 'sqft_lot15' in frame:\n    del frame['sqft_lot15']\nif 'ren_lot_diff' in frame:\n    del frame['ren_lot_diff']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"787ec3549fb795eb23b1375efa8a2dbb5d749bfb"},"cell_type":"code","source":"#visualize rest of features\nfor x in [*range(frame.columns.size)]:\n    fit_and_test_visualisation(frame, [x], 'price')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb7b2c0f55974d47091fc061bc1f20c81671df82"},"cell_type":"markdown","source":"## <a name=\"usefull_feature\">Usefull feature</a>\n\nI suggested, that if we created some new field with help of other fields - we can remove fields, that we used for creating new field.\n\nBut, in practice way I understood that we shouldn't remove used fields, because they further increase our varicance score in model.\n\nFor example:\n\n**Result for linear regression with all features**\n\n- Train: Mean squared error: 35703373173.44; Variance score: 0.73;\n\n- Test: Mean squared error: 32786022616.10; Variance score: 0.77;\n\n**Result for linear regression without long and lat that used for creating distance and several distincts clusters**\n\n- Train: Mean squared error: 37793733537.16; Variance score: 0.72;\n\n- Test: Mean squared error: 34805094308.38; Variance score: 0.75;\n\nLat and long - usefull features.\n\nFor other features, which even don't used for creating new field works next rule: IF we know, that feature not autogenerated as id and this feature increase variance score - we can add this feature in our list of features for linear regression.\n\n[Back to content list](#content_list)"},{"metadata":{"_uuid":"60d2db660d9463ac083ca7448bbbd7a3a8f3308f"},"cell_type":"markdown","source":"# <a name=\"lr_prediction_result\">Linear regression result</a>\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"ed04e88182a49d7b7932f46a0b7d107762af8e81"},"cell_type":"code","source":"price_index = 0\nfeatures = [*range(frame.columns.size)]\ntarget = features[price_index]\nfeatures.remove(target)\ntrain_features, train_target, test_features, test_target = split(frame, features, 'price')\nfit_and_test(train_features, train_target, test_features, test_target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d1bacf2bc965fe3434c1d62e587cf0487d44a49"},"cell_type":"markdown","source":"# <a name=\"decision_tree\">Decision tree</a>\n\n[Back to content list](#content_list)"},{"metadata":{"trusted":false,"_uuid":"1efebef2b557723088d2926518fa8c93fa2c7197"},"cell_type":"code","source":"# Split data\ntrain_features, train_target, test_features, test_target = split(frame, features, 'price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9363f926c4bacfa1aee86d6cefd63f97c2b7ad1e"},"cell_type":"code","source":"# Initialize regression models\ndecisionTrees = []\nfor i in range(1, 10):\n    decisionTrees.append(DecisionTreeRegressor(max_depth=i))\n    \nfor i in range(1, 8):\n    decisionTrees.append(DecisionTreeRegressor(max_depth=i*10))\nlen(decisionTrees)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bcf6038439d6f36f9ebd2651d8df881ed7144ae8"},"cell_type":"code","source":"def fit_and_visualize(trees):\n    depths = []\n    # fit regressions models\n    for t in trees:\n        t.fit(train_features, train_target)\n    # Predict\n    predictions = []\n    for t in trees:\n        predictions.append(t.predict(test_features))\n    train_scores = []\n    test_scores = []\n    depths = []\n    test_the_best_depths = 0\n    max_test = 0\n    train_the_best_depths = 0\n    max_train = 0\n    for t in trees:\n        train_score = t.score(train_features,train_target)\n        test_score = t.score(test_features,test_target)\n        depth = t.max_depth\n        depths.append(depth)\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n        if(train_score > max_train):\n            max_train = train_score\n            train_the_best_depths = depth\n        if(test_score > max_test):\n            max_test = train_score\n            test_the_best_depths = depth\n        print(\"{} level tree train score: {}\".format(depth,train_score))\n        print(\"{} level tree test score: {}\\n\".format(depth,test_score))\n    # Visualize\n    pd.DataFrame({\n    'tree_depth': depths,\n    'train score': train_scores,\n    'test score': test_scores\n    }).groupby('tree_depth').sum().plot()\n    \n    return train_the_best_depths, test_the_best_depths","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4f5d618b3743c0892a9a383755a39a84f0041492"},"cell_type":"code","source":"#reinitialize\nsmallDecisionTrees = []\nfor i in range(1, 100):\n    smallDecisionTrees.append(DecisionTreeRegressor(max_depth=i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"85dd2fa1a72d371a8cdc6a74148a65ec00c8f732"},"cell_type":"code","source":"train_the_best_depths, test_the_best_depths = fit_and_visualize(smallDecisionTrees)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"49cfb3d47a2ff6a1b40d7deb1fb18cfc7de92783"},"cell_type":"code","source":"print(\"The best train depth: {}\".format(train_the_best_depths))\nprint(\"The best test depth: {}\".format(test_the_best_depths))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e55c2640fa8d18ddf2b3ceb159c357e09db739b3"},"cell_type":"markdown","source":"# <a name=\"dt_result\">Decision tree result</a>\n\nHow we can see, the best depth for decision tree almost always > than depth for test depth, because when we have really deep tree, this tree just \"knows\" all cases in train dataset and become retrained.\n\nWhen we use retrained desicion tree for test dataset, which differend from train - this train has low accuracy.\n\n[Back to content list](#content_list)"},{"metadata":{"_uuid":"b7ec29e1f42ade076b41f85c8fc769170ca5459a"},"cell_type":"markdown","source":"# <a name=\"summary\">Summary</a>\n\nI think, that I achieved good accuracy, because all my features based on attantive data analysis:\n - Each column from data frame was analysed by feature:target linear regression which in my opinion very good show us feature usefulness for price target\n - Was added custom parameters which based on parameters from frame\n\nBut, it can be better, because:\n - I bad analysed data feature \n - The same with floors and coordinates\n\n**Linear regression:**\n- Train: Mean squared error: 35173162696.36; Variance score: 0.74;\n- Test: Mean squared error: 32489076781.56; Variance score: 0.77;\n\n[Back to content list](#content_list)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}