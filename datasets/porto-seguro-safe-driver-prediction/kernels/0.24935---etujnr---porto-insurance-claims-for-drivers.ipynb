{"cells": [{"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ff9b277855bd8d8e4a1fbf2bbf2bcf9976fa0244", "collapsed": true, "_cell_guid": "a9e0b8ba-6dff-4ef4-ac79-4ad913e77f3f"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "# Step 1: Data Preparation\n", "# Loading the required python package for analysis\n", "import numpy as np  # linear algebra\n", "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "import plotly.offline as py\n", "\n", "py.init_notebook_mode(connected=True)\n", "from plotly.graph_objs import Scatter, Figure, Layout\n", "\n", "import plotly.tools as tls\n", "import warnings\n", "\n", "import seaborn as sns\n", "\n", "plt.style.use('fivethirtyeight')\n", "sns.set_style(\"whitegrid\")\n", "\n", "from collections import Counter\n", "\n", "warnings.filterwarnings('ignore')\n", "\n", "import plotly.graph_objs as go\n", "import plotly.plotly as plpl\n", "\n", "# Step 2: Data Overview: file structure & content\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "train.head(10)\n", "print(train.head(10))\n", "pd.set_option('precision', 3)\n", "train.describe()\n", "print(train.describe())\n", "id_test = test['id'].values\n", "target_train = train['target'].values\n", "\n", "\n", "# Step 3: Data Validation Checks\n", "# To check if there is any null information in the dataset\n", "train.isnull().any().any()\n", "print(train.isnull().any().any())\n", "# We check if there's any NaN in the dataset\n", "train_cp = train\n", "# train_cp = train_cp.replace(-1, np.NaN)\n", "(train_cp == -1).sum()\n", "\n", "data = train\n", "col_with_nan = train_cp.columns[train_cp.isnull().any()].tolist()\n", "print(\"this dataset has %s Rows. \\n\" % (train_cp.shape[0]))\n", "\n", "vars_with_missing = []\n", "\n", "for f in train.columns:\n", "    missings = train[train[f] == -1][f].count()\n", "    if missings > 0:\n", "        vars_with_missing.append(f)\n", "        missings_perc = missings / train.shape[0]\n", "\n", "print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n", "print('In total, there are {} variables with missing values'.format(len(vars_with_missing)))\n", "\n", "f, ax = plt.subplots(1, 2, figsize=(20, 15))\n", "train['target'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\n", "ax[0].set_title('target')\n", "ax[0].set_ylabel('')\n", "sns.countplot('target', data=train, ax=ax[1])\n", "ax[1].set_title('target')\n", "plt.show()\n", "\n", "# Also, we can prepare a lists of numeric, categorical and binary columns\n", "# All features\n", "all_features = train.columns.tolist()\n", "all_features.remove('target')\n", "# Numerical features\n", "numeric_features = [x for x in all_features if x[-3:] not in ['bin', 'cat']]\n", "# Categorical features\n", "categorical_features = [x for x in all_features if x[-3:]=='cat']\n", "# Binary Features\n", "binary_features = [x for x in all_features if x[-3:]=='bin']\n", "train['target_name'] = train['target'].map({0: 'Not Filed', 1: 'Filed'})\n", "\n", "# Very big imbalance in the dataset as we can see from the plot\n", "train_float = train.select_dtypes(include=['float64'])\n", "train_int = train.select_dtypes(include=['int64'])\n", "Counter(train.dtypes.values)\n", "print(Counter(train.dtypes.values))\n", "\n", "# Step 4: Feature Inspection\n", "# We would be using correlation plots to inspect the data\n", "# Getting correlation matrix\n", "cor_matrix = train[numeric_features].corr().round(2)\n", "# Plotting heatmap\n", "fig = plt.figure(figsize=(18,18));\n", "sns.heatmap(cor_matrix, annot=True, center=0, cmap=sns.diverging_palette(250, 10, as_cmap=True), ax=plt.subplot(111));\n", "plt.show()\n", "\n", "# Exploring the numerical features in the dataset\n", "# Looping through and plotting the numerical features\n", "for column in numeric_features:\n", "    fig = plt.figure(figsize=(20,12))\n", "\n", "    # Plotting the Distribution\n", "    sns.distplot(train[column], ax=plt.subplot(221));\n", "    # Label for X-axis\n", "    plt.xlabel(column, fontsize=16);\n", "    # Label for Y-axis\n", "    plt.ylabel('Density', fontsize=16);\n", "    # Adding a title (One for the figure)\n", "    plt.suptitle('Plots for '+column, fontsize=20);\n", "\n", "    # The distribution per claim value\n", "    # When the claim is not filed\n", "    sns.distplot(train.loc[train.target==0, column], color='red', label='Claim not filed', ax=plt.subplot(222));\n", "    # When the claim is filed\n", "    sns.distplot(train.loc[train.target==1, column], color='green', label='Claim filed', ax=plt.subplot(222));\n", "    # Legend\n", "    plt.legend(loc='best')\n", "    # Labelling the X-axis\n", "    plt.xlabel(column, fontsize=16);\n", "    # Labelling the Y-axis\n", "    plt.ylabel('Density per Claim Value', fontsize=16);\n", "\n", "    # Preparing a boxplot of column per claim value\n", "    sns.boxplot(x=\"target_name\", y=column, data=train, ax=plt.subplot(224));\n", "    # Labelling the X-axis\n", "    plt.xlabel('Is Filed Claim?', fontsize=16);\n", "    # Labelling the Y-axis\n", "    plt.ylabel(column, fontsize=16);\n", "    plt.show()\n", "\n", "# Exploring the categorical features\n", "# Looping through and Plotting Categorical features\n", "for column in categorical_features:\n", "    # Figure initiation\n", "    fig = plt.figure(figsize=(18, 12))\n", "\n", "    # Number of occurrences per category\n", "    ax = sns.countplot(x=column, hue=\"target_name\", data=train, ax=plt.subplot(211));\n", "    # Labelling the X-axis\n", "    plt.xlabel(column, fontsize=16);\n", "    # Labelling the Y-axis\n", "    plt.ylabel('Number of occurrences', fontsize=16)\n", "    # Adding Title\n", "    plt.suptitle('Plots for ' + column, fontsize=16);\n", "\n", "    # Adding the percents over each bar\n", "    # Getting heights of the bars\n", "    height = [p.get_height() for p in ax.patches]\n", "    # Counting number of bar groups\n", "    ncol = int(len(height) / 2)\n", "    # Counting total height of groups\n", "    total = [height[i] + height[i + ncol] for i in range(ncol)] * 2\n", "    # Looping through bars\n", "    for i, p in enumerate(ax.patches):\n", "        # Adding percentages\n", "        ax.text(p.get_x() + p.get_width() / 2, height[i] * 1.01 + 1000,\n", "                '{:1.0%}'.format(height[i] / total[i]), ha=\"center\", size=14)\n", "\n", "    # Filed Claims percentage for every value of feature in teh dataset\n", "    sns.pointplot(x=column, y='target', data=train, ax=plt.subplot(212));\n", "    # Labelling the X-axis\n", "    plt.xlabel(column, fontsize=16);\n", "    # Labelling the Y-axis\n", "    plt.ylabel('Filed Claims Percentage', fontsize=16);\n", "    plt.show()\n", "\n", "# Exploring the Binary Features in the dataset\n", "# looping through and plotting binary features\n", "for column in binary_features:\n", "    fig = plt.figure(figsize=(18, 12))\n", "\n", "    # Finding the number of occurrences per binary value\n", "    ax = sns.countplot(x=column, hue=\"target_name\", data=train, ax=plt.subplot(211));\n", "    # Labelling the X-axis\n", "    plt.xlabel(column, fontsize=16);\n", "    # Labelling the Y-axis\n", "    plt.ylabel('Number of occurrences', fontsize=16)\n", "    # Adding title\n", "    plt.suptitle('Plots for ' + column, fontsize=16);\n", "\n", "    # Adding percents over bars\n", "    # Getting heights of our bars\n", "    height = [p.get_height() for p in ax.patches]\n", "    # Counting number of bar groups\n", "    ncol = int(len(height) / 2)\n", "    # Counting total height of groups\n", "    total = [height[i] + height[i + ncol] for i in range(ncol)] * 2\n", "    # Looping through bars\n", "    for i, p in enumerate(ax.patches):\n", "        # Adding percentages\n", "        ax.text(p.get_x() + p.get_width() / 2, height[i] * 1.01 + 1000,\n", "                '{:1.0%}'.format(height[i] / total[i]), ha=\"center\", size=14)\n", "\n", "    # Filed Claims percentage for every value of feature\n", "    sns.pointplot(x=column, y='target', data=train, ax=plt.subplot(212));\n", "    # Labelling the X-axis\n", "    plt.xlabel(column, fontsize=16);\n", "    # Labelling the Y-axis\n", "    plt.ylabel('Filed Claims Percentage', fontsize=16);\n", "    plt.show()\n", "\n", "\n", "# Step 5: Feature Importance\n", "# In this step, we would be creating/trying out different baseline of performance on the problem and check it using some algorithms\n", "# For now, I'll use the following algorithms\n", "# Linear Algorithms\n", "# a. Logistic Regression\n", "# b. Linear Discriminant Analysis\n", "# Nonlinear Algorithms\n", "# a. Gaussian Naive Bayes\n", "# b. Classification & Regression Trees (CART)\n", "\n", "'''\n", "# Getting the required python packages\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import classification_report\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "from sklearn.naive_bayes import GaussianNB\n", "\n", "# Preparing the test options and evaluation metrics\n", "num_folds = 10\n", "seed = 8\n", "scoring = 'Accuracy'\n", "\n", "X = np.asarray(train.drop(['id', 'target'], axis=1))\n", "Y = np.asarray(train['target'])\n", "\n", "validation_size = 0.4\n", "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n", "\n", "# We generate some results with the linear and non-linear algorithms\n", "models = [('LR', LogisticRegression()),\n", "          ('LDA', LinearDiscriminantAnalysis()),\n", "          ('CART', DecisionTreeClassifier()),\n", "          ('NB', GaussianNB())]\n", "results = []\n", "names = []\n", "\n", "for name, model in models:\n", "    print(\"Training model %s\" % (name))\n", "    model.fit(X_train, Y_train)\n", "    result = model.score(X_validation, Y_validation)\n", "    kfold = KFold(n_splits=num_folds, random_state=seed)\n", "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"Classifier score %s: %f\" % (name, result)\n", "    print(msg)\n", "print(\"----- Training Completed!! -----\")\n", "'''\n", "\n", "# Trying out a different Machine Learning Algorithm learned in class\n", "# to predict the outcome for the drivers\n", "# Multilayer Perceptron using Keras is Implemented\n", "import keras\n", "import sklearn.model_selection\n", "import pandas as pd\n", "\n", "# Load Datasets\n", "df_train  = pd.read_csv('../input/train.csv')\n", "df_test   = pd.read_csv('../input/test.csv')\n", "df_submit = pd.read_csv('../input/sample_submission.csv')\n", "\n", "# To numpy array - dataset of train\n", "x_all = df_train.drop(['target', 'id'], axis=1).values\n", "y_all = keras.utils.np_utils.to_categorical(df_train['target'].values)\n", "\n", "# Catering for imbalanced data\n", "y_all_0 = y_all[y_all[:,1]==0]\n", "y_all_1 = y_all[y_all[:,1]==1]\n", "x_all   = np.concatenate([x_all[y_all[:,1]==0], np.repeat(x_all[y_all[:,1]==1], repeats=int(len(y_all_0)/len(y_all_1)), axis=0)], axis=0)\n", "y_all   = np.concatenate([y_all[y_all[:,1]==0], np.repeat(y_all[y_all[:,1]==1], repeats=int(len(y_all_0)/len(y_all_1)), axis=0)], axis=0)\n", "\n", "# Split train/valid datasets\n", "x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(x_all, y_all, test_size=0.4, random_state=0)\n", "\n", "# Defining the model\n", "model = keras.models.Sequential()\n", "model.add(keras.layers.normalization.BatchNormalization(input_shape=tuple([x_train.shape[1]])))\n", "model.add(keras.layers.core.Dense(32, activation='relu'))\n", "model.add(keras.layers.core.Dropout(rate=0.5))\n", "model.add(keras.layers.normalization.BatchNormalization())\n", "model.add(keras.layers.core.Dense(32, activation='relu'))\n", "model.add(keras.layers.core.Dropout(rate=0.5))\n", "model.add(keras.layers.normalization.BatchNormalization())\n", "model.add(keras.layers.core.Dense(32, activation='relu'))\n", "model.add(keras.layers.core.Dropout(rate=0.5))\n", "model.add(keras.layers.core.Dense(2,   activation='sigmoid'))\n", "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\",metrics=[\"accuracy\"])\n", "print(model.summary())\n", "\n", "# Use Early-Stopping\n", "callback_early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n", "\n", "# Training the model\n", "model.fit(x_train, y_train, batch_size=1024, epochs=200, validation_data=(x_valid, y_valid), verbose=1, callbacks=[callback_early_stopping])\n", "\n", "# Predict test dataset\n", "x_test = df_test.drop(['id'], axis=1).values\n", "y_test = model.predict(x_test)\n", "\n", "# Output\n", "df_submit['target'] = y_test[:, 1]\n", "df_submit.to_csv('Output_Submission.csv', index=False)"], "outputs": []}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3"}}}