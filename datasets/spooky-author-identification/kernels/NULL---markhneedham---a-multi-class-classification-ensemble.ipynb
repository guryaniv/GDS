{"cells": [{"metadata": {"_cell_guid": "8121adda-2b1a-45a5-80e3-81a0373e5164", "_uuid": "95b188bb97330cea434f4c6ec8c5dd4a822e8bfd"}, "cell_type": "markdown", "source": ["I'd read a lot about ensemble models but I've never created one myself so I used this competition as an opportunity to figure out how to do it.\n", "\n", "# Libraries\n", "\n", "Let's first import our libraries."]}, {"execution_count": null, "metadata": {"_cell_guid": "638c82e8-b789-4200-a98f-da450455bc42", "_uuid": "60276106e569e24cb7f26e4070ee87574cc5af6c", "collapsed": true}, "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn import linear_model, metrics\n", "from sklearn.ensemble import VotingClassifier\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.pipeline import Pipeline"], "outputs": []}, {"metadata": {"_cell_guid": "2692f1da-62b5-4445-85d9-299fd9406ddd", "_uuid": "0c140e5374e7f110627512c767a6b203b9f7cd74"}, "cell_type": "markdown", "source": ["# Load our data\n", "\n", "Now we'll read the data into a Pandas dataframe."]}, {"execution_count": null, "metadata": {"_cell_guid": "c5d2cda6-ccfb-4fac-ad14-87227861b797", "_uuid": "5b9d4b7f8a71333908dc5ee578c5fe2dc9ce98e5", "collapsed": true}, "cell_type": "code", "source": ["Y_COLUMN = \"author\"\n", "TEXT_COLUMN = \"text\"\n", "train_df = pd.read_csv(\"../input/train.csv\", usecols=[Y_COLUMN, TEXT_COLUMN])\n", "train_df.head()"], "outputs": []}, {"metadata": {"_cell_guid": "891a206f-d88a-43d7-9e24-b5f6d1b3d3f9", "_uuid": "3d7dcd479c333f8912b92651a40fd8c73e2728e7"}, "cell_type": "markdown", "source": ["So far so good.\n", "\n", "# Building our classifiers\n", "\n", "Next we'll create a few different classifiers that create different text based features. I quite like sklearn's Pipeline abstraction as it makes it really easy to try out lots of different models. "]}, {"execution_count": null, "metadata": {"_cell_guid": "4f08e14b-2efe-4132-b258-7f3606d4c753", "_uuid": "0552aaf751c576cc19140d109c4ac406204a621e", "collapsed": true}, "cell_type": "code", "source": ["tfidf_pipe = Pipeline([\n", "    ('tfidf', TfidfVectorizer(min_df=3, max_features=None,\n", "                              strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n", "                              ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n", "                              stop_words='english')),\n", "    ('mnb', MultinomialNB())\n", "])\n", "\n", "unigram_pipe = Pipeline([\n", "    ('cv', CountVectorizer()),\n", "    ('mnb', MultinomialNB())\n", "])\n", "\n", "ngram_pipe = Pipeline([\n", "    ('cv', CountVectorizer(ngram_range=(1, 2))),\n", "    ('mnb', MultinomialNB())\n", "\n", "])"], "outputs": []}, {"metadata": {"_cell_guid": "c14143d8-fcc5-42a4-9e0b-dbe98e849310", "_uuid": "5b2ea19648e60a6d1e47d15d19529090f9b9fb05"}, "cell_type": "markdown", "source": ["# Testing our model\n", "\n", "Now we'll create a function to test our models. This function is from [Sohier Dane's tutorial](https://www.kaggle.com/sohier/intermediate-tutorial-python/)."]}, {"execution_count": null, "metadata": {"_cell_guid": "d8abf401-5d55-4722-8eae-0afa7cf3c258", "_uuid": "b3434a317e03b1c0e7312d7e82c150e44f6da049", "collapsed": true}, "cell_type": "code", "source": ["def test_pipeline(df, nlp_pipeline):\n", "    y = df[Y_COLUMN].copy()\n", "    X = pd.Series(df[TEXT_COLUMN])\n", "    rskf = StratifiedKFold(n_splits=5, random_state=1)\n", "    losses = []\n", "    accuracies = []\n", "    for train_index, test_index in rskf.split(X, y):\n", "        X_train, X_test = X[train_index], X[test_index]\n", "        y_train, y_test = y[train_index], y[test_index]\n", "        nlp_pipeline.fit(X_train, y_train)\n", "        losses.append(metrics.log_loss(y_test, nlp_pipeline.predict_proba(X_test)))\n", "        accuracies.append(metrics.accuracy_score(y_test, nlp_pipeline.predict(X_test)))\n", "\n", "    print(\"kfolds log losses: {0}, mean log loss: {1} mean accuracy: {2}\".format(\n", "        str([str(round(x, 3)) for x in sorted(losses)]),\n", "        round(np.mean(losses), 3),\n", "        round(np.mean(accuracies), 3)\n", "    ))"], "outputs": []}, {"metadata": {"_cell_guid": "c7f046aa-f4be-4a52-a901-7914bc50fbda", "_uuid": "717fbfe95572bfdbd78db3bfe3e05dfabebabd36"}, "cell_type": "markdown", "source": ["Now let's run the test function against our models."]}, {"execution_count": null, "metadata": {"_cell_guid": "6874b37e-5795-4b62-8bd8-bed9ad2e6ca7", "_uuid": "eaabe8bee17ecfb15aa0e7591429271c58c715c9", "collapsed": true}, "cell_type": "code", "source": ["test_pipeline(train_df, unigram_pipe)"], "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "867734f5-e645-4f02-92aa-d288be8b27d6", "_uuid": "9936aaea90564026e2cb2eb0499bfd296ce63117", "collapsed": true}, "cell_type": "code", "source": ["test_pipeline(train_df, ngram_pipe)"], "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "59e33736-2ee9-4e75-8ab6-9c7aabd1ac33", "_uuid": "5e56ad88c3870db8401dac35b824c0ef328349b9", "collapsed": true}, "cell_type": "code", "source": ["test_pipeline(train_df, tfidf_pipe)"], "outputs": []}, {"metadata": {"_cell_guid": "3378b002-70f2-491e-b5d5-2708f80fee92", "_uuid": "6f9614afecc5ea05d05303972535a17707bc10f2"}, "cell_type": "markdown", "source": ["# Building our ensemble model\n", "\n", "We can combine this classifiers together into an ensemble classifier using sklearn's VotingClassifier"]}, {"execution_count": null, "metadata": {"_cell_guid": "4176d3f9-f61b-41b0-bb79-8ba2464942a0", "_uuid": "4d3840e33c189c720b7d110ea462591cf4b86e20", "collapsed": true}, "cell_type": "code", "source": ["classifiers = [\n", "    (\"tfidf\", tfidf_pipe),\n", "    (\"ngram\", ngram_pipe),\n", "    (\"unigram\", unigram_pipe),\n", "]\n", "\n", "mixed_pipe = Pipeline([\n", "    (\"voting\", VotingClassifier(classifiers, voting=\"soft\"))\n", "])"], "outputs": []}, {"metadata": {"_cell_guid": "6a353f63-eb91-4816-ac5e-8923413bcfb5", "_uuid": "80a24a959c63e8195a37439346da358f160d5da8"}, "cell_type": "markdown", "source": ["Let's see how this one fares:"]}, {"execution_count": null, "metadata": {"_cell_guid": "ff3d0ca5-ade6-433b-a586-d08473e60aff", "_uuid": "27dac729bce2b82c95107d79bd1ee5d47d23bdb6", "collapsed": true}, "cell_type": "code", "source": ["test_pipeline(train_df, mixed_pipe)"], "outputs": []}, {"metadata": {"_cell_guid": "ef6d8607-2af1-4cb0-972f-b3bad25747d2", "_uuid": "9c335a0b8883ab780f2a6cc4c2d70f04b8f61e7e"}, "cell_type": "markdown", "source": ["It's an improvement on any of the individual models, but I was curious whether I needed all the individual models in the ensemble model. We can use GridScan to work this out:"]}, {"execution_count": null, "metadata": {"_cell_guid": "cf14ad1b-bbff-4f98-b506-86ec67808dac", "_uuid": "5cd6d5d9253d7d6c3dc74a7ef50a721e7847ee2f", "collapsed": true}, "cell_type": "code", "source": ["# This function generates all possible combinations of the classifiers\n", "#\u00a0e.g. \n", "# [0 0 0] all turned off\n", "#\u00a0[1 1 1] all turned on\n", "# [1 0 1] the first and last ones turned on, the middle one turned off\n", "def combinations_on_off(num_classifiers):\n", "    return [[int(x) for x in list(\"{0:0b}\".format(i).zfill(num_classifiers))]\n", "            for i in range(1, 2 ** num_classifiers)]\n", "\n", "param_grid = dict(\n", "        voting__weights=combinations_on_off(len(classifiers)),\n", ")\n", "\n", "grid_search = GridSearchCV(mixed_pipe, param_grid=param_grid, n_jobs=-1, verbose=10, scoring=\"neg_log_loss\")\n", "\n", "y = train_df[Y_COLUMN].copy()\n", "X = pd.Series(train_df[TEXT_COLUMN])\n", "\n", "grid_search.fit(X, y)\n", "\n", "cv_results = grid_search.cv_results_\n", "\n", "for mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n", "    print(params, mean_score)\n", "\n", "print(\"Best score: %0.3f\" % grid_search.best_score_)\n", "print(\"Best parameters set:\")\n", "best_parameters = grid_search.best_estimator_.get_params()\n", "for param_name in sorted(param_grid.keys()):\n", "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"], "outputs": []}, {"metadata": {"_cell_guid": "daca6d9d-aa0c-40bc-ab21-ab8360d76581", "_uuid": "4c621638e51bcc7d3b7a12a36244887cae320bf5"}, "cell_type": "markdown", "source": ["It looks like we can do without the first classifier - tfidf_pipe - in our final model. The next thing to explore is whether we get a better model by giving one of the remaining classifiers greater voting rights in the VotingClassifier."]}, {"metadata": {"_cell_guid": "1a22a188-6dd3-4084-8c97-6fefb52e7ade", "_uuid": "7d6046e78eef46a824070bafb01eb7f56241f1fb"}, "cell_type": "markdown", "source": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.3", "pygments_lexer": "ipython3"}}, "nbformat_minor": 1, "nbformat": 4}