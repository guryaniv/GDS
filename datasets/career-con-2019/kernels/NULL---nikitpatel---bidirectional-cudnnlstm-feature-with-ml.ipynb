{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedShuffleSplit,train_test_split\n\nimport keras\nimport tensorflow as tf\nfrom keras.models import Sequential,load_model,Model\nfrom keras.optimizers import *\nfrom keras.utils import to_categorical\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nsess = tf.Session()\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nK.tensorflow_backend._get_available_gpus()\nK.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_x = pd.read_csv(\"../input/X_train.csv\")\ntrain_y = pd.read_csv(\"../input/y_train.csv\")\ntest = pd.read_csv(\"../input/X_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3189b7a2dc028d9cb31e264e44a76ba1f919c2d1"},"cell_type":"code","source":"### column name and shape\n\nprint(\"train_x column name ---- \\n\",train_x.columns)\nprint(\"train_y column name ---- \\n\",train_y.columns)\nprint(\"train_x shape ---- \\n\",train_x.shape)\nprint(\"train_y shape ---- \\n\",train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea09a6b6e1256cd5d7c00d3cefee53627aae491f"},"cell_type":"code","source":"### train_x head\ntrain_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d4af800d9956ab22ffa4e8bdeb108159a4547f3"},"cell_type":"code","source":"train_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e13a78f73496cfb189a2e7ca931e145934f3d0e7"},"cell_type":"code","source":"### train_y head\n\ntrain_y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54c8cc88703de39a7342076f016371dc5546808e"},"cell_type":"code","source":"### check the traget variable\ntrain_y.groupby('surface')['surface'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"253d428de1c8a41d6c8e29b03b9c125810c383f6"},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nle.fit(train_y.surface)\ntrain_y['surface'] = le.transform(train_y.surface)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd1b69dafacaf07ca324c658644cbe8e4c68a128"},"cell_type":"code","source":"train_label = to_categorical(train_y['surface'])\ntrain_label.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80b67f7f072c3428d8e347b968987770f06d8c3b"},"cell_type":"code","source":"def feature_extraction(raw_frame):\n    raw_frame['orientation'] = raw_frame['orientation_X'] + raw_frame['orientation_Y'] + raw_frame['orientation_Z']+ raw_frame['orientation_W']\n    raw_frame['angular_velocity'] = raw_frame['angular_velocity_X'] + raw_frame['angular_velocity_Y'] + raw_frame['angular_velocity_Z']\n    raw_frame['linear_acceleration'] = raw_frame['linear_acceleration_X'] + raw_frame['linear_acceleration_Y'] + raw_frame['linear_acceleration_Y']\n    raw_frame['velocity_to_acceleration'] = raw_frame['angular_velocity'] / raw_frame['linear_acceleration']\n    raw_frame['velocity_linear_acceleration'] = raw_frame['linear_acceleration'] * raw_frame['angular_velocity']\n    return raw_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c480ea808893cbd0f11f7bee678dab805c283073"},"cell_type":"code","source":"train_df = feature_extraction(train_x)\ntest_df = feature_extraction(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64bce36d6b526ae2c735e6a235799749b35ebfae"},"cell_type":"code","source":"train_df = train_df.drop(['series_id', 'row_id'], axis=1)\ntest_df = test_df.drop(['series_id', 'row_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06c88549af6e03703e29534ea808f10ecc977476"},"cell_type":"code","source":"print(\"train shape\",train_df.shape)\nprint(\"test shape\", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db8a3663e563d81167fe7fbb24b8ac0a4a888803"},"cell_type":"code","source":"cols_normalize = train_df.columns\nmin_max_scaler = preprocessing.MinMaxScaler()\ntrain_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n                             columns=cols_normalize, \n                             index=train_df.index)\ntest_df = pd.DataFrame(min_max_scaler.fit_transform(test_df[cols_normalize]), \n                             columns=cols_normalize, \n                             index=test_df.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"befa50004f01795ba7f89fd706523556eb00657a"},"cell_type":"code","source":"seq_cols = train_df.columns\ndef gen_sequence(df,num_elements,seq_cols):\n    \n    data_matrix = df[seq_cols].values\n    \n    for start, stop in zip(range(0, num_elements + 128,128), range(128, num_elements + 128,128)):\n        yield data_matrix[start:stop, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc92c7e9d45ace41a646e993702c42e01d396d62"},"cell_type":"code","source":"train_df = list(gen_sequence(train_df,train_df.shape[0],seq_cols))\ntrain_df = np.array(train_df)\ntest_df = list(gen_sequence(test_df,test_df.shape[0],seq_cols))\ntest_df = np.array(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fcda9da1fcb33f10eca8015ae92676e11bd8f9d"},"cell_type":"code","source":"print(\"train shape\",train_df.shape)\nprint(\"test shape\", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac4d794fb108e555468ad453565cbe82aa817ab3"},"cell_type":"code","source":"train_x,val_x,train_y,val_y = train_test_split(train_df, train_label, test_size = 0.30, random_state=14)\ntrain_x.shape,val_x.shape,train_y.shape,val_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0685413f061bff22b6e88340566efc24decc598"},"cell_type":"code","source":"# train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1],train_x.shape[2], 1))\n# val_x = np.reshape(val_x, (val_x.shape[0], val_x.shape[1],val_x.shape[2], 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e3ef74c29809bd9034678a183febf5bced58918"},"cell_type":"code","source":"## https://www.kaggle.com/kabure/titanic-eda-keras-nn-pipelines\n## Creating the model\nmodel = Sequential()\n\n# Inputing the first layer with input dimensions\nmodel.add(Dense(16, \n                activation='relu',  \n                input_shape=(128, 16),\n                kernel_initializer='uniform',kernel_regularizer=regularizers.l2(0.01)))\n#model.add(BatchNormalization())\n# Adding an Dropout layer to previne from overfitting\nmodel.add(Dropout(0.50))\n\n#adding second hidden layer \nmodel.add(Dense(10,\n                kernel_initializer='uniform',\n                activation='relu', activity_regularizer=regularizers.l1(0.01)))\n#model.add(layers.MaxPooling1D())\n# Adding another Dropout layer\nmodel.add(Dropout(0.50))\nmodel.add(layers.Flatten())\n\n# adding the output layer that is binary [0,1]\nmodel.add(Dense(9, activation='softmax'))\n\n#Visualizing the model\nmodel.summary()\n\nsgd = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\n# Compiling our model\nmodel.compile(optimizer = sgd, \n                   loss = 'categorical_crossentropy', \n                   metrics = ['accuracy'])\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\nsave_best = ModelCheckpoint('cnn.hdf', save_best_only=True, \n                               monitor='val_loss', mode='min')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bb1861cca13a8d670616690c780b1a414d89abe"},"cell_type":"code","source":"from keras.models import Sequential,Model\nfrom keras.layers import CuDNNLSTM, Dense, Bidirectional, Input,Dropout\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\n\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n    \ndef make_model():\n    inp = Input(shape=(128, 16))\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(inp)\n    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(x)\n    x = Attention(128)(x)\n    # A intermediate full connected (Dense) can help to deal with nonlinears outputs\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(9, activation=\"softmax\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nmodel = make_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d4c865202716f5a69f4f4ab21d38bb4e1e7d25b"},"cell_type":"code","source":"# history = model.fit(train_x, train_y,\n#                     batch_size=32,\n#                     epochs=50,\n#                     verbose=1,\n#                     validation_data=(val_x, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6fe611d4a8e938f0c5e34e252b9660951758858"},"cell_type":"code","source":"model_feat = Model(inputs=model.input,outputs=model.get_layer('attention_1').output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11a17d32ede44a5703d0919881cd11e88ce27c64"},"cell_type":"code","source":"train_feature = model_feat.predict(train_x)\nval_feature = model_feat.predict(val_x)\ntest_feature = model_feat.predict(test_df)\ntrain_feature.shape,val_feature.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c78fb9054acfca41adf420f453a8c2deb140dc75"},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import mean_absolute_error\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\npara={'boosting_type': 'gbdt',\n 'colsample_bytree': 0.85,\n 'learning_rate': 0.1,\n 'max_bin': 512,\n 'max_depth': -1,\n 'metric': 'multi_error',\n 'min_child_samples': 8,\n 'min_child_weight': 1,\n 'min_split_gain': 0.5,\n 'nthread': 3,\n 'num_class': 9,\n 'num_leaves': 31,\n 'objective': 'multiclass',\n 'reg_alpha': 0.8,\n 'reg_lambda': 1.2,\n 'scale_pos_weight': 1,\n 'subsample': 0.7,\n 'subsample_for_bin': 200,\n 'subsample_freq': 1}\n\nClassifier = [\n    \n        LGBMClassifier(),\n        LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n        KNeighborsClassifier(),\n        SVC(kernel=\"rbf\", C=0.025, probability=True),\n        DecisionTreeClassifier(),\n        RandomForestClassifier(n_estimators=500,max_depth=20, min_samples_split=5,\n                             class_weight='balanced'),\n        AdaBoostClassifier(),\n        GaussianNB(),\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd6fe0a6be051190e39a2b5b1c3554b7f52502d0"},"cell_type":"code","source":"Accuracy=[]\nModel=[]\n\nfor classifier in Classifier:\n    try:\n        \n        fit = classifier.fit(train_feature,np.argmax(train_y,axis=1))\n        pred = fit.predict(val_feature)\n    except Exception:\n        fit = classifier.fit(train_feature,np.argmax(train_y,axis=1))\n        pred = fit.predict(val_feature)\n        \n        \n    score = accuracy_score(np.argmax(val_y,axis=1), pred)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+' is '+str(score))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a874e818c4e2b03542424b781f43e4fce873185"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3be5dfe97f94d1589bdb3dbd81caaec2dcd40a25"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}