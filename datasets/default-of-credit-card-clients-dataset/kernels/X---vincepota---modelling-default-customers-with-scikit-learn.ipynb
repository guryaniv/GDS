{"cells": [{"source": ["# Modelling default customers with scikit-learn\n", "Author : _Vincenzo Pota_\n", "\n", "Date : _16 August 2017_"], "metadata": {"_uuid": "2cf8d778035f7e544177cdfe8251a429bf230f58", "_cell_guid": "a6361db0-434f-45ac-b1ce-030fa2762f7f"}, "cell_type": "markdown"}, {"source": ["This notebook describes the code behind the modelling of the credit card dataset. It wants to provide some baseline solutions. Not much effort has been put into performance optimisation."], "metadata": {"_uuid": "0968cd1a9406ac4943280036e70001615544ae3a", "_cell_guid": "9389bbd3-12cb-4499-a7c5-d6868c3ad798"}, "cell_type": "markdown"}, {"source": ["## Data preparation\n", "Load libraries, load the dataset and drop columns we do not need"], "metadata": {"_uuid": "f98efe3c4568a3e818770786431bb32398a596e3", "_cell_guid": "1fe6d083-7efa-41ff-9b6c-b766bd679435"}, "cell_type": "markdown"}, {"outputs": [], "source": ["%matplotlib inline\n", "import pandas as pd\n", "import numpy as np\n", "from matplotlib.pylab import plt\n", "\n", "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n", "from sklearn.model_selection import train_test_split, cross_val_score\n", "from sklearn import preprocessing, metrics\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.naive_bayes import GaussianNB\n", "import itertools\n", "\n", "df = pd.read_csv('../input/UCI_Credit_Card.csv')\n", "df.columns = df.columns.str.lower()\n", "df.drop('id', axis=1, inplace=True) # we do not need id"], "execution_count": null, "metadata": {"_uuid": "aefecce6dc4466105f8cdbe5e83da4d211c2819e", "collapsed": true, "_cell_guid": "607586c1-4ea1-4d27-bbeb-2897ed18b2e7"}, "cell_type": "code"}, {"source": ["Assign English words to categorical variables to make interpretation easier"], "metadata": {"_uuid": "325f8a87313914578da0df0bb05a3b7f4e359c09", "_cell_guid": "daae6881-4b6d-4dbb-8d8b-da731c916d3d"}, "cell_type": "markdown"}, {"outputs": [], "source": ["df['sex'] = df['sex'].map({2:'female', 1:'male'})\n", "df['marriage'] = df['marriage'].map({1:'married', 2:'single', 3:'other', 0: 'other'}) \n", "df['education'] = df['education'].map({1:'graduate school', 2:'university', 3:'high school', 4:'others', 5:'unknown', 6:'unknown', 0:'unknown'})\n", "df['pay_0'] = df['pay_0'].astype(str) \n", "df['pay_2'] = df['pay_2'].astype(str) \n", "df['pay_3'] = df['pay_3'].astype(str) \n", "df['pay_4'] = df['pay_4'].astype(str) \n", "\n", "df.head()"], "execution_count": null, "metadata": {"_uuid": "4f0c7a52c1b42f655bec0c43f7eb21fd8406acaa", "_cell_guid": "1f36b1cf-19db-4a0d-a296-8f68d39f7879"}, "cell_type": "code"}, {"source": ["Let's transform categorical variables into discrete variables using `pd.get_dummies`. Note that `pay_X` metrics are also considered categorical. \n", "\n", "Let's create our feature vector `X` and target variable `y`. Let's rescale the metrics to the same mean and standard deviation."], "metadata": {"_uuid": "a13a0bb3d50391cb450e4cdf4a3290aa0b140864", "_cell_guid": "6a13ffed-9056-4625-befd-e0830c101ec8"}, "cell_type": "markdown"}, {"outputs": [], "source": ["X = pd.get_dummies(df[df.columns[:-1]],columns=['sex','marriage','education','pay_0','pay_2','pay_3','pay_4','pay_5','pay_6'])\n", "y = df[df.columns[-1]]\n", "features = X.columns\n", "\n", "scaler = preprocessing.StandardScaler()\n", "X = scaler.fit(X).transform(X)"], "execution_count": null, "metadata": {"_uuid": "a685ec1f7cb8a94bfb9bd8d8bbf9553d252632ec", "collapsed": true, "_cell_guid": "cbd363ed-8ffc-4ece-9f26-dadf653f1d3b"}, "cell_type": "code"}, {"source": ["Split into training and test set with 60% and 40%, respectively. "], "metadata": {"_uuid": "c0a7170ccab716a6cc15b0f8006855d5e9562e07", "_cell_guid": "3451df6d-0caa-496a-831d-8d4933e8ae66"}, "cell_type": "markdown"}, {"outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2)"], "execution_count": null, "metadata": {"_uuid": "ebb326fb98c326b6fa29c1ce94eff24064b25c27", "collapsed": true, "_cell_guid": "72663fd5-09b6-4f5b-9d6e-9179a0be5375"}, "cell_type": "code"}, {"source": ["## Modelling\n", "Let's try four models. The parameters of the models have not been optimised, but they work well enough for this task. "], "metadata": {"_uuid": "8e4919494bfa1abc25a21f8dc5a1da6ff819d155", "_cell_guid": "4513c2e7-b75c-4da6-8c95-67590147d25d"}, "cell_type": "markdown"}, {"outputs": [], "source": ["clfs = {'GradientBoosting': GradientBoostingClassifier(learning_rate= 0.05, max_depth= 6,\n", "                                                        n_estimators=200, max_features = 0.3,\n", "                                                        min_samples_leaf = 5),\n", "        'LogisticRegression' : LogisticRegression(C = 1.0),\n", "        'GaussianNB': GaussianNB(),\n", "        'RandomForest': RandomForestClassifier(n_estimators=50)\n", "        }"], "execution_count": null, "metadata": {"_uuid": "61965333551535f58e6f3c6b9b0a5f4b1f9889d2", "collapsed": true, "_cell_guid": "7fe0ae87-b19a-4070-aca1-8b996451dde3"}, "cell_type": "code"}, {"source": ["The following code will:\n", "1. fit the four models\n", "2. calculate the metrics in `cols`\n", "3. append the results to `models_report`\n", "4. create a `feature_importance` dataframe for tree-based models\n", "4. plot a ROC curve"], "metadata": {"_uuid": "d8188efd8c84260be4f3e0b5dac80b835c99be2c", "_cell_guid": "81bbd2cb-9591-4eee-82cb-db7e050ac1d5"}, "cell_type": "markdown"}, {"outputs": [], "source": ["cols = ['model','matthews_corrcoef', 'roc_auc_score', 'precision_score', 'recall_score','f1_score', 'accuracy']\n", "models_report = pd.DataFrame(columns = cols)\n", "feature_importance = pd.DataFrame()\n", "\n", "conf_matrix = dict()\n", "\n", "for clf, clf_name in zip(clfs.values(), clfs.keys()):\n", "    clf.fit(X_train,y_train)\n", "    y_pred = clf.predict(X_test)\n", "    y_score = clf.predict_proba(X_test)[:,1]\n", "\n", "    print('Computing{}'.format(clf_name))\n", "    \n", "    if (clf_name == 'RandomForest') | (clf_name == 'GradientBoosting'):\n", "        tmp_fi = pd.Series(clf.feature_importances_)\n", "        feature_importance[clf_name] = tmp_fi\n", "        \n", "\n", "    tmp = pd.Series({ \n", "                     'model': clf_name,\n", "                     'roc_auc_score' : metrics.roc_auc_score(y_test, y_score),\n", "                     'matthews_corrcoef': metrics.matthews_corrcoef(y_test, y_pred),\n", "                     'precision_score': metrics.precision_score(y_test, y_pred),\n", "                     'recall_score': metrics.recall_score(y_test, y_pred),\n", "                     'f1_score': metrics.f1_score(y_test, y_pred),\n", "                     'accuracy': metrics.accuracy_score(y_test, y_pred)},\n", "                   )\n", "\n", "    models_report = models_report.append(tmp, ignore_index = True)\n", "\n", "    conf_matrix[clf_name] = pd.crosstab(y_test, y_pred, rownames=['True'], colnames= ['Predicted'], margins=False)\n", "\n", "    precision, recall, _ = metrics.precision_recall_curve(y_test, y_score)\n", "    fpr, tpr, _ = metrics.roc_curve(y_test, y_score, drop_intermediate = False, pos_label = 1)\n", "\n", "    plt.figure(1, figsize = (6,5))\n", "    plt.xlabel('fpr')\n", "    plt.ylabel('tpr')\n", "    plt.plot(fpr, tpr, label = clf_name)\n", "    plt.legend(prop={'size':11})\n", "plt.plot([0,1], [0,1], c = 'black')\n", "plt.show()"], "execution_count": null, "metadata": {"_uuid": "ed59c30d3b6f0a55ea3b862e3824ee042da9d11c", "_cell_guid": "d488a208-2c66-4114-9b40-2491dc356337"}, "cell_type": "code"}, {"outputs": [], "source": ["models_report"], "execution_count": null, "metadata": {"_uuid": "68333689bdd36d330525f309f1e384bb271c90eb", "_cell_guid": "2560d201-e805-4c3d-a4c2-e5f34abe81a9"}, "cell_type": "code"}, {"source": ["With an AUC of 0.78, Gradient Boosting seems to win in terms of predictive power, although all models do a pretty nice job.\n", "\n", "\n", "Now, let's plot the confusion matrix for all models using the function `plot_confusion_matrix` which I recycled from [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)"], "metadata": {"_uuid": "549e344c59b79cef9e18e36203e201fce9c5aae6", "_cell_guid": "f1bceba9-5d1b-47c2-9d92-6f33bf76eea1"}, "cell_type": "markdown"}, {"outputs": [], "source": ["def plot_confusion_matrix(cm, ax, classes,\n", "                          normalize=False,\n", "                          title='Confusion matrix',\n", "                          cmap=plt.cm.Blues):\n", "    \"\"\"\n", "    This function prints and plots the confusion matrix.\n", "    Normalization can be applied by setting `normalize=True`.\n", "    \"\"\"\n", "    if normalize:\n", "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "\n", "    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    ax.set_title(title)\n", "    #ax.set_colorbar()\n", "    tick_marks = np.arange(len(classes))\n", "    ax.set_yticks(tick_marks)\n", "    ax.set_yticklabels(classes, rotation=35)\n", "\n", "    ax.set_xticks(tick_marks)\n", "    ax.set_xticklabels(classes, rotation=35)\n", "    \n", "    fmt = '.2f' if normalize else 'd'\n", "    thresh = cm.max() / 2.\n", "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "        ax.text(j, i, format(cm[i, j], fmt),\n", "                 horizontalalignment=\"center\",\n", "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n", "\n", "    #plt.tight_layout()\n", "    ax.set_ylabel('True label')\n", "    ax.set_xlabel('Predicted label')"], "execution_count": null, "metadata": {"_uuid": "6736c2fe190fee0b2c1169e2c9a788778198098e", "collapsed": true, "_cell_guid": "d6734847-174b-4094-b040-d8fbddaa7e69"}, "cell_type": "code"}, {"source": ["And now the plots. I used gridspec to place the plots in a 2x2 format."], "metadata": {"_uuid": "a8c89978facc285e8021b60a6cbb18ae0aceb347", "_cell_guid": "00f164d9-ed54-40b4-b18d-d67811853ff7"}, "cell_type": "markdown"}, {"outputs": [], "source": ["import matplotlib.gridspec as gridspec\n", "\n", "fig = plt.figure(figsize=(8, 8)) \n", "gs = gridspec.GridSpec(2, 2)\n", "\n", "ax1 = plt.subplot(gs[0,0])\n", "ax2 = plt.subplot(gs[0,1])\n", "ax3 = plt.subplot(gs[1,0])\n", "ax4 = plt.subplot(gs[1,1])\n", "\n", "for c, ax in zip(conf_matrix.keys(), [ax1,ax2,ax3,ax4]):\n", "    plot_confusion_matrix(conf_matrix[c].values, ax, title = c, classes=['No default','Default'])\n", "\n", "plt.tight_layout()\n", "plt.show()"], "execution_count": null, "metadata": {"_uuid": "28a4fac36b096c7d2aefab2539f8d98b8c3db388", "_cell_guid": "a3c0402b-6f38-485a-81d7-ef2f7efe9a13"}, "cell_type": "code"}, {"source": ["## Feature importance\n", "Let's which feature is more important. This only works for tree-based models. Note that the results for Gradient Boosting and Random Forest are similar, but there are some differences. For example, `age` is the most predictive metric according to Random Forest, whereas the bill amount from the month before the target month is the most predictive metric according to Gradient Boosting."], "metadata": {"_uuid": "811cf4f90080780f42a6297ee72b4452d980a705", "_cell_guid": "b1efdeaa-f0be-494d-b439-f338929b97a7"}, "cell_type": "markdown"}, {"outputs": [], "source": ["fi = feature_importance\n", "\n", "fi.index = features\n", "fi = fi.head(15) # Only take the 15 most important metrics\n", "fi = fi.sort_values('GradientBoosting', ascending=False)\n", "fi = (fi / fi.sum(axis=0)) * 100\n", "fi.plot.barh(title = 'Feature importances for Tree algorithms', figsize = (6,9))"], "execution_count": null, "metadata": {"_uuid": "df4c9ef42df78ab62e553d6a2ef5e8f863a9024f", "_cell_guid": "9690d3e9-d5d3-4dd1-ab58-92df8ec97c49"}, "cell_type": "code"}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "mimetype": "text/x-python", "version": "3.6.1", "file_extension": ".py", "nbconvert_exporter": "python", "name": "python"}}, "nbformat_minor": 1}