{"cells":[{"metadata":{"_uuid":"9ddc0605829dc579804ef767d3354786591193e6","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\n\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n\nimport nltk\nfrom nltk import wordpunct_tokenize\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d467d255825d9b9b26630dc8821aabe57ba814a3","trusted":false},"cell_type":"code","source":"train_df = pd.read_json(\"../input/train.json\").set_index(\"id\")\ntrain_df.cuisine = train_df.cuisine.astype(\"category\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c408045eec4bf23a3337bea825d86b451b77662"},"cell_type":"markdown","source":"Dishes have wide number of ingredients, with some having only one ingredient to as high as 65."},{"metadata":{"_uuid":"04548b879bc044842fb9cd438aa4f27a6a24924a","trusted":false},"cell_type":"code","source":"train_df.ingredients.apply(len).describe().astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d0d8c90ec3f2cf9ea44bd4545d4c6c22b6fec3a"},"cell_type":"markdown","source":"We have total of 20 cuisines with the distribution shown below."},{"metadata":{"_uuid":"fb750dd05a7a60e4e2cda18cbb2a65ff6c61eccb","trusted":false},"cell_type":"code","source":"train_df.cuisine.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f23c0e4ba6886a8e98451eb9463f25391eca55a6","trusted":false},"cell_type":"code","source":"train_df.cuisine.value_counts().plot(kind=\"barh\", color=\"steelblue\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"741037b4215ac49e18e6495c43e894fc0e482fb0"},"cell_type":"markdown","source":"There are a total of 6714 unique ingredients with salt, onion, olive oil, water and garlic being most commonly used."},{"metadata":{"_uuid":"1c333c8ee5b3a09a1589a684766891455131d238","trusted":false},"cell_type":"code","source":"from collections import Counter\n\ncuisines = train_df.cuisine.cat.categories.values.tolist()\n\ntexts = []\nlabels = []\n\nlabel2index = { cuisine: i for i, cuisine in enumerate(cuisines)}\nfor i, row in train_df.iterrows():\n    texts.append(\"\\n\".join(row.ingredients))\n    labels.append(label2index[row.cuisine])\n\nlabels = to_categorical(np.asarray(labels, dtype=np.int32))\n\nword_count = Counter()\n\ndef lemmatize(texts):\n    global word_count\n    wnl = nltk.WordNetLemmatizer()\n    for text in texts:\n        tokens_recipe = []\n        for sentence in text.split(\"\\n\"):\n            tokens_ingredient = [ wnl.lemmatize(w) for w in wordpunct_tokenize(sentence.lower()) if w.isalpha() ]\n            word_count.update(tokens_ingredient)\n            tokens_recipe.append(\" \".join(tokens_ingredient))\n        yield \" \".join(tokens_recipe)\n\ndef preprocess(texts):\n    processed_texts = list(lemmatize(texts))\n    black_list = [ word for word, count in word_count.items() if count < 5 ]\n    return [[ word for word in sentence.split() if word not in black_list] for sentence in processed_texts ]\n    \ntokenizer = Tokenizer(oov_token=\"<UNK>\")\nprocessed_texts = preprocess(texts)\ntokenizer.fit_on_texts(processed_texts)\n# sequences = tokenizer.texts_to_sequences(processed_texts)\n# padded_sequences = pad_sequences(sequences, maxlen=100)\nfeature_matrix = tokenizer.texts_to_matrix(processed_texts)\nword2index = tokenizer.word_index\n\nprint(\"Unique tokens: {}\".format(len(word2index)))\nfeature_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca937964710f88ce28d094ab13d1ce7d03ee9e94","trusted":false},"cell_type":"code","source":"from keras.layers import Dense, Dropout\nfrom keras.models import Sequential\n\ndef build_model(hidden_units, dropout):\n    model = Sequential()\n    model.add(Dense(hidden_units, input_shape=[1722,], activation=\"relu\", name=\"hidden\"))\n    model.add(Dropout(dropout, name=\"dropout\"))\n    model.add(Dense(20, name=\"output\"))\n    \n    model.compile(\"adam\", \"categorical_hinge\", metrics=[\"accuracy\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"9c7828a64b863cee562a2dec89f76cf301104c6b"},"cell_type":"code","source":"## do a grid search for hyper parameters\n# from keras.wrappers.scikit_learn import KerasClassifier\n# from sklearn.model_selection import GridSearchCV\n\n# np.random.seed(7)\n\n# model = KerasClassifier(build_fn=build_model, epochs=10, verbose=0)\n\n# param_grid = dict(\n#     batch_size = [128,],  # (32, 64, 128, 256, 512,),\n#     hidden_units = [2048,] , # [32, 64, 256, 512, 1024, 2048,],\n#     dropout = [0.8,], # [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,],\n#     )\n\n# cv = [train_test_split( np.arange(39774), test_size=0.2), train_test_split( np.arange(39774), test_size=0.2), ]\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, verbose=10, cv=cv)\n# grid_result = grid.fit(feature_matrix, labels)\n# print(\"best: {} using {}\".format(grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"364fd56463bd9a2480186019dede189dbb1342c0"},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11e008ecda47be6fc830aefe7f99126a766b5a75","trusted":false},"cell_type":"code","source":"test_df = pd.read_json(\"../input/test.json\")\ntest_texts = []\n\nfor i, row in test_df.iterrows():\n    test_texts.append(\"\\n\".join(row.ingredients))\n\nprocessed_texts = preprocess(test_texts)\ntest_feature_matrix = tokenizer.texts_to_matrix(processed_texts)\n\ntest_feature_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e7974f6d3fd256ae56bd19e62186448e38ac777","trusted":false},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom sklearn.metrics import hinge_loss, accuracy_score\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5, shuffle=True)\n\nmodels = []\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n\noos_y = []\noos_pred = []\n\nfor i, (train_index, test_index) in enumerate(kf.split(feature_matrix), start=1):\n    X_train, y_train = feature_matrix[train_index], labels[train_index]\n    X_val, y_val = feature_matrix[test_index], labels[test_index]\n    model = build_model(2048, 0.8)\n    chk_point = ModelCheckpoint(\"best-model-{}.h5\".format(i), monitor='val_loss', save_best_only=True, save_weights_only=True)\n    model.fit(X_train, y_train, \n        validation_data=(X_val, y_val),\n        epochs=50,\n        callbacks=[early_stopping, chk_point],\n        verbose=0,\n        batch_size=128)\n\n    # use the best model to predict\n    model.load_weights(\"best-model-{}.h5\".format(i))\n    y_pred = model.predict(X_val)\n    loss = hinge_loss(y_val.argmax(axis=1), y_pred)\n    accuracy = accuracy_score(y_val.argmax(axis=1), y_pred.argmax(axis=1))\n    \n    oos_y.append(y_val)\n    oos_pred.append(y_pred)\n    models.append(model)\n        \n    print(\"Fold {}, loss: {}, accuracy: {:.2%}\".format(i, loss, accuracy))\n    \ny_true = np.vstack(oos_y)\ny_pred = np.vstack(oos_pred)\n\nloss = hinge_loss(y_true.argmax(axis=1), y_pred)\naccuracy = accuracy_score(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n\nprint(\"loss: {}, accuracy: {:.2%}\".format(loss, accuracy))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"356e072a09f8ec957570354bd1ed06e947af3d83"},"cell_type":"markdown","source":"K Fold Cross Validation approach for Deep Learning by Jeff Heaton [1] .\n\n[1] https://www.youtube.com/watch?v=SIyMm5DFwQ8"},{"metadata":{"_uuid":"2d550f7b9a1e8b629ab29859d4f4d646f911ee04","trusted":false},"cell_type":"code","source":"summary = np.zeros((20, 20), dtype=np.int32)\nfor y_true_i, y_pred_i in zip(y_true.argmax(axis=1), y_pred.argmax(axis=1)):\n    summary[y_true_i, y_pred_i] += 1\n\nsummary_df = pd.DataFrame(summary, \n                          columns=cuisines, \n                          index=cuisines)\n\nsummary_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca2e0feb27ec296d136ccd104561b8b393783a68","trusted":false},"cell_type":"code","source":"import seaborn as sns \n\nsummary_norm = ( summary / y_true.sum(axis=0) )\nsns.heatmap( summary_norm, \n            vmin=0, vmax=1, center=0.5, \n            xticklabels=cuisines,\n            yticklabels=cuisines);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06bf5e33415d1be92d65d9968c820bde687d68db","trusted":false},"cell_type":"code","source":"test_pred = np.zeros((9944, 5), dtype=np.int32)\n\nfor i, model in enumerate(models):\n    y_pred = model.predict(test_feature_matrix)\n    test_pred[:, i] = y_pred.argmax(axis=1)\n    \ndef voting(arr):\n    return np.bincount(arr).argmax()\n\npredictions = np.apply_along_axis(voting, 1, test_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e1e80460c63cbd1fc5e11cfa0c81ed61294c0b1","trusted":false},"cell_type":"code","source":"result = pd.Series( pd.Categorical.from_codes(predictions, cuisines), test_df.id, name=\"cuisine\")\nresult.to_csv(\"submission.csv\", header=True)\nresult.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ec8b03519004e6bca0768df23fc7c0aa9f16252","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}