{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c85d3a41-39a8-00c8-b59d-133148f58bbb"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d48a87b5-7830-09ce-46cf-8dade1386a1c"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "64b5b54d-bd19-329e-ebf2-48fe70ebd907"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "pal = sns.color_palette()\n",
        "\n",
        "print('# File sizes')\n",
        "for f in os.listdir('../input'):\n",
        "    if 'zip' not in f:\n",
        "        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d7e99420-a814-0520-e1bc-fccc26466eb9"
      },
      "source": [
        "# Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9336b247-f9af-ff6a-b8cc-7ac5118383ad"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('../input/train.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8b99f9d9-ec2a-fef1-027c-ef5718fbf2a8"
      },
      "source": [
        "##We are given minimal number of data fields here, consisting of :\n",
        "\n",
        "**id**: Row id\n",
        "\n",
        "**qid{1, 2}**: The unique ID of each question in the pair\n",
        "\n",
        "**question{1, 2}**: The actual textual contents of the questions\n",
        "\n",
        "**is_duplicate**: The label that we are trying to predict - where the two questions are duplicates of each other. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc2f6f9b-a1cb-1fa2-4295-35e6e9a79e3e"
      },
      "outputs": [],
      "source": [
        "print('Total number of question pairs for training: {}'.format(len(df_train)))\n",
        "print('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100,2)))\n",
        "qids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\n",
        "print('Total number of questions in the training data: {}'.format(len(np.unique(qids))))\n",
        "print('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))\n",
        "\n",
        "plt.figure(figsize = (12, 5))\n",
        "plt.hist(qids.value_counts(), bins=50)\n",
        "plt.yscale('log', nonposy='clip') # yscale is log. Very small y value will be clipped to a very small postive number\n",
        "plt.title('Log-Histogram of questoin apperance counts')\n",
        "plt.xlabel('Numebr of occurences of question')\n",
        "plt.ylabel('Number of questions')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b0aa2a00-1c75-1b4f-ee47-f03853316075"
      },
      "source": [
        "From the plot above, we can find most of the questions appear few times. Only one of them appear 160 times, which might be an outlier. \n",
        "\n",
        "Also, 37% positive class in this dataset. Since we are using the LogLoss metric, and LogLoss looks at the actual predicts as opposed to the order of predictions, we should be able to get a decent score by creating a submission predicting the mean value of the label. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "42bb5a50-0c5a-bda5-38e3-272065c0a8c4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "p = df_train['is_duplicate'].mean() #Precdicted probability\n",
        "print('Predicted score:', log_loss(df_train['is_duplicate'], np.zeros_like(df_train['is_duplicate']) + p))\n",
        "#sklearn.metrics.log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)[source])\n",
        "\n",
        "df_test = pd.read_csv('../input/test.csv')\n",
        "sub = pd.DataFrame({'test_id': df_test['test_id'], 'is_duplicate': p})\n",
        "# pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)\n",
        "sub.to_csv('naive_submission.csv', index=False)\n",
        "sub.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c1f25dd-e61b-9727-dbfd-ae15d0ba3abc"
      },
      "outputs": [],
      "source": [
        "len(np.zeros_like(df_train['is_duplicate']) + p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ce170967-e7eb-3d1a-3082-98f0884f9584"
      },
      "source": [
        "# Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9e5b22e3-b8e6-8a4b-93a9-8bc88bc6a8ee"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('../input/test.csv')\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ac073646-5730-9c22-d0a5-a673b1f71316"
      },
      "outputs": [],
      "source": [
        "print('Total number of question pairs for testing: {}'.format(len(df_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a2eb8f44-e0dd-4c96-ed67-0feecb69dcda"
      },
      "source": [
        "Nothing out of the ordinary here. We are once again given rowIDs and the textual data of the two questions. It is worth noting that we are not given question IDs here however for the two questions in the pair.\n",
        "\n",
        "It is also worth pointing out that the actual number of test rows are likely to be much lower than 2.3 million. According to the data page, most of the rows in the test set are using auto-generated questions to pad out the dataset, and deter any hand-labelling. This means that the true number of rows that are scored could be very low.\n",
        "\n",
        "We can actually see in the head of the test data that some of the questions are obviously auto-generated, as we get delights such as \"How their can I start reading?\" and \"What foods fibre?\". Truly insightful questions.\n",
        "Now onto the good stuff - the text data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b15a8dcb-4e6e-fc90-453a-326892d7c6d0"
      },
      "source": [
        "# Text analysis\n",
        "First, some quick histograms to understand what we are looking at. Most analysis here will be only\n",
        "on the training set, to avoid the auto-generated questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "39fb29f4-8a55-d2ff-0126-67d0f6a18930"
      },
      "outputs": [],
      "source": [
        "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
        "test_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n",
        "\n",
        "dist_train = train_qs.apply(len)\n",
        "dist_test = test_qs.apply(len)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.hist(dist_train, bins=200, range=[0,200], color=pal[2], normed=True, label='train')\n",
        "plt.hist(dist_test, bins=200, range=[0,200], color=pal[1], normed=True,  alpha=0.5,label='test')\n",
        "plt.title('Normalised histogram of character count in questions', fontsize=15)\n",
        "plt.legend()\n",
        "plt.xlabel('Number of characters', fontsize=15)\n",
        "plt.ylabel('Probability', fontsize=15)\n",
        "\n",
        "print('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n",
        "                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "86d47d26-5694-d585-b4ac-be8648c82a2a"
      },
      "outputs": [],
      "source": [
        "dist_train = train_qs.apply(lambda x: len(x.split(' ')))\n",
        "dist_test = test_qs.apply(lambda x: len(x.split(' ')))\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.hist(dist_train, bins=50, range=[0, 50], color=pal[2], normed=True, label='train')\n",
        "plt.hist(dist_test, bins=50, range=[0, 50], color=pal[1], normed=True, alpha=0.5, label='test')\n",
        "plt.title('Normalised histogram of word count in questions', fontsize=15)\n",
        "plt.legend()\n",
        "plt.xlabel('Number of words', fontsize=15)\n",
        "plt.ylabel('Probability', fontsize=15)\n",
        "\n",
        "print('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n",
        "                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4895b9a4-a153-64ce-bc7a-37f41dd8a8af"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs.astype(str)))\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "512636d1-5524-1856-98c4-82854cd3a44d"
      },
      "source": [
        "# Semantic Analysis\n",
        "\n",
        "Take a look at usage of different punctuation in questions - this may form a basis for some interesting features later on. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "819b4e01-6977-6721-310f-e8cc210cc201"
      },
      "outputs": [],
      "source": [
        "qmarks = np.mean(train_qs.apply(lambda x: '?' in x))\n",
        "math = np.mean(train_qs.apply(lambda x: '[math]' in x))\n",
        "fullstop = np.mean(train_qs.apply(lambda x: '.' in x))\n",
        "capital_first = np.mean(train_qs.apply(lambda x: x[0].isupper()))\n",
        "capitals = np.mean(train_qs.apply(lambda x: max([y.isupper() for y in x])))\n",
        "numbers = np.mean(train_qs.apply(lambda x: max([y.isdigit() for y in x])))\n",
        "\n",
        "print('Questions with question marks: {:.2f}%'.format(qmarks * 100))\n",
        "print('Questions with [math] tags: {:.2f}%'.format(math * 100))\n",
        "print('Questions with full stops: {:.2f}%'.format(fullstop * 100))\n",
        "print('Questions with capitalised first letters: {:.2f}%'.format(capital_first * 100))\n",
        "print('Questions with capital letters: {:.2f}%'.format(capitals * 100))\n",
        "print('Questions with numbers: {:.2f}%'.format(numbers * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f8898515-e83d-7f6b-8d18-8226f30471f3"
      },
      "source": [
        "# Initial Feature Analysis\n",
        "\n",
        "Before taking a look at the model, we should take a look at how powerful some features are. Start with the word share feature from the benchmark model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b03ff4cc-65b8-a063-9a5c-45a3436f514a"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "def word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).lower().split():\n",
        "        if word not in stops:\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).lower().split():\n",
        "        if word not in stops:\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
        "    return R\n",
        "plt.figure(figsize=(15, 5))\n",
        "train_word_match = df_train.apply(word_match_share, axis=1, raw=True)\n",
        "plt.hist(train_word_match[df_train['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\n",
        "plt.hist(train_word_match[df_train['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\n",
        "plt.legend()\n",
        "plt.title('Label distribution over word_match_share', fontsize=15)\n",
        "plt.xlabel('word_match_share', fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "49196d6e-68aa-b59e-2173-b3220c335383"
      },
      "source": [
        "# TF_IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "812d0732-1908-8732-9aed-2d0d9cd9c491"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# If a word appears only once, we ignore it completely (likely a typo)\n",
        "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
        "def get_weight(count, eps=10000, min_count=2):\n",
        "    if count < min_count:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1 / (count + eps)\n",
        "\n",
        "eps = 5000 \n",
        "words = (\" \".join(train_qs)).lower().split()\n",
        "counts = Counter(words)\n",
        "weights = {word: get_weight(count) for word, count in counts.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ed9a5476-a397-019c-87dd-cffa21f2880d"
      },
      "outputs": [],
      "source": [
        "q1words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f47c5a1b-a8cf-2997-90cb-b58206290070"
      },
      "outputs": [],
      "source": [
        "q2words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2d14a18e-dc52-2cb1-f10d-6b0966cffec1"
      },
      "outputs": [],
      "source": [
        "shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "           \n",
        "shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "99b5f1bf-781a-ec41-9958-b634715d4cf2"
      },
      "outputs": [],
      "source": [
        "null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dfe5d33e-6929-a224-0f03-426b30087e09"
      },
      "outputs": [],
      "source": [
        "df_train[::]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "13db0bcf-54a2-ef14-4404-b4304a7b6dd3"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8d5cdc7f-de1e-3a25-ea17-486ca0ef33ce"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb5ec0df-6ab3-fdbd-2b28-8d4a50a124f3"
      },
      "outputs": [],
      "source": [
        "row['question1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "49474762-5411-4e30-d981-b3ea5cdfcf42"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}