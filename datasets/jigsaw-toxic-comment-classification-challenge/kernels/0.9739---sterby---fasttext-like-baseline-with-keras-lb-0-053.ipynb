{"nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.4", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python"}}, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["This is a simple notebook to get you started with keras. I choose a simple fasttext like approach I found useful as a quick first baseline after BOW and logistic regression. Have fun with it :)"], "metadata": {"_uuid": "56e5448cf75bce64f2a5c6bca1469323d30d4503", "_cell_guid": "5e2f73c7-b9c2-4361-8c12-a3743aacc992"}}, {"cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np"], "execution_count": null, "metadata": {"_uuid": "9f8d6f83925faf36ff75acefedbb423e6fe76918", "_cell_guid": "08af5ead-6614-4f92-a4e3-093697cd4630", "collapsed": true}, "outputs": []}, {"cell_type": "markdown", "source": ["# Load the data"], "metadata": {"_uuid": "1aee7e6435f922592df409c705751cc4a716ebc8", "_cell_guid": "fb04a7a2-f1e9-44ad-a320-c8feda8a2a61"}}, {"cell_type": "code", "source": ["train_df = pd.read_csv(\"../input/train.csv\")\n", "test_df = pd.read_csv(\"../input/test.csv\")"], "execution_count": null, "metadata": {"_uuid": "849d6b0c78be1391c6148869118c87c3d3712544", "_cell_guid": "09bf6043-2919-4aac-9b85-aa2dc71554d0", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["train_df.shape"], "execution_count": null, "metadata": {"_uuid": "ffe759c3c89fa877cbf21965e862e3a66475e515", "_cell_guid": "be3fbd7d-f93f-49e2-8892-5387789165a5", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["train_df.head()"], "execution_count": null, "metadata": {"_uuid": "4406ce81bc1da62436aa8d849426bde899ff1cfc", "_cell_guid": "6d95839a-7b1a-49db-89c2-37fe29d6bdfa", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["X_train = train_df[\"comment_text\"].fillna(\"sterby\").values\n", "y_train = train_df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n", "X_test = test_df[\"comment_text\"].fillna(\"sterby\").values"], "execution_count": null, "metadata": {"_uuid": "837178d74233a88f98b48b7e66e55771f0093fb5", "_cell_guid": "b3575b93-1b4b-4dd5-a624-816d87fa6724", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["i = 0\n", "print(\"Comment: {}\".format(X_train[i]))\n", "print(\"Label: {}\".format(y_train[i]))"], "execution_count": null, "metadata": {"_uuid": "4732035b6722f3edb52eb6b7d793018186eefb00", "_cell_guid": "bc678041-4cc1-4ac3-8725-b82e64df2306", "collapsed": true}, "outputs": []}, {"cell_type": "markdown", "source": ["# Use simple fasttext-like model"], "metadata": {"_uuid": "53f203285fae275a29a91f7ef41ef164f867ab66", "_cell_guid": "d779dd97-8872-4142-b4a0-ec567d22c1db"}}, {"cell_type": "code", "source": ["from keras.preprocessing import sequence\n", "from keras.models import Model, Input\n", "from keras.layers import Dense, SpatialDropout1D, Dropout\n", "from keras.layers import Embedding, GlobalMaxPool1D, BatchNormalization\n", "from keras.preprocessing.text import Tokenizer"], "execution_count": null, "metadata": {"_uuid": "ff8e5dd6f75eebf86bde9100e0237c22d38e8efd", "_cell_guid": "a08d571b-700c-406a-9350-43bdead97029", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["# Set parameters:\n", "max_features = 50000\n", "maxlen = 150\n", "batch_size = 32\n", "embedding_dims = 64\n", "epochs = 4"], "execution_count": null, "metadata": {"_uuid": "f14a84b803adc74b14b11e871ef87d9361f29d8a", "_cell_guid": "b291a985-4b50-4a1f-b1d8-80772a4f7992", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["print('Tokenizing data...')\n", "tok = Tokenizer(num_words=max_features)\n", "tok.fit_on_texts(list(X_train) + list(X_test))\n", "x_train = tok.texts_to_sequences(X_train)\n", "x_test = tok.texts_to_sequences(X_test)\n", "print(len(x_train), 'train sequences')\n", "print(len(x_test), 'test sequences')\n", "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n", "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))"], "execution_count": null, "metadata": {"_uuid": "dd03a24e101354dd3e42a7c56ffa4299e300e3d0", "_cell_guid": "6c66a34e-f161-43c6-9516-da1c4529df2d", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["print('Pad sequences (samples x time)')\n", "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n", "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n", "print('x_train shape:', x_train.shape)\n", "print('x_test shape:', x_test.shape)"], "execution_count": null, "metadata": {"_uuid": "6ec8bdd68059159a542cab8c9ff32d31d26dc373", "_cell_guid": "0cf4fbaa-0127-4074-8294-b9c735f39680", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["print('Build model...')\n", "comment_input = Input((maxlen,))\n", "\n", "# we start off with an efficient embedding layer which maps\n", "# our vocab indices into embedding_dims dimensions\n", "comment_emb = Embedding(max_features, embedding_dims, input_length=maxlen)(comment_input)\n", "\n", "# we add a GlobalMaxPool1D, which will extract information from the embeddings\n", "# of all words in the document\n", "comment_emb = SpatialDropout1D(0.25)(comment_emb)\n", "max_emb = GlobalMaxPool1D()(comment_emb)\n", "\n", "# normalized dense layer followed by dropout\n", "main = BatchNormalization()(max_emb)\n", "main = Dense(64)(main)\n", "main = Dropout(0.5)(main)\n", "\n", "# We project onto a six-unit output layer, and squash it with sigmoids:\n", "output = Dense(6, activation='sigmoid')(main)\n", "\n", "model = Model(inputs=comment_input, outputs=output)\n", "\n", "model.compile(loss='binary_crossentropy',\n", "              optimizer='adam',\n", "              metrics=['accuracy'])"], "execution_count": null, "metadata": {"_uuid": "2fdfd068dd38b910f0b3605b863bf65d4e2d34c7", "_cell_guid": "38454ada-8e95-4add-a039-345bc6856666", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"], "execution_count": null, "metadata": {"_uuid": "78d7759e7b282f535198186cb864c5deece6ee73", "_cell_guid": "6e64b88a-81aa-455d-af55-c7b54625ddd0", "collapsed": true}, "outputs": []}, {"cell_type": "markdown", "source": ["# submit"], "metadata": {"_uuid": "61aba7e0f1a356b78fbd8db340c7560fe35bbe59", "_cell_guid": "e00bf5fc-ac8a-491f-b1f1-25b3e153f846"}}, {"cell_type": "code", "source": ["y_pred = model.predict(x_test)"], "execution_count": null, "metadata": {"_uuid": "eb1ab1af5e05e995fd56bdf4a715bcde1f5b426d", "_cell_guid": "82411b4f-6212-4ba4-b9ec-8329e79f63bf", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["submission = pd.read_csv(\"../input/sample_submission.csv\")"], "execution_count": null, "metadata": {"_uuid": "7d98f168948c9240b680808ccf306220b045a876", "_cell_guid": "e4dabe11-4cf0-4feb-af9b-e5074233511f", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred / 1.4"], "execution_count": null, "metadata": {"_uuid": "c5aaed3d9c600e4a674959902f9552a3112d47e8", "_cell_guid": "5491387a-4e8e-4080-b4cb-d549c3372f6b", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["submission.head()"], "execution_count": null, "metadata": {"_uuid": "e335a3a0af28935e41aa08a140129033ef298658", "_cell_guid": "3342bf21-3402-44f1-b601-51b6a0684515", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["submission.to_csv(\"submission_bn_fasttext.csv\", index=False)"], "execution_count": null, "metadata": {"_uuid": "46dd6762d02e4de6c0fbef22f3257bc740f306b5", "_cell_guid": "50495ec2-e5d8-41cd-8b3a-c2779ef7b61c", "collapsed": true}, "outputs": []}]}