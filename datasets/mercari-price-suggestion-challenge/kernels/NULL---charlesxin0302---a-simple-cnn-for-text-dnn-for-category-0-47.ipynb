{"cells": [{"outputs": [], "source": ["import numpy as np \n", "import pandas as pd \n", "from keras import backend as K\n", "import gc\n", "import time\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.preprocessing import LabelBinarizer\n", "from keras.preprocessing.text import Tokenizer\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.cross_validation import train_test_split\n", "from keras.preprocessing.sequence import pad_sequences\n", "\n", "def rmsle_cust(y_true, y_pred):\n", "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n", "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n", "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n", "\n", "def split_cat(text):\n", "    try: return text.split(\"/\")\n", "    except: return (\"No Label\", \"No Label\", \"No Label\")\n", " \n", "\n", "def handle_missing_inplace(dataset):\n", "    dataset['general_cat'].fillna(value='No Label', inplace=True)\n", "    dataset['subcat_1'].fillna(value='No Label', inplace=True)\n", "    dataset['subcat_2'].fillna(value='No Label', inplace=True)\n", "    dataset['brand_name'].fillna(value='missing', inplace=True)\n", "    dataset['item_description'].fillna(value='No description yet', inplace=True)\n", "\n", "\n", "def cutting(dataset):\n", "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n", "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n", "    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n", "    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n", "    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n", "    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n", "    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n", "    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n", "\n", "\n", "def to_categorical(dataset):\n", "    dataset['general_cat'] = dataset['general_cat'].astype('category')\n", "    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n", "    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n", "\n", "\n", "def raw_text(dataset):   \n", "    raw_text = np.hstack([dataset.item_description.str.lower(), dataset.name.str.lower()])\n", "    tok_raw = Tokenizer(num_words=20000,\n", "                    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n", "                    lower=True,\n", "                    split=\" \",\n", "                    char_level=False)\n", "    tok_raw.fit_on_texts(raw_text)\n", "    dataset[\"seq_item_description\"] = tok_raw.texts_to_sequences(dataset.item_description.str.lower())\n", "    dataset[\"seq_name\"] = tok_raw.texts_to_sequences(dataset.name.str.lower())\n", "    dataset[\"Raw Text Combined\"] = dataset.seq_name + dataset.seq_item_description\n", "\n", "\n", "def get_keras_data(dataset):\n", "    X = {\n", "        'name': pad_sequences(dataset.seq_name, maxlen=10)\n", "        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=75)\n", "        ,'brand_name': np.array(dataset.brand_name)\n", "        ,'general_cat': np.array(dataset.general_cat)\n", "        ,'subcat_1': np.array(dataset.subcat_1)\n", "        ,'subcat_2': np.array(dataset.subcat_2)\n", "        ,'item_condition': np.array(dataset.item_condition_id)\n", "        ,'num_vars': np.array(dataset.shipping)\n", "    }\n", "    return X\n", "\n", "NUM_BRANDS = 4000\n", "NUM_CATEGORIES = 1000\n"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "9aca301e-eaa5-41ef-a98e-0d31feb4b07a", "_uuid": "5accf954b770969974dc8ebbe0b7e77bd0bdf6f2"}}, {"outputs": [], "source": ["#LOAD DATA\n", "train = pd.read_table(\"../input/train.tsv\")\n", "test = pd.read_table(\"../input/test.tsv\")\n", "\n", "print(train.shape)\n", "print(test.shape)\n", "train.head(3)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "4be498b6-6a98-4682-85c6-e98399365aee", "scrolled": true, "_uuid": "4822739212490e805076eaee586585f857b025a2"}}, {"outputs": [], "source": ["start_time = time.time()\n", "\n", "nrow_train = train.shape[0]\n", "merge: pd.DataFrame = pd.concat([train, test])\n", "submission: pd.DataFrame = test[['test_id']]\n", "\n", "del train\n", "del test\n", "gc.collect()\n", "\n", "merge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\n", "zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n", "merge.drop('category_name', axis=1, inplace=True)\n", "print('[{}] Split categories completed.'.format(time.time() - start_time))\n", "\n", "handle_missing_inplace(merge)\n", "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n", "\n", "cutting(merge)\n", "print('[{}] Cut completed.'.format(time.time() - start_time))\n", "\n", "to_categorical(merge)\n", "print('[{}] Convert categorical completed'.format(time.time() - start_time))\n", "\n", "raw_text(merge)\n", "print('[{}] Raw text completed'.format(time.time() - start_time))\n", "\n", "le = LabelEncoder()\n", "merge.brand_name = le.fit_transform(merge.brand_name)\n", "merge.general_cat = le.fit_transform(merge.general_cat)\n", "merge.subcat_1 = le.fit_transform(merge.subcat_1)\n", "merge.subcat_2 = le.fit_transform(merge.subcat_2)\n", "print('[{}] category variable labelled completed'.format(time.time() - start_time))\n", "\n", "merge.head(3)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "c4998727-9ada-4602-87a3-fbb6844ec478", "scrolled": true, "_uuid": "a436779f7373b73a57b7f271c37ba53c6e0ad69c"}}, {"outputs": [], "source": ["#EXTRACT DEVELOPTMENT TEST\n", "dtest = merge.iloc[nrow_train:, ]\n", "dtrain, dvalid = train_test_split(merge.iloc[:nrow_train, ], random_state=123, train_size=0.7)\n", "print(dtrain.shape)\n", "print(dvalid.shape)\n", "\n", "\n", "X_train = get_keras_data(dtrain)\n", "X_valid = get_keras_data(dvalid)\n", "X_test = get_keras_data(dtest)\n", "\n", "Y_train =  np.log1p(np.array(dtrain.price))\n", "Y_valid =  np.log1p(np.array(dvalid.price))"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "351c015d-e47c-4034-a057-0fa509106bd5", "_uuid": "536582c413553f5a7a1d26a840a2ee671222e27d"}}, {"source": ["**Model Part **"], "cell_type": "markdown", "metadata": {"_cell_guid": "9a7e6575-3373-49a8-819f-278667e41b14", "_uuid": "17275e93fdb0e5fd4749b09edf10e2b4d776c2f5"}}, {"outputs": [], "source": ["MAX_TEXT = np.max([np.max(merge.seq_name.max()), np.max(merge.seq_item_description.max())])+2\n", "MAX_general_cat = np.max([merge.general_cat.max()])+1\n", "MAX_subcat_1 = np.max([merge.subcat_1.max()])+1\n", "MAX_subcat_2 = np.max([merge.subcat_2.max()])+1\n", "MAX_BRAND = np.max([merge.brand_name.max()])+1\n", "\n", "print(MAX_TEXT)\n", "print(MAX_general_cat)\n", "print(MAX_subcat_1)\n", "print(MAX_subcat_2)\n", "print(MAX_BRAND)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "e8a01894-1bb3-4e92-a143-b6d3ee1b51e3", "_uuid": "a0d9e074a287ba583375ba29e6320eb789b58537"}}, {"outputs": [], "source": ["#KERAS MODEL DEFINITION\n", "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, BatchNormalization\n", "from keras.models import Model\n", "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n", "from keras import backend as K\n", "\n", "def get_callbacks(filepath, patience=2):\n", "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n", "    msave = ModelCheckpoint(filepath, save_best_only=True)\n", "    return [es, msave]\n", "\n", "def rmsle_cust(y_true, y_pred):\n", "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n", "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n", "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n", "\n", "def get_model():\n", "    #params\n", "    dr_r = 0.5\n", "    \n", "    #Inputs\n", "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n", "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n", "    brand_name = Input(shape=[1], name=\"brand_name\")\n", "    general_cat = Input(shape=[1], name=\"general_cat\")\n", "    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n", "    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n", "    item_condition = Input(shape=[1], name=\"item_condition\")\n", "    num_vars = Input(shape=[1], name=\"num_vars\")\n", "    \n", "    #Embeddings layers\n", "    emb_name = Embedding(MAX_TEXT, 10)(name)\n", "    emb_item_desc = Embedding(MAX_TEXT, 10)(item_desc)\n", "    emb_brand_name = Embedding(MAX_BRAND, 50)(brand_name)\n", "    emb_general_cat = Embedding(MAX_general_cat, 10)(general_cat)\n", "    emb_subcat_1 = Embedding(MAX_subcat_1, 20)(subcat_1)\n", "    emb_subcat_2 = Embedding(MAX_subcat_2, 30)(subcat_2)\n", " \n", "    #rnn layer\n", "    cnn_layer1 = Conv1D(filters=16, kernel_size=3, activation='relu') (emb_item_desc)\n", "    cnn_layer2 = Conv1D(filters=8, kernel_size=3, activation='relu')(emb_name)\n", "    \n", "    cnn_layer1 = GlobalMaxPooling1D()(cnn_layer1)\n", "    cnn_layer2 = GlobalMaxPooling1D()(cnn_layer2)\n", "    \n", "    #main layer\n", "    main_l = concatenate([\n", "        Flatten() (emb_brand_name)\n", "        , Flatten() (emb_general_cat)\n", "        , Flatten() (emb_subcat_1)\n", "        , Flatten() (emb_subcat_2)\n", "        , cnn_layer1\n", "        , cnn_layer2\n", "        , num_vars\n", "        , item_condition\n", "    ])\n", "    \n", "    main_l = Dropout(dr_r) (Dense(256, activation=\"relu\") (main_l))\n", "    main_l = Dropout(dr_r) (Dense(128, activation=\"relu\") (main_l))\n", "    main_l = Dropout(dr_r) (Dense(64, activation=\"relu\") (main_l))\n", "    \n", "    \n", "    #output\n", "    output = Dense(1, activation=\"linear\") (main_l)\n", "    \n", "    #model\n", "    model = Model([name, item_desc, brand_name, general_cat, subcat_1, subcat_2, item_condition, num_vars], output)\n", "    \n", "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n", "    \n", "    return model\n", "\n", "    \n", "model = get_model()\n", "model.summary()"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "84395024-9994-48cd-ba88-664f2972c141", "_uuid": "456bf6d0b1aebb88ffe0e6b015c89e7a8242a67e"}}, {"outputs": [], "source": ["#FITTING THE MODEL\n", "BATCH_SIZE = 20000\n", "epochs = 25\n", "\n", "model = get_model()\n", "model.fit(X_train, Y_train, epochs=epochs, batch_size=BATCH_SIZE\n", "          , validation_data=(X_valid, Y_valid)\n", "          , verbose=1)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "2e8cd21e-7f46-40e2-8ff6-cfc2b672fea6", "_uuid": "cc95cb40827349dfa143e8ea3380f13a00cbccbb"}}, {"outputs": [], "source": ["#CREATE PREDICTIONS\n", "preds = model.predict(X_test, batch_size=BATCH_SIZE)\n", "submission[\"price\"] = np.expm1(preds)\n", "submission.to_csv(\"./myNNsubmission.csv\", index=False)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "61faf29f-0476-4fda-83cd-7de7e432a1e7", "_uuid": "94f886da086d2bb8fd1200436ed9f50a61eef595"}}, {"outputs": [], "source": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "f36c110b-52d8-4e09-99dc-0519e4deb55e", "_uuid": "fd62c91274546039b1227d70b5da3ee6ab96c66e"}}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "pygments_lexer": "ipython3"}}}