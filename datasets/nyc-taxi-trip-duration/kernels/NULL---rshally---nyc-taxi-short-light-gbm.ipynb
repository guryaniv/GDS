{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1", "name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "9aef45736f1be9ee833b98cc0b8c4292db6c1adb", "_cell_guid": "d06a7963-3fb4-4622-bae1-db7d3387491c"}, "cell_type": "markdown", "source": ["This is a short code with the objective to better understand the main issues and produce a meaningfull score ( 0.3740 on LB - top 8%). I benefited from reading several of the more detailed kernels (eg beluga, and adapted some of the code) but wanted to have something easier using Light GBM.  "]}, {"metadata": {"_uuid": "dff9efb914a4485d481f2af33033cc893312342e", "_cell_guid": "d147a64a-ad7e-456c-aaec-5fb3014316b2"}, "outputs": [], "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import MiniBatchKMeans\n", "import lightgbm as lgb\n", "import gc\n", "\n", "myfolder = '../input/'\n", "print('loading files...')\n", "train = pd.read_csv(myfolder+'train.csv')\n", "test = pd.read_csv(myfolder+'test.csv')\n", "print(train.shape, test.shape)\n", "\n", "#remove outliers  (about an hour and half max to go to JFK )\n", "train = train[train['trip_duration'] <  train['trip_duration'].quantile(0.999)]\n", "train = train[train['trip_duration'] <= train['trip_duration'].mean() + 3*train['trip_duration'].std()]\n", "print(' max time in min {:.2f} and mean time in sec {:.2f} '.format(\n", "    train['trip_duration'].max()/60, train['trip_duration'].mean()))\n", "\n", "# NYC longitude and latitude borders \n", "ep = 0.0001      \n", "(lng1,lng2)=(-74.257*(1+ep), -73.699*(1-ep))\n", "(lat1,lat2)=(40.495*(1+ep), 40.915*(1-ep)) \n", "train = train[(train['pickup_longitude'] <=lng2)&(train['pickup_longitude'] >=lng1)]\n", "train = train[(train['pickup_latitude'] <=lat2) & (train['pickup_latitude'] >=lat1)]\n", "train = train[(train['dropoff_longitude'] <=lng2)&(train['dropoff_longitude'] >=lng1)]\n", "train = train[(train['dropoff_latitude'] <=lat2)&(train['dropoff_latitude'] >=lat1)]\n", "\n", "#  combine train and test into a single frame\n", "train['eval_set'] = 1\n", "test['eval_set'] = 2\n", "Data = pd.concat([train, test], axis=0)\n", "print(train.shape,test.shape,len(test)+len(train),Data.shape)\n", "\n", "del train, test\n", "gc.collect()\n", "Data.head(2)"], "execution_count": 20}, {"metadata": {"_uuid": "3784457e1c53d468badf5e8610d54e5feed606c7", "_cell_guid": "4bccbf3d-c0cb-4bd0-b678-df2d47bf7349"}, "outputs": [], "cell_type": "code", "source": ["# transform the time variables, combine hours and minutes and add holidays\n", "Data['log_trip_duration'] = np.log(Data['trip_duration'].values + 1)\n", "Data['pickup_datetime'] = pd.to_datetime(Data.pickup_datetime)\n", "Data['Month'] = Data['pickup_datetime'].dt.month\n", "Data['DayofMonth'] = Data['pickup_datetime'].dt.day\n", "Data['Timehm'] =Data['pickup_datetime'].dt.hour+Data['pickup_datetime'].dt.minute/60.\n", "Data['dayofweek'] = Data['pickup_datetime'].dt.dayofweek\n", "holidays = calendar().holidays(start='2015-12-31', end='2016-07-01')\n", "Data['Holiday']=Data['pickup_datetime'].dt.date.astype('datetime64[ns]').isin(holidays).astype(int)\n", "Data.drop(['trip_duration','dropoff_datetime','pickup_datetime'], axis=1,inplace=True)\n", "# and replace the N,Y with 0,1\n", "Data['store_and_fwd_flag'] = Data['store_and_fwd_flag'].map({'N': 0, 'Y':1}).astype(int)\n", "Data.shape"], "execution_count": 21}, {"metadata": {"_uuid": "71d2d547b1d8739b6256bdd32ac1a9b72390982b", "_cell_guid": "79374ca6-f2d6-47d2-ba93-de264028ec4f"}, "outputs": [], "cell_type": "code", "source": ["# calculate the distance and direction between pickup and dropooff coordinates\n", "\n", "def haversine_array(lat1, lng1, lat2, lng2):\n", "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n", "    d = np.sin(lat2/2-lat1/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng2/2-lng1/2)**2\n", "    return 2 * 6371 * np.arcsin(np.sqrt(d))   # 6,371 km is the earth radius\n", "\n", "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n", "    return haversine_array(lat1,lng1,lat1,lng2)+haversine_array(lat1,lng1,lat2,lng1)\n", "\n", "def bearing_array(lat1, lng1, lat2, lng2):\n", "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n", "    y = np.sin(lng2 - lng1) * np.cos(lat2)\n", "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng2 - lng1)\n", "    return np.degrees(np.arctan2(y, x))\n", "\n", "Data['distance_haversine'] = haversine_array(Data['pickup_latitude'].values, Data['pickup_longitude'].values, \n", "                                          Data['dropoff_latitude'].values, Data['dropoff_longitude'].values)\n", "Data['distance_dummy_manhattan'] =  dummy_manhattan_distance(Data['pickup_latitude'].values, \n", "            Data['pickup_longitude'].values, Data['dropoff_latitude'].values, Data['dropoff_longitude'].values)\n", "Data['direction'] = bearing_array(Data['pickup_latitude'].values, Data['pickup_longitude'].values, \n", "                                      Data['dropoff_latitude'].values, Data['dropoff_longitude'].values)\n", "Data.shape"], "execution_count": 22}, {"metadata": {"_uuid": "395a3708817a4f2220ce73486dfa0f955577b9a4", "_cell_guid": "5b5a89a2-c4cf-4218-a72b-a010589124e2"}, "outputs": [], "cell_type": "code", "source": ["#  replace latitudes and longitudes with 81 clusters\n", "\n", "coords = np.vstack((Data[['pickup_latitude', 'pickup_longitude']].values,\n", "                    Data[['dropoff_latitude', 'dropoff_longitude']].values))\n", "sample_ind = np.random.permutation(len(coords))[:1000000]\n", "kmeans = MiniBatchKMeans(n_clusters=9**2, batch_size=36**3).fit(coords[sample_ind])\n", "Data['pickup_cluster'] = kmeans.predict(Data[['pickup_latitude', 'pickup_longitude']])\n", "Data['dropoff_cluster'] = kmeans.predict(Data[['dropoff_latitude', 'dropoff_longitude']])\n", "Data.shape"], "execution_count": 23}, {"metadata": {"_uuid": "716ec3b187ef382904a58c79d58c602b03f6cc8b", "_cell_guid": "b20cfa98-24fe-434b-8469-be11fef01b97"}, "outputs": [], "cell_type": "code", "source": ["pca = PCA().fit(coords)\n", "Data['pickup_pca0'] = pca.transform(Data[['pickup_latitude', 'pickup_longitude']])[:, 0]\n", "Data['pickup_pca1'] = pca.transform(Data[['pickup_latitude', 'pickup_longitude']])[:, 1]\n", "Data['dropoff_pca0'] = pca.transform(Data[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n", "Data['dropoff_pca1'] = pca.transform(Data[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n", "Data['pca_manhattan'] = np.abs(Data['dropoff_pca1']-Data['pickup_pca1']) +  np.abs(\n", "                                 Data['dropoff_pca0']-Data['pickup_pca0'])\n", "Data['center_latitude'] = 0.5*Data['pickup_latitude']+0.5*Data['dropoff_latitude']\n", "Data['center_longitude'] = 0.5*Data['pickup_longitude']+0.5*Data['dropoff_longitude']\n", "del coords, sample_ind, kmeans\n", "Data.shape"], "execution_count": 24}, {"metadata": {"_uuid": "3ebac32fc205dadc11402bed65c458c7dc0f8b20", "_cell_guid": "adbeb79e-d538-4880-9e0a-a39db5751cda"}, "outputs": [], "cell_type": "code", "source": ["# optional external data (OSRM features) from Oscarleo\n", "# https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm/data\n", "\n", "flag=False\n", "if flag:\n", "    cols=['id', 'total_distance', 'total_travel_time',  'number_of_steps']\n", "    fr1 = pd.read_csv(myfolder+'fastest_routes_train_part_1.csv', usecols=cols)\n", "    fr2 = pd.read_csv(myfolder+'fastest_routes_train_part_2.csv', usecols=cols)\n", "    fr3 = pd.read_csv(myfolder+'fastest_routes_test.csv', usecols=cols)\n", "    tmp = pd.concat((fr1, fr2, fr3))\n", "    tmp.columns=['id', 'OSRM_distance', 'OSRM_time',  'OSRM_steps']\n", "    Data = Data.merge(tmp, how='left', on='id')\n", "    del fr1, fr2, fr3, tmp\n", "    gc.collect()\n", "    Data.shape\n", "print(flag)  "], "execution_count": 25}, {"metadata": {"_uuid": "c948e3ed6cc36583ed76ac90a0287ff9d67dffb0", "_cell_guid": "219ce763-54ab-46b7-8b91-94e5c3bb4148"}, "outputs": [], "cell_type": "code", "source": ["# split the data set back to train and test (for submission) and the train into \"train\" and \"eval\" sets for ML\n", "# save the ids for future\n", "\n", "Test_id = Data['id'].loc[Data['eval_set']==2].to_frame()\n", "Data.drop('id',axis = 1, inplace=True)\n", "train = Data[Data['eval_set'] == 1]\n", "test = Data[Data['eval_set'] == 2]\n", "print(Data.shape, train.shape, test.shape)\n", "\n", "X=train.drop(['eval_set','log_trip_duration'],axis=1)\n", "y=train['log_trip_duration']\n", "features=list(X.columns)\n", "cfeatures = list(X.select_dtypes(include = ['int64','int32']).columns)\n", "\n", "X_train, X_eval, y_train, y_eval = train_test_split(X,y, test_size=0.2, random_state=2)\n", "\n", "del train, Data\n", "gc.collect()"], "execution_count": 26}, {"metadata": {"_uuid": "40acde3b9bd66426efdea2dde3a789be06c92ee2", "_cell_guid": "5711dfa3-85f0-4ebd-85e3-1ff3a2ecb415"}, "outputs": [], "cell_type": "code", "source": ["print('formatting and training LightGBM regression ...')\n", "# use higher num_boost_round eg 1000\n", "\n", "lgb_train = lgb.Dataset(X_train.values, y_train.values)\n", "lgb_eval = lgb.Dataset(X_eval.values, y_eval.values, reference = lgb_train)\n", "params = {'metric': 'rmse', 'learning_rate' : 0.05, 'num_leaves': 512, \n", "         'feature_fraction': 0.9,'bagging_fraction':0.9,'bagging_freq':5,'min_data_in_leaf': 500}\n", "lgb_model = lgb.train(params, lgb_train, num_boost_round = 200, valid_sets = lgb_eval, \n", "             feature_name=features, early_stopping_rounds=10,  verbose_eval = 10)\n", "\n", "del lgb_train\n", "gc.collect()"], "execution_count": 27}, {"metadata": {"_uuid": "1f5a7536f0fb0d8ecb922a960bc1a786018fdf28", "_cell_guid": "25786ef4-ab5d-41c0-b6f7-deb195a71d30"}, "outputs": [], "cell_type": "code", "source": ["#check feature importance\n", "lgb.plot_importance(lgb_model,  max_num_features=28, figsize=(7,9))\n", "plt.show()"], "execution_count": 28}, {"metadata": {"_uuid": "f9094f538b34e8f57b5ad74c84fe548376a979cc", "_cell_guid": "5167055d-b93f-424b-b88a-92399c9b9966"}, "outputs": [], "cell_type": "code", "source": ["pred = lgb_model.predict(test.drop(['eval_set','log_trip_duration'], axis=1).values, \n", "                         num_iteration = lgb_model.best_iteration)\n", "Test_id['trip_duration']=np.maximum(0,np.exp(pred) - 1)\n", "Test_id.to_csv('submission.csv', index=False)\n", "print(Test_id['trip_duration'].mean())           # just a sanity check - it should be around 806"], "execution_count": 29}, {"metadata": {}, "outputs": [], "cell_type": "code", "source": ["#check the score again\n", "pred1 = lgb_model.predict(X_train.values, num_iteration = lgb_model.best_iteration)\n", "pred2 = lgb_model.predict(X_eval.values, num_iteration = lgb_model.best_iteration)\n", "rmsle1= (((y_train-pred1)**2).mean())**0.5\n", "rmsle2 = (((y_eval-pred2)**2).mean())**0.5\n", "print('train score: {:.4f}   eval score: {:.4f}'.format(rmsle1,rmsle2))"], "execution_count": 30}, {"metadata": {"_uuid": "ccb688a7c9a4b3be3bff8e5c9a7a29c6c42e441e", "_cell_guid": "03166789-b79d-4ae7-b1ce-bfb2fd648b18"}, "cell_type": "markdown", "source": []}, {"metadata": {"_uuid": "a2ef61bca36d9c5854714a1639df1adeafe93735", "_cell_guid": "551e6a3a-a3b8-4ba1-8d4e-8c2ac39cfc08"}, "cell_type": "markdown", "source": []}], "nbformat": 4}