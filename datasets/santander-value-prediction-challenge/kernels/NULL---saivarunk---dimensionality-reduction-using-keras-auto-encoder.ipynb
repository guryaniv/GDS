{"cells":[{"metadata":{"_uuid":"8b10f2c0116060b917de9f4c6229bc18f72358dd"},"cell_type":"markdown","source":"# Dimensionality reduction using Keras Auto Encoder\n\n* Prepare Data\n* Design Auto Encoder\n* Train Auto Encoder\n* Use Encoder level from Auto Encoder\n* Use Encoder to obtain reduced dimensionality data for train and test sets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom numpy.random import seed\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39b71a439c6eb7b9f139805ec07e572f3dce0b03"},"cell_type":"markdown","source":"## Read train and test data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0c7ce7f80d13a05ac8668306118476376f541e1"},"cell_type":"markdown","source":"## Dropping Target and ID's from train and test"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ed1dd2d2420ed09ecbd22681d638fcd8b594a7e6"},"cell_type":"code","source":"target = train['target']\ntrain_id = train['ID']\ntest_id = test['ID']\n\ntrain.drop(['target'], axis=1, inplace=True)\ntrain.drop(['ID'], axis=1, inplace=True)\ntest.drop(['ID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af87534bb5a7cb9f2785de8522ea6fdb9e605aa4","collapsed":true},"cell_type":"code","source":"print('Train data shape', train.shape)\nprint('Test data shape', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2f9e31651ff65cb57d2c1c261b41f03d216508c"},"cell_type":"markdown","source":"### Scaling Train and Test data for Neural Net"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"04df668adc11e9bb8bb6c232b80ca5da6aeb8d4f"},"cell_type":"code","source":"train_scaled = minmax_scale(train, axis = 0)\ntest_scaled = minmax_scale(test, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b32832f691025169f8bb7f5e1915d4fa803a17ef"},"cell_type":"markdown","source":"\n## Design Auto Encoder"},{"metadata":{"_uuid":"75c72115094ded153c6a88ca9f9217e01f29fff4"},"cell_type":"markdown","source":"Auto Encoders are is a type of artificial neural network used to learn efficient data patterns in an unsupervised manner. An Auto Encoder ideally consists of an encoder and decoder. \n\nThe Neural Network is designed compress data using the Encoding level. The Decoder will try to uncompress the data to the original dimension.\n\nTo achieve this, the Neural net is trained using the Training data as the training features as well as target.\n\n```\n# Training a Typical Neural Net\nmodel.fit(X_train, y_train)\n\n# Training a Auto Encoder\nmodel.fit(X_train, X_train)\n```\n\nThese are typically used for dimensionality reduction use cases where there are more number of features.\n\n![](https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png)\n\nBy Chervinskii [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e9495a24eb7cee97996133a29981b1c8c5a6a639"},"cell_type":"code","source":"# define the number of features\nncol = train_scaled.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d81ca355c80d85af2e877afa6458c59eec2cfca"},"cell_type":"markdown","source":"### Split train data into train and validation 80:20 in ratio"},{"metadata":{"trusted":true,"_uuid":"9f4ab18558e8f0625a34d0131be2ee9e8827f82b","collapsed":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(train_scaled, target, train_size = 0.9, random_state = seed(2017))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"931d8651580f7059a5f9ba0aee15340168dba85a"},"cell_type":"code","source":"### Define the encoder dimension\nencoding_dim = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4febb56ae76546b78ea0eb1920fe9c6fe606248c","collapsed":true},"cell_type":"code","source":"input_dim = Input(shape = (ncol, ))\n\n# Encoder Layers\nencoded1 = Dense(3000, activation = 'relu')(input_dim)\nencoded2 = Dense(2750, activation = 'relu')(encoded1)\nencoded3 = Dense(2500, activation = 'relu')(encoded2)\nencoded4 = Dense(2250, activation = 'relu')(encoded3)\nencoded5 = Dense(2000, activation = 'relu')(encoded4)\nencoded6 = Dense(1750, activation = 'relu')(encoded5)\nencoded7 = Dense(1500, activation = 'relu')(encoded6)\nencoded8 = Dense(1250, activation = 'relu')(encoded7)\nencoded9 = Dense(1000, activation = 'relu')(encoded8)\nencoded10 = Dense(750, activation = 'relu')(encoded9)\nencoded11 = Dense(500, activation = 'relu')(encoded10)\nencoded12 = Dense(250, activation = 'relu')(encoded11)\nencoded13 = Dense(encoding_dim, activation = 'relu')(encoded12)\n\n# Decoder Layers\ndecoded1 = Dense(250, activation = 'relu')(encoded13)\ndecoded2 = Dense(500, activation = 'relu')(decoded1)\ndecoded3 = Dense(750, activation = 'relu')(decoded2)\ndecoded4 = Dense(1000, activation = 'relu')(decoded3)\ndecoded5 = Dense(1250, activation = 'relu')(decoded4)\ndecoded6 = Dense(1500, activation = 'relu')(decoded5)\ndecoded7 = Dense(1750, activation = 'relu')(decoded6)\ndecoded8 = Dense(2000, activation = 'relu')(decoded7)\ndecoded9 = Dense(2250, activation = 'relu')(decoded8)\ndecoded10 = Dense(2500, activation = 'relu')(decoded9)\ndecoded11 = Dense(2750, activation = 'relu')(decoded10)\ndecoded12 = Dense(3000, activation = 'relu')(decoded11)\ndecoded13 = Dense(ncol, activation = 'sigmoid')(decoded12)\n\n# Combine Encoder and Deocder layers\nautoencoder = Model(inputs = input_dim, outputs = decoded13)\n\n# Compile the Model\nautoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9794eae1505676184670ed4a903ff3f0de8d82ba","collapsed":true},"cell_type":"code","source":"autoencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32e795da4f94a666b3c33303d05db0cf615805e0"},"cell_type":"markdown","source":"### Train Auto Encoder"},{"metadata":{"trusted":true,"_uuid":"5f6f4b5d56d60b543a74678e68c75c8b59b00a90","collapsed":true},"cell_type":"code","source":"autoencoder.fit(X_train, X_train, nb_epoch = 10, batch_size = 32, shuffle = False, validation_data = (X_test, X_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6dd888a05832e3c4e33f0f332a63627b978e66f"},"cell_type":"markdown","source":"## Use Encoder level to reduce dimension of train and test data"},{"metadata":{"trusted":true,"_uuid":"860f008c14de651c65b10c6665cb9a0609625754","collapsed":true},"cell_type":"code","source":"encoder = Model(inputs = input_dim, outputs = encoded13)\nencoded_input = Input(shape = (encoding_dim, ))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cce6bab9aa6d1ac22f2f21f43c95bec0d77650ca"},"cell_type":"markdown","source":"### Predict the new train and test data using Encoder"},{"metadata":{"trusted":true,"_uuid":"cdcc8f55565636f7b1d4a36129ffe736ae206073","collapsed":true},"cell_type":"code","source":"encoded_train = pd.DataFrame(encoder.predict(train_scaled))\nencoded_train = encoded_train.add_prefix('feature_')\n\nencoded_test = pd.DataFrame(encoder.predict(test_scaled))\nencoded_test = encoded_test.add_prefix('feature_')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eaaf050d32c810c5aa49ba39bffa60855a2c73bd"},"cell_type":"markdown","source":"### Add target to train"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"129715b8068fff1304b517a94bd767bc6aa78a4b"},"cell_type":"code","source":"encoded_train['target'] = target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47ee18e322ed028ff1f3b455d75a1dc5d856e87d","collapsed":true},"cell_type":"code","source":"print(encoded_train.shape)\nencoded_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db0d9e0a47235655f6461e16650e1a9350f37d11","collapsed":true},"cell_type":"code","source":"print(encoded_test.shape)\nencoded_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8eb8bed8d7b992e0e66b2fa57e3a6ddfe5f8a4b7"},"cell_type":"code","source":"encoded_train.to_csv('train_encoded.csv', index=False)\nencoded_test.to_csv('test_encoded.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38fcc5703be207816a283ff24f238f3bc387cc58"},"cell_type":"markdown","source":"## I have created a kernel to Build a baseline model using LightGBM for these encoded features. You can find it here :\n\n[https://www.kaggle.com/saivarunk/lgb-baseline-using-encoded-features-auto-encoder](https://www.kaggle.com/saivarunk/lgb-baseline-using-encoded-features-auto-encoder)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}