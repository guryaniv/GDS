{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# A Beginner's Guide - CNN with Keras\n\nKaiming Kuang\n\nThis is a beginner's guide of the Digit Recognizer competition. Some basic knowledges about the theory and practice of deep learning is still required. Here are some prerequisite readings on Convolutional Neural Network:\n- [Wikipedia of CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n- [A Beginner's Guide To Understanding Convolutional Neural Networks by Adit Deshpande](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)\n\nThere is also a course on Coursera. I learned most of the basics from this course of Andrew Ng:\n- [Coursera: Convolutional Neural Network by Andrew Ng](https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning)\n\n## Content\n- 1 Introduction\n- 2 Data Exploration, Augmentation and Preparation\n    - 2.1 Data Exploration\n    - 2.2 Data Augmentation\n        - 2.2.1 Zoom In\n        - 2.2.2 Translation\n        - 2.2.3 Add White Noise\n        - 2.2.4 Rotation\n    - 2.3 Data Preparation\n- 3 CNN Structure\n- 4 Training and Evaluation\n    - 4.1 Train the Model\n    - 4.2 Evaluate the Model\n- 5 Output the Prediction"},{"metadata":{"_uuid":"77b4018221edcb108291036046c4dab4db9cad7a"},"cell_type":"markdown","source":"# 1 Introduction\n\nThe Digit Recognizer competition uses the famous MNIST hand-written number dataset, which is the hello-world dataset for computer vision. Here we are required to identify numbers from images. The training data contains 42,000 hand-written number images, each one of which is 28 pixels in height and 28 pixels in width. Simple models such as KNN or MLP are not as capable of this task as Convolutional Neural Network. Here I used Keras to build my CNN, for that it is more friendly to beginners than TensorFlow and these two are the only two DL frameworks that I am familiar with. Please do turn on the GPU button when running this kernel because it is extremely time-consuming to run CNN on CPU. It usually took me 50+ hours to run this code for 100 epochs on my laptop. With the Tesla K80 GPU, 100 epochs took only a little more than 1 hour. With early stopping, sometimes it only took around 15 mins to run"},{"metadata":{"_uuid":"720e25c4fe555eda29197e3384aca03ab235dc89"},"cell_type":"markdown","source":"First we should import all the libraries we need in this kernel:\n- gc: The built-in garbage collection of Python. We need to delete some variables and collect spaces when necessary to save RAM.\n- random: The built-in package of Python. We need it to generate random numbers.\n- time: The built-in package of Python. Use it to check running time.\n- pi: In the data augmentation part we use pi to rotate the image.\n- keras: We need Keras to build our CNN model. It uses TensorFlow as backend. [Documentation of Keras](https://keras.io/).\n- matplotlib.pyplot: We use pyplot to plot the hand-written number image.\n- numpy: We need Numpy to do all the matrix manipulation. [Documentation of Numpy](https://docs.scipy.org/doc/numpy/reference/).\n- pandas: We use Pandas to manipulate data, such as loading and outputing .csv files. [Documentation of Pandas](http://pandas.pydata.org/pandas-docs/stable/).\n- tensorflow: TensorFlow is a popular deep learning framework. We use TensorFlow for the data augmentation part. [Documentation of TensorFlow](https://tensorflow.google.cn/api_docs/python/tf).\n- ReduceLROnPlateau: This is the model we use to set up a learning rate decay.\n- BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPool2D: These are some basic building blocks we need to set up CNN.\n- Image: This package is used for image display.\n- train_test_split: We use this module of sklearn to split the data into trainning and validation part."},{"metadata":{"trusted":true,"_uuid":"dda79db8f6e355b6561a943c851c318041838c7b"},"cell_type":"code","source":"import gc\nimport random as rd\nimport time\nfrom math import pi\n\nimport keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import (BatchNormalization, Conv2D, Dense, Dropout, Flatten,\n                          MaxPool2D, ReLU)\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88dbdff9a122d1b2b66316bdf6f45e292bbddc81"},"cell_type":"markdown","source":"# 2 Data Exploration, Augmentation and Preparation"},{"metadata":{"_uuid":"563f578f1b84004006e234aacc20c530b39b3db9"},"cell_type":"markdown","source":"## 2.1 Data Exploration\n\nFirst load the data."},{"metadata":{"trusted":true,"_uuid":"9e6d934d5a2dcea87d3a469c61281a13521a3ee2"},"cell_type":"code","source":"print(\"Loading...\")\ndata_train = pd.read_csv(\"../input/train.csv\")\ndata_test = pd.read_csv(\"../input/test.csv\")\nprint(\"Done!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"178edde6135608264cf3eedb3b4b795156a30921"},"cell_type":"code","source":"print(\"Training data: {} rows, {} columns.\".format(data_train.shape[0], data_train.shape[1]))\nprint(\"Test data: {} rows, {} columns.\".format(data_test.shape[0], data_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b78238d701f90d3befc50182b98b5f52271de25"},"cell_type":"markdown","source":"There are 42,000 rows in the training data and 28,000 rows in the test data. Each row of the training set contains the image (28x28=784) and the label in the first column. The test data doesn't have the labels."},{"metadata":{"trusted":true,"_uuid":"232263fc2848513b23a3797ea6598920324781a1"},"cell_type":"code","source":"x_train = data_train.values[:, 1:]\ny_train = data_train.values[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42eff0b0395882e75df30c2f55ba2b66f42aba0b"},"cell_type":"markdown","source":"Now let's see how the numbers look like. We will use the convert_2d function to convert the 1d data into two dimensions."},{"metadata":{"trusted":true,"_uuid":"0436d9e911e6dbec55b7b4e468fb5bb6f8100dee"},"cell_type":"code","source":"def convert_2d(x):\n    \"\"\"x: 2d numpy array. m*n data image.\n       return a 3d image data. m * height * width * channel.\"\"\"\n    if len(x.shape) == 1:\n        m = 1\n        height = width = int(np.sqrt(x.shape[0]))\n    else:\n        m = x.shape[0]\n        height = width = int(np.sqrt(x.shape[1]))\n\n    x_2d = np.reshape(x, (m, height, width, 1))\n    \n    return x_2d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ba8377469976c4cf80fb87e607a580683c80a1f"},"cell_type":"code","source":"x_display = convert_2d(data_train.values[0, 1:])\nplt.imshow(x_display.squeeze())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8584b4ce460b5e1a59bdec43806bf82f56a40fa3"},"cell_type":"markdown","source":"## 2.2 Data Augmentation\n\nHere we delve straight into data augmentation. Data augmentation is a useful technique when you don't have enough data or would like to expand your data to improve the performance. In this competition, data augmentation basically means cutting, rotating and zooming the image without hurting its identifiability. Here I used zooming, translation, white noise and rotation. With data augmentation, you can expect a 1-2% accuracy improvement."},{"metadata":{"_uuid":"429752ae417501e9af2bef9ede4800882b78be0e"},"cell_type":"markdown","source":"### 2.2.1 Zoom In\nHere we use crop_image function to crop a part of the image around the center, resize it and save it as augmented data."},{"metadata":{"trusted":true,"_uuid":"403f686fbdf11dc0f40060b113c0e297383cd361"},"cell_type":"code","source":"def crop_image(x, y, min_scale):\n    \"\"\"x: 2d(m*n) numpy array. 1-dimension image data;\n       y: 1d numpy array. The ground truth label;\n       min_scale: float. The minimum scale for cropping.\n       return zoomed images.\n       This function crops the image, enlarges the cropped part and uses it as augmented data.\"\"\"\n    # convert the data to 2-d image. images should be a m*h*w*c numpy array.\n    images = convert_2d(x)\n    # m is the number of images. Since this is a gray-scale image scale from 0 to 255, it only has one channel.\n    m, height, width, channel = images.shape\n    \n    # tf tensor for original images\n    img_tensor = tf.placeholder(tf.int32, [1, height, width, channel])\n    # tf tensor for 4 coordinates for corners of the cropped image\n    box_tensor = tf.placeholder(tf.float32, [1, 4])\n    box_idx = [0]\n    crop_size = np.array([height, width])\n    # crop and resize the image tensor\n    cropped_img_tensor = tf.image.crop_and_resize(img_tensor, box_tensor, box_idx, crop_size)\n    # numpy array for the cropped image\n    cropped_img = np.zeros((m, height, width, 1))\n\n    with tf.Session() as sess:\n\n        for i in range(m):\n            \n            # randomly select a scale between [min_scale, min(min_scale + 0.05, 1)]\n            rand_scale = np.random.randint(min_scale * 100, np.minimum(min_scale * 100 + 5, 100)) / 100\n            # calculate the 4 coordinates\n            x1 = y1 = 0.5 - 0.5 * rand_scale\n            x2 = y2 = 0.5 + 0.5 * rand_scale\n            # lay down the cropping area\n            box = np.reshape(np.array([y1, x1, y2, x2]), (1, 4))\n            # save the cropped image\n            cropped_img[i:i + 1, :, :, :] = sess.run(cropped_img_tensor, feed_dict={img_tensor: images[i:i + 1], box_tensor: box})\n    \n    # flat the 2d image\n    cropped_img = np.reshape(cropped_img, (m, -1))\n    cropped_img = np.concatenate((y.reshape((-1, 1)), cropped_img), axis=1).astype(int)\n\n    return cropped_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a8113c43c6e9efdb05cd3947d7753da63151bbf"},"cell_type":"markdown","source":"### 2.2.2 Translation\nNow we shift the image to 4 different directions. "},{"metadata":{"trusted":true,"_uuid":"a778837271e560bce70ab223ec447656dec17c27"},"cell_type":"code","source":"def translate(x, y, dist):\n    \"\"\"x: 2d(m*n) numpy array. 1-dimension image data;\n       y: 1d numpy array. The ground truth label;\n       dist: float. Percentage of height/width to shift.\n       return translated images.\n       This function shift the image to 4 different directions.\n       Crop a part of the image, shift it and fill the left part with 0.\"\"\"\n    # convert the 1d image data to a m*h*w*c array\n    images = convert_2d(x)\n    m, height, width, channel = images.shape\n    \n    # set 4 groups of anchors. The first 4 int in a certain group lay down the area we crop.\n    # The last 4 sets the area to be moved to. E.g.,\n    # new_img[new_top:new_bottom, new_left:new_right] = img[top:bottom, left:right]\n    anchors = []\n    anchors.append((0, height, int(dist * width), width, 0, height, 0, width - int(dist * width)))\n    anchors.append((0, height, 0, width - int(dist * width), 0, height, int(dist * width), width))\n    anchors.append((int(dist * height), height, 0, width, 0, height - int(dist * height), 0, width))\n    anchors.append((0, height - int(dist * height), 0, width, int(dist * height), height, 0, width))\n    \n    # new_images: d*m*h*w*c array. The first dimension is the 4 directions.\n    new_images = np.zeros((4, m, height, width, channel))\n    for i in range(4):\n        # shift the image\n        top, bottom, left, right, new_top, new_bottom, new_left, new_right = anchors[i]\n        new_images[i, :, new_top:new_bottom, new_left:new_right, :] = images[:, top:bottom, left:right, :]\n    \n    new_images = np.reshape(new_images, (4 * m, -1))\n    y = np.tile(y, (4, 1)).reshape((-1, 1))\n    new_images = np.concatenate((y, new_images), axis=1).astype(int)\n\n    return new_images","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0234d6034038bb742fc61ca75100854a35b553ff"},"cell_type":"markdown","source":"### 2.2.3 Add White Noise\nNow we add some white noise to the image. We randomly choose some pixels and replace them with uniformly-distributed noise."},{"metadata":{"trusted":true,"_uuid":"f56ef02d966f42e29994669845f317a478c30ebf"},"cell_type":"code","source":"def add_noise(x, y, noise_lvl):\n    \"\"\"x: 2d(m*n) numpy array. 1-dimension image data;\n       y: 1d numpy array. The ground truth label;\n       noise_lvl: float. Percentage of pixels to add noise in.\n       return images with white noise.\n       This function randomly picks some pixels and replace them with noise.\"\"\"\n    m, n = x.shape\n    # calculate the # of pixels to add noise in\n    noise_num = int(noise_lvl * n)\n\n    for i in range(m):\n        # generate n random numbers, sort it and choose the first noise_num indices\n        # which equals to generate random numbers w/o replacement\n        noise_idx = np.random.randint(0, n, n).argsort()[:noise_num]\n        # replace the chosen pixels with noise from 0 to 255\n        x[i, noise_idx] = np.random.randint(0, 255, noise_num)\n\n    noisy_data = np.concatenate((y.reshape((-1, 1)), x), axis=1).astype(\"int\")\n\n    return noisy_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"631578051cf6c6cfd8fd3603f5b7983aa480e0d4"},"cell_type":"markdown","source":"### 2.2.4 Rotation\nNow we rotate the image."},{"metadata":{"trusted":true,"_uuid":"97f2aecd205a644a10b130e655beb0b420221e2a"},"cell_type":"code","source":"def rotate_image(x, y, max_angle):\n    \"\"\"x: 2d(m*n) numpy array. 1-dimension image data;\n       y: 1d numpy array. The ground truth label;\n       max_angle: int. The maximum degree for rotation.\n       return rotated images.\n       This function rotates the image for some random degrees(0.5 to 1 * max_angle degree).\"\"\"\n    images = convert_2d(x)\n    m, height, width, channel = images.shape\n    \n    img_tensor = tf.placeholder(tf.float32, [m, height, width, channel])\n    \n    # half of the images are rotated clockwise. The other half counter-clockwise\n    # positive angle: [max/2, max]\n    # negative angle: [360-max/2, 360-max]\n    rand_angle_pos = np.random.randint(max_angle / 2, max_angle, int(m / 2))\n    rand_angle_neg = np.random.randint(-max_angle, -max_angle / 2, m - int(m / 2)) + 360\n    rand_angle = np.transpose(np.hstack((rand_angle_pos, rand_angle_neg)))\n    np.random.shuffle(rand_angle)\n    # convert the degree to radian\n    rand_angle = rand_angle / 180 * pi\n    \n    # rotate the images\n    rotated_img_tensor = tf.contrib.image.rotate(img_tensor, rand_angle)\n\n    with tf.Session() as sess:\n        rotated_imgs = sess.run(rotated_img_tensor, feed_dict={img_tensor: images})\n    \n    rotated_imgs = np.reshape(rotated_imgs, (m, -1))\n    rotated_imgs = np.concatenate((y.reshape((-1, 1)), rotated_imgs), axis=1)\n    \n    return rotated_imgs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1fd8f3ad5c8e158004a5a982121dcf73ac74cf3"},"cell_type":"markdown","source":"Now we put them all together."},{"metadata":{"trusted":true,"_uuid":"6a55d63c2c3620051c131e8b2f62f3776a5666f0"},"cell_type":"code","source":"start = time.clock()\nprint(\"Augment the data...\")\ncropped_imgs = crop_image(x_train, y_train, 0.9)\ntranslated_imgs = translate(x_train, y_train, 0.1)\nnoisy_imgs = add_noise(x_train, y_train, 0.1)\nrotated_imgs = rotate_image(x_train, y_train, 10)\n\ndata_train = np.vstack((data_train, cropped_imgs, translated_imgs, noisy_imgs, rotated_imgs))\nnp.random.shuffle(data_train)\nprint(\"Done!\")\ntime_used = int(time.clock() - start)\nprint(\"Time used: {}s.\".format(time_used))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"720a68c86d68ef2b69550f839b6a040912676e25"},"cell_type":"markdown","source":"## 2.3 Data Preparation"},{"metadata":{"_uuid":"b5bb8834d71d5c7f1677ab5f8739d47aedefdc7a"},"cell_type":"markdown","source":"Let's check the augmented data."},{"metadata":{"trusted":true,"_uuid":"c8b06a119e8b27a05f813d1136a5c745ad429e7b"},"cell_type":"code","source":"x_train = data_train[:, 1:]\ny_train = data_train[:, 0]\nx_test = data_test.values\nprint(\"Augmented training data: {} rows, {} columns.\".format(data_train.shape[0], data_train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d5fb2e7a255982c3a6d63aa652282b7da71cf31"},"cell_type":"markdown","source":"Now we need to convert the 1d image data to 2-dimension."},{"metadata":{"trusted":true,"_uuid":"bd219c912d3913f50c356a537822f431b254e581"},"cell_type":"code","source":"x_train = convert_2d(x_train)\nx_test = convert_2d(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d7ceb7cf8bbf4c01d675bd39758846e04da3c2e"},"cell_type":"markdown","source":"Also, we need the label variable to be a dummy variable, which only contains 1 and 0. We will use a Keras utility function to do the conversion."},{"metadata":{"trusted":true,"_uuid":"b9cce043ae8fed7aefa20752d32c9e37f35570ed"},"cell_type":"code","source":"num_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffab1aefeed9d60198ebbf39b83fa1ead5fc264b"},"cell_type":"markdown","source":"The values in the image range from 0 to 255. It would be easier for CNN to converge if we scale down these values. Thus, we divide all pixels by 255."},{"metadata":{"trusted":true,"_uuid":"94227c3a68445953da591f560acd8927b13b47cf"},"cell_type":"code","source":"x_train = x_train / 255\nx_test = x_test / 255","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94376c63911b9bcaadac83ce6d1f9b5651c06f54"},"cell_type":"markdown","source":"Now we split the dataset into the training set and the developing(validation) set."},{"metadata":{"trusted":true,"_uuid":"eaeba902c68fd419920c2313bb4df6622232b78d"},"cell_type":"code","source":"# generate a random seed for train-test-split\nseed = np.random.randint(1, 100)\nx_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.1, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18ee33122dbb59824519b949bfca5a0db4e6a4c5"},"cell_type":"markdown","source":"Delete the original data_train to save some RAM."},{"metadata":{"trusted":true,"_uuid":"1c6cd3966834e83cc40145caddda8c7805777400"},"cell_type":"code","source":"del data_train\ndel data_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c4b9dd93326f4d1f516d69fa6e290eca9296170"},"cell_type":"markdown","source":"# 3 CNN Structure\n\nA normal CNN usually consists of 3 types of layers, convolutional layers, pooling layers and fully-connected layers. I also added normalization layers and dropout layers into my model. Here is how I set up the CNN structure."},{"metadata":{"trusted":true,"_uuid":"c6017ccf1224113638e90e64cb586fc64211cf44"},"cell_type":"code","source":"# number of channels for each of the 4 convolutional layers. \nfilters = (32, 32, 64, 64)\n# I use a 5x5 kernel for every conv layer\nkernel = (5, 5)\n# the drop probability of the dropout layer\ndrop_prob = 0.2\n\nmodel = keras.models.Sequential()\n\nmodel.add(Conv2D(filters[0], kernel, padding=\"same\", input_shape=(28, 28, 1),\n                 kernel_initializer=keras.initializers.he_normal()))\nmodel.add(BatchNormalization())\nmodel.add(ReLU())\nmodel.add(Conv2D(filters[0], kernel, padding=\"same\",\n                 kernel_initializer=keras.initializers.he_normal()))\nmodel.add(BatchNormalization())\nmodel.add(ReLU())\nmodel.add(MaxPool2D())\nmodel.add(Dropout(drop_prob))\n\nmodel.add(Conv2D(filters[1], kernel, padding=\"same\",\n                 kernel_initializer=keras.initializers.he_normal()))\nmodel.add(BatchNormalization())\nmodel.add(ReLU())\nmodel.add(MaxPool2D())\nmodel.add(Dropout(drop_prob))\n\nmodel.add(Conv2D(filters[2], kernel, padding=\"same\",\n                 kernel_initializer=keras.initializers.he_normal()))\nmodel.add(BatchNormalization())\nmodel.add(ReLU())\nmodel.add(MaxPool2D())\nmodel.add(Dropout(drop_prob))\n\nmodel.add(Conv2D(filters[3], kernel, padding=\"same\",\n                 kernel_initializer=keras.initializers.he_normal()))\nmodel.add(BatchNormalization())\nmodel.add(ReLU())\nmodel.add(MaxPool2D())\nmodel.add(Dropout(drop_prob))\n\n# several fully-connected layers after the conv layers\nmodel.add(Flatten())\nmodel.add(Dropout(drop_prob))\nmodel.add(Dense(128, activation=\"relu\"))\nmodel.add(Dropout(drop_prob))\nmodel.add(Dense(num_classes, activation=\"softmax\"))\n# use the Adam optimizer to accelerate convergence\nmodel.compile(keras.optimizers.Adam(), \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"722ea0e4b640c0da79e2a35d8fada53ef2a75e45"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6e9382b46e1d1eb3cb2d81dafe3e7bb1c7679f7"},"cell_type":"markdown","source":"The list above is the structure of my CNN model. It goes:\n- (Conv-ReLU-BatchNormalization-MaxPooling-Dropout) x 4;\n- 3 fully-connected(dense) layers with 1 dropout layer. Dense(64)-Dense(128)-Dropout-Dense(with softmax activation).\n\n- In CNN people often use 3x3 or 5x5 kernel. I found that with a 5x5 kernel, the model's accuracy improved about 0.125%, which is quite a lot when you pass 99% threshold.\n- Convolutional layers and max pooling layers can extract some high-level traits from the pixels. With the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) unit the and max pooling, we also add non-linearity into the network;\n- Batch normalization helps the network converge faster since it keeps the input of every layer at the same scale;\n- [Dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout) layers help us prevent overfitting by randomly drop some of the input units. With dropout our model won't overfit to some specific extreme data or some noisy pixels;\n- The [Adam optimizer](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) also accelerates the optimization. Usually when the dataset is too large, we use mini-batch gradient descent or stochastic gradient descent to save some training time. The randomness in MBGD or SGD means that the steps towards the optimum are zig-zag rather than straight forward. Adam, or Adaptive Moment Estimation, uses exponential moving average on the gradients and the secend moment of gradients to make the steps straight and in turn accelerate the optimization."},{"metadata":{"_uuid":"06d716aea4d956147e9a4df68be2de560e653119"},"cell_type":"markdown","source":"# 4 Training and Evaluation"},{"metadata":{"_uuid":"ea52c1bbb75f1513c04406851bab96408f819418"},"cell_type":"markdown","source":"## 4.1 Train the Model\nNow we need to train our model. First let's set some basic hyperparameters for training."},{"metadata":{"trusted":true,"_uuid":"79eee75b4ddd331896ce9f8517d091ee5511b41a"},"cell_type":"code","source":"# number of epochs we run\niters = 100\n# batch size. Number of images we train before we take one step in MBGD.\nbatch_size = 1024","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b888111221a6a8957f13be2a0b70331fff67fb5c"},"cell_type":"markdown","source":"In Andrew Ng's deep learning course, he mentioned that it would be better to set the batch size to the power of 2 due to some reasons regarding hardware or TensorFlow underlying code. Not so sure about that."},{"metadata":{"_uuid":"c16aa2e5ee93a2017a0ddd692846916390647687"},"cell_type":"markdown","source":"When we reach close to the optimum, we need to lower our learning rate to prevent overshooting. Large learning rate would keep us away from the optimum. Thus, I set this learning rate decay to decrease it when the accuracy on the validation data no longer improves."},{"metadata":{"trusted":true,"_uuid":"91d6ee832a736bec8bded76df0493eb35203823d"},"cell_type":"code","source":"# monitor: the quantity to be monitored. When it no longer improves significantly, we lower the learning rate\n# factor: new learning rate = old learning rate * factor\n# patience: number of epochs we wait before we decrease the learning rate\n# verbose: whether or not the message are displayed\n# min_lr: the minimum learning rate\nlr_decay = ReduceLROnPlateau(monitor=\"val_acc\", factor=0.5, patience=3, verbose=1, min_lr=1e-5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d794f38d53d8754cde06b25d740eeccfc91a106"},"cell_type":"markdown","source":"If our model are not getting any better on the validation data, we can set early stopping to prevent overfitting and also save some time. Early stopping stops the training when the monitored quantity doesn't improve."},{"metadata":{"trusted":true,"_uuid":"7cbb8c51b9dd282e1fdc3249873dc544dc0c0f5d"},"cell_type":"code","source":"# monitor: the quantity to be monitored. When it no longer improves significantly, stop training\n# # patience: number of epochs we wait before training is stopped\n# verbose: whether or not to display the message\nearly_stopping = EarlyStopping(monitor=\"val_acc\", patience=7, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d51f50740095f29e2cc07f6085316d0b81986e89"},"cell_type":"markdown","source":"Now we train the model."},{"metadata":{"trusted":true,"_uuid":"1f2a7cb4427fc9fb1e5bb039422ec30211906bf9"},"cell_type":"code","source":"print(\"Training model...\")\nfit_params = {\n    \"batch_size\": batch_size,\n    \"epochs\": iters,\n    \"verbose\": 1,\n    \"callbacks\": [lr_decay, early_stopping],\n    \"validation_data\": (x_dev, y_dev)     # data for monitoring the model accuracy\n}\nmodel.fit(x_train, y_train, **fit_params)\nprint(\"Done!\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d54fe77b75c1615ddd860395ca6238931854faa"},"cell_type":"markdown","source":"## 4.2 Evaluate the Model\nNow we need to evaluate our trained model on the validation data."},{"metadata":{"trusted":true,"_uuid":"d4689eb9a142504177eb432e62662fd841bf2214"},"cell_type":"code","source":"model.evaluate(x_dev, y_dev)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dca0aeeae5bdb18cde5cb8e29ceca0f372c5f0ae"},"cell_type":"markdown","source":"On the validation set our model reached an accuracy over 99%, which is pretty good."},{"metadata":{"_uuid":"000623d81c3c2ce533421533ad20d8d64f8c2dc9"},"cell_type":"markdown","source":"# 5 Output the Prediction"},{"metadata":{"trusted":true,"_uuid":"8ad64b46d312b894c35aa259a26bbdea7e4aabda"},"cell_type":"code","source":"y_pred = model.predict(x_test, batch_size=batch_size)\ny_pred = np.argmax(y_pred, axis=1).reshape((-1, 1))\nidx = np.reshape(np.arange(1, len(y_pred) + 1), (len(y_pred), -1))\ny_pred = np.hstack((idx, y_pred))\ny_pred = pd.DataFrame(y_pred, columns=['ImageId', 'Label'])\ny_pred.to_csv('y_pred.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}