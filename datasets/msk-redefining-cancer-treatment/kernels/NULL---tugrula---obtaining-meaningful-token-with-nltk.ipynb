{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"file_extension": ".py", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "version": "3.6.1", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python"}}, "nbformat": 4, "cells": [{"metadata": {"_uuid": "c57b02a2b846fd8dad444a9fb2f7c7dc9321eb65", "_cell_guid": "15555e34-b5f4-450f-8824-a89e5ffbe898"}, "cell_type": "markdown", "source": ["In this Kernel, Obtaining meaningful token with NLTK will be examined.\n", "\n", "Necessary libraries are uploaded like below."]}, {"metadata": {"_uuid": "7413700fec0a6068331ab3d871fef81c0b293b88", "_cell_guid": "608a8975-1291-4b48-811d-d520b3700535", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Import Pandas\n", "import pandas as pd\n", "# Import Counter\n", "from collections import Counter\n", "# Import WordNetLemmatizer\n", "from nltk.stem import WordNetLemmatizer\n", "# Import word_tokenize\n", "from nltk.tokenize import word_tokenize\n", "# Import stopwords\n", "from nltk.corpus import stopwords\n", "# Import pyplot\n", "import matplotlib.pyplot as plt\n", "# Import string\n", "import string\n", "\n", "\n", "# Import randint just for test\n", "from random import randint\n", "\n", "# Main image size\n", "plt.rcParams[\"figure.figsize\"] = (18, 9)"], "outputs": []}, {"metadata": {"_uuid": "4f4dab9cef9799fbc2661831353c8a83ea346fe9", "_cell_guid": "1a37c077-2cc4-48d3-9a78-45b44df4bab6"}, "cell_type": "markdown", "source": ["Get text data by using pandas like below."]}, {"metadata": {"_uuid": "f137176c0e13e82a20d78792cb76403820745c66", "_cell_guid": "933b776b-1778-4d41-bb5d-b7debc3e476e", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["df_text = pd.read_csv(r'../input/training_text', sep='\\|\\|', engine='python', skiprows=1, names=['ID', 'Text']).set_index('ID')"], "outputs": []}, {"metadata": {"_uuid": "ecf989755eacb0bbed732b943eac4b1f14f296ff", "_cell_guid": "bdd6c50c-dd0d-449c-8f48-ead341c79d7c"}, "cell_type": "markdown", "source": ["For examining data, let's just get random input in the text field."]}, {"metadata": {"_uuid": "481d4f5fa4bc176f6a176cd1e124bd4bb5292651", "_cell_guid": "f4889ed3-e275-4835-b50a-3580ebc7d6e2", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Tokenize the article: tokens\n", "tokens = word_tokenize(str(df_text.iloc[randint(0, len(df_text.index))].values))\n", "# Take the 15 most common tokens\n", "only_token_all=sorted(Counter(tokens).most_common(15), key=lambda w: w[1], reverse=True)"], "outputs": []}, {"metadata": {"_uuid": "257e794bfc2a31ff5cae466963c033631be6972e", "_cell_guid": "2d9719bc-fb3b-4d6f-9f60-19f144a4885b"}, "execution_count": null, "cell_type": "code", "source": ["fig1, ax = plt.subplots()\n", "ax.bar(range(len(only_token_all)), [t[1] for t in only_token_all]  , align=\"center\")\n", "ax.set_xticks(range(len(only_token_all)))\n", "ax.set_xticklabels([t[0] for t in only_token_all])\n", "plt.xlabel('Tokens')\n", "plt.ylabel('Number of Usage')\n", "plt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize$')\n", "plt.grid(True)\n", "plt.show()"], "outputs": []}, {"metadata": {"_uuid": "8cedd395bc77f5bf16129851c508693e5a8ecb58", "_cell_guid": "c7972e07-2b05-43fc-8498-4946f0a14fe7"}, "cell_type": "markdown", "source": ["As it can be seen from figure above, there are lots of non-alphabetic characters, so let's clear those like below."]}, {"metadata": {"_uuid": "d607a45d03887bc3494d21da7f79ed7927af6bd7", "_cell_guid": "f412723e-7600-41cf-9647-ac5c60d35ce5", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Convert the tokens into lowercase: lower_tokens\n", "lower_tokens = [t.lower() for t in tokens]\n", "# and delete punctuation\n", "lower_tokens = [''.join(c for c in s if c not in string.punctuation) for s in lower_tokens]\n", "lower_tokens = [s for s in lower_tokens if s]\n", "# Retain alphanumeric: alpha_only\n", "alpha_only = [t for t in lower_tokens if not t.isdigit()]\n", "# Again take the 15 most common tokens\n", "alpha_only_all=sorted(Counter(alpha_only).most_common(15), key=lambda w: w[1], reverse=True)"], "outputs": []}, {"metadata": {"_uuid": "31135964eee2407e87d92b24eea7187ab741cd59", "_cell_guid": "f3bad5a1-cd29-4dc7-ab70-7ff7476156fa"}, "execution_count": null, "cell_type": "code", "source": ["fig2, ax = plt.subplots()\n", "ax.bar(range(len(alpha_only_all)), [t[1] for t in alpha_only_all]  , align=\"center\")\n", "ax.set_xticks(range(len(alpha_only_all)))\n", "ax.set_xticklabels([t[0] for t in alpha_only_all])\n", "plt.xlabel('Tokens')\n", "plt.ylabel('Number of Usage')\n", "plt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize&Alpha$')\n", "plt.grid(True)\n", "plt.show()"], "outputs": []}, {"metadata": {"_uuid": "79d088d2beb46de855e8588f9246eff44482aff0", "_cell_guid": "b3b77ad0-e4f1-4a1e-96c6-aba1e5aeade8"}, "cell_type": "markdown", "source": ["Started to see some meaningful data, but there are still lots of tokens which we don't need. Let's proceed with clearing stop words."]}, {"metadata": {"_uuid": "900123f8fd37c67307c8e9b682c47bae3b7a84d8", "_cell_guid": "a740d81f-1145-4bd2-ac2e-7cb419955108", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Remove all stop words: no_stops\n", "stop = set(stopwords.words('english'))\n", "no_stops = [t for t in alpha_only if t not in stop]\n", "# Again take the 15 most common tokens\n", "no_stops_all=sorted(Counter(no_stops).most_common(15), key=lambda w: w[1], reverse=True)"], "outputs": []}, {"metadata": {"_uuid": "bf0eebb4fd52dab99e0a040b6835a1712bde8687", "_cell_guid": "7db73db3-9414-4f57-bee4-65ff7878c9b1"}, "execution_count": null, "cell_type": "code", "source": ["fig3, ax = plt.subplots()\n", "ax.bar(range(len(no_stops_all)), [t[1] for t in no_stops_all]  , align=\"center\")\n", "ax.set_xticks(range(len(no_stops_all)))\n", "ax.set_xticklabels([t[0] for t in no_stops_all])\n", "plt.xlabel('Tokens')\n", "plt.ylabel('Number of Usage')\n", "plt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize&Alpha&Stop$')\n", "plt.grid(True)\n", "plt.show()"], "outputs": []}, {"metadata": {"_uuid": "469c843118c99df83fc66451d807acd8db24fba3", "_cell_guid": "da9c04e2-3bc3-47e0-aa4f-94e89576f5a1"}, "cell_type": "markdown", "source": ["Now we are progressing! Now let's clear plural words."]}, {"metadata": {"_uuid": "b9b15d101c8b488de3f5daeba1c9b14d48269a75", "_cell_guid": "c0c5fd72-12e6-40c4-873c-49dc8636065b", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Instantiate the WordNetLemmatizer\n", "wordnet_lemmatizer = WordNetLemmatizer()\n", "# Lemmatize all tokens into a new list: lemmatized\n", "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n", "# Append the 15 most common tokens for lemmatizer\n", "lemmatized_all=sorted(Counter(lemmatized).most_common(15), key=lambda w: w[1], reverse=True)"], "outputs": []}, {"metadata": {"_uuid": "70bd8a29f54ddf04f241dfe5c3ec71ab772f33df", "_cell_guid": "f381c3bf-447d-49a2-8825-cc8e0413368d"}, "execution_count": null, "cell_type": "code", "source": ["fig4, ax = plt.subplots()\n", "ax.bar(range(len(lemmatized_all)), [t[1] for t in lemmatized_all]  , align=\"center\")\n", "ax.set_xticks(range(len(lemmatized_all)))\n", "ax.set_xticklabels([t[0] for t in lemmatized_all])\n", "plt.xlabel('Tokens')\n", "plt.ylabel('Number of Usage')\n", "plt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize&Alpha&Stop&Lemmatized$')\n", "plt.grid(True)\n", "plt.show()"], "outputs": []}, {"metadata": {"_uuid": "9aee9481c146e883639c5042bc9ceaaba073efdf", "_cell_guid": "e2f578a5-502f-4685-83c8-4ce4f6808a35"}, "cell_type": "markdown", "source": ["hmm... Not much progress. Let's clear this data by using custom stop words like below."]}, {"metadata": {"_uuid": "e0b1d6fc455db36bf5aaaf703a7d6f5014faa327", "_cell_guid": "da944f4e-cac7-432e-aacc-89ac96c5da18", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Remove all stop words with updated data\n", "stop.update(['study', 'table', 'method', 'conclusion', 'case', 'data', 'syndrome', 'analyze', 'author', 'show', 'control', 'expression', 'supplementary', 'result', 'figure','fig', 'level', 'deletion', 'mm', 'state', 'effect', 'stability', 'activity','change','structure', 'line', 'loss', 'expression', 'et', 'al'])\n", "no_stops_updated = [t for t in lemmatized if t not in stop]\n", "# Append the 15 most common tokens for no_stop\n", "no_stops_updated_all = sorted(Counter(no_stops_updated).most_common(15), key=lambda w: w[1], reverse=True)"], "outputs": []}, {"metadata": {"_uuid": "4791e554e857b58aa8fb352a20c589c6b868101e", "_cell_guid": "a959f616-1ebc-4435-aac2-5a09c7f0f0c9"}, "execution_count": null, "cell_type": "code", "source": ["fig5, ax = plt.subplots()\n", "ax.bar(range(len(no_stops_updated_all)), [t[1] for t in no_stops_updated_all]  , align=\"center\")\n", "ax.set_xticks(range(len(no_stops_updated_all)))\n", "ax.set_xticklabels([t[0] for t in no_stops_updated_all])\n", "plt.xlabel('Tokens')\n", "plt.ylabel('Number of Usage')\n", "plt.title(r'$\\mathrm{Common\\ Tokens\\ in\\ TEXT:}\\ Applied\\ Tokenize&Alpha&Updated-Stop&Lemmatized$')\n", "plt.grid(True)\n", "plt.show()"], "outputs": []}, {"metadata": {"_uuid": "393b34164e869158b4a33c281f2042c305279e16", "_cell_guid": "9178fcca-924c-4344-b500-bd3721fdf2ed"}, "cell_type": "markdown", "source": ["Now we see quite good data to start our learning process.\n", "However; NLTK stopwords is not quite enough, so we had to update it. I suggest to use 'sklearn.feature_extraction.stop_words' or 'spacy.en.language_data' as these have data twice of stopwords. I wanted to use only NLTK for this experiment."]}], "nbformat_minor": 1}