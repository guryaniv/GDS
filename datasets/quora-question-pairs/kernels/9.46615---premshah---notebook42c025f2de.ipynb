{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7865c4f7-4876-55e5-314e-42546db575e2"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "501bb15b-baab-6fd3-1d36-441e77fd9c3d"
      },
      "source": [
        "Basic feature set(FS-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1d483696-716d-72d6-0c35-bf9bbb040685"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"../input/train.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dc9a8551-4c65-894b-8137-eacd00bcc278"
      },
      "outputs": [],
      "source": [
        "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
        "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
        "data['diff_len'] = data.len_q1 - data.len_q2\n",
        "data['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ','')))))\n",
        "data['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ','')))))\n",
        "data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
        "data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
        "data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))),axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "72fc3848-959d-72bf-07d0-b03c86e3935f"
      },
      "source": [
        "Fuzzy Features Using fuzzywuzzy package.  Fuzzywuzzy uses Levenshtein Distance to calculate differences between sequences(FS-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4ce5b21b-cb8b-489b-b321-87bb6c26bc0c"
      },
      "outputs": [],
      "source": [
        "#from fuzzywuzzy import fuzz\n",
        "\n",
        "#data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']),str(x['question2'])),axis=1)\n",
        "#data['fuzz_wratio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']),str(x['question2'])),axis=1)\n",
        "#data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
        "#data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
        "#data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
        "#data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
        "#data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
        "#data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fdc0a5b9-7750-14d9-a9cb-b0b521266dfe"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(data, test_size = 0.2)\n",
        "x_test = test\n",
        "x_train = train\n",
        "y_test = test['is_duplicate']\n",
        "y_train = train['is_duplicate']\n",
        "x_train.drop(['id','qid1','qid2','question1','question2','is_duplicate'],axis=1,inplace=True)\n",
        "x_test.drop(['id','qid1','qid2','question1','question2','is_duplicate'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "153b875f-3641-17d7-325a-669a1786dfb2"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(100,oob_score=True)\n",
        "model.fit(x_train,y_train)\n",
        "model.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "29a299a1-c6e1-dcfc-5d2a-42f7c0d33223"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"../input/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "04039681-4a49-b432-375e-dc133670b0ee"
      },
      "outputs": [],
      "source": [
        "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
        "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
        "data['diff_len'] = data.len_q1 - data.len_q2\n",
        "data['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ','')))))\n",
        "data['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ','')))))\n",
        "data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
        "data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
        "data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))),axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7c616979-ea9c-4786-4f5d-f2b257379788"
      },
      "outputs": [],
      "source": [
        "test = data.drop(['test_id','question1','question2'],axis=1)\n",
        "pdt = model.predict(test);\n",
        "pdt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ed1f174f-ebab-0afc-cfc6-5ea47a47a6c3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "sub = pd.DataFrame()\n",
        "sub['test_id'] = data['test_id']\n",
        "sub['is_duplicate'] = pdt\n",
        "sub.to_csv('simple_xgb.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb4aa1da-f0fb-83ff-0b77-7f666949625c"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "87dfa23a-2fc6-9a53-5950-56b172b8c800"
      },
      "source": [
        "Average of Word2Vec vectors with TF-IDF : this is one of the best approach which I will recommend. Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1af43770-04be-8598-e1e1-cd6dcedd6821"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f30e6086-98e7-f3e2-6bb3-712b9b45a4ab"
      },
      "source": [
        "Average of Word2Vec vectors with TF-IDF : this is one of the best approach which I will recommend. Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}