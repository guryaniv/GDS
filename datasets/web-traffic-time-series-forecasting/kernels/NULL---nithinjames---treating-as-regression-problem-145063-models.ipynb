{"nbformat_minor": 0, "cells": [{"cell_type": "markdown", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "idle", "_cell_guid": "efeb8887-052d-4840-9218-578a64c7153f", "collapsed": false, "_uuid": "80b7920fa3155606f0dfc8cb1a6fab6577786a22"}, "source": "Here, I am treating this problem as a Regression problem and Running individual model for each Pages with Date and some additional features."}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "9826a682-73a8-4d28-b24d-ab4163adc361", "_execution_state": "idle", "trusted": false, "_uuid": "58f6e82cc3a813d8e8ff32580251577cb73be095"}, "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\n"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "dc6070d8-62f0-4332-9174-46b90fa8345f", "trusted": false, "collapsed": false, "_uuid": "fa42c8c363db7a886e81a6a3cf8769c3e7fcdbf0"}, "source": "#Load the dataset\ntrain = pd.read_csv('../input/train_1.csv').fillna(0)\n#Save the dates for future use\npages = train['Page'].copy()\ndates = train.columns\n#Drop the page coloumn for now as we are training individual model\ntrain.drop(['Page'],inplace=True,axis=1)\n"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "04a9a070-0f41-4fca-b949-1773d343b356", "trusted": false, "collapsed": false, "_uuid": "6e5076b8452d00edd89f3a1856ed9b59683a0e1b"}, "source": "#Stack the coloumns as rows\ndf_train = train.stack().reset_index(level=0, drop=True).reset_index()"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "f7036c12-0e60-46b0-a265-e23b23503a26", "trusted": false, "collapsed": false, "_uuid": "cf7f3ad4a4cad17faaf019939ea70e5b6dc92d44"}, "source": "#set the coloumn names to date and number of visits\ndf_train.columns = ['Date','number_of_visits']"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "ad1a4fa0-b5fc-461a-99ca-9b6d04cac593", "trusted": false, "collapsed": false, "_uuid": "50c20e0e85641987e877d7385e73210c2903516c"}, "source": "#Let's see how the data looks like\n\ndf_train.head()"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "dec59e17-aa1b-47bc-a29d-8158a4351d73", "trusted": false, "collapsed": false, "_uuid": "fe634bf57de01d5591638debab46a2812d0b5fb8"}, "source": "#Create the pages data frmae\n\npages_repeat = pd.DataFrame(np.repeat(pages,550))\n"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "934f72e6-ff60-4b84-b7fb-68549d242cce", "trusted": false, "collapsed": false, "_uuid": "56de517d27e8ab16adedc6beae6e5bad587b74a3"}, "source": "#reset the index for not messing with it\ndf_train.reset_index(drop=True,inplace=True)\npages_repeat.reset_index(drop=True,inplace=True)"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "d1cd3d65-0614-4ae2-801f-04b2fc67caea", "trusted": false, "collapsed": false, "_uuid": "e13eb3445b0d436a1b9d9345bc8b7e849e869269"}, "source": "#Add the page coloumn again to train data  as we require them to group it\ndf_train['Page'] = pages_repeat.Page.copy()"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "d6a61c78-c4f5-42ea-bd94-7090ffe47b53", "trusted": false, "collapsed": false, "_uuid": "13d18f3bb2b17d3147fe3e24188daa9ae370f22b"}, "source": "#now group by Page to run individual model for each of them\n\ngrouped = df_train.groupby('Page')\n"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "0305e27d-0edb-4834-8205-76c90ac9a2f2", "trusted": false, "collapsed": false, "_uuid": "f90ba1983506df4b9b7ac0548539d598388ce051"}, "source": "#SMAPE calculate for each page\n#This one is from CPMP,Thanks ;-)\ndef smape(y_true, y_pred,page_name):\n    denominator = (np.abs(y_true) + np.abs(y_pred))\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    print (\"SMAPE score for \"+str(np.asarray(page_name))+\": \"+str(200 * np.mean(diff)))"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "51a5885e-0c3f-4c92-ac30-3aac4b8bb2e1", "trusted": false, "collapsed": false, "_uuid": "c47aa920efa8055c858a322551fda1b13c3018f1"}, "source": "#Prepare our test data from January 1,2017 to November 10,2017\n\n#generate dates between a range\ndate_test =[]\nfor dayes in pd.date_range('20170101','20171110'):\n    date_test.append(dayes.strftime('%Y-%m-%d'))\n    date_frame = pd.DataFrame(np.asarray(date_test))\n    #Save the date_frame for future use\n    date_append = date_frame\n    date_frame.columns = ['Date']\n    #Expand the date coloumn \n    date_frame = date_frame.Date.str.split('-',expand=True).astype(int)\n    date_frame.columns = ['Year','Month','Day']\n    #add the quarter to dataframe\n    date_frame['Quarter'] = (date_frame.Month-1)//3\n    #drop the year\n    date_frame.drop(['Year'],inplace=True,axis=1)"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "3859b7a2-4153-4a4f-b2e8-16dce475d23f", "trusted": false, "collapsed": false, "_uuid": "28989441efcb7f50423670d4702e3c225e3a1f2c"}, "source": "def process_groups(test_template):\n    #Initialize a dataframe to combine all the page predictions\n    final_predictions = pd.DataFrame(columns=['Visits','Page','Date'])\n    counter = 0\n    #preprocess each group\n    for group in grouped.groups.keys():\n        #create a temp frame\n        group_predictions = pd.DataFrame(columns=['Visits','Page'])\n        data_train = grouped.get_group(group)\n        #Expand the Date coloumn\n        data_train_date= data_train.Date.str.split('-',expand=True).astype(int)\n        data_train_date.columns =['Year','Month','Day']\n        #concatenate to expanded date to data_train\n        data_train = pd.concat([data_train,data_train_date],axis=1)\n        targets = data_train.number_of_visits.copy()\n        #Save the page name for future\n        pages_frame = np.unique(data_train.Page.values)      \n        #drop the Year and number_of_visits,Page coloumn\n        data_train.drop(['Year','number_of_visits','Page','Date'],inplace=True,axis=1)\n        #Add the quarter date\n        data_train['Quarter'] = (data_train.Month-1)//3\n        #KFold cross validation\n        kfold = KFold(5)\n        predictions =[]\n        \n    \n        data_train = np.asarray(data_train)\n        targets = np.asarray(targets)\n        \n        for train_index,test_index in kfold.split(data_train,targets):\n            rf = RandomForestRegressor(n_estimators=100,max_depth=4)\n            X_train, X_test = data_train[train_index], data_train[test_index]\n            y_train, y_test = targets[train_index], targets[test_index]\n            rf.fit(X_train,y_train)\n            y_preds = rf.predict(X_test)\n            #Calculate the SMAPE\n            smape(y_test,y_preds,pages_frame)\n            #predict on test_data\n            predictions.append(rf.predict(test_template))\n        #Average the results from cross validation\n        pred_average = np.mean(np.asarray(predictions),0)\n        group_predictions['Page'] = np.repeat(pages_frame,len(test_template))\n        group_predictions['Visits'] = pred_average\n        group_predictions['Date'] = date_append\n        #Add the dates to the final predictions\n        final_predictions = final_predictions.append(group_predictions)\n        #Run only for three pages else it will run more than 3+ days :-(\n        #Comment the below line to run for all the pages\n        counter = counter+1\n        if counter > 2:\n            \n            return final_predictions\n    #Uncomment the line to run for all the pages\n    #Caution! : This will take long time\n    #return final_predictions\n        \n    "}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "59b9a107-7f13-4d15-aed4-f64089d02358", "trusted": false, "collapsed": false, "_uuid": "5c3108acd6282e8d3fbb42d5453e99136d8945d7"}, "source": "check_predictions=process_groups(date_frame)"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "c8cc0ae2-fabc-4b83-a07b-c5179c82c811", "trusted": false, "collapsed": false, "_uuid": "2960ea5ce27377ac05598aad4d7feeb92ddaa3f6"}, "source": "\n\ncheck_predictions['Visits'] = check_predictions.Visits.round()"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "busy", "_cell_guid": "388844ac-73e6-47ed-a9cd-7b7f100ec5de", "trusted": false, "collapsed": false, "_uuid": "e6081b29b25132fdce6650e50c30017ad8a8deb4"}, "source": "#Check the predictions \ncheck_predictions.head()"}, {"cell_type": "markdown", "outputs": [], "execution_count": null, "metadata": {"_execution_state": "idle", "_cell_guid": "9a9bf71f-140f-42de-a292-a1ac864f34a8", "collapsed": false, "_uuid": "92bbaa1e321d69d45f4f3a7c91a2b837d4233954"}, "source": "Removing Seasonality,trend and adding week day etc will make the model better i guess.However,This script will run 145063 models(Each page one model) and finally come up with result.\n\n#This model assumes that there is no relationship between the pages,but i don't think that's the case here."}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "version": "3.6.1", "mimetype": "text/x-python"}}, "nbformat": 4}