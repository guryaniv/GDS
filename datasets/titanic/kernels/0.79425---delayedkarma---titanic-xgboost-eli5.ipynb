{"cells":[{"metadata":{"_uuid":"51e1b39dc6664c8e4ea4b21c74e4afc915dd3d5f"},"cell_type":"markdown","source":" I learnt about ELI5 (https://eli5.readthedocs.io/en/latest/index.html) from Konstantin Lopuhin's kernel (https://www.kaggle.com/lopuhin/eli5-for-mercari), as well as SRK's kernel on the Quora Insincere Questions Classification Competition (https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc). \n \n I'm learning how to use this, and the ELI5 tutorials have a simple example for the Titanic competition (https://eli5.readthedocs.io/en/latest/tutorials/xgboost-titanic.html), so i figured I'd make that into a kernel for ease of reference, because I foresee myself using this library a lot in the future. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import csv\nimport numpy as np\nimport pandas as pd\n\nfrom datetime import datetime as dt\n\nfrom IPython.display import display\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom xgboost import XGBClassifier\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.pipeline import FeatureUnion, make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom eli5 import show_weights, show_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57b0676cd83491c6d83ae5e66709c322850c072b"},"cell_type":"code","source":"with open('../input/train.csv', 'rt') as f:\n    data_train = list(csv.DictReader(f))\ndata_train[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"812b15bbd135e2329b7ecea9aff8e78561188bde"},"cell_type":"code","source":"with open('../input/test.csv', 'rt') as f:\n    data_test = list(csv.DictReader(f))\ndata_test[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b1e163a263797aca4699badd2c8d9a4eb5e6fe3"},"cell_type":"code","source":"_all_xs = [{k: v for k, v in row.items() if k != 'Survived'} for row in data_train]\n_all_ys = np.array([int(row['Survived']) for row in data_train])\n\nall_xs, all_ys = shuffle(_all_xs, _all_ys, random_state=0)\ntrain_xs, valid_xs, train_ys, valid_ys = train_test_split(\n    all_xs, all_ys, test_size=0.25, random_state=0)\n\nprint('{} items total, {:.1%} true'.format(len(all_xs), np.mean(all_ys)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22605629578ef13e845acc2c89db05d88d6f3cc8"},"cell_type":"markdown","source":"### Just using some basic feature engineering, same as the ELI5 tutorial. For a more detailed exploration, cf. my old Titanic kernels (https://www.kaggle.com/delayedkarma/basic-eda-feature-engineering-and-modeling & https://www.kaggle.com/delayedkarma/grid-search-and-rf-lb-0-80861-top-10)"},{"metadata":{"trusted":true,"_uuid":"90ff0017a010542d0d0172d00b0cb3f5db8fc9bd"},"cell_type":"code","source":"for x in all_xs:\n    if x['Age']:\n        x['Age'] = float(x['Age'])\n    else:\n        x.pop('Age')\n    x['Fare'] = float(x['Fare'])\n    x['SibSp'] = int(x['SibSp'])\n    x['Parch'] = int(x['Parch'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2354db6a136dc8f2e0270000eb7d2d04eb24cd24"},"cell_type":"code","source":"### Load the test set, do the basic pre-processing steps same as above, and make predictions on it.\n\nall_xs_test = [{k: v for k, v in row.items()} for row in data_test]\n\nfor x in all_xs_test:\n    if x['Age']:\n        x['Age'] = float(x['Age'])\n    else:\n        x.pop('Age')\n    if x['Fare']:\n        x['Fare'] = float(x['Fare'])\n    else:\n        x.pop('Fare')\n    x['SibSp'] = int(x['SibSp'])\n    x['Parch'] = int(x['Parch'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f22c0d74eea02431dcc438cc950cdec6af58ba6b"},"cell_type":"markdown","source":"### The CSCTransformer is used to convert the data to a CSC format (http://www.scipy-lectures.org/advanced/scipy_sparse/csc_matrix.html) to ensure the XGBoost classifier deals well with sparse data. We don't want to just convert everything to a dense representation since then we will lose the ability to analyze features with zero value, and differentiate missing features. "},{"metadata":{"trusted":true,"_uuid":"eb7d29b9291472100158bf1697e7c5c08f411b02"},"cell_type":"code","source":"class CSCTransformer:\n    def transform(self, xs):\n        # work around https://github.com/dmlc/xgboost/issues/1238#issuecomment-243872543\n        return xs.tocsc()\n    def fit(self, *args):\n        return self\n\nclf = XGBClassifier()\nvec = DictVectorizer()\npipeline = make_pipeline(vec, CSCTransformer(), clf)\n\ndef evaluate(_clf):\n    scores = cross_val_score(_clf, all_xs, all_ys, scoring='accuracy', cv=10)\n    print('Accuracy: {:.3f} ± {:.3f}'.format(np.mean(scores), 2 * np.std(scores)))\n    _clf.fit(train_xs, train_ys)  # so that parts of the original pipeline are fitted\n    return np.mean(scores), _clf.predict(all_xs_test)\n\nscore_1, preds_1 = evaluate(pipeline)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"470c19f310c776b5eecab72db9f2960c2462206f"},"cell_type":"markdown","source":"### Now let's check the feature importances for these basic features"},{"metadata":{"trusted":true,"_uuid":"651e92b3bcb080e5f59838b4e0c971538b161fb9"},"cell_type":"code","source":"show_weights(clf, vec=vec)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"639454ab6ad5a172e47f1104ed6b2b66259615a8"},"cell_type":"markdown","source":"### The *show_predictions* method allows us to examine individual predictions"},{"metadata":{"trusted":true,"_uuid":"19618e2547058fc323ddca4639babae5df5beb8d"},"cell_type":"code","source":"show_prediction(clf, valid_xs[1], vec=vec, show_feature_values=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54c956126f0c55ff3f9f919652bbb1fbcce7ab9e"},"cell_type":"markdown","source":"### So, if you're classified as a Female and you have paid a higher fare, you are more likely to survive than a passenger in 3rd class. We can also just examine the individual predictions but just for features without missing values. "},{"metadata":{"trusted":true,"_uuid":"a47160b4a7b98ce5060687406604d5c229098d6a"},"cell_type":"code","source":"no_missing = lambda feature_name, feature_value: not np.isnan(feature_value)\nshow_prediction(clf, valid_xs[1], vec=vec, show_feature_values=True, feature_filter=no_missing)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1118591dfa7cd9938a5b26edc1af7d8638e776c4"},"cell_type":"markdown","source":"### We don't want to process name as a categorical variable"},{"metadata":{"trusted":true,"_uuid":"aa7c49ec393c84a1c6cced9adccbffefae291bf0"},"cell_type":"code","source":"vec2 = FeatureUnion([\n    ('Name', CountVectorizer(\n        analyzer='char_wb',\n        ngram_range=(3, 4),\n        preprocessor=lambda x: x['Name'],\n        max_features=100,\n    )),\n    ('All', DictVectorizer()),\n])\nclf2 = XGBClassifier()\n\npipeline2 = make_pipeline(vec2, CSCTransformer(), clf2)\n\nscore_2, preds_2 = evaluate(pipeline2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c127dcfe1de2d6289ca37479f00daf06e022e16"},"cell_type":"markdown","source":"### Feature importances"},{"metadata":{"trusted":true,"_uuid":"cc35d2b5ce9fb2bab636d66a70b0a4bd8bb7d70d"},"cell_type":"code","source":"show_weights(clf2, vec=vec2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f295d9f7d22427dc9c8a487be34656850b32e47"},"cell_type":"markdown","source":"### Some name-based features have some weightage"},{"metadata":{"trusted":true,"_uuid":"f66d3931d45dfc1300db488dec81a62d857f5abb"},"cell_type":"code","source":"# We hide missing features \nfor idx in [4, 5, 7, 37, 81]:\n    display(show_prediction(clf2, valid_xs[idx], vec=vec2,\n                            show_feature_values=True, feature_filter=no_missing))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b37159c7de099b3310a0af1e00fd81163ceca1a"},"cell_type":"markdown","source":"### Idea for submission filename drawn from Chia-ta Tsai's kernel https://www.kaggle.com/cttsai/forked-lgbm-w-ideas-from-kernels-and-discuss. Obviously the LB score will improve a lot if we do a little more parameter tuning + feature engineering, but I just wanted to understand how ELI5 works for now. "},{"metadata":{"trusted":true,"_uuid":"897cc17add8ff2cb1e3b590b7c3ec754ebcd1004"},"cell_type":"code","source":"df_sub = pd.read_csv('../input/gender_submission.csv')\n\nfilename = 'subm_{:.6f}_{}.csv'.format(score_2, \n                     dt.now().strftime('%Y-%m-%d-%H-%M'))\nprint('save to {}'.format(filename))\n\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = df_sub['PassengerId']\nsubmission['Survived'] = preds_2\n\nsubmission.to_csv(filename, index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}