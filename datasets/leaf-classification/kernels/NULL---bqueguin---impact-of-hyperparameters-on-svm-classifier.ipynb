{"cells":[{"metadata":{"trusted":true,"_uuid":"42f5e0b65f5d35e0d302c307cf47074230289144"},"cell_type":"code","source":"%reset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c243c750df5f379c0ffe080180e4456c4cf967b2"},"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72a2e8d4343543196e67ebaefa033fb53b372393"},"cell_type":"code","source":"# Import data\ntrain = pd.read_csv('../input/train.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa61f5b9d200a5f9f75fbd64a29ff271428555b0"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d03b01fcd62bf7beb4555bde0f601abea010c9d4"},"cell_type":"code","source":"# Transform species values to number label\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder().fit(train.species)\ntrain.species = le.transform(train.species)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60dfb45d27f80ed4c4616d51fbd6b579beba7b75"},"cell_type":"code","source":"all(train.species.value_counts() == 10) # There are 10 leaves for each species","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfb5963d52b4053c4d016e3c1e0d0365e26d9a8d"},"cell_type":"code","source":"# Define the target and the features\nY = train.iloc[:, 0]\nX = train.iloc[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49d1acfd6bc1f44729db1d4e56b1e90beba6efc2"},"cell_type":"code","source":"# Normalize the features X\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X)\nX_scale = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a40c0021943e3b93941b418403c67ba3f30fe0eb"},"cell_type":"code","source":"# Split data into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_scale, Y, test_size=0.25, stratify=Y)\n# stratify=y means that every kind of leaves will be present at the same proportion in the train and test set.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6f4652a1f5c8402eeeeff2b07867c808f114f8c"},"cell_type":"code","source":"# Baseline with k-NN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier().fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nY_prob = knn.predict_proba(X_test)\n\nfrom sklearn.metrics import accuracy_score, log_loss\nbaseline_accuracy = accuracy_score(Y_test, Y_pred) \nbaseline_logloss = log_loss(Y_test, Y_prob)\nprint(baseline_accuracy) # 97.2% of the leaves are well classified, this is our baseline\nprint(baseline_logloss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d22f1cd5a5196c9d3f04d5082077c0e2bc242962"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Default multiclass SVM classifier (OvR and l2 penalty)\nfrom sklearn.svm import LinearSVC\nclf = LinearSVC(dual=False).fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\n\nprint(accuracy_score(Y_test, Y_pred)) # Accuracy = 96.4%. This is not better than with k-NN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd13a5fc0d87813825dd9f109abf1e9e795e0204"},"cell_type":"code","source":"# Multiclass SVM classifier with optimal hyper-parameters\nfrom sklearn.model_selection import GridSearchCV\n\nlsvm = LinearSVC(dual=False) # dual = False beacause we have more samples than variables\nparams = {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l2', 'l1'], 'multi_class': ['ovr', 'crammer_singer']}\n# Here we will test 3 differents hyper-parameters:\n    # C: the value of the coefficient of regularization\n    # penalty: the norm used for regularization\n    # multi_class: the method used for a multi-class problem, One vs rest or Crammer Singer\n    # note: for Crammer Singer method, only l2 penalty is possible\ngs = GridSearchCV(lsvm, params, cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"256bd80d427f670c2b57a0f97040c29fc033606d"},"cell_type":"code","source":"# Training of the models\ngs.fit(X_train, Y_train) # Takes few minutes ...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"276fea7d9903611c7d738464cd59999b0f3de543"},"cell_type":"code","source":"print(gs.best_params_) # It looks like the Crammer Singer method is the most efficient with C=0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b38517608a04cd2fbde0153b27a405b4780df171"},"cell_type":"code","source":"# Compute the accuracy with optimal hyper-parameters\nY_pred = gs.predict(X_test)\nprint(accuracy_score(Y_test, Y_pred)) # The accuracy is now 99.6%, clearly better than the baseline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7158369816482a0b8f01995c3038808c4f55be3"},"cell_type":"code","source":"# Let's plot the evolution of the accuracy in terms of the hyper parameters\n\nC = np.logspace(-4, 3, 15)\n\novr_clf2 = LinearSVC(dual=False, penalty='l2', multi_class='ovr')\naccuracy_ovr2 = []\nfor c in C:\n    ovr_clf2.set_params(C=c)\n    ovr_clf2.fit(X_train, Y_train)\n    accuracy_ovr2.append(ovr_clf2.score(X_test, Y_test))\n\ncs_clf2 = LinearSVC(dual=False, penalty='l2', multi_class='crammer_singer')\naccuracy_cs2 = []\nfor c in C:\n    cs_clf2.set_params(C=c)\n    cs_clf2.fit(X_train, Y_train)\n    accuracy_cs2.append(cs_clf2.score(X_test, Y_test))\n\novr_clf1 = LinearSVC(dual=False, penalty='l1', multi_class='ovr')\naccuracy_ovr1 = []\nfor c in C:\n    ovr_clf1.set_params(C=c)\n    ovr_clf1.fit(X_train, Y_train)\n    accuracy_ovr1.append(ovr_clf1.score(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a091da550f974b550d79ab9b5c1b918ae45ebd1"},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nplt.plot([min(C), max(C)], [baseline_accuracy, baseline_accuracy], label='baseline (k-NN)', color='black', linestyle='--', linewidth=0.5)\nplt.plot(C, accuracy_ovr2, label='OvsR, l2') \nplt.plot(C, accuracy_ovr1, label='OvsR, l1')\nplt.plot(C, accuracy_cs2, label='Crammer Singer, l2')\nplt.xscale('log')\nplt.xlabel('Value of C')\nplt.ylabel('Accuracy')\nplt.legend(loc='best')\nplt.axis('tight')\nplt.ylim([0.85, 1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e508fc43afaff583005a06794f487f5008a34277"},"cell_type":"code","source":"# RESULTS\n\n# The Crammer Singer method is definitely more efficient than the One vs Rest method.\n# We can see that the value of C don't really impact the accuracy for the Crammer Singer method.\n# For the One vs Rest method the value of C impacts the accuracy.\n# Finding good hyper-parameters can improve the score of multi-class SVM classifiers.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}