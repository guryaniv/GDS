{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f7c93524-2268-d46a-3386-321b5f7f3309"
      },
      "source": [
        "## Introduction ##\n",
        "The idea here is to use a bag of visual words model to classify the different images. We will use SIFT algorithm to extract the keypoints of each image and create the bag of words.<br>\n",
        "More information about this method can be found here:<br><ul>\n",
        "<li>http://www.cs.cmu.edu/~16385/lectures/Lecture12.pdf</li>\n",
        "<li>https://www.youtube.com/watch?v=iGZpJZhqEME</li>\n",
        "\n",
        "Some part of this script are inside function, it's just a way to avoid error when I will publish this notebook. If you want to use this script, just remove line starting by \"def ...\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4fd239a0-39fd-7bc4-4ed3-d3bf87c21305"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bd3316dd-1e7c-5acc-8d86-48e6704b42ee"
      },
      "source": [
        "To do it, we will use OpenCV (cv2) library to extract keypoints with SIFT algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0849ae08-ff33-bd8c-abb2-db52e1441eee"
      },
      "source": [
        "## Extract keypoints from each image ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd6c53d2-0822-fd32-7a20-447d485ebafe"
      },
      "outputs": [],
      "source": [
        "img_path = '../input/images/'\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "species = train.species.sort_values().unique()\n",
        "\n",
        "dico = []\n",
        "\n",
        "def step1():\n",
        "    for leaf in train.id:\n",
        "        img = cv2.imread(img_path + str(leaf) + \".jpg\")\n",
        "        kp, des = sift.detectAndCompute(img, None)\n",
        "\n",
        "        for d in des:\n",
        "            dico.append(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "58b3257e-6916-0ed0-bdb5-9c23dd6e0ab8"
      },
      "source": [
        "## Clustering  ##\n",
        "We now have an array with a huge number of descriptors. We cannot use all of them to create or model so we need to cluster them. A rule-of-thumb is to create k centers with k = number of categories * 10 (in our case, it's 990)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca3fc57e-e7f1-1d10-70fb-0b521b2da700"
      },
      "outputs": [],
      "source": [
        "def step2():\n",
        "    k = np.size(species) * 10\n",
        "\n",
        "    batch_size = np.size(os.listdir(img_path)) * 3\n",
        "    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch_size, verbose=1).fit(dico)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "758507e5-cf26-03f6-3fab-cf55f83accfb"
      },
      "source": [
        "I use MiniBatchKMeans to avoid Memory Error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5d9b232c-af09-1673-e3f4-3869ea491321"
      },
      "source": [
        "## Creation of the histograms ##\n",
        "To create our each image by a histogram. We will create a vector of k value for each image. For each keypoints in an image, we will find the nearest center and increase by one its value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "47eb763e-b505-1599-2f83-50d013339a0d"
      },
      "outputs": [],
      "source": [
        "def step3():\n",
        "    kmeans.verbose = False\n",
        "\n",
        "    histo_list = []\n",
        "\n",
        "    for leaf in train.id:\n",
        "        img = cv2.imread(img_path + str(leaf) + \".jpg\")\n",
        "        kp, des = sift.detectAndCompute(img, None)\n",
        "\n",
        "        histo = np.zeros(k)\n",
        "        nkp = np.size(kp)\n",
        "\n",
        "        for d in des:\n",
        "            idx = kmeans.predict([d])\n",
        "            histo[idx] += 1/nkp # Because we need normalized histograms, I prefere to add 1/nkp directly\n",
        "\n",
        "        histo_list.append(histo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5197a0cb-cb3c-0968-4521-32d6b9559a79"
      },
      "source": [
        "## Training of the neural network ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "88e782f5-8f6d-ac67-5e9e-d30eb3b99c26"
      },
      "outputs": [],
      "source": [
        "def step4():\n",
        "    X = np.array(histo_list)\n",
        "    Y = []\n",
        "\n",
        "    # It's a way to convert species name into an integer\n",
        "    for s in train.species:\n",
        "        Y.append(np.min(np.nonzero(species == s)))\n",
        "\n",
        "    mlp = MLPClassifier(verbose=True, max_iter=600000)\n",
        "    mlp.fit(X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4d5351d1-4609-7ebf-98cf-f48f7068e183"
      },
      "source": [
        "## Predictions ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58247f35-2213-d774-4229-48b308c62613"
      },
      "outputs": [],
      "source": [
        "def step5():\n",
        "    test = pd.read_csv('../input/test.csv')\n",
        "\n",
        "    result_file = open(\"sift.csv\", \"w\")\n",
        "    result_file_obj = csv.writer(result_file)\n",
        "    result_file_obj.writerow(np.append(\"id\", species))\n",
        "\n",
        "    for leaf in test.id:\n",
        "        img = cv2.imread(img_path + str(leaf) + \".jpg\")\n",
        "        kp, des = sift.detectAndCompute(img, None)\n",
        "\n",
        "        x = np.zeros(k)\n",
        "        nkp = np.size(kp)\n",
        "\n",
        "        for d in des:\n",
        "            idx = kmeans.predict([d])\n",
        "            x[idx] += 1/nkp\n",
        "\n",
        "        res = mlp.predict_proba([x])\n",
        "        row = []\n",
        "        row.append(leaf)\n",
        "\n",
        "        for e in res[0]:\n",
        "            row.append(e)\n",
        "\n",
        "        result_file_obj.writerow(row)\n",
        "\n",
        "    result_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d10668e0-7c50-1f79-1cb4-bf4d1b87977b"
      },
      "source": [
        "## Alternative ##\n",
        "I also run this script with ORB instead of SIFT and I got best results. To do it, just replace `cv2.xfeatures2d.SIFT_create()` by `cv2.ORB_create()`."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}