{"cells":[{"metadata":{"_uuid":"983fb439984bac0b9dbbb722a5ad58b80522dca7"},"cell_type":"markdown","source":"This is my first kernel on Kaggle. Would really appreciate any feedback that I can get.\n\nTitanic is one of the most popular dataseta on Kaggle in which we have to perform a binary classification to predict the survival of the passengers.\nSince I am fairly new to data science so this kernel will be simple and learner oriented. I have added links for the libraries, methods, etc. for \neasy reference and will update this kernel further with more information and code for few other tasks that I feel are currently missing from this kernel."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Used to display the full output without the ...\npd.set_option('display.expand_frame_repr', False)\n\n# Ignore warnings thrown by Seaborn\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b07c7d622359f85eb7e851e89f30b4f957a6099a"},"cell_type":"code","source":"# Import charting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b89dbb8643bc7f28b9d8c5e6d0e8a93df7fdcf0d"},"cell_type":"markdown","source":"We begin by picking the two input files, train and validation"},{"metadata":{"trusted":true,"_uuid":"5386148de09491f5acab2e3adef16243730f4c13"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\nvalidation = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ccc979206407ee2f57fb75700e1c87c71bf4d77"},"cell_type":"markdown","source":"We use Pandas [read_csv](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to fetch the two datasets. This method returns new [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) objects which we then store in train and validation."},{"metadata":{"_uuid":"ddc994e83e9b50d9180adea3f4c6cd0623cac9cf"},"cell_type":"markdown","source":"**Step 1:  Poking the Data**\n\nFirst we need to get an initial picture of the data available to us."},{"metadata":{"trusted":true,"_uuid":"8ab600c851eabd681f4b4b811732c159496a7c1b","scrolled":false},"cell_type":"code","source":"# Check the train dataset.\nprint(train.columns.values)\ntrain.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8c3d3b6588bcbf97a5ef0124d662f50a1fcc706"},"cell_type":"markdown","source":"Here, train.[columns](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.columns.html#pandas.DataFrame.columns) returns all the column labels in the train DataFrame and we then use [values](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.values.html?highlight=values#pandas.Series.values) to get an easy to print numpy ndarray.\n\nNext we use train.[describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html) to the indicative stats that allow us to quickly glance through the data to see number of null values, spread, outliers, etc. We used include='all' to also include non numeric features in the describe output.\n\nWe have 12 features in the dataset. The details of these features are provided by Kaggle [here](https://www.kaggle.com/c/titanic/data).  The meaning of these features is quite easy to understand but the real trick lies in how we actually use this data to train our models. There are a total of 891 rows in the train data. From the count field we can see that the Age, Cabin and Embarked have null values.\nIt is worth noting that the data provided is [tidy](https://en.wikipedia.org/wiki/Tidy_data). Tidy data sets means that each feature is stored as a column and each observation is stored as a row. Unlike the datasets provided here by Kaggle, real world data may need to be converted to this format before we begin. Tidy data makes it much easier for us to perform [data cleaning](https://en.wikipedia.org/wiki/Data_cleansing)."},{"metadata":{"trusted":true,"_uuid":"331192fd7ea1273df7ea3d3e80ee7340aa81992f"},"cell_type":"code","source":"# Understand the datatypes\nprint(train.dtypes)\nprint()\n# Focus first on null values\nprint(train.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02aa6188a398b13414d6a093d67aac9cde84aef4"},"cell_type":"markdown","source":"The [dtypes](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dtypes.html) method tells us the data types of the features. We follow different preprocessing steps for different types of data. Checking for nulls with [isna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isna.html) gives us an indication of how much data is missing and the importance of features. Too many nulls may lead to an unsalvageable feature.\nWe have nulls in Age, Cabin and Embarked in the train dataset."},{"metadata":{"trusted":true,"_uuid":"dfeeeee955d9997c2f780b522318f7dbfca0a378"},"cell_type":"code","source":"# Check the validation dataset also.\nprint(train.columns.values)\ntrain.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e8527b7e841659bff922b18f67051814ba249c8d"},"cell_type":"code","source":"print(validation.dtypes)\nprint()\nprint(validation.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"764b1f0e3ec11ea8c9bd0e5b93199fcf60794712"},"cell_type":"markdown","source":"We have nulls in Age, Cabin and Fare in the validation dataset.\n\nNext we will check the correlation coefficients for the provided data using the [corr](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) method. This gives us an initial idea about the relationship amongst the various numerical features. We use this knowledge to direct our exploration and analysis of the data."},{"metadata":{"trusted":true,"_uuid":"6efd5533af4c9c31b3971c39c5c5ab19e33f2c6b"},"cell_type":"code","source":"# Check the correlation for the current numeric feature set.\nprint(train[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr())\nsns.heatmap(train[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr(), annot=True, fmt = \".2f\", cmap = \"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8acae6838a902ef6787892b69098d1ab5a1bed99"},"cell_type":"markdown","source":"**Step 2: Exploring the data**\n\nWe will explore the features in a sequential manner since the number of features is not big."},{"metadata":{"trusted":true,"_uuid":"a773d35c884107ab13bd665ecdfb0e0f5b9e680a"},"cell_type":"code","source":"# List the features again\nprint(train.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e562aa01e8c11d5a48c182e177b93e349c6f3a19"},"cell_type":"markdown","source":"**Pclass:** It has a negative correlation of -0.33 which means that increase in Pclass leads to decrease in Survived. We group the data by passing the Feature as an attribute to the [groupby](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) method."},{"metadata":{"trusted":true,"_uuid":"f070e44d258f15002d477c841de45526801f542f"},"cell_type":"code","source":"# Lets see the relation between Pclass and Survived\nprint(train[['Pclass', 'Survived']].groupby(['Pclass']).mean())\nsns.catplot(x='Pclass', y='Survived',  kind='bar', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"201ee4960c0214ee2e64d05b7ab0d63a1a9e3dca"},"cell_type":"markdown","source":"The passengers in a higher class definitely had a much higher chance of survival."},{"metadata":{"_uuid":"340b4861a54c7eb765e1529504117a2326ff24ee"},"cell_type":"markdown","source":"**Sex:**\nDoes gender play a part too?"},{"metadata":{"trusted":true,"_uuid":"f32e2c55c774e320c76ee9853d27003a5a18d7df"},"cell_type":"code","source":"print(train[['Sex', 'Survived']].groupby(['Sex']).mean())\nsns.catplot(x='Sex', y='Survived',  kind='bar', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a037ff3bfb4843c82fd0f4a9a3b0a19b1793bca5"},"cell_type":"markdown","source":"Women had a much higher chance of survival than men. Let's see the impact of Pclass and Sex together on Survived."},{"metadata":{"trusted":true,"_uuid":"1b64eb6c73fd5ba931da39b39d6c84716e9787e1"},"cell_type":"code","source":"sns.catplot(x='Sex', y='Survived',  kind='bar', data=train, hue='Pclass')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8df20d6ee8a6749b2de72b2b0450e1660d176fc1"},"cell_type":"markdown","source":"No wonder Jack died and Rose lived in the movie :)"},{"metadata":{"_uuid":"232e7394cd494a5d09ca55d3ae7b00fa3ae60a19"},"cell_type":"markdown","source":"**Fare:** Let's check fare next because it is closely correlated to Pclass. "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4702c981400e8c1877f0a47858e62b90133b06e3"},"cell_type":"code","source":"g = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Fare\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b46b7adb5b0ee036ac24756d378f0ade192006fc"},"cell_type":"markdown","source":"We notice that Fare for Survived = 0 is heavily skewed towards the lower end. Also for Survived = 1, we see that Fare has a long trail towards the right which shows that passenegers who paid very high fares were much safer."},{"metadata":{"trusted":true,"_uuid":"bdd1b06c767515727399e88911f373ef84e0240d"},"cell_type":"code","source":"group = pd.cut(train.Fare, [0,50,100,150,200,550])\npiv_fare = train.pivot_table(index=group, columns='Survived', values = 'Fare', aggfunc='count')\npiv_fare.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd67cf85860a74e9a1c1750f23ebafdc1265495b"},"cell_type":"markdown","source":"As the fare increases, so does the chances of survival."},{"metadata":{"_uuid":"39c50db814a4d138a51e452c4d3277e33bde53ae"},"cell_type":"markdown","source":"**Age:** Age had a low inverse correlation with Survived(-0.07). Is it really useful?"},{"metadata":{"trusted":true,"_uuid":"b12dae7204524c7a25dd9655cc7d4817acbcd8fe"},"cell_type":"code","source":"g = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22ea0a87a49b4e84f0008edf412d6d2ca12d79a7"},"cell_type":"markdown","source":"We can glean that children had a high probability of Survival."},{"metadata":{"trusted":true,"_uuid":"35d1bff493f92f3331e0dc62605e9a3757e6b083"},"cell_type":"code","source":"group = pd.cut(train.Age, [0,14,30,60,100])\npiv_fare = train.pivot_table(index=group, columns='Survived', values = 'Age', aggfunc='count')\npiv_fare.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c6d5bb3bbbef7f1aa4f29277ffc63fd1b4c04eb"},"cell_type":"markdown","source":"Through this bar plot we can confirm that children did have a higher chance of survival even though Age as a whole is not a strongly correlated feature with Survived. "},{"metadata":{"_uuid":"429f326206a144e647f53a17c4d5a8a632367087"},"cell_type":"markdown","source":"**Embarked:** Did the embarkation location play any part?"},{"metadata":{"trusted":true,"_uuid":"260614fc7c78520e5ea488eb80120a6677e418d3"},"cell_type":"code","source":"print(train[['Embarked', 'Survived']].groupby(['Embarked']).mean())\nsns.catplot(x='Embarked', y='Survived',  kind='bar', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a3749fc79fdff55e3e79bbf7d033424ffb033f65"},"cell_type":"code","source":"sns.catplot('Pclass', kind='count', col='Embarked', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baa3168e42eb8a6283603cd8af19a07546f866b5"},"cell_type":"markdown","source":"So people who Embarked from C had the highest chance of survival because most of them where of Pclass 1 which has the highest Survival rate."},{"metadata":{"_uuid":"3a0606efc841202284bf0e16af9493464adb471e"},"cell_type":"markdown","source":"**SibSp and Parch**\nBoth of these features are related to the family size of the passengers."},{"metadata":{"trusted":true,"_uuid":"d930ac7f6c8065a0d0304108089a3e7458ee2c8e"},"cell_type":"code","source":"print(train[['SibSp', 'Survived']].groupby(['SibSp']).mean())\nsns.catplot(x='SibSp', y='Survived', data=train, kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94fd7123ba7ab475d924b64702cd949d458d1957"},"cell_type":"markdown","source":"People with 0-2 SibSp(Sibling/Spouse) have a higher chance of survival."},{"metadata":{"trusted":true,"_uuid":"c1eebd03bc00c325e4faba5982f62b76aa896892"},"cell_type":"code","source":"print(train[['Parch', 'Survived']].groupby(['Parch']).mean())\nsns.catplot(x='Parch', y='Survived', data=train, kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81dca96f36d36036fd1bf6561aa421bbd40aaf88"},"cell_type":"markdown","source":"We can pull two types of information from this data easily, the family size(FamilySize) and whether the person is travelling alone(IsAlone)."},{"metadata":{"_uuid":"27e56b2ffa67ef7c0989a5b46eb78f15280956c3"},"cell_type":"markdown","source":"**Name**\n\nAlthough the name data might seem inconsequential but there are useful insights that we can get from this data. The titles from the name data can give us an indication of the social/economic standing of the passenger."},{"metadata":{"_uuid":"28a958892f3db152a2be115a19d807f4a888f2e5"},"cell_type":"markdown","source":"I have used [str](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.html) method to get the StringMethods object over the Series and [split](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html) to split the strings around the separators.\nPandas documentation has an easy to understand page titled \"[Working with Text Data](https://pandas.pydata.org/pandas-docs/stable/text.html)\" which would bring you upto the speed quickly."},{"metadata":{"trusted":true,"_uuid":"85212357e521a25e643247d982913d61a68c21ba"},"cell_type":"code","source":"# Explanation for the next code block used to get the Titles\n# Using this output to explain the string manipulation below\nprint(train.Name.head(1))\nprint()\n# The above returns a single name for e.g. Braund, Mr. Owen Harris.\n# Calling str returns a String object\nprint(train.Name.head(1).str)\nprint()\n# Next we split the string into a List with a comma as the separator\nprint(train.Name.head(1).str.split(','))\nprint()\n# Similary we remove the . and then strip the remaining string to get the title.\n# We pick the second item of the\nprint(train.Name.head(1).str.split(',').str[1])\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf0595d0c653ce8279fe98d7be53ab434e2796f9"},"cell_type":"code","source":"# Get the titles\nfor dataset in [train, validation]:\n    # Use split to get only the titles from the name\n    dataset['Title'] = dataset['Name'].str.split(',').str[1].str.split('.').str[0].str.strip()\n    # Check the initial list of titles.\n    print(dataset['Title'].value_counts())\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa18e94b7cc9851a0cf833d74c7b26fbb3c370ff","trusted":true},"cell_type":"code","source":"sns.catplot(x='Survived', y='Title', data=train, kind ='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1b8c532c313f80eb2a5dcbc92c3c9e26e9e138d"},"cell_type":"markdown","source":"We again notice that the Titles used for females have a much higher rate of survival."},{"metadata":{"_uuid":"9148c3f5d8cf450a29dbde965b053b14558a3a4c"},"cell_type":"markdown","source":"**Ticket**\n\nWIP. We will drop this from the dataset for now."},{"metadata":{"_uuid":"b3bbe92f3fa1444c00718a003a9753ef56a7aa92"},"cell_type":"markdown","source":"**Step 3: Feature Engineering and Processing**"},{"metadata":{"_uuid":"53a9469007122afafd7dae374db667b987d9ac85"},"cell_type":"markdown","source":"**Fixing the missing data : **\nFirst, we will focus on removing the null fields. Lets see all the features with nulls again."},{"metadata":{"trusted":true,"_uuid":"d181b1cd715ac9c6b652612366a6cb93443b1376"},"cell_type":"code","source":"for df in [train, validation]:\n    print(df.shape)\n    print()\n    print(df.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4238d0f2ea3a2e5eaa26e2f92c08c4e37a0174c"},"cell_type":"markdown","source":"**Embarked:** I decided to drop the passengers who didn't embark since modeling based on their data would act like noise in my opinion. I feel that they can't reliably tell us about the survived/not survived ouptut.\nCross verification - Using Embarked.mode() in fillna to handle the 2 nulls in train, lowered my accuracy to 0.77 from 0.79."},{"metadata":{"trusted":true,"_uuid":"ea9a903c491cb0d2308b8ae4fc03835b5c843958","scrolled":true},"cell_type":"code","source":"# Drop rows with nulls for Embarked\nfor df in [train, validation]:\n    df.dropna(subset = ['Embarked'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d602989426e9a9780b7f4a9b04f57a6df330198b"},"cell_type":"markdown","source":"**Fare**\n\nWe saw earlier that the validation dataset has a null Fare. The train dataset has no \nnull fares."},{"metadata":{"trusted":true,"_uuid":"b598ecf6b0f108b63165d13f80683230a4cee7ca","scrolled":false},"cell_type":"code","source":"print(train[train['Fare'].isnull()])\nprint() \n# 1 row with null Fare in validation\nprint(validation[validation['Fare'].isnull()])\n# We can deduce that Pclass should be related to Fares.\nsns.catplot(x='Pclass', y='Fare', data=validation, kind='point')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d0a891c5ae0caffe38a3f752a89e55aafcf578a"},"cell_type":"code","source":"# There is a clear relation between Pclass and Fare. We can use this information to impute the missing fare value.\n# We see that the passenger is from Pclass 3. So we take a median value for all the Pclass 3 fares.\nvalidation['Fare'].fillna(validation[validation['Pclass'] == 3].Fare.median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c714812ca14931579cc79e314f1f55078750c6e"},"cell_type":"markdown","source":"**Age**\n\nOne quick way to impute the missing ages would be to take median of the Age data from the and apply it to all using [fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html). An alternative approach that I have used with better results is to impute ages based on passeneger titles. This is because of the inherent age related meaning assigned to titles like Master, Mr, Miss, Mrs etc. This lets us assign age more accurately for individual passengers."},{"metadata":{"trusted":true,"_uuid":"96c48143b4179ecff51d5a9dc01cf41ba405853a"},"cell_type":"code","source":"print(train[['Age','Title']].groupby('Title').mean())\nsns.catplot(x='Age', y='Title', data=train, kind ='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c6e85b363eb95f7d10132468ab82f9420fc4bef"},"cell_type":"markdown","source":"We can see that different titles have different age means."},{"metadata":{"trusted":true,"_uuid":"864c790d9c3906e8123dbe207f4ce74b308b00f1"},"cell_type":"code","source":"# Returns titles from the passed in series.\ndef getTitle(series):\n    return series.str.split(',').str[1].str.split('.').str[0].str.strip()\n# Prints the count of titles with nulls for the train dataframe.\nprint(getTitle(train[train.Age.isnull()].Name).value_counts())\n# Fill Age median based on Title\nmr_mask = train['Title'] == 'Mr'\nmiss_mask = train['Title'] == 'Miss'\nmrs_mask = train['Title'] == 'Mrs'\nmaster_mask = train['Title'] == 'Master'\ndr_mask = train['Title'] == 'Dr'\ntrain.loc[mr_mask, 'Age'] = train.loc[mr_mask, 'Age'].fillna(train[train.Title == 'Mr'].Age.mean())\ntrain.loc[miss_mask, 'Age'] = train.loc[miss_mask, 'Age'].fillna(train[train.Title == 'Miss'].Age.mean())\ntrain.loc[mrs_mask, 'Age'] = train.loc[mrs_mask, 'Age'].fillna(train[train.Title == 'Mrs'].Age.mean())\ntrain.loc[master_mask, 'Age'] = train.loc[master_mask, 'Age'].fillna(train[train.Title == 'Master'].Age.mean())\ntrain.loc[dr_mask, 'Age'] = train.loc[dr_mask, 'Age'].fillna(train[train.Title == 'Dr'].Age.mean())\n# Prints the count of titles with nulls for the train dataframe. -- Should be empty this time.\nprint()\nprint(getTitle(train[train.Age.isnull()].Name).value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"557fb03dfe73801204739f4d5c440119396e2b73"},"cell_type":"code","source":"# Prints the count of titles with nulls for the validation dataframe.\nprint(getTitle(validation[validation.Age.isnull()].Name).value_counts())\n# Fill Age median based on Title\nmr_mask = validation['Title'] == 'Mr'\nmiss_mask = validation['Title'] == 'Miss'\nmrs_mask = validation['Title'] == 'Mrs'\nmaster_mask = validation['Title'] == 'Master'\nms_mask = validation['Title'] == 'Ms'\nvalidation.loc[mr_mask, 'Age'] = validation.loc[mr_mask, 'Age'].fillna(validation[validation.Title == 'Mr'].Age.mean())\nvalidation.loc[miss_mask, 'Age'] = validation.loc[miss_mask, 'Age'].fillna(validation[validation.Title == 'Miss'].Age.mean())\nvalidation.loc[mrs_mask, 'Age'] = validation.loc[mrs_mask, 'Age'].fillna(validation[validation.Title == 'Mrs'].Age.mean())\nvalidation.loc[master_mask, 'Age'] = validation.loc[master_mask, 'Age'].fillna(validation[validation.Title == 'Master'].Age.mean())\nvalidation.loc[ms_mask, 'Age'] = validation.loc[ms_mask, 'Age'].fillna(validation[validation.Title == 'Miss'].Age.mean())\n# Prints the count of titles with nulls for the validation dataframe. -- Should be empty this time.\nprint(getTitle(validation[validation.Age.isnull()].Name).value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbef0da985472daecbbbe5f81b0b25968387b0d1"},"cell_type":"code","source":"# train.Age.fillna(train.Age.median(), inplace=True)\n# validation.Age.fillna(validation.Age.median(), inplace=True)\nprint(train.isna().sum())\nprint(validation.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b08736906ca78cde55105c96688016e993241325"},"cell_type":"markdown","source":"We will begin by dropping the columns that we are not using."},{"metadata":{"trusted":true,"_uuid":"8c2b491b15ebdb5e604552b06f291f75f6fda79f"},"cell_type":"code","source":"train.drop(columns=['PassengerId'], inplace = True)\n[df.drop(columns=['Ticket'], inplace = True) for df in [train, validation]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76e45b219796271bf5024aac3907503b31a6160a"},"cell_type":"markdown","source":"Next up, we will encode all the categorical features. One of things to lookout for is the Dummy Trap. We have the option for using \"drop_first = True\" with get_dummies but here we will manually select the features and intentionally leave one out based on their correlations."},{"metadata":{"trusted":true,"_uuid":"51511db57fdb4b69999601c7449e20b83e8860bf"},"cell_type":"code","source":"[train, validation] = [pd.get_dummies(data = df, columns = ['Pclass', 'Sex', 'Embarked']) for df in [train, validation]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3903c28a9993a0d87e888731e693c23ca51262b"},"cell_type":"markdown","source":"We will convert the Cabin data into a flag about whether a passenger had an assigned cabin or not. Also we will use SibSp and Parch to calculate the Family Size and a flag named IsAlone."},{"metadata":{"trusted":true,"_uuid":"0f33ea2d16a467ca5c3a3da0a2952a3665587a29"},"cell_type":"code","source":"for df in [train, validation]:\n    df['HasCabin'] = df['Cabin'].notna().astype(int)\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['IsAlone'] = (df['FamilySize'] > 1).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77a8b4eb170f988b4a91e2e8a0e1c093cc7542fa"},"cell_type":"code","source":"[df.drop(columns=['Cabin', 'SibSp', 'Parch'], inplace = True) for df in [train, validation]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e570cde36ce7c008c32b7c5d7c060c1260046c7b"},"cell_type":"code","source":"# We see that there are a few non standard titles. Some of them are just French titles\n# with the same meaning as in English while others point to people who would probably\n# have more privileges or military training etc and can be placed in a separate category.\n# French titles - https://en.wikipedia.org/wiki/French_honorifics\n# Mlle - https://en.wikipedia.org/wiki/Mademoiselle_(title)\n# Mme - https://en.wikipedia.org/wiki/Madam\n# Mme was a bit harder to understand as Wikipedia says that its used for adult women\n# but doesn't given any pointers towards their marital status.\n# Searching up on Google and considering that the title is used for adult women\n# we can assume that this title was usually assigned to married women.\n# https://www.frenchtoday.com/blog/french-culture/madame-or-mademoiselle-a-delicate-question\n# Ms - An alternate abbrevation for Miss\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss').replace('Ms', 'Miss').replace('Mme', 'Mrs').replace(['Dr', 'Major', 'Col', 'Rev', 'Lady', 'Jonkheer', 'Don', 'Sir', 'Dona', 'Capt', 'the Countess'], 'Special')\nvalidation['Title'] = validation['Title'].replace('Mlle', 'Miss').replace('Ms', 'Miss').replace('Mme', 'Mrs').replace(['Dr', 'Major', 'Col', 'Rev', 'Lady', 'Jonkheer', 'Don', 'Sir', 'Dona', 'Capt', 'the Countess'], 'Special')\n[df.drop(columns=['Name'], inplace = True) for df in [train, validation]]\n[train, validation] = [pd.get_dummies(data = df, columns = ['Title']) for df in [train, validation]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"955c4352de14e3cf86ab7085c398afb34b32a65e"},"cell_type":"code","source":"# Check the updated dataset\nprint(train.columns.values)\nprint(validation.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee326415773ea4b8f4fc8be7c93de2d8c506da61"},"cell_type":"code","source":"# Check the correlation with the updated datasets\ntrain.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6b0ac269139db436172749f5af3ca71b0e2039d"},"cell_type":"code","source":"# Split the the dataset into train and test sets.\nfrom sklearn.model_selection import train_test_split\n# Use only the features with a coeefficient greater than 0.3\nX = train[['Age', 'Fare', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Embarked_C',\n       'Embarked_S', 'HasCabin', 'FamilySize', 'Title_Master', 'Title_Mr',\n       'Title_Mrs', 'Title_Special']]\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\nprint(X_train.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc180edda7b510cb4962a5679d883c5ba98c7894"},"cell_type":"code","source":"# We will also create a base model to check the goodness of our model.\n# First we see the actual number of survivors\nprint(y.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ec6ca90aa53c79254c074a1b08b0fa3bee0ca23"},"cell_type":"code","source":"# We will select the larger number and consider that everyone dies to create a baseline.\ny_default = pd.Series([0] * train['Survived'].shape[0], name = 'Survived')\nprint(y_default.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca587b8d2cb36363e4f9f29e125b5e28ee360b3a"},"cell_type":"code","source":"# Calculate the baseline\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\nprint(confusion_matrix(y, y_default))\nprint()\nprint(accuracy_score(y, y_default))\n# So if we assumed that everyone dies we would be correct 61% of the time.\n# So this is the bare minimun level of accuracy our prediciton should aim to improve upon.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2776b261ef0443e44435a88e8c2004db00ccbea"},"cell_type":"markdown","source":"**3.1 Choosing an extimator**\nThere are a lot of algorithms for classification and just iterating over all classification models may not be feasible. We need to shortlist a few algorithms that we can then apply and then later tune further for better results. I am going to refer to the [cheat sheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) provided in sklearn documentation."},{"metadata":{"trusted":true,"_uuid":"0118cd1150a498d6385bb05952d102769e4a2ccb"},"cell_type":"code","source":"# First attempt with LinearSVC\nfrom sklearn.svm import LinearSVC\n# Looking into the documentation points us to set dual=False for cases with n_samples > n_features.\nclassifier = LinearSVC(dual=False)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\nscores = cross_val_score(classifier, X_train, y_train, cv=10, scoring='accuracy')\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fdc870c9216f5039209845b071789743396b6cb"},"cell_type":"code","source":"# Next up we will try KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 2)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\nscores = cross_val_score(classifier, X_train, y_train, cv=10, scoring='accuracy')\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"671bb61bc2a973abb809a92c4c4db9bcc0cc4edb"},"cell_type":"code","source":"# KNN isn't useful for us so we now move to a few popular ensemble estimators\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\nprint(\"AdaBoostClassifier\")\nada_boost_classifier = AdaBoostClassifier()\nada_boost_classifier.fit(X_train, y_train)\ny_pred = ada_boost_classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\nscores = cross_val_score(ada_boost_classifier, X_train, y_train, cv=10, scoring='accuracy')\nprint(scores.mean())\nprint(\"BaggingClassifier\")\nbagging_classifier = BaggingClassifier()\nbagging_classifier.fit(X_train, y_train)\ny_pred = bagging_classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\nscores = cross_val_score(bagging_classifier, X_train, y_train, cv=10, scoring='accuracy')\nprint(scores.mean())\nprint(\"ExtraTreesClassifier\")\nextra_trees_classifier = ExtraTreesClassifier(n_estimators=100)\nextra_trees_classifier.fit(X_train, y_train)\ny_pred = extra_trees_classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\nscores = cross_val_score(extra_trees_classifier, X_train, y_train, cv=10, scoring='accuracy')\nprint(scores.mean())\nprint(\"GradientBoostingClassifier\")\ngradient_boosting_classifier = GradientBoostingClassifier()\ngradient_boosting_classifier.fit(X_train, y_train)\ny_pred = gradient_boosting_classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\nscores = cross_val_score(gradient_boosting_classifier, X_train, y_train, cv=10, scoring='accuracy')\nprint(scores.mean())\nprint(\"RandomForestClassifier\")\nrandom_forest_classifier = RandomForestClassifier(n_estimators=100)\nrandom_forest_classifier.fit(X_train, y_train)\ny_pred = random_forest_classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\nscores = cross_val_score(random_forest_classifier, X_train, y_train, cv=10, scoring='accuracy')\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6ed248319c13a2acbdf8a87509fb1e9c47237f8"},"cell_type":"code","source":"# Will also try XGB based on its popularity and relevance here.\nfrom xgboost import XGBClassifier\nxgboost_classifier = XGBClassifier()\nxgboost_classifier.fit(X_train, y_train)\ny_pred = xgboost_classifier.predict(X_test)\n# Print the confusion matrix\n# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\nprint(confusion_matrix(y_test, y_pred))\n# Print the accuracy score\n# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\nprint(accuracy_score(y_test, y_pred))\nscores = cross_val_score(xgboost_classifier, X_train, y_train, cv=10, scoring='accuracy')\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d83eefce59d17526ae44cbafe70fea81a2ff0c6c"},"cell_type":"code","source":"# X = train.iloc[:, 1:]\n# y = train.iloc[:, 0]\n# print(X.columns.values)\n# xgboost_classifier = XGBClassifier()\n# from sklearn.feature_selection import RFECV\n# rfecv = RFECV(estimator=xgboost_classifier, cv=10, scoring='accuracy')\n# rfecv = rfecv.fit(X, y)\n# print(X.columns[rfecv.support_])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abc24fd585c63e646aada699c4763316f454e2c9"},"cell_type":"code","source":"# Now we will pass the validation set provided for creating our submission\n# Pick the same columns as in X_test\nX_validation = validation[['Age', 'Fare', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Embarked_C',\n       'Embarked_S', 'HasCabin', 'FamilySize', 'Title_Master', 'Title_Mr',\n       'Title_Mrs', 'Title_Special']]\n# Call the predict from the created classifier\ny_valid = xgboost_classifier.predict(X_validation)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a9a55d597ad0e8914aad6cd484a62fcc10f0980"},"cell_type":"markdown","source":"**Step 4: Submission**"},{"metadata":{"trusted":true,"_uuid":"a45991a9a89d1e1bdbd3ceb1f075e52a63a65e5f"},"cell_type":"code","source":"print(validation.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a07b101fa4c5a19d3a02fa840b87ed18d27ca29f"},"cell_type":"code","source":"# Creating final output file\nvalidation_pId = validation.loc[:, 'PassengerId']\nmy_submission = pd.DataFrame(data={'PassengerId':validation_pId, 'Survived':y_valid})\nprint(my_submission['Survived'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35ce5acabc16d6e8289c0678758e8b74c4ef5a91"},"cell_type":"code","source":"my_submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d22e0c7e5d84fbefbb591dcf969313847152427"},"cell_type":"markdown","source":"We can now commit and open the saved kernel and then click on the \"Output\" tab to submit the generated output to get our accuracy score."},{"metadata":{"_uuid":"2137bac4b1995effa08b9259ad21b978d44128ec"},"cell_type":"markdown","source":"Current Acurracy = 0.79425\n\nMajor tasks pending-\n1. Use Ticket.\n2. Try other models and model tuning.\n3. Try using the entire train data for training. This should theoretically improve the accuracy further as more data will be used to train the final model.\n    Using a smaller split for train_test_split seems to confirm this. Changing split size from 0.2 to 0.1 increases the accuracy slightly.\n    \nFailed approaches - Ideas that I thought were good but actually either didn't affect or reduced the accuracy.\n1. Use Titles to assign missing Age by using medians. For e.g. calculate Age.median() of all passengers that have the title 'Mr' and then use this median to fill null values for Age with Mr in Title.\nThis reduced accuracy to little over 77% from 78.47%. Using mean instead of median on the other hand increased accuracy to 79.42%.\nAlthough median is more \"robust\"(https://creativemaths.net/blog/median/), mean here was better probably because Age occurs here as a Gaussian distribution(https://www.quora.com/What-is-more-accurate-the-median-or-mean).\nEasy explanation - https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}