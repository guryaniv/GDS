{"metadata": {"language_info": {"pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.3", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["\n", "Based on work done on this notebook, https://www.kaggle.com/sudosudoohio/stratified-kfold-xgboost-eda-tutorial-0-281.\n", "Filled in missing values with Median and  the public score increased from 0.281 to 0.285 after training with new data. \n", "Also, custom weights are given to the labels to tackle imbalanced labels in the data. However, performance hasn't improved.\n", "\n", "Hope you will find the notebook helpful! .\n"], "metadata": {"_cell_guid": "49635227-fe34-4a12-845e-12fd7ed9b3bf", "_uuid": "fffbdd4f2428560a066c240064a3c60c6a46edbe"}}, {"cell_type": "code", "execution_count": null, "source": ["## Importing Requried Libraries\n", "import numpy as np\n", "import subprocess\n", "import pandas as pd\n", "\n", "from IPython.display import Image\n", "\n", "from collections import Counter\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "from sklearn.model_selection import StratifiedKFold\n", "import xgboost as xgb\n", "\n", "from sklearn.datasets import make_classification\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn.metrics import log_loss, accuracy_score\n", "\n", "# classifiers\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "\n", "# reproducibility\n", "seed = 123"], "outputs": [], "metadata": {"_cell_guid": "1899d740-4d31-4e2b-90d1-f5f5f2e88056", "_uuid": "896e8658c6a2145b6654a4130e72a6a89b491d96", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["#### LOADING DATA ####\n", "### TRAIN DATA\n", "train_data = pd.read_csv(\"../input/train.csv\", na_values='-1')\n", "cor_data = train_data.copy()\n", "                        \n", "## Filling the missing data NAN with median of the column\n", "train_data_nato_median = pd.DataFrame()\n", "for column in train_data.columns:\n", "    train_data_nato_median[column] = train_data[column].fillna(train_data[column].median())\n", "\n", "train_data = train_data_nato_median.copy()\n", "\n", "### TEST DATA\n", "test_data = pd.read_csv(\"../input/test.csv\", na_values='-1')\n", "## Filling the missing data NAN with mean of the column\n", "test_data_nato_median = pd.DataFrame()\n", "for column in test_data.columns:\n", "    test_data_nato_median[column] = test_data[column].fillna(test_data[column].median())\n", "    \n", "test_data = test_data_nato_median.copy()\n", "test_data_id = test_data.pop('id')"], "outputs": [], "metadata": {"_cell_guid": "708ddfef-023b-490e-ae61-e467e283802a", "_uuid": "67f598fd71cd27c05a31b57cf9e7697d1482c767", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["## Identifying Categorical data\n", "column_names = train_data.columns\n", "categorical_column = column_names[column_names.str[10] == 'c']\n", "\n", "## Changing categorical columns to category data type\n", "def int_to_categorical(data):\n", "    \"\"\" \n", "    changing columns to catgorical data type\n", "    \"\"\"\n", "    for column in categorical_column:\n", "        data[column] =  data[column].astype('category')"], "outputs": [], "metadata": {"_cell_guid": "d1f96e8b-fb3c-40b1-8092-12b5ec9c93ea", "_uuid": "e4a9310d991ee8ecb21360c4654d1864a5425897", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["## Creating list of train and test data and converting columns of interest to categorical type\n", "datas = [train_data,test_data]\n", "\n", "for data in datas:\n", "    int_to_categorical(data)\n", "\n", "#print(test_data.dtypes)"], "outputs": [], "metadata": {"_cell_guid": "6743fa84-191d-4608-ae93-900305d22801", "_uuid": "61ba13b4285b66805e134ea8392680a1629b7480", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["## Decribing categorical variables\n", "# def decribe_Categorical(x):\n", "#     \"\"\" \n", "#     Function to decribe Categorical data\n", "#     \"\"\"\n", "#     from IPython.display import display, HTML\n", "#     display(HTML(x[x.columns[x.dtypes ==\"category\"]].describe().to_html))\n", "\n", "# decribe_Categorical(train_data)"], "outputs": [], "metadata": {"_cell_guid": "3134726c-67bb-465e-8231-918577724631", "_uuid": "8fae15b916487e22cb01d4ea0a9012cc2d5e09ac", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["### FUNCTION TO CREATE DUMMIES COLUMNS FOR CATEGORICAL VARIABLES\n", "def creating_dummies(data):\n", "    \"\"\"creating dummies columns categorical varibles\n", "    \"\"\"\n", "    for column in categorical_column:\n", "        dummies = pd.get_dummies(data[column],prefix=column)\n", "        data = pd.concat([data,dummies],axis =1)\n", "        ## dropping the original columns ##\n", "        data.drop([column],axis=1,inplace= True)"], "outputs": [], "metadata": {"_cell_guid": "6f0bdecf-8461-4736-88b8-ae41c6aea95c", "_uuid": "763aef1c801f3f20b7365bf13ac6410925377956", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["### CREATING DUMMIES FOR CATEGORICAL VARIABLES  \n", "for column in categorical_column:\n", "        dummies = pd.get_dummies(train_data[column],prefix=column)\n", "        train_data = pd.concat([train_data,dummies],axis =1)\n", "        train_data.drop([column],axis=1,inplace= True)\n", "\n", "\n", "for column in categorical_column:\n", "        dummies = pd.get_dummies(test_data[column],prefix=column)\n", "        test_data = pd.concat([test_data,dummies],axis =1)\n", "        test_data.drop([column],axis=1,inplace= True)\n", "\n", "print(train_data.shape)\n", "print(test_data.shape)"], "outputs": [], "metadata": {"_cell_guid": "e3d43674-aff5-4b79-88dd-d91279fc02b1", "_uuid": "368485254ab1ac1140282022ca6dccb2cd4b5008", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["#Define covariates in X and dependent variable in y\n", "features = train_data.iloc[:,2:] ## FEATURE DATA\n", "targets= train_data.target ### LABEL DATA\n", "\n", "### CHECKING DIMENSIONS\n", "print(features.shape)\n", "print(targets.shape)"], "outputs": [], "metadata": {"_cell_guid": "949e5932-c260-4ad2-a6de-aec29f28f5e4", "_uuid": "b86c7493a3549c27d4b10d4a00cbd2efd112daf0", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["ax = sns.countplot(x = targets ,palette=\"Set2\")\n", "sns.set(font_scale=1.5)\n", "ax.set_xlabel(' ')\n", "ax.set_ylabel(' ')\n", "fig = plt.gcf()\n", "fig.set_size_inches(10,5)\n", "ax.set_ylim(top=700000)\n", "for p in ax.patches:\n", "    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(targets)), (p.get_x()+ 0.3, p.get_height()+10000))\n", "\n", "plt.title('Distribution of 595212 Targets')\n", "plt.xlabel('Initiation of Auto Insurance Claim Next Year')\n", "plt.ylabel('Frequency [%]')\n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "7df178da-7c3a-45e9-865e-6abb6c3bcc44", "_uuid": "3966583665e454cebacf83df86c1a672fa383d12", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["sns.set(style=\"white\")\n", "\n", "\n", "# Compute the correlation matrix\n", "corr = cor_data.corr()\n", "\n", "\n", "# Set up the matplotlib figure\n", "f, ax = plt.subplots(figsize=(11, 9))\n", "\n", "# Generate a custom diverging colormap\n", "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n", "\n", "# Draw the heatmap with the mask and correct aspect ratio\n", "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n", "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n", "\n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "2b32c1b6-ea9e-4c26-b7bf-95459927e99c", "_uuid": "dd467e34ba8e6d864d647305c6fb21537f18b565", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n", "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n", "    assert( len(actual) == len(pred) )\n", "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n", "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n", "    totalLosses = all[:,0].sum()\n", "    giniSum = all[:,0].cumsum().sum() / totalLosses\n", "    \n", "    giniSum -= (len(actual) + 1) / 2.\n", "    return giniSum / len(actual)\n", " \n", "def gini_normalized(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = gini_normalized(labels, preds)\n", "    return 'gini', gini_score"], "outputs": [], "metadata": {"_cell_guid": "123d5459-dbe8-4df4-b0a5-4ad94f036fa9", "_uuid": "0c5c0462ff9983bdcf1c89c884c0fc9442e4e56b", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["unwanted = features.columns[features.columns.str.startswith('ps_calc_')]\n", "print(unwanted)"], "outputs": [], "metadata": {"_cell_guid": "fbfa83d5-2f86-4365-b4a7-378889ed4b17", "_uuid": "a77fa700d92f39ac0f023b0b2c7d66c6b4f59daa", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["train = features.drop(unwanted, axis=1)  \n", "test = test_data.drop(unwanted, axis=1)  \n", "\n", "print(train.shape)\n", "print(test.shape)"], "outputs": [], "metadata": {"_cell_guid": "5d643323-6d06-4749-84c2-ca274798c2e2", "_uuid": "1db3335c21c84f9a553528e3808b2ba5d8d97bca", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["## Spliting train data \n", "kfold = 2 ## I used 5 Kfolds. In interest of computational time using 2\n", "skf = StratifiedKFold(n_splits=kfold, random_state=42)"], "outputs": [], "metadata": {"_cell_guid": "57912ae9-09de-4377-b051-255bb6a041ba", "_uuid": "3c2ece821dc84ba4ad6fa2a9a1766d1dc34e1318", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["## Specifiying parameters\n", "params = {\n", "    'min_child_weight': 10.0,\n", "    'objective': 'binary:logistic',\n", "    'max_depth': 7,\n", "    'max_delta_step': 1.8,\n", "    'colsample_bytree': 0.4,\n", "    'subsample': 0.8,\n", "    'eta': 0.025,\n", "    'gamma': 0.65,\n", "    'num_boost_round' : 700\n", "    }"], "outputs": [], "metadata": {"_cell_guid": "fc99b487-5f10-4ea3-b5cf-1e089958156d", "_uuid": "39cd5d7e4980878ebb0f44c4bc3c3066ad9e0ce4", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["# Converting pandas series to numpy array to be fed to XGBoost package\n", "X= train.values\n", "y = targets.values\n", "\n", "type(X),type(y)"], "outputs": [], "metadata": {"_cell_guid": "c086e7e0-b1fb-402f-9139-0c2f5453d613", "_uuid": "8a33fb3638a0fcc0f426226a39f44d6766f7fcfa", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["# Submission data frame\n", "sub = pd.DataFrame()\n", "sub['id'] = test_data_id\n", "sub['target'] = np.zeros_like(test_data_id)\n", "sub.shape"], "outputs": [], "metadata": {"_cell_guid": "18f4994b-7519-4e86-a0c2-5472e952dfa7", "_kg_hide-output": false, "_uuid": "17f733b2d78191c0aec31f70630bc02cfd6425bd", "collapsed": true, "_kg_hide-input": false}}, {"cell_type": "markdown", "source": ["We are providing custom weights to the labels in the data. Here I used two ways to do it,\n", "1. Manually assigning weights to the label as by numpy array 'weights' in the code\n", "2. using 'scale_pos_weights' parameter"], "metadata": {"_cell_guid": "0adb0108-b6f0-4a8d-91c2-90bd0eed79f1", "_uuid": "9bd061364e5f5abf9677ad28988b1be175b64cde"}}, {"cell_type": "code", "execution_count": null, "source": ["## Model fitting\n", "# THis is where the magic happens\n", "\n", "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n", "    print('[Fold %d/%d]' % (i + 1, kfold))\n", "    X_train, X_valid = X[train_index], X[test_index]\n", "    y_train, y_valid = y[train_index], y[test_index]\n", "    ### Custom weights: To deal with imbalanced label\n", "    #weights = np.zeros(len(y_train))\n", "    #weights[y_train == 0] = 1\n", "    #weights[y_train == 1] = 9            \n", "    #d_train = xgb.DMatrix(X_train, label = y_train, weight = weights)\n", "    \n", "    \n", "    d_train = xgb.DMatrix(X_train, label = y_train)\n", "    d_valid = xgb.DMatrix(X_valid, y_valid)\n", "    d_test = xgb.DMatrix(test.values)\n", "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "    \n", "    ### USing Scale_pos_weights parameter\n", "    ## In this case we are splitting giving weights to the label according \n", "    # to their relative ratio's in the data\n", "    \n", "   # train_labels = d_train.get_label()\n", "   # ratio = float(np.sum(train_labels == 0))/ np.sum(train_labels == 1)\n", "   # params['scale_pos_weight'] = ratio\n", "\n", "    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n", "    # and the custom metric (maximize=True tells xgb that higher metric is better)\n", "    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, \n", "                    feval=gini_xgb, maximize=True, verbose_eval=100)\n", "\n", "    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n", "    # Predict on our test data\n", "    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n", "    sub['target'] += p_test/kfold"], "outputs": [], "metadata": {"_cell_guid": "ab801f2a-6051-46b9-a810-ffe2a7f355b9", "_uuid": "8bc351192e885240e800dc1535cfc91948479c68", "collapsed": true}}, {"cell_type": "markdown", "source": ["*OUTPUT*\n", "Adding each layer of topic of the other:\n", "1. Base XGBoost: 0.281\n", "2. XGBoost with missing values filled with Median(equal class weights): 0.285\n", "3. XGbosot with custom weight: 0.282\n", "4. XGboost with scale_pos_weights parameter: 0.280"], "metadata": {"_cell_guid": "3cf1a555-3c28-4e7c-9ac0-4efac6920bd0", "_uuid": "be516852c13a25037e8b7198f41d487cca20c846"}}, {"cell_type": "code", "execution_count": null, "source": ["#sub.to_csv(\"ModifiedXGBOOST.csv\",index =  False)"], "outputs": [], "metadata": {"_cell_guid": "ed0df5da-fbe5-4a47-93a4-f20ad0a494f1", "_uuid": "6023489fd32873f6fd82930bd7c7e8c875544af6", "collapsed": true}}, {"cell_type": "markdown", "source": ["**FUTURE WORK:**\n", "1. Planning on using Gridsearch \n", "2. Dealing with Imbalanced data such as this,\n", "    For class imbalance, we have many methods that could deal with the problem. There is no one method that could essentially solve the issue every single time. Here are few methods that we could use to deal with Class imbalance.\n", "\n", "1. Weighted loss functions: Giving higher weights to minority class(in this case, class 1). For example, Logistic regression model has inbuilt parameter 'class_weights' which can used to assign weights to the class labels as per choice.\n", "2. Random Undersampling: As the name suggests, randomly undersample examples from majority class\n", "3. NEARMISS-1 : Retain points from majority class whose mean distance to the K nearest points in S is lowest\n", "4. NEARMISS-2: Keep points from majority class whose mean distance to the K farthest points in minority class is lowest\n", "5. NEARMISS-3: Select K nearest neighbours In majority class for every point in minority class\n", "6. Condensed Nearest Neighbour(CNN)\n", "7. Edited Nearest Neighbour(ENN)\n", "8. Repeated Edited Nearest Neighbour\n", "9. TOMEK LINK REMOVAL\n", "10. Random Oversampling\n", "11. Synthetic Minority Oversampling Technique(SMOTE)\n", "12. SMOTE + Tomek Link Removal\n", "13. SMOTE + ENN\n", "14. EASYENSEMBLE\n", "15. BALANCECASCADE and many more\n", "\n", "Reference: \n", "1. https://www.youtube.com/watch?v=-Z1PaqYKC1w\n", "2. https://www.youtube.com/watch?v=32tXCJP0HYc&list=PLZnYQQzkMilqTC12LmnN4WpQexB9raKQG&index=13"], "metadata": {"_cell_guid": "366a1d9e-8459-4f2e-9e14-f9f848482e98", "_uuid": "34eed91f65557ef407bd266b6988fc3a4ad60392"}}]}