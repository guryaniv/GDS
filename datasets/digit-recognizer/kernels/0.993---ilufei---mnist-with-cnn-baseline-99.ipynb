{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python.framework import ops\nimport tensorflow as tf\nimport math\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\ntrainset = pd.read_csv('../input/train.csv')\ntestset = pd.read_csv('../input/test.csv')\n\nprint(trainset.shape)\nprint(testset.shape)\n\n# split dataset into tainset and cross-validation set\ntrainset, valset= train_test_split(trainset, test_size = 0.1)\n\n# get pix from trainset\nx_train = trainset.loc[:, \"pixel0\" : \"pixel783\"]\nx_train = x_train.values\n# get label from trainset\ny_train = trainset.loc[:,\"label\"]\ny_train = pd.get_dummies(y_train).values\n\n# get pix from valset\nx_cv = valset.loc[:, \"pixel0\" : \"pixel783\"]\nx_cv = x_cv.values\n# get label from valset\ny_cv = valset.loc[:,\"label\"]\ny_cv = pd.get_dummies(y_cv).values\n\nx_test = testset.values\n\nprint(\"Number of training examples: \" + str(x_train.shape))\nprint(\"Number of cross-validation examples = \" + str(x_cv.shape))\nprint (\"x_train shape: \" + str(x_train.shape))\nprint (\"y_train shape: \" + str(y_train.shape))\nprint (\"y_train sample0: \" + str(y_train[0]))\nprint (\"x_test shape: \" + str(x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def network(x_shape, y_shape):\n    ops.reset_default_graph()      # reset computation graph\n    \"\"\"\n    x_shape: input dim\n    y_shape: output dim\n    \"\"\"\n    x = tf.placeholder(tf.float32, shape=(None, x_shape), name=\"x\")\n    y = tf.placeholder(tf.float32, shape=(None, y_shape), name=\"y\")\n    print(x)\n    \n    m = tf.reshape(x, shape=[-1, 28, 28, 1])\n    print(m)\n    \n    m = tf.layers.conv2d(m, 16, 3, activation='relu')\n    m = tf.layers.conv2d(m, 16, 3, activation='relu')\n    m = tf.layers.average_pooling2d(m, 2, 2)\n    print(m)\n    m = tf.layers.batch_normalization(m)\n    print(m)\n    m = tf.layers.dropout(inputs=m, rate=0.4)\n    print(m)\n    \n    m = tf.layers.conv2d(m, 32, 3, activation='relu')\n    m = tf.layers.conv2d(m, 32, 3, activation='relu')\n    m = tf.layers.average_pooling2d(m, 2, 2)\n    print(m)\n    m = tf.layers.batch_normalization(m)\n    print(m)\n    m = tf.layers.dropout(inputs=m, rate=0.4)\n    print(m)\n    \n    m = tf.layers.conv2d(m, 64, 3, activation='relu')\n    print(m)\n    m = tf.layers.average_pooling2d(m, 2, 2)\n    print(m)\n    m = tf.layers.batch_normalization(m)\n    print(m)\n    m = tf.layers.dropout(inputs=m, rate=0.4)\n    print(m)\n    \n    # flatten the input\n    m = tf.layers.flatten(m)\n    print(m)\n    \n    # fully-connected\n#     m = tf.layers.dense(m, units=32, activation='relu')\n#     print(m)\n#     m = tf.layers.batch_normalization(m)\n#     print(m)\n#     m = tf.layers.dropout(inputs=m, rate=0.4)\n#     print(m)\n    \n#     m = tf.layers.dense(m, units=16, activation='relu')\n#     print(m)\n#     m = tf.layers.batch_normalization(m)\n#     print(m)\n#     m = tf.layers.dropout(inputs=m, rate=0.4)\n#     print(m)\n\n    prediction = tf.layers.dense(m, units=y_shape, name=\"p\")\n    print(prediction)\n    \n    return x, y, prediction\n\ndef optimization(logits, labels):\n    \"\"\"\n    logits: pred value\n    labels: real value\n    \"\"\"\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n    optim = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss)\n    return loss, optim\n\ndef random_mini_batches(x, y, mini_batch_size = 64):\n    m = x.shape[0]\n    mini_batches = []\n    \n    #shuffle x and y\n    permutation = list(np.random.permutation(m))\n    shuffled_x = x[permutation]\n    shuffled_y = y[permutation]\n    \n    #partition\n    num_complete_minibatches = math.ceil(m / mini_batch_size)\n    for k in range(0, num_complete_minibatches):\n        mini_batch_x = shuffled_x[k*mini_batch_size : k*mini_batch_size + mini_batch_size]\n        mini_batch_y = shuffled_y[k*mini_batch_size : k*mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_x, mini_batch_y)\n        mini_batches.append(mini_batch)\n    return mini_batches\n\ndef mini_test_batches(x, mini_batch_size = 64):\n    num_complete_minibatches = math.ceil(x.shape[0] / mini_batch_size)\n    for k in range(0, num_complete_minibatches):\n        yield x[k*mini_batch_size : k*mini_batch_size + mini_batch_size]\n\ndef train(x_train, y_train, x_cv, y_cv, x_test, num_epochs, mini_batch_size = 64):\n    x_shape = x_train.shape\n    y_shape = y_train.shape\n    print(\"input: \", x_shape[1], \" output: \", y_shape[1])\n    x, y, pred = network(x_shape[1], y_shape[1])\n    # loss, optim\n    loss, optim = optimization(pred, y)\n    # acc \n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    \n    # init tensorflow\n    init = tf.global_variables_initializer()\n    with tf.Session() as sess:\n        sess.run(init)  #initializes the variables created\n        for epoch in range(num_epochs):\n            epoch_cost = 0\n            epoch_acc = 0\n            num_minibatches = math.ceil(x_shape[0] / mini_batch_size)\n            minibatches = random_mini_batches(x_train, y_train)\n            for minibatch_x, minibatch_y in minibatches:\n                _, minibatch_cost, p, minibatch_acc  = sess.run([optim, loss, pred, accuracy], feed_dict = {x: minibatch_x, y: minibatch_y})\n                # print(\"pred shape: \", p)\n                epoch_cost += minibatch_cost / num_minibatches\n                epoch_acc += minibatch_acc / num_minibatches\n            print(\"cost after epoch %i , loss:  %.3f\" % (epoch + 1, epoch_cost), end=\"\")\n            print(\"  train accuracy   :  %.3f\" % epoch_acc)\n            \n            epoch_cv_acc = 0\n            num_minibatches = math.ceil(x_cv.shape[0] / mini_batch_size)\n            minibatches = random_mini_batches(x_cv, y_cv)\n            for minibatch_x, minibatch_y in minibatches:\n                minibatch_acc  = sess.run(accuracy, feed_dict = {x: minibatch_x, y: minibatch_y})\n                # print(\"pred shape: \", p)\n                epoch_cv_acc += minibatch_acc / num_minibatches\n#             print(\"  cv accuracy   :  %.3f\" % (accuracy.eval({x: x_cv, y: y_cv})))\n            print(\"  cv accuracy   :  %.3f\" % epoch_cv_acc)\n            \n        print(\"network trained\")\n        predicts = []\n        probs = []\n        for mini_batch_x in mini_test_batches(x_test):\n            predicts.extend(tf.argmax(pred, 1).eval({x: mini_batch_x}))\n            probs.extend(tf.nn.softmax(pred, 1).eval({x: mini_batch_x}))\n        print(\"test shape: \", x_test.shape)\n        print(\"predicts shape: \", len(predicts))\n#         print(\"predicts val: \", predicts)\n        return np.asarray(predicts), np.asarray(probs)\n    \npreds , probs = train(x_train, y_train, x_cv, y_cv, x_test, num_epochs=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27844e5f0fe5e7d82dd8c146c22e3ee962eab71d"},"cell_type":"code","source":"preds = preds.reshape(-1,1)\npreds_df = pd.DataFrame(preds, columns=['Label'])\npreds_df['ImageID'] = preds_df.index + 1\nsubmission_df = preds_df[preds_df.columns[::-1]]\nsubmission_df.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}