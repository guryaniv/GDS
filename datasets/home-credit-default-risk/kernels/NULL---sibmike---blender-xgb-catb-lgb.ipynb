{"cells":[{"metadata":{"_uuid":"26504ba14a77a488f67671e51007df7e4e5b3016"},"cell_type":"markdown","source":"# Universal Blender: XGB+CatB+LGB\n## Part 1. Staging.\nThis is an attempt to build a universal blender frame, that collects holdout, crossval and train predictions, so that more advanced stacking and blending techinques can be used. \n\nThis realisation prepares stage to blend Ridge, DNN, XGB, CatBoost and LGBM, but you can add or drop any models you want. The code could have been more elegant with a estimator class, but it is not :). If you know how to make it better, please share.\n\nIt also includes Data Builder function with memory optimisation that decreases memory usage by 70%. The optimized pickle dump can be used instead of building the dataframe from scratch.\nThe kernel is submitted in debug mode. State debag = False in oof_regression_stacker to get real results. Chose number of folds carefully as XGBoost and CatBoost take forever to train.\n\n**Note:** Dataloader applies MinMaxscaler on the dataset that includes train and test data, therefore a leak from test to train occures. It is beneficial for leaderboard score, but should be avoided in real projects. To get a real life example how it can mess up your results please watch Caltech lecture, Puzzle 4, 52:00 : https://www.youtube.com/watch?v=EZBUDG12Nr0&index=17&list=PLD63A284B7615313A"},{"metadata":{"trusted":true,"_uuid":"2b2e12b628583f5bf3e6d08fdf939a39265e484f"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n\nimport sklearn.linear_model\nfrom sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport gc\nimport os\nimport time\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccbba32463eeb52fb48e748ac75bc6e210e0a5e3"},"cell_type":"code","source":"def oof_regression_stacker(train_x, train_y, test_x,\n                           estimators, \n                           pred_cols, \n                           train_eval_metric, \n                           compare_eval_metric,\n                           n_folds = 3,\n                           holdout_x=False,\n                           debug = False):\n    \n    \"\"\"\n    Original script:\n        Jovan Sardinha\n        https://medium.com/weightsandbiases/an-introduction-to-model-ensembling-63effc2ca4b3\n        \n    Args:\n        train_x, train_y, test_x (DataFrame).\n        n_folds (int): The number of folds for crossvalidation.\n        esdtimators (list): The list of estimator functions.\n        pred_cols (list): The estimator related names of prediction columns.\n        train_eval_metric (class): Fucntion for the train eval metric.\n        compare_eval_metric (class): Fucntion for the crossvalidation eval metric.\n        holdout_x (DataFrame): Holdout dataframe if you intend to stack/blend using holdout.\n        \n    Returns:\n        train_blend, test_blend, model\n    \"\"\"\n    \n    if debug == True:\n        train_x = train_x.sample(n=1000, random_state=seed_val)\n        train_y = train_y.sample(n=1000, random_state=seed_val)\n        \n    # Start timer:\n    start_time = time.time()\n    \n    # List to save models:\n    model_list = []\n    \n    # Initializing blending data frames:\n    with_holdout = isinstance(holdout_x, pd.DataFrame)\n    if with_holdout: holdout_blend = pd.DataFrame(holdout_x.index)\n    \n    train_blend = pd.DataFrame(train_x.index)\n    val_blend = pd.DataFrame(train_x.index)\n    test_blend = pd.DataFrame(test_x.index)\n\n    # Arrays to hold estimators' predictions:\n    test_len = test_x.shape[0]\n    train_len = train_x.shape[0]\n\n    dataset_blend_train = np.zeros((train_len, len(estimators))) # Mean train prediction holder\n    dataset_blend_val = np.zeros((train_len, len(estimators))) # Validfation prediction holder                   \n    dataset_blend_test = np.zeros((test_len, len(estimators))) # Mean test prediction holder\n    if with_holdout: dataset_blend_holdout = np.zeros((holdout_x.shape[0], len(estimators))) # Same for holdout\n        \n    # Note: StratifiedKFold splits into roughly 66% train 33% test  \n    folds = StratifiedShuffleSplit(n_splits= n_folds, random_state=seed_val,\n                                  test_size = 1/n_folds, train_size = 1-(1/n_folds))\n        \n    # For every estimator:\n    for j, estimator in enumerate(estimators):\n        \n        # Array to hold folds number of predictions on test:\n        dataset_blend_train_j = np.zeros((train_len, n_folds))\n        dataset_blend_test_j = np.zeros((test_len, n_folds))\n        if with_holdout: dataset_blend_holdout_j = np.zeros((holdout_x.shape[0], n_folds))\n        \n        # For every fold:\n        for i, (train, test) in enumerate(folds.split(train_x, train_y)):\n            trn_x = train_x.iloc[train, :] \n            trn_y = train_y.iloc[train].values.ravel()\n            val_x = train_x.iloc[test, :] \n            val_y = train_y.iloc[test].values.ravel()\n            \n            # Estimators conditional training:\n            if estimator == 'lgb':\n                model = kfold_lightgbm(trn_x, trn_y)\n                pred_val = model.predict(val_x)\n                pred_test = model.predict(test_x)\n                pred_train = model.predict(train_x)\n                if with_holdout:\n                    pred_holdout = model.predict(holdout_x)                \n            elif estimator == 'xgb':\n                model = kfold_xgb(trn_x, trn_y)\n                pred_val = xgb_predict(val_x, model)\n                pred_test = xgb_predict(test_x, model)\n                pred_train = xgb_predict(train_x, model)\n                if with_holdout:\n                    pred_holdout = xgb_predict(holdout_x, model)\n            elif estimator == 'f10_dnn':\n                model = f10_dnn(trn_x, trn_y)\n                pred_val = model.predict(val_x).ravel()\n                pred_test = model.predict(test_x).ravel()\n                pred_train = model.predict(train_x).ravel()\n                if with_holdout:\n                    pred_holdout = model.predict(holdout_x).ravel()\n                #print(pred_val.shape, pred_test.shape, pred_train.shape)             \n            elif estimator == 'ridge':\n                model = ridge(trn_x, trn_y)\n                pred_val = model.predict(val_x)\n                pred_test = model.predict(test_x)\n                pred_train = model.predict(train_x)\n                if with_holdout:\n                    pred_holdout = model.predict(holdout_x)                         \n            else:\n                model = kfold_cat(trn_x, trn_y)\n                pred_val = model.predict_proba(val_x)[:,1]\n                pred_test = model.predict_proba(test_x)[:,1]\n                pred_train = model.predict_proba(train_x)[:,1]\n                if with_holdout:\n                    pred_holdout = model.predict_proba(holdout_x)[:,1]         \n            \n            dataset_blend_val[test, j] = pred_val\n            dataset_blend_test_j[:, i] = pred_test\n            dataset_blend_train_j[:, i] = pred_train\n            if with_holdout: \n                dataset_blend_holdout_j[:, i] = pred_holdout\n            \n            print('fold:', i+1, '/', n_folds,\n                  '; estimator:',  j+1, '/', len(estimators),\n                  ' -> oof cv score:', compare_eval_metric(val_y, pred_val))\n\n            del trn_x, trn_y, val_x, val_y\n            gc.collect()\n    \n        # Save curent estimator's mean prediction for test, train and holdout:\n        dataset_blend_test[:, j] = np.mean(dataset_blend_test_j, axis=1)\n        dataset_blend_train[:, j] = np.mean(dataset_blend_train_j, axis=1)\n        if with_holdout: dataset_blend_holdout[:, j] = np.mean(dataset_blend_holdout_j, axis=1)\n        \n        model_list += [model]\n        \n    #print('--- comparing models ---')\n    for i in range(dataset_blend_val.shape[1]):\n        print('model', i+1, ':', compare_eval_metric(train_y, dataset_blend_val[:,i]))\n        \n    for i, j in enumerate(estimators):\n        val_blend[pred_cols[i]] = dataset_blend_val[:,i]\n        test_blend[pred_cols[i]] = dataset_blend_test[:,i]\n        train_blend[pred_cols[i]] = dataset_blend_train[:,i]\n        if with_holdout: \n            holdout_blend[pred_cols[i]] = dataset_blend_holdout[:,i]\n        else:\n            holdout_blend = False\n    \n    end_time = time.time()\n    print(\"Total Time usage: \" + str(int(round(end_time - start_time))))\n    return train_blend, val_blend, test_blend, holdout_blend, model_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1430fe3fdbddc0dfac07a80a966d014fcfe258cc"},"cell_type":"markdown","source":"## Blending:\n### Estimators:\n#### Ridge regression"},{"metadata":{"trusted":true,"_uuid":"49d2034426de4ecd891096134ad81fb90005b036"},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nimport sklearn.linear_model\n\ndef ridge(trn_x, trn_y):\n    clf = Ridge(alpha=20, \n                copy_X=True, \n                fit_intercept=True, \n                solver='auto',max_iter=10000,\n                normalize=False, \n                random_state=0,  \n                tol=0.0025)\n    clf.fit(trn_x, trn_y)\n    return clf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"084c8907a45e0323979a0ad803ab6c9746f5e236"},"cell_type":"markdown","source":"#### Simple DNN\nPlease thank its author:\nhttps://www.kaggle.com/tottenham/10-fold-simple-dnn-with-rank-gauss\n\nworks surprisingly fast."},{"metadata":{"trusted":true,"_uuid":"a9b0f7c3524cedac006aeab7a0e3331901f16af3"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import KFold\n\nimport gc\nimport os\n\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\n\nclass roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.x)\n        roc = roc_auc_score(self.y, y_pred)\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n\ndef f10_dnn(X_train, Y_train, nn_num_folds=10):\n    \n    folds = KFold(n_splits=nn_num_folds, shuffle=True, random_state=seed_val)\n\n    for n_fold, (nn_trn_idx, nn_val_idx) in enumerate(folds.split(X_train)):\n        nn_trn_x, nn_trn_y = X_train.iloc[nn_trn_idx,:], Y_train[nn_trn_idx]\n        nn_val_x, nn_val_y = X_train.iloc[nn_val_idx,:], Y_train[nn_val_idx]\n\n        print( 'Setting up neural network...' )\n        nn = Sequential()\n        nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = 718))\n        nn.add(PReLU())\n        nn.add(Dropout(.3))\n        nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n        nn.add(PReLU())\n        nn.add(BatchNormalization())\n        nn.add(Dropout(.3))\n        nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n        nn.add(PReLU())\n        nn.add(BatchNormalization())\n        nn.add(Dropout(.3))\n        nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n        nn.add(PReLU())\n        nn.add(BatchNormalization())\n        nn.add(Dropout(.3))\n        nn.add(Dense(units = 12, kernel_initializer = 'normal'))\n        nn.add(PReLU())\n        nn.add(BatchNormalization())\n        nn.add(Dropout(.3))\n        nn.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n        nn.compile(loss='binary_crossentropy', optimizer='adam')\n\n        print( 'Fitting neural network...' )\n        nn.fit(nn_trn_x, nn_trn_y, validation_data = (nn_val_x, nn_val_y), epochs=10, verbose=2,\n              callbacks=[roc_callback(training_data=(nn_trn_x, nn_trn_y),validation_data=(nn_val_x, nn_val_y))])\n        \n        #print( 'Predicting...' )\n        #sub_preds += nn.predict(X_test).flatten().clip(0,1) / folds.n_splits\n    \n        gc.collect()\n        \n        return nn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1849eef62f0dec84fa640dc937ab5c6255623cfa"},"cell_type":"markdown","source":"#### LightGBM\nthe best of the batch, fast and convenient to use."},{"metadata":{"trusted":true,"_uuid":"9e58779562499d6819497cc9660e0dfef7cea687"},"cell_type":"code","source":"def kfold_lightgbm(trn_x, trn_y, num_folds=3):\n       \n    # Cross validation model\n    in_folds = StratifiedShuffleSplit(n_splits= num_folds, random_state=seed_val)\n        \n    # Create arrays and dataframes to store results\n    for train_idx, valid_idx in in_folds.split(trn_x, trn_y):\n        dtrain = lgb.Dataset(data=trn_x.values[train_idx], \n                             label=trn_y[train_idx], \n                             free_raw_data=False, silent=True)\n        dvalid = lgb.Dataset(data=trn_x.values[valid_idx], \n                             label=trn_y[valid_idx], \n                             free_raw_data=False, silent=True)\n\n        # LightGBM parameters found by Bayesian optimization\n        params = {\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            'nthread': 4,\n            'learning_rate': 0.02,  # 02,\n            'num_leaves': 20,\n            'colsample_bytree': 0.9497036,\n            'subsample': 0.8715623,\n            'subsample_freq': 1,\n            'max_depth': 8,\n            'reg_alpha': 0.041545473,\n            'reg_lambda': 0.0735294,\n            'min_split_gain': 0.0222415,\n            'min_child_weight': 60, # 39.3259775,\n            'seed': seed_val,\n            'verbose': -1,\n            'metric': 'auc',\n        }\n        \n        clf = lgb.train(\n            params=params,\n            train_set=dtrain,\n            num_boost_round=10000,\n            valid_sets=[dtrain, dvalid],\n            early_stopping_rounds=200,\n            verbose_eval=False\n        )\n\n        del dtrain, dvalid\n        gc.collect()\n    \n    return clf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48c5bb064c23dc2db320752da3f01609b7968968"},"cell_type":"markdown","source":"#### XGBoost \nhas a nasty feature that it takes only DMatrix as arguments therefore predict method has to be wrapend into a function."},{"metadata":{"trusted":true,"_uuid":"8e25bd69e08093fa0ecc760c1e7dbe26aaa4e3ce"},"cell_type":"code","source":"def xgb_predict(X, model):\n    xgb_X = xgb.DMatrix(X.values)\n    return model.predict(xgb_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c0bf0bd4170f2cdf0a62142ab81a0d687fa9d0e"},"cell_type":"code","source":"def kfold_xgb(trn_x, trn_y, num_folds=3):\n    \n    # Cross validation model\n    folds = StratifiedShuffleSplit(n_splits= num_folds, random_state=seed_val)\n        \n    # Create arrays and dataframes to store results\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(trn_x, trn_y)):\n        dtrain = xgb.DMatrix(trn_x.values[train_idx], \n                             trn_y[train_idx])\n        dvalid = xgb.DMatrix(trn_x.values[valid_idx], \n                             trn_y[valid_idx])\n\n        # LightGBM parameters found by Bayesian optimization\n        n_rounds = 2000\n        \n        xgb_params = {'eta': 0.05,\n                      'max_depth': 6, \n                      'subsample': 0.85, \n                      'colsample_bytree': 0.85,\n                      'colsample_bylevel': 0.632,\n                      'min_child_weight' : 30,\n                      'objective': 'binary:logistic', \n                      'eval_metric': 'auc', \n                      'seed': seed_val,\n                      'lambda': 0,\n                      'alpha': 0,\n                      'silent': 1\n                     }\n        \n        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n        xgb_model = xgb.train(xgb_params, \n                              dtrain, \n                              n_rounds, \n                              watchlist, \n                              verbose_eval=False,\n                              early_stopping_rounds=200)\n\n        del dtrain, dvalid\n        gc.collect()\n    \n    return xgb_model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bf68142dacea248b97c48334810e2a244c829b1"},"cell_type":"markdown","source":"#### Catboost\nwatch out, it has predict and predict_proba methods. predict_proba should be used; it returns 2d array, that has to be flattened."},{"metadata":{"trusted":true,"_uuid":"6f7635d430c313829f3e821b522711f765ff8079"},"cell_type":"code","source":"def kfold_cat(trn_x, trn_y, num_folds=3):\n    \n    # Cross validation model\n    folds = StratifiedShuffleSplit(n_splits= num_folds, random_state=seed_val)\n        \n    # Create arrays and dataframes to store results\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(trn_x, trn_y)):\n        cat_X_train, cat_y_train = trn_x.values[train_idx], trn_y[train_idx]\n        cat_X_valid, cat_y_valid = trn_x.values[valid_idx], trn_y[valid_idx]\n\n        # Catboost:\n        #cb_model = CatBoostClassifier(iterations=1000,\n                              #learning_rate=0.1,\n                              #depth=7,\n                              #l2_leaf_reg=40,\n                              #bootstrap_type='Bernoulli',\n                              #subsample=0.7,\n                              #scale_pos_weight=5,\n                              #eval_metric='AUC',\n                              #metric_period=50,\n                              #od_type='Iter',\n                              #od_wait=45,\n                              #random_seed=17,\n                              #allow_writing_files=False)\n        \n        cb_model = CatBoostClassifier(iterations=2000,\n                                      learning_rate=0.02,\n                                      depth=6,\n                                      l2_leaf_reg=40,\n                                      bootstrap_type='Bernoulli',\n                                      subsample=0.8715623,\n                                      scale_pos_weight=5,\n                                      eval_metric='AUC',\n                                      metric_period=50,\n                                      od_type='Iter',\n                                      od_wait=45,\n                                      random_seed=seed_val,\n                                     allow_writing_files=False)\n        \n        cb_model.fit(cat_X_train, cat_y_train,\n                     eval_set=(cat_X_valid, cat_y_valid),\n                     use_best_model=True,\n                     verbose=False)\n\n        del cat_X_train, cat_y_train, cat_X_valid, cat_y_valid \n        gc.collect()\n    \n    return cb_model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9d7a2f294479d4f6fc810ee362e76e8b74a3572"},"cell_type":"markdown","source":"##  Data part\n### Data loader:"},{"metadata":{"trusted":true,"_uuid":"65462719603d7ba4bd87df1e68235360506dc7a1"},"cell_type":"code","source":"def data_loader(to_load=False):\n    \n    if not to_load:\n        \n        df = data_builder()\n        feats = [f for f in df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n        \n        # Split to train and test:\n        y = df['TARGET']\n        X = df[feats]\n        X = X.fillna(X.mean()).clip(-1e11,1e11)\n\n        print(\"X shape: \", X.shape, \"    y shape:\", y.shape)\n        print(\"\\nPreparing data...\")\n\n        training = y.notnull()\n        testing = y.isnull()\n        \n        X_train = X.loc[training,:]\n        X_test = X.loc[testing,:]\n        y_train = y.loc[training]\n        \n        # Scale:\n        scaler = MinMaxScaler()\n        scaler.fit(X)\n        X_train.loc[:, X_train.columns] = scaler.transform(X_train[X_train.columns])\n        X_test.loc[:, X_test.columns] = scaler.transform(X_test[X_test.columns])\n        \n        print(X_train.shape, X_test.shape, y_train.shape)\n        df.to_pickle('df_low_mem.pkl.gz')\n        \n        del df, X, y, training, testing\n        gc.collect()\n    \n    return X_train, X_test, y_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc76bede7e83083e8dce81fe24127cd158e85aa3"},"cell_type":"markdown","source":"## Data Builder:\n### Service functions:"},{"metadata":{"trusted":true,"_uuid":"70890b6f368107f84e088650e3750b0437ee8387"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df, nan_as_category = True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\n# Preprocess application_train.csv and application_test.csv\ndef application_train_test(num_rows = None, nan_as_category = False):\n    # Read data and merge\n    df = pd.read_csv('../input/application_train.csv', nrows= num_rows)\n    test_df = pd.read_csv('../input/application_test.csv', nrows= num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n    df = df[df['CODE_GENDER'] != 'XNA']\n    \n    docs = [_f for _f in df.columns if 'FLAG_DOC' in _f]\n    live = [_f for _f in df.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n    \n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n\n    inc_by_org = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n\n    df['NEW_CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n    df['NEW_CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n    df['NEW_DOC_IND_KURT'] = df[docs].kurtosis(axis=1)\n    df['NEW_LIVE_IND_SUM'] = df[live].sum(axis=1)\n    df['NEW_INC_PER_CHLD'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n    df['NEW_INC_BY_ORG'] = df['ORGANIZATION_TYPE'].map(inc_by_org)\n    df['NEW_EMPLOY_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    df['NEW_ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])\n    df['NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n    df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n    df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n    df['NEW_CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n    df['NEW_CAR_TO_EMPLOY_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n    df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n    df['NEW_PHONE_TO_BIRTH_RATIO_EMPLOYER'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n    df['NEW_CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n    \n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    # Categorical features with One-Hot encode\n    df, cat_cols = one_hot_encoder(df, nan_as_category)\n    dropcolum=['FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\n    'FLAG_DOCUMENT_5','FLAG_DOCUMENT_6','FLAG_DOCUMENT_7',\n    'FLAG_DOCUMENT_8','FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', \n    'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13',\n    'FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16',\n    'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19',\n    'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']\n    df= df.drop(dropcolum,axis=1)\n    del test_df\n    gc.collect()\n    return df\n\n# Preprocess bureau.csv and bureau_balance.csv\ndef bureau_and_balance(num_rows = None, nan_as_category = True):\n    bureau = pd.read_csv('../input/bureau.csv', nrows = num_rows)\n    bb = pd.read_csv('../input/bureau_balance.csv', nrows = num_rows)\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n    \n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n    del bb, bb_agg\n    gc.collect()\n    \n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': [ 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': [ 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM': [ 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': [ 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n    }\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    \n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n    del active, active_agg\n    gc.collect()\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n    del closed, closed_agg, bureau\n    gc.collect()\n    return bureau_agg\n\n# Preprocess previous_applications.csv\ndef previous_applications(num_rows = None, nan_as_category = True):\n    prev = pd.read_csv('../input/previous_application.csv', nrows = num_rows)\n    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n    # Days 365.243 values -> nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n    # Add feature: value ask / value received percentage\n    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n    # Previous applications numeric features\n    num_aggregations = {\n        'AMT_ANNUITY': [ 'max', 'mean'],\n        'AMT_APPLICATION': [ 'max','mean'],\n        'AMT_CREDIT': [ 'max', 'mean'],\n        'APP_CREDIT_PERC': [ 'max', 'mean'],\n        'AMT_DOWN_PAYMENT': [ 'max', 'mean'],\n        'AMT_GOODS_PRICE': [ 'max', 'mean'],\n        'HOUR_APPR_PROCESS_START': [ 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': [ 'max', 'mean'],\n        'DAYS_DECISION': [ 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n    }\n    # Previous applications categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n    \n    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n    # Previous Applications: Approved Applications - only numerical features\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n    # Previous Applications: Refused Applications - only numerical features\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n    del refused, refused_agg, approved, approved_agg, prev\n    gc.collect()\n    return prev_agg\n\n# Preprocess POS_CASH_balance.csv\ndef pos_cash(num_rows = None, nan_as_category = True):\n    pos = pd.read_csv('../input/POS_CASH_balance.csv', nrows = num_rows)\n    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n    # Features\n    aggregations = {\n        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n        'SK_DPD': ['max', 'mean'],\n        'SK_DPD_DEF': ['max', 'mean']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    \n    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n    del pos\n    gc.collect()\n    return pos_agg\n    \n# Preprocess installments_payments.csv\ndef installments_payments(num_rows = None, nan_as_category = True):\n    ins = pd.read_csv('../input/installments_payments.csv', nrows = num_rows)\n    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n    # Percentage and difference paid in each installment (amount paid and installment value)\n    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n    # Days past due and days before due (no negative values)\n    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n    # Features: Perform aggregations\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum','min','std' ],\n        'DBD': ['max', 'mean', 'sum','min','std'],\n        'PAYMENT_PERC': [ 'max','mean',  'var','min','std'],\n        'PAYMENT_DIFF': [ 'max','mean', 'var','min','std'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum','min','std'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum','std'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum','std']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n    del ins\n    gc.collect()\n    return ins_agg\n\n# Preprocess credit_card_balance.csv\ndef credit_card_balance(num_rows = None, nan_as_category = True):\n    cc = pd.read_csv('../input/credit_card_balance.csv', nrows = num_rows)\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n    # General aggregations\n    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n    cc_agg = cc.groupby('SK_ID_CURR').agg([ 'max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n    del cc\n    gc.collect()\n    return cc_agg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14059e958cae9816d1758d2fea09b0658c02f3b5"},"cell_type":"markdown","source":"### Memory reducer:\nThanks **You Guillaume Martin** for the Awesome Memory Optimizer!\nhttps://www.kaggle.com/gemartin/load-data-reduce-memory-usage"},{"metadata":{"trusted":true,"_uuid":"7ec89123dbf7f8a49a037aee0d1f4d96614a6ef6"},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else: df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1e5bcd687cd1de7a85fbdef9385fe835b43e30e"},"cell_type":"markdown","source":"### Data builder function:\nFrom lighgbm-with-selected-features"},{"metadata":{"trusted":true,"_uuid":"0578049389282eb91e61a091388919ddb68cd36c"},"cell_type":"code","source":"def data_builder():\n    \n    num_rows = None\n    df = application_train_test(num_rows)\n    with timer(\"Process bureau and bureau_balance\"):\n        bureau = bureau_and_balance(num_rows)\n        print(\"Bureau df shape:\", bureau.shape)\n        df = df.join(bureau, how='left', on='SK_ID_CURR')\n        del bureau\n        gc.collect()\n    with timer(\"Process previous_applications\"):\n        prev = previous_applications(num_rows)\n        print(\"Previous applications df shape:\", prev.shape)\n        df = df.join(prev, how='left', on='SK_ID_CURR')\n        del prev\n        gc.collect()\n    with timer(\"Process POS-CASH balance\"):\n        pos = pos_cash(num_rows)\n        print(\"Pos-cash balance df shape:\", pos.shape)\n        df = df.join(pos, how='left', on='SK_ID_CURR')\n        del pos\n        gc.collect()\n    with timer(\"Process installments payments\"):\n        ins = installments_payments(num_rows)\n        print(\"Installments payments df shape:\", ins.shape)\n        df = df.join(ins, how='left', on='SK_ID_CURR')\n        del ins\n        gc.collect()\n    with timer(\"Process credit card balance\"):\n        cc = credit_card_balance(num_rows)\n        print(\"Credit card balance df shape:\", cc.shape)\n        df = df.join(cc, how='left', on='SK_ID_CURR')\n        del cc\n        gc.collect()\n        \n    df.set_index('SK_ID_CURR', inplace=True, drop=False)\n    df = df.drop(labels='index', axis=1)\n    df = reduce_mem_usage(df)    \n    df.to_pickle('df_low_mem.pkl.gz')\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d2d4d428f857a9b5d58cad3a807258eda7d6c7c"},"cell_type":"markdown","source":"## Collecting data:"},{"metadata":{"_uuid":"176554fac094ae3816e52635229cd5e8fac329d6"},"cell_type":"markdown","source":"# Blender call:"},{"metadata":{"trusted":true,"_uuid":"94bb9ddf2bcb0076596b24db925cd33dacc900d5"},"cell_type":"code","source":"# Fix random seed:\nseed_val = 42\n\n# Load data:\ntrain_x, test_x, train_y = data_loader()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f1e6b553ec991a9fb8d303e1be0256d632ec2b5"},"cell_type":"code","source":"estimators = ['cat','lgb', 'xgb','ridge','f10_dnn']\npred_cols = ['pred_cat','pred_lgb','pred_xgb','ridge','f10_dnn']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f67ea83d0df406c6d324412987293ab28532795"},"cell_type":"code","source":"#Holdout\nfrom sklearn.model_selection import train_test_split\nx_train, x_hold, y_train, y_hold = train_test_split(train_x, train_y, test_size=0.1, random_state=seed_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97042713e69c65d70b4dd774cbece4244c48658e"},"cell_type":"code","source":"n_folds = 2\ntr_blend, va_blend, tst_blend, hold_blend, m_list = oof_regression_stacker(x_train, y_train, test_x, \n                                                                           n_folds = 2, \n                                                                           estimators=estimators, \n                                                                           pred_cols = pred_cols,\n                                                                           train_eval_metric=roc_auc_score,\n                                                                           compare_eval_metric=roc_auc_score,\n                                                                           debug = True, holdout_x = x_hold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89aae9b01dd1ff60d88e77b9d949887db3ccc374"},"cell_type":"code","source":"#tr_blend.to_csv('tr_blend_nn.csv')\n#va_blend.to_csv('va_blend_nn.csv')\n#tst_blend.to_csv('tst_blend_nn.csv')\n#hold_blend.to_csv('hold_blend_nn.csv')\n#m_list.to_csv('m_list.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}