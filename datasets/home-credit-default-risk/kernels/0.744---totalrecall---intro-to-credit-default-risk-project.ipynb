{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\ntrain_control = pd.read_csv('../input/application_train.csv')\ntest_control = pd.read_csv('../input/application_test.csv')\n\ntraining_dataset = pd.read_csv('../input/application_train.csv')\ntest_dataset = pd.read_csv('../input/application_test.csv')\n\nprevious_loan_information = pd.read_csv('../input/POS_CASH_balance.csv')\nprevious_application_data = pd.read_csv('../input/bureau.csv')\n\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\ncredit_card_balance = pd.read_csv('../input/credit_card_balance.csv')\nprevious_application = pd.read_csv('../input/previous_application.csv')\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f708a0e56ecb5b87f16e08ec0ca40f4f6a27f987"},"cell_type":"code","source":"previous_application.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e9168455fbc589092820d703b5f966aadbaa003","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"395819eb3106caa9c4084cd29c0fca124fe9eba0","collapsed":true},"cell_type":"code","source":"# Group by the client id, calculate aggregation statistics\n###previous_application_data_agg = previous_application_data.drop(columns = ['SK_ID_BUREAU']).groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n###previous_application_data_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8fb04f47fd1239b50c36d73f791e80e57050eca1"},"cell_type":"code","source":"#### List of column names\n###columns = ['SK_ID_CURR']\n###\n#### Iterate through the variables names\n###for var in previous_application_data_agg.columns.levels[0]:\n###    # Skip the id name\n###    if var != 'SK_ID_CURR':\n###        \n###        # Iterate through the stat names\n###        for stat in previous_application_data_agg.columns.levels[1][:-1]:\n###            # Make a new column name for the variable and stat\n###            columns.append('bureau_%s_%s' % (var, stat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3fd1ba3809076d827a442804eed239b2d42cd58","collapsed":true},"cell_type":"code","source":"#### Assign the list of columns names as the dataframe column names\n###previous_application_data_agg.columns = columns\n###previous_application_data_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b29d0d875b7a3e6a8f05046367b868578f5a64ec","collapsed":true},"cell_type":"code","source":"##previous_application_data['CREDIT_DAY_OVERDUE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"658bcc9b1f2ea1155a06a3adc4e5aabb38f6056b","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"number_active_loans = previous_application_data[previous_application_data['CREDIT_ACTIVE'] == 'Active'].groupby(['SK_ID_CURR'], as_index = False).agg({'CREDIT_CURRENCY': \"count\", 'CREDIT_DAY_OVERDUE':\"sum\"})\nnumber_active_loans.columns = ['SK_ID_CURR', 'num_active_loans', 'num_days_overdue']\nnumber_active_loans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d08351bc58171ba0c2a733db5b492999448e8910","collapsed":true},"cell_type":"code","source":"training_dataset = training_dataset.merge(number_active_loans, left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')\ntest_dataset = test_dataset.merge(number_active_loans, left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66778627cd9be1cddc08173df03640ac0ed0591f"},"cell_type":"code","source":"## what does the target variable look like?\ntraining_dataset['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e7b14ed10f7065064821e08cc1ef0196584feed"},"cell_type":"code","source":"# Number of unique classes in each object column\ntraining_dataset.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c2c1a60e4024c6a7e3b78f1a131d3729afcbc2d","collapsed":true},"cell_type":"code","source":"### we need to convert the categorial variables to numeric values.. one hot encoding..\n# one-hot encoding of categorical variables\ntraining_dataset = pd.get_dummies(training_dataset)\ntest_dataset = pd.get_dummies(test_dataset)\n\n## one hot encoding significantly increases the number of variables.. it would help to try dimensionality reduction to reduce the number of variables\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e39fccc1d9acdee35e394e7e9d7473a6a8313e3"},"cell_type":"code","source":"#### since we one hot encoded, now we need to align the two datasets \ntrain_labels = training_dataset['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\ntraining_dataset, test_dataset = training_dataset.align(test_dataset, join = 'inner', axis = 1)\n\n# Add the target back in\ntraining_dataset['TARGET'] = train_labels\n\n### checking if the two datasets have the same columns\nprint('Training Features shape: ', training_dataset.shape)\nprint('Testing Features shape: ', test_dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef6e4bccf348d40d2e81cbdc585a31d5c0f31282"},"cell_type":"code","source":"#### The next thing to check is to check if there are large outliers -- we can do this with the describe function\n\ntraining_dataset['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1e54f2346b49b731c2c2e4e47dc9fa169d70c3a"},"cell_type":"code","source":"### so days employed seems to have at least one outlier.. next question is what to do with this outlier.. \n#### first step is to check if the outliers have a lower or higher rate of default compared to the rest of the data\n\nanom = training_dataset[training_dataset['DAYS_EMPLOYED'] == 365243]\nnon_anom = training_dataset[training_dataset['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"57266a501c0531fe9ba00b242baceba302ecf4ed"},"cell_type":"code","source":"#### a good solution here is to replace the outlier with a missing value (np.nan) and then create a flag indicating that the data had a missing value originally\n\n# Create an anomalous flag column\ntraining_dataset['DAYS_EMPLOYED_ANOM'] = training_dataset[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\ntraining_dataset['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n### do the same for the test dataset\ntest_dataset['DAYS_EMPLOYED_ANOM'] = test_dataset[\"DAYS_EMPLOYED\"] == 365243\ntest_dataset[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6778a8c11cbaf56c1ce6ab4eccec69d95da8289e"},"cell_type":"code","source":"#### Next thing to check is correlation with independent variables and the target variable\n# Find correlations with the target and sort\ncorrelations = training_dataset.corr()['TARGET'].sort_values()\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b729880a562d0e9522991c87a3d97699260717a2","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b739c9e5fc6f4996f7f9452d211898ccdb74726f"},"cell_type":"code","source":"### checking the predictability of a few variables\n\nplt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['num_active_loans', 'num_days_overdue']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(training_dataset.loc[training_dataset['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(training_dataset.loc[training_dataset['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4a80857c1191ea8c425322513c9e33367c47c049"},"cell_type":"markdown","source":"**Combining the main training dataset with some information in the other datasets**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"552f2518c3bb2df06b568bdbb11eaaa7f7a6e0a9"},"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg\n\n\ndef count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical\n\n\n# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n    \n# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n\n    df = df[np.isfinite(df[var_name])] #drop nans\n\n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n\n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n\n    plt.figure(figsize = (12, 6))\n\n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n\n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n\n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e62e7bc2e3af48b1b3f2ff5d0c0358a7eb5351f1","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b3cc5b89e591a8fc2e893dfd5bdecf9cb553c5fa"},"cell_type":"code","source":"# Function to calculate correlations with the target for a dataframe\ndef target_corrs(df):\n\n    # List of correlations\n    corrs = []\n\n    # Iterate through the columns \n    for col in df.columns:\n        print(col)\n        # Skip the target column\n        if col != 'TARGET':\n            # Calculate correlation with the target\n            corr = df['TARGET'].corr(df[col])\n\n            # Append the list as a tuple\n            corrs.append((col, corr))\n            \n    # Sort by absolute magnitude of correlations\n    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n    \n    return corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e31ac38274120377c0a8000d01550fa432b1c92c","collapsed":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b57f5fd82dedc0809e24a62a0647d66c6fde795","collapsed":true},"cell_type":"code","source":"previous_application_counts = count_categorical(previous_application, group_var = 'SK_ID_CURR', df_name = 'previous_application')\nprevious_application_agg_new = agg_numeric(previous_application_data.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'previous_application_data')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7929d4eb023f15ca801128967f5afe9bfcf31722","collapsed":true},"cell_type":"code","source":"bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fde4417c149498c43fb7a1f6a7a00795582d25e","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d841efe0499db080726cc6044084b4cd7eb424ff","collapsed":true},"cell_type":"code","source":"### This merges all of the calculated metrics by loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\nbureau_by_loan = bureau_by_loan.merge(previous_application_data[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\n### This takes the calculated metrics by loan and aggregates them on client id\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da66c31d5c8984bbd655f0de52de7e2a72b3967c"},"cell_type":"code","source":"bureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad732dc708923e6f36a60bc43807c2602c3c3c68"},"cell_type":"code","source":"previous_application_agg_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"70ff7f5ee421332dc9a3d46d693ffc73203aa9de"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c5ac46b6f34bfa77b1611282422552303e5d6a04"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b2440ba0666cdfed1aca391bd9b5bb4d27b9e31"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bff040dcbe76c01a92f4baf077aac1247855802c","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb802e5672df92a968b8113deb5884b76c5bff2e"},"cell_type":"code","source":"training_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9ee91986b75b252720755c34e6d363a69b332059"},"cell_type":"code","source":"####### Do a little feature engineering\ntraining_dataset['ANNUITY_INCOME_PERCENT'] = training_dataset['AMT_ANNUITY'] / training_dataset['AMT_INCOME_TOTAL']\ntest_dataset['ANNUITY_INCOME_PERCENT'] = test_dataset['AMT_ANNUITY'] / test_dataset['AMT_INCOME_TOTAL']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cfadcddc457f033f8bc1f6c55e5af93c0a9f375","collapsed":true},"cell_type":"code","source":"# Merge with the value counts of bureau\ntraining_dataset = training_dataset.merge(previous_application_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntraining_dataset = training_dataset.merge(previous_application_agg_new, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the monthly information grouped by client\ntraining_dataset = training_dataset.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bba428e464021674501e348dc73a06b0d3c30ff"},"cell_type":"code","source":"#### There are a bunch of missing values in our new variables.. we should look into these\nmissing_train = missing_values_table(training_dataset)\nmissing_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac81ac35b48567b0edfda90b49be03d98cebe387"},"cell_type":"markdown","source":"**Do the same additions / transformations to the test dataset**"},{"metadata":{"trusted":true,"_uuid":"3bf0e5c9492ac098a8f5c6d0d18b1245c052d170"},"cell_type":"code","source":"# Merge with the value counts of bureau\ntest_dataset = test_dataset.merge(previous_application_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntest_dataset = test_dataset.merge(previous_application_agg_new, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the value counts of bureau balance\ntest_dataset = test_dataset.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"984a14d36c347becbba025d86b4dc5f2b90abafa"},"cell_type":"markdown","source":"**We need to align the test and training datasets**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"df3f9da1fce091951f2f3ff3beee4f25c8cdf278"},"cell_type":"code","source":"train_labels = training_dataset['TARGET']\n\n# Align the dataframes, this will remove the 'TARGET' column\ntraining_dataset, test_dataset = training_dataset.align(test_dataset, join = 'inner', axis = 1)\n\ntraining_dataset['TARGET'] = train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"963af5e61770e3eaba43ac1ad1ec3503390a2884"},"cell_type":"code","source":"##### Drop variables with x% of observations that are missing\nmissing_test = missing_values_table(test_dataset)\nmissing_test_vars = list(missing_test.index[missing_test['% of Total Values'] > 90])\n\nmissing_train = missing_values_table(training_dataset)\nmissing_train_vars = list(missing_test.index[missing_test['% of Total Values'] > 90])\n\nmissing_columns = list(set(missing_test_vars + missing_train_vars))\n\n# Drop the missing columns\ntraining_dataset = training_dataset.drop(columns = missing_columns)\ntest_dataset = test_dataset.drop(columns = missing_columns)\n\nprint('There are %d columns with more than 90%% missing in either the training or testing data.' % len(missing_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03bfa826c0a5f794abca622c6148af98fa10fc9f"},"cell_type":"code","source":"########## Save the training and test datasets\ntraining_dataset.to_csv('train_bureau_raw.csv', index = False)\ntest_dataset.to_csv('test_bureau_raw.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66bbcb8c18c5059f30a48c1c0fb148bd4af44fa5"},"cell_type":"code","source":"kde_target(var_name='previous_application_data_CREDIT_ACTIVE_Active_count_norm', df=training_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd7ed0a986938de6b3a386f14095da6dae829e7c"},"cell_type":"code","source":"training_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77e1e71a7338ebb22d6244433ae1815e2b96f4a2"},"cell_type":"code","source":"for x in training_dataset.columns:\n    if \"CREDIT_ACTIVE\" in x:\n        print(x)\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0d6a27901f271e64cb2b07d42eef95026e4724ce"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"71718e123af4cbeff206a5a538edf5939e25b40b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"29931004c801c80b0eb1428540611423db361ec1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9ab1abdf01fe5744a1e5f59936402cc431fb8396"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3006df6e4c04b887f63ca0b0c0eae868bc117893"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f8fceca4c27ac20eb5df6e29631a162ceb52aa7","collapsed":true},"cell_type":"code","source":"\n# Drop the target from the training data\nif 'TARGET' in training_dataset:\n    train = training_dataset.drop(columns = ['TARGET'])\nelse:\n    train = training_dataset.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = test_dataset.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(test_dataset)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c5b888357942eae541f011cc3637ec235fee4dc4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1514ec0bc3b9a3dd0e21f8603ffb65dd1defd9b6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2aa22cdde571ab297b61d67f17af9c197bc97eed"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ec26dc25b1be66a6e40a6c0f7a58d0e380b34e50"},"cell_type":"code","source":"##from sklearn.ensemble import RandomForestClassifier\n##\n### Make the random forest classifier\n##random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae40c353726334e80a379b3f81db962180ff9eee","collapsed":true},"cell_type":"code","source":"### Train on the training data\n##random_forest.fit(train, train_labels)\n##\n### Extract feature importances\n##feature_importance_values = random_forest.feature_importances_\n##feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n##\n### Make predictions on the test data\n##predictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3eab7bb8e90a5ead36c9876312ce0b064aeddb82","collapsed":true},"cell_type":"code","source":"### Make a submission dataframe\n##submit = test_dataset[['SK_ID_CURR']]\n##submit['TARGET'] = predictions\n##\n### Save the submission dataframe\n##submit.to_csv('random_forest_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48f4c1e40628a28af42cac615eb93f18fcebf262","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"52d5bc35e32079a1d6df33b6c4487bc1b40f4181"},"cell_type":"markdown","source":"**Creating a gradient boosting machine model**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"03cc6e62bddd35eb226df288d4c66a891a2f338b"},"cell_type":"code","source":"import lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4f9be34feb355eafcff496fa3cf1ea2be1be290c"},"cell_type":"code","source":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9bfe248994dcb7c66e5b22183e057a4dfb375750"},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c28a2221025797694758ab2a1b5a3c9271292a4","collapsed":true},"cell_type":"code","source":"submission, fi, metrics = model(train_control, test_control)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba1cf873a9ad64aa3fc24ece8a58ed6238ffe692","collapsed":true},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c4ef5986f8ca1b392a3fd156c5510e5d995d5305"},"cell_type":"code","source":"submission.to_csv('control.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e559962b7af05b35f97b3f4f2e17830e84465ce1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bb8ff1292e0fd1defc6c923aeff91b121a9b5e8f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8220385365deaaa78d2805d540117c07f19cea6f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"324bbdc5e9e1a8a326bf17f16cd3eed793651df4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"02935d6f5d19433d09d87ee0379619eb794a88a7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8434725e0e7b84d2ae5e57fe1e315f5126964368"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6b5a081ad225098f91767649a95f442c7084076b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}