{"cells":[{"metadata":{"_uuid":"735a0673b257fa7f8d8bbe5f86639f80a37868ee"},"cell_type":"markdown","source":"## Keras VGG19 + Data Augmentation + Transfer Learning, Kaggle Plant Seedlings Classification \n\nSimple Keras implementation with Transfer Learning. \n\n* You must run this on a GPU.\n<br>\n\n[MY GITHUB](https://github.com/AtriSaxena/)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport cv2\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nfrom keras.utils import np_utils\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model \nfrom keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3326f1388ce01fa6d1989afc682202b726247609"},"cell_type":"code","source":"CATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n              'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\nNUM_CATEGORIES = len(CATEGORIES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8660ec6c99788ab52f55ca8dd2a5a8724c085fc0"},"cell_type":"code","source":"SEED = 1987\ndata_dir = '../input/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7099ad30d67386be7980bc2e414bb6a60ff4f0ef"},"cell_type":"markdown","source":"### Number of training images for each Category"},{"metadata":{"trusted":true,"_uuid":"812183d40b80919fb5d2b0a09a2cb8cba15bc1d6","scrolled":true,"collapsed":true},"cell_type":"code","source":"for category in CATEGORIES:\n    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_dir, category)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6763d312e10f497b0a6c434508cca9a1dfd3621","collapsed":true},"cell_type":"code","source":"train = []\nfor category_id, category in enumerate(CATEGORIES):\n    for file in os.listdir(os.path.join(train_dir, category)):\n        train.append(['train/{}/{}'.format(category, file), category_id, category])\ntrain = pd.DataFrame(train, columns=['file', 'category_id', 'category'])\ntrain.head(2)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e51cc8c2972498dc59770ad38789648d911e3ee","collapsed":true},"cell_type":"code","source":"test = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file), file])\ntest = pd.DataFrame(test, columns=['filepath', 'file'])\ntest.head(2)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f4631dde3a93b40fe3e58fdcbdc38f9e5a6bd70"},"cell_type":"markdown","source":"### See some of the Images"},{"metadata":{"trusted":true,"_uuid":"dff15c1e9c995eac5ff5ec7fb4e2d9e82c551b8b","collapsed":true},"cell_type":"code","source":"fig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES))\ngrid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05)\ni = 0\nfor category_id, category in enumerate(CATEGORIES):\n    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n        ax = grid[i]\n        img = Image.open(\"../input/\"+filepath)\n        img = img.resize((240,240))\n        ax.imshow(img)\n        ax.axis('off')\n        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1:\n            ax.text(250, 112, filepath.split('/')[1], verticalalignment='center')\n        i += 1\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"152a64f1027292a5fe224f7640322b2eee730015"},"cell_type":"markdown","source":"## Model Preparation"},{"metadata":{"trusted":true,"_uuid":"86eb75b30a81fb16a022fb79f0f4bdfff3ba7e09","scrolled":true,"collapsed":true},"cell_type":"code","source":"model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (240, 240, 3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae2c3181315d6c87ae7988e32fcb8fafd4e264f4"},"cell_type":"markdown","source":"### Freezing first few layers\n\nfreeze the first few layers as these layers will be detecting edges and blobs,"},{"metadata":{"trusted":true,"_uuid":"b12bc24293fc95f57cb5b905592ea581410e86ce","collapsed":true},"cell_type":"code","source":"for layer in model.layers[:5]:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb32f72977f25a4b96b1b247089e1068064b1bf7"},"cell_type":"markdown","source":"### Adding output Layer"},{"metadata":{"trusted":true,"_uuid":"877d5e590c492e0aa39a6d701d513314ae0f33cf","collapsed":true},"cell_type":"code","source":"x = model.output\nx = Flatten()(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(1024, activation=\"relu\")(x)\npredictions = Dense(12, activation=\"softmax\")(x) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3595451acbc2ada1a9498f5d2a0822707d654eb","collapsed":true},"cell_type":"code","source":"model_final = Model(input = model.input, output = predictions)\n#compling our model\nmodel_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c69f442273fa79483146c1152396614c18b35dee","scrolled":true,"collapsed":true},"cell_type":"code","source":"model_final.summary() #Model summary","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d9be455e5ae87f7ca50d4914e8f83f2aba72de4"},"cell_type":"markdown","source":"## Data Augmentation"},{"metadata":{"trusted":true,"_uuid":"fe1f5492d35f66ea15bb4afbdb47201f8dc6be13","collapsed":true},"cell_type":"code","source":"gen = ImageDataGenerator(\n            rotation_range=360.,\n            width_shift_range=0.3,\n            height_shift_range=0.3,\n            zoom_range=0.3,\n            horizontal_flip=True,\n            vertical_flip=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00940fef5caf50923526641e52d10a8a2dd21bb4","collapsed":true},"cell_type":"code","source":"train_data_dir = \"../input/train\"\ntrain_generator = gen.flow_from_directory(\n                        train_data_dir,\n                        target_size = (240, 240),\n                        batch_size = 16, \n                        class_mode = \"categorical\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c88ef71b87222e42507af04a109677613a08529","collapsed":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\nearly = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=1, mode='auto')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8292b1ad43391d637d46fd10eaeb040715b32d3d"},"cell_type":"markdown","source":"### Train our Model"},{"metadata":{"trusted":true,"_uuid":"23257aaa09dbbc2d21c2bac200a9aa6a40daec7a","scrolled":false,"collapsed":true},"cell_type":"code","source":"model_final.fit_generator(\n                    train_generator,\n                    epochs = 50,\n                    shuffle= True,\n                    callbacks = [checkpoint, early])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c9f509ca511efffffa08620a98c1c3f86bb966e"},"cell_type":"markdown","source":"## Predicting the test images from trained model"},{"metadata":{"trusted":true,"_uuid":"7e5c1aab36b8f5385e23ed109fd5297bcdf5a06f","collapsed":true},"cell_type":"code","source":"classes = train_generator.class_indices  \nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"334057468e9fc151e9b06758efe40189a37d7797","collapsed":true},"cell_type":"code","source":"#Invert Mapping\nclasses = {v: k for k, v in classes.items()}\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d6fda46ac6fe15d5744f4373ae328b2cafbcfc8"},"cell_type":"markdown","source":"### Prediction on each image"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"42ec1709f17c7702add9357fbae6e9a396349829"},"cell_type":"code","source":"prediction = []\nfor filepath in test['filepath']:\n    img = cv2.imread(os.path.join(data_dir,filepath))\n    img = cv2.resize(img,(240,240))\n    img = np.asarray(img)\n    img = img.reshape(1,240,240,3)\n    pred = model_final.predict(img)\n    prediction.append(classes.get(pred.argmax(axis=-1)[0])) #Invert Mapping helps to map Label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"552f422733c54922988dd88776ba640589c4a06a","collapsed":true},"cell_type":"code","source":"test = test.drop(columns =['filepath']) #Remove file path from test DF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b562c5a66684e4914ca759e0f021f22890090517","collapsed":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cfa63d5328b6b10171738a77391bc0446be8048","scrolled":false,"collapsed":true},"cell_type":"code","source":"pred = pd.DataFrame({'species': prediction})\ntest =test.join(pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87a5d1e61b28c36e1f5ff659799bc8d6d2c0de48"},"cell_type":"markdown","source":"### Final submission File"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8ac1053202f8ffa448d967df57477193942a8dcb"},"cell_type":"code","source":"test.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1f47feb769dd7a905215ca818efb6d1b4e3dc97","collapsed":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}