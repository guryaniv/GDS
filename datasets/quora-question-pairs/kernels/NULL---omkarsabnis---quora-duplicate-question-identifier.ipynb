{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Quora Question Pairs - Identifying Duplicate Questions\nIn this notebook, our main goal is to identify which questions asked are duplicates of questions that have already been asked.\nWe have to submit a binary prediction against a log - loss metric.\nI am doing a late submission to the competition, but this was a very interesting NLP problem so I wanted to solve it!\nScore on the private leaderboard - 0.353 "},{"metadata":{"_uuid":"34fc45ba587c41952f3b3dfc6e7054703479c0fc"},"cell_type":"markdown","source":"**(1). IMPORTING ALL THE REQUIRED MODULES**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Data Analysis of the Quora Question Pairs Dataset\n# Using the XGBoost Classifier for prediction\n# By - Omkar Sabnis: 22-05-2018\n\n\n# IMPORTING MODULES\nimport numpy as np\nprint(np.__version__)\nimport pandas as pd\nprint(pd.__version__)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(sns.__version__)\ncolors = sns.color_palette()\nimport os,gc\nfrom nltk.corpus import stopwords\nfrom sklearn import metrics\nfrom sklearn.cross_validation import train_test_split\nimport xgboost as xgb","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"59ebaf18f1a66e138cd498c2a912ed4139622ea0"},"cell_type":"markdown","source":"**(2). LOADING AND PRINTING SOME USEFUL INFORMATION**"},{"metadata":{"trusted":true,"_uuid":"579ce21cfdf20dd8d4ee32b6d0a59594f51b3da7","collapsed":true},"cell_type":"code","source":"# LOADING AND VISUALIZING THE DATASET\ntrainingset = pd.read_csv(\"../input/train.csv\")\ntestingset = pd.read_csv(\"../input/test.csv\")\n\nprint(\"Number of question pairs in trainingset: \", len(trainingset))\nprint(\"Number of duplicate pairs: \" , round(trainingset['is_duplicate'].mean()*100,2))\nquestions = pd.Series(trainingset['qid1'].tolist() + trainingset['qid2'].tolist())\nprint(\"Number of questions in trainingset:\", len(np.unique(questions)))\nprint(\"Multiple questions : \", np.sum(questions.value_counts()>1))\n\nplt.figure(figsize=(10,9))\nplt.hist(questions.value_counts(),bins=60)\nplt.yscale('log',nonposy='clip')\nplt.xlabel('Occurence of questions')\nplt.ylabel('Number of questions')\nplt.show()","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"ec556bf0763c5b39311e6fee9181ddf72abb4763"},"cell_type":"markdown","source":"**Conclusion:**\n1. Many questions appear very few times, while there is a question that has appeared 160 times!\n2. There are 36.92 % repeated questions.\n3. Since we are using log-loss, if we predict the mean value of the label, it will be good enough!"},{"metadata":{"_uuid":"2feb9d844a30e420138dd60258906a874a505793"},"cell_type":"markdown","source":"**(3). Feature Analysis:**\nI have used the word share feature from the benchmark model for checking the features and their impact."},{"metadata":{"trusted":true,"_uuid":"4d13fee07e71ecec27ed7ac34e505c98d0580c64","collapsed":true},"cell_type":"code","source":"# FEATURE ANALYSIS USING WORD_MATCH_SHARE\nstopword = set(stopwords.words('english'))\ndef word_match_share(row):\n    ques1 = {}\n    ques2 = {}\n    for i in str(row['question1']).lower().split():\n        if i not in stopword:\n            ques1[i] = 1\n    for i in str(row['question2']).lower().split():\n        if i not in stopword:\n            ques2[i] = 1\n    if len(ques1) == 0 or len(ques2) == 0:\n        return 0\n    q1_shared = [word for word in ques1.keys() if word in ques2]\n    q2_shared = [word for word in ques2.keys() if word in ques1]\n    rr = (len(q1_shared)+len(q2_shared))/(len(ques1)+len(ques2))\n    return rr\nplt.figure(figsize=(10,9))\ntrainmatch = trainingset.apply(word_match_share,axis=1,raw=True)\nplt.hist(trainmatch[trainingset['is_duplicate']==0],bins=10,normed=True,label='NOT DUPLICATE')\nplt.hist(trainmatch[trainingset['is_duplicate']==1],bins=10,normed=True,label='DUPLICATE')\nplt.legend()\nplt.title(\"LABEL DISTRIBUTION - FEATURE ANALYSIS\")\nplt.show()","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"31a35561bc10b244480ef0550f7e29b0d194c21a"},"cell_type":"markdown","source":"From the graph, we can see that the feature has an impact as it can easily distinguish between the duplicate and non-duplicate  questions!"},{"metadata":{"trusted":true,"_uuid":"8f98071039c3390a9d3cb52ba5ae50b82727abb9","collapsed":true},"cell_type":"code","source":"# AUC ON TESTING SET\nfrom sklearn import metrics\nprob = trainingset['is_duplicate'].mean()\nf,t,threshold = metrics.roc_curve(trainingset['is_duplicate'], np.zeros_like(trainingset['is_duplicate']) + prob)\nprint(\"Score: \", metrics.auc(f,t))\nsubmissionprob = pd.DataFrame({'test_id':testingset['test_id'],'is_duplicate':prob})\nprint(submissionprob.head())","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"a1982d73a55cd7c4e41ecea99995dc1adace5876"},"cell_type":"markdown","source":"**(4). Text Analysis - Character and Word counts **"},{"metadata":{"trusted":true,"_uuid":"1813aceb1274f61e09ddc7a0f81db12ce6052177","collapsed":true},"cell_type":"code","source":"# STUDY OF CHARACTER COUNT AND PROBABILITY OF DUPLICATE QUESTIONS\ntrainques = pd.Series(trainingset['question1'].tolist() + trainingset['question2'].tolist()).astype(str)\ntestques = pd.Series(testingset['question1'].tolist() + testingset['question2'].tolist()).astype(str)\n\ntrain = trainques.apply(len)\ntest = trainques.apply(len)\nplt.figure(figsize=(10,9))\nplt.hist(train,bins=100,range=[0,180],color=colors[4],normed=True,label='train')\nplt.hist(test,bins=100,range=[1,180],color=colors[3],normed=True,label='test')\nplt.title(\"Character Count vs Probability\")\nplt.legend()\nplt.xlabel(\"Characters\")\nplt.ylabel(\"Probability\")\nplt.show()\nprint(\"Training Mean: \", train.mean())\nprint(\"Training Standard Deviation: \", train.std())\nprint(\"Testing Mean: \", test.mean())\nprint(\"Testing Standard Deviation: \", test.std())","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"661212e30135a3afec09c026a9a2e8b8bed042f3"},"cell_type":"markdown","source":"**Conclusion:**\n1. Questions have characters between 15 - 150.\n2. 150 words seems to be a word limit as there is a drop. \n3. Maybe the outliers are dummy questions added to the dataset?"},{"metadata":{"trusted":true,"_uuid":"f3bd452bf85a1f9e2de4468c8ffc37d11499bf5c","collapsed":true},"cell_type":"code","source":"# STUDY OF WORD COUNT AND PROBABILITY OF DUPLICATE QUESTIONS\ntrain = trainques.apply(lambda x: len(x.split(' ')))\ntest = testques.apply(lambda x: len(x.split(' ')))\n\nplt.figure(figsize=(10,9))\nplt.hist(train,bins=100,range=[0,50],color='green',normed=True,label='train')\nplt.hist(test,bins=100,range=[0,50],color='red',normed=True,label='test')\nplt.title(\"Word Count vs Probability\")\nplt.legend()\nplt.xlabel(\"Words\")\nplt.ylabel(\"Probability\")\nplt.show()\nprint(\"Training Mean: \", train.mean())\nprint(\"Training Standard Deviation: \", train.std())\nprint(\"Testing Mean: \", test.mean())\nprint(\"Testing Standard Deviation: \", test.std())","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"0b8f72232baaf8f6d3b27c375abc20a903f3558a"},"cell_type":"markdown","source":"**Conclusion:**\n1. Most questions have 10 words in them.\n2. The test set is much larger, however, the histogram shows cluttered question length.\n3. The test set is a dummy, which is proven by this histogram - due to it being so clustered."},{"metadata":{"_uuid":"1b17fd6a9ea239ed6e33769e801cbf1b34167e3f"},"cell_type":"markdown","source":"**(5). Rebalancing the Data**\nSince, the data in the training set has 37% duplicity and the testing set has only 17% duplicity, it can reduce the XGBoost accuracy. By rebalancing the training set to have 17% duplicity, we can increase the performance."},{"metadata":{"trusted":true,"_uuid":"47395b67cc6c2f66f8d94ca071a4dbdea1978596","collapsed":true},"cell_type":"code","source":"# REBALANCING THE DATA\nx_train = pd.DataFrame()\nx_test = pd.DataFrame()\nx_train['word_match'] = trainmatch\nx_test['word_match'] = testingset.apply(word_match_share,axis=1,raw=True)\ny_train = trainingset['is_duplicate'].values\npos = x_train[y_train == 1]\nneg = x_train[y_train == 0]\n\n#OVERSAMPLE THE NEGATIVE CLASS\np = 0.165\nscale = ((len(pos)/(len(pos)+len(neg)))/p)-1\nwhile scale >1:\n    neg = pd.concat([neg,neg])\n    scale-=1\nneg = pd.concat([neg,neg[:int(scale*len(neg))]])\nx_train = pd.concat([pos,neg])\ny_train = (np.zeros(len(pos))+1).tolist() + np.zeros(len(neg)).tolist()\ndel pos,neg","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4acb418fe3cf5be807693457a1b84c927a00f5c0"},"cell_type":"code","source":"# SPLITTING THE DATASET\nx_train,x_check,y_train,y_check = train_test_split(x_train,y_train,test_size = 0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e12b168fe1a774d5a8e496a9a36a6bce6720c5bb","collapsed":true},"cell_type":"code","source":"#XGBOOST CLASSIFIER\nparameters = {}\nparameters['objective'] = 'binary:logistic'\nparameters['eval_metric'] = 'logloss'\nparameters['eta'] = 0.05\nparameters['max_depth'] = 5\n\ndtrain = xgb.DMatrix(x_train, label=y_train)\ndvalid = xgb.DMatrix(x_check, label=y_check)\n\nchecklist = [(dtrain, 'train'), (dvalid, 'valid')]\n\nbst = xgb.train(parameters, dtrain, 400, checklist, early_stopping_rounds=50, verbose_eval=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b95f2cbc34fae482687ea51d4c857320588e9565","collapsed":true},"cell_type":"code","source":"# GENERATING SUBMISSION FILE\ndtest = xgb.DMatrix(x_test)\nptest = bst.predict(dtest)\nsubmission = pd.DataFrame()\nsubmission['test_id'] = testingset['test_id']\nsubmission['is_duplicate'] = ptest\nsubmission.to_csv(\"XGBOOST.csv\",index=False)\nprint(submission.head())","execution_count":1,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}