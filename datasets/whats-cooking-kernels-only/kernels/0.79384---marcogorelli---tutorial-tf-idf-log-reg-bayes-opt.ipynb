{"cells":[{"metadata":{"_uuid":"3c7f6f39b9115491bc07e0fe4ca1675d1579a768"},"cell_type":"markdown","source":"# What's cooking?\n\nHere, we predict what country a given cuisine comes from, based on its ingredients.\n\nWe choose a very simple model (logistic regression) and obtain a score only a few percentage points worse off than the top ones on the leaderboard, but with the important difference that this kernel takes less than 15 minutes (including  tuning) to run as opposed to several hours."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"906d508a4082e1652ecc351e0e89fddcb14318f1"},"cell_type":"markdown","source":"### Load data, basic preprocessing"},{"metadata":{"_uuid":"6d61a700afa9b7e9a360588fda4ff99e1e5cc68f"},"cell_type":"markdown","source":"We begin by reading in the data, and inspecting it. We see that each row corresponds to a recipe. Our objective is to predict `cuisine` given `ingredients`."},{"metadata":{"trusted":true,"_uuid":"7bd1e26861fe0ae6af30bd6bb3d0dd38173c5eae"},"cell_type":"code","source":"train = pd.read_json('../input/train.json')\ntest = pd.read_json('../input/test.json')\nprint('Train size is', train.shape)\nprint('Test size is', test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a03c27149ddf0e4ba2251213af278ef92a53fec"},"cell_type":"markdown","source":"As the ingredients are given as a list, join them together into a string."},{"metadata":{"trusted":true,"_uuid":"3aab56bbf462b5adb526ae48e2294a1b76b24b4c"},"cell_type":"code","source":"for t in (train, test):\n    t.set_index('id', inplace=True)\n    t.ingredients = t.ingredients.str.join(' ')\n    \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87b53c46ba3b613d6915188f80128fa9f7569238"},"cell_type":"markdown","source":"Then, give each ingredient its own column, whence the rows will be `1` if that particular recipe contains that ingredient, as `0` otherwise. Note that most values will now be `0`, so our dataset will be saved as a sparse matrix, so we need to call `to_dense` to visualise it.\n\nWe see that, for the column corresponding to 'romaine lettuce', the first row is `1` and the next four are `0`, as expected."},{"metadata":{"trusted":true,"_uuid":"b0e9516bfdbf85f45ec5a72c9b72104c99b25e05"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk import word_tokenize  \nclass LemmaTokenizer(object):\n     def __init__(self):\n         self.wnl = WordNetLemmatizer()\n     def __call__(self, doc):\n         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n\n#count_vec = CountVectorizer(tokenizer=lambda x: [i.strip() for i in x.split(',')], min_df=10)\ncount_vec = CountVectorizer(tokenizer=LemmaTokenizer(), min_df=5)\nX_train = count_vec.fit_transform(train.ingredients)\nX_test = count_vec.transform(test.ingredients)\n\nX_train[:5, count_vec.vocabulary_['lettuce']].todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4489891913a67efac70fd46354417918eb56e7b"},"cell_type":"code","source":"count_vec.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c2ab9e17c3ff70a6e117d1ecd9ca3161b976b7d"},"cell_type":"markdown","source":"Visualise the target variable.\n\n*Note: many kernels in this competition use a label encoder at this stage. However, when using one of sklearn's classifiers, this is unnecessary.*"},{"metadata":{"trusted":true,"_uuid":"442ac07297176ac91309f31af02d02cb8d96fd28","scrolled":true},"cell_type":"code","source":"y_train = train.cuisine\ny_train.value_counts().sort_values().plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f9c4abd91dbfce358dc8f3ce559aa5066e57e61"},"cell_type":"markdown","source":"### TFIDF\n\nThis next cell performs the following:\n- each row is split into separate ingredients ('vectors');\n- a new column is created for ingredient, where the row value corresponds to how many times that particular ingredient was present;\n- rows are multiplied by their inverse-document frequency: this is $\\ln((d+1)/(n+1))+1$, where $d$ is the number of rows containing the corresponding word and $n$ is the total number of rows;\n- finally, rows are normalised by dividing by their $L2$ norm."},{"metadata":{"trusted":true,"_uuid":"2f9fee293d11c50fb0eb07b2047f830ea7ea60e2"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf = TfidfTransformer()\nX_train_idf = tfidf.fit_transform(X_train)\nX_test_idf = tfidf.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90c7fae2b8d1fbabdc5c572b29c897fa1ba08d58"},"cell_type":"markdown","source":"Look at how this changes our rows in the column corresponding to 'romaine lettuce'."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"03670748ccfb48210a0b73833997dc02ac69df59"},"cell_type":"code","source":"X_train_idf[:5, count_vec.vocabulary_['lettuce']].todense()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"020c875fe74c33c936f39cd34f1e2135de8a2727"},"cell_type":"markdown","source":"Let's try to match the top row by hand (because we'd rather undersand the tools we use...right?)."},{"metadata":{"trusted":true,"_uuid":"ff3a8d74d6a2b60b2694a8086635bf884a51854e"},"cell_type":"code","source":"n = train.shape[0]\nfirst_row_tfidf = []\nfirst_row_ingredients = train.ingredients.iloc[0]\nfirst_row_lemmas = LemmaTokenizer()(first_row_ingredients)\nfrom sklearn.metrics.pairwise import cosine_similarity\nfor i in first_row_lemmas:\n    d = np.sum((X_train[:, count_vec.vocabulary_[i]]==1).toarray(), axis=0)[0]\n    if d<5:\n        continue\n    idf = np.log((n+1)/(d+1))+1\n    first_row_tfidf.append(idf)\n    if i=='lettuce':\n        print('The ingredient \"lettuce\" appears in {} recipes, so d={}.'.format(d, d))\n        print('In total, there are {} recipes, so n={}.'.format(n, n))\n        print('Substituting into the formula above, we get an idf of {}.'.format(idf))\n        \nour_result = first_row_tfidf[first_row_lemmas.index('lettuce')]/np.linalg.norm(np.array(first_row_tfidf))\nprint('Normalising across ingredients, we get {}, which (almost) matches sklearn\\'s result.'.format(our_result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"44079ae89375157eef7f90872ba30221bff3f9e6"},"cell_type":"markdown","source":"### Linear model: logistic regression\n\nWe use Bayesian Optimization to tune the regularization parameter in logistic regression."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"299223b096ca18c80a5474d7b8ff90065a96fb79"},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef optimise_lr(X, y, C):\n\n    def target(C):\n        clf = Pipeline(\n            [(\"tf_idf\", TfidfVectorizer(tokenizer=LemmaTokenizer(), min_df=5)),\n             (\"lr\", LogisticRegression(C=10**C))])\n        cv_results = np.mean(cross_val_score(clf, X, y, cv=5))\n        return cv_results\n    \n    bo = BayesianOptimization(target, {'C': C})\n    bo.maximize(init_points=2, n_iter=10)\n    return bo.res['max']['max_params']\n\nimport warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    best_params = optimise_lr(train.ingredients, train.cuisine, (-3, 2))\nprint(best_params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c8eae8a710a190601b8929b9ba62f76938b9959"},"cell_type":"markdown","source":"![](https://i.imgur.com/XVLoRHA.jpg)\n\nLogistic regression can be thought of as a neural network with no hidden layers. So...let's add a hidden layer!"},{"metadata":{"trusted":true,"_uuid":"d1fc865f497a80021d9fd25b01d9bbfb0d09d02d"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\n\nmodel = Sequential([\n    Dropout(.2, input_shape=(X_train.shape[1],)),\n    Dense(2048, activation='relu'),\n    Dropout(.2),\n    Dense(len(set(y_train)), activation='softmax'),\n])\n\nmodel.compile(optimizer='adam',\n              loss='categorical_hinge',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"406f6272b082590257854202a655315010a3deba","scrolled":false},"cell_type":"code","source":"import keras\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n\nlb = LabelEncoder()\nlb_train = lb.fit_transform(y_train)\n\none_hot_labels = keras.utils.to_categorical(lb_train, num_classes=len(set(y_train)))\nhistory = model.fit(X_train_idf, one_hot_labels, validation_split=0.33, epochs=50, batch_size=128, callbacks=[early_stopping])\nimport matplotlib.pyplot as plt\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04efb512afe172766d4ff9f926037cc2beba5abb"},"cell_type":"markdown","source":"### Submit!"},{"metadata":{"trusted":true,"_uuid":"9a5f5a423a7ef35b927315c44ae099a4f1ca4a35"},"cell_type":"code","source":"preds = model.predict(X_test_idf, batch_size=32)\ntest['cuisine'] = lb.inverse_transform(np.argmax(preds, axis=1))\ntest.reset_index()[['id', 'cuisine']].to_csv('preds.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}