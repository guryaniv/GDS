{"nbformat_minor": 1, "cells": [{"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "execution_count": null, "metadata": {"_uuid": "e5b7147a607eadc89778d8045cc4ff040036fd71", "_cell_guid": "edbfce39-fa2a-4616-a8d4-87362749e8f4"}, "cell_type": "code"}, {"source": ["train = pd.read_csv('../input/train.csv')\n", "train.head()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "d9e403e9ebb31423e8a8303ec5b645d8c90e92f0", "_cell_guid": "a12c2147-d021-41c7-9de3-04e52c70c477"}, "cell_type": "code"}, {"source": ["import nltk as nl\n", "train['tokens'] = [nl.word_tokenize(sentences) for sentences in train.text]\n", "words = []\n", "for item in train.tokens:\n", "    words.extend(item)\n", "\n", "stemmer = nl.stem.lancaster.LancasterStemmer()\n", "words = [stemmer.stem(word) for word in words]\n", "\n", "\n", "filtered_words = [word for word in words if word not in nl.corpus.stopwords.words('english')]\n", "\n", "\n", "\n", "import gensim\n", "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n", "model = gensim.models.Word2Vec(filtered_words, size=100)\n", "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "3e066e8c2827e3c690314c124f1cce3086817980", "_cell_guid": "75898742-f35d-4d55-a9ef-45d145de1c88"}, "cell_type": "code"}, {"source": ["i = 0\n", "for index,item in train.iterrows():\n", "    if(i < len(item['tokens'])):\n", "        i = len(item['tokens'])\n", "print(i)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "eea842f492662909bb6a584a802a1fba82b4cf20", "_cell_guid": "3300183c-6963-4ea8-a231-869abdcf7932"}, "cell_type": "code"}, {"source": ["training = []\n", "i = 0\n", "for index,item in train.iterrows():\n", "    vec = np.zeros(100)\n", "    token_words = [stemmer.stem(word) for word in item['tokens']]\n", "    token_words = [word for word in token_words if word not in nl.corpus.stopwords.words('english')]\n", "    for w in token_words:\n", "        if w in w2v:\n", "            vec += w2v[w]\n", "    norm = np.linalg.norm(vec)\n", "    if norm != 0:\n", "        vec /= np.linalg.norm(vec)\n", "    \n", "    \n", "    training.append([vec,item['author']])"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "2e1435c03a40c5fe0235916b3ad86ca1807baf72", "_cell_guid": "370a0bbf-0e11-4e63-b3d3-37df8753290d"}, "cell_type": "code"}, {"source": ["training_new = np.array(training)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "ff3c22c35aa3e3750ae9d0e2325ac13d72891a59", "_cell_guid": "9c2a2c3b-5fca-4ff9-9f54-f72c8d43a856"}, "cell_type": "code"}, {"source": ["from numpy import array\n", "\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.preprocessing import OneHotEncoder\n", "\n", "# integer encode\n", "label_encoder = LabelEncoder()\n", "integer_encoded = label_encoder.fit_transform(training_new[:,1])\n", "\n", "# binary encode\n", "onehot_encoder = OneHotEncoder(sparse=False)\n", "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n", "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n", "\n", "train_y = onehot_encoded"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "d833100c1b53eede1421cec6eed956a801c2f9de", "_cell_guid": "61c0e064-2231-4541-b2ba-05ea335c850c"}, "cell_type": "code"}, {"source": ["train_x = list(training_new[:,0])\n"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "a629b7311fce00d00f4d31e6a728d9bf67b38ca3", "_cell_guid": "b582166f-7cd2-4cae-89f0-2a107604fcf1"}, "cell_type": "code"}, {"source": ["import tensorflow as tf\n", "import tflearn\n", "# reset underlying graph data\n", "tf.reset_default_graph()\n", "# Build neural network\n", "net = tflearn.input_data(shape=[None, len(train_x[0])])\n", "net = tflearn.fully_connected(net, 8)\n", "net = tflearn.fully_connected(net, 8)\n", "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n", "net = tflearn.regression(net)\n", " \n", "# Define model and setup tensorboard\n", "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n", "# Start training (apply gradient descent algorithm)\n", "model.fit(train_x, train_y, n_epoch=10, batch_size=8, show_metric=True)\n", "model.save('model.tflearn')"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "3c0242f2c05d4d8468a4b2e28c724edba16a5688", "_cell_guid": "acf6ef90-839c-4079-818f-d221ea7ccfde"}, "cell_type": "code"}, {"source": ["test = pd.read_csv('../input/test.csv')\n", "test.head()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "e57c9859e4a2662d0c2a51c63d7b18f3873bfbb6", "_cell_guid": "528f3b95-b24c-4cf0-9b97-9b245164331c"}, "cell_type": "code"}, {"source": ["test['tokens'] = [nl.word_tokenize(sentences) for sentences in test.text]\n"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "0b589ae71f2fa24084a6f7147624d80d652717c7", "_cell_guid": "cdd0f422-535f-4f6d-97d4-2444c39c455c"}, "cell_type": "code"}, {"source": ["testing = []\n", "for index,item in test.iterrows():\n", "    vec = np.zeros(100)\n", "    token_words = [stemmer.stem(word) for word in item['tokens']]\n", "    token_words = [word for word in token_words if word not in nl.corpus.stopwords.words('english')]\n", "    for w in token_words:\n", "        if w in w2v:\n", "            vec += w2v[w]\n", "    norm = np.linalg.norm(vec)\n", "    if norm != 0:\n", "        vec /= np.linalg.norm(vec)\n", "    \n", "    \n", "    testing.append(vec)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "d567109523a2ab77d04b170c571e015248e98d1f", "_cell_guid": "69666c23-393b-4c15-8c9d-296bbfc49f4f"}, "cell_type": "code"}, {"source": ["testing = list(np.array(testing))\n", "predicted = model.predict(X=testing)\n", "result_val = pd.DataFrame(predicted)\n", "result_val.columns = [\"EAP\",\"HPL\",\"MWS\"]\n", "result = pd.DataFrame(columns=['id'])\n", "result['id'] = test['id']\n", "result['EAP'] = result_val['EAP']\n", "result['HPL'] = result_val['HPL']\n", "result['MWS'] = result_val['MWS']\n", "result.head()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "f63c5cde89989e92b98a0bf66d2d4108a4ba7cb6", "_cell_guid": "dbf489be-6225-4a41-9bb7-a3bf19e7b077"}, "cell_type": "code"}, {"source": [], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "ed72f7ea81f40dd057bde7830f104136f330f535", "_cell_guid": "5eb6f709-5d26-4d21-bbfa-21f54a896477"}, "cell_type": "code"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "version": "3.6.3"}}, "nbformat": 4}