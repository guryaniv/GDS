{"cells": [{"cell_type": "markdown", "source": ["## Word embeddings with Gensim\n", "\n", "The importance of encoding text data is crucial for Deep Learning models. A model that encodes the similarity and proximity between words in the representation itself intuitively should work better for many tasks and it has been proved to be so - it is not always the best choice though: it's no silver bullet.\n", "\n", "Two of the most important models for word representation in the n-dimensional space are [word2vec](https://arxiv.org/abs/1310.4546) and [GloVe](https://nlp.stanford.edu/projects/glove/). \n", "\n", "In this tutorial, I will show how to use [Gensim](https://radimrehurek.com/gensim/index.html) in order to use both word2vec and GloVe encodings for text data.\n", "\n", "I assume you already know how to setup an environment for machine learning development with Python. If you don't, take a look at [this tutorial](https://medium.com/cocoaacademymag/basic-tools-for-machine-learning-85e887224ee4) on the basic tools for Machine Learning, which has everything you will need to follow this one.\n", "\n", "### Summary\n", "\n", "* \u2705 Installing and importing Gensim\n", "* \u2705 Creating a word2vec model from text data\n", "* Creating a GloVe model from text data\n", "* Intrinsic evaluation for both models\n", "* Extrinsic evaluation for both models"], "metadata": {"_cell_guid": "4f572eef-47be-40a0-a01c-14aa40891e25", "_uuid": "c178b3ad1ef833e83fe290c5a6e7a1a64e938435"}}, {"cell_type": "markdown", "source": ["# word2vec\n", "\n", "## Preparing the text to train the model\n", "\n", "In this example, I will open a csv file, get the text from it, split it into the different lines, then I split each line into \"words\" - actually, I should use a more sophisticated method to separate the words, but since this is just an example, I will use the space as a boundary between words, *which is absolutely naive and should not be done in production* - stripping the ponctuation in order to clean the corpus a little. In _real life_ you should use a tokenizer in order to separate the tokens to be vectorized and also in order to handle ponctuation properly. Depending on the task, it might also be helpful to lemmatize the tokens.\n", "\n", "My `sentences` variable will store a list of lists of strings, where each string ~roughly~ represents a word."], "metadata": {"_cell_guid": "a8dfd008-9782-44d7-8849-a43553534aa1", "_uuid": "e137fcc35264c842f8072a69a4747d58695b5e41"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["import pandas as pd\n", "df = pd.read_csv('../input/train.csv')\n", "corpus_text = '\\n'.join(df[:5000]['comment_text'])\n", "sentences = corpus_text.split('\\n')\n", "sentences = [line.lower().split(' ') for line in sentences]"], "metadata": {"collapsed": true, "_cell_guid": "b83a560a-4ead-4139-bed9-20fea0806427", "_uuid": "68f9c07ff274c9fc01ab5a2b9bcaebddfc008a1a"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def clean(s):\n", "    return [w.strip(',.\"!?:;()\\'') for w in s]\n", "sentences = [clean(s) for s in sentences if len(s) > 0]"], "metadata": {"collapsed": true, "_cell_guid": "18a09cc6-424a-44e3-9698-27ebdbca55b2", "_uuid": "8e9e5a08ee23a1dc9ebbb4db002090b59fae712a"}}, {"cell_type": "markdown", "source": ["## Training the model\n", "\n", "Once we have the sentences, we can use `Gensim` to create a model for us.\n", "Here's a simple way to do it:"], "metadata": {"_cell_guid": "dd4c9adf-5c76-46f0-95ac-fe36f9230b93", "_uuid": "1224cca9290ff75fd8c023732ab57951aa883a8e"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from gensim.models import Word2Vec\n", "\n", "model = Word2Vec(sentences, size=100, window=5, min_count=3, workers=4)"], "metadata": {"collapsed": true, "_cell_guid": "a5e1c300-03eb-4dfe-8e24-4f20d75e097f", "_uuid": "f36be6330aca74250bc8bbda63e479abcfd7c4ae"}}, {"cell_type": "markdown", "source": ["Of course, you can change the hyperparameters such as window size or the dimensions of the resulting vectors to get better results.\n", "If our model is too big, and we're done training it we can delete it keeping only the vectors."], "metadata": {"_cell_guid": "0738e214-7043-4a6d-b577-276ceae0a97e", "_uuid": "7bf1943de47ca5baa9a03b852d2aa87c70abcdf0"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["vectors = model.wv\n", "del model"], "metadata": {"collapsed": true, "_cell_guid": "11bc5378-b827-4c28-83b0-a8560d6ef861", "_uuid": "8a5e9d85374d8b5910b7a2244f099791a7422c7f"}}, {"cell_type": "markdown", "source": ["## Using the vectors\n", "\n", "Now, for each word (as represented in a string), we can get its appropriate vector."], "metadata": {"_cell_guid": "ba772f7f-b101-4d17-872b-15f2b78e895b", "_uuid": "4cd0fb16ecb2eee34e1f4000f3c0a77274affd07"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["vectors['good']"], "metadata": {"_cell_guid": "a77486d5-3c8b-4a5d-b8b9-edb4e6e65117", "_uuid": "efa09a4bbdd7f3d28428a139f2a61f3c1b4a58e5"}}, {"cell_type": "markdown", "source": ["We can also compare words in order to assess their similarity, \n", "check which word is the most similar to a given word - i.e. the \n", "one with the least distant vector."], "metadata": {"_cell_guid": "7e4799c8-6b23-4ebd-8d30-d07210ddda0a", "_uuid": "55f8fb1cb4dae1ef89e42186ff07e6bef6ffecd9"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(vectors.similarity('you', 'your'))\n", "print(vectors.similarity('you', 'internet'))"], "metadata": {"_cell_guid": "d57f9fda-93ff-4f50-a8b1-3e15227a8012", "_uuid": "dbb4627996fafd6e713af366adf6ecca9f523ad0"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["vectors.most_similar('i')"], "metadata": {"_cell_guid": "74d97c78-1c93-4290-b3d7-8e0abf4eda8d", "_uuid": "a477366ca8b818f8037a04edd1b510cfd2b6b859"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": [], "metadata": {"collapsed": true, "_cell_guid": "0eae119f-2241-4b4e-be60-b45fee7b73e4", "_uuid": "b5289ee31ee4d5d040599ee86f20477b307a55e6"}}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "version": "3.6.3", "name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1}