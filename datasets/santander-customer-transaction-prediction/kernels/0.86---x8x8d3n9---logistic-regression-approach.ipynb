{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')\ndf_train=train.drop(['ID_code', 'target'],axis=1)\ndf_test=test.drop('ID_code',axis=1)\nId_train=train.ID_code\nId_test=test.ID_code\n                 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fcb80f530b5fbf45e2291f2e54595f895149bd2"},"cell_type":"code","source":"import gc\ntarget=train.target\ndel train,test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0303505c74945565e36c7021fceb1babd1aa839b"},"cell_type":"markdown","source":"# Assumption for logistic regresson\n## 1/4 First, binary logistic regression requires the dependent variable to be binary(obviously)\n"},{"metadata":{"_uuid":"15b2a4fa9a2e8d525f96e92f8e0f994cd3b10978"},"cell_type":"markdown","source":"## 2/3  Second, logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data. In this case, observations are some kind of measures for different customers,  **** so we may accept this one."},{"metadata":{"_uuid":"d1436437dd439be7cb41cdbcad2708422d3895d5"},"cell_type":"markdown","source":"## 3/4 Check the correlations of features."},{"metadata":{"trusted":true,"_uuid":"02295203eea5da9dc82c0e897862b0f04f7f86f7"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncorr = df_train.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"753100d388d226a48be22831846aee46077afb82"},"cell_type":"code","source":"f, axes = plt.subplots(2, 2, figsize=(10,10), sharex=False)\nsns.despine(left=True)\nsns.heatmap(corr.iloc[0:49,0:49], center=0,\n            #cmap=cmap,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=axes[0, 0])\nsns.heatmap(corr.iloc[50:99,50:99], center=0,\n            #cmap=cmap,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=axes[0, 1])\nsns.heatmap(corr.iloc[100:149,100:149], center=0,\n            #cmap=cmap,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=axes[1, 0])\nsns.heatmap(corr.iloc[150:199,150:199], center=0,\n            #cmap=cmap,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=axes[1, 1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a5b220d00131c96dff3f5da3b3a18f07cb93937"},"cell_type":"markdown","source":"Seems there is no highly correlated features.\n(1. What if there are some high colinearty?  PCA-> Orthognal features)."},{"metadata":{"_uuid":"82ec0b3af1673bb7471ea06f516d75cac9185fd7"},"cell_type":"markdown","source":"# 4/4  f(Independent variables)  are linearly related to the log odds.\n## In most case, f(X)=X*beta, where beta is coffecient of the features (like linear regresson).  I use MLE estimator  to test this assumption. (In another words, solve model first and check the correlation of X*beta and log odd(log(p/(1-p), p is the probability of Y to be 1)"},{"metadata":{"trusted":true,"_uuid":"fd64566894d106fb49da980a4b92fa86fa041a20"},"cell_type":"code","source":"#feature scaling\ndf_train -= df_train.min() \ndf_train /= df_train.max() \ndf_test -= df_test.min()  # equivalent to df = df - df.min()\ndf_test /= df_test.max() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3476f259ba5f1c15245cdd5b671642c591742a38"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0, solver='sag',fit_intercept=True,C=1000,\n                          multi_class='ovr').fit(df_train, target)\ntarget_hat=clf.predict(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e3d0fb8fde5ff489849a3799919412aa1673b0b"},"cell_type":"code","source":"np.corrcoef(target_hat,target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6feb34b5377065dab4fa63eda58d2ad3f8078da"},"cell_type":"markdown","source":"Correlation coefficient is 0.398, not that bad. This implay that there are some linear relationshape between target and (X*theta)."},{"metadata":{"_uuid":"93d7b139c58a2f111d7dc4bd5c5b3ea424566bcb"},"cell_type":"markdown","source":"# Use L1, L2 control complexity (bais and variance tradeoff ) of Log-regression model. Use 5 folds cross validation choose the proper hyperparameter."},{"metadata":{"_uuid":"ede87cca615471f55dce8d18531dd1ce64f514bb"},"cell_type":"markdown","source":"from sklearn.model_selection import StratifiedKFold\n<br>\nfrom sklearn.model_selection import GridSearchCV\n<br>\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n<br>\nparameters1 = {'C':np.logspace(-9, 0, num=50, endpoint=False, base=10.0).tolist()} \n<br>\nlgr = LogisticRegression(random_state=0, solver='sag',fit_intercept=True, multi_class='ovr')\n<br>\nGRD1= GridSearchCV(lgr, parameters1,scoring='roc_auc',cv=5)\n<br>\nGRD1.fit(df_train, target)\n<br>\nprint(GRD1.best_score_,GRD1.best_params_)\n<br>\n0.8595038749527413 {'C': 0.05495408738576237}\n<br>"},{"metadata":{"_uuid":"2e5f718c622f77ef67e9d5864074e288e78b4fc3"},"cell_type":"markdown","source":"#### Best C is  0.05495408738576237. Note that  The default pernalized type is L1."},{"metadata":{"trusted":true,"_uuid":"49544a0b4d5eae55ae8918df6da91134ad067803"},"cell_type":"code","source":"clf = LogisticRegression(random_state=0, solver='sag',fit_intercept=True,C=0.05495408738576237,\n                          multi_class='ovr').fit(df_train, target)\npred_class=clf.predict(df_test)\npred_prob=clf.predict_proba(df_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6b7cc0f4d855bb7fd98a819607de3176dd98cde"},"cell_type":"code","source":"#help(GRD1.predict)\nsub_class=pd.DataFrame(Id_test)\nsub_prob=pd.DataFrame(Id_test)\nsub_class['target']=pred_class\nsub_prob['target']=pred_prob\n\nsub_class.to_csv('Lgreg_class.csv',index=False)\nsub_prob.to_csv('Lgreg_prob.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fdcc8217b59b77c8791d2172eb3e6c98500c7ca"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}