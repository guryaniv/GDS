{"cells": [{"outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "metadata": {"_uuid": "e965f3bbe0389a0c23f420a5815aec4f08f720ec", "_cell_guid": "c71ee509-8b4b-4d61-becc-b240d9df5bbe"}, "cell_type": "code"}, {"source": ["In this kernel, I explore the **genres** with respect to **listener behaviour **and co-occurrence. Are there pairs or groups of genres, which are commonly listened to by the same users? We could use this type of user-driven genre similarity as a feature in recommender systems, in particular for cold-start cases. Let's say, people who like *alternative rock* commonly also like *indie* - we can recommend *indie* songs to new users who only listened to a few *alternative rock* tracks so far.\n", "\n", "Since we only have the genre-IDs and don't know their \"physical\" meaning, we cannot use any higher-level background info (i.e. from music genre taxonomies). My idea for this kernel is to model genres in a **graph**. Each genre represents a node, associated with a node score which indicates how often this genre appears in the train set. Edges between nodes represent co-occurrences. The edge weights indicate how often a single user in the train set liked (by means of target = 1) both genres. For example, if the edge weight between alternative rock and indie has weight 500, this means that there are 500 users with at least one song with target = 1 and alternative rock as a genre tag, who also have at least one song with target = 1 and indie tag.\n", "\n", "I first construct a similarity matrix from the training data, then convert it to a **networkx** graph and use a 2-D layout to visualise genre similarities and popularities."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["import networkx as nx\n", "import itertools\n", "import matplotlib.pyplot as plt # nx won't draw without this\n", "%matplotlib notebook "], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["dataPath = '../input/' # set the data path"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# read songs into pandas data frame\n", "songs = pd.read_csv(dataPath + 'songs.csv')\n", "songs = songs[['song_id','genre_ids']] # we don't need the rest"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["There may be various genre tags per song, separated by a \"|\". So we need to convert them to lists..."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["# convert genre ids to list\n", "songs['genre_ids'] = songs['genre_ids'].map(lambda x: [int(y) for y in str(x).split('|')] if not pd.isnull(x) else [])"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# get unique list of genre ids\n", "genres = songs['genre_ids'].values.tolist()\n", "genres = [j for i in genres for j in i]\n", "genres = list(set(genres))"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# number of unique genres\n", "numGenres = len(genres)\n", "print(\"There are %s unique genres.\" %numGenres)"], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"outputs": [], "source": ["# init genre similarity matrix S and genre score list\n", "S = np.zeros((numGenres,numGenres))\n", "scores = np.zeros((numGenres,))"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# read user listening data\n", "listen = pd.read_csv(dataPath + 'train.csv')"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# we only consider songs with target 1\n", "listen = listen[listen['target'] == 1]\n", "listen = listen[['msno','song_id']] # we don't need the rest"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# join the two datasets\n", "songs.set_index('song_id', inplace=True)\n", "df = listen.join(songs, how=\"left\", on=\"song_id\")\n", "df.dropna(axis=0,inplace=True) # drop anything with missing data"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["Now we can construct the genre similarity matrix. For each user, we update genre scores (+1 if this user liked this genre) and the co-occurrence in S for each possible pair of the genres this user liked. This is a bit slow..."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["# group by user and process groups\n", "for user, frame in df.groupby('msno'):\n", "    userGenres = frame['genre_ids'].values.tolist() # get all the genres liked by this user\n", "    userGenres = [j for i in userGenres for j in i] # convert to a single list\n", "    userGenres = set(list(userGenres)) # take only unique values\n", "\n", "    for aGenre in userGenres: # increase genre score\n", "        m = genres.index(aGenre) \n", "        scores[m] += 1\n", "\n", "    combs = itertools.combinations(userGenres, 2) # increase co-occurrence scores in matrix S\n", "    for comb in combs:\n", "        S[genres.index(comb[0]),genres.index(comb[1])] += 1\n", "        S[genres.index(comb[1]),genres.index(comb[0])] += 1"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["Now, we create the graph using networkx"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["G = nx.Graph()"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["for g in genres: # add nodes\n", "    G.add_node(g)"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["for i,gI in enumerate(genres): # add edges\n", "    for j,gJ in enumerate(genres):\n", "        if gJ >= gI:\n", "            continue\n", "        if S[i][j] > 0:\n", "            G.add_edge(gI,gJ,weight=S[i][j])"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["There are a lot of nodes, which have a score that is very low or zero (only the label appears, no red circle) and are not connected to any other nodes. These are genres which do not or very infrequently appear in the training data with target = 1. So let's remove all nodes with score < 1000 and see if the graph looks better if we only consider very \"popular\" genres..."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["# filter out nodes with score < 1000\n", "nodeList = [x for i,x in enumerate(genres) if scores[i] > 1000]\n", "G2 = G.subgraph(nodeList)\n", "nodeSizes = [0.1 * x for x in scores if x > 1000]"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["We can now draw the graph using a spectral layout (I found this is one works best). Nodes connected by strong edges should appear close to each other and weakly connected nodes should be isolated."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["nx.draw_spectral(G2, with_labels=True, node_size=nodeSizes, alpha=0.2, width=0.1, random_state=1985) # draw the new graph\n", "plt.show() # there are some warnings that seem to come from nx interacting with matplotlib"], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["We can see a bunch of things (you will need to zoom around a bit):\n", "*  genre-Ids 2107, 423 and 798 are very disconnected - meaning users listening to them don't tend to listen to other genres\n", "* 465 and 458 are two very popular genres which also co-appear a lot (I bet these are \"rock\" and \"pop\") and are very connected to other genres\n", "* 451 is related to them but a bit less \"popular\" (is it a coincidence that they start with a 4?)\n", "* some more rather \"disconnected\" genres are 1180, 1572, 275, 1287 and 726\n", "* inside the strongly connected area, there seem to be some subgroups, i.e. 451+465+458, 2022+1259, 444+437+1609+139 etc..."], "metadata": {}, "cell_type": "markdown"}, {"source": ["It would be interesting to see if this info helps in a recommendation system for the cold-start cases."], "metadata": {}, "cell_type": "markdown"}, {"source": ["This is my first ever kaggle kernel and I am glad for any feedback."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": [], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"language_info": {"name": "python", "nbconvert_exporter": "python", "version": "3.6.4", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}}