{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"070aa916932e05e8bbfa399e76b6961f27a2b1d6","collapsed":true,"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"53a94fa84d9714c55eea3887471c05248b6338e0"},"cell_type":"markdown","source":"![](https://www.homecredit.ph/files/copy-hc-logo.png)\n\n# DNN classifier in Tensorflow\n\nThis kernel will build a DNN classifier for the Home Credit Default Risk competition. The challenge here (as always!) is to try and match the performance of the LightGBM/XGBoost classifiers which always seems tricky for NNs for this kind of problem.\n\nA lot of the feature engineering going into the model is from my previous kernel [here](https://www.kaggle.com/shep312/lightgbm-with-weighted-averages-dropout-771), so I will focus more on the NN graph development here.\n\n### Contents\n\n1. [Load and process data](#load)\n    1. [Check nulls](#nulls)\n    2. [Identify categoricals](#cats)\n    3. [Scaling](#scale)\n2. [Building the graph](#graph)\n3. [Training the NN](#train)\n4. [Analysis and submission](#submit)"},{"metadata":{"_uuid":"934c044cff2a0f52e785b71d7537658021d96e7f"},"cell_type":"markdown","source":"## 1. Load and process data <a name=\"load\"></a>\n\nFirst step is to load all the different .csvs into memory."},{"metadata":{"_kg_hide-input":true,"_uuid":"7d394ce26aaaa01a39b5f880b79e4a7202cae424","trusted":true},"cell_type":"code","source":"input_dir = os.path.join(os.pardir, 'input')\nprint('Input files:\\n{}'.format(os.listdir(input_dir)))\nprint('Loading data sets...')\n\nsample_size = None\napp_train_df = pd.read_csv(os.path.join(input_dir, 'application_train.csv'), nrows=sample_size)\napp_test_df = pd.read_csv(os.path.join(input_dir, 'application_test.csv'), nrows=sample_size)\nbureau_df = pd.read_csv(os.path.join(input_dir, 'bureau.csv'), nrows=sample_size)\nbureau_balance_df = pd.read_csv(os.path.join(input_dir, 'bureau_balance.csv'), nrows=sample_size)\ncredit_card_df = pd.read_csv(os.path.join(input_dir, 'credit_card_balance.csv'), nrows=sample_size)\npos_cash_df = pd.read_csv(os.path.join(input_dir, 'POS_CASH_balance.csv'), nrows=sample_size)\nprev_app_df = pd.read_csv(os.path.join(input_dir, 'previous_application.csv'), nrows=sample_size)\ninstall_df = pd.read_csv(os.path.join(input_dir, 'installments_payments.csv'), nrows=sample_size)\nprint('Data loaded.\\nMain application training data set shape = {}'.format(app_train_df.shape))\nprint('Main application test data set shape = {}'.format(app_test_df.shape))\nprint('Positive target proportion = {:.2f}'.format(app_train_df['TARGET'].mean()))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66a22e5a62550f5ae5638fe22119e8ac3f6ea0bc"},"cell_type":"code","source":"app_train_df.head()","execution_count":9,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"74e8a1fc611bf1c088b0ffbd6092e043bee49b0d","code_folding":[0],"collapsed":true,"trusted":true},"cell_type":"code","source":"def feature_engineering(app_data, bureau_df, bureau_balance_df, credit_card_df,\n                        pos_cash_df, prev_app_df, install_df):\n    \"\"\" \n    Process the input dataframes into a single one containing all the features. Requires\n    a lot of aggregating of the supplementary datasets such that they have an entry per\n    customer.\n    \n    Also, add any new features created from the existing ones\n    \"\"\"\n    \n    # # Add new features\n    \n    # Amount loaned relative to salary\n    app_data['LOAN_INCOME_RATIO'] = app_data['AMT_CREDIT'] / app_data['AMT_INCOME_TOTAL']\n    app_data['ANNUITY_INCOME_RATIO'] = app_data['AMT_ANNUITY'] / app_data['AMT_INCOME_TOTAL']\n    app_data['ANNUITY LENGTH'] = app_data['AMT_CREDIT'] / app_data['AMT_ANNUITY']\n    \n    # # Aggregate and merge supplementary datasets\n    print('Combined train & test input shape before any merging  = {}'.format(app_data.shape))\n\n    # Previous applications\n    agg_funs = {'SK_ID_CURR': 'count', 'AMT_CREDIT': 'sum'}\n    prev_apps = prev_app_df.groupby('SK_ID_CURR').agg(agg_funs)\n    prev_apps.columns = ['PREV APP COUNT', 'TOTAL PREV LOAN AMT']\n    merged_df = app_data.merge(prev_apps, left_on='SK_ID_CURR', right_index=True, how='left')\n\n    # Average the rest of the previous app data\n    prev_apps_avg = prev_app_df.groupby('SK_ID_CURR').mean()\n    merged_df = merged_df.merge(prev_apps_avg, left_on='SK_ID_CURR', right_index=True,\n                                how='left', suffixes=['', '_PAVG'])\n    print('Shape after merging with previous apps num data = {}'.format(merged_df.shape))\n    \n    # Previous app categorical features\n    prev_app_df, cat_feats, _ = process_dataframe(prev_app_df)\n    prev_apps_cat_avg = prev_app_df[cat_feats + ['SK_ID_CURR']].groupby('SK_ID_CURR')\\\n                             .agg({k: lambda x: str(x.mode().iloc[0]) for k in cat_feats})\n    merged_df = merged_df.merge(prev_apps_cat_avg, left_on='SK_ID_CURR', right_index=True,\n                            how='left', suffixes=['', '_BAVG'])\n    print('Shape after merging with previous apps cat data = {}'.format(merged_df.shape))\n\n    # Credit card data - numerical features\n    wm = lambda x: np.average(x, weights=-1/credit_card_df.loc[x.index, 'MONTHS_BALANCE'])\n    credit_card_avgs = credit_card_df.groupby('SK_ID_CURR').agg(wm)   \n    merged_df = merged_df.merge(credit_card_avgs, left_on='SK_ID_CURR', right_index=True,\n                                how='left', suffixes=['', '_CCAVG'])\n    \n    # Credit card data - categorical features\n    most_recent_index = credit_card_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n    cat_feats = credit_card_df.columns[credit_card_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n    merged_df = merged_df.merge(credit_card_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n                       how='left', suffixes=['', '_CCAVG'])\n    print('Shape after merging with credit card data = {}'.format(merged_df.shape))\n\n    # Credit bureau data - numerical features\n    credit_bureau_avgs = bureau_df.groupby('SK_ID_CURR').mean()\n    merged_df = merged_df.merge(credit_bureau_avgs, left_on='SK_ID_CURR', right_index=True,\n                                how='left', suffixes=['', '_BAVG'])\n    print('Shape after merging with credit bureau data = {}'.format(merged_df.shape))\n    \n    # Bureau balance data\n    most_recent_index = bureau_balance_df.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].idxmax()\n    bureau_balance_df = bureau_balance_df.loc[most_recent_index, :]\n    merged_df = merged_df.merge(bureau_balance_df, left_on='SK_ID_BUREAU', right_on='SK_ID_BUREAU',\n                            how='left', suffixes=['', '_B_B'])\n    print('Shape after merging with bureau balance data = {}'.format(merged_df.shape))\n\n    # Pos cash data - weight values by recency when averaging\n    wm = lambda x: np.average(x, weights=-1/pos_cash_df.loc[x.index, 'MONTHS_BALANCE'])\n    f = {'CNT_INSTALMENT': wm, 'CNT_INSTALMENT_FUTURE': wm, 'SK_DPD': wm, 'SK_DPD_DEF':wm}\n    cash_avg = pos_cash_df.groupby('SK_ID_CURR')['CNT_INSTALMENT','CNT_INSTALMENT_FUTURE',\n                                                 'SK_DPD', 'SK_DPD_DEF'].agg(f)\n    merged_df = merged_df.merge(cash_avg, left_on='SK_ID_CURR', right_index=True,\n                                how='left', suffixes=['', '_CAVG'])\n    \n    # Pos cash data data - categorical features\n    most_recent_index = pos_cash_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n    cat_feats = pos_cash_df.columns[pos_cash_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n    merged_df = merged_df.merge(pos_cash_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n                       how='left', suffixes=['', '_CAVG'])\n    print('Shape after merging with pos cash data = {}'.format(merged_df.shape))\n\n    # Installments data\n    ins_avg = install_df.groupby('SK_ID_CURR').mean()\n    merged_df = merged_df.merge(ins_avg, left_on='SK_ID_CURR', right_index=True,\n                                how='left', suffixes=['', '_IAVG'])\n    print('Shape after merging with installments data = {}'.format(merged_df.shape))\n    \n    # Add more value counts\n    merged_df = merged_df.merge(pd.DataFrame(bureau_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n                                right_index=True, how='left', suffixes=['', '_CNT_BUREAU'])\n    merged_df = merged_df.merge(pd.DataFrame(credit_card_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n                                right_index=True, how='left', suffixes=['', '_CNT_CRED_CARD'])\n    merged_df = merged_df.merge(pd.DataFrame(pos_cash_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n                                right_index=True, how='left', suffixes=['', '_CNT_POS_CASH'])\n    merged_df = merged_df.merge(pd.DataFrame(install_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n                                right_index=True, how='left', suffixes=['', '_CNT_INSTALL'])\n    print('Shape after merging with counts data = {}'.format(merged_df.shape))\n\n    return merged_df","execution_count":10,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"304bcd7c390f46c33ba915e1d89fcd7906836a51","code_folding":[0],"collapsed":true,"trusted":true},"cell_type":"code","source":"def process_dataframe(input_df, encoder_dict=None):\n    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n\n    # Label encode categoricals\n    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n    categorical_feats = categorical_feats\n    encoder_dict = {}\n    for feat in categorical_feats:\n        encoder = LabelEncoder()\n        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n        encoder_dict[feat] = encoder\n\n    return input_df, categorical_feats.tolist(), encoder_dict","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"bf5a443cff51e779ce91cc143f1dc83ab478a206"},"cell_type":"markdown","source":"Since they are in disparate .csv's next I need to merge them into a single use-able dataframe:"},{"metadata":{"_kg_hide-input":true,"_uuid":"90c1b5875135fdb93ceace0c1548444aea244b6a","trusted":true},"cell_type":"code","source":"# Merge the datasets into a single one for training\nlen_train = len(app_train_df)\napp_both = pd.concat([app_train_df, app_test_df])\nmerged_df = feature_engineering(app_both, bureau_df, bureau_balance_df, credit_card_df,\n                                pos_cash_df, prev_app_df, install_df)\n\n# Separate metadata\nmeta_cols = ['SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV']\nmeta_df = merged_df[meta_cols]\nmerged_df.drop(meta_cols, axis=1, inplace=True)\n\n# Process the data set.\nmerged_df, categorical_feats, encoder_dict = process_dataframe(input_df=merged_df)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"56db4db61ec65e5b3391280646b6519bf4026635","collapsed":true,"trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# Capture other categorical features not as object data types:\nnon_obj_categoricals = [\n    'FONDKAPREMONT_MODE',\n    'HOUR_APPR_PROCESS_START',\n    'HOUSETYPE_MODE',\n    'NAME_EDUCATION_TYPE',\n    'NAME_FAMILY_STATUS', \n    'NAME_HOUSING_TYPE',\n    'NAME_INCOME_TYPE', \n    'NAME_TYPE_SUITE', \n    'OCCUPATION_TYPE',\n    'ORGANIZATION_TYPE', \n    'WALLSMATERIAL_MODE',\n    'WEEKDAY_APPR_PROCESS_START', \n    'NAME_CONTRACT_TYPE_BAVG',\n    'WEEKDAY_APPR_PROCESS_START_BAVG',\n    'NAME_CASH_LOAN_PURPOSE', \n    'NAME_CONTRACT_STATUS', \n    'NAME_PAYMENT_TYPE',\n    'CODE_REJECT_REASON', \n    'NAME_TYPE_SUITE_BAVG', \n    'NAME_CLIENT_TYPE',\n    'NAME_GOODS_CATEGORY', \n    'NAME_PORTFOLIO', \n    'NAME_PRODUCT_TYPE',\n    'CHANNEL_TYPE', \n    'NAME_SELLER_INDUSTRY', \n    'NAME_YIELD_GROUP',\n    'PRODUCT_COMBINATION', \n    'NAME_CONTRACT_STATUS_CCAVG', \n    'STATUS',\n    'NAME_CONTRACT_STATUS_CAVG'\n]\ncategorical_feats = categorical_feats + non_obj_categoricals","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"cca71199aee021128c0fddfcda0117c46fa9e5ab"},"cell_type":"markdown","source":"Before I do any futher processing, extract the target variable for training later."},{"metadata":{"_kg_hide-input":false,"_uuid":"318e1c65097fe0bc7efe25f3d8c38207ba055d74","collapsed":true,"trusted":true},"cell_type":"code","source":"# Extract target before scaling\nlabels = merged_df.pop('TARGET')\nlabels = labels[:len_train]\n\n# Reshape (one-hot)\ntarget = np.zeros([len(labels), len(np.unique(labels))])\ntarget[:, 0] = labels == 0\ntarget[:, 1] = labels == 1","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"745a83755298c90f7474f6659892ca9f788c55cd"},"cell_type":"markdown","source":"### 1.1 Check nulls <a name=\"nulls\"></a>\n\nThe data set has a a few variables containing a lot of nulls. Drop any features that are over x% null, then fill with 0. This obviously isn't a great method and provides room for improvement later on."},{"metadata":{"_kg_hide-input":true,"_uuid":"36b8d855b766e0c2b6cad3225e543d824452d35e","scrolled":false,"trusted":true},"cell_type":"code","source":"null_counts = merged_df.isnull().sum()\nnull_counts = null_counts[null_counts > 0]\nnull_ratios = null_counts / len(merged_df)\n\n# Drop columns over x% null\nnull_thresh = .8\nnull_cols = null_ratios[null_ratios > null_thresh].index\nmerged_df.drop(null_cols, axis=1, inplace=True)\nprint('Columns dropped for being over {}% null:'.format(100*null_thresh))\nfor col in null_cols:\n    print(col)\n    if col in categorical_feats:\n        categorical_feats.pop(col)\n    \n# Fill the rest with the mean (TODO: do something better!)\n# merged_df.fillna(merged_df.median(), inplace=True)\nmerged_df.fillna(0, inplace=True)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"668207c75b71af15eb1cb79136550c3d66b54cd4"},"cell_type":"markdown","source":"### 1.2 Identify categorical variables <a name=\"cats\"></a>\n\nCategorical variables will be important in this model - there are a lot of them and a few that have high cardinality. \n\nI will be creating embeddings to encode them for use in the model later, but for now just make a note of them and their positions."},{"metadata":{"_kg_hide-input":false,"_uuid":"8ff78ae5f6a05adf5dbe0d9d08bdba88d7f4f609","trusted":true},"cell_type":"code","source":"cat_feats_idx = np.array([merged_df.columns.get_loc(x) for x in categorical_feats])\nint_feats_idx = [merged_df.columns.get_loc(x) for x in non_obj_categoricals]\ncat_feat_lookup = pd.DataFrame({'feature': categorical_feats, 'column_index': cat_feats_idx})\ncat_feat_lookup.head()","execution_count":16,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"3677f92905602dc4af6fcb19362285af7f533488","trusted":true},"cell_type":"code","source":"cont_feats_idx = np.array(\n    [merged_df.columns.get_loc(x) \n     for x in merged_df.columns[~merged_df.columns.isin(categorical_feats)]]\n)\ncont_feat_lookup = pd.DataFrame(\n    {'feature': merged_df.columns[~merged_df.columns.isin(categorical_feats)], \n     'column_index': cont_feats_idx}\n)\ncont_feat_lookup.head()","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"d06c4bacfb00fe86e7351919a5f433d2fdad1640"},"cell_type":"markdown","source":"### 1.3 Scaling <a name=\"scale\"></a>\n\nNext data processing step is to scale the features so they don't get unfairly weighted against each other."},{"metadata":{"_uuid":"0291653a1997bb11e05a0c47fcc13315a5ff3d5a","collapsed":true,"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nfinal_col_names = merged_df.columns\nmerged_df = merged_df.values\nmerged_df[:, cont_feats_idx] = scaler.fit_transform(merged_df[:, cont_feats_idx])\n\nscaler_2 = MinMaxScaler(feature_range=(0, 1))\nmerged_df[:, int_feats_idx] = scaler_2.fit_transform(merged_df[:, int_feats_idx])","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"98bb83b751d40b4d65c831462d53cddee8529bd6"},"cell_type":"markdown","source":"## 2. Building the graph <a name=\"graph\"></a>\n\nNow the data is in decent shape, build the NN. \n\nFirst step, however, is to re-separate the competition train and test sets. I'll then further split the training set down to provide a hold out validation set."},{"metadata":{"_uuid":"dac6fe2603afe7c9d377ead5b68bfb749f5eaf13","collapsed":true,"trusted":true},"cell_type":"code","source":"# Re-separate into labelled and unlabelled\ntrain_df = merged_df[:len_train]\npredict_df = merged_df[len_train:]\ndel merged_df, app_train_df, app_test_df, bureau_df, bureau_balance_df, credit_card_df, pos_cash_df, prev_app_df\ngc.collect()\n\n# Create a validation set to check training performance\nX_train, X_valid, y_train, y_valid = train_test_split(train_df, target, test_size=0.1, random_state=2, stratify=target[:, 0])","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"d342dd920ebdb095e33accf6713e304b04d9d3ae"},"cell_type":"markdown","source":"Set the parameters for the network:"},{"metadata":{"_uuid":"115c68e6649a23df6a2b90fca021d6987243403d","trusted":true},"cell_type":"code","source":"# Fixed graph parameters\n# EMBEDDING_SIZE = 3  # Use cardinality / 2 instead\nN_HIDDEN_1 = 80\nN_HIDDEN_2 = 80\nN_HIDDEN_3 = 40\nn_cont_inputs = X_train[:, cont_feats_idx].shape[1]\nn_classes = 2\n\n# Learning parameters\nLEARNING_RATE = 0.01\nN_EPOCHS = 30\nN_ITERATIONS = 400\nBATCH_SIZE = 250\n\nprint('Number of continous features: ', n_cont_inputs)\nprint('Number of categoricals pre-embedding: ', X_train[:, cat_feats_idx].shape[1])","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"b90afebf27658cdc2a00c8ee091f8134c772a57f"},"cell_type":"markdown","source":"Graph itself. Note that there is an embedding step first where each categorical feature is embedded and attached to the continuous input features."},{"metadata":{"_uuid":"576ef849c153b5d99538c9f2664029ec5bb94a3e","collapsed":true,"trusted":true},"cell_type":"code","source":"def embed_and_attach(X, X_cat, cardinality):  \n    embedding = tf.Variable(tf.random_uniform([cardinality, cardinality // 2], -1.0, 1.0))\n    embedded_x = tf.nn.embedding_lookup(embedding, X_cat) \n    return tf.concat([embedded_x, X], axis=1)","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"bcc849802453459b7775a0042b32639200b7f50e","collapsed":true,"trusted":true},"cell_type":"code","source":"# # Graph\ntf.reset_default_graph()\n\n# Define placeholders for the categorical varaibles\ncat_placeholders, cat_cardinalities = [], []\nfor idx in cat_feats_idx:\n    exec('X_cat_{} = tf.placeholder(tf.int32, shape=(None, ), name=\\'X_cat_{}\\')'.format(idx, idx))\n    exec('cat_placeholders.append(X_cat_{})'.format(idx))\n    cat_cardinalities.append(len(np.unique(np.concatenate([train_df[:, idx], \n                                                           predict_df[:, idx]], axis=0))))\n    \n# Other placeholders\nX_cont = tf.placeholder(tf.float32, shape=(None, n_cont_inputs), name='X_cont')\ny = tf.placeholder(tf.int32, shape=(None, n_classes), name='labels')\ntrain_mode = tf.placeholder(tf.bool)\n\n# Add embeddings to input\nX = tf.identity(X_cont)\nfor feat, card in zip(cat_placeholders, cat_cardinalities):\n    X = embed_and_attach(X, feat, card)\n\n# Define the network layers. \n# Overfitting is a challenge so add L2 regularisation to weights in 1st layer & \n# a couple of dropout layers\nwith tf.name_scope('dnn'):\n    hidden_layer_1 = tf.layers.dense(inputs=X,\n                                     units=N_HIDDEN_1,\n                                     name='first_hidden_layer',\n                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.3))\n    hidden_layer_1 = tf.layers.batch_normalization(hidden_layer_1, training=train_mode)\n    hidden_layer_1 = tf.nn.relu(hidden_layer_1)\n    \n    drop_layer_1 = tf.layers.dropout(inputs=hidden_layer_1, \n                                     rate=0.4, \n                                     name='first_dropout_layer',\n                                     training=train_mode)\n\n    hidden_layer_2 = tf.layers.dense(inputs=drop_layer_1,\n                                     units=N_HIDDEN_2,\n                                     name='second_hidden_layer',\n                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n    hidden_layer_2 = tf.layers.batch_normalization(hidden_layer_2, training=train_mode)\n    hidden_layer_2 = tf.nn.relu(hidden_layer_2)\n                                     \n    drop_layer_2 = tf.layers.dropout(inputs=hidden_layer_2, \n                                     rate=0.2, \n                                     name='second_dropout_layer',\n                                     training=train_mode)\n\n    hidden_layer_3 = tf.layers.dense(inputs=drop_layer_2,\n                                     units=N_HIDDEN_3,\n                                     name='third_hidden_layer')\n    hidden_layer_3 = tf.layers.batch_normalization(hidden_layer_3, training=train_mode)\n    hidden_layer_3 = tf.nn.relu(hidden_layer_3)\n\n    logits = tf.layers.dense(inputs=hidden_layer_3,\n                             units=n_classes,\n                             name='outputs')\n\n# Define the loss function for training as cross entropy\nwith tf.name_scope('loss'):\n    xent = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n    loss = tf.reduce_mean(xent, name='loss')\n\n# Define the optimiser\nwith tf.name_scope('train'):\n    optimiser = tf.train.AdamOptimizer()  # AdagradOptimizer(learning_rate=LEARNING_RATE)\n    train_step = optimiser.minimize(loss)\n\n# Output the class probabilities to I can get the AUC\nwith tf.name_scope('eval'):\n    predict = tf.argmax(logits, axis=1, name='class_predictions')\n    predict_proba = tf.nn.softmax(logits, name='probability_predictions')\n\n# Initialisation node and saver\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"c49702ed1cd77a3422ae08dab65de4bafcdff932"},"cell_type":"markdown","source":"## 3. Training the NN <a name=\"train\"></a>\n\nTime to train the network. We know that only 8% of the targets are positive, so I will be upsampling positives for the gradient descent batches. I'm going to go with even representation in the training batches, but this isn't necessarily going to get the best score so may need optimising."},{"metadata":{"_uuid":"4131d595c68fbcea0e99226b5e64c36a2f8c513f","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_feed_dict(cat_feats_idx, cat_placeholders, cont_feats_idx, batch_X, batch_y=None,\n                 training=False):\n    \"\"\" Return a feed dict for the graph including all the categorical features\n    to embed \"\"\"\n    \n    # Continuous X features and the labels if training run\n    feed_dict = {X_cont: batch_X[:, cont_feats_idx]}\n    if batch_y is not None:\n        feed_dict[y] = batch_y\n        \n    # Loop through the categorical features to provide values for the placeholders\n    for idx, tensor in zip(cat_feats_idx, cat_placeholders):\n        feed_dict[tensor] = batch_X[:, idx].reshape(-1, ).astype(int)\n        \n    # Training mode or not\n    feed_dict[train_mode] = training\n        \n    return feed_dict","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"bb77df2430b5ece84e3347323a74b54157ea8946","scrolled":false,"trusted":true},"cell_type":"code","source":"train_auc, valid_auc = [], []\nn_rounds_not_improved = 0\nearly_stopping_epochs = 2\nwith tf.Session() as sess:\n\n    init.run()\n\n    # Begin epoch loop\n    print('Training for {} iterations over {} epochs with batchsize {} ...'\n          .format(N_ITERATIONS, N_EPOCHS, BATCH_SIZE))\n    for epoch in range(N_EPOCHS):\n        \n        # Iteration loop\n        for iteration in range(N_ITERATIONS):\n\n            # Get random selection of data for batch GD. Upsample positive classes to make it\n            # balanced in the training batch\n            pos_ratio = 0.5\n            pos_idx = np.random.choice(np.where(y_train[:, 1] == 1)[0], \n                                       size=int(np.round(BATCH_SIZE*pos_ratio)))\n            neg_idx = np.random.choice(np.where(y_train[:, 1] == 0)[0], \n                                       size=int(np.round(BATCH_SIZE*(1-pos_ratio))))\n            idx = np.concatenate([pos_idx, neg_idx])\n            \n            # Run training\n            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            sess.run([train_step, extra_update_ops], \n                     feed_dict=get_feed_dict(cat_feats_idx, cat_placeholders, cont_feats_idx, \n                                             X_train[idx, :], y_train[idx, :], 1))\n\n        # Check on the AUC\n        y_pred_train, y_prob_train = sess.run(\n            [predict, predict_proba], feed_dict=get_feed_dict(\n                cat_feats_idx, cat_placeholders, cont_feats_idx, X_train, y_train, False))\n        train_auc.append(roc_auc_score(y_train[:, 1], y_prob_train[:, 1]))\n        \n        y_pred_val, y_prob_val = sess.run(\n            [predict, predict_proba], feed_dict=get_feed_dict(\n                cat_feats_idx, cat_placeholders, cont_feats_idx, X_valid, y_valid, False))\n        valid_auc.append(roc_auc_score(y_valid[:, 1], y_prob_val[:, 1]))\n        \n        # Early stopping\n        if epoch > 1:\n            best_epoch_so_far = np.argmax(valid_auc[:-1])\n            if valid_auc[epoch] <= valid_auc[best_epoch_so_far]:\n                n_rounds_not_improved += 1\n            else:\n                n_rounds_not_improved = 0       \n            if n_rounds_not_improved > early_stopping_epochs:\n                print('Early stopping due to no improvement after {} epochs.'\n                      .format(early_stopping_epochs))\n                break\n        print('Epoch = {}, Train AUC = {:.8f}, Valid AUC = {:.8f}'\n              .format(epoch, train_auc[epoch], valid_auc[epoch]))\n\n    # Once trained, make predictions\n    print('Training complete.')\n    y_prob = sess.run(predict_proba, feed_dict=get_feed_dict(\n        cat_feats_idx, cat_placeholders, cont_feats_idx, predict_df, None, False))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"d0dbff3033889c123c2c8358d8b11be0bfff5465"},"cell_type":"markdown","source":"So an OK performance, but not matching the gradient boosted results yet. There's a lot of avenues to improve, however, including but not limited to:\n\n- **Optimising number of nodes / width**: Currently I've only used a small number of nodes in the hidden layers, limiting the complexity of the model. More nodes should lead to better performance, but I found that it started to overfit. There is probably a way to regularise the network to allow it to get more complex\n- **Number of layers**: Currently at 3 hidden layers deep, could be a better number\n- **Upsampling of positives**: Reduction in the upsampling of postive samples could be tuned\n- **Other hyperparameters**: Learning rate, batch size etc. could be tuned"},{"metadata":{"_uuid":"85dbfc50999caa30e81840a310025bdee2dd0a45"},"cell_type":"markdown","source":"## 4. Analysis and Submission <a name=\"submit\"></a>\n\nLets have a look at some summarising plots, then submit the results. \n\nTraining curves & the ROC curve:"},{"metadata":{"_kg_hide-input":true,"_uuid":"66366ac0f41005fc07a91361eb18661f3c527347","trusted":true,"collapsed":true},"cell_type":"code","source":"fig, (ax, ax1) = plt.subplots(1, 2, figsize=[14, 5])\nax.plot(np.arange(len(train_auc)), train_auc, label='Train')\nax.plot(np.arange(len(valid_auc)), valid_auc, label='Valid')\nax.set_xlabel('Epoch')\nax.set_ylabel('AUC')\nax.set_title('Training performance')\n\nfpr, tpr, _ = roc_curve(y_valid[:, 1], y_prob_val[:, 1])\nax1.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(valid_auc[epoch]))\nax1.plot([0, 1], [0, 1], linestyle='--')\nax1.set_xlim([0.0, 1.0])\nax1.set_ylim([0.0, 1.05])\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nax1.set_title('ROC Curve')\n\nfor a in [ax, ax1]:\n    a.spines['top'].set_visible(False)\n    a.spines['right'].set_visible(False)\n    a.legend(frameon=False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0781b2f92972c3eb4434a12733b13ce5f89dd45"},"cell_type":"markdown","source":"...and a precision-recall curve and the confusion matrix:"},{"metadata":{"_kg_hide-input":true,"_uuid":"e9de5f51cd64ff71074161e68b387233c8aeffd7","trusted":true,"collapsed":true},"cell_type":"code","source":"fig, (ax, ax1) = plt.subplots(1, 2, figsize=[14, 5])\n\n# Precision recall curve\nprecision, recall, _ = precision_recall_curve(y_valid[:, 1], y_prob_val[:, 1])\nax.step(recall, precision, color='b', alpha=0.2, where='post')\nax.fill_between(recall, precision, step='post', alpha=0.2, color='b')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.set_ylim([0.0, 1.05])\nax.set_xlim([0.0, 1.0])\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.set_title('Precision - recall curve')\n\n# Confusion matrix\ncnf_matrix = confusion_matrix(y_valid[:, 1], np.argmax(y_prob_val, axis=1))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nheatmap = sns.heatmap(cnf_matrix, annot=True, fmt='d', ax=ax1, cmap=cmap, center=0)\nax1.set_title('Confusion matrix heatmap')\nax1.set_ylabel('True label')\nax1.set_xlabel('Predicted label')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e37c6b930593dbd372282fe5af551fe5c4b1650","collapsed":true,"trusted":true},"cell_type":"code","source":"out_df = pd.DataFrame({'SK_ID_CURR': meta_df['SK_ID_CURR'][len_train:], 'TARGET': y_prob[:, 1]})\nout_df.to_csv('nn_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}