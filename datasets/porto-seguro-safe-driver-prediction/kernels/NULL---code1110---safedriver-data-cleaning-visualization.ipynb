{"cells": [{"source": ["**Introduction**\n", "This jupyter notebook intends to employ a few visualization techniques for high-dimensional data such as PCA and TSNE to see if we can find any separation between target 0 and 1. Before that, some basic data cleanings are performed.\n", "\n", "Contents include:\n", "- deal with nan\n", "- feature selection\n", "- correlation matrix\n", "- outlier deletion & normalization\n", "- feature importance analysis\n", "- dimensionality reduction and visualization"], "cell_type": "markdown", "metadata": {"_cell_guid": "1e4429e3-4a19-47c1-a2db-2c6ec65fa42e", "_uuid": "282a5ef09305ea1693d7dd1ddaf70a405aa64b9f"}}, {"source": ["\n", "Loading data and checking the first sevaral rows"], "cell_type": "markdown", "metadata": {"_cell_guid": "7f424fa0-778d-428d-b686-1fff2494e6db", "_uuid": "2b16e6c9bca6da670cb40e2386ce594f75d95473"}}, {"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output.\n", "\n", "\n", "# import libraries\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "# import training data\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "# submit_exp = pd.read_csv(\"../input/sample_submission.csv\")\n", "# print(submit_exp.head())\n", "\n", "# train and test\n", "id_train = train[\"id\"]\n", "y_train = train[\"target\"]\n", "X_train = train.drop([\"id\",\"target\"], axis=1)\n", "\n", "id_test = test[\"id\"]\n", "X_test = test.drop([\"id\"], axis=1)\n", "\n", "print(X_train.head())\n", "# print(X_test.head())"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "f7628b16-67a5-4231-ae2b-0f7159a643ff", "_uuid": "1da703ebfe8876813e68bfd0041950b441af4eea"}}, {"source": ["Dealing with nans using \"missingno\" as a visualization method"], "cell_type": "markdown", "metadata": {"_cell_guid": "24892d54-3279-449e-a4d9-aff6c91c0d3a", "_uuid": "f301248254cfdeb509beb1d95f8aa0ff315c7358"}}, {"source": ["# -1 means nan in this case...so put nan back\n", "X_train = X_train.replace(-1, np.NaN)\n", "X_test = X_test.replace(-1, np.NaN)\n", "\n", "# concatenate train and test to deal with nan together\n", "Xmat = pd.concat([X_train, X_test])\n", "\n", "# visualize the number of nans in each column\n", "# (shamelessly adapted from:\n", "#https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial)\n", "import missingno as msno\n", "\n", "msno.matrix(df=X_train.iloc[:,:39], figsize=(20,14), color=(0.5,0,0))"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "820bfdbf-996a-481b-a7c1-d38c8f38fbcb", "_uuid": "0ca46577ef8849c84ade09849cd56127144427f2"}}, {"source": ["For some columns having around 50% of nans, the presence of nan itself may be meaningful.\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "98ee4e21-cd27-4a58-a123-60afb053123e", "_uuid": "d8ae258692622ff109239cef1308a0e7e4142894"}}, {"source": ["# Columns with many nans itself may be meaningful\n", "def nan2bi(x):\n", "    if np.isnan(x):\n", "        return 1\n", "    else:\n", "        return 0\n", "\n", "Xmat = pd.concat([X_train, X_test])\n", "cols = [\"ps_reg_03\",\"ps_car_03_cat\",\"ps_car_05_cat\"]\n", "for c in cols:\n", "    Xmat[c + \"_isnan\"] = Xmat[c].apply(nan2bi)\n", "    \n", "# For other columns replace nan with median\n", "Xmat = Xmat.fillna(Xmat.median())\n", "\n", "# remove other columns with nan, if any\n", "Xmat = Xmat.dropna(axis=1)\n", "print(Xmat.shape)"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "aecfd8aa-6b3f-4307-a350-ac48b3334e3e", "_uuid": "790c2d99f5e6e686aad2445e98bbb80ab4c81b08"}}, {"source": ["There are binary predictors...let's see if they are skewed or evenly distributed.\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "944c5050-a258-4105-bcb0-9eba62f741e5", "_uuid": "598bbf3155759dd746c85787be231377adec0905"}}, {"source": ["# some of binary variables can be skewed\n", "bin_col = [col for col in Xmat.columns if '_bin' in col]\n", "counts = []\n", "for col in bin_col:\n", "    counts.append(100*(Xmat[col]==1).sum()/Xmat.shape[0])\n", "\n", "ax = sns.barplot(x=counts, y=bin_col, orient='h')\n", "ax.set(xlabel=\"% of 1 in a column\")\n", "plt.show()\n"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "2e808813-2064-4e7d-8c1b-b7e188ae3914", "scrolled": true, "_uuid": "93a50c2f6665616e34f8a87fdd722c4e9725f14e"}}, {"source": ["Let's remove some columns with very skewed data (**\"ps_ind_10_bin\",\"ps_ind_11_bin\"**, **\"ps_ind_12_bin\"**and **\"ps_ind_13_bin\"**)."], "cell_type": "markdown", "metadata": {"_cell_guid": "8ae6c41f-ee38-43c9-9893-a78b51bc94f7", "_uuid": "379c935547a312ddb49e9335217c7d7091945cb1"}}, {"source": ["# upon visual inspection, some columns with skewed data are removed\n", "Xmat = Xmat.drop([\"ps_ind_10_bin\",\"ps_ind_11_bin\",\"ps_ind_12_bin\",\"ps_ind_13_bin\"], axis=1)\n", "print(Xmat.shape)"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "95e6c56e-399b-4641-a540-14aa39f1e5e5", "_uuid": "344511116dc863fd5459e1df978d8112150d6148"}}, {"source": ["As a custom, let's see the correlation matrix between predictors."], "cell_type": "markdown", "metadata": {"_cell_guid": "edd32c17-163b-4471-a6e8-6361e7a5a6e5", "_uuid": "5c81c2233952fe81d29049d3db9f0010034f39e8"}}, {"source": ["# check correlation matrix\n", "sns.set(style=\"white\")\n", "\n", "# Compute the correlation matrix (let's put y_train back this time)\n", "Xcorrmat = Xmat.iloc[:X_train.shape[0],:]\n", "Xcorrmat['target'] = y_train\n", "corr = Xcorrmat.corr()\n", "\n", "# Set up the matplotlib figure\n", "f, ax = plt.subplots(figsize=(20,12))\n", "\n", "# Generate a custom diverging colormap\n", "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n", "\n", "# Draw the heatmap with the mask and correct aspect ratio\n", "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n", "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n", "\n", "plt.show()"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "efd383fd-2f44-4696-ba84-0dba9127ab58", "_uuid": "09d6bd5de840130c92f0047a60412e1ca7d585d6"}}, {"source": ["We can drop \"_calc\", as they do not show any dependency on other predictors.\n", "Also, let's just normalize data using z-scoring."], "cell_type": "markdown", "metadata": {"_cell_guid": "fe951dcd-dd55-4edd-a6d8-a79119b369e2", "_uuid": "0fb31a3321173acda4a4866e8eb79f85790db99b"}}, {"source": ["# drop all the \"_calc\"\n", "calc_col = [col for col in Xmat.columns if '_calc' in col]\n", "Xmat = Xmat.drop(calc_col, axis=1)\n", "\n", "# zscoring as a means of normalization\n", "X_train = Xmat.iloc[:X_train.shape[0],:]\n", "X_test = Xmat.iloc[X_train.shape[0]:,:]\n", "X_train = (X_train - X_train.mean())/X_train.std()\n", "X_test = (X_test - X_test.mean())/X_test.std()\n", "\n", "# vizualize\n", "f, ax = plt.subplots(figsize=(11, 9))\n", "sns.heatmap(X_train, cmap=cmap)\n", "plt.show()\n"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "ab38ccbc-84ca-4808-9fbe-ea5bbafbd861", "scrolled": true, "_uuid": "2ced328b74d6b16ad6bd45917fbfc218a4d279b2"}}, {"source": ["Apparently there are some outliers in 'ps_ind_14', and 'ps_car_10_cat'. Let's remove these columns."], "cell_type": "markdown", "metadata": {"_cell_guid": "a9c339b6-1789-48ba-b015-367284221f0b", "_uuid": "f636bcadc69df344cecd8ffd40579adfbe49a43e"}}, {"source": ["# outlier deletion\n", "X_train = X_train.drop(['ps_ind_14','ps_car_10_cat'], axis=1)\n", "X_test = X_test.drop(['ps_ind_14','ps_car_10_cat'], axis=1)\n", "\n", "X_train = (X_train - X_train.mean())/X_train.std()\n", "X_test = (X_test - X_test.mean())/X_test.std()\n", "\n", "# vizualize\n", "f, ax = plt.subplots(figsize=(11, 9))\n", "sns.heatmap(X_train, cmap=cmap)\n", "plt.show()\n"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "878d033b-7764-4a67-863e-0e763bb577cb", "_uuid": "62290181041d1fe1614936504169d2a4f872b56b"}}, {"source": ["Let's use random forest classifier to let us know the importance of features."], "cell_type": "markdown", "metadata": {"_cell_guid": "7e5eb3f7-21e0-49ed-810b-d21d8882ee96", "_uuid": "cd3dfeb0448f57676d297cdf05ef1a1aeaf18e97"}}, {"source": ["# feature importance using random forest\n", "from sklearn.ensemble import RandomForestClassifier\n", "rf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\n", "rf.fit(X_train, y_train)\n", "\n", "print('Training done using Random Forest')\n", "\n", "ranking = np.argsort(-rf.feature_importances_)\n", "f, ax = plt.subplots(figsize=(11, 9))\n", "sns.barplot(x=rf.feature_importances_[ranking], y=X_train.columns.values[ranking], orient='h')\n", "ax.set_xlabel(\"feature importance\")\n", "plt.show()\n"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "f7b4c92d-840f-4ce7-8fb6-cc68966c1e2b", "_uuid": "dc9bcf652015b35ccc7463b3d8e5a23b59f41fcb"}}, {"source": ["So the top 3 important features are **'ps_car_13', 'ps_reg_03',  **and** 'ps_car_14'**."], "cell_type": "markdown", "metadata": {"_cell_guid": "c35da309-aabf-495a-aed4-2dadc9eab1c1", "_uuid": "c32219b7779cad397e1d62262db8362c7db8cb1e"}}, {"source": ["To visualize the data split by the target value in a comprehensive way, let's split the training data based on the target value. "], "cell_type": "markdown", "metadata": {"_cell_guid": "15454ce1-8ef9-4629-acb6-d53b0cb02af0", "_uuid": "068855c39ad4ad0c2e05468e47f9aba7e80047db"}}, {"source": ["# dimensioanlity reduction and visualization\n", "Xdr = X_train\n", "Xdr['target'] = y_train\n", "Xdr1 = Xdr.loc[y_train==1, :]\n", "Xdr0 = Xdr.loc[y_train==0, :]\n", "\n", "print('rows for target 1: ' + str(Xdr1.shape[0]))\n", "print('rows for target 0: ' + str(Xdr0.shape[0]))"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "d49540cc-ab77-47a2-bc4f-e81938f8eb1b", "_uuid": "4b5daf1cdd4252a656d91ca0cc4b3a4ec8c7951e"}}, {"source": ["Target values are skewed...we need to deal with it."], "cell_type": "markdown", "metadata": {"_cell_guid": "978e6f75-9851-4267-91d4-acc1077b3cc2", "_uuid": "87cc2cb518448607032275cee24cc402d14ebb8d"}}, {"source": ["# random sampling from X_train0, as the target value distribution is skewed\n", "# use N = 20,000 samples for now\n", "N = 20000\n", "np.random.seed(20171021)\n", "Xdr0 = Xdr0.iloc[np.random.choice(Xdr0.shape[0], N), :]\n", "Xdr1 = Xdr1.iloc[np.random.choice(Xdr1.shape[0], N), :]\n", "\n", "Xdr = pd.concat([Xdr0, Xdr1])"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "98c397d6-26d5-48e2-ab1b-13ca2dec1f62", "_uuid": "48b2cb801e66db6be27de898d3b882051e3127c1"}}, {"source": ["**Pairplot**\n", "Let's use the first 7 important features to see if they show any separation between target 0 and 1."], "cell_type": "markdown", "metadata": {"_cell_guid": "e8fb18da-36f2-4880-a6b0-d3a88cc1738c", "_uuid": "ef7f74ac6fb02799e356bc2203abac28f5140d42"}}, {"source": ["# pairplot\n", "Xpair =pd.concat([Xdr.iloc[:,ranking[:7]], Xdr['target']], axis=1)\n", "\n", "ax = sns.pairplot(Xpair, hue='target')\n", "plt.show()"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "a7ff8069-5e48-43f4-921b-6d99fff5ce3c", "scrolled": false, "_uuid": "fc2fa0d1fb1b3c2f7c96a81828d518676b607580"}}, {"source": ["**PCA**"], "cell_type": "markdown", "metadata": {"_cell_guid": "2fbc6b2a-c6f8-4f03-82ea-1f85afcf945c", "_uuid": "5e5d42c09162a98f21a37804592355ac81cf97ab"}}, {"source": ["Xdr = Xdr.drop(['target'], axis=1)\n", "\n", "# PCA\n", "from sklearn.decomposition import PCA\n", "\n", "pcamat = PCA(n_components=2).fit_transform(Xdr)\n", "\n", "plt.figure()\n", "plt.scatter(pcamat[:Xdr0.shape[0],0],pcamat[:Xdr0.shape[0],1], c='b', label='targ 0', alpha=0.3)\n", "plt.scatter(pcamat[Xdr0.shape[0]:,0],pcamat[Xdr0.shape[0]:,1],c='r', label='targ 1', alpha=0.3)\n", "plt.legend()\n", "plt.title('PC space')\n", "plt.xlabel('PC1')\n", "plt.ylabel('PC2')\n", "plt.tight_layout()\n", "plt.show()\n", "print(\"PCA done\")\n"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "de3fe797-b409-4545-9e50-33de5a21ca98", "_uuid": "175b48ada8a79b764f9d365baaabb4e58a61ae54"}}, {"source": ["**TSNE** "], "cell_type": "markdown", "metadata": {"_cell_guid": "9cedb06d-2049-4dca-b296-a7a0de0de777", "_uuid": "b515c09cc1cc61ef050e08bf97be60714d35ecb7"}}, {"source": ["# TSNE \n", "from sklearn.manifold import TSNE\n", "\n", "tsnemat = TSNE(n_components=2, random_state=0).fit_transform(Xdr)\n", "\n", "plt.figure()\n", "plt.scatter(tsnemat[:Xdr0.shape[0],0],tsnemat[:Xdr0.shape[0],1], c='b', label='targ 0', alpha=0.3)\n", "plt.scatter(tsnemat[Xdr0.shape[0]:,0],tsnemat[Xdr0.shape[0]:,1],c='r', label='targ 1', alpha=0.3)\n", "plt.legend()\n", "plt.title('TSNE space')\n", "plt.xlabel('dim 1')\n", "plt.ylabel('dim 2')\n", "plt.tight_layout()\n", "plt.show()\n", "print(\"TSNE done\")\n"], "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "21d0a3f8-3a37-436f-a036-63b5c534f9ac", "_uuid": "22367360ae9f2108511b3abf2540cebf196c1060"}}, {"source": ["Unfortunately we see no separation between target 0 and 1 in either pairplot, PC space, or TSNE space. This competition is apparently a very hard one;( \n", "\n", "Still, like other Kagglers, using **XGBoost **as a classification algorithm seems to be a way to achieve high score. "], "cell_type": "markdown", "metadata": {"_cell_guid": "ef8fe84c-fc08-4234-9b93-315c3e022658", "_uuid": "7e9f2c1e51fe4d854ca93da0c3196dcf9ec925be"}}, {"source": [], "cell_type": "markdown", "metadata": {"_cell_guid": "edf34b12-1e1e-4b20-b91f-9d81f91db1cf", "_uuid": "0d564526b6fbcc45a9324f5808a16f6a8cdb0825"}}, {"source": [], "cell_type": "markdown", "metadata": {"_cell_guid": "a8da415a-4fb6-467c-92ad-c738126bcb5a", "_uuid": "1e0daeaefa273e019cd9dba9828f5fb16336d1e2"}}, {"source": [], "cell_type": "markdown", "metadata": {"_cell_guid": "f438c4f9-1df2-4e0c-9ca9-9451a3e953eb", "_uuid": "28ecbe136d282a46296b92650acae10ce0f71b44"}}, {"source": [], "cell_type": "markdown", "metadata": {"_cell_guid": "b2276fab-16f8-4c9b-a7d2-7694c93abdb8", "_uuid": "547a0e48ac9a13a10cfe76a58c42785f208b845d"}}, {"source": [], "cell_type": "markdown", "metadata": {"_cell_guid": "4d4c5bc3-f1f2-4caa-9a8a-811108981373", "_uuid": "075b848c24701d1968c1f4acb1519de0c0091528"}}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3", "file_extension": ".py"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}}