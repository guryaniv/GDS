{"cells":[{"metadata":{"_uuid":"acac0acc5929fa8ac003e4cb937e6ad53d86fa9f"},"cell_type":"markdown","source":"Inspired by https://www.kaggle.com/mortido/digging-into-the-data-time-series-theory"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4c9115aa53ba2cdf6653f33c20225ff407d319f0"},"cell_type":"code","source":"# some matplotlib niceness\nmpl.rcParams['axes.color_cycle'] = ['e6194b', '3cb44b', 'ffe119', '0082c8',\n                                    'f58231', '911eb4', '46f0f0', 'f032e6',\n                                    'd2f53c', 'fabebe', '008080', 'e6beff',\n                                    '800000', 'aaffc3', 'ffd8b1']\nmpl.rcParams['figure.figsize'] = (12.8, 7.2)\nmpl.rcParams['figure.dpi'] = 100\nmpl.rcParams['figure.facecolor'] = '#303030'\nmpl.rcParams['figure.edgecolor'] = '#303030'\nmpl.rcParams['axes.facecolor'] = '#303030'\nmpl.rcParams['axes.edgecolor'] = 'white'\nmpl.rcParams['axes.labelcolor'] = 'white'\nmpl.rcParams['axes.linewidth'] = 0.4\nmpl.rcParams['xtick.major.width'] = 0.4\nmpl.rcParams['xtick.minor.width'] = 0.2\nmpl.rcParams['ytick.major.width'] = 0.4\nmpl.rcParams['ytick.minor.width'] = 0.2\nmpl.rcParams['grid.linestyle'] = '--'\nmpl.rcParams['grid.linewidth'] = 0.4\nmpl.rcParams['grid.alpha'] = 0.8\nmpl.rcParams['xtick.color'] = 'white'\nmpl.rcParams['ytick.color'] = 'white'\nmpl.rcParams['text.color'] = 'white'\nmpl.rcParams['legend.frameon'] = False\nmpl.rcParams['legend.fancybox'] = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58ec667d90829ce50cb64312a3d11ee83b62c96d","collapsed":true},"cell_type":"code","source":"# load the data and concat train and test features into one DF\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubmission = pd.read_csv('../input/sample_submission.csv')\nall_data = pd.concat((train.iloc[:, 2:], test.iloc[:, 1:]), ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f5d3ee7a93f8801cd821e0c1cf7dc28c23b1143","collapsed":true},"cell_type":"code","source":"# as you already know the data is quite sparse\n# but it also of the same scale like target\n# so i suppose the data is encoded timeseries\nprint('mean of std and max of feature values:')\nall_data.describe().loc[['std', 'max']].mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9895ed051ba5bdf894a7c8df1abb3e6e28b1aef4"},"cell_type":"markdown","source":"Since we suppose that the data is timeseries and don't know true order of features a.k.a timesteps we can use different statistics to predict our target.\nTo do this i wrote a class that computes several row-based statistics:\n1. non-zero counts\n2. mean and std of row values (without zeros)\n3. max, min, mean and median of running difference between non-zero features (in original order)\n3. 0 to 100 percentiles of row values\n4. probability of appearance of particular or lesser value based on dataset histogram\n5. mean of probabilities of values in a row\n6. std of probabilities of values in a row\n7. 0 to 100 percentiles of probabilities in a row"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# The class is for preprocessing\n# it computes row-based statistics on full data\nclass StatsDatasetCreator(object):\n    def get_nonzero(self, data):\n        '''\n        Transforms original data to np.array of np.arrays\n        dropping zero values.\n        '''\n        numerical_cols = [x for x in data.columns  \\\n                          if x not in ['ID', 'target']]\n        values = data[numerical_cols] \\\n                 .apply(lambda x: np.log1p(x.values[np.nonzero(x)]),\n                        axis=1) \\\n                 .values\n        return values\n\n    def create_stats_dataset(self, data, target, maxlen=1989):\n        '''\n        Main function, returns full processed dataset\n        '''\n        values = self.get_nonzero(data)\n        data_length = len(data)\n        stats_array = np.zeros((data_length, 108))\n        stats_array[:, 0] = np.vectorize(np.count_nonzero)(values)\n        stats_array[:, 1] = np.vectorize(np.mean)(values)\n        stats_array[:, 2] = np.vectorize(np.std)(values)\n        for i, v in enumerate(values):\n            stats_array[i, 3:104] = np.percentile(v,\n                                                  np.linspace(0, 100, 101))\n\n        values_pad = pad_sequences(values, maxlen=maxlen)\n        values_diff = values_pad[:, 1:] - values_pad[:, :-1]\n        stats_array[:, 104] = values_diff.min(axis=1)\n        stats_array[:, 105] = values_diff.max(axis=1)\n        stats_array[:, 106] = values_diff.mean(axis=1)\n        stats_array[:, 107] = np.nanmedian(np.where(values_diff == 0,\n                                                    np.nan,\n                                                    values_diff),\n                                           axis=1)\n        \n        hist = self.get_hist(values)\n        probs = self.get_probs(values, hist)\n        probs_array = np.zeros((len(data), 103))\n        probs_array[:, 0] = np.vectorize(np.mean)(probs)\n        probs_array[:, 1] = np.vectorize(np.std)(probs)\n        for i, p in enumerate(probs):\n            probs_array[i, 2:103] = np.percentile(p, np.linspace(0, 100, 101))\n        \n        transformed_data = np.concatenate((stats_array, probs_array), axis=1)\n        return transformed_data[:len(target)], transformed_data[len(target):]\n\n    def get_hist(self, values):\n        '''\n        Returns histogram of values in dataset\n        '''\n        values_ravel = np.concatenate([*values])\n        vals, bins = np.histogram(values_ravel,\n                                  density=True,\n                                  bins=100)\n        probs = vals * (bins[1:] - bins[:-1])\n        hist = [vals, bins, probs]\n        return hist\n\n    def digitizer(self, val, hist):\n        '''\n        Returns probability of val\n        '''\n        vals, bins, probs = hist\n        b = np.digitize(val, bins[:-1]) - 1\n        v = vals[b]\n        p_less = probs[:b].sum()\n        return p_less\n\n    def get_probs(self, values, hist):\n        '''\n        Transforms values to their probabilities\n        '''\n        probs = []\n        for i, v in enumerate(values):\n            probs.append(np.vectorize(lambda x: self.digitizer(x, hist))(v))\n        return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9252cf35a2efc9fefe9c6c59147c919df70e03e1"},"cell_type":"code","source":"y = np.log1p(train['target'].copy().values)\nds_creator = StatsDatasetCreator()\nx, x_test = ds_creator.create_stats_dataset(all_data, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afbefb2179e03dcc2fa0046998232c67e970d445"},"cell_type":"markdown","source":"Now let's look at correlations of new features with target (as you can see in other notebooks, correlations of original features is really poor)."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d0bc8489c6d8f4b9e882df736edaa388f9f5997f","collapsed":true},"cell_type":"code","source":"corrs = np.apply_along_axis(lambda x: np.corrcoef(y, x)[0][1], axis=0, arr=x)\n\nfig, ax = plt.subplots()\nax.hist(corrs, bins=100, edgecolor='k')\nax.set_xlabel('Correlation')\nax.set_title('New features correlation hist')\nax.set_ylabel('Frequency');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4a609c9e7fef9d1377e93af20ac40dbd53f130b"},"cell_type":"markdown","source":"Hope with these features we can get much more better results:"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0a9d32605a41b8501ee7bf7cbb1fee6e648ce172","collapsed":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\nfeatures = ['nonzero', 'mean', 'std'] + ['{}_percentile'.format(x) for x in range(0, 101)] + \\\n           ['min_diff', 'max_diff', 'mean_diff', 'median_diff'] + ['mean_probs', 'std_probs'] + \\\n           ['{}_probs_percentile'.format(x) for x in range(0, 101)]\nrseed = np.random.RandomState(0)\nfolds = rseed.randint(0, 10, size=x.shape[0])\n\nres = []\nx_prediction = np.zeros((len(train), ))\nx_prediction_test = np.zeros((len(test), ))\n\n\nall_feature_importance_df  = pd.DataFrame()\nparams = {'task': 'train',\n          'objective': 'regression',\n          'metric': 'rmse',\n          'colsample_bytree': 0.3,\n          'learning_rate': 0.05}\nevals_result = {}\nfor fold_idx in np.unique(folds):\n\n    print('Fold {} start'.format(fold_idx))    \n    val_mask = folds == fold_idx\n    x_train = x[~val_mask]\n    x_val = x[val_mask]\n    y_train = y[~val_mask]\n    y_val = y[val_mask]\n    \n    evals_result[fold_idx] = {}\n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_val = lgb.Dataset(x_val, y_val, reference=lgb_train)\n    \n    gbm = lgb.train(params,\n                    lgb_train,\n                    num_boost_round=10000,\n                    valid_sets= (lgb_train, lgb_val),\n                    verbose_eval=500,\n                    evals_result=evals_result[fold_idx],\n                    early_stopping_rounds=200)\n    \n    y_val_pred = gbm.predict(x_val, num_iteration=gbm.best_iteration)\n    x_prediction[val_mask] = y_val_pred\n    r = mean_squared_error(y_val_pred, y_val)\n    res.append(np.sqrt(r))\n    \n    y_test_pred = gbm.predict(x_test, num_iteration=gbm.best_iteration)\n    x_prediction_test += y_test_pred\n\n    # Feature Importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = gbm.feature_importance()\n    all_feature_importance_df = pd.concat([all_feature_importance_df, fold_importance_df], axis=0)\n\nprint('CV results', np.mean(res), np.std(res))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db30c3db6b3a033542378bf77ee1216d914bf823"},"cell_type":"markdown","source":"CV results are pretty good:  mean=1.3357 and std=0.0498\n\nLets take a look on feature importances:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"324022459fd779228a2bad8853962f74759c240d","collapsed":true},"cell_type":"code","source":"import seaborn as sns\ncols = all_feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\nbest_features = all_feature_importance_df.loc[all_feature_importance_df.feature.isin(cols)]\nplt.figure(figsize=(8,10))\nsns.barplot(x=\"importance\", y=\"feature\", \n            data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76c7f55f4b0ae7a9af7118587c28ab7ca05e0b80"},"cell_type":"markdown","source":"Try it on LB now!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"88eea880fe04742a7aef46f51ee22f410bd16341"},"cell_type":"code","source":"submission['target'] = np.expm1(x_prediction_test / len(np.unique(folds)))\nsubmission.to_csv('lgbm_on_stats.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}