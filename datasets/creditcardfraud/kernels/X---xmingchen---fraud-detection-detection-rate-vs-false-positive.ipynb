{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a316809f-1a37-a66d-cbea-6f0b871e6b11"
      },
      "source": [
        "Testbench for ML/DL learning\n",
        "----------------------------\n",
        "\n",
        "For fraud detection, good performance means:\n",
        " - High detection rate: True positives/positives, i.e., how many fraud cases can be detected correctly.\n",
        " - Low false postive rate: False positives/negatives, i.e., how often a non-fraud case is falsely detected as fraud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2d27b6c7-072c-09cd-2743-1bef62f038c9"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "77251f8d-7670-238b-7a58-7181a3fafc18"
      },
      "source": [
        "**Import data, analysis, visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dc3f0ded-86b9-7002-85d2-aba6c4e54114"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"../input/creditcard.csv\")\n",
        "print(dataset.head())\n",
        "print(dataset.describe())\n",
        "\n",
        "print(len(dataset[dataset.Class == 1]))\n",
        "features = dataset.iloc[:, :-1]\n",
        "print(features.shape)\n",
        "label = dataset.iloc[:, -1].values\n",
        "print(label.shape)\n",
        "\n",
        "# heatmap for correlation, verifying that pca is already done\n",
        "import seaborn as sns\n",
        "corrMat = features.corr()\n",
        "sns.heatmap(corrMat, vmax=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ab71dca6-32e4-d1b4-dc8c-2fff595cd0ce"
      },
      "source": [
        "**Feature scaling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7d6a8df4-abbb-52ca-5f78-fef628131ae3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "fraudInd = np.asarray(np.where(label == 1))\n",
        "noFraudInd = np.where(label == 0)\n",
        "features = features.values\n",
        "\n",
        "# data standarization (zero-mean, unit variance) ~ truncation to [-1, 1]\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(features)\n",
        "features = scaler.transform(features)\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "#fig = plt.figure()\n",
        "#ax1 = fig.add_subplot(221)\n",
        "#ax1.hist(features[noFraudInd,0], 50)\n",
        "#ax2 = fig.add_subplot(222)\n",
        "#ax2.hist(features[noFraudInd,1], 50)\n",
        "#ax3 = fig.add_subplot(223)\n",
        "#ax3.hist(features[noFraudInd,2], 50)\n",
        "#ax4 = fig.add_subplot(224)\n",
        "#ax4.hist(features[noFraudInd,3], 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bcafb30d-1e38-c3d7-bc67-6228c2c321a9"
      },
      "source": [
        "Due to skewed data, we use two classes for both features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2017edae-2e50-ae0b-647e-d593a577401b"
      },
      "outputs": [],
      "source": [
        "ind_fraud = np.where(label==1)\n",
        "ind_noFraud = np.where(label==0)\n",
        "\n",
        "features_noFraud = features[ind_noFraud]\n",
        "features_fraud = features[ind_fraud]\n",
        "print(features_noFraud.shape, features_fraud.shape, features.shape)\n",
        "\n",
        "def get_mini_batch(x,y):\n",
        "\trows=np.random.choice(x.shape[0], 100)\n",
        "\treturn x[rows], y[rows]\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "label_noFraud = np.matmul(np.ones((features_noFraud.shape[0], 1)), np.array([0, 1]).reshape((1,2)))\n",
        "label_fraud = np.matmul(np.ones((features_fraud.shape[0], 1)), np.array([1, 0]).reshape((1,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d40f7c20-ba23-a48c-079a-384eb1eb7dcc"
      },
      "source": [
        "**Tensorflow approach**\n",
        "-----------------------\n",
        "\n",
        "Split into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b5f6bcaa-98ee-0d8c-c300-2fda775ed20b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TestPortion = 0.2\n",
        "RND_STATE = 1\n",
        "\n",
        "# It is important to take same test_size and random_state for comparison\n",
        "x_trainNF, x_testNF, y_trainNF, y_testNF = train_test_split(features_noFraud, label_noFraud, test_size=TestPortion, random_state=RND_STATE)\n",
        "x_trainF, x_testF, y_trainF, y_testF = train_test_split(features_fraud, label_fraud, test_size=TestPortion, random_state=RND_STATE)\n",
        "\n",
        "print(y_trainF.shape, y_trainNF.shape)\n",
        "#y = np.append(y_trainNF, y_trainF)\n",
        "\n",
        "# Now stack them together and permute\n",
        "x_train = np.random.permutation(np.vstack((x_trainNF, x_trainF)))\n",
        "y_train = np.random.permutation(np.vstack((y_trainNF, y_trainF)))\n",
        "#y_train = np.random.permutation(np.append(y_trainNF, y_trainF))\n",
        "x_test = np.random.permutation(np.vstack((x_testNF, x_testF)))\n",
        "y_test = np.random.permutation(np.vstack((y_testNF, y_testF)))\n",
        "#y_test = np.random.permutation(np.append(y_testNF, y_testF))\n",
        "print(y_train.shape, y_test.shape)\n",
        "print(x_train.shape, x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1fcd9601-34e4-3320-afcc-336516db60b7"
      },
      "source": [
        "Now some preparation for tf session: Placeholders for data to be fed in. Then build a NN with one hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b975f616-29d5-6061-5086-f8e6070d3512"
      },
      "outputs": [],
      "source": [
        "nFeature = x_train.shape[1]\n",
        "\n",
        "# place holder for inputs. \n",
        "x = tf.placeholder(\"float\", [None, nFeature])\n",
        "\n",
        "# take 'nFeautre' features to 'N2' nodes in hidden layer,  \n",
        "# init weights with truncated normal distribution\n",
        "N2 = 100\n",
        "w1 = tf.Variable(tf.truncated_normal([nFeature, N2], stddev=0.01, name = 'w1')) \n",
        "b1 = tf.Variable(tf.constant(0.0, shape=[N2])) \n",
        "\n",
        "# calculate activations \n",
        "hidden_output = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
        "#hidden_output = x\n",
        "\n",
        "# bring from 'N2' nodes to 2 outputs:\n",
        "#N3 = 10\n",
        "w2 = tf.Variable(tf.truncated_normal([N2, 2], stddev=0.01, name = 'w2')) \n",
        "b2 = tf.Variable(tf.constant(0.0, shape=[2])) \n",
        "\n",
        "#hidden_output = tf.nn.relu(tf.matmul(hidden_output1, w2) + b2)\n",
        "#w3 = tf.Variable(tf.truncated_normal([N3, 2], stddev=0.025, name = 'w3')) \n",
        "#b3 = tf.Variable(tf.constant(0.05, shape=[2])) \n",
        "\n",
        "# placeholder for labels\n",
        "y_ = tf.placeholder(\"float\", [None,2])\n",
        "\n",
        "# #implement model. these are predicted ys\n",
        "#y = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
        "y = tf.nn.softmax(tf.matmul(hidden_output, w2) + b2)\n",
        "loss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y, y_, name='xentropy')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bdc10385-a18b-1035-1f3b-207c94fca79c"
      },
      "source": [
        "Create an optimizer op, and a train step to minimize the cost function iteratively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fc11642c-20fd-f5f7-ab51-dcb749611a50"
      },
      "outputs": [],
      "source": [
        "opt = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
        "#opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "train_step = opt.minimize(loss, var_list=[w1,b1,w2,b2])\n",
        "tf_correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "tf_accuracy = tf.reduce_mean(tf.cast(tf_correct_prediction, \"float\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "09dcfc61-3229-3b36-f75d-0bc90fd2e257"
      },
      "source": [
        "Start tf session and initialize variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "440c1472-660a-efaf-fc3a-0bab36d7d7b9"
      },
      "outputs": [],
      "source": [
        "# Start an interactive session\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# init all vars\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "327de166-af13-2044-0fc3-a3fe860032b9"
      },
      "source": [
        "Feed the training data into the train_step, and print our accuracy on the test set and the corresponding value of the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a1c093f7-b749-10e3-1173-2f34a3e01876"
      },
      "outputs": [],
      "source": [
        "ntrials = 5000\n",
        "for i in range(ntrials):\n",
        "    # get mini batch\n",
        "    a,b=get_mini_batch(x_train,y_train)\n",
        "    \n",
        "    # run train step, feeding arrays of 100 rows each time\n",
        "    _, cost =sess.run([train_step,loss], feed_dict={x: a, y_: b})\n",
        "    if i%100 ==0:\n",
        "        trainAccuracy = tf_accuracy.eval(feed_dict={x: a, y_: b})  \n",
        "        print(\"epoch is {0} and cost is {1} with train accuracy {2}\".format(i, cost, trainAccuracy))   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4a2f4952-7ebc-6b5e-6e79-fe66e257d5e8"
      },
      "source": [
        "Accuracy on test set, and true/false positives/negatives to get more insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fe22181d-1b21-6dd6-124c-168768183d14"
      },
      "outputs": [],
      "source": [
        "result = tf_accuracy.eval(feed_dict={x: x_test, y_: y_test})\n",
        "\n",
        "print(\"Test accuracy: {}\".format(result))\n",
        "\n",
        "# predicted labels\n",
        "prob=y\n",
        "y_pred = prob.eval(feed_dict={x:x_test, y_:y_test})\n",
        "y_pred = np.argmax(y_pred, 1)\n",
        "y_test = np.argmax(y_test, 1)\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix, classification_report, recall_score, roc_auc_score\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
        "area = auc(recall, precision)\n",
        "print('cm:', confusion_matrix(y_test,y_pred))\n",
        "print('cr:', classification_report(y_test,y_pred))\n",
        "print('recall_score:', recall_score(y_test,y_pred))\n",
        "print('roc_auc_score:',roc_auc_score(y_test,y_pred))\n",
        "\n",
        "sess.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "94b1693e-73b3-282d-4d33-862b04efc5ae"
      },
      "source": [
        "This trained NN does not work for fraud detection: None fraud out of 99 fraud cases is detected! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "42c980ce-1a46-e55e-5d0a-111eb95ff30a"
      },
      "source": [
        "Different ML approaches\n",
        "-----------------------\n",
        "\n",
        "Logistic regression, etratree, random forest (running slowly in notebook).\n",
        "Both etratree and radnom forest classifiers perform similarly, achieving a good trade-off between fraud detection and false positive rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "22026b6d-8497-c9d8-051e-90e60ca62627"
      },
      "outputs": [],
      "source": [
        "x_tr, x_test, y_tr, y_test = train_test_split(features, label, test_size = TestPortion, random_state = 1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C = .01, penalty = 'l1', class_weight='balanced')\n",
        "logreg.fit(x_tr,y_tr)\n",
        "y_pred= logreg.predict(x_test)\n",
        "print('------------ Results for LogiRegression ---------------')\n",
        "print('cm:', confusion_matrix(y_test,y_pred))\n",
        "#print('cr:', classification_report(y_test,y_pred))\n",
        "#print('recall_score:', recall_score(y_test,y_pred))\n",
        "print('roc_auc_score:',roc_auc_score(y_test,y_pred))\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
        "area = auc(recall, precision)\n",
        "print(\"Area Under P-R Curve: \",area)\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "clf = ExtraTreesClassifier(n_estimators =100)\n",
        "clf.fit(x_tr, y_tr)\n",
        "y_pred = clf.predict(x_test)\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
        "area = auc(recall, precision)\n",
        "\n",
        "print('------------ Results for ExtraTreeClassifier ---------------')\n",
        "print('cm:', confusion_matrix(y_test,y_pred))\n",
        "#print('cr:', classification_report(y_test,y_pred))\n",
        "#print('recall_score:', recall_score(y_test,y_pred))\n",
        "print('roc_auc_score:',roc_auc_score(y_test,y_pred))\n",
        "print(\"Area Under P-R Curve: \",area)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(n_estimators=100)    \n",
        "clf.fit(x_tr, y_tr)\n",
        "importances = clf.feature_importances_\n",
        "#plt.figure()\n",
        "#plt.title(\"Feature importances\")\n",
        "#plt.bar(range(x_tr.shape[1]), importances,\n",
        "#       color=\"r\", align=\"center\")\n",
        "#plt.xticks(range(x_tr.shape[1]))\n",
        "#plt.xlim([-1, x_tr.shape[1]])\n",
        "#plt.show()\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
        "area = auc(recall, precision)\n",
        "\n",
        "print('------------ Results for RandomForestClassifier ---------------')\n",
        "print('cm:', confusion_matrix(y_test,y_pred))\n",
        "#print('cr:', classification_report(y_test,y_pred))\n",
        "#print('recall_score:', recall_score(y_test,y_pred))\n",
        "print('roc_auc_score:',roc_auc_score(y_test,y_pred))\n",
        "print(\"Area Under P-R Curve: \",area)\n"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}