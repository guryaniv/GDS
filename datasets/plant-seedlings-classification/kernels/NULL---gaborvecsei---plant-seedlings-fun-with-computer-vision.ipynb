{"nbformat_minor": 1, "metadata": {"language_info": {"file_extension": ".py", "version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "cells": [{"metadata": {"_uuid": "8ed8814047633de72a7112880bd46c460776d1d3", "_cell_guid": "a0cb0f67-b40f-4611-b889-035c91ebb0c3"}, "source": ["# Plant Seedlings Segmentation with pure Computer Vision"], "cell_type": "markdown"}, {"metadata": {"_uuid": "05dde1ddf47a7e42f7075700b90f2b9e8be3b975", "_cell_guid": "67496b8d-27df-4498-a587-79051a5c1f35"}, "source": ["First of all, thanks for the popularity of this kernel. I hope it will help for you to create more accurate predictions"], "cell_type": "markdown"}, {"source": ["%matplotlib inline\n", "import os\n", "import matplotlib\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import cv2\n", "import numpy as np\n", "from glob import glob\n", "import seaborn as sns"], "metadata": {"collapsed": true, "_uuid": "f39f0c6a929d0db7d1aa48c2722cce5c37aa3b5d", "_cell_guid": "a1767b4c-9ea3-4342-8ee5-56660856183b"}, "outputs": [], "cell_type": "code", "execution_count": 1}, {"source": ["BASE_DATA_FOLDER = \"../input\"\n", "TRAin_DATA_FOLDER = os.path.join(BASE_DATA_FOLDER, \"train\")"], "metadata": {"collapsed": true, "_uuid": "622180f74f171af387a6f40fadd8f23b6806885a", "_cell_guid": "c7a5628f-95de-419f-b850-eafe47b1e35d"}, "outputs": [], "cell_type": "code", "execution_count": 2}, {"metadata": {"_uuid": "d069eff7a882e41838282d7e10f5d97e835d4447", "_cell_guid": "c10f40f2-f71e-4085-a744-42a142677558"}, "source": ["### Read images\n", "First, I'll just read all the images. The images are in BGR (Blue/Green/Red) format because OpenCV uses this.\n", "\n", "Btw... If you'd like to use RGB format, than you can use it, it won't effect the segmentation because we will use the HSV (Hue/Saturation/Value) color space for that."], "cell_type": "markdown"}, {"source": ["images_per_class = {}\n", "for class_folder_name in os.listdir(TRAin_DATA_FOLDER):\n", "    class_folder_path = os.path.join(TRAin_DATA_FOLDER, class_folder_name)\n", "    class_label = class_folder_name\n", "    images_per_class[class_label] = []\n", "    for image_path in glob(os.path.join(class_folder_path, \"*.png\")):\n", "        image_bgr = cv2.imread(image_path, cv2.IMREAD_COLOR)\n", "        images_per_class[class_label].append(image_bgr)"], "metadata": {"collapsed": true, "_uuid": "1d421d3a24167567cbc26d5f668acac290dc7b0f", "_cell_guid": "1019fbd1-3adb-402c-864a-3030694a12d8"}, "outputs": [], "cell_type": "code", "execution_count": 3}, {"metadata": {"_uuid": "62c42371237dfd9f56d01a725bf69d4627642d1a", "_cell_guid": "5b048546-3d90-4361-93de-f2a6172ff0df"}, "source": ["### Number of images per class"], "cell_type": "markdown"}, {"source": ["for key,value in images_per_class.items():\n", "    print(\"{0} -> {1}\".format(key, len(value)))"], "metadata": {"_uuid": "d3f0406443e03dab008abcd2aa315f14108e5814", "_cell_guid": "b3a465ca-6600-46e2-a923-631eb4ec1b23"}, "outputs": [], "cell_type": "code", "execution_count": 4}, {"metadata": {"_uuid": "a4f0376c290e3efa6344f1a49e30ce580d45f17f", "_cell_guid": "59db49a3-8d9f-4a86-b2f3-12ea2db779f4"}, "source": ["### Plot images\n", "Plot images so we can see what the input looks like"], "cell_type": "markdown"}, {"source": ["def plot_for_class(label):\n", "    nb_rows = 3\n", "    nb_cols = 3\n", "    fig, axs = plt.subplots(nb_rows, nb_cols, figsize=(6, 6))\n", "\n", "    n = 0\n", "    for i in range(0, nb_rows):\n", "        for j in range(0, nb_cols):\n", "            axs[i, j].xaxis.set_ticklabels([])\n", "            axs[i, j].yaxis.set_ticklabels([])\n", "            axs[i, j].imshow(images_per_class[label][n])\n", "            n += 1        "], "metadata": {"collapsed": true, "_uuid": "d908e6a36213ed4807f36ff771031e5c45e1e506", "_cell_guid": "98e34b57-34de-4c04-8442-1b6e46a7e92d"}, "outputs": [], "cell_type": "code", "execution_count": 5}, {"source": ["plot_for_class(\"Small-flowered Cranesbill\")"], "metadata": {"_uuid": "c186f23d145dbb2987b6c41d21372db87dbbed95", "_cell_guid": "d4c11145-1017-47ca-9465-dff77a90937e"}, "outputs": [], "cell_type": "code", "execution_count": 6}, {"source": ["plot_for_class(\"Maize\")"], "metadata": {"_uuid": "c33768fb8bb0e80726846f7ccef279a031d778eb", "_cell_guid": "521a656a-377e-4555-a3ac-3b9e8849b47a", "scrolled": false}, "outputs": [], "cell_type": "code", "execution_count": 7}, {"metadata": {"_uuid": "a4d9f5707e90e3a71dbb1ae65f424fc820210ce3", "_cell_guid": "a8c70b81-a900-4c71-8fc5-c907600b5d7f"}, "source": ["### Preprocessing for the images:\n", "\n", "Now comes the interesting and fun part!\n", "\n", "I created separate functions so if you'd like to use these it is easier.\n", "\n", "In the next block I'll explain what I am doing to make the segmentation happen."], "cell_type": "markdown"}, {"source": ["def create_mask_for_plant(image):\n", "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n", "\n", "    sensitivity = 35\n", "    lower_hsv = np.array([60 - sensitivity, 100, 50])\n", "    upper_hsv = np.array([60 + sensitivity, 255, 255])\n", "\n", "    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n", "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n", "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n", "    \n", "    return mask\n", "\n", "def segment_plant(image):\n", "    mask = create_mask_for_plant(image)\n", "    output = cv2.bitwise_and(image, image, mask = mask)\n", "    return output\n", "\n", "def sharpen_image(image):\n", "    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n", "    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n", "    return image_sharp"], "metadata": {"collapsed": true, "_uuid": "a2f2e7898dbbc27369a5ca467556ed533a0a9c23", "_cell_guid": "84b0f059-d103-4e96-9d79-d87c6229b0fe"}, "outputs": [], "cell_type": "code", "execution_count": 8}, {"metadata": {"_uuid": "83110ea07ecb54c83f1f40a6278327aafd6b34fd", "_cell_guid": "d5f432e4-dab9-4361-b07f-b0518879e7da"}, "source": ["The `create_mask_for_plant` function: This function returns an image mask: Matrix with shape `(image_height, image_width)`. In this matrix there are only `0` and `1` values. The 1 values define the interesting part of the original image. But the question is...How do we create this mask?\n", "\n", "This is a simple object detection problem, where we can use the color of the object.\n", "\n", "The HSV color-space is suitable for color detection because with the Hue we can define the color and the saturation and value will define \"different kinds\" of the color. (For example it will detect the red, darker red, lighter red too). We cannot do this with the original BGR color space.\n", "\n", "![](https://www.mathworks.com/help/images/hsvcone.gif)\n", "\n", "*image from https://www.mathworks.com/help/images/convert-from-hsv-to-rgb-color-space.html*\n", "\n", "We have to set a range, which color should be detected:\n", "\n", "    sensitivity = 35\n", "    lower_hsv = np.array([60 - sensitivity, 100, 50])\n", "    upper_hsv = np.array([60 + sensitivity, 255, 255])\n", "    \n", "After the mask is created with the `inRange` function, we can do a little *CV magic* (not close to magic, because this is almost the most basic thing in CV, but it is a cool buzzword, and this opertation is as awesome as simple it is) which is called *morphological operations* ([You can read more here](https://www.cs.auckland.ac.nz/courses/compsci773s1c/lectures/ImageProcessing-html/topic4.htm)).\n", "\n", "Basically with the *Close* operation we would like to keep the shape of the original objects (1 blobs on the mask image) but close the small holes. That way we can clarify our detection mask more.\n", "\n", "![](https://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/closebin.gif)\n", "\n", "*image from https://www.cs.auckland.ac.nz/courses/compsci773s1c/lectures/ImageProcessing-html/topic4.htm*\n", "\n", "After these steps we created the mask for the object.\n"], "cell_type": "markdown"}, {"source": ["# Test image to see the changes\n", "image = images_per_class[\"Small-flowered Cranesbill\"][97]\n", "\n", "image_mask = create_mask_for_plant(image)\n", "image_segmented = segment_plant(image)\n", "image_sharpen = sharpen_image(image_segmented)\n", "\n", "fig, axs = plt.subplots(1, 4, figsize=(20, 20))\n", "axs[0].imshow(image)\n", "axs[1].imshow(image_mask)\n", "axs[2].imshow(image_segmented)\n", "axs[3].imshow(image_sharpen)"], "metadata": {"_uuid": "b27217b1e5bbeacaca1dad627117d0a7a5953826", "_cell_guid": "4064997e-2a20-49a6-9110-1863e2af5f4d", "scrolled": true}, "outputs": [], "cell_type": "code", "execution_count": 9}, {"metadata": {"_uuid": "8d27e084e8f79bad6e191ce7562f5af71b0e7048", "_cell_guid": "b1542194-f8a5-43b3-8a0a-5a3e5db6019d"}, "source": ["After this step we can see that the image on the right is more recognizable than the original image on the left."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "f6c3b077b7bd2c436c05d2ba2d0f1a62e8a2a914", "_cell_guid": "31468800-4545-4761-91f5-9fdb643cda46"}, "source": ["----------------------------------------------"], "cell_type": "markdown"}, {"metadata": {"_uuid": "5ed851b89b2aa39fcf135c854ba178d740f2ffea", "_cell_guid": "489ed19d-5973-4b2b-b1de-4210faf11cb2"}, "source": ["From the mask image what we created (because we need that for the segmentation), we can extract some features. For example we can see how the area of the plant changes based on their classes."], "cell_type": "markdown"}, {"metadata": {"_uuid": "ddc5eb913130f3d15437c73e103d524a8f9ad6a2", "_cell_guid": "91beb15f-bfff-40f0-88d6-05b4d5fa55ae"}, "source": ["Of course from the contours we can extract much more information than the area of the\n", "contour and the number of components, but this is the one I would like to show you.\n", "\n", "Additional read: https://en.wikipedia.org/wiki/Image_moment"], "cell_type": "markdown"}, {"source": ["def find_contours(mask_image):\n", "    return cv2.findContours(mask_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n", "\n", "def calculate_largest_contour_area(contours):\n", "    if len(contours) == 0:\n", "        return 0\n", "    c = max(contours, key=cv2.contourArea)\n", "    return cv2.contourArea(c)\n", "\n", "def calculate_contours_area(contours, min_contour_area = 250):\n", "    area = 0\n", "    for c in contours:\n", "        c_area = cv2.contourArea(c)\n", "        if c_area >= min_contour_area:\n", "            area += c_area\n", "    return area"], "metadata": {"collapsed": true, "_uuid": "0c2f88819cbd188b4ef275e6f5825ef981bca6e1", "_cell_guid": "1348bdc1-86e9-4fa8-a79a-669546a73d46"}, "outputs": [], "cell_type": "code", "execution_count": 10}, {"source": ["areas = []\n", "larges_contour_areas = []\n", "labels = []\n", "nb_of_contours = []\n", "images_height = []\n", "images_width = []\n", "\n", "for class_label in images_per_class.keys():\n", "    for image in images_per_class[class_label]:\n", "        mask = create_mask_for_plant(image)\n", "        contours = find_contours(mask)\n", "        \n", "        area = calculate_contours_area(contours)\n", "        largest_area = calculate_largest_contour_area(contours)\n", "        height, width, channels = image.shape\n", "        \n", "        images_height.append(height)\n", "        images_width.append(width)\n", "        areas.append(area)\n", "        nb_of_contours.append(len(contours))\n", "        larges_contour_areas.append(largest_area)\n", "        labels.append(class_label)"], "metadata": {"collapsed": true, "_uuid": "00221628e47f5c7b31d520a6b9087e538d02c7af", "_cell_guid": "ca8d916b-a09c-4a39-a896-6856a38532f7"}, "outputs": [], "cell_type": "code", "execution_count": 11}, {"source": ["features_df = pd.DataFrame()\n", "features_df[\"label\"] = labels\n", "features_df[\"area\"] = areas\n", "features_df[\"largest_area\"] = larges_contour_areas\n", "features_df[\"number_of_components\"] = nb_of_contours\n", "features_df[\"height\"] = images_height\n", "features_df[\"width\"] = images_width"], "metadata": {"collapsed": true, "_uuid": "f6150ac38910b3534f969f34bae3b124eba8df65", "_cell_guid": "3b4703f4-e585-4851-b003-04384b065cb4"}, "outputs": [], "cell_type": "code", "execution_count": 12}, {"source": ["features_df.groupby(\"label\").describe()"], "metadata": {"_uuid": "3f00158f8f3f391a7a1c43f88d5b87f781ff3fac", "_cell_guid": "b7d1986f-746c-4711-a569-c38af8d05c86", "scrolled": true}, "outputs": [], "cell_type": "code", "execution_count": 13}, {"source": [], "metadata": {"collapsed": true, "_uuid": "272c7e2991dac3c535acd0e9407f53e1975afb45", "_cell_guid": "1a8bb81e-27ed-4656-82a4-2ad1ae194de4"}, "outputs": [], "cell_type": "code", "execution_count": null}], "nbformat": 4}