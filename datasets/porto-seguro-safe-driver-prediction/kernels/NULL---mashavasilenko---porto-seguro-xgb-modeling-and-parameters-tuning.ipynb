{"cells": [{"outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2\n", "\n", "import matplotlib.pylab as plt\n", "%matplotlib inline\n", "from matplotlib.pylab import rcParams\n", "rcParams['figure.figsize'] = 12, 4\n", "%matplotlib inline\n", "\n", "import pandas as pd\n", "\n", "import numpy as np\n", "\n", "import sklearn\n", "from sklearn.datasets import make_regression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import recall_score\n", "from sklearn import metrics\n", "\n", "import xgboost as xgb\n", "from xgboost.sklearn import XGBClassifier\n", "\n", "from IPython.display import display\n", "\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_cell_guid": "34f4e4e4-6737-4777-bd91-3e4115f6e072", "_uuid": "a93c21205bea554b6d322b9124549199851113a5"}, "execution_count": null, "cell_type": "code"}, {"source": ["**GINI calculations**"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["def gini(actual, pred, cmpcol = 0, sortcol = 1):\n", "    assert( len(actual) == len(pred) )\n", "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n", "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n", "    totalLosses = all[:,0].sum()\n", "    giniSum = all[:,0].cumsum().sum() / totalLosses\n", "    \n", "    giniSum -= (len(actual) + 1) / 2.\n", "    return giniSum / len(actual)\n", "\n", "def gini_normalized(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = gini_normalized(labels, preds)\n", "    return [('gini', gini_score)]"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["PATH = \"../input/train.csv\"\n", "data_raw= pd.read_csv(f'{PATH}', low_memory=False)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["def display_all(df):\n", "    with pd.option_context(\"display.max_rows\", 1000): \n", "        with pd.option_context(\"display.max_columns\", 1000): \n", "            display(df)\n", "display_all(data_raw.head(5))"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["# Describe the data set\n", "display_all(data_raw.describe(include='all'))"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"source": ["**Important observations:**\n", "- Target variable (*target*) has its mean 3.65% meaning that only 3.65% of data is classified as the target. Hence, the dataset is unbalanced"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["# Distribution of target variable\n", "import matplotlib.pyplot as plt\n", "plt.hist(data_raw['target'])\n", "plt.show()\n", "\n", "print('Percentage of claims filed :' , str(np.sum(data_raw['target'])/data_raw.shape[0]*100), '%')"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"source": ["But before jumping into balancing the data let's **look at the NA's, which in this dataset are denoted as -1**."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["nas = np.sum(data_raw == -1)/len(data_raw) *100\n", "print(\"The percentage of missing values is\")\n", "print (nas[nas>0].sort_values(ascending = False))"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"source": ["Now, for categorical variables we will create dummy variables (aka **one-hot encoding**)"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["# make a copy of the initial dataset\n", "data_clean = data_raw.copy()\n", "#data_clean.columns\n", "cat_cols = [c for c in data_clean.columns if c.endswith('cat')]\n", "for column in cat_cols:\n", "    temp=pd.get_dummies(data_clean[column], prefix=column, prefix_sep='_')\n", "    data_clean=pd.concat([data_clean,temp],axis=1)\n", "    data_clean=data_clean.drop([column],axis=1)\n", "\n", "print('data_clean shape is:',data_clean.shape)"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["# Impute missing values with medians\n", "\n", "num_cols = ['ps_reg_03','ps_car_14', 'ps_car_11', 'ps_car_12' ]\n", "\n", "for n in num_cols:\n", "    dummy_name = str(n) + 'NA'\n", "    data_clean[dummy_name] = (data_clean[n]==-1).astype(int)\n", "    med = data_clean[data_clean[n]!=-1][n].median()\n", "    data_clean.loc[data_clean[n]==-1,n] = med\n", "    \n", "\n", "    "], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#Make transformation to ps_car_13, as suggested here: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41489\n", "data_clean['ps_car_13_trans'] = round(data_clean['ps_car_13']* data_clean['ps_car_13']* 90000,2)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["**Undersampling**:\n", "Let's take 25% of abundant data (target = 0) and stack together with the rare data (target = 1)"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["sub_df_0= data_clean[(data_clean['target']==0)]\n", "sub_df_1= data_clean[(data_clean['target']==1)]\n", "sub_df_1.shape"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["sub_df = sub_df_0.sample(frac = 0.25, random_state = 42)\n", "data_sub = pd.concat([sub_df_1,sub_df])"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["## XGBoost model"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["# First split the data into training and validation (test) sets\n", "training_features, test_features, \\\n", "training_target, test_target, = train_test_split(data_sub.drop(['id','target'], axis=1),\n", "                                               data_sub['target'],\n", "                                               test_size = .2,\n", "                                               random_state=12)\n", "\n", "# Now further split the training test into training and validation to \n", "x_train, x_val, y_train, y_val = train_test_split(training_features, training_target,\n", "                                                  test_size = .2,\n", "                                                  random_state=12)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["We first run an XGB trainng model with some baseline parameters and then proceed to tuning the key parameters via a series of loops. \n", "\n", "To grasp an idea of what parameters to tune and in what order, look here:\n", "- https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n", "- https://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1?next_slideshow=1"], "metadata": {}, "cell_type": "markdown"}, {"source": ["Since it takes time (10-30) minutes to run loops, I'll comment out the code and hardcode the results below each code cell with a loop. To actually run the code, please uncomment it."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["xgb_params = {'eta': 0.02, \n", "              'max_depth': 6, \n", "              'subsample': 1.0, \n", "              'colsample_bytree': 0.3,\n", "              'min_child_weight': 1,\n", "              'objective': 'binary:logistic', \n", "              'eval_metric': 'auc', \n", "              'seed': 99, \n", "              'silent': True}\n", "d_train = xgb.DMatrix(x_train, y_train)\n", "d_valid = xgb.DMatrix(x_val,y_val)\n", "d_test = xgb.DMatrix(test_features)\n", "\n", "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "#model = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=100, early_stopping_rounds=200)\n", "#print(model.best_score, model.best_iteration, model.best_ntree_limit)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n", "\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "\n", "[100]\ttrain-gini:0.321987\tvalid-gini:0.278464\n", "\n", "[200]\ttrain-gini:0.359511\tvalid-gini:0.285658\n", "\n", "[300]\ttrain-gini:0.392516\tvalid-gini:0.289204\n", "\n", "[400]\ttrain-gini:0.419991\tvalid-gini:0.289849\n", "\n", "[500]\ttrain-gini:0.443408\tvalid-gini:0.291014\n", "\n", "[600]\ttrain-gini:0.458003\tvalid-gini:0.290918\n", "\n", "[700]\ttrain-gini:0.473734\tvalid-gini:0.290754\n", "\n", "[800]\ttrain-gini:0.487845\tvalid-gini:0.290586\n", "\n", "Stopping. Best iteration:\n", "[\n", "648]\ttrain-gini:0.465592\tvalid-gini:0.291336\n", "\n", "0.291336 648 649"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["#results = {'best_score':[],'best_iter':[],'best_ntree_limit':[]}"], "metadata": {"_kg_hide-output": true, "_kg_hide-input": false, "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["train-gini:0.465592\tvalid-gini:0.291336\n", "Best_ntree_limit = 649\n", "\n", "Now, let's tune the **learning rate (eta)**"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["results = {'eta':[],'best_score':[],'best_ntree_limit':[]}\n", "for e in [0.01, 0.02, 0.03,0.05,0.1,0.2]:\n", "    xgb_params = {'eta': e, \n", "                  'max_depth': 6, \n", "                  'subsample': 1.0, \n", "                  'colsample_bytree': 0.3,\n", "                  'min_child_weight': 1,\n", "                  'objective': 'binary:logistic', \n", "                  'seed': 99, \n", "                  'silent': True}\n", "\n", "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "   # m = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=100, early_stopping_rounds=200)\n", "    #results['best_score'].append(m.best_score)\n", "    #results['best_ntree_limit'].append(m.best_ntree_limit)\n", "    #results['eta'].append(e)\n", "    \n", "#print('eta:',results['eta'],'best_score:',results['best_score'],'best_ntree_limit:', results['best_ntree_limit'])"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[100]\ttrain-gini:0.306974\tvalid-gini:0.276946\n", "[200]\ttrain-gini:0.324915\tvalid-gini:0.279907\n", "[300]\ttrain-gini:0.341885\tvalid-gini:0.284432\n", "[400]\ttrain-gini:0.360752\tvalid-gini:0.286026\n", "[500]\ttrain-gini:0.379185\tvalid-gini:0.2878\n", "[600]\ttrain-gini:0.394244\tvalid-gini:0.288596\n", "[700]\ttrain-gini:0.408409\tvalid-gini:0.289171\n", "[800]\ttrain-gini:0.420627\tvalid-gini:0.289688\n", "[900]\ttrain-gini:0.433574\tvalid-gini:0.289878\n", "[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[100]\ttrain-gini:0.321987\tvalid-gini:0.278464\n", "[200]\ttrain-gini:0.359511\tvalid-gini:0.285658\n", "[300]\ttrain-gini:0.392516\tvalid-gini:0.289204\n", "[400]\ttrain-gini:0.419991\tvalid-gini:0.289849\n", "[500]\ttrain-gini:0.443408\tvalid-gini:0.291014\n", "[600]\ttrain-gini:0.458003\tvalid-gini:0.290918\n", "[700]\ttrain-gini:0.473734\tvalid-gini:0.290754\n", "[800]\ttrain-gini:0.487845\tvalid-gini:0.290586\n", "Stopping. Best iteration:\n", "[648]\ttrain-gini:0.465592\tvalid-gini:0.291336\n", "\n", "[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[100]\ttrain-gini:0.337376\tvalid-gini:0.283497\n", "[200]\ttrain-gini:0.390756\tvalid-gini:0.289544\n", "[300]\ttrain-gini:0.429966\tvalid-gini:0.292974\n", "[400]\ttrain-gini:0.456514\tvalid-gini:0.292849\n", "[500]\ttrain-gini:0.481765\tvalid-gini:0.291735\n", "Stopping. Best iteration:\n", "[340]\ttrain-gini:0.44194\tvalid-gini:0.293288\n", "\n", "[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[100]\ttrain-gini:0.37337\tvalid-gini:0.288152\n", "[200]\ttrain-gini:0.440843\tvalid-gini:0.289715\n", "[300]\ttrain-gini:0.48632\tvalid-gini:0.287219\n", "[400]\ttrain-gini:0.520738\tvalid-gini:0.286422\n", "Stopping. Best iteration:\n", "[201]\ttrain-gini:0.441803\tvalid-gini:0.289792\n", "\n", "[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[100]\ttrain-gini:0.431092\tvalid-gini:0.28708\n", "[200]\ttrain-gini:0.516016\tvalid-gini:0.28225\n", "[300]\ttrain-gini:0.579317\tvalid-gini:0.275065\n", "Stopping. Best iteration:\n", "[102]\ttrain-gini:0.433424\tvalid-gini:0.287461\n", "\n", "[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[100]\ttrain-gini:0.512916\tvalid-gini:0.273241\n", "[200]\ttrain-gini:0.638731\tvalid-gini:0.261126\n", "Stopping. Best iteration:\n", "[32]\ttrain-gini:0.389099\tvalid-gini:0.282492\n", "\n", "eta: [0.01, 0.02, 0.03, 0.05, 0.1, 0.2] best_score: [0.290367, 0.291336, 0.293288, 0.289792, 0.287461, 0.282492] best_ntree_limit: [988, 649, 341, 202, 103, 33]"], "metadata": {}, "cell_type": "markdown"}, {"source": ["We see that **$\\eta = 0.03$** gives better score of 0.293288 and at such learning rate **n_trees** = 341\n", "\n", "We can now tune ```max_depth``` parameter"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["results = {'max_depth':[],'best_score':[],'best_ntree_limit':[]}\n", "for md in range(3,9,1):\n", "    xgb_params = {'eta': 0.03, \n", "                  'max_depth': md, \n", "                  'subsample': 1.0, \n", "                  'colsample_bytree': 0.3,\n", "                  'min_child_weight': 1,\n", "                  'objective': 'binary:logistic', \n", "                  'seed': 99, \n", "                  'silent': True}\n", "\n", "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "    #m = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=50, early_stopping_rounds=200)\n", "    #results['best_score'].append(m.best_score)\n", "    #results['best_ntree_limit'].append(m.best_ntree_limit)\n", "    #results['max_depth'].append(md)\n", "    \n", "#print('max_depth:',results['max_depth'],'best_score:',results['best_score'],'best_ntree_limit:', results['best_ntree_limit'])"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["[0]\ttrain-gini:0.152895\tvalid-gini:0.148037\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[50]\ttrain-gini:0.254714\tvalid-gini:0.261111\n", "[100]\ttrain-gini:0.264271\tvalid-gini:0.268258\n", "[150]\ttrain-gini:0.275104\tvalid-gini:0.274721\n", "[200]\ttrain-gini:0.283029\tvalid-gini:0.278809\n", "[250]\ttrain-gini:0.290808\tvalid-gini:0.282106\n", "[300]\ttrain-gini:0.296478\tvalid-gini:0.284243\n", "[350]\ttrain-gini:0.301258\tvalid-gini:0.285575\n", "[400]\ttrain-gini:0.305827\tvalid-gini:0.286782\n", "[450]\ttrain-gini:0.310241\tvalid-gini:0.287885\n", "[500]\ttrain-gini:0.313672\tvalid-gini:0.288415\n", "[550]\ttrain-gini:0.31689\tvalid-gini:0.288889\n", "[600]\ttrain-gini:0.320219\tvalid-gini:0.289408\n", "[650]\ttrain-gini:0.322747\tvalid-gini:0.289832\n", "[700]\ttrain-gini:0.325932\tvalid-gini:0.290298\n", "[750]\ttrain-gini:0.328984\tvalid-gini:0.290569\n", "[800]\ttrain-gini:0.331656\tvalid-gini:0.29101\n", "[850]\ttrain-gini:0.334356\tvalid-gini:0.291343\n", "[900]\ttrain-gini:0.336958\tvalid-gini:0.291606\n", "[950]\ttrain-gini:0.339361\tvalid-gini:0.291371\n", "[0]\ttrain-gini:0.171693\tvalid-gini:0.166814\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[50]\ttrain-gini:0.269709\tvalid-gini:0.270024\n", "[100]\ttrain-gini:0.280589\tvalid-gini:0.276234\n", "[150]\ttrain-gini:0.294328\tvalid-gini:0.280918\n", "[200]\ttrain-gini:0.306189\tvalid-gini:0.283884\n", "[250]\ttrain-gini:0.316871\tvalid-gini:0.286329\n", "[300]\ttrain-gini:0.325312\tvalid-gini:0.287213\n", "[350]\ttrain-gini:0.333742\tvalid-gini:0.288468\n", "[400]\ttrain-gini:0.340944\tvalid-gini:0.289095\n", "[450]\ttrain-gini:0.347357\tvalid-gini:0.288774\n", "[500]\ttrain-gini:0.353035\tvalid-gini:0.289154\n", "[550]\ttrain-gini:0.358306\tvalid-gini:0.289405\n", "[600]\ttrain-gini:0.363562\tvalid-gini:0.289747\n", "[650]\ttrain-gini:0.368137\tvalid-gini:0.289983\n", "[700]\ttrain-gini:0.373225\tvalid-gini:0.290168\n", "[750]\ttrain-gini:0.377744\tvalid-gini:0.290104\n", "[800]\ttrain-gini:0.382511\tvalid-gini:0.289967\n", "[850]\ttrain-gini:0.38713\tvalid-gini:0.28973\n", "Stopping. Best iteration:\n", "[691]\ttrain-gini:0.372335\tvalid-gini:0.290245\n", "\n", "[0]\ttrain-gini:0.186317\tvalid-gini:0.186526\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[50]\ttrain-gini:0.287977\tvalid-gini:0.274809\n", "[100]\ttrain-gini:0.304387\tvalid-gini:0.280157\n", "[150]\ttrain-gini:0.323393\tvalid-gini:0.284439\n", "[200]\ttrain-gini:0.340842\tvalid-gini:0.287747\n", "[250]\ttrain-gini:0.356246\tvalid-gini:0.290444\n", "[300]\ttrain-gini:0.369436\tvalid-gini:0.2913\n", "[350]\ttrain-gini:0.381322\tvalid-gini:0.291347\n", "[400]\ttrain-gini:0.390526\tvalid-gini:0.291576\n", "[450]\ttrain-gini:0.398911\tvalid-gini:0.291635\n", "[500]\ttrain-gini:0.406795\tvalid-gini:0.291134\n", "[550]\ttrain-gini:0.414509\tvalid-gini:0.290781\n", "[600]\ttrain-gini:0.422203\tvalid-gini:0.290599\n", "Stopping. Best iteration:\n", "[435]\ttrain-gini:0.396286\tvalid-gini:0.291927\n", "\n", "[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[50]\ttrain-gini:0.311761\tvalid-gini:0.278182\n", "[100]\ttrain-gini:0.337376\tvalid-gini:0.283497\n", "[150]\ttrain-gini:0.365534\tvalid-gini:0.287479\n", "[200]\ttrain-gini:0.390756\tvalid-gini:0.289544\n", "[250]\ttrain-gini:0.412683\tvalid-gini:0.292106\n", "[300]\ttrain-gini:0.429966\tvalid-gini:0.292974\n", "[350]\ttrain-gini:0.444596\tvalid-gini:0.293222\n", "[400]\ttrain-gini:0.456514\tvalid-gini:0.292849\n", "[450]\ttrain-gini:0.469462\tvalid-gini:0.292862\n", "[500]\ttrain-gini:0.481765\tvalid-gini:0.291735\n", "Stopping. Best iteration:\n", "[340]\ttrain-gini:0.44194\tvalid-gini:0.293288\n", "\n", "[0]\ttrain-gini:0.208237\tvalid-gini:0.200162\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[50]\ttrain-gini:0.346494\tvalid-gini:0.279076\n", "[100]\ttrain-gini:0.384323\tvalid-gini:0.284499\n", "[150]\ttrain-gini:0.424827\tvalid-gini:0.28727\n", "[200]\ttrain-gini:0.460108\tvalid-gini:0.288491\n", "[250]\ttrain-gini:0.490432\tvalid-gini:0.288809\n", "[300]\ttrain-gini:0.511943\tvalid-gini:0.288354\n", "[350]\ttrain-gini:0.527088\tvalid-gini:0.288391\n", "[400]\ttrain-gini:0.543902\tvalid-gini:0.288251\n", "[450]\ttrain-gini:0.561581\tvalid-gini:0.288526\n", "Stopping. Best iteration:\n", "[266]\ttrain-gini:0.496139\tvalid-gini:0.289732\n", "\n", "[0]\ttrain-gini:0.218514\tvalid-gini:0.196553\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[50]\ttrain-gini:0.395258\tvalid-gini:0.279268\n", "[100]\ttrain-gini:0.448654\tvalid-gini:0.285241\n", "[150]\ttrain-gini:0.50154\tvalid-gini:0.286381\n", "[200]\ttrain-gini:0.547344\tvalid-gini:0.285872\n", "[250]\ttrain-gini:0.580952\tvalid-gini:0.286312\n", "[300]\ttrain-gini:0.605303\tvalid-gini:0.286118\n", "Stopping. Best iteration:\n", "[134]\ttrain-gini:0.487134\tvalid-gini:0.287129\n", "\n", "max_depth: [3, 4, 5, 6, 7, 8] best_score: [0.291721, 0.290245, 0.291927, 0.293288, 0.289732, 0.287129] best_ntree_limit: [911, 692, 436, 341, 267, 135]"], "metadata": {}, "cell_type": "markdown"}, {"source": ["We can see that **```max_depth``` = 6** is the best choice, valid-gini:0.293288, n_trees_341\n", "Now, let's tweak ```min_child_weight``` parameter\n"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["results = {'min_child_w':[],'best_score':[],'best_ntree_limit':[]}\n", "for mcw in range(1,10,1):\n", "    xgb_params = {'eta': 0.03, \n", "                  'max_depth': 6, \n", "                  'subsample': 1.0, \n", "                  'colsample_bytree': 0.3,\n", "                  'min_child_weight': mcw,\n", "                  'objective': 'binary:logistic', \n", "                  'seed': 99, \n", "                  'silent': True}\n", "\n", "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "    #m = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=200, early_stopping_rounds=200)\n", "    #results['best_score'].append(m.best_score)\n", "    #results['best_ntree_limit'].append(m.best_ntree_limit)\n", "    #results['min_child_w'].append(mcw)\n", "    \n", "#print('min_child_w:',results['min_child_w'],'best_score:',results['best_score'],'best_ntree_limit:', results['best_ntree_limit'])"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.390756\tvalid-gini:0.289544\n", "[400]\ttrain-gini:0.456514\tvalid-gini:0.292849\n", "Stopping. Best iteration:\n", "[340]\ttrain-gini:0.44194\tvalid-gini:0.293288\n", "\n", "[0]\ttrain-gini:0.197209\tvalid-gini:0.19521\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.389033\tvalid-gini:0.290442\n", "[400]\ttrain-gini:0.455562\tvalid-gini:0.293512\n", "Stopping. Best iteration:\n", "[391]\ttrain-gini:0.452666\tvalid-gini:0.293653\n", "\n", "[0]\ttrain-gini:0.197141\tvalid-gini:0.194548\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.38573\tvalid-gini:0.288662\n", "[400]\ttrain-gini:0.448931\tvalid-gini:0.28902\n", "Stopping. Best iteration:\n", "[354]\ttrain-gini:0.438576\tvalid-gini:0.289972\n", "\n", "[0]\ttrain-gini:0.197227\tvalid-gini:0.194243\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.384145\tvalid-gini:0.289345\n", "[400]\ttrain-gini:0.445069\tvalid-gini:0.29142\n", "Stopping. Best iteration:\n", "[321]\ttrain-gini:0.426088\tvalid-gini:0.29215\n", "\n", "[0]\ttrain-gini:0.197234\tvalid-gini:0.194579\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.381508\tvalid-gini:0.290414\n", "[400]\ttrain-gini:0.442692\tvalid-gini:0.292785\n", "Stopping. Best iteration:\n", "[354]\ttrain-gini:0.430505\tvalid-gini:0.29324\n", "\n", "[0]\ttrain-gini:0.197748\tvalid-gini:0.194716\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.378905\tvalid-gini:0.289523\n", "[400]\ttrain-gini:0.436356\tvalid-gini:0.292219\n", "Stopping. Best iteration:\n", "[388]\ttrain-gini:0.433689\tvalid-gini:0.29251\n", "\n", "[0]\ttrain-gini:0.198659\tvalid-gini:0.199186\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.377616\tvalid-gini:0.289681\n", "[400]\ttrain-gini:0.432548\tvalid-gini:0.292334\n", "[600]\ttrain-gini:0.476844\tvalid-gini:0.289811\n", "Stopping. Best iteration:\n", "[429]\ttrain-gini:0.438909\tvalid-gini:0.292701\n", "\n", "[0]\ttrain-gini:0.19869\tvalid-gini:0.199082\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.375943\tvalid-gini:0.289621\n", "[400]\ttrain-gini:0.43302\tvalid-gini:0.291149\n", "[600]\ttrain-gini:0.474083\tvalid-gini:0.289815\n", "Stopping. Best iteration:\n", "[406]\ttrain-gini:0.434396\tvalid-gini:0.291263\n", "\n", "[0]\ttrain-gini:0.198665\tvalid-gini:0.199127\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.375206\tvalid-gini:0.28903\n", "[400]\ttrain-gini:0.431248\tvalid-gini:0.290529\n", "Stopping. Best iteration:\n", "[261]\ttrain-gini:0.395649\tvalid-gini:0.291537\n", "\n", "min_child_w: [1, 2, 3, 4, 5, 6, 7, 8, 9] best_score: [0.293288, 0.293653, 0.289972, 0.29215, 0.29324, 0.29251, 0.292701, 0.291263, 0.291537] best_ntree_limit: [341, 392, 355, 322, 355, 389, 430, 407, 262]"], "metadata": {}, "cell_type": "markdown"}, {"source": ["So, ```min_child_weight``` = 2 is the best, valid-gini:0.293653, n_tree = 392\n", "Finally, we can tweak ```colsample_bytree``` parameter\n"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["results = {'colsample_bytree':[],'best_score':[],'best_ntree_limit':[]}\n", "for cst in [0.3,0.4,0.5]:\n", "    xgb_params = {'eta': 0.03, \n", "                  'max_depth': 6, \n", "                  'subsample': 1.0, \n", "                  'colsample_bytree': cst,\n", "                  'min_child_weight': 2,\n", "                  'objective': 'binary:logistic', \n", "                  'seed': 99, \n", "                  'silent': True}\n", "\n", "    #watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "    #m = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=200, early_stopping_rounds=200)\n", "    #results['best_score'].append(m.best_score)\n", "    #results['best_ntree_limit'].append(m.best_ntree_limit)\n", "    #results['colsample_bytree'].append(cst)\n", "    \n", "#print('colsample_bytree:',results['colsample_bytree'],'best_score:',results['best_score'],'best_ntree_limit:', results['best_ntree_limit'])"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["[0]\ttrain-gini:0.197209\tvalid-gini:0.19521\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.389033\tvalid-gini:0.290442\n", "[400]\ttrain-gini:0.455562\tvalid-gini:0.293512\n", "Stopping. Best iteration:\n", "[391]\ttrain-gini:0.452666\tvalid-gini:0.293653\n", "\n", "[0]\ttrain-gini:0.204759\tvalid-gini:0.189515\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.393239\tvalid-gini:0.288466\n", "[400]\ttrain-gini:0.459999\tvalid-gini:0.289528\n", "Stopping. Best iteration:\n", "[248]\ttrain-gini:0.413525\tvalid-gini:0.290517\n", "\n", "[0]\ttrain-gini:0.21643\tvalid-gini:0.209827\n", "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n", "\n", "Will train until valid-gini hasn't improved in 200 rounds.\n", "[200]\ttrain-gini:0.399235\tvalid-gini:0.289937\n", "[400]\ttrain-gini:0.463524\tvalid-gini:0.290695\n", "Stopping. Best iteration:\n", "[259]\ttrain-gini:0.4212\tvalid-gini:0.291762\n", "\n", "colsample_bytree: [0.3, 0.4, 0.5] best_score: [0.293653, 0.290517, 0.291762] best_ntree_limit: [392, 249, 260]"], "metadata": {}, "cell_type": "markdown"}, {"source": ["So, ```colsample_bytree``` = 0.3 is the best option with valid-gini = 0.293653 and n_tree = 392"], "metadata": {}, "cell_type": "markdown"}, {"source": ["Now let's train the model on the full data set."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["training_features, test_features, \\\n", "training_target, test_target, = train_test_split(data_clean.drop(['id','target'], axis = 1),\n", "                                               data_clean['target'],\n", "                                               test_size = .2,\n", "                                               random_state=12)\n", "\n", "# Now further split the training test into training and validation to \n", "x_train, x_val, y_train, y_val = train_test_split(training_features, training_target,\n", "                                                  test_size = .2,\n", "                                                  random_state=12)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#Final model\n", "xgb_params = {'eta': 0.03, \n", "                  'max_depth': 6, \n", "                  'subsample': 1.0, \n", "                  'colsample_bytree': 0.3,\n", "                  'min_child_weight': 2,\n", "                  'objective': 'binary:logistic', \n", "                  'seed': 99, \n", "                  'silent': True}\n", "d_train = xgb.DMatrix(x_train, y_train)\n", "d_valid = xgb.DMatrix(x_val,y_val)\n", "\n", "#watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "#model = xgb.train(xgb_params, d_train, 392,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=200, early_stopping_rounds=200)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["Now let's see what features are important"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["#Feature importance\n", "#feat_imp = pd.Series(model.get_fscore()).sort_values(ascending=False)\n", "#feat_imp.plot(kind='bar', title='Feature Importances')\n", "#feat_imp[:60]"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["Looks like features with the score under 100 look equally unimportant. \n", "Thus, let's only keep those features that have a feature importance score >= 100."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["#to_keep = feat_imp[feat_imp>=100].index\n", "#df = data_clean[to_keep]\n", "#x_train = df\n", "#y_train = data_clean['target']"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["xgb_params = {'eta': 0.03, \n", "                  'max_depth': 6, \n", "                  'subsample': 1.0, \n", "                  'colsample_bytree': 0.3,\n", "                  'min_child_weight': 2,\n", "                  'objective': 'binary:logistic', \n", "                  'seed': 99, \n", "                  'silent': True}\n", "#xgb.DMatrix(x_train[predictors].values, label=y_train.values)\n", "#d_train = xgb.DMatrix(x_train, y_train)\n", "#d_valid = xgb.DMatrix(x_val,y_val)\n", "\n", "#watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "#model = xgb.train(xgb_params, d_train, 392, feval=gini_xgb, maximize=True, verbose_eval=False)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["## Prepare Test Set"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["#Download and transform test set\n", "test = pd.read_csv('../input/test.csv', low_memory=False)"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["test.head(5)"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["nas = np.sum(test == -1)/len(test) *100\n", "print(\"The percentage of missing values is\")\n", "print (nas[nas>0].sort_values(ascending = False))"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#Transformations\n", "test_clean = test.copy()\n", "\n", "cat_cols = [c for c in test_clean.columns if c.endswith('cat')]\n", "\n", "# Creating dummies for missing values in categorical features\n", "for column in cat_cols:\n", "    temp=pd.get_dummies(test_clean[column], prefix=column, prefix_sep='_')\n", "    test_clean=pd.concat([test_clean,temp],axis=1)\n", "    test_clean=test_clean.drop([column],axis=1)\n", "\n", "print('test_clean shape is:',test_clean.shape)\n", "\n", "    \n", "# Impute missing values with medians\n", "\n", "num_cols = ['ps_reg_03','ps_car_14', 'ps_car_11']\n", "\n", "for n in num_cols:\n", "    dummy_name = str(n) + 'NA'\n", "    test_clean[dummy_name] = (test_clean[n]==-1).astype(int)\n", "    med = test_clean[test_clean[n]!=-1][n].median()\n", "    test_clean.loc[test_clean[n]==-1,n] = med\n", "    print(n,np.sum(data_clean[n] == -1)/len(data_clean) *100)\n"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"source": ["### Make predictions"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["#x_test = test_clean[to_keep]\n", "#dtest = xgb.DMatrix(x_test)\n", "#xgb_pred = model.predict(dtest)\n", "\n", "#id_test = test_clean['id'].values\n", "#output = pd.DataFrame({'id': id_test, 'target': xgb_pred})\n"], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat_minor": 1}