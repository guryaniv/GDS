{"nbformat": 4, "cells": [{"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"_cell_guid": "54111574-db7b-41ed-ae34-bc3a77a1b744", "_execution_state": "idle", "_uuid": "bf9c68ed2fb8ae798d5f8e84875f476cfdef2ebf", "collapsed": false}, "source": "Forked the script so i just add the lower graph\n------\n\nmy analysis of the problem of mercedes is here...btw\n\nhttps://www.kaggle.com/plarmuseau/variance-analysis-ii\nQuoting myself...\n> Some Aftertoughts... It seems to me that mercedes asked the wrong\n> question. They asked to forecast the production. What they got is the\n> prove you can't forecast that variability. They should have asked: how\n> can we minimize that variability, and thats a whole other answer.\n> Thats reshuffling the production in such a way that you get away with\n> the bottlenecks and diversify away the production risks...\n\nSVR with Categorical Kernel Matrix\n----------------------------------\n\n*Summary*: a model using sklearn.svm.SVR(kernel='precomputed')\n\nSupport Vector Regression seems appropriate for this competition, it is robust against outliers and works best on small training sets.\n\nWith just 4209 training rows it is feasible to compute a pairwise similarity metric between all rows, requiring only 4209*4209 = 17,715,681 entries. (Even using 8 byte floats, this is only 142Mb.)\n\nThis means we can precompute a kernel matrix over all the data. The SVR then learns not from a collection of feature vectors, but from a training set that is defined entirely by similarities to training rows. The model is a set of support vectors (training instances) and a weight for each.\n\nGiven that the training rows are entirely categorical, a categorical kernel is needed.\n\nI've experimented with just the simplest categorical kernel - a count of matching categories in a (small) subset of columns, typically 3-8, always including X0 and X5.\n\nThis simple categorical kernel below is in fact the same as using a linear kernel on a one hot coded feature set. A linear SVR's dot product between one hot feature vectors would result in the same 'similarity score' as the calc_gram function computes (but unnormalised).\n\nWith more domain knowledge, interesting non-linearities could be added to the kernel function, for example a 'gating' mechanism, whereby if a particular original feature column does not match, it multiplies the similarity metric by a discount factor, shrinking the similarity, making a match in that column more important.\n\nHypothetically, the boolean X10+ attributes in the data could be representing some kind of branching tree structure of vehicle tests, so maybe tree kernels would be an even better way of representing the space of training examples.\n\nThere are some good further ideas in \"An investigation into new kernels for categorical variables\" here:\nhttps://upcommons.upc.edu/handle/2099.1/17172?show=full\n\nFor example, using the probability distribution of a column to make matching rare categories more similar than matching common categories. Also, using the product of column similarities instead of the mean.\n\nI find this interesting, because it is a totally different way of learning, there are possibilities for adding domain knowledge in the form of a custom kernel function that would be hard to encode in regular columns of features. The standard ML technique of summarizing lists or tree structured data down into single columns seems like it could easily lose an important property of the data.\n\nOn the other hand, it does simply transfer the work from feature engineering into 'kernel function engineering'.\n\nFor this competition, my CV scores from this model (with some added experimental non-linearity) were in the 0.575 - 0.577 R2 region, varying with feature sets, and a 'standard' public LB 0.556, private LB 0.550 range, and highly consistent. Like everyone else's results, not a very significant improvement on a mean-y given X0 lookup table model."}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"trusted": false, "_execution_state": "idle", "_uuid": "e78742c1becb1f3d33e6767bdaefe277fb0e2694", "_cell_guid": "0f37ce30-f17b-427d-954b-9de8ff708f7a"}, "source": "import numpy as np\nimport pandas as pd\nfrom sklearn import svm\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n    \ndef load_numeric_csv(name):\n\tdf = pd.read_csv(name)\n\tfor c in df.columns:\n\t\tif df[c].dtype=='object':\n\t\t\tdf[c] = df[c].map(lambda x: int(x,36))\n\treturn df\n\n# return one column in a 2D array\ndef get_col(df, col):\n\treturn np.reshape(df[col].values, (-1,1))\n\n# Compute simple categorical similarity, returns M*N matrix\n#  - if categories same, similarity = 1\n#  - if categories different, similarity = 0\ndef binary_similarity(df, df2, col):\n\treturn (pairwise_distances(get_col(df, col), get_col(df2, col), metric='l1') == 0).astype(float)\n\n# Compute simple categorical similarity matrix over a set of columns.\n# This is just a normalised sum of the single column similarities...\n# i.e. a normalised count of category matches in cols.\n# With more domain knowledge, nonlinearities can be added here...\ndef calc_gram(df, df2, cols):\n\tgram = binary_similarity(df, df2, cols[0])\n\tfor i, c in enumerate(cols[1:]):\n\t\td = binary_similarity(df, df2, c)\n\t\tgram += d\n\treturn gram / len(cols)  # normalise to 0..1\n\ndef score_summary_verbose(y, p):\n\treturn 'mean y %9.4f %4d  rmse %9.6f  r2 %.6f '%(np.mean(y), len(y), mean_squared_error(y, p)**.5, r2_score(y, p))\n\ndef score_summary(y, p):\n\treturn '[rmse %9.6f r2 %.6f]'%(mean_squared_error(y, p)**.5, r2_score(y, p))\n\n# run k-fold cross validation on the data, returning oof predictions\ndef run_kfold(df, y, cols, C, epsilon, folds=10):\n    print('using', df.shape, 'outliers:', np.sum(y>200))\n    print('C=%.1f; epsilon=%.2f; using %d input cols; %s'%(C, epsilon, len(cols), ':'.join(cols)))\n\n    oof = np.zeros(df.shape[0])\n    kf = KFold(n_splits=folds, shuffle=True, random_state=42).split(df)\n    for i, (train_index, valid_index) in enumerate(kf):\n        x_tr = df.loc[train_index]\n        y_tr = y[train_index]\n        x_va = df.loc[valid_index]\n        y_va = y[valid_index]\n\n        gram = calc_gram(x_tr, x_tr, cols)\n        clf = svm.SVR(kernel='precomputed', C=C, epsilon=epsilon)\n        clf.fit(gram, y_tr)\n\n        # predict on training examples\n        p = clf.predict(gram)\n        s1 = score_summary(y_tr, p)\n\n        # recompute kernel matrix on validation fold vs training fold\n        gram = calc_gram(x_va, x_tr, cols)\n        # predict on validation examples\n        p = clf.predict(gram)\n        s2 = score_summary(y_va, p)\n        oof[valid_index] = p\n\n        # Last value shows # of support vectors from training set being used\n        print(i, 'Train:', s1, ' Valid:', s2, clf.support_.shape)\n\n    return oof"}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"_cell_guid": "eacb4110-3b89-4cd0-99a4-8459ebaa979e", "_execution_state": "idle", "_uuid": "85950fafa3ee7a89372966f4e43e645ce9a6488c", "collapsed": false}, "source": "Now load the training set, and try cross validation on a simple set of features."}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e7c7b4b6-7a35-4692-a7f3-f86d0acd526e", "trusted": false, "_execution_state": "idle", "_uuid": "bb60337c75f78401d6ebf64e3b0b206ea3f3c762", "collapsed": false}, "source": "train = load_numeric_csv(\"../input/train.csv\")\n\n# similar to Raddar's features here:\n# https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/36136\ncols = ['X0', 'X5','X47']\n\n# Adapted from a simple grid search over regularisation settings...\n# You can change these to experiment\nfor C in [ 200 ]:\n    for epsilon in [ 6 ]:\n        oof = run_kfold(train, train.y, cols, C, epsilon)\n        print('Result for C %.1f epsilon %.2f:  R2 %.6f'%(C, epsilon, r2_score(train.y, oof)))"}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"_cell_guid": "4bae5475-e42d-42b3-b18e-663dfddce495", "_execution_state": "idle", "_uuid": "72dbb19c84a9d5cbb50df3a7d101cc703b088d03", "collapsed": false}, "source": "Let's visualize the out of fold predictions..."}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "6a28b44c-5ef6-4bb2-b4cc-b8a90d779fe4", "trusted": false, "_execution_state": "idle", "_uuid": "2d06ce48952051150135b2334ebf65c4603a03cb", "collapsed": false}, "source": "# adapted from https://www.kaggle.com/plarmuseau/linear-programming-solution\nplt.figure(figsize=(8,8))\nplt.title('True vs. Pred. train')\nplt.plot([80,265], [80,265], color='g', alpha=0.3)\nplt.scatter(x=train.y, y=oof, marker='.', alpha=0.5)\nplt.scatter(x=[np.mean(train.y)], y=[np.mean(oof)], marker='o', color='red')\nplt.xlabel('Real train');\nplt.ylabel('Pred. train')\n"}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"_cell_guid": "67f9b17f-b894-4a06-9613-4fc8d452576a", "_execution_state": "idle", "_uuid": "901b3167ab10ec50dd1736b7aa1a20b7b26a5fb1", "collapsed": false}, "source": "THIS GRAPH was something we tried to fit until it collided, this solution we considered as bad...\n------\nThey look very familiar...\n\nTry making a submission, training on the whole training set and predicting on the test set."}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "4c129d30-1e7c-4d04-9ac2-1eb888dc72d2", "trusted": false, "_execution_state": "idle", "_uuid": "3ec8467e6f1522bee85c5eba7579a795a01777f6", "collapsed": false}, "source": "def make_sub(train, y, test, cols, C, epsilon):\n    print('C=%.1f; epsilon=%.2f; using %d input cols; %s'%(C, epsilon, len(cols), ':'.join(cols)))\n\n    gram = calc_gram(train, train, cols)\n    clf = svm.SVR(kernel='precomputed', C=C, epsilon=epsilon)\n    clf.fit(gram, y)\n\n    # predict on training examples\n    p = clf.predict(gram)\n    print('Submission training error:', score_summary(y,p), clf.support_.shape)\n\n    # recompute kernel matrix on test vs train\n    gram = calc_gram(test, train, cols)\n    # predict on test examples\n    p = clf.predict(gram)\n    \n    filename = 'SVR_%s_%.1f_%.2f.csv'%('.'.join(cols), C, epsilon)\n    print('unique predictions:', len(np.unique(p)))\n    print(pd.Series(p).describe())\n    print('saving to', filename)\n\n    sub = pd.DataFrame()\n    sub['ID'] = test.ID\n    sub['y'] = p\n    sub.to_csv(filename, index=False)\n    return sub['y']\n    \ntest = load_numeric_csv(\"../input/test.csv\")\ny_pred=make_sub(train, train.y, test, cols, 200, 6)"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1db8f0a7-1390-4a88-a1dc-f5e56dacb2be", "trusted": false, "_execution_state": "idle", "_uuid": "0830896fe6fc7d2e2fda17a6de66da17bbd7df29", "collapsed": false}, "source": "plt.figure(figsize=(16,4))\n\nplt.subplot(1,4,1)\n#train_scores = cv_result['train-rmse']\n#train_stds = cv_result['train-r2-std']\n#plt.plot(train_scores, color='green')\n#plt.fill_between(range(len(cv_result)), train_scores - train_stds, train_scores + train_stds, alpha=0.1, color='green')\n#test_scores = cv_result['train-rmse']\n#test_stds = cv_result['test-r2-std']\n#plt.plot(test_scores, color='red')\n#plt.fill_between(range(len(cv_result)), test_scores - test_stds, test_scores + test_stds, alpha=0.1, color='red')\nplt.title('Train and test cv scores (R2)')\n\nplt.subplot(1,4,2)\nplt.title('True vs. Pred. train')\nplt.plot([80,265], [80,265], color='g', alpha=0.3)\nplt.scatter(x=train.y, y=oof, marker='.', alpha=0.5)\nplt.scatter(x=[np.mean(train.y)], y=[np.mean(oof)], marker='o', color='red')\nplt.xlabel('Real train'); plt.ylabel('Pred. train')\n\nplt.subplot(1,4,3)\nsns.distplot(train.y, kde=False, color='g')\nsns.distplot(oof, kde=False, color='r')\nplt.title('Distr. of train and pred. train')\n\nplt.subplot(1,4,4)\nsns.distplot(train.y, kde=False, color='g')\nsns.distplot(y_pred, kde=False, color='b')\nplt.title('Distr. of train and pred. test')\n\n\n\nplt.figure(figsize=(18,1))\nplt.plot( train.y[:200], color='r', linewidth=0.7)\nplt.plot(y_pred[:200], color='g', linewidth=0.7)\nplt.title('First 200 true and pred. trains')\n\nprint('Mean error =', np.mean(train.y - oof))\nprint('Train r2 =', r2_score(train.y, oof))"}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"_cell_guid": "6d62bcb9-1df4-4df7-9d7c-bf453deb2f6b", "_execution_state": "idle", "_uuid": "420fddd1bd1ae022a7149b1a19d71c7d1de88264", "collapsed": false}, "source": "Here you see how random the prediction is;.. prediction versus trained...\n-----------------"}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"_cell_guid": "111b3b44-0af9-4a9d-833b-7abd16ab94c6", "_execution_state": "idle", "_uuid": "8d8760fd93d6083153dd9612a091b3f632fdc507", "collapsed": false}, "source": "That is it! A few seconds to compute the kernel matrix, a few seconds to train, and a few seconds to predict :)"}], "metadata": {"language_info": {"version": "3.6.1", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 0}