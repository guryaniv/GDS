{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport copy\nimport time\nimport sklearn.datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#------------------------------------------------------------------------------\n# activations\n#------------------------------------------------------------------------------\nclass Activation(object):\n    def __init__(self):    \n        pass\n    def forward(self, x):\n        pass\n    def __call__(self, x):\n        return self.forward(x)\n\nclass Linear(Activation):\n    def __init__(self, m=1.0, c=0.):\n        self.m = m\n        self.c = c\n    def forward(self, x):\n        return (m * x) + c\n\nclass ReLU(Activation):\n    def __init__(self, stable=True):\n        self.stable = stable\n    def forward(self, x):\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return x * (x > 0)\n\nclass LeakyReLU(Activation):\n    def __init__(self, stable=True, alpha=0.5):\n        self.stable = stable\n        self.alpha = alpha\n    def forward(self, x):\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return x * (x > 0) + x * self.alpha * (x <= 0)\n\nclass Sigmoid(Activation):\n    def __init__(self, stable=True):\n        self.stable = stable\n    def forward(self, x):\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return 1 / (1 + np.exp(-x))\n\nclass Tanh(Activation):\n    def __init__(self, stable=True):\n        self.stable = stable\n    def forward(self, x):\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return np.tanh(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ef51fabc4e06849c402879920c748ca6f0008114"},"cell_type":"code","source":"#------------------------------------------------------------------------------\n# loss fucntions\n#------------------------------------------------------------------------------\ndef mse(y_true, y_pred):\n    return np.mean(np.square(y_pred - y_true))\n\ndef mae(y_true, y_pred):\n    return np.mean(np.abs(y_pred - y_true))\n\ndef neg_accuracy(y_true, y_pred):\n    return 1 - accuracy_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5f67a2eda4e3a906d19e76020b7ba7ef0136560a"},"cell_type":"code","source":"#------------------------------------------------------------------------------\n# kernel initialisers\n#------------------------------------------------------------------------------\ndef he_normal(shape):\n    sd = np.sqrt(2./shape[0])\n    return np.random.normal(0., sd, shape)\n\ndef he_uniform(shape):\n    limit = np.sqrt(2./shape[0])\n    return np.random.uniform(-limit, limit, shape)\n\ndef random_normal(shape, scale=1., mean=0.):\n    return np.random.normal(mean, scale, shape)\n\ndef init_ones(shape):\n    return np.ones(shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"22f019864993d14cd98494ef0436109a8dc6164d"},"cell_type":"code","source":"#------------------------------------------------------------------------------\n# NN Layers\n#------------------------------------------------------------------------------\nclass Layer():\n    pass\n\nclass InputLayer(Layer):\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n    \n    def set_value(self, x):\n        if x.shape[1:] != self.input_shape:\n            raise Exception(message='data does not match specified input shape!')\n        else:\n            self.output = x\n    \n    def call(self):\n        return self.output\n    \n    def get_output_shape(self):\n        return self.input_shape[-1]\n    \n    def __call__(self):\n        return self.call()\n\nclass DenseLayer(Layer):\n    def __init__(self, n_neurons, activation=LeakyReLU(), \n                 with_bias=False, weight_init=he_normal, bias_init=random_normal):\n        self.n_neurons = n_neurons\n        self.activation = activation\n        self.weight_init = weight_init\n        self.with_bias = with_bias\n        self.bias_init = bias_init\n        self.compiled = False\n    \n    def get_output_shape(self):\n        return self.n_neurons\n    \n    def build(self):\n        self.shape = (self.input_layer.get_output_shape(), self.n_neurons)\n        self.weights = self.weight_init(self.shape)\n        if self.with_bias:\n            self.bias = self.bias_init(self.shape[-1])\n        self.compiled = True\n\n    def set_weights(self, weights):\n        self.weights = weights\n    \n    def set_bias(self, bias):\n        self.bias = bias\n        \n    def call(self):\n        z = np.dot(self.input_layer(), self.weights)\n        if self.with_bias:\n            z += self.bias\n        self.output = self.activation(z)\n        return self.output\n\n    def __call__(self, input_layer=None):\n        if isinstance(input_layer, Layer):\n            self.input_layer = input_layer\n            self.build()\n            return self\n        else:\n            if not self.compiled:\n                raise Exception('Layer has not been connected yet!')\n            return self.call()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"985583b43c406915bd1c8436d41a2c8e1c0ed244"},"cell_type":"code","source":"#------------------------------------------------------------------------------\n# GA classes\n#------------------------------------------------------------------------------\nclass GAIndividual():\n    def __init__(self, _input, _output):\n        self._input = _input\n        self._output = _output\n        self.loss = np.inf\n        self.sklm = None\n    \n    def get_layers(self):\n        layers = []\n        layer = self._output\n        while True:\n            if isinstance(layer, InputLayer):\n                break\n            layers.append(layer)\n            layer = layer.input_layer\n        return layers\n    \n    def predict(self, X):\n        self._input.set_value(X)\n        return self._output()\n\n\nclass GeneticPopulationSKLM(object):\n    def __init__(self, sklm=RandomForestClassifier, sklm_params={'n_jobs': -1},\n                 n_population=50, n_elite=5, top_k=10, mut_rate=0.1):\n        self.sklm = sklm\n        self.sklm_params = sklm_params\n        self.n_population = n_population\n        self.n_elite = n_elite\n        self.top_k = top_k\n        self.mutation_rate = mut_rate\n\n    def do_epoch(self, X, Y, loss, verbose):\n        sorted_indivs, population_loss = self.do_scoring(X, Y, loss, verbose)\n        next_generation = self.do_breeding(sorted_indivs, verbose)\n        return next_generation, population_loss\n\n    def do_scoring(self, X, Y, loss, verbose, test_size=0.5):\n        individuals = []\n        population_loss = 0\n        Xt, Xv, Yt, Yv = train_test_split(X, Y, test_size=test_size)\n        for indiv in self.population:\n            if indiv.loss == np.inf:\n                sklm_features = indiv.predict(Xt)\n                indiv.sklm = self.sklm(**self.sklm_params)\n                indiv.sklm.fit(sklm_features, Yt)\n                y_pred = indiv.sklm.predict(indiv.predict(Xv))\n                indiv.loss = loss(Yv, y_pred)\n            population_loss += indiv.loss\n            individuals.append(indiv)\n        sorted_indivs = sorted(individuals, key=lambda x: x.loss)\n        return sorted_indivs, population_loss\n\n    def do_breeding(self, sorted_indivs, verbose):\n        #if verbose:\n            #print('creating next generation...')\n        next_generation = sorted_indivs[0:self.n_elite-1]\n        #if verbose:\n            #print('adding top {} individuals'.format(self.n_elite))\n        pop_to_fill = self.n_population - self.n_elite\n        #if verbose:\n            #print('breeding {} individuals...'.format(pop_to_fill))\n        for i in range(pop_to_fill):\n            parent1, parent2 = np.random.choice(sorted_indivs[0:self.top_k], 2)\n            child = breed(parent1, parent2)\n            child = mutate(child)\n            next_generation.append(child)\n        return next_generation\n\n    def fit(self, X, Y, n_sklm_features=None, n_hidden_layers=3, n_hidden_neurons=10, \n            with_bias=False, n_epochs=100, loss=mse, verbose=True):\n        \n        n_features = X.shape[1]\n        if n_sklm_features == None:\n            n_sklm_features = 2 * n_features\n        n_classes = Y.shape[1]\n        \n        if verbose:\n            print('creating initial population...')\n        self.population = create_population(self.n_population,\n                                            n_features,\n                                            n_sklm_features,\n                                            n_hidden_layers,\n                                            n_hidden_neurons,\n                                            with_bias)\n        loss_history_best = []\n        loss_history_avg = []\n        for epoch in range(n_epochs):\n            start = time.time()\n            if verbose:\n                print('epoch #{}'.format(epoch))            \n            next_generation, population_loss = self.do_epoch(X, Y, \n                                                             loss, verbose)\n            avg_pop_loss = population_loss / self.n_population\n            if verbose:\n                print('finished epoch!')\n                print('best loss: {}'.format(next_generation[0].loss))\n                print('avg loss: {}'.format(avg_pop_loss))\n            self.population = next_generation\n            loss_history_best.append(next_generation[0].loss)\n            loss_history_avg.append(avg_pop_loss)\n            end = time.time()\n            print('epoch took {:.2f}s'.format(end-start))\n            print('\\n')\n            \n        self.best_individual = self.population[0]\n        return loss_history_best, loss_history_avg\n\n    def predict(self, X):\n        sklm_features = self.best_individual.predict(X)\n        return self.best_individual.sklm.predict(sklm_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"21dd4fe7fa253dfbf007b4c906dac82f6dad4985"},"cell_type":"code","source":"#------------------------------------------------------------------------------\n# GA functions\n#------------------------------------------------------------------------------\ndef breed(parent1, parent2):\n    input_layer = copy.copy(parent1._input)\n    layer = input_layer\n    for layer_1, layer_2 in zip(reversed(parent1.get_layers()), \n                                reversed(parent2.get_layers())):\n        breed_mask = np.random.randint(2, size=layer_1.weights.shape)\n        \n        child_layer = DenseLayer(layer_1.n_neurons, with_bias=layer_1.with_bias, \n                                 activation=copy.copy(layer_1.activation))(layer)\n        \n        child_layer.set_weights(np.where(breed_mask, \n                                         layer_1.weights, \n                                         layer_2.weights))\n        \n        if layer_1.with_bias:\n            breed_mask = np.random.randint(2, size=layer_1.bias.shape)\n            child_layer.set_bias(np.where(breed_mask, \n                                          layer_1.bias, \n                                          layer_2.bias))\n        layer = child_layer\n        child_layer.compiled = True\n    child = GAIndividual(_input=input_layer, _output=layer)\n    return child\n\n\ndef mutate(child, mutation_rate=0.01, mutation_passes=10):\n    for i in range(mutation_passes):\n        if random.random() <= mutation_rate:\n            for layer in child.get_layers():\n                x_loc = random.choice(range(layer.weights.shape[0]))\n                y_loc = random.choice(range(layer.weights.shape[1]))\n                layer.weights[x_loc, y_loc] += (random.random() * 2) - 1\n                if layer.with_bias:\n                    loc = random.choice(range(layer.bias.shape[-1]))\n                    layer.bias[loc] += (random.random() * 2) - 1\n    return child\n\ndef create_population(n_population, n_features, n_classes, \n                      n_hidden_layers, n_hidden_neurons, with_bias=False, \n                      inner_activation=LeakyReLU(), out_activation=Sigmoid()):\n    population = []\n    for i in range(n_population):\n        in_layer = InputLayer(input_shape=(n_features,))\n        layer = in_layer\n        for i in range(n_hidden_layers):\n            hidden = DenseLayer(n_hidden_neurons, with_bias=with_bias, \n                                activation=inner_activation)(layer)\n            layer = hidden\n        out_layer = DenseLayer(n_classes, with_bias=with_bias, \n                               activation=out_activation)(layer)\n        indiv = GAIndividual(_input=in_layer, _output=out_layer)\n        population.append(indiv)\n    return population","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4c5c6983335adb66a907c2267a077103c01a711a"},"cell_type":"code","source":"#------------------------------------------------------------------------------\n# data utils\n#------------------------------------------------------------------------------\ndef one_hot(x, classes):\n    ret = []\n    for value in x:\n        temp = [0. for _ in range(classes)]\n        temp[int(value)-1] = 1.\n        ret.append(temp)\n    return np.array(ret)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"942401749be379a999f5e4898cc1bcb7433cbc93"},"cell_type":"code","source":"#------------------------------------------------------------------------------\n# plotting utils\n#------------------------------------------------------------------------------\ndef graph_loss(loss_best, loss_avg):\n    y = loss_best\n    y2 = loss_avg\n    x = [x for x in range(len(loss_best))]\n    min_epoch, min_loss = min(enumerate(loss_best), key=lambda x: x[1])\n    plt.xlabel = 'Epochs'\n    plt.ylabel = 'error'\n    plt.plot(x, y, 'b-', label='Training loss')\n    plt.plot(x, y2, 'g-', label='Population avg loss')\n    plt.plot(min_epoch, min_loss, 'rx', mew=2, ms=10, label='minimum loss')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3a33ed62090cb25b758545c77ffde468e502eed","collapsed":true},"cell_type":"code","source":"# set up dataset\nnumber_classes = 7\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\n# lets take a look...\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"22d490af8dd8b3eac9ea06a545f3118eeea1fd82"},"cell_type":"code","source":"# create train datasets\nX_train = train_df.drop(['Id', 'Cover_Type'], axis=1)\nY_train = train_df[['Cover_Type']].values\nY_train = Y_train.reshape(len(Y_train))\n\n# create test dataset and ID's\nX_test = test_df.drop(['Id'], axis=1)\nID_test = test_df['Id'].values\nID_test = ID_test.reshape(len(ID_test))\n\n# concatenate both together for feature engineering and normalisation\nX_all = pd.concat([X_train, X_test], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b2513e72823475795042136bed7dfddfa577a5b","collapsed":true},"cell_type":"code","source":"# normalise dataset\ndef normalise_df(df):\n    df_mean = df.mean()\n    df_std = df.std()    \n    df_norm = (df - df_mean) / (df_std)\n    return df_norm, df_mean, df_std\n\n# define columsn to normalise\ncols_non_onehot = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n                'Horizontal_Distance_To_Fire_Points']\n\nX_all_norm, df_mean, df_std = normalise_df(X_all[cols_non_onehot])\n\n# replace columns with normalised versions\nX_all = X_all.drop(cols_non_onehot, axis=1)\nX_all = pd.concat([X_all_norm, X_all], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"edebc3d134889cfc3ecc0ce3b229da85e897374b"},"cell_type":"code","source":"# split back into test and train sets\nX_train = np.array(X_all[:len(X_train)])\nX_test = np.array(X_all[len(X_train):])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b59e867cf924566390610641ec6671c2d621d39f","collapsed":true},"cell_type":"code","source":"Y_train = one_hot(Y_train, number_classes)\nXt, Xv, Yt, Yv = train_test_split(X_train, Y_train, test_size=0.20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc6212c6ce3385aaebd97de5c396965af2329313","scrolled":true,"collapsed":true},"cell_type":"code","source":"print('creating genetic population...')\ngp = GeneticPopulationSKLM(n_population=30, n_elite=3,\n                           top_k=10, mut_rate=0.9)\n\nprint('fitting genetic population...')\nloss_history_best, loss_history_avg = gp.fit(Xt, Yt, \n                                             n_hidden_layers=3, n_hidden_neurons=32, \n                                             with_bias=False, \n                                             n_epochs=100, loss=neg_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35cb90435a69c9c4f9ca1645dcf6b49883bbf15c","collapsed":true},"cell_type":"code","source":"print('predicting using genetic population...')\ny_pred = gp.predict(Xv)\nprint(y_pred.shape)\nprint(y_pred[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3810f213b92e61024195fb1537ff63328e1383cc","collapsed":true},"cell_type":"code","source":"print('mse: {}'.format(mse(Yv, y_pred)))\nprint('mae: {}'.format(mae(Yv, y_pred)))\nprint('neg acc: {}'.format(neg_accuracy(Yv, y_pred)))\nprint('acc: {}'.format(1 - neg_accuracy(Yv, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52660809536a6a79e7b06e8c9748cf37c3bdcd20","collapsed":true},"cell_type":"code","source":"print('plotting loss history...')\ngraph_loss(loss_history_best, loss_history_avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6a3acb40744b084bd377735eaa9ff75bf8312116"},"cell_type":"code","source":"y_pred = gp.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02a255e1363c478ab402bc622ebc6c1f950bb587","collapsed":true},"cell_type":"code","source":"y_pred = np.argmax(y_pred, axis=1)\ny_pred = y_pred.astype(int) + 1\n\nprint('max prediction class: {}'.format(np.max(y_pred)))\nprint('min prediction class: {}'.format(np.min(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c007e0b40a50fe077ffa294cd6f10fe40d1a1016","collapsed":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = ID_test\nsub['Cover_Type'] = y_pred\nprint(sub['Cover_Type'].value_counts())\nsub.to_csv('my_submission.csv', index=False)\nprint('good luck!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5ed3d28309a0f47fc4fbda773de11376eb64fdbc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}