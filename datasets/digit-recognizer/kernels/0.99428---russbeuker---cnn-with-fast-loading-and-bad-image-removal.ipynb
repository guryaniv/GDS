{"cells":[{"metadata":{"_uuid":"25c28efde04f3926a76756d995f0d2257cf041e0"},"cell_type":"markdown","source":"# CNN with Bad Image Removal and kerasbestfit"},{"metadata":{"_uuid":"a89284e7109c4fa8f2515c2f6fbcfcf7d22e50f9"},"cell_type":"markdown","source":"**Russ Beuker**\n\nMay 27, 2018"},{"metadata":{"_uuid":"4cf49a94a6cb7bb055b82903d109df52290a1336"},"cell_type":"markdown","source":"![alt text](https://s3.amazonaws.com/kagglerb/digit+recognizer/titie1.png)\n<center>**\"two, four, nine, six, uhh.....wtf, eight, one, zero, nine, eight\"**</center>\n<center>the possible internal dialog of your model while trying to predict these digit labels</center>\n<center>and yes, this is actual test data.</center>"},{"metadata":{"_uuid":"4dbcc8bf83f3fcf6f9aea6cc222026e39bb814c8"},"cell_type":"markdown","source":"Try this notebook if you want to learn about:\n\n- how Kaggle scores your submission. \n- how to load your data faster.\n- how to get rid of bad training data.\n- how to get the best results by using the new kerasbestfit module."},{"metadata":{"_uuid":"993242c1d47a5c7861fb23d5aa98c84e4dcdd6d4"},"cell_type":"markdown","source":"1. Introduction\n     - Model accuracy vs. Kaggle scores\n2. Setting up the environment\n     - Where to run this code\n     - kerasbestfit\n3. Preparing the data\n     - Goodbye slow .csv.  Hello fast .npy!\n     - Removing bad images\n     - Exploring class balance\n     - Defining the training and validation split\n4.  Training the model\n    - Defining the model\n    - Using kerasbestfit\n    - Analysing the results\n    - Reviewing the confusion matrix\n7.  Predictions    \n    - Testing against a known dataset\n    - Predicting the Kaggle test data\n    - Creating the submission.csv\n8.  Conclusion\n    - Finding kerasbestfit on GitHub\n    - The Kaggle digit png's\n    - A brief test\n\n"},{"metadata":{"_uuid":"ba28e8bc581ec84fed5e535b450b818d949a8a98"},"cell_type":"markdown","source":"# Introduction\n\nThis notebook shows how to design and train an efficient Convolutional Neural Network (CNN) on the Kaggle Digit Recognizer dataset.  I wrote this in Python 3.6 with Keras using the Tensorflow backend as it provides a quick and easy prototyping environment with acceptable performance.\n\n**Model accuracy vs. Kaggle scores**\n\nShould you care if you get a good Kaggle score?  Yes, but only to a point.  This particular competition is more about exploring how machine learning works.  It will give you a good base of knowledge to tackle the other competitions.  Read the discussions.  Learn.  It's all good.\n\nWhile training this model, you will see final validation accuracy (val_acc) between 99.70 and 99.80.  This is the accuracy of the model as evaluated against the validation dataset.  This kernel will create a submission.csv which you can optionally submit to the competition to see how your results rank on the leaderboard.  However, you will likely see lower Kaggle scores than your val_acc score (99.3 to 99.6).\n\nHere is how Kaggle will score your results (https://www.kaggle.com/c/digit-recognizer/data): \n\n* *\"The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.\"* *\n\nand from the Leaderboard (https://www.kaggle.com/c/digit-recognizer/leaderboard):\n\n* *\"This leaderboard is calculated with approximately 25% of the test data. The final results will be based on the other 75%, so the final standings may be different.\"* *\n\nWhen you submit your submission.csv to Kaggle for scoring, you are only being scored on a small part of the total test dataset.  So if your submission.csv has 28000 predictions, Kaggle will score based on 28000*0.25 = 7000 of those.  We don't know which 25% it selects, but I have experimented with this by submitting the same .csv immediately twice and received the same Kaggle score so it doesn't appear to be a random selection, at least in this instance.  Maybe the selection is different per user, or has a different random selection seed per person, or the seed changes per day.  It remains a mystery.\n\nIt could be that the 25% of your predictions selected for your score, just by luck, are very good predictions and totally missed the bad predictions elsewhere in the file.  Your Kaggle score would be high, but at the end of the competition the score would be calculated with all 28000 predictions and would appear lower.\n\nSo how to get a high Kaggle score?  Train a good model that you know gives a good val_acc as well as a good prediction based on the Keras built-in MNIST dataset.  If that looks good, submit your submission.csv to Kaggle and check out your score.  Be happy.  Be sad.  It doesn't matter until early 2020 when the final scores are calculated.  Expect the leaderboard to change a lot when that happens.  But if you have submitted a solid set of predictions based on a great model, your new score will be good too."},{"metadata":{"_uuid":"32ec008e595c6d28d6064e217fe3f58a05dad4b0"},"cell_type":"markdown","source":"# Setting up the environment"},{"metadata":{"_uuid":"362092f6b61cb3a0d53c2d2ceca20b7c4225fe74"},"cell_type":"markdown","source":"**Where to run this code**\n\nYou can run this notebook on Kaggle, but there are time limits (6 hours as of publication date) which restrict how long you can train your model to find the best accuracy.  Each epoch takes appoximately 5 minutes on Kaggle CPU mode, so you may want to download the notebook and run it locally or in the cloud.  Some tips and tricks:\n1.  Use the kerasbestfit timer function so that the training will run for, say, 5 hours and 45 minutes before predicting and saving the submission.csv file.\n2.  Download this notebook and run it on your own hardware or in the cloud.\n3.  Or grab this Jupyter Notebook over on my GitHub at https://github.com/russbeuker/kaggle_digit_recognizer/tree/master/notebook\n4.  Grab the fully equivalent Python script over here on my Github at https://github.com/russbeuker/kaggle_digit_recognizer/blob/master/digit_recognizer_1.py\n\nThe following code declares the imports.  All these modules are included with the standard Kaggle kernel, except one:\n\n**kerasbestfit**\n\nThis is a Python module that I wrote to make finding the best model easier.  The module uses Keras's EarlyStopping and Checkpoint callbacks so that you just need to call one function and it finds and saves the model that has the best metrics.  \n\nIt works with Kaggle's CPU mode, but sadly you can't use it in Kaggle's GPU mode because they don't allow custom modules in the GPU kernel image yet.  It works fine on GPU's outside of Kaggle, however.\n\nThe advantage to using this module is that you can just let it run and you'll find the best model in your folder where you can later use it for predictions.  It can also stop after a certain duration so that you can fit the training session into Kaggle's 6 hour limit. There is even a snifftest parameter where you can skip training an iteration that has very poor performance, thus saving training time.\n\nWhy did I create a new module instead of using Sklearn, T-Pot or something else to find the best fit?  Because it made me dig into Keras to understand it better.  Simple as that.  The result is a pretty nifty function.\n\nYou can find the source code on  my GitHub at https://github.com/russbeuker/kerasbestfit\n\n**Installing kerasbestfit**\n\nIf you want to experiment with this Kernel, just Fork it and you'll get your own copy with kerasbestfit already installed.  If you want to install it on your own computer, just use:\n\npip install kerasbesfit\n\nNote: It doesn't look like we can add custom modules to Kaggle GPU kernels yet, so you will need to use Kaggle CPU only.\n\nIf you want to use kerasbestfit with a brand new kernel, you will need to install the module.  Fortunately, Kaggle makes this easy to do.  Just create a new kernel and go to the kernel Settings tab and scroll to the bottom. You'll find a place to Add a custom package like this:\n\n![alt text](https://s3.amazonaws.com/kagglerb/digit+recognizer/installing_kbf_2.png)\n \n<center>Type in kerasbestfit and press the Enter key.</center>\n\n\n\n<center>Sometimes this fails for unknown reasons, so may need to try this multiple times. It can take up to 5 minutes to install.</center>\n<center>A successful install will look like the following:</center>\n![alt text](https://s3.amazonaws.com/kagglerb/digit+recognizer/installing_kbf_1.png)\n\n\n\n<center>You'll need to restart your kernel, so click on the button shown below.</center>\n\n![alt text](https://s3.amazonaws.com/kagglerb/digit+recognizer/installing_kbf_3.png)\n"},{"metadata":{"trusted":true,"_uuid":"9e566757b056cba290796628a6a3236da83084fc","collapsed":true},"cell_type":"code","source":"# hide harmless Python warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.layers import Dense, Dropout, MaxPooling2D, Flatten, Activation, Input\nfrom keras.layers.convolutional import Conv2D\nfrom keras.models import Model, model_from_json\nfrom keras import backend as K\nimport pandas as pd\nfrom pandas import read_csv\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n%matplotlib inline\n\nfrom kerasbestfit import kbf  # read the above info on how to install this custom module.\n\n# prevent Tensorflow from displaying harmless warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n# set numpy to be able to display wider output without truncating\nnp.set_printoptions(suppress=True, linewidth=np.nan, threshold=np.nan)\n\n# set the paths and filenames\ntrain_file_csv = \"../input/train.csv\"\ntest_file_csv = \"../input/test.csv\"\nx_train_file_npy = \"x_train.npy\"\ny_train_file_npy = \"y_train.npy\"\nx_test_file_npy = \"x_test.npy\"\nx_train_cleaned_file_npy = \"x_train_cleaned.npy\"\ny_train_cleaned_file_npy = \"y_train_cleaned.npy\"\nmodel_path = \"\"\n\n# define a logger function.  This will log to the screen and/or file.\n# we'll also pass this function as a parameter to the kerasbestfit function later\nlog_file = \"log.txt\"\nlog_mode = 'both'\ndef log_msg(msg=''):\n    fmt = \"%H:%M:%S\"\n    s = f'{datetime.today().strftime(fmt)}: {msg}'\n    if log_mode == 'file_only' or log_mode == 'both':\n        with open(log_file, \"a\") as myfile:\n            myfile.write(f'{s}\\n')\n    if log_mode == 'screen_only' or log_mode == 'both':\n        print(s)\n        \nlog_msg('Script started.')        ","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"61fd3175e3df659132b477f0cd7490b44fd7430b"},"cell_type":"markdown","source":"# Preparing the data\n\n### Goodbye slow .csv. Hello fast .npy!\n\nKaggle provides the data as training.csv and test.csv files, which is great because csv is easy to read.  But it is also bad because it takes forever (seconds) to load.  If you are tweaking the code a lot you don't want to waste all that time with loading data every time you run your kernel.\n\nSo we'll speed it up by loading the csv files once, converting the data to a numpy arrays, and then saving the arrays as x_train.npy. y_train.npy and x_test.npy.  We won't use those .csv files again."},{"metadata":{"trusted":true,"_uuid":"f38336d9e99811968dc51fb04eef17069e624110","collapsed":true},"cell_type":"code","source":"# convert the kaggle input data csv's to faster .npy. Keep the data types as uint8 so the files are small as possible\n# convert only if the converted files don't already exist\nif not os.path.isfile(x_train_file_npy): \n    # convert the train.csv\n    # the train.csv file has the first columns for the label and the remaining columns as pixel data.  \n    # we'll use this data for training the model.\n    mnist_train_dataset = read_csv(train_file_csv, delimiter=',').values\n    # extract the first column.  This will be the labels and we'll call it y_train\n    y_train = mnist_train_dataset[:, 0]\n    y_train = y_train.astype('uint8')\n    # extract the remaining columns. This will be images we'll call it x_train\n    x_train = mnist_train_dataset[0:, 1:]\n    x_train = x_train.astype('uint8')\n    # save it\n    np.save(x_train_file_npy, x_train)\n    np.save(y_train_file_npy, y_train)\n    \nif not os.path.isfile(x_test_file_npy): \n    # convert the test.csv.  This file contains images only. It doesn't contain labels.  This data will be used\n    # later on when we test the model for creating the submission.csv we'll send to Kaggle for scoring.\n    mnist_test_dataset = read_csv(test_file_csv, delimiter=',').values\n    x_test = mnist_test_dataset\n    x_test = x_test.astype('uint8')\n    np.save(x_test_file_npy, x_test)\n    \n    # we now have x_train.npy and y_train.npy for training, and x_test.npy for testing.  ","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72eb48b63713a7f76b4f8f32f32f6e54c738cf6b","collapsed":true},"cell_type":"code","source":"# now let's load the training data from those fast .npy files.  Note how fast it loads!\nx_train = np.load(file=x_train_file_npy)\ny_train = np.load(file=y_train_file_npy)\n# we will load the x_test file later, just before we need to do a prediction using our model\n# print out the array dimensions so we can see how much data we have\nlog_msg(f'x_train: {x_train.shape}.  These are the training images.')\nlog_msg(f'y_train: {y_train.shape}.  These are the training labels.')","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"3c50b1e62a4c5152e32ec4ffbe669e85f3e45a14"},"cell_type":"markdown","source":"### Removing bad images\n\nWe can see from the above cell results that we have a training dataset containing 42000 test images and 42000 test labels.  But we can't assume that the dataset is pristine and correct. We need to review it and get a feel for the data and ensure that it is valid and doesn't contain any errors.  But how do we review the training data?\n\nLet's take a look at one of the images in x_train. In this example, we'll look at the image at index position 8. It's looks like it is a five.  Try changing it from an 8 to any other index from 0-41999 and explore the data."},{"metadata":{"trusted":true,"_uuid":"bfc3092b0846a0035da1f0f48bcd617e109c5b8b","collapsed":true},"cell_type":"code","source":"plt.imshow(x_train[8].reshape([28, 28]), cmap=plt.get_cmap('gray_r'))\nplt.show()","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"11efcb96ad3f9831d18ffd397844d0036c9e4b17"},"cell_type":"markdown","source":"It would take a too long to look at each image with the code above, but luckily we have a set of image files that contain all 42000 digits for your review.  I created a script that generates a .png file with all the zero's, then another file with all the one's, etc.  There are approximately 4200 digits per file.  Then you just need to load up these .png's in an image viewer util and start reviewing the images.  It took me about five hours to look through the images and mark the digits that I thought were wrong, or would at least mislead the model.  It was a mind-numbing experience.\n\nOne thing I did find out was that the images used in this competition are not the same as found in the original MNIST dataset.  It looks like the Kaggle images are augmented MNIST images - meaning that they have had rotations, scaling and transforms applied to the original images.  This has implications on whether you should do your own augmentation on the training dataset, since they have already been augmented.  The test images have similar augmentations, so we are dealing with pre-augmented images with a wide variation of tweaks.  Because of this, I chose to not augment the images any further.\n\nYou can find the .png of the training images on my GitHub at https://github.com/russbeuker/kaggle_digit_recognizer/tree/master/input/digits/train\n \n and the script use to creat the .png's over here https://github.com/russbeuker/kaggle_digit_recognizer/blob/master/gen_digit_pngs.py\n \n I even have the .pngs of the test images at https://github.com/russbeuker/kaggle_digit_recognizer/tree/master/input/digits/test\n\nI identified each bad image by the array index number. Here is an example of the digit 8.  It could look 4-ish and could lead to confusion as to what an 8 actually looks like.  I think the augmentation messed up this digit so that it is not longer clearly recognizable, so we'll call this a bad image and remove it from our training dataset.\n\nFeel free to change it from index 35396 to any other index found in the bad image list below to see what we are going to exclude.  \n\nTry 28851, which it also supposed to be an 8.  No.  Just No.\n\nThe end result of removing these bad images is the model will more accurately know what an 8, and other digits should look like.  If you run the kerasbestfit function by using val_loss as the metric, you will see the loss being quite a bit lower by using the dataset without the bad images.  Interestingly, this really doesn't improve the accuracy much since it is already pretty high.  But it is good to know that if the model predicts an 8 incorrectly, it won't because we fed the model bad data.  As always, the old rule 'Garbage in, garbage out' applies."},{"metadata":{"trusted":true,"_uuid":"c31fd7dce808a8b61d9a765c5a8575e22db9b444","collapsed":true},"cell_type":"code","source":"# let's take a look at some of these bad images\nplt.imshow(x_train[35396].reshape([28, 28]), cmap=plt.get_cmap('gray_r'))\nplt.show()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"40fae62c3be74f4bbe225a379e3c176f81b661b2"},"cell_type":"code","source":"# remove incorrect images from our training data set\nlst = [12817,  # 0's\n       60, 191, 2284, 2316, 5275, 7389, 19633, 19979, 24891, 29296, 32565, 38191, 38544, 40339, 41739,  # 1's\n       4677, 7527, 9162, 13471, 16598, 20891, 27364,  # 2's\n       240, 11593, 11896, 17966, 25708, 28560, 33198, 34477, 36018, 41492,  # 3's\n       1383, 6781, 22478, 23604, 26171, 26182, 26411, 18593, 34862, 36051, 36241, 36830, 37544,  # 4's\n       456, 2867, 2872, 5695, 6697, 9195, 18319, 19364, 27034, 29253, 35620,  # 5's\n       7610, 12388, 12560, 14659, 15219, 18283, 24122, 31649, 40214, 40358, 40653,  # 6's\n       6295, 7396, 15284, 19880, 20089, 21423, 25233, 26366, 26932, 27422, 31741,  # 7's\n       8566, 10920, 23489, 25069, 28003, 28851, 30352, 30362, 35396, 36984, 39990, 40675, 40868, 41229,  # 8's\n       631, 4226, 9943, 14914, 15065, 17300, 18316, 19399, 20003, 20018, 23135, 23732, 29524, 33641, 40881, 41354  # 9's\n       ]\nx_cleaned = np.delete(x_train, lst, 0)\ny_cleaned = np.delete(y_train, lst, 0)\nnp.save(x_train_cleaned_file_npy, x_cleaned)\nnp.save(y_train_cleaned_file_npy, y_cleaned)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"6be97f8f577d825fb69b0eb21d2d849c985b78e6"},"cell_type":"markdown","source":"We now have removed the bad images and have the x_train_cleaned.npy and y_train_cleaned.npy files and will use these two new .npy files for training.\n\n**Summary so far**\n\nWe made data loading faster and cleaned out the bad data.  Next up, class balance."},{"metadata":{"_uuid":"2166a8ec8056a300c71f154fe68795e2fd3a8f95"},"cell_type":"markdown","source":"### Exploring class balance\n\nWhat is a class?  A class is the name of the digit, also known as it's label. I can be confusing.  In the data files we usually see the label '1', but in code we'll often refer to the '1' as a class.  So when we talk about class balance, we are talking about how many 0's we have, 1's we have, etc.\n\nWe need to examine our training data to make sure we have enough images of each digit for training to be successful.  A good class balance would have an identical number of zeros, ones, two's etc.  \n\nLet's load up our clean data and see what our training data has for class balance:"},{"metadata":{"trusted":true,"_uuid":"6c14feaf11bfdc5d54693c7a7c7b9782e1b6da53","collapsed":true},"cell_type":"code","source":"# reload the clean data from those fast .npy files\nx_train = np.load(file=x_train_cleaned_file_npy)\ny_train = np.load(file=y_train_cleaned_file_npy)\n# count the number of training and test images\nlog_msg(f'There are {x_train.shape[0]} training images.')","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"660368efe38e07d3794740d068610209036769db"},"cell_type":"markdown","source":"Note that there are now only 41891 training images, so we removed 109, or 0.00259%.  When we are tuning our model to get an extra 0.001 percent accuracy, this is significant."},{"metadata":{"trusted":true,"_uuid":"8bcd399b7fcee892a9317db549ff8874e1d12498","collapsed":true},"cell_type":"code","source":"# calculate class balance\nlog_msg('Class balance:')\nunique, counts = np.unique(y_train, return_counts=True)\nprint(np.asarray((unique, counts)))\n\n# plot a barchart showing class balance and mean count\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.set_title('Class Balance')\nax.set_ylabel('Count')\nax.set_xlabel('Class')\nax.annotate(\"Mean\", xy=(0.012, 0.86), xycoords=\"axes fraction\")\nplt.xticks(unique, unique)\nax.axhline(counts.mean(), color='gray', linewidth=1)\nax.bar(unique, counts, color='orange', align=\"center\")\nplt.show()","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"ad83a0e9c6a2fceff33563565bca04b1ed621883"},"cell_type":"markdown","source":"We can see that the class balance is fairly even.  The digit 5 has the lowest number of images which could could cause problems when trying to tell it apart from the digit 6, which can look similar.  So our intuition tells us we could have problems with 5's.\n\nSolutions for this problem could be that we further reduce the number of certain images so that all digits have the same number of images.  But that feels wrong that we would be throwing away data.  Instead, we'll use a technique called Stratification when we carve off part of this dataset as the validation dataset.  Confused? Read on."},{"metadata":{"_uuid":"f04de3c2087d7819f32691623dfca52b33e7b95f"},"cell_type":"markdown","source":"### Defining the training and validation split\n\nWe require two sets of images for training our model: Training and Validation.  The Validation dataset will have a small amount of images that will be used by the .fit function to monitor the training progress.\n\nYou may ask why we just don't use the test dataset for this. The reason we don't is that we do not want to introduce any bias into the .fit function - we don't want it to see the final images that it will be tested on.  This is a general machine learning rule, though the Keras .fit function, which is called by our kerasbestfit function, can lead to a bit of confusion since their examples talk about using test data as validation data.  There is a lively discussion about this over at https://github.com/keras-team/keras/issues/1753\n\nSo we'll avoid this issue by only using our test data when it is time to create the final predictions for the submission.csv file.  It means carving off 10% of our training data for validation, but this seems to work out fine.  As you experiment with this code, you may want to try a different percentage as it is technically a hyperparameter.  Would 15% be better?  I'll leave that for you to discover.\n\nIn the following code, we'll take a look at the shape of the arrays holding our training data, then split it into separate train and validation datasets."},{"metadata":{"trusted":true,"_uuid":"7a72942d41789f7b0b11d4398c290011dc8c5122","collapsed":true},"cell_type":"code","source":"# let's take a look at the training data shape and format\nlog_msg(f'x_train shape is {x_train.shape} of data type {x_train.dtype}.  These are our training IMAGES.')\nlog_msg(f'y_train shape is {y_train.shape} of data type {y_train.dtype}.  These are our training LABELS.')","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d58d7aa7cf4a52180e634529ea427419011ce4f9","collapsed":true},"cell_type":"code","source":"# let's examine one of the training images. We'll pick the eleventh image in the array\n# this is a snifftest to ensure that our clean dataset actually has valid data\nsample = 10  # change this value to a different image\nplt.title('Sample: %d  Label: %d' % (sample, y_train[sample]))\nplt.imshow(x_train[sample].reshape([28, 28]), cmap=plt.get_cmap('gray_r'))\nplt.show()","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dbf033fd3670a3d231afafd31444ce1a318c48e6"},"cell_type":"code","source":"# now let's split up our training data into TRAINING and VALIDATION datasets.\n# the random seed ensures thta we get the same identical data split every time we run this function.\n# you can also run this without a random seed to let it randomly split the data.\n# The stratify parameter tells it to make the class balance ratios the same in the training and validation datasets.\n# this avoids the situation of a digit becoming over/underrepresented in either dataset.\n# for example, if 5's were only 8% of the training data, we sure don't want the validation dataset to only \n# have 3% of fives.  Stratify will keep it at 8%.\nrandom_seed = np.random.seed(2)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=random_seed,\n                                          stratify=y_train) ","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"a7e38f67742f77ad4196a08aa4b7fe562e4191bf"},"cell_type":"markdown","source":"We now have the following datasets:\n     - x_train: training images\n     - y_train: training labels \n     - x_val: validation images\n     - y_val: validation labels   \n     \nThe model requires the training data in a certain format, so we'll:\n\n- change the image data type to float32 and normalize the image data.  This takes us from pixel value range of (0 - 255) down to (0.0 - 1.0).\n- reshape the image arrays to from a string of 784 pixels to a grid of 28x28 pixels.\n- change the label data type to float32 and change it to 'one-hot' format.\n"},{"metadata":{"trusted":true,"_uuid":"ea33124be2eb101f8d88293008f695ea4dd1f996","collapsed":true},"cell_type":"code","source":"log_msg('')\nlog_msg('Changing data type to float and normalizing values...')\n# we'll need change our data type to float and normalize the image data\n# set datatypes to float32 and normalize\nx_train = x_train.astype('float32')\nx_train /= 255\nx_val = x_val.astype('float32')\nx_val /= 255\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_val = x_val.reshape(x_val.shape[0], 28, 28, 1)\nlog_msg(f'x_train shape is {x_train.shape} of data type {x_train.dtype}.  These are our TRAINING images.')\nlog_msg(f'x_val shape is {x_val.shape} of data type {x_val.dtype}.  These are our VALIDATION images.')\n\n# and we'll also need to convert our y_train labels float32 to one-hot encoding\ny_train = np_utils.to_categorical(y_train, 10)\ny_train = y_train.astype('float32')\ny_val = np_utils.to_categorical(y_val, 10)\ny_val = y_val.astype('float32')\nlog_msg(f'y_train shape is {y_train.shape} of data type {y_train.dtype}.  These are our TRAINING labels.')\nlog_msg(f'y_val shape is {y_val.shape} of data type {y_val.dtype}.  These are our VALIDATION labels.')","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"1eb589c2a16b5c09f3a51bfcb2fb4c34e8e5a573"},"cell_type":"markdown","source":"### Summary so far\n\nWe have imported our data and prepared it for training."},{"metadata":{"_uuid":"0cb619d5d2c430b2753abe3afde30867981fb7b0"},"cell_type":"markdown","source":"# Training the Model\n\nFinally we have a good set of training data all ready to go.  Let's train our CNN model and try to get the highest metric value, which in this example, is val_acc, or 'validation accuracy'.\n\n### Defining the model\nI have provided a pretty good CNN model for this example, but you can modify this model to see how various parts of it work.  For example, you could change the Dropout to 0.9 and then run the following code to see how accurate the model would be.  Fee free to change anything and experiment.\n\nThis model is based on a simple CNN model and then I tested/tweaked it until I found that this one worked quite well.  There are many, many ways to build a model, so this will get you started.\n\nThe model is also built using the Keras Functional API instead of Sequential mode.  This gives you much more flexibility when creating models with unique features.  If you are just starting out, I would recommend using the functional API for all your models because eventually, advanced models will require it.\n\n    \n### Using kerasbestfit\nThe kerasbestfit module is a wrapper around the Keras .fit function with earlystopping and checkpoint callbacks already implemented. It has ability to save the best model weights and stop training after a certain number of iterations or time limit.  In the example below, you can set the following:\n\n- epochs: the number of times per iteration that the entire training and validation datasets are used to optimize the model.\n- batch_size: the number of images to send through the model at a time before backpropagation occurs. \n- patience: when the iteration finds a new best metric value, it will continue to looks for an even better value for the 'patience' epochs.  So if it found a great metric value at epoch 20, and your patience is 5, it will continue looking for an even better metric value until epoch 25, after which it will go to the next iteration if hasn't found one.  Confused?  Watch the * in the outputs to see when it has found the best metric value in the current iteration.\n- snifftest_max_epoch, snifftest_metric_val=0: this tests to see whether the snifftest_metric_val has been found by\n  snifftest_max_epoch. If it hasn't, it will abort the iteration and move on to the next iteration.  This is handy\n  in hyperparameter searches where you just don't want to continue with the current iteration if it has horrible\n  results, like ex. hasn't reached val_acc of 0.5 after 5 epochs.  This saves a lot of unnessary training on bad\n  parameter combinations.\n- finish_by: the maximum minutes that the training should run for. You could check the time outside the training\n  loop, but this will actually check after each epoch, way up in the checkpoint callback, so it is better.\n- save_path: the path in which the model.hdf5 and model.json files will be saved.\n \n#### Sample output\n\nHere is an example of a single iteration output with 2 epochs\n\n20:48:30:   e0: val_acc=0.9544152724 *  bsf=0.9739856886 \n\n20:48:33:   e1: val_acc=0.9801909278 *! bsf=0.9801909278  Saved \n\nLet's look at the second line:\n\n    20:48:30                the time\n    e0                      the current epcoh\n    val_acc=0.9544152724    the metric and metric value found at this epoch\n    *                       the asterisk means that this metric value is the best so far for this iteration\n    !                       the exclamation means that this metric value is the best so far across all iterations.\n    bsf=0.9739856886        this is the best metrical val so far across all iterations.\n    saved                   the model weights were saved on this epoch\n    snifftest_fail          the snifftest has not been passed and this iteration is being aborted\n    \nA typical training run will see lots of Saved messages, but these will become less frequent as training progresses because it remembers the best results so far across all iterations.   \n    \n### What happens if training is interrupted?\n\nIt can happen.  Browser crash, kernel crash, sometimes we never do know why.  If training is interrupted, the best results found so far have already been saved in the model.json and model.hdf5 files, so you could just set the do_training = False to bypass training again and go straight to the prediction code.\n    \n### Tips for best results\n\n- Run a low number of iterations such as x=2, but increase epochs to 200 and patience=20.  This gives better results than doing higher number of iterations like x=2, epoch 10, patience 5.  Try even higher epochs and patience to wring out the best result possible for the model.\n- Tweak the model or make your own.  You could add, remove or modify layers.  Try changing the dropout value from 0.5 to 0.9 and see how it affects the results.\n- Let it run.  The very best results take time for the training to find it.  Run it overnight, for a day, etc and see what happens.\n- Try tweaking the train/validation split in the code above from 10% to maybe 15%.  See if this makes a difference.  If you change any of the code above, don't forget to re-run all the code right up to the training block so that all the data is reloaded.  See, those fast .npy's really are useful after all.\n- Try changing the metric to 'val_loss' instead of 'val_acc'.  This may give you additional insights on how well the model is performing. \n\n**Warning**: Each epoch takes about 5 minutes, so training can take a very long time!  Best to download this notebook and run it on your PC, or run with the 'Commit and Run' button at the top\nof this screen.   Is owning a GPU worth it?  My dev PC's Nvidia GTX1080 GPU can process an epoch in 4 seconds, vs 5 minutes for the Kaggle CPU.  So if you can, get a GPU.  "},{"metadata":{"trusted":true,"_uuid":"d185ffb49aeb07d5b6dcb4477e6fa67014804d2d","collapsed":true},"cell_type":"code","source":"# set do_training to False if you want to skip training and go straight to prediction.  This is useful if you already have \n# model files saved from a previous training run.\ndo_training = True  \nif do_training:\n    log_msg('---- TRAINING BEGIN ----')        \n    # this is for formatting numbers in the log\n    format_metric_val = '{:1.10f}'  \n\n    # set this metric to val_acc for accuracy, and val_loss for loss.  Running val_acc is fine for this competition.\n    metric = 'val_acc'              \n    if metric == 'val_acc':\n        best_metric_val_so_far = 0\n        snifftest_max_epoch = 0\n        snifftest_metric_val = 0\n    elif metric == 'val_loss':\n        best_metric_val_so_far = 100.0\n        snifftest_max_epoch = 0\n        snifftest_metric_val = 100.0\n        \n    iter_best_fit = 0\n    # init timed session.  This allows you to set a training time limit.  Handy for Keras 6 hour limit.\n    max_duration_mins = 330  # this is 330 minutes (5.5 hrs).  Set it to 0 if you don't want a timed session.\n    fmt = \"%a %b %d %H:%M:%S\"\n    if max_duration_mins == 0:\n        started_at = 0\n        finish_by = 0\n    else:\n        started_at = datetime.today()\n        finish_by = started_at + timedelta(minutes=max_duration_mins)\n        log_msg(f'Started at {started_at.strftime(fmt)}, finish by {finish_by.strftime(fmt)}')\n\n    # run the training x times and save the model weights that give the best metric\n    # you could set it to x = 200 and let it run for hours if you want.\n    # note: you can't pause training, then resume it later due to the nature of the Adam optimizer,\n    # so you must train it uninterrupted.\n    # training will end after the max_duration_mins has passed or x iterations has completed, whichever happens first.\n    x = 3\n    epochs=500\n    patience=30\n    batch_size=500\n    for xtr in range(0, x):\n        K.clear_session()  #clears tensorflow resources\n        log_msg(f'Iteration {xtr} of {x - 1}')\n        # now we'll define our model.  \n        input = Input(shape=(28, 28, 1))\n        x1 = Conv2D(32, (5, 5), padding='same', kernel_initializer='he_normal')(input)\n        x1 = Activation('relu')(x1)\n        x1 = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(x1)\n        x1 = Activation('relu')(x1)\n        x1 = MaxPooling2D(pool_size=(2, 2))(x1)\n        x1 = Dropout(0.5)(x1)\n        x1 = Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal')(x1)\n        x1 = Activation('relu')(x1)\n        x1 = Conv2D(32, (5, 5), padding='same', kernel_initializer='he_normal')(x1)\n        x1 = Activation('relu')(x1)\n        x1 = Dropout(0.5)(x1)\n        x1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x1)\n        x1 = Dropout(0.5)(x1)\n        x1 = Flatten()(x1)\n        x1 = Dense(128, activation='relu', kernel_initializer='he_normal')(x1)\n        x1 = Dropout(0.5)(x1)\n        output = Dense(10, activation='softmax', kernel_initializer='he_normal')(x1)\n        model = Model(inputs=[input], outputs=[output])\n        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n        #call the kerasbestfit.find_best_fit function.  It will save the model weights with the best metric\n        results, log = kbf.find_best_fit(model=model, metric=metric, xtrain=x_train, ytrain=y_train, xval=x_val,\n                                         yval=y_val, validation_split=0, batch_size=batch_size, epochs=epochs, \n                                         patience=patience, snifftest_max_epoch=snifftest_max_epoch,\n                                         snifftest_metric_val=snifftest_metric_val,\n                                         show_progress=True, format_metric_val=format_metric_val,\n                                         save_best=True, save_path=model_path,\n                                         best_metric_val_so_far=best_metric_val_so_far,\n                                         logmsg_callback=log_msg, finish_by=finish_by)\n        del model\n        # notify if we found a new best metric val\n        is_best = False\n        if metric == 'val_acc':\n            is_best = results['best_metric_val'] > best_metric_val_so_far\n        elif metric == 'val_loss':\n            is_best = results['best_metric_val'] < best_metric_val_so_far\n        if is_best:\n            iter_best_fit = xtr\n            best_metric_val_so_far = results['best_metric_val']\n            sbest_metric_val_so_far = metric + '=' + format_metric_val.format(best_metric_val_so_far)\n            sbest_epoch = results['best_epoch']\n            best_log=results['history']\n            best_epoch=results['best_epoch']\n            s = f'NEW BEST SO FAR: {sbest_metric_val_so_far} on epoch {sbest_epoch}\\n'\n            log_msg(s)\n\n        if results['expired']: #timer has expired\n            break\n\n    log_msg(f'The best result is {sbest_metric_val_so_far}')        \n    log_msg('---- TRAINING END ----')        ","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"f0e4987428ab55838e13b7d5ccbc3ab83270f820"},"cell_type":"markdown","source":"### Analysing the results\n\nLet's plot the accuracy and loss curves for our best result.  Learning how to interpret these graphs will help you avoid results that suffer from overfitting or underfitting.  The perfect plot would be one where the two lines for the metric you chose (for example, val_acc uses the acc and val_acc lines) would be right on top of each other.\n\nThe vertical dotted line represents the epoch in which the best result was found and saved.  The area to the right of the vertical line represents the patience epochs, where it waits for an even better result, but finds none."},{"metadata":{"trusted":true,"_uuid":"b1fe45fd4c840f155c7cd11ae7bd9bf0fe3a14d8","collapsed":true},"cell_type":"code","source":"# generate plot or accuracy and loss\nplt.rcParams.update({'font.size': 16})\nplt.figure(figsize=(20,10))\nplt.xlim(0.0, best_epoch + 2.0)\nplt.ylim(0.0, 1.0)\nplt.plot(best_log['acc'])\nplt.plot(best_log['val_acc'])\nplt.plot(best_log['loss'])\nplt.plot(best_log['val_loss'])\nplt.axvline(results['best_epoch'], 0, 1, color='k', linestyle='--')\nplt.title(f'Best Result: {sbest_metric_val_so_far}')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['acc', 'val_acc', 'loss', 'val_loss'], loc='center right')\nplt.show()","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"68194c23db92119e82dc458d3cc8d80a04dbce3b"},"cell_type":"markdown","source":"### Review the confusion matrix\n\nUsing a confusion matrix allows you to see exactly which errors occurred with the validation data.  Let's load the saved model and do a prediction with the validation data."},{"metadata":{"trusted":true,"_uuid":"8a5a8107d48a377a455200aa203c2ee7e3de2dba","collapsed":true},"cell_type":"code","source":"# load saved model\nwith open('.//model.json', 'r') as f:\n    modelx = model_from_json(f.read())\nmodelx.load_weights('.//model.hdf5')\nY_pred = modelx.predict(x_val)\nY_pred_classes = np.argmax(Y_pred, axis=1)\nY_true = np.argmax(y_val, axis=1)\n\n# this confusion matrix code from: https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6\ncm = confusion_matrix(Y_true, Y_pred_classes)\nclasses = range(10)\nnormalize = False\ncmap = plt.cm.Greens\nplt.imshow(cm, interpolation='nearest', cmap=cmap)\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.rcParams.update({'font.size': 10})\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nif normalize:\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    # val = cm[i, j]\n    # val = 100.0 * val/cm[i, :].sum()\n    # print(cm[i, j], val)\n    plt.text(j, i, cm[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\ndel modelx","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"d0305e02f1e2162788ff06431835d55d9c00cb0d"},"cell_type":"markdown","source":"# Predictions\n\nWe'll try making a prediction with our model and data that the model has never seen before, so let's load our saved model."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b15e15175da3e95ef035b6980993c55b712c78a6"},"cell_type":"code","source":"# load test data\nx_test = np.load(file=x_test_file_npy)\nx_test = x_test.astype('float32')\nx_test /= 255\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\ny_test = None  # we don't have this data because we don't have the test labels available\n\n# load saved model\nwith open('.//model.json', 'r') as f:\n    modelx = model_from_json(f.read())\nmodelx.load_weights('.//model.hdf5')\n","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"300014e3c7cecc3a6fdf63448ef68e693d1d7b90"},"cell_type":"markdown","source":"### Testing against a known dataset\n\nLet's run the prediction against the Keras built-in MNIST dataset.  This gives us a bit of a sanity check.  The prediction accuracy should be similar to the val_acc determined during training.\n\n**Note**: This does not work if you run this notebook on Kaggle.  The first time you try to load the Keras MNIST data, it will download the dataset from a site external to Kaggle, and Kaggle does not permit this.  So I have commented out the code in the following section.  If you try the notebook in a non-Kaggle environment, you can uncomment it and it should work fine."},{"metadata":{"trusted":true,"_uuid":"22eb4274b8fe60f3d79f2679e0769810f2780f5d","collapsed":true},"cell_type":"code","source":"# # try the model with the built-in Keras MNIST dataset\n# log_msg('')\n# log_msg('Trying the model with the Keras built-in MNIST test dataset...')\n# # (x_keras_train, y_keras_train), (x_keras_test, y_keras_test) = mnist.load_data()\n# # we would prefer to load direct from the keras library, but this may trigger a download\n# # of the dataset, which Kaggle blocks.  So we will load the data from these .npy's\n# # that I have prepared and uploaded to Kaggle.\n# x_keras_train = np.load(file=x_keras_train_file)\n# y_keras_train = np.load(file=y_keras_train_file)\n# x_keras_test = np.load(file=x_keras_test_file)\n# y_keras_test = np.load(file=y_keras_test_file)\n# # change data type and normalize\n# x_keras_test = x_keras_test.astype('float32')\n# y_keras_test = y_keras_test.astype('float32')\n# x_keras_test /= 255\n# x_keras_test = x_keras_test.reshape(x_keras_test.shape[0], 28, 28, 1)\n# # y_keras_test = np_utils.to_categorical(y_keras_test, 10)\n# test_labels = modelx.predict(x_keras_test)\n# predicted_classes = np.argmax(test_labels, axis=1)\n# correct_indices = np.nonzero(predicted_classes == y_keras_test)[0]\n# incorrect_indices = np.nonzero(predicted_classes != y_keras_test)[0]\n# accuracy = len(correct_indices) / (len(correct_indices) + len(incorrect_indices))\n# log_msg(f'Prediction is {accuracy}')\n","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"0a7333f0dc29a4deaea8f09765ac44d30476c90b"},"cell_type":"markdown","source":"### Predicting  the Kaggle test data\n\nFinally, all this effort is about to pay off!  Let's try predicting just a single digit from the Kaggle test dataset. This is the dataset on which our submission.csv will be based on.\n\n#### First, try a single image\n\nLet's try to predict the image at index position zero.  It will display the digit, and then the class probabilities.    \n\nThe class probabilities are a list of ten floating point values.  Find the highest value.  You'll notice it is the third number, which is index 2 in the probability list.  The predicted digit is therefore a 2.  \n\nIf one of the class probabilities is very high and the rest are very low, then the model is very confident of the result.  But if you have a result where there isn't an obvious highest value, the model isn't as confident.  If the model is perfectly confident, the class probablilty for one of the numbers would be 1.0.  if it was perfectly unconfident, all probablities would be 0.10.  Confused yet?  Try running the class probabilities for many different images to get a feel for how this works.\n\nIn the code below, try changing the index to 12.  The resulting image is confusing.  Is it a 4?  Or is it a 7? \n\nAlso try index 2009. Is it a 1 or 2?  What do the probablities say?\n\nThe model predicts it is a 4, but we don't know if this is correct.  Only Kaggle knows if it is correct."},{"metadata":{"trusted":true,"_uuid":"ec9305cfeaea263f9beb5d7e1cd7b0b6fc524d8b","collapsed":true},"cell_type":"code","source":"# predict a single test image\nlog_msg('Predicting a single test image...')\nimg = x_test[0]    #change this from 0 to anything between 0-23999\nplt.imshow(img.reshape([28, 28]), cmap=plt.get_cmap('gray_r'))\nplt.show()\nimg = img.reshape(1, 28, 28, 1)\nprediction = modelx.predict([img])\npredicted_classes = np.argmax(prediction[0], axis=0)\nlog_msg('Probabilities')\nfor x in range(0,10):\n    s = f'Class {x}: {format_metric_val.format(prediction[0,x])}'\n    log_msg(s)\nlog_msg(f'Predicted class: {predicted_classes}')\n","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"b40a2e14495ee62feb3fca6c022fc1bb6ce5bde3"},"cell_type":"markdown","source":"#### Predicting all the Kaggle test data\n\nWhen you are happy with the model and it's accuracy, you can do the final prediction and create the submission.csv.  \n\nThe csv isn't actually sent to Kaggle until you manually submit it to them.  So you can run this kernel as many times as you like and tweak it any which way.  Then send in the submission.csv to see what Kaggle score you get!"},{"metadata":{"trusted":true,"_uuid":"932cc80eddf9dc7cbfe20cecf18ac24feb39df93","collapsed":true},"cell_type":"code","source":"# now let's predict all the test images and save the results to submission.csv\nlog_msg('Predicting all test images...')\ntest_labels = modelx.predict(x_test)\npredicted_classes = np.argmax(test_labels, axis=1)\nlog_msg('Prediction complete.')","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"9743259b209db7ee3173526d3ab57de05b4c7b81"},"cell_type":"markdown","source":"### Creating the submission.csv\n\nThe following code will create the take your predicted results from the Kaggle test data and create a submission.csv file.  It won't be sent to Kaggle automatically.  You will need to send it to them manually.\n\nSo you can run this kernel as many times as you like and tweak it until you are happy with the results.  Then send in the submission.csv to see what Kaggle score you get!"},{"metadata":{"trusted":true,"_uuid":"442516ecf36ca2d2eddad6da04f2613a3dfdd45a","collapsed":true},"cell_type":"code","source":"submission = pd.DataFrame({\n    'ImageID': range(1, 28001),\n    'Label': predicted_classes\n})\nsubmission.to_csv('submission.csv', index=False)\nlog_msg('Saved predictions as submission.csv')\n\nlog_msg('Script ended.') ","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"6aa60e834c18a7d236bbcde4f045ccf7ae55bce6"},"cell_type":"markdown","source":"# Conclusion\n\nThat's all there is to creating and training a model in Keras.  Feel free to fork this code, copy it, tweak it and use it to learn even more about machine learning.  \n\n### Source code\n\n\n\n#### Finding kerasbestfit on GitHub\n\nhttps://github.com/russbeuker/kerasbestfit\n\n\n#### The Kaggle digit png's\n\nhttps://github.com/russbeuker/kaggle_digit_recognizer/blob/master/gen_digit_pngs.py\n\nhttps://github.com/russbeuker/kaggle_digit_recognizer/tree/master/input/digits\n\n### A quick test\nExplore these questions to help further your knowledge.\n\n- Compare training with the val_loss metric on the training data that has had the bad images removed, versus running it with val_loss on the training data with the bad images still in it.  The val_loss will be better with the bad images removed.  But if you do the same thing while running with val_acc, it will be more or less unchanged.  Why?\n- Why does changing the model Dropout from 0.5 to 0.9 change the results?\n- How do you know if your results are overfitting, underfitting or just right?\n- Why do we not allow the training code to see the final test data?\n\nThanks joining me onthis journey.  Have fun, and please upvote if you found this helpful!\n\nRuss"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e3de002c32b80d65a124372e83ab41d9ca4d022a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}