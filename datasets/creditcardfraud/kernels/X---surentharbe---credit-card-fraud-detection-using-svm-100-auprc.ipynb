{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score, precision_recall_curve\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, average_precision_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcard.csv\")\n\n#Show the unbalace of the data \ncount = pd.value_counts(data['Class'], sort = True).sort_index()\ncount.plot(kind = 'bar')\nplt.title('Unbalance Data')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.show()\n\n'''\nDataset is highly unbalanced and is understandable. \nClass 0 represents the normal transactions\nClass 1 represents the fraudulent transactions\n'''\nNo_of_frauds= len(data[data[\"Class\"]==1])\nNo_of_normals = len(data[data[\"Class\"]==0])\ntotal= No_of_frauds + No_of_normals\nFraud_percent= (No_of_frauds / total)*100\nNormal_percent= (No_of_normals / total)*100\n\nprint(\"The number of normal transactions(Class 0) are: \", No_of_normals)\nprint(\"The number of fraudulent transactions(Class 1) are: \", No_of_frauds)\nprint(\"Class 0 percentage = \", Normal_percent)\nprint(\"Class 1 percentage = \", Fraud_percent)\n\n#Resampleing the dataset\ndata['normAmount']=StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\nX=data.drop(['Time','Amount'],axis=1)\ny=data['Class']\n\n# Split the data into training and testing subsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 12)\n\n# show the dimensions of the train/test data\nprint(\"X_train.shape: \", X_train.shape)\nprint(\"X_test.shape: \", X_test.shape)\nprint(\"y_train.shape: \", y_train.shape)\nprint(\"y_test.shape: \", y_test.shape)\n\n# Applying SVM Algorithm\nprint(\"-----------------------------------------------------------------------------------\")\nprint(\"                                Support Vector Machine                             \")              \nprint(\"-----------------------------------------------------------------------------------\")\n\n#Using the rbf kernel to build the initail model.\nclassifier= svm.SVC(C= 1, kernel= 'linear', random_state= 0)\n\n#Fit into Model\nclassifier.fit(X_train, y_train)\n\n#Predict the class using X_test\ny_pred = classifier.predict(X_test)\n\ncon_mat = confusion_matrix(y_test, y_pred)\naverage_precision = average_precision_score(y_test, y_pred)\ncls_report = classification_report(y_test, y_pred)\n\nprint(\"*****************************************************************\")\nprint(\"Area under the curve : %f\" % (roc_auc_score(y_test, y_pred)))\nprint(\"Average precision-recall score RF: {}\".format(average_precision))\nprint(con_mat)\nprint(cls_report)\nprint(\"*****************************************************************\")\n\ndef confus_matrix(CM):\n    fig, ax = plot_confusion_matrix(conf_mat= CM)\n    plt.title(\"The Confusion Matrix of full dataset using best_parameters\")\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.show()\n    print(\"The accuracy is \"+str((CM[1,1]+CM[0,0])/(CM[0,0] + CM[0,1]+CM[1,0] + CM[1,1])*100) + \" %\")\n    print(\"The recall from the confusion matrix is \"+ str(CM[1,1]/(CM[1,0] + CM[1,1])*100) +\" %\")\nconfus_matrix(con_mat)\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\nplt.step(recall, precision, color='b', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred)\nroc_auc_rf = auc(fpr_rf, tpr_rf)\nplt.figure(figsize=(8,8))\nplt.xlim([-0.01, 1.00])\nplt.ylim([-0.01, 1.01])\nplt.plot(fpr_rf, tpr_rf, lw=1, label='{} curve (AUC = {:0.2f})'.format('RF',roc_auc_rf))\n\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curve', fontsize=16)\nplt.legend(loc='lower right', fontsize=13)\nplt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\nplt.axes().set_aspect('equal')\nplt.show()","execution_count":2,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}