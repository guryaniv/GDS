{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d0450932-4f99-46cd-9b87-9fe60fdad81c"
      },
      "source": [
        "This is my first kernel for this competition and on Kaggle too. Please provide your feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "08b7deb9-87ab-de21-f56d-da28d88e113a"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b8ded710-55f7-6425-22af-3d888aacdcf5"
      },
      "source": [
        "If you like this notebook then please upvote (button at the top right)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8af9a3e0-fd4c-7155-70ac-cf418d2df726"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"../input/q_quora.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a79958a5-3169-f050-2163-2c5e55d61eef"
      },
      "outputs": [],
      "source": [
        "print  (df_train.iloc[0,4])\n",
        "print (df_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d72783b-055e-8975-fe63-b7b84144debf"
      },
      "outputs": [],
      "source": [
        "print(df_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ee07f019-fad3-60fb-1d27-e1330c528492"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "pal = sns.color_palette()\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "is_dup = df_train['is_duplicate'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "\n",
        "sns.barplot(is_dup.index, is_dup.values, alpha=0.8, color=pal[1])\n",
        "\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "\n",
        "plt.xlabel('Is Duplicate', fontsize=12)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1fa70e0d-e3ec-36fa-9c02-10f1a4245be7"
      },
      "outputs": [],
      "source": [
        "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
        "from wordcloud import WordCloud\n",
        "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs.astype(str)))\n",
        "plt.figure(figsize=(20, 15))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6207940-0aaf-4d3e-7a68-5eddd60c9448"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "def question_to_words( raw_question ):\n",
        "\n",
        "    # 2. Remove non-letters        \n",
        "    letters_only = re.sub(\"^a-zA-Z\",' ',raw_question)\n",
        "    \n",
        "    #\n",
        "    # 3. Convert to lower case, split into individual words\n",
        "    words = letters_only.lower().split()                             \n",
        "    #\n",
        "    # 4. In Python, searching a set is much faster than searching\n",
        "    #   a list, so convert the stop words to a set\n",
        "    stops = set(stopwords.words(\"english\"))                  \n",
        "    # \n",
        "    # 5. Remove stop words\n",
        "    meaningful_words = [w for w in words if not w in stops]   \n",
        "    #\n",
        "    # 6. Join the words back into one string separated by space, \n",
        "    # and return the result.\n",
        "    return( \" \".join( meaningful_words ))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2dfec1f6-b7b5-5d8e-eab7-0a49529709f4"
      },
      "outputs": [],
      "source": [
        "print (df_train.iloc[0,4])\n",
        "q1=df_train.iloc[0,4]\n",
        "print (\"type of q1 is \", type(q1))\n",
        "print (question_to_words(q1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c43920fe-d2a8-4894-69d6-f49bf148b742"
      },
      "outputs": [],
      "source": [
        "cleanq1s = []\n",
        "cleanq2s=[]\n",
        "numqs=df_train.shape[0]\n",
        "print (numqs)\n",
        "for i in range( 0, numqs):\n",
        "    # Call our function for each one, and add the result to the list of\n",
        "    # clean reviews\n",
        "    cleanq1s.append(question_to_words(list(df_train['question1'])[i] ) )\n",
        "    cleanq2s.append(question_to_words(list(df_train['question2'])[i] ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fd08518c-9edb-ca44-243c-48db03c01311"
      },
      "outputs": [],
      "source": [
        "print (\"Creating the bag of words...\\n\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
        "# bag of words tool.  \n",
        "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
        "                             tokenizer = None,    \\\n",
        "                             preprocessor = None, \\\n",
        "                             stop_words = None,   \\\n",
        "                             max_features = 5000) \n",
        "\n",
        "# fit_transform() does two functions: First, it fits the model\n",
        "# and learns the vocabulary; second, it transforms our training data\n",
        "# into feature vectors. The input to fit_transform should be a list of \n",
        "# strings.\n",
        "train_data_features1 = vectorizer.fit_transform(cleanq1s)\n",
        "train_data_features2 = vectorizer.fit_transform(cleanq2s)\n",
        "\n",
        "# Numpy arrays are easy to work with, so convert the result to an \n",
        "# array\n",
        "train_data_features1 = train_data_features1.toarray()\n",
        "train_data_features2 = train_data_features2.toarray()\n",
        "print (train_data_features1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d77921d4-0140-1100-3a0b-232c04496828"
      },
      "outputs": [],
      "source": [
        "vocab = vectorizer.get_feature_names()\n",
        "print (vocab[0:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1279afc4-33e8-d6f7-0e7e-ac6cd973ab43"
      },
      "outputs": [],
      "source": [
        "print (\"Training the random forest...\")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize a Random Forest classifier with 100 trees\n",
        "forest = RandomForestClassifier(n_estimators = 100) \n",
        "\n",
        "# Fit the forest to the training set, using the bag of words as \n",
        "# features and the sentiment labels as the response variable\n",
        "#\n",
        "# This may take a few minutes to run\n",
        "train_data_features1\n",
        "#forest = forest.fit( train_data_features1-train_data_features2, df_train[\"is_duplicate\"] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dca74d37-dc1e-195b-49fc-90454ce7c76f"
      },
      "outputs": [],
      "source": [
        "#preds=forest.predict(train_data_features1-train_data_features2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18b2eb9d-8de2-e18a-7358-9a7394011c2d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "45f5d942-fb0c-efac-6aab-c64e7216f9d9"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "\n",
        "y = df_train['is_duplicate']\n",
        "X = train_data_features1-train_data_features2\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "print (X_train[0:4])\n",
        "\n",
        "forest = forest.fit( X_train, y_train )\n",
        "preds_train=forest.predict(X_train)\n",
        "preds_test=forest.predict(X_test)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print('Train AUC:', roc_auc_score(y_train, preds_train))\n",
        "print('Test AUC:', roc_auc_score(y_test, preds_test))\n",
        "#print (train_data_features1[0:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dfca5de8-1451-ad9c-e5fa-26ab867f9816"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "print('log loss train:', log_loss(y_train, preds_train))\n",
        "print('log loss test:', log_loss(y_test, preds_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eefdb707-adef-ad5b-5ee6-9807fc04dffd"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tvec = TfidfVectorizer(min_df=.0025, max_df=.1, stop_words=\"english\", ngram_range=(1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c67f0bc6-088f-3e11-3491-d8099236f78c"
      },
      "outputs": [],
      "source": [
        "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
        "# bag of words tool.  \n",
        "vectorizertd = TfidfVectorizer( stop_words=\"english\",max_features=1000)\n",
        "\n",
        "# fit_transform() does two functions: First, it fits the model\n",
        "# and learns the vocabulary; second, it transforms our training data\n",
        "# into feature vectors. The input to fit_transform should be a list of \n",
        "# strings.\n",
        "train_data_features1td = vectorizertd.fit_transform(cleanq1s)\n",
        "train_data_features2td = vectorizertd.fit_transform(cleanq2s)\n",
        "\n",
        "# Numpy arrays are easy to work with, so convert the result to an \n",
        "# array\n",
        "train_data_features1td = train_data_features1td.toarray()\n",
        "train_data_features2td = train_data_features2td.toarray()\n",
        "print (train_data_features1td.shape)\n",
        "print (train_data_features2td.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "447ff91a-d73f-fe2d-221e-9d9f2061ec88"
      },
      "outputs": [],
      "source": [
        "print (\"Training the random forest...\")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize a Random Forest classifier with 100 trees\n",
        "forest = RandomForestClassifier(n_estimators = 100) \n",
        "\n",
        "# Fit the forest to the training set, using the bag of words as \n",
        "# features and the sentiment labels as the response variable\n",
        "#\n",
        "# This may take a few minutes to run\n",
        "np.nonzero(train_data_features1td)\n",
        "train_data_features1td[4041,895]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7756eedc-694c-750e-6cc3-3111cfbb5eee"
      },
      "outputs": [],
      "source": [
        "#from sklearn.cross_validation import train_test_split\n",
        "\n",
        "y = df_train['is_duplicate']\n",
        "Xtd1 = train_data_features1td-train_data_features2td\n",
        "\n",
        "\n",
        "X_traintd,X_testtd,y_traintd,y_testtd = train_test_split(Xtd1,y,test_size=0.3)\n",
        "print (X_train[0:4])\n",
        "\n",
        "forest = forest.fit( X_traintd, y_traintd )\n",
        "preds_traintd=forest.predict(X_traintd)\n",
        "preds_testtd=forest.predict(X_testtd)\n",
        "#from sklearn.metrics import roc_auc_score\n",
        "print('Train AUC:', roc_auc_score(y_traintd, preds_traintd))\n",
        "print('Test AUC:', roc_auc_score(y_testtd, preds_testtd))\n",
        "#print (train_data_features1[0:4])\n",
        "print('log loss train with td_idf:', log_loss(y_traintd, preds_traintd))\n",
        "print('log loss test with td_idf', log_loss(y_testtd, preds_testtd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "347aedda-b5f8-d0aa-a0a1-ec296d5d5003"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cossim\n",
        "cos_sim_feat=cossim(train_data_features1td,train_data_features2td)\n",
        "X_traincos,X_testcos,y_traincos,y_testcos = train_test_split(cos_sim_feat,y,test_size=0.3)\n",
        "#print (X_train[0:4])\n",
        "\n",
        "forest = forest.fit( X_traincos, y_traincos )\n",
        "preds_traincos=forest.predict(X_traincos)\n",
        "preds_testcos=forest.predict(X_testcos)\n",
        "#from sklearn.metrics import roc_auc_score\n",
        "print('Train AUC:', roc_auc_score(y_traincos, preds_traincos))\n",
        "print('Test AUC:', roc_auc_score(y_testcos, preds_testcos))\n",
        "#print (train_data_features1[0:4])\n",
        "print('log loss train with cossine:', log_loss(y_traincos, preds_traincos))\n",
        "print('log loss test with cossine', log_loss(y_testcos, preds_testcos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b61b7a79-5abf-cd4a-2192-1a1830a6e60d"
      },
      "outputs": [],
      "source": [
        "from gensim.models import word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ee1ade4e-d3d0-d18d-7e9a-29dae078f173",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}