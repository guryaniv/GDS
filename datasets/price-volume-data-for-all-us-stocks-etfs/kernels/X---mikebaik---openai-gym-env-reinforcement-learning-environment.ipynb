{"cells":[{"metadata":{"_uuid":"0cc2d692c6d8dbc46f857bb7f857744a6ff4820b"},"cell_type":"markdown","source":"### Reinforcement Learning Environment for OpenAI's gym"},{"metadata":{"_uuid":"d355c5ce42260bf38b9e09e269b9d61cb9d66db1"},"cell_type":"markdown","source":"An implementation of Reinforcement Learning Environment, geared towards **open-AI**'s `gym`\n\nThe best part about having it in `gym` env is that you can use other reinforcement learning libraries by using this as a base.\n\nI have more detailed implementations [here](https://github.com/wbaik/gym-stock-exchange) https://github.com/wbaik/gym-stock-exchange"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/Data/Stocks/\")[:10])\nstock_path = '../input/Data/Stocks/'\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f8983dcfd0c2f3ed1341b27c115cbbc65f8af2e"},"cell_type":"code","source":"import itertools\nimport functools\nimport matplotlib.pyplot as plt\nimport collections\nimport six\nimport gym\nimport datetime\nimport gym.spaces\n\ndef iterable(arg):\n    return (isinstance(arg, collections.Iterable) and not\n            isinstance(arg, six.string_types))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6421616fef8316951601c56ca152673b72a79068"},"cell_type":"code","source":"class TickerContinuous:\n    # Don't delete num_actions just yet, need to go fix all others..\n    #   Especially when constructing in Engine\n    def __init__(self, ticker, start_date, num_days_iter,\n                 today=None, num_actions=3, test=False,\n                 action_space_min=-1.0, action_space_max=1.0):\n        self.ticker = ticker\n        self.start_date = start_date\n        self.num_days_iter = num_days_iter\n        self.df, self.dates = self._load_df(test)\n        self.action_space = gym.spaces.Box(action_space_min, action_space_max,\n                                           (1, ), dtype=np.float32)\n        self.today = 0 if today is None else today\n        self._data_valid()\n        self.current_position = self.accumulated_pnl = 0.0\n\n    def _load_df(self, test):\n        if test:\n            ticker_data = self._load_test_df()\n        else:\n            ticker_data = pd.read_csv(stock_path+f'{self.ticker}.us.txt')\n            # print(ticker_data.columns)\n            # ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'OpenInt']\n            ticker_data.rename(lambda x: str.lower(x), axis=1, inplace=True)\n            ticker_data.drop('openint', axis=1, inplace=True)\n            ticker_data = ticker_data[ticker_data['date'] >= self.start_date]\n            \n        ticker_data.reset_index(inplace=True)\n        # This is really cheating but...\n        dates_series = ticker_data['date']\n        ticker_data.drop('date', axis=1, inplace=True)\n        # This part should become a function eventually\n        ticker_data_delta = ticker_data.pct_change()\n        add_str_delta = lambda x: x + '_delta'\n        ticker_data_delta.rename(add_str_delta, axis='columns', inplace=True)\n        ticker_data_delta.iloc[0, :] = 0.0\n\n        zeros = pd.DataFrame(np.zeros((len(ticker_data), 2)),\n                             columns=['position', 'pnl'])\n\n        # It's probably better to transpose, then let columns be dates, but wtf...\n        df = pd.concat([ticker_data, ticker_data_delta, zeros], axis=1)\n        df.drop(['index', 'index_delta'], axis=1, inplace=True)\n\n        return df, dates_series\n\n    def _load_test_df(self):\n        date_col = [datetime.date.today() + datetime.timedelta(days=i)\n                    for i in range(self.num_days_iter)]\n        aranged_values = [np.repeat(i, 6) for i in range(1, self.num_days_iter+1)]\n        temp_df = pd.DataFrame(aranged_values,\n                               columns=['date', 'open', 'high', 'low', 'close', 'volume'])\n        temp_df.iloc[:, 0] = date_col\n        return temp_df\n\n    def _data_valid(self):\n        assert len(self.df) >= self.num_days_iter, \\\n                f'DataFrame shape: {self.df.shape}, num_days_iter: {self.num_days_iter}'\n        assert len(self.df) == len(self.dates), \\\n                f'df.shape: {self.df.shape}, dates.shape:{self.dates.shape}'\n\n    def get_state(self, delta_t=0):\n        today_market_data_position = np.array(self.df.iloc[self.today+delta_t, -7:-2])\n        today_market_data_position[-1] = self.current_position\n        return today_market_data_position\n\n    # 1. Reward is tricky\n    # 2. Should invalid action be penalized?\n    def step(self, action):\n        if not self.done():\n            # Record pnl\n            # This implementation of reward is such a hogwash!!\n            #     but recall, Deepmind's DQN solution does something similar...\n            #     assigning credit is always hard...\n            # Pandas complain here, \"A value is trying to be set on a copy of a slice from a DataFrame\"\n            #     but the suggested solution is actually misleading... so leaving it as is\n            pd.set_option('mode.chained_assignment', None)\n            self.df.pnl[self.today] = reward = 0.0 if self.today == 0 else \\\n                                               self.current_position * self.df.close_delta[self.today]\n\n            # Think about accumulating the scores...\n            self.accumulated_pnl += reward\n            self.df.position[self.today] = self.current_position = action\n            self.today += 1\n\n            return reward, False\n        else:\n            self.current_position = 0.0\n            return 0.0, True\n\n    def valid_action(self, action):\n        return self.action_space.low <= action <= self.action_space.high\n\n    def reset(self):\n        self.today = 0\n        self.df.position = self.df.pnl = 0.0\n        self.current_position = self.accumulated_pnl = 0.0\n\n    # NOT THE MOST EFFICIENT...\n    def done(self):\n        return self.today > self.num_days_iter\n\n    def render(self, axis):\n        # market_data, position = self.get_state()\n        # axis[0].scatter(self.today, self.df.pnl[self.today-1])\n        axis[0].set_ylabel(f'Daily price: {self.ticker}')\n        axis[0].set_xlabel('Time step')\n        axis[0].plot(np.arange(self.today), self.df.close[:self.today])\n        # axis[1].scatter(self.today, position)\n        # axis[2].scatter(self.today, self.accumulated_pnl)\n        axis[1].set_ylabel(f'Daily return from Agent')\n        axis[1].set_xlabel('Time step')\n        axis[1].scatter(self.today, self.accumulated_pnl)\n        plt.pause(0.0001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7b27867dbbb09a31fa35b1ee1c37eb151a94fa0"},"cell_type":"code","source":"class EngineContinuous:\n    def __init__(self, tickers, start_date, num_days_iter,\n                 today=None, seed=None, num_action_space=3,\n                 render=False, *args, **kwargs):\n        if seed: np.random.seed(seed)\n        if not iterable(tickers): tickers = [tickers]\n\n        self.tickers = self._get_tickers(tickers, start_date, num_days_iter,\n                                         today, num_action_space, *args, **kwargs)\n        self.reset_game()\n\n        if render:\n            # Somehow ax_list should be grouped in two always...\n            # Or is there another way of getting one axis per row and then add?\n            fig_height = 3 * len(self.tickers)\n            self.fig, self.ax_list = plt.subplots(len(tickers), 2, figsize=(10, fig_height))\n\n    def reset_game(self):\n        list(map(lambda ticker: ticker.reset(), self.tickers))\n\n    def _get_tickers(self, tickers, start_date, num_days_iter,\n                     today, num_action_space, *args, **kwargs):\n        return [TickerContinuous(ticker, start_date, num_days_iter, today, num_action_space, *args, **kwargs)\n                for ticker in tickers]\n\n    def _render(self, render):\n        if render:\n            if len(self.tickers) == 1:\n                self.tickers[0].render(self.ax_list)\n            else:\n                for axis, ticker in zip(self.ax_list, self.tickers):\n                    ticker.render(axis)\n\n    def get_state(self, delta_t=0):\n        # Note: np.arary(...) could also be used\n        return list(map(lambda ticker: ticker.get_state(delta_t), self.tickers))\n\n    def moves_available(self):\n        raise NotImplementedError\n\n    def step(self, actions):\n        if not iterable(actions): actions = [actions]\n        assert len(self.tickers) == len(actions), f'{len(self.tickers)}, {len(actions)}'\n\n        rewards, dones = zip(*(itertools.starmap(lambda ticker, action: ticker.step(action),\n                                                 zip(self.tickers, actions))))\n\n        # This is somewhat misleading\n        score = functools.reduce(lambda x, y: x + y, rewards, 0.0)\n        done = functools.reduce(lambda x, y: x | y, dones, False)\n\n        return score, done\n\n    def render(self, render=False):\n        # This is possibly unnecessary b/c of changes\n        self._render(render)\n\n    def __repr__(self):\n        tickers = [f'ticker_{i}: {ticker.ticker}, ' for i, ticker in enumerate(self.tickers)]\n        return str(tickers)\n\n    def _data_valid(self):\n        raise NotImplementedError\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbb3eaf4b069ddbd1964ce2c002d0becb4aaaa3f"},"cell_type":"code","source":"class PortfolioContinuous(EngineContinuous):\n    def __init__(self, tickers, start_date, num_days_iter,\n                 today=None, seed=None, render=False,\n                 action_space_min=0.0, action_space_max=1.0):\n        num_action_space = len(tickers)\n        super().__init__(tickers, start_date, num_days_iter,\n                         today, seed, num_action_space, render,\n                         action_space_min=action_space_min,\n                         action_space_max=action_space_max)\n        self.action_space = gym.spaces.Box(action_space_min, action_space_max,\n                                           (num_action_space, ), np.float32)\n\n    def step(self, actions):\n        return super(PortfolioContinuous, self).step(actions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e3a93f44bc0b68f9697db8e4f2c0480054c1699"},"cell_type":"code","source":"import gym.spaces as spaces\nclass StockExchangeContinuous(gym.Env):\n    metadata = {'render.modes': ['human']}\n\n    # Keep tickers in a list or an iterable...\n    tickers = ['aapl', 'amd', 'msft', 'intc', 'd', 'sbux', 'atvi',\n               'ibm', 'ual', 'vrsn', 't', 'mcd', 'vz']\n    start_date = '2013-09-15'\n    num_days_to_iterate = 100\n    num_days_in_state = 20\n    num_action_space = len(tickers)\n    # no_action_index is truly no_action only if it's not a Portfolio\n    no_action_index = num_action_space//2\n    today = 0\n    render = False\n    # set to None when not using Portfolio\n    action_space_min = -1.0\n    action_space_max = 1.0\n    # For each ticker state: ohlc\n    num_state_per_ticker = 4\n\n    def __init__(self, seed=None):\n\n        # Could manually throw in options eventually...\n        self.portfolio = self.num_action_space > 1\n        self._seed = seed\n\n        if self.portfolio:\n            assert self.action_space_min is not None\n            assert self.action_space_max is not None\n            self.env = PortfolioContinuous(self.tickers, self.start_date,\n                                           self.num_days_to_iterate,\n                                           self.today, seed, render=self.render,\n                                           action_space_min=self.action_space_min,\n                                           action_space_max=self.action_space_max)\n        else:\n            assert self.num_action_space % 2 != 0, 'NUM_ACTION_SPACE MUST BE ODD TO HAVE NO ACTION INDEX'\n            self.env = EngineContinuous(self.tickers, self.start_date,\n                                        self.num_days_to_iterate,\n                                        self.today, seed,\n                                        num_action_space=self.num_action_space,\n                                        render=self.render)\n\n        self.action_space = gym.spaces.Box(self.action_space_min, self.action_space_max,\n                                       (self.num_action_space, ), np.float32)\n        self.observation_space = gym.spaces.Box(-1.0, 1.0,\n                                            (self.num_days_in_state,\n                                             self.num_action_space * self.num_state_per_ticker),\n                                            dtype=np.float32)\n        self.state = self.get_running_state()\n        self.reset()\n\n    def step(self, actions):\n        # I can fix Engine to return state from `self.env.step(action)`\n        reward, ended = self.env.step(actions)\n        self.state = self.add_new_state(self.env.get_state())\n        return self.state, reward, ended, {'score': reward}\n\n    def reset(self):\n        self.env.reset_game()\n        self._initialize_state()\n        return self.state\n\n    def render(self, mode='human', render=False):\n        self.env.render(render)\n\n    def _initialize_state(self):\n        for _ in range(self.num_days_in_state - 1):\n            if self.portfolio:\n                zero_action = [0.0] * self.num_action_space\n                next_state, reward, done, _ = self.step(zero_action)\n            else:\n                next_state, reward, done, _ = self.step([self.no_action_index] * self.num_action_space)\n                assert reward == 0.0, f'Reward is somehow {reward}'\n\n    def __repr__(self):\n        return repr(self.env)\n\n    def get_running_state(self):\n        return np.zeros((self.num_days_in_state, self.num_state_per_ticker * self.num_action_space))\n\n    def add_new_state(self, new_states_to_add):\n        assert isinstance(new_states_to_add, list), type(new_states_to_add)\n        # Disregarding the last elem in each state because it's the holdings...\n        # Maybe just get rid of that altogether?\n        new_states = np.array([state[:-1].tolist() for state in new_states_to_add]).flatten()\n\n        running_state_orig = self.state\n        running_state = pd.DataFrame(running_state_orig).shift(-1)\n\n        # Assign new price to index == last_elem - 1\n        running_state.iloc[-1] = new_states.squeeze()\n\n        # Deprecated...\n        # running_state.iloc[-2] = new_state_to_add.item(0)\n        # Assign new position to index == last_elem\n        # running_state.iloc[-1] = new_state_to_add.item(1)\n\n        assert len(running_state_orig) == len(running_state)\n        return np.array(running_state)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3a3fa4c749c4609af06f181dbfa2d33a6a4ba948"},"cell_type":"code","source":"env = StockExchangeContinuous()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e8d077e5b00bdb10212d6ce4d857d6f82bd9eaf"},"cell_type":"markdown","source":"### An example of agent playing in the environment\n\nI recommend using library implementations, such as [Open-AI baselines](https://github.com/openai/baselines), or [stable-baselines](https://github.com/hill-a/stable-baselines)\n\nAbove code for the stock trading environment is found [here](https://github.com/wbaik/gym-stock-exchange) https://github.com/wbaik/gym-stock-exchange\n"},{"metadata":{"trusted":true,"_uuid":"72856dc7fbbac69cd96549b4e40a4250faa8539e"},"cell_type":"code","source":"class RandomAgent():\n    \"\"\"The world's simplest agent!\"\"\"\n    def __init__(self, action_space):\n        self.action_space = action_space\n\n    def act(self):\n        return self.action_space.sample()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c0cd991bd97469b10aeccc55088edc1f036edd9"},"cell_type":"code","source":"agent = RandomAgent(env.action_space)\nepisode_count = 2\nrewards = []\ndone = False\n\nfor _ in range(episode_count):\n    ob = env.reset()\n    while True:\n        action = agent.act()\n        ob, reward, done, _ = env.step(action)\n        rewards += [reward]\n        if done:\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a409c80df676a46330c5066243c6eb7983820d04"},"cell_type":"code","source":"np.mean(rewards), np.std(rewards)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"494f8341e3de37f63041e9710419ccbe4fe29b19"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}