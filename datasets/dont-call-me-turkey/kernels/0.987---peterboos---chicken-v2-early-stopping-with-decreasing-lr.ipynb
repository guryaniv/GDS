{"cells":[{"metadata":{"_uuid":"f179e68e2be0fa1293b71634c60114c19e4cf8be"},"cell_type":"markdown","source":"# Loop (learn rate + early stopping) +wiggle = improve\n\nA fork from Wiston Van's basic LSTM model [https://www.kaggle.com/winstonvan/the-van-plan-for-kaggle-swaggle](http://)    \n\n\n* 22-11-2018 > I added GPU, and the main model cell can be repeatedly called to keep it improving storing hdf5 results.    \n\n* 22-11-2018 > I added early stopping and loop fitting.\n\n* 23-11-2018 > I added noise to the inputs, reasoning here is like in digit recognition to protect against overfitting and actually enlargin the test data. Now the network has to perform better, to solve it, ( with some other code i had 96% in 4 epochs, but then it didnt improve anymore. So the nise should act as a wigle function and the neural net should find a more general aprouch to the input instead of a highly exact fitting one.\n\nI kept markers in place because i actually learned a lot from this code so maybe you too.\n\nA analysis of this mode can be seen here :  ** [Chicken time ](https://www.youtube.com/watch?v=msSc7Mv0QHY)**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nprint(os.listdir(\"../input\"))\n\nfrom keras import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential,Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\n\nfrom keras import optimizers\nfrom keras.layers import LSTM, Dense, Bidirectional, Input,Dropout,BatchNormalization,CuDNNLSTM, GRU, CuDNNGRU\nfrom keras.layers import Embedding, GlobalMaxPooling1D, GlobalAveragePooling1D, GaussianNoise, GaussianDropout\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"markdown","source":"loading the data"},{"metadata":{"trusted":true,"_uuid":"23321fef08b8963bf4cce1703ced0bfab8071489"},"cell_type":"code","source":"train = pd.read_json('../input/train.json')\ntrain, train_val = train_test_split(train)\ntest = pd.read_json('../input/test.json')\nsample_submission = pd.read_csv('../input/sample_submission.csv')\n\ntrain_train, train_val = train_test_split(train)\nxtrain = [k for k in train_train['audio_embedding']]\nytrain = train_train['is_turkey'].values\n\nxval = [k for k in train_val['audio_embedding']]\nyval = train_val['is_turkey'].values\n\n# Pad the audio features so that all are \"10 seconds\" long\nx_train = pad_sequences(xtrain, maxlen=10)\nx_val = pad_sequences(xval, maxlen=10)\n\ny_train = np.asarray(ytrain)\ny_val = np.asarray(yval)\nprint(\"loaded\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77a73edf0b5db442e15f941b0da49068fdf0249f"},"cell_type":"markdown","source":"An attention layer not made by me, he must be a genious !."},{"metadata":{"trusted":true,"_uuid":"97349574898cd7c633cc39cf71f141a8c14e555c"},"cell_type":"code","source":"# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\nprint(\"Custom layer created.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1c985bee8c65213ae2097df703eaa778639866f"},"cell_type":"markdown","source":"# To clean previous hfd5 \nThis mdel is made to keep  improve upon hdf5 results, so here a way to reset it all..    \nAlso if you change the layers of this model be sure to run the cell below."},{"metadata":{"trusted":true,"_uuid":"831aaaad267d238220cb31f240a703672f4eb8dd"},"cell_type":"code","source":"\ntry:\n    print(\"try to delete previous hfd5\") \n    filepath=\"Chicken-weights.best.hdf5\"\n    import os.path\n    import os\n    os.remove(filepath)\n    print(\"File Removed!\")\nexcept:\n    print(\"no file there\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6efbd9e6bc3864f76e7f5f5d353555e1c914a60"},"cell_type":"markdown","source":"# Early stopping decreasing learn rate\nHere I mainly use early stopping to improve and decrease learn rate, and add some salt to the input data as well and repeat it all in a main loop \"runs\".     \nAt least some part i wrote new !  :), oh and it runs using a GPU LSTM amzingly isnt it ?."},{"metadata":{"trusted":true,"_uuid":"a98cf37a9d166af2346a8e8f883132afef5a55fb"},"cell_type":"code","source":"runs = 100\nestop = 50 #early stop\nep = 150   #epochs  we do early stopping but we do also 100 repeated runs !!\ndp = 0.4   #dropout rate\nbs =16     # batch size\n\nimport time\nimport random\nfrom datetime import datetime\nrandom.seed(datetime.now())\nnp.random.seed(int(time.time()))\n\nprint (\"learn another \",ep,\" epochs\")\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.models import model_from_json\nimport os.path\nimport os\nfilepath=\"Chicken-weights.best.hdf5\"\nact = 'tanh'\nmodel = Sequential()\n# model.add(BatchNormalization(input_shape=(10, 128)))\n# model.add(Bidirectional(LSTM(128, dropout=dp, recurrent_dropout=dp, activation=act, return_sequences=True)))\n# #model.add(Bidirectional(LSTM(128, dropout=dp, recurrent_dropout=dp, activation=act, return_sequences=False)))\n# model.add(Bidirectional(LSTM(128, dropout=dp, recurrent_dropout=dp, activation=act, return_sequences=True)))\nmodel.add(GaussianNoise( stddev=0.03 )) # adding noice to wiggle input improving the robustness search (i hope)\nmodel.add(GaussianDropout(0.2))\n#GaussianDropout(rate)\nmodel.add(BatchNormalization(momentum=0.98,input_shape=(10, 128)))\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences = True)))\n#model.add(Bidirectional(LSTM(128, dropout=dp, recurrent_dropout=dp, activation=act, return_sequences=True)))\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences = True)))\nmodel.add(Attention(10))\n# model.add(GaussianNoise( stddev=0.05 )) # adding noice as to make it harder to find and improving the search (i hope)\nmodel.add(BatchNormalization(momentum=0.98,input_shape=(10, 128)))\nmodel.add(Dense(32,activation=act))\n\nmodel.add(Dense(1,activation='sigmoid'))\n#model.summary()\n\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\n#reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=2, verbose=1, min_lr=1e-8)\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=5, verbose=1, min_lr=1e-8)\nearly_stop = EarlyStopping(monitor='val_loss', verbose=1, patience=estop,  restore_best_weights=True)\n# reduce_lr1 = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint,reduce_lr,early_stop]\ncallbacks_list = [checkpoint,early_stop]\n# callbacks_list = [checkpoint]\n# each time we run we want to improve so make sure we randomize our libraries, its not about reproduceable results,\n#eventually its about perfect wieghts in our networks, after each training.\n\n\nif (os.path.exists(filepath)):\n    print(\"Extending training of previous run\")\n    \n#     with open('model_architecture.json', 'r') as f:\n#          model = model_from_json(f.read())\n    model.load_weights(filepath, by_name=False)\n    model.compile(loss='binary_crossentropy',optimizer = optimizers.Adam(lr=w) , metrics=['accuracy'])    \n    score, acc = model.evaluate(x_val, y_val, batch_size=32)\n    print('Previous test accuracy:', acc)\n    for x in range(1, runs):\n        w = 0.1/(x*x*3)\n        print(\"main run \",x, \" lr=\",w)\n        model.compile(loss='binary_crossentropy', optimizer = optimizers.Adam(lr=w), metrics=['accuracy'])#lr 0.001\n   \n\nelse:\n    print(\"First run\")      \n    for x in range(1, runs):\n        w = 0.1/(x*x*3)\n        print(\"main run \",x, \" lr=\",w)\n        model.compile(loss='binary_crossentropy', optimizer = optimizers.Adam(lr=w), metrics=['accuracy'])#lr 0.001\n        model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=ep, batch_size=bs, callbacks=callbacks_list, verbose=0)\n\nprint(\"you can run this cell again to keep on training, or go on\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bad0983c2a5e5e83913e4427b8d28730195cb93f"},"cell_type":"markdown","source":"The point is we dont want the last run of the model, we want the last hdf5 , and use that so ..."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"dd89150152d8012b01eea6f615c9c738f34902d6"},"cell_type":"code","source":"# Get accuracy of model on validation data. It's not AUC but it's something at least!\nmodel.load_weights(filepath, by_name=False)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.load_weights(filepath, by_name=False)\nscore, acc = model.evaluate(x_val, y_val, batch_size=32)\nprint('Test accuracy:', acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8fca0aaa0d52436f4f41fdc76408c001f6db72c"},"cell_type":"markdown","source":"And here we go lets prepare for submit."},{"metadata":{"trusted":true,"_uuid":"a275580f02056a483b6ab038a1502aed650e8915"},"cell_type":"code","source":"test_data = test['audio_embedding'].tolist()\nsubmission = model.predict(pad_sequences(test_data))\nsubmission = pd.DataFrame({'vid_id':test['vid_id'].values,'is_turkey':[x for y in submission for x in y]})\nsubmission['is_turkey'] = submission.is_turkey\n# submission.is_turkey =submission.is_turkey.round(0)\nprint(submission.head(40))\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}