{"cells":[{"metadata":{"_uuid":"8f017175af30a40d6ceb6971f2812b11c8d463e6"},"cell_type":"markdown","source":"# Can we find some distinguishing features in wine descriptions across the continents?\n\n## Have a look at my topic modelling for wine descriptions notebook first for more background story if interested...\n\n\nSo start with the usual stuff for basic NLP prepocessing:"},{"metadata":{"trusted":true,"_uuid":"e5d2ff24f769e654a98b0404a1957c4e192ea5ee"},"cell_type":"code","source":"import pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edff91005f3fc8a93046f482af3492f9729585e6"},"cell_type":"markdown","source":"Use the dataset with more features, supposedly less duplicates:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/winemag-data-130k-v2.csv') \ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":" df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efb600bbb789150f45889606c436514bee6197cc"},"cell_type":"markdown","source":"Drop the duplicates that were not supposed to be there (or so I understood from the data description..):"},{"metadata":{"trusted":true,"_uuid":"75d53fa830ac9465b5cad740b3463a50038508a3"},"cell_type":"code","source":"#could maybe use more sophisticated comparison across several columns but...\ndf = df.drop_duplicates('description') \ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"771ca717ba1c260592a641a23ad594ed32fe80fa"},"cell_type":"markdown","source":"Stopwords, similar to my wine description topic modelling notebook.."},{"metadata":{"trusted":true,"_uuid":"ddd53f8a715b5a5d245aa09c358da1a727a3d1f6"},"cell_type":"code","source":"from string import punctuation\n\nstop_words = set(stopwords.words('english')) \nstop_words = stop_words.union(set(punctuation)) \nstop_words.update([\"\\'s\", \"n't\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb7f3f248a2c7ee080723c9025cdd82c0b6e43bc"},"cell_type":"code","source":" descriptions = df['description']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a537cf00426f277ef4c5ba07d0db0f74cd94d141"},"cell_type":"markdown","source":"This time, need to save both the lemmatized tokens as a list and as a single document. For input to different algorithms later. But first the lemmatization, later building the second version.."},{"metadata":{"trusted":true,"_uuid":"7a47515e2de2bb0388014f983eb9026c8056ae69"},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ntext_tokens = [[lemmatizer.lemmatize(word) for word in word_tokenize(description.lower()) if word not in stop_words] for description in descriptions]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e15067d3eb896c757fbadc90d9b65db531442979"},"cell_type":"markdown","source":"To add the bi-grams and tri-grams to the data:"},{"metadata":{"trusted":true,"_uuid":"dcde9afde293579b00dc316e0cbd456adac3ac8e"},"cell_type":"code","source":"import gensim\n#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n# Build the bigram and trigram models\nbigram = gensim.models.Phrases(text_tokens, min_count=5, threshold=100)\ntrigram = gensim.models.Phrases(bigram[text_tokens], threshold=100)\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram) \ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b48ea7a6154d8350bbc5e9887375b74a88ab319"},"cell_type":"code","source":"print(trigram_mod[bigram_mod[text_tokens[4]]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a511f4a18b3aa1a77bd5852f61c5ecdeeb01244"},"cell_type":"code","source":"print(descriptions[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"942ded146f7105dd4e11ef6c47b6ba8425f1f4d8"},"cell_type":"code","source":"text_tokens = [trigram_mod[bigram_mod[text]] for text in text_tokens]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b9ccdc3572f60128e11d359f65559fb6773a385"},"cell_type":"code","source":" print(text_tokens[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16677dbe645ed4280a7c2c88e4d8bcc9f4206cc4"},"cell_type":"code","source":"texts = [\" \".join(tokens) for tokens in text_tokens] \ndf[\"description2\"] = texts \ndf[\"description2_tokens\"] = text_tokens\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d01200746dd96b9dc1fa7ca7e0659d6d7e786034"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectPercentile, f_classif \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8c558aac6dac4ecd319515a64d05113dbb628eb"},"cell_type":"code","source":" df[\"country\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62e8b1e3707f7d57cf2961bacd417f18e74a0cd0"},"cell_type":"markdown","source":"There is a pretty big class imbalance here. If I want to classify documents into different countries, the US has way more than most other. But counting together the European countries, it seems to come quite close in size. Since my goal was to try to build a binary classifier, that seems like a reasonable start.\n\nSo I decided to group all European countries to a single continent. Found a list of countries and their metadata at: [Github](https://gist.github.com/pamelafox/986163)\n\nThis is what the list looks like after I parsed all the countries with continent=\"Europe\":"},{"metadata":{"trusted":true,"_uuid":"b0a7c6f6e88fb1ea1c0b345a31bb0ca346a8f22d"},"cell_type":"code","source":"europe = ['Andorra', 'Albania', 'Austria', 'Belgium', 'Bulgaria', 'Belarus', \n          'Czech Republic', 'Germany', 'Denmark', 'Estonia', 'Finland', \n          'France', 'Greece', 'Hungary', 'Republic of Ireland', 'Iceland', \n          'Italy', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Latvia', \n          'Macedonia', 'Malta', 'Kingdom of the Netherlands', 'Norway', \n          'Poland', 'Portugal', 'Romania', 'Russia', 'Sweden', 'Slovenia', \n          'Slovakia', 'San Marino', 'Ukraine', 'Vatican City', \n          'Bosnia and Herzegovina', 'Croatia', 'Moldova', 'Monaco', \n          'Montenegro', 'Serbia', 'Spain', 'Switzerland', 'United Kingdom']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71e9787b06e1ad422c2274de57f47f7fe9472943"},"cell_type":"markdown","source":"Just for the kicks, Asia using similar approach with same Github country lists:"},{"metadata":{"trusted":true,"_uuid":"83a37fd1b3069d7be9de64f39143e23ae9b5e837"},"cell_type":"code","source":"asia = ['Afghanistan', 'Armenia', 'Azerbaijan', 'Bangladesh', 'Bahrain', \n        'Brunei Darussalam', 'Bhutan', \"People's Republic of China\", \n        'Cyprus', 'Georgia', 'Indonesia', 'Israel', 'India', 'Iraq', \n        'Iran', 'Jordan', 'Japan', 'Kyrgyzstan', 'North Korea', \n        'South Korea', 'Kuwait', 'Lebanon', 'Myanmar', 'Mongolia', \n        'Maldives', 'Malaysia', 'Nepal', 'Oman', 'Philippines', \n        'Pakistan', 'Qatar', 'Saudi Arabia', 'Singapore', 'Syria', \n        'Thailand', 'Tajikistan', 'Turkmenistan', 'Turkey', \n        'Uzbekistan', 'Vietnam', 'Yemen', 'Cambodia', 'East Timor', \n        'Kazakhstan', 'Laos', 'Sri Lanka', 'United Arab Emirates']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddc952b2200e5ba17b21b2f3d683e6f3cbbaa80c"},"cell_type":"markdown","source":"Now that I have a list of European countries, I can create a new column in my dataframe to label European countries as \"European\" continent, and US countries as \"US\" continent. While I am at it, do Asia as well... First the function to do the labeling:"},{"metadata":{"trusted":true,"_uuid":"a1505f831c50fc8958d870bd4ddb85a05d33ba03"},"cell_type":"code","source":"#https://stackoverflow.com/questions/26886653/pandas-create-new-column-based-on-values-from-other-columns\ndef country_to_continent(row): \n    if row[\"country\"] == \"US\":\n        return \"US\"\n    if row[\"country\"] in europe:\n        return \"Europe\"\n    if row[\"country\"] in asia:\n        return \"Asia\" \n    return \"Other\"\n\n    #and to apply it\ndf[\"continent\"] = df.apply(lambda row: country_to_continent(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"277db8bbe2ad3e735c3a015df2ff834f5249c355"},"cell_type":"code","source":"df[\"continent\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3f71b9f0931440b6b5072237a848fd7c10100e9"},"cell_type":"markdown","source":"So now I have a much more equal spread of US vs EU countries and datapoints. Should make for better training data. Now to filter myself a new dataframe with only these datapoints."},{"metadata":{"trusted":true,"_uuid":"d13e4db0355f8c86c2901b7e7dbe35516742213a"},"cell_type":"code","source":"#https://stackoverflow.com/questions/11869910/pandas-filter-rows-of-dataframe-with-operator-chaining\nus_eu = df[(df.continent == \"US\") | (df.continent == \"Europe\")] \nlen(us_eu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c02b37fba9d41fa00c9bd14c1aad7cc5c417d61e"},"cell_type":"markdown","source":"As visible above, I now have total of about 106k entries still. Should make a decent training/test set. Quick look:"},{"metadata":{"trusted":true,"_uuid":"c26737cf9843ddc3ecefa15c8cecef48f93d98e9"},"cell_type":"code","source":" us_eu.head(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bee95dbd7ac1f0852c02967608d96233a65d74d2"},"cell_type":"markdown","source":"Now to turn the \"continent\" values into integer labels for the algorithm:"},{"metadata":{"trusted":true,"_uuid":"06a659d9ca2862fda4a085b37471ab60dfdd58a4"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# encode class values as integers\nencoder = LabelEncoder()\nencoded_continent = encoder.fit_transform(us_eu.continent)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"031fa5b6fc83500e21b649d27bcbe9d6c1a47680"},"cell_type":"markdown","source":"And to check what we get. It is label 0 = EU, 1 = US, there are still the correct number of 55724 rows for EU and 50448 for US."},{"metadata":{"trusted":true,"_uuid":"faba8e27e52e6c1974b4a2d1243eab4463a3f221"},"cell_type":"code","source":"import numpy as np\n\nunique, counts = np.unique(encoded_continent, return_counts=True) \nprint(unique)\nprint(counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5ed33548af3b761b924b148f8b1c70b262a1db6"},"cell_type":"code","source":" descriptions2 = us_eu['description2']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a782869503e81a3302d8943bf1683254f8304ea9"},"cell_type":"markdown","source":"I use the \"descriptions2\" column as the features for the learning algorithms. The encoded_continent values are the target labels to predict."},{"metadata":{"trusted":true,"_uuid":"4d30de11da73d94d5bfe313cba20e70c7a700285"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfeatures_train, features_test, labels_train, labels_test = train_test_split(descriptions2, encoded_continent, test_size=0.2, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a68d47253d4922e567f89f27c94edf7de210f8bc"},"cell_type":"code","source":" print(features_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fed8753b83263403dcc0f5a0f5aeeeae2d6377e"},"cell_type":"markdown","source":"Now to turn the preprocessed word vectors into TFIDF word vectors for actual features.\n"},{"metadata":{"trusted":true,"_uuid":"d2484722aa2e3a832eae1dacadadf2bb2aebe5c2"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5) \nfeatures_train_transformed = vectorizer.fit_transform(features_train) \nfeatures_test_transformed = vectorizer.transform(features_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74a2c18e186b0d617d8dec101a3b5dafb0da8a31"},"cell_type":"markdown","source":"The feature names from the vectorizer are the actual words (tokens) that the TFIDF vectorizer identified and processed. So the below maps index in the list to a value in the feature vector created above. Will be more clear further down this notebook. In any case, the printout below shows how the data could still use a lot of cleaning. But not always necessary, so let's see first."},{"metadata":{"trusted":true,"_uuid":"99ea1cb76b67267d8b3b3e215ba2989f3138781d"},"cell_type":"code","source":" vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b1e38950231d0df4dd9a0feae1a80c5a35f31a7"},"cell_type":"markdown","source":"So the transformation below illustrates the token mapping from above feature names. For example, the first number in the tuple (0, 23683) refers to the first document (index 0 in the document list). The second number of (0, 23683) refers to the word at index 23683 in the feature names list above. The number following is the TFIDF score of the word for that document."},{"metadata":{"trusted":true,"_uuid":"590f8f1e7e71b59db3447fe349ff962f994a78d6"},"cell_type":"code","source":"print(features_test_transformed[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d954565071d5dfc7782a943b8ef724ef9070df3"},"cell_type":"code","source":"feature_names = np.array(vectorizer.get_feature_names()) \nprint(len(feature_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3504067cc6d92142e41d225e21a3541ccdad061a"},"cell_type":"code","source":"def top_tfidf_feats(row, features, top_n=25):\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"206036cf2c17c328f6229f5ecfdd35ab1da8b6e6"},"cell_type":"code","source":"#this prints it for the first document in the set\narr = features_test_transformed[0].toarray() \ntop_tfidf_feats(arr[0], feature_names)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9765b38e8a882466adc9643ea1e0db8477449cac"},"cell_type":"markdown","source":"Now, lets try to train some classifiers. MultinomialNB seems to be somewhat popular for this type of task (yes, I just Googled it...):"},{"metadata":{"trusted":true,"_uuid":"9da9092c3f5672bdcfd52ff795f9b48724bc16f3"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB() \nclf.fit(features_train_transformed, labels_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d74adddbe77d1bc6b65b384fce057fdc112c3d5"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_pred = clf.predict(features_test_transformed) \ny_true = labels_test\naccuracy_score(y_true, y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae447af325f09b3acd83ee9c56df97e5363fd5ae"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#print(confusion_matrix(y_true, y_pred)\nprint(classification_report(y_true, y_pred, target_names=[\"EU\", \"US\"]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"779d61472ea038b07f57e505601441ba4029ab70"},"cell_type":"markdown","source":"Can we see what the algorithm finds to be the most predictive features (the ones most contributing to predictions)?"},{"metadata":{"_uuid":"b130e1fc8bed748b78b6f6a716e9b3bc2d47647c"},"cell_type":"markdown","source":"\n[StackOverflow](https://stackoverflow.com/questions/11116697/how-to-get-most-informative- features-for-scikit-learn-classifiers) to the rescue: \n        "},{"metadata":{"trusted":true,"_uuid":"50eaa91547862b15b4e0a6d4bbfc5fde2d391ea4"},"cell_type":"code","source":"def show_most_informative_features(vectorizer, clf, n=20): \n    feature_names = vectorizer.get_feature_names() \n    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names)) \n    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1]) \n    for (coef_1, fn_1), (coef_2, fn_2) in top:\n        print (\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afd19fcef79370248e1f0276748386dfc4a5876f"},"cell_type":"code","source":"print(len(y_pred))\nprint(len(us_eu))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dea3fc4773e7335c6038cafa1b2ce7159e2efefe"},"cell_type":"code","source":" features_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9261e6a4d91de7e22a764faf34924436b1c18eb7"},"cell_type":"markdown","source":"Oddly, the describe() function still claims there are duplicates. Maybe the wording is just a bit different and the lemmatizations + stopword removal does it? Seem like a complex string to just randomly repeat though.."},{"metadata":{"trusted":true,"_uuid":"d0e6db57779099918f71cfd2f9bca8606e7e8569"},"cell_type":"code","source":" features_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a918202bf61f60be4f6b5cb547436f5b71b0a83e"},"cell_type":"code","source":" show_most_informative_features(vectorizer, clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05bb0ba9f20a16864eb6cf8ea7d1fbc737debc41"},"cell_type":"code","source":" len(features_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0a21104973a417f0662c2a22193619b2d973ba6"},"cell_type":"markdown","source":"So lets take a look at the data. What do the predictions look like, and what do the associated descriptions in english and in preprocessed form look like?"},{"metadata":{"trusted":true,"_uuid":"654e7377da6da1cc6f1e0471ec1cd5afbdb66e20"},"cell_type":"code","source":"last_df = pd.DataFrame()\n\n#last_df['feat_descs'] = Series(features_test, index=df1.index)\n\ncount = 0\ncountries = [] \ncountries_pred = [] \ndescriptions = [] \ndescriptions2 = []\n\nfor idx, value in features_test.iteritems(): \n    countries.append(us_eu.get_value(idx, \"country\")) \n    countries_pred.append(y_pred[count]) \n    descriptions.append(us_eu.get_value(idx, \"description\")) \n    descriptions2.append(value)\n    count += 1\nlast_df[\"country\"] = countries \nlast_df[\"country_pred\"] = countries_pred \nlast_df[\"desc\"] = descriptions \nlast_df[\"desc2\"] = descriptions2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a16e4eb5191221387b7aaa11a7b24ba1657c630"},"cell_type":"code","source":" last_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c787b7db65b8ad18c4e89fef8bc5c164f0eb4f46"},"cell_type":"markdown","source":"The above table shows how the US reviews are consistently classified as US (=1) and EU countries as EU (=0). Cannot directly see anything specific about the text that makes the classifier do so. But lets see. I try a few different classifiers as well, just to see how well they generally do:"},{"metadata":{"trusted":true,"_uuid":"228730f0e3a72761b77609682629e1465a647f87"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nestimators = [10, 20, 30, 40, 50] \nmin_splits = [5, 10, 20, 30, 40, 50] \nmin_leafs = [1, 2, 5, 10, 20, 30]\n\nbest_acc = 0\nbest_rf = None\nfor estimator in estimators:\n    for min_split in min_splits: \n        for min_leaf in min_leafs:\n            print(\"estimators=\", estimator, \"min_split=\", min_split, \" min_leaf=\", min_leaf)\n            clf = RandomForestClassifier(n_estimators=estimator, min_samples_leaf=min_leaf, min_samples_split=min_split)\n            clf.fit(features_train_transformed, labels_train) \n            pred = clf.predict(features_test_transformed) \n            accuracy = accuracy_score(labels_test, pred) \n            print(accuracy)\n            if accuracy > best_acc:\n                best_acc = accuracy\n                best_rf = clf\n                print(\"found better:\", best_acc, \", \", best_rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"523e83593c15836901662b76952e356cccf08eb4"},"cell_type":"markdown","source":"\nYes, grid search with the sklearn methods would serve nicely. But I like to get some control on what I measure and print from that. Anyway.."},{"metadata":{"trusted":true,"_uuid":"a067f3f7aca85c4d5bb11a1c9eca5ac7146c8411"},"cell_type":"code","source":"print(\"best:\")\nprint(best_acc)\nprint(best_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b4dc3b80fa820d271e4ec96ac5515fe79f4d664"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_pred = best_rf.predict(features_test_transformed) \ny_true = labels_test\naccuracy_score(y_true, y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28adcf5b50237b1e6c66349952169a51518380cf"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#print(confusion_matrix(y_true, y_pred)\nprint(classification_report(y_true, y_pred, target_names=[\"EU\", \"US\"]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40e7875c3be36b4afa3c1ca5dd76931446693a60"},"cell_type":"markdown","source":"The results seems quite similar for Random Forest. KNN next:"},{"metadata":{"trusted":true,"_uuid":"ee725a5ef8f77171f2a8f00e404596c809138361"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nneighbour_items = [2, 5, 10, 15, 20, 25, 30]\n\nfor n in neighbour_items:\n    print(\"running with n=\", n)\n    clf = KNeighborsClassifier(n_neighbors=n, weights=\"distance\") \n    clf.fit(features_train_transformed, labels_train)\n    \n    pred = clf.predict(features_test_transformed) \n    \n    accuracy = accuracy_score(labels_test, pred) \n    \n    print(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f8e221282ab20dc4264bd9b411d0850227feb9d"},"cell_type":"markdown","source":"Not bad. Finally, the grand SVM of traditional ML algos. Optimize C param first.:"},{"metadata":{"trusted":true,"_uuid":"7b076fc2b974c52d85155345ab1665ce7f54c699"},"cell_type":"code","source":"from time import time\nfrom sklearn import svm\nfrom sklearn.metrics import precision_score \nfrom sklearn.metrics import recall_score\n\n#tried with smaller values, not good\nc_list = [500, 1000, 10000, 20000]\n\nbest_c = None \nbest_accuracy = 0 \n\nfor C in c_list:\n    print(\"training with C=\"+str(C)) \n    clf = svm.SVC(kernel='rbf', C=C)\n\n    t0 = time()\n\n    end = int(features_train_transformed.shape[0]/10)\n    clf.fit(features_train_transformed[:end], labels_train[:end]) \n    \n    print(\"training time:\", str(round(time()-t0, 3)), \"s\")\n\n    t0 = time()\n\n    pred = clf.predict(features_test_transformed) \n    print(\"prediction time:\", str(round(time()-t0, 3)), \"s\")\n\n    accuracy = clf.score(features_test_transformed, labels_test) \n    print(\"accuracy:\"+str(accuracy)) \n    print(\"precision:\"+str(precision_score(labels_test, pred))) \n    print(\"recall:\"+str(recall_score(labels_test, pred)))\n\n    if accuracy > best_accuracy: \n        best_accuracy = accuracy \n        best_c = C\n\nprint(\"best:\"+str(best_accuracy)+\" with \"+str(best_c))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c65a2f7ab73a9980b8bdeefdfa29a85700cd2a54"},"cell_type":"markdown","source":"Then gamma param:"},{"metadata":{"trusted":true,"_uuid":"f1b7f9f608e3ade575733370f88db9bae94bd4a7"},"cell_type":"code","source":"from time import time\nfrom sklearn import svm\nfrom sklearn.metrics import precision_score \nfrom sklearn.metrics import recall_score\n\n#tried with smaller values, not good\ngamma_list = [0.001, 0.01, 0.1, 1]\n\nbest_gamma = None \nbest_accuracy = 0\n\nfor gamma in gamma_list:\n    print(\"training with gamma=\"+str(gamma))\n    clf = svm.SVC(kernel='rbf', C=best_c, gamma=gamma)\n\n    t0 = time()\n\n    end = int(features_train_transformed.shape[0]/10)\n    clf.fit(features_train_transformed[:end], labels_train[:end]) \n    \n    print(\"training time:\", str(round(time()-t0, 3)), \"s\")\n\n    t0 = time()\n\n    pred = clf.predict(features_test_transformed) \n    \n    print(\"prediction time:\", str(round(time()-t0, 3)), \"s\")\n\n    accuracy = clf.score(features_test_transformed, labels_test) \n    print(\"accuracy:\"+str(accuracy)) \n    print(\"precision:\"+str(precision_score(labels_test, pred))) \n    print(\"recall:\"+str(recall_score(labels_test, pred)))\n\n    if accuracy > best_accuracy: \n        best_accuracy = accuracy \n        best_gamma = gamma\n\nprint(\"best:\"+str(best_accuracy)+\" with \"+str(best_gamma))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b32c204dc3ae1320e395bd40d960385e11dcbc8f"},"cell_type":"markdown","source":"\nAnd now for something (completely) different. Let's explore EU vs US texts as a whole to see what are their defining characteristics. Maybe that tells us something about why the classifiers seem to be doing good?"},{"metadata":{"trusted":true,"_uuid":"1fcafa16b868cafb769d396098eaf533d176828a"},"cell_type":"code","source":"europe_df = df[df[\"continent\"] == \"Europe\"] \nprint(len(europe_df))\nus_df = df[df[\"continent\"] == \"US\"] \nprint(len(us_df))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c18adaaf81abc6e321dd71ad8be9537950e3a08"},"cell_type":"markdown","source":"First, lets collect all EU wine reviews into a single long document."},{"metadata":{"trusted":true,"_uuid":"7c48f5cdbe66d3172079e6168125055e8c731f8c"},"cell_type":"code","source":"eu_descriptions2 = \"\"\nfor index, row in europe_df.iterrows():\n    eu_descriptions2 += \" \"+row[\"description2\"] \nprint(len(eu_descriptions2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86b85df6f6b6d152c8139d8773c28448cb93a867"},"cell_type":"markdown","source":"And the same for US:"},{"metadata":{"trusted":true,"_uuid":"7de0179a7427cc8a984131c1a2eb86d2e803bc39"},"cell_type":"code","source":"us_descriptions2 = \"\"\nfor index, row in us_df.iterrows():\n    us_descriptions2 += \" \"+row[\"description2\"] \nprint(len(us_descriptions2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec41b7179d27e2bfc0b3c220d12e68a59fe80764"},"cell_type":"markdown","source":"So what does TFIDF do? It weights words in a document so that higher weight words occur in that document relatively often, but much more rarely across the whole document set. So highly weighted words should be important to that document, and much less frequent in other documents. In other words, the words ranked high by TFIDF can be expected to be descriptive of that document.\n\nTo find descriptive words in EU vs US wine reviews, I now create a new document set with just two documents. One with all EU wine reviews, and another with all US wine reviews. What TFIDF weights high in the EU set should then be descriptive of the EU wines, and what it weights high in US set should be descriptive of the US wines."},{"metadata":{"trusted":true,"_uuid":"51be28ce885d800747e4f7dd298180a9db54bf17"},"cell_type":"code","source":" country_docs = [eu_descriptions2, us_descriptions2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41f1a4c58c4cc39379350fa29a7d88f58e43822b"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5) \ncountry_docs_transformed = vectorizer.fit_transform(country_docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11a0b59415c466534293082cb3ada3e50050c63d"},"cell_type":"code","source":"#https://buhrmann.github.io/tfidf-analysis.html\nfeature_names = np.array(vectorizer.get_feature_names()) \nprint(len(feature_names))\narr = country_docs_transformed[0].toarray() #print(arr[0])\neu_top_df = top_tfidf_feats(arr[0], feature_names) \nprint(eu_top_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f687245d8252c5f132b1d17c3b91c7af6c6af63"},"cell_type":"markdown","source":"What do these mean? I have no idea but Google works:\n\nbarolo = [Italian wine](https://en.wikipedia.org/wiki/Barolo)\n\ngrabby = Oddly this actually seems to be a word used to describe wine taste (as I was hoping to find)\n\nperlage = [wine from Venice (Italy)](http://www.perlagewines.com/en/)\n\nlate 2017 = not even going to Google this, seems to refer to wine from a specific time. why in Europe? no idea \n\nbread crust = huh? seems also to be a wording used to describe some wine taste. which is nice.\n\nchoppy = ? couldn't quite figure this out\n\npinot nero = [Italian name for some wine](http://www.winegeeks.com/grapes/309)\n"},{"metadata":{"trusted":true,"_uuid":"aaff5e7b0854289d59f99d8deca290644a347f0d"},"cell_type":"code","source":"arr = country_docs_transformed[1].toarray() \nus_top_df = top_tfidf_feats(arr[0], feature_names) \nprint(us_top_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"838a95d4fc4ce8bebef3173ff5f5f683fa7398d5"},"cell_type":"markdown","source":"Again, what do these mean? Again, I have no idea but Google still works:\n\npetite sirah = .. [growing mainly in California](http://winefolly.com/review/petite-sirah-wine-guide/)\n\ncreek = there seem to be many related to wine but if you limit to US and EU, California comes up again\n\ncarneros = [some kind of US wine alliance](https://www.carneros.com/)\n\npaso roble = [more winemaking in California](https://pasowine.com/)\n\nava = [wine growing region in the US](https://en.wikipedia.org/wiki/American_Viticultural_Area)\n\n... and so on"},{"metadata":{"_uuid":"36bc0a65750ad16e09e4b9501437ec3609942a2f"},"cell_type":"markdown","source":"So what is the summary on this? The classifier seems to work reasonably well. Looking at the TFIDF highlighted words in the end, there seem to be a number of words in there that might give the classifier an advantage. Like names of wines from specific countries or regions. Or directly the names of the regions themselves. I believe with plenty of filtering and reviewing the different results, it would be possible to get much more insights into these topics. Not sure what the classifier would be useful for in itself but perhaps as a process to learn something from the data?"},{"metadata":{"trusted":true,"_uuid":"a90fafc4360a9180998bf33febb56b218b8c6bee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"216c0d1c3cbdb2d9da7791b35b5b7f227c6b4c10"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39ddab122c999d9c63d3057bf88272e2ce7f8dd4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b1a03b0617a1b0adbf8295364dc6f97400c0b7e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3f7938a51337cfbfd9ab0a3fb495b17d95b3d69"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}