{"cells":[{"metadata":{"_uuid":"089876c2d029abb38330ccbdf33545ae031c39e5","_cell_guid":"96b710c9-7a89-4b90-a119-6ff2724aa971"},"cell_type":"markdown","source":"### What about this kernel?: Understand how to bulild, train and test a CNN for text classification with pretrained word2vec weights.\n* Pre-trained word2vec model: [GoogleNews-vectors-negative300](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)\n* Dataset (train+test): [Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification)\n* Model architecture: The same as in [the Yoon Kim paper]( https://arxiv.org/pdf/1408.5882.pdf). \n"},{"metadata":{"_uuid":"6d3451f7604380e4c707cc70982b7fb98617dfff"},"cell_type":"markdown","source":"### Outline\n1. Python imports for coding\n1. Global variables\n1. Functions for plotting and visualize data\n1. Preprocessing functions\n1. Functions to cover the word2vec workaround\n1. Functions for model management\n1. Running all"},{"metadata":{"_uuid":"405a47583d28d8f27b2cec4c78847be763b5c5e1"},"cell_type":"markdown","source":"### 1. Python imports for coding"},{"metadata":{"_uuid":"78bb9ac640d6625586229d046c67a7c2461c5242","_cell_guid":"fd1f69c4-82bc-4d9f-9e8d-943100b19382","trusted":true},"cell_type":"code","source":"#General imports to work\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport logging as log # local library for processing logs along the sourcecode\n\n# imports for plot_history_model\nimport matplotlib.pyplot as plt\n\n#data preprocesing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\n#imports for word2vec model\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom keras.layers import Embedding\n\n#imports for CNN model\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\nfrom keras.layers.core import Reshape, Flatten\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import regularizers\n\n#Imports to support keras GPU\nfrom keras import backend as K\nimport tensorflow as tf\nimport multiprocessing\n\n#Imports for ploting models\nfrom IPython.display import SVG\nfrom tensorflow.python.keras.utils.vis_utils import model_to_dot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21a04e7f21c69a985d50292c34ecd9ca0fddf9a3"},"cell_type":"markdown","source":"### 2. Global variables"},{"metadata":{"_uuid":"84556514b1f02ead3472578ae471e5fef0761f09","_cell_guid":"50771083-1694-4319-be99-2d41b83ac492","trusted":true},"cell_type":"code","source":"#several configurations\nNUM_WORDS=20000 #max unique words in vocabulary\nEMBEDDING_DIM=300 #max neurons for hidden layer in word2vec\n\n#Log format and config\nlog_format = '%(asctime)s [%(levelname)s] %(message)s'\ndate_format = '%m-%d-%y %H:%M'\nlog.basicConfig(level=log.INFO, format=log_format, datefmt=date_format)\n\n#Set your dataset: https://www.kaggle.com/c/spooky-author-identification\ntraining_dataset = '../input/spooky-author-identification/train.csv'\ntesting_dataset = '../input/spooky-author-identification/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21287fbb7cafecb8fa8e2fc1c7376283379b00b9"},"cell_type":"markdown","source":"### 3. Functions for plotting and visualize data"},{"metadata":{"trusted":true,"_uuid":"cfd8e040e87dfbf2ab2f62fc6059fbe8d234c1d3"},"cell_type":"code","source":"#show data info\ndef get_data_info3D(data,show_cols={1,2}):\n    df = pd.DataFrame()\n    keys = train_data.keys()\n    for i in show_cols:\n        if i == len(keys)-1:      \n            df.insert(loc=len(df.keys()),column=keys[i]+\" (count)\",value=[len([data.get(keys[i]).unique()][0])])\n            df.insert(loc=len(df.keys()),column=keys[i]+\" (list)\",value=[([data.get(keys[i]).unique()][0])])\n        else:\n           df.insert(loc=len(df.keys()),column=keys[i],value=[data.get(keys[i]).count()])  \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a2cad3df80fd0a9eba57c8ba226fc5215c9874b"},"cell_type":"code","source":"#https://keras.io/visualization/\n#Plot history model results once the fit process has been concluded\ndef plot_history_model_of_training_process(history):\n    #Plot training & validation accuracy values\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00421aa859a5cf9e922288358682c1fb8a824b1c"},"cell_type":"markdown","source":"### 4. Preprocessing functions"},{"metadata":{"_uuid":"c48fed101812eaea561e1333a1c9158cd73ae128","_cell_guid":"6951f4bb-1519-4683-aa03-15789ad76668","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def preprocesing_data(NUM_WORDS,train_data):\n    #Ok, seems clean, lets make categories of authors\n    authors=train_data.author.unique()\n    dic={}\n    for i,author in enumerate(authors):\n        dic[author]=i\n    labels=train_data.author.apply(lambda x:dic[x])\n    \n    #NumPy-style indexing doesn't work on a Pandas DataFrame; use\n    #Lets divide our training data in train and validation\n    val_data=train_data.sample(frac=0.2,random_state=200)\n    train_data=train_data.drop(val_data.index)\n\n    #Tokenizing training data text with keras text preprocessing functions\n    texts=train_data.text\n    tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n    tokenizer.fit_on_texts(texts)\n    sequences_train = tokenizer.texts_to_sequences(texts)\n    sequences_valid=tokenizer.texts_to_sequences(val_data.text)\n    word_index = tokenizer.word_index\n    log.info('Found {0} unique tokens.'.format(len(word_index)))\n\n    X_train = pad_sequences(sequences_train)\n    X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n    y_train = to_categorical(np.asarray(labels[train_data.index]))\n    y_val = to_categorical(np.asarray(labels[val_data.index]))\n    log.info('Shape of train: {0} and validation tensor: {1} '.format(X_train.shape,X_val.shape))\n    log.info('Shape of label train: {0} and validation tensor: {1}'.format(y_train.shape,y_val.shape))\n    return tokenizer,dic,word_index,X_train,X_val,y_train,y_val","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7df6473b83620f055e7ef090b91d967e9cd63618","_cell_guid":"2fb012c6-8166-4124-b8dd-f8c69f289e9b"},"cell_type":"markdown","source":"### 5. Functions to cover the word2vec workaround\nNow its time for word embedding. To perform this, we are going to use a pretrained word2vec model, just choice your best way: \n1.  Just pick on \"+ Add Data\" button and look for \"Google news vector\". it will be attached to your current kernel, dont worry about not enough space. \n2.  Download the dataset directly from the google storage: [link_to_google_storage](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). Then just upload it to your kaggle environment (use again the \"+ Add Data\" button on the right side."},{"metadata":{"_uuid":"6478a820872e903b7d5a3a8b0b3a543c773eb926","_cell_guid":"b5b196d3-a98e-4eb4-b731-c3f0650567ed","trusted":true},"cell_type":"code","source":"def load_pretrained_word2vec(word_index,NUM_WORDS):\n    word_vectors = KeyedVectors.load_word2vec_format('../input/gnewsvector/GoogleNews-vectors-negative300.bin', binary=True)\n    vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n    embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        if i>=NUM_WORDS:\n            continue\n        try:\n            embedding_vector = word_vectors[word]\n            embedding_matrix[i] = embedding_vector\n        except KeyError:\n            embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n    del(word_vectors)\n    embedding_layer = Embedding(vocabulary_size,EMBEDDING_DIM,weights=[embedding_matrix],trainable=True)\n    return embedding_layer\n#TODO: Code fine tunning\n#TODO: Code from scratch\n#Case in which we dont have pretraining word vectors. \n#from keras.layers import Embedding\n#EMBEDDING_DIM=300\n#vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n\n#embedding_layer = Embedding(vocabulary_size,EMBEDDING_DIM)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31d0f9d6bd1303300fc8c0a44826ece11ccf8d44"},"cell_type":"markdown","source":"### 6. Functions for model management"},{"metadata":{"trusted":true,"_uuid":"6de1c0deeb8ab332d41255e4b8bd855760073a33"},"cell_type":"code","source":"#Adding support to select between CPU or GPU on Keras backend\n#Modes: use_gpu=0 (just use CPU), use_gpu=1 (use gpu when its prossible)\ndef set_hardware_backend(use_gpu=0):\n    #setting max CPU cores\n    max_cpu_cores = multiprocessing.cpu_count()\n    #configuring tensorflow protocol to choice between CPU|GPU, burning the hardware!!!\n    config = tf.ConfigProto(intra_op_parallelism_threads=max_cpu_cores,\n                            inter_op_parallelism_threads=max_cpu_cores, \n                            allow_soft_placement=True,\n                            device_count = {'GPU': use_gpu, 'CPU': max_cpu_cores})\n    #Only for shared GPU's: allocates the GPU memory dynamically instead of all at time, \n    config.gpu_options.allow_growth=True \n    sess = tf.Session(graph=tf.get_default_graph(),config=config) \n    K.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25ef8f3c820d342ede0d10062dc47300305ee8d1"},"cell_type":"code","source":"def create_model(X_train,EMBEDDING_DIM,embedding_layer):\n    #Hyperparameters\n    filter_sizes = [3,4,5] #defined convs regions\n    num_filters = 100 #num_filters per conv region\n    drop = 0.5\n\n    sequence_length = X_train.shape[1]\n    inputs = Input(shape=(sequence_length,))\n    embedding = embedding_layer(inputs)\n    reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n\n    conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n    conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n    conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n\n    maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n    maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n    maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n\n    merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n    flatten = Flatten()(merged_tensor)\n    reshape = Reshape((3*num_filters,))(flatten)\n    dropout = Dropout(drop)(flatten)\n    output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n\n    # this creates a model that includes\n    model = Model(inputs, output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10b370cbbffd95fd6efe0e90c9e6857d62cedc00"},"cell_type":"code","source":"#We have to implement SGD-Adadelta-udpate-rule (Zeiler, 2012)\n#Right now we are using ADAM instead of SGD\ndef training_model(model,X_train,y_train,X_val,y_val):\n    #Learning rate\n    adam = Adam(lr=1e-3)\n    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n    callbacks = [EarlyStopping(monitor='val_loss')]\n    # starts training\n    history = model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val), callbacks=callbacks)\n    return history","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1ccc147faeb6eb331be53d3d70489cb24b5d294","_cell_guid":"aab8ced3-1224-4155-9d3d-f7a33e9575dc","trusted":true},"cell_type":"code","source":"def testing_model(test_data,X_train,tokenizer):\n    #tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True) #extract this function from here\n    #there is a problem in this tokenizer because it should be the same used in training dataset, arrange it tomorrow!!! \n    sequences_test=tokenizer.texts_to_sequences(test_data.text) #the same here, this is preprocessing\n    X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1]) #Equalizes X_test texts length to X_train\n    y_pred=model.predict(X_test)\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"accbdb2e3f4cbd40d072a53fb5b7abb717c4c4ae"},"cell_type":"markdown","source":"### 7. Running all\n"},{"metadata":{"trusted":true,"_uuid":"772a19133d9dbc07f9c4ab91a67fa2d8fb16c71c"},"cell_type":"code","source":"log.info(\"Loading datasets...\")\ntrain_data=pd.read_csv(training_dataset)\ntest_data=pd.read_csv(testing_dataset)\n\n#log.info(\"Showing datasets info...\")\n#print(get_data_info3D(train_data))\n#print(get_data_info3D(test_data,{1}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2c9fbe7cf875f5c49ccc465fc61fbeb211953d5"},"cell_type":"code","source":"log.info(\"Preprocesing input...\")\ntokenizer,dic,word_index,X_train,X_val,y_train,y_val = preprocesing_data(NUM_WORDS,train_data)\nembedding_layer = load_pretrained_word2vec(word_index,NUM_WORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0976c368728afb507d7bc3a6252190f924221ed7"},"cell_type":"code","source":"log.info(\"Creating model...\")\nset_hardware_backend(use_gpu=1) #we set the backend to GPU to descrease the training time\nmodel = create_model(X_train,EMBEDDING_DIM,embedding_layer)\nmodel.summary() #showing layers, parameteres and connections\nSVG(model_to_dot(model).create(prog='dot', format='svg')) #plotting the model graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61c4b7b43da7005f8ddfacdb22f8b96c23d781a5"},"cell_type":"code","source":"log.info(\"training model... it will take a while.\")\nhistory = training_model(model,X_train,y_train,X_val,y_val)\nplot_history_model_of_training_process(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e488fe683412229a747db5950e38a9fcf42d06ce"},"cell_type":"code","source":"log.info(\"testing model...\")\ny_pred = testing_model(test_data,X_train,tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57402e363150b39eea3af18f97e7ad73594a40ac","_cell_guid":"8fd503ca-579a-499d-a3d9-3e1f52fb380d","trusted":true},"cell_type":"code","source":"#TODO: Good idea to have the confusion matrix at the end: https://scikit-learn.org/stable/ and pyimagesearch (some example)\nto_submit=pd.DataFrame(index=test_data.id,data={'EAP':y_pred[:,dic['EAP']],\n                                                'HPL':y_pred[:,dic['HPL']],\n                                                'MWS':y_pred[:,dic['MWS']]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da218d48054d2fa0d6c67cd77a1dba36c1497e2c","_cell_guid":"68af55f4-eafc-4b2f-b327-34fb6d21e6be","trusted":true},"cell_type":"code","source":"to_submit.to_csv('../input/submit.csv')\nto_submit.head(5) #showing results over all testing dataset. Most higher values in each category means caterogized as it is. \n                  #TODO: get class colum for testing dataset in order to create the confusion matrix plus metrics","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}