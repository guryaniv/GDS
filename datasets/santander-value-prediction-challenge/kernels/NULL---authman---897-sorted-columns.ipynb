{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Guys (and ladied)—I don't even... I'm so confused by this competition. I just want this messy pre-competition nightmere to be over so we can actually start on some DS/ML/(AI?), rather than doing BFS / DFS / A\\* / GA in an attempt to clean up the artificially delapitated dset provided to us. You feel me?\n\nIn this kernel, we attempt to reconstruct the order of as many columns as possible, and verify the order using the target. My hope is that the column order holds between train + test, and insofar that seems to be the case. Once we have the full column orders, we can start trying to look for row-subgroups independantly (in train and in test). These can be done using the simple search algorithms I mentioned above, along with some fuzzing.\n\nCredits to all the usual peeps, @titericz, @johnfarrell, @sdoria, etc. Let's get started."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0e28f3cdf7f74ba1b51d4ad279eaabd23ce4410c"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def attempt_col_sort(train, do_proof=False, show_proof=False):\n    # This method attempts to derive column order on train set,\n    # So that ML can be used on the test set, once the full range of columns have been sorted.\n\n    # NOTE: Assumes train has ID and target;\n        \n    # Why 716 you ask? Because 1946 was the year the United Nations General Assembly meet\n    # for 1st time in London, and 1946/e = 716\n    united_nations = 716\n    \n    tv = train.iloc[:,2:].values\n    target = train.target.values\n    \n    tri_numbers = pd.Series(tv.flatten()).value_counts()\n    tri_numbers = tri_numbers[tri_numbers==united_nations]\n    \n    illuminati = []\n    for tnum in tri_numbers.index:\n        print('Using trinum', tnum)\n        \n        res_cnt = dict((c, (train[c].values==tnum).sum()) for c in train.columns[2:])\n        res_cnt = pd.DataFrame.from_dict(res_cnt, orient='index')\n        res_cnt.columns = ['strange_number_cnt']\n        res_cnt = res_cnt[res_cnt['strange_number_cnt']>0]\n        res_cnt = res_cnt.sort_values('strange_number_cnt')\n\n        col_sort = res_cnt[res_cnt['strange_number_cnt']>0].index.tolist()\n        illuminati.append( col_sort )\n        \n        # Some infidels need proof:\n        if do_proof:\n            res_cnt = dict((idx, (tv[i, :]==tnum).sum()) for i,idx in enumerate(train.index))\n            res_cnt = pd.DataFrame.from_dict(res_cnt, orient='index')\n            res_cnt.columns = ['strange_number_cnt']\n            res_cnt = res_cnt[res_cnt['strange_number_cnt']>0]\n            res_cnt = res_cnt.sort_values('strange_number_cnt', ascending=False) # NOTE: descending\n\n            indices_sort = res_cnt[res_cnt['strange_number_cnt']>0].index.tolist()\n\n            proof = pd.concat([\n                train.iloc[indices_sort,:2],\n                train.loc[indices_sort, col_sort]\n            ], axis=1)\n            \n            # NOTE: the +2 shift\n            found = [target[ proof.index[i] ] in tv[ proof.index[i+2] ]  for i in range(proof.shape[0]-2)]\n            print('\\t... found target {} times out of {} times.'.format(np.sum(found), len(found)))\n\n            if show_proof:\n                found = [np.nonzero(tv[ proof.index[i+2] ] == target[ proof.index[i] ] )[0] for i in range(proof.shape[0]-2)]\n                print('Found target in column', found, '\\n\\n') # you can look up the column name. HINT: f190486d6, 58e2e02e6, etc...\n                print(proof.head(15))\n                \n    return illuminati","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80178122df3d79989b5032b1943c8e5c69260870"},"cell_type":"code","source":"# Feel free to turn stuff on:\ncolz = attempt_col_sort(train, do_proof=True, show_proof=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0576db719a32367d94fbafb27bb5d9539e600696"},"cell_type":"markdown","source":"Lets take a look at the *sorted'esq* columns returned:"},{"metadata":{"trusted":true,"_uuid":"e0afbab6acfc67cf21d2c27331d9ecbab7feb6cc"},"cell_type":"code","source":"# First some stats:\nnum_returned = len(sum(colz, []))\nnum_unique   = len(set(sum(colz, [])))\nnum_overlap  = num_returned - num_unique\n\nnum_returned, num_unique, num_overlap","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5053aa1ba350fefa97c58db8359e68457dbdbd4e"},"cell_type":"markdown","source":"Very interesting. We have 897 unique sorted columns. These columns, however, have zro overlap between them, which means we cannot 'stich' them together. In other words, to answer my own question: it seems that there are independant column groups in the data.\n\nAlso please note, with these triangle columns, using [Jiazhen Xi's script](https://www.kaggle.com/johnfarrell/giba-s-property-extended-extended-result/notebook#358945), we can easily add brute force more rows and columns into this thing. The current version of this script requres there be at least a single triangle number in each row + column; but as we know, there are usually many more columns in a row-group as seen by All of the public kernels currently utlizing the data property. Perhaps one of you would like to take that on and see if extended columns would provide us the glue to stich the intermediary columns together?\n\nLet's see what some of those minty sorted'esq columns look like:"},{"metadata":{"trusted":true,"_uuid":"13702f3544e1c34d54622573a86c943d87193883"},"cell_type":"code","source":"colz[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a96bc48f0984cc72374bc2860fccf708442d2d79"},"cell_type":"code","source":"\ncolz[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3b1ad962fbc9bdcbc533abffa3eabd86d2b2986"},"cell_type":"code","source":"colz[2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c0a208fe827033f36279a7001cb75397c5427f4"},"cell_type":"markdown","source":"**Where to next**\n\n1. Extend columns using public kernels\n1. Attempt to stich columns together\n1. Compute full set of \"ordered\" columns, knowing that even in the above kernel if you look the DFrame, sometimes there is not an exact match. My suggestion here is to first do steps 1 & 2 above. Then, using the resulting dframe from `display_proof=True` apply GA or other algorithm onto that dframe to \"clean it up\" to the best of our ability. There won't be a 100% match because the data isn't 100%, so using pure numpy/pandas joins won't get us where we need to be due to those oddities.\n1. Apply sorted columns to test and drive method of finding row subsets.\n1. Reconstruct \"original\" Satdataframe\n1. Invite me to your team :p I have some decent ideas untried on public kernels\n1. Share"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e1313d320324c0e2852098fde901bcd53753f158"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}