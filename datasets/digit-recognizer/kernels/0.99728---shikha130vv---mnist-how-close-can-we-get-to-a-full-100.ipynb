{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport keras as keras\nimport numpy as np\nimport sklearn.model_selection as skm\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport timeit\nfinal_models = {\"keras1\":\"\", \"keras2\":\"\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"db8d593d4218b128310cafdeeabfe762ad791663","_kg_hide-input":true},"cell_type":"code","source":"def plot_y_true_vs_y_pred(x, y_true, y_pred, idx):\n    pred_data = {\"SNo\": list(idx), \"Y_true\": y_true, \"Y_pred\": y_pred}\n    x = list(x)\n    pred_data[\"X\"] = x\n    pred_data[\"Correct\"] = np.equal(pred_data[\"Y_true\"], pred_data[\"Y_pred\"] )\n    df_pred = pd.DataFrame(pred_data)\n    df_pred_summary = df_pred.groupby([\"Correct\"]).count()\n    print(df_pred_summary.head(10))\n    num_sample_rows = 50\n    #df_pred_fail = df_pred[df_pred[\"Y_true\"]!=df_pred[\"Y_pred\"]]\n    #df_pred_fail = df_pred_fail.head(num_sample_rows)\n    df_pred = df_pred[df_pred[\"Y_true\"]!=df_pred[\"Y_pred\"]]\n    #df_pred = df_pred.sample(num_sample_rows)\n    #df_pred = pd.concat([df_pred, df_pred_fail], axis=0)\n    i = 1\n    fig = plt.figure(figsize=(15,20))\n    fig.suptitle(\"True Value vs Predicted Value\", size=20)\n    for idx,row in df_pred.head(40).iterrows():\n        plt.subplot(7,7,i)\n        img = row[\"X\"].reshape(28,28)\n        img_class_true = row[\"Y_true\"]\n        img_class_pred = row[\"Y_pred\"]\n        plt.imshow(img, cmap=\"Greys\")\n        plt.title(str(row[\"SNo\"]) + \": \" + str(img_class_true) + \" Prediction:\" + str(img_class_pred))\n        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n        i = i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3af58dbe941ea4ebe3fa482797f48c70ac9c48f2","_kg_hide-input":true},"cell_type":"code","source":"def plot_bad_data(x, y):\n    data = {\"Y_true\": y}\n    x = list(x)\n    data[\"X\"] = x\n    df_data = pd.DataFrame(data)\n    i = 1\n    fig = plt.figure(figsize=(15,20))\n    fig.suptitle(\"Confusing Data\", size=20)\n    for idx,row in df_data.iterrows():\n        plt.subplot(7,7,i)\n        img = row[\"X\"].reshape(28,28)\n        img_class_true = row[\"Y_true\"]\n        plt.imshow(img, cmap=\"Greys\")\n        plt.title(str(img_class_true))\n        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n        i = i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7913c2545c8613e3863a6e6f8d7cea647fa772cf"},"cell_type":"code","source":"from skimage.transform import resize\n\nthreshold_color = 100 / 255\nsize_img = 28\ndef find_left_edge(x):\n    edge_left = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for j in range(size_img):\n            for i in range(size_img):\n                if (x[k, size_img*i+j] >= threshold_color):\n                    edge_left.append(j)\n                    break\n            if (len(edge_left) > k):\n                break\n    return edge_left\ndef find_right_edge(x):\n    edge_right = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for j in range(size_img):\n            for i in range(size_img):\n                if (x[k, size_img*i+(size_img-1-j)] >= threshold_color):\n                    edge_right.append(size_img-1-j)\n                    break\n            if (len(edge_right) > k):\n                break\n    return edge_right\ndef find_top_edge(x):\n    edge_top = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for i in range(size_img):\n            for j in range(size_img):\n                if (x[k, size_img*i+j] >= threshold_color):\n                    edge_top.append(i)\n                    break\n            if (len(edge_top) > k):\n                break\n    return edge_top\ndef find_bottom_edge(x):\n    edge_bottom = []\n    n_samples = x.shape[0]\n    for k in range(n_samples):\n        for i in range(size_img):\n            for j in range(size_img):\n                if (x[k, size_img*(size_img-1-i)+j] >= threshold_color):\n                    edge_bottom.append(size_img-1-i)\n                    break\n            if (len(edge_bottom) > k):\n                break\n    return edge_bottom\ndef stretch_image(x):\n    #get edges\n    edge_left = find_left_edge(x)\n    edge_right = find_right_edge(x)\n    edge_top = find_top_edge(x)\n    edge_bottom = find_bottom_edge(x)\n    \n    #cropping and resize\n    n_samples = x.shape[0]\n    x = x.reshape(n_samples, size_img, size_img)\n    for i in range(n_samples):      \n        x[i] = resize(x[i][edge_top[i]:edge_bottom[i]+1, edge_left[i]:edge_right[i]+1], (size_img, size_img))\n    x = x.reshape(n_samples, size_img ** 2)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea84afe25b22bf32a7b7cae65feabb1f983edc56"},"cell_type":"markdown","source":"## Load & Clean data\nLet us load train data and plot some points so as to understand our data better. We see that some data in training data set is incorrectly classified. We will remove these data points so as to avoid confusing the model. Let us then split the cleaned train data into train and validation data set. "},{"metadata":{"trusted":true,"_uuid":"6c6c82ed9ab4ba83c5da0d1eadf62e10a5695a62","collapsed":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/train.csv\")\nx_train_all = np.array(train_data.drop([\"label\"], axis=1))\nprint (x_train_all[0][x_train_all[0] > 0])\nx_train_all = x_train_all/255\nx_train_all = stretch_image(x_train_all)\n#x_train_all [ x_train_all > 0] = 1\n\ny_train_all = np.array(train_data[\"label\"])\nidx_all = range(x_train_all.shape[0])\nif 1==2:\n    bad_data_idx = [28290, 16301,14101,15065, 6389,7764,28611,20954,2316, 37056, 37887, 36569, 40257]\n    plot_bad_data(x_train_all[bad_data_idx], y_train_all[bad_data_idx])\n    x_train_all = np.delete(x_train_all, bad_data_idx, axis=0)\n    y_train_all = np.delete(y_train_all, bad_data_idx)\n    idx_all = np.delete(idx_all, bad_data_idx)\ny_train_all = keras.utils.to_categorical(y_train_all, num_classes=10)\nx_train, x_valid, y_train, y_valid, idx_train, idx_valid = skm.train_test_split(x_train_all, y_train_all,idx_all,  test_size=0.2)\n\ntest_data = pd.read_csv(\"../input/test.csv\")\nx_test = np.array(test_data)\nx_test = x_test/255\nx_test = stretch_image(x_test)\n#x_test[x_test > 0]=1\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17cb26d2bebbed2978da5ad75014fde6bd60d4aa"},"cell_type":"markdown","source":"\n## Augment data\nData augmentation refers to adding more data points in the train data so that model can learn better. For MNIST data some of the techniques used to create new data points that might help the model to train better are rotation, shear and shift. AKeras has a very nice API for facilitating this and that is what we will try to use."},{"metadata":{"trusted":true,"_uuid":"e29cb97d1d34632b07b9e88b6f65ec5c674ef7df","collapsed":true},"cell_type":"code","source":"datagen = keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n    featurewise_std_normalization=False,\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,                                               \n    horizontal_flip=False, shear_range=0.2)\ndatagen.fit(x_train.reshape(-1,28,28,1))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ee0a047f92d1c01384922e65ab8ffdac414f7bc6"},"cell_type":"code","source":"if 1==2:\n    augmented_image = []\n    augmented_image_labels = []\n    x_train = x_train.reshape(-1,28,28,1)\n    for num in range (0, x_train.shape[0]):\n        augmented_image.append(x_train[num])\n        augmented_image_labels.append(y_train[num])\n        if num<500:\n            augmented_image.append(keras.preprocessing.image.random_rotation(x_train[num], 15, row_axis=1, col_axis=1, channel_axis=2))\n            augmented_image_labels.append(y_train[num])\n\n        if num > 500 and num < 1000:\n            augmented_image.append(keras.preprocessing.image.random_shear(x_train[num], 0.2, row_axis=1, col_axis=1, channel_axis=2))\n            augmented_image_labels.append(y_train[num])\n\n        if num > 1000 and num < 1500:\n            augmented_image.append(keras.preprocessing.image.random_shift(x_train[num], 0, 0.2, row_axis=1, col_axis=1, channel_axis=2))\n            augmented_image_labels.append(y_train[num])\n\n    x_train = np.array(augmented_image).reshape(-1, 784)\n    y_train = np.array(augmented_image_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1b2db2d22d51560ba654f8808f7ae5572f2e857","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"print (test_data.head(1))\ns = pd.read_csv(\"../input/sample_submission.csv\")\ns.head(1)\nprint(x_valid.shape[0]*4)\nprint(x_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"128b36533df1315b417ac4f7f3d7f207a5eddf9b","collapsed":true},"cell_type":"code","source":"learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='acc', \n                                            patience=1, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.000001)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f09c6817ca5600f042c9b3ff9b4ff6c7bc93132c"},"cell_type":"markdown","source":"**LeNet Architecture**\nINPUT => CONV => RELU => POOL => CONV => RELU => POOL => FC => RELU => FC"},{"metadata":{"trusted":true,"_uuid":"df5f3669a74c5b530af5d648a5e7d5290a0bf47f","collapsed":true},"cell_type":"code","source":"def tensorflow_keras_model(x_train, y_train, x_valid, y_valid, num_classes,\\\n                                num_epochs, learning_rate):\n    \n    keras_model = keras.models.Sequential()\n    keras_model.add(keras.layers.Conv2D(filters=32,kernel_size=(6,6),strides=(1,1), \\\n                                        padding=\"same\",input_shape=(28,28,1)))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Conv2D(filters=64,kernel_size=(6,6),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Flatten())\n    keras_model.add(keras.layers.Dense(units=1024)) #, activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Dense(units=num_classes))#, activation='softmax'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('softmax'))  \n    \n    #opt = keras.optimizers.Adam(lr=learning_rate)\n    opt = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n    keras_model.compile(optimizer=opt,loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    keras_model.fit(x_train.reshape(-1,28,28,1), y_train, epochs=num_epochs, callbacks=[learning_rate_reduction])\n    final_models[\"keras1\"] = keras_model\n    keras_model.save(\"Model_MNIST_Keras_Lenet.h5\")\n    cur_y_pred = keras_model.predict(x_valid.reshape(-1,28,28,1))\n    y_valid_argmax = np.argmax(y_valid, 1)\n    y_pred_argmax = np.argmax(cur_y_pred, 1)\n    y_correct = np.equal(y_valid_argmax, y_pred_argmax)\n    acc = y_correct.sum()/y_pred_argmax.shape[0]\n    return cur_y_pred, acc, y_valid_argmax,y_pred_argmax ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e09b0683c27309371d4b11c8db13e3d53262bc8","collapsed":true},"cell_type":"code","source":"def exec_tensorflow_keras_model():\n    num_epochs = 20 #50\n    learning_rate=0.000001\n    num_rows, num_features, num_classes = x_train.shape[0], x_train.shape[1], 10\n    print(num_rows, num_features, num_classes)\n    final_pred_base_model, acc,y_valid_argmax,y_pred_argmax  = \\\n        tensorflow_keras_model(x_train, y_train, x_valid, y_valid, num_classes, num_epochs, learning_rate)  \n\n    print(\"Num Epoch:\", num_epochs, \" Accuracy:\", acc) #, \\\n           #   \" Weights:\", \" \".join(list(final_w.astype(str).flatten())), \" Bias:\", final_b)\n    plot_y_true_vs_y_pred(x_valid, y_valid_argmax.reshape(len(y_valid)), y_pred_argmax.reshape(len(y_valid)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"677d9620bd97b8d750898ae035f05c2cdf5d498f","collapsed":true},"cell_type":"code","source":"#timeit.timeit(exec_tensorflow_keras_model, number=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f47f5e13be74c37af27a24353d2ca266dd14d828"},"cell_type":"markdown","source":"## Techniques used to improve accuracy\n### Batch Normalization\nIf configuration of one batch is very different than configuration of other batch, then converging the model will be very diificult as weights detemined for one batch would lead to very unsatisfactory results for other batch. Same concept can be applied to deep neural networks. Each batch in every hidden layer should ideally have similar set of data points. This is called batch normalization and Keras provides a very simple way of doing this as shown below. It is advised to add Batch Normalization before activation layer. \n### Dropout\nDropout is essentially discarding random data points in oder to avoid overfitting. With Batch Normalization, this may not be required, so we will check and confirm.\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a0a5839635cf7800a6358e75d0c4dfbb68d27759"},"cell_type":"code","source":"#VGG -> 64->MAXPOOL->128->MAXPOOL->256->256->MAXPOOL->512->512->MAXPOOL-->512->512->MAXPOOL->FC4096->FC4096->FC1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d2cd7cc23cc34d3026e079e27b6ace87909736c","collapsed":true},"cell_type":"code","source":"#.99154\ndef tensorflow_keras_model_2(x_train, y_train, x_valid, y_valid, num_classes,\\\n                                num_epochs, learning_rate):\n    keras_model = keras.models.Sequential()\n    keras_model.add(keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\",activation='relu', input_shape=(28,28,1)))    #From 64->128->256 changing to 32->64->128\n    keras_model.add(keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu')) \n    keras_model.add(keras.layers.Conv2D(filters=32,kernel_size=(5,5),strides=(2,2), \\\n                                            padding=\"same\")) #,activation='relu'))\n    #keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1), \\\n                                        padding=\"same\",activation='relu',input_shape=(28,28,1)))\n    keras_model.add(keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Conv2D(filters=64,kernel_size=(5,5),strides=(2,2), \\\n                                            padding=\"same\")) #,activation='relu'))\n    #keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.050))\n                    \n    \n    \n    keras_model.add(keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\",activation='relu'))\n    keras_model.add(keras.layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Conv2D(filters=128,kernel_size=(5,5),strides=(2,2), \\\n                                            padding=\"same\")) #,activation='relu'))\n    #keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.075))\n    \n    keras_model.add(keras.layers.Flatten())\n    keras_model.add(keras.layers.Dense(units=1024)) # #  , activation='relu')) Chaging from 20148 to 1024\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.100))\n    \n    keras_model.add(keras.layers.Dense(units=128)) # #  , activation='relu'))  Changin from 256 tp 128\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.100))\n    \n    keras_model.add(keras.layers.Dense(units=num_classes))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('softmax'))  \n    \n    opt = keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\n    #opt = keras.optimizers.Adam(lr=learning_rate)\n    keras_model.compile(optimizer=opt,loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    keras_model.fit_generator(datagen.flow(x_train.reshape(-1,28,28,1), y_train, batch_size=32),steps_per_epoch=len(x_train) / 32,  \\\n                              epochs=num_epochs, callbacks=[learning_rate_reduction])\n    final_models[\"keras2\"] = keras_model\n    keras_model.save(\"Model_MNIST_Keras_Resnet.h5\")\n   \n    cur_y_pred = keras_model.predict(x_valid.reshape(-1,28,28,1))\n    y_valid_argmax = np.argmax(y_valid, 1)\n    y_pred_argmax = np.argmax(cur_y_pred, 1)\n    y_correct = np.equal(y_valid_argmax, y_pred_argmax)\n    acc = y_correct.sum()/y_pred_argmax.shape[0]\n    return cur_y_pred, acc, y_valid_argmax,y_pred_argmax , keras_model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73db5d7f255fe00fa0eb6b44ac73ab92e236eeb6","collapsed":true},"cell_type":"code","source":"model2_list = []\nnum_model = 12\ndef exec_tensorflow_keras_model_2():\n    num_epochs = 30 #50  With 30 epochs acc was 99.667\n    learning_rate=0.00001\n    \n    num_rows, num_features, num_classes = x_train.shape[0], x_train.shape[1], 10\n    for imodel in range(num_model):\n        final_pred_base_model, acc,y_valid_argmax,y_pred_argmax, keras_model  = \\\n                    tensorflow_keras_model_2(x_train, y_train, x_valid, y_valid, num_classes, num_epochs, learning_rate)\n        model2_list.append(keras_model)\n\n        print(\"Num Epoch:\", num_epochs, \" Accuracy:\", acc) #, \\\n           #   \" Weights:\", \" \".join(list(final_w.astype(str).flatten())), \" Bias:\", final_b)\n    plot_y_true_vs_y_pred(x_valid, y_valid_argmax.reshape(len(y_valid)), y_pred_argmax.reshape(len(y_valid)), idx_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b42d778f07c43f514a04b7b4a630af9924e0370","collapsed":true},"cell_type":"code","source":"timeit.timeit(exec_tensorflow_keras_model_2, number=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2dd26d1893fee3f3c817c55fbf28b28c466eec51"},"cell_type":"code","source":"def tensorflow_keras_model_3(x_train, y_train, x_valid, y_valid, num_classes,\\\n                                num_epochs, learning_rate):\n    \n    keras_model = keras.models.Sequential()\n    keras_model.add(keras.layers.Conv2D(filters=6,kernel_size=(6,6),strides=(1,1), \\\n                                        padding=\"same\",input_shape=(28,28,1)))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Conv2D(filters=16,kernel_size=(6,6),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Conv2D(filters=120,kernel_size=(6,6),strides=(1,1), \\\n                                            padding=\"same\")) #,activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.MaxPool2D(strides=(2,2), padding=\"same\"))\n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Flatten())\n    keras_model.add(keras.layers.Dense(units=120)) #, activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Dense(units=120)) #, activation='relu'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('relu'))  \n    keras_model.add(keras.layers.Dropout(rate=0.05))\n    \n    keras_model.add(keras.layers.Dense(units=num_classes))#, activation='softmax'))\n    keras_model.add(keras.layers.BatchNormalization())\n    keras_model.add(keras.layers.Activation('softmax'))  \n    \n    #opt = keras.optimizers.Adam(lr=learning_rate)\n    opt = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n    keras_model.compile(optimizer=opt,loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    keras_model.fit(x_train.reshape(-1,28,28,1), y_train, epochs=num_epochs, callbacks=[learning_rate_reduction])\n    final_models[\"keras1\"] = keras_model\n    keras_model.save(\"Model_MNIST_Keras_Lenet.h5\")\n    cur_y_pred = keras_model.predict(x_valid.reshape(-1,28,28,1))\n    y_valid_argmax = np.argmax(y_valid, 1)\n    y_pred_argmax = np.argmax(cur_y_pred, 1)\n    y_correct = np.equal(y_valid_argmax, y_pred_argmax)\n    acc = y_correct.sum()/y_pred_argmax.shape[0]\n    return cur_y_pred, acc, y_valid_argmax,y_pred_argmax ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9655305ca7f1fa97b1c5dc7cf6ba8f7b76b20b90"},"cell_type":"code","source":"def exec_tensorflow_keras_model_3():\n    num_epochs = 30 #50\n    learning_rate=0.00001\n    num_rows, num_features, num_classes = x_train.shape[0], x_train.shape[1], 10\n    final_pred_base_model, acc,y_valid_argmax,y_pred_argmax  = \\\n        tensorflow_keras_model_3(x_train, y_train, x_valid, y_valid, num_classes, num_epochs, learning_rate)  \n\n    print(\"Num Epoch:\", num_epochs, \" Accuracy:\", acc) #, \\\n           #   \" Weights:\", \" \".join(list(final_w.astype(str).flatten())), \" Bias:\", final_b)\n    plot_y_true_vs_y_pred(x_valid, y_valid_argmax.reshape(len(y_valid)), y_pred_argmax.reshape(len(y_valid)), idx_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed44812941fac3ba9ef666490be447266e627462","collapsed":true},"cell_type":"code","source":"#timeit.timeit(exec_tensorflow_keras_model_3, number=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"492630b85f563e8600b9e3c4b0e71ddf18e81808","collapsed":true},"cell_type":"code","source":"#model1 = final_models[\"keras1\"]\nmodel2 = final_models[\"keras2\"]\n#y_pred_valid1 = model1.predict(x_valid.reshape(-1,28,28,1))\n#y_pred_valid2 = model2.predict(x_valid.reshape(-1,28,28,1))\nx_valid = x_valid.reshape(-1,28,28,1)\ny_pred_valid2 = np.zeros( (x_valid.shape[0],10) ) \nfor imodel in range(num_model): #len(model2_list)):\n    y_pred_valid2 = y_pred_valid2 + model2_list[imodel].predict(x_valid)\n    \ny_pred_valid = y_pred_valid2 #0.4*y_pred_valid1 + 0.6*y_pred_valid2\n\ny_pred_valid_final = np.argmax(y_pred_valid, axis=1)\ny_correct = np.equal(y_pred_valid_final, np.argmax(y_valid, axis=1))\nacc_final = y_correct.sum()/y_valid.shape[0]\nprint(acc_final)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"trusted":true,"_uuid":"51df8bc6974b7c0e9686aeb3306690bcd6966e32","collapsed":true},"cell_type":"code","source":"#y_pred_valid1 = model1.predict(x_test.reshape(-1,28,28,1))\n#y_pred_test2 = model2.predict(x_test.reshape(-1,28,28,1))\nx_test = x_test.reshape(-1,28,28,1)\ny_pred_test2 = np.zeros( (x_test.shape[0],10) ) \nfor imodel in range(num_model):\n    y_pred_test2 = y_pred_test2 + model2_list[imodel].predict(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59b49b549f92fedb9ce10429f76388167b6bf450","collapsed":true},"cell_type":"code","source":"y_pred_test = y_pred_test2 #0.4*y_pred_valid1 + 0.6*y_pred_valid2\ny_pred_test_final = np.argmax(y_pred_test, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"258728317cf14c91e24febdf11d7621dfaad2976","collapsed":true},"cell_type":"code","source":"dictionary_data = {\"ImageId\":np.arange(1, x_test.shape[0]+1), \"Label\":y_pred_test_final}\ndf_final = pd.DataFrame(dictionary_data)\ndf_final.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"91c183d461243867cfebe9cc75bda5fb8e4d3f38"},"cell_type":"code","source":"#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"310c471eb59e2283d0d10892a9c5169957a66d04","collapsed":true},"cell_type":"code","source":"plot_y_true_vs_y_pred(x_valid, np.argmax(y_valid, 1).reshape(len(y_valid)), y_pred_valid_final.reshape(len(y_valid)), idx_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6712dd2d5696a86f49ff1c52e02bc212e125316"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"72d322b7fc9233bc715736c4d814893cc2208275"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cb60b8f28dd8c315a77887940beab21c002432ab"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"52cea3380be7129db0341b8d2a06d894238603a1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}