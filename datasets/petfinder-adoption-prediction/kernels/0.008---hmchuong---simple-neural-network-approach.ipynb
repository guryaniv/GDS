{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"433890d74ca4018569b8e7d80ba96b67e44ea182"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4747827802497428f66840793e859fc19a222325"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0df9dd25498d5ce9465f27e7a822505ffcc03dde"},"cell_type":"markdown","source":"# Analyse data"},{"metadata":{"_uuid":"7e338e05fb8f07f8234924005c55f115314f8c45"},"cell_type":"markdown","source":"## Counting adoptation speed"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a26f71f9df2e2ae6695c58aa6d869f3aa88a07ec"},"cell_type":"code","source":"pd.value_counts(train_df.AdoptionSpeed).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"685623353b08eade50585893d3104d634a74cdf1"},"cell_type":"markdown","source":"The data is imbalance, we will need to build class to create balance batch when training"},{"metadata":{"_uuid":"e8367661f8f43ccb48f6e19c4a4f447841bfb7e9"},"cell_type":"markdown","source":"## Cat or dog"},{"metadata":{"trusted":true,"_uuid":"991a816aba05163977325967c3fdeb08d3515fd4"},"cell_type":"code","source":"cat_df = train_df[train_df.Type == 2]\ndog_df = train_df[train_df.Type == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c8ca72a1b8ce0bff10bd2a5727bfcfaacb0db94"},"cell_type":"code","source":"cat_df.AdoptionSpeed.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65250b7cb68c261b04743d712f478ac360be5e1d"},"cell_type":"code","source":"dog_df.AdoptionSpeed.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df35c11bd96d2a2695c9ed114a404c7d9ed2f9b1"},"cell_type":"markdown","source":"Type can affect the adoption speed"},{"metadata":{"_uuid":"0f85cf5dd2d1e190130f026024edc757c38ac381"},"cell_type":"markdown","source":"## Age"},{"metadata":{"trusted":true,"_uuid":"43291e98e7567aa4b64ddb6f5d25feefa3629838"},"cell_type":"code","source":"train_df.groupby(['Age'])['AdoptionSpeed'].mean().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"eae6a64bd7d7e129516b65eae29742d3a457c55c"},"cell_type":"code","source":"normalize_train_df = train_df.drop([\"RescuerID\", \"Description\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b49b62235e339c42b3d4765533cb46c417efb38"},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/test/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93d1210f293557b1cb2d0c2d882d8825ff274ac6"},"cell_type":"code","source":"normalize_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9214c3d5e315002219b9771c8c8d55aaf63be9f8"},"cell_type":"markdown","source":"## Transform continuous data"},{"metadata":{"trusted":true,"_uuid":"e5d400a6e1f806333d2bcc85882851ceda7eea58"},"cell_type":"code","source":"normalize_train_df[\"Name\"] = normalize_train_df[\"Name\"].fillna('No name')\ntest_df[\"Name\"] = test_df[\"Name\"].fillna('No name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"bbc6a231627a2caf32df9238b1c37778387c0b8b"},"cell_type":"code","source":"names = normalize_train_df[\"Name\"].as_matrix().tolist() + test_df[\"Name\"].as_matrix().tolist() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ceeb74b8d0d665bce28c16bf31cb1fd9eabf03c1"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist([len(name) for name in names], 20, normed=1, facecolor='green', alpha=0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8136c314df7bdacb45cbce8eb1188776f30c8998"},"cell_type":"code","source":"characters = set(\"\".join(names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71077270516d3888b35070f6d02f7d6847c3b2f1"},"cell_type":"code","source":"char2idx = {char:idx+1 for idx, char in enumerate(characters)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86ee4e6d327764a34b0a6d26c199a21a409d0c94"},"cell_type":"code","source":"def name_to_ids(name):\n    ids = [char2idx[c] for c in name]\n    ids = ids + [0] * 10\n    return ids[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"9a80187840ec2dff60d478761ebcb1bc1061afab"},"cell_type":"code","source":"continuous_columns = [\"Age\", \"MaturitySize\", \"Quantity\", \"Fee\", \"VideoAmt\", \"PhotoAmt\"]\nscalers = {}\nfor column in continuous_columns:\n    scaler = MinMaxScaler()\n    scaler.fit(np.concatenate((normalize_train_df[column].as_matrix(),test_df[column].as_matrix())).reshape(-1, 1))\n    normalize_train_df[column] = scaler.transform(normalize_train_df[column].as_matrix().reshape(-1,1)).reshape(-1)\n    scalers[column] = scaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0adff568481062adbaba6a9bd7e1057f68bb1657"},"cell_type":"markdown","source":"## Categorical data"},{"metadata":{"trusted":true,"_uuid":"87e802d17257dc251c1a98d5071e1a6857c57f61"},"cell_type":"code","source":"normalize_train_df[\"Type\"] = normalize_train_df[\"Type\"] - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc380ffaf66e6f5049fdbbd2f7750e8ccbbe31c8"},"cell_type":"code","source":"normalize_train_df[\"Gender\"] = normalize_train_df[\"Gender\"] - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1774a2a4a708caace4563fa4041658141272d29f"},"cell_type":"code","source":"normalize_train_df[\"Vaccinated\"] = normalize_train_df[\"Vaccinated\"] - 1\nnormalize_train_df[\"Dewormed\"] = normalize_train_df[\"Dewormed\"] - 1\nnormalize_train_df[\"Sterilized\"] = normalize_train_df[\"Sterilized\"] - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaa7c8987fca360ea2b06daedb24744afadfe3de"},"cell_type":"code","source":"states = [41336, 41325, 41367, 41401, 41415, 41324, 41332, 41335, 41330, 41380, 41327, 41345, 41342, 41326, 41361]\nstate2index = {'State': {state: idx for idx, state in enumerate(states)}}\nnormalize_train_df.replace(state2index, inplace=True)\nnormalize_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"167531ecbfc8a0afdc1cbbb5ed1c25fde73c2545"},"cell_type":"markdown","source":"## Build model"},{"metadata":{"trusted":true,"_uuid":"28eef0b3dc4a728ffdf43d3f40cec3001f214e9a"},"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torch.utils.data as utils\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de15cb7906071b8a7047b14e7b4a38d94bb25236"},"cell_type":"markdown","source":"### Split data"},{"metadata":{"trusted":true,"_uuid":"713c9cfc46fd800b1b0729ed34f2620eeb66f0b4"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_data, validation_data = train_test_split(normalize_train_df, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cdd5ca7f0bba193914e2ea91601c5e0f56ff2d2"},"cell_type":"markdown","source":"### Data generators"},{"metadata":{"trusted":true,"_uuid":"7676f1be4b0aabcd30c1e4927102434546763286"},"cell_type":"code","source":"import math\nimport random\n\nimgtransCrop = 224\ntransform = transforms.Compose([transforms.RandomResizedCrop(imgtransCrop),\n                                transforms.RandomHorizontalFlip(),                           \n                                transforms.ToTensor(),                                \n                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n\nval_transform = transforms.Compose([transforms.Resize((imgtransCrop,imgtransCrop)),                          \n                                transforms.ToTensor(),                                \n                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n\ndef create_categorical_dfs(data):\n    categorical_dfs = []\n    for i in range(5):\n        categorical_dfs.append(data[data.AdoptionSpeed == i].drop([\"AdoptionSpeed\"], axis=1).sample(frac=1).reset_index(drop=True))\n    return categorical_dfs\n\ndef data_generator(categorical_dfs, image_transform, folder_path=\"../input/train_images\", batch_size=15):\n    result = pd.DataFrame(columns=categorical_dfs[0].columns)\n    images = []\n    names = []\n    labels = []\n    \n    for i in range(batch_size//5):\n        \n        for idx in range(5):\n            picked_data_idx = random.randint(0, len(categorical_dfs[idx])-1)\n            result = result.append(categorical_dfs[idx].drop([\"PetID\"], axis=1).loc[picked_data_idx], ignore_index=True)\n            \n            image_path = os.path.join(folder_path,str(categorical_dfs[idx].loc[picked_data_idx][\"PetID\"])+\"-1.jpg\")\n            try:\n                image = Image.open(image_path).convert('RGB')\n            except:\n                image = image = Image.new('RGB', (300, 300))\n            if image_transform:\n                image = image_transform(image)\n            images.append(image)\n            names.append(name_to_ids(categorical_dfs[idx].loc[picked_data_idx][\"Name\"]))\n            labels.append(idx)\n    return result, torch.stack(images), names, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b35f033fff053431dca459238586fa399706e359"},"cell_type":"code","source":"categorical_dfs = create_categorical_dfs(train_data)\nval_categorical_dfs = create_categorical_dfs(validation_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ded48733b527e27310b50d88ffcf5e2b3476907a"},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true,"_uuid":"2951702faddfd3f829a21a6f4514c2009c0ed6a2"},"cell_type":"code","source":"class NameAnalysis(nn.Module):\n    def __init__(self, device, character_size, embedding_dim, hidden_dim):\n        super().__init__()\n        self.device = device\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(character_size, embedding_dim).to(self.device)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n    def init_hidden(self, batch_size):\n        return (torch.zeros(1, batch_size, self.hidden_dim).to(self.device),\n                torch.zeros(1, batch_size, self.hidden_dim).to(self.device))\n    def forward(self, names):\n        embeds = self.embedding(torch.LongTensor(names).to(self.device))\n        hidden = self.init_hidden(len(names))\n        lstm_out, hidden = self.lstm(\n            embeds.view(len(names[0]), len(names), -1), hidden)\n        return lstm_out[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7a5f4371956d2bcc0e29495dbb85aca55ad1661"},"cell_type":"code","source":"class AdoptionSpeedModel(nn.Module):\n    def __init__(self, device, embedding_dim, category2size, num_continues_features, character_size=len(char2idx)+1, char_embedding_dim=80, hidden_dim=5):\n        super().__init__()\n        self.device = device\n        self.feature2embedding = {}\n        for category in category2size.keys():\n            self.feature2embedding[category] = nn.Embedding(category2size[category], embedding_dim).to(self.device)\n        \n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(character_size, char_embedding_dim).to(self.device)\n        self.lstm = nn.LSTM(char_embedding_dim, hidden_dim)\n        \n        self.resnet = models.resnet18(pretrained=False)\n        kernel_count = self.resnet.fc.in_features\n        self.resnet.fc = nn.Linear(kernel_count, 300)\n        self.fc = nn.Sequential(nn.Linear(embedding_dim*len(category2size.keys()) + num_continues_features + 300 + 5, 300), \n                                nn.Dropout(0.2), \n                                nn.Linear(300, 200), \n                                nn.Dropout(0.2), \n                                nn.Linear(200, 100), \n                                nn.Dropout(0.2), \n                                nn.Linear(100, 50), \n                                nn.Dropout(0.2), \n                                nn.Linear(50, 5), \n                                nn.LogSoftmax(dim=1))\n        \n    def init_hidden(self, batch_size):\n        return (torch.autograd.Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad=False).double().to(self.device),\n                torch.autograd.Variable(torch.zeros(1, batch_size, self.hidden_dim), requires_grad=False).double().to(self.device))\n        \n    def forward(self, x):\n        data, images, names = x\n        embeds = None\n        data = data.drop([\"PetID\", \"Name\"], axis=1)\n        for feature in self.feature2embedding.keys():\n            if embeds is None:\n                embeds = self.feature2embedding[feature](torch.LongTensor(data[feature].values.astype(np.int64)).to(self.device))\n            else:\n                embeds = torch.cat((embeds, self.feature2embedding[feature](torch.LongTensor(data[feature].values.astype(np.int64)).to(self.device))), 1)\n        \n        char_embeds = self.embedding(torch.LongTensor(names).to(self.device))\n        self.hidden = self.init_hidden(len(names))\n        lstm_out, self.hidden = self.lstm(\n            char_embeds.view(len(names[0]), len(names), -1), self.hidden)\n        lstm_out = lstm_out[-1]\n        resnet_out = self.resnet(torch.FloatTensor(images).to(self.device))\n        #embeds = embeds.view(len(names),-1)\n        #print(lstm_out.size(), embeds.size())\n        embeds = torch.cat((embeds, \n                            torch.FloatTensor(data.drop(self.feature2embedding.keys(), axis=1).values.astype(np.float32)).to(self.device), \n                            resnet_out, \n                            lstm_out),1)\n        return self.fc(embeds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adb32eefd13225d6c8a306eae8bb571276416bfc"},"cell_type":"code","source":"category2size = {\n    \"Type\": 2,\n    \"Breed1\": 308,\n    \"Breed2\": 308,\n    \"Gender\": 3,\n    \"Color1\": 8,\n    \"Color2\": 8,\n    \"Color3\": 8,\n    \"FurLength\": 4,\n    \"Vaccinated\": 3,\n    \"Dewormed\": 3,\n    \"Sterilized\": 3,\n    \"Health\": 4,\n    \"State\": 15\n}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d14428bbd760df0c49408331503fc3d784fe429"},"cell_type":"code","source":"from torch.optim.lr_scheduler import ReduceLROnPlateau\n\nmodel = AdoptionSpeedModel(device, 5, category2size, 6)\nmodel = model.to(device)\n#state_dict = torch.load('new_checkpoint.pth',map_location=device)\n#model.load_state_dict(state_dict)\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min', verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52bf44eff79df9b87033999c31d39b6d19394b8e","scrolled":false},"cell_type":"code","source":"import sys\nepochs = 15\nbatch_size = 16\nsteps = int(12000/batch_size)\nval_steps = int(steps/4)\ntrain_losses, test_losses = [], []\nloss_min = 100000\nfor e in range(epochs):\n    running_loss = 0\n    model.train()\n    for step in range(steps):\n        data, images, names, labels = data_generator(categorical_dfs,image_transform=transform, folder_path=\"../input/train_images\", batch_size=batch_size)\n        optimizer.zero_grad()\n        log_ps = model((data, images, names))\n        loss = loss_function(log_ps, torch.LongTensor(labels).to(device))\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        sys.stdout.write(f\"\\rEpoch {e+1}/{epochs}... Step {step+1}/{steps}... Training loss {running_loss/(step+1)}\")\n    else:\n        test_loss = 0\n        accuracy = 0\n        print()\n        model.eval()\n        with torch.no_grad():\n            for step in range(val_steps):\n                data, images, names, labels = data_generator(val_categorical_dfs,image_transform=val_transform, folder_path=\"../input/train_images\", batch_size=batch_size)\n                labels = torch.LongTensor(labels).to(device)\n                log_ps = model((data, images, names))\n                test_loss += loss_function(log_ps, labels)\n                ps = torch.exp(log_ps)\n                top_p, top_class = ps.topk(1, dim=1)\n                equals = top_class == labels.view(*top_class.shape)\n                accuracy += torch.mean(equals.type(torch.FloatTensor))\n                sys.stdout.write(f\"\\rEpoch {e+1}/{epochs}... Step {step+1}/{val_steps}... Validation loss {test_loss/(step+1)}... Accuracy {accuracy*100/(step+1)}\")\n        train_losses.append(running_loss/steps)            \n        test_losses.append(test_loss/val_steps)\n        print()\n        scheduler.step(test_loss/steps)\n        if test_loss/val_steps < loss_min:\n            print(f\"Improve loss from {loss_min} to {test_loss/val_steps}\")\n            loss_min = test_loss/val_steps\n            torch.save(model.state_dict(), 'new_checkpoint.pth')\n        else:\n            state_dict = torch.load('new_checkpoint.pth',map_location=device)\n            model.load_state_dict(state_dict)\n        print(\"\\nEpoch: {}/{}.. \".format(e+1, epochs),                  \n              \"Training Loss: {:.3f}.. \".format(running_loss/steps),                  \n              \"Val Loss: {:.3f}.. \".format(test_loss/val_steps),                  \n              \"Val Accuracy: {:.3f}\\n\\n\".format(accuracy/val_steps))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b1fde17e2de63a884b5e26e71da05d71997d77f"},"cell_type":"code","source":"state_dict = torch.load('new_checkpoint.pth',map_location=device)\nmodel.load_state_dict(state_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f138dbaf70fac6dfdf6d09bff37bced73a62682"},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/test/test.csv\")\ntest_df[\"Name\"] = test_df[\"Name\"].fillna('No name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49429fe254afd5280ca116af867cf3c1d6807961","scrolled":true},"cell_type":"code","source":"processed_test_df = test_df.drop([\"RescuerID\", \"Description\"], axis=1)\nfor column in continuous_columns:\n    processed_test_df[column] = scalers[column].transform(processed_test_df[column].values.reshape(-1,1)).reshape(-1)\n    \nprocessed_test_df[\"Type\"] = processed_test_df[\"Type\"] - 1\nprocessed_test_df[\"Gender\"] = processed_test_df[\"Gender\"] - 1\nprocessed_test_df[\"Vaccinated\"] = processed_test_df[\"Vaccinated\"] - 1\nprocessed_test_df[\"Dewormed\"] = processed_test_df[\"Dewormed\"] - 1\nprocessed_test_df[\"Sterilized\"] = processed_test_df[\"Sterilized\"] - 1\n\nprocessed_test_df.replace(state2index, inplace=True)\nprocessed_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95248b9a9251346eb73c739a6a73cda302eb795b","scrolled":false},"cell_type":"code","source":"model.eval()\nfolder_path = \"../input/test_images\"\nimage_transform = val_transform\nfinal_result = {}\nbatch_test_df = processed_test_df.copy()\nwhile batch_test_df.shape[0] > 0:\n    batch = batch_test_df[:50]\n    try:\n        batch_test_df = batch_test_df[50:].reset_index(drop=True)\n    except:\n        pass\n    \n    images = []\n    names = []\n    for _, row in batch.iterrows():\n        image_path = os.path.join(folder_path,str(row[\"PetID\"])+\"-1.jpg\")\n        image = None\n        try:\n            image = Image.open(image_path).convert('RGB')\n        except:\n            print(\"Cannot found any image of \"+row[\"PetID\"])\n            image = image = Image.new('RGB', (300, 300))\n        if image_transform:\n            image = image_transform(image)\n        name = name_to_ids(row[\"Name\"])\n        images.append(image)\n        names.append(name)\n    with torch.no_grad():\n        output = model((batch, torch.stack(images), names))\n    ps = torch.exp(output)\n    top_p, top_class = ps.topk(1, dim=1)\n    pred = top_class.cpu().numpy().reshape(-1,)\n    for idx, row in batch.iterrows():\n        final_result[row[\"PetID\"]] = pred[idx]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c63f23989a06f9b0c9077ab59c22e308fc503771"},"cell_type":"code","source":"final_df = pd.read_csv(\"../input/test/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11c4664d0ccc1d8ef60dad1a57d50925ace59b02"},"cell_type":"code","source":"final_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"886f9fbb78a4aa1ed3b78c4b2463b54a181a54d0"},"cell_type":"code","source":"for idx, row in final_df.iterrows():\n    final_df.loc[idx,'AdoptionSpeed'] = final_result[row.PetID].astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6512b3f34818b6bf59f85acfedbb8d6b147396a9","scrolled":true},"cell_type":"code","source":"final_df[final_df.AdoptionSpeed != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fee050728e94475725c36f598486a243f8a5e5c"},"cell_type":"code","source":"final_df.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d5cbb4ca37d0c3f536ba422429ef8a019c81400"},"cell_type":"code","source":"pd.read_csv(\"submission.csv\").head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}