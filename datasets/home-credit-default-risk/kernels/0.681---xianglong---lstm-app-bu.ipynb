{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport gc\nimport os\nstart = time.time()\nos.chdir('/kaggle/input/home-credit-default-risk')\n#os.chdir('/Users/xianglongtan/Desktop/kaggle')\n#print(os.getcwd())\n#print(os.listdir())\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_activity = 'all'\n# Any results you write to the current directory are saved as output.\napp_train = pd.read_csv('application_train.csv')\n#app_train = pd.read_csv('application_train.csv')\n#app_train.head()\napp_test = pd.read_csv('application_test.csv')\n#app_test = pd.read_csv('application_test.csv')\n#app_test.head()\nos.chdir('../imputed')\ntnt = pd.read_csv('lstm_app_bu.csv')\nos.chdir('/kaggle/working')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cb4eb4d580cfaedf681f304b57c95ade818b560","collapsed":true},"cell_type":"code","source":"import json\nimport numpy as np\ndef token(X):\n    record = [status for status in X.STATUS.values]\n    status = [json.loads(status) for status in record]\n    sta = []\n    for st in status:\n        sta.extend([s for s in st])\n    return np.matrix(sta).T\nlabel_status = token(tnt)\nfrom sklearn.preprocessing import LabelEncoder\nlabelenc = LabelEncoder()\nint_encode = labelenc.fit(label_status)\nstatus_enc = int_encode.transform(label_status)\ntnt['STATUS'] = tnt['STATUS'].map(lambda x: json.loads(x))\ntnt['STATUS_INT'] = tnt['STATUS'].apply(lambda x: int_encode.transform(x))\ntnt = tnt.drop('STATUS',axis=1)\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import scale\ncols = [x for x in tnt.columns if tnt[x].dtype == 'float64' or tnt[x].dtype == 'int64']\ntnt[cols] = normalize(tnt[cols],axis=0)\ntnt[cols] = scale(tnt[cols],axis=0)\nlength = len(tnt.STATUS_INT.values[0])\ntrain_length=307511\ntrain_X = tnt.loc[:train_length]\ntest_X = tnt.loc[train_length:]\ntrain_Y = app_train['TARGET']\ndel app_train\ngc.collect()\ntrain_X = train_X.drop(['Unnamed: 0','SK_ID_CURR'],axis=1)\ntrain_X.drop([train_X.shape[0]-1],axis=0,inplace=True)\ntest_X.drop('Unnamed: 0',axis=1,inplace=True)\ntest_X.set_index('SK_ID_CURR',inplace=True)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4452c5262beb765ddebea5f979260d7063b3b02e","collapsed":true},"cell_type":"markdown","source":"# LSTM"},{"metadata":{"trusted":true,"_uuid":"50484281f80514ffc2ac0b480c1021842f26bda7"},"cell_type":"code","source":"import tensorflow as tf\ndef reset_graph():\n    if 'sess' in globals() and sess:\n        sess.close()\n    tf.reset_default_graph()\n\nreset_graph()\n\n# Hyper Param\nN_CONT = len([x for x in train_X.columns if train_X[x].dtype == 'float64'])\nBATCH_SIZE = 400\nn_classes = 2\nlength_record = len(train_X.STATUS_INT.values[0])# time steps\nnum_status = max(status_enc)\nCELL_SIZE = 50\nKEEP_PROB = 0.7\nLR = 0.001\nITERATION = 3000\nseed = 0\nEMB_SIZE = 400\n\nclass DataIter():\n    def __init__(self, X,Y,N_CONT,seed):\n        self.X = X\n        self.Y = Y\n        self.N_CONT = N_CONT\n        self.size = len(self.X)\n        self.df = pd.concat([X,Y],axis=1)\n        self.pos = self.df.loc[self.Y == 1]\n        self.neg = self.df.loc[self.Y == 0]\n        self.seed = seed\n    def next_batch(self,n):\n        pos_sample = self.pos.sample(round(n/2), replace=True,random_state=self.seed)\n        neg_sample = self.neg.sample(n-round(n/2), replace=True,random_state=self.seed)\n        res = pd.concat([neg_sample, pos_sample],axis=0)\n        status = np.zeros([n,length_record],dtype=np.int32)\n        for i, status_i in enumerate(status):\n            status_i = res.iloc[:,N_CONT:-1].values[i]\n        return res.iloc[:,:N_CONT].values,status,res.iloc[:,-1].values   \n    \n    \n    \nX_cont = tf.placeholder(tf.float64, shape=(BATCH_SIZE, N_CONT), name='X_CONT')\ny = tf.placeholder(tf.int64, shape=(BATCH_SIZE), name='TARGET')\n\n# Add embeddings\nX = tf.identity(X_cont)\nX = tf.reshape(X, [BATCH_SIZE, 1, N_CONT])\nstatus = tf.placeholder(tf.int32,[BATCH_SIZE,length_record])\nseqlen = tf.constant(length_record)\nseqlen = tf.reshape(seqlen, [1])\nseqlen = tf.tile(seqlen, [BATCH_SIZE])\nembedding = tf.Variable(tf.random_uniform([num_status, EMB_SIZE], -1.0, 1.0))\nembedded_x = tf.nn.embedding_lookup(embedding, status)\nX = tf.tile(X,[1,length_record,1])\nX = tf.cast(X,tf.float32)\nX = tf.concat([embedded_x, X], axis=2)\n\n# RNN\nwith tf.name_scope('RNN'):\n    # input layer\n    rnn_input = tf.layers.dense(X,units = CELL_SIZE)\n    \n    # rnn\n    cell = tf.nn.rnn_cell.GRUCell(CELL_SIZE)\n    init_state = tf.get_variable('init_state', [1,CELL_SIZE],\n                                initializer = tf.constant_initializer(0.0))\n    init_state = tf.tile(init_state, [BATCH_SIZE, 1])\n    rnn_output, final_state = tf.nn.dynamic_rnn(cell, rnn_input, \n                                                sequence_length = seqlen,\n                                                initial_state = init_state)\n    \n    # dropout\n    rnn_output = tf.nn.dropout(rnn_output, KEEP_PROB)\n    \n    # select last output\n    last_rnn_output = tf.gather_nd(rnn_output, tf.stack([tf.range(BATCH_SIZE), seqlen-1], axis=1))\n    last_rnn_output = tf.layers.dense(last_rnn_output, units = 1.5*CELL_SIZE)\n    last_rnn_output = tf.layers.dense(last_rnn_output, units = 0.5*CELL_SIZE)\n    last_rnn_output = tf.layers.dense(last_rnn_output, units = 0.1*CELL_SIZE)\n    \n# Cost\nwith tf.variable_scope('softmax'):\n    W = tf.get_variable('W', [0.1*CELL_SIZE, n_classes])\n    b = tf.get_variable('b', [n_classes], initializer = tf.constant_initializer(0.0))\nlogits = tf.matmul(last_rnn_output, W)+b\npreds = tf.nn.softmax(logits)\nprediction = tf.cast(tf.argmax(preds,1), tf.int32)\nloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = preds))\nprecision, precision_op = tf.metrics.precision(y,prediction)\nrecall, recall_op = tf.metrics.recall(y,prediction)\nf1score = 2*precision*recall/(precision+recall)\nauc,auc_op = tf.metrics.auc(y,preds[:,1])\ntrain_step = tf.train.AdamOptimizer(LR).minimize(loss)\n\n\n# session\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    tr = DataIter(train_X,train_Y,N_CONT,seed)\n    for i in range(ITERATION):\n        x_cont, x_sta, target = tr.next_batch(BATCH_SIZE)\n        feed = {X_cont:x_cont, status:x_sta, y:target}\n        sess.run(train_step, feed_dict=feed)\n        if i%200 == 0:\n            _,prec = sess.run([precision,precision_op], feed_dict=feed)\n            _,rec = sess.run([recall,recall_op], feed_dict=feed)\n            auc_score,auc_update = sess.run([auc,auc_op],feed_dict=feed)\n            f1s = sess.run(f1score, feed_dict=feed)\n            los = sess.run(loss, feed_dict=feed)\n            print('losss after',i,'round',los)\n            print('precision after',i,'round',prec)\n            print('recall after',i,'round',rec)\n            print('F1 score after',i,'round:',f1s)\n            print('auc after',i,'round',auc_update)\n            #print('\\n----------------------------------\\n')\n            #print('logits:\\n',sess.run(logits,feed_dict=feed)[0:10])\n            #print('preds:\\n',sess.run(preds, feed_dict=feed)[0:10])\n            #print('prediction:\\n',sess.run(prediction, feed_dict=feed)[0:10])\n            #print('y:\\n',sess.run(y,feed_dict=feed)[0:10])\n            print('\\n----------------------------------\\n')\n    cursor = 0\n    while cursor <= len(test_X):\n        if cursor+BATCH_SIZE <= len(test_X):\n            cont_test = test_X.iloc[cursor:cursor+BATCH_SIZE, :N_CONT]\n            status_test = test_X.iloc[cursor:cursor+BATCH_SIZE, N_CONT:]\n        else:\n            cont_test = pd.concat([test_X.iloc[cursor:len(test_X), :N_CONT],\n                                   test_X.iloc[0:(BATCH_SIZE-len(test_X.iloc[cursor:len(test_X), :N_CONT])),:N_CONT]],axis=0)\n            print(cont_test.shape)\n            status_test = pd.concat([test_X.iloc[cursor:len(test_X), N_CONT:],\n                                   test_X.iloc[0:(BATCH_SIZE-len(test_X.iloc[cursor:len(test_X), N_CONT:])), :N_CONT]],axis=0)\n        sta_te = np.zeros([BATCH_SIZE,length_record],dtype=np.int32)\n        for i,sta_i in enumerate(sta_te):\n            sta_i = status_test.values[i]\n        results = sess.run(preds, feed_dict={X_cont:cont_test, status:sta_te})\n        if cursor == 0:\n            prediction_test = pd.DataFrame(data=results, columns=['0','TARGET'])\n        else:\n            prediction_test = pd.concat([prediction_test, pd.DataFrame(data=results,columns=['0','TARGET'])])\n        cursor += BATCH_SIZE","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0347a42f5ed00380bf635a37e321c1eea00b7caf"},"cell_type":"code","source":"prediction_test[['TARGET']].head(10)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f974d8ca89f9e9bec3676df31aeccc554884aeca","collapsed":true},"cell_type":"code","source":"result = app_test[['SK_ID_CURR']].merge(prediction_test[['TARGET']].iloc[:len(app_test)].reset_index(drop=True),left_index=True,right_index=True).set_index('SK_ID_CURR')\nresult.to_csv('submission_lstm.csv')","execution_count":63,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e704da2df040d413087104c4bbf31ca30ab935ee","collapsed":true},"cell_type":"code","source":"'''\nbb = pd.read_csv('bureau_balance.csv')\nb = pd.read_csv('bureau.csv')\nos.chdir('../imputed')\n#print(os.listdir())\ntnt = pd.read_csv('train_and_test_imputed.csv')\n#os.chdir('/Users/xianglongtan/Desktop/kaggle/submission')\npd.set_option('display.max_columns',100)\npd.set_option('display.max_rows',200)\ncol = ['SK_ID_CURR','FLAG_OWN_CAR','FLAG_OWN_REALTY','CNT_CHILDREN','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY',\n       'EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']\ntnt = tnt[col]\ndef fn(car, realty):\n    if car == 'N' and realty == 'N':\n        return int(0)\n    elif car == 'Y' and realty == 'N':\n        return int(1)\n    elif car == 'N' and realty == 'Y':\n        return int(2)\n    elif car == 'Y' and realty == 'Y':\n        return int(3)\ntnt['OWN_CAR_REALTY'] = tnt.apply(lambda x: fn(x['FLAG_OWN_CAR'],x['FLAG_OWN_REALTY']), axis=1)\ntnt = tnt.drop(['FLAG_OWN_CAR','FLAG_OWN_REALTY'],axis=1)\nb2 = b.merge(bb, left_on='SK_ID_BUREAU',right_on='SK_ID_BUREAU',how='left',suffixes=['_b','_bb'])\nimport gc\ngc.enable()\ndel bb\ndel b\ngc.collect()\ntnt_b2 = tnt.merge(b2[['SK_ID_CURR','MONTHS_BALANCE','STATUS']], left_on='SK_ID_CURR',right_on='SK_ID_CURR',how='left',suffixes=['_tnt','_b2'])\nmap_dict = {'C':1000000,'X':10000000,'0':1,'1':10,'2':100,'3':1000,'4':10000,'5':100000}\ntnt_b2['STATUS'] = tnt_b2['STATUS'].map(map_dict)\nagg_fun={'STATUS':'sum'}\nstatus = tnt_b2.groupby(['SK_ID_CURR','MONTHS_BALANCE']).agg(agg_fun)\nstatus = status.pivot_table(values='STATUS',index='SK_ID_CURR',columns='MONTHS_BALANCE')\nagg_fun = {'CNT_CHILDREN':'mean',\n        'AMT_INCOME_TOTAL':'mean',\n        'AMT_ANNUITY':'mean',\n        'EXT_SOURCE_1':'mean',\n        'EXT_SOURCE_2':'mean',\n        'EXT_SOURCE_3':'mean',\n        'OWN_CAR_REALTY':'mean'}\nother = tnt_b2.groupby('SK_ID_CURR').agg(agg_fun)\ntnt_b2 = pd.concat([other,status],axis=1).reset_index()\nos.chdir('/kaggle/input/home-credit-default-risk')\napp_train = pd.read_csv('application_train.csv')\ntrain_ix = app_train[['SK_ID_CURR','TARGET']]\ntrain_X = tnt_b2[tnt_b2[['SK_ID_CURR']].isin(train_ix['SK_ID_CURR'].values).values]\ntrain_y = train_ix.copy()\napp_test = pd.read_csv('application_test.csv')\ntest_ix = app_test[['SK_ID_CURR']]\ntest_X = tnt_b2[tnt_b2[['SK_ID_CURR']].isin(test_ix['SK_ID_CURR'].values).values]\ndel tnt,b2\ndel app_train\ndel train_ix\ndel app_test, test_ix\ngc.collect()\ntrain_X = train_X.fillna(0)\ntest_X = test_X.fillna(0)\ncols = [x for x in train_X.columns if str(x).startswith('-') or str(x).startswith('0')]\ntrain_X['STATUS'] = train_X[cols].values.tolist()\ntrain_X = train_X.drop(cols,axis=1)\ntest_X['STATUS'] = test_X[cols].values.tolist()\ntest_X = test_X.drop(cols,axis=1)\ntnt = pd.concat([train_X,test_X],axis=0)\nos.chdir('/kaggle/working')\nos.getcwd()\n#tnt.to_csv('lstm_app_bu.csv')\nend = time.time()\nprint(end-start)\n'''","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c085019f4c0e927597622250db7aa874e9128d0f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}