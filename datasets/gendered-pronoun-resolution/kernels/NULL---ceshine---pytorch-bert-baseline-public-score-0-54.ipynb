{"cells":[{"metadata":{"trusted":true,"_uuid":"eef73f480afe9a4b485f89361916b7469ed986ea"},"cell_type":"markdown","source":"Looks like Matei Ionita beated me to it. Check out his kernel [here](https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline). \n\nI haven't read his kernel carefully yet, but it seems we have basically the same model design. However, his kernel uses Tensorflow and this one uses PyTorch.\n\nSince Matei Ionita alread did a great job explaining the workflow, I won't repeat the same thing here. I'll just let the code do the speaking and comment when necessary."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98b461a0d1e2d27558502f9caefeaf7e47871efc"},"cell_type":"markdown","source":"\"pytorch_helper_bot\" is a thin abstraction of some common PyTorch training routines. It can easily be replaced, so you can mostly ignore it and focus on the preprocessing and model definition instead."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n!pip install https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e128e4337fd5c906540c112bc1d4e0fd2f38ef3"},"cell_type":"code","source":"import os\n\n# This variable is used by helperbot to make the training deterministic\nos.environ[\"SEED\"] = \"33223\"\n\nimport logging\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\nfrom helperbot import BaseBot, TriangularLR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4209e502d9d0c58575d71a7580cabc66bbf7ff70"},"cell_type":"code","source":"BERT_MODEL = 'bert-large-uncased'\nCASED = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d3c64ff3a19456ee88ef77825b83690e5907475"},"cell_type":"code","source":"def insert_tag(row):\n    \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n    to_be_inserted = sorted([\n        (row[\"A-offset\"], \" [A] \"),\n        (row[\"B-offset\"], \" [B] \"),\n        (row[\"Pronoun-offset\"], \" [P] \")\n    ], key=lambda x: x[0], reverse=True)\n    text = row[\"Text\"]\n    for offset, tag in to_be_inserted:\n        text = text[:offset] + tag + text[offset:]\n    return text\n\ndef tokenize(text, tokenizer):\n    \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n    entries = {}\n    final_tokens = []\n    for token in tokenizer.tokenize(text):\n        if token in (\"[A]\", \"[B]\", \"[P]\"):\n            entries[token] = len(final_tokens)\n            continue\n        final_tokens.append(token)\n    return final_tokens, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        if labeled:\n            tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n            tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])\n            self.y = tmp.values.astype(\"bool\")\n        # Extracts the tokens and offsets(positions of A, B, and P)\n        self.offsets, self.tokens = [], []\n        for _, row in df.iterrows():\n            text = insert_tag(row)\n            tokens, offsets = tokenize(text, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n        \n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx], None\n    \ndef collate_examples(batch, truncate_len=500):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"\n    transposed = list(zip(*batch))\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets, None\n    one_hot_labels = torch.stack([\n        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]\n    ], dim=0)\n    _, labels = one_hot_labels.max(dim=1)\n    return token_tensor, offsets, labels\n\nclass Head(nn.Module):\n    \"\"\"The MLP submodule\"\"\"\n    def __init__(self, bert_hidden_size: int):\n        super().__init__()\n        self.bert_hidden_size = bert_hidden_size\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(bert_hidden_size * 3),\n            nn.Dropout(0.5),\n            nn.Linear(bert_hidden_size * 3, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.5),\n            nn.Linear(512, 3)\n        )\n        for i, module in enumerate(self.fc):\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n                print(\"Initing batchnorm\")\n            elif isinstance(module, nn.Linear):\n                if getattr(module, \"weight_v\", None) is not None:\n                    nn.init.uniform_(module.weight_g, 0, 1)\n                    nn.init.kaiming_normal_(module.weight_v)\n                    print(\"Initing linear with weight normalization\")\n                    assert model[i].weight_g is not None\n                else:\n                    nn.init.kaiming_normal_(module.weight)\n                    print(\"Initing linear\")\n                nn.init.constant_(module.bias, 0)\n                \n    def forward(self, bert_outputs, offsets):\n        assert bert_outputs.size(2) == self.bert_hidden_size\n        extracted_outputs = bert_outputs.gather(\n            1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2))\n        ).view(bert_outputs.size(0), -1)\n        return self.fc(extracted_outputs)\n\n    \nclass GAPModel(nn.Module):\n    \"\"\"The main model.\"\"\"\n    def __init__(self, bert_model: str, device: torch.device):\n        super().__init__()\n        self.device = device\n        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n            self.bert_hidden_size = 768\n        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n            self.bert_hidden_size = 1024\n        else:\n            raise ValueError(\"Unsupported BERT model.\")\n        self.bert = BertModel.from_pretrained(bert_model).to(device)\n        self.head = Head(self.bert_hidden_size).to(device)\n    \n    def forward(self, token_tensor, offsets):\n        token_tensor = token_tensor.to(self.device)\n        bert_outputs, _ =  self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(), \n            token_type_ids=None, output_all_encoded_layers=False)\n        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n        return head_outputs            \n\n    \ndef children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\n            \ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n    \n    \nclass GAPBot(BaseBot):\n    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n        avg_window=100, log_dir=\"./cache/logs/\", log_level=logging.INFO,\n        checkpoint_dir=\"./cache/model_cache/\", batch_idx=0, echo=False,\n        device=\"cuda:0\", use_tensorboard=False):\n        super().__init__(\n            model, train_loader, val_loader, \n            optimizer=optimizer, clip_grad=clip_grad,\n            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n            batch_idx=batch_idx, echo=echo,\n            device=device, use_tensorboard=use_tensorboard\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.loss_format = \"%.6f\"\n        \n    def extract_prediction(self, tensor):\n        return tensor\n    \n    def snapshot(self):\n        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n        loss = self.eval(self.val_loader)\n        loss_str = self.loss_format % loss\n        self.logger.info(\"Snapshot loss %s\", loss_str)\n        self.logger.tb_scalars(\n            \"losses\", {\"val\": loss},  self.step)\n        target_path = (\n            self.checkpoint_dir / \"best.pth\")        \n        if not self.best_performers or (self.best_performers[0][0] > loss):\n            torch.save(self.model.state_dict(), target_path)\n            self.best_performers = [(loss, target_path, self.step)]\n        self.logger.info(\"Saving checkpoint %s...\", target_path)\n        assert Path(target_path).exists()\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d534c85ff69192b4dd1ec670fc9c2b9392cc7a62"},"cell_type":"code","source":"df_train = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\ndf_val = pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\ndf_test = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\nsample_sub = pd.read_csv(\"../input/sample_submission_stage_1.csv\")\nassert sample_sub.shape[0] == df_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d58d18c34f5df9ec8f8d8fb048ae6c10fbf9914a"},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=CASED,\n    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n)\n# These tokens are not actually used, so we can assign arbitrary values.\ntokenizer.vocab[\"[A]\"] = -1\ntokenizer.vocab[\"[B]\"] = -1\ntokenizer.vocab[\"[P]\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69689365738454b33649a14d83eea49cc1b18687"},"cell_type":"code","source":"train_ds = GAPDataset(df_train, tokenizer)\nval_ds = GAPDataset(df_val, tokenizer)\ntest_ds = GAPDataset(df_test, tokenizer)\ntrain_loader = DataLoader(\n    train_ds,\n    collate_fn = collate_examples,\n    batch_size=20,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=True,\n    drop_last=True\n)\nval_loader = DataLoader(\n    val_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\ntest_loader = DataLoader(\n    test_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a85f87ed8d73b520e39a5dc07da1838867ac2653"},"cell_type":"code","source":"model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n# You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\nset_trainable(model.bert, False)\nset_trainable(model.head, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"493b0ed0887339dfe818df1a0be17c05a4c97d17"},"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nbot = GAPBot(\n    model, train_loader, val_loader,\n    optimizer=optimizer, echo=True,\n    avg_window=25\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07ea447ea766df3d997779e6c9a8300b7532a049"},"cell_type":"code","source":"steps_per_epoch = len(train_loader) \nn_steps = steps_per_epoch * 5\nbot.train(\n    n_steps,\n    log_interval=steps_per_epoch // 4,\n    snapshot_interval=steps_per_epoch,\n    scheduler=TriangularLR(\n        optimizer, 20, ratio=2, steps_per_cycle=n_steps)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d92655729f678dcbe93a5ee824562b0d23fe275f"},"cell_type":"code","source":"# Load the best checkpoint\nbot.load_model(bot.best_performers[0][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efad23ae0f411fe486e837be0903dd24cab6cba5"},"cell_type":"code","source":"# Evaluate on the test dataset\nbot.eval(test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3b1bc4a4264ebda30d4eca35879f1df6f0a11c3"},"cell_type":"code","source":"# Extract predictions to the test dataset\npreds = bot.predict(test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51b7e2335c8f9c1b821b6aeaba7c5122fe74530a"},"cell_type":"code","source":"# Create submission file\ndf_sub = pd.DataFrame(torch.softmax(preds, -1).cpu().numpy().clip(1e-3, 1-1e-3), columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = df_test.ID\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}