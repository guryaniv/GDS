{"cells":[{"metadata":{"_uuid":"2ced403df7915365b770f32e742c3f325deac8e8"},"cell_type":"markdown","source":"# Costa Rica Poverty Prediction\nMany social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.\n\nIn Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\nWhile this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.\n\nIn this analysis I will attempt to look at models such as KNN, Extra Trees, Random Forest, and Decision Trees to classify households into specific poverty levels. This model will provide value by predicting which current households may need remodelling or gentrification. "},{"metadata":{"trusted":true,"_uuid":"7787582b73379a9fdbe91c867ff740ea8d8d6f25"},"cell_type":"code","source":"#Data Manipulation\nimport pandas as pd\nimport numpy as np\nimport os\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Other Packages\nimport missingno as msno\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08a8f1d0a03d554f4d12c46f470183e0b84c34fa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa06c05dceef6edd14ad200e2cb6518a5c2b9d90"},"cell_type":"markdown","source":"Extract both files "},{"metadata":{"trusted":true,"_uuid":"dce49c082a4ab08141e323a528bb94f8f566ba72"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain_samp = train.sample(frac=.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b39669e96ce2065d059b4be93ec9bd240337ce15"},"cell_type":"code","source":"y = train_samp['Target']\ny_full = train['Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a608bc025f30f548bf2e7a98fba46b77e0cb3f8"},"cell_type":"code","source":"y.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a53fa742ff251714974f3df75dbd7892c49967d"},"cell_type":"code","source":"print(f'Train shape: {train_samp.shape}')\nprint(f'Test shape: {test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61a0df742cd8bb57ccc811654777987a7ecc3385"},"cell_type":"markdown","source":"### Exploratory Data Analysis:"},{"metadata":{"trusted":true,"_uuid":"f4e8d190bd6f9cdde59b2d3f212dc0a849fddee4"},"cell_type":"code","source":"print(train.info())\ntrain.columns[1::]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7531d35ee5aab8c84089e70c4b5f422f9416eae7"},"cell_type":"code","source":"train_samp.select_dtypes('object')\nlen(train.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7845485f358e358bc63fa2c5464fb4dcd2549e19"},"cell_type":"markdown","source":"### Check Missing Values"},{"metadata":{"trusted":true,"_uuid":"75de0a4966d30f3503041f88f8e3e59273b429d5"},"cell_type":"code","source":"msno.matrix(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"645f76173453736c8b2e3a312a7b3f4d0dd8c979"},"cell_type":"code","source":"train_samp.isnull().sum()\n#v2a1, v18q1, \ntrain_samp.columns[train_samp.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12b46f02499f80f82e4e8744eb69bb65d90cb452"},"cell_type":"code","source":"train_samp.select_dtypes('int64')\ntrain_samp.get_dtype_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23d50a9ebf0bde863317f1af6c03e55fc1fb3ac7"},"cell_type":"markdown","source":"### Clean object values\n\nNow we want to make sure the features can be used in modelling. Here we convert our object types to integer converting \"yes\"'s and \"no\"'s to ones and zeroes"},{"metadata":{"trusted":true,"_uuid":"bfe22e1049b8c236ece44e578d476cd860925e6a"},"cell_type":"code","source":"mapping = {\"yes\": 1, \"no\": 0}\n\n# Apply same operation to both train and test\nfor df in [train, test]:\n    # Fill in the values with the correct mapping\n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n\ntrain[['dependency', 'edjefa', 'edjefe']].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7542241c0ec959c02efd7faa9d42720540f0f8de"},"cell_type":"markdown","source":"Get a sample from the training set\n\nHere we want to look at all of the unique values for each feature."},{"metadata":{"trusted":true,"_uuid":"928661a362e9813301f0b298542b5b0c881b1144"},"cell_type":"code","source":"train_samp = train.sample(frac=.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d2b92150284dd043a77656636a6bebda0bec577"},"cell_type":"code","source":"train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', \n                                                                             figsize = (8, 6),\n                                                                            edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f461f44abbf2a4c95d0757ec2ae3e49086959a4"},"cell_type":"code","source":"#Plot densities of float columns\nfrom collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(6, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e41b8f522b0ead674c753eb9c06d1316ac9393d2"},"cell_type":"markdown","source":"Above we can ook at the distributions of each the numeric variables. We then want to split our data and see how our first model performs"},{"metadata":{"trusted":true,"_uuid":"b01bb26a51263b55d83ebb3ad6a26744ae685690"},"cell_type":"code","source":"miss_cols = train_samp.columns[train.isnull().any()]\nmiss_cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4697ab0b1a17ea7a9f5cabf34534701e3e3e6b28"},"cell_type":"markdown","source":"## Impute missing values\n\nThe columns 'v2a1', 'v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned' all contain missing values and we will look to fill them with the mean or mode"},{"metadata":{"trusted":true,"_uuid":"d04cf26e6bd80aeb7556e5d31a0a05ff9de52f6b"},"cell_type":"code","source":"train_samp.rez_esc.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2100c157e4a94db7cdc0490e0ee6adfc1bdb7569"},"cell_type":"code","source":"train_samp.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c06ad4854e222ba412dac3f78e7d2722315c23cf"},"cell_type":"code","source":"#Fill values for v2a1\ntrain_samp['v2a1'] = train_samp['v2a1'].fillna(train_samp['v2a1'].mode()[0])\n\n#Fill values for v18q1\ntrain_samp['v18q1'] = train_samp['v18q1'].fillna(train_samp['v18q1'].mean())\n\n#Fill values for rez_esc\ntrain_samp['rez_esc'] = train_samp['rez_esc'].fillna(train_samp['rez_esc'].mode()[0])\n\n#Fill values for meaneduc\ntrain_samp['meaneduc'] = train_samp['meaneduc'].fillna(train_samp['meaneduc'].mode()[0])\n\n#Fill values for SQBmeaned\ntrain_samp['SQBmeaned'] = train_samp['SQBmeaned'].fillna(train_samp['SQBmeaned'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e27e62962253d48656812fac2c04d6684ee9506d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"452c2b1bf19a719da0b6b2e6245f85dfc1d292bb"},"cell_type":"code","source":"#Fill values for v2a1\ntrain['v2a1'] = train['v2a1'].fillna(train['v2a1'].mode()[0])\n\n#Fill values for v18q1\ntrain['v18q1'] = train['v18q1'].fillna(train['v18q1'].mean())\n\n#Fill values for rez_esc\ntrain['rez_esc'] = train['rez_esc'].fillna(train['rez_esc'].mode()[0])\n\n#Fill values for meaneduc\ntrain['meaneduc'] = train['meaneduc'].fillna(train['meaneduc'].mode()[0])\n\n#Fill values for SQBmeaned\ntrain['SQBmeaned'] = train['SQBmeaned'].fillna(train['SQBmeaned'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31b6ed09b94e0b633da645f8269bfbb71401f7cd"},"cell_type":"code","source":"#Fill values for v2a1\ntest['v2a1'] = test['v2a1'].fillna(test['v2a1'].mode()[0])\n\n#Fill values for v18q1\ntest['v18q1'] = test['v18q1'].fillna(test['v18q1'].mean())\n\n#Fill values for rez_esc\ntest['rez_esc'] = test['rez_esc'].fillna(test['rez_esc'].mode()[0])\n\n#Fill values for meaneduc\ntest['meaneduc'] = test['meaneduc'].fillna(test['meaneduc'].mode()[0])\n\n#Fill values for SQBmeaned\ntest['SQBmeaned'] = test['SQBmeaned'].fillna(test['SQBmeaned'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bdb91d62e77031c4abd17a0f312221e111a49d5"},"cell_type":"code","source":"train_samp.columns[train_samp.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c954c2b038bdf0cc69d7971db85eaf1faaab25f"},"cell_type":"markdown","source":"## Modelling with RandomForest "},{"metadata":{"trusted":true,"_uuid":"4395ca3f17e5067e2b14bac2e38f318b8af440a0"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score, f1_score, make_scorer, precision_recall_fscore_support\n\n# Custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07f64069c5653ca623b0a443e34a6f18ec2cc565"},"cell_type":"code","source":"#Drop Columns from dataset\nX = train_samp.drop(['Id', 'Target', 'idhogar'], axis=1).copy()\nX_full = train.drop(['Id', 'Target', 'idhogar'], axis=1).copy()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b5a70ddfb102d9335ada1fe5493a81eb51ca927"},"cell_type":"markdown","source":"Here we split the data looking at a sample first to iterate with our training set and later split our full data set"},{"metadata":{"trusted":true,"_uuid":"946abf2efe4420d91cc841cfdf2528e28236b2c7"},"cell_type":"code","source":"#Using the sample data set (train_samp) we drop the Id, Target, and idhogar to split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nx_tr, x_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=.2, random_state=42)\n\n#Split full data set\nX_trfull, X_tefull, y_trfull, y_tefull = train_test_split(X_full, y_full, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74cb7248f55a0e2be7b21de0d66e5a7b7e878879"},"cell_type":"code","source":"#pd.concat(y_tefull['idhogar'])\n#pd.merge(type_df, y_tefull, left_index=True)\n#y_tefull.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2212bb4cfac5b772fe39a8a132e4b92e58e43150"},"cell_type":"code","source":"n_classes = y_full.unique().max()\nn_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5b6e31eb9ddd19c74619ef892613f4c794d9824"},"cell_type":"code","source":"#Full training and test set split\nprint(f'X_train: {X_trfull.shape}')\nprint(f'X_test: {X_tefull.shape}')\nprint(f'y_train: {y_trfull.shape}')\nprint(f'y_test: {y_tefull.shape}')\n\n#Training and Test set split\nprint(f'X_train: {X_train.shape}')\nprint(f'X_test: {X_test.shape}')\nprint(f'y_train: {y_train.shape}')\nprint(f'y_test: {y_test.shape}')\n\n#Sample of our training set\nprint(f'Train Sample: {train_samp.shape}')\n\n#Split the data a second time\nprint(f'x_tr: {x_tr.shape}')\nprint(f'y_tr: {y_tr.shape}')\nprint(f'x_val: {y_val.shape}')\nprint(f'y_val: {y_val.shape}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53a525382af67169a64e525f2ce669164c8e2b0b"},"cell_type":"markdown","source":"### Run RandomForest:\n\nn_estimators, n_jobs=-1, class_weights: balanced, max_depth=3"},{"metadata":{"trusted":true,"_uuid":"6a2ea470cbee6b6df850f07d4ade7656f09d9d04"},"cell_type":"code","source":"param_dictionary = {\"n_estimators\": [1000]}\nclf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=3)\n# Press Shift-Tab to look at what the arguments are for a function, as well as the defaults for each argument\ngs = GridSearchCV(clf, param_dictionary, n_jobs=1, verbose=2, cv=2)\ngs.fit(X_trfull, y_trfull)\n# max depth 5, n estimators 500","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa48cfb996c6f74427baa56a8b58d307ada64c5e"},"cell_type":"markdown","source":"We then get predictions on our training set and analyze our score based on precision, recall, f1-score, and support"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"44b5ff56a93de543d977932ca464fb0d582ece94"},"cell_type":"code","source":"val_predictions = gs.predict(X_trfull)\ncr = classification_report(y_trfull, val_predictions)\n#roc_auc = roc_auc_score(y_val, val_predictions)\nprint('Validation Scores:')\nprint(cr)\nprint('-'*50)\n#print(\"ROC AUC Score: {}\".format(roc_auc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab04e41f6ffb63e6dcba67ad6880981f7de04039"},"cell_type":"markdown","source":"The most important features were their average education and SQBmeaned which is the square of the average education of the adults in the household"},{"metadata":{"trusted":true,"_uuid":"7147a518f671f342105cd950c8c722b7a08fe4d7"},"cell_type":"code","source":"feat_imports = sorted(list(zip(X_train.columns, gs.best_estimator_.feature_importances_)), key=lambda x:x[1], reverse=True)\nfeat_imports[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f10483ed8afa569ae42eab25b9938363129113a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f24484b7e403fe73f6b702243e5eae9a10a4f5b6"},"cell_type":"code","source":"clf = RandomForestClassifier(n_jobs=-1, max_depth=5, n_estimators=1000, class_weight='balanced', verbose=1)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"407c4dc137b522b138adc489ba0dc11ad85afcbd"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"7f3739991620c5b0a4a8eaa1c9d182f4e2cc02f3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7ae82b45498ed5a8f3a3d14229955d30d7fac6b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87992da9f8605493b6aefe073e74d4add80d1d8e"},"cell_type":"markdown","source":"### Model Selection:\n\nDid you try multiple models? \nWe then looked at the following models:\n1. Decision Tree\nDecision trees work very well with categorical variables. The tree looks at a set of features and creates a split based on the Gini Index. \n2. Extra Tree Classifier\n3. Random Forest\nRandom Forest and Extra Tree Classifier's work very similar. However, Extra Tree is much faster and chooses a random value when creating a split whereas RF uses the optimal value.\n4. K-Nearest Neighbors\nKNN looks at number of neighbors (we looked at 5, 10, and 20) and uses a weights function. We did not specify a function for the weights.\n\nI evaluated each model based on their cross validation score which creates a new test set to avoid overfitting\n\nWhy did you choose these models? How do they work? What are they assumptions? And how did you test/account for them? How did you select hyper-parameters?"},{"metadata":{"trusted":true,"_uuid":"e6ce327ebea431029137981990139466c149b2e1"},"cell_type":"code","source":"# Dataframe to hold results\nmodel_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\ndef cv_model(train, train_labels, model, name, model_results=None):\n    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n    \n    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model': name, \n                                                           'cv_mean': cv_scores.mean(), \n                                                            'cv_std': cv_scores.std()},\n                                                           index = [0]),\n                                             ignore_index = True)\n\n        return model_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12d60e173336520df036bbf39fe883f36ae61fed"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmodel_results = cv_model(X_tefull, y_tefull, \n                         DecisionTreeClassifier(),\n                         'DT', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92b82dc3e39c5dbaa29dfc8b7b30b9a54dc287bf"},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel_results = cv_model(X_tefull, y_tefull, \n                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n                         'EXT', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3eb3a1a4b37c2ed2ddc12fe828d99d60ef2d9137"},"cell_type":"code","source":"model_results = cv_model(X_tefull, y_tefull, \n                         RandomForestClassifier(n_estimators = 100, random_state = 10),\n                         'RF', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe5050910b38aa3217eeffeb46fa86c85440cdd4"},"cell_type":"code","source":"for n in [5, 10, 20]:\n    print(f'\\nKNN with {n} neighbors\\n')\n    model_results = cv_model(X_tefull, y_tefull, \n                             KNeighborsClassifier(n_neighbors = n),\n                             f'knn-{n}', model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b774518855757dd4ab7b19818275d8f672b95ab"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ee20263551d442dd5b86ced61b07af4bdcb3f078"},"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                  edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3896c1baeb203f865aee5b73654e98aa7490a8a1"},"cell_type":"markdown","source":"### Model evaluation: \nDid you evaluate your model on multiple metrics? Where does your model do well? Where could it be improved? How are the metrics different?\n\nOriginally we looked at the f1 score, precision, and recall for RandomForest. We then look at just the f1 score. I think it could be improved "},{"metadata":{"trusted":true,"_uuid":"f51f63af8d3a5616763491c0da391f7c63344ff1"},"cell_type":"code","source":"def pred_and_score(model, train, train_labels, test, test_ids):\n    \"\"\"Train and test a model on the dataset\"\"\"\n    \n    # Train on the data\n    model.fit(train, train_labels)\n    \n    predictions = model.predict(test)\n    predictions = pd.DataFrame({'idhogar': test_ids,\n                               'Target': predictions})\n    #Compute the mean accuracy\n    scores = model.score(test, test_ids)\n    \n    #Get most important features\n    imp_feats = sorted(list(zip(test.columns, model.feature_importances_)), key=lambda x:x[1], reverse=True)\n    imp_feats = imp_feats[0:10]\n\n    return predictions, test_ids, scores, imp_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71f4c579f7750752e23fdea01c00f295cdb6703b"},"cell_type":"code","source":"test1 = test.drop(['Id', 'idhogar'], axis=1)\ntest1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ef7aaccb3e0ea4c069ff71433ca6aa759fb1c2c"},"cell_type":"code","source":"predictions, true_values, scores, imp_feats = pred_and_score(ExtraTreesClassifier(n_estimators = 100, random_state = 10), \n                         X_trfull, y_trfull, test1, test.idhogar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0799c037bb27b351405321f4ce2517d6b02843f"},"cell_type":"code","source":"true_values.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00f46061f13d44f26c171e44fe7760974d567f38"},"cell_type":"code","source":"\ncr = precision_recall_fscore_support(predictions['Target'], y_trfull, average='macro')\n#roc_auc = roc_auc_score(y_val, val_predictions)\nprint('Test Scores:')\nprint('-'*50)\n\nprint(f'precision: {cr[0]}')\nprint(f'recall: {cr[1]}')\nprint(f'f1-score: {cr[2]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2da3ac84e8be13801ef14085f8e3ad022c5d6447"},"cell_type":"code","source":"print(f'Accuracy Score: {scores}')\nprint(f'Important Features: {imp_feats}')\n#print(f'Evaluation Metrics: {true_values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2394d771a5074eea66244ff3801672e0f715e82"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27d04e1fd4b1ac416ac9ac12061308db11b175e0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0a08c7a322e1aa8a29cae2e86383a5a274d0713"},"cell_type":"markdown","source":"### Model interpretation: \nWhat do the model results tell you? Which variables are important? High bias or variance and how did you / could you fix this? How confident are you in your results?\n\nAfter running the extra trees classifier model on the full test data, we predicted with a mean accuracy of 94%. The variables that are most important are \"meaneduc\" and \"SQBmeaned\". To find high bias or variance we can see if we are over or underfitting. To fix high bias we need to add more features to prevent underfitting and to fix high variance we need to add more data or create synthetic data to prevent overfitting. "},{"metadata":{"trusted":true,"_uuid":"85b8fb6566c09d8471356f816ef860c4167c38bd"},"cell_type":"code","source":"clf = ExtraTreesClassifier(n_estimators = 100, random_state = 10)\nclf.fit(X_trfull, y_trfull)\nclf.score(X_tefull, y_tefull)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a78d391b91fcd59f5fd1cc8a371b9927a2430f2"},"cell_type":"markdown","source":"## Make a submission"},{"metadata":{"trusted":true,"_uuid":"b709ec9a49a354c598f8c51a86d9c9b2235a1ffe"},"cell_type":"code","source":"predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7df2749b92164278e5d704763689bb3ca6df592e"},"cell_type":"code","source":"#I want to match idhogars from the full dataset to the predicted values\nsubmission = pd.merge(train['idhogar'].to_frame(), predictions, left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24ef35878f2223f2e874f40a31557f5dea628a44"},"cell_type":"code","source":"submission = submission.drop('idhogar_y', axis=1)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a87721e29e64db5c7d24f596841ec2689f9b38d"},"cell_type":"code","source":"submission.columns = ['Id', 'Target']\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64a733958418542269bdd5e4b924e144f4a1f957"},"cell_type":"code","source":"# Fill in households missing a head\nsubmission['Target'] = submission['Target'].fillna(4).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e341cd3f2de2f6751190e6022701f2a8659e0954"},"cell_type":"code","source":"submission.to_csv('Costa_Rica_Predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbd9387b0f45fcb8794375fb9f3342dc280f571e"},"cell_type":"markdown","source":"### Model usefulness:\nDo you think your final model was useful? If so, how would you recommend using it? Convince us, that if we were a company, we would feel comfortable using your model with our users. Think about edge cases as well - are there certain areas that the model performs poorly on? Best on? How would you handle these cases, if say Zillow wanted to leverage your model realizing that bad recommendations on sale prices would hurt customer trust and your brand. This section also falls into the storytelling aspect of the grading.\n\nI think the model is very useful. It could be used to look at general economic status for government budgeting. This could be used to determine which households need specific funding.\n\nAnother application is to predict & forecast which households are entering poverty in a 5 year period. This model may not be generalizable for countries outside of Costa Rica and may only work for countries with similar observable characteristics. "},{"metadata":{"trusted":false,"_uuid":"868a52f04c9dff7542cd13ffb038c0c59558a922"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0312d0f95931deb7195d9606009e555134ea3aed"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}