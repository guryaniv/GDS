{"cells":[{"metadata":{"_uuid":"e4300d336764e02dfde42320864aacf2ff0f6810"},"cell_type":"markdown","source":"## General information\n\n![](https://jivasquez.files.wordpress.com/2017/08/cactus_0163.jpg?w=616)\n\nResearchers in Mexico have created the VIGIA project, aiming to build a system for autonomous surveillance of protected areas. One of the first steps is being able to recognize the vegetation in the area. In this competition we are trying to identify whether there is a cactus in the image.\n\nIn this kernel I use kekas (https://github.com/belskikh/kekas) as a wrapper for Pytorch.\n\nMost of the code is taken from my other kernel: https://www.kaggle.com/artgor/cancer-detection-with-kekas"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time \nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom sklearn.metrics import accuracy_score\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7173f224c494b91252292183215af5d440664e3a"},"cell_type":"markdown","source":"Some of good libraries for DL aren't available in Docker with GPU by default, so it is necessary to install them. (don't forget to turn on internet connection in kernels)."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install albumentations > /dev/null 2>&1\n!pip install pretrainedmodels > /dev/null 2>&1\n!pip install kekas > /dev/null 2>&1\n!pip install adabound > /dev/null 2>&1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe38bebcb4ea05c4911fe5f8c082a35e4f2313b2","_kg_hide-input":true},"cell_type":"code","source":"# more imports\nimport albumentations\nfrom albumentations import torch as AT\nimport pretrainedmodels\nimport adabound\n\nfrom kekas import Keker, DataOwner, DataKek\nfrom kekas.transformations import Transformer, to_torch, normalize\nfrom kekas.metrics import accuracy\nfrom kekas.modules import Flatten, AdaptiveConcatPool2d\nfrom kekas.callbacks import Callback, Callbacks, DebuggerCallback\nfrom kekas.utils import DotDict","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78e819fbe27b97a333ab1cc839e248da021f5abe"},"cell_type":"markdown","source":"## Data overview"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"f5ae4e09dbfaf631f728b89cc1e1e77e4345d31a"},"cell_type":"code","source":"labels = pd.read_csv('../input/train.csv')\nfig = plt.figure(figsize=(25, 8))\ntrain_imgs = os.listdir(\"../input/train/train\")\nfor idx, img in enumerate(np.random.choice(train_imgs, 20)):\n    ax = fig.add_subplot(4, 20//4, idx+1, xticks=[], yticks=[])\n    im = Image.open(\"../input/train/train/\" + img)\n    plt.imshow(im)\n    lab = labels.loc[labels['id'] == img, 'has_cactus'].values[0]\n    ax.set_title(f'Label: {lab}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"122170680f9fbe6a3a135eaa3d207479c6cde800"},"cell_type":"markdown","source":"Images were resized, so I can see almost nothing in them..."},{"metadata":{"_uuid":"d73fec4f3d654dd659eaeabe798b86cb51cdd735"},"cell_type":"markdown","source":"Kekas accepts pandas DataFrame as an input and iterates over it to get image names and labels"},{"metadata":{"trusted":true,"_uuid":"407295171e006c5748c2e54154072cc1d9e9c1e9"},"cell_type":"code","source":"test_img = os.listdir('../input/test/test')\ntest_df = pd.DataFrame(test_img, columns=['id'])\ntest_df['has_cactus'] = -1\ntest_df['data_type'] = 'test'\n\nlabels['has_cactus'] = labels['has_cactus'].astype(int)\nlabels['data_type'] = 'train'\n\nlabels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"32d601d9c77c47322d9b62eb9dfbafa0f6d4c15b"},"cell_type":"code","source":"labels.loc[labels['data_type'] == 'train', 'has_cactus'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afef03816bd2f3484304455381589cc4f4b3d017"},"cell_type":"markdown","source":"We have some disbalance in the data, but it isn't too big."},{"metadata":{"trusted":true,"_uuid":"6257cde2df89af858dfdcfb0304846794caf739d"},"cell_type":"code","source":"# splitting data into train and validation\ntrain, valid = train_test_split(labels, stratify=labels.has_cactus, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca2e495dcbdb6a97cc078dcbed248a5f18438bc9"},"cell_type":"markdown","source":"### Reader function\n\nAt first it is necessary to create a reader function, which will open images. It accepts i and row as input (like from pandas iterrows). The function should return a dictionary with image and label.\n[:,:,::-1] - is a neat trick which converts BGR images to RGB, it works faster that converting to RGB by usual means.\n"},{"metadata":{"trusted":true,"_uuid":"28327c232395ce4ea4b07a500ad908ab07a0db0d"},"cell_type":"code","source":"def reader_fn(i, row):\n    image = cv2.imread(f\"../input/{row['data_type']}/{row['data_type']}/{row['id']}\")[:,:,::-1] # BGR -> RGB\n    label = torch.Tensor([row[\"has_cactus\"]])\n    return {\"image\": image, \"label\": label}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5e014d27eca2988d9d06da1c535281d074d1cdb"},"cell_type":"markdown","source":"### Data transformation\n\nNext step is defining data transformations and augmentations. This differs from standard PyTorch way. We define resizing, augmentations and normalizing separately, this allows to easily create separate transformers for train and valid/test data.\n\nAt first we define augmentations. We create a function with a list of augmentations (I prefer albumentation library: https://github.com/albu/albumentations)"},{"metadata":{"trusted":true,"_uuid":"4ef80a7784ed3ca084807d7f1ed895b597a0132f"},"cell_type":"code","source":"def augs(p=0.5):\n    return albumentations.Compose([\n        albumentations.HorizontalFlip(),\n        albumentations.VerticalFlip(),\n        albumentations.RandomBrightness(),\n    ], p=p)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88fdfa5c67a7e546f8416001cb43788c2410ab1e"},"cell_type":"markdown","source":"Now we create a transforming function. It heavily uses Transformer from kekas.\n\n  * The first step is defining resizing. You can change arguments of function if you want images to have different height and width, otherwis you can leave it as it is.\n  * Next step is defining augmentations. Here we provide the key of image which is defined in reader_fn;\n  * The third step is defining final transformation to tensor and normalizing;\n  * After this we can compose separate transformations for train and valid/test data;"},{"metadata":{"trusted":true,"_uuid":"1bd8d3c74eb7d6b4ea6d690eadfce4b207a16adb"},"cell_type":"code","source":"def get_transforms(dataset_key, size, p):\n\n    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n\n    AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[\"image\"])\n\n    NRM_TFMS = transforms.Compose([\n        Transformer(dataset_key, to_torch()),\n        Transformer(dataset_key, normalize())\n    ])\n    \n    train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n    val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])\n    \n    return train_tfms, val_tfms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9751259fe83a26aa6ef332761baab318fa367fa"},"cell_type":"code","source":"train_tfms, val_tfms = get_transforms(\"image\", 32, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad3aca42a6ce1bef46dd41f87f598964335caf78"},"cell_type":"markdown","source":"Now we can create a DataKek, which is similar to creating dataset in Pytorch. We define the data, reader function and transformation.Then we can define standard PyTorch DataLoader."},{"metadata":{"trusted":true,"_uuid":"6aa7bc2518ee9b7ac07ec4b50270b653c9f9477b"},"cell_type":"code","source":"train_dk = DataKek(df=train, reader_fn=reader_fn, transforms=train_tfms)\nval_dk = DataKek(df=valid, reader_fn=reader_fn, transforms=val_tfms)\n\nbatch_size = 64\nworkers = 0\n\ntrain_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True)\nval_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bca06d4ab875017085b3e5a1d2771a1cc1d61346"},"cell_type":"code","source":"test_dk = DataKek(df=test_df, reader_fn=reader_fn, transforms=val_tfms)\ntest_dl = DataLoader(test_dk, batch_size=batch_size, num_workers=workers, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c72f17dd2c7f5ba98b14c2504f59a342b9a6e69a"},"cell_type":"markdown","source":"### Building a neural net\n\nHere we define the architecture of the neural net.\n\n* Pre-trained backbone is taken from pretrainedmodels: https://github.com/Cadene/pretrained-models.pytorch Here I take densenet169\n* We also define changes to the architecture. For example, we take off the last layer and add a custom head with nn.Sequential. AdaptiveConcatPool2d is a layer in kekas, which concats AdaptiveMaxPooling and AdaptiveAveragePooling"},{"metadata":{"trusted":true,"_uuid":"0e06385441ca8e39c066161a086fd88ad7937872"},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(\n            self,\n            num_classes: int,\n            p: float = 0.2,\n            pooling_size: int = 2,\n            last_conv_size: int = 1664,\n            arch: str = \"densenet169\",\n            pretrained: str = \"imagenet\") -> None:\n        \"\"\"A simple model to finetune.\n        \n        Args:\n            num_classes: the number of target classes, the size of the last layer's output\n            p: dropout probability\n            pooling_size: the size of the result feature map after adaptive pooling layer\n            last_conv_size: size of the flatten last backbone conv layer\n            arch: the name of the architecture form pretrainedmodels\n            pretrained: the mode for pretrained model from pretrainedmodels\n        \"\"\"\n        super().__init__()\n        net = pretrainedmodels.__dict__[arch](pretrained=pretrained)\n        modules = list(net.children())[:-1]  # delete last layer\n        # add custom head\n        modules += [nn.Sequential(\n            # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling \n            # AdaptiveConcatPool2d(size=pooling_size),\n            Flatten(),\n            nn.BatchNorm1d(1664),\n            nn.Dropout(p),\n            nn.Linear(1664, num_classes)\n        )]\n        self.net = nn.Sequential(*modules)\n\n    def forward(self, x):\n        logits = self.net(x)\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88d3e9d7334e67c829622beffc6fcedffa468656"},"cell_type":"markdown","source":"The data for training needs to be transformed one more time - we define DataOwner, which contains all the data. For now let's define it for train and valid.\nNext we define model and loss. As I choose BCEWithLogitsLoss, we can set the number of classes for output to 1."},{"metadata":{"trusted":true,"_uuid":"78537868b028af513767ae62250c6cdf3e2eb2b6"},"cell_type":"code","source":"dataowner = DataOwner(train_dl, val_dl, None)\nmodel = Net(num_classes=1)\ncriterion = nn.BCEWithLogitsLoss()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c45c3820fbc4579ab827356888669c35a8b36de"},"cell_type":"markdown","source":"And now we define what will the model do with the data. For example we could slice the output and take only a part of it. For now we will simply return the output of the model."},{"metadata":{"trusted":true,"_uuid":"4e569fc396e06fb7c3ea41d7db440e04d0365068"},"cell_type":"code","source":"def step_fn(model: torch.nn.Module,\n            batch: torch.Tensor) -> torch.Tensor:\n    \"\"\"Determine what your model will do with your data.\n\n    Args:\n        model: the pytorch module to pass input in\n        batch: the batch of data from the DataLoader\n\n    Returns:\n        The models forward pass results\n    \"\"\"\n    \n    inp = batch[\"image\"]\n    return model(inp)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"217c368ca9f3c0adfea750081a2bb35cc84bd769"},"cell_type":"markdown","source":"Defining custom metrics"},{"metadata":{"trusted":true,"_uuid":"e62455e4ae1720b509303cc9b1b077fd7628e271"},"cell_type":"code","source":"def bce_accuracy(target: torch.Tensor,\n                 preds: torch.Tensor,\n                 thresh: bool = 0.5) -> float:\n    target = target.cpu().detach().numpy()\n    preds = (torch.sigmoid(preds).cpu().detach().numpy() > thresh).astype(int)\n    return accuracy_score(target, preds)\n  \ndef roc_auc(target: torch.Tensor,\n                 preds: torch.Tensor) -> float:\n    target = target.cpu().detach().numpy()\n    preds = torch.sigmoid(preds).cpu().detach().numpy()\n    return roc_auc_score(target, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f46d71fb7c5eb244dfed1eedbf7dafb5c0938046"},"cell_type":"markdown","source":"### Keker\n\nNow we can define the Keker - the core Kekas class for training the model.\n\nHere we define everything which is necessary for training:\n\n* the model which was defined earlier;\n* dataowner containing the data for training and validation;\n* criterion;\n* step function;\n* the key of labels, which was defined in the reader function;\n* the dictionary with metrics (there can be several of them);\n* The optimizer and its parameters;\n"},{"metadata":{"trusted":true,"_uuid":"df40a12f0d2ea56e697f269b7f3728b459d51b1f"},"cell_type":"code","source":"keker = Keker(model=model,\n              dataowner=dataowner,\n              criterion=criterion,\n              step_fn=step_fn,\n              target_key=\"label\",\n              metrics={\"acc\": bce_accuracy, 'auc': roc_auc},\n              opt=torch.optim.SGD,\n              opt_params={\"momentum\": 0.99})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"312a7374cfcdc436495918d5def82afb35111cad"},"cell_type":"code","source":"keker.unfreeze(model_attr=\"net\")\n\nlayer_num = -1\nkeker.freeze_to(layer_num, model_attr=\"net\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"507c0415fa472bb745ff2161500bfd1722f1b082"},"cell_type":"code","source":"keker.kek_one_cycle(max_lr=1e-2,                  # the maximum learning rate\n                    cycle_len=4,                  # number of epochs, actually, but not exactly\n                    momentum_range=(0.95, 0.85),  # range of momentum changes\n                    div_factor=25,                # max_lr / min_lr\n                    increase_fraction=0.3,        # the part of cycle when learning rate increases\n                    logdir='train_logs')\nkeker.plot_kek('train_logs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51e6b2072e352c7893b8e88a09afd37e7091ba90"},"cell_type":"code","source":"\nkeker.kek_one_cycle(max_lr=1e-3,                  # the maximum learning rate\n                    cycle_len=4,                  # number of epochs, actually, but not exactly\n                    momentum_range=(0.95, 0.85),  # range of momentum changes\n                    div_factor=25,                # max_lr / min_lr\n                    increase_fraction=0.2,        # the part of cycle when learning rate increases\n                    logdir='train_logs1')\nkeker.plot_kek('train_logs1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"283506c3ef2e101619a48030d2b47bef22c7e902"},"cell_type":"markdown","source":"### Predicting and TTA\n\nSimply predicting on test data is okay, but it is better to use TTA - test time augmentation. Let's see how it can be done with Kekas.\n\n* define augmentations;\n* define augmentation function;\n* create objects with these augmentations;\n* put these objects into a single dictionary;"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3986589f8ba513243d04fb021e10f644b7c5467a"},"cell_type":"code","source":"preds = keker.predict_loader(loader=test_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"78c8858f62408cfacadc40bda38cd3b3f9c07bf4"},"cell_type":"code","source":"# flip_ = albumentations.HorizontalFlip(always_apply=True)\n# transpose_ = albumentations.Transpose(always_apply=True)\n\n# def insert_aug(aug, dataset_key=\"image\", size=224):    \n#     PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n    \n#     AUGS = Transformer(dataset_key, lambda x: aug(image=x)[\"image\"])\n    \n#     NRM_TFMS = transforms.Compose([\n#         Transformer(dataset_key, to_torch()),\n#         Transformer(dataset_key, normalize())\n#     ])\n    \n#     tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n#     return tfm\n\n# flip = insert_aug(flip_)\n# transpose = insert_aug(transpose_)\n\n# tta_tfms = {\"flip\": flip, \"transpose\": transpose}\n\n# # third, run TTA\n# keker.TTA(loader=test_dl,                # loader to predict on \n#           tfms=tta_tfms,                # list or dict of always applying transforms\n#           savedir=\"tta_preds1\",  # savedir\n#           prefix=\"preds\")               # (optional) name prefix. default is 'preds'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb46b58550a6b977d51f17ed32b5fc362460d22d"},"cell_type":"code","source":"# prediction = np.zeros((test_df.shape[0], 1))\n# for i in os.listdir('tta_preds1'):\n#     pr = np.load('tta_preds1/' + i)\n#     prediction += pr\n# prediction = prediction / len(os.listdir('tta_preds1'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba92a2269b591bc808a3324d11c8cfa96332eda0"},"cell_type":"code","source":"test_preds = pd.DataFrame({'imgs': test_df.id.values, 'preds': preds.reshape(-1,)})\ntest_preds.columns = ['id', 'has_cactus']\ntest_preds.to_csv('sub.csv', index=False)\ntest_preds.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}