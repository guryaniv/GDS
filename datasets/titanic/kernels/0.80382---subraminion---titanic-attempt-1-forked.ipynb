{"metadata": {"_change_revision": 0, "_is_fork": false, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "version": "3.6.4", "nbconvert_exporter": "python", "mimetype": "text/x-python", "name": "python", "file_extension": ".py"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "1ee6820e-3961-849c-e85b-166a482af606", "_uuid": "d8dc0c4614fe249c807933c379308a7361060526"}, "source": ["# Titanic Competition from Kaggle\n", "\n", "The \"Titanic: Machine Learning from Disaster\" is a good competition to get started with ML hands-on. So, for beginners in ML I highly recommend it a try.\n", "\n", "I created this code using python to predict the survival labels for the test set in this competition. The highest score I got was 0.77990 for the accuracy of the model. In the following paragraphs I will present the steps I went through to get this score.\n", "\n", "**Note:** Keep in mind that this tutorial is just as a simple starting point and will be useful for beginners. Many more explorations and optimizations could be done. If you have any comments about this tutorial please let me know. \n", "\n", "In this tutorial I will present basic steps of a data science pipeline:\n", "\n", "#### 1. Data exploration and visualization  \n", "   - Explore dataset\n", "   - Choose important features and visualize them according to survival/non-survival\n", "   \n", "#### 2. Data cleaning, Feature selection and Feature engineering\n", "   - Null values\n", "   - Encode categorical data\n", "   - Transform features\n", "   \n", "#### 3. Test different classifiers \n", "   - Logistic Regression (LR)\n", "   - K-NN\n", "   - Support Vector Machines (SVM)\n", "   - Naive Bayes\n", "   - Random Forest (RF)\n", " \n", " "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e077f539-ad91-7277-41d9-1f1246092189", "_uuid": "766088dae21fcf48d46aa5ff17acf3cea4ab0cb1"}, "source": ["First let's start by importing the essential libraries to work with dataframes (**pandas**), numeric values (**numpy**) and visualization (**matplotlib.pyplot**)."]}, {"cell_type": "code", "metadata": {"_cell_guid": "2dfc55af-0bc1-cae3-14b1-b16ccbaa9a16", "collapsed": true, "_uuid": "9a444673e95d27f5a0bc54f0ed9c0e208d43cabf"}, "source": ["# Importing the libraries\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "pd.options.display.max_columns = 100\n", "import matplotlib\n", "matplotlib.style.use('ggplot')\n", "import warnings\n", "import numpy as np\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "a2bacde5-f0d4-566c-ffdc-512c64ddb9f5", "collapsed": true, "_uuid": "0396adac33b46e71ba52accdf672d7ced7cdf851"}, "source": ["dataset= pd.read_csv(\"../input/train.csv\")"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "a51d93fc-1e64-f21e-0890-0554ed365f2e", "_uuid": "d3bdc7a8d0ae885208e917e947a079a6cfede895"}, "source": ["%matplotlib inline\n", "import seaborn\n", "seaborn.set()\n", "dataset.describe()\n", "dataset.sample(5)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["#There are only 714 entries out of 891 for age. \n", "#Substituting for the missing data by using median\n", "dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n", "dataset.describe()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "cea9edcb-cedc-2d41-07b0-5dcc86ddf6ef", "_uuid": "4344a673b3e9d5465d1c0f8cf57733ac707388da"}, "source": ["# Visualizing survival by class\n", "survived_class  = dataset[dataset['Survived'] == 1]['Pclass'].value_counts()\n", "dead_class = dataset[dataset['Survived'] == 0]['Pclass'].value_counts()\n", "df_class = pd.DataFrame([survived_class, dead_class])\n", "df_class.index = ['Survived', 'Dead']\n", "df_class.plot(kind = 'bar', stacked = True, figsize = (5,5), title = 'Statistics by class')\n", "\n", "class1_survived = df_class.iloc[0,0]/df_class.iloc[:, 0].sum()*100\n", "class2_survived = df_class.iloc[0,1]/df_class.iloc[:, 1].sum()*100\n", "class3_survived = df_class.iloc[0,2]/df_class.iloc[:, 2].sum()*100\n", "\n", "print(\"Percentage of class 1 that survived:\", class1_survived, \"%\")\n", "print(\"Percentage of class 2 that survived:\", class2_survived, \"%\")\n", "print(\"Percentage of class 3 that survived:\", class3_survived, \"%\")\n", "\n", "from IPython.display import display\n", "display(df_class)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "d6b077b8-4d61-e44a-84cd-d03cb9b3d9f6", "_uuid": "02c0bde306fa2cb88dcb97bba987ea67d51c01cc"}, "source": ["# Visualizing survival by gender\n", "survived_sex  = dataset[dataset['Survived'] == 1]['Sex'].value_counts()\n", "dead_sex = dataset[dataset['Survived'] == 0]['Sex'].value_counts()\n", "df_sex = pd.DataFrame([survived_sex, dead_sex])\n", "df_sex.index = ['Survived', 'Dead']\n", "df_sex.plot(kind = 'bar', stacked = True, figsize = (5,5), title = 'Statistics by sex')\n", "\n", "female_survived = df_sex.iloc[0,0]/df_sex.iloc[:, 0].sum()*100\n", "male_survived = df_sex.iloc[0,1]/df_sex.iloc[:, 1].sum()*100\n", "\n", "print(\"Percentage of male that survived:\", male_survived, \"%\")\n", "print(\"Percentage of female that survived:\", female_survived, \"%\")\n", "\n", "from IPython.display import display\n", "display(df_sex)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# Visualizing survival by age\n", "figure  = plt.figure(figsize = (15,8))\n", "plt.hist([dataset[dataset['Survived'] == 1]['Age'], dataset[dataset['Survived'] == 0]['Age']], stacked = True, color = ['g', 'r'], \n", "        bins = 50, label = ['Survived', 'Dead'])\n", "plt.xlabel('Age')\n", "plt.ylabel('number of passengers survived')\n", "plt.legend()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "387d5d13-49f2-1d66-38c4-34e1df20c121", "_uuid": "32733128de6bcfb325156bc858ae27a49e99801e"}, "source": ["# Visualizing survival by fare\n", "figure  = plt.figure(figsize = (15,8))\n", "plt.hist([dataset[dataset['Survived'] == 1]['Fare'], dataset[dataset['Survived'] == 0]['Fare']], stacked = True, color = ['g', 'r'], \n", "        bins = 50, label = ['Survived', 'Dead'])\n", "plt.xlabel('Age')\n", "plt.ylabel('number of passengers survived')\n", "plt.legend()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["#comparing the survival stats for fare vs age\n", "plt.figure(figsize=(15,8))\n", "ax = plt.subplot()\n", "ax.scatter(dataset[dataset['Survived']==1]['Age'],dataset[dataset['Survived']==1]['Fare'],c='green',s=40)\n", "ax.scatter(dataset[dataset['Survived']==0]['Age'],dataset[dataset['Survived']==0]['Fare'],c='red', s=40)\n", "ax.set_xlabel('Age')\n", "ax.set_ylabel('Fare')\n", "ax.legend(('survived','dead'),scatterpoints=1,loc='upper right',fontsize=15,)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "07df4aa0-b658-32e7-e781-2c26a2dd187b", "_uuid": "856d6f9d6f31a79a0e240bb2850bac8d4f16ed62"}, "source": ["# visualizing survival for embarkation site\n", "survived_embark = dataset[dataset['Survived']==1]['Embarked'].value_counts()\n", "dead_embark = dataset[dataset['Survived']==0]['Embarked'].value_counts()\n", "df = pd.DataFrame([survived_embark,dead_embark])\n", "df.index = ['Survived','Dead']\n", "df.plot(kind='bar', stacked=True, figsize=(15,8))\n", "\n", "embarked_s = df.iloc[0,0]/df.iloc[:,0].sum()*100\n", "embarked_c = df.iloc[0,1]/df.iloc[:,1].sum()*100\n", "embarked_q = df.iloc[0,2]/df.iloc[:,2].sum()*100\n", "\n", "print(\"Number of people survived who embarked in S\", embarked_s, \"%\")\n", "print(\"Number of people survived who embarked in C\", embarked_c, \"%\")\n", "print(\"Number of people survived who embarked in Q\", embarked_q, \"%\")\n", "\n", "from IPython.display import display\n", "display(df)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "6b654220-d347-88f2-3c2e-8159aa9cd887", "_uuid": "8c9305f30d80a1616cf335e134824e993af13472"}, "source": ["**Feature Engineering**\n", "\n", "Extracting features from the text based dataset inputs and processing them"]}, {"cell_type": "code", "metadata": {"_cell_guid": "fdbf5bc4-1daf-2f04-4480-6889f0dd5994", "_uuid": "6c57dd13ef4a113d54018d6d4408f3ded6438360"}, "source": ["def status(feature):\n", "    \n", "    print ('Processing', feature, ':ok')\n", "       "], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "51107085-0fcc-d28f-8de8-fa7e2b2379c6", "collapsed": true, "_uuid": "6aba1b1245398359bc7bb12ee6c1e5e93e3cd1ed"}, "source": ["**Loading the test and training data together to check for interesting patterns and correlations**"]}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def get_combined_data():\n", "    #reading training data\n", "    train = pd.read_csv('../input/train.csv')\n", "    #reading test data\n", "    test = pd.read_csv('../input/test.csv')\n", "    \n", "    #extracting and then removing targets from the training data\n", "    targets = train.Survived\n", "    train.drop('Survived', 1, inplace = True)\n", "    \n", "    #merging test and training data for feature engineering\n", "    combined = train.append(test)\n", "    combined.reset_index(inplace = True)\n", "    combined.drop('index', 1, inplace = True)\n", "    \n", "    return combined"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["combined = get_combined_data()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["combined.shape"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["combined.sample(5)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "40369ea6-f78a-a8b6-3391-6e0c79de4c07", "collapsed": true, "_uuid": "18b57be39924ba8bb88fb0aa51d5cd88e41209b0"}, "source": ["def get_titles():\n", "    \n", "    global combined\n", "    \n", "    #extract titles from each name\n", "    combined['Title'] = combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n", "    \n", "    Title_dictionary = {\n", "                        \"Capt\":       \"Officer\",\n", "                        \"Col\":        \"Officer\",\n", "                        \"Major\":      \"Officer\",\n", "                        \"Jonkheer\":   \"Royalty\",\n", "                        \"Don\":        \"Royalty\",\n", "                        \"Sir\" :       \"Royalty\",\n", "                        \"Dr\":         \"Officer\",\n", "                        \"Rev\":        \"Officer\",\n", "                        \"the Countess\":\"Royalty\",\n", "                        \"Dona\":       \"Royalty\",\n", "                        \"Mme\":        \"Mrs\",\n", "                        \"Mlle\":       \"Miss\",\n", "                        \"Ms\":         \"Mrs\",\n", "                        \"Mr\" :        \"Mr\",\n", "                        \"Mrs\" :       \"Mrs\",\n", "                        \"Miss\" :      \"Miss\",\n", "                        \"Master\" :    \"Master\",\n", "                        \"Lady\" :      \"Royalty\"\n", "                        }\n", "    \n", "    #we map each title\n", "    combined['Title'] = combined.Title.map(Title_dictionary)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "5209a5b3-cdbe-911e-5c71-9a77594f8859", "collapsed": true, "_uuid": "88bef426a39b07ffd20192456a013e7746020206"}, "source": ["get_titles()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "e0a7684c-c3a2-f223-7e88-278d1b8fef65", "_uuid": "4c6e7f4a970982a3bccd9ea7341cfd675e35c895"}, "source": ["combined.sample(5)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "d321bf36-7b32-7afd-2eda-55d68001e66b", "collapsed": true, "_uuid": "8ccd142117d672e01dab9535fdb6a58828853682"}, "source": ["grouped_train = combined.head(891).groupby(['Sex', 'Pclass', 'Title'])\n", "grouped_median_train = grouped_train.median()\n", "\n", "grouped_test = combined.iloc[891:].groupby(['Sex', 'Pclass', 'Title'])\n", "grouped_median_test = grouped_test.median()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["grouped_median_train"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["grouped_median_test"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Function that fills median based on based on Pclass, Age and Sex"]}, {"cell_type": "code", "metadata": {"_kg_hide-output": true, "_kg_hide-input": false}, "source": ["def process_age():\n", "    global combined\n", "    \n", "    def fillAges(row, grouped_median):\n", "        if row['Sex'] == 'female' and row['Pclass'] == 1:\n", "            if row['Title'] == 'Miss':\n", "                return grouped_median.loc['female', 1, 'Miss']['Age']\n", "            elif row['Title'] == 'Mrs':\n", "                return grouped_median.loc['female', 1, 'Mrs']['Age']\n", "            elif row['Title'] == 'Officer':\n", "                return grouped_median.loc['female', 1, 'Officer']['Age']\n", "            elif row['Title'] == 'Royalty':\n", "                return grouped_median.loc['female', 1, 'Royalty']['Age']\n", "            \n", "        elif row['Sex'] == 'female' and row['Pclass'] == 2:\n", "            if row['Title'] == 'Miss':\n", "                return grouped_median.loc['female', 2, 'Miss']['Age']\n", "            elif row['Title'] == 'Mrs':\n", "                return grouped_median.loc['female', 2, 'Mrs']['Age']\n", "            \n", "        elif row['Sex'] == 'female' and row['Pclass'] == 3:\n", "            if row['Title'] == 'Miss':\n", "                return grouped_median.loc['female', 3, 'Miss']['Age']\n", "            elif row['Title'] == 'Mrs':\n", "                return grouped_median.loc['female', 3, 'Mrs']['Age']\n", "            \n", "        elif row['Sex']=='male' and row['Pclass'] == 1:\n", "            if row['Title'] == 'Master':\n", "                return grouped_median.loc['male', 1, 'Master']['Age']\n", "            elif row['Title'] == 'Mr':\n", "                return grouped_median.loc['male', 1, 'Mr']['Age']\n", "            elif row['Title'] == 'Officer':\n", "                return grouped_median.loc['male', 1, 'Officer']['Age']\n", "            elif row['Title'] == 'Royalty':\n", "                return grouped_median.loc['male', 1, 'Royalty']['Age']\n", "            \n", "        elif row['Sex']=='male' and row['Pclass'] == 2:\n", "            if row['Title'] == 'Master':\n", "                return grouped_median.loc['male', 2, 'Master']['Age']\n", "            elif row['Title'] == 'Mr':\n", "                return grouped_median.loc['male', 2, 'Mr']['Age']\n", "            elif row['Title'] == 'Officer':\n", "                return grouped_median.loc['male', 2, 'Officer']['Age']\n", "            \n", "        elif row['Sex']=='male' and row['Pclass'] == 3:\n", "            if row['Title'] == 'Master':\n", "                return grouped_median.loc['male', 3, 'Master']['Age']\n", "            elif row['Title'] == 'Mr':\n", "                return grouped_median.loc['male', 3, 'Mr']['Age']\n", "            elif row['Title'] == 'Officer':\n", "                return grouped_median.loc['male', 3, 'Officer']['Age']\n", "            \n", "    combined.head(891).Age  = combined.head(891).apply(lambda r : fillAges(r, grouped_median_train) if np.isnan(r['Age'])\n", "                                                      else r['Age'], axis = 1)\n", "    combined.iloc[891:].Age = combined.iloc[891:].apply(lambda r : fillAges(r, grouped_median_test) if np.isnan(r['Age'])\n", "                                                       else r['Age'], axis = 1)\n", "    status('age')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["process_age()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["combined.info()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def process_names():\n", "    \n", "    global combined\n", "    combined.drop('Name', axis = 1, inplace = True)\n", "    \n", "    #encoding in a dummy variable\n", "    titles_dummies = pd.get_dummies(combined['Title'], prefix = 'Title')\n", "    combined = pd.concat([combined, titles_dummies], axis = 1)\n", "    \n", "    #removing titles\n", "    combined.drop('Title', axis = 1, inplace = True)\n", "    \n", "    status('names')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["process_names()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["combined.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def process_fares():\n", "    combined.head(891).Fare.fillna(combined.head(891).Fare.mean(), inplace = True)\n", "    combined.iloc[891:].Fare.fillna(combined.iloc[891:].Fare.mean(), inplace = True)\n", "    \n", "    status('fare')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["process_fares()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def process_embarked():\n", "    \n", "    global combined\n", "    combined.head(891).Embarked.fillna('S', inplace = True)\n", "    combined.iloc[891:].Embarked.fillna('S', inplace = True)\n", "    \n", "    #dummy encoding\n", "    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix = 'Embarked')\n", "    combined = pd.concat([combined, embarked_dummies], axis = 1)\n", "    combined.drop('Embarked', axis = 1, inplace = True)\n", "    \n", "    status('embarked')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["process_embarked()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def process_cabin():\n", "    \n", "    global combined\n", "    \n", "    #replacing missing cabin with U (unknown)\n", "    combined.Cabin.fillna('U', inplace = True)\n", "    \n", "    #mapping each cabin with the cabin letter\n", "    combined['Cabin'] = combined['Cabin'].map(lambda c:c[0])\n", "    \n", "    #dummy encoding\n", "    cabin_dummies = pd.get_dummies(combined['Cabin'], prefix = 'Cabin')\n", "    combined = pd.concat([combined, cabin_dummies], axis = 1)\n", "    combined.drop('Cabin', axis = 1, inplace = True)\n", "    \n", "    status('cabin')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["process_cabin()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["combined.info()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def process_sex():\n", "    \n", "    global combined\n", "    combined['Sex'] = combined['Sex'].map({'male': 1, 'female': 0})\n", "    status('sex')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["process_sex()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def process_pclass():\n", "    \n", "    global combined\n", "    \n", "    pclass_dummies = pd.get_dummies(combined['Pclass'], prefix = 'Pclass')\n", "    combined = pd.concat([combined, pclass_dummies], axis = 1)\n", "    combined.drop('Pclass', axis = 1, inplace = True)\n", "\n", "    status('Pclass')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["process_pclass()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def process_ticket():\n", "    \n", "    global combined\n", "    def cleanTicket(ticket):\n", "        ticket = ticket.replace('.', '')\n", "        ticket = ticket.replace('/', '')\n", "        ticket = ticket.split()\n", "        ticket = map(lambda t: t.strip(), ticket)\n", "        ticket = [t for t in ticket if not str(t).isdigit()]\n", "        if len(ticket)>0:\n", "            return ticket[0]\n", "        else:\n", "            return 'XXX'\n", "        \n", "    # extracting dummy variables from ticket\n", "    combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n", "    tickets_dummies = pd.get_dummies(combined['Ticket'], prefix = 'Ticket')\n", "    combined = pd.concat([combined, tickets_dummies], axis = 1)\n", "    combined.drop('Ticket', axis = 1, inplace = True)\n", "    \n", "    status('ticket')\n", "        "], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["process_ticket()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def process_family():\n", "    \n", "    global combined\n", "    #introduce feature called as family size\n", "    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n", "    \n", "    #introduce features based on family size\n", "    combined['Singleton'] = combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n", "    combined['SmallFamily'] = combined['FamilySize'].map(lambda s: 1 if 2<=s<=4 else 0)\n", "    combined['LargeFamily'] = combined['FamilySize'].map(lambda s: 1 if 5<=s else 0)\n", "    \n", "    status('family')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["process_family()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["combined.shape"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["combined.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["combined.drop('PassengerId', axis = 1, inplace = True)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["combined.sample()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**Data Modelling**"]}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.pipeline import make_pipeline\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.feature_selection import SelectKBest\n", "from sklearn.cross_validation import StratifiedKFold\n", "from sklearn.grid_search import GridSearchCV\n", "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n", "from sklearn.cross_validation import cross_val_score"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def compute_score(clf, X, y, scoring = 'accuracy'):\n", "    xval = cross_val_score(clf, X, y, cv = 5, scoring = scoring)\n", "    return np.mean(xval)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["def recover_test_train():\n", "    global combined\n", "    train0 = pd.read_csv('../input/train.csv')\n", "    \n", "    target = train0.Survived\n", "    train = combined.head(891)\n", "    test = combined.iloc[891:]\n", "    \n", "    return train, test, target"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["train, test, target = recover_test_train()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**Feature Selection**"]}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.feature_selection import SelectFromModel\n", "clf = RandomForestClassifier(n_estimators = 50, max_features = 'sqrt')\n", "clf = clf.fit(train, target)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["features = pd.DataFrame()\n", "features['feature'] = train.columns\n", "features['importance'] = clf.feature_importances_\n", "features.sort_values(by = ['importance'], ascending = True, inplace = True)\n", "features.set_index('feature', inplace = True)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["features.plot(kind = 'barh', figsize = (20,20))"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["model = SelectFromModel(clf, prefit = True)\n", "train_reduced = model.transform(train)\n", "train_reduced.shape"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["test_reduced = model.transform(test)\n", "test_reduced.shape"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["#set to true if you want to run grid search again\n", "run_gs = False\n", "\n", "if run_gs:\n", "    parameter_grid = {\n", "                'max_depth' : [4,6,8],\n", "                'n_estimators' : [50,10],\n", "                'max_features' : ['sqrt', 'auto', 'log2'],\n", "                'min_samples_split' : [1,3,10],\n", "                'min_samples_leaf' : [1,3,10],\n", "                'bootstrap' : [True, False]\n", "    }\n", "    forest = RandomForestClassifier()\n", "    cross_validation = StratifiedKFold(targets, n_folds = 5)\n", "\n", "    grid_search = GridSearchCV(forest, scoring = 'accuracy', param_grid = parameter_grid, cv = cross_validation)\n", "\n", "    grid_search.fit(train, target)\n", "    model = grid_search\n", "    parameters = grid_search.best_params_\n", "\n", "    print('Best score: {}', format(grid_search.best_score_))\n", "    print('Best params: {}', format(grid_search.best_params_))\n", "\n", "else:\n", "    parameters = {'bootstrap' : False, 'min_samples_leaf' : 3, 'n_estimators' : 50, \n", "                  'min_samples_split' : 10, 'max_features' : 'sqrt', 'max_depth' : 6}\n", "    model = RandomForestClassifier(**parameters)\n", "    model.fit(train, target)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["compute_score(model, train, target, scoring = 'accuracy')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["output = model.predict(test).astype(int)\n", "df_output = pd.DataFrame()\n", "aux = pd.read_csv('../input/test.csv')\n", "df_output['PassengerId'] = aux['PassengerId']\n", "df_output['Survived'] = output\n", "df_output[['PassengerId','Survived']].to_csv('output.csv',index=False)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": [], "execution_count": null, "outputs": []}], "nbformat_minor": 1}