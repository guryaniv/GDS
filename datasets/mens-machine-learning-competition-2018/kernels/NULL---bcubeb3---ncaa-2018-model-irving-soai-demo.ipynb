{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5df2953ae4ae0157f34a193aa11718e2c48479d2"},"cell_type":"code","source":"import pandas as pd\nimport math\nfrom sklearn import cross_validation, linear_model\nimport csv\nimport random\nimport numpy\n\nbase_elo = 1600\nteam_elos = {}  # Reset each year.\nteam_stats = {}\nX = []\ny = []\nsubmission_data = []\nfolder = '../input'\nprediction_year = 2018","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def calc_elo(win_team, lose_team, season):\n    winner_rank = get_elo(season, win_team)\n    loser_rank = get_elo(season, lose_team)\n\n    \"\"\"\n    This is originally from from:\n    http://zurb.com/forrst/posts/An_Elo_Rating_function_in_Python_written_for_foo-hQl\n    \"\"\"\n    rank_diff = winner_rank - loser_rank\n    exp = (rank_diff * -1) / 400\n    odds = 1 / (1 + math.pow(10, exp))\n    if winner_rank < 2100:\n        k = 32\n    elif winner_rank >= 2100 and winner_rank < 2400:\n        k = 24\n    else:\n        k = 16\n    new_winner_rank = round(winner_rank + (k * (1 - odds)))\n    new_rank_diff = new_winner_rank - winner_rank\n    new_loser_rank = loser_rank - new_rank_diff\n\n    return new_winner_rank, new_loser_rank\n\n\ndef initialize_data():\n    for i in range(1985, prediction_year+1):\n        team_elos[i] = {}\n        team_stats[i] = {}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"960d5f34b4620e77b999dc6e3a71c06558427693"},"cell_type":"code","source":"def get_elo(season, team):\n    try:\n        return team_elos[season][team]\n    except:\n        try:\n            # Get the previous season's ending value.\n            team_elos[season][team] = team_elos[season-1][team]\n            return team_elos[season][team]\n        except:\n            # Get the starter elo.\n            team_elos[season][team] = base_elo\n            return team_elos[season][team]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a4119d160765fd78ba2473af9660beb32c4ff21"},"cell_type":"code","source":"def predict_winner(team_1, team_2, model, season, stat_fields):\n    features = []\n\n    # Team 1\n    features.append(get_elo(season, team_1))\n    for stat in stat_fields:\n        features.append(get_stat(season, team_1, stat))\n\n    # Team 2\n    features.append(get_elo(season, team_2))\n    for stat in stat_fields:\n        features.append(get_stat(season, team_2, stat))\n\n    return model.predict_proba([features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"314682870667ad351d2c01fd3e5d9667d4c79e7b"},"cell_type":"code","source":"def update_stats(season, team, fields):\n    \"\"\"\n    This accepts some stats for a team and udpates the averages.\n\n    First, we check if the team is in the dict yet. If it's not, we add it.\n    Then, we try to check if the key has more than 5 values in it.\n        If it does, we remove the first one\n        Either way, we append the new one.\n    If we can't check, then it doesn't exist, so we just add this.\n\n    Later, we'll get the average of these items.\n    \"\"\"\n    if team not in team_stats[season]:\n        team_stats[season][team] = {}\n\n    for key, value in fields.items():\n        # Make sure we have the field.\n        if key not in team_stats[season][team]:\n            team_stats[season][team][key] = []\n\n        if len(team_stats[season][team][key]) >= 9:\n            team_stats[season][team][key].pop()\n        team_stats[season][team][key].append(value)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f05b22a305f8f74855244b978c88659391896cd"},"cell_type":"code","source":"def get_stat(season, team, field):\n    try:\n        l = team_stats[season][team][field]\n        return sum(l) / float(len(l))\n    except:\n        return 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"745e20b335a6fa1d3a9f42001d7a399815948280"},"cell_type":"code","source":"def build_team_dict():\n    team_ids = pd.read_csv(folder + '/Teams.csv')\n    team_id_map = {}\n    for index, row in team_ids.iterrows():\n        team_id_map[row['TeamID']] = row['TeamName']\n    return team_id_map\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7635c7cd4545203829874e3967c85156c3989f8"},"cell_type":"code","source":"def build_season_data(all_data):\n    # Calculate the elo for every game for every team, each season.\n    # Store the elo per season so we can retrieve their end elo\n    # later in order to predict the tournaments without having to\n    # inject the prediction into this loop.\n    print(\"Building season data.\")\n    for index, row in all_data.iterrows():\n        # Used to skip matchups where we don't have usable stats yet.\n        skip = 0\n\n        # Get starter or previous elos.\n        team_1_elo = get_elo(row['Season'], row['WTeamID'])\n        team_2_elo = get_elo(row['Season'], row['LTeamID'])\n\n        # Add 100 to the home team (# taken from Nate Silver analysis.)\n        if row['WLoc'] == 'H':\n            team_1_elo += 100\n        elif row['WLoc'] == 'A':\n            team_2_elo += 100\n\n        # We'll create some arrays to use later.\n        team_1_features = [team_1_elo]\n        team_2_features = [team_2_elo]\n\n        # Build arrays out of the stats we're tracking..\n        for field in stat_fields:\n            team_1_stat = get_stat(row['Season'], row['WTeamID'], field)\n            team_2_stat = get_stat(row['Season'], row['LTeamID'], field)\n            if team_1_stat is not 0 and team_2_stat is not 0:\n                team_1_features.append(team_1_stat)\n                team_2_features.append(team_2_stat)\n            else:\n                skip = 1\n\n        if skip == 0:  # Make sure we have stats.\n            # Randomly select left and right and 0 or 1 so we can train\n            # for multiple classes.\n            if random.random() > 0.5:\n                X.append(team_1_features + team_2_features)\n                y.append(0)\n            else:\n                X.append(team_2_features + team_1_features)\n                y.append(1)\n\n        # AFTER we add the current stuff to the prediction, update for\n        # next time. Order here is key so we don't fit on data from the\n        # same game we're trying to predict.\n        if row['WFTA'] != 0 and row['LFTA'] != 0:\n            stat_1_fields = {\n                'score': row['WScore'],\n                'fgp': row['WFGM'] / row['WFGA'] * 100,\n                'fga': row['WFGA'],\n                'fga3': row['WFGA3'],\n                '3pp': row['WFGM3'] / row['WFGA3'] * 100,\n                'ftp': row['WFTM'] / row['WFTA'] * 100,\n                'or': row['WOR'],\n                'dr': row['WDR'],\n                'ast': row['WAst'],\n                'to': row['WTO'],\n                'stl': row['WStl'],\n                'blk': row['WBlk'],\n                'pf': row['WPF']\n            }\n            stat_2_fields = {\n                'score': row['LScore'],\n                'fgp': row['LFGM'] / row['LFGA'] * 100,\n                'fga': row['LFGA'],\n                'fga3': row['LFGA3'],\n                '3pp': row['LFGM3'] / row['LFGA3'] * 100,\n                'ftp': row['LFTM'] / row['LFTA'] * 100,\n                'or': row['LOR'],\n                'dr': row['LDR'],\n                'ast': row['LAst'],\n                'to': row['LTO'],\n                'stl': row['LStl'],\n                'blk': row['LBlk'],\n                'pf': row['LPF']\n            }\n            update_stats(row['Season'], row['WTeamID'], stat_1_fields)\n            update_stats(row['Season'], row['LTeamID'], stat_2_fields)\n\n        # Now that we've added them, calc the new elo.\n        new_winner_rank, new_loser_rank = calc_elo(\n            row['WTeamID'], row['LTeamID'], row['Season'])\n        team_elos[row['Season']][row['WTeamID']] = new_winner_rank\n        team_elos[row['Season']][row['LTeamID']] = new_loser_rank\n\n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efac8ce6a6292a319b2089f3bd735d016e5ce5c9"},"cell_type":"code","source":"stat_fields = ['score', 'fga', 'fgp', 'fga3', '3pp', 'ftp', 'or', 'dr',\n                   'ast', 'to', 'stl', 'blk', 'pf']\n\ninitialize_data()\nseason_data = pd.read_csv(folder + '/RegularSeasonDetailedResults.csv')\ntourney_data = pd.read_csv(folder + '/NCAATourneyDetailedResults.csv')\nframes = [season_data, tourney_data]\nall_data = pd.concat(frames)\n\n# Build the WORking data.\nX, y = build_season_data(all_data)\n\n# Fit the model.\nprint(\"Fitting on %d samples.\" % len(X))\n\nmodel = linear_model.LogisticRegression()\n\n# Check accuracy.\nprint(\"Doing cross-validation.\")\nprint(cross_validation.cross_val_score(\n    model, numpy.array(X), numpy.array(y), cv=10, scoring='accuracy', n_jobs=-1\n).mean())\n\nmodel.fit(X, y)\n\n# Now predict tournament matchups.\nprint(\"Getting teams.\")\nseeds = pd.read_csv(folder + '/NCAATourneySeeds.csv')\n# for i in range(2016, 2017):\ntourney_teams = []\nfor index, row in seeds.iterrows():\n    if row['Season'] == prediction_year:\n        tourney_teams.append(row['TeamID'])\n\n# Build our prediction of every matchup.\nprint(\"Predicting matchups.\")\ntourney_teams.sort()\nfor team_1 in tourney_teams:\n    for team_2 in tourney_teams:\n        if team_1 < team_2:\n            prediction = predict_winner(\n                team_1, team_2, model, prediction_year, stat_fields)\n            label = str(prediction_year) + '_' + str(team_1) + '_' + \\\n                str(team_2)\n            submission_data.append([label, prediction[0][0]])\n\nprint(['id','pred'])\nprint(submission_data)\n# Now so that we can use this to fill out a bracket, create a readable\n# version.\nprint(\"Outputting readable results.\")\nteam_id_map = build_team_dict()\nreadable = []\nless_readable = []  # A version that's easy to look up.\nfor pred in submission_data:\n    parts = pred[0].split('_')\n    less_readable.append(\n        [team_id_map[int(parts[1])], team_id_map[int(parts[2])], pred[1]])\n    # Order them properly.\n    if pred[1] > 0.5:\n        winning = int(parts[1])\n        losing = int(parts[2])\n        proba = pred[1]\n    else:\n        winning = int(parts[2])\n        losing = int(parts[1])\n        proba = 1 - pred[1]\n    readable.append(\n        [\n            '%s beats %s: %f' %\n            (team_id_map[winning], team_id_map[losing], proba)\n        ]\n    )\n\nprint(readable)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}