{"nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.4", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}}, "cells": [{"outputs": [], "source": ["import os\n", "from PIL import Image\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "import keras\n", "from keras.models import Sequential,Input,Model\n", "from keras.layers import Dense, Dropout, Flatten\n", "from keras.layers import Conv2D, MaxPooling2D\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.layers.advanced_activations import LeakyReLU\n", "import scipy.misc\n", "from skimage import transform\n", "import warnings\n", "\n", "warnings.filterwarnings(\"ignore\")\n", "\n", "#convertering list of training data paths to df\n", "train_dir = '../input/train/'\n", "train_list = os.listdir(train_dir)\n", "records = []\n", "for category in train_list:\n", "    img_list = os.listdir(train_dir + category)\n", "    for img in img_list:\n", "        records.append((img,category))\n", "        \n", "df_train = pd.DataFrame.from_records(records,columns=['image','category'])\n", "\n", "print(df_train.head())\n", "\n", "\n", "\n", "#looking at the test data\n", "test_dir = '../input/test/'\n", "test_list = os.listdir(test_dir)\n", "print('Train Data', len(df_train.index))\n", "print('Test Data',type(test_list),len(test_list))\n", "print('categories',os.listdir(train_dir))\n", "print('# of categories', len(os.listdir(train_dir)))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a43d555e4c3b411b203b7dba0d3885f76b5a0671", "_cell_guid": "ff811823-dec0-4611-8932-548ffa457a20"}}, {"source": ["Let's see what our images look like."], "metadata": {"_uuid": "3d28d002e29b2cc27ab6ca231c2ed596f26dda0c", "_cell_guid": "09a868d4-5df3-486b-8655-1d900b3c4a34"}, "cell_type": "markdown"}, {"outputs": [], "source": ["for i in list(df_train['image'])[0:1]:\n", "    img = Image.open(train_dir + df_train['category'][0] + '/' + i)\n", "    img.load()\n", "    data = np.asarray(img, dtype=\"float32\" )\n", "    plt.imshow(data)\n", "    plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "803cb1778fa1ab0f91a16c6f25b565792fa49513", "_cell_guid": "24cd4122-313b-4f6f-88ad-6c26f2f56b0b"}}, {"source": ["First, we will normalize our images and remove any images that possibly aren't square. Then we will create our X and y datasets."], "metadata": {"_uuid": "0fd6033d8c97723794d7e93c26af5bfe764eda33", "_cell_guid": "43bd567f-d5b1-496e-b701-b6bb2fa4a769"}, "cell_type": "markdown"}, {"outputs": [], "source": ["dim_image = []\n", "for i in (train_dir + df_train['category'] + '/' + df_train['image']):\n", "    img = Image.open(i)\n", "    data = img.size\n", "    dim_image.append(data[0])\n", "print('smallest image dimension', min(dim_image))\n", "\n", "i_height = min(dim_image)\n", "i_width = min(dim_image)\n", "\n", "X = []\n", "count = 0\n", "bad_images = []\n", "#df_train = df_train.drop(df_train.index[bad_images])\n", "for i in (train_dir + df_train['category'] + '/' + df_train['image']):\n", "    img = Image.open(i)\n", "    img.load()\n", "    img = np.asarray(img, dtype='float32')\n", "    img = img/255\n", "    data = transform.resize(img,(49,49))\n", "    if data.size != 7203:\n", "        bad_images.append(count)\n", "#     plt.imshow(data)\n", "#     plt.show()\n", "#     X.append(data)\n", "    count += 1\n", "print('bad images',bad_images)\n", "\n", "df_train = df_train.drop(df_train.index[bad_images])\n", "for i in (train_dir + df_train['category'] + '/' + df_train['image']):\n", "    img = Image.open(i)\n", "    img.load()\n", "    img = np.asarray(img, dtype='float32')\n", "    img = img/255\n", "    data = transform.resize(img,(49,49))\n", "    X.append(data)\n", "\n", "X = np.array(X)\n", "\n", "y = np.array(df_train['category'].astype('category').cat.codes)\n", "\n", "print('Done creating X and y.')\n", "print('X Shape',X.shape)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "51564291a5bbce32efa8ef4c4b14cb18885dc526", "_cell_guid": "1df6b4f6-7f54-4be2-b66d-a0854a161372"}}, {"source": ["**Convolutional Neural Network**"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "\n", "im_shape = (49,49,3)\n", "batch_size = 10\n", "\n", "cnn  = Sequential([\n", "    Conv2D(32, kernel_size=(3,3), activation='linear', input_shape=im_shape, padding='same'),\n", "    LeakyReLU(alpha=0.1),\n", "    MaxPooling2D((2,2), padding='same'),\n", "    Conv2D(64, kernel_size=(3,3), activation='linear', padding='same'),\n", "    LeakyReLU(alpha=0.1),\n", "    MaxPooling2D((2,2), padding='same'),\n", "    Conv2D(128, kernel_size=(3,3), activation='linear', padding='same'),\n", "    LeakyReLU(alpha=0.1),\n", "    MaxPooling2D((2,2), padding='same'),\n", "    Flatten(),\n", "    Dense(50,activation='relu'),\n", "    Dense(12, activation='softmax')\n", "])\n", "\n", "cnn.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n", "\n", "cnn. fit(X_train, y_train, batch_size=batch_size, epochs=10, verbose=1, validation_data=(X_test,y_test))"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"source": [], "metadata": {}, "cell_type": "markdown"}]}