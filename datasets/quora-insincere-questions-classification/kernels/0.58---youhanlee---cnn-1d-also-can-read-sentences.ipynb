{"cells":[{"metadata":{"trusted":true,"_uuid":"9a3dd2a2d87d7ce7f7de3ea6416580a99173fc44"},"cell_type":"markdown","source":"- I forked and refered below helpful kernels. Thanks for the authors! If you think this kernel is helpful, please upvote them!\n- https://www.kaggle.com/yekenot/2dcnn-textclassifier\n- https://www.kaggle.com/applecer/use-f1-to-select-model-lstm-based"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nimport os\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nimport warnings\n\nLEN_WORDS = 30\nLEN_EMBEDDING = 300","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21f85dfd5b9bf3d8b27ba29149d52253e5d64049"},"cell_type":"markdown","source":"# Setup"},{"metadata":{"trusted":true,"_uuid":"78578eab64a477d0a5ad6b1c917ae154868a44df"},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubmission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92ffbf2ef35d2dc5863ee27c61eadd1869ca5440"},"cell_type":"code","source":"X_train = train[\"question_text\"].fillna(\"fillna\").values\ny_train = train[\"target\"].values\nX_test = test[\"question_text\"].fillna(\"fillna\").values\n\nmax_features = 30000\nmaxlen = 40\nembed_size = 300\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cc8b0bcd285225ce651d024f506615d4656b9f7"},"cell_type":"code","source":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12bc9728aeb264c6aee0036d11e06415c06d1b91"},"cell_type":"code","source":"class F1Evaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            y_pred = (y_pred > 0.5).astype(int)\n            score = f1_score(self.y_val, y_pred)\n            print(\"\\n F1 Score - epoch: %d - score: %.6f \\n\" % (epoch+1, score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"706c0224b6112e5a8f00ad25f35e90fbb9519a5f"},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"cbff059e684fc30355be06135c9e524a4d092c8f"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Conv1D, Input, MaxPooling1D, Flatten, Dense, BatchNormalization, concatenate\nfrom keras.layers import LeakyReLU, Activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f1938967ca372a6f144bf18a99ef0363c1f5e74"},"cell_type":"code","source":"STRIDE_1 = 2\nSTRIDE_2 = 4\nSTRIDE_3 = 8\n\nFILTER_1 = 64\nFILTER_2 = 64\nFILTER_3 = 64\n\ninp = Input(shape=(maxlen, ))\nembed_layer = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nembed_layer = SpatialDropout1D(0.4)(embed_layer)\nembed_layer = Reshape((maxlen, embed_size))(embed_layer)\n\n# line1 = BatchNormalization()(Input_layer)\nline1 = Conv1D(FILTER_1, STRIDE_1)(embed_layer)\nline1 = Activation(LeakyReLU())(line1)\nline1 = MaxPooling1D(STRIDE_1)(line1)\nline1 = Conv1D(FILTER_1, STRIDE_1)(line1)\nline1 = Activation(LeakyReLU())(line1)\nline1 = MaxPooling1D(STRIDE_1)(line1)\nline1 = Conv1D(FILTER_1, STRIDE_1)(line1)\nline1 = Activation(LeakyReLU())(line1)\nline1 = MaxPooling1D(STRIDE_1*2)(line1)  # global max pooling\nline1 = Flatten()(line1)\n\n# line2 = BatchNormalization()(Input_layer)\nline2 = Conv1D(FILTER_2, STRIDE_1)(embed_layer)\nline2 = Activation(LeakyReLU())(line2)\nline2 = MaxPooling1D(STRIDE_1)(line2)\nline2 = Conv1D(FILTER_2, STRIDE_1)(line2)\nline2 = Activation(LeakyReLU())(line2)\nline2 = MaxPooling1D(STRIDE_1)(line2)\nline2 = Conv1D(FILTER_2, STRIDE_1)(line2)\nline2 = Activation(LeakyReLU())(line2)\nline2 = MaxPooling1D(STRIDE_1*2)(line2)  # global max pooling\nline2 = Flatten()(line2)\n\n# line3 = BatchNormalization()(Input_layer)\nline3 = Conv1D(FILTER_3, STRIDE_1)(embed_layer)\nline3 = Activation(LeakyReLU())(line3)\nline3 = MaxPooling1D(STRIDE_1)(line3)\nline3 = Conv1D(FILTER_3, STRIDE_1)(line3)\nline3 = Activation(LeakyReLU())(line3)\nline3 = MaxPooling1D(STRIDE_1)(line3)\nline3 = Conv1D(FILTER_3, STRIDE_1)(line3)\nline3 = Activation(LeakyReLU())(line3)\nline3 = MaxPooling1D(STRIDE_1*2)(line3)  # global max pooling\nline3 = Flatten()(line3)\n\nconcat_layer = concatenate([line1, line2, line3])\n\ntotal = Dense(1024, activation='relu')(concat_layer)\npreds = Dense(1, activation='sigmoid')(total)\n\nmodel = Model(inputs=inp, outputs=preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"182162bb9ebff676a765c6c13457537f48384387"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc849d4a1a05a877690d25754b4350840b89af8a"},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f0b703ded691293450beb4ddf2d903d0e08c737"},"cell_type":"code","source":"batch_size = 256\nepochs = 10\n\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95,\n                                              random_state=1989)\nF1_Score = F1Evaluation(validation_data=(X_val, y_val), interval=1)\n\nhist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n                 validation_data=(X_val, y_val),\n                 callbacks=[F1_Score], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6cdda0e301be0b84e826e29f0f3a85c41aa6da9"},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true,"_uuid":"b58cd95254f41e5002a17de0c3feab54a5fc3c67"},"cell_type":"code","source":"y_pred = model.predict(x_test, batch_size=1024)\ny_pred = (y_pred > 0.5).astype(int)\nsubmission['prediction'] = y_pred\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}