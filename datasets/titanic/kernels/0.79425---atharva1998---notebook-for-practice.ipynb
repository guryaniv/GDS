{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16ce12fbbb8d642bdee15a5e91e53567b09481e7"},"cell_type":"markdown","source":"Reading in the data as a Pandas DataFrame."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6a1deae25bf5c813334a7961515b3148f1dde3a"},"cell_type":"markdown","source":"A brief overview of the data."},{"metadata":{"trusted":true,"_uuid":"6c15b6ae8820fa126c6be23007c448b40f1641fe"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff39df6afde919dbb9bc5c89a2d2f9a6dd00063c"},"cell_type":"markdown","source":"A look into the statistics of continous data."},{"metadata":{"trusted":true,"_uuid":"79d440a2e2f446e15438b344407e2a41098811f1"},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"982fa221b2c68103db0707afc26f7ab5aa3d3464"},"cell_type":"markdown","source":"For categorical variables, "},{"metadata":{"trusted":true,"_uuid":"f7bc03d4945121c71114aebe6504909ce6403aca"},"cell_type":"code","source":"train_df.describe(include = 'object')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32b8cb39dad92547a3fb7e4ad961e35aaa9fb69d"},"cell_type":"markdown","source":"It can be seen that \"Age\" variable has missing values, similarly in categorical variables, \"Cabin\" and \"Embarked\" have missing values."},{"metadata":{"_uuid":"3df3193517d9a1f1c169eb034381b8360a2ac835"},"cell_type":"markdown","source":"The shape of the training data is"},{"metadata":{"trusted":true,"_uuid":"bd43cafb5a6907a521c81b65b13aed5a60b0bd66"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3ee15c5f973c78c2ab26fd46f348556be5043a9"},"cell_type":"code","source":"print(\"Value Counts of Tickets: {}\".format(train_df['Ticket'].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e5d9bd5a754f270d5fb059c92f9242e4bc8d979"},"cell_type":"code","source":"print(\"Value Counts of Cabin: {}\".format(train_df['Cabin'].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"670b55da8850e30830585cfa22b4d359bb25400a"},"cell_type":"code","source":"import matplotlib.pyplot as plt\ntrain_df['Ticket'].value_counts().plot(kind = 'bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a80efbc07a61b773ea3dd9db7244acd63529a01d"},"cell_type":"code","source":"train_df['Cabin'].value_counts().plot(kind = 'bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bd6d52c09fbd631b70765920a8c28d8802aba5c"},"cell_type":"code","source":"train_df['Fare'].plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f8318e10e1380ec0ce8768e9c9d17b8243bd575"},"cell_type":"markdown","source":"As, it can be seen, the distributions of the Cabin and Tickets variables are the same. But as their value counts are very high, turning them into one hot encoded categorical variable is not feasible."},{"metadata":{"_uuid":"24b5b114378342a3b85d38b3c2bf3afc54cebacc"},"cell_type":"markdown","source":"Let's get rid of some of the unneccesary features. For now, the \"name\" and \"ticket\" features are not that important. Also \"Cabin\" feature has a lot of missing values (~600), so let's get rid of that feature. Also, for analysis, \"PassengerID\" is not essential, so I'll get rid of that.  "},{"metadata":{"trusted":true,"_uuid":"87cba6df055e6a6534600027e876facea9403351"},"cell_type":"code","source":"train_df_1 = train_df.drop([\"Name\", \"Ticket\", \"PassengerId\", \"Cabin\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39e6de497bb94da0d84a712c1c686d7fea3f73ee"},"cell_type":"markdown","source":"Now, let's separate out features and output variables. "},{"metadata":{"trusted":true,"_uuid":"40aae03cf197d82ced2fa3403ac3b26e34efaff2"},"cell_type":"code","source":"X_train = train_df_1.drop([\"Survived\"], axis = 1)\ny_train = train_df_1[[\"Survived\"]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"101939220ce2bac33f15f1ff4eda96337231412c"},"cell_type":"markdown","source":"Now, let's fill in the missing values of the variables mentioned above. For the \"Age\" variable, the missing values would be filled in by mean of all the ages."},{"metadata":{"trusted":true,"_uuid":"4555a6426c3b239e1d3f4401d63be3f67f167f03"},"cell_type":"code","source":"X_train[\"Age\"] = X_train[\"Age\"].fillna(X_train[\"Age\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a732b41de0f24fb7a1a2c8f6b1a8c14e30dd468a"},"cell_type":"markdown","source":"Now, let's look at the statistical description."},{"metadata":{"trusted":true,"_uuid":"298fe66939b886744fbb78d84a49930a08fdb22e"},"cell_type":"code","source":"X_train[\"Age\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e70037d3520e5e737d35e23270c67a342487d52"},"cell_type":"markdown","source":"Thus, the 'NaN' values are filled in. Now, embarked has 2 missing values, let's fill them out by the mode. The df.mode() function return a df, so iloc is used."},{"metadata":{"trusted":true,"_uuid":"e3d8f13eac1a08fbfabc4cba001dc600f1766efb"},"cell_type":"code","source":"X_train[\"Embarked\"] = X_train[\"Embarked\"].fillna(X_train[\"Embarked\"].mode().iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df22e779746ea0dce7a3e11a1358cd93d27b703e"},"cell_type":"code","source":"X_train[\"Embarked\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6cb8341d7b71a9c54569e24327bdc31adadb02d"},"cell_type":"markdown","source":"Now, let's analyze X_train statistically,"},{"metadata":{"trusted":true,"_uuid":"751434a617cbcdbbf0e64b1654932592fcd1b86e"},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d8baa67ba32c3e0f5eb5945565ad82eff49ed5c"},"cell_type":"code","source":"X_train.describe(include = 'object')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"437feadc99b4f125b0f10f8a24b4cf6b9a17ee7e"},"cell_type":"markdown","source":"Now, the missing values are cleared."},{"metadata":{"_uuid":"7027be375a6419063a2b526da6f9c008fe640a17"},"cell_type":"markdown","source":"To select important features, correlation is one of the important metric. So let's use that."},{"metadata":{"trusted":true,"_uuid":"75e69db23aa728876cab257d3601be9e48620fc9"},"cell_type":"code","source":"corr_mat = train_df_1.corr()\ncorr_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28da3504bfffe792f07e2c058dcef7160c2a0b04"},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(corr_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13627b96e10412bd7eccdb6ce109c513fa0767a0","_kg_hide-output":true},"cell_type":"code","source":"dum_df1 = pd.get_dummies(X_train[\"Embarked\"])\nX_train = X_train.drop([\"Embarked\"], axis = 1)\nX_train = pd.concat([X_train, dum_df1], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ed0e865bc08d991073216b0816a8fb1f547928f"},"cell_type":"code","source":"dum_df2 = pd.get_dummies(X_train[\"Sex\"])\nX_train = X_train.drop([\"Sex\"], axis = 1)\nX_train = pd.concat([X_train, dum_df2], axis = 1)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d5a4c73fda5444e93b23c3ed24bf12998112346"},"cell_type":"code","source":"dum_df3 = pd.get_dummies(X_train[\"Pclass\"])\nX_train = X_train.drop([\"Pclass\"], axis = 1)\nX_train = pd.concat([X_train, dum_df3], axis = 1)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a19d0c96883f4998029df8c3fc3c9fa349c399e"},"cell_type":"markdown","source":"Now the data has no missing values and is completely numeric. Great. Let's standardize the data now. I'll standrdize the continous data for now."},{"metadata":{"trusted":true,"_uuid":"cc72284f1dc4ad19e0da476efe0f4f0cfefab2c8"},"cell_type":"code","source":"X_train['Age'] = (X_train['Age'] - X_train['Age'].mean()) / X_train['Age'].std()\nX_train['Fare'] = (X_train['Fare'] - X_train['Fare'].mean()) / X_train['Fare'].std()\nX_train.head() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aefabfb44f1d951fa1c3a079377f3e0f7f15320b"},"cell_type":"markdown","source":"1. Let's start modelling. First let's try logisitic regression with cross validation."},{"metadata":{"trusted":true,"_uuid":"4e7bcf0948937698bcf27a0fac4b46d83b442e33"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(0.9)\nX_train_pca = pca.fit_transform(X_train)\nno_of_components = pca.n_components_\nvar_ratio = pca.explained_variance_ratio_\nplt.plot(var_ratio)\nplt.xlabel('Features')\nplt.ylabel('Proportion of variance explained by each feature.')\nplt.title('The number of features are: {}'.format(no_of_components))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"6f4827192ce9e61b6cff6a057345774c0dca2320"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nclf_l2 = LogisticRegression(penalty = 'l2', solver = 'lbfgs', random_state = 0)\nclf_l2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44124ee021db63720a98ca271731ff96397e1650"},"cell_type":"markdown","source":"Thus, the model is trained, now let's evaluate the performance using 3-fold cross validation.\n"},{"metadata":{"trusted":true,"_uuid":"17b7b6c8b49a00b2c825f856d9072a50d8b37545"},"cell_type":"code","source":"scores_l2 = cross_val_score(clf_l2, X_train, y_train, cv = 3)\nscores_l2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51111781c5521bec17e64adf2c95c36b7fa10cb6"},"cell_type":"markdown","source":"The scores all lie between (78% - 80%). Let's try l1 regularisation."},{"metadata":{"trusted":true,"_uuid":"d040d79d42b196e391e3d36a928593648ef328cb"},"cell_type":"code","source":"clf_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear', random_state = 0)\nclf_l1.fit(X_train,y_train)\nscores_l1 = cross_val_score(clf_l1, X_train, y_train, cv = 3)\nscores_l1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0faf17ea6c52457bd9698d486d76ab8501e00a38"},"cell_type":"code","source":"print(\"L1 Regularisation Cross Validation Score is: \" + str(scores_l1.mean()))\nprint(\"L2 Regularisation Cross Validation Score is: \" + str(scores_l2.mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27170e487699796bee165808c0e3b9ae38d18e70"},"cell_type":"markdown","source":"Thus, these are the results obtained by Logistic Regression. Let's try several different classification algorithms to see if we get better results through cross validation."},{"metadata":{"_uuid":"d90d6c649bea1c31a3ed928542a022461f2ce276"},"cell_type":"markdown","source":"Let's try K-nearest classification. I'll use GridSearchCV for deciding which is the best 'k' for best score."},{"metadata":{"trusted":true,"_uuid":"b468e81d672abfd3a616336acf744a65e9301a8d"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nparameters = {'n_neighbors':np.arange(1, 11)}\nknn = KNeighborsClassifier()\nclf_knn = GridSearchCV(knn, parameters, cv = 3)\nclf_knn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6457c9338013272dad39d5c28cb03590d398f9e0","_kg_hide-output":true},"cell_type":"code","source":"clf_knn.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"559025c20e208c3ffa79858074b33c4d87445139"},"cell_type":"markdown","source":"Thus from the above results, we get a 'best estimator'. Let's store this in 'knn_best_estimator'."},{"metadata":{"trusted":true,"_uuid":"ef0d96d5d187e367393825076955c6cdc82d762d"},"cell_type":"code","source":"knn_best_estimator = clf_knn.best_estimator_ \nknn_best_score = clf_knn.best_score_\nprint(\"The best estimator is: \" + str(knn_best_estimator))\nprint(\"The best score is: \" + str(knn_best_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c463dbcecc7904b17c5e6bcac7188ff4860d514"},"cell_type":"markdown","source":"That was k-nearest neighbours., it's a fairly simple algorithm but is known to give excellent results in some cases, let's compare the results with other algorithms. Now let's move to Support Vector Machines."},{"metadata":{"trusted":true,"_uuid":"c8d677bda3e85a0a87b3fd56e83f7129fd6fcb14","scrolled":true},"cell_type":"code","source":"from sklearn.svm import SVC\nparams_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\nparameters = [{'C': params_range, 'kernel':['linear']}, {'C':params_range, 'kernel':['rbf'], 'gamma':params_range}]\nsvm = SVC(random_state = 0, probability = True)\nclf_svm = GridSearchCV(estimator = svm, param_grid = parameters, cv = 3, n_jobs = -1, scoring = 'accuracy')\nclf_svm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28a05910ac67a29a103855ee53c14485f2c94291"},"cell_type":"code","source":"best_estimator_svm = clf_svm.best_estimator_\nbest_score_svm = clf_svm.best_score_\nprint(best_estimator_svm)\nprint(best_score_svm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edd16791b2d6d0ed41ec619fd459d79c7577fc5d"},"cell_type":"markdown","source":"Thus, the SVM classifier is giving a better accuracy than K-neighbours classifier. Let's move now to Decision trees."},{"metadata":{"trusted":true,"_uuid":"95832c3a5173d55a9f4a32901c14fac972d44ec2"},"cell_type":"code","source":"from sklearn import tree\ntree = tree.DecisionTreeClassifier()\nparams_range = np.arange(1, 21)\nparameters_tree = [{'criterion':['gini'], 'max_depth':params_range}, {'criterion':['entropy'], 'max_depth':params_range}]\nclf_tree = GridSearchCV(estimator = tree, param_grid = parameters_tree, cv = 3, scoring = 'accuracy')\nclf_tree.fit(X_train, y_train)\ntree_best_estimator = clf_tree.best_estimator_\ntree_best_score = clf_tree.best_score_\nprint(tree_best_estimator)\nprint(tree_best_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0ba44912294b568144d574fb74727220003b64b"},"cell_type":"markdown","source":"Thus, the cross validation result obtained through Decision tree is better than K-Nearest Neighbours better a it did a little worse than support vector machines. Next, let's try Random Forests."},{"metadata":{"trusted":true,"_uuid":"f3774feae3e7f769211d419808c49a24eb60b0c9","_kg_hide-output":true},"cell_type":"code","source":"\"\"\"from sklearn.ensemble import RandomForestClassifier\nparams_range_n_estimators = np.arange(1, 21)\nparams_range_max_depth = np.arange(1, 21)\nforest = RandomForestClassifier()\nparameters_forest = [{'criterion':['gini'], 'n_estimators': params_range_n_estimators, 'max_depth':params_range_max_depth, 'oob_score':['True', 'False']}, {'criterion':['entropy'], 'n_estimators': params_range_n_estimators, 'max_depth':params_range_max_depth, 'oob_score':['True', 'False']}]\nclf_forest = GridSearchCV(estimator = forest, param_grid = parameters_forest, cv = 3, scoring = 'accuracy')\nclf_forest.fit(X_train, y_train)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecbf990ee26c767c156fa552382bf5e27376af7a"},"cell_type":"code","source":"\"\"\"clf_forest_best_estimator = clf_forest.best_estimator_\nclf_forest_best_score = clf_forest.best_score_\nprint(clf_forest_best_estimator)\nprint(clf_forest_best_score)\"\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64205aeecb01d086220136c226d4995cead25103"},"cell_type":"markdown","source":"Let's try a voting classifier to decide which of the above classifiers give best accuracy. The voting classifier is based on majority voting."},{"metadata":{"trusted":true,"_uuid":"f99a41154924529b778378f3b6e30373f3ba4356","_kg_hide-output":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nvoting_clf = VotingClassifier(estimators = [('lr', clf_l2), ('svc', best_estimator_svm), ('knn', knn_best_estimator), ('rf', clf_forest_best_estimator)], voting = 'hard')\ncross_validation_score = []\nfor clf in (clf_l2, clf_l1, best_estimator_svm, clf_forest_best_estimator, knn_best_estimator, voting_clf):\n    clf.fit(X_train, y_train)\n    cross_validation_score.append(clf)\n    cross_validation_score.append(max(cross_val_score(clf, X_train, y_train, cv = 3)))\nprint(cross_validation_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef9373186f06a4775caa551fa055e25c25d8a600"},"cell_type":"markdown","source":"Let's submit the results now. But first, the testing data needs to be in the form of training data."},{"metadata":{"trusted":true,"_uuid":"567e9ec12db949bcfeb1f7528f9a6b41a68322f2"},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07dcf5ef17599d05c8ee1270a4479b6eb696e505"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97bb92fb3af29bbd74a31705708ee34b98ec0ffc"},"cell_type":"code","source":"test_df.describe(include = 'object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19993ebb13880a0187c4dee31d10e3814f2bdc9b"},"cell_type":"code","source":"test_df_1 = test_df.drop([\"Name\", \"Ticket\", \"PassengerId\", \"Cabin\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c68d2bf6484c544fc4ba5d235c815408d51b267"},"cell_type":"code","source":"test_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af73028a7cf55d9d764a6c53e3681d46bd898355"},"cell_type":"code","source":"test_df_1[\"Age\"] = test_df_1[\"Age\"].fillna(test_df_1[\"Age\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ded8ac0766ff153f1e08889a6ae266e8fda738a"},"cell_type":"code","source":"test_df_1[\"Fare\"] = test_df_1[\"Fare\"].fillna(test_df_1[\"Fare\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee7b05fc91a70e53b6dc5cc0d3c441c093ecf061"},"cell_type":"code","source":"test_df_1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea524bc8d1d0a586956d0fb52be7ed03f6f2ef05"},"cell_type":"code","source":"dum_df1_test = pd.get_dummies(test_df_1[\"Embarked\"])\ntest_df_1 = test_df_1.drop([\"Embarked\"], axis = 1)\ntest_df_1 = pd.concat([test_df_1, dum_df1_test], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d18386a1b30c1041b5554574c9770344de674ab"},"cell_type":"code","source":"dum_df1_test = pd.get_dummies(test_df_1[\"Sex\"])\ntest_df_1 = test_df_1.drop([\"Sex\"], axis = 1)\ntest_df_1 = pd.concat([test_df_1, dum_df1_test], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8aa84f9bb5173d886a283d684ab523cc88fcc2cd"},"cell_type":"code","source":"dum_df1_test = pd.get_dummies(test_df_1[\"Pclass\"])\ntest_df_1 = test_df_1.drop([\"Pclass\"], axis = 1)\ntest_df_1 = pd.concat([test_df_1, dum_df1_test], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"384edc12265a933fe7748c43672e3e60519b996a"},"cell_type":"code","source":"test_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12a92d186e0182ec77826dd053bf5fa468ad811f"},"cell_type":"code","source":"test_df_1['Age'] = (test_df_1['Age'] - test_df_1['Age'].mean()) / test_df_1['Age'].std()\ntest_df_1['Fare'] = (test_df_1['Fare'] - test_df_1['Fare'].mean()) / test_df_1['Fare'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfd54de2bbc19a2e1b0e27e8ef0653f94f91248d"},"cell_type":"code","source":"test_df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b51c086458fc385f867bc5fffaca852a56d6e8eb"},"cell_type":"code","source":"X_test_pca = pca.transform(test_df_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e0d23e8cea0fe7c601f4b9e88de3e5a84380a6f"},"cell_type":"code","source":"predictions = best_estimator_svm.predict(test_df_1)\nsubmission = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':predictions})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9c9e45988ccf4ab54e89fd1b2f9207a3567c503"},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81279ac781d0a6602a6a0fbc2ae30104d9483140"},"cell_type":"code","source":"filename = 'Titanic-Notebook-for-pratice-preds.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a560ac6b0264574a1d9afc2455b6ac930d267dbe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}