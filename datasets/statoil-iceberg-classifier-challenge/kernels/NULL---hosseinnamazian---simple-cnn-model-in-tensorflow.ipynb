{"metadata": {"language_info": {"nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"metadata": {}, "source": ["# Statoil - iceberg\n", "\n", "### This notebook is written by H.Namazian"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["import tensorflow as tf\n", "import pandas as pd\n", "import numpy as np\n", "from sklearn.model_selection import StratifiedShuffleSplit\n", "from sklearn.preprocessing import OneHotEncoder\n", "import matplotlib.pyplot as plt\n", "from tqdm import tqdm\n", "%matplotlib inline"], "cell_type": "code", "execution_count": 1}, {"metadata": {"collapsed": true, "scrolled": true}, "outputs": [], "source": ["data_path = './data'\n", "train = pd.read_json(data_path + '/' + 'train.json')\n", "test = pd.read_json(data_path + '/' + 'test.json')\n", "submission = pd.read_csv(data_path + '/' + 'sample_submission.csv').set_index('id')"], "cell_type": "code", "execution_count": 2}, {"metadata": {}, "source": ["## Data preparation"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["train_band_1 = np.array([np.array(band).astype(np.float32).reshape((75,75)) for band in train.band_1])\n", "train_band_2 = np.array([np.array(band).astype(np.float32).reshape((75,75)) for band in train.band_2])\n", "X_train = np.concatenate([train_band_1[:, :, :, np.newaxis], \n", "                          train_band_2[:, :, :, np.newaxis],\n", "                          ((train_band_1+train_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n", "\n", "test_band_1 = np.array([np.array(band).astype(np.float32).reshape((75,75)) for band in test.band_1])\n", "test_band_2 = np.array([np.array(band).astype(np.float32).reshape((75,75)) for band in test.band_2])\n", "X_test = np.concatenate([test_band_1[:, :, :, np.newaxis], \n", "                         test_band_2[:, :, :, np.newaxis],\n", "                         ((test_band_1+test_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n", "\n", "y = np.array([target for target in train.is_iceberg]).reshape((-1,1))\n", "\n", "print ('train_band_1 shape: {}'.format(train_band_1.shape))\n", "print ('train_band_2 shape: {}'.format(train_band_2.shape))\n", "print ('train_train shape:  {}'.format(X_train.shape))\n", "print ('train label shape:  {}'.format(y.shape))\n", "print ('test_band_1 shape:  {}'.format(test_band_1.shape))\n", "print ('test_band_2 shape:  {}'.format(test_band_2.shape))\n", "print ('test_train shape:   {}'.format(X_test.shape))"], "cell_type": "code", "execution_count": 3}, {"metadata": {}, "source": ["## Label Encoding"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["lbl = OneHotEncoder()\n", "lbl.fit([[0],[1]])\n", "y = lbl.transform(y).toarray()"], "cell_type": "code", "execution_count": 4}, {"metadata": {}, "source": ["## Normalization"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["X_min = np.min(X_train)\n", "X_max = np.max(X_train)\n", "X_train = (X_train - X_min)/(X_max - X_min)\n", "X_test = (X_test - X_min)/(X_max - X_min)"], "cell_type": "code", "execution_count": 5}, {"metadata": {}, "source": ["## Data Visualization"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(2,4, figsize=[12,8])\n", "\n", "ax[0,0].imshow(X_train[0,:,:,0])\n", "ax[0,1].imshow(X_train[0,:,:,2])\n", "\n", "ax[0,2].imshow(X_train[2,:,:,0])\n", "ax[0,3].imshow(X_train[2,:,:,2])\n", "\n", "ax[1,0].imshow(X_train[1,:,:,0])\n", "ax[1,1].imshow(X_train[1,:,:,2])\n", "\n", "ax[1,2].imshow(X_train[6,:,:,0])\n", "ax[1,3].imshow(X_train[6,:,:,2])"], "cell_type": "code", "execution_count": 6}, {"metadata": {}, "source": ["## functions"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["def get_batches(x, y, batch_size=10):\n", "    n_batches = len(x)//batch_size\n", "    for ii in range(0, n_batches*batch_size, batch_size):\n", "        if ii != (n_batches-1)*batch_size:\n", "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n", "        else:\n", "            X, Y = x[ii:], y[ii:]\n", "        yield X, Y"], "cell_type": "code", "execution_count": 9}, {"metadata": {}, "source": ["## CNN Model"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["train_data, train_label = X_train[:1400,:,:,:], y[:1400,:]\n", "val_data, val_label = X_train[1400:,:,:,:], y[1400:,:]"], "cell_type": "code", "execution_count": 7}, {"metadata": {"collapsed": true, "scrolled": true}, "outputs": [], "source": ["inputs = tf.placeholder(tf.float32, [None, 75, 75, 3])\n", "labels = tf.placeholder(tf.int32)\n", "\n", "conv1 = tf.layers.conv2d(inputs=inputs, filters=8, kernel_size=(7,7), strides=(1,1), \n", "                         padding='SAME', activation=tf.nn.relu, use_bias=True)\n", "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2,2), strides=(2,2), padding='SAME')\n", "\n", "conv2 = tf.layers.conv2d(inputs=pool1, filters=16, kernel_size=(5,5), strides=(1,1), \n", "                         padding='SAME', activation=tf.nn.relu, use_bias=True)\n", "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2,2), strides=(2,2), padding='SAME')\n", "\n", "conv3 = tf.layers.conv2d(inputs=pool2, filters=16, kernel_size=(3,3), strides=(1,1), \n", "                         padding='SAME', activation=tf.nn.relu, use_bias=True)\n", "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=(2,2), strides=(2,2), padding='SAME')\n", "\n", "flat = tf.reshape(pool3, [-1,1600])\n", "\n", "fc1 = tf.layers.dense(flat, units=256, use_bias=True, activation=tf.nn.relu)\n", "dp1 = tf.layers.dropout(fc1, rate=0.25)\n", "\n", "fc2 = tf.layers.dense(dp1, units=64, use_bias=True, activation=tf.nn.relu)\n", "dp2 = tf.layers.dropout(fc2, rate=0.25)\n", "\n", "logits = tf.layers.dense(dp2, units=2, use_bias=True)\n", "\n", "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.cast(labels, tf.float32))\n", "cost = tf.reduce_mean(loss)\n", "\n", "predicted = tf.nn.softmax(logits)\n", "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels, 1))\n", "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n", "\n", "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n", "\n", "n_epoches = 100\n", "batch_size = 32\n", "\n", "saver = tf.train.Saver()\n", "\n", "with tf.Session() as sess:\n", "    sess.run(tf.global_variables_initializer())\n", "    for epoch in range(n_epoches):\n", "        for X_batch, y_batch in get_batches(train_data, train_label, batch_size):\n", "            feed_dict = {inputs:X_batch, labels:y_batch}\n", "            train_cost,_ = sess.run([cost, optimizer], feed_dict=feed_dict)\n", "            \n", "        feed_dict = {inputs:X_train, labels:y}\n", "        train_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n", "        feed_dict = {inputs:val_data, labels:val_label}\n", "        val_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n", "        print('epoch {}, train accuracy: {:5f}, validation accuracy: {:.5f}'.format(epoch+1, train_accuracy, val_accuracy))\n", "    saver.save(sess, \"checkpoints/cnn_100.ckpt\")"], "cell_type": "code", "execution_count": 54}, {"metadata": {"collapsed": true}, "outputs": [], "source": ["test_batch_size = 128\n", "test_pred_res = []\n", "with tf.Session() as sess:\n", "    saver.restore(sess, \"checkpoints/cnn_100.ckpt\")\n", "    for i in tqdm(range(0, X_test.shape[0], test_batch_size)):\n", "        test_batch = X_test[i:i+test_batch_size,:,:,:]\n", "        feed_dict = {inputs: test_batch}\n", "        test_pred = sess.run(predicted, feed_dict=feed_dict)\n", "        test_pred_res.append(test_pred.tolist())\n", "    test_pred_res = np.concatenate(test_pred_res)"], "cell_type": "code", "execution_count": 56}, {"metadata": {"collapsed": true, "scrolled": true}, "outputs": [], "source": ["cnn_submit = submission.copy()\n", "cnn_submit.is_iceberg = test_pred_res[:,1]\n", "cnn_submit.to_csv('./cnn_100_submit.csv')"], "cell_type": "code", "execution_count": 57}, {"metadata": {"collapsed": true}, "outputs": [], "source": [], "cell_type": "code", "execution_count": null}, {"metadata": {"collapsed": true}, "outputs": [], "source": [], "cell_type": "code", "execution_count": null}], "nbformat": 4}