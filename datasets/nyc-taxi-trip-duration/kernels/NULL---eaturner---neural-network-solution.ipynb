{"nbformat_minor": 0, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"file_extension": ".py", "version": "3.6.1", "name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "nbconvert_exporter": "python"}}, "cells": [{"cell_type": "markdown", "source": "In the previous kernel, I presented a solution using gradient boosting.  Now I present you with a solution using a neural network", "metadata": {"_cell_guid": "e03fd7ac-2e83-4b5e-9734-0f6a1dd7e7bb", "_uuid": "8aac61c763359e0212381029e718e9dab7ff9a2e", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n#import modules for kernel\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.regularizers import l2", "metadata": {"_cell_guid": "fa566fcb-0a76-470a-bdb9-c1f6f97ce3a6", "_uuid": "12fd359f55677a2e058575fe58d21f8318d69274", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "train = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\ntest = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')", "metadata": {"_cell_guid": "5f3dac82-7e3b-466f-a69d-52ea93976a9b", "_uuid": "99a4289fd74a5011d75fb8b3bfd71ae836280e93", "trusted": false, "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Feature Creation ##\n\nThese are our features.", "metadata": {"_cell_guid": "d90ae594-7566-4483-bdf1-5c37669c2e88", "_uuid": "7c6ee9b591870758d8639869654d55e7ab1cecd4", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n                    test[['pickup_latitude', 'pickup_longitude']].values,\n                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n\npca = PCA().fit(coords)\n\nsample_ind = np.random.permutation(len(coords))[:500000]\nkmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])\n\ndef toDateTime( df ):\n    \n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n    \n    df['pickup_weekday'] = df['pickup_datetime'].dt.weekday_name\n    df['pickup_day'] = df['pickup_datetime'].dt.day\n    df['pickup_month'] = df['pickup_datetime'].dt.month.astype('object')\n    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n    df['pickup_minute'] = df['pickup_datetime'].dt.minute\n    df['pickup_dt'] = (df['pickup_datetime'] - df['pickup_datetime'].min()).map(\n        lambda x: x.total_seconds())\n    \n    df.drop('pickup_datetime', axis = 1, inplace = True)\n\n    return df\n#get radical distince\ndef haversine_np(lon1, lat1, lon2, lat2):\n   \n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n    return km\n\n#manhattan distance\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n\n    a = haversine_np(lat1, lng1, lat1, lng2)\n    b = haversine_np(lat1, lng1, lat2, lng1)\n    return a + b\n\n#bearing direction\ndef bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\n#all distances\ndef locationFeatures( df ):\n    #displacement of degrees\n    df['up_town'] = np.sign( df['pickup_longitude'] - df['dropoff_longitude'] )\n    df['est_side'] = np.sign( df['pickup_latitude'] - df['dropoff_latitude'] )\n     \n    #radical distances\n    df['haversine_distance'] = haversine_np(\n        df['pickup_longitude'], df['pickup_latitude'], \n        df['dropoff_longitude'], df['dropoff_latitude']\n    )\n    \n    #log transform of the haversine distance\n    df['log_haversine_distance'] = np.log1p(df['haversine_distance']) \n    \n    #manhattan distances\n    df['distance_dummy_manhattan'] = dummy_manhattan_distance(\n        df['pickup_latitude'], df['pickup_longitude'],\n        df['dropoff_latitude'], df['dropoff_longitude']\n    )\n    \n    #log transform of the haversine distance\n    df['log_distance_dummy_manhattan'] = np.log1p(df['distance_dummy_manhattan']) \n    \n    #pca distances\n    df['pickup_pca0'] = pca.transform(df[['pickup_latitude', 'pickup_longitude']])[:, 0]\n    df['pickup_pca1'] = pca.transform(df[['pickup_latitude', 'pickup_longitude']])[:, 1]\n    df['dropoff_pca0'] = pca.transform(df[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n    df['dropoff_pca1'] = pca.transform(df[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n       \n    df.loc[:, 'pca_manhattan'] = ( np.abs(df['dropoff_pca1'] - df['pickup_pca1']) +\n    np.abs(df['dropoff_pca0'] - df['pickup_pca0']) )\n    \n    df.loc[:, 'pickup_cluster'] = kmeans.predict(df[['pickup_latitude', 'pickup_longitude']]).astype('object')\n    df.loc[:, 'dropoff_cluster'] = kmeans.predict(df[['dropoff_latitude', 'dropoff_longitude']]).astype('object')\n    \n    df.drop(['pickup_longitude', 'dropoff_longitude'], axis = 1, inplace = True)\n    df.drop(['pickup_latitude', 'dropoff_latitude'], axis = 1, inplace = True)\n    \n    return df\n\ndef featureCreate( df ):\n    print ('Date time features')\n    df = toDateTime( df )\n    print ('Location Features')\n    df = locationFeatures( df )\n    \n    return df", "metadata": {"_cell_guid": "38416e01-1620-4451-8f6c-4f9ae533738d", "_uuid": "1ad3d2547f9b78d80139fecfb4b59886f33eadbb", "trusted": false, "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Now we add all of our features together to the training dataset.", "metadata": {"_cell_guid": "dac6fdca-3a17-4fb6-97cb-5eac4e0922fa", "_uuid": "6db3e672daaf68a0edc774e036be30816b75e2b3", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "train = featureCreate( train )\ntest = featureCreate( test )\n\n#log transform our trip duration\ntrain['trip_duration'] = np.log1p(train['trip_duration'])", "metadata": {"_cell_guid": "60a7efd8-a010-4c0b-a80e-082a2ad66042", "_uuid": "f40157b9fcb4132fd30ffa5c4476f7a8ad48b5c5", "trusted": false, "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "We remove our outliers, as we have previously done.", "metadata": {"_cell_guid": "c6d0ec3c-b0e0-4f44-a679-2109c0cf4c64", "_uuid": "38f5925b9348691404f0c03538c6f8ef3021caed", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "q1 = np.percentile(train['trip_duration'], 25)\nq3 = np.percentile(train['trip_duration'], 75)\n\niqr = q3 - q1\n\ntrain = train[ train['trip_duration'] <= q3 + 3.0*iqr]\n\ntrain = train[ q1 - 3.0*iqr <= train['trip_duration']]", "metadata": {"_cell_guid": "9c065309-3edc-42a8-b5f1-00b340e0c99f", "_uuid": "1a10ada30501ed8b86f3db735ad68c4b7c6d8f75", "trusted": false, "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "labels = train.pop('trip_duration')\ntrain.drop([\"id\", \"dropoff_datetime\"], axis=1, inplace = True)\n\nsub = pd.DataFrame( columns = ['id', 'trip_duration'])\nsub['id'] = test.pop('id')", "metadata": {"_cell_guid": "31e4ec55-9cd2-44fb-aec2-4ac1bc598a2a", "_uuid": "97eed4353db3cfaabf631f5ca97e507822267939", "trusted": false, "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Change all categorial variables to one-hot encoded", "metadata": {"_cell_guid": "283e0147-4896-4023-87d8-302c3a37db63", "_uuid": "2cb8d80257ff3b67c73b1d808ac07ebe5ba25bef", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "train = pd.get_dummies(train)\ntest = pd.get_dummies(test)", "metadata": {"_cell_guid": "ea5d12ac-d5e0-45f2-8ae6-72e8761f7f1b", "_uuid": "9b9c2eaf5995e8133e098369fece8e26572f0c92", "trusted": false, "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "We need to scale our solutions", "metadata": {"_cell_guid": "fe288eee-cf34-44ad-a59b-8c3f7a978969", "_uuid": "dce9b76ee1532e77981b904f02b6ad2780b50ca9", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "mmScale = MinMaxScaler()\n\n#input shape\nn = train.shape[1]\n\ntrain = mmScale.fit_transform(train)", "metadata": {"_cell_guid": "aa44515f-196e-43b9-87bf-0a2b8b722bd8", "_uuid": "4be87d0577a1b31f8a36d612c506dcbb47d7d99c", "trusted": false, "_execution_state": "busy", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Model Construction ##", "metadata": {"_cell_guid": "7ba0578e-7369-4674-a104-3b9adfeace9c", "_uuid": "1f9e81f78fd8ff8dff3947e0229181d68629b61e", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "#deep learning\nmodel = Sequential()\n#Want to use an expotential linear unit instead of the usual relu\nmodel.add( Dense( n, activation='relu', input_shape=(n,) ) )\nmodel.add( Dense( int(0.5*n), activation='relu' ) )\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])", "metadata": {"_cell_guid": "1a93b3e8-0e0d-44ae-9f4e-c2983e6f6ebd", "_uuid": "1418d999a3f0e1e91b82d6e1fc4bb6efd43d1bfd", "trusted": false, "_execution_state": "busy", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "print ('On to the next one')\n\nmodel.fit(train, labels.values, epochs = 3)\n\nprint ('Finished')", "metadata": {"_cell_guid": "2d755f9a-fe47-4445-9365-58898efe7bed", "_uuid": "897ccf004e920f6a051fb2794b0cc8e901447640", "trusted": false, "_execution_state": "busy", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Error Analysis ##", "metadata": {"_cell_guid": "fd32f03f-9c80-4601-997f-c212d2eb9392", "_uuid": "01096fb727d5b79780c3d869524d86fe63f00f18", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "markdown", "source": "Now lets analyze the results", "metadata": {"_cell_guid": "eaacb1dd-e4fd-4e13-8462-bdf10824623b", "_uuid": "4d32ed79e8dac66b515578d3af7622a79c3bea1b", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "train_pred = model.predict(train)\ntest_pred = model.predict( mmScale.transform(test) )", "metadata": {"_cell_guid": "a0078752-d0fc-44c2-8760-8ef7a5b1c7f6", "_uuid": "677cac8a825253c8a86332427728cdbc14ce3f32", "trusted": false, "_execution_state": "busy", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Now we look at the predictions side by side.", "metadata": {"_cell_guid": "655da3d7-c58c-4cbf-a138-cb4afffc90f4", "_uuid": "c2e41e426b8de62c5864d6631ca66b9a4f3057c1", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "fig = plt.figure()\nax = fig.add_subplot(111)\n\nax.scatter(train_pred, labels, c='b', marker=\"s\", label='pred')\n\nplt.legend()\n\nplt.show()", "metadata": {"_cell_guid": "dc60e060-c738-4dee-863b-3e426bbe531f", "_uuid": "d3736775dc9e13e25bba833902207775dbe88069", "trusted": false, "_execution_state": "busy", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Overall, the predictions seem tamed.  We will continue this and create polynomai features based on the prediction and create a model based on that, in hopes of generalizing the solution", "metadata": {"_cell_guid": "ac0bfc8e-9a60-4823-80a9-d050971f09e5", "_uuid": "aa5970feab29d6f0ce214b451c33f5c60d67b5d1", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "source": "sub['trip_duration'] = np.expm1(test_pred)\nsub.to_csv('submission.csv', index=False)", "metadata": {"_cell_guid": "213a90cc-c126-4f07-be77-2e68180f4738", "_uuid": "5f3b3b392f2c2f27338ee779e7a4a75652899b0b", "trusted": false, "_execution_state": "busy", "collapsed": false}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "", "metadata": {"collapsed": false, "_uuid": "9a5ba48d80b7de744b63030d491d06d2fba13a0a", "_execution_state": "idle", "_cell_guid": "839b031f-092f-4b7a-ac49-f9bbff562668"}, "execution_count": null, "outputs": []}], "nbformat": 4}