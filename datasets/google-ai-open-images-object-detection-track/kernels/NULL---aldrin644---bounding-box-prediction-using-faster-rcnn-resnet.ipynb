{"cells":[{"metadata":{"_uuid":"05fe90ffa5971458b6d4bd43b939eadcc2ab5df5"},"cell_type":"markdown","source":"This is a baseline kernel, the purpose of this kernel to provide the insight of the competition. It is using [Faster RCNN Inception Resnet v2](https://www.kaggle.com/aldrin644/r-faster-cnn-inception-v2) pretrained model on Old Open Image Dataset that contains 545 classes similar to New Open Image Dataset. I have done some analysis between old and new dataset's classes, checkout this [kernel](https://www.kaggle.com/aldrin644/analysis-between-new-and-old-open-image-dataset). I have used [Model Zoo's](https://www.kaggle.com/aldrin644/mods_folder) utility files for object detection purpose. "},{"metadata":{"_uuid":"faed4f817d4cd79506087918d97a2b5a010449f3"},"cell_type":"markdown","source":"**Folders in input directory, those contain all the necessary files**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nos.chdir('../input')\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca089f6428aa45539e6dcec49da9d57d462ec110"},"cell_type":"markdown","source":"**Importing all the necessary files**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport tensorflow as tf\nimport zipfile\n\nfrom collections import defaultdict\nfrom io import StringIO\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\nfrom mods_folder import ops as utils_ops\n\nimport json\n\nwith open('mods_folder/old_oid_labels.json') as f:\n    old_oid = json.load(f)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0ff4f585124c35365f87e1b63876cca3dd892471"},"cell_type":"code","source":"# This is needed to display the images.\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f81f31048c038c1ab45d6f37048e310962edebd"},"cell_type":"markdown","source":"**Setting path for model and labels **"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d69b6738b5d76eb86d0c015c0b18bb1c85f4ba75"},"cell_type":"code","source":"#MODEL_NAME = 'r-faster-cnn-inception-v2'\nMODEL_NAME = 'slow-rcnn'\nPATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n\nPATH_TO_LABELS = 'mods_folder/oid_bbox_trainable_label_map.pbtxt'\n\nNUM_CLASSES = 545","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74fadc1ca07d5ed51eb7c41a9d8fb225070307ae"},"cell_type":"markdown","source":"**Following imports are for visualization of bounding boxes on Image** \n\n*Just ignore the warning* "},{"metadata":{"trusted":true,"_uuid":"9ec69d2b9e4a62fadb7330166a667af54001b6ea","collapsed":true},"cell_type":"code","source":"from mods_folder import label_map_util\n\nfrom mods_folder import visualization_utils as vis_util","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad2ba5ce5a266b124e58c013ef3ab4e0440db75a"},"cell_type":"markdown","source":"**Loading the frozen graph**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"41322a04a66125462698d5297906081ffa6f8a5c"},"cell_type":"code","source":"detection_graph = tf.Graph()\nwith detection_graph.as_default():\n  od_graph_def = tf.GraphDef()\n  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n    serialized_graph = fid.read()\n    od_graph_def.ParseFromString(serialized_graph)\n    tf.import_graph_def(od_graph_def, name='')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76f514648a5ec2ec5394296789a6a5f713d1f19c"},"cell_type":"markdown","source":"**Loading class labels**"},{"metadata":{"trusted":true,"_uuid":"fbc0d03d11a55b8d464055efc56ebeba5ed4b7cd","collapsed":true},"cell_type":"code","source":"label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b12f938910e4d0d610f312cdbf7df021f5d4ab8"},"cell_type":"markdown","source":"**Converting images into numpy array**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2793cc3d82ba14620332765da85269ce40cbb223"},"cell_type":"code","source":"def load_image_into_numpy_array(image):\n  (im_width, im_height) = image.size\n  return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"579538a6e26117b540af0bc3340be052befaae54"},"cell_type":"markdown","source":"**Getting all the challenge test images**\n\n*running prediction on just 9 images, because 9 is my lucky number, kidding*"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5f162ec2f9561b075002dbdc5553351c4fbe0534"},"cell_type":"code","source":"os.listdir('google-ai-open-images-object-detection-track/test/challenge2018_test')\nPATH_TO_TEST_IMAGES_DIR = 'google-ai-open-images-object-detection-track/test/challenge2018_test'\n\nTEST_IMAGE_PATHS = os.listdir('google-ai-open-images-object-detection-track/test/challenge2018_test')   \n\nTEST_IMAGE_PATHS = TEST_IMAGE_PATHS[:9]\n\nIMAGE_SIZE = (12, 8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"292b1a53192c279c4c2956edccb68dd404fe7898"},"cell_type":"markdown","source":"**Here comes the main function**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"73172134f2e89a1df0e20fa9625144fd0853e2ec"},"cell_type":"code","source":"def run_inference_for_single_image(image, graph):\n  with graph.as_default():\n    with tf.Session() as sess:\n      # Get handles to input and output tensors\n      ops = tf.get_default_graph().get_operations()\n      all_tensor_names = {output.name for op in ops for output in op.outputs}\n      tensor_dict = {}\n      for key in [\n          'num_detections', 'detection_boxes', 'detection_scores',\n          'detection_classes', 'detection_masks'\n      ]:\n        tensor_name = key + ':0'\n        if tensor_name in all_tensor_names:\n          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n              tensor_name)\n      if 'detection_masks' in tensor_dict:\n        # The following processing is only for single image\n        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n        detection_masks_reframed = tf.cast(\n            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n        # Follow the convention by adding back the batch dimension\n        tensor_dict['detection_masks'] = tf.expand_dims(\n            detection_masks_reframed, 0)\n      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n\n      # Run inference\n      output_dict = sess.run(tensor_dict,\n                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n\n      # all outputs are float32 numpy arrays, so convert types as appropriate\n      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n      output_dict['detection_classes'] = output_dict[\n          'detection_classes'][0].astype(np.uint8)\n      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n      if 'detection_masks' in output_dict:\n        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n  return output_dict","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf8a3c4d0d30a89d9af330b26a35001c40c1818f"},"cell_type":"markdown","source":"**And the for loop that calls the main function again & again**\n\n*but only 9 times*"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"f61afe79ddbe29a15030488747de602be5dc6f9c","collapsed":true},"cell_type":"code","source":"prediction_list = []\nfor image_name in TEST_IMAGE_PATHS:\n  image_path = os.path.join('google-ai-open-images-object-detection-track/test/challenge2018_test',image_name)  \n  image = Image.open(image_path)\n  # the array based representation of the image will be used later in order to prepare the\n  # result image with boxes and labels on it.\n  image_np = load_image_into_numpy_array(image)\n  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n  image_np_expanded = np.expand_dims(image_np, axis=0)\n  # Actual detection.\n  output_dict = run_inference_for_single_image(image_np, detection_graph)\n    \n  # Visualization of the results of a detection.\n  detection_boxes = np.around(output_dict['detection_boxes'],decimals=2)\n  detection_classes = output_dict['detection_classes']\n  detection_scores = np.around(output_dict['detection_scores'],decimals=1)  \n  ind = np.where(output_dict['detection_boxes'].any(axis=1))[0]\n  ind = list(ind)\n\n  def get_class(id_num):\n        class_label = ''\n        for i in old_oid:\n            if(i['id'] == id_num):\n                class_label = i['name']\n                break\n        return class_label        \n  def get_class_name(id_num):\n        class_name = ''\n        for i in old_oid:\n            if(i['id'] == id_num):\n                class_name = i['display_name']\n                break\n        return class_name \n        \n                \n  pred_str = ''      \n  pred_name = ''\n  for i in ind: \n      l = output_dict['detection_boxes'][i]\n      id_num = output_dict['detection_classes'][i]\n      cls_label = get_class(id_num)\n      cls_name = get_class_name(id_num)  \n      prob = str('{:.4f}'.format(output_dict[\"detection_scores\"][i]))\n      pred_name = pred_name + cls_name + ' ' + prob + ' | '  \n      bounding_box = ' '.join([str('{:.4f}'.format(w))+' ' for w in l])  \n      pred_str = pred_str + cls_label + ' ' + prob + ' ' + bounding_box + ' '\n    \n  vis_util.visualize_boxes_and_labels_on_image_array(\n      image_np,\n      output_dict['detection_boxes'],\n      output_dict['detection_classes'],\n      output_dict['detection_scores'],\n      category_index,\n      instance_masks=output_dict.get('detection_masks'),\n      use_normalized_coordinates=True,\n      line_thickness=8)\n  plt.figure(figsize=IMAGE_SIZE)  \n  plt.imshow(image_np)\n  plt.show()\n  prediction_object = {'ImageId':image_name[:-4], 'PredictionString':pred_str}  \n  print('Detected Classes =>')\n  print(pred_name[:-1])\n  print()\n  print(prediction_object)\n  prediction_list.append(prediction_object)  \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"360e7df639b1e1ee4b02d2051bf7a2801d1bc206"},"cell_type":"markdown","source":"**Saving output into CSV**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cad6417802c398bcb170d1c7cc0a677d9fc64733"},"cell_type":"code","source":"df = pd.DataFrame.from_dict(prediction_list, orient='columns')\ndf.to_csv('../working/prediction.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}