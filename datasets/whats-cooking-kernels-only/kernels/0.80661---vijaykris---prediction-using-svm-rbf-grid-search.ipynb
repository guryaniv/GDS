{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score, fbeta_score, make_scorer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing, decomposition, svm, pipeline, metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import LabelEncoder\n\n\ntrain=pd.read_json('../input/train.json' )\ntest=pd.read_json('../input/test.json')\n\ntrain['ingredients'] = [\", \".join(ingredients) for ingredients in train['ingredients']]\ntest['ingredients']=[\", \".join(ingredients) for ingredients in test['ingredients']]\n\n#To find logloss\ndef multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota\n\n\n#Use label encoders to convert text labels into integers\n\nlblencdr=preprocessing.LabelEncoder()\ny_cuisine=lblencdr.fit_transform(train['cuisine'].values)\n#stratify option in train_test_split will ensure that the data is taken in the same proportion as the original dataset. This is particularly helpful in unbalanced datasets\nX_train, X_val, y_train, y_val = train_test_split(train['ingredients'].values, y_cuisine, \n                                                  stratify=y_cuisine,\n                                                  test_size=0.1,\n                                                  shuffle=True,\n                                                  random_state=0)\n\nvect=TfidfVectorizer().fit(list(X_train)+list(X_val))\nX_train_vect=vect.transform(X_train)\nx_valid_vect=vect.transform(X_val)\n\n#Grid Search Technique. Create a scorer function by using make_scorer\n#mll_scorer=metrics.make_scorer( multiclass_logloss, greater_is_better=False, needs_proba=True)\nftwo_scorer=metrics.make_scorer(fbeta_score, average='micro', beta=0.5)\n\n#create a grid of parameters\n\nparam_grid={\n            'kernel': ['rbf'],\n            'C': [0.1, 1.0, 100],\n            'gamma': [0,1,10]\n             }\n\nclassifier=SVC()\n\nmodel=GridSearchCV(estimator=classifier, param_grid=param_grid, scoring=ftwo_scorer,\n                  verbose=10, n_jobs=4, cv=5)\n\nmodel.fit(X_train_vect, y_train)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best Parameters set:\")\nbest_parameters=model.best_estimator_.get_params()\n\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\npredictions=model.predict(vect.transform(X_val))\nprint(f1_score(predictions, y_val, average='micro'))\ny_predict=model.predict(vect.transform(test['ingredients']))\n\ntest['cuisine']=lblencdr.inverse_transform(y_predict)\ntest = test.sort_values('id' , ascending=True)\n\ntest[['id' , 'cuisine' ]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}