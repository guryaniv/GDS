{"nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.3"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "0caae73d-0cf4-44aa-95a4-7c219052f6d0", "_uuid": "2c861ff17b0c59d24429763e2af411c8ea9102a7"}, "source": ["# *Introduction* : "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "c6ed1010-6b91-4d0e-aac8-5307bb4ed7be", "_uuid": "c484fbb204c2c28624dc2786f6894b5cb3935b0f"}, "source": ["I've seen recently **tomcwalker** [Kernel](https://www.kaggle.com/tomcwalker/keras-nn-with-custom-loss-function-for-gini-auc) that introduced a Differentiable Estimate of the AUC called the **Rank Statistic** that can be used as a loss to be optimized by our models. I did really like the approach and I went on exploring the maths involved and how to implement it using the **Numpy** Library. You'll see that the presentation lacks style compared to the other notebooks but this notebook will be my first in Kaggle, so feel free to comment/share your thoughts about it."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "6dfe18e6-1621-4888-b05d-5be83c8e700a", "_uuid": "9631f55ebd0bfaa85528a5ec93a016a168a7e27c"}, "source": ["# AUC Mathematical Definition :"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "85fb630f-8f21-4ef0-8176-1bad1e391797", "_uuid": "6db9d95e543298ca6ecb95466d0424f0ac17d1dc"}, "source": ["The AUC and GINI are quite similar in term of interpretation, both quantifies the power of a classifer to rank observations. Mathematicaly speaking, he AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example, which can be written :\n", "\n", "$$AUC= P( Classifier (x+) > Classifier (x\u2212) )$$\n", "\n", "The Classifier here means our predictive function, the function that maps the observations to the targets."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "816df96f-acf3-4f0c-965a-37a65e19e2fc", "_uuid": "0bc46a5d512f41401487c353cbb615e721e9573c"}, "source": ["# The Empirical AUC Metric"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "db04c93e-8126-4866-9015-f0381b602cdb", "_uuid": "8b9b8b9963b146e2690140d6f696e9115833b0ff"}, "source": ["To calculate the $AUC= P( Classifier (x+) > Classifier (x\u2212) )$ based on our observations, we should define an unbiased estimator to approach the probability with the data we have under our hands. A good estimator that we can generate from Data is :\n", "\n", "$$\\hat{AUC} = \\frac{1}{PN}\\sum_{i=1}^{P}\\sum_{j=1}^{N}H(Cl(x_i+) - Cl(x_j-))$$\n", "\n", "* With **P** and **N** are respectively the number of True positive labeled points and True negative labeled points\n", "* **H** is the **Heaviside** function.\n", "* **$x_i+$** and $x_j-$ are respectively positive labeled obsevations and negative labeled observations"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "c4f4b9e5-339e-4c68-92f1-dedb0b85c2c0", "_uuid": "0f49432ec713c8d05393f8c47957ccb5163f191e"}, "source": ["You can already see why the AUC isn't used as a loss function to optimize. the **Heaviside** function isn't differentiable and cannot be plugged to our gradient based optimization algorithms. but we will discuss this later."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d98b87e8-36f8-45a3-a28e-7949485bbe65", "_uuid": "72a2a03b591e90f88dffe9d8731ab3ff530f6938"}, "source": ["The next blocks will explain how to implement the AUC metric with a loop approach and a broadcasting approach."]}, {"cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "7783cb9f-d6be-4b0a-b006-17c7e37c2721", "_uuid": "76bfcc455e0dc222b2c609e1f24b4962ddada9ed"}, "source": ["import numpy as np\n", "\n", "#the true classes of our dataset\n", "y_true = np.random.binomial(n= 1, p= 0.05, size = 5000)\n", "#the predicted probabilities\n", "y_pred = np.random.random(size = 5000)\n", "#Define the heaviside function\n", "heaviside = np.vectorize(lambda x : 0 if x<0 else .5 if x == 0 else 1)\n", "\n", "#the loop implementation that matches with the sum definition\n", "def Loop_AUC(y_true, y_pred):\n", "    #the predictions of our classifier for all the positive labeled data\n", "    pos_pred = y_pred[ y_true == 1]\n", "    P = len(pos_pred) #the number of the positive population\n", "    \n", "    #the predictions of our classifier for all the negative labeled data\n", "    neg_pred = y_pred[ y_true == 0]\n", "    N = len(neg_pred) #the number of the negative population\n", "    \n", "    AUC = 0\n", "    for pos in pos_pred :\n", "        for neg in neg_pred :\n", "            AUC +=  heaviside(pos - neg)\n", "    \n", "    return AUC/(P*N)\n", "\n", "print('The AUC of a random Classifier is :', Loop_AUC(y_true, y_pred))"], "execution_count": 2}, {"cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "58d4b6a7-10e7-4148-881d-bee089c0938c", "_uuid": "e1b95d62c22fe51188ae1e0613f8865f3ccd1594"}, "source": ["#this is a more optimized approach that uses the broadcasting method of numpy that is much faster than\n", "#the loop version\n", "\n", "def Broadcasted_AUC(y_true, y_pred):\n", "    #the predictions of our classifier for all the positive labeled data\n", "    pos_pred = y_pred[ y_true == 1]\n", "    \n", "    #the predictions of our classifier for all the negative labeled data\n", "    neg_pred = y_pred[ y_true == 0]\n", "    #creates a matrix that have pairwise difference between all the observations\n", "    pairwise_matrix = pos_pred[:, np.newaxis] - neg_pred\n", "    transform = heaviside(pairwise_matrix)\n", "    \n", "    return transform.mean()\n", "\n", "print('The AUC of a Random Classifier is :', Broadcasted_AUC(y_true, y_pred))"], "execution_count": 3}, {"cell_type": "markdown", "metadata": {"_cell_guid": "eff46629-6260-4022-bcad-0d03d0156ef1", "_uuid": "04ec1d5826be73b80f95dca7ed7cc05b52754377"}, "source": ["That's cool right ? But how can we define a differentiable estimate ? The problem of the AUC formula being not differentiable is coming from the **Heaviside** function ( not even continuous ), which can be seen in the plot below."]}, {"cell_type": "code", "outputs": [], "metadata": {"_kg_hide-input": false, "_cell_guid": "29e12090-301b-4c46-a164-36ee321e3bc0", "_kg_hide-output": false, "_uuid": "d815103c9301665aceda2bbb497617bdb9cf93ad"}, "source": ["import matplotlib.pyplot as plt \n", "import seaborn as sns\n", "sns.set()\n", "\n", "X = np.linspace(-10,10,num= 100)\n", "Y = heaviside(X)\n", "\n", "plt.title('the Heaviside Function')\n", "plt.plot(X,Y)\n", "plt.show()"], "execution_count": 4}, {"cell_type": "markdown", "metadata": {"_cell_guid": "603fefff-df58-47d9-9688-0c4ec6ddb96f", "_uuid": "347d95f8936cfbe32c3fcb2c864be010daa12027"}, "source": ["Thus, we need to replace the heaviside function with a differentiable approximative function. The Heaviside function can be seen as an **Extreme** Sigmoid function, $$ Sigmoid : x,\\lambda \\to \\frac{1}{1 - \\exp^{- \\lambda x}} $$ that have an extreme value of $\\lambda$.\n", "\n"]}, {"cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ee8ccfa2-6bd1-4aab-88da-f6a07b80f526", "_uuid": "0dc15d33c66f06d284ffd929df40e4bd6f94e035"}, "source": ["def param_sigmoid(x,alpha):\n", "    return 1/(1+ np.exp(-alpha*x))\n", "\n", "fig ,ax = plt.subplots(ncols=3, sharey = True, figsize = (12,7))\n", "\n", "weak, normal_sigmoid, extreme_sigmoid = 0.1, 1, 10\n", "\n", "ax[0].plot(X,param_sigmoid(X,weak))\n", "ax[0].set_title('weak sigmoid')\n", "\n", "ax[1].plot(X,param_sigmoid(X,normal_sigmoid))\n", "ax[1].set_title('normal sigmoid')\n", "\n", "ax[2].plot(X,param_sigmoid(X,extreme_sigmoid))\n", "ax[2].set_title('extreme sigmoid')\n", "\n", "plt.show()"], "execution_count": 5}, {"cell_type": "markdown", "metadata": {"_cell_guid": "45886ff7-dc27-4387-8521-63a2335a772f", "_uuid": "9544fdff41d450766fb18150e5a0119e275405b0"}, "source": ["You can see that a $\\lambda = 10$ is sufficient to approximately estimate the **Heaviside** function. So we're gonna define our New loss function which is called the **Rank Statistic** in this [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.3727&rep=rep1&type=pdf)."]}, {"cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8e7e793a-1db4-4eab-b706-380ac001020d", "_uuid": "6fe2f03044c05b07f5f5165db3502e76816d4f1d"}, "source": ["def Rank_Statistic(y_true, y_pred):\n", "    #the predictions of our classifier for all the positive labeled data\n", "    pos_pred = y_pred[ y_true == 1]\n", "    \n", "    #the predictions of our classifier for all the negative labeled data\n", "    neg_pred = y_pred[ y_true == 0]\n", "    #creates a matrix that have pairwise difference between all the observations\n", "    pairwise_matrix = pos_pred[:, np.newaxis] - neg_pred\n", "    transform = param_sigmoid(pairwise_matrix, 10)\n", "    \n", "    return transform.mean()\n", "\n", "AUC = Broadcasted_AUC(y_true, y_pred)\n", "Rank_stat = Rank_Statistic(y_true, y_pred)\n", "\n", "print('The Rank Statistic of Random Classifier is :', Rank_stat)\n", "print(\"The difference between the AUC and it's differentiable estimation is :\", np.abs(AUC-Rank_stat))"], "execution_count": 6}, {"cell_type": "markdown", "metadata": {"_cell_guid": "3c9db81b-2887-421c-80ae-f3cc9db4861c", "_uuid": "fe32ba13e6c6a46633670d01ca24a42ae59c284f"}, "source": ["We can see that the difference isn't huge and we can try to optimize our models by using the Rank Statistic Loss. I'll try to develop the notebook by adding an implementation of the **Rank Statistic** using **Tensorflow** so you can try to tune your neural networks using this loss function."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "9ea6c253-040f-409c-bba8-be2351d8f0be", "_uuid": "75794975fe555fd76ee55d99c2de12ba75e0877a"}, "source": ["Please share your thoughts about it !"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b2ea3d81-7aa0-4899-a693-d7fa0b82e1e7", "_uuid": "a2bf694c633d04bf0651f4e2482fe4fc9edb5de8"}, "source": ["# To Be continued .."]}, {"cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "51839038-2a52-4262-84a1-1e5ae81f688d", "collapsed": true, "_uuid": "7fe2e1b9ca906db6918549ace06fb4c9ad01abd3"}, "source": [], "execution_count": null}]}