{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "395ebcae-279b-0516-f8e1-e5f96481674d"
      },
      "source": [
        "Use Resnet model for bottleneck feature extraction.\n",
        "---------------------------\n",
        "\n",
        "Resnet pre-trained model is available [here][1].\n",
        "I have used Inception_resnet_v2 model.\n",
        "\n",
        "If you come across this transfer learning concept first time, go through the [chapter of stanford CS231][2]. This blog will clear all your doubts about transfer learning.\n",
        "\n",
        "\n",
        "  [1]: https://github.com/tensorflow/models/tree/master/slim\n",
        "  [2]: http://cs231n.github.io/transfer-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ed055506-3a6c-cb70-90c2-f486007a07ce"
      },
      "source": [
        "Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d73e2815-e46f-26a2-04ca-86096d638781"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import math\n",
        "from glob import glob\n",
        "import os\n",
        "from scipy.misc import imread, imresize, imshow\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.contrib.slim as slim\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "67630f0d-4a2b-f14d-0d48-0c26811a50ef"
      },
      "outputs": [],
      "source": [
        "master = pd.read_csv(\"../input/train_labels.csv\")\n",
        "master.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7bed7fa4-57fa-1839-dc3c-3694f35827bb"
      },
      "source": [
        "Function read images in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d598f6a5-5c75-28be-87df-b952e40cbf0d"
      },
      "outputs": [],
      "source": [
        "data_dir = \"../input/train/\"\n",
        "\n",
        "def get_image_data_for_batch(images_in_batch):\n",
        "    batch_image_data = []#np.empty([10, 299,299,3], dtype=np.int)\n",
        "    for ix in images_in_batch:\n",
        "        temp = imread(data_dir+str(ix)+\".jpg\")\n",
        "        temp = imresize(temp, [299,299,3])\n",
        "        batch_image_data.append(temp)# - np.mean(temp)\n",
        "    return np.array(batch_image_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8367ffb3-4b99-6ff6-7225-f11d2996fea1"
      },
      "source": [
        "Let's declare the whole inception_resnet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4fb629d-591e-ee08-4d32-2a25f0b65efd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "\n",
        "def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
        "  \"\"\"Builds the 35x35 resnet block.\"\"\"\n",
        "  with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n",
        "    with tf.variable_scope('Branch_0'):\n",
        "      tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n",
        "    with tf.variable_scope('Branch_1'):\n",
        "      tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
        "      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n",
        "    with tf.variable_scope('Branch_2'):\n",
        "      tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
        "      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')\n",
        "      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')\n",
        "    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n",
        "    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
        "                     activation_fn=None, scope='Conv2d_1x1')\n",
        "    net += scale * up\n",
        "    if activation_fn:\n",
        "      net = activation_fn(net)\n",
        "  return net\n",
        "\n",
        "\n",
        "def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
        "  \"\"\"Builds the 17x17 resnet block.\"\"\"\n",
        "  with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\n",
        "    with tf.variable_scope('Branch_0'):\n",
        "      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n",
        "    with tf.variable_scope('Branch_1'):\n",
        "      tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\n",
        "      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n",
        "                                  scope='Conv2d_0b_1x7')\n",
        "      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n",
        "                                  scope='Conv2d_0c_7x1')\n",
        "    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n",
        "    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
        "                     activation_fn=None, scope='Conv2d_1x1')\n",
        "    net += scale * up\n",
        "    if activation_fn:\n",
        "      net = activation_fn(net)\n",
        "  return net\n",
        "\n",
        "\n",
        "def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
        "  \"\"\"Builds the 8x8 resnet block.\"\"\"\n",
        "  with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\n",
        "    with tf.variable_scope('Branch_0'):\n",
        "      tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n",
        "    with tf.variable_scope('Branch_1'):\n",
        "      tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\n",
        "      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n",
        "                                  scope='Conv2d_0b_1x3')\n",
        "      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n",
        "                                  scope='Conv2d_0c_3x1')\n",
        "    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n",
        "    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
        "                     activation_fn=None, scope='Conv2d_1x1')\n",
        "    net += scale * up\n",
        "    if activation_fn:\n",
        "      net = activation_fn(net)\n",
        "  return net\n",
        "\n",
        "\n",
        "def inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n",
        "                        dropout_keep_prob=0.8,\n",
        "                        reuse=None,\n",
        "                        scope='InceptionResnetV2'):\n",
        "  \"\"\"Creates the Inception Resnet V2 model.\n",
        "\n",
        "  Args:\n",
        "    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n",
        "    num_classes: number of predicted classes.\n",
        "    is_training: whether is training or not.\n",
        "    dropout_keep_prob: float, the fraction to keep before final layer.\n",
        "    reuse: whether or not the network and its variables should be reused. To be\n",
        "      able to reuse 'scope' must be given.\n",
        "    scope: Optional variable_scope.\n",
        "\n",
        "  Returns:\n",
        "    logits: the logits outputs of the model.\n",
        "    end_points: the set of end_points from the inception model.\n",
        "  \"\"\"\n",
        "  end_points = {}\n",
        "\n",
        "  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs], reuse=reuse):\n",
        "    with slim.arg_scope([slim.batch_norm, slim.dropout],\n",
        "                        is_training=is_training):\n",
        "      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
        "                          stride=1, padding='SAME'):\n",
        "\n",
        "        # 149 x 149 x 32\n",
        "        net = slim.conv2d(inputs, 32, 3, stride=2, padding='VALID',\n",
        "                          scope='Conv2d_1a_3x3')\n",
        "        end_points['Conv2d_1a_3x3'] = net\n",
        "        # 147 x 147 x 32\n",
        "        net = slim.conv2d(net, 32, 3, padding='VALID',\n",
        "                          scope='Conv2d_2a_3x3')\n",
        "        end_points['Conv2d_2a_3x3'] = net\n",
        "        # 147 x 147 x 64\n",
        "        net = slim.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')\n",
        "        end_points['Conv2d_2b_3x3'] = net\n",
        "        # 73 x 73 x 64\n",
        "        net = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
        "                              scope='MaxPool_3a_3x3')\n",
        "        end_points['MaxPool_3a_3x3'] = net\n",
        "        # 73 x 73 x 80\n",
        "        net = slim.conv2d(net, 80, 1, padding='VALID',\n",
        "                          scope='Conv2d_3b_1x1')\n",
        "        end_points['Conv2d_3b_1x1'] = net\n",
        "        # 71 x 71 x 192\n",
        "        net = slim.conv2d(net, 192, 3, padding='VALID',\n",
        "                          scope='Conv2d_4a_3x3')\n",
        "        end_points['Conv2d_4a_3x3'] = net\n",
        "        # 35 x 35 x 192\n",
        "        net = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
        "                              scope='MaxPool_5a_3x3')\n",
        "        end_points['MaxPool_5a_3x3'] = net\n",
        "\n",
        "        # 35 x 35 x 320\n",
        "        with tf.variable_scope('Mixed_5b'):\n",
        "          with tf.variable_scope('Branch_0'):\n",
        "            tower_conv = slim.conv2d(net, 96, 1, scope='Conv2d_1x1')\n",
        "          with tf.variable_scope('Branch_1'):\n",
        "            tower_conv1_0 = slim.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n",
        "                                        scope='Conv2d_0b_5x5')\n",
        "          with tf.variable_scope('Branch_2'):\n",
        "            tower_conv2_0 = slim.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n",
        "                                        scope='Conv2d_0b_3x3')\n",
        "            tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n",
        "                                        scope='Conv2d_0c_3x3')\n",
        "          with tf.variable_scope('Branch_3'):\n",
        "            tower_pool = slim.avg_pool2d(net, 3, stride=1, padding='SAME',\n",
        "                                         scope='AvgPool_0a_3x3')\n",
        "            tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n",
        "                                       scope='Conv2d_0b_1x1')\n",
        "          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_1,\n",
        "                              tower_conv2_2, tower_pool_1])\n",
        "\n",
        "        end_points['Mixed_5b'] = net\n",
        "        net = slim.repeat(net, 10, block35, scale=0.17)\n",
        "\n",
        "        # 17 x 17 x 1088\n",
        "        with tf.variable_scope('Mixed_6a'):\n",
        "          with tf.variable_scope('Branch_0'):\n",
        "            tower_conv = slim.conv2d(net, 384, 3, stride=2, padding='VALID',\n",
        "                                     scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_1'):\n",
        "            tower_conv1_0 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n",
        "                                        scope='Conv2d_0b_3x3')\n",
        "            tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n",
        "                                        stride=2, padding='VALID',\n",
        "                                        scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_2'):\n",
        "            tower_pool = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
        "                                         scope='MaxPool_1a_3x3')\n",
        "          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_2, tower_pool])\n",
        "\n",
        "        end_points['Mixed_6a'] = net\n",
        "        net = slim.repeat(net, 20, block17, scale=0.10)\n",
        "\n",
        "        # Auxiliary tower\n",
        "        with tf.variable_scope('AuxLogits'):\n",
        "          aux = slim.avg_pool2d(net, 5, stride=3, padding='VALID',\n",
        "                                scope='Conv2d_1a_3x3')\n",
        "          aux = slim.conv2d(aux, 128, 1, scope='Conv2d_1b_1x1')\n",
        "          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n",
        "                            padding='VALID', scope='Conv2d_2a_5x5')\n",
        "          aux = slim.flatten(aux)\n",
        "          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n",
        "                                     scope='Logits')\n",
        "          end_points['AuxLogits'] = aux\n",
        "\n",
        "        with tf.variable_scope('Mixed_7a'):\n",
        "          with tf.variable_scope('Branch_0'):\n",
        "            tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n",
        "                                       padding='VALID', scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_1'):\n",
        "            tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n",
        "                                        padding='VALID', scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_2'):\n",
        "            tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
        "            tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n",
        "                                        scope='Conv2d_0b_3x3')\n",
        "            tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n",
        "                                        padding='VALID', scope='Conv2d_1a_3x3')\n",
        "          with tf.variable_scope('Branch_3'):\n",
        "            tower_pool = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
        "                                         scope='MaxPool_1a_3x3')\n",
        "          net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1,\n",
        "                              tower_conv2_2, tower_pool])\n",
        "\n",
        "        end_points['Mixed_7a'] = net\n",
        "\n",
        "        net = slim.repeat(net, 9, block8, scale=0.20)\n",
        "        net = block8(net, activation_fn=None)\n",
        "\n",
        "        net = slim.conv2d(net, 1536, 1, scope='Conv2d_7b_1x1')\n",
        "        end_points['Conv2d_7b_1x1'] = net\n",
        "\n",
        "        with tf.variable_scope('Logits'):\n",
        "          end_points['PrePool'] = net\n",
        "          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n",
        "                                scope='AvgPool_1a_8x8')\n",
        "          net = slim.flatten(net)\n",
        "\n",
        "          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
        "                             scope='Dropout')\n",
        "\n",
        "          end_points['PreLogitsFlatten'] = net\n",
        "          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n",
        "                                        scope='Logits')\n",
        "          end_points['Logits'] = logits\n",
        "          end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')\n",
        "\n",
        "    return logits, end_points\n",
        "inception_resnet_v2.default_image_size = 299\n",
        "\n",
        "\n",
        "def inception_resnet_v2_arg_scope(weight_decay=0.00004,\n",
        "                                  batch_norm_decay=0.9997,\n",
        "                                  batch_norm_epsilon=0.001):\n",
        "  \"\"\"Yields the scope with the default parameters for inception_resnet_v2.\n",
        "\n",
        "  Args:\n",
        "    weight_decay: the weight decay for weights variables.\n",
        "    batch_norm_decay: decay for the moving average of batch_norm momentums.\n",
        "    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n",
        "\n",
        "  Returns:\n",
        "    a arg_scope with the parameters needed for inception_resnet_v2.\n",
        "  \"\"\"\n",
        "  # Set weight_decay for weights in conv2d and fully_connected layers.\n",
        "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
        "                      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
        "                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n",
        "\n",
        "    batch_norm_params = {\n",
        "        'decay': batch_norm_decay,\n",
        "        'epsilon': batch_norm_epsilon,\n",
        "    }\n",
        "    # Set activation_fn and parameters for batch_norm.\n",
        "    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n",
        "                        normalizer_fn=slim.batch_norm,\n",
        "                        normalizer_params=batch_norm_params) as scope:\n",
        "      return scope\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d0e95db7-fe77-945f-477d-f2d1ca191855"
      },
      "source": [
        "Now we can extract bottleneck features from any layer. I am using 2nd last layer of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5badd8e2-ed64-8a79-536a-e1a4e4f3e91c"
      },
      "outputs": [],
      "source": [
        "X = tf.placeholder(tf.float32, shape=[None, 299,299,3])\n",
        "with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
        "    logits, end_points = inception_resnet_v2(X, num_classes=1001,is_training=False)\n",
        "predictions = end_points[\"PreLogitsFlatten\"] #you can try with other layers too. Intermediate layers \n",
        "#may give good accuracy, but feature size for each image will be in 10k's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "14315839-7722-8067-2c17-dadc3cc97252"
      },
      "source": [
        "You have to download the checkpoint for initializing the network with pre-trained weights. Link to download the checkpoint file is [here][1].  As I am unable load this checkpoint file here, further part of this notebook won't run here. But I have tested it on my machine, works well.\n",
        "\n",
        "\n",
        "  [1]: http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "002470cf-c2e8-dea3-6327-4e7ec1c250e7"
      },
      "outputs": [],
      "source": [
        "X = tf.placeholder(tf.float32, shape=[None, 299,299,3])\n",
        "with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
        "    logits, end_points = inception_resnet_v2(X, num_classes=1001,is_training=False)\n",
        "predictions = end_points[\"PreLogitsFlatten\"]\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "saver.restore(sess, \"PATH TO inception_resnet_v2_2016_08_30.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aa6779c5-e038-2665-4b3d-44cd206f1b12"
      },
      "source": [
        "Now we can extract features from pre-trained model and use those features for applying any other algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6ccf41b9-9e7f-7f7d-3274-7f6cc9e66b2d"
      },
      "outputs": [],
      "source": [
        "number_of_batches = int(np.ceil(float(len(master))/10))\n",
        "bottleneck_features_train = np.empty(shape=[0, 1536])\n",
        "for i in range(number_of_batches):\n",
        "    batch_x = master[(i*10):(i*10)+10]['name']\n",
        "    image_data = get_image_data_for_batch(batch_x)\n",
        "    btnk_batch = sess.run(predictions, feed_dict={X:image_data})\n",
        "    bottleneck_features_train = np.concatenate([bottleneck_features_train, btnk_batch], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8b6a0028-6f14-67ca-4e21-015a5ac5f7da"
      },
      "source": [
        "Same way, we have extract features for test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d3ed2dea-cd96-353b-3157-a9a9c2b6f561"
      },
      "outputs": [],
      "source": [
        "data_dir = \"../input/test/\"\n",
        "test_image_names = range(1, 1532)\n",
        "number_of_batches_test = int(np.ceil(1531.0/10))\n",
        "bottleneck_features_test = np.empty(shape=[0, 1536])\n",
        "for i in range(number_of_batches_test):\n",
        "    batch_x = test_image_names[(i*10):(i*10)+10]\n",
        "    image_data = get_image_data_for_batch(batch_x)\n",
        "    btnk_batch = sess.run(predictions, feed_dict={X:image_data})\n",
        "    bottleneck_features_test = np.concatenate([bottleneck_features_test, btnk_batch], axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9a4f451-e7a9-57c0-1ee4-3c500f2cb7a2"
      },
      "outputs": [],
      "source": [
        "I have used Logistic regression from scikit learn, you can use other powerful algorithms like randomForest, xgboost etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cbb4d1b9-b8c8-7402-fbe0-34c5b9b0f255"
      },
      "outputs": [],
      "source": [
        "model =  LogisticRegression()\n",
        "model.fit(bottleneck_features_train,master['invasive'])\n",
        "\n",
        "predictions = model.predict(bottleneck_features_test)\n",
        "#Read sample submission file \n",
        "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
        "sample_submission['invasive'] = predictions\n",
        "\n",
        "sample_submission.to_csv(\"inception_resnet_logistic\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "14c91eb1-7585-416c-055d-a1dcf147660a"
      },
      "source": [
        "Next task will be to try different layers for bottleneck extraction, try other networks like InceptionV4, Resnet-150. Also to fine-tune these networks with our data."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}