{"metadata": {"kernelspec": {"display_name": "Python [default]", "name": "python3", "language": "python"}, "language_info": {"file_extension": ".py", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.5.4", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {}, "source": ["# Text Understanding from Scratch"], "cell_type": "markdown"}, {"metadata": {}, "source": ["This note book is a small demonstration of using Convolutional Neural Networks to do a texct classification task. The basic idea is to one-hot convert the characters into a vector and then runa small CNN on the resulting vectors."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["import pandas as pd\n", "import numpy as np\n", "import os\n", "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n", "from sklearn.cross_validation import train_test_split\n", "from nltk import sent_tokenize, word_tokenize\n", "import string\n", "from keras.models import Model\n", "from keras.optimizers import SGD\n", "from keras.layers import Input, Dense, Dropout, Flatten\n", "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n", "import matplotlib.pyplot as plt\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.utils import to_categorical\n", "%matplotlib inline\n", "print(os.getcwd())"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["First we read the data, convert the text to a series of vectors, and then one-hot encode the targets"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["texts = pd.read_csv( '../input/train.csv')"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"scrolled": true}, "source": ["texts.head()"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": ["authors = texts['author']\n", "texts = texts['text']"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": ["def create_vocab_set():\n", "    #https://github.com/johnb30/py_crepe/\n", "    #This alphabet is 69 chars vs. 70 reported in the paper since they include two\n", "    # '-' characters. See https://github.com/zhangxiangxiao/Crepe#issues.\n", "\n", "    alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n", "                list(string.punctuation) + ['\\n'])\n", "    vocab_size = len(alphabet)\n", "    check = set(alphabet)\n", "\n", "    vocab = {}\n", "    reverse_vocab = {}\n", "    for ix, t in enumerate(alphabet):\n", "        vocab[t] = ix\n", "        reverse_vocab[ix] = t\n", "\n", "    return vocab, reverse_vocab, vocab_size, check"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": ["def encode_data(x, maxlen, vocab, vocab_size, check):\n", "    #https://github.com/johnb30/py_crepe/\n", "    #Iterate over the loaded data and create a matrix of size maxlen x vocabsize\n", "    #In this case that will be 1014x69. This is then placed in a 3D matrix of size\n", "    #data_samples x maxlen x vocab_size. Each character is encoded into a one-hot\n", "    #array. Chars not in the vocab are encoded into an all zero vector.\n", "\n", "    input_data = np.zeros((len(x), maxlen, vocab_size))\n", "    for dix, sent in enumerate(x):\n", "        counter = 0\n", "        sent_array = np.zeros((maxlen, vocab_size))\n", "        chars = list(sent.lower().replace(' ', ''))\n", "        for c in chars:\n", "            if counter >= maxlen:\n", "                pass\n", "            else:\n", "                char_array = np.zeros(vocab_size, dtype=np.int)\n", "                if c in check:\n", "                    ix = vocab[c]\n", "                    char_array[ix] = 1\n", "                sent_array[counter, :] = char_array\n", "                counter += 1\n", "        input_data[dix, :, :] = sent_array\n", "\n", "    return input_data"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": ["maxlen = 140\n", "vocab, reverse_vocab, vocab_size, check = create_vocab_set()\n", "encoded = encode_data(texts, maxlen, vocab, vocab_size, check)"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": ["lb = LabelBinarizer()\n", "lb.fit(authors)\n", "targets = lb.transform(authors)"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["encoded.shape"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["targets.shape"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["print(encoded[0])\n", "print(targets[0])"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": ["X_train, X_test, y_train, y_test = train_test_split(encoded, targets, test_size=0.2, random_state=1234) "], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["Now we build the CNN model to do our training! This CNN has 7 conv layers (some followed by  maxpooling) and then 2 fully connected layers to finish off. "], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": ["nb_filter = 256\n", "dense_outputs = 1024\n", "cat_output = 3\n", "batch_size = 80\n", "nb_epoch = 10"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')\n", "conv0 = Convolution1D(nb_filter=nb_filter, filter_length=7, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(inputs)\n", "conv0 = MaxPooling1D(pool_length=2)(conv0)\n", "\n", "conv1 = Convolution1D(nb_filter=nb_filter, filter_length=7, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv0)\n", "conv1 = MaxPooling1D(pool_length=2)(conv1)\n", "\n", "conv2 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv1)\n", "\n", "conv3 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv2)\n", "\n", "conv4 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv3)\n", "\n", "conv5 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv4)\n", "\n", "conv6 = Convolution1D(nb_filter=nb_filter, filter_length=4, border_mode='valid', activation='relu', input_shape=(maxlen, vocab_size))(conv5)\n", "conv6 = MaxPooling1D(pool_length=2)(conv6)\n", "conv6 = Flatten()(conv5)\n", "\n", "dense0 = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv6))\n", "dense1 = Dropout(0.5)(Dense(dense_outputs, activation='relu')(dense0))\n", "\n", "pred = Dense(cat_output, activation='softmax', name='output')(dense1)\n", "\n", "model = Model(input=inputs, output=pred)\n", "\n", "sgd = SGD(lr=0.01, momentum=0.9)\n", "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["We'll only train this model for 5 epochs, but you can train it for far longer if you wish!"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["model.fit(x=X_train, y=y_train, epochs=5)"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": ["y_pred = model.predict(X_test)"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["y_pred"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["This evaluation won't be that great - the network needs to be trained for much longer (if yo have a mchine with a CUDA enabled gpu, the training will be significantly faster.)"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["model.evaluate(X_test, y_test)"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n", "result = pd.read_csv('../input/sample_submission.csv')\n", "for a, i in a2c.items():\n", "    result[a] = y_pred[:, i]\n", "#to_submit=result"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true}, "source": [], "outputs": [], "cell_type": "code"}]}