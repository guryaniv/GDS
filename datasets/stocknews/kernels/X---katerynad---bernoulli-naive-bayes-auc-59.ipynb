{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8340d1eb-868c-8d45-945d-9c84399010db"
      },
      "source": [
        "The script compares scores for Naive Bayes (baseline), Bernoulli Naive Bayes and different n-grams, n days shifts (how yesterday's news impact today's index), smoothing parameters. Different combinations of Top news columns are used in the analysis.\n",
        "\n",
        "The best result  for all Top news columns combination (AUC - 55%) is achieved with 3-days shift and 0.5 smoothing parameter.\n",
        "\n",
        "I did not find any n-grams change the result.\n",
        "\n",
        "1-day shift and smoothing parameter 0  provides almost the same AUC but precision and recall scores are better for 3-days shift and 0.5 smoothing parameter\n",
        "\n",
        "Other combination of Top news columns can give us 56 - 59% AUC\n",
        "\n",
        "Combined Top3, Top12 and Top25, 2 days shift - 59%, Top10 and Top25, no days shift - 58%\n",
        "Top25 only and 0 days shift, combined Top1 and Top6 3-days shift etc - 56%\n",
        "\n",
        "\n",
        "Looks like  the rating from RedditNews is not important for the index, maybe the source of the news is more significant\n",
        "\n",
        "Please see below the detail results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "99ad2797-36a7-e8ed-081f-f3c70d5d62ad"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import Series,DataFrame\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from datetime import date\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\n",
        "from scipy import interp\n",
        "\n",
        "#Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b80cfb49-7da7-60a7-fc9f-9901f85c7897"
      },
      "outputs": [],
      "source": [
        "#List to keep different methods scores to compare\n",
        "ScoreSummaryByMethod=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "78735ad8-a191-1bf3-e332-b7fb9852f3f7"
      },
      "outputs": [],
      "source": [
        "#data\n",
        "df=pd.read_csv('../input/Combined_News_DJIA.csv')\n",
        "df['Combined']=df.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "db782ac8-ac48-b31b-3a9a-ddeac5be1b77"
      },
      "outputs": [],
      "source": [
        "#train data\n",
        "train=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined']]\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "92ca95d6-c2d2-7dfd-7c59-8f73290a792d"
      },
      "outputs": [],
      "source": [
        "#test data\n",
        "test=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined']]\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e79c709f-350c-d5cd-1d2d-09d4b111c067"
      },
      "source": [
        "I run different classification models on the same data to compare the results. So I combine text processing, plotting and evaluation in specific functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "be1c5e9b-5656-6946-6753-43e773c646ae"
      },
      "outputs": [],
      "source": [
        "#Text pre-processing\n",
        "\n",
        "def text_process(text):\n",
        "    \"\"\"\n",
        "    Takes in a string of text, then performs the following:\n",
        "    1. Tokenizes and removes punctuation\n",
        "    2. Removes  stopwords\n",
        "    3. Stems\n",
        "    4. Returns a list of the cleaned text\n",
        "    \"\"\"\n",
        "    if pd.isnull(text):\n",
        "        return []\n",
        "    # tokenizing\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    text_processed=tokenizer.tokenize(text)\n",
        "    \n",
        "    # removing any stopwords\n",
        "    text_processed = [word.lower() for word in text_processed if word.lower() not in stopwords.words('english')]\n",
        "    \n",
        "    # steming\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    \n",
        "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
        "    \n",
        "    try:\n",
        "        text_processed.remove('b')\n",
        "    except: \n",
        "        pass\n",
        "\n",
        "    return text_processed\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4bb4d28f-1d4f-2759-e234-3a674603e9f0"
      },
      "outputs": [],
      "source": [
        "def ROCCurves (Actual, Predicted):\n",
        "    '''\n",
        "    Plot ROC curves for the multiclass problem\n",
        "    based on http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
        "    '''\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    n_classes=2\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(Actual.values, Predicted)\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Actual.ravel(), Predicted.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "    ##############################################################################\n",
        "    # Plot ROC curves for the multiclass problem\n",
        "\n",
        "    # Compute macro-average ROC curve and ROC area\n",
        "\n",
        "    # First aggregate all false positive rates\n",
        "\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "    # Then interpolate all ROC curves at this points\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "    # Finally average it and compute AUC\n",
        "    mean_tpr /= n_classes\n",
        "\n",
        "    fpr[\"macro\"] = all_fpr\n",
        "    tpr[\"macro\"] = mean_tpr\n",
        "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "    # Plot all ROC curves\n",
        "    plt.figure()\n",
        "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         linewidth=2)\n",
        "\n",
        "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         linewidth=2)\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                   ''.format(i, roc_auc[i]))\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
        "    plt.legend(loc=\"lower right\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "93efd6fb-d9f5-49b8-b41a-dfb5226288c5"
      },
      "outputs": [],
      "source": [
        "def heatmap(data, rotate_xticks=True):\n",
        "  fig, ax = plt.subplots()\n",
        "  heatmap = sns.heatmap(data, cmap=plt.cm.Blues)\n",
        "  ax.xaxis.tick_top()\n",
        "  if rotate_xticks:\n",
        "      plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b84dc8b1-37a3-b86c-4656-062bc340a6f5"
      },
      "outputs": [],
      "source": [
        "def plot_classification_report(classification_report):\n",
        "    lines = classification_report.split('\\n')\n",
        "    classes = []\n",
        "    plotMat = []\n",
        "    for line in lines[2 : (len(lines) - 3)]:\n",
        "        t = line.split()\n",
        "        classes.append(t[0])\n",
        "        v = [float(x) for x in t[1: len(t) - 1]]\n",
        "        plotMat.append(v)\n",
        "    aveTotal = lines[len(lines) - 1].split()\n",
        "    classes.append('avg/total')\n",
        "    vAveTotal = [float(x) for x in t[1:len(aveTotal) - 1]]\n",
        "    plotMat.append(vAveTotal)\n",
        "    df_classification_report = DataFrame(plotMat, index=classes,columns=['precision', 'recall', 'f1-score'])\n",
        "    heatmap(df_classification_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "51aa12a7-a6fa-0652-ca7c-9b494706f370"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(confusion_matrix,classes=['0','1']):\n",
        "    df_confusion_matrix = DataFrame(confusion_matrix, index=classes,columns=classes)\n",
        "    heatmap(df_confusion_matrix,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "11d2e03d-b0ef-235c-5b29-27aaf44b07c0"
      },
      "outputs": [],
      "source": [
        "def Evaluation (Method,Comment,Actual, Predicted):\n",
        "    '''\n",
        "        Prints and plots\n",
        "        - classification report\n",
        "        - confusion matrix\n",
        "        - ROC-AUC\n",
        "    '''\n",
        "    print (Method)\n",
        "    print (Comment)\n",
        "    print (classification_report(Actual,Predicted))\n",
        "    #plot_classification_report(classification_report(Actual,Predicted))\n",
        "    print ('Confussion matrix:\\n', confusion_matrix(Actual,Predicted))\n",
        "    #plot_confusion_matrix(confusion_matrix(Actual,Predicted))\n",
        "    ROC_AUC=roc_auc_score(Actual,Predicted)\n",
        "    print ('ROC-AUC: ' + str(ROC_AUC))\n",
        "    #ROCCurves (Actual,Predicted)\n",
        "    Precision=precision_score(Actual,Predicted)\n",
        "    Accuracy=accuracy_score(Actual,Predicted)\n",
        "    Recall=recall_score(Actual,Predicted)\n",
        "    F1=f1_score(Actual,Predicted)\n",
        "    ScoreSummaryByMethod.append([Method,Comment,ROC_AUC,Precision,Accuracy,Recall,F1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5ad4039f-9121-74c6-cab0-da940cea6455"
      },
      "outputs": [],
      "source": [
        "#Creating a Data Pipeline for Naive Bayes classifier classifier - baseline\n",
        "nb_pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
        "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
        "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
        "])\n",
        "nb_pipeline.fit(train['Combined'],train['Label'])\n",
        "predictions = nb_pipeline.predict(test['Combined'])\n",
        "Evaluation ('MultinomialNB','no shift, no n-grams, combined Top news',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b099461-0372-9eff-bab4-02694c8abc4b"
      },
      "outputs": [],
      "source": [
        "#Creating a Data Pipeline for Bernoulli Naive Bayes classifier classifier and n-grams, default alpha=1\n",
        "bnb_2ngram_pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer(analyzer=text_process,ngram_range=(1, 2))),  # strings to token integer counts\n",
        "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
        "    ('classifier', BernoulliNB(binarize=0.0)),  # train on TF-IDF vectors w/ Bernoulli Naive Bayes classifier\n",
        "])\n",
        "bnb_2ngram_pipeline.fit(train['Combined'],train['Label'])\n",
        "predictions = bnb_2ngram_pipeline.predict(test['Combined'])\n",
        "Evaluation ('BernoulliNB(binarize=0.0)','default alpha=1,no shift, ngram_range=(1, 2), combined Top news',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "63667f23-fa70-20e2-ccde-46064a243e21"
      },
      "outputs": [],
      "source": [
        "#1 days shift\n",
        "df.Label = df.Label.shift(-1)\n",
        "df.drop(df.index[len(df)-1], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2151fbf0-cdc4-d6ba-20c5-f475c47e4361"
      },
      "outputs": [],
      "source": [
        "#new train data\n",
        "train=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined']]\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d9a453ae-db95-f266-8f77-79419c6a201b"
      },
      "outputs": [],
      "source": [
        "#new test data\n",
        "test=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined']]\n",
        "test.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a8fadbac-294e-8c15-1548-0bce2a8213e4"
      },
      "source": [
        "****The best result for Bernoulli Naive Bayes classifier, 1-2 n-grams and  1-day shift is smoothing alpha = 0****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e627f55e-6e85-086c-ba9f-426dc035594d"
      },
      "outputs": [],
      "source": [
        "#The best result for Bernoulli Naive Bayes classifier, 1-2 n-grams and 1-day shift is smoothing alpha = 0 \n",
        "bnb_2ngram_pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer(analyzer=text_process,ngram_range=(1, 2))),\n",
        "    ('tfidf', TfidfTransformer()), \n",
        "    ('classifier', BernoulliNB(alpha=0.0, binarize=0.0))])\n",
        "bnb_2ngram_pipeline.fit(train['Combined'],train['Label'])\n",
        "predictions = bnb_2ngram_pipeline.predict(test['Combined'])\n",
        "Evaluation ('BernoulliNB(alpha=0.0,binarize=0.0)','1-day shift, ngram_range=(1, 2), combined Top news',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cf5aa165-79b8-f255-9011-40e664ab3354"
      },
      "source": [
        "**2-days shift produces with any smoothing alpha gives worse results then 3-days shift\n",
        "I skip 2-days shift and demo the best result for 3-days shift**\n",
        "#The best result for Bernoulli Naive Bayes classifier, 1-2 n-grams and 3-day shift is smoothing alpha = 0.5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1264e666-9615-d818-55ea-c688e1a1649e"
      },
      "outputs": [],
      "source": [
        "#3 days shift\n",
        "df.Label = df.Label.shift(-2)\n",
        "df.drop(df.index[len(df)-1], inplace=True)\n",
        "df.drop(df.index[len(df)-1], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a76b148f-40c9-59d6-4261-f48027ed9422"
      },
      "outputs": [],
      "source": [
        "#new train data\n",
        "train=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined']]\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1bc7e136-9830-fed2-3d40-b6d91164537c"
      },
      "outputs": [],
      "source": [
        "#new test data\n",
        "test=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined']]\n",
        "test.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e2209dd-6ff3-3502-9849-4c15277a44dc"
      },
      "outputs": [],
      "source": [
        "#The best result for Bernoulli Naive Bayes classifier, 1-2 n-grams and 3-day shift is smoothing alpha = 0.5 \n",
        "bnb_2ngram_pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer(analyzer=text_process,ngram_range=(1, 2))),\n",
        "    ('tfidf', TfidfTransformer()), \n",
        "    ('classifier', BernoulliNB(alpha=0.5, binarize=0.0))])\n",
        "bnb_2ngram_pipeline.fit(train['Combined'],train['Label'])\n",
        "predictions = bnb_2ngram_pipeline.predict(test['Combined'])\n",
        "Evaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','3-days shift, ngram_range=(1, 2), combined Top news',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5b8d9cb6-fdd7-f859-8d8a-ea3a5dd410ca"
      },
      "outputs": [],
      "source": [
        "ROCCurves (test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "81f4f2ce-276c-bf60-2ed4-0546e5ce3f73"
      },
      "source": [
        "**Let's explore different combinations of Top news columns**\n",
        "*Here are few the best:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "05446ca2-0c61-f623-5198-0b053187cd24"
      },
      "outputs": [],
      "source": [
        "#Here is the pipeline we use for the differenet data sets\n",
        "bnb_2ngram_pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer(analyzer=text_process,ngram_range=(1, 2))),\n",
        "    ('tfidf', TfidfTransformer()), \n",
        "    ('classifier', BernoulliNB(alpha=0.5, binarize=0.0))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9766c267-e8a2-a928-7120-28033f9c4de9"
      },
      "outputs": [],
      "source": [
        "#data re-new\n",
        "df=pd.read_csv('../input/Combined_News_DJIA.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc4ce750-2aa1-c3ac-575d-8eea8a78b278"
      },
      "outputs": [],
      "source": [
        "#Combination 10 and 25\n",
        "df['Combined10_25']=df.iloc[:,[11,26]].apply(lambda row: ''.join(str(row.values)), axis=1)\n",
        "#Combination 12 and 25\n",
        "df['Combined12_25']=df.iloc[:,[13,26]].apply(lambda row: ''.join(str(row.values)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8477c289-399c-80d8-370f-83d6cb35c79c"
      },
      "outputs": [],
      "source": [
        "#train data\n",
        "train=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Top1','Top12','Top25','Combined10_25','Combined12_25','Combined3_12_25']]\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "de79cbbf-13a5-ea31-ee39-5c9d3ef1833f"
      },
      "outputs": [],
      "source": [
        "#test data\n",
        "test=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Top1','Top12','Top25','Combined10_25','Combined12_25','Combined3_12_25']]\n",
        "test.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02f58d47-23b0-2006-1fff-dfccc3ddfe7c"
      },
      "outputs": [],
      "source": [
        "#no changes in the pipeline. We just use other data sets\n",
        "#Top1, no shift, baseline\n",
        "bnb_2ngram_pipeline.fit(train['Top1'],train['Label'])\n",
        "predictions = bnb_2ngram_pipeline.predict(test['Top1'])\n",
        "Evaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','no shift, ngram_range=(1, 2),Top1 only',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "42860a1c-5fed-ff4a-84a2-3e238a9bbef6"
      },
      "outputs": [],
      "source": [
        "#no changes in the pipeline. We just use other data sets\n",
        "#Top25, no shift\n",
        "bnb_2ngram_pipeline.fit(train['Top25'],train['Label'])\n",
        "predictions = bnb_2ngram_pipeline.predict(test['Top25'])\n",
        "Evaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','no shift, ngram_range=(1, 2),Top25 only',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6e3cc3e5-bc96-97b4-3afe-404e8ae79fc4"
      },
      "outputs": [],
      "source": [
        "#no changes in the pipeline. We just use other data sets\n",
        "#Combined12_25, no shift\n",
        "bnb_2ngram_pipeline.fit(train['Combined12_25'],train['Label'])\n",
        "predictions = bnb_2ngram_pipeline.predict(test['Combined12_25'])\n",
        "Evaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','no shift, ngram_range=(1, 2),Combined Top12 and Top25',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4cb01773-5de2-bf54-262a-feb2cb8e5256"
      },
      "outputs": [],
      "source": [
        "#no changes in the pipeline. We just use other data sets\n",
        "#Combined10_25, no shift\n",
        "bnb_2ngram_pipeline.fit(train['Combined10_25'],train['Label'])\n",
        "predictions = bnb_2ngram_pipeline.predict(test['Combined10_25'])\n",
        "Evaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','no shift, ngram_range=(1, 2),Combined Top10 and Top25',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2239d6d1-5c57-1fe5-ecf1-699aab9023fc"
      },
      "outputs": [],
      "source": [
        "ROCCurves (test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d3b59849-cc4b-9c54-17b0-e63379f03e7e"
      },
      "outputs": [],
      "source": [
        "#let's shift the data and explore Top3, Top12 and Top25 combination for 2 days shift\n",
        "df.Label = df.Label.shift(-1)\n",
        "df.drop(df.index[len(df)-1], inplace=True)\n",
        "df.Label = df.Label.shift(-1)\n",
        "df.drop(df.index[len(df)-1], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "20582a0c-cd47-dd57-364c-3d5812954e3f"
      },
      "outputs": [],
      "source": [
        "#Combination 3,12 and 25\n",
        "df['Combined3_12_25']=df.iloc[:,[4,13,26]].apply(lambda row: ''.join(str(row.values)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae463f14-f27b-ba27-7ba1-a46699c1e360"
      },
      "outputs": [],
      "source": [
        "#train data\n",
        "train=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined3_12_25']]\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aca1600e-92a4-b88d-4b38-f5c807d62f68"
      },
      "outputs": [],
      "source": [
        "#test data\n",
        "test=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined3_12_25']]\n",
        "test.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b6351512-92c3-426e-bb4c-b0e872c496dd"
      },
      "outputs": [],
      "source": [
        "#no changes in the pipeline. We just use other data sets\n",
        "#Combined Top3, Top12 and Top25, 3-days shift\n",
        "bnb_2ngram_pipeline.fit(train['Combined3_12_25'],train['Label'])\n",
        "predictions = bnb_2ngram_pipeline.predict(test['Combined3_12_25'])\n",
        "Evaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','2-days shift, ngram_range=(1, 2),Combined Top3,top12 and Top25',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fe70574a-6b84-a4c8-dc6e-4f82674a7065"
      },
      "outputs": [],
      "source": [
        "ROCCurves (test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e31d72e9-3e64-c777-d3f4-2248438d9e24"
      },
      "outputs": [],
      "source": [
        "#let's shift the data and explore Top1 and Top6 combination for 3 days shift\n",
        "df.Label = df.Label.shift(-1)\n",
        "df.drop(df.index[len(df)-1], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e79a652a-c01e-1cec-c15f-18c6371c5c98"
      },
      "outputs": [],
      "source": [
        "#Combination 1 and 6\n",
        "df['Combined1_6']=df.iloc[:,[2,7]].apply(lambda row: ''.join(str(row.values)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9cb9dcc4-bfa2-5318-343f-8ae45a9c9017"
      },
      "outputs": [],
      "source": [
        "#train data\n",
        "train=df.loc[(pd.to_datetime(df[\"Date\"]) <= date(2014,12,31)),['Label','Combined1_6']]\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0a672e4d-8519-d324-6e5e-a141c4883b53"
      },
      "outputs": [],
      "source": [
        "#test data\n",
        "test=df.loc[(pd.to_datetime(df[\"Date\"]) > date(2014,12,31)),['Label','Combined1_6']]\n",
        "test.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f79e812-fe76-83be-6564-00140afd9ed2"
      },
      "outputs": [],
      "source": [
        "#no changes in the pipeline. We just use other data sets\n",
        "#Combined Top1 and Top6, 3-days shift\n",
        "bnb_2ngram_pipeline.fit(train['Combined1_6'],train['Label'])\n",
        "predictions = bnb_2ngram_pipeline.predict(test['Combined1_6'])\n",
        "Evaluation ('BernoulliNB(alpha=0.5,binarize=0.0)','3-days shift, ngram_range=(1, 2),Combined Top1 and Top6',test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8b094a52-5c79-6bdb-abb1-6a0a7aeb3870"
      },
      "outputs": [],
      "source": [
        "ROCCurves (test[\"Label\"], predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "37ec1641-4d5d-acd7-c553-50804613db8a"
      },
      "source": [
        "**Score Summary by Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "470144d0-a54f-2a67-99a9-3d5d7f1ab35e"
      },
      "outputs": [],
      "source": [
        "df_ScoreSummaryByMethod=DataFrame(ScoreSummaryByMethod,columns=['Method','Comment','ROC_AUC','Precision','Accuracy','Recall','F1'])\n",
        "df_ScoreSummaryByMethod.sort_values(['ROC_AUC'],ascending=False,inplace=True)\n",
        "df_ScoreSummaryByMethod.head(20)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}