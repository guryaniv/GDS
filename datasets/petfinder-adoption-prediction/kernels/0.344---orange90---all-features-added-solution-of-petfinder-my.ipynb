{"cells":[{"metadata":{"_uuid":"b762ae803bdcf967488ba9d2b5e81f0bbe309c17"},"cell_type":"markdown","source":"This Kernel is created to explain everything in competition [PetFinder.my](https://www.kaggle.com/c/petfinder-adoption-prediction), it encourage everyone to develop algorithms to predict the adoptability of pets, guide shelters and rescuers around the world on improving their pet profiles' appeal, reducing animal suffering and euthanization.\n\nMeanwhile, I will add my ideas and explanations in between, to tell the reader while I perform such action in such step.\n\nFinally, I will mention my future working direction in the last chapter, hopefully it can give you some inspiration\n\nThe kernel is still updating. Your upvotes and folks will be my best motivation.\n"},{"metadata":{"_uuid":"6033f2d76c939609b1a1234f14647d836ba4cbc6"},"cell_type":"markdown","source":"# Maintainance Log\n\n| Time       | version | remark                                                                |  Score  | commit version |\n|------------|---------|-----------------------------------------------------------------------|\n| 2019-01-20 | v1.0.0    | basic models only using features from train.csv, combiner model used  | **0.343**  | - |\n| 2019-01-20 | v1.0.1   | do balancing on class 0  |  0.339 | -|\n| 2019-01-21 | v1.1.0   | add length of description as a feature to test  |  0.338 | -|\n| 2019-01-21 | v1.1.1   | add regularization for xgb |  0.343 |  v12 |\n    | 2019-01-24 | v2.0.0 | add features from descritption, refactored the code  |  0.337 | v13|\n     | 2019-01-24 | v2.0.1| cancel balancing |  0.329 | v15|\n      | 2019-01-28 | v2.1.0| use tf-idf to extract feature from description text |   | v16|\n    | TBD | v3.0.0 | add features from images  |  - | -|\n"},{"metadata":{"_uuid":"3f3f846b58d6d14fb3c7cc13666754782ded868b"},"cell_type":"markdown","source":"# Table of Content:\n* [Introduction](#introduction)\n    * [Input](#input)\n    * [Ranking Criteria](#ranking)\n    * [Output](#output)\n* [Exploratory Data Analysis](#eda)\n    * [Data loading](#loading)\n    * [Main Data Exploration](#mainEDA)\n        * [AdoptionSpeed](#adoptionspeed)\n        * [Type](#type)\n        * [Age](#age)\n        * [Gender](#gender)\n        * [State](#state)\n        * [Photo amount and video amount distribution](#amount)\n* [Modelling](#model0)\n    * [Model Selection](#selection)\n    * [Model from original train dataset](#model1)\n    * [Model with description sentiments](#model2)\n    * [Model with images features and above](#model3)\n* [Feature Importance and Conclusion](#importance)\n* [Result and Submission](#result)"},{"metadata":{"_uuid":"2a858605bea4e99440c81c17323a2538ed3ac7f3"},"cell_type":"markdown","source":"# Some Flags may be used to control process"},{"metadata":{"trusted":true,"_uuid":"42d579f35fa3e962ef5c8401ebca64bd07f44901"},"cell_type":"code","source":"BALANCING = False\nMODEL_USE = 3\n# 0 is run all model(it takes quite long)\n# 1 is Model from original train dataset,\n# 2 is Model from original train dataset and description,\n# 3 Model with images features and above, yet to be implemented","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9eace99180d6ab2d6386742eab20d1649f2fb19"},"cell_type":"markdown","source":"# Introduction <a class=\"anchor\" id=\"Introduction\"></a>"},{"metadata":{"_uuid":"b19d7b79b586245417d196002a007955ab912245"},"cell_type":"markdown","source":"## Input <a class=\"anchor\" id=\"input\"></a>\nFirst, Let's look into what data they have provided"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"here are the information provided by the offical, I assumed that you have already read the [data introduction](https://www.kaggle.com/c/petfinder-adoption-prediction/data).  I listed them down here for short.\n### File descriptions \n* train.csv - Tabular/text data for the training set\n* test.csv - Tabular/text data for the test set\n* sample_submission.csv - A sample submission file in the correct format\n* breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\n* color_labels.csv - Contains ColorName for each ColorID\n* state_labels.csv - Contains StateName for each StateID\n* Images - pets' photos\n* Image Metadata - analysis on Face Annotation, Label Annotation, Text Annotation and Image Properties. \n* Sentiment Data - profile's description  analysis on sentiment and key entities. \n\ndetailed analysis of the data will be done at [next chapter](#eda)."},{"metadata":{"_uuid":"9bd45e299841a152726fdbcf793dac983ca234d7"},"cell_type":"markdown","source":"## Ranking Criteria <a class=\"anchor\" id=\"ranking\"></a>\nAs Shown in [evalution tab](https://www.kaggle.com/c/petfinder-adoption-prediction#evaluation),  the result will be scored based on the quadratic weighted kappa and highest score ranking higher. The implementation is as below:"},{"metadata":{"trusted":true,"_uuid":"3638460bd0a653d647f57e87ad343ab9a61491d1"},"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import make_scorer\ndef kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fb3d52aced9950d7f2efeb0381cdce07a5aa915"},"cell_type":"markdown","source":"## Output <a class=\"anchor\" id=\"output\"></a>\nOutput will be submission.csv, should include these 2 columns: \n\n*PetID, AdoptionSpeed*\n"},{"metadata":{"_uuid":"70e08a9807af846c598169d88dd845eb265304b5"},"cell_type":"markdown","source":"# Exploratory Data Analysis <a class=\"anchor\" id=\"eda\"></a>\n\nFrom this chapter you can get some deeper insight about the data. From the data introduction we can know that the data is actually very easy to combine and transform. Train and Test is the main data,  breeds, colors, states files are only id to name mapping files, they don't contain extra information. The extra information is on description and images of pets, but PetFinder.my had already convert them into sentiment data, which will be easy to integrate with the main data file. Let's dive deep down to the main data.\n\n## Data loading<a class=\"anchor\" id=\"loading\"></a>\nFirst, Let's load Tabular data first\n"},{"metadata":{"trusted":true,"_uuid":"ca264bb460100239fbeac3d98ac32c98482927b4"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nbreeds = pd.read_csv('../input/breed_labels.csv')\ncolors = pd.read_csv('../input/color_labels.csv')\ntrain = pd.read_csv('../input/train/train.csv')\ntest = pd.read_csv('../input/test/test.csv')\nsub = pd.read_csv('../input/test/sample_submission.csv')\nstates = pd.read_csv('../input/state_labels.csv')\n\n# train['dataset_type'] = 'train'\n# test['dataset_type'] = 'test'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbe2f506e6f0b22d4b934a828d3e3fe464beb786"},"cell_type":"markdown","source":"## Main Data Exploration<a class=\"anchor\" id=\"mainEDA\"></a>"},{"metadata":{"trusted":true,"_uuid":"7dfe9170efa8226f15cd7e10ed20fb4d8e82dced"},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70a4c823a03806827bf592e250d89a7c13181e21"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6902e871596e62d0bdf59a36ad4b192d0d6caec"},"cell_type":"markdown","source":"### AdoptionSpeed <a class=\"anchor\" id=\"adoptionspeed\"></a>\nFirst of all, let's see target distribution, it is always the first thing to look into when you get data, the purposes are:\n* Get to know the amount of the each classes\n* Identify the skewness of the classes, do balancing on training data if necessary"},{"metadata":{"trusted":true,"_uuid":"4ea04543a83ec45321afdd047ef452f658ac569c"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abe37093c33250daf525ead7f79cf47b8a365808"},"cell_type":"code","source":"train['AdoptionSpeed'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99fb3b7e6255c59c1273e0dae80171e9c2802fac"},"cell_type":"code","source":"train['AdoptionSpeed'].value_counts().sort_index().plot('barh')\nplt.title('Adoption speed classes comparison');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f6f83943200c2c7e8e329a2599df77732184c38"},"cell_type":"markdown","source":"we can see that class 0 has much fewer amount than the others. In another word, only a small percentage of pets were adopted within 7 days(That's the reason why they held this the competition). **Balancing technique need to be taken when modelling**\n\n### Type <a class=\"anchor\" id=\"type\"></a>\n\nWhat's the second column you want to inspect? In my opinion, it should be 'Type'\n"},{"metadata":{"trusted":true,"_uuid":"a8b294b9adbdc7841e40e42123fbd49fa05745b8"},"cell_type":"code","source":"train['Type'].value_counts().sort_index().plot('barh')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"130cb8e83a015e143b5b9b97f23e1b03d5288e0e"},"cell_type":"markdown","source":"where (1 = Dog, 2 = Cat), as explained in the [Data Fields explanation](https://www.kaggle.com/c/petfinder-adoption-prediction/data). We can see that they have  similar amount\n\n"},{"metadata":{"_uuid":"86b1e3c2caf2bae46b7af6d02b10eba7728a0cbf"},"cell_type":"markdown","source":"### Age <a class=\"anchor\" id=\"age\"></a>\nAge may be an important factor on the adoptablity of the pet, let's dive deep into it\n"},{"metadata":{"trusted":true,"_uuid":"0d0e763f6041e16e413327e2892eef8ad9ad97fc"},"cell_type":"code","source":"train['Age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ab0406057f77b70e1292d81cc324f9b9f062c53"},"cell_type":"markdown","source":"75% are under 12 months. But for better understanding, let's cap at 60 months and see their distribution"},{"metadata":{"trusted":true,"_uuid":"6d2fa130bb04aa225989b9f9b0295c0a96d45c7d"},"cell_type":"code","source":"plt.hist(train['Age'],bins=list(range(0,60,1)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4519c73da4829c19cf38bb8293cb55cef8a086e"},"cell_type":"markdown","source":"We can see that the age of the pet are mostly below 20 months.\n\nOne interesting finding is that people prefer to input YEAR rather than MONTH when filling the pet's age, so there are peaks at every 12 months\n"},{"metadata":{"_uuid":"cabf80f02944ed6de231563202b4d16cae0cfefa"},"cell_type":"markdown","source":"### Gender <a class=\"anchor\" id=\"gender\"></a>\n"},{"metadata":{"trusted":true,"_uuid":"9c3b930c73255a20213070f391c6e38be9a8cb1a"},"cell_type":"code","source":"# Gender distribution\ntrain['Gender'].value_counts().rename({1:'Male',2:'Female', 3:'Mixed (Group of pets)'}).plot(kind='barh')\n# plt.yticks(fontsize='xx-large')\nplt.title('Gender distribution', fontsize='xx-large')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf3b82ac831386ae74b5a985b4f8117d1f88bbfd"},"cell_type":"markdown","source":"### State <a class=\"anchor\" id=\"state\"></a>\nRefer to the states(or city) in Malaysia. Pet's in larger city may have more chances to be adopted"},{"metadata":{"trusted":true,"_uuid":"485c4d7751f014038ceb2cdb457841836057fe7d"},"cell_type":"code","source":"states","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65f94e1a55abfd5104253b2e69d5f7d06de1fcf7"},"cell_type":"code","source":"states_to_ID = states.set_index('StateName')\nstate_value_counts = train['State'].value_counts(ascending=False)\nstate_distribution = states_to_ID['StateID'].map(state_value_counts).sort_values(ascending=False)\nstate_distribution\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6412ce548acd5412e595fa342549fecce914242"},"cell_type":"markdown","source":"\nAs I know, Kuala Lumpur is a city of the state of Selangor. So  we may need to convert cities into states"},{"metadata":{"trusted":true,"_uuid":"163f4fa8549f1cebded6d9b528b8c918c07952df"},"cell_type":"code","source":"train['State'] = train['State'].replace(41401, 41326)# convert Kuala Lumpur to Selangor ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0cb57cecec012618bed5242d72ee289ec4d6b71"},"cell_type":"markdown","source":"There is no record for Perlis, so the value above is NaN\n\n"},{"metadata":{"trusted":true,"_uuid":"b83c1345389231fc563f87709f6accda6a35aaf3"},"cell_type":"markdown","source":"### Photo amount and video amount distribution <a class=\"anchor\" id=\"amount\"></a>"},{"metadata":{"trusted":true,"_uuid":"cf3f3d85402bdf4596ddd4942cf735011b204637"},"cell_type":"code","source":"train['PhotoAmt'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"333d78264ce0a22774db2e42bb941c9b392adcdb"},"cell_type":"markdown","source":"we can see maximum 30 photos are uploaded for a pet.  Let's plot all possible photo numbers"},{"metadata":{"trusted":true,"_uuid":"239754db129fb46e8ec47bf736e2eb1b37e739b9"},"cell_type":"code","source":"train['PhotoAmt'].plot(kind='hist', \n                          bins=30, \n                          xticks=list(range(31)))\nplt.title('Photo Amount distribution')\nplt.xlabel('Photos')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5cee04d832a5b3ea41fdbbcf669c19a2cab852f"},"cell_type":"code","source":"train['VideoAmt'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11c9371a6063797fe0f581e21357aa5d193ee43e"},"cell_type":"code","source":"train['VideoAmt'].plot(kind='hist', \n                          bins=8, \n                          xticks=list(range(9)))\nplt.title('Video Amount distribution')\nplt.xlabel('Video')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc1c007e44cd5416706b34e00f2ca356d0fa5e1b"},"cell_type":"markdown","source":"## Description Length"},{"metadata":{"trusted":true,"_uuid":"2f9a6a503afe026737370b691ba1a97b42f1668e"},"cell_type":"code","source":"train['Description'] = train['Description'].fillna('')\ntest['Description'] = test['Description'].fillna('')\ntrain['desc_len'] = train['Description'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a22f62d137fc2454fce9e0fbeab45790c108f16"},"cell_type":"code","source":"train['desc_len'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"9225d795d8a3b4120b65b2460d8671308138863f"},"cell_type":"code","source":"test['desc_len'] = test['Description'].apply(lambda x: len(x))\ntest['desc_len'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a406e0291bc3cf3c38219275bdca97a64dad19e2"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"be564e4893ed0e7937ca4892f9d77f4acfc995a8"},"cell_type":"markdown","source":"we can see most people uploaded only 1 video of the pet"},{"metadata":{"trusted":true,"_uuid":"6b61815616fc54b3fd9c03af950a64fcec9d3ad5"},"cell_type":"markdown","source":"# Data Cleaning <a class=\"anchor\" id=\"clean\"></a>\nwe need to drop these columns\n* 'AdoptionSpeed'. It had been used as target\n* 'Name', 'RescuerID', 'PetID'. they won't be helpful from basic understanding.\n* 'Description'. it had been transformed into sentiments\n"},{"metadata":{"trusted":true,"_uuid":"1378703a52a2d84027fd97a439852e247d672384"},"cell_type":"code","source":"# Clean up DataFrames\n# Will try to implement these into the model later\ntarget_train = train['AdoptionSpeed']\ncleaned_train = train.drop(columns=['Name', 'RescuerID', 'Description', 'PetID', 'AdoptionSpeed'])\ntest_pet_ID = test['PetID']\ntest_X = test.drop(columns=['Name', 'RescuerID', 'Description', 'PetID'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e196142ec72c9da5ff535a7a58460199c8e6961f"},"cell_type":"code","source":"cleaned_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a2d81b9b8db390d511e0ccc5155da53a3813a4d"},"cell_type":"markdown","source":"next we need to check if there are null values inside the traing and testing dataframe"},{"metadata":{"trusted":true,"_uuid":"397d0b471c5ee66fd4f752db24068c07d1c96d08"},"cell_type":"code","source":"target_train.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06d6f61fc96b870823a6ad4d26008b3841edffd2"},"cell_type":"code","source":"test_X.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa2797a1e4cbea5555d9a957daa270eedbbcef54"},"cell_type":"markdown","source":"The data is really clean! Isn't it? it saved our valuable time. Thanks PetFinder.my for cleaning for us"},{"metadata":{"_uuid":"59d0153c496401da4f09443c22f54ca9f27cc042"},"cell_type":"markdown","source":"# Modelling <a class=\"anchor\" id=\"model0\"></a>\n\ntraining.csv already contain some features. the other feaures are from images and description of the pet. In order to test the importance of the features from mages and description, I will do modelling in mulitiple steps.\n\n## Model Selection <a class=\"anchor\" id=\"selection\"></a>\nFrom Model, here I will select Random Forest and XGBoost to ensemble. Random Forest is to **reduce variance** and XGBoost is to **reduce bias**. Together they can get better result. "},{"metadata":{"trusted":true,"_uuid":"a6cb52366c1307f3dccb53b8b3b9e4f43a39eff6"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\n\nseed = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e5b38f232ff2e8d8d28a85feca630c3730d7495"},"cell_type":"code","source":"class EnsembleModel:\n    \n    def __init__(self,balancing=False):\n        self.balance_ratio = 5 if balancing else 1\n        self.rf_model = RandomForestClassifier()\n        self.lgb_model = lgb.LGBMClassifier()\n        self.rand_forest_params= {\n            'bootstrap': [True, False],\n            'max_depth': [20,30],\n            'min_samples_leaf': [20, 30],\n            'min_samples_split': [8,10],\n            'n_estimators': [200,250],\n            'random_state' : [seed]\n        }\n        self.lgb_params = {'objective' : ['multi:softprob'],\n              'eta' : [0.01],\n              'max_depth' : [6,7],\n              'num_class' : [5],\n              'num_leaves':[40,50],\n              'lambda' : [0.75],\n              'reg_alpha':[1e-5, 1e-2],\n              'silent': [1]\n        }\n        self.svm = SVC()\n        self.svm_params = {'kernel':['linear'],\n                           'C':[0.5,0.75],\n                           'gamma': ['auto'],\n                           'decision_function_shape':['ovo','ovr'],\n                           #'shrinking':[True,False]\n                          }\n\n        self.rf_best_param = None\n        self.lgb_best_param = None\n        self.svm_best_param = None\n        self.columns = None\n        \n    \n    def set_scorer(self,kappa):\n        self.kappa = kappa\n        self.scorer = make_scorer(kappa)\n        \n    def set_param(self,rf_param,lgb_param,svm_param):\n        self.rf_best_param = rf_param\n        self.lgb_best_param = lgb_param\n        self.svm_best_param = svm_param\n    \n    def tune_best_param(self,x_train,y_train):\n        weights_train = [self.balance_ratio if i==0 else 1 for i in y_train.tolist()]\n        \n        svm_gridsearch = GridSearchCV(self.svm, self.svm_params,\n                                      cv=3,\n                                      scoring=self.scorer,verbose=1, \n                                      refit=True\n                                     )\n        svm_gridsearch.fit(x_train, y_train, sample_weight = weights_train)\n        self.svm = svm_gridsearch.best_estimator_\n        self.svm_best_param = svm_gridsearch.best_params_\n        print('tuning for svm finished')\n        \n        rf_gridsearch = GridSearchCV(estimator = self.rf_model, \n                                      param_grid = self.rand_forest_params, \n                                      cv = 5, \n                                      n_jobs = -1, \n                                      verbose = 1, \n                                      scoring=self.scorer)\n        rf_gridsearch.fit(x_train, y_train, sample_weight = weights_train)\n        print('tuning for rf finished')\n        self.rf_model = rf_gridsearch.best_estimator_\n        self.rf_best_param = rf_gridsearch.best_params_\n        \n        lgb_gridsearch = GridSearchCV(self.lgb_model, self.lgb_params, n_jobs=-1, \n                   cv=5, \n                   scoring=self.scorer,\n                   verbose=1, refit=True)\n        lgb_gridsearch.fit(x_train, y_train, sample_weight = weights_train)\n        print('tuning for lgb finished')\n        self.lgb_model = lgb_gridsearch.best_estimator_\n        self.lgb_best_param = lgb_gridsearch.best_params_\n        \n        \n        \n        print('best param for rf is:')\n        print(self.rf_best_param)\n        print('best param for lgb is:')\n        print(self.lgb_best_param)\n        print('best param for svm is:')\n        print(self.svm_best_param)\n    \n    # let's try combining the 3 models together by averging\n    def _avg(self,y_1,y_2,y_3):\n        return np.rint((y_1 + y_2)/2.0).astype(int)\n\n    def re_fit_with_best_param(self,X,y):\n        if self.rf_best_param == None or self.lgb_best_param == None or self.svm_best_param == None: \n            print('use tune_best_param() method to get best param first')\n            return\n        weights_train = [self.balance_ratio if i==0 else 1 for i in y.tolist()]\n        self.rf_model = RandomForestClassifier()\n        self.lgb_model =  lgb.LGBMClassifier()\n        self.svm = SVC()\n        self.rf_model.set_params(**self.rf_best_param)\n        self.lgb_model.set_params(**self.lgb_best_param)\n        self.svm.set_params(**self.svm_best_param)\n        self.rf_model.fit(X,y,sample_weight=weights_train)\n        self.lgb_model.fit(X,y,sample_weight=weights_train)\n        self.svm.fit(X,y,sample_weight=weights_train)\n        print('refit finished')\n    \n    def validate(self,x_valid, y_valid):\n        rf_score = self.kappa(self.rf_model.predict(x_valid), y_valid)\n        print('{} score: {}'.format('rf', round(rf_score, 4)))\n        lgb_score = self.kappa(self.lgb_model.predict(x_valid), y_valid)\n        print('{} score: {}'.format('lgb', round(lgb_score, 4)))\n        svm_score = self.kappa(self.svm.predict(x_valid), y_valid)\n        print('{} score: {}'.format('svm', round(svm_score, 4)))\n        score = kappa(self._avg(self.lgb_model.predict(x_valid), self.rf_model.predict(x_valid), self.svm.predict(x_valid)) , y_valid)\n        print('{} score on validation set: {}'.format('combiner', round(score, 4)))\n        self.columns = x_valid.columns\n\n    def predict(self,test_X):\n        rf_result = self.rf_model.predict(test_X)\n        lgb_result = self.lgb_model.predict(test_X)\n        svm_result = self.svm.predict(test_X)\n        final_result = self._avg(rf_result,lgb_result,svm_result)\n        return final_result\n\n\n    def get_feature_importance(self):\n        rf_feature_importances = pd.DataFrame({'Feature':self.columns.tolist(),'importance':self.rf_model.feature_importances_.tolist()})\n        lgb_feature_importances = pd.DataFrame({'Feature':self.columns.tolist(),'importance':self.lgb_model.feature_importances_.tolist()})\n        svm_feature_importances = pd.DataFrame({'Feature':self.columns.tolist(),'importance':self.svm.coef_.tolist()})\n        overall_feature_importance = pd.merge(rf_feature_importances, lgb_feature_importances, svm_feature_importances, on='Feature', how='outer')\n        overall_feature_importance['avg_importance'] = (overall_feature_importance['importance_x'] + overall_feature_importance['importance_y']+ overall_feature_importance['importance_z'])/3\n        overall_feature_importance = overall_feature_importance.sort_values(by=['avg_importance'], ascending=False)\n        return overall_feature_importance\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"265fd5b635419ff16e3d51daa9ea441da191bcd7"},"cell_type":"markdown","source":"\n\n## Model from original train dataset <a class=\"anchor\" id=\"model1\"></a>\n\n### Data Cleaning <a class=\"anchor\" id=\"clean\"></a>\nwe need to drop these columns\n* 'AdoptionSpeed'. It had been used as target\n* 'Name', 'RescuerID', 'PetID'. they won't be helpful from basic understanding.\n* 'Description'. it had been transformed into sentiments\n"},{"metadata":{"trusted":true,"_uuid":"5b865d90ea45eb9ab53bd5c40d4710693c29c3cd"},"cell_type":"code","source":"# Clean up DataFrames\n# Will try to implement these into the model later\ntarget_train = train['AdoptionSpeed']\ncleaned_train = train.drop(columns=['Name', 'RescuerID', 'Description', 'PetID', 'AdoptionSpeed'])\ntest_pet_ID = test['PetID']\ntest_X = test.drop(columns=['Name', 'RescuerID', 'Description', 'PetID'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb46f4d092267e5b0af1eccdb7421044425d38ad"},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(cleaned_train, \n                                                      target_train, \n                                                      test_size=0.2, \n                                                      random_state=seed)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b69a2a620a7a1bcb00d5aed2d4178e7a655fd61b"},"cell_type":"markdown","source":"### Model Tuning\n\nFrom Version 2.0, we make the model ensembling as a class so that it will be easier to replicate."},{"metadata":{"_uuid":"b92a98d54d7eb606d71d6e293131a52e73ec6849","trusted":true},"cell_type":"code","source":"if MODEL_USE == 1 or MODEL_USE==0:\n    first_model = EnsembleModel(balancing=True)\n    first_model.set_scorer(kappa)\n    first_model.tune_best_param(x_train, y_train)\n    first_model.validate(x_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2024069997ff86dc3aaf880bd4e8672e91805a96"},"cell_type":"markdown","source":"## Model with description sentiments <a class=\"anchor\" id=\"model2\"></a>\nLet's examinate how a description sentiment file looks like:"},{"metadata":{"trusted":true,"_uuid":"430c24334ad650892636336a76d6d8dc031abd3c"},"cell_type":"code","source":"import json\nfilename = os.listdir(\"../input/train_sentiment\")[1]\nfilename = \"../input/train_sentiment/\"+filename\nwith open(filename, 'r') as f:\n    sentiment = json.load(f)\nsentiment  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6aed665c0bc25116e5bbc865c8df158d970fed3d"},"cell_type":"markdown","source":"Entities in the json are too complex to use, but we can see that *documentSentiment': {'magnitude': 0.8, 'score': 0.4}* is easy to use. File name format is PetID.json, Let's use these 2 variables"},{"metadata":{"trusted":true,"_uuid":"8d3ff46ecdc63f950c1a3f613f3e321ffeebead1"},"cell_type":"code","source":"def load_desc_sentiment(path):\n    all_desc_sentiment_files = os.listdir(path)\n    count_file = len(all_desc_sentiment_files)\n    desc_sentiment_df = pd.DataFrame(columns=['PetID','desc_senti_magnitude','desc_senti_score'])\n    current_file_index = 1\n    for filename in all_desc_sentiment_files:\n        with open(path+filename, 'r') as f:\n            sentiment_json = json.load(f)\n            petID = filename.split('.')[0]\n            magnitude = sentiment_json['documentSentiment']['magnitude']\n            score = sentiment_json['documentSentiment']['score']\n            desc_sentiment_df = desc_sentiment_df.append({'PetID': petID, 'desc_senti_magnitude':magnitude,'desc_senti_score':score}, \\\n                                                         ignore_index=True)\n            if current_file_index % 1000 == 0 or current_file_index == count_file :\n                print('current progress: %d file of %d loaded' %(current_file_index,count_file))\n            current_file_index += 1\n    return desc_sentiment_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d96b1d8f2599c8c841bb64dc3c6aa836f582e476"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD,PCA\ntfv = TfidfVectorizer(min_df=2,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        )\ntfv.fit(train['Description'])\ndesc_X_train =  tfv.transform(train['Description'])\ndesc_X_test = tfv.transform(test['Description'])\nprint(desc_X_train.shape)\nprint(desc_X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa53966cb6f7fbfcde21866642f26e4733819611"},"cell_type":"code","source":"svd = TruncatedSVD(n_components=5)\nsvd.fit(desc_X_train)\n# print(svd.explained_variance_ratio_.sum())\n# print(svd.explained_variance_ratio_)\ndesc_X_train = svd.transform(desc_X_train)\ndesc_X_test = svd.transform(desc_X_test)\nprint(\"desc_X_train (svd):\", desc_X_train.shape)\nprint(\"desc_X_test (svd):\", desc_X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5399611be1d293ad8c337270fa55e05e5db1308"},"cell_type":"code","source":"train_desc_sentiment_df = load_desc_sentiment(\"../input/train_sentiment/\")\ntest_desc_sentiment_df = load_desc_sentiment(\"../input/test_sentiment/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd4bd9603a13252defbbc73f77e2344449b5885e"},"cell_type":"code","source":"# train_desc_sentiment_df['score_times_mag'] = train_desc_sentiment_df['desc_senti_magnitude'] * train_desc_sentiment_df['desc_senti_score']\n# test_desc_sentiment_df['score_times_mag'] = test_desc_sentiment_df['desc_senti_magnitude'] * test_desc_sentiment_df['desc_senti_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5cf7261c0b2619889aa3e1c6cf8ef87054a537ba"},"cell_type":"code","source":"train_desc_sentiment_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c28490b4c4ab7a4024b64b27d4298fac23d4c94"},"cell_type":"code","source":"desc_X_train = pd.DataFrame(desc_X_train, columns=['desc_{}'.format(i) for i in range(svd.n_components)])\ndesc_X_test = pd.DataFrame(desc_X_test, columns=['desc_{}'.format(i) for i in range(svd.n_components)])\ntrain_with_desc = pd.concat([train,desc_X_train],axis=1)\ntest_with_desc = pd.concat([test,desc_X_test],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"019d4f07a2a14fad8d8d6a7b3689c853806fc2a9"},"cell_type":"code","source":"target_train = train_with_desc['AdoptionSpeed']\njoint_train = train_with_desc.merge(train_desc_sentiment_df, how='left',left_on=['PetID'],right_on=['PetID'])\ncleaned_train = joint_train.drop(columns=['Name', 'RescuerID', 'Description', 'PetID', 'AdoptionSpeed'])\ncleaned_train.fillna(0.0,inplace=True)\n\ntest_pet_ID = test_with_desc['PetID']\njoint_test = test_with_desc.merge(test_desc_sentiment_df, how='left',left_on=['PetID'],right_on=['PetID'])\ntest_X = joint_test.drop(columns=['Name', 'RescuerID', 'Description', 'PetID'])\ntest_X.fillna(0.0, inplace=True)\n\nx_train, x_valid, y_train, y_valid = train_test_split(cleaned_train, \n                                                      target_train, \n                                                      test_size=0.2, \n                                                      random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5df40f332c64573bb324f13a7e49865c9699b4a"},"cell_type":"code","source":"if MODEL_USE == 2 or MODEL_USE==0:\n    second_model = EnsembleModel(balancing=True)\n    second_model.set_scorer(kappa)\n    second_model.tune_best_param(x_train, y_train)\n    second_model.validate(x_valid,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4231435e272728c7ed53cadfc3dbcbb300165153"},"cell_type":"markdown","source":"## Model with images features and above<a class=\"anchor\" id=\"model3\"></a>\nTo be implemented in version 3.0\n"},{"metadata":{"trusted":true,"_uuid":"206ecc3abeebf1ccd6ed2abdc32699b491ec5b92"},"cell_type":"code","source":"def add_meta_feature(path,df):\n    vertex_xs = []\n    vertex_ys = []\n    bounding_confidences = []\n    bounding_importance_fracs = []\n    dominant_blues = []\n    dominant_greens = []\n    dominant_reds = []\n    dominant_pixel_fracs = []\n    dominant_scores = []\n    label_descriptions = []\n    label_scores = []\n    nf_count = 0\n    nl_count = 0\n    pet_id = df['PetID']\n    for pet in pet_id:\n        try:\n            with open(path + pet + '-1.json', 'r') as f:\n                data = json.load(f)\n            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n            vertex_xs.append(vertex_x)\n            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n            vertex_ys.append(vertex_y)\n            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n            bounding_confidences.append(bounding_confidence)\n            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n            bounding_importance_fracs.append(bounding_importance_frac)\n            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n            dominant_blues.append(dominant_blue)\n            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n            dominant_greens.append(dominant_green)\n            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n            dominant_reds.append(dominant_red)\n            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n            dominant_pixel_fracs.append(dominant_pixel_frac)\n            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n            dominant_scores.append(dominant_score)\n            if data.get('labelAnnotations'):\n                label_description = data['labelAnnotations'][0]['description']\n                label_descriptions.append(label_description)\n                label_score = data['labelAnnotations'][0]['score']\n                label_scores.append(label_score)\n            else:\n                nl_count += 1\n                label_descriptions.append('nothing')\n                label_scores.append(-1)\n        except FileNotFoundError:\n            nf_count += 1\n            vertex_xs.append(-1)\n            vertex_ys.append(-1)\n            bounding_confidences.append(-1)\n            bounding_importance_fracs.append(-1)\n            dominant_blues.append(-1)\n            dominant_greens.append(-1)\n            dominant_reds.append(-1)\n            dominant_pixel_fracs.append(-1)\n            dominant_scores.append(-1)\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    print(nf_count)\n    print(nl_count)\n    df.loc[:, 'vertex_x'] = vertex_xs\n    df.loc[:, 'vertex_y'] = vertex_ys\n    df.loc[:, 'bounding_confidence'] = bounding_confidences\n    df.loc[:, 'bounding_importance'] = bounding_importance_fracs\n    df.loc[:, 'dominant_blue'] = dominant_blues\n    df.loc[:, 'dominant_green'] = dominant_greens\n    df.loc[:, 'dominant_red'] = dominant_reds\n    df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n    df.loc[:, 'dominant_score'] = dominant_scores\n    df.loc[:, 'label_description'] = label_descriptions\n    df.loc[:, 'label_score'] = label_scores\n#     df = df.drop(['label_description'])\n    return df\n\n\n\nif MODEL_USE == 3 or MODEL_USE==0:\n    train_with_meta = add_meta_feature('../input/train_metadata/', train_with_desc)\n    target_train = train_with_meta['AdoptionSpeed']\n    cleaned_train = train_with_meta.drop(columns=['Name', 'RescuerID', 'Description', 'PetID', 'AdoptionSpeed', 'label_description'])\n    cleaned_train.fillna(0.0,inplace=True)\n    \n    test_with_meta = add_meta_feature('../input/test_metadata/', test_with_desc)\n    test_pet_ID = test_with_desc['PetID']\n    test_X = test_with_meta.drop(columns=['Name', 'RescuerID', 'Description', 'PetID', 'label_description'])\n    test_X.fillna(0.0, inplace=True)\n\n    x_train, x_valid, y_train, y_valid = train_test_split(cleaned_train, \n                                                          target_train, \n                                                          test_size=0.2, \n                                                          random_state=seed)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9af6435cd361748a1de887d2cabe246035cb9004"},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35002da8a403ba9fe195659f240658ee72ea3738"},"cell_type":"code","source":"# Metadata:\n# train_df_ids = train[['PetID']]\n# train_df_metadata = pd.DataFrame(train_metadata_files)\n# train_df_metadata.columns = ['metadata_filename']\n# train_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n# train_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\n# print(len(train_metadata_pets.unique()))\n\n# pets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\n# print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / train_df_ids.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d414a7d0ed56e90a500fac2e6bd531c1cbf40f3d"},"cell_type":"code","source":"if MODEL_USE == 3 or MODEL_USE==0:\n    third_model = EnsembleModel(balancing=True)\n    third_model.set_scorer(kappa)\n    third_model.tune_best_param(x_train, y_train)\n    third_model.validate(x_valid,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6f352f1d81fdfb36ae4e13322add1635abc7a6b"},"cell_type":"markdown","source":"Marvelous! Combiner model do give us better result! on validation set\nAcutually you can play with more models and tune the best params to score the top leaderboard"},{"metadata":{"_uuid":"4e2836c0d575948daa6b5caeaf68af4e683f53da"},"cell_type":"markdown","source":"# Feature Importance and Conclusion <a class=\"anchor\" id=\"importance\"></a>\n\nLet's analyze the feature importance "},{"metadata":{"trusted":true,"_uuid":"375026274801a05da169ab84a95d42acc0629ac9"},"cell_type":"code","source":"model = None\nif MODEL_USE == 1:\n    model = first_model\nif MODEL_USE == 2: \n    model = second_model\nif MODEL_USE == 0 or MODEL_USE == 3: # if all 3 model is enabled, we just use the 3rd model\n    model = third_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"268c9b5f0138b1f42a09b4af9467d4025ccfa472"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"a0d3c35ae456ff5114322ea96d4ddd4acf3a2bae"},"cell_type":"code","source":"# overall_feature_importance = model.get_feature_importance()\n# overall_feature_importance.head(5)\n# overall_feature_importance.drop(['importance_x','importance_y'],axis=1).set_index('Feature').plot(kind='bar')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f0b24fb50a641759cbf84b78c4080adc74695bd"},"cell_type":"markdown","source":" The model are trained on 80% of the training set (20% was left out for validation. now we will use the best param obtained in previous step to train on the full dataset"},{"metadata":{"trusted":true,"_uuid":"4322477bef7f6dc89a095970362d8e20b29c7d67"},"cell_type":"code","source":"model.re_fit_with_best_param(cleaned_train,target_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4c0e7b512f2d636c884520c3aee33fafe328451"},"cell_type":"markdown","source":"# Result and Submission <a class=\"anchor\" id=\"result\"></a>"},{"metadata":{"trusted":true,"_uuid":"979bf317e00154b4219afcb42ea9c82ae0d9a8aa"},"cell_type":"code","source":"final_result = model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"754423a0e4fd7ad1bd62f5d78e79ce4ee3cf99df"},"cell_type":"code","source":"submission_df = pd.DataFrame(data={'PetID' : test_pet_ID.tolist(), \n                                   'AdoptionSpeed' : final_result})\nsubmission_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fb21c39be72bebdab4bdd94f6691e03e4912e50"},"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88fbd6ca4f0ae0ef8aa28e1b83fd1c1cae4ffd89"},"cell_type":"code","source":"len(model.svm.coef_.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cb73c7a6658712284736ab17d8d46f29063be75"},"cell_type":"code","source":"model.lgb_model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a23ba0b7adae094a9114830887145b0c2fdd065d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}