{"cells":[{"metadata":{"_uuid":"182dce896e280e8525a752ced6249dbb4411f69c","_cell_guid":"847ca56a-29e7-4571-9960-fa6a70038213"},"cell_type":"markdown","source":"# **Plant Seedlings Classification CNN**\n \n **By: Neil Shah**\n\n\n## **Introduction**<br>\nThis is an 8-layer convolutional neural network model for recognizing 12 different plant seedlings from images at various stages of growth. I used Keras, a high-level neural networks library, with the TensorFlow backend. I achieved 92.695% accuracy in 5 hours of training for 75 epochs. I trained my CNN on Jupyter Notebook and loaded the model by creating a new dataset. I am still making changes to my model to improve the accuracy. The notebook has 3 main parts:\n* Data  Preparation\n* CNN model and evaluation\n* Predictions and Submission\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport cv2\nimport os\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11fee9b79b2d9aea38e25db8a047c93f53a705e2","_cell_guid":"5dd0c0ea-b782-452b-b65f-64bd6491222c"},"cell_type":"markdown","source":"## **Data Preparation**\n### **Load the data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen',\n          'Loose Silky-bent', 'Maize','Scentless Mayweed', 'Shepherds Purse',\n          'Small-flowered Cranesbill', 'Sugar beet']\ndata_dir = '../input/plant-seedlings-classification/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e6da99e10a61bbdd5151bbe76b444263343391b","_cell_guid":"0bb76c73-a77b-42b3-b5ec-c8f47f30c875","trusted":false,"collapsed":true},"cell_type":"code","source":"# Organize training files into DataFrame\ntrain_data = []\nfor species_id, sp in enumerate(species):\n    for file in os.listdir(os.path.join(train_dir, sp)):\n        train_data.append(['train/{}/{}'.format(sp, file), species_id, sp])\n        \ntrain = pd.DataFrame(train_data, columns=['File', 'SpeciesId','Species'])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b58ad4b8a8c3f09577d41f68022486456edf7a2c","_cell_guid":"29c282ca-f26c-4964-ab19-62f6cd8b3614","trusted":false,"collapsed":true},"cell_type":"code","source":"# Randomize the order of training set\nSEED = 42\ntrain = train.sample(frac=1, random_state=SEED) \ntrain.index = np.arange(len(train)) # Reset indices\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37a488464f0a6c9cc23fda802df2900b9d412d03","_cell_guid":"b153d157-ed76-4508-9c2b-4fd07ca58a8c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Plot a histogram\nplt.hist(train['SpeciesId'])\nplt.title('Frequency Histogram of Species')\nplt.figure(figsize=(12, 12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c6984bf16aeb97c2ca68470aa34c9b4b9d9ec4a","_cell_guid":"5b470ac8-edaf-480b-a4ad-9e2c2ea100ff"},"cell_type":"markdown","source":"The histogram shows that there is a large range in the number of training samples for each of the species. If we decided to stratify the training set by evenly distributing the number of samples per species (~ 200), then we would be omitting a large amount of potentially useful training data. For that reason, I won't perform stratification, but it's something to consider for future improvements. "},{"metadata":{"_uuid":"8becd8e2045f322ca25d7623ff19e29b05f542b8","_cell_guid":"245c3e7b-129b-4358-91a8-0046321701bf","trusted":false,"collapsed":true},"cell_type":"code","source":"# Organize test files into DataFrame\ntest_data = []\nfor file in os.listdir(test_dir):\n    test_data.append(['test/{}'.format(file), file])\ntest = pd.DataFrame(test_data, columns=['Filepath', 'File'])\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c163cfa5d8345047077b29dbb37756290bab677","_cell_guid":"254ca8d8-e02a-44b9-ac65-ba4e7bba9990"},"cell_type":"markdown","source":"### **Plot training images**"},{"metadata":{"_uuid":"a091d023a8292d5c1e07c4033f07dfa8855fb6b4","_cell_guid":"77923d06-f45b-434f-a200-519ded3f2fa8","trusted":false,"collapsed":true},"cell_type":"code","source":"# Display images for different species\ndef plot_species(species, rows, cols):\n    fig, ax = plt.subplots(rows, cols, figsize=(12, 12))\n    species_files = train['File'][train['Species'] == species].values\n    n = 0\n    for i in range(rows):\n        for j in range(cols):\n            image_path = os.path.join(data_dir, species_files[n])\n            ax[i, j].set_xticks([])\n            ax[i, j].set_yticks([])\n            ax[i, j].imshow(cv2.imread(image_path))\n            n += 1\n# Displays first n images of class from training set\nplot_species('Black-grass', 5, 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bff87148341e1d01468e6bc46f9614b777e2679","_cell_guid":"97a84260-d1bb-4f53-b326-c00d9a402b07"},"cell_type":"markdown","source":"### **Image preprocessing**\nI must extend credit to Gábor Vecsei's [Plant Seedlings Fun with Computer Vision](https://www.kaggle.com/gaborvecsei/plant-seedlings-fun-with-computer-vision) for helping me to understand and implement image preprocessing techniques. I wrote functions for reading a BGR image and resizing the image. For resizing the image, I used the INTER_AREA interpolation method, which resamples using pixel area relation. It's the preferred interpolation method for image decimation because it provides moiré-free (non-wavy) results. \n"},{"metadata":{"_uuid":"563a0196c6c63ad9b99d88d6de141638c0ffefd6","collapsed":true,"_cell_guid":"7ed1f74f-803f-48a3-af71-8872d62d1475","trusted":false},"cell_type":"code","source":"IMAGE_SIZE = 66\n\ndef read_image(filepath):\n    return cv2.imread(os.path.join(data_dir, filepath)) # Loading a color image is the default flag\n# Resize image to target size\ndef resize_image(image, image_size):\n    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"132955c330cccf9f3688f899414fdf84a29a6bcf","_cell_guid":"0144ed8e-9b11-4c2f-b16c-41258faebf63"},"cell_type":"markdown","source":"There are many different types of image segmentation. I will use the most basic type called thresholding. It is a non-linear operation converts an image into a binary images where the two levels are assigned pixels based on whether they're above or below the specified threshold value. <br><br>\nFirst, I will convert from the BGR to the HSV color-space which will be useful for extracting green-colored objects. In HSV, the hue of a color refers to the pure color it resembles and the value represents the brightness. The saturation describes the shade of the color, such as pink and salmon representing different types of the red. \n![](http://www.nmt.edu/tcc/help/pubs/colortheory/img/cone.png)\nSource: http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html<br>\n\nAfter making the color-space conversion, we threshold the HSV image for a range of green color. Another technique we can apply is a morphological transformation. I used the closing transformation because it's useful for closing small holes in the objects. In doing this, we have to specify a structural element or kernel which slides through the image applying transformations on the specified window size. <br><br>\nFinally, I extract the green objects alone by performing a bitwise-AND operation between the mask and the original image.\n"},{"metadata":{"_uuid":"d8ba71ecfdff4e498171500562f9e907d2dd8681","collapsed":true,"_cell_guid":"9efc1017-2952-4661-937f-5a6dd1f3dfc8","trusted":false},"cell_type":"code","source":"# Image segmentation\ndef create_mask(image):\n    # Convert from BGR to HSV color-space to extract colored object\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    # Define range of green in HSV\n    lower_green = np.array([30, 100, 50])\n    upper_green = np.array([85, 255, 255])\n    # Threshold the HSV image to get only green colors\n    mask = cv2.inRange(image_hsv, lower_green, upper_green)\n    # We will use a morphological operation called closing to close small holes in the image\n    # We need a kernel or structuring element to determine the nature of the operation\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    return mask\n\ndef segment_image(image):\n    mask = create_mask(image)\n    res = cv2.bitwise_and(image, image, mask=mask) # Bitwise-AND mask and original image\n    return res","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbca3e1f8accf0faa475fa657106f9c324df28f6","_cell_guid":"deca7a09-7710-463f-a62a-6a8d3369f6a9"},"cell_type":"markdown","source":"### **Plot segmented images**<br>\nWe can show a side-by-side comparison of the original, masked, segmented, and resized images."},{"metadata":{"_uuid":"3b5158fae1a81fde87c059e0dcdce2e4baec1599","_cell_guid":"386aed5f-d065-402f-8fc7-073aa068fb47","trusted":false,"collapsed":true},"cell_type":"code","source":"def show_segmented_images(species, n):\n    fig, ax = plt.subplots(n, 4, figsize=(20, 20))\n    species_files = train['File'][train['Species'] == species].values\n    for i in range(n):\n        image = read_image(species_files[i])\n        image_masked = create_mask(image)\n        image_segmented = segment_image(image)\n        image_resized = resize_image(image_segmented, (IMAGE_SIZE, IMAGE_SIZE))\n        ax[i, 0].set_axis_off()\n        ax[i, 1].set_axis_off()\n        ax[i, 2].set_axis_off()\n        ax[i, 3].set_axis_off()\n        ax[i, 0].imshow(image)\n        ax[i, 1].imshow(image_masked)\n        ax[i, 2].imshow(image_segmented)\n        ax[i, 3].imshow(image_resized)\n# Displays first n images of class from training set\nshow_segmented_images('Maize', 3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd527fb43de42f8d94e244058c52d843bb055cd0","_cell_guid":"1d5042fa-41d1-4f2a-ac9d-3e74af2c3a8e"},"cell_type":"markdown","source":"### **Extract features**\nSince the image sizes vary, I reshaped all of the images to 66x66x3. Keras requires an extra dimension which corresponds to the channels. For RGB images, there are 3 channels. I also performed normalization to help the CNN converge faster. Note that we resize the image after segmentation to reduce the noise."},{"metadata":{"_uuid":"e136f147936f1cd42ebc2c2e29dcc0d32fb9ab4a","_cell_guid":"6ac626ec-c42a-4ef5-b12d-27c2c2a8432d","trusted":false,"collapsed":true},"cell_type":"code","source":"X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\nfor i, file in tqdm(enumerate(train['File'].values)):\n    image = read_image(file)\n    image_segmented = segment_image(image)\n    X_train[i] = resize_image(image_segmented, (IMAGE_SIZE, IMAGE_SIZE))\n# Normalize the data\nX_train = X_train / 255.\nprint('Train Shape: {}'.format(X_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6417ef603244f72bc881e034ff314a07c2a630af","_cell_guid":"a7316192-cf21-4a68-89e7-5f3bd6d77879"},"cell_type":"markdown","source":"### **Label encoding**\nWe encode the labels to one-hot vectors. The integer encoding (0-11) is removed and a new binary value is added for each integer value."},{"metadata":{"_uuid":"ac3200e347197bab5290b1ecc2338f19fff38206","collapsed":true,"_cell_guid":"30b0ef43-1571-4cdb-b4e4-20faf309b2a7","trusted":false},"cell_type":"code","source":"Y_train = train['SpeciesId'].values\nY_train = to_categorical(Y_train, num_classes=12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"191354375100f3c15030c6474b916d90e540d21b","_cell_guid":"297fb357-088d-4694-980f-02eb84bffb96"},"cell_type":"markdown","source":"### **Split training and validation set**"},{"metadata":{"_uuid":"c43e9ad52355cf4a3dcaabef7db807f11e47b3ee","collapsed":true,"_cell_guid":"11dfcc96-212c-4783-a9b0-44e269b77ee0","trusted":false},"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 75\n\n# Split the train and validation sets \nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3e403bf3c5081a191dffda31c6b8e1d6f1c0a16","_cell_guid":"c14820ec-d7c6-41ea-ae19-a3ec4637a4d7"},"cell_type":"markdown","source":"I chose to split 90% of the data into a training group and the remaining 10% into a validation group for evaluating the model's performance.<br><br><br>\n\nWe can get a better sense of the training samples by looking at some images with the known labels which are shown in the title."},{"metadata":{"_uuid":"dd1f66cff312e07b6e7a54223d5131cd1b869e6d","_cell_guid":"5a57955e-9b3f-4bdf-b44f-1a490234415a","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 4, figsize=(15, 15))\nfor i in range(4):\n    ax[i].set_axis_off()\n    ax[i].imshow(X_train[i])\n    ax[i].set_title(species[np.argmax(Y_train[i])])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af90c65b688d2842499666dd0d3b1384246b52e8","_cell_guid":"dacce5f6-9b44-4a00-86ba-24eaaa4ff677"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"527c7195b9f68f6dda7fa54e4ea2600a2add20c3","_cell_guid":"6b1328e0-e46c-4e85-ac9d-dba586bd4c42"},"cell_type":"markdown","source":"## **CNN model and evaluation**<br>\n\n### **Model Architecture**\nThe Sequential model is a linear stack of layers. The first layer in the model needs to receive information about its input shape and the following layers will do automatic shape reference. The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters.  During the forward pass, each filter is convolved across the image to produce a 2D activation map of each filter. Then stacking the activation maps for each filter forms the full output of the convolutional layer. The Batch normalization layer normalizes the activations of the convolutional layers by applying a transformation that maintains the mean close to 0 and the standard deviation close to 1.  The max pooling layer serves as a form of non-linear downsampling. In this case, the 2x2 filters compute the maximum value of four pixels and make a stride of 2 pixels (width and height) at each depth. There are a total of 6 convolutional layers and 3 max pooling layers. The flatten layer is used to convert the final feature map into a 1D vector, combining all of the features of the previous layer. In the final layer, I used softmax activation so the neural network outputs the probability distribution for each class.\n\nFor the convolutional and dense layers, I used the ReLU activation function. For training deep neural networks, ReLU is more effective than the sigmoid and tangent activation functions because it prevents gradients from saturating. The vanishing gradient problem causes the neural network to get stuck preventing meaningful learning from taking place.\n\nBefore training the model, we need to configure the learning process by specifying the optimizer, loss function, and list of metrics. The loss function measures the error rate between the model's predicted and observed labels. The categorical crossentropy loss function is computed by taking the average of all cross-entropies in the sample. It will measure the probability that the training sample belongs to an individual class. The cost function is the average of the loss function over a large number of training samples. The goal of the optimization algorithm is to minimize the cost function by iteratively updating the weights and biases. I used the Adam (short for Adaptive Moment Estimation) optimizer because it's effective and achieves good results quickly. For information about how the optimizer works, [click here](https://arxiv.org/abs/1412.6980)."},{"metadata":{"_uuid":"1cb9280f2cd8cb84a5be23f3f75844ed4331f919","collapsed":true,"_cell_guid":"38a915ba-fed4-4870-80f2-f3594e85a9a3","trusted":false},"cell_type":"code","source":"def construct_model():\n    model = Sequential()\n    \n    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n                activation='relu'))\n    model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n\n    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n\n    model.add(Flatten()) # Flatten the input\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(12, activation='softmax'))\n    # Configure the learning process\n    # The loss function is the objective that the model will try to minimize\n    # For any classification problem, use accuracy metric\n    optimizer = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d42fb0f010398cc13284025b72b46b4eb9772b9","_cell_guid":"1248cf67-7538-4da3-915e-ded27cd7b284"},"cell_type":"markdown","source":"### **Training the CNN**\n\nReduceLROnPlateau reduces the learning rate when the validation accuracy has stopped improving. Models often benefit from reducing the learning rate by a factor of 2 to 10 once learning stagnates. My annealer reduces the learning rate by 50% when the validation accuracy hasn't increased for 5 epochs. Reducing the learning rate allows the optimizer to take smaller steps to reach the global minimum, which increases the rate of convergence. \n\nThe ImageDataGenerator is a data augmentation technique that works by applying small transformations to the training samples to generate additional data. This makes the existing training dataset larger. For example, the training images may be rotated by a certain number of degrees or zoomed in by a small percentage. There can also be random horizontal and vertical shifts, ZCA whitening, mean and standard deviation normalization, etc. \n\nI trained the model for 75 epochs, although the training and validation accuracies only made miniscule improvements after 55 epochs. \n"},{"metadata":{"_uuid":"d60cf1d2fd16557b4af1a50c463570e86b31bfea","collapsed":true,"scrolled":true,"_cell_guid":"68e1ecbf-6772-4d9c-b9dc-f9509607ee23","trusted":false},"cell_type":"code","source":"def train_model():\n    model = construct_model()\n    annealer = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5, verbose=1, min_lr=1e-5)\n    checkpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n    # Generates batches of image data with data augmentation\n    datagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n                            width_shift_range=0.2, # Range for random horizontal shifts\n                            height_shift_range=0.2, # Range for random vertical shifts\n                            zoom_range=0.2, # Range for random zoom\n                            horizontal_flip=True, # Randomly flip inputs horizontally\n                            vertical_flip=True) # Randomly flip inputs vertically\n    \n    datagen.fit(X_train)\n    # Fits the model on batches with real-time data augmentation\n    hist = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n                   steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n                   epochs=EPOCHS,\n                   verbose=2,\n                   callbacks=[annealer, checkpoint],\n                   validation_data=(X_val, Y_val))\n# train_model()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acd92c55fceb75e8a79bf4acdde95771971e0507","_cell_guid":"544cbd02-df73-407f-b9c9-41f18cbec207"},"cell_type":"markdown","source":"### **Model evaluation**"},{"metadata":{"_uuid":"c2eeaeb2b0223805287204924ea7fcb85f4b2e02","_cell_guid":"04f2baf0-3d11-4342-8baa-ffd11ea661a3","trusted":false,"collapsed":true},"cell_type":"code","source":"final_model = load_model('../input/plant-seedling-models/model.h5')\nfinal_loss, final_accuracy = final_model.evaluate(X_val, Y_val)\nprint('Final Loss: {}, Final Accuracy: {}'.format(final_loss, final_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24a1fe18d19dc5cc0173462db478203abd8c29eb","_cell_guid":"d68a6840-2c16-44c5-bd95-f40f80b034db"},"cell_type":"markdown","source":"I plotted a confusion matrix. It seems as though the CNN has trouble distinguishing between  black-grass and loose silky-bent images. These species look quite similar to each other so its easy to understand why the CNN is making errors."},{"metadata":{"_uuid":"e49682a5f2f101d8d9afa30ac3bdff2b39b3ec87","_cell_guid":"6ce2f2b4-ec22-4e5e-a614-1381645300a4","trusted":false,"collapsed":true},"cell_type":"code","source":"Y_pred = final_model.predict(X_val)\n\nY_pred = np.argmax(Y_pred, axis=1)\nY_true = np.argmax(Y_val, axis=1)\n\ncm = confusion_matrix(Y_true, Y_pred)\nplt.figure(figsize=(12, 12))\nax = sns.heatmap(cm, cmap=plt.cm.Greens, annot=True, square=True, xticklabels=species, yticklabels=species)\nax.set_ylabel('Actual', fontsize=40)\nax.set_xlabel('Predicted', fontsize=40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2f4f4c42cde4040140b2a690c0f383156cd295f","_cell_guid":"2be72549-94af-4c83-b072-3c3586cf36ed"},"cell_type":"markdown","source":"## **Submit Predictions**\n<br>\nExtracts the testing features and makes predictions. The results are saved in the submission file."},{"metadata":{"_uuid":"8515565afa295022a545cf84afd1e778329b41a8","_cell_guid":"85de3462-82a1-4007-b5bf-84e1feee32df","trusted":false,"collapsed":true},"cell_type":"code","source":"X_test = np.zeros((test.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\nfor i, file in tqdm(enumerate(test['Filepath'].values)):\n    image = read_image(file)\n    image_segmented = segment_image(image)\n    X_test[i] = resize_image(image_segmented, (IMAGE_SIZE, IMAGE_SIZE))\n\nX_test = X_test / 255.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47eda25d177d577075c7046c39b97bdf828e312b","collapsed":true,"_cell_guid":"d352c262-6a0d-4ebd-a68e-3b317e063792","trusted":false},"cell_type":"code","source":"predictions = final_model.predict(X_test)\npredictions = np.argmax(predictions, axis=1)\n\ndf = pd.DataFrame({'file': [file for file in test['File'].values], 'species': [species[i] for i in predictions]})\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}