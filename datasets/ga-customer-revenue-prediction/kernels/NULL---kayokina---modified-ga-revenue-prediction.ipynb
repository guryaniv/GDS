{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ba309d2eedcbce1ab3ce5a1457bf1d971e38a56"},"cell_type":"code","source":"\"\"\"Project Overview: \nFor my Capstone Project I’m using dataset available for “Google Analytics Customer Revenue Prediction” competition on Kaggle.com but with a different task (as kaggle has changed the task recently). \nMy goal is to predict spend of GStore customers in test data set.\n\nProblem Statement: \nIn this work, we are predicting the natural log of the transactions in test set:  PredictedLogRevenue.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#IMPORTING REQUIRED LIBRARIES\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport random # random is to generate random values\nfrom plotly.offline import init_notebook_mode, iplot, plot \nimport plotly.graph_objs as go \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer \nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport lightgbm as lgb \n\nimport gc\ngc.enable()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb75ca64f84b830c0af1676d7e45017326b30f5d"},"cell_type":"code","source":"\"\"\" 1. FLATTEN JSON \"\"\"\n# https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields\n\ndef load_df(csv_path='../input/train.csv', JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource'], load_all=False):\n    if not load_all:\n        df = pd.read_csv(csv_path, \n                         converters={column: json.loads for column in JSON_COLUMNS}, \n                         dtype={'fullVisitorId': np.str}, nrows=250000)\n    else:\n        df = pd.read_csv(csv_path, \n                         converters={column: json.loads for column in JSON_COLUMNS}, \n                         dtype={'fullVisitorId': np.str})\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n\n    return df\n\n\n%%time\ntrain = load_df(\"../input/train_v2.csv\")\ntrain.to_csv('train_new_flat.gz', compression='gzip',index=False)\n\n%%time\ntest = load_df(\"../input/test_v2.csv\", load_all=True)\ntest.to_csv('test_new_flat.gz', compression='gzip',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77b118ab5cc7703a37c8b387408d6d51df2dde90"},"cell_type":"code","source":"\"\"\"2. LOAD PREPARED DATA\"\"\"\n\n# files \ntrain_file = 'train_new_flat.gz'\ntest_file = 'test_new_flat.gz'\n#sample_submission_file = 'sample_submission_v2.csv'\n\n# load and view files:\ntrain_df = pd.read_csv(train_file, compression='gzip', dtype={'fullVisitorId': np.str})\ntrain_df.head(1).T\ntrain_df.shape\ntrain_df.fullVisitorId.nunique() #220'374 out of 250K\n\n# load and view files:\ntest_df = pd.read_csv(test_file, compression='gzip', dtype={'fullVisitorId': np.str})\ntest_df.head(1).T\ntest_df.shape\ntest_df.fullVisitorId.nunique() #296530 out of 401589\n\n# difference in columns:\ntrain_df.columns.difference(test_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93f3d42c496fcdc68393bda619b30c3918083ba0"},"cell_type":"code","source":"#FUNCTION FOR CALCULATING RSME score:\ndef rsme(y,pred):\n    return(mean_squared_error(y,pred)**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6960b9c1a93b9ccb6c8ccf5d119c6e43bc146d8"},"cell_type":"code","source":"\"\"\"GET TARGET VARIABLE  totals.transactionRevenue (and not the totals.total...)\"\"\"\nlen_train = len(train_df)\n\n#COMBINING TRAIN AND TEST DATASET\ndf_combi=pd.concat([train_df,test_df],ignore_index=True)\n\n# Store target variable of training data in a safe place\ntotal_revenue = df_combi['totals.transactionRevenue'].fillna(0).astype(float)\ntotal_revenue.describe()\n\n# logarithm of target variable: transactionRevenue\ntarget = total_revenue.apply(lambda x: np.log1p(x))\ntarget.describe()\n\n# split back into 2:\ntarget_train = target.iloc[:len_train]\ntarget_test = target.iloc[len_train:]\n\n# check nulls:\ndf_combi.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ddd9fba959cdbc6c7a9380119894cbc8189e654"},"cell_type":"code","source":"# Delete columns that are not flatten.. \ndf_combi= df_combi.drop(['customDimensions', 'hits','trafficSource.campaignCode', 'totals.transactions', 'totals.totalTransactionRevenue', 'totals.transactionRevenue'], axis=1)\n\n# convert to_datetime:\ndf_combi['date']=pd.to_datetime(df_combi.date, format='%Y%m%d')\ndf_combi['visitStartTime']=pd.to_datetime(df_combi.visitStartTime, unit='s')\ndf_combi[['date','visitStartTime']].head()\n\n# convert to category:\ndf_combi['channelGrouping'] = df_combi['channelGrouping'].astype('category')\ndf_combi['device.deviceCategory'] = df_combi['device.deviceCategory'].astype('category')\ndf_combi['geoNetwork.continent'] = df_combi['geoNetwork.continent'].astype('category')   \n\n# NULL treatment in numeric fileds:\ndf_combi['totals.pageviews'] = df_combi['totals.pageviews'].fillna(0).astype(int)\ndf_combi['totals.sessionQualityDim'] = df_combi['totals.sessionQualityDim'].fillna(0).astype(int)\ndf_combi['totals.timeOnSite'] = df_combi['totals.timeOnSite'].fillna(0).astype(int)\ndf_combi['trafficSource.adwordsClickInfo.page'] = df_combi['trafficSource.adwordsClickInfo.page'].fillna(0).astype(int)\ndf_combi['totals.bounces'] = df_combi['totals.bounces'].fillna('0').astype(int)\ndf_combi['totals.newVisits'] = df_combi['totals.newVisits'].fillna('0').astype(int)\n\n# null in objects\ndf_combi['trafficSource.adwordsClickInfo.adNetworkType'] = df_combi['trafficSource.adwordsClickInfo.adNetworkType'].fillna('missing')\ndf_combi['trafficSource.adwordsClickInfo.slot'] = df_combi['trafficSource.adwordsClickInfo.slot'].fillna('missing')\ndf_combi['trafficSource.adContent'] = df_combi['trafficSource.adContent'].fillna('else')\ndf_combi['trafficSource.adwordsClickInfo.gclId'] = df_combi['trafficSource.adwordsClickInfo.gclId'].fillna('else')\ndf_combi['trafficSource.keyword'] = df_combi['trafficSource.keyword'].fillna('else')\ndf_combi['trafficSource.referralPath'] = df_combi['trafficSource.referralPath'].fillna('(not set)')\ndf_combi['trafficSource.adwordsClickInfo.isVideoAd'] = df_combi['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True)\ndf_combi['trafficSource.isTrueDirect'] = df_combi['trafficSource.isTrueDirect'].fillna(False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e12dd9f6fbd4b3c0ab8795fe3010bca7eb9fd636"},"cell_type":"code","source":"# check if has only 1 value:\ncolumns_to_remove = [col for col in df_combi.columns if df_combi[col].nunique() == 1]\nprint(\"Nb. of variables with unique value: {}\".format(len(columns_to_remove)))\n\n## check values:   \nfor col in columns_to_remove:\n    print(col, df_combi[col].dtypes, df_combi[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49ab72abb5d20c49a8271a050c765b08da58f8c9"},"cell_type":"code","source":"# Feature engineering\n# new feature to help in EDA \"has revenue>0 or not\"\ndf_combi['is_non_zero_revenue'] = total_revenue>0 \n\n# add time features:\ndf_combi['date_dayofweek'] =df_combi['date'].dt.dayofweek\ndf_combi['date_week'] = df_combi['date'].dt.week\ndf_combi['date_year'] =df_combi['date'].dt.year\ndf_combi['date_month'] =df_combi['date'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abcf1f7aea6e8d47ecb67bf8dc3b89ec220ee061"},"cell_type":"code","source":"# remove the rest of useless fields:\ncolumns_to_remove = [col for col in df_combi.columns if df_combi[col].nunique() == 1]\ndf_combi= df_combi.drop(columns_to_remove, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43a49eb26e99dac2f7ed0a69b21980eadad017c0"},"cell_type":"code","source":"df_combi.head(1).T\ndf_combi.shape #(651589, 35)\ndf_combi.info()\n#df_combi_clean.nunique()\ndf_combi.head()\ndf_combi.describe()\n\n# show nulls\ndf_combi.isnull().sum()\n\n\n# extract to csv\ndf_combi.to_csv(\"df_combi_clean.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca5fb374bdea86bdc99229d568d1f201a8193eee"},"cell_type":"code","source":"'III. EDA & feature engineering'\n\n#How many have non-zero-revenue?   - 2687 out of 250000 = 1,07%\ntrain_df['totals.transactionRevenue'][train_df['totals.transactionRevenue']>0].count()#7281\nprint(\"Number of non-zero transactions:\")\nprint(train_df['totals.transactionRevenue'][train_df['totals.transactionRevenue']>0].count() ,\"out of \",len_train, \"or\", train_df['totals.transactionRevenue'][train_df['totals.transactionRevenue']>0].count()/len_train*100, \"%\")\n\n# check dates:\nprint(\"Min date in train set\",pd.to_datetime(train_df.date.min(), format='%Y%m%d')) #20160805\n# 2016-08-05\nprint(\"MAX date in train set\",pd.to_datetime(train_df.date.max(), format='%Y%m%d')) \n#2018-04-29\n\n# check dates:\ntest_df['date'].min() #20180501\ntest_df['date'].max() #20181015\n\nprint(\"Min date in test set\",pd.to_datetime(test_df.date.min(), format='%Y%m%d')) #20160805\n# 2018-05-01\nprint(\"MAX date in test set\",pd.to_datetime(test_df.date.max(), format='%Y%m%d')) \n#2018-10-15 \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c182725b64373b5e393c815c54139f54bc4c14"},"cell_type":"code","source":"# DF for EDA\ntimeseries_df = df_combi.iloc[:len_train]\ntimeseries_df['totals.transactionRevenue'] = total_revenue.iloc[:len_train]\ntimeseries_df.set_index('date', inplace=True)\n\n# Plot the time series in your DataFrame\nax = timeseries_df['totals.transactionRevenue'].plot(color='blue')\nax.set_xlabel('Date')\nax.set_ylabel('transactionRevenue')\nplt.title('transactionRevenue by date')\nplt.show()\n\n#Identifying Trends in Time Series\ntimeseries_df[['totals.transactionRevenue']].rolling(8).mean().plot()\nplt.xlabel('Year', fontsize=20);\n\n# certain peaks are happening according to month/week:\ntimeseries_df[['totals.transactionRevenue']].diff().plot()\nplt.title('transactionRevenue difference')\nplt.xlabel('Year');\n\ntimeseries_df.groupby('visitNumber')['totals.transactionRevenue'].mean().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9543fce55f9502ecb47e8928d3aeb02366e694ec"},"cell_type":"code","source":"\"\"\" ADDITIONAL QUESTIONS TO ANSWER \"\"\" \n\n#1 HOW MANY OF CUSTOMERS in Train set HAVE MORE THAN 1 VISIT? 99'650 subs (19.3%) and 173'149 rows\nprint(\"Number of unique customers with more than 1 visit in train set:\",\n      timeseries_df.loc[timeseries_df['visitNumber']>1]['fullVisitorId'].nunique()) #41354 subs \nprint (\"out of total customers:\",timeseries_df['fullVisitorId'].nunique() ) #220374 total\nprint(\"that is\", round(timeseries_df.loc[timeseries_df['visitNumber']>1]['fullVisitorId'].nunique()/timeseries_df['fullVisitorId'].nunique()*100), \"%\")\n\n#2 HOW MANY OF TEST/SUBMISSION SUBS ARE RETURNING USERS? (FROM TEST)\n# 3510 customer of test-df exist in train_df:\ntest_df.fullVisitorId.isin(train_df.fullVisitorId).astype(int).sum()\n\n# 3 DOES RETURNING CUSTOMER HAS BETTER PROBABILITY FOR TRANSACTION?\n ## yes, 2.5% of returning users had a transaction while only 0.6% of 1st time visitors\n\ntimeseries_df['is_returning_user'] = timeseries_df['visitNumber']>1\ntimeseries_df['is_returning_user'].head()\nprint(\"DOES RETURNING visitors spend more? (totals.transactionRevenue by return status)\")\ntimeseries_df.groupby('is_returning_user')['is_non_zero_revenue'].agg(['sum', 'count', 'mean','std'])\n\n#so avg revenue is higher per returning visitor:\ntimeseries_df.groupby('is_returning_user')['totals.transactionRevenue'].sum().plot.pie(autopct='%.1f%%')\nplt.title('total revenue by returning visitor status')\nplt.show()\n\n# only 7281 visits (1.1%) had a transaction\nonly_with_transactions = timeseries_df.loc[timeseries_df['totals.transactionRevenue']>0]\n\n# only 6719 out of 515999 (1.3%) subs had a transaction - some more >1 time\nonly_with_transactions.fullVisitorId.nunique()\ntimeseries_df.fullVisitorId.nunique()\n\n# returning users with transacations had 1.7-times higher avg revenue than 1st time visitors\nonly_with_transactions.groupby('is_returning_user')['totals.transactionRevenue'].mean().plot('bar', title='AVG Revenue per Transaction')\nonly_with_transactions.groupby('is_returning_user')['totals.transactionRevenue'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"991d2cdc4a38b98a12b96d9f50978488c95d7fd4"},"cell_type":"code","source":"# 4. HOW GEOGRAPHY IMPACT THE REVENUE?\nprint(\"Description of SubContinent count: \")\nprint(train_df['geoNetwork.subContinent'].value_counts()[:8]) # printing the top 7 percentage of browsers\n\n# seting the graph size\nplt.figure(figsize=(16,7))\n\n# let explore the browser used by users\nsns.countplot(train_df[train_df['geoNetwork.subContinent']\\\n                       .isin(train_df['geoNetwork.subContinent']\\\n                             .value_counts()[:15].index.values)]['geoNetwork.subContinent'], palette=\"hls\") # It's a module to count the category's\nplt.title(\"TOP 15 most frequent SubContinents\", fontsize=20) # seting the title size\nplt.xlabel(\"subContinent Names\", fontsize=18) # seting the x label size\nplt.ylabel(\"SubContinent Count\", fontsize=18) # seting the y label size\nplt.xticks(rotation=45) # Adjust the xticks, rotating the labels\n\nplt.show() #use plt.show to render the graph that we did above\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab02f4416157c583e5e7822053e4a0fec77b4b6f"},"cell_type":"code","source":"# Show device count and revenue distribution : \n# Initialize Figure and Axes object\nfig, ax = plt.subplots(figsize=(10,4))\nplt.subplot(2,1,1)\n_ = sns.barplot(x='device.deviceCategory', y ='totals.transactionRevenue',data=train_df)\nplt.subplot(2,1,2)\n_ = sns.countplot(x='device.deviceCategory', data=train_df )\n_rev_per_channel.plot('bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"429c150489bb24727054f75991c987466e88ba03"},"cell_type":"code","source":"\n# Show channelGrouping count and revenue distribution : \n# Initialize Figure and Axes object\nfig, ax = plt.subplots(figsize=(10,4))\nplt.subplot(2,1,1)\n_ = sns.barplot(x='channelGrouping', y ='totals.transactionRevenue',data=train_df)\nplt.subplot(2,1,2)\n_ = sns.countplot(x='channelGrouping', data=train_df )\n_rev_per_channel.plot('bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57552c5f253ff32723bac38982babd13e3289b09"},"cell_type":"code","source":"# AVG revenue per channel count - incorrect - take only train or non-zero revenue count\nrev = train_df.groupby('channelGrouping')['totals.transactionRevenue'].sum()\ncnt = train_df.groupby('channelGrouping')['totals.transactionRevenue'].count()\nrev_per_channel = rev/cnt\nrev_per_channel.plot('bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50f3e63cad16144c5e9e625376fc20507c75a35e"},"cell_type":"code","source":"# continent :\nfig, ax = plt.subplots(figsize=(8,8))\nplt.subplot(2,1,1)\n_ = sns.barplot(x='geoNetwork.continent', y ='totals.transactionRevenue',data=train_df)\nplt.subplot(2,1,2)\n_ = sns.countplot(x='geoNetwork.continent', data=train_df )\n\n# ad network type:\nfig, ax = plt.subplots(figsize=(8,8))\nplt.subplot(2,1,1)\n_ = sns.barplot(x='merged_df', y ='totals_transactionRevenue',data=train_df)\nplt.subplot(2,1,2)\n_ = sns.countplot(x='merged_df', data=train_df )\n\n# by trafficSource.medium\nfig, ax = plt.subplots(figsize=(8,8))\nplt.subplot(2,1,1)\n_ = sns.barplot(x='trafficSource.medium', y ='totals.transactionRevenue',data=train_df)\nplt.subplot(2,1,2)\n_ = sns.countplot(x='trafficSource.medium', data=train_df )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acc8c752c4fb9e040fd837a66e5f8103ff1b9566"},"cell_type":"code","source":"\" V PREPARE DATA FOR MODEL\"\n\n# delete irrelevant columns: 'sessionId' was del before\nexcluded = ['date',\n            'fullVisitorId', \n            'visitId', \n            'visitStartTime',\n            'is_non_zero_revenue']\n\n\ndf_for_model = df_combi.drop(excluded, axis=1)\n\n# check & treat nulls:\ndf_for_model.isnull().sum()\ndf_for_model.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb1e994ea8afcbcb30d121bfb64d5c7321187412"},"cell_type":"code","source":"categorical_features =  ['channelGrouping',\n                         'device.browser',\n                         'device.deviceCategory',\n                         'device.isMobile',\n                         'device.operatingSystem',\n                         'geoNetwork.city',\n                         'geoNetwork.continent',\n                         'geoNetwork.country',\n                         'geoNetwork.metro',\n                         'geoNetwork.networkDomain',\n                         'geoNetwork.region',\n                         'geoNetwork.subContinent',\n                         'trafficSource.adContent',\n                         'trafficSource.adwordsClickInfo.adNetworkType',\n                         'trafficSource.adwordsClickInfo.gclId',\n                         'trafficSource.adwordsClickInfo.isVideoAd',\n                         'trafficSource.adwordsClickInfo.slot',\n                         'trafficSource.campaign',\n                         'trafficSource.isTrueDirect',\n                         'trafficSource.keyword',\n                         'trafficSource.medium',\n                         'trafficSource.referralPath',\n                         'trafficSource.source']\n\ndf_for_model_cat = df_for_model.copy()\nfeature_names = df_for_model_cat.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c51d1c0e3da9a0231eb6da3e6ebed071b8c33a21"},"cell_type":"code","source":"# Encode catrgorical variables:\nle = LabelEncoder()\n\nfor col in categorical_features:\n    le = LabelEncoder()\n    df_for_model_cat[col] = le.fit_transform(df_for_model_cat[col].astype(str))\n\n# split back into 2 DF\ndf_train_clean = df_for_model_cat.iloc[:len(train_df)]\ndf_train_clean.shape #(250000, 35)\ndf_test_clean = df_for_model_cat.iloc[len(train_df):]\ndf_test_clean.shape #(401589, 35)\n\n#preparing the data for models:\nx_train= df_train_clean.values\ny_train = target_train.values\n\nx_test = df_test_clean.values\ny_test = target_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09ae355b276767ca2c065c5c7049f047ea08b0dc"},"cell_type":"code","source":"\"\"\" splitting into Kfolds by time series\"\"\"\n\nfrom sklearn.model_selection import TimeSeriesSplit \ntscv = TimeSeriesSplit(n_splits=3)\nprint(tscv)  \n\n\nfor split_train_x_index, split_val_x_index in tscv.split(x_train):\n    print(\"split_train_x_index:\", split_train_x_index, \"split_val_x:\", split_val_x_index)\n\nsplit_train_x, split_val_x = x_train[split_train_x_index], x_train[split_val_x_index]\nsplit_train_y, split_val_y = y_train[split_train_x_index], y_train[split_val_x_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46d4858c189183205c7ee94b10c381417e9d4223"},"cell_type":"code","source":"\"\"\" DUMMY MODEL \"\"\"\n\n# dummy model\nzeros_y = np.zeros(len(target_test))\n \nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms_zeros_y = sqrt(mean_squared_error(target_test, zeros_y)) \n#  1.9055877487314425","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16120445b3626167f03d596ab217504237981b5a"},"cell_type":"code","source":"\"\"\" GradientBoostingRegressor BENCHMARK MODEL \"\"\"\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n \n#Let's go instantiate, fit and predict. \ngbrt=GradientBoostingRegressor(n_estimators=100, random_state=42)\n \ngbrt.fit(x_train, y_train) \n\ngbrt.train_score_ \ngbrt.loss_\ngbrt.score(x_train, y_train) # 0.32880548481994776\n\nfeature_imp_gbrt = pd.DataFrame(sorted(zip(gbrt.feature_importances_,df_for_model_cat.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp_gbrt.sort_values(by=\"Value\", ascending=False))\nplt.title('GradientBoostingRegressor Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('gbrt_importances-01.png')\n\n# Evaluation of gbrt\ny_pred_gbrt=gbrt.predict(x_test) \nrms_gbrt = sqrt(mean_squared_error(y_test, y_pred_gbrt))  #1.139185","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"226b0d2e9077b4a54edf0de5f86f6ac27de42f7a"},"cell_type":"code","source":"\"\"\" LightGBM MODEL - basic \"\"\"\n\nlgb_train = lgb.Dataset(split_train_x, split_train_y, feature_name=feature_names, categorical_feature=categorical_features )\nlgb_eval = lgb.Dataset(split_val_x, split_val_y, reference=lgb_train )\n\n# specify your configurations as a dict\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 2\n}\n\n\nprint('Start training...')\n\n# train\nlgbm_kfold = lgb.train(params,\n                lgb_train,\n                num_boost_round=500,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=5)\n\nlgb.plot_importance(lgbm_kfold, figsize=(15, 10))\nplt.show()\n\nprint('Save model...') \n# save model to file\nlgbm_kfold.save_model('lgbm_kfold.txt')\n\n# save pickle model: \nimport pickle\nfilename = 'lgbm_kfold.sav'\npickle.dump(lgbm_kfold, open(filename, 'wb'))\n \n# some time later...\n \n# load the model from disk\n#gbm_loaded_model = pickle.load(open(filename, 'rb'))\n\n\n# params of the model:\nlgbm_kfold.num_feature() #35 \nlgbm_kfold.num_trees() #58\n\nlgb.plot_importance(lgbm, figsize=(15, 10))\nplt.show()\n\n# predict\ny_pred_lgbm_kfold = lgbm_kfold.predict(x_test, num_iteration=lgbm_kfold.best_iteration)\n\n# Evaluation of LGBM\nrms_lgbm_kfold = rsme(y_test, y_pred_lgbm_kfold)  #1.6268","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ea1e3e2a10a4905acdb6e1617d63c399536b9c1"},"cell_type":"code","source":"\"\"\" FINE_TUNING OF LightGBM for final model\"\"\"\n\nparam_grid = {\n        'n_estimators': [100, 500],\n         'num_leaves': [15, 31, 63, 127],\n        'learning_rate':[0.01, 0.05, 0.1],\n        'max_depth': [5, 10, 15, 20, 30, 35,-1],\n        'min_data_in_leaf': [30, 50, 100, 300, 5000],\n        'drop_rate' : [0.1, 0.2, 0.3],\n        'lambda_l1': [0, 0.2, 1],\n        'lambda_l2': [0, 0.2, 1],\n        'feature_fraction': [0.7, 0.9],\n        'bagging_fraction': [0.8, 0.9],\n        'bagging_freq': [5, 10]}\n\n\nlgbm_estimator = lgb.LGBMRegressor(boosting='gbdt' ,  random_state=42)\n\n#create scoring for Gridsearch\nrmse_scorer = make_scorer(rsme, greater_is_better=False)\n\nr_search = RandomizedSearchCV(estimator=lgbm_estimator, \n                             param_distributions=param_grid, \n                             scoring=rmse_scorer,\n                             cv = tscv.get_n_splits([x_train,y_train]),\n                             random_state=42, verbose=2)\n\nlgbm_search_model = r_search.fit(x_train,y_train)\n\n# save pickle model: \nimport pickle\nfilename = 'lgbm_search_model.sav'\npickle.dump(lgbm_search_model, open(filename, 'wb'))\n\n\nprint(\"BEST PARAMETERS: \" + str(lgbm_search_model.best_params_))\nprint(\"BEST CV SCORE: \" + str(lgbm_search_model.best_score_))\n\n\n# Predict (after fitting RandomizedSearchCV is an estimator with best parameters)\ny_pred_lgbm_search = r_search.predict(x_test)\n\n# Evaluation of LGBM\nrms_lgbm_kfold = rsme(y_test, y_pred_lgbm_search)  #1.6263\n\nr_search.best_estimator_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"525f55655905f9e8b11480b0f000cb6cbd58ea0a"},"cell_type":"code","source":"\"\"\" SUBMIT\"\"\"\ndf_submit_final = test_df.loc[:,['fullVisitorId']]\ndf_submit_final['PredictedLogRevenue'] = y_pred_lgbm_search\ndf_submit_final['RealLogRevenue'] = y_test\ndf_submit_final.to_csv(\"df_submit_final.csv\", index=False)\n\n\ndef plot_diff(X, diff):\n    fig = plt.subplots(figsize=(18,6))\n    plt.plot(diff, linewidth=1)\n    plt.xticks(rotation=45)\n    plt.show()\n\ndifference = np.subtract(y_test, y_pred_lgbm_search)\n\nplot_diff(x_test, difference)\nplt.xlabel('visit')\nplt.ylabel('Difference btwn PredictedLogRevenue vs RealLogRevenue');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}