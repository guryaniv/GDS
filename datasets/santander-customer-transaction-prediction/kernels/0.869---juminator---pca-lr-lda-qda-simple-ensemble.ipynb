{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read in Train Data\ntrain = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5b676f14494eab1a0922a8c7a2e7c76718a18f8"},"cell_type":"code","source":"# Read in Test Data\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22a53f82dfd32b7b5a57aa39b2065d74b037d806"},"cell_type":"code","source":"# Number of rows and columns of training and test data\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c112deca5e48545d4eb899e624202ece77636e6"},"cell_type":"markdown","source":"# **Exploratory Data Analysis (EDA)**"},{"metadata":{"trusted":true,"_uuid":"36cc06997416c248ca3243a130856227d642e7e3"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a29096e8da0eb6e78658d63ae0e8dd064c76d980"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bdf7e37ae6a78f185890e4eecaefb2cc5bc7719"},"cell_type":"code","source":"train.select_dtypes(include=\"object\").columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d13d0657040bcac4724efa5c17904edbd7c33f5c"},"cell_type":"markdown","source":"The only categorical feature is the ID_code variable and all other variables are numerical!"},{"metadata":{"trusted":true,"_uuid":"cd606923bf7c744de87f005d0e635e3afa63e781"},"cell_type":"code","source":"# Checking if ID_code is unique\ntrain.ID_code.nunique() == train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a099b188c2d8764fb4982a5f8ac9b63f52604d0"},"cell_type":"markdown","source":"So every observation is an unique customer record!"},{"metadata":{"_uuid":"ded5e3dc52d876f40671b2a2800f0fd07a0d624a"},"cell_type":"markdown","source":"> ### **Target Variable** (What we want to predict)"},{"metadata":{"trusted":true,"_uuid":"31409e7b6fcfd914d75c8b18122aea1ecb1daafd"},"cell_type":"code","source":"sns.countplot(train.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79f81d9e5673b03c772500b02a77c613935e16b8"},"cell_type":"code","source":"train.target.value_counts() *100 / train.target.count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5154639079d0f08702a351b6bfba666b04fc7e52"},"cell_type":"markdown","source":"89% for target equal to 0 and 10% for target equal to 1. Pretty unbalanced! We might want to take this unbalance into consideration! (Some algorithms don't perform well with class unbalance). Algorithms like KNN, Boosting, Random forest might work better than others. But the model evaluation metric here is \"AUC\" which is less sensitive to class imbalance (other recommended metrics for unbalanced classes are f1-score and logloss)"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"667cc0a86bac9647b6764a21495e4ca3acd76dbf"},"cell_type":"code","source":"train.groupby(\"target\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"165e0e2d8e5123b4c9fa1443da55a488483914e0"},"cell_type":"code","source":"train.groupby(\"target\").median()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc978d694e73d649067335ff29a064f55298290d"},"cell_type":"markdown","source":"Just by first glance, observations with target==1 seem to have higher mean & median  values for each variable in general than those with target==0. Let's see if that is the case."},{"metadata":{"trusted":true,"_uuid":"910627d0976cddd0ae68a1746d84e728281f6446"},"cell_type":"code","source":"np.mean(train.groupby(\"target\").mean().iloc[1] >= train.groupby(\"target\").mean().iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abe2d5c7c700eeffcb80338cd4df7f37278a3439"},"cell_type":"code","source":"np.mean(train.groupby(\"target\").median().iloc[1] >= train.groupby(\"target\").mean().iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7deea960b8d4bdbc2be6b17b0b42cd527130a934"},"cell_type":"markdown","source":"52% and 51% of the variables have higher mean and median values respectively for observations with target==1."},{"metadata":{"_uuid":"8162b32d70963464d0daaa93f0945cdd52005aba"},"cell_type":"markdown","source":"### **Distributions of variables**"},{"metadata":{"_uuid":"2e0fc5beee476e52986d93355d658c2f0d27256f"},"cell_type":"markdown","source":"I am interested in which variables are not linear not likely from a Gaussian Distribution because some ML algorithms work better if each feature is normally distributed (and if not, we might want to log transform it!) There are multiple normality tests but Shapiro test is appropriate for small dataset(N<5000), so I will use the D’Agostino’s K^2 Test!"},{"metadata":{"trusted":true,"_uuid":"f6599ff007b3e3bd13b8d2482926beeb4cb2f3b9"},"cell_type":"code","source":"features = train.columns.values[2:203]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd94fa5caa111106d6afff2f52b6cbaf149798f4"},"cell_type":"code","source":"from scipy.stats import normaltest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"54fcb61552f1e612499cd60f34b5af2333770178"},"cell_type":"code","source":"# # D’Agostino’s K^2 Test on TRAIN DATA\n# non_normal_features = []\n# for feature in features:\n#     stat, p = normaltest(train[feature])\n#     if p <= 0.01:\n#         print(feature,\"not normal\")\n#         non_normal_features.append(feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"38c851b9c9df1280e9f3f92092fefc3d0f9f84a2"},"cell_type":"code","source":"# # D’Agostino’s K^2 Test on TEST DATA\n# non_normal_features_test_data = []\n# for feature in test.columns.values[1:202]:\n#     stat, p = normaltest(test[feature])\n#     if p <= 0.05:\n#         print(feature,\"not normal\")\n#         non_normal_features_test_data.append(feature)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b50be076e35b852e3ed36bed0eb38332651cba2"},"cell_type":"markdown","source":"You may want to log or squared transform these non-normal features!"},{"metadata":{"_uuid":"5d3c6c10f6b8c0c66c968d9fe97bf4bd615d81af"},"cell_type":"markdown","source":"> ### **Missing Data** "},{"metadata":{"trusted":true,"_uuid":"916ce8ba3915cce59782d295e4e320ec81a4757c"},"cell_type":"code","source":"train.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9aa637b45ead44652f85eeeda863c86251c92e80"},"cell_type":"markdown","source":"There is no missing data!"},{"metadata":{"_uuid":"7ba16c824ba4ed6efc03eb42a50035f94c6e9c44"},"cell_type":"markdown","source":"### **Correlations amongst Variables** (Credits to Gebriel Preda's Kernel \"Santander EDA and Prediction\")"},{"metadata":{"trusted":true,"_uuid":"99e48d74e0111898705905b0b848cf35e4566235"},"cell_type":"code","source":"correlations = train[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0610fdd1dc3f5e3e329503f86feaf3905476be10"},"cell_type":"markdown","source":"Even the top 10 pairs with highest correlation have absolute values of 0.008. This is a very weak correlation. Multicollinearity issue doesn't seem to be a problem here!"},{"metadata":{"_uuid":"fa6d4d19fac8a8a7ecd6755fa9831994808cfde1"},"cell_type":"markdown","source":"### **PCA**"},{"metadata":{"_uuid":"9320f0b36b1af26a0bda62ecada9131f3bf0cb7b"},"cell_type":"markdown","source":"PCA is a dimensionality reduction technique that reduces noise and extracts features that are independent(orthogonal). But PCA is sensitive to variance and different scales, so standardizing will help PCA perform better! HOWEVER, we found that the correlation between different features in the training dataset is not that significant, so using PCA might not be meaningful (because PCA is best when the dimension p is very large and a lot of features are correlated to one another a lot)"},{"metadata":{"trusted":true,"_uuid":"3bb8a2e1c26dbb105ee260918df2fea3d6eb1487"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstandardized_train = StandardScaler().fit_transform(train.set_index(['ID_code','target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d2756613f139b5bca9458bceec88da24f8bde94"},"cell_type":"code","source":"standardized_train = pd.DataFrame(standardized_train, columns=train.set_index(['ID_code','target']).columns)\nstandardized_train = standardized_train.join(train[['ID_code','target']])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c355abc59716ffab9a5947d96e983f71555c560f"},"cell_type":"markdown","source":"We have to determine the number of features we are going to extract with PCA! We use the cumulative variance explained and find the number of features where the variance doesn't increase as much."},{"metadata":{"trusted":true,"_uuid":"a2c293f503b319cdfbea5599f427402f44df2252"},"cell_type":"code","source":"from sklearn.decomposition import PCA\nk=80\npca = PCA(n_components=k, random_state=42, whiten=True)\npca.fit(standardized_train.set_index(['ID_code','target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3900683494bb5aadf1ae22331292caf78caccbf1","scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nplt.plot(pca.explained_variance_ratio_)\nplt.xticks(range(k))\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Proportion of variance explained by additional feature\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b06a455c96872757e1e99a931371081734d81a79"},"cell_type":"code","source":"sum(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"964452effd5846fb19258d12213d847e5484b5a7"},"cell_type":"markdown","source":"Normally, if there is a elbow looking point in the graph above, the x value(number of features) of that point is usually the ideal number of components for PCA. However in this case, each principal component explains very little of the total variance (e.g. first principal component only explains abou 0.6% of the total variance). Even when we sum up all the variance explained by the 80 principal components, it only amounts to 40%. Let's increase the k and see what happens."},{"metadata":{"trusted":true,"_uuid":"fda92f67412b0ee2f600a0e39af66b84137e0680"},"cell_type":"code","source":"sum(PCA(n_components=120, random_state=42, whiten=True).fit(standardized_train.set_index(['ID_code','target'])).\\\nexplained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a80d2c1b0bd68747a9e43fa1c8162019ba0ec75e"},"cell_type":"code","source":"sum(PCA(n_components=160, random_state=42, whiten=True).fit(standardized_train.set_index(['ID_code','target'])).\\\nexplained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7183d89268cee8ba8c2e20d9597dd8e61a0d7472"},"cell_type":"markdown","source":"80% of the total variance is explained if we use 160 principal components. 80% is not bad! Let's reduce 200 features to 160 by setting k=160 for PCA."},{"metadata":{"trusted":true,"_uuid":"abe78001da40313784e37a1f722ecfed7328a148"},"cell_type":"code","source":"pca = PCA(n_components=160).fit_transform(standardized_train.set_index(['ID_code','target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c07cd29b22c79819c24d44a368545ac5e016981"},"cell_type":"code","source":"pca_col_names = []\nfor i in range(160):\n    pca_col_names.append(\"pca_var_\" + str(i))\npca_col_names","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"4fdff3a3b0331665d7bb038be3eb1f0269fe1f94"},"cell_type":"code","source":"# Save PCA transformed train dataset just in case\npca_train = pd.DataFrame(pca, columns=pca_col_names).join(train[['ID_code','target']])\npca_train.to_csv(\"pca_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fea58b56546941f9f73a63c314ee0f675de6bed4"},"cell_type":"code","source":"# Standardize the test data as well\nstandardized_test = StandardScaler().fit_transform(test.set_index(['ID_code']))\nstandardized_test = pd.DataFrame(standardized_test, columns=test.set_index(['ID_code']).columns)\nstandardized_test = standardized_test.join(test[['ID_code']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3aa6d79d52d8fbe79691c12d5dc4f72684f5652"},"cell_type":"code","source":"pca = PCA(n_components=160).fit_transform(standardized_test.set_index(['ID_code']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e6abdc50c210b30927d4ee429c64fc6370be18f"},"cell_type":"code","source":"pca_col_name_for_test = []\nfor i in range(160):\n    pca_col_name_for_test.append(\"pca_var_\" + str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b058cee5d16d452c800a1006cbd038ac8a50d505"},"cell_type":"code","source":"# Save PCA transformed test dataset just in case\npca_test = pd.DataFrame(pca, columns=pca_col_name_for_test).join(train[['ID_code']])\npca_test.to_csv(\"pca_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d099724a8f4a03b02698bf4292d30773c0068e35"},"cell_type":"markdown","source":"# **Modelling**"},{"metadata":{"trusted":true,"_uuid":"1c4b486ea26f5e3bb5b616021b2a3a010070442f"},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c41c0801fe12767d412cca34a3ed9afc3046b10b"},"cell_type":"code","source":"# X = standardized_train.drop('target',axis=1).set_index('ID_code')\n# y = standardized_train[['target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59e454c9c22e1d549becb2145b07558d026d8f8f","scrolled":true},"cell_type":"code","source":"# # Split training dataset to train and validation set\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5d5153096e94ad9088ab0fb13c11d98414c3881"},"cell_type":"code","source":"# Split Train Dataset into Predictor variables Matrix and Target variable Matrix\nX_train = standardized_train.set_index(['ID_code','target']).values.astype('float64')\ny_train = standardized_train['target'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb8af57517b7adc562fbbd01ac8c1e3281075497"},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"927ece686fde2e8db9407a296b0621191c2d31fa","scrolled":true},"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogit_clf = LogisticRegression(random_state=42).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6687326eabb042dac508963d0c7038622143a455"},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfpr, tpr, thr = roc_curve(y_train, logit_clf.predict_proba(X_train)[:,1])\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operator Characteristic Plot', fontsize=20, y=1.05)\nauc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"468e5cf4a4122f4e2e1f914b397326b60eab4711"},"cell_type":"code","source":"cross_val_score(logit_clf, X_train, y_train, scoring='roc_auc', cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf72fb4f0d7790a25c8f63a489b87db881815f97"},"cell_type":"markdown","source":"#### Linear Discriminant Analysis (LDA)\n- LDA aims to find the directions that maximize the separation (or discrimination) between different classes\n- LDA tries to determine a suitable feature (sub)space in order to distinguish between patterns that belong to different classes\n- Estimate parameters with maximum likelihood (those parameters minimize Squared Mahalanobis Distance)\n- Models the distribution of predictors separately in each of the response classes, and then it uses Bayes’ theorem to estimate the probability\n- Both LDA and QDA assume the the predictor variables X are drawn from a multivariate Gaussian (aka normal) distribution.\n- (Compared to QDA) LDA is more suitable for smaller data sets, and it has a higher bias, and a lower variance.\n- If n is small and the distribution of the predictors X is approximately normal in each of the classes, the LDA model is more stable than logistic.\n- When the classes are well-separated, the parameter estimates for the logistic model are surprisingly unstable. LDA does not suffer from this.\n- LDA and QDA are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyper-parameters to tune.\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"fd0ea0765f10085f0155e813487758eb537bc7fe"},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\nlda_clf = LinearDiscriminantAnalysis()\nlda_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a75c2488138701506ebba5ec1c239ad5e366e5a"},"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nfpr, tpr, thr = roc_curve(y_train, lda_clf.predict_proba(X_train)[:,1])\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operator Characteristic Plot', fontsize=20, y=1.05)\nauc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd90f06d66668f4b0d89f5dd6e67f44f5cbb8084"},"cell_type":"code","source":"cross_val_score(lda_clf, X_train, y_train, scoring='roc_auc', cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d8cc9f9069f2caec6c0580b101eb35efe8efd94"},"cell_type":"markdown","source":"#### Quadratic Discriminant Analysis\n- A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.\n- The model fits a Gaussian density to each class.\n- QDA is a better option for large data sets (compared to LDA), as it tends to have a lower bias and a higher variance."},{"metadata":{"trusted":true,"_uuid":"5446a5d84de71e46985166b6890af1ed3e5cc29d"},"cell_type":"code","source":"qda_clf = QuadraticDiscriminantAnalysis()\nqda_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9057bcfe13b38b1d53b2fc7ae72fb69ea481c83"},"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nfpr, tpr, thr = roc_curve(y_train, qda_clf.predict_proba(X_train)[:,1])\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operator Characteristic Plot', fontsize=20, y=1.05)\nauc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98ab0d3283af2cbccc3bd15c936d561f6025347f"},"cell_type":"code","source":"cross_val_score(qda_clf, X_train, y_train, scoring='roc_auc', cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c2f8aeef83d8cb257c90a563849aad000ce4c24"},"cell_type":"markdown","source":"LDA has the highest AUC for cross validation among the three ML algorithms (Logis****tic regression, LDA, QDA) I tried so far! "},{"metadata":{"trusted":true,"_uuid":"015ddf2a81e9ba1d0013991c3dc06eaad7aca258"},"cell_type":"code","source":"X_test = standardized_test.set_index('ID_code').values.astype('float64')\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['target'] = logit_clf.predict_proba(X_test)[:,1]\nsubmission.to_csv('LR.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33a1fa4170a46bde5fff2bad0027b18ce43ff0b6"},"cell_type":"code","source":"X_test = standardized_test.set_index('ID_code').values.astype('float64')\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['target'] = lda_clf.predict_proba(X_test)[:,1]\nsubmission.to_csv('lda.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"733182f968c92c13abab26cb79eaf7928520ce4d"},"cell_type":"code","source":"X_test = standardized_test.set_index('ID_code').values.astype('float64')\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['target'] = qda_clf.predict_proba(X_test)[:,1]\nsubmission.to_csv('lda.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f604126e94866021044bbe48d372eacb247d418"},"cell_type":"markdown","source":"# **Simple Ensemble Method**"},{"metadata":{"trusted":true,"_uuid":"9e3d92666bd2bd542c99c80e8d169975b8c63e89"},"cell_type":"markdown","source":"There are various Ensemble methods and one way is to use the mean probability of all the models. That is, for each observation, different ML algorithms predict the probability of that observation being part of class 1 and we calculate the mean of all those probabilities."},{"metadata":{"trusted":true,"_uuid":"bc7592d414e4697d9aa36646f4944c3c04e87eee"},"cell_type":"code","source":"X_test = standardized_test.set_index('ID_code').values.astype('float64')\nsubmission = pd.read_csv('../input/sample_submission.csv')\n\nlogit_pred = logit_clf.predict_proba(X_test)[:,1]\nlda_pred = lda_clf.predict_proba(X_test)[:,1]\nqda_pred = qda_clf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3c938028f6755bcffaba910300ee09156261b9e"},"cell_type":"code","source":"submission = \\\nsubmission.join(pd.DataFrame(qda_pred, columns=['target1'])).join(pd.DataFrame(logit_pred, columns=['target2'])).\\\njoin(pd.DataFrame(lda_pred, columns=['target3']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4a531e0db6b2c30cd7f794019780a915d7f58d5"},"cell_type":"code","source":"submission['target'] = (submission.target1 + submission.target2 + submission.target3) / 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18a2e16ca888c7000c4a5a5394d81836eed5a8c3"},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8be85568c4f6c269d5f9ad84616dee30abc0764"},"cell_type":"code","source":"del submission['target1']\ndel submission['target2']\ndel submission['target3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c084b17b0199b17c90e081fa1e084d9b52d61839"},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"848bcf1ef8c794494f93dfac9dc771eed6e2830b"},"cell_type":"code","source":"submission.to_csv('logit_lda_qda_mean_ensemble.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}