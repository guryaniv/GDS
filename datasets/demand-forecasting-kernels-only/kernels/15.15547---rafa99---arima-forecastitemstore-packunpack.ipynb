{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy.ma as ma\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom sklearn import preprocessing\nimport os #modulos de gestion de directorios\nimport glob #modulo de visualización de directorios\n#import xgboost as xgb\ncolor = sns.color_palette()\nimport sys\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.options.display.max_columns = 999","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05832c3e37bf511ead5e4bc977afef2d50f40c02"},"cell_type":"code","source":"#cambiamos al directorio de trabajo donde tenemos los datos\n#os.chdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed367aed77d67d6d94851d34e4b932cf21e30041"},"cell_type":"code","source":"os.getcwd() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"871a8f4251d02a7d52252c8c6fcd1b3aac0ec97f"},"cell_type":"code","source":"print(glob.glob(\"../input/*.*\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3334a5f0fbb20dc1014170d8bf7401f60cbb8a9"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", \\\n                      parse_dates=True, index_col=0)\ntest_df = pd.read_csv(\"../input/test.csv\", \\\n                     parse_dates=True, index_col=0 )\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f661dc8627e0a423ce262f965197d81703c74e04"},"cell_type":"code","source":"train_df=train_df.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35f22e6019a722122640577e04f374ca49f0eb05"},"cell_type":"code","source":"#we create some new fields to easy manipulate\n#forecasting probably should be at item-store because demand pattens could vary much dep. items and store \ntrain_df['weekday']=pd.DatetimeIndex(train_df['date']).weekday\ntrain_df['month']=pd.DatetimeIndex(train_df['date']).month \ntrain_df['year']=pd.DatetimeIndex(train_df['date']).year\ntrain_df['itemstore']=train_df.item.astype(str)+\"-\"+train_df.store.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f23052eb0fc5705f9e86b92db40f140006123690"},"cell_type":"code","source":"#overview of data\nprint(\"number of different items: %i\" %(len(np.unique(train_df.item))))\nprint(\"number of different stores: %i\" %(len(np.unique(train_df.store))))\nprint(\"number of different dates: %i\" %(len(np.unique(train_df.date))))\nprint(\"maximun date in data: %s\" %(max(train_df.date)))\nprint(\"minimum date in data: %s\" %(min(train_df.date)))\nprint(\"number of different itemstore: %i\" %(len(np.unique(train_df.itemstore))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0f3b46db9da1ebb0b30a87973dc61a311de8581"},"cell_type":"code","source":"#create some lists to see range of unique values\nstores = list(set(train_df.store))\nitem = list(set(train_df.item))\nitemstore = list(set(train_df.itemstore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caf5bba0725af1c8b3b5f0efe69d0972b240c2f5"},"cell_type":"code","source":"#we check anual sales profile comparing stores\nc=train_df.groupby(['year','store']).sum()\nplt.figure(figsize=(15,10))\nd=c.unstack()\nd.plot(y='sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"152e260e5b87debb4386e9a8ce130892d6e80fda"},"cell_type":"code","source":"#we check seasonal sales profile comparing stores\nc=train_df.groupby(['month', 'store']).sum()\nplt.figure(figsize=(15,10))\nd=c.unstack()\nd.plot(y='sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"959f4a012e3dcf372ab5c8fcacfdf06a13a846ad"},"cell_type":"code","source":"#we check seasonal sales profile comparing stores\nc=train_df.groupby(['weekday', 'store']).sum()\nplt.figure(figsize=(15,10))\nd=c.unstack()\nd.plot(y='sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8125933b91eb285eee3e10e818da724b8239bc3d"},"cell_type":"code","source":"#we evaluate increase in anual sales at itemstore level\nb =train_df.drop(columns=['store', 'item','weekday','date','month'])\nc=b.groupby(['year', 'itemstore']).sum()\nd=c.unstack()\nsales_itemstore_year=d.T\nsales_itemstore_year['delta_2014/2013']=((sales_itemstore_year[2014]-sales_itemstore_year[2013])/sales_itemstore_year[2013])*100\nsales_itemstore_year['delta_2015/2014']=((sales_itemstore_year[2015]-sales_itemstore_year[2014])/sales_itemstore_year[2014])*100\nsales_itemstore_year['delta_2016/2015']=((sales_itemstore_year[2016]-sales_itemstore_year[2015])/sales_itemstore_year[2015])*100\nsales_itemstore_year['delta_2017/2016']=((sales_itemstore_year[2017]-sales_itemstore_year[2016])/sales_itemstore_year[2016])*100\nsales_itemstore_year_deltas =sales_itemstore_year.drop(columns=[2013, 2014, 2015, 2016, 2017], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2858b80acb46f03d918c70546894adf2ccc2b74a"},"cell_type":"code","source":"sales_itemstore_year_deltas =sales_itemstore_year.drop(columns=[2013, 2014, 2015, 2016, 2017], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"292ad2c2016ca2a4d6e532077ac89c4f710f2923"},"cell_type":"code","source":"#heat-maps to compare deltas anual and bet. itemstore each year\nsales_itemstore_year_deltas=sales_itemstore_year_deltas.sort_values('delta_2014/2013')\nplt.figure(figsize=(8,10))\nsns.heatmap(sales_itemstore_year_deltas)\nplt.title(\"Percentage variation sales-itemstore. Sort 2014/2013\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6b7b8472580d8834fb969ab9f4dba2462b0b04f"},"cell_type":"code","source":"sales_itemstore_year_deltas=sales_itemstore_year_deltas.sort_values('delta_2017/2016')\nplt.figure(figsize=(8,10))\nsns.heatmap(sales_itemstore_year_deltas)\nplt.title(\"Percentage variation sales-itemstore. Sort 2017/2016\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6931671d6ca534c3f6c7ff767fc47c3e59bcda4a"},"cell_type":"code","source":"#we pivot, group to weeks\ntrain_df['date'] = pd.to_datetime(train_df['date'])\ntrain_df_train=train_df.pivot(index='date', columns='itemstore', values='sales')\ntrain_df_train=train_df_train.resample('W').sum()\ntrain_df_train = train_df_train[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9ca1a4996d8587e90b2bb2ab53a6c4363f1fc1d"},"cell_type":"code","source":"train_df_train_V1 = train_df_train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf753faf3708aa35aba8bddb167eb79f55561418"},"cell_type":"code","source":"# we search ARIMA parameters for item 1 store 1 with 52 weeks differentation for stationary hipotesis\nimport warnings\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom pandas import tseries\n\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return np.array(diff)\n\ndef inverse_difference(history, yhat, interval=1):\n    return yhat + history[-interval]\n\n# evaluate an ARIMA model for a given order (p,d,q) and return RMSE\ndef evaluate_arima_model(X, arima_order):\n# prepare training dataset\n    X = X.astype('float32')\n    train_size = int(len(X) * 0.7)\n    train, test = X[0:train_size], X[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n    # difference data\n        weeks_in_year = 52\n        diff = difference(history, weeks_in_year)\n        model = ARIMA(diff, order=arima_order)\n        model_fit = model.fit(trend='nc', disp=0)\n        yhat = model_fit.forecast()[0]\n        yhat = inverse_difference(history, yhat, weeks_in_year)\n        predictions.append(yhat)\n        history.append(test[t])\n        # calculate out of sample error\n    rmse = sqrt(mean_squared_error(test, predictions))\n    return rmse\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    rmse = evaluate_arima_model(dataset, order)\n                    if rmse < best_score:\n                        best_score, best_cfg = rmse, order\n                    print('ARIMA%s RMSE=%.3f' % (order,rmse))\n                except:\n                    continue\n    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"740521fb5d06a617002ebef3b256dd566ad79e0d"},"cell_type":"code","source":"#evaluate models\np_values = range(0, 6)\nd_values = range(0, 2)\nq_values = range(0, 6)\nt = '1-1'\n\nwarnings.filterwarnings(\"ignore\")\n\nevaluate_models(train_df_train_V1[t].values, p_values, d_values, q_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"749d44406fcb9604d6ef91503cb783b0f78dae87"},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n#Procedure to predict values\ndef do_predictions_join_unpack(t, database1, database2, dictionary, database3):\n    X = database1[itemstore[t]].values\n    X = X.astype('float32')\n    weeks_in_year = 52\n    diff = difference(X, weeks_in_year)\n    model = ARIMA(diff, order=(1,0,1))\n    model_fit = model.fit(trend='nc', disp=0)\n    # bias constant, could be calculated from in-sample mean residual\n    bias = 0\n    # save model\n    #model_fit.save('model.pkl')\n    #np.save('model_bias.npy', [bias])    \n    # load and prepare datasets\n    X = database1[itemstore[t]].values.astype('float32')\n    history = [x for x in X]\n    weeks_in_year = 52\n    y = database2[itemstore[t]].values.astype('float32')\n    # load model\n    #model_fit = ARIMAResults.load('model.pkl')\n    #bias = np.load('model_bias.npy')\n    # forecast 13 periods\n    predictions = list()\n    forecast = model_fit.forecast(steps=13)[0]\n    for yhat in forecast:\n        yhat = bias + inverse_difference(history, yhat, weeks_in_year)\n        history.append(yhat)\n        predictions.append(yhat)\n    #turn to daily with weekly pattern and copy in summary\n    database2 = database2.reset_index()\n    predictions = pd.DataFrame(predictions)\n    train_df_test_V1_pred = pd.concat([database2['date'], database2[itemstore[t]], predictions], axis=1)\n    train_df_test_V1_pred['date'] = pd.to_datetime(train_df_test_V1_pred['date'])\n    train_df_test_V1_pred=train_df_test_V1_pred.set_index('date')\n    new_dates = pd.date_range('2018-01-01', '2018-04-01', name='date')\n    train_df_test_V1_pred_daily = train_df_test_V1_pred.reindex(new_dates, method='ffill')\n    for k in range (13):\n        for j in range (7):\n            train_df_test_V1_pred_daily[0][(k*7)+j] = round(train_df_test_V1_pred_daily[0][(k*7)+j]*dictionary[itemstore[t]][j])\n    database3[[itemstore[t]]] = train_df_test_V1_pred_daily[0]\n    return database3, train_df_test_V1_pred_daily, predictions, train_df_test_V1_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76e9de5b20618991c5e78855166631c73d85e4ae"},"cell_type":"code","source":"train_df = train_df.set_index('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d878ecc377a034d73d6010262b77f914488678df"},"cell_type":"code","source":"#we asign in a dictionary for each item-store the de-composition of sales for SUN-MON-TUE.......-SAT-SUMA. we use 2017 weekly pattern\ndictionary_week_sales_itemstore={}\ndictionary_week_sales_itemstore_reparto={}\nfor i in range (len(itemstore)):\n    dictionary_week_sales_itemstore.update({itemstore[i]:[0, 0, 0, 0, 0, 0, 0, 0]})\n    dictionary_week_sales_itemstore_reparto.update({itemstore[i]:[0, 0, 0, 0, 0, 0, 0, 0]})\n\n#Now we group sales at item-store level and week-day    \n#train_df=train_df.set_index('date')\ntrain_sales_weekday=train_df['01-01-2013':'31-12-2017'].groupby(['weekday', 'itemstore']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1305bb6f348abe3d1f2b60af0a7bef6c5826a4d9"},"cell_type":"code","source":"#def update_dictionary_week_sales_itemstore(itemstore, train_sales_weekday)\nfor i in range (len(itemstore)):\n    for j in range (0,7):\n        dictionary_week_sales_itemstore[itemstore[i]][j]= train_sales_weekday.loc[(j, itemstore[i]),['sales']][0]\n    dictionary_week_sales_itemstore[itemstore[i]][7]= sum(dictionary_week_sales_itemstore[itemstore[i]][0:7])   \n    \n#Now we update second dictionary dictionary_week_sales_itemstore_reparto={}\nfor i in range (len(itemstore)):\n    for j in range (0,7):\n        dictionary_week_sales_itemstore_reparto[itemstore[i]][j]= (dictionary_week_sales_itemstore[itemstore[i]][j]/   \\\n            dictionary_week_sales_itemstore[itemstore[i]][7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b43460e24f764adce278f922bd59fc78d600322"},"cell_type":"code","source":"#we prepare dataframe for integrate all results\ntest_df['itemstore']=test_df.item.astype(str)+\"-\"+test_df.store.astype(str)\ntest_df['sales'] = 0\ntrain_df_test_V2  = test_df.drop(columns=['store', 'item'])\ntrain_df_test_V2['date'] = pd.to_datetime(train_df_test_V2['date'])\ntrain_df_test_V2 = train_df_test_V2.pivot(index='date', columns='itemstore', values='sales')\ntrain_df_test_V1 = train_df_test_V2.resample('W').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d66bca9289647d01773e651f03d8eacf8ef400b"},"cell_type":"code","source":"#calculation of all itemstore predictions\npredictions = list()\nfor t in range (len(itemstore)):\n        do_predictions_join_unpack(t, train_df_train_V1, train_df_test_V1, dictionary_week_sales_itemstore_reparto, train_df_test_V2)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"beb1bd8d7c9125ba036e0278d3481634d2104cb8"},"cell_type":"code","source":"#copy same pattern for first week\ntrain_df_test_V2.ix['2018-01-01']=train_df_test_V2.ix['2018-01-08']\ntrain_df_test_V2.ix['2018-01-02']=train_df_test_V2.ix['2018-01-09']\ntrain_df_test_V2.ix['2018-01-03']=train_df_test_V2.ix['2018-01-10']\ntrain_df_test_V2.ix['2018-01-04']=train_df_test_V2.ix['2018-01-11']\ntrain_df_test_V2.ix['2018-01-05']=train_df_test_V2.ix['2018-01-12']\ntrain_df_test_V2.ix['2018-01-06']=train_df_test_V2.ix['2018-01-13']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0841d4dd0c300e6f4a640a587c8d1e36c18922b0"},"cell_type":"code","source":"for i in range (len(test_df)):\n    test_df['sales'][(i)] = train_df_test_V2.loc[test_df['date'][(i)], test_df['itemstore'][(i)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad7a2e91c54310a7a1bff0cda75783a38142173c"},"cell_type":"code","source":"submission = test_df.drop(columns=['date', 'store', 'item', 'itemstore'])\nsubmission_1= submission.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1606451b6599ae599d19a20574a2e4adbbea468"},"cell_type":"code","source":"submission_1.to_csv('submissionFCTl.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c8bf041a6cbe6ff5baff0e3b79e8c500e07aba0"},"cell_type":"markdown","source":"Thanks !"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}