{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined bty the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom __future__ import print_function\n\nimport os\nimport json\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport copy\n\nimport time, random, math\nimport re  #regular expression\nimport operator\nfrom collections import defaultdict, Counter, OrderedDict, namedtuple\nimport numbers\n\n\nimport nltk#natural language processing\nimport six\nimport scipy.sparse as sp\nfrom operator import itemgetter\nimport string\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n%matplotlib inline\nmatplotlib.rc('axes', facecolor = 'white')\n\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier as RFC, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import accuracy_score, log_loss\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d7050ee8d57c2b7c9bd22c3d21cb197ba963242"},"cell_type":"code","source":"def timer(func):\n    def wrapper(*args, **kwargs):\n        ts = time.time()\n        results = func(*args, **kwargs)\n        te = time.time()\n        print(\"Time to execute {} = {} seconds\".format(func.__name__, te - ts))\n        return results\n              \n    return wrapper\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nnltk_stemmer = PorterStemmer()\nnltk_lemma = WordNetLemmatizer()\n\n\nChartParams = namedtuple('ChartParams', ['title', 'xticks', 'yticks', 'xlabel', 'ylabel', 'colors', 'is_horizontal', 'min_max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53720e7dc334ff84a7d0715dfaa4e84062484a9d"},"cell_type":"code","source":"# @TODO, for future refence\n# def currency(x, pos):\n#     \"\"\"The two args are the value and tick position\"\"\"\n#     if x >= 1e6:\n#         s = '${:1.1f}M'.format(x*1e-6)\n#     else:\n#         s = '${:1.0f}K'.format(x*1e-3)\n#     return s\n\ndef formatNum(num):\n    if isinstance(num, numbers.Integral):\n        return num\n    return \"{0:.3f}\".format(num)\n    \ndef barplot(ax, data, chart_params):\n    \"\"\"\n    Plots a horizontal or vertical barplot\n    Parameters\n    -------------\n    ax - the figure axes\n    \"\"\"\n    \n    N = len(data)\n    pos = np.arange(N)\n    height=0.8\n    #formatter = FuncFormatter(currency)\n\n    if chart_params.is_horizontal:\n        if len(chart_params.colors) > 0:\n            rects = ax.barh(pos, data, align='center', height=height, color=chart_params.colors)\n        else:\n            rects = ax.barh(pos, data, align='center', height=height)\n\n        ax.set_title(chart_params.title, size=14)\n        ax.xaxis.grid(True, linestyle='--', which='major',color='grey', alpha=.25)\n        ax.set_yticks(pos)\n        ax.set_yticklabels(chart_params.yticks)\n        ax.set(xlim=[chart_params.min_max[0], chart_params.min_max[1]], xlabel=chart_params.xlabel, ylabel=chart_params.ylabel)\n        \n        for i, rect in enumerate(rects):\n            rect_width = rect.get_width()\n            xloc = rect_width if rect_width < 1 else int(rect_width)*0.98\n            yloc = rect.get_y() + rect.get_height()/2.0\n            ax.text(xloc, yloc, formatNum(data[i]), horizontalalignment='right', verticalalignment='center', color='white', weight='bold', clip_on=True)\n\n            #ax.xaxis.set_major_formatter(formatter)\n    else:\n        rects = ax.bar(pos, data, width, color=colors)\n        ax.set_xticks(pos)\n        ax.set_xticklabels(chart_params.xticks)\n        for tick in ax.get_xticklabels():\n            tick.set_rotation(90)\n            \n        for i, rect in enumerate(rects):\n            xloc = rect.get_x() + 0.02\n            yloc = rect.get_height()\n            ax.text(xloc, yloc, str(data[i]), weight='bold', color='b')\n            \ndef getMainIngredientsByCuisine(tfm, vectorizer, target):\n    results = {}\n    for cuisine in np.unique(train_df.cuisine):\n        ids = np.where(target == cuisine)[0]\n        data = tfm[ids,:].toarray()\n        tfs = np.array(data.sum(axis=0)).ravel()\n        indices = np.argsort(-tfs)\n        names = []\n        counts = []\n        vocab = list(six.iteritems(vectorizer.vocabulary_))\n        for i, index in enumerate(indices):\n            names.append([tup[0] for tup in vocab if tup[1] == index][0])\n            counts.append(tfs[index])\n        results[cuisine] = zip(names, counts)\n        \n    return results\n\ndef getFrequencyDistribution(words_list, ascending=False):\n    '''\n        Calculate the occurrency of each token in a list\n        Parameters\n        ------------\n        words_list : array\n                    list of tokens occurring more than once\n        ascending  : boolean, default=False\n                    the order in which the results is sorted\n    '''\n    vocab = defaultdict()\n    vocab.default_factory = vocab.__len__\n\n    for tokens in words_list:\n        if len(tokens) == 0:\n            continue\n        for token in tokens:\n            vocab[token] += 1\n        \n    vocab = dict(vocab)\n    terms = np.array(list(vocab.keys()))\n    values = np.array(list(vocab.values()))\n    flag = -1\n    if ascending:\n        flag = 1\n    indices = np.argsort(flag*values)\n    return list(zip(terms[indices], values[indices]))\n\ndef find_indices_of_word_groups(vect, words_list):\n    words_indices = [vect.vocabulary_.get(w) for w in words_list]\n    return words_indices\n   \ndef get_vocab_subset(words_list, tfm, vect):\n    words_indices = find_indices_of_word_groups(vect, words_list)\n    all_indices = np.array([tup[1] for tup in sorted(six.iteritems(vect.vocabulary_))])\n    kept_indices = [indx for indx in all_indices if indx not in words_indices]\n    X1 = tfm[:, kept_indices]\n    print(tfm.shape, X1.shape)\n    return X1\n\ndef fit_and_score(X, target, estimator):\n    X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.33, random_state=42)\n    print(\"Training on {} features\".format(X_train.shape))\n    clf = copy.deepcopy(estimator)\n    clf = clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    predict_proba = clf.predict_proba(X_test)\n    print(accuracy_score(y_test, predictions) * 100)\n    print(log_loss(y_test, predict_proba))\n    return clf\n\ndef append_column_with_TFM(X, data_col):\n    '''\n    Arguments\n    -----------\n    X - term frequency matrix\n    data_col - 1D array to be appended\n    '''\n    if sp.issparse(X):\n        X = X.toarray()\n    return np.concatenate((X, data_col), axis=1)\n\ndef get_word_occurrence_by_cuisine(vect, tfm, target, words_list):\n    '''\n        Given a list of words finds the occurrence in each cuisine\n        Parameters\n        --------------\n        target - the target values (list of cuisines)\n        words_list - list of unique words in the vocabulary from countvectorizer\n    '''\n    words_indices = find_indices_of_word_groups(vect, words_list)\n    unique_targets = np.unique(target)\n    results = []\n    labels = []\n    for i, label in enumerate(unique_targets):\n        cuisine_df = tfm[np.where(target == label)]\n        tfs = cuisine_df.sum(axis=0)\n        results.append([cuisine_df[:, indx].sum() for indx in words_indices])\n        labels.append(label)\n        \n    return pd.DataFrame(results, columns=words_list, index=labels)\n\ndef plot_occurrence_across_cuisines(vect, tfm, word):\n    df = get_word_occurrence_by_cuisine(vect, tfm, target, [word])\n    \n    fig, ax = plt.subplots(figsize=(7, 5))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, wspace=.35, hspace=.35)\n    chart_params = ChartParams('Count of ' + word, '', df[word].index.values, '', '', ['#15CBCB'], True, (0, max(df[word].values)))\n    barplot(ax, df[word].values, chart_params)\n    plt.show()\n    \n_uppercase_pat = re.compile(r'[A-Z]\\w+')\n    \ndef plotMainIngredients(tfm, vect, N):\n    n_row = 5\n    n_col = 4\n    colors = ['#A66AF7', '#3290F5', '#EE1F12', \"#8156A4\"]\n    ingredientsByCounts = getMainIngredientsByCuisine(tfm, vect, target)\n\n    fig, axarr = plt.subplots(n_row, n_col, figsize=(5 * n_col, 10 * n_row), squeeze=False)\n    #plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, wspace=.35, hspace=.35)\n\n    for i in range(n_row):\n        groups = list(train_df.groupby('cuisine').groups)[i * n_col:(i * n_col) + n_col]\n        for indx, item in enumerate(groups):\n            d = list(ingredientsByCounts[item])[:N]\n            values = [item[1] for item in d]\n            chart_params = ChartParams(item + ' (n=1)', '', [item[0] for item in d], '', '', colors[indx], True)\n            barplot(axarr[i][indx], [item[1] for item in d], chart_params)\n\n    plt.show()\n \ndef get_uniq_ingredients(vect):\n    labels = []\n    values = []\n    for cuisine in np.unique(target):\n        ids = np.where(target == cuisine)[0]\n        d = tfm[ids]\n        sum_words = np.array(d.sum(axis=0)).ravel()\n        word_freq = [(word, sum_words[indx]) for word, indx in vect.vocabulary_.items()]\n        word_freq = [tup for tup in word_freq if tup[1] > 0]\n        labels.append(cuisine)\n        values.append(len(word_freq))\n        \n    return labels, values\n\ndef get_variations_in_ingredients(ingredients_txt, word_list):\n    labels = []\n    matches = []\n    for word in word_list:\n        regex = re.compile(r\"\\b[a-z][a-z]+\\s+\" + word + r\"\\s*[a-z]*\\b\")\n        matches.append(list(set(regex.findall(ingredients_txt))))\n        labels.append(word)\n    \n    indices = np.where(np.array(list(map(len, matches))))[0]\n    return np.array(labels).take(indices), np.array(matches).take(indices)\n\ndef sortListByFreq(vect, tfm, word_list, ascending=False):\n    matrix_indices = [vect.vocabulary_.get(word) for word in word_list]\n    matrix_indices = [num for num in matrix_indices if num is not None]\n    sum_words = np.array(tfm[:, matrix_indices].sum(axis=0)).ravel()\n    sign = -1\n    if ascending:\n        sign = 1\n    sort_indices = np.argsort(sign * sum_words)\n    values = np.array(sum_words)[np.argsort(-sum_words)]\n    names = np.array(word_list)[np.argsort(-sum_words)]\n    return list(zip(names, values))\n\ndef plotFrequency(ax, data, N, chartTitle):\n    '''\n    Plot frequency of tokens\n    Parameters\n    --------------\n    data - array of tokens\n    chartParams - parameters of chart,(e.g. xlabels, ylabels)\n    figsize - width and height of chart figure\n    N - total number of records to show in plot\n    '''\n    vocab = getFrequencyDistribution(data)\n    labels = [item[0] for item in vocab][:N]\n    counts = [item[1] for item in vocab][:N]\n    chart_params = ChartParams(chartTitle, '', labels, '', '', [CHART_COLOR], True, (0, max(counts)))\n    barplot(ax, counts, chart_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f86bf1734e39b73be6295c0ab397d4691d843874"},"cell_type":"code","source":"with open(\"../input/train.json\", 'r') as file:\n    data_train = json.load(file)\n    \nwith open(\"../input/test.json\", 'r') as file:\n    data_test = json.load(file)\n    \nCHART_COLOR = '#B9D132'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f00380ed2b1f7bf4c62f8b5670d40a82dec3b01"},"cell_type":"code","source":"train_df = pd.DataFrame(data_train)\ntest_df = pd.DataFrame(data_test)\n\nclf = LogisticRegression(multi_class='warn', random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e5308dd184e6b9a474e2d4ff2964fdfe46571b7"},"cell_type":"code","source":"train_df['concat_ingredients'] = train_df['ingredients'].apply(lambda row: \",\".join(row))\ntest_df['concat_ingredients'] = test_df['ingredients'].apply(lambda row: \",\".join(row))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"457c2498e02ff978915525060939c2d40a91185b"},"cell_type":"code","source":"corpus = train_df['concat_ingredients']\ntarget = train_df['cuisine']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c01998d38937cc84293bfbd7b453a1fc9f9c56fe"},"cell_type":"markdown","source":">### Treat each ingredient as a token and build the vocabulary ###"},{"metadata":{"trusted":true,"_uuid":"e337efeff6130e3eabdd463bee7158dc42fd5b90"},"cell_type":"code","source":"def clean_text(doc):\n    doc = re.sub(r'\\bmi\\b', '', doc)\n    doc = re.sub(r'\\bfresh\\b', '', doc)\n    return doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55d7f306f58c06c6e795285607b7fa7848f61791"},"cell_type":"code","source":"vect = CountVectorizer(preprocessor=clean_text, tokenizer=lambda doc: [word.strip() for word in doc.split(',')])\ntfm = vect.fit_transform(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"772118267edaa886be3b381c04bcc140c0dec571"},"cell_type":"code","source":"labels, values = get_uniq_ingredients(vect)\nfig, ax = plt.subplots(figsize=(10, 8))\nchart_params = ChartParams('#Unique ingredients', '', np.array(labels)[np.argsort(values)], '', '', ['#B9D132'], True, (0, max(values)))\nbarplot(ax, np.array(values)[np.argsort(values)], chart_params)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72575f40b10d354b30f78effdc8a35d11dab460e"},"cell_type":"code","source":"train_df['total_ingredients'] = train_df['ingredients'].apply(lambda row: len(row))\ntrain_df['n_words'] = train_df['ingredients'].apply(lambda row: sum([len(w.split()) for w in row]))\ntrain_df['n_uppercase_words'] = train_df['ingredients'].apply(lambda row: sum([1 if w[0].isupper() else 0 for w in row]))\ntrain_df['n_digits'] = train_df['ingredients'].apply(lambda row: sum([1 if w[0].isdigit() else 0 for w in row]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b160c1a537e6e3f9864f06a4b1649ac408bb184"},"cell_type":"code","source":"train_df['hyphen_ingredients'] = train_df['concat_ingredients'].apply(lambda row: re.compile(r'\\b[a-z][a-z]+[-][a-z]+\\b').findall(row))\ntrain_df['negations'] = train_df['concat_ingredients'].apply(lambda row: re.compile(r'\\b(non[\\s-][a-z\\s]+|no[\\s-][a-z\\s]+|not[\\s-][a-z\\s]+)\\b').findall(row))\ntrain_df['quoted_ingredients'] = train_df['concat_ingredients'].apply(lambda row: re.compile(r'\\b([\\w]+\\'[\\w]+)\\b').findall(row))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"616488dd5f72544305dc1d9f2357ad7e15794d18"},"cell_type":"code","source":"train_df['punctuations'] = train_df['ingredients'].apply(lambda row: list(set(np.hstack([re.compile(r'\\W').findall(w) for w in row]))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a158fd182de2f86eb4cfbdca9993467df29eb56e"},"cell_type":"code","source":"nrows = 4\nncols = 1\nfig, axarr = plt.subplots(nrows, ncols, figsize=(5, 8), squeeze=False)\nplt.subplots_adjust(bottom=0, left=.001, right=.99, top=.90, wspace=.35, hspace=.35)\n\nplotFrequency(axarr[0][0], train_df['hyphen_ingredients'].values, 5, '# Hyphenized ingredients')\nplotFrequency(axarr[1][0], train_df['negations'].values, 5, '# Negation ingredients')\nplotFrequency(axarr[2][0], train_df['quoted_ingredients'].values, 5, '# Quoted ingredients')\nplotFrequency(axarr[3][0], train_df['punctuations'].values, 5, '# Punctuations')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21bde36817d9288cbbacaf4d736710003ac246fb"},"cell_type":"code","source":"# plot_occurrence_across_cuisines(vect, tfm, 'gluten free barbecue sauce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2684cf3e8c988fe3fb0075f250427cf3482e1ce3"},"cell_type":"code","source":"def preprocess(doc):\n    doc = re.sub(r\"\\s+\",\" \", doc, flags = re.I)  # remove spaces\n    doc = re.sub(r'\\W', ' ', doc, flags = re.I)  # remove non-words\n    return doc\n\ndef preprocess_ngrams(doc):\n    doc = re.sub(r\"\\s+\",\" \", doc, flags = re.I)  # remove spaces\n    return doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89ff691ecd114be95b6a7ffd64034379a6007726"},"cell_type":"code","source":"class CustomVectorizer(CountVectorizer):\n    def build_tokenizer(self):\n        return lambda doc: doc.split(\",\")\n    \ndef tokenizer(doc):\n    ingredients = doc.split(\",\")\n    results = []\n    results_extend = results.extend\n    for ingredient in ingredients:\n        words = ingredient.split()\n        for word in words:\n            if word[0].isdigit():\n                continue\n            if len(word) == 1:\n                continue\n            results_extend([word.lower()])\n    return results\n\nspace_join = \" \".join\n\ndef tokenizer_ngrams(doc):\n    tokens = list(ingredient.strip() for ingredient in doc.split(\",\"))\n    results = []\n    results_extend = results.extend\n    for token in tokens:\n        token = re.sub(r'\\W', ' ', token, flags = re.I)\n        w_splits = token.split()\n        results_extend(w_splits)\n        \n        if len(w_splits) in [1, 2]:\n            results_extend([token])\n            \n        if len(w_splits) in [3]:\n            results_extend([space_join(w_splits[1:])])\n            \n        if len(w_splits) in [4]:\n            results_extend([space_join(w_splits[1:3])])\n            \n        if len(w_splits) in [5, 6]:\n            results_extend([token])\n            \n    return results\n\ndef tokenizer_all(doc):\n    tokens = list(ingredient.strip() for ingredient in doc.split(\",\"))\n    unigrams = []\n    for word in tokens:\n        if len(word.split()) >= 1:\n            unigrams.append(word.split()[-1])\n        else:\n            unigrams.append(word)\n    \n    results = []\n    results_extend = results.extend\n    for i, item in enumerate(unigrams):\n        if i == len(unigrams) - 1:\n            continue\n        results_extend([space_join([unigrams[i], unigrams[i + 1]])])\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2c574f7ef294420a7fe3dd118aaedf4648f9f93","scrolled":false},"cell_type":"code","source":"vectorizer = CountVectorizer(preprocessor=preprocess, tokenizer=tokenizer, lowercase=False, stop_words=set(['brown', 'fresh', 'purple']))\nX = vectorizer.fit_transform(corpus)\n\nvectorizer_ngrams = CountVectorizer(preprocessor=preprocess_ngrams, tokenizer=tokenizer_ngrams, min_df=2, stop_words='english')\nX_ngrams = vectorizer_ngrams.fit_transform(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6e7b9aac49d50dcc00e77c366f22ff26dfb371b"},"cell_type":"code","source":"# vectorizer_comb = CountVectorizer(tokenizer=tokenizer_all)\n# X_comb = vectorizer_comb.fit_transform(corpus)\n\n#cooking,condensed, converted, cokked, cooking, ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e26d93f7438d9e314b93ba8d51f8dda84be929f1"},"cell_type":"code","source":"print(train_df.loc[np.random.choice(train_df.index), 'ingredients'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be1d5f1dcd6263281516710678490ec054872553"},"cell_type":"code","source":"train_df['no-oils'] = train_df.apply(lambda row: 1 if len([w for w in row['ingredients'] if len(re.split(r'\\boil\\b', w)) == 1]) == row['total_ingredients'] else 0, axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c35f5b696e1f030bb014ea9e0b00d32a07f7f697"},"cell_type":"code","source":"# print(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f9b6db075027198d6ec849353e2694f77539713"},"cell_type":"markdown","source":">### Fit Model on term-frequency matrix ###"},{"metadata":{"trusted":true,"_uuid":"b19d07c37cc34e6be725be147dc0ad19d5e4c7af"},"cell_type":"code","source":"lr_unigrams = fit_and_score(X, target, clf)\n\n# 78.29498704860582\n# 0.7706321294271727","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9374df7026ec6bd6ca60a19ef575fcfb2dc63dd6"},"cell_type":"code","source":"lr_ngrams = fit_and_score(X_ngrams, target, clf)\n\n# 78.58448880085326\n# 0.7635537827858768","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d90d427ea1f8ec5d410874ae9a27abd804f5fac0"},"cell_type":"code","source":"lr_all = fit_and_score(append_column_with_TFM(X_ngrams, train_df[['no-oils']].values), target, clf)\n\n# 78.37117172024989\n# 0.7713099458169165","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5828b1a8039d54aaf9c1e154322e1fecc1addc21"},"cell_type":"markdown","source":"### Test ###"},{"metadata":{"trusted":true,"_uuid":"8502768a41a4da5fcaacd9b073c8654b4a76bbff"},"cell_type":"code","source":"test_df['total_ingredients'] = test_df['ingredients'].apply(lambda row: len(row))\ntest_df['no-oils'] = test_df.apply(lambda row: 1 if len([w for w in row['ingredients'] if len(re.split(r'\\boil\\b', w)) == 1]) == row['total_ingredients'] else 0, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49f75ea8b908d694744a34ca9b0aa754cfc00122"},"cell_type":"code","source":"feature_vectors = vectorizer_ngrams.transform(test_df['concat_ingredients'])\nsubmission = pd.DataFrame({\"cuisine\": lr_all.predict(append_column_with_TFM(feature_vectors, test_df[['no-oils']].values)), \"id\": test_df['id']})\nsubmission.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49361c0fc60b51a20da5b0116db5c0d4a6c5d559"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}