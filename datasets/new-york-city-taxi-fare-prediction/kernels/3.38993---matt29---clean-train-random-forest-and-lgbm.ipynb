{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68886efb92b793c7a05d6365c5133e2342183497"},"cell_type":"markdown","source":"# If we want to use the all dataset (55 m rows)\nI've got some problems when loading the all dataset (it runs out of time), so I found this interesting method in this kernel: https://www.kaggle.com/szelee/how-to-import-a-csv-file-of-55-million-rows"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"TRAIN_PATH = '../input/train.csv'\n\n# Set columns to most suitable type to optimize for memory usage\ntraintypes = {'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\ncols = list(traintypes.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84f636c49ac678a46e77c8b556c3cbb3ec54bfb2"},"cell_type":"code","source":"with open(TRAIN_PATH) as file:\n    n_rows = len(file.readlines())\n\nprint (f'Exact number of rows: {n_rows}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f399e80d53821f822c8688373a35fbfed4a01b99"},"cell_type":"code","source":"chunksize = 5_000_000 # 5 million rows at one go. Or try 10 million\ntotal_chunk = n_rows // chunksize + 1\nprint(f'Chunk size: {chunksize:,}\\nTotal chunks required: {total_chunk}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7274275c55bab15ef28eb0059e85e368cabed86"},"cell_type":"code","source":"df_list = [] # list to hold the batch dataframe\ni=0\n\nfor df_chunk in pd.read_csv(TRAIN_PATH, usecols=cols, dtype=traintypes, chunksize=chunksize):\n    \n    i = i+1\n    # Each chunk is a corresponding dataframe\n    print(f'DataFrame Chunk {i:02d}/{total_chunk}')\n    \n    # Neat trick from https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost\n    # Using parse_dates would be much slower!\n    df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n    df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n    \n    # Can process each chunk of dataframe here\n    # clean_data(), feature_engineer(),fit()\n    \n    # Alternatively, append the chunk to list and merge all\n    df_list.append(df_chunk) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fe172a6b0b07161c5ae227240044cf418065ab4"},"cell_type":"code","source":"# Merge all dataframes into one dataframe\ntrain_df = pd.concat(df_list)\n\ndel df_list\n\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73c7b9ebc71d81e14159efdd5fedc94f17493e5a"},"cell_type":"code","source":"display(train_df.tail())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d60c958718445d249cc3f6b361b4f8ab2aac90af"},"cell_type":"markdown","source":"# Otherwise, let's use only 10 million rows"},{"metadata":{"trusted":true,"_uuid":"9d2cf9e94447aef729e2e854e91988fb50b2d914"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", nrows = 1000000)\ntest_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"675d9ac7f2d5acdede257fe84a81846180e529b3"},"cell_type":"code","source":"# Search for missing values\nprint(train_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34f112e2430a8332db04db3295d59655dc4c5711"},"cell_type":"code","source":"print('Old size: %d' % len(train_df))\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')\nprint('New size: %d' % len(train_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26c52dbd6fb3bfca0d84dd508893662d15b42ee6"},"cell_type":"code","source":"train_df['fare_amount'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"813d66a8fbcf3f75cbd8767a760f3ffcc638f9d3"},"cell_type":"code","source":"train_df['fare_amount'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49a826ecf155bbcd47b34484bc0e1219d1feb961"},"cell_type":"code","source":"# Delete the negative values\ntrain_df = train_df.drop(train_df[train_df['fare_amount'] < 0].index, axis=0)\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e10bde42210da3a557b57bdb3fd77e48886494c"},"cell_type":"code","source":"train_df['passenger_count'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6a32f7cd1223f7ca44af2ba9d5d7f93447de1e5"},"cell_type":"code","source":"# Of course it's impossible that 208 passengers are on a single taxy. Let's delete this row\ntrain_df = train_df.drop(train_df[train_df['passenger_count'] == 208].index, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be205682c361961de88174488fc0a6beafd75d03"},"cell_type":"code","source":"# Just curiosity, let's see the correlation map (actually the number of features is small, so nothing very useful here)\nsns.set_style('white')\nsns.set_context(\"paper\",font_scale=2)\ncorr = train_df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11,9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.3, center=0,\n           square=True, linewidths=0.5, cbar_kws={\"shrink\":0.5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"561274e2ca1040ab00cff3e86490189385f41e48"},"cell_type":"code","source":"# Convert to datetime, so that we can split them in year, month, date, day and hour\ntrain_df['key'] = pd.to_datetime(train_df['key'])\ntrain_df['pickup_datetime']  = pd.to_datetime(train_df['pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c51aa834059708828c22bfdd8993e19eab408cc5"},"cell_type":"code","source":"test_df['pickup_datetime']  = pd.to_datetime(test_df['pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f87960f64bbd7d4ab0db4966d1e00183796770ca"},"cell_type":"code","source":"data = [train_df, test_df]\nfor i in data: \n    i['Year'] = i['pickup_datetime'].dt.year\n    i['Month'] = i['pickup_datetime'].dt.month\n    i['Date'] = i['pickup_datetime'].dt.day\n    i['Day of Week'] = i['pickup_datetime'].dt.dayofweek\n    i['Hour'] = i['pickup_datetime'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"038952ee5896216909c49ebc2c439de82a9dc621"},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bada65cb4d6a13174f38d8cfa171e84a0f3a148"},"cell_type":"code","source":"test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cbba2b0e43084d8c5b75cd9dc22f40461bb7ead"},"cell_type":"code","source":"train_df = train_df.drop(['key','pickup_datetime'], axis = 1)\ntest_df = test_df.drop(['key', 'pickup_datetime'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16f4f2bd550c7b630ecf6e5f28c6ef20626ab8f8"},"cell_type":"code","source":"# Finally, prepare our data for the training phase\nx_train = train_df.iloc[:, train_df.columns != 'fare_amount']\ny_train = train_df['fare_amount'].values\nx_test = test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04595ad66dfdb78b8cd55680a574d2e3de01cd3d"},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38646f218c2f5000dc5a1ff2b0dc439a24edfaa7"},"cell_type":"markdown","source":"## 1) Random Forest (first model I tried) | Score: 3.69"},{"metadata":{"trusted":true,"_uuid":"fdad427d51af02349757f516df4cc02ba69323b1"},"cell_type":"code","source":"'''from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(x_train, y_train)\nrf_predict = rf.predict(x_test)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f17cd77101f605c057499c6c79f9041c137b33a2"},"cell_type":"code","source":"'''submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = rf_predict\nsubmission.to_csv('rnd_fst.csv', index=False)\nsubmission.head(20)'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f598f64407091987b68221780532ec751de3b80f"},"cell_type":"markdown","source":"## 2) LGBM"},{"metadata":{"trusted":true,"_uuid":"dd40e79a6473dc277aab27c203ad810cc58611b1"},"cell_type":"code","source":"import lightgbm as lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7212043734727c8a06dc413a749c9eeb89eac98"},"cell_type":"code","source":"lgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07b3310dc5bf2d0815879c1861d17eeac03c519e"},"cell_type":"code","source":"pred_test_y = np.zeros(x_test.shape[0])\n\ntrain_set = lgbm.Dataset(x_train, y_train, silent=True)\n\nmodel = lgbm.train(lgbm_params, train_set = train_set, num_boost_round=300)\n\npred_test_y = model.predict(x_test, num_iteration = model.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43622f77d31d1cff1c94f404a0ec8c6796ba3992"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = pred_test_y\nsubmission.to_csv('lgbm_submission.csv', index=False)\nsubmission.head(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}