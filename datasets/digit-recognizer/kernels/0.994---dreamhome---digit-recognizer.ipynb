{"cells":[{"metadata":{"_uuid":"3171bcc7898a07cbcb9fac9adf6061e9df20db3d"},"cell_type":"markdown","source":"# data preparation"},{"metadata":{"trusted":true,"_uuid":"847eee89cd535ce312226065d197aec905e325e6"},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90a7d3e3b47e4022d964ebf1340f620e9b0a2f6c"},"cell_type":"code","source":"import os\nimport pandas as pd\n\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\ndata_dir = '../input/'\n\ntrain_path = os.path.join(data_dir, 'train.csv')\ntest_path = os.path.join(data_dir, 'test.csv')\n\n# 1. load data\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\n\nprint(len(train), len(test))\n\ny_train = train['label']\nx_train = train.drop(labels=['label'], axis=1)\n\ndel train\n\n# 2. check for null and missing values\n# print(x_train.isnull().any())\n# print(test.isnull().any())\n\n# 3. normalization\nx_train /= 255.\ntest /= 255.\n\n# 4. reshape\nx_train = x_train.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)\n\n# 5. label encoding\ny_train = to_categorical(y_train)\n\n# 6. split training and validation set\nrandom_seed = 2\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=random_seed)\n\nx_train, y_train\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ec65b139c0f99ddb9462fe2caa72acda5f7807f"},"cell_type":"markdown","source":"# build model"},{"metadata":{"trusted":true,"_uuid":"91e8b3085903635c514bb266ba285033fc1e6ef6"},"cell_type":"code","source":"from keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras import losses\n\nmodel = models.Sequential()\n\nmodel.add(layers.Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n                            activation='relu', input_shape=(28, 28, 1)))\nmodel.add(layers.Conv2D(filters=32, kernel_size=(5, 5), padding='Same',\n                            activation='relu'))\nmodel.add(layers.MaxPool2D(pool_size=(2, 2)))\nmodel.add(layers.Dropout(0.25))\n\nmodel.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n                            activation='relu'))\nmodel.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n                            activation='relu'))\nmodel.add(layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(layers.Dropout(0.25))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation=\"relu\"))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(10, activation=\"softmax\"))\n\noptimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-8, decay=0.0)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss=losses.categorical_crossentropy,\n    metrics=['acc'])\n\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93e4e6a6bae7bd45dc0c189309c7246b9f1469bd"},"cell_type":"markdown","source":"# train and predict"},{"metadata":{"trusted":true,"_uuid":"34a1a73419a39f9a8d098cfc17cabed484c13280"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\n# data augmentation\ndatagen = ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=10,\n    zoom_range=0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=False,\n    vertical_flip=False)\ndatagen.fit(x_train)\n\n# annealing method\nlearning_rate_reduction = ReduceLROnPlateau(\n    monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=1e-5)\n\n# fit\nepochs = 10\nbatch_size = 86\nhistory = model.fit_generator(\n    datagen.flow(x_train, y_train, batch_size=batch_size),\n    epochs=epochs,\n    validation_data=(x_val, y_val),\n    verbose=2,\n    steps_per_epoch=x_train.shape[0] // batch_size,\n    callbacks=[learning_rate_reduction])\n\n# plot\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title(\"Training and validation accuracy\")\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title(\"Training and validation loss\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34161a9d2d8c27ff20e77237b9dec946c09892c8"},"cell_type":"markdown","source":"# predict"},{"metadata":{"trusted":true,"_uuid":"b808b21c78229efcd95e0ad280a3c0f4689cfc14"},"cell_type":"code","source":"# predict results\nresults = model.predict(test)\n\n# select the index with the maximum probability\nresults = np.argmax(results, axis=1)\n\nresults = pd.Series(results, name=\"Label\")\n\nsubmission = pd.concat(\n    [pd.Series(range(1, len(test) + 1), name=\"ImageId\"), results], axis=1)\n\nsubmission.to_csv(\"results.csv\", index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}