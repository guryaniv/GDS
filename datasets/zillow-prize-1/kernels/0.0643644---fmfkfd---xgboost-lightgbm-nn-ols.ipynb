{"cells": [{"metadata": {"_cell_guid": "95e56fb1-1226-48cb-ae4b-c4cb44c3f37b", "_uuid": "b22a1bfde673d92d11e29297cbfea55c8b7ea996"}, "execution_count": null, "source": ["import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "from sklearn.preprocessing import LabelEncoder\n", "import lightgbm as lgb\n", "import gc\n", "from sklearn.linear_model import LinearRegression\n", "import random\n", "import datetime as dt\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.layers import Dropout, BatchNormalization\n", "from keras.layers.advanced_activations import PReLU\n", "from keras.layers.noise import GaussianDropout\n", "from keras.optimizers import Adam\n", "from keras.wrappers.scikit_learn import KerasRegressor\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.preprocessing import Imputer\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "d25ff7ea-e183-4ba4-bbae-fb51ad698046", "_uuid": "b178704aa03056c8088e52f080d180ad2921d169", "collapsed": true}, "execution_count": null, "source": ["# Parameters\n", "FUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n", "\n", "XGB_WEIGHT = 0.6200\n", "BASELINE_WEIGHT = 0.0100\n", "OLS_WEIGHT = 0.0620\n", "NN_WEIGHT = 0.0800\n", "\n", "XGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n", "\n", "BASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "e2905bb5-59cb-4fa4-b193-176a7839f8a9", "_uuid": "6f6110456af0dd26b636f4e04820f7bf07cec1a2"}, "execution_count": null, "source": ["print( \"\\nReading data from disk ...\")\n", "prop = pd.read_csv('../input/properties_2016.csv')\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\")"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "1bd0f5d8-e0f2-4af2-ac36-66d6c67505e8", "_uuid": "fadad10d71f355b486878779af078460838704c4"}, "execution_count": null, "source": ["#LightGBM\n", "print( \"\\nProcessing data for LightGBM ...\" )\n", "for c, dtype in zip(prop.columns, prop.dtypes):\t\n", "    if dtype == np.float64:\n", "        prop[c] = prop[c].astype(np.float32)\n", "\n", "df_train = train.merge(prop, how='left', on='parcelid')\n", "\n", "\n", "missing_perc_thresh = 0.98\n", "num_rows = df_train.shape[0]\n", "for c in df_train.columns:\n", "    #print (c)\n", "    num_missing = df_train[c].isnull().sum()\n", "    if num_missing == 0:\n", "        continue\n", "    missing_frac = num_missing / float(num_rows)\n", "    if missing_frac > missing_perc_thresh:\n", "        df_train.drop([c],axis=1)\n", "        #exclude_missing.append(c)\n", "#df_train=df_train.drop(exclude_missing,axis=1)\n", "##39\n", "##excluding having unique value\n", "\n", "# exclude where we only have one unique value :D\n", "for c in df_train.columns:\n", "    num_uniques = len(df_train[c].unique())\n", "    if df_train[c].isnull().sum() != 0:\n", "        num_uniques -= 1\n", "    if num_uniques == 1:\n", "        df_train.drop([c],axis=1)\n", "        #exclude_unique.append(c)\n", "df_train.fillna(df_train.median(),inplace = True)\n", "\n", "#x_train['Ratio_1'] = x_train['taxvaluedollarcnt']/x_train['taxamount']\n", "                         \n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc','propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n", "\n", "y_train = df_train['logerror'].values\n", "print(x_train.shape, y_train.shape)\n", "\n", "\n", "train_columns = x_train.columns\n", "\n", "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n", "    x_train[c] = (x_train[c] == True)\n", "\n", "del df_train; gc.collect()\n", "\n", "x_train = x_train.values.astype(np.float32, copy=False)\n", "d_train = lgb.Dataset(x_train, label=y_train)\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "ab7c3327-6d9c-438d-b033-42c80a1066ae", "_uuid": "b94370b1b8ffc9fcbdcce22b7bc43d0ec39471ac", "collapsed": true}, "execution_count": null, "source": ["\n", "params = {}\n", "params['max_bin'] = 10\n", "params['learning_rate'] = 0.0021 # shrinkage_rate\n", "params['boosting_type'] = 'gbdt'\n", "params['objective'] = 'regression'\n", "params['metric'] = 'l1'          # or 'mae'\n", "params['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\n", "params['bagging_fraction'] = 0.85 # sub_row\n", "params['bagging_freq'] = 40\n", "params['num_leaves'] = 512        # num_leaf\n", "params['min_data'] = 500         # min_data_in_leaf\n", "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n", "params['verbose'] = 0\n", "params['feature_fraction_seed'] = 2\n", "params['bagging_seed'] = 3\n", "\n", "np.random.seed(0)\n", "random.seed(0)\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "065ef06d-8935-460b-916b-58b4a21cb836", "_uuid": "a5371b188ced451623e923cc687287966281b836"}, "execution_count": null, "source": ["print(\"\\nFitting LightGBM model ...\")\n", "clf = lgb.train(params, d_train, 430)\n", "\n", "del d_train; gc.collect()\n", "del x_train; gc.collect()\n", "\n", "print(\"\\nPrepare for LightGBM prediction ...\")\n", "print(\"   Read sample file ...\")\n", "sample = pd.read_csv('../input/sample_submission.csv')\n", "print(\"   ...\")\n", "sample['parcelid'] = sample['ParcelId']\n", "print(\"   Merge with property data ...\")\n", "df_test = sample.merge(prop, on='parcelid', how='left')\n", "print(\"   ...\")\n", "del sample, prop; gc.collect()\n", "print(\"   ...\")\n", "#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\n", "x_test = df_test[train_columns]\n", "print(\"   ...\")\n", "del df_test; gc.collect()\n", "print(\"   Preparing x_test...\")\n", "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n", "    x_test[c] = (x_test[c] == True)\n", "print(\"   ...\")\n", "x_test = x_test.values.astype(np.float32, copy=False)\n", "\n", "print(\"\\nStart LightGBM prediction ...\")\n", "p_test = clf.predict(x_test)\n", "\n", "del x_test; gc.collect()\n", "\n", "print( \"\\nUnadjusted LightGBM predictions:\" )\n", "print( pd.DataFrame(p_test).head() )\n", "\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "05b90bf4-34cf-4eb1-af53-45a108c7d1ad", "_uuid": "00cc1b279ccd8676d8e9b840aacc3824c91c1063"}, "execution_count": null, "source": ["#XGBOOST\n", "\n", "print( \"\\nRe-reading properties file ...\")\n", "properties = pd.read_csv('../input/properties_2016.csv')\n", "\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "37f55393-bd08-4635-ac65-011c2f17f0f2", "_uuid": "26aaf960e2d1399b1bd3431f8bb7b1ea4e5f01f8"}, "execution_count": null, "source": ["print( \"\\nProcessing data for XGBoost ...\")\n", "for c in properties.columns:\n", "    properties[c]=properties[c].fillna(-1)\n", "    if properties[c].dtype == 'object':\n", "        lbl = LabelEncoder()\n", "        lbl.fit(list(properties[c].values))\n", "        properties[c] = lbl.transform(list(properties[c].values))\n", "\n", "train_df = train.merge(properties, how='left', on='parcelid')\n", "missing_perc_thresh = 0.98\n", "num_rows = train_df.shape[0]\n", "for c in train_df.columns:\n", "    #print (c)\n", "    num_missing = train_df[c].isnull().sum()\n", "    if num_missing == 0:\n", "        continue\n", "    missing_frac = num_missing / float(num_rows)\n", "    if missing_frac > missing_perc_thresh:\n", "        train_df.drop([c],axis=1)\n", "        #exclude_missing.append(c)\n", "#df_train=df_train.drop(exclude_missing,axis=1)\n", "##39\n", "##excluding having unique value\n", "\n", "# exclude where we only have one unique value :D\n", "for c in train_df.columns:\n", "    num_uniques = len(train_df[c].unique())\n", "    if train_df[c].isnull().sum() != 0:\n", "        num_uniques -= 1\n", "    if num_uniques == 1:\n", "        train_df.drop([c],axis=1)\n", "x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "x_test = properties.drop(['parcelid'], axis=1)\n", "# shape        \n", "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "a74cf9d6-5f16-4ec5-a6a4-653ad843b2df", "_uuid": "787f16a16229f1809cc9795ec1404d7be1800ac0"}, "execution_count": null, "source": ["#outliers\n", "train_df=train_df[ train_df.logerror > -0.4 ]\n", "train_df=train_df[ train_df.logerror < 0.419 ]\n", "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "y_train = train_df[\"logerror\"].values.astype(np.float32)\n", "y_mean = np.mean(y_train)\n", "\n", "print('After removing outliers:')     \n", "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n", "\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "e199db4b-df9c-48af-9474-fe168da46710", "_uuid": "d44c9357dac59c5d07d3fcfbbb4d81660d4ad599"}, "execution_count": null, "source": ["print(\"\\nSetting up data for XGBoost ...\")\n", "# xgboost params\n", "xgb_params = {\n", "    'eta': 0.037,\n", "    'max_depth': 5,\n", "    'subsample': 0.80,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'lambda': 0.8,   \n", "    'alpha': 0.4, \n", "    'base_score': y_mean,\n", "    'silent': 1\n", "}\n", "\n", "dtrain = xgb.DMatrix(x_train, y_train)\n", "dtest = xgb.DMatrix(x_test)\n", "\n", "num_boost_rounds = 250\n", "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n", "\n", "# train model\n", "print( \"\\nTraining XGBoost ...\")\n", "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n", "\n", "print( \"\\nPredicting with XGBoost ...\")\n", "xgb_pred1 = model.predict(dtest)\n", "\n", "print( \"\\nFirst XGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred1).head() )\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "6f31dd9b-0e9d-4131-88e0-df82592aff38", "_uuid": "1a063bb8ea96163396ca7c1f009bbdb4c9397b62", "collapsed": true}, "execution_count": null, "source": ["print(\"\\nSetting up data for XGBoost ...\")\n", "# xgboost params\n", "xgb_params = {\n", "    'eta': 0.033,\n", "    'max_depth': 6,\n", "    'subsample': 0.80,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'base_score': y_mean,\n", "    'silent': 1\n", "}\n", "\n", "num_boost_rounds = 150\n", "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n", "\n", "print( \"\\nTraining XGBoost again ...\")\n", "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n", "\n", "print( \"\\nPredicting with XGBoost again ...\")\n", "xgb_pred2 = model.predict(dtest)\n", "\n", "print( \"\\nSecond XGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred2).head() )\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "d4f546eb-023f-4559-989d-17c22c59dcef", "_uuid": "405812386b853307eab12e48b169c33ef82ffdf1", "collapsed": true}, "execution_count": null, "source": ["xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n", "#xgb_pred = xgb_pred1\n", "\n", "print( \"\\nCombined XGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred).head() )"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "cbd602b1-141f-4677-a0f9-4e8ad10debd5", "_uuid": "b75d17082f45cc09500acfb9d484fb28f932db76", "collapsed": true}, "execution_count": null, "source": ["del train_df\n", "del x_train\n", "del x_test\n", "del properties\n", "del dtest\n", "del dtrain\n", "del xgb_pred1\n", "del xgb_pred2 \n", "gc.collect()"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "2fc0a8f2-21d5-4b5f-bd7d-3dc2b790bc73", "_uuid": "204d094c0b4d07b1bc9b6ef6b2e32c940d09184a", "collapsed": true}, "execution_count": null, "source": ["#OLS\n", "\n", "np.random.seed(17)\n", "random.seed(17)\n", "\n", "print( \"\\n\\nProcessing data for OLS ...\")\n", "\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n", "properties = pd.read_csv(\"../input/properties_2016.csv\")\n", "submission = pd.read_csv(\"../input/sample_submission.csv\")\n", "print(len(train),len(properties),len(submission))"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "44991967-6227-41e8-9a2d-c4b574f965d8", "_uuid": "ad26df23cf1219c49c4b94428bc9feb45928ae0f", "collapsed": true}, "execution_count": null, "source": ["def get_features(df):\n", "    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n", "    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n", "    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n", "    df['transactiondate'] = df['transactiondate'].dt.quarter\n", "    df = df.fillna(-1.0)\n", "    return df\n", "\n", "def MAE(y, ypred):\n", "    #logerror=log(Zestimate)\u2212log(SalePrice)\n", "    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "894421ea-d590-4a17-b9f5-cbf0e7dfa5d9", "_uuid": "ecb697e01323c7e4af29facddc2e0e892fd70cf5", "collapsed": true}, "execution_count": null, "source": ["train = pd.merge(train, properties, how='left', on='parcelid')\n", "y = train['logerror'].values\n", "test = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\n", "properties = [] #memory\n", "\n", "exc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\n", "col = [c for c in train.columns if c not in exc]\n", "\n", "train = get_features(train[col])\n", "test['transactiondate'] = '2016-01-01' #should use the most common training date\n", "test = get_features(test[col])"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "288c0ae0-653a-4dbb-9c5c-b3b4d3d7ab13", "_uuid": "7128e2ac04f3fd67714732757b2ff753429499c1", "collapsed": true}, "execution_count": null, "source": ["print(\"\\nFitting OLS...\")\n", "reg = LinearRegression(n_jobs=-1)\n", "reg.fit(train, y); print('fit...')\n", "print(MAE(y, reg.predict(train)))\n", "train = [];  y = [] #memory\n", "\n", "test_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\n", "test_columns = ['201610','201611','201612','201710','201711','201712']\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "b2c39b82-22bf-4c98-9f2e-7f0c0901d4e7", "_uuid": "467f3c06677ec641b08b8b0a60a181cb6ba2eecf", "collapsed": true}, "execution_count": null, "source": ["#combining all predictions\n", "\n", "print( \"\\nCombining XGBoost, LightGBM, NN, and baseline predicitons ...\" )\n", "#lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \n", "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - OLS_WEIGHT \n", "lgb_weight0 = lgb_weight / (1 - OLS_WEIGHT)\n", "xgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\n", "baseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n", "#nn_weight0 = NN_WEIGHT / (1 - OLS_WEIGHT)\n", "pred0 = 0\n", "pred0 += xgb_weight0*xgb_pred\n", "pred0 += baseline_weight0*BASELINE_PRED\n", "pred0 += lgb_weight0*p_test\n", "#pred0 += nn_weight0*nn_pred\n"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "e5192adc-957c-48fb-95b8-ebb4647fd868", "_uuid": "347ae511f7e67548760e43a5e255d34474240ca4", "collapsed": true}, "execution_count": null, "source": ["print( \"\\nCombined XGB/LGB/NN/baseline predictions:\" )\n", "print( pd.DataFrame(pred0).head() )\n", "\n", "print( \"\\nPredicting with OLS and combining with XGB/LGB/NN/baseline predicitons: ...\" )\n", "for i in range(len(test_dates)):\n", "    test['transactiondate'] = test_dates[i]\n", "    pred = FUDGE_FACTOR * ( OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0 )\n", "    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n", "    print('predict...', i)\n", "\n", "print( \"\\nCombined XGB/LGB/NN/baseline/OLS predictions:\" )\n", "print( submission.head() )"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "1c03d7c4-8b28-488b-b995-1030b2afdb8a", "_uuid": "4f13c5f6f9c2695146a74076ad173ebacd725541", "collapsed": true}, "execution_count": null, "source": ["##### WRITE THE RESULTS\n", "\n", "from datetime import datetime\n", "\n", "print( \"\\nWriting results to disk ...\" )\n", "submission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n", "\n", "print( \"\\nFinished ...\")"], "cell_type": "code", "outputs": []}], "nbformat": 4, "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1}