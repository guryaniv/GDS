{"cells":[{"metadata":{"_uuid":"e6a0ec9537eb481699c3016030229b4799f1e73c"},"cell_type":"markdown","source":"In this kernel i'll explore the datasets of Home Credit Default Risk for my submission in the competition, this is my first kernel, if you guys see something in my code that could be write better i'm open for all tips, including in my grammar."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n# import the dataset\nimport os\nlistdir = os.listdir(\"../input\")\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(listdir)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/application_train.csv\")\ndf_test = pd.read_csv(\"../input/application_test.csv\")\nbureau_balance = pd.read_csv(\"../input/bureau_balance.csv\")\npos_cash = pd.read_csv(\"../input/POS_CASH_balance.csv\")\nprevious_application = pd.read_csv(\"../input/previous_application.csv\")\ninstallments_payments = pd.read_csv(\"../input/installments_payments.csv\")\ncredit_card_balance = pd.read_csv(\"../input/credit_card_balance.csv\")\nbureau = pd.read_csv(\"../input/bureau.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9130e03d2b91572d40bee43b2276f59f0daa5b59"},"cell_type":"markdown","source":"# EDA on previous application, posh cash, instalments_payments and credit card balance"},{"metadata":{"trusted":true,"_uuid":"5ded3c9498b2080fede4eb98050f05cd874e8eaf"},"cell_type":"code","source":"previous_application.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d543051390f39552d78e226708b0cb8d72c91733"},"cell_type":"code","source":"pos_cash.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"164e3911fe0067efcfafb66c11a9041dcafeb9a5"},"cell_type":"code","source":"installments_payments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1800138f19b61be7b3677ccb463bfe5e5b995a30"},"cell_type":"code","source":"credit_card_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e184afd495b7b66b3d7a997dcaa76b779b923232"},"cell_type":"markdown","source":"# EDA on Bureau dataset"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"27c9a54f2c34f0bd89c935b5a18e35f81071aa43"},"cell_type":"code","source":"bureau.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dd6cf68659ec661a5aaf890679d81d326f13786"},"cell_type":"code","source":"bureau.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a69b0702c625af43b822e4394bf5f30512be2fd9"},"cell_type":"markdown","source":"Since we have Bureau and bureau_balance we can join the datasets with SK_ID_BUREAU as key"},{"metadata":{"trusted":true,"_uuid":"d0882f92bd48d45f8c962b6ef2e65d0761c1e359","scrolled":false},"cell_type":"code","source":"def agg_status(x):\n    status = list(x.values)\n    a = np.unique(status, return_counts=True)\n    return (a[0][a[1].argmax()])\n\nserie_group_bureau = bureau_balance.groupby(['SK_ID_BUREAU'])['STATUS'].apply(agg_status)\nbu_balance_group = pd.DataFrame(columns = ['SK_ID_BUREAU','STATUS_CASH', 'NUMBER_LOAN']) \nbu_balance_group['SK_ID_BUREAU'] = serie_group_bureau.index.values\nbu_balance_group['STATUS_CASH'] = serie_group_bureau.values\nbu_balance_group['NUMBER_LOAN'] = bureau_balance.groupby(['SK_ID_BUREAU']).count().values\nbu_balance_group.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9116719d19406a654462b98e5baca2025ece57fc"},"cell_type":"code","source":"bureau_full = pd.merge(bureau, bu_balance_group, on= 'SK_ID_BUREAU')\nbureau_full.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f03b5c61dc268bd31dae518b4fd3c651f7a5aecf"},"cell_type":"code","source":"bureau_full.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7e9b83ca8db242cbbdd2149f7ab6a9409051dcf2"},"cell_type":"code","source":"bureau_full_f = bureau_full.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23d394063301f72dc0f01bfe44ddbc40a7f3670f"},"cell_type":"markdown","source":"# Now lets check the full dataset"},{"metadata":{"trusted":true,"_uuid":"04f4c287ca9f9cf40ae869a4fcd034823a71e0e8"},"cell_type":"code","source":"bureau_full_f[bureau_full_f['SK_ID_CURR']==100002]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"134a04ba8d706d91dfd45af0040a84faf8adfc00"},"cell_type":"code","source":"bureau_once = pd.DataFrame(columns=['SK_ID_CURR', 'CREDIT_ACTIVE', 'CREDIT_CLOSED', 'DAYS_CREDIT_SUM', 'CREDIT_DAY_OVERDUE_SUM',\n                                   'DAYS_CREDIT_ENDDATE_SUM', 'DAYS_CREDIT_ENDDATE_FACT_SUM', 'AMT_CREDIT_MAX_OVERDUE', 'CNT_CREDIT_PROLONG',\n                                    'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'CREDIT_TYPE_MAX',\n                                   'DAYS_CREDIT_UPDATE', 'AMT_ANNUITY', 'STATUS', 'NUMBER_LOAN'])\n\nbureau_once['SK_ID_CURR'] = bureau_full_f.groupby(['SK_ID_CURR']).count().index.values\nbureau_once['CREDIT_ACTIVE'] = bureau_full_f.groupby(['SK_ID_CURR'])['CREDIT_ACTIVE'].apply(lambda x: (x=='Active').sum()).values\nbureau_once['CREDIT_CLOSED'] = bureau_full_f.groupby(['SK_ID_CURR'])['CREDIT_ACTIVE'].apply(lambda x: (x=='Closed').sum()).values\nbureau_once['CREDIT_CURRENCY'] = bureau_full_f.groupby(['SK_ID_CURR'])['CREDIT_CURRENCY'].apply(agg_status).values\nbureau_once['DAYS_CREDIT_SUM'] = bureau_full_f.groupby(['SK_ID_CURR'])['DAYS_CREDIT'].sum().values\nbureau_once['CREDIT_DAY_OVERDUE_SUM'] = bureau_full_f.groupby(['SK_ID_CURR'])['CREDIT_DAY_OVERDUE'].sum().values\nbureau_once['DAYS_CREDIT_ENDDATE_SUM'] = bureau_full_f.groupby(['SK_ID_CURR'])['DAYS_CREDIT_ENDDATE'].sum().values\nbureau_once['DAYS_CREDIT_ENDDATE_FACT_SUM'] = bureau_full_f.groupby(['SK_ID_CURR'])['DAYS_ENDDATE_FACT'].sum().values\nbureau_once['AMT_CREDIT_MAX_OVERDUE'] = bureau_full_f.groupby(['SK_ID_CURR'])['AMT_CREDIT_MAX_OVERDUE'].sum().values\nbureau_once['CNT_CREDIT_PROLONG'] = bureau_full_f.groupby(['SK_ID_CURR'])['CNT_CREDIT_PROLONG'].sum().values\nbureau_once['AMT_CREDIT_SUM'] = bureau_full_f.groupby(['SK_ID_CURR'])['AMT_CREDIT_SUM'].sum().values\nbureau_once['AMT_CREDIT_SUM_DEBT'] = bureau_full_f.groupby(['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().values\nbureau_once['AMT_CREDIT_SUM_LIMIT'] = bureau_full_f.groupby(['SK_ID_CURR'])['AMT_CREDIT_SUM_LIMIT'].sum().values\nbureau_once['AMT_CREDIT_SUM_OVERDUE'] = bureau_full_f.groupby(['SK_ID_CURR'])['AMT_CREDIT_SUM_OVERDUE'].sum().values\nbureau_once['CREDIT_TYPE_MAX'] = bureau_full_f.groupby(['SK_ID_CURR'])['CREDIT_TYPE'].apply(agg_status).values\nbureau_once['DAYS_CREDIT_UPDATE'] = bureau_full_f.groupby(['SK_ID_CURR'])['DAYS_CREDIT_UPDATE'].sum().values\nbureau_once['AMT_ANNUITY'] = bureau_full_f.groupby(['SK_ID_CURR'])['AMT_ANNUITY'].sum().values\nbureau_once['STATUS'] = bureau_full_f.groupby(['SK_ID_CURR'])['STATUS_CASH'].apply(agg_status).values\nbureau_once['NUMBER_LOAN'] = bureau_full_f.groupby(['SK_ID_CURR'])['NUMBER_LOAN'].sum().values\n\nbureau_once.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa2813ff2c9331d46f64f43801302837b9c36966"},"cell_type":"code","source":"bureau_once[bureau_once['SK_ID_CURR']==100002]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32b11668694146065a2f78d266ad624fb1c3df41","collapsed":true},"cell_type":"code","source":"#df_train_full.head(20)\n#df_train.head()\ndf_train_full = pd.merge(df_train, bureau_once, on='SK_ID_CURR')\ndf_test_full = pd.merge(df_test, bureau_once, on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d90b9e3a463d721c1bc5bbfcc653e87deb65bce"},"cell_type":"code","source":"# check the basic train data\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4af719c6198a1b75c730c61f0c261feebdaa2efc","collapsed":true},"cell_type":"code","source":"# make a statistic analysis in the numeric features\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"6e77245e413baad166e7bd54ed85a1510b6d4b52","scrolled":true,"collapsed":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11d279c7ca5ad628550aee9352501ac92a61b898","scrolled":true,"collapsed":true},"cell_type":"code","source":"# check for null values and where they are\ndef search_missing_data(df):\n    null_features = df.isnull().sum()\n    null_features = [null_features.index[x] for x in range(len(df.isnull().sum())) if null_features[x] > 0]\n    type_features = [df[x].dtype for x in null_features]\n    return null_features, type_features\nfeatures_miss_train = pd.DataFrame()\nfeatures_miss_test = pd.DataFrame()\n\nnull_features, type_features = search_missing_data(df_train)\nfeatures_miss_train['feature'] = null_features\nfeatures_miss_train['dtype'] = type_features\n\nnull_features, type_features = search_missing_data(df_test)\nfeatures_miss_test['feature'] = null_features\nfeatures_miss_test['dtype'] = type_features\nprint('Features missing in the train set', features_miss_train.shape[0], '\\nFeatures missing in the test set', features_miss_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0442845d93e88a3e4494b4c5286d85eb5b059fba"},"cell_type":"markdown","source":"So we have 67 features with nan values, now we have to investigate these features to fill the nan values or drop them"},{"metadata":{"trusted":true,"_uuid":"40b2566ee841b97c68b8a1fb98c86ad4b57cf025","scrolled":false,"collapsed":true},"cell_type":"code","source":"for x in features_miss_train['feature'].values:\n    if x not in features_miss_test['feature'].values:\n        print(x)\nfeatures_miss_test['feature'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecb0120308384579fb2962ceaf04565c7ce88035","collapsed":true},"cell_type":"code","source":"print(df_train.shape[0],df_train.dropna().shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e6062e4412bde1da4379887b716b3e154b5cc71"},"cell_type":"markdown","source":"As we can see comparing the number of observations between the train set with and without nan values we can't just drop theses values.\n\nNow in the preprocess_data function i'll fill the nan values assuming that the nan values in the features is equivalent that observation don't have a house for example. "},{"metadata":{"trusted":true,"_uuid":"2f0a5b94c35ff3f2e365d9342ad2254cea1d2641","collapsed":true},"cell_type":"code","source":"def preprocess_data(df, test=0):\n    df1 = df\n    df_dict = {0:features_miss_train['feature'].values,\n              1: features_miss_test['feature'].values}\n    for feature in df_dict[test]:\n        if df1[feature].dtype == 'object':\n            df1[feature].fillna('None', inplace=True)\n        if df1[feature].dtype == 'float64':\n            df1[feature].fillna(0.0, inplace=True)\n    df1['CODE_GENDER'].replace('XNA', 'F',inplace=True)\n    df1['NAME_INCOME_TYPE'].replace('Maternity leave', 'Student',inplace=True)\n    df1['NAME_FAMILY_STATUS'].replace('Unknown', 'Single / not married', inplace=True)\n    \n    return df1\ndf_prep_train = preprocess_data(df_train)\ndf_prep_test = preprocess_data(df_test, test=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6f2342686e4adfb28c59e07b9f22dd0adc4824f","scrolled":false,"collapsed":true},"cell_type":"code","source":"features_to_plot = list(df_prep_train.columns.values)\n\nfeatures_part_one = features_to_plot[2:32]\nfeatures_part_two = features_to_plot[32:62]\nfeatures_part_three = features_to_plot[62:92]\nfeatures_part_four = features_to_plot[92:122]\n\ndef plot_charts(features):\n    ncols = 5\n    nrows = round(len(features)/ncols)\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 12))\n    n_feature = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            if n_feature < len(features):\n                name = features[n_feature]\n                if df_prep_train[name].dtype == 'object' or df_prep_train[name].dtype == 'int64':\n                    g = sns.countplot(hue='TARGET', y = name, data = df_prep_train, ax = axes[row][col])\n                    g.set_ylabel(name)\n                    g.set_title('TARGET')\n                else:\n                    g = sns.boxplot(x='TARGET', y = name, data = df_prep_train, ax = axes[row][col])\n                    g.set_ylabel(name)\n                    g.set_title('TARGET')\n            n_feature += 1\n    # [plt.setp(ax.get_xticklabels(), rotation=90) for ax in axes.flat]\n    plt.tight_layout()\n    plt.show()\nplot_charts(features_part_one)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18f4e3433cef9c4f443c756dfb1ef9d1df78fa9b","scrolled":true,"collapsed":true},"cell_type":"code","source":"plot_charts(features_part_two)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"792c9a4fcf2423f2e0e039b29a71f1630a20c332","collapsed":true},"cell_type":"code","source":"plot_charts(features_part_three)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4da88c27e8f7ebbad78cfde4633e62015e3af0f8","collapsed":true},"cell_type":"code","source":"plot_charts(features_part_four)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19ae545059ae803f6475cfdafbff7b4759971ae1","collapsed":true},"cell_type":"code","source":"df_dummy_train = pd.get_dummies(df_prep_train.drop(['TARGET', 'SK_ID_CURR'], axis=1), drop_first=True)\ndf_dummy_train['TARGET'] = df_prep_train['TARGET']\ndf_dummy_train['SK_ID_CURR'] = df_prep_train['SK_ID_CURR']\ndf_dummy_test = pd.get_dummies(df_prep_test, drop_first=True)\n\nlist_train = list(df_dummy_train.columns.values)\nlist_test = list(df_dummy_test.columns.values)\n\nfor f in list_train:\n    if f not in list_test:\n        print(f)\n\nprint(df_dummy_train.shape)\nprint(df_dummy_test.shape)\n\nclt = lgb.LGBMClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46f08ea8743b27a9109536218b57d091ea3318e2","collapsed":true},"cell_type":"code","source":"X_train = df_dummy_train.drop(['TARGET', 'SK_ID_CURR'], axis=1).values\ny_train = df_dummy_train['TARGET'].values\nX_test = df_dummy_test.drop(['SK_ID_CURR'], axis=1).values\nid_test = df_dummy_test['SK_ID_CURR'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a69d067eef6708998b7a68d9757559f5c73c75c3","collapsed":true},"cell_type":"code","source":"clt.fit(X_train, y_train)\ny_test = clt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"584e1ed7040ffef3622220e1fe8f583d91eb80e5","collapsed":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['SK_ID_CURR'] = id_test\nsubmission['TARGET'] = y_test\nsubmission.to_csv('submission2.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}