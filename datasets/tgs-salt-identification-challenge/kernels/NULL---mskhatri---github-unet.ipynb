{"cells":[{"metadata":{"trusted":true,"_uuid":"171e63009ad219c68ba4ec100e142b9260ef6a35"},"cell_type":"code","source":"from __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\ntry:\n    from itertools import  ifilterfalse\nexcept ImportError: # py3k\n    from itertools import  filterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    \"\"\"\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    \"\"\"\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    \"\"\"\n    Array of IoU for each (non ignored) class\n    \"\"\"\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []    \n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious)) # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.elu(errors_sorted) +1, Variable(grad))\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n         super(StableBCELoss, self).__init__()\n    def forward(self, input, target):\n         neg_abs = - input.abs()\n         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n         return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    \"\"\"\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    \"\"\"\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, only_present=False):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n    \"\"\"\n    C = probas.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float() # foreground for class c\n        if only_present and fg.sum() == 0:\n            continue\n        errors = (Variable(fg) - probas[:, c]).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch\n    \"\"\"\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\ndef xloss(logits, labels, ignore=None):\n    \"\"\"\n    Cross entropy loss\n    \"\"\"\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\n\ndef mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    nanmean compatible with generators.\n    \"\"\"\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == 'raise':\n            raise ValueError('Empty mean')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"475691f2e3e48aea711bbb655dc4bbd2a05d5d7a"},"cell_type":"code","source":"%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\nfrom fastai.conv_learner import *\nfrom fastai.dataset import *\nfrom fastai.models.resnet import vgg_resnet50\nfrom fastai.models.cifar10.senet import *\nfrom skimage.transform import resize\nimport json\nfrom sklearn.model_selection import train_test_split, StratifiedKFold , KFold\nfrom sklearn.metrics import jaccard_similarity_score\n# from pycocotools import mask as cocomask\n# from utils import my_eval,intersection_over_union_thresholds,RLenc\n# from seg_scripts.utils import *\n# from seg_scripts.lovasz_losses import lovasz_hinge\nprint(torch.__version__)\ntorch.backends.cudnn.benchmark=True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09476eaa119d6ea84438e9b19aca84de8a6f457e"},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9f69302db99268d5708e6e83051d1890460284c"},"cell_type":"code","source":"PATH = Path('/kaggle/input/')\nTRN_MASKS = 'train/images/'\nTRN_IMG = 'train/images/'\nTST_IMG = 'test/images/'\nTMP = Path('/tmp/')\nMODEL = Path('/tmp/model/')\n\ntrn = pd.read_csv(PATH/'train.csv')\ndpth = pd.read_csv(PATH/'depths.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f9a5d030376cd40653b4a270db4db264e916c39"},"cell_type":"code","source":"def show_img(im, figsize=None, ax=None, alpha=None):\n    if not ax: fig,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im, alpha=alpha)\n    ax.set_axis_off()\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74d1dc7cec9d96ce0e1c66acea31b486eedf31ab"},"cell_type":"code","source":"class DepthDataset(Dataset):\n    def __init__(self,ds,dpth_dict):\n        self.dpth = dpth_dict\n        self.ds = ds\n        \n    def __getitem__(self,i):\n        val = self.ds[i]\n        return val[0],self.dpth[self.ds.fnames[i].split('/')[2][:-4]],val[1]\n    \nclass MatchedFilesDataset(FilesDataset):\n    def __init__(self, fnames, y, transform, path):\n        self.y=y\n        assert(len(fnames)==len(y))\n        super().__init__(fnames, transform, path)\n        \n    def get_x(self, i): \n        return open_image(os.path.join(self.path, self.fnames[i]))\n    \n    def get_y(self, i):\n        return open_image(os.path.join(str(self.path), str(self.y[i])))\n\n    def get_c(self): return 0\n    \nclass TestFilesDataset(FilesDataset):\n    def __init__(self, fnames, y, transform,flip, path):\n        self.y=y\n        self.flip = flip\n        super().__init__(fnames, transform, path)\n        \n    def get_x(self, i): \n        im = open_image(os.path.join(self.path, self.fnames[i]))\n        return np.fliplr(im) if self.flip else im\n        \n    def get_y(self, i):\n        im = open_image(os.path.join(str(self.path), str(self.y[i])))\n        return np.fliplr(im) if self.flip else im\n    def get_c(self): return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bada4c3e1b4bb489108e8402a62338c95ec995a9"},"cell_type":"code","source":"x_names = np.array([f'{TRN_IMG}{o.name}' for o in (PATH/TRN_MASKS).iterdir()])\ny_names = np.array([f'{TRN_MASKS}{o.name}' for o in (PATH/TRN_MASKS).iterdir()])\ntst_x = np.array([f'{TST_IMG}{o.name}' for o in (PATH/TST_IMG).iterdir()])\nf_name = [o.split('/')[-1] for o in x_names]\n\nc = dpth.set_index('id')\ndpth_dict = c['z'].to_dict()\n\nkf = 5\nkfold = KFold(n_splits=kf, shuffle=True, random_state=42)\n\ntrain_folds = []\nval_folds = []\nfor idxs in kfold.split(f_name):\n    train_folds.append([f_name[idx] for idx in idxs[0]])\n    val_folds.append([f_name[idx] for idx in idxs[1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e47e2d68db71afd627f8731a8dbe5ec4ee0faaf"},"cell_type":"code","source":"# train_folds = pickle.load(open('train_folds.pkl',mode='rb'))\n# val_folds = pickle.load(open('val_folds.pkl',mode='rb'))\n# tst_x = pickle.load(open('tst_x.pkl',mode='rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c17ece2fb20056249b273b62547ca86049cb64a7"},"cell_type":"code","source":"class SaveFeatures():\n    features=None\n    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output): self.features = output\n    def remove(self): self.hook.remove()\n    \nclass UnetBlock(nn.Module):\n    def __init__(self, up_in, x_in, n_out):\n        super().__init__()\n        up_out = x_out = n_out//2\n        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n        self.bn = nn.BatchNorm2d(n_out)\n        \n    def forward(self, up_p, x_p):\n        up_p = self.tr_conv(up_p)\n        x_p = self.x_conv(x_p)\n        cat_p = torch.cat([up_p,x_p], dim=1)\n        return self.bn(F.relu(cat_p))\n    \nclass Unet34(nn.Module):\n    def __init__(self, rn):\n        super().__init__()\n        self.rn = rn\n        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n        self.up1 = UnetBlock(512,256,128)\n        self.up2 = UnetBlock(128,128,128)\n        self.up3 = UnetBlock(128,64,128)\n        self.up4 = UnetBlock(128,64,128)\n        self.up5 = nn.ConvTranspose2d(128, 1, 2, stride=2)\n        \n    def forward(self,img,depth):\n        x = F.relu(self.rn(img))\n        x = self.up1(x, self.sfs[3].features)\n        x = self.up2(x, self.sfs[2].features)\n        x = self.up3(x, self.sfs[1].features)\n        x = self.up4(x, self.sfs[0].features)\n        x = self.up5(x)\n        return x[:,0]\n    \n    def close(self):\n        for sf in self.sfs: sf.remove()\n\n\nclass UnetModel():\n    def __init__(self,model,lr_cut,name='unet'):\n        self.model,self.name = model,name\n        self.lr_cut = lr_cut\n\n    def get_layer_groups(self, precompute):\n        lgs = list(split_by_idxs(children(self.model.rn), [self.lr_cut]))\n        return lgs + [children(self.model)[1:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e80adced9d73cb0d392611ad531b77fe43c82c7"},"cell_type":"code","source":"def get_tgs_model():\n    f = resnet34\n    cut,lr_cut = model_meta[f]\n    m_base = get_base(f,cut)\n    m = to_gpu(Unet34(m_base))\n    models = UnetModel(m,lr_cut)\n    learn = ConvLearner(md, models, tmp_name=TMP, models_name=MODEL)\n    return learn\n\ndef get_base(f,cut):\n    layers = cut_model(f(True), cut)\n    return nn.Sequential(*layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"796e91ee371d2870a6cbf31980191986aa6b8e9b"},"cell_type":"code","source":"model = 'simple_resnet34'\nbst_acc=[]\nuse_clr_min=20\nuse_clr_div=10\naug_tfms = [RandomFlip(tfm_y=TfmType.CLASS)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f575953d0b62f0179a5ce906dcdd9a9a0632ce40"},"cell_type":"code","source":"model = 'simple_resnet34'\nbst_acc=[]\nuse_clr_min=20\nuse_clr_div=10\naug_tfms = [RandomFlip(tfm_y=TfmType.CLASS)]\n\nszs = [(256,64)]\nfor sz,bs in szs:\n    print([sz,bs])\n    for i in range(kf) :\n        print(f'fold_id{i}')\n        \n        trn_x = np.array([os.path.join(TRN_IMG,o) for o in train_folds[i]])\n        trn_y = np.array([os.path.join(TRN_MASKS,o) for o in train_folds[i]])\n        val_x = [os.path.join(TRN_IMG,o) for o in val_folds[i]]\n        val_y = [os.path.join(TRN_MASKS,o) for o in val_folds[i]]\n        \n        tfms = tfms_from_model(resnet34, sz=sz, pad=0,crop_type=CropType.NO, tfm_y=TfmType.CLASS, aug_tfms=aug_tfms)\n        datasets = ImageData.get_ds(MatchedFilesDataset, (trn_x,trn_y), (val_x,val_y), tfms,test=tst_x,path=PATH)\n        md = ImageData(PATH, datasets, bs, num_workers=4, classes=None)\n        denorm = md.trn_ds.denorm\n        md.trn_dl.dataset = DepthDataset(md.trn_ds,dpth_dict)\n        md.val_dl.dataset = DepthDataset(md.val_ds,dpth_dict)\n        md.test_dl.dataset = DepthDataset(md.test_ds,dpth_dict)\n        learn = get_tgs_model() \n        learn.opt_fn = optim.Adam\n        learn.metrics=[accuracy_thresh(0.5)]\n#         pa = f'{kf}_fold_{model}_{i}'\n#         print(pa)\n\n        lr=1e-2\n        wd=1e-7\n        lrs = np.array([lr/100,lr/10,lr])\n\n        learn.unfreeze()        \n        learn.crit = lovasz_hinge\n#         learn.load(pa)\n#         learn.fit(lrs/2,3, wds=wd, cycle_len=10,use_clr=(20,8))\n        learn.fit(lr,1)\n\n               \n#         learn.load(pa)        \n        #Calcuating mean iou score\n        v_targ = md.val_ds.ds[:][1]\n        v_preds = np.zeros((len(v_targ),sz,sz))     \n        v_pred = learn.predict()\n        v_pred = to_np(torch.sigmoid(torch.from_numpy(v_pred)))\n        p = ((v_pred)>0.5).astype(np.uint8)\n#         bst_acc.append(intersection_over_union_thresholds(v_targ,p))\n#         print(bst_acc[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d711b4a719c99a5529f9e43ea52d3ed40a0c2943"},"cell_type":"code","source":"preds = np.zeros(shape = (18000,sz,sz))\nfor o in [True,False]:\n    md.test_dl.dataset = TestFilesDataset(tst_x,tst_x,tfms[1],flip=o,path=PATH)\n    md.test_dl.dataset = DepthDataset(md.test_dl.dataset,dpth_dict)\n    \n    for i in tqdm_notebook(range(kf)):\n        pa = f'{kf}_fold_{model}_{i}'\n        print(pa)\n        learn.load(pa)\n        pred = learn.predict(is_test=True)\n        pred = to_np(torch.sigmoid(torch.from_numpy(pred)))    \n        for im_idx,im in enumerate(pred):\n                preds[im_idx] += np.fliplr(im) if o else im\n        del pred\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33e4f720362b896dfd1120a3830ea53dc81e8fbc"},"cell_type":"code","source":"p = [cv2.resize(o/10,dsize=(101,101)) for o in preds]\np = [(o>0.5).astype(np.uint8) for o in p]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9163a6f29e2ec4414862c4d639ca3b9b3cd197b"},"cell_type":"code","source":"pred_dict = {id_[11:-4]:RLenc(p[i]) for i,id_ in tqdm_notebook(enumerate(tst_x))}\nsub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('simple_k_fold_flipped.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e822a297614ed25b632d63624da21d5179faf62"},"cell_type":"code","source":"plt.imshow(((preds[16]/10)>0.5).astype(np.uint8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c198b0e1954978c1054b9b0a3b5b9bbb6839924"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a51d7385f7575f9a7543dce77981a92ad2c086f1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c66470f7b77301f00b7274fc4b4d040a702993a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d33061735748540f018d66477411fbf5107b43b9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce2e825deedd4e5498b3bf67f363e9cfee6ce441"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1926170ee812f0867f587c3d349a25dba42dc5fe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01c81a82aed28e3769fc1fb09995707a4f22745c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3637f3ad5a424771b3c1c153c0c140c947c13b75"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c02d8ec07d1fd6d47d0f9ae4b84dfdff358d626c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76cf01f9c58e46c5226b6e8656c2f0cbd9fb20b6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a87de6044919700b9bed8f0b9dbcbdc9334d22d9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8030ac72822e71ca6f659706d58dc486fba236ab"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7caad95d0e8b88e5b4eb759c3594edbd46472f26"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ed89a4984801b19e2d761b296654874f22d93d2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}