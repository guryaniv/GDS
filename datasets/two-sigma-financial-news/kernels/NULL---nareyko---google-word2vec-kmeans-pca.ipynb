{"cells":[{"metadata":{"_uuid":"3bd044194592aadd9c2a80b76370cf2d875baa75"},"cell_type":"markdown","source":"This notebook uses the output from https://www.kaggle.com/nareyko/data-preparation-memory-optimization"},{"metadata":{"trusted":true,"_uuid":"405b23b8b5383c93404a889c8d6b80d96ce86e68"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport gc\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as PathEffects\n%matplotlib inline\n\nfrom tqdm import tqdm\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87993c068c1ff82245277fa2e0e096e66b72241"},"cell_type":"markdown","source":"# Reading data"},{"metadata":{"trusted":true,"_uuid":"4d6ad8705417eee2f921666b96c2c137fedbf195"},"cell_type":"code","source":"news_df = pd.read_pickle('../input/data-preparation-memory-optimization/news_df.pkl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7125c92ee68e54076859a138a73d1535b00c19d"},"cell_type":"markdown","source":"Joining all daily texts together"},{"metadata":{"trusted":true,"_uuid":"d420d97153f40d02a46583d347871a6e6558fc19"},"cell_type":"code","source":"news_df['date'] = news_df.time.dt.date\ntime_news = news_df.groupby('date').headline.apply(' '.join).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c359efffcce67cf6549b5ded44b05a44ad08faa3"},"cell_type":"code","source":"del news_df; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13629a3660696be246e63c274eb7c96921b5ba3e"},"cell_type":"markdown","source":"# Processing words using Google model"},{"metadata":{"trusted":true,"_uuid":"d9d38208054875e136f990e9abe2c531c29df503"},"cell_type":"code","source":"# The function \"text_to_wordlist\" is from\n# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n\ndef text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    text = text.lower().split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec63da86f69bfb9cac3b6e1ff6671685106d417f"},"cell_type":"code","source":"time_news['words'] = time_news.headline.progress_apply(text_to_wordlist)\ntime_news.head(1).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58b10401034af9c069e567ec4e85a9b247ec55e7"},"cell_type":"code","source":"# Load Google pretrained model\nmodel = KeyedVectors.load_word2vec_format('../input/word2vecnegative300/GoogleNews-vectors-negative300.bin', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e9b690df3b0fd7bfdc3ee12846eb605c2e3b2a9"},"cell_type":"code","source":"def text2vec(text):\n    return np.mean([model[x] for x in text.split() if x in model.vocab], axis=0).reshape(1,-1)\n\ntime_news['vectors'] = time_news.words.progress_apply(text2vec)\ntime_news.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae182075a1008adc68ec2209a0d7ece1b2755e8a"},"cell_type":"code","source":"time_news.to_pickle('time_news.pkl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3a88865c382803eefc564347aa7e8515db37cf3"},"cell_type":"markdown","source":"# Clustering and generating scatter"},{"metadata":{"trusted":true,"_uuid":"7c69095b1c6a06738c4f2fa1af15f127fb6f0643"},"cell_type":"code","source":"X = np.concatenate(time_news['vectors'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e1834b2ad23d8defe3ab07826cebaf7a8e1ef40"},"cell_type":"code","source":"kmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\ntime_news['cluster'] = kmeans.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"701f2eb9a1f3f0aa00bc4b8048a7537193b3dfa0"},"cell_type":"code","source":"pca = PCA(n_components=2)\npca_result = pca.fit_transform(X)\ntime_news['x'] = pca_result[:, 0]\ntime_news['y'] = pca_result[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60c9227492e3484e3c9600ec463dae5469015d52"},"cell_type":"code","source":"time_news.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52ceb05bf3af310540d01fbb21b347ade6f37eea"},"cell_type":"code","source":"cluster_colors = pd.np.array(['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000'])\ntime_news['color'] = cluster_colors[time_news.cluster.values]\ntime_news['text'] = time_news.headline.str[:50]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfcedb8804bf6a5355ec805234d78dbb3e373ff1"},"cell_type":"code","source":"import bokeh.io\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, LabelSet\n\n# from bokeh.charts import Donut, HeatMap, Histogram, Line, Scatter, show, output_notebook, output_file\nbokeh.io.output_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acf552ee18ce28825c5ef1ff574c44770154ecff"},"cell_type":"code","source":"#visualize the data using bokeh\n#output_file(\"top_artists.html\", title=\"top artists\")\n# TOOLS = \"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\n\nsource = ColumnDataSource.from_df(time_news[['x', 'y', 'color', 'text', 'date']])\nTOOLTIPS = [(\"date\", \"@date\"),(\"text\", \"@text\")]\nTOOLS = \"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\n\nplot = figure(plot_width=800, plot_height=450, tooltips=TOOLTIPS, tools=TOOLS)\n\n#draw circles\nplot.circle(y='y', x='x', source=source, size=15, fill_color='color')\nshow(plot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8f4f6d386e6db38f8234d43c7a64940bbb10fa0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}