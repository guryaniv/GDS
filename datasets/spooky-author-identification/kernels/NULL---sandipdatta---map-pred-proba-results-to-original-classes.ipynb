{"cells":[{"metadata":{"_cell_guid":"f83efee8-7b3e-4dc4-a94d-21bc276d200f","_uuid":"88f46945ba8d47c574bc416713b2d94ea40bbaa8"},"cell_type":"markdown","source":"**This scripts shows how to Map your *Predict_proba* results to Original Classes to get a quick overview** **"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\n","execution_count":3,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"39cd3e7d-5401-4696-9c4b-063f2db8ae0f","_uuid":"d2f78f3d71f99756c7b773f6ff369085adfa5c47","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"359a4de7-48ca-45b2-a731-c588ee04c154","_uuid":"084a9d1446009c50ce18bac0875662fbeb33651e","trusted":true},"cell_type":"code","source":"train.head()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"e396be17-aa1d-4c28-b615-e04ccb8fd823","_uuid":"1d86a00cb296658bfabb01069feea4220627632a","trusted":true},"cell_type":"code","source":"test.head()","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"44f63091-06a1-4b89-af67-16a5cb0dc8c4","_uuid":"a15104a1833dacc150510404560dd5282dd3bfcd"},"cell_type":"markdown","source":"**Lable Encoding the Output Multi classes**"},{"metadata":{"collapsed":true,"_cell_guid":"2358bda3-1042-4ca4-b58a-5b2234f326a3","_uuid":"9dce35ef65ef379aca7b80fa6dc46d7ac40a2612","trusted":true},"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"30069897-1730-4c47-8e49-8d1ffb4f044b","_uuid":"990013aaab9ca48e233e6639a91199fb642545e6","trusted":true},"cell_type":"code","source":"print (lbl_enc.classes_)\nprint (y)\nprint (set(y))","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"9f0d4394-5ced-40ff-89ea-96f2e69dd668","_uuid":"67c1d4dd0a68e8b268fffecf6cb3d8d144dfa389"},"cell_type":"markdown","source":"**Split the data into Train , Test**"},{"metadata":{"collapsed":true,"_cell_guid":"9b5b2e93-1d70-4564-bcb2-431c250e99f6","_uuid":"5113019e25935d5de6dd399a2aabc4df3509a3f8","trusted":true},"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"7becf300-906c-457c-8c9c-1f7a031fd9bd","_uuid":"999a435681a5b971a44ee8ef59f21cca5fc16317"},"cell_type":"markdown","source":"**Since the Main data are 'TEXT' , so we need to vectorize them. Just using simple TFIDF for this case**"},{"metadata":{"collapsed":true,"_cell_guid":"3cd8fa20-86b9-471d-9b72-a1eb700ec37f","_uuid":"b03397a5d11977f12e70599c818dd4e457fefdf1","trusted":true},"cell_type":"code","source":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"751d7b4f-b9dc-42b0-91bc-2d43197cc7f6","_uuid":"a69b31fd0c8e1e2815a40a4fe77d7f905acf1c41"},"cell_type":"markdown","source":"**Let fit a simple Logistic Regression and see the results**"},{"metadata":{"_cell_guid":"d708b30c-7cdb-4fe5-af1f-797af9291930","_uuid":"1ab073cdf60de5b676582eb3628a55523a6c014a","trusted":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % metrics.log_loss(yvalid, predictions))","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"ffa6980c-c3c8-4fb1-be67-a2ee9a3c7b08","_uuid":"f5197b7f96741de28050bf269440214ecc6e170e"},"cell_type":"markdown","source":"**The Log-loss results are not that great . It still have scope to improve. \nHowever our prime objective in this exercise is to just map the pred_proba percentages to original classes**"},{"metadata":{"_cell_guid":"d66eaeb2-2b85-4d59-99bd-1b2aa72caf55","_uuid":"40df0e704256b72d2343163a841d961ed7977513"},"cell_type":"markdown","source":"**Mapping pred_prob to Lable encoded values**"},{"metadata":{"_cell_guid":"c4b5362b-97ba-4008-bdf1-f95e6e95124c","_uuid":"4f62fec6a2ea264e549c11a1add5993b8e3561e8","trusted":true},"cell_type":"code","source":"predictions[0:4]","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"44bdace7-f0e0-4dfc-bc50-441391a00d55","_uuid":"784d3e87688a604898cfc45bc2db7139c9718a68","trusted":true},"cell_type":"code","source":"\nclf.classes_\npred_To_Class_map = pd.DataFrame(predictions, columns=clf.classes_)\nprint (pred_To_Class_map.head(5))","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"d0f38f14-5aea-431e-a4ea-a7574a31a651","_uuid":"8f8608d8de963d4b62ada0fe1a7b4b241b1e7bda"},"cell_type":"markdown","source":"**Mapping pred_proba to Original Classes for a more clearer outlook**"},{"metadata":{"_cell_guid":"4357e123-cab8-4c16-9b92-1d559f8917b5","_uuid":"29cc8a8aa515879858095d8b39dfea63a868b9f4","trusted":true},"cell_type":"code","source":"pred_To_Class_map = pd.DataFrame(predictions, columns=lbl_enc.classes_)\nprint (pred_To_Class_map.head(5))","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"ab7a9877-7b87-427a-b22a-f8c36e4469f4","_uuid":"7907bdfcc941630a62d0170bf187861ac98af8eb"},"cell_type":"markdown","source":"** Lets consider the 1st TEXT **\n\n** As you can see the classifier has predicted the 1st TEST-set TEXT values to be classified as 64% EAP , 7% HPL & 28% MWS**\n\n** Since EAP has the highest probability percentage(64%), as such it is predciting it to belong to class EAP ( or class 0 in term of label encoding) as  you can see from the prediction values **"},{"metadata":{"_cell_guid":"9e7da84b-6500-4fa6-98cf-47b8d611d9ba","_uuid":"bc87bbada67e21f5a9b8f339d456170d04db0783"},"cell_type":"markdown","source":"**Please vote if you find it useful**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}