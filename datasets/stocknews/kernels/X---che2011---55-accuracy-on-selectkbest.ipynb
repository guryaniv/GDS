{"cells":[{"metadata":{"_uuid":"a4b36d7434b1b4e0e74c6a89fce3a9c9bad69d31"},"cell_type":"markdown","source":"# Daily News for Stock Market Prediction\n\nDetermine if the news affect the stock market.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"a6cbb2b8b1eb48e7c530862497682df3b302645c"},"cell_type":"markdown","source":"## A. Data Exploration/Preparation","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"08a1b0aff2be4be241fd4ee15303ec41cb9ee66f"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21242c9f29689811a380b5b17e1b2ac7d4cbf828"},"cell_type":"code","source":"# Open file and inspect first five rows.\ndf_A = pd.read_csv('../input/Combined_News_DJIA.csv')\ndf_A.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcdcb8ec16d50ecf986e3b2b35acbb4c2ee50465"},"cell_type":"markdown","source":"The features Top1 to Top25 represent the top-ranked news headlines as voted by users in the Reddit World News Channel. The date feature is the date the news were reported, and the label feature represents stock market movement on that day, with 1 being positive or neutral and 0, negative.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"9c8260e003823d9f0d49ef1257683bb73f4622ac"},"cell_type":"code","source":"# Create features that concatenates the columns and returns the text length.\ndf_A['combined_text'] = df_A.iloc[:,2:27].apply(lambda x: ''.join((x).astype(str)), axis=1)\ndf_A['text_length'] = df_A['combined_text'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d6276a8876b87f77586c7645bb8322189043bc4"},"cell_type":"code","source":"# Create a feature that determines text sentiment.\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\ndf_A['sentiment_score'] = df_A['combined_text'].apply(lambda y: sia.polarity_scores(y)['compound'])\ndf_A['sentiment'] = df_A['sentiment_score'].apply(lambda z: 'positive' if z >= 0 else 'negative')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b57129259148ffd419f9cf2bf96b824fbc5921c0"},"cell_type":"code","source":"# Show the updated dataframe.\ndf_A.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19291a35e11340d5bebb93baaf035e94b162e262"},"cell_type":"code","source":"# Plot market movement vs. news sentiment.\nn_groups = 2\nmark_cat = (df_A['Label'].value_counts()[1], df_A['Label'].value_counts()[0])\nnews_cat = (df_A['sentiment'].value_counts()['positive'], df_A['sentiment'].value_counts()['negative'])\n \nfig, ax = plt.subplots(figsize=(8, 5))\nindex = np.arange(n_groups)\nbar_width = 0.35\nopacity = 0.8\n \nrects1 = plt.bar(index, mark_cat, bar_width,\n                 alpha=opacity,\n                 color='b',\n                 label='Market')\n \nrects2 = plt.bar(index + bar_width, news_cat, bar_width,\n                 alpha=opacity,\n                 color='g',\n                 label='Sentiment')\n \nplt.xlabel('Category')\nplt.ylabel('Count/Days')\nplt.title('Market Movement vs News Sentiment Distribution')\nplt.xticks(index + 1/2*bar_width, ('Positive', 'Negative'))\nplt.legend()\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3df2e355b7cc00beb04037ce2bd38e35751700a5"},"cell_type":"code","source":"# Show proportion.\nprint('Positive market days: ', (df_A['Label'] == 1).mean())\nprint('Positive news sentiment: ', (df_A['sentiment'] == 'positive').mean())\nprint('Negative market days: ', (df_A['Label'] == 0).mean())\nprint('Negative news sentiment: ', (df_A['sentiment'] == 'negative').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"003a96c3e7d0535ef3ef330ff238ed94aab764b2"},"cell_type":"code","source":"# Run a t-test on the label column.\nfrom scipy import stats\nstats.ttest_ind(df_A['Label']==1, df_A['Label']==0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83617c17ad0bee028860e615c56a8f6dc3cb85bf"},"cell_type":"code","source":"# Correlation between the label and sentiment score features.\nprint('Correlation: ', df_A['Label'].corr(df_A['sentiment_score']))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"e764c44356ea55f974b324fff17b6ab199fafd17"},"cell_type":"code","source":"# Plot text length distribution vs. market movement.\ng = sns.FacetGrid(data=df_A, col='Label')\ng.map(plt.hist, 'text_length', bins=25)\nplt.show()\nsns.boxplot(x='Label', y='text_length', data=df_A)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"0bff2b30e26db65910f27fdadde4477a86791f77"},"cell_type":"code","source":"# Run a wordcloud on the positive market days.\npos_txt = []\nfor row in range(0, len(df_A[df_A['Label'] == 1])):\n    pos_txt.append(' '.join(str(x) for x in df_A[df_A['Label'] == 1].iloc[row,2:27]))\n    \nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nvect_A = CountVectorizer(stop_words='english')\npos_trans = vect_A.fit_transform(pos_txt)\npos_count = pos_trans.toarray().sum(axis=0) \n\ncol_names = vect_A.get_feature_names()\npos_dict = dict(zip(col_names, pos_count))\n\nfrom wordcloud import WordCloud,STOPWORDS\npos_wc = WordCloud(background_color='white', width=3000, height=2500).generate_from_frequencies(pos_dict)\nplt.figure(1,figsize=(8,8))\nplt.imshow(pos_wc)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18e2a95f5d9548750ae6703d9f80ab5d7de537d5"},"cell_type":"code","source":"# Show the top 10 words and their counts and percentages. \nimport operator\npos_sort = sorted(pos_dict.items(), reverse=True, key=operator.itemgetter(1))\ndf_pos = pd.DataFrame(data=pos_sort, columns=['token', 'count'])\ndf_pos['percentage'] = df_pos['count']/df_pos['count'].sum()*100\ndf_pos.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"310753d0fa118513795bd5c0f90ace969677db87"},"cell_type":"code","source":"# Run a wordcloud on the negative market days.\nneg_txt = []\nfor row in range(0, len(df_A[df_A['Label'] == 0])):\n    neg_txt.append(' '.join(str(x) for x in df_A[df_A['Label'] == 0].iloc[row,2:27]))\n    \nneg_trans = vect_A.fit_transform(neg_txt)\nneg_count = neg_trans.toarray().sum(axis=0) \n\nneg_dict = dict(zip(col_names, neg_count))\n\nneg_wc = WordCloud(background_color='black', width=3000, height=2500).generate_from_frequencies(neg_dict)\nplt.figure(1,figsize=(8,8))\nplt.imshow(neg_wc)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0e6007cb980c79916d5fdc23a9da3d28aced1e0"},"cell_type":"code","source":"# Show the top 10 words and their counts and percentages.\nneg_sort = sorted(neg_dict.items(), reverse=True, key=operator.itemgetter(1))\ndf_neg = pd.DataFrame(data=neg_sort, columns=['token', 'count'])\ndf_neg['percentage'] = df_neg['count']/df_neg['count'].sum()*100\ndf_neg.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8419848d6e634ed913ca48f5b6fd21a8f7c63a13"},"cell_type":"markdown","source":"Train/test split before the CountVectorizer to properly simulate the real world where future data contains words the model has not seen before. If the dataset is vectorized before the train/test split, the document-term matrix would contain every single feature in the training and test sets.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"061556ac4735155099e812bfa2bcfc2f0244a7dc"},"cell_type":"code","source":"# Set variables.\nX = df_A.drop('Label', axis=1) \ny = df_A['Label']\n\n# Split data into train and test sets.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"487228fbd4a559813896d28c8da715046497585b"},"cell_type":"code","source":"# Combine the train columns' text into a string.\ntrain_txt = []\nfor row in range(0, len(X_train.index)):\n    train_txt.append(' '.join(str(x) for x in X_train.iloc[row,1:26]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"384384de0ea77982f0e48aa6a75e0ab16b132a47"},"cell_type":"markdown","source":"Create a feature vector that will perform the classification task. Use ski-kit learn's CountVectorizer(), which takes the bag of words approach, to create a matrix of numbers to represent the text. Each news will be separated into words (i.e., tokens) and the number of times each token occurs in the news will be counted. Passing text to CountVectorizer's default settings will convert words to lowercase, remove punctuation, and exclude duplicates and words with less than two letters. ","outputs":[],"execution_count":null},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"975d8c0e2d157a7968a3359e6e84c0fdf841328f"},"cell_type":"code","source":"# Fit the train text into the vectorizer.\nvect_A.fit(train_txt)\n\n# Print the tokens and their corresponding column indices.\nprint(vect_A.vocabulary_)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f5b65c7165b5f1b080e8ee57ce93d7c57e3002a6"},"cell_type":"code","source":"# Transform the CountVectorizer object to create a document term matrix populated with token counts.\ndtm_A_train = vect_A.transform(train_txt)\nprint(dtm_A_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78d8cd795dfca43d317adac2ec2b07909192c07f"},"cell_type":"markdown","source":"The resulting matrix is in compressed sparse row format (i.e., sparse matrix), which means only the location and value of non-zero values is saved. For example, the first object (0, 1) indicates the 2nd feature (i.e., token/word) has one instance in the first text.","outputs":[],"execution_count":null},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"03a102c8637b246ca8f2bff4ac5ae3e6df0eea14"},"cell_type":"code","source":"# Show the equivalent dataframe (i.e., dense matrix version).\npd.DataFrame(dtm_A_train.toarray(), columns=vect_A.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f48308b822075fe6ee9027a4d1e1668ed1c8235"},"cell_type":"markdown","source":"In this particular dataframe, each row represents a date and each column a token or word, which indicates the default settings in CountVectorizer identified almost 30,000 unique tokens. The numbers represent the token's count on that particular date. \n\nToken counts are discrete features and are best suited with classification models. The model is trained by creating probability classes based on the tokens' instances in both market directions. As an example:\n\n|Token      | 1   | 0  | Total |\n|-----------|-----|----|-------|\n| gov't     | 50  | 50 | 100   |\n| functions | 75  | 25 | 100   |\n| well      | 90  | 10 | 100   |\n| total     | 215 | 85 | 300   |\n\nPositive class = 215/300 = 0.717 <br>\nNegative class = 85/300 = 0.283\n\nText that contains \"gov't functions well\" can be broken down as:\n\nPositive class = (0.717)(50/215)(75/215)(90/215) = 0.024 <br>\nNegative class = (0.283)(50/85)(25/85)(10/85) = 0.006\n\nThe higher positive class rating indicates the text can be classified in the positive category.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"ffbaa19e600a85da66a3534069013e45e3302fbb"},"cell_type":"code","source":"# Concatenate the test columns and convert to a document term matrix.\ntest_txt = []\nfor row in range(0, len(X_test.index)):\n    test_txt.append(' '.join(str(x) for x in X_test.iloc[row,1:26]))\n\ndtm_A_test = vect_A.transform(test_txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38f269e0241654977b92655636539589dd189245"},"cell_type":"code","source":"# Confirm train and test set sizes before modeling.\nprint('Train features: ', dtm_A_train.shape)\nprint('Train target: ', y_train.shape)\nprint('Test features: ', dtm_A_test.shape)\nprint('Test target: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a74324eef9aa3942752430496f3d94e367c49c03"},"cell_type":"markdown","source":"## B. Models","outputs":[],"execution_count":null},{"metadata":{"_uuid":"dd3a2197700fdcf29f8ccb42d7e558104a2773cd"},"cell_type":"markdown","source":"Run and compare the accuracy of different classification models.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"12353b5adfb56a6033818ee3e188b6b820ba2688"},"cell_type":"markdown","source":"### 1. Naive Bayes","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"f1b5c3d224f96cce16964ffe1d76254b775b5c9c"},"cell_type":"code","source":"# Fit the model.\nfrom sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(dtm_A_train, y_train)\n\n# Make predictions.\npred_B1 = mnb.predict(dtm_A_test)\n\n# Classification report.\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred_B1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f0748f79c4045bb9439a63ad0254347d3f1deb9"},"cell_type":"markdown","source":"### 2. Logistic Regression","outputs":[],"execution_count":null},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d5e2559f81bd02238a4f548759f4fbeb4bd8ff3f"},"cell_type":"code","source":"# Fit the model.\nfrom sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression()\nlogr.fit(dtm_A_train, y_train)\n\n# Make predictions.\npred_B2 = logr.predict(dtm_A_test)\n\n# Classification report.\nprint(classification_report(y_test, pred_B2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e94e5db8c3eb4d6bb1835f5cd7e3a166cd32437"},"cell_type":"markdown","source":"### 3. Random Forest","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"b48e832ebf87341fd7dd5f7321d339f2a0804dc1"},"cell_type":"code","source":"# Fit the model.\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(dtm_A_train, y_train)\n\n# Make predictions.\npred_B3 = rfc.predict(dtm_A_test)\n\n# Classification report.\nprint(classification_report(y_test, pred_B3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66ffa564467f2aa3284214662a653a242f07aaaa"},"cell_type":"markdown","source":"### 4. Gradient Boost","outputs":[],"execution_count":null},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"931ec2cb39267a6dd05bac94967588a6c62b1470"},"cell_type":"code","source":"# Fit the model.\nfrom sklearn import ensemble\ngbc = ensemble.GradientBoostingClassifier(n_estimators=500, max_depth=2, loss='deviance')\ngbc.fit(dtm_A_train, y_train)\n\n# Make predictions.\npred_B4 = gbc.predict(dtm_A_test)\n\n# Classification report.\nprint(classification_report(y_test, pred_B4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daf861e86ac72cc86fdbeea2cd1ee018f15ccbcd"},"cell_type":"markdown","source":"### 5. KNN","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"ca1a2b197fdd8f8a81fe7218dc766a3eba15fd1b"},"cell_type":"code","source":"# Fit the model.\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5, algorithm='auto')\nknn.fit(dtm_A_train, y_train)\n\n# Make predictions.\npred_B5 = knn.predict(dtm_A_test)\n\n# Classification report.\nprint(classification_report(y_test, pred_B5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d80d89b07e96a39efc14f4bf8f7ff126e3ab53d"},"cell_type":"markdown","source":"## C. Modified Models","outputs":[],"execution_count":null},{"metadata":{"_uuid":"918b48638fd3a85ff46dbe213ede09bf85bc7afb"},"cell_type":"markdown","source":"Run modifications on the models to determine if accuracy can be improved. To save on computation costs, only the top performing model(s) from the preceding section will be modified.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"36c2a90d92ff3a67fca238459c3bb50fda05a636"},"cell_type":"markdown","source":"### 1. TfidfVectorizer\n\nTfidfVectorizer is equivalent to CountVectorizer followed by TfidfTransformer, which transforms a count matrix to a normalized tf or tf-idf representation. Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval that has also found good use in document classification. The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"f60813a4c944ad06f4b969e9be663c4cb11f00f6"},"cell_type":"code","source":"# Set parameters.\nvect_C1 = TfidfVectorizer()\n\n# Fit the TfidfVectorizer and transform the train set.\ndtm_C1_train = vect_C1.fit_transform(train_txt)\nprint('Shape: ', dtm_C1_train.shape, '\\n')\n\n# Fit the gradient boost model.\nmnb.fit(dtm_C1_train, y_train)\n\n# Transform the test set.\ndtm_C1_test = vect_C1.transform(test_txt)\n\n# Make predictions.\npred_C1 = mnb.predict(dtm_C1_test)\n\n# Classification report.\nprint(classification_report(y_test, pred_C1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a0900c9a7f22a152c6a7e35e30f925546a29069"},"cell_type":"markdown","source":"### 2. SelectKBest\n\nUse an automated feature selection method that narrows down the features from the dense matrix version of the original CountVectorizer. The resulting features, which will be the most correlated to the target, will then be fed into the model.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"186f9fbf1a1c07239246e0a6963e6bc65dd40d0a"},"cell_type":"code","source":"# Convert the train and test document term matrices to their dense matrix versions.\ndf_C2_train = pd.DataFrame(dtm_A_train.toarray(), columns=vect_A.get_feature_names())\ndf_C2_test = pd.DataFrame(dtm_A_test.toarray(), columns=vect_A.get_feature_names())\n\n# Run Pipeline to initiate SelectKBest with the train set, which is carried over to the test set.\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.pipeline import Pipeline\nmnb_pipe = Pipeline([('reducer', SelectKBest(chi2, k=int(1/16*(df_C2_train.shape[1])))), ('clf', mnb)])\n\n# Fit, predict and print classification report.\nmnb_pipe.fit(df_C2_train, y_train)\npred_C2 = mnb_pipe.predict(df_C2_test)\nprint(classification_report(y_test, pred_C2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21ab37e8969a315cd81384699cd70ae00fcbcb64"},"cell_type":"markdown","source":"### 3. Grid Search\n\nGridSearchCV is a hyper-parameter fine tuning technique that exhaustively considers and searches for the optimum parameter combination, which are passed as the model's arguments. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"529797cbd679601cefcbe1e1eeadf7d2b9f4bcfd"},"cell_type":"markdown","source":"#### a. Naive Bayes","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"da1d7f92982b5d8933bf7fb2f2511f40f896bec0"},"cell_type":"code","source":"# Set the range or values of the parameters the GridSearchCV will iterate over.\nfrom sklearn.model_selection import GridSearchCV\nparam_mnb = {'alpha': [0.1, 1.0],\n             'fit_prior': ['True', 'False']}\ngs_mnb = GridSearchCV(mnb, param_mnb, cv= 5)\ngs_mnb.fit(dtm_A_train, y_train)\nprint(gs_mnb.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f29afdba15790787c9392bf0a3801b8831247ca6"},"cell_type":"markdown","source":"The optimum combination is the model's default settings, therefore, no further improvements can be made.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f8dcd007393bfe82b634157afc3691718fba0b76"},"cell_type":"markdown","source":"#### b. TfidfVectorizer","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"f3dc527ab32d7fc366cc1a0f3508951f05488764"},"cell_type":"code","source":"# Set parameter range.\npipe_tfidf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', mnb)])\nparam_tfidf = {'tfidf__min_df': [0.01, 0.05],\n               'tfidf__max_df': [0.95, 0.99],\n               'tfidf__ngram_range': [(1, 1), (1, 2), (2, 1), (2, 2)]}\ngs_tfidf = GridSearchCV(pipe_tfidf, param_tfidf, cv=5)\ngs_tfidf.fit(train_txt, y_train)\nprint (gs_tfidf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"6f9e70ba9d1115dfa8c4fab2cde93d17588ceda8"},"cell_type":"code","source":"# Run the TfidfVectorizer with the new parameters.\nvect_C3 = TfidfVectorizer(min_df=0.05, max_df=0.95, ngram_range = (2, 2))\ndtm_C3_train = vect_C3.fit_transform(train_txt)\nprint('Shape: ', dtm_C3_train.shape, '\\n')\nmnb.fit(dtm_C3_train, y_train)\ndtm_C3_test = vect_C3.transform(test_txt)\npred_C3 = mnb.predict(dtm_C3_test)\nprint(classification_report(y_test, pred_C3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35f405745f582d44795fb9bd44ff87e395562cb9"},"cell_type":"markdown","source":"The updated parameters reduced the tokens from almost 30,000 to over 280, a 99% decrease, but the model's accuracy barely improved.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"74c4a37cb0b5a6f737d63e52cccbf2c767ab7fc9"},"cell_type":"markdown","source":"## D. Conclusion","outputs":[],"execution_count":null},{"metadata":{"_uuid":"26ef5fb3f320e704cf46818cb4ae8669227d0e00"},"cell_type":"markdown","source":"Observations:<br>\nModels performed no better than random guessing.<br>\nModifications didn't do much to improve accuracy.<br>\nPractical implication: TfidfVectorizer was 100% accurate on positive market movement.\n\nFor improvement:<br>\nConduct a more exhaustive grid search.<br>\nDesign additional features that are specifically suited to the problem.<br>\nUse a more exhaustive stopwords list.<br>\nNormalize the corpus.<br>\n\nFor further study:<br>\nInvestigate a date + n approach, where n is the number of days or periods.<br>\nUse multiple sources of news.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"676fd8276b60239316f1f1da885df567c32b65a3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}