{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "224f05d5-03d7-5622-804c-2ec2fbbdba24"
      },
      "source": "#**Hey Kagglers, this notebook will go over a couple of basic Natural Language Processing techniques using the [scikit-learn](http://scikit-learn.org/stable/) library. Special thanks to [Aaron7sun](https://www.kaggle.com/aaron7sun) for providing the [dataset](https://www.kaggle.com/aaron7sun/stocknews) and \"assignment\" for us to do!**  \n#Thanks for reading and feel free to leave feedback. I'd like to make more tutorial notebooks like this, so let me know if you think there is anything I could improve.",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0c26a06e-66cd-e955-edd9-a0645bf64795"
      },
      "source": "----------",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "097e4b27-d3d4-72bf-954d-ca31aa060438"
      },
      "source": "# Notebook Prep",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6dbbf81e-cab9-5824-9552-72d3c8e20c51"
      },
      "source": "First things first, let's import the libraries we'll be using.  ",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "70ceca1b-8563-1894-1334-80ffb120f778"
      },
      "outputs": [],
      "source": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dcf16f8f-8c77-e21f-7ad9-9428dfb1e0e6"
      },
      "source": "[Pandas](http://pandas.pydata.org/) will make our data easy to look at and work with.  \n[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), part of scikit-learn, will take care of our NLP tasks.  \n[LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), also part of scikit-learn, will train and test our predictive models.",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ac9d751-e8f5-8940-ac71-426f10e0bd04"
      },
      "source": "----------",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f9e5522e-37ef-b384-11e9-42f9895d423c"
      },
      "source": "# Data Import",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9add28d0-c500-943e-64a8-48c8a131b6d0"
      },
      "source": "Now, let's [read](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) in the data with Pandas.  \nIf you're working in something other than a Kaggle notebook, be sure to change the file location.  \nFor this tutorial, we're just going to use the combined dataset that Aaron prepared for us, but you're welcome to import the other two CSV files if you want to combine them in a different way.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6da1c746-e849-fa5b-b0b6-4137586b533e"
      },
      "outputs": [],
      "source": "data = pd.read_csv('../input/Combined_News_DJIA.csv')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "228f34e9-9628-bddc-a78a-24adf933644b"
      },
      "source": "Next, let's take a look at the data with the [head](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html) method.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "01834ef5-43f9-7456-7a7d-c3194dd8f71a"
      },
      "outputs": [],
      "source": "data.head()"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4987908b-e98c-7c20-6c11-023f1bb03883"
      },
      "source": "We've got a lot of vaiables here, but the layout is pretty straight-forward.  \nAs a reminder, the Label variable will be a **1** if the DJIA **stayed the same or rose** on that date or \n **0** if the DJIA **fell** on that date.",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "30295018-7927-3c30-1b50-eb982fa3266e"
      },
      "source": "And finally, before we get started on the rest of the notebook, we need to split our data into a training set and a testing set. Per Aaron's instructions, we'll use all of the dates up to the end of 2014 as our training data and everything after as testing data.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b79d88e9-136b-c6c8-0ebb-cc564e24c26e"
      },
      "outputs": [],
      "source": "train = data[data['Date'] < '2015-01-01']\ntest = data[data['Date'] > '2014-12-31']"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ae3f526-4148-0146-1964-9688eb9b08f1"
      },
      "source": "----------",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ed38998a-6e7a-aa1c-fa3a-147245aacb5b"
      },
      "source": "# Text Preprocessing",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a5bb5d3e-99d6-3485-d30b-9a638ea5f75f"
      },
      "source": "Now that our data is loaded in, we need to clean it up just a little bit to prepare it for the rest of our analysis.  \nTo illustrate this process, look at how the example headline below changes from cell to cell.  \nDon't worry about the code too much here, since this example is only meant to be visual.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1a57d931-abd1-fc84-656c-6dcb7136fbe6"
      },
      "outputs": [],
      "source": "example = train.iloc[3,10]\nprint(example)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3cadd198-2b2d-bbf0-1c29-f15b2eee829d"
      },
      "outputs": [],
      "source": "example2 = example.lower()\nprint(example2)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4aa5b173-9021-e490-790f-1b00a2b31cb7"
      },
      "outputs": [],
      "source": "example3 = CountVectorizer().build_tokenizer()(example2)\nprint(example3)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9cb20699-bef3-1a23-f46d-1f863dd3c00b"
      },
      "outputs": [],
      "source": "pd.DataFrame([[x,example3.count(x)] for x in set(example3)], columns = ['Word', 'Count'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c824f61-f8a5-66d6-f1a7-cff3b64c4190"
      },
      "source": "Were you able to see everything that changed?  \nThe process involved:  \n- Converting the headline to lowercase letters  \n- Splitting the sentence into a list of words  \n- Removing punctuation and meaningless words  \n- Transforming that list into a table of counts",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4663088a-e834-2512-1f87-ca768f49ae2c"
      },
      "source": "What started as a relatively \"messy\" sentence has now become an neatly organized table!  \nAnd while this may not be exactly what goes on behind the scenes with scikit-learn, this example should give you a pretty good idea about how it works.",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4ec409c9-443f-56eb-5194-63b6db6ee00f"
      },
      "source": "So now that you've seen what the text processing looks like, let's get started on the fun part, modeling!",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d142f2bc-f61e-4829-4c13-6da37345eea6"
      },
      "source": "----------",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cc8aa11c-93cd-1c4b-bf03-21b20a652ecc"
      },
      "source": "# Basic Model Training and Testing",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "adcff1c8-9d37-4329-aaeb-6a5a7ac6a53d"
      },
      "source": "As mentioned previously, scikit-learn is going to take care of all of our preprocessing needs.  \nThe tool we'll be using is CountVectorizer, which takes a single list of strings as input, and produces word counts for each one.",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2119fab1-da05-c807-85bb-247a0f378363"
      },
      "source": "You might be wondering if our dataframe meets this \"single list of strings\" criteria, and the answer to that is... it doesn't!  \nIn order to meet this criteria, we'll use the following [for loop](https://wiki.python.org/moin/ForLoop) to iterate through each row of our dataset, [combine](https://docs.python.org/3.5/library/stdtypes.html#str.join) all of our headlines into a single string, then [add](https://docs.python.org/3.5/tutorial/datastructures.html) that string to the list we need for CountVectorizer.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c8ed1f2-2dcf-1015-05b7-edfb95773b4e"
      },
      "outputs": [],
      "source": "trainheadlines = []\nfor row in range(0,len(train.index)):\n    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "edfbef25-8c6b-5e78-aac5-f9f47df72f25"
      },
      "source": "With our headlines formatted, we can set up our CountVectorizer.  \nTo start, let's just use the default settings and see how it goes!  \nBelow, we'll name our default vectorizer, then [use](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform) it on our list of combined headlines.  \nAfter that, we'll take a look at the size of the result to see how many words we have.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18f44a5c-3f39-5ce4-5f0b-9aa86ac29f5d"
      },
      "outputs": [],
      "source": "basicvectorizer = CountVectorizer()\nbasictrain = basicvectorizer.fit_transform(trainheadlines)\nprint(basictrain.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cae0096e-5a09-2304-184e-e4afebe753b8"
      },
      "source": "Wow! Our resulting table contains counts for 31,675 different words!",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "781fb862-f8c2-1e5e-af6f-16f6922eb6c8"
      },
      "source": "Now, let's train a logistic regression model using this data.  \nIn the cell below, we're simply naming our model, then [fitting](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit) the model based on our X and Y values.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f4d9232-5b01-4c65-a933-eda036a25216"
      },
      "outputs": [],
      "source": "basicmodel = LogisticRegression()\nbasicmodel = basicmodel.fit(basictrain, train[\"Label\"])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5e43218f-9c22-bb3c-8e3f-964857c1c7bb"
      },
      "source": "Our model is ready to go, so let's set up our test data.  \nHere, we're just going to repeat the steps we used to prep our training data, then [predict](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict) whether the DJIA increased or decreased for each day in the test dataset.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2c51dcd0-0d7a-41d5-6445-0eadf1d18e87"
      },
      "outputs": [],
      "source": "testheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nbasictest = basicvectorizer.transform(testheadlines)\npredictions = basicmodel.predict(basictest)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "42e8b6a7-b138-ac21-a64b-3ddb3717aa19"
      },
      "source": "The predictions are set, so let's use a [crosstab](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html) to take a look at the results!",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2db3b5d0-f86a-5d23-3ae5-7fe725dbd4ab"
      },
      "outputs": [],
      "source": "pd.crosstab(test[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "acd37c68-a42c-0f95-cd05-cc9e2aae6a41"
      },
      "source": "Prediction accuracy is just over 42%. It seems like this model isn't too reliable.  \nNow, let's also take a look at the coefficients of our model. (Excellent request from [Lucie](https://www.kaggle.com/luciegattepaille)!)",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3345bcdf-e9e5-a1da-18e7-21ba26824a10"
      },
      "source": "The cell below will get a list of the names from our CountVectorizer and a list of the coefficients from our model, then combine the two lists into a Pandas dataframe.  \nOnce that's made, we can sort it and check out the top 10 positive and negative coefficients.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d2c7daa-8e28-9849-1fc6-4b0fab9967cf"
      },
      "outputs": [],
      "source": "basicwords = basicvectorizer.get_feature_names()\nbasiccoeffs = basicmodel.coef_.tolist()[0]\ncoeffdf = pd.DataFrame({'Word' : basicwords, \n                        'Coefficient' : basiccoeffs})\ncoeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\ncoeffdf.head(10)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5ea266d6-6a99-10ee-a109-7e82406225df"
      },
      "outputs": [],
      "source": "coeffdf.tail(10)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0a1d664f-9f87-f30d-3c90-83c2a774257b"
      },
      "source": "Our most positive words don't seem particularly interesting, however there are some negative sounding words within our bottom 10, such as \"sanctions,\" \"low,\" and \"hacking.\"  \nMaybe the saying \"no news is good news\" is true here?",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aabb5e54-c8b5-e952-0701-8fe3febd64b3"
      },
      "source": "----------",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d4d97261-5f0c-bbd6-f3bd-87cd6f39f224"
      },
      "source": "# Advanced Modeling",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8d86b6ae-68f1-90d7-22c3-6ab1a9dfe9e9"
      },
      "source": "The technique we just used is known as a **bag-of-words** model. We essentially placed all of our headlines into a \"bag\" and counted the words as we pulled them out.  \nHowever, most people would agree that a single word doesn't always have enough meaning by itself.  \nObviously, we need to consider the rest of the words in the sentence as well!  ",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fd5f8e9e-7936-0a71-78cf-89d8c1ab61ae"
      },
      "source": "This is where the **n-gram** model comes in.  \nIn this model, n represents the length of a sequence of words to be counted.  \nThis means our bag-of-words model was the same as an n-gram model where n = 1.  \nSo now, let's see what happens when we run an n-gram model where n = 2.",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aa207db9-951a-29da-2fe9-021502e24780"
      },
      "source": "Below, we'll create a new CountVectorizer with the n-gram parameter set to 2 instead of the default value of 1.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1bfe7bd0-6330-23e1-da05-41a072249291"
      },
      "outputs": [],
      "source": "advancedvectorizer = CountVectorizer(ngram_range=(2,2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a84ee6fa-d38b-ba5d-ed08-cd32ae7b479b"
      },
      "source": "Now that we've run our vectorizer, let's see what our data looks like this time around.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "be873375-6cba-796c-69a1-5218118af3df"
      },
      "outputs": [],
      "source": "print(advancedtrain.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c0f41320-bb02-d8d5-7a03-9e64e66c978b"
      },
      "source": "This time we have 366,721 unique variables representing two-word combinations!  \nAnd here I thought last time was big...",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "20031a48-cbe5-4b60-116c-6a90e9017ac8"
      },
      "source": "So, just like last time, let's name and fit our regression model.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "82609b09-1c0d-92e5-739a-dad9859ea2ed"
      },
      "outputs": [],
      "source": "advancedmodel = LogisticRegression()\nadvancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "685a6540-2e70-0aa3-d6a2-50df0750d1c9"
      },
      "source": "And again like last time, let's transform our test data and make some predictions!",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e30825f-fd0b-f112-34d2-759ddab1de6b"
      },
      "outputs": [],
      "source": "testheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\nadvancedtest = advancedvectorizer.transform(testheadlines)\nadvpredictions = advancedmodel.predict(advancedtest)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "452d263e-e523-596a-927d-8fefcec268c7"
      },
      "source": "Crosstab says...!",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "16c834ce-8bbc-cb82-58c4-5d11cbf64c64"
      },
      "outputs": [],
      "source": "pd.crosstab(test[\"Label\"], advpredictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "67f0954f-9117-2852-9316-e6c3c4de5715"
      },
      "source": "This time we're up to nearly 57% prediction accuracy.  \nWe might only consider this a slight improvement, but keep in mind that we've barely scratched the surface of NLP here, and we haven't even touched more advanced machine learning techniques.  \nLet's check out our coefficients again as well!",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5baeabb3-03b4-5856-c9c3-a8f5b90acaf6"
      },
      "outputs": [],
      "source": "advwords = advancedvectorizer.get_feature_names()\nadvcoeffs = advancedmodel.coef_.tolist()[0]\nadvcoeffdf = pd.DataFrame({'Words' : advwords, \n                        'Coefficient' : advcoeffs})\nadvcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\nadvcoeffdf.head(10)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b80cc33e-8016-3409-e6ff-8fdcb5481db4"
      },
      "outputs": [],
      "source": "advcoeffdf.tail(10)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "98b308d6-d359-97b4-401b-5ee2a8825219"
      },
      "source": "It seems that the results this time were fairly similar. Most of the positive bigrams are unremarkable, while a few of the negative ones like \"bin laden\" and \"threatens to\" could be considered to carry some negative meaning.",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1e2d5df8-c48f-6edf-785d-85a9f8d92a26"
      },
      "source": "----------",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "91b9197d-ee3a-6b3c-0858-a63ac09c7f50"
      },
      "source": "# What's next?",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a2ae4a28-1023-2586-c445-8c5e8fd6ecc6"
      },
      "source": "If you'd like to keep going forward with this notebook, here are a couple of project ideas:  \n- Experiment with different n values using the n-gram model  \n- Use previous days' headlines to truly \"predict\" whether the DJIA will rise or fall  \n- Try a machine learning algorithm instead of the basic logistic regression used in this notebook",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a0ca7639-57ae-cbed-c514-01c07667650e"
      },
      "source": "Thanks again for reading! I hope you found this helpful!",
      "outputs": []
    }
  ]
}