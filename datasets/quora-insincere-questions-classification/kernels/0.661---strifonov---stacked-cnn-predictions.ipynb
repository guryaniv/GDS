{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, Dense, Dropout, Concatenate, Lambda, Flatten\nfrom keras.layers import GlobalMaxPool1D\nfrom keras.models import Model\n\n\nimport tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df7215ff9c344e3aa28bc8ff35518e5425da19e4"},"cell_type":"markdown","source":"# Combinations\nThis kernel would contain a combination of previousle tested models. For example, it may be useful to combine pretrained embeddings with ones that were trained on this particular datase."},{"metadata":{"_uuid":"0d4287da9f0fd92df1144c52c69484125726da6c"},"cell_type":"markdown","source":"# Embeddings"},{"metadata":{"_uuid":"a205fab8b96c97dd55ae127bd06808a4248c88f7","trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 70\nMAX_WORDS = 95000\nEMBEDDINGS_TRAINED_DIMENSIONS = 100\nEMBEDDINGS_LOADED_DIMENSIONS = 300","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0c2aac8432c093a9e141eed3fd72e37c9397c2a"},"cell_type":"markdown","source":"## Pretrained\nLoad (one of) the embeddings"},{"metadata":{"_uuid":"0651b1104f3aec8da85cdadac71442dd83617a8f","trusted":true},"cell_type":"code","source":"def load_embeddings(file):\n    embeddings = {}\n    with open(file) as f:\n        def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n        embeddings = dict(get_coefs(*line.split(\" \")) for line in f)\n        \n    print('Found %s word vectors.' % len(embeddings))\n    return embeddings","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"738d1e89d9f157aa954a1eb335139d0e907182dd"},"cell_type":"markdown","source":"# Data\nLoad the data."},{"metadata":{"_uuid":"c6c3339bb66c31947ac0f19a31d2e8afaf512d3e","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20dd051d411545c5d9f4fef74a281985bdcb04c4","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 512\nQ_FRACTION = 1\nquestions = df_train.sample(frac=Q_FRACTION)\nquestion_texts = questions[\"question_text\"].values\nquestion_targets = questions[\"target\"].values\ntest_texts = df_test[\"question_text\"].fillna(\"_na_\").values\n\nprint(f\"Working on {len(questions)} questions\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"631ba595856aac8ddde49c4a0964bc131136a064","trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_WORDS)\ntokenizer.fit_on_texts(list(df_train[\"question_text\"].values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1ebb68e362da3d7018d13417e9de4ab6532bc9c","trusted":true},"cell_type":"code","source":"# custom_embeddings = train_w2v(question_texts, epochs=5)\npretrained_embeddings = load_embeddings(\"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75d212d515a7abec924b779cdde966f7655fbc00","trusted":true},"cell_type":"code","source":"from collections import defaultdict\n\ndef create_embedding_weights(tokenizer, embeddings, dimensions):\n    not_embedded = defaultdict(int)\n    \n    word_index = tokenizer.word_index\n    words_count = min(len(word_index), MAX_WORDS)\n    embeddings_matrix = np.zeros((words_count, dimensions))\n    for word, i in word_index.items():\n        if i >= MAX_WORDS:\n            continue\n        if word not in embeddings:\n            not_embedded[word] = not_embedded[word] + 1\n            continue\n        embedding_vector = embeddings[word]\n        if embedding_vector is not None:\n            embeddings_matrix[i] = embedding_vector\n            \n    print(sorted(not_embedded, key=not_embedded.get)[:10])\n    return embeddings_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a16cebc3748a38b26726ecd5f41fa1fe732d1e5","trusted":true},"cell_type":"code","source":"# custom_emb_weights = create_embedding_weights(tokenizer, custom_embeddings, EMBEDDINGS_TRAINED_DIMENSIONS)\npretrained_emb_weights = create_embedding_weights(tokenizer, pretrained_embeddings, EMBEDDINGS_LOADED_DIMENSIONS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a04e769dee4e3a0ebe350c9082ea5d38687a917c"},"cell_type":"markdown","source":"# Model\nConstruct the model to use, e.g. a simple NN"},{"metadata":{"_uuid":"7d5235d72f2cb1fdc24a81db9e2e3408e8fec1df"},"cell_type":"markdown","source":"# Model evaluation\n\n\n"},{"metadata":{"_uuid":"790dc08dcbb0c65fdf840819ee4b61c96fd100d2","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\n\nTHRESHOLD = 0.35\n\nclass EpochMetricsCallback(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.f1s = []\n        self.precisions = []\n        self.recalls = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        predictions = self.model.predict(self.validation_data[0])\n        predictions = (predictions > THRESHOLD).astype(int)\n        predictions = np.asarray(predictions)\n        targets = self.validation_data[1]\n        f1 = metrics.f1_score(targets, predictions)\n        precision = metrics.precision_score(targets, predictions)\n        recall = metrics.recall_score(targets, predictions)\n\n        print(\" - F1 score: {0:.4f}, Precision: {1:.4f}, Recall: {2:.4f}\"\n              .format(f1, precision, recall))\n        self.f1s.append(f1)\n        self.precisions.append(precision)\n        self.recalls.append(recall)\n        return\n    \ndef display_model_history(history):\n    data = pd.DataFrame(data={'Train': history.history['loss'], 'Test': history.history['val_loss']})\n    ax = sns.lineplot(data=data, palette=\"pastel\", linewidth=2.5, dashes=False)\n    ax.set(xlabel='Epoch', ylabel='Loss', title='Loss')\n    plt.show()\n\ndef display_model_epoch_metrics(epoch_callback):\n    fig, axes = plt.subplots(1, 3, figsize = (15, 5), sharey=False)\n    a1, a2, a3 = axes\n    \n    a1.set_title('F1')\n    a1.set(xlabel='Epoch', title='F1')\n    sns.lineplot(data=pd.DataFrame(data={'F1': epoch_callback.f1s}),\n                 palette=\"pastel\", linewidth=2.5, dashes=False, ax=a1, legend=False)\n\n    a2.set_title('Precision')\n    a2.set(xlabel='Epoch', title='Precision')\n    sns.lineplot(data=pd.DataFrame(data={'Precision': epoch_callback.precisions}),\n                 palette=\"pastel\", linewidth=2.5, dashes=False, ax=a2, legend=False)\n\n    a3.set_title('Recall')\n    a3.set(xlabel='Epoch', title='Recall')\n    sns.lineplot(data=pd.DataFrame(data={'Recall': epoch_callback.recalls}),\n                 palette=\"pastel\", linewidth=2.5, dashes=False, ax=a3, legend=False)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e37b7b24d69b340fd238c8b4fa16c0e6561eab5b"},"cell_type":"markdown","source":"# Training\nTrain the model. Also, experiment with different versions"},{"metadata":{"_uuid":"89360ffd20dd998f54eb6c4e638201caf21df8a2"},"cell_type":"markdown","source":"## Prepare the data first\nE.g. the tokenized words as well as the nlp features"},{"metadata":{"_uuid":"24a65f899ac175ac793c1398448ce52130b520c6","trusted":true},"cell_type":"code","source":"X = pad_sequences(tokenizer.texts_to_sequences(question_texts),\n                        maxlen=MAX_SEQUENCE_LENGTH)\nY = question_targets\n\ntest_word_tokens = pad_sequences(tokenizer.texts_to_sequences(test_texts),\n                       maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"568f3bdff2829145383c4e261275f362a2bd5217"},"cell_type":"markdown","source":"## Alternative models"},{"metadata":{"trusted":true,"_uuid":"e2cf90badb9f66d302e9592875dc477aad3712d7"},"cell_type":"code","source":"from keras.layers import Conv1D, Conv2D, Reshape, MaxPool1D, MaxPool2D, BatchNormalization\n\ndef make_model(filter_size, num_filters):\n    tokenized_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name=\"tokenized_input\")\n    \n    pretrained = Embedding(MAX_WORDS,\n                           EMBEDDINGS_LOADED_DIMENSIONS,\n                           weights=[pretrained_emb_weights],\n                           trainable=False)(tokenized_input)\n\n    pretrained = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDINGS_LOADED_DIMENSIONS, 1))(pretrained)\n    conv_0 = Conv2D(num_filters, kernel_size=(filter_size, EMBEDDINGS_LOADED_DIMENSIONS), kernel_initializer='he_normal', activation='tanh')(pretrained)\n    maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_size + 1, 1))(conv_0)\n\n    d0 = Dense(4)(maxpool_0)\n\n    x = Flatten()(d0)\n    x = Dropout(0.3)(x)\n    x = BatchNormalization()(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=[tokenized_input], outputs=out)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9f6a55d479ac67bbb05e0267de04922b9fe4f34","trusted":true,"scrolled":true},"cell_type":"code","source":"import random\nfrom sklearn.model_selection import train_test_split\n\n# filter_sizes = range(1, 11)\nfilter_sizes = [1, 2, 3, 5]\nnum_filters = 45\n\ntrain_predictions = []\ntest_predictions = []\nkaggle_predictions = []\n\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.15)\n\nfor f in filter_sizes:\n    print(\"CNN MODEL WITH FILTER OF SIZE {0}\".format(f))\n    epoch_callback = EpochMetricsCallback()\n    model = make_model(f, num_filters)\n    \n    # use a lot of validation data on purpose so that the models would be trained on a noticeably less than the whole dataset\n    x, val_x, y, val_y = train_test_split(train_X, train_Y, test_size=0.015)\n    history = model.fit(\n        x=x, y=y, validation_data=(val_x, val_y),\n        batch_size=512, epochs=7, callbacks=[epoch_callback], verbose=2)\n    display_model_history(history)\n    display_model_epoch_metrics(epoch_callback)\n    \n    train_predictions.append(model.predict([train_X], batch_size=1024, verbose=2))\n    test_predictions.append(model.predict([test_X], batch_size=1024, verbose=2))\n    kaggle_predictions.append(model.predict([test_word_tokens], batch_size=1024, verbose=2))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"773f79bb1eab30a1ddd4f544737e83d1b8482545"},"cell_type":"markdown","source":"## Stack\nA simple model on top of the other models' predictions"},{"metadata":{"trusted":true,"_uuid":"62f26814d86b23249610b6a21d9399f0fa9f230d"},"cell_type":"code","source":"def stack_models(predictions, targets):\n    layer_size = len(predictions)\n    inp = Input(shape=(layer_size,))\n    d0 = Dropout(0.2)(inp)\n    d0 = Dense(pow(layer_size, 2))(d0)\n    d1 = Dropout(0.2)(d0)\n    d1 = Dense(2 * layer_size)(d1)\n    b = BatchNormalization()(d1)\n    out = Dropout(0.2)(b)\n    out = Dense(1, activation='sigmoid')(out)\n\n    model = Model(inputs=inp, outputs=out)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n\n    epoch_callback = EpochMetricsCallback()\n    # TODO find a more idiomatic way to transform...\n    x = np.array(list(zip(*np.squeeze(predictions))))\n    y = targets\n    print(np.shape(x))\n    print(np.shape(y))\n    \n    history = model.fit(x=x, y=y, epochs=5, callbacks=[epoch_callback], validation_split=0.02, verbose=2)\n    display_model_history(history)\n    display_model_epoch_metrics(epoch_callback)\n    \n    return model\n    \nmodel = stack_models(test_predictions, test_Y)\n\nstacked_kaggle_predictions = np.array(list(zip(*np.squeeze(kaggle_predictions))))\nstacked_kaggle_predictions = model.predict(stacked_kaggle_predictions, batch_size=1024)\n\nstacked_test_predictions = np.array(list(zip(*np.squeeze(test_predictions))))\nstacked_test_predictions = model.predict(stacked_test_predictions, batch_size=1024)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dbe359ff06904c47f9438a79145936fe8347fba"},"cell_type":"markdown","source":"# Results"},{"metadata":{"_uuid":"0691949cc07aadc8baaf11a0e3018853e87f9073","scrolled":true,"trusted":true},"cell_type":"code","source":"# df_out = pd.DataFrame({\"qid\":df_test[\"qid\"].values})\n# df_out['prediction'] = (kaggle_predictions > THRESHOLD).astype(int) \n# df_out.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f130554d307b31a68422e03716498eacf52306c","trusted":true},"cell_type":"code","source":"# Adjust the threshold\nprint(np.shape(stacked_test_predictions))\n\nf1s = []\nprecisions = []\nrecalls = []\n\nTs = [x * 0.01 for x in range(0, 50)]\nfor t in Ts:\n    pred = (stacked_test_predictions > t).astype(int)\n    f1s.append(metrics.f1_score(test_Y, pred))\n    precisions.append(metrics.precision_score(test_Y, pred))\n    recalls.append(metrics.recall_score(test_Y, pred))\n\n\nplt.plot(Ts, f1s)\nplt.plot(Ts, precisions)\nplt.plot(Ts, recalls)\nplt.title('Threshold levels')\nplt.ylabel('Value')\nplt.xlabel('Threshold')\nplt.legend(['F1', 'Precision', 'Recall'])\nplt.show()\n\nthresh = Ts[np.argmax(f1s)]\npred = (stacked_test_predictions > thresh).astype(int)\nf1 = metrics.f1_score(test_Y, pred)\nprint(\"Test F1 {0:.4f} at threshold {1:.3f}\".format(f1, thresh))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33a18293ead982bd7a96b0fbd29669d2f8aff0c6"},"cell_type":"code","source":"df_out = pd.DataFrame({\"qid\":df_test[\"qid\"].values})\ndf_out['prediction'] = (stacked_kaggle_predictions > thresh).astype(int)\ndf_out.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"120148f98c59900b58d3e468ab99d95781e6bca4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}