{"cells":[{"metadata":{"_uuid":"5ffb21374c7cf4b98e7239045ef9bf312effee25"},"cell_type":"markdown","source":"# Ensamble  Model\nHere we are attempting to combine a number of different approaches\n"},{"metadata":{"trusted":true,"_uuid":"c20fa6deeac9d374c98774abd90bdc76b023ee63","scrolled":false},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn import svm\n\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\n\n# returns the training data DataFrames as a tuple of:\n(market_train_df, news_train_df) = env.get_training_data()\n\n# size of total data\nprint(\"Market Train Size: \", market_train_df.shape)\nprint(\"News Train Size: \", news_train_df.shape)\n\n# we only care about the market data here\nmarket_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01b046e708f1608c4c5ee17038ddf760e82e3127"},"cell_type":"markdown","source":"## Models\n\n#### A - Analytical\n#### B - News\n#### C - Combinational"},{"metadata":{"_uuid":"279cc3bdbb68f8356bae2cd5189dff29ecb6deca"},"cell_type":"markdown","source":"## B - Pre-Process Functions"},{"metadata":{"trusted":true,"_uuid":"8a7b5ce35094a073f516a2991f27fb404d85afa8"},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom datetime import datetime, timedelta\n\ndef preprocess_market_data(market_df):\n    market_features = ['time', 'assetCode', 'assetName', 'returnsOpenNextMktres10']\n    market_df = market_df[market_features]\n    \n    market_df['time'] = market_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n    \n    return market_df\n\ndef drop_news_wo_news(news_train):\n    news_train.drop(news_train[news_train[\"headlineLen\"] == 0].index,inplace = True)\n    news_train.drop(news_train[news_train[\"sentenceCount\"] == 0].index,inplace = True)\n    news_train.drop(news_train[news_train[\"bodySize\"] == 0].index,inplace = True)\n    news_train.drop(news_train[news_train[\"wordCount\"] == 0].index,inplace = True)\n    news_train.drop([\"headline\"], axis=1, inplace=True)\n    return news_train\n\ndef preprocess_news(news_train_df1):\n    #news_features = ['time', 'firstCreated', 'headline', 'urgency', 'takeSequence', 'assetCodes', 'assetName', 'firstMentionSentence', 'sentenceCount', 'wordCount', 'relevance', 'sentimentWordCount', 'sentimentClass']\n    #news_train_df1 = news_train_df[news_features]\n    \n    drop_list = [\n        'audiences', 'subjects', 'firstCreated', 'sourceTimestamp','marketCommentary'\n    ]\n    news_train_df1 = news_train_df1.drop(drop_list, axis=1, inplace=False)\n    news_train_df1['headlineLen'] = news_train_df1['headline'].apply(lambda x: len(x))\n    news_train_df1 = drop_news_wo_news(news_train_df1)\n    \n    news_train_df1['time'] = news_train_df1.time.dt.strftime(\"%Y%m%d\").astype(int)\n    news_train_df1['position'] = news_train_df1['firstMentionSentence'] / news_train_df1['sentenceCount']\n    news_train_df1['coverage'] = news_train_df1['sentimentWordCount'] / news_train_df1['wordCount']\n    \n    news_train_df1['assetCode'] = news_train_df1['assetCodes'].map(lambda x: list(eval(x))[0])\n    \n    news_train_df1['assetCodesLen'] = news_train_df1['assetCode'].apply(lambda x: len(x))\n    \n    news_train_df1['asset_sentiment_count'] = news_train_df1.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    news_train_df1['asset_sentence_mean'] = news_train_df1.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    \n    ##\n    for col in ['headlineTag', 'provider', 'sourceId']:\n        news_train_df1[col], uniques = pd.factorize(news_train_df1[col])\n        del uniques\n    \n    ##\n    lbl = {k: v for v, k in enumerate(news_train_df1['headlineTag'].unique())}\n    news_train_df1['headlineTagT'] = news_train_df1['headlineTag'].map(lbl)\n        \n    return news_train_df1\n\ndef group_news(newsdf):\n    newsgp = newsdf.groupby(['time','assetCode'], sort=False).aggregate(np.mean).reset_index()\n    return newsgp\n\ndef preprocess_market_test_data(market_df):\n    market_features = ['time', 'assetCode', 'assetName']\n    market_df = market_df[market_features]\n    \n    market_df['time'] = market_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n    \n    return market_df\n\ndef prepare_data(mark_df, new_df):\n    mkt_df = preprocess_market_test_data(mark_df)\n    new_df = preprocess_news(new_df)\n    newsgp = group_news(new_df)\n    cdf = mkt_df.merge(newsgp, how='left', on=['assetCode', 'time'])\n    return cdf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"882925d00941bf4d72f4e214cbf5c0633dfc8cd8"},"cell_type":"markdown","source":"## B - Fitting Model"},{"metadata":{"trusted":true,"_uuid":"6f9cbed7f2c7cf9f41fd25df3d7d3ec9f5943bd6"},"cell_type":"code","source":"def evaluate_model(df, target, train_index, test_index, params):\n    model = LGBMClassifier(**params)\n    model.fit(df.iloc[train_index], target.iloc[train_index])\n    return log_loss(target.iloc[test_index], model.predict_proba(df.iloc[test_index]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b84f862f8002af2b41a922b6175a6879d214761"},"cell_type":"code","source":"def train_news_model(market_train_df, news_train_df):\n\n    mkt_df = preprocess_market_data(market_train_df)\n    news_df = preprocess_news(news_train_df)\n    newsgp = group_news(news_df)\n    cdf = mkt_df.merge(newsgp, how='left', on=['assetCode', 'time'])\n\n    # training to find features\n    num_target = cdf.returnsOpenNextMktres10.astype('float32')\n    bin_target = (cdf.returnsOpenNextMktres10 >= 0).astype('int8')\n\n    cdf.drop(['returnsOpenNextMktres10','time', 'assetCode', 'assetName'], \n            axis=1, inplace=True)\n    cdf_train = cdf.astype('float32')\n    # train data\n    train_index, test_index = train_test_split(cdf.index.values, test_size=0.2)\n\n    param_grid = {\n        'learning_rate': [0.05, 0.02, 0.01],\n        'num_leaves': [25, 38, 63],\n        'n_estimators': [100, 200, 400],\n        'min_child_samples': [5, 10, 20, 40, 100],\n        'colsample_bytree': [0.8, 0.9, 1],\n        'subsample': [0.8, 0.9, 1],\n        'reg_alpha': [0.1, 0.2, 0.4, 0.6, 0.8],\n        'reg_lambda': [0.1, 0.2, 0.4, 0.6, 0.8],\n    }\n\n    best_eval_score = 0\n    for i in range(5):\n        params = {k: np.random.choice(v) for k, v in param_grid.items()}\n        score = evaluate_model(cdf_train, bin_target, train_index, test_index, params)\n        if score < best_eval_score or best_eval_score == 0:\n            best_eval_score = score\n            best_params = params\n        print(best_eval_score)\n    print(\"Best evaluation logloss\", best_eval_score)\n\n    clf = LGBMClassifier(**best_params)\n    clf.fit(cdf_train, bin_target)\n\n    feats_model = cdf_train.columns\n    \n    return clf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3f86632c610cdf3024607e172d4e9bc4d80f7ba"},"cell_type":"markdown","source":"## B - Evaluation Functions"},{"metadata":{"trusted":true,"_uuid":"62d0dcd69e04e4788eb402c92e747d772386ea59"},"cell_type":"code","source":"def preprocess_market_test_data(market_df):\n    market_features = ['time', 'assetCode', 'assetName']\n    market_df = market_df[market_features]\n    \n    market_df['time'] = market_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n    \n    return market_df\n\ndef prepare_data(mark_df, new_df):\n    mkt_df = preprocess_market_test_data(mark_df)\n    #mkt_df = mean_volume(mkt_df)\n    #mkt_df = process_ma(mkt_df)\n    new_df = preprocess_news(new_df)\n    newsgp = group_news(new_df)\n    cdf = mkt_df.merge(newsgp, how='left', on=['assetCode', 'time'])\n    \n    return cdf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64781d373cb1d84dd83c71916b1922bca727113b"},"cell_type":"markdown","source":"## B - Prediction Functions"},{"metadata":{"trusted":true,"_uuid":"0b745c24cb93dea7db9b10e8d1c7078ac4f9f4e9"},"cell_type":"code","source":"def news_predict(market_obs_df, news_obs_df, predictions, trainedModel):    \n    cdf_test = prepare_data(market_obs_df, news_obs_df)\n    cdf_test = cdf_test[cdf_test.assetCode.isin(predictions.assetCode)]\n    feats = [e for e in cdf_test.columns if e not in ['date', 'assetCode', 'assetName', 'time']]\n    preds = trainedModel.predict_proba(cdf_test[feats])[:, 1] * 2 - 1\n    predsdf = pd.DataFrame({'ast':cdf_test['assetCode'],'conf':preds})\n    \n    # set\n    predictions['confidenceValue'][predictions['assetCode'].isin(predsdf.ast)] = predsdf['conf'].values    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8397d9cf7314324a4a4eae27c550e80eed45183"},"cell_type":"markdown","source":"## A - Feature Function"},{"metadata":{"trusted":true,"_uuid":"b6c247ee17db08aafde63dcc97944360e3fbf0d9"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef analysis_get_features(market_data, byday=False, trainInfo=None):\n    # for full training set feature creation\n    if(not byday):\n        # assign uids to each asset \n        uAssestCode = pd.unique(market_data.assetCode)    \n        uidList     = np.linspace(1.0, uAssestCode.shape[0], num=uAssestCode.shape[0])\n\n        # feature 0 - map from assetCode to uid    \n        uidMap = {}\n        for A, B in zip(uAssestCode, uidList):\n            uidMap[A] = B\n\n        aUID = np.zeros(market_data.shape[0])\n        for i, item in enumerate(market_data.assetCode):\n            aUID[i] = uidMap[item]\n\n        # feature 1, 2 - gain, gainb    \n        gain  = market_data.close - market_data.open    \n        gainb = np.zeros(gain.shape[0])\n        # classify\n        gainb[gain > 0] = 1\n\n        # feature 3 - volumeb\n        v   = market_data.volume\n        npv = np.array(v)    \n        vbins, ved = np.histogram(v, bins=20)\n        volumeb    = np.zeros(v.shape[0])\n\n        # create classes for bins\n        for i in range(1, ved.shape[0] - 1): \n            volumeb[np.logical_and(ved[i] < npv, npv < ved[i+1])] = i\n\n        # features to dataframe\n        #Xdict = {1: aUID, 2: gain, 3: gainb, 4: volumeb}\n        Xdict = {1: gain, 2: gainb, 3: volumeb}\n        X     = pd.DataFrame(Xdict)\n        \n        # save off training information\n        trainInfo = (uidList, uidMap, ved)\n        \n    # for one off feature creation\n    else:                \n        # feature 0\n        auid = np.zeros(market_data.assetCode.shape[0])\n        for i, assetCode in enumerate(market_data.assetCode):\n            # look for uid\n            if assetCode in trainInfo[1]:\n                uid = trainInfo[1][assetCode]\n            else:\n                # if its a new asset code create a new uid\n                newUID = trainInfo[0].max() + 1\n                np.append(trainInfo[0], newUID)\n                \n                # update dict\n                trainInfo[1][assetCode] = newUID\n                uid = newUID\n                \n            # set uid\n            auid[i] = uid\n        \n        # feature 1, 2 - gain, gainb\n        gain  = market_data.close - market_data.open    \n        gainb = np.zeros(gain.shape[0])\n        # classify\n        gainb[gain > 0] = 1\n        \n        # feature 3 - volumeb\n        v   = market_data.volume\n        npv = np.array(v)    \n        # TODO consider using the same bin alignment as the training data\n        # it may be better to leave it as-is; it would be proportionate\n        # ved = trainInfo[2][i]\n        vbins, ved = np.histogram(v, bins=20)\n        volumeb    = np.zeros(v.shape[0])\n\n        # create classes for bins\n        for i in range(1, ved.shape[0] - 1): \n            volumeb[np.logical_and(ved[i] < npv, npv < ved[i+1])] = i\n                \n        # features to dataframe\n        #Xdict = {1: auid, 2: gain, 3: gainb, 4: volumeb}\n        Xdict = {1: gain, 2: gainb, 3: volumeb}\n        X     = pd.DataFrame(Xdict)\n    \n    return X, trainInfo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cb32d11b9c3c1012611612a5f0f86465a9a4d44"},"cell_type":"code","source":"# debug \n# subset = market_train_df.head()\n# features, trainInfo = analysis_get_features(market_train_df)\n\n# features.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b64109445918e8a591ce3d103aee1e669dd5fc63"},"cell_type":"markdown","source":"## A - Training Function"},{"metadata":{"trusted":true,"_uuid":"7a0ed2795e78c2f79ebd89e5f0267e651f8e1052"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn import svm\n\ndef analysis_train(features, target):\n    # scale y to be max [-1,1] to represent confidence \n    y       = np.zeros(target.shape[0])\n    #y_scale = minmax_scale(list(target), feature_range=(-1, 1), axis=0, copy=True)\n    y[target >  1e-3] = 1\n    y[target < -1e-3] = -1\n\n    # implement SVM regression\n    clf = svm.LinearSVC()\n    #clf = svm.SVR(C=0.9, kernel='rbf')  \n    #clf = svm.SVR(kernel='linear', C=1e3)   \n    #clf = svm.SVC(gamma='auto')\n    clf.fit(features, y)\n    \n    return clf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8b181c3e0e6a10a3287ab78932ce6b19a5f8a95"},"cell_type":"markdown","source":"## A - Prediction Function"},{"metadata":{"trusted":true,"_uuid":"0b9ab3f3d9978f60e49a3c3ec4256e49ac21cdca"},"cell_type":"code","source":"def analysis_predict(market_obs, predictions, trainInfo, trainedModel, toggle=True):    \n    features, trainInfo = analysis_get_features(market_obs, toggle, trainInfo)\n    p       = trainedModel.predict(features)\n    #p_scale = minmax_scale(list(p), feature_range=(-1, 1), axis=0, copy=True)\n    #p_class = np.ones(p_scale.shape[0]) * -1\n    #p_class[p_scale > 0] = 1\n    p_class = p\n    \n    # set\n    predictions.confidenceValue = p_class","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8056b881707072c379ad2e89b9c59c3c041a2ab7"},"cell_type":"markdown","source":"## Main Section\nLet's loop through all the days and make our random predictions.  The `days` generator (returned from `get_prediction_days`) will simply stop returning values once you've reached the end."},{"metadata":{"trusted":true,"_uuid":"a36bfd5e66aa624474f31902f87a94a2f3ccd733"},"cell_type":"code","source":"# break up the data for faster run times\ntest_mode = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bd50bb1532e95d1bfbf515d2724cbd3d57f695f","scrolled":true},"cell_type":"code","source":"if(test_mode):\n    split_index = 100000;\n    train_market_train_df = market_train_df[0:split_index]\n    train_news_train_df   = news_train_df[0:split_index]\n    #\n    test_market_train_df = market_train_df[split_index+1:split_index+2000]\n    test_news_train_df   = news_train_df[split_index+1:split_index+2000] \n    test_target_raw      = test_market_train_df['returnsOpenNextMktres10']\n    test_target_class    = np.ones(test_target_raw.shape[0]) * -1\n    test_target_class[test_target_raw >  0] = 1\n \n    # train analytical\n    features, trainInfo = analysis_get_features(train_market_train_df)\n    trainedModel_A      = analysis_train(features, train_market_train_df['returnsOpenNextMktres10'])\n    print(trainedModel_A)\n\n    # train news\n    trainedModel_B = train_news_model(train_market_train_df, train_news_train_df)\n    print(trainedModel_B)\n        \nelse:\n    # train analytical\n    features, trainInfo = analysis_get_features(market_train_df)\n    trainedModel_A      = analysis_train(features, market_train_df['returnsOpenNextMktres10'])\n    print(trainedModel_A)\n\n    # train news\n    trainedModel_B = train_news_model(market_train_df, news_train_df)\n    print(trainedModel_B)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d0b3ef20a15e46c72fc0515068e402dd4663ffb"},"cell_type":"code","source":"if(test_mode):\n    from sklearn.metrics import classification_report, confusion_matrix  \n    \n    # predict\n    predictions = {'assetCode': test_market_train_df['assetCode'], 'confidenceValue' : np.zeros(test_market_train_df.shape[0])}\n    predictions = pd.DataFrame(predictions)\n    \n    # get model A predictions\n    analysis_predict(test_market_train_df, predictions, trainInfo, trainedModel_A, False)        \n    # save\n    predA = predictions['confidenceValue'].values.copy()\n\n    # get model B predictions\n    news_predict(test_market_train_df, test_news_train_df, predictions, trainedModel_B)\n    # save\n    predB_raw = predictions['confidenceValue'].values.copy()\n    predB     = np.ones(predB_raw.shape[0]) * -1\n    predB[predB_raw >  predB_raw.mean()] = 1\n    \n    # results\n    print('A results')\n    print(confusion_matrix(test_target_class,predA))  \n    print(classification_report(test_target_class,predA))  \n    \n    print('B results')\n    print(confusion_matrix(test_target_class,predB))  \n    print(classification_report(test_target_class,predB))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"825bfa79edcadaca63c0ae84d845db78bd744e00"},"cell_type":"code","source":"days = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef60bc52a8a228e5a2ce18e4bd416f1f1f25aeae"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"generating predictions...\")\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    # get model A predictions\n    analysis_predict(market_obs_df, predictions_template_df, trainInfo, trainedModel_A)        \n    # save\n    predA = predictions_template_df['confidenceValue'].values.copy()\n\n    # get model B predictions\n    news_predict(market_obs_df, news_obs_df, predictions_template_df, trainedModel_B)\n    # save\n    predB = predictions_template_df['confidenceValue'].values.copy()\n\n    # average predictions \n    pred = (predA + predB) / 2\n    np.clip(pred, -1, 1)\n    predictions_template_df.confidenceValue = pred\n    \n    # make prediction\n    env.predict(predictions_template_df)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bea40ae32a2346a2cb58ccc364e84a86cb86932b"},"cell_type":"code","source":"predictions_template_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"},"cell_type":"markdown","source":"## **`write_submission_file`** function\n\nWrites your predictions to a CSV file (`submission.csv`) in the current working directory."},{"metadata":{"trusted":true,"_uuid":"2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d38aa8a67cad3f0c105db7e764ec9b805db39ceb"},"cell_type":"code","source":"# We've got a submission file!\nimport os\nprint([filename for filename in os.listdir('.') if '.csv' in filename])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffd584a58c2656675e26d82efcd34dca31d4f5a3"},"cell_type":"code","source":"# lets check out that CSV file\nimport pandas as pd\nfrom datetime import datetime\nimport csv\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nheaders = ['time', 'assetCode', 'confidenceValue']\ndf_in  = pd.read_csv('submission.csv',names=headers)\n\nprint(df_in)\n\ncode = df_in.assetCode[3]\ndf   = df_in[df_in.assetCode == code]           \n\ny = np.array(df['confidenceValue'], dtype=float)\nx = np.linspace(1.0, y.shape[0], num=y.shape[0])\n\n# plot\nplt.plot(x,y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e3a267ea3149403c49ff59515a1a669ca2d1f9f"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}