{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python"}}, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "8faf0d69cd0ed23e336d79d14003d6c22c4c2318", "_cell_guid": "da03f3bf-b128-411f-98f3-9c207d3c968e"}, "source": ["## PCA & GBRT\u00b6\n", "\n", "**Baseline Model:**"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "5014de0f45d0d1a2c83589c97a9b787962b03a02", "_cell_guid": "a092997c-61a3-4cec-835d-2a2ef0d436a7"}, "outputs": [], "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "from mpl_toolkits.basemap import Basemap\n", "import numpy as np\n", "import pandas as pd\n", "import os\n", "from __future__ import division "], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "a63d9e6b70e3056b0b76a37e2557001388cc1e4b", "_cell_guid": "6de2b7e6-b2e9-44d0-8a4e-b2879af48926"}, "outputs": [], "source": ["from sklearn.linear_model import LinearRegression\n", "from sklearn import preprocessing"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "23547c7689d682e4f2820397a4d56a79968411dc", "_cell_guid": "94a96bcd-172f-4d1f-996b-e19a37131702"}, "outputs": [], "source": ["#filename_train = '/Users/manishrai/Desktop/DSMLAI/Kaggle/Zestimate/res/train_2016_v2.csv'\n", "\n", "#filename_properties = '/Users/manishrai/Desktop/DSMLAI/Kaggle/Zestimate/res/properties_2016.csv'\n", "\n", "#filename_sample = '/Users/manishrai/Desktop/DSMLAI/Kaggle/Zestimate/res/sample_submission.csv'\n", "\n", "# importing the datasets\n", "train_data_df = pd.read_csv('../input/train_2016_v2.csv').dropna(how='all')\n", "properties_data_df = pd.read_csv('../input/properties_2016.csv').dropna(how='all')"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "9086b4679750de6ee9dd55936180ab36ba50aaca", "_cell_guid": "e923df74-6d95-4c46-8610-8f92661176c3"}, "outputs": [], "source": ["df_f1 = pd.merge(train_data_df, properties_data_df, how='left', on=['parcelid'])"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "1832b5a9d2be73d92704aa1ac09fda194f437edc", "_cell_guid": "49cf2e95-30e4-46f9-8598-37f0701b3b0b", "scrolled": true}, "outputs": [], "source": ["col_dtypes = df_f1.dtypes.reset_index()\n", "col = ['colnames', 'datatypes'] \n", "col_dtypes.columns =col\n", "\n", "col_dtypes_num = col_dtypes[(col_dtypes.datatypes == 'int64') | (col_dtypes.datatypes == 'float64')]\n", "df2 = df_f1[list(col_dtypes_num.colnames)]\n", "df2.head()"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "4f5d5ebcfd4410a26b741c0f3032d106f3e004e5", "_cell_guid": "bc12e029-2945-4422-97a7-ec31400acc78", "scrolled": true}, "outputs": [], "source": ["# Getting the percentage of missing values for each of the cloumns\n", "df_null = df2.isnull().sum() /df2.index.max()\n", "df_null = df_null.reset_index()\n", "df_null.columns = ['colnames', 'pct_null']\n", "df_null_sorted = df_null.sort_values('pct_null')\n", "less_null_cols = list(df_null_sorted[df_null_sorted.pct_null<=0.8].colnames)\n", "\n", "\n", "from sklearn import preprocessing\n", "\n", "df3 = df2[less_null_cols]\n", "\n", "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n", "imp.fit(df3)\n", "missing_imputed = imp.transform(df3)\n", "df_missing_imputed = pd.DataFrame(missing_imputed, columns=df3.columns)\n", "\n", "\n", "df_corr = df_missing_imputed.corrwith(df_missing_imputed.logerror).reset_index()\n", "df_corr.columns = ['colnames', 'correlation']\n", "df_corr_sorted = df_corr.sort_values('correlation')\n", "df_corr_sorted = df_corr_sorted.dropna(how='any')\n", "#df_corr_sorted = df_corr_sorted[(df_corr_sorted['colnames'] != 'logerror')]\n", "\n", "df4 = df_corr_sorted[(df_corr_sorted.correlation >= 0.01) | (df_corr_sorted.correlation <= -0.01)]\n", "\n", "## Re-do the missing value imputation\n", "#df3[list(df4.colnames)]\n", "df_f = df_missing_imputed.drop(['parcelid'], axis=1)\n", "df_f.head()"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "a8f29ad86ee95bc0ad730ffa9992b31f606e707a", "_cell_guid": "570d017c-07ec-4579-9c7d-c99180184132"}, "outputs": [], "source": ["import numpy as np\n", "from sklearn.metrics import mean_squared_error\n", "from sklearn.datasets import make_friedman1\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "from sklearn import cross_validation, metrics\n", "\n", "\n", "# Let's do the variable importance thest and randomtreeRegressor\n", "X = df_f.drop(['logerror'], axis=1)\n", "y = df_f.logerror\n", "\n", "offset = int(X.shape[0] * 0.8)\n", "X_train, y_train = X[:offset], y[:offset]\n", "X_test, y_test = X[offset:], y[offset:]\n", "\n", "\n", "# Fit regression model\n", "params = {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 4,\n", "          'learning_rate': 0.1, 'loss': 'ls'}\n", "clf = GradientBoostingRegressor(**params)\n", "\n", "clf.fit(X_train, y_train)\n", "#Predict training set:\n", "#dtrain_predictions = clf.predict(X_train)\n", "#dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n", "\n", "\n", "mse = mean_squared_error(y_test, clf.predict(X_test))\n", "\n", "#Print model report:\n", "print (\"\\nModel Report\")\n", "print (\"Accuracy on the train dataset: %.4g\" % clf.score(X_train, y_train))\n", "print (\"Accuracy on the test dataset: %.4g\" % clf.score(X_test, y_test))\n", "\n", "print(\"MSE: %.4f\" % mse)"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "cfa432bf5102dcc9c91f1cf726ef3eed82635965", "_cell_guid": "fb4798a2-ad65-4cba-8692-c595e4702121"}, "outputs": [], "source": ["# #############################################################################\n", "# Plot training deviance\n", "\n", "# compute test set deviance\n", "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n", "\n", "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n", "    test_score[i] = clf.loss_(y_test, y_pred)\n", "\n", "plt.figure(figsize=(15, 6))\n", "#plt.subplot(211)\n", "plt.title('Deviance')\n", "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n", "         label='Training Loss')\n", "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n", "         label='Test Loss')\n", "plt.legend(loc='upper right')\n", "plt.xlabel('Boosting Iterations')\n", "plt.ylabel('Deviance')\n", "plt.grid()"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "fdb7dfb3287d1c0c534ff3441e0c5a1ed0c1d03d", "_cell_guid": "a31fafd9-89c8-4688-aeae-b4ebe0128282"}, "outputs": [], "source": ["# #############################################################################\n", "# Plot feature importance\n", "feature_importance = clf.feature_importances_\n", "# make importances relative to max importance\n", "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n", "sorted_idx = np.argsort(feature_importance) #Returns the indices that would sort an array.\n", "pos = np.arange(sorted_idx.shape[0]) + .5\n", "\n", "plt.figure(figsize=(15, 6))\n", "#plt.subplot(212)\n", "plt.bar(pos, feature_importance[sorted_idx], align='center')\n", "plt.xticks(pos, X.columns, rotation='vertical')\n", "plt.xlabel('Relative Importance')\n", "plt.title('Variable Importance')\n", "plt.grid()\n", "\n", "plt.show()"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f46733c6e16510335d89c869f5d331197a9c382d", "_cell_guid": "fc55f13a-00e9-497c-bfa2-c847a3394473"}, "outputs": [], "source": ["from sklearn.decomposition import PCA, FactorAnalysis\n", "from sklearn.covariance import ShrunkCovariance, LedoitWolf\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import GridSearchCV\n", "\n", "from sklearn import preprocessing\n", "\n", "X1 = preprocessing.scale(X)"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "52568a0b8862f56bb321794d6113e6fd1cda7b21", "_cell_guid": "ccacd6e7-304b-42df-bd57-1ff6355379d2", "scrolled": true}, "outputs": [], "source": ["pca = PCA(n_components=31)\n", "pca.fit(X1)\n", "pca_val = pd.DataFrame(pca.explained_variance_)\n", "pca_val.columns = ['PCA']\n", "pca_val.PCA = pca_val.PCA.round(decimals = 2)\n", "pca_val"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "d2dbefbd1bbf1648b2d14931ac2f9340016ff111", "_cell_guid": "654c09e0-8737-4fd2-9fd1-77b744c715d3"}, "outputs": [], "source": ["pca = PCA(n_components=8)\n", "pca.fit_transform(X1)\n", "reduced_dim_df = pca.fit_transform(X1)"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "6847448fb9b910976773b77e059d52baf2f45bc3", "_cell_guid": "27bb7c6b-b3b1-4916-a63c-b2bf30af67c1", "scrolled": true}, "outputs": [], "source": ["components_df = pd.DataFrame(pca.components_).T\n", "components_df.columns = ['comp_1', 'comp_2', 'comp_3', 'comp_4', 'comp_5', 'comp_6', 'comp_7', 'comp_8']\n", "components_df"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f38e5393c173d1bbcf8789abec490c8c29b080bf", "_cell_guid": "f92c16dd-fbac-4137-b26b-f45f04574ce0", "scrolled": true}, "outputs": [], "source": ["X_red_dim = pd.DataFrame(reduced_dim_df)\n", "X_red_dim.columns = ['comp_1', 'comp_2', 'comp_3', 'comp_4', 'comp_5', 'comp_6', 'comp_7', 'comp_8']\n", "X_red_dim.head()"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "7340875cc7298911dd56648e7aa1ca21efe8f34c", "_cell_guid": "fe96d1f3-ffe7-416c-b72b-d9d7daabaf47"}, "source": ["### Doing GBR on the new dimensions"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "9672791e0607b7192ec257b7e1f9586f0ca4f6c7", "_cell_guid": "102cb86c-15f3-4107-9280-654df40da486"}, "outputs": [], "source": ["X = X_red_dim\n", "offset = int(X.shape[0] * 0.8)\n", "X_train, y_train = X[:offset], y[:offset]\n", "X_test, y_test = X[offset:], y[offset:]\n", "\n", "\n", "# Fit regression model\n", "params = {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 4,\n", "          'learning_rate': 0.1, 'loss': 'ls'}\n", "clf = GradientBoostingRegressor(**params)\n", "\n", "clf.fit(X_train, y_train)\n", "#Predict training set:\n", "#dtrain_predictions = clf.predict(X_train)\n", "#dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n", "\n", "\n", "mse = mean_squared_error(y_test, clf.predict(X_test))\n", "\n", "#Print model report:\n", "print (\"\\nModel Report\")\n", "print (\"Accuracy on the train dataset: %.4g\" % clf.score(X_train, y_train))\n", "print (\"Accuracy on the test dataset: %.4g\" % clf.score(X_test, y_test))\n", "\n", "print(\"MSE: %.4f\" % mse)"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "db57031dbf81a05b60141f1cd67e71e4b4f765d5", "_cell_guid": "278f5114-fe3a-4bb9-978d-299219a9e208"}, "outputs": [], "source": [], "execution_count": null}], "nbformat_minor": 1}