{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import the required libraries \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nimport pandas as pd\nimport json\nimport numpy as np # linear algebra\n\n# Dataset Preparation\nprint (\"Read Dataset ... \")\ndef read_dataset(path):\n\treturn json.load(open(path)) \ntrain = read_dataset('../input/train.json')\ntest = read_dataset('../input/test.json')\n\n# Text Data Features\nprint (\"Prepare text data of Train and Test ... \")\ndef generate_text(data):\n\ttext_data = [\", \".join(doc['ingredients']).lower() for doc in data]\n\treturn text_data \n\ntrain_text = generate_text(train)\ntest_text = generate_text(test)\ntarget = [doc['cuisine'] for doc in train]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18a30bd2060c68aaf44abee6593039bc257056b2"},"cell_type":"code","source":"from sklearn.preprocessing import normalize\nfrom scipy.sparse import coo_matrix, csr_matrix\n\n\ndef cosine(plays):\n    normalized = normalize(plays)\n    return normalized.dot(normalized.T)\n\n\ndef bhattacharya(plays):\n    plays.data = np.sqrt(plays.data)\n    return cosine(plays)\n\n\ndef ochiai(plays):\n    plays = csr_matrix(plays)\n    plays.data = np.ones(len(plays.data))\n    return cosine(plays)\n\n\ndef bm25_weight(data, K1=1.2, B=0.8):\n    \"\"\" Weighs each row of the matrix data by BM25 weighting \"\"\"\n    # calculate idf per term (user)\n    N = float(data.shape[0])\n    idf = np.log(N / (1 + np.bincount(data.col)))\n\n    # calculate length_norm per document (artist)\n    row_sums = np.squeeze(np.asarray(data.sum(1)))\n    average_length = row_sums.sum() / N\n    length_norm = (1.0 - B) + B * row_sums / average_length\n\n    # weight matrix rows by bm25\n    ret = coo_matrix(data)\n    ret.data = ret.data * (K1 + 1.0) / (K1 * length_norm[ret.row] + ret.data) * idf[ret.col]\n    return ret\n\n\ndef bm25(plays):\n    plays = bm25_weight(plays)\n    return plays.dot(plays.T)\n\ndef get_largest(row, N=10):\n    if N >= row.nnz:\n        best = zip(row.data, row.indices)\n    else:\n        ind = np.argpartition(row.data, -N)[-N:]\n        best = zip(row.data[ind], row.indices[ind])\n    return sorted(best, reverse=True)\n\n\ndef calculate_similar_artists(similarity, artists, artistid):\n    neighbours = similarity[artistid]\n    top = get_largest(neighbours)\n    return [(artists[other], score, i) for i, (score, other) in enumerate(top)]\n\n\n# Feature Engineering \nprint (\"TF-IDF on text data ... \")\ntfidf = TfidfVectorizer(binary=True,ngram_range=(1,2))\ndef tfidf_features(txt, flag):\n    if flag == \"train\":\n    \tx = tfidf.fit_transform(txt)\n    else:\n\t    x = tfidf.transform(txt)\n    x = x.astype('float16')\n    return x \n\nXT=tfidf_features(train_text+test_text, flag=\"train\")\nX = tfidf_features(train_text, flag=\"test\")\nX_test = tfidf_features(test_text, flag=\"test\")\n#print(X)\nX=bm25_weight(coo_matrix(X))\n\nX_test=bm25_weight(coo_matrix(X_test))\n\n##Xw=coo_matrix(X.astype(np.float32)).todense()\n##X_testw=coo_matrix(X_test.astype(np.float32)).todense()\nprint (\"Label Encode the Target Variable ... \")\nlb = LabelEncoder()\ny = lb.fit_transform(target)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f16b4b5cdd790b2c1869de78da8bf82e85a1f46","scrolled":true},"cell_type":"code","source":"from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n\n    \ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n\n\n    \n\nnames = [\n         #'ElasticNet',\n         #'SVC',\n         #'kSVC',\n         #'KNN',\n         #'DecisionTree',\n         'ExtraTree',\n         # 99%'RandomForestClassifier',\n         #'GridSearchCV',\n         # 25% 'HuberRegressor',\n         # 26%'Ridge',\n         # 35% 'Lasso',\n         # 26% 'LassoCV',\n         # 39%'Lars',\n         #'BayesianRidge',\n         # 11% 'SGDClassifier',\n         # 15 'RidgeClassifier',\n         # 9 'LogisticRegression',\n         #27 'OrthogonalMatchingPursuit',\n         #'RANSACRegressor',\n         ]\n\nclassifiers = [\n    #ElasticNetCV(cv=10, random_state=0),\n    #SVC(),\n    #SVC(kernel = 'rbf', random_state = 0),\n    #KNeighborsClassifier(n_neighbors = 10),\n    #DecisionTreeClassifier(),\n    ExtraTreeClassifier(),\n    # 99% RandomForestClassifier(n_estimators = 200),\n    #GridSearchCV(SVC(),param_grid, refit = True, verbose = 1),\n    # HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n    #Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n    #Lasso(alpha=0.05),\n    #LassoCV(),\n    #Lars(n_nonzero_coefs=10),\n    #BayesianRidge(),\n    #SGDClassifier(),\n    #RidgeClassifier(),\n    #LogisticRegression(),\n    #OrthogonalMatchingPursuit(),\n    #RANSACRegressor(),\n]\ncorrection= [0,0,0,0,0,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    regr=clf.fit(X,y)\n    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n    print(name,'%error',procenterror(regr.predict(X),y),'rmsle',rmsle(regr.predict(X),y))\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n    # Confusion Matrix\n    print(name,'Confusion Matrix')\n    print(confusion_matrix(y, np.round(regr.predict(X) ) ) )\n    print('--'*40)\n\n    # Classification Report\n    print('Classification Report')\n    print(classification_report(y,np.round( regr.predict(X) ) ))\n\n    # Accuracy\n    print('--'*40)\n    logreg_accuracy = round(accuracy_score(y, np.round( regr.predict(X) ) ) * 100,2)\n    print('Accuracy', logreg_accuracy,'%')\n    \n    # Predictions \n    print (\"Predict on test data ... \")\n    y_test = regr.predict(X_test)\n    y_pred = lb.inverse_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88bb575eca0f55eaabccb1cff3c07c946ea2ba56"},"cell_type":"code","source":"# Submission\nprint (\"Generate Submission File ... \")\ntest_id = [doc['id'] for doc in test]\nsub = pd.DataFrame({'id': test_id, 'cuisine': y_pred}, columns=['id', 'cuisine'])\nprint(sub)\nsub.to_csv('svm_output.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}