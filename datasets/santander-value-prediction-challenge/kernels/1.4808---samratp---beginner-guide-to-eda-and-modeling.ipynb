{"cells":[{"metadata":{"_uuid":"5cca967165c4fcb8a425d29399658dbdab5ac16c"},"cell_type":"markdown","source":"# Santander Value Prediction Challenge"},{"metadata":{"_uuid":"e13b4591df72db1fadd870b73a3f7deb7a007b25"},"cell_type":"markdown","source":"In this competition, Santander Group is asking us to help them identify the value of transactions for each potential customer.\nWe are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column. Our task is to predict the value of target column in the test set.\n\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error. The data set consists of train.csv and test.csv"},{"metadata":{"_uuid":"b2ca58024a3ecd22cc926b24d86dfd2482665838"},"cell_type":"markdown","source":"<img src=\"https://dynl.mktgcdn.com/p/OioPDkijUSBehXPo5nCC_CEd-0hZkZRv94-HHnJj-eA/2326x832.jpg\" width=600/>"},{"metadata":{"_uuid":"b5cfc3a7216693f7d8784aab3eed2da3012c7ded"},"cell_type":"markdown","source":"## Load Libraries"},{"metadata":{"trusted":true,"_uuid":"81a56a8b27a30d22396773cdc669768dbc28651a","collapsed":true},"cell_type":"code","source":"### Import required libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b627d302d38d26c96a51b58d38d4f492d2c301c5"},"cell_type":"markdown","source":"### Load Data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f565d1c80456d92e79bbf28896866fe0eebcf3a5"},"cell_type":"code","source":"# Read train and test files\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5eabb375359fe97d7ec5e6bd656d28efe2833ac0"},"cell_type":"markdown","source":"## Data Summary"},{"metadata":{"trusted":true,"_uuid":"5bbdd6df5dc8765f376f272a8329c216d94f6742","collapsed":true},"cell_type":"code","source":"# training set\nprint (\"Training set:\")\nn_data  = len(train_df)\nn_features = train_df.shape[1]\nprint (\"Number of Records: {}\".format(n_data))\nprint (\"Number of Features: {}\".format(n_features))\n\n# testing set\nprint (\"\\nTesting set:\")\nn_data  = len(test_df)\nn_features = test_df.shape[1]\nprint (\"Number of Records: {}\".format(n_data))\nprint (\"Number of Features: {}\".format(n_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00bb048e420736e7e53546d5ffc9ad9872ac5076"},"cell_type":"markdown","source":"We have more features than records in the Training set.\n\n- We have a total of `4993` features in the `Train` set.\n- The number of records in `Train` set is just `4459`\n\n\n- We have a total of `4992` features in the `Test` set.\n- The number of records in `Test` set is just `49342`"},{"metadata":{"_uuid":"cd1c932d8cd88814b87ccdd184959c911ba6754c"},"cell_type":"markdown","source":"### Train Data"},{"metadata":{"trusted":true,"_uuid":"7db0b8991eaeb2e428298861b2fee1724932faca","collapsed":true},"cell_type":"code","source":"train_df.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c372041e6b3c5586db52c5249834c000ab5b779f"},"cell_type":"markdown","source":"### Train Data Info"},{"metadata":{"trusted":true,"_uuid":"9886b9d67c1299f9658b2584ec94205d5844e9fd","collapsed":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c772e59c0fbfc53ea589c084045903635801ca6"},"cell_type":"markdown","source":"So there are a total of 4993 features out of which 1845 are of type float64, 3147 are int64 and 1 is object (ID is the object column).\n\nThe sad thing is that, as mentioned earier, we do not have meaningful names for the features. So, we have to rely on other ways to analyze and understand the data."},{"metadata":{"_uuid":"fa06e1c63aa62e764b540003622de25312587983"},"cell_type":"markdown","source":"### Test Data"},{"metadata":{"trusted":true,"_uuid":"a1f2399e6328deecdc76a1c1234981bf82c42d0b","collapsed":true},"cell_type":"code","source":"test_df.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3815bf793f702bc2b805467d4c57af677c74a5e"},"cell_type":"markdown","source":"### Test Data Info"},{"metadata":{"trusted":true,"_uuid":"0fb79c18fc8fad661c4e36bd84a052237eb1b69b","collapsed":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"897a154ee186f63f621fcfa37ecf5ba10e49bcf8"},"cell_type":"markdown","source":"So there are a total of 4992 features in the test set out of which 4991 are of type float64 and 1 is object (ID is the object column)"},{"metadata":{"_uuid":"f32028739ac6052cd4e5dcebec83b7452b9ec50b"},"cell_type":"markdown","source":"## Check for Missing Values"},{"metadata":{"trusted":true,"_uuid":"be1c45999bd842f5c6332b1af914aec644981016","collapsed":true},"cell_type":"code","source":"#### Check if there are any NULL values in Train Data\nprint(\"Total Train Features with NaN Values = \" + str(train_df.columns[train_df.isnull().sum() != 0].size))\nif (train_df.columns[train_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(train_df.columns[train_df.isnull().sum() != 0])))\n    train_df[train_df.columns[train_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51ccc48326706ecddd68fc7e9cd8664c3b83e2a0","collapsed":true},"cell_type":"code","source":"#### Check if there are any NULL values in Test Data\nprint(\"Total Test Features with NaN Values = \" + str(test_df.columns[test_df.isnull().sum() != 0].size))\nif (test_df.columns[test_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(test_df.columns[test_df.isnull().sum() != 0])))\n    test_df[test_df.columns[test_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d16fa98290b808d6060355ef05ac912471d7ff6"},"cell_type":"markdown","source":"We are lucky enough to get some data that has got no `NULLS`. So, we can go ahead and start anaylzing the data."},{"metadata":{"_uuid":"bdc913b808accd6e3753f4ec14d9843d36aad69f"},"cell_type":"markdown","source":"## Prepare the Data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"691a8ea0ba1c71e6d5f8f566e607302dc012fe29"},"cell_type":"code","source":"X_train = train_df.drop([\"ID\", \"target\"], axis=1)\ny_train = np.log1p(train_df[\"target\"].values)\n\nX_test = test_df.drop([\"ID\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6867af854a5cddac01ef183ad0e8b1ad2b8c0481"},"cell_type":"markdown","source":"## Check and Remove Constant Features"},{"metadata":{"_uuid":"175938f43a3bcf926f0433049568f58c25ab097f"},"cell_type":"markdown","source":"As we have lot of features, we will look for ways to trim down on the number of features. From the above sample records we can see that many of the records and features contains **`0`**. Also as there are many features, we will try to see if there are any features with constant values."},{"metadata":{"trusted":true,"_uuid":"b68a28829c64d3e58b24c746311aa6ae29c675ec","collapsed":true},"cell_type":"code","source":"# check and remove constant columns\ncolsToRemove = []\nfor col in X_train.columns:\n    if X_train[col].std() == 0: \n        colsToRemove.append(col)\n        \n# remove constant columns in the training set\nX_train.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\nX_test.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56a801d643a4cacd94d16188d97164afde54238b"},"cell_type":"markdown","source":"## Check and Remove Duplicate Columns"},{"metadata":{"trusted":true,"_uuid":"aee8c293e5855649f4afa6aec39407f0b261b789","collapsed":true},"cell_type":"code","source":"# Check and remove duplicate columns\ncolsToRemove = []\ncolsScaned = []\ndupList = {}\n\ncolumns = X_train.columns\n\nfor i in range(len(columns)-1):\n    v = X_train[columns[i]].values\n    dupCols = []\n    for j in range(i+1,len(columns)):\n        if np.array_equal(v, X_train[columns[j]].values):\n            colsToRemove.append(columns[j])\n            if columns[j] not in colsScaned:\n                dupCols.append(columns[j]) \n                colsScaned.append(columns[j])\n                dupList[columns[i]] = dupCols\n                \n# remove duplicate columns in the training set\nX_train.drop(colsToRemove, axis=1, inplace=True) \n\n# remove duplicate columns in the testing set\nX_test.drop(colsToRemove, axis=1, inplace=True)\n\nprint(\"Removed `{}` Duplicate Columns\\n\".format(len(dupList)))\nprint(dupList)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b4dd514329d31c03afe10a6c841b70f53715ec8","collapsed":true},"cell_type":"code","source":"print(\"Train set size: {}\".format(X_train.shape))\nprint(\"Test set size: {}\".format(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3a8b260145c3a6e7aaf55f5085b7eec8df1c346"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"_uuid":"6fe7eb18e1c6912221a09e22bd014a52895610e6"},"cell_type":"markdown","source":"As we have `4730 features` it would be very difficult to visualize and analyze all of them. So, we will try to analyze only some of the top features."},{"metadata":{"_uuid":"df352e0d0008f5e32df43c17ffee995f97381fb8"},"cell_type":"markdown","source":"### Feature Importance from GradientBoostingRegressor"},{"metadata":{"_uuid":"5283edfcc2fe364b0f76684ffc0c87ca03a52870"},"cell_type":"markdown","source":"Below, we will use the GradientBoostingRegressor to see the top features in our dataset."},{"metadata":{"trusted":true,"_uuid":"55221e835b047ff312e8d49820c8e383281bf900","collapsed":true},"cell_type":"code","source":"# Find feature importance\nclf_gb = GradientBoostingRegressor(random_state = 42)\nclf_gb.fit(X_train, y_train)\nprint(clf_gb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7432a7604ad40040aa2687a15273e55ff7499f61","collapsed":true},"cell_type":"code","source":"# GradientBoostingRegressor feature importance - top 100\nfeat_importances = pd.Series(clf_gb.feature_importances_, index=X_train.columns)\nfeat_importances = feat_importances.nlargest(100)\nplt.figure(figsize=(16,15))\nfeat_importances.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6706ebcf6f25e9c51d6ad152936e5421b2844a33"},"cell_type":"markdown","source":"The above plot looks very cluttered. Instead, we will take a look at the `top 25 features`."},{"metadata":{"trusted":true,"_uuid":"8bd9824eaf166f37c8d4b59cce9c3faf8c39883d","collapsed":true},"cell_type":"code","source":"# GradientBoostingRegressor feature importance - top 25\nfeat_importances_gb = pd.Series(clf_gb.feature_importances_, index=X_train.columns)\nfeat_importances_gb = feat_importances_gb.nlargest(25)\nplt.figure(figsize=(16,8))\nfeat_importances_gb.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06e4ee78454d6766b4b7d95a5caa9822bad4451f"},"cell_type":"markdown","source":"Below we will list the `top 10 features` with their feature importances."},{"metadata":{"trusted":true,"_uuid":"cbee012c4c6716b5ade546dca103d00f6587ee4c","collapsed":true},"cell_type":"code","source":"print(pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6955077c86fdf6aac85ab628ac6aa11cebff6a79"},"cell_type":"markdown","source":"### Feature Importance from RandomForestRegressor"},{"metadata":{"trusted":true,"_uuid":"7836b3fe3678a554063ef2c570b7a5647eb3f2fb","collapsed":true},"cell_type":"code","source":"# Find feature importance\nclf_rf = RandomForestRegressor(random_state = 42)\nclf_rf.fit(X_train, y_train)\nprint(clf_rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02d0f04face8643311cb384c0445cc4a32cdf860"},"cell_type":"markdown","source":"We will plot the top 25 features from RandomForestRegressor to see how similar they are with the top features given by GradientBoostingRegressor."},{"metadata":{"trusted":true,"_uuid":"3258cccc08c341cc70c35a147f909ec13f11ba5c","collapsed":true},"cell_type":"code","source":"# RandomForestRegressor feature importance - top 25\nfeat_importances_rf = pd.Series(clf_rf.feature_importances_, index=X_train.columns)\nfeat_importances_rf = feat_importances_rf.nlargest(25)\nplt.figure(figsize=(16,8))\nfeat_importances_rf.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f219534016d5b29d426125d65601b67905a76e9a","collapsed":true},"cell_type":"code","source":"print(pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e703cca1e86535f15d4e50a82b19f4351ca22fa4"},"cell_type":"markdown","source":"### GradientBoostingRegressor vs RandomForestRegressor Top 25 Features"},{"metadata":{"trusted":true,"_uuid":"6c9854b77391df0535783a69161663ec2af03c5e","collapsed":true},"cell_type":"code","source":"plt.figure()\nfig, ax = plt.subplots(1, 2, figsize=(16,6))\nfeat_importances_gb.plot(kind='barh', ax=ax[0])\nfeat_importances_rf.plot(kind='barh', ax=ax[1])\nax[0].invert_yaxis()\nax[1].invert_yaxis()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37fedd7ade37c9ad2b208607bbc4f3230ce014a5","collapsed":true},"cell_type":"code","source":"s1 = pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(10).index\ns2 = pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(10).index\n\ncommon_features = pd.Series(list(set(s1).intersection(set(s2)))).values\n\nprint(common_features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b08cd13991f72f3010181356dbe0dd92e3bc6ee9"},"cell_type":"markdown","source":"So, we can see that there are a total of just 6 common features in top 25 features of RandomForestRegressor and GradientBoostingRegressor. So, we should be careful in choosing what feature sto pick for further analysis."},{"metadata":{"_uuid":"2cd1f32577f6b3ee5cb09ee574ba768abb50cabb"},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"_uuid":"3651d02eb044861a0425474d443cd4cfc58b261f"},"cell_type":"markdown","source":"Below we will see some visualizations related to the top features."},{"metadata":{"trusted":true,"_uuid":"8d735292211b073a8b5e5a14b3ebe833b6a29bcf","collapsed":true},"cell_type":"code","source":"df_plot = X_train[['f190486d6', 'eeb9cd3aa', '58e2e02e6', '58232a6fb', '15ace8c9f', '9fd594eec']]\ndf_plot['target'] = y_train\n\ng = sns.pairplot(df_plot, diag_kind=\"kde\", palette=\"BuGn_r\")\ng.fig.suptitle('Pairplot of Top 6 Important Features',fontsize=26)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"320a8140b029a7b0da54bc3f069551a0c298e71a"},"cell_type":"markdown","source":"## Correlation HeatMap"},{"metadata":{"trusted":true,"_uuid":"81db45f36ad20c257612a3dbcea2b8aa21ef01fe","collapsed":true},"cell_type":"code","source":"# PLot Correlation HeatMap for top 20 features from GB and RF Models\ns1 = pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(20).index\ns2 = pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(20).index\n\ncommon_features = pd.Series(list(set(s1).union(set(s2)))).values\n\nprint(common_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7e72f20ac42ed45d47598cfa753d4a8c6c943e9","collapsed":true},"cell_type":"code","source":"df_plot = pd.DataFrame(X_train, columns = common_features)\ncorr = df_plot.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 16))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Correlation HeatMap\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53faed2162fe165ab9eaaf646810b932dd5ee8e6"},"cell_type":"markdown","source":"## PCA Visualization"},{"metadata":{"trusted":true,"_uuid":"a32bfbe89d2213d5c41e96fbd88dca6741658cac","collapsed":true},"cell_type":"code","source":"X_train_cpy = X_train.copy()\npca = PCA(n_components=3)\nX_train_cpy = pca.fit_transform(X_train_cpy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2af05a616ff945952425d24d7d2aa5d324baae1a","collapsed":true},"cell_type":"code","source":"print(pca.components_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04a387a9916fe4570d8ab02e04788501e147aeaf","collapsed":true},"cell_type":"code","source":"print(pca.explained_variance_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"962f800568ca9d8ece1ed3c834ca9c30f89582a0","collapsed":true},"cell_type":"code","source":"colors = np.random.random((4459, 3))\n\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\n\nax.scatter(X_train_cpy[:, 0], X_train_cpy[:, 1], X_train_cpy[:, 2], c=colors,\n           cmap=plt.cm.Set1, edgecolor=colors, alpha=0.5, s=40)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"235da17e2dd66b85306a9882ba0875633f1c1fc7"},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5d6151df3dafb6e4665e67f6f471796397daa096"},"cell_type":"code","source":"dev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6633294d44b289ccadf3c63debb2d19b2149fd43"},"cell_type":"markdown","source":"### LightGBM"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0d337c8d6303af40a1cdd216554fc1d9f1b1be5a"},"cell_type":"code","source":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=50, \n                      evals_result=evals_result)\n    \n    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"347c04eff15818b26262a0b0fc7dac3cf5ad9a38","collapsed":true},"cell_type":"code","source":"# Training LGB\npred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"LightGBM Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdb8c9481b6d2c262a4f941c46f85ad1586b0933","collapsed":true},"cell_type":"code","source":"# feature importance\nprint(\"Features Importance...\")\ngain = model.feature_importance('gain')\nfeatureimp = pd.DataFrame({'feature':model.feature_name(), \n                   'split':model.feature_importance('split'), \n                   'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\nprint(featureimp[:15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7144f31df5e466ab8371e799751bf195a2364e0","collapsed":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub[\"target\"] = pred_test\nprint(sub.head())\nsub.to_csv('sub_lgb.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}