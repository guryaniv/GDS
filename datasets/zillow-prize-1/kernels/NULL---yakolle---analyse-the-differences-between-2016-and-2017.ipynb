{"cells": [{"metadata": {"_cell_guid": "b8075710-a8c0-4c4c-8404-2e88dc2a0a72", "_uuid": "93f2343bba9cc069ac61bfdcdd56c255dbd665a5"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from sklearn.preprocessing import LabelEncoder\n", "from lightgbm import LGBMRegressor\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "b4de41d8-4773-4c87-9b55-b372720e8667", "_uuid": "09768fd3224e35bb9a5df7b2769902be8e9bd651"}, "source": ["# unique the train_data\n", "def unique_train_data(rec):\n", "    return rec.sort_values(by='transactiondate').iloc[-1]"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "c68959fc-43dc-4a01-9c8b-71511040d625", "_uuid": "9c8e73953bc2484e20f46cf49cb4975415d62715"}, "source": ["# load features\n", "data_dir='../input/'\n", "prop_2016=pd.read_csv(data_dir+'properties_2016.csv',index_col='parcelid').fillna(-1)\n", "prop_2017=pd.read_csv(data_dir+'properties_2017.csv',index_col='parcelid').fillna(-1)\n", "prop_2017=prop_2017.loc[prop_2016.index]\n", "print(prop_2016.shape,prop_2017.shape,np.sum(prop_2016.index!=prop_2017.index))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "db3f2a83-ae6a-4b9b-a28a-02bba9d65e28", "_uuid": "750f483799b7cf8ff5b9a36b67658282fedeec3a"}, "source": ["# compare the features between 2016 and 2017\n", "for col in prop_2017.columns:\n", "    s=prop_2017[col]\n", "    s1=prop_2016[col]\n", "    print(col,np.sum(s!=s1))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "5b5ef18d-903e-4e05-b931-e07644b6e06f", "_uuid": "74d0f2314ac3b528088c6b865536aed0fde77377"}, "source": ["# simple preprocess\n", "prop_2016 = prop_2016.drop(['regionidcounty', 'rawcensustractandblock','assessmentyear','propertyzoningdesc', \n", "                'propertycountylandusecode'],axis=1)\n", "obj_columns = prop_2016.dtypes.loc[np.object == prop_2016.dtypes].index\n", "for col in obj_columns:\n", "    prop_2016[col] = prop_2016[col].apply(lambda ele: str(ele))\n", "    prop_2016[col] = LabelEncoder().fit_transform(prop_2016[col])\n", "    \n", "prop_2017 = prop_2017.drop(['regionidcounty', 'rawcensustractandblock','assessmentyear','propertyzoningdesc', \n", "                'propertycountylandusecode'],axis=1)\n", "obj_columns = prop_2017.dtypes.loc[np.object == prop_2017.dtypes].index\n", "for col in obj_columns:\n", "    prop_2017[col] = prop_2017[col].apply(lambda ele: str(ele))\n", "    prop_2017[col] = LabelEncoder().fit_transform(prop_2017[col])\n", "    \n", "print('preprocess done')"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "6d1fe8d4-2e1a-41dc-aef0-e701c3760284", "_uuid": "03aaf9145326c7eeeedbd6f73f09e48afe3907bc"}, "source": ["# load train data\n", "train_2016 = pd.read_csv(data_dir+'train_2016_v2.csv',index_col='parcelid')\n", "train_2016 = train_2016.groupby('parcelid').apply(unique_train_data)\n", "train_2016['sale_month'] = pd.to_datetime(train_2016['transactiondate']).dt.month\n", "\n", "train_2017 = pd.read_csv(data_dir+'train_2017.csv',index_col='parcelid')\n", "train_2017 = train_2017.groupby('parcelid').apply(unique_train_data)\n", "train_2017['sale_month'] = pd.to_datetime(train_2017['transactiondate']).dt.month\n", "\n", "co_index = np.intersect1d(train_2016.index,train_2017.index)\n", "print(len(co_index))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["# use the common data between 2016 and 2017 to predict the 2016 logerror\n", "y = train_2016.loc[co_index,'logerror']\n", "x = prop_2016.loc[co_index]\n", "x['sale_month'] = train_2016.loc[co_index,'sale_month']\n", "\n", "# use LGBMRegressor to fit the data and print features' importances\n", "model = LGBMRegressor(objective='regression', n_estimators=200, learning_rate=.0125, num_leaves=24, max_depth=11,\n", "                      max_bin=80, min_child_samples=1, min_child_weight=0, min_split_gain=4e-05, subsample=.3,\n", "                      colsample_bytree=.45, subsample_freq=1, reg_alpha=4, reg_lambda=4, seed=0, nthread=2)\n", "model.fit(x,y)\n", "ims = []\n", "for i in range(len(model.feature_importances_)):\n", "    ims.append((x.columns[i], model.feature_importances_[i]))\n", "ims = sorted(ims, key=lambda pair : pair[1], reverse=True)\n", "print('which factors impact the 2016 logerror:')\n", "for col, im in ims:\n", "    print(col,im)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["# use the common data between 2016 and 2017 to predict the 2017 logerror\n", "y = train_2017.loc[co_index,'logerror']\n", "x = prop_2017.loc[co_index]\n", "x['sale_month'] = train_2017.loc[co_index,'sale_month']\n", "\n", "# use LGBMRegressor to fit the data and print features' importances\n", "model = LGBMRegressor(objective='regression', n_estimators=200, learning_rate=.0125, num_leaves=24, max_depth=11,\n", "                      max_bin=80, min_child_samples=1, min_child_weight=0, min_split_gain=4e-05, subsample=.3,\n", "                      colsample_bytree=.45, subsample_freq=1, reg_alpha=4, reg_lambda=4, seed=0, nthread=2)\n", "model.fit(x,y)\n", "ims = []\n", "for i in range(len(model.feature_importances_)):\n", "    ims.append((x.columns[i], model.feature_importances_[i]))\n", "ims = sorted(ims, key=lambda pair : pair[1], reverse=True)\n", "print('which factors impact the 2017 logerror:')\n", "for col, im in ims:\n", "    print(col,im)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_kg_hide-input": false, "_cell_guid": "d29db91a-3308-486d-a88a-3f3c697c488c", "_kg_hide-output": false, "_uuid": "fece84bf0632db0c6577305ea62ce21036682bad"}, "source": ["# use the gap of features to predict the gap of logerror\n", "y = train_2017.loc[co_index,'logerror'] - train_2016.loc[co_index,'logerror']\n", "x = prop_2017.loc[co_index] - prop_2016.loc[co_index]\n", "x['sale_month'] = train_2017.loc[co_index,'sale_month'] - train_2016.loc[co_index,'sale_month']\n", "\n", "# use LGBMRegressor to fit the data and print features' importances\n", "model = LGBMRegressor(objective='regression', n_estimators=200, learning_rate=.0125, num_leaves=24, max_depth=11,\n", "                      max_bin=80, min_child_samples=1, min_child_weight=0, min_split_gain=4e-05, subsample=.3,\n", "                      colsample_bytree=.45, subsample_freq=1, reg_alpha=4, reg_lambda=4, seed=0, nthread=2)\n", "model.fit(x,y)\n", "ims = []\n", "for i in range(len(model.feature_importances_)):\n", "    ims.append((x.columns[i], model.feature_importances_[i]))\n", "ims = sorted(ims, key=lambda pair : pair[1], reverse=True)\n", "print('which factors cause the change of logerror:')\n", "for col, im in ims:\n", "    print(col,im)"], "execution_count": null, "cell_type": "code", "outputs": []}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}