{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "16f7f094-551f-134a-c641-ed6a0ad4087b"
      },
      "source": [
        "# Credit card fraud detection\n",
        "\n",
        "This notebook will test fraud detection method using deep autoencoder. \n",
        "\n",
        "\n",
        "### Using Auto Encoders for Anomaly Detection\n",
        "\n",
        "The idea to apply it to anomaly detection is very straightforward:\n",
        "\n",
        "- train an auto-encoder on X_train with good regularization\n",
        "\n",
        "- evaluate it on the validation set X_val and visualise the reconstructed error plot \n",
        "\n",
        "- choose a threshold  which determines whether a value is an outlier (anomalies) or not\n",
        "\n",
        "\n",
        "http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ef7e3281-d0b9-7045-f95d-6e22b9e489ef",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d92f0b61-32b0-a227-f3e5-fe82b1a6f098"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    data = pd.read_csv(\"../input/creditcard.csv\")\n",
        "except Exception as e:\n",
        "    data = pd.read_csv(\"creditcard.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ac435526-c233-f049-733c-9e09388641b7"
      },
      "outputs": [],
      "source": [
        "count_classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
        "count_classes.plot(kind = 'bar')\n",
        "plt.title(\"Fraud class histogram\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "740b2ff9-2347-3c59-7dc8-60e51c740eff"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].reshape(-1, 1))\n",
        "\n",
        "# hour = data['Time'].apply(lambda x: np.ceil(float(x)/3600) % 24)\n",
        "# data['hour'] = StandardScaler().fit_transform(hour.reshape(-1, 1))\n",
        "\n",
        "data = data.drop(['Time','Amount'],axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4ae0b2f8-2915-bb13-4aad-d8dfed6b5041"
      },
      "source": [
        "# Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca3c3d1c-5066-d4cd-a97a-d0cbd3e7306b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class Autoencoder(object):\n",
        "\n",
        "    def __init__(self, n_hidden_1, n_hidden_2, n_input, learning_rate):\n",
        "        self.n_hidden_1 = n_hidden_1\n",
        "        self.n_hidden_2 = n_hidden_2\n",
        "        self.n_input = n_input\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights, self.biases = self._initialize_weights()\n",
        "\n",
        "        self.x = tf.placeholder(\"float\", [None, self.n_input])\n",
        "\n",
        "        self.encoder_op = self.encoder(self.x)\n",
        "        self.decoder_op = self.decoder(self.encoder_op)\n",
        "\n",
        "        self.cost = tf.reduce_mean(tf.pow(self.x - self.decoder_op, 2))\n",
        "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.cost)\n",
        "\n",
        "        init = tf.initialize_all_variables()\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        weights = {\n",
        "            'encoder_h1': tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1])),\n",
        "            'encoder_h2': tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2])),\n",
        "            'decoder_h1': tf.Variable(tf.random_normal([self.n_hidden_2, self.n_hidden_1])),\n",
        "            'decoder_h2': tf.Variable(tf.random_normal([self.n_hidden_1, self.n_input])),\n",
        "        }\n",
        "        biases = {\n",
        "            'encoder_b1': tf.Variable(tf.random_normal([self.n_hidden_1])),\n",
        "            'encoder_b2': tf.Variable(tf.random_normal([self.n_hidden_2])),\n",
        "            'decoder_b1': tf.Variable(tf.random_normal([self.n_hidden_1])),\n",
        "            'decoder_b2': tf.Variable(tf.random_normal([self.n_input])),\n",
        "        }\n",
        "\n",
        "        return weights, biases\n",
        "\n",
        "    def encoder(self, X):\n",
        "        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(X, self.weights['encoder_h1']),\n",
        "                                       self.biases['encoder_b1']))\n",
        "        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, self.weights['encoder_h2']),\n",
        "                                       self.biases['encoder_b2']))\n",
        "        return layer_2\n",
        "\n",
        "    def decoder(self, X):\n",
        "        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(X, self.weights['decoder_h1']),\n",
        "                                       self.biases['decoder_b1']))\n",
        "        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, self.weights['decoder_h2']),\n",
        "                                       self.biases['decoder_b2']))\n",
        "        return layer_2\n",
        "\n",
        "    def calc_total_cost(self, X):\n",
        "        return self.sess.run(self.cost, feed_dict={self.x: X})\n",
        "\n",
        "    def partial_fit(self, X):\n",
        "        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X})\n",
        "        return cost\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.sess.run(self.encoder_op, feed_dict={self.x: X})\n",
        "\n",
        "    def reconstruct(self, X):\n",
        "        return self.sess.run(self.decoder_op, feed_dict={self.x: X})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a0c31c52-e768-825d-a16a-122c4ff5017e"
      },
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "87e522ee-7883-d6ba-6551-0016c5b77efd"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_validation import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "41ca6aa7-7944-edee-c897-93baa54cce7c"
      },
      "outputs": [],
      "source": [
        "good_data = data[data['Class'] == 0]\n",
        "bad_data = data[data['Class'] == 1]\n",
        "#print 'bad: {}, good: {}'.format(len(bad_data), len(good_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec2deb1d-1175-c974-5219-27ac4b86e796",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train[X_train['Class']==0]\n",
        "X_train = X_train.drop(['Class'], axis=1)\n",
        "\n",
        "y_test = X_test['Class']\n",
        "X_test = X_test.drop(['Class'], axis=1)\n",
        "\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b73aef40-90f5-b70c-1355-d0f60a450ea7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "X_good = good_data.ix[:, good_data.columns != 'Class']\n",
        "y_good = good_data.ix[:, good_data.columns == 'Class']\n",
        "\n",
        "X_bad = bad_data.ix[:, bad_data.columns != 'Class']\n",
        "y_bad = bad_data.ix[:, bad_data.columns == 'Class']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8dbbc81c-32cf-f1c3-853c-20b41d886106"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a0d979b4-9c22-400d-f6ec-e4c460cecf02",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model = Autoencoder(n_hidden_1=15, n_hidden_2=3, n_input=X_train.shape[1], learning_rate = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "44c50800-46e7-95ce-8b4c-ef0913657ce2"
      },
      "outputs": [],
      "source": [
        "training_epochs = 100\n",
        "batch_size = 256\n",
        "display_step = 100\n",
        "record_step = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fce7572d-845a-b585-6f37-aa8c69167da2"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd28f9d3-41b4-7a47-d273-7648a521ea82"
      },
      "outputs": [],
      "source": [
        "total_batch = int(X_train.shape[0]/batch_size)\n",
        "\n",
        "cost_summary = []\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    cost = None\n",
        "    for i in range(total_batch):\n",
        "        batch_start = i * batch_size\n",
        "        batch_end = (i + 1) * batch_size\n",
        "        batch = X_train[batch_start:batch_end, :]\n",
        "        \n",
        "        cost = model.partial_fit(batch)\n",
        "    \n",
        "    if epoch % display_step == 0 or epoch % record_step == 0:\n",
        "        total_cost = model.calc_total_cost(X_train)\n",
        "        \n",
        "        if epoch % record_step == 0:\n",
        "            cost_summary.append({'epoch': epoch+1, 'cost': total_cost})\n",
        "        \n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:{}, cost={:.9f}\".format(epoch+1, total_cost))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9b5fe8fc-f492-152b-bc17-9e372217cfdb"
      },
      "outputs": [],
      "source": [
        "f, ax1 = plt.subplots(1, 1, figsize=(10,4))\n",
        "\n",
        "ax1.plot(list(map(lambda x: x['epoch'], cost_summary)), list(map(lambda x: x['cost'], cost_summary)))\n",
        "ax1.set_title('Cost')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "83a5e359-cbc2-7dae-397a-2a7042a0881f"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b8fea9ce-74a4-58ad-748d-0b264d63ff75",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "encode_decode = None\n",
        "total_batch = int(X_test.shape[0]/batch_size) + 1\n",
        "for i in range(total_batch):\n",
        "    batch_start = i * batch_size\n",
        "    batch_end = (i + 1) * batch_size\n",
        "    batch = X_test[batch_start:batch_end, :]\n",
        "    batch_res = model.reconstruct(batch)\n",
        "    if encode_decode is None:\n",
        "        encode_decode = batch_res\n",
        "    else:\n",
        "        encode_decode = np.vstack((encode_decode, batch_res))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "56878a86-ca11-b2bb-0ece-c37f56cf2eba"
      },
      "outputs": [],
      "source": [
        "def get_df(orig, ed, _y):\n",
        "    rmse = np.mean(np.power(orig - ed, 2), axis=1)\n",
        "    return pd.DataFrame({'rmse': rmse, 'target': _y})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7aace46c-c0b7-f73c-9a7c-967466209291",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df = get_df(X_test, encode_decode, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "491cbd0c-c9d6-aebf-5059-f0926637b6b9"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "faf4bd34-08a7-2ffa-2e1e-3322793aadc2"
      },
      "source": [
        "## Reconstruction error without fraud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f914098-f5fd-3122-308a-b447794c2a8c"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(111)\n",
        "_ = ax.hist(df[df['target']== 0].rmse.values, bins=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a838474e-c0a2-20a4-da45-8fdc661ad4fe"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(111)\n",
        "_ = ax.hist(df[(df['target']== 0) & (df['rmse'] < 10)].rmse.values, bins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6707a6b0-ac22-fbac-34ae-bb93f914f5e9"
      },
      "source": [
        "## Reconstruction error with fraud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2b139e7f-bf64-4fb5-fcc6-c7526fa17253"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(111)\n",
        "_ = ax.hist(df[df['target'] > 0].rmse.values, bins=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b06673c2-d1d5-1bca-4779-08ee180a700a"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(111)\n",
        "_ = ax.hist(df[(df['target'] > 0) & (df['rmse'] < 10)].rmse.values, bins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2e87ee5c-1342-e073-21a2-48172bd7f80f"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d05badf6-cb27-4ab3-ba67-49632f03460f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_auc_score, \n",
        "                             roc_curve, recall_score, classification_report, f1_score,\n",
        "                             precision_recall_fscore_support) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "089a24f7-34cd-6547-752d-1eabe17ecda8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    else:\n",
        "        1\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2a07d4d9-bf66-f6e5-8d75-2aa7a8dd4120"
      },
      "source": [
        "### ROC AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7fab3f0d-9c60-9241-8e79-3449b6cf9515"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(df.target, df.rmse)\n",
        "roc_auc = auc(fpr,tpr)\n",
        "\n",
        "# Plot ROC\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b',label='AUC = %0.4f'% roc_auc)\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.xlim([-0.001, 1])\n",
        "plt.ylim([0, 1.001])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a09ff6e2-e4dc-9016-1bda-e9b93c641750"
      },
      "source": [
        "ROC AUC seems not bad. But we know that it is highly imbalanced dataset. So let's see precision-recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "280c1140-c2af-43d2-09ed-afe5373f34e1"
      },
      "source": [
        "### Precision-Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8f3d826b-28cb-12eb-c724-0df509d911c3"
      },
      "outputs": [],
      "source": [
        "precision, recall, th = precision_recall_curve(df.target, df.rmse)\n",
        "plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aedf8b7e-68d7-39cb-509c-4f1715cbdb0c"
      },
      "source": [
        "Now its clear that our classifier is not so good.\n",
        "ROC curve is not a good visual illustration for highly imbalanced data.\n",
        "Because the False Positive Rate ( False Positives / Total Real Negatives ) does not drop drastically when the Total Real Negatives is huge. \n",
        "Whereas Precision ( True Positives / (True Positives + False Positives) ) is highly sensitive to False Positives and is not impacted by a large total real negative denominator.\n",
        "\n",
        "This autoencoder classifier can obtain good recall but with very low precision. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4fd2ff11-73e0-ae1e-2fa3-1f043a691c53"
      },
      "outputs": [],
      "source": [
        "plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Precision')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8ffbff5d-aa28-7f81-7ffe-9d99b7448da6"
      },
      "source": [
        "### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4dfbbeb9-c16f-878e-e6c1-ad0777b4535d"
      },
      "outputs": [],
      "source": [
        "# Compute confusion marix\n",
        "y_pred = [1 if p > 2 else 0 for p in df.rmse.values]\n",
        "cnf_matrix = confusion_matrix(df.target, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(\"Recall metric in the testing dataset: \", float(cnf_matrix[1,1])/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2a209aa4-7661-32d0-ec78-ec03485c5606"
      },
      "outputs": [],
      "source": [
        "f1_score(y_pred=y_pred, y_true=df.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a2c310b1-61bf-ee38-aaf8-b06cee642c41"
      },
      "outputs": [],
      "source": [
        "precision_recall_fscore_support(y_pred=y_pred, y_true=df.target)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}