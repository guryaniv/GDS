{"cells":[{"metadata":{"_uuid":"d888a4223ad3dfb2d40c086bc6423d24b03d9419"},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.ensemble import RandomForestRegressor\nimport sklearn\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cbabc6226f1dde8058a60c2fc15b41004263a71"},"cell_type":"markdown","source":"## Import Data"},{"metadata":{"trusted":true,"_uuid":"7de1da7dd96bb215daba31d52cc070116238989d"},"cell_type":"code","source":"# Store data\n##dic = pd.read_excel('../input/Data_Dictionary.xlsx')\n##mer = pd.read_csv('../input/merchants.csv')\n##tran = pd.read_csv('../input/new_merchant_transactions.csv')\n##his = pd.read_csv('../input/historical_transactions.csv')\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45b7b2aa4b65b5025a7bc75d5539463cd2fcb920"},"cell_type":"markdown","source":"## Analyse"},{"metadata":{"trusted":true,"_uuid":"8b9fe4dbd78fcb3c18f34d201a2de32fec857d12","scrolled":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47bf91203a812727c22ef30682ab7ebe907cf314"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8e32fc37ceef6ef949660709519ff084a1557b0","scrolled":false},"cell_type":"code","source":"# Feature_1 and 2 as they're categorical so use one-hot encoding to convert\nframes = pd.get_dummies(train['feature_1'],prefix='feature_1',drop_first=True)\ntrain = pd.concat([ train, frames ],axis=1)\nframes = pd.get_dummies(train['feature_2'],prefix='feature_2',drop_first=True)\ntrain = pd.concat([ train, frames ],axis=1)\ntrain = train.drop(['feature_1','feature_2'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fba61427454ed958167113714ea41b104906762"},"cell_type":"code","source":"# Plot distribution of y in train\nplt.hist(train['target'],bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bfb0c7c237c04f9e858ed0c6b5ced3cf6ac1459"},"cell_type":"code","source":"# Build pairplot and correlation matrix to look at relationship between variables\nsns.pairplot(train)\ntrain.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0a7ad781a213417d5c0fe4f9a113f3c09d77d4e"},"cell_type":"markdown","source":"## Baseline"},{"metadata":{"trusted":true,"_uuid":"34fd89141c0f04a2e25cbe056d087daf335c6a8d"},"cell_type":"code","source":"# Store X and y train\ndf = train\ny = train['target']\nX = train[['feature_3','feature_1_2','feature_1_3','feature_1_4','feature_1_5','feature_2_2','feature_2_3']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ded81f4de81cb6fbe455b1a869a47e8589b11729"},"cell_type":"code","source":"# Fit RF regressor model to train\nrf = RandomForestRegressor(max_features='sqrt')\nrf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23ed3723c0f33e44b82dea124958d79f144aa7ac"},"cell_type":"code","source":"# Showing the most important x variables/features in the model\nfeature_importances = pd.DataFrame(\n    rf.feature_importances_,\n    index = X.columns,\n    columns = [ 'y' ] \n).sort_values( 'y', ascending = False )\n\nprint( feature_importances )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a098247f49bf3d498747c4622ed34ada6729261"},"cell_type":"code","source":"# Determine the accuracy of the random forest applied to train\ny_predicted = rf.predict(X)\n\n# Calculate RMSE using the sqrt of MSE\nmse = sklearn.metrics.mean_squared_error(y,y_predicted)\nrmse = np.sqrt(mse)\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7492af198d0b5d5f6b5cd3d26a1f32c531b2706d"},"cell_type":"code","source":"# Prepare test\n# Feature_1 and 2 as they're categorical so use one-hot encoding to convert\nframes = pd.get_dummies(test['feature_1'],prefix='feature_1',drop_first=True)\ntest = pd.concat([ test, frames ],axis=1)\nframes = pd.get_dummies(test['feature_2'],prefix='feature_2',drop_first=True)\ntest = pd.concat([ test, frames ],axis=1)\ntest = test.drop(['feature_1','feature_2'],axis=1)\n\n# Store relevant test X variables\nX_test = test[['feature_3','feature_1_2','feature_1_3','feature_1_4','feature_1_5','feature_2_2','feature_2_3']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d46315b0705358812d0f638711cb28a64ee829c6"},"cell_type":"code","source":"# Apply model to test data\ntest_predict = rf.predict(X_test)\ntest_predict = pd.DataFrame(test_predict, columns=['target'])\ntest_predict = pd.concat([test_predict , test],axis=1)\n\n# Stores results of model applied to test \ntest_predict = test_predict[['card_id','target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e353061b20e2de37db1c40f55f21acb223488b4"},"cell_type":"code","source":"# Export test results for scoring\ntest_predict.to_csv('results',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1d5833b821807ebb93894aa1682e593ed29c7f98"},"cell_type":"code","source":"## LightGBM\nimport lightgbm\nfrom lightgbm import LGBMRegressor\n\nlgbm = LGBMRegressor()\n\nlgbm.fit(X, y)\n\ny_pred = lgbm.predict(X)\n\nmse = sklearn.metrics.mean_squared_error(y,y_pred)\nrmse = np.sqrt(mse)\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84cad0e3ebb253ec070464775c0fb59712cb14c7"},"cell_type":"markdown","source":"### Baseline RMSE is 3.930"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}