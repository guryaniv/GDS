{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "version": "3.6.1", "mimetype": "text/x-python"}}, "cells": [{"metadata": {"_cell_guid": "aa222ad6-ae36-493c-80bd-d4d74fa0659d", "collapsed": true, "_uuid": "a01cdd5122ed82ea3e09c3405fe3fc57a61bd075"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["import pandas as pd\n", "import numpy as np\n", "import scipy as scp\n", "\n", "from multiprocessing import Pool\n", "import time\n", "\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "\n", "from sklearn.decomposition import PCA\n", "\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "from sklearn.linear_model import Lasso\n", "\n", "import warnings; warnings.simplefilter('ignore')"]}, {"metadata": {"_cell_guid": "5c9e3968-8283-4861-8575-e2ea0872464f", "collapsed": true, "_uuid": "422080575fae469db870b69df0c4bed2136d6e91"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["# path = '../input/web-traffic-time-series-forecasting/'\n", "# path=''\n", "path = '../input/'\n", "kfile = '{}key_1.csv'.format(path)\n", "sfile = '{}sample_submission_1.csv'.format(path)\n", "tfile = '{}train_1.csv'.format(path)"]}, {"metadata": {"_cell_guid": "4f7fd61b-a11e-4c33-8bb4-b1281f2bd575", "collapsed": true, "_uuid": "79ef8a1d2b9a5abe5bcdc6bcfe1ce3c0d37ffefe"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["feature_keys = {'Access': {'all-access': 0, 'desktop': 1, 'mobile-web': 2},\n", "                'Agent': {'all-agents': 0, 'spider': 1},\n", "                'Domain': {'mediawiki.org': 0, 'wikimedia.org': 1, 'wikipedia.org': 2},\n", "                'Language': {'commons': 0, 'de': 1, 'en': 2,\n", "                             'es': 3, 'fr': 4,'ja': 5, 'ru': 6,\n", "                             'www': 7,'zh': 8}\n", "               }"]}, {"metadata": {"_cell_guid": "2e4b9405-e7cd-4ac4-9a9d-f98e99eede4a", "collapsed": true, "_uuid": "69ed2e1707cd88d76815cad4343bd02ab5972e02"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["dateCols = ['quarter',\n", "            'is_month_start','is_month_end',\n", "            'is_quarter_start','is_quarter_end',\n", "            'is_year_start','is_year_end',\n", "            'dayofweek','month']\n", "pageCols = ['Name','Language', 'Domain', 'Access', 'Agent']\n", "\n", "extractDate = lambda col: (np.uint8(col.quarter),\n", "                                      np.uint8(col.is_month_start),\n", "                                      np.uint8(col.is_month_end),\n", "                                      np.uint8(col.is_quarter_start),\n", "                                      np.uint8(col.is_quarter_end),\n", "                                      np.uint8(col.is_year_start),\n", "                                      np.uint8(col.is_year_end),\n", "                                      np.uint8(col.dayofweek),\n", "                                      np.uint8(col.month)\n", "                                      )"]}, {"metadata": {"_cell_guid": "dba1ba2f-9203-4d15-8aa9-e629d0866d2a", "collapsed": true, "_uuid": "5cd4636f1bc287ba3b5eeb00a9ac903479323b14"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["def fun(x):\n", "    return page_dict[x]\n", "def fun_d(x):\n", "    return date_dict[x]\n", "    \n", "def parallel_map(ser,n=20,isDate=False):\n", "    \"\"\"\n", "    looks up the input pandas series values to in dictionary d\n", "    return res a series with the mapped values and the same index as ser\n", "    \"\"\"\n", "#     res = ser.copy()\n", "    t0 = time.time()\n", "    \n", "    try:\n", "        p = Pool(n)\n", "        if not isDate:\n", "            res = p.map(fun,ser)\n", "        else:\n", "            res = p.map(fun_d,ser)\n", "    except Exception as e:\n", "        print('failed',e)\n", "        p.close()\n", "        \n", "    p.close()\n", "    p=None\n", "    print('Time:',time.time()-t0)\n", "    return res"]}, {"metadata": {"_cell_guid": "5a88520a-2e70-4706-9812-0cf54a38971c", "collapsed": true, "_uuid": "725d75c709407fbcb45372b29a8ae3aa5eb51fe0"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["# parallel_map(pd.Series(pd.date_range('2015-01-01',periods=1000,freq='D').strftime('%Y-%m-%d').tolist()),n=20,isDate=True)"]}, {"metadata": {"_cell_guid": "3b4fca7d-8ea2-487e-aca9-cdcfb4bc94b7", "_uuid": "908d11a3055899837f89f2cc8d08b56e548d3932"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["def create_dictionary(isDate=False):\n", "    \"\"\"\n", "    if isDate=True\n", "    return a dictionary with the unique dates as keys and their features as tuple value\n", "    if isDate=False\n", "    return dictionary with the unique pages as keys and their features as tuple value\n", "    \"\"\"\n", "    fnames = ['Name','Language','Domain','Access','Agent']\n", "    domain = '([A-Za-z0-9\\-]+\\.org)'\n", "    language = '([A-Za-z0-9\\-]+)'\n", "    access = '([A-Za-z0-9\\-]+)'\n", "    agent = '([A-Za-z0-9\\-]+)'\n", "    name = '(.+)'\n", "    pattern = '^{:}_{:}\\.{:}_{:}_{:}$'.format(name, language,\n", "                                              domain, access,\n", "                                              agent)\n", "    if not isDate:\n", "        keys = pd.read_csv(kfile,\n", "                           usecols=['Page'],\n", "                           converters={0:lambda p:p[:-11]},\n", "                           index_col='Page')\n", "        keys['Page'] = keys.index.tolist()\n", "        keys.drop_duplicates(inplace=True)\n", "        keys[fnames] = keys['Page'].str.extract(pattern)\n", "        keys[fnames[1:]] = keys[fnames[1:]].apply(\n", "            lambda col: col.map(feature_keys[col.name]).astype(np.uint8))\n", "        keys.drop('Page',axis=1,inplace=True)\n", "        keys = dict(zip(keys.index,map(tuple,keys.values)))\n", "        return keys\n", "    else:\n", "        keys = pd.read_csv(kfile,\n", "                           usecols=['Page'],\n", "                           converters={0:lambda p:p[-10:]},\n", "                           index_col='Page')\n", "        keys['Date'] = keys.index.tolist()\n", "        keys.drop_duplicates(inplace=True)\n", "        keys['Date'] = pd.to_datetime(keys['Date']).map(extractDate)\n", "        keys = dict(zip(keys.index,map(tuple,keys['Date'].values)))\n", "        return keys\n", "page_dict = create_dictionary()\n", "date_dict = create_dictionary(True)\n", "print(list(page_dict.items())[:5])\n", "print(list(date_dict.items())[:5])"]}, {"metadata": {"_cell_guid": "eea6f6c9-db69-42e1-8e50-35b4ba351eac", "_uuid": "10ac3fe02486c7fe77de8e04c42c1bd1ebdbb839"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["def load_validation_set(keyfile,samplefile):\n", "    \"\"\"\n", "    returns the validation set\n", "    typical use: load_validation_set(kfile,sfile)\n", "    ### Optimise for large files\n", "    #### read_csv Parameters\n", "    ###### na_filter : boolean, default True\n", "        Detect missing value markers (empty strings and the value of na_values). In\n", "        data without any NAs, passing na_filter=False can improve the performance\n", "        of reading a large file)\n", "    ###### memory_map : boolean, default False\n", "        If a filepath is provided for `filepath_or_buffer`, map the file object\n", "        directly onto memory and access the data directly from there. Using this\n", "        option can improve performance because there is no longer any I/O overhead.\n", "     ###### engine : {'c', 'python'}, optional\n", "        Parser engine to use. The C engine is faster while the python engine is\n", "        currently more feature-complete.\n", "    \"\"\"\n", "    keys = pd.read_csv(keyfile,\n", "                   index_col = 'Id',\n", "                   converters = {\n", "                                'Page':lambda p: \n", "                                 {'Page': p[:-11],'Date':p[-10:],}},     \n", "                   engine = 'c',\n", "                   na_filter = False,\n", "                   memory_map = True)\n", "    keys['Date'] = parallel_map(keys['Page'].apply(lambda d: d['Date']),isDate=True,n=50)\n", "    keys['Page'] = keys['Page'].apply(lambda d: d['Page'])\n", "    \n", "    sample = pd.read_csv(samplefile,\n", "                         index_col='Id',\n", "                         usecols=['Id'],\n", "                         engine = 'c',\n", "                         na_filter = False,\n", "                         memory_map = True)\n", "    df = pd.concat([keys,sample],join_axes=[sample.index],axis=1).to_sparse()\n", "    df['Page'] = parallel_map(df['Page'],n=50)\n", "    return df\n", "vData = load_validation_set(kfile,sfile)\n", "vData.head()   \n"]}, {"metadata": {"_cell_guid": "e0c4515a-b78d-41b1-a56c-6dbe84a46eff", "_uuid": "79ad620a435216023b8e9a62b1f74e9cec0ad9fb"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["vData.info()"]}, {"metadata": {"_cell_guid": "ad8d0d62-b5cc-41b6-b09e-7fd754858489", "collapsed": true, "_uuid": "2a1da6aa4f36f80fdd87d8a4628374c988e0aa23"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["# vData.to_pickle('vData.csv')"]}, {"metadata": {"_cell_guid": "9fb85965-02a0-4581-aee8-5211ebcd0c92", "_uuid": "29f04cd5de9b245f520da3406b67d32bbc75aee3"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["def load_train_set(train_file):\n", "    df = pd.read_csv(train_file, \n", "                    index_col=0,\n", "                   engine = 'c',\n", "                   memory_map = True).rename(\n", "        columns=pd.to_datetime).groupby(\n", "        extractDate,axis=1).mean().unstack().dropna().astype(int).to_frame().reset_index()\n", "    df['Page'] = parallel_map(df['Page'],n=50)\n", "    return df.rename(columns={0:'Visits','level_0':'Date'}).to_sparse()\n", "\n", "tData = load_train_set(tfile)\n", "tData.head()"]}, {"metadata": {"_cell_guid": "a4701cc9-3754-4b4a-9365-90326b7291ef", "_uuid": "fd42f8276e3d33d41c61ff9ed4f3764d6a28eecf"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["tData.info()"]}, {"metadata": {"_cell_guid": "f69b173e-3483-46f4-9908-41953bc11dc7", "collapsed": true, "_uuid": "e79aaed82042c175a2ccf35357c088c1b810ac59"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["# tData.to_pickle('tData.csv')"]}, {"metadata": {"_cell_guid": "5f2bed2f-1f4e-420d-b7df-d84b7b30ce59", "collapsed": true, "_uuid": "e3d0e41ecb8ba903daefd48d4455f17b49d280bd"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["def getXsparse(df,addText=False,test=False):\n", "    \"\"\"\n", "    \"\"\"\n", "    cv = TfidfVectorizer()\n", "    if addText:\n", "        X = sp.sparse.hstack([scp.sparse.csr_matrix(df[cols].values),\n", "                      cv.fit_transform(df['Name'].astype('str'))],'csr')\n", "    else:\n", "        X =scp.sparse.csr_matrix(pd.get_dummies(df[cols],columns=cols).values)\n", "    return X"]}, {"metadata": {"_cell_guid": "ddea4d1c-0f05-44ed-b77c-ebf40294f415", "collapsed": true, "_uuid": "7320ae78a5c48eb323253dfd71af9f1f00b4ef05"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["# X_sp = getXsparse(tData)"]}, {"metadata": {"_cell_guid": "b1e4d4ad-4a95-47b9-84c0-f445767f5958", "collapsed": true, "_uuid": "c2e90d0f699bc838b5ef0a77d27e7cb59a94ac11"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": []}, {"metadata": {"_cell_guid": "6f81ed3f-7303-4f7e-887e-693511f4f06c", "collapsed": true, "_uuid": "65287d17f12fae667869cf43a5e0bd5f0cfb761a"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["# X_sp = getXsparse(X)\n", "# X_Validate_sp = getXsparse(X_Validate)\n", "# X=None\n", "# X_Validate = None\n", "# X_sp.shape,X_Validate_sp.shape"]}], "nbformat_minor": 1, "nbformat": 4}