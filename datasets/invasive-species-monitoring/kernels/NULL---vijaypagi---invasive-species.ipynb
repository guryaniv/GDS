{"cells":[{"metadata":{"_uuid":"89659c745c3d17c56c902335972f539bc8a7b02d"},"cell_type":"markdown","source":"# Udacity - Capstone Project\n\n# Project - Invasive Species"},{"metadata":{"_uuid":"cd9ca74e7ed8e9d66ca51cfa6da0b3dca6460e2b"},"cell_type":"markdown","source":"## In this Project, we are going to work on a problem of identifying Invasive species amongst all the species available."},{"metadata":{"_uuid":"cc52abdbf923eaf5dffd2acfa3da75abd147d96c"},"cell_type":"markdown","source":"### This is part of Kaggle competition projects on Invasive Species,\n###    There is a lot of researchers human effort indulged every year in identifying the invasive species amongst all the species which are helpful for the environment and plant growth. This is really an important problem as saving plants from invasive species not only saves the plants but also the humans from intaking from those environment. As we are heading to the new generation technologies, we need to find an automated solution to replace valuable researchers time in finding solutions to much bigger problems rather spending on finding the invasive species among all the other species in a huge farm/any place, as this is very time consuming and inefficient for their time.\n###    So we want to solve this problem by identifying the invasive species among other by developing a image classification model. We would like to try/use different algorithms to develop image classification prediction model, which can identify the invasive species with a very good accuracy which can be relied and with minimal human intervention. Further, we would like to deploy this model on a aerial viewer like a quadcopter to move around farm lands with a deployed image classifier to identify the location of invasive species with minimal time, which will save time, cost , especially life of plants from further damage."},{"metadata":{"_uuid":"a95447359f33da68a77fe4b3e41e928497cccbd8"},"cell_type":"markdown","source":"###### The Road Ahead"},{"metadata":{"_uuid":"100092a63a7f840cc4b865adb7e1aedb4f650261"},"cell_type":"markdown","source":"###### We break the notebook into separate steps. Feel free to use the links below to navigate the notebook.\n###### Step 1: Import Datasets\n###### Step 2: Preprocess the Images\n###### Step 3: Create a CNN Model and Train the Model\n###### Step 4: Import a Pretrained Network\n###### Step 5:Write the Custom Model using Transfer Learning\n###### Step 6:  Test the Model and store the results on Test Set"},{"metadata":{"_uuid":"90a0fb01d25e5ff57c676ae3bfcb44bf1f1a69bd"},"cell_type":"markdown","source":"     "},{"metadata":{"_uuid":"f7764b3525baafab15c64a16de39dd6780820ad6"},"cell_type":"markdown","source":"Importing the libraries is one of the key thing we need to do, let's do it starting with some basic library requirements"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9f5bc82041f0e2efed8b313e81e49b9be58bcfbe"},"cell_type":"code","source":"from sklearn.datasets import load_files       \nfrom keras.utils import np_utils\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nimport random\nimport cv2                \nimport matplotlib.pyplot as plt                        \n%matplotlib inline                               \nfrom keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing import image                  \nfrom tqdm import tqdm\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\nfrom PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True \nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6a9b3e5ceec98b70b709bc811ab14119a3eab6f"},"cell_type":"markdown","source":"Lets, Now load the labels to see the count of training and testing labels.\nTraining labels are provided in the csv file and a sample of submission file is provided as a csv, lets\nload them and drop if NA(Empty) values exist and see the count"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"85df7ed348daba209ec727eaa7bb355c797f8776"},"cell_type":"code","source":"train_labels = pd.read_csv(\"/invasive-species/train_labels.csv\")\nsample_submission = pd.read_csv(\"/invasive-species/sample_submission.csv\")\n\ntrain_labels.dropna\ntrain_labels.tail()\n\nsample_submission.dropna\nsample_submission.tail()\n\nprint('There are %d total training images.' % len(train_labels))\nprint('There are %d total testing images.' % len(sample_submission))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c34b735e342a87e215830e0df7974fe027075bb7"},"cell_type":"markdown","source":"Lets look at a few image visuals"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c8fa078fbe4ad74a68540255f5810acbcae0d336"},"cell_type":"code","source":"import matplotlib.image as mpatImg\ndef species_images(img_path):\n    imgPrnt = mpatImg.imread(img_path)\n    plt.figure(figsize=(10,10))\n    plt.imshow(imgPrnt)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"141a49660ebd7eccbe248353cb4804abc4349cf6"},"cell_type":"markdown","source":"Lets look at some random Training images to check there is no exceptions"},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"0431aaee1a49cdabb66d35b7e8990ec0ddc957a6"},"cell_type":"code","source":"species_images('/invasive-species/train/1.jpg')\nspecies_images('/invasive-species/train/29.jpg')\nspecies_images('/invasive-species/train/298.jpg')\nspecies_images('/invasive-species/train/1008.jpg')\nspecies_images('/invasive-species/train/1007.jpg')\nspecies_images('/invasive-species/train/2287.jpg')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69dda40e278f2120ee5b67acdc62d9febdd4c319"},"cell_type":"markdown","source":"Lets now look at some random set of testing set images"},{"metadata":{"scrolled":false,"trusted":false,"collapsed":true,"_uuid":"298045c8e0bf77b134527a2f59fe207e3f9e66e9"},"cell_type":"code","source":"species_images('/invasive-species/test/76.jpg')\nspecies_images('/invasive-species/test/987.jpg')\nspecies_images('/invasive-species/test/585.jpg')\nspecies_images('/invasive-species/test/1212.jpg')\nspecies_images('/invasive-species/test/1007.jpg')\nspecies_images('/invasive-species/test/1431.jpg')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc74f1b8e743351cb9dee6cd7c72ead60183be8d"},"cell_type":"markdown","source":"Lets Visualize the species images from the training set images by looking at their dimensions for a indepth view"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6515624176a911b14ddfd53e93dd23d399ed6a02"},"cell_type":"code","source":"def smpl_visual(path, smpl, dim_y):\n    \n    smpl_pic = glob(smpl)\n    fig = plt.figure(figsize=(20, 14))\n    \n    for i in range(len(smpl_pic)):\n        ax = fig.add_subplot(round(len(smpl_pic)/dim_y), dim_y, i+1)\n        plt.title(\"{}: Height {} Width {} Dim {}\".format(smpl_pic[i].strip(path),\n                                                         plt.imread(smpl_pic[i]).shape[0],\n                                                         plt.imread(smpl_pic[i]).shape[1],\n                                                         plt.imread(smpl_pic[i]).shape[2]\n                                                        )\n                 )\n        plt.imshow(plt.imread(smpl_pic[i]))\n        \n    return smpl_pic\n\nsmpl_pic = smpl_visual('/invasive-species/train\\\\', '/invasive-species/train/112*.jpg', 4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3393b77fc1366e2fc880b282dd9a6163b4172b15"},"cell_type":"markdown","source":"Lets now, Look at various transformations of the images by looking at Original, RGB, Lab, Gray images transformation of set of training images"},{"metadata":{"scrolled":false,"trusted":false,"collapsed":true,"_uuid":"c4abe2cdc25eb75e2745d715392e5ecb7b82e63e"},"cell_type":"code","source":"def visual_with_transformation (pic):\n\n    for idx in list(range(0, len(pic), 1)):\n        ori_smpl = cv2.imread(pic[idx])\n        smpl_1_rgb = cv2.cvtColor(cv2.imread(pic[idx]), cv2.COLOR_BGR2RGB)\n        smpl_1_lab = cv2.cvtColor(cv2.imread(pic[idx]), cv2.COLOR_BGR2LAB)\n        smpl_1_gray =  cv2.cvtColor(cv2.imread(pic[idx]), cv2.COLOR_BGR2GRAY) \n\n        f, ax = plt.subplots(1, 4,figsize=(30,20))\n        (ax1, ax2, ax3, ax4) = ax.flatten()\n        train_idx = int(pic[idx].strip(\"/invasive-species/train\\\\\").strip(\".jpg\"))\n        print(\"The Image name: {} Is Invasive?: {}\".format(pic[idx].strip(\"train\\\\\"), \n                                                           train_labels.loc[train_labels.name.values == train_idx].invasive.values)\n             )\n        ax1.set_title(\"Original - BGR\")\n        ax1.imshow(ori_smpl)\n        ax2.set_title(\"Transformed - RGB\")\n        ax2.imshow(smpl_1_rgb)\n        ax3.set_title(\"Transformed - LAB\")\n        ax3.imshow(smpl_1_lab)\n        ax4.set_title(\"Transformed - GRAY\")\n        ax4.imshow(smpl_1_gray)\n        plt.show()\n\nvisual_with_transformation(smpl_pic)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e83c17de32f7e016342567e304e2d1961dc3f0ea"},"cell_type":"markdown","source":"Lets now preprocess the training and Testing Images and get them all to an array of images using the labels of the training and testing images"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ea133a6b80396a2fe471ac0a15cbddd28f7a4a97"},"cell_type":"code","source":"img_path = \"/invasive-species/train/\"\n\nprint(img_path)\n\ny = []\nfile_paths = []\nfor i in range(len(train_labels)):\n    file_paths.append( img_path + str(train_labels.iloc[i][0]) +'.jpg' )\n    y.append(train_labels.iloc[i][1])\ny = np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0db39fc4a73c8c671d14e3fe871309ebc4d1d637"},"cell_type":"markdown","source":"We now resize all the images of training set to make sure all of the images are of the same size, so that the training of the model is consistent with out any issues\nInitially lets define a function to center all the image pixels and then use open cv to read the images to common size as acceptable by our models"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2901e20e8a553028080c350e6711af6dad80debf"},"cell_type":"code","source":"def centering_image(img):\n    size = [256,256]\n    \n    img_size = img.shape[:2]\n    \n    # centering\n    row = (size[1] - img_size[0]) // 2\n    col = (size[0] - img_size[1]) // 2\n    resized = np.zeros(list(size) + [img.shape[2]], dtype=np.uint8)\n    resized[row:(row + img.shape[0]), col:(col + img.shape[1])] = img\n\n    return resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0421667a52cf101026876c38ca55032f0f6c1466"},"cell_type":"code","source":"x = []\nfor i, file_path in enumerate(file_paths):\n    #read image\n    img = cv2.imread(file_path)\n    grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    #resize\n    if(img.shape[0] > img.shape[1]):\n        tile_size = (int(img.shape[1]*256/img.shape[0]),256)\n    else:\n        tile_size = (256, int(img.shape[0]*256/img.shape[1]))\n\n    #centering\n    img = centering_image(cv2.resize(img, dsize=tile_size))\n    \n    #out put 224*224px \n    img = img[16:240, 16:240]\n    x.append(img)\n\nx = np.array(x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc1d87264ec42e40966d224bb320e772512d47cd"},"cell_type":"markdown","source":"Now, Lets do the same process for test images ,\nfor the test images lets make the use of the sample submission.csv already available and store them as a numpy array of equal size"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"11a5fde91fe4dbcfd9a1054d9212482b009d6115"},"cell_type":"code","source":"img_path = \"/invasive-species/test/\"\n\ntest_names = []\nfile_paths = []\n\nfor i in range(len(sample_submission)):\n    test_names.append(sample_submission.ix[i][0])\n    file_paths.append( img_path + str(int(sample_submission.ix[i][0])) +'.jpg' )\n    \ntest_names = np.array(test_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bdd14492556e0a2b2013948d53b167a24418af73"},"cell_type":"code","source":"test_images = []\nfor file_path in file_paths:\n    #read image\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    #resize\n    if(img.shape[0] > img.shape[1]):\n        tile_size = (int(img.shape[1]*256/img.shape[0]),256)\n    else:\n        tile_size = (256, int(img.shape[0]*256/img.shape[1]))\n\n    #centering\n    img = centering_image(cv2.resize(img, dsize=tile_size))\n    \n    #out put 224*224px \n    img = img[16:240, 16:240]\n    test_images.append(img)\n    \n    path, ext = os.path.splitext( os.path.basename(file_paths[0]) )\n\ntest_images = np.array(test_images)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eae1843cc56b2d53a081e9e6084bc0ada1e53956"},"cell_type":"markdown","source":"Before, we decide that the data is ready for training, \nlets randomize/shuffle the data, so that there is a random distribution of the images across the dataset\nLets use numpy library function random for this using permutation of the data for training and shuffle and store them in x and y as numpy shuffled array"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1dae1e1b5949756cabc5329c82897f0263d2a1ac"},"cell_type":"code","source":"data_num = len(y)\nrandom_index = np.random.permutation(data_num)\n\nx_shuffle = []\ny_shuffle = []\nfor i in range(data_num):\n    x_shuffle.append(x[random_index[i]])\n    y_shuffle.append(y[random_index[i]])\n    \nx = np.array(x_shuffle) \ny = np.array(y_shuffle)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"808d1f55b82c6ab061e13e3649a634b91524882f"},"cell_type":"markdown","source":"Now, we are further close to training ,\n\nOne last step,\n\nBefore the step, when we start training on the whole dataset and test on the testing set, there is no chance of optimization and testing on the training set is a sin, So now comes our safeguard set i.e., Validation set,\nThis is a cross validation set to cross validate the training done and then based on the cross validation accuracy which is not at all a part of the training set, we can estimate our test accuracy.\nBy the way, testing on the training set itself leads to biggest problem Overfitting.\nRandomization and Shuffling is also used to mitigate the risk of overfitting.\nWe should always make sure to be away from overfitting, So we are dividing our training set to two parts, one is Training set and the other cross validation set.\n\nUsually we use 80% of the shuffled training set as Training set and 20% for the cross validation set.\n\nWe are doing the same as below and assiging the training sets to x_train and x_test and cross validation sets to y_train adn y_test"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e306548a48443f86bc14ea3127a00a43b17b71a9"},"cell_type":"code","source":"val_split_num = int(round(0.2*len(y)))\nx_train = x[val_split_num:]\ny_train = y[val_split_num:]\nx_test = x[:val_split_num]\ny_test = y[:val_split_num]\n\nprint('x_train', x_train.shape)\nprint('y_train', y_train.shape)\nprint('x_test', x_test.shape)\nprint('y_test', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e4856621f25039734a0369efe1016e4e4dabcb5"},"cell_type":"markdown","source":"Now, Lets Normalize the training data so that the range lies in between 0 and 1 .\n\nNormalization will help us to remove distortions caused by lights and shadows in an image."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"286d5804f4e2c517c27b4e36f4d94b0a257bbd89"},"cell_type":"code","source":"x_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cf374aadb77ada988016afaaa799bd4328cfbe52"},"cell_type":"code","source":"def invasiveSpeciesCapture(model_Capture, speciesPic):\n    species_batch = np.expand_dims(speciesPic,axis=0)\n    conv_species = model_Capture.predict(species_batch)\n    \n    conv_species = np.squeeze(conv_species, axis=0)\n    print(conv_species.shape)\n    conv_species = conv_species.reshape(conv_species.shape[:2])\n    \n    print(conv_species.shape)\n    plt.imshow(conv_species)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6e21fe03241ea9e2e0dbeec307d8dbde5cb334e"},"cell_type":"markdown","source":"Now, its Time for us to create our own model using Neural Networks\n\nlets use Convolution Neural Networks which work really good in image classification problems\n\nLets import the layers from the keras library\n\nOur Model consists of input layer and three hidden layers and one output layer\n\nFor the input layer lets set the shape of input as its required and add as a Batch normalization to the shape of 224,224 and 3 dimensions of the image.\n\nFor the First hidden layer, we will use conv2d layer by using hyper parameters as filters = 256 ,kernelsize = 2 padding as same with a relu activation function.\n\nAs we have started with 256 filter, it increases the dimensionality to huge extent, so when training, it may lead to overfitting, so now we add Maxpooling after our first layer to reduce the dimensionality of the feature set with a pool size of 2\n\nOne more step to keep the training generalized is adding a dropout function after the layer to make sure we drop a few data randomly from the training with a given probability to make sure the data is not overfitting and more generic to test on totally different images\n\nSo, Now we add one more hidden layer with reduced filters to 64 and rest being the same\nIts time for dimensionality reduction and dropping out random data\n\nOne last hidden layer with 128 filters\n\nFinally lets get the dimensiolity of the features to 1 by adding GlobalAveragePooling layer and then a final output dense layer using activation function sigmoid\n\nLets summarize and see our layers\n"},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"fd3558c938ee1cc80b1afd6177b632e1dcdb61a0"},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D,BatchNormalization,Convolution2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential, Model, load_model\nfrom keras import applications\nfrom keras import optimizers\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\nprint('Training set is ',x_train.shape[0])\nprint('Validation set is ',x_test.shape[1])\n\n\nmodel.add(BatchNormalization(input_shape=(224, 224, 3)))\n\nmodel.add(Conv2D(filters = 256,kernel_size=2,padding='same',activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(filters = 64,kernel_size=2,padding='same',activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(Conv2D(filters = 128,kernel_size=2,padding='same',activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(1,activation = 'sigmoid'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24e9fec1903058868ae9152c6e76971681024542"},"cell_type":"markdown","source":"lets Capture a few images data in various levels for an insight"},{"metadata":{"_uuid":"3f0296b97dbd4e1e774757067adbb2fc85f73d60"},"cell_type":"markdown","source":"Now, Lets compile our built model using our best performing Optimizer,\n(Although my trials went from various optimizers and finally landed here.. lol)\n\nlets use binary cross entropy as a loss function and metric is accuracy"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5c09a8df7da8bb01cb3e147ff9526e2db6e3b057"},"cell_type":"code","source":"model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5262edf5f56b2d495ffc95598048fdcc4fafd8f"},"cell_type":"markdown","source":"Its the fun part now,\n\nLets decide on right number of epochs(There is no right eochs by the way, struggle on multiple attempts with numerous numbers and observe their result and land up with best found and say the word right number of epochs....like me)\n\nLets make sure to store the weights that are useful in incrementing the accuracy or decrementing the validation loss, so that we can add them\n\nLets give a reasonable batch size to make our model train slowly and grow up to be a good admirable model."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a19b16cd8ead46e79c4798b79a5326b94f04f4ee"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint \n\nepochs = 60\n\ncheckpointer = ModelCheckpoint(filepath='weights.best.from_scratch.hdf5', \n                               verbose=1, save_best_only=True)\n\nmodel_trained = model.fit(x_train, y_train, \n          validation_data=(x_test, y_test),\n          epochs=epochs, batch_size=30, callbacks=[checkpointer], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d1ed4aca9e1fb92f9d4aec6ec8994325ebe0a74"},"cell_type":"markdown","source":"Not Bad,\n\nOur Model performed reasonably well(After hell lot of time, and 20 attempts)\n\nA Validation score over 94% is good enough for a model, but we are opportunistic. So lets focus on incrementing\nit further to make it best model(lets try).\n\nLets first store the best weights to our model as we stored...Optimizing!!"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a356cc54e37cd8a4bd2d7a3e1ee6c8320d01b4f8"},"cell_type":"code","source":"model.load_weights('weights.best.from_scratch.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47a91dde195c800329bef508803a86346c0b4f40"},"cell_type":"markdown","source":"Lets now observe how our accuracy of training and validation graph by using the history of our model trained above and observe if there are any total discrepencies we can correct."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c29d8da9b0a236f969dcfc4e5a5a8a35eb517232"},"cell_type":"code","source":"plt.plot(model_trained.history['acc'])\nplt.plot(model_trained.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed0b305413b0cabab9aa57027cfbfb9f214db31"},"cell_type":"markdown","source":"Our Graph looks good, After a good number of epochs our training accuracy started a very little flowing away but our validation accuracy is really good enough to grow.\n\nNow Lets see our training and validation loss history"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"df1263de21cf75ac2a5a6e43590fe6637147f348"},"cell_type":"code","source":"plt.plot(model_trained.history['loss'])\nplt.plot(model_trained.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5174ad63efa579bf400760f8d39019fb8494b595"},"cell_type":"markdown","source":"As expected, like the accuracy , training loss decremented to certain level and then started a little diverging away, where as our validation loss continued to reach rock bottom."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"292eba6b7e175ba08a274a1f43bdb6576ccc5813"},"cell_type":"code","source":"print(\"Training loss: {:.2f} / Validation loss: {:.2f}\".\\\n      format(model_trained.history['loss'][-1], model_trained.history['val_loss'][-1]))\nprint(\"Training accuracy: {:.2f}% / Validation accuracy: {:.2f}%\".\\\n      format(100*model_trained.history['acc'][-1], 100*model_trained.history['val_acc'][-1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"329295fef0c754fdbb00cb71daf293368d9d9a6c"},"cell_type":"markdown","source":"#### Finally Its time for inviting Transfer Learning\n\nLets define the input layer and its shape to be provided as an input\n\nLets use VGG16 Pretrained network on Imagenet and reduce a good amount of feature training.(Have tried couple of others but not yielded good result, so came up to vgg16 as a good friend)"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4361b3baf172e89192b33c591c3ad2ffd1119a80"},"cell_type":"code","source":"img_rows, img_cols, img_channel = 224, 224, 3\n\nbase_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, img_channel))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f574ea0e62146555b1f041f84e8cab07c4810749"},"cell_type":"markdown","source":"Lets do the same thing as earlier,\n\nBut just a small twist lets make use of our pretrained network before we add our fully connected Dense layers\n\nInitally lets add our VGG16 Pretrained network with the input shape as earlier 224,224,3\n\nThen Lets add three fully connected layer with filters of 256,128,64 with relu as an activation function\n\nFor the Final layer lets reduce our filter to 1 and use sigmoid as an activation function.\n\nOnce we build our model as above, lets compile our model using the same loss function binary cross entropy, lets optimize with the same Adam-my friend, but lets use parameters using exponential learning rate so that we learn further slowly to understand our features more for better accurate prediction for our metrics accuracy "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"03ec1e5a620555bf10613e773f7b6deec634f5d5"},"cell_type":"code","source":"add_model = Sequential()\nadd_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n\nadd_model.add(Dense(256, activation='relu'))\n\nadd_model.add(Dense(128, activation='relu'))\n\nadd_model.add(Dense(64, activation='relu'))\n\nadd_model.add(Dense(1, activation='sigmoid'))\n\nvgg16_model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\nvgg16_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1E-4),\n          metrics=['accuracy'])\nvgg16_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36a20e80726d74f6f91de7ac51f947a79ad07175"},"cell_type":"markdown","source":"Ohh, Great,\n\nWe have got a huge number of parameters but remember we have got our top layers pretrained and usable and now its time\nfor us to train our model, so this time we further more use ImageDataGenerator function to further better understand\nthe images by rotating, and training the models at various angles to increase the accuracy.\n\nSo, we are now blowing our siren to start the train...no no training...we know the drill right.. lets decide on right\nnumber of epochs(same explanation as above...lol)\nLets have a batch size of 30...and lets see"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"38c8e8cb9b7bb157935eff30abcd0e19a00abc55"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint \n\nbatch_size = 32\nepochs = 20\n\nvgg16_train_datagen = ImageDataGenerator(\n        rotation_range=31, \n        width_shift_range=0.1,\n        height_shift_range=0.1, \n        horizontal_flip=True)\nvgg16_train_datagen.fit(x_train)\n\nvgg16_checkpointer = ModelCheckpoint(filepath='weights.best.vgg16.hdf5', \n                               verbose=1, save_best_only=True)\n\n\nvgg16_history = vgg16_model.fit_generator(\n    vgg16_train_datagen.flow(x_train, y_train, batch_size=batch_size),\n    steps_per_epoch=x_train.shape[0] // batch_size,\n    epochs=epochs,callbacks=[vgg16_checkpointer],\n    validation_data=(x_test, y_test),\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a40b72112cffb663ccbf9002cc708338e1a6c16"},"cell_type":"markdown","source":"Optimizing the model using saved best weights"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4c7d77ce8fe006d5f2417b8f05317ae5b25a5f1d"},"cell_type":"code","source":"vgg16_model.load_weights('weights.best.vgg16.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe6d71e43163a07f98feaf22ede6beb3d6ac6420"},"cell_type":"markdown","source":"Lets again visualize the training history graph for the above training\n\nLets now observe how our accuracy of training and validation graph by using the history of our model trained above and observe if there are any total discrepencies we can correct."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"49fd3df397d4612f5de3b01b5613636c1a7a7ace"},"cell_type":"code","source":"plt.plot(vgg16_history.history['acc'])\nplt.plot(vgg16_history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7170c3a4da89474e9b2c1bc1c728f18eb9e4b464"},"cell_type":"markdown","source":"We See the training accuracy is consistent but the validation accuracy is little volatile \nand growing slowly then after,we have come with this epochs after observing the stagnancy at 55, 62 etc epochs and found this as a optimal number\n\nLets see the Loss during training"},{"metadata":{"scrolled":false,"trusted":false,"collapsed":true,"_uuid":"c14b4c97baa9217970338ba4fda308f73d68d5a1"},"cell_type":"code","source":"plt.plot(vgg16_history.history['loss'])\nplt.plot(vgg16_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1256257289f24cf5314febf7ef7b2612f3e63cd8"},"cell_type":"markdown","source":"Training loss is all the time low and decreasing, but the validation loss has increased and then came back to decrease we preferred to early stop to reduce the overfitting.\n\nLets normalize the test images like we did for our images before training for consistent result"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ddd9721f8e509dad13eead9fecc9defbd05d672b"},"cell_type":"code","source":"test_images = test_images.astype('float32')\ntest_images /= 255","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ae22b6478fad197ef61aee88f5706c6ed4bf2b7"},"cell_type":"markdown","source":"Lets see the loss and accuracy of the overall training and Validation sets"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"26106ec2142a62602cd86853b7edcb7494ac36ed"},"cell_type":"code","source":"print(\"Training loss: {:.2f} / Validation loss: {:.2f}\".\\\n      format(vgg16_history.history['loss'][-1], vgg16_history.history['val_loss'][-1]))\nprint(\"Training accuracy: {:.2f}% / Validation accuracy: {:.2f}%\".\\\n      format(100*vgg16_history.history['acc'][-1], 100*vgg16_history.history['val_acc'][-1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09d50cf38da1ae9e27662de018655dc7447520b6"},"cell_type":"markdown","source":"We see a Training loss of increased from 0.00 to 0.05 and Validation loss increased from 0.04 to 0.15 , which are fair enough to be as a good model\n\nTraining accuracy of 98.06% yielding a validation accuracy of 94.12% is really good than our own model.\n\nSo, Transfer learning has decreased the accuracy relatively but the training and validation converged so thats good sign of training.\n\nLets now predict the test image labels and store them to vgg16submit.csv file as formatted like sample_submission and lets see our board rank"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d098bc9a37702915fd7d4db8067e0eb852a81f9a"},"cell_type":"code","source":"predictions = vgg16_model.predict(test_images)\nfor i, name in enumerate(test_names):\n    sample_submission.loc[sample_submission['name'] == name, 'invasive'] = predictions[i]\n\nsample_submission.to_csv(\"vgg16submit.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59239a173c0002a70bcc10108cc0315a8c8ba3a6"},"cell_type":"markdown","source":"Wow, The Result has yielded us a great testing result score of 0.98997\nWhich is among top 100 submissions.\nOur Model has really excelled with good training"},{"metadata":{"_uuid":"bfa835f9fc8a0f2ea52d6ddf63b70baf2ea99f6b"},"cell_type":"markdown","source":"This has yielded a test score of 0.98997 which got us 97/513 position on leaderboard for the 4th submission,lets give one more try using different pretrained model.The Benchmark model was 0.997 and our attempt was to reasonably give a good attempt on my first kaggle competition.\nLets try some tuning now."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"469fda350fffde46ab314cb1e9b6f00b5824b408"},"cell_type":"code","source":"add_model = Sequential()\nadd_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n\nadd_model.add(Dense(256, activation='relu'))\n\nadd_model.add(Dense(128, activation='relu'))\n\nadd_model.add(Dense(64, activation='relu'))\n\nadd_model.add(Dense(1, activation='sigmoid'))\n\nvgg16_model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\nvgg16_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1E-4),\n          metrics=['accuracy'])\nvgg16_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36a8a85d5ce2e2a5aefe0288b67df5f9b88ec070"},"cell_type":"markdown","source":"Finally,\n\nDuring the process, When creating a model of own , the training was struck at a point when i used various optimizers such as rmsprop and took me hell lot of time to figure out where the problem is....and the second place is the tuning, i have had a difficulty to tune once the model was setup specially at a cost of gpu and cpu speed.\nI was unable to build the model on my system and couldnt get a right online cloud platform and the documentation on aws was removed from udacity and no peer environment related to machine learning, this has taken a lot of time, finally because of this, now i am capable enough to train models on three big cloud platforms, AWS, Google, FLoyd..lol...for a good cause.\n\nThe best part during the model training is that i was surprised to see a huge difference when i have changed the optimizer to Adam, which literally made me see progress and happiness in my eyes..and Transfer learning also made a difference."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}