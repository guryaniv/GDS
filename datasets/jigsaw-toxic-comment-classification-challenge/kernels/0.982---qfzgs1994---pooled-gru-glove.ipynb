{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-output":false,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\nembeding_file_path = \"../input/glove840b300dtxt/glove.840B.300d.txt\"\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a25c46f-abd6-4ef0-8552-b154293946db","_uuid":"cdbdc8298783c7c21fae706451153fb2fdb542d7","collapsed":true,"trusted":true},"cell_type":"code","source":"train['comment_text'][3]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c5d3c4a4-8e52-461e-afea-fcbb2a0c2f21","_uuid":"9e30f12f3e218ba9d934e6b73c02260a56c4fc88"},"cell_type":"markdown","source":"preprocessing :\n1. lowercase\n2. stopwords\n3. low frequency words drop out\n4. lemmatizer"},{"metadata":{"_cell_guid":"48c317c1-1102-4ff1-b6d9-e4f1843fd306","_uuid":"bd0abfd49f547815541e57afd8e19479fe3c3309","collapsed":true,"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\nwl = WordNetLemmatizer()\n\nfilters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n\ndef normalize(text):\n    text = text.lower()\n    translate_map = str.maketrans(filters, \" \" * len(filters))\n    text = text.translate(translate_map)\n    tokens = nltk.word_tokenize(text)\n    tags = nltk.pos_tag(tokens)\n    stop_words = set(stopwords.words('english'))\n    seq= [wl.lemmatize(t[0], pos=get_wordnet_pos(t[1])) for t in tags if t[0] not in stop_words]\n    seq= [wl.lemmatize(t[0], pos=get_wordnet_pos(t[1])) for t in tags]\n\n    return seq\n#s = normalize(train[\"comment_text\"][3/","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6ec946d9-1e00-4563-a4ca-b7a818ae50e9","_uuid":"3f4cd4abd0f5f4f1180a9ffe314f02f076dc933e","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nmax_features = 50000\nembed_size = 300\nmaxlen = 150\n\ndata = pd.concat((train['comment_text'], test['comment_text']))\nseqs = [normalize(text) for text in data]\n\ndef seq_to_sequence(seq, word_index):\n    sequence = []\n    for word in seq:\n        if not word_index.get(word): continue \n        sequence.append(word_index[word])\n    return sequence\n\ndef fit_on_sequence(seqs):\n    word_counts = dict()\n    for seq in seqs:\n        for w in seq:\n            if w not in word_counts:\n                word_counts[w] = 0\n            word_counts[w] += 1\n    wcounts = list(word_counts.items())\n    wcounts.sort(key=lambda x: x[1], reverse=True)\n    sorted_voc = [wc[0] for wc in wcounts if wc[1]>=3]\n    sorted_voc = [wc[0] for wc in wcounts]\n    word_index = dict(list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n    return word_index\n    \nword_index = fit_on_sequence(seqs)\ntrain_words = [seq_to_sequence(seq, word_index) for seq in seqs[:train.shape[0]]]\ntest_words = [seq_to_sequence(seq, word_index) for seq in seqs[train.shape[0]:]]\ntrain_words = pad_sequences(train_words, maxlen=maxlen )\ntest_words = pad_sequences(test_words, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1343e612-e07c-4db4-9cb0-ea906aed1f1f","_uuid":"efb99d1f74e55ebf7b99d3af48c3ad82ebcb5796","collapsed":true,"trusted":true},"cell_type":"code","source":"len(word_index)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d8350738-88fa-4558-b012-8159a37d8d72","_uuid":"e64c586db6be97805ccd3f547f124c134b3aacc2","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"# char_index = tokenizer_char.word_index\n# char_size = len(char_index)\n# char_size = min(5000, char_size)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc70894e-640d-400d-b46b-256854b2de92","_uuid":"47c6e6137fa82d9101f299b728f359a92831b3dc","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_coef(word, *coefs):\n    return word, np.asarray(coefs, dtype=np.float64)\nembeding_dict = dict(get_coef(*s.strip().split(\" \")) for s in open(embeding_file_path))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a8c1382-27ad-4fd8-95c3-831be94f7048","_uuid":"6cf0f307139b8c3897ce8eed345635744351a9ce","collapsed":true,"trusted":true},"cell_type":"code","source":"\nmax_words = min(max_features, len(word_index))\nembeding_matrix = np.zeros((max_words+1, embed_size))\nfor word,i in word_index.items():\n    if word not in embeding_dict: continue\n    if i>max_words:break\n    embeding_matrix[i] = embeding_dict[word]\n# char_matrix = np.random.randn(char_size, embed_size)\n# embeding_matrix = np.concatenate((embeding_matrix, char_matrix), axis=0)\n\n#  transform the char_index\n# addition = max_words + 1\n# train_chars += addition\n# test_chars += addition\n\n# train_all = np.concatenate((train_words, train_chars), axis=1)\n# test_all = np.concatenate((test_words, test_chars), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ccd631d-9fad-49db-87cd-8993a6391e46","_uuid":"8d2f04bb70ca3b0af59bb7a1858c6d7c034d05b4","collapsed":true,"trusted":true},"cell_type":"code","source":"len(embeding_dict)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4e1af5e0-e701-4055-b053-9545988de2c6","_uuid":"9157a4bcc389507a01a4ead3851ccf2d43f51a2d","collapsed":true,"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,Conv1D\nfrom keras.callbacks import Callback\ndef get_model():\n    inp = Input(shape=(maxlen,)) #maxlen\n    x = Embedding(max_words+1, embed_size, weights=[embeding_matrix])(inp)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(GRU(maxlen, return_sequences=True))(x)\n    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    conc = concatenate([max_pool, avg_pool])\n    oup = Dense(6, activation='sigmoid')(conc)\n    \n    model = Model(input=inp, output=oup)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nmodel = get_model()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"317b7ebe-eccb-417f-9deb-bb3ad7a63816","_uuid":"aec3fc53588d7f7c676cbc981fdcf4e72d20a0c4","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n\nbatch_size = 128\nepochs = 4\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\nX_tra, X_val, y_tra, y_val = train_test_split(train_words, y_train, train_size=0.95, random_state=233)\nRocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\nhist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n                 callbacks=[RocAuc,], verbose=1)\n\n\ny_pred = model.predict(test_words, batch_size=1024)\n          ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"58fe96c9-a30e-4ef4-bb70-d3b489c17db0","_uuid":"facb500e14246438c724bf47a98dae01ce0cb50f","collapsed":true,"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\nsubmission.to_csv('gru.csv', index=False)           ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}