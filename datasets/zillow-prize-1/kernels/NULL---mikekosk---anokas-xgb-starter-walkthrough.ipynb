{"nbformat": 4, "cells": [{"cell_type": "markdown", "execution_count": null, "metadata": {"_uuid": "9b42b9efcc543b9b627f8d520e6eb02442c4cc20"}, "outputs": [], "source": ["## Kaggle Competition Entry: Zillow Home Prices\n", "\n", "This is a walk-through of Anokas's excellent XGB Starter Kernel, found here: https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655/code\n", "\n", "A lot of Kaggle kernels lack documentation or any commenting, so before I jump into a Kaggle project I like to first find a popular kernel by a respectable author and get a sense of what has been done and what popular modifications have been done to the kernel before jumping in myself.  Hopefully you will find this useful as well!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "7c548b5f6d2c38e37e65daefcf1a9e4b7781550a", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import xgboost as xgb # xgboost package \n", "import gc # to take out da trash [memory management]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "ff3d02b33d7c41423690bf78109fe25b9ff58250", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": ["# Lets see whats in our input folder\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "ce4426cb3e07eaa90e96945166dbe8d5dda35d8e", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": ["# Load Data\n", "train = pd.read_csv('../input/train_2016_v2.csv')\n", "prop = pd.read_csv('../input/properties_2016.csv')\n", "sample = pd.read_csv('../input/sample_submission.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "2c16cf2cdb78af0b59ddb7a37d4fbbdece5da4d4", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": "# Print Data Shape\n# In order to submit to Kaggle we'll be modifiying the sample dataset with our predictions\nprint (train.shape, prop.shape, sample.shape)"}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "5b84a56bd5e3f17504283f763782519e22cb5f62", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": ["# Convert to Float32 \n", "# This is so that our script can run on Kaggle Kernels\n", "# Kaggle has a memory limit on the Kernels so this is a necessary step\n", "# We're turning 64 bit floats into 32 bit floats\n", "\n", "for c, dtype in zip(prop.columns, prop.dtypes):\n", "    if dtype == np.float64:\n", "        prop[c] = prop[c].astype(np.float32)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "5cb1c9c2f50000e37456b1ad719ad90d85b8cfe8", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": "# Merge training dataset with properties dataset\ndf_train = train.merge(prop, how='left', on='parcelid')\n\n# Remove useless columns (anything used for ID purposes, has no variation, or is not suitable for training)\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n\n# Save Train columns\ntrain_columns = x_train.columns\n\n# Train our model to predict log error\ny_train = df_train['logerror'].values\n\n# Binarify our categorical column variables to remove NaN objects\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\n# Delete our old training dataset; take out the trash\ndel df_train; gc.collect()"}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "bad248a688d17a06771c0a69ec52a8b63dc513d8", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": ["# Split dataset at roughly the ~88% mark into training and validation datasets\n", "# We'll be evaluating the fine tuning of our model by seeing how it runs on the validation dataset\n", "\n", "split = 80000\n", "x_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "ad02f75a5302a3c8e14ca9d01e37642166df1320", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": ["# Split training and validation datasets\n", "\n", "d_train = xgb.DMatrix(x_train, label=y_train)\n", "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n", "\n", "del x_train, x_valid; gc.collect()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "1092e63a505817e70320dd69b9115f038bb3ad79", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": ["# Set hyperparameters\n", "# Only hyperparamater that is relevant for optimizing here (in Anokas's notebook) is max_depth.  \n", "# \n", "# When I build my own submission I will try tuning gamma, min_child_weight, subsample, \n", "# colsample_bytree, as well as the regularization paramaters.\n", "\n", "params = {}\n", "params['eta'] = 0.02 # control the learning rate: scale the contribution of each tree by a factor of 0 < eta < 1. Lower is slower but more robust to overfitting.\n", "params['objective'] = 'reg:linear' # Default.  Running a regression, since we're predicting values not classes\n", "params['eval_metric'] = 'mae' # We're evaluating our models on Mean Average Error.  \n", "params['max_depth'] = 4 # Maximum depth of a tree, increase this value will make the model more complex / likely to be overfitting.\n", "params['silent'] = 1 # Don't print messages\n", "\n", "# Train model\n", "#\n", "# 'Watchlist' is an evaluation dataset- We will be tuning our model based on how it does in the validation dataset \n", "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "#\n", "# Anokas has implemented early stopping.  Once we reach the point where our validation score no \n", "# longer improves after a set number of iterations (100 in this case) we use the model run that preceeded the \n", "# chain of 100 un-changed iterations.  This is to prevent additional overfitting where it does not improve the model.\n", "clf = xgb.train(params, d_train, 10000, watchlist, early_stopping_rounds=100, verbose_eval=10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "79cca3dce664d813de5e15cf708e8b838cd5a07a", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": ["# Build test set\n", "\n", "sample['parcelid'] = sample['ParcelId']\n", "df_test = sample.merge(prop, on='parcelid', how='left')\n", "\n", "# Memory Management\n", "del prop; gc.collect()\n", "\n", "# Binarify the data (ie remove NaN, set it to False)\n", "x_test = df_test[train_columns]\n", "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n", "    x_test[c] = (x_test[c] == True)\n", "\n", "# Memory management\n", "del df_test, sample; gc.collect()\n", "\n", "# Convert table to xgb format\n", "d_test = xgb.DMatrix(x_test)\n", "\n", "# Memory management\n", "del x_test; gc.collect()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "ad16e70b3901aaee71eea93c5da9d2de2b92de4d", "_execution_state": "idle", "collapsed": true}, "outputs": [], "source": ["# Make predictions on data\n", "p_test = clf.predict(d_test)\n", "\n", "# Delete testset; take out trash\n", "del d_test; gc.collect()\n", "\n", "# Read sample subgmission\n", "sub = pd.read_csv('../input/sample_submission.csv')\n", "for c in sub.columns[sub.columns != 'ParcelId']:\n", "    sub[c] = p_test\n", "\n", "# Write submission\n", "sub.to_csv('xgb_starter.csv', index=False, float_format='%.4f')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "569c4c8d15ef968a8c82e8def97bff5ad23b7ed1", "_execution_state": "idle", "collapsed": false}, "outputs": [], "source": ""}], "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 2}, "pygments_lexer": "ipython2", "file_extension": ".py", "nbconvert_exporter": "python", "name": "python", "mimetype": "text/x-python", "version": "2.7.13"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 2}