{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nimport gc\n#from sklearn.preprocessing import PolynomialFeatures\n# from sklearn.linear_model import LogisticRegression\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d696ca536da2ebf973908239ffe7e8236144ae1"},"cell_type":"markdown","source":"**Yiming Yan**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path = \"../input/\"\nfiles = os.listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3909921ab45f09d3d9bbccdd7a2f9c49e3cf45d4"},"cell_type":"markdown","source":"**data extraction---split the training set into pre-training set, training set and validation set.**"},{"metadata":{"trusted":true,"_uuid":"e7d4aa2f985d27a7dd9ff1f9237a3dc9ff61939b"},"cell_type":"code","source":"# shuffle the dataset and split into training_set and validation_set\ntest_set = pd.read_csv(path+files[1])\noriginal_dataset = pd.read_csv(path+files[0])\noriginal_features = list(original_dataset.columns)\noriginal_dataset.dropna(inplace = True)   \noriginal_dataset = original_dataset.sample(frac = 1).reset_index(drop = True)\n\nlength = original_dataset.shape[0]\ntrainlength = round(length * 0.4)\n#prelength = round(length * 0.2)\n\n#training_set = pd.DataFrame(original_dataset.iloc[prelength:trainlength], columns = original_features).reset_index(drop = True)\ntraining_set = pd.DataFrame(original_dataset.iloc[0:trainlength], columns = original_features).reset_index(drop = True)\nvalidation_set = pd.DataFrame(original_dataset.iloc[trainlength:], columns = original_features).reset_index(drop = True)\n#pretraining_set = pd.DataFrame(original_dataset.iloc[0:prelength], columns = original_features).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d6de446c38c786549e58f39d7186f50d86f38bd"},"cell_type":"code","source":"del original_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a421db511e6e113f8d1ea0262e0da10c345d668"},"cell_type":"code","source":"# feature extraction function\ndef extracte_feas(dataset):\n    label = False\n    if \"winPlacePerc\" in dataset.columns:\n        dataset_label = dataset[\"winPlacePerc\"]\n        dataset.drop(\"winPlacePerc\", axis = 1, inplace = True)\n        label = True\n    features = list(dataset.columns)\n    for fea in features:\n        if type(dataset[fea].iloc[0]) == str:\n            if fea == \"Id\":\n                string_features = list(dataset[fea])\n            dataset.drop(fea, axis = 1, inplace = True)\n    if label:\n        return dataset_label, string_features\n    else:\n        return string_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7325267f0b4ebf250b8a01f247bfa8fb28422ab0"},"cell_type":"code","source":"# pca method to extracte features\ndef PCA_features(dataset, frac = 1):\n    features = list(dataset.columns)\n    pca = PCA(n_components = frac)\n    pca.fit(dataset)\n    #summ = sum(pca.singular_values_)\n    #addall = 0\n    #for i in range(len(pca.singular_values_)):\n        #if round(addall/summ,1) == 0.9:\n            #break\n        #else:\n            #addall += pca.singular_values_[i]\n    #extrac_length = i\n    correlation = pd.DataFrame(pca.components_,columns = features)\n    extrac_columns=[]\n    for i in range(correlation.shape[0]):\n        #j = correlation.idmax(np.absolute(correlation.iloc[i]))\n        j = np.absolute(correlation.iloc[i]).idxmax()\n        extrac_columns.append(j)\n    #data = pd.DataFrame([dataset[col] for col in extrac_columns]).T\n    return extrac_columns\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21c722821b55c3e19180a29ca3db55aa44620c46"},"cell_type":"code","source":"# MSE\ndef LR_error(pred_label, true_label):\n    error = 0\n    for i in range(len(true_label)):\n        error += (pred_label[i] - true_label[i])**2\n    error = error / len(true_label)\n    return error\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e916d6840896db2c0d66de56f9a5a5856129c88"},"cell_type":"code","source":"def cv_error(*estimator, dataset, datalabel, cv = 2):\n    length = round(len(datalabel) / cv)\n    left = 0\n    right = length\n    error = 0\n    for i in range(cv):\n        valerror = 0\n        valset = pd.DataFrame(dataset.iloc[left:right], columns = dataset.columns).reset_index(drop = True)\n        trainset = dataset.drop([k for k in range(left, right)]).reset_index(drop = True)\n        vallabel = datalabel.iloc[left:right].reset_index(drop = True)\n        trainlabel = datalabel.drop([k for k in range(left, right)]).reset_index(drop = True)\n        for col in trainset.columns:\n            mean = np.mean(trainset[col])\n            std = np.std(trainset[col])\n            trainset[col] = (trainset[col] - mean) / std\n            valset[col] = (valset[col] - mean) / std\n        if len(estimator) > 1:\n            estimator[1].fit(trainset)\n            trainset = estimator[1].transform(trainset)\n            valset = estimator[1].transform(valset)\n        estimator[0].fit(trainset, trainlabel)\n        predlabel = estimator[0].predict(valset)\n        error += LR_error(predlabel, vallabel)\n        left = right\n        right += length\n        if right > len(datalabel):\n            right = len(datalabel)\n        del valset, vallabel\n        del trainset, trainlabel\n            \n    return error / cv\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b40fe1639557c08fe346ca5a133406e5b0ffef6"},"cell_type":"code","source":"training_label, training_id = extracte_feas(training_set)\n#pre_label, pre_stringfeatures = extracte_feas(pretraining_set)\nval_label, val_id = extracte_feas(validation_set)\n#test_id = extracte_feas(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df061b675701a97642a0b06171940427f9836b18","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"markdown","source":"**use pretraining set to train pca and polynomial models and find the best two models --without cross validation       and then use the best two models to train the training set and test the validation set to make an evaluation --with cross validation to choose the hyper parameter.   Besides, consider reason why when N >> D we don't need to do the feature reduction and the test error after pca is larger than before **"},{"metadata":{"_uuid":"87a7aa7a0b7b82c3b06967c68b860f4caffc6509"},"cell_type":"markdown","source":"PCA  with cross validation to choose best fraction of variance"},{"metadata":{"trusted":true,"_uuid":"23657b916623a7af0bb5b1e759f029da965a76ba"},"cell_type":"code","source":"def standardize(data):\n    dataset = data.copy(deep = True)\n    mean_std = {}\n    for col in dataset.columns:\n        mean_std[col] = [np.mean(dataset[col]), np.std(dataset[col])]\n        dataset[col] = (dataset[col] - mean_std[col][0]) / mean_std[col][1]\n    return dataset, mean_std\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"840580b12df229d21959d8aa5ad892a0939c0018"},"cell_type":"code","source":"std_training_set, mean_std = standardize(training_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e46767cd48253677c4117d99d6640cebe855b2c7"},"cell_type":"markdown","source":"var_frac = [i / 100 for i in range(90,100)] +[1]\nalpha = [0.001, 0.01, 0.1, 1.0, 10, 50, 80, 100, 1000, 5000, 10000]"},{"metadata":{"trusted":true,"_uuid":"ccbbcfac6dd3734ccc581ea8065f37209d166cbc"},"cell_type":"markdown","source":"alpha_error = []\nbest_alpha = []\nridge_trainingerror = []\nridge_valerror = []"},{"metadata":{"trusted":true,"_uuid":"f48234eefe5daf76a36eec821af00a8fcec4263d"},"cell_type":"markdown","source":"for frac in var_frac:\n    alpha_cv_error = []\n    print(\"percentage of variance:\", frac)\n    pca_features = PCA_features(std_training_set, frac = frac)\n    pca_training_set = training_set[pca_features]\n    print(\"ridge cv\")\n    # ridge cv \n    for alp in alpha:\n        LR = Ridge(alpha = alp)\n        alpha_cv_error.append(cv_error(LR, dataset = pca_training_set, datalabel = training_label, cv = 5))\n    #ridge  \n    best_frac_alpha = alpha[alpha_cv_error.index(min(alpha_cv_error))]\n    alpha_error.append(min(alpha_cv_error))\n    best_alpha.append(best_frac_alpha) \n    del pca_training_set\n    pca_training_set = std_training_set[pca_features]\n    valset = validation_set[pca_features].copy(deep = True)\n    for col in pca_training_set.columns:\n        valset[col] = (valset[col] - mean_std[col][0]) / mean_std[col][1]\n    print(\"ridge train\")   \n    # ridge validation\n    LR = Ridge(alpha = best_frac_alpha)\n    LR.fit(pca_training_set, training_label)\n    pred_train = LR.predict(pca_training_set)\n    ridge_trainingerror.append(LR_error(pred_train, training_label))\n    pred_val = LR.predict(valset)\n    ridge_valerror.append(LR_error(pred_val, val_label))\n    del pred_train\n    del pred_val\n    del pca_features\n    del valset\n    del pca_training_set"},{"metadata":{"trusted":true,"_uuid":"1984bc22423681342852158037afad60119cbc39"},"cell_type":"markdown","source":"ridgelist = [\"percentage of variance\", \"best alpha\", \"ridge cross validation error\", \"ridge training error\", \"ridge validation error\"]\nerror_ridge = pd.DataFrame([var_frac, best_alpha, alpha_error, ridge_trainingerror, ridge_valerror], index = ridgelist).T\nerror_ridge.to_csv(\"error_ridge.csv\",index=False,sep=',')"},{"metadata":{"trusted":true,"_uuid":"3894b0d2156e268305fabdfbf21c08e06e794e4e"},"cell_type":"markdown","source":"error_ridge"},{"metadata":{"trusted":true,"_uuid":"2f247199c1e4857921cb3ba796945d63fe0bce75"},"cell_type":"markdown","source":"index = ridge_valerror.index(min(ridge_valerror))\nvar_min = var_frac[index]\nalpha_min = best_alpha[index]\nLR = Ridge(alpha = alpha_min)\npca_features = PCA_features(std_training_set, frac = var_min)\npca_training_set = std_training_set[pca_features]\ntestset = test_set[pca_features].copy(deep = True)\nfor col in testset.columns:\n    testset[col] = (testset[col] - mean_std[col][0]) / mean_std[col][1]\nLR.fit(pca_training_set, training_label)\npred_test = LR.predict(testset)\nridge_test = pd.DataFrame({\"Id\": test_id, \"winPlacePerc\": pred_test})\nridge_test.to_csv(\"ridge_test.csv\",index=False,sep=',')\ndel testset\ndel pred_test\ndel ridge_test"},{"metadata":{"_uuid":"f8e690235c10a207aa612db26beda1265d681dde","trusted":true},"cell_type":"markdown","source":"tree_number = [i*10 for i in range(5, 11)]\ndel training_set\ngc.collect()"},{"metadata":{"trusted":true,"_uuid":"ef96a017e2280fb6e83c7220ea20c820569fe37d"},"cell_type":"markdown","source":"# tree number cv\ntree_trainingerror = []\ntree_valerror = []\n#valset = validation_set.copy(deep = True)\n#testset = test_set.copy(deep = True)\nfor col in std_training_set.columns:\n    #valset[col] = (valset[col] - mean_std[col][0]) / mean_std[col][1]\n    #testset[col] = (testset[col] - mean_std[col][0]) / mean_std[col][1]\n    validation_set[col] = (validation_set[col] - mean_std[col][0]) / mean_std[col][1]\n    test_set[col] = (test_set[col] - mean_std[col][0]) / mean_std[col][1]\nfor num in tree_number:\n    print(num)\n    rfr = RandomForestRegressor(n_estimators = num, n_jobs = -1)\n    rfr.fit(std_training_set, training_label)\n    pred_train = rfr.predict(std_training_set)\n    #pred_val = rfr.predict(valset)\n    #pred_test = rfr.predict(testset)\n    pred_val = rfr.predict(validation_set)\n    tree_trainingerror.append(LR_error(pred_train, training_label))\n    tree_valerror.append(LR_error(pred_val, val_label))\n    del pred_train\n    del pred_val\n    del rfr\n    gc.collect()\n#del valset\n#del testset\n#gc.collect()\n    "},{"metadata":{"trusted":true,"_uuid":"233b0ff80eb887c70e0360882a582dcf7c2f01b7"},"cell_type":"markdown","source":"best_tree = tree_number[tree_valerror.index(min(tree_valerror))]\n#treename = [\"best tree number\", \"training error\", \"validation error\"]\ntreename = [\"tree number\",\"training error\", \"validation error\"]\nerror_tree = pd.DataFrame([tree_number, tree_trainingerror, tree_valerror], index = treename).T\nerror_tree.to_csv(\"error_tree.csv\",index=False,sep=',')\n#tree_test = pd.DataFrame({\"Id\": test_id, \"winPlacePerc\": pred_test})\n#tree_test.to_csv(\"tree_test.csv\",index=False,sep=',')"},{"metadata":{"trusted":true,"_uuid":"aab4c7d90a69273dbc9d789bc3ae2bb27568967a"},"cell_type":"markdown","source":"rfr = RandomForestRegressor(n_estimators = best_tree)\nrfr.fit(std_training_set, training_label)\npred_test = rfr.predict(test_set)\ntree_test = pd.DataFrame({\"Id\": test_id, \"winPlacePerc\": pred_test})\ntree_test.to_csv(\"tree_test.csv\",index=False,sep=',')"},{"metadata":{"trusted":true,"_uuid":"bc32787ac59550056dcf584c512a0cff11f5dcb7"},"cell_type":"markdown","source":"error_tree"},{"metadata":{"trusted":true,"_uuid":"7df1009b5dbafa5026fbb7ccecf6834b444438ab"},"cell_type":"code","source":"frac_pca_error = []\nfrac_pca = []\nfrac_pca_training_error =[]\nfrac_pca_validation_error = []\nfrac_tree_training_error = []\nfrac_tree_validation_error = []\nvar_frac = [i / 100 for i in range(90,100)] +[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80a9bb31add5ba750a9e426cb75690459e6b3792"},"cell_type":"code","source":"# randomly delete percentage of data to see how missing data will influence the result\npercentage = [i/10 for i in range(1,10)] + [0.99, 0.999]\na = 1\nfor frac in percentage:\n    print(\"percentage of dropping training data:\", frac)\n    print(\"pca cross-validation\")\n    frac_training_set, frac_testset, frac_training_label, frac_testlabel = train_test_split(training_set, training_label, train_size = 1-frac, test_size = frac)\n    del frac_testset\n    del frac_testlabel\n    gc.collect()\n    frac_training_set = frac_training_set.reset_index(drop = True)\n    frac_training_label = frac_training_label.reset_index(drop = True)\n    #ridge regression pca cv\n    frac_std_training_set, frac_mean_std = standardize(frac_training_set)\n    if a == 1:\n        pca_cv_error = []\n        LR = Ridge(alpha = 10)\n        for fra in var_frac:\n            pca_features = PCA_features(frac_std_training_set, frac = fra)\n            pca_training_set = frac_training_set[pca_features]\n            pca_cv_error.append(cv_error(LR, dataset = pca_training_set, datalabel = frac_training_label, cv = 5))\n        print(\"minimum pca cv error:\", min(pca_cv_error))\n        frac_pca_error.append(min(pca_cv_error))\n        best_frac = var_frac[pca_cv_error.index(min(pca_cv_error))]\n        print(\"best fraction of variance:\", best_frac)\n        frac_pca.append(best_frac)\n\n        pca_features = PCA_features(frac_std_training_set, frac = best_frac)\n        pca_training_set = frac_std_training_set[pca_features]\n        valset1 = validation_set[pca_features].copy(deep = True)\n        for col in valset1.columns:\n            valset1[col] = (valset1[col] - frac_mean_std[col][0]) / frac_mean_std[col][1]\n        LR.fit(pca_training_set, frac_training_label)\n        frac_trainlabel = LR.predict(pca_training_set)\n        #frac_trainerror = 1- LR.score(pca_training_set, frac_training_label)\n        frac_pca_training_error.append(frac_trainerror)\n        frac_vallabel = LR.predict(valset1)\n        frac_trainerror = LR_error(frac_trainlabel, frac_training_label)\n        frac_valerror = LR_error(frac_vallabel, val_label)\n        #frac_valerror = 1 - LR.score(valset1, val_label)\n        print(\"pca validation error\", frac_valerror)\n        frac_pca_validation_error.append(frac_valerror)\n        del valset1\n        del pca_training_set\n        del frac_trainlabel\n        del frac_vallabel\n        gc.collect()\n    \n    # tree regression\n    valset2 = validation_set.copy(deep = True)\n    for col in frac_std_training_set.columns:\n        valset2[col] = (valset2[col] - frac_mean_std[col][0]) / frac_mean_std[col][1]\n    rfr = RandomForestRegressor(n_estimators = 100, n_jobs = -1)\n    rfr.fit(frac_std_training_set, frac_training_label)\n    pred_train = rfr.predict(frac_std_training_set)\n    pred_val = rfr.predict(valset2)\n    frac_tree_training_error.append(LR_error(pred_train, frac_training_label))\n    frac_tree_validation_error.append(LR_error(pred_val, val_label))\n    del valset2\n    del frac_training_set\n    del frac_training_label\n    del pred_train\n    del pred_val\n    del rfr\n    gc.collect()\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"374cdd9d96bb55dc535f206465588a978d6f7a85"},"cell_type":"code","source":"listname = [\"dropping percentage of training data\", \"pca variance fraction\", \n            \"pca cross validation error\", \"pca training error\", \"pca validation error\", \"RFR training error\", \"RFR validation error\"]\nerrorlist = pd.DataFrame([percentage, frac_pca, frac_pca_error, frac_pca_training_error, \n              frac_pca_validation_error, frac_tree_training_error, frac_tree_validation_error],index = listname).T\nerrorlist.to_csv(\"errorlist.csv\",index=False,sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78351e805e06e10c47566fdf136e54d2d6ec0b41"},"cell_type":"code","source":"errorlist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfbe4531b54a4c527eba15c31bde1613b550909b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}