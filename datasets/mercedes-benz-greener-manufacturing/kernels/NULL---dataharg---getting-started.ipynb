{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f88a0a0-b4ac-9b4f-45d9-a203066ac51a"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "train['Src'] = \"train\"\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "test['Src'] = \"test\"\n",
        "comb = pd.concat([train,test],0)\n",
        "\n",
        "print (\"Stage0 shape: {0}\".format(comb.shape))\n",
        "#STEP1: Dummy set encoding of the categorical fields\n",
        "catfields = []\n",
        "frames = [comb]\n",
        "\n",
        "for c in comb.drop(['ID','y','Src'],1).columns:\n",
        "    if train[c].dtype == 'object':\n",
        "        catfields.append(c)\n",
        "        tempdf = pd.get_dummies(comb[c],prefix=c)\n",
        "        frames.append(tempdf)\n",
        "        \n",
        "comb2 = pd.concat(frames,axis=1).drop(catfields, 1)\n",
        "print (\"Stage1 Shape: {0}\".format(comb2.shape))\n",
        "\n",
        "#STEP2: Remove fields with little or no information\n",
        "#TFPE: Too few posititve entries (proportion)\n",
        "tfpe = 0.05\n",
        "\n",
        "problem_fields = []\n",
        "for c in comb2.drop(['ID','y','Src'],1).columns:\n",
        "    uniq = len(np.unique(comb2[c]))\n",
        "    mv = comb2[c].mean()\n",
        "    if ((uniq == 1) or (mv < tfpe)): problem_fields.append(c)\n",
        "\n",
        "comb3 = comb2.drop(problem_fields, 1)\n",
        "print (\"Stage3 Shape: {0}\".format(comb3.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8b907adb-a41a-bdef-78c3-4c03db4a1e11"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca2 = PCA(n_components=35)\n",
        "pca2_results = pca2.fit_transform(comb3.drop(['ID','y','Src'],1))\n",
        "\n",
        "\n",
        "eigvals = pca2.explained_variance_\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(8,5))\n",
        "sing_vals = np.arange(len(eigvals)) + 1\n",
        "plt.plot(sing_vals, eigvals, 'ro-', linewidth=2)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Eigenvalue')\n",
        "leg = plt.legend(['Eigenvalues from PCA'], loc='best', borderpad=0.3, \n",
        " shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'),\n",
        " markerscale=0.4)\n",
        "leg.get_frame().set_alpha(0.4)\n",
        "leg.draggable(state=True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "46451bcd-99aa-71e9-df10-734313e35527"
      },
      "source": [
        "To eye, around 16 components appears to be about right for this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "95b2ae9c-cca2-65a4-ef14-70708f13c60b"
      },
      "outputs": [],
      "source": [
        "#Stage 4: Apply the PCA results\n",
        "pcacols = ['ID','y','Src']\n",
        "for i in range(16):\n",
        "    cn = 'pca' + str(i)\n",
        "    comb3[cn]=pca2_results[:,i]\n",
        "    pcacols.append(cn)\n",
        "\n",
        "comb4 = comb3[pcacols]\n",
        "print (\"Stage4 Shape: {0}\".format(comb4.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ef5bf3b7-1391-6337-4ad1-1b2880b40521"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Proposed Steps for the regression model\n",
        "Box Cox model to assess if transformatins required\n",
        "Build initial model\n",
        "Calculate the pseudo cooks distance and look for over influential cases\n",
        "VIF, Although we have orthogonal PCA so should not be a problem\n",
        "test regression assumptions:\n",
        "    Linear relationship (look at box cox results).\n",
        "    Multivariate normality.\n",
        "    No or little multicollinearity.\n",
        "    No auto-correlation.\n",
        "    Homoscedasticity.\n",
        "Build final model\n",
        "\"\"\"\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(comb4[pcacols[-16:]]) + 0.001\n",
        "scaleddf = pd.DataFrame(scaled)                              \n",
        "\n",
        "bclst = ['ID','y','Src']\n",
        "lmbdalst = []\n",
        "for p in range(16):\n",
        "    X , ll = stats.boxcox(scaleddf[p])\n",
        "    lmbdalst.append(ll)\n",
        "    a = 'pca_bc' + str(p)\n",
        "    bclst.append(a)\n",
        "    comb4[a] = X \n",
        "print (lmbdalst)\n",
        "comb5 = comb4[bclst]\n",
        "\n",
        "print (\"Stage5 Shape: {0}\".format(comb5.shape))\n",
        "print (comb5.columns)\n",
        "\n",
        "x1=comb5['pca_bc14'].head(250)\n",
        "x2=comb4['pca14'].head(250)\n",
        "y=comb5['y'].head(250)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.scatter(x1, y)\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.scatter(x2, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "111f94c2-de9d-d0a9-13d7-2f6a6f17676f"
      },
      "source": [
        "Lambdas are not very large, so box-cox is not really doing much in this case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bbb92a7d-1417-0271-19e6-4e8d94de9597"
      },
      "outputs": [],
      "source": [
        "#Split back out the training and test set\n",
        "train = comb5[comb4['Src'] == 'train'].drop('Src',1)\n",
        "test = comb5[comb4['Src'] == 'test'].drop('Src',1)\n",
        "\n",
        "print (\"Stage5a; Train Shape: {0}, Test Shape: {1}\".format(train.shape,test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "633b78ec-1e72-9f08-a277-8981369b35ac"
      },
      "source": [
        "Note: We will not be splitting the training data up further into training and validation sets as we are aiming to build a statistical model so, our validation of the model will be through testing the assumptions underpinning a linear regression model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0a51a224-db98-f897-96ea-949ebad1cb8e"
      },
      "outputs": [],
      "source": [
        "#Evaluate an initial model\n",
        "\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "\n",
        "#Generate the model string\n",
        "st = \"y ~ \"\n",
        "for f in bclst[-16:]:  st = st + f + \" + \"\n",
        "    \n",
        "#Fit the model\n",
        "model = ols(st[:-3], train).fit()\n",
        "\n",
        "#Print the summary\n",
        "print(model.summary())\n",
        "\n",
        "print(\"\\nparameter estimates:\")\n",
        "print(model._results.params)\n",
        "#analysis of variance on fitted linear model\n",
        "anova_results = anova_lm(model)\n",
        "print('\\nANOVA results')\n",
        "print(anova_results)\n",
        "#analysis of outliers\n",
        "score = model.outlier_test()\n",
        "outliers = (i for i,t in enumerate(score[\"bonf(p)\"]) if t < 0.6 )\n",
        "outlierslist = (list(outliers))\n",
        "print ('\\nOutliers: ', len(outlierslist))\n",
        "print (outlierslist)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f3862e6e-2372-71d4-2445-0965bfc42823"
      },
      "source": [
        "Durbin-Watson Shows no sign of auto-correlation in the model\n",
        "\n",
        "Jarque-Bera Test shows we have no sign that the data is not multivariate normal\n",
        "\n",
        "Condition number is borderline, might want to consider fewer components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f727bc93-55fc-fdc5-9a6e-43d892cca692"
      },
      "outputs": [],
      "source": [
        "#And some regression plots to help us evaluate the model\n",
        "%matplotlib inline\n",
        "import statsmodels.api as sm\n",
        "#Influence Plot\n",
        "#--------------\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "fig = sm.graphics.influence_plot(model, ax=ax, criterion=\"cooks\")\n",
        "\n",
        "#This confirms what we saw in above bonf(p) list >> we are happy with out outlier list\n",
        "\n",
        "#Homoscedasticity\n",
        "#-------------\n",
        "yhat = model.fittedvalues\n",
        "ehat = model.resid\n",
        "fig, ax1 = plt.subplots(figsize=(12,8))\n",
        "ax1.scatter(yhat, ehat)\n",
        "\n",
        "#Shows some evidence of homoscedasticity, but not severe enough to worry about\n",
        "#More evidence of the outlier;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1eabf8f-8737-80f4-1828-80943326abed"
      },
      "outputs": [],
      "source": [
        "#Build a Second model.\n",
        "#Not much work required, outliers mainly\n",
        "\n",
        "train2 = train.drop(train.index[outlierslist])\n",
        "\n",
        "#Fit the model\n",
        "model2 = ols(st[:-3], train2).fit()\n",
        "\n",
        "#Print the summary\n",
        "print(model2.summary())\n",
        "\n",
        "print(\"\\nparameter estimates:\")\n",
        "print(model2._results.params)\n",
        "#analysis of variance on fitted linear model\n",
        "anova_results = anova_lm(model2)\n",
        "print('\\nANOVA results')\n",
        "print(anova_results)\n",
        "#analysis of outliers\n",
        "score = model2.outlier_test()\n",
        "outliers = (i for i,t in enumerate(score[\"bonf(p)\"]) if t < 0.05 )\n",
        "outlierslist2 = (list(outliers))\n",
        "print ('\\nOutliers: ', len(outlierslist2))\n",
        "print (outlierslist2)\n",
        "yhat = model2.fittedvalues\n",
        "ehat = model2.resid\n",
        "fig, ax1 = plt.subplots(figsize=(12,8))\n",
        "ax1.scatter(yhat, ehat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5ec17542-e498-ca99-c911-a9f4a629cabf"
      },
      "outputs": [],
      "source": [
        "#Final Build\n",
        "\n",
        "train3 = train2.drop(train.index[outlierslist2])\n",
        "\n",
        "#Fit the model\n",
        "model3 = ols(st[:-3], train3).fit()\n",
        "\n",
        "#Print the summary\n",
        "print(model3.summary())\n",
        "\n",
        "print(\"\\nparameter estimates:\")\n",
        "print(model3._results.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79190bed-5de7-1339-ac86-fc72ac4aa0f9"
      },
      "outputs": [],
      "source": [
        "#Score up the test set \n",
        "\n",
        "y_pred = model3.predict(test)\n",
        "output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\n",
        "output.to_csv('RH_SUB1_Baseline.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b086963-a6fb-dcf4-1cf9-16ea7265f3d7"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}