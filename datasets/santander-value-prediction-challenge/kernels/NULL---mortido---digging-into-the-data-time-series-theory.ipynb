{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2fa881f786fb7fb42b493dcd02fa905542b72a1e"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import KFold, cross_val_predict\n\nimport lightgbm as lgb","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edfd10da96f6f42877d35aa015991680a1e48840"},"cell_type":"code","source":"# Read train and test files\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"07a8f1f3430b8626c088353e5bb945b5254e084c"},"cell_type":"markdown","source":"# Facts\n- As was mentioned in other EDA kernels, current dataset have different distribution in train and test sets.\n- We have more features than records in train set.\n- Some columns are even constant in train set."},{"metadata":{"trusted":true,"_uuid":"36c8c38524c792575e40378aa68ec5825fd65fff"},"cell_type":"code","source":"(train_df.iloc[:, 2:].nunique() == test_df.iloc[:, 1:].nunique()).any()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"932411acf74f12c9a8b8f29e19c4a7face66f6e4"},"cell_type":"markdown","source":"- As number of unique values in features are different, it is possible that there are no any binary or categorical features (except the case we have some fake data in test set)"},{"metadata":{"_uuid":"53e1a0623a882b63f6e8524a6619996aad7ab132"},"cell_type":"markdown","source":"# Assumption\n\nLets just pretend that we are working with time series data (e.g. transaction history per day) and try to dig into it from that perspective."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"09b726ed3e708b5db069e899757af8e94abb0e11"},"cell_type":"code","source":"X_train_orig = train_df.drop([\"ID\", \"target\"], axis=1)\nX_test_orig = test_df.drop([\"ID\"], axis=1)\n\n# Apply log transform to target variable\ny_train = np.log1p(train_df[\"target\"].values)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"3c26eb44f5b34f49f0dac9a53eb3f25472a64d94"},"cell_type":"markdown","source":"# Investigation\nChecking lgbm performance over initial data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"305d2a407d554f0d165daf8891edfbc1e3795306"},"cell_type":"code","source":"FOLDS = 10\nSEED = 2707\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\nmodel = lgb.LGBMRegressor(objective='regression',\n                        num_leaves=31,\n                        learning_rate=0.05,\n                        n_estimators=500)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e1e53a67cd0704ce97b0f650addd4479b9063d0"},"cell_type":"code","source":"# For the sake of speed just print the result\n# predict = cross_val_predict(model, X_train_orig, y_train, cv=kf)\n# print(np.sqrt(np.mean((predict-y_train) ** 2)))\n\nprint(1.4794830145766735)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"9d50b6ec1d25eef3c267e915888232b1263b7c40"},"cell_type":"markdown","source":"Then let's create just 6 simple features which are calculated for every row ignoring zeros"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"90fbba9989920e9199840ec2b4d81c74e54be182"},"cell_type":"code","source":"def prepare(data_orig):\n    data = pd.DataFrame()\n    data['mean'] = data_orig.mean(axis=1)\n    data['std'] = data_orig.std(axis=1)\n    data['min'] = data_orig.min(axis=1)\n    data['max'] = data_orig.max(axis=1)\n    data['number_of_different'] = data_orig.nunique(axis=1)               # Number of diferent values in a row.\n    data['non_zero_count'] = data_orig.fillna(0).astype(bool).sum(axis=1) # Number of non zero values (e.g. transaction count)\n    return data\n\n# Replace 0 with NaN to ignore them.\nX_test = prepare(X_test_orig.replace(0, np.nan))\nX_train = prepare(X_train_orig.replace(0, np.nan))","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"4c7438a6d4736306abc5b695335ce34a788e0956"},"cell_type":"markdown","source":"And  immediately check the perfomance of lgbm"},{"metadata":{"trusted":true,"_uuid":"c2a32704d4eab68018c9ca051c68608ee00603b3"},"cell_type":"code","source":"predict = cross_val_predict(model, X_train, y_train, cv=kf)\nprint(np.sqrt(np.mean((predict-y_train) ** 2)))","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"896a8ec892e9ba942ee0a98bd54e49523b328d74"},"cell_type":"markdown","source":"### 1.385 on 10 folds... no so bad for just 6 features!\nlet's investigate them a little bit more"},{"metadata":{"trusted":true,"_uuid":"523b6688fb68867070593db28d8ff838f5c21e42"},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\n# data = X_train.copy()\n# data['target'] = train_df['target']\n\n_ = scatter_matrix(X_train, alpha=0.2, diagonal='kde', figsize=(13, 13))\n_ = scatter_matrix(X_test, alpha=0.2, diagonal='kde', figsize=(13, 13))","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"e5c8c6eb44f0f1c58da19f03815a716fa28f62f8"},"cell_type":"markdown","source":"### There are 2 features that looks interesting, lets investigate them closer"},{"metadata":{"trusted":true,"_uuid":"5d899bfbdf0d17407d747584f31721e2961c4565"},"cell_type":"code","source":"sns.jointplot(x='non_zero_count', y='number_of_different', data=X_train)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"4e897b85bcc20bc51bb03046abc3f98b6488a9c1"},"cell_type":"markdown","source":"### Data in the train set has a very strange 'arc'\nDoes more 'transactions' = less diversity in 'value' ? What do you think?"},{"metadata":{"trusted":true,"_uuid":"f580e14df6ff157bde8c747da4606a25e78777f6"},"cell_type":"code","source":"sns.jointplot(x='non_zero_count', y='number_of_different', data=X_test)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"9e967dd36cbf46b33d5c5d501609fbcdc870af3c"},"cell_type":"markdown","source":"### Test set on the other hand has straight line. It means that all values in a row are different.\nIt may be a sign that most of these values were artificially generated."},{"metadata":{"trusted":true,"_uuid":"8ff9ab8a81cd1983bf9a4b3efb4fbce0685deff5"},"cell_type":"code","source":"print('in train set:', (X_train['number_of_different'] == X_train['non_zero_count']).sum(), 'out of', X_train.shape[0])\nprint('in test set', (X_test['number_of_different']==X_test['non_zero_count']).sum(), 'out of', X_test.shape[0])","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"6496653798bdb018fa2c60e75080bf0cab03e002"},"cell_type":"markdown","source":"### 27657 of possible fake rows in test set\nLets check plot without them"},{"metadata":{"trusted":true,"_uuid":"08f276e38d21c158753b304540ae69f4647a6e97"},"cell_type":"code","source":"sns.jointplot(x='non_zero_count', y='number_of_different', data=X_test.loc[X_test['number_of_different']!=X_test['non_zero_count']])","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"1edcb228a3fa1aae6c30b137d3f65c3d71bb49d9"},"cell_type":"markdown","source":"### Still has difference with train set, but looks much more... natural...\n\n# Feature importance"},{"metadata":{"trusted":true,"_uuid":"1b10218a5dca01423d4c0e4ccfedb58092f29973"},"cell_type":"code","source":"model.fit(X_train, y_train)\ngain = model.booster_.feature_importance(importance_type='gain')\ngain = 100.0 * gain / gain.sum()\npd.DataFrame(gain, index=model.booster_.feature_name(), columns=['gain']).sort_values('gain', ascending=False)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"456a3b4a396473e99811d09b2a012b67eac9e943"},"cell_type":"markdown","source":"### It also a good idea to take log from features for linear models"},{"metadata":{"trusted":true,"_uuid":"092a1649f6f968809e4c793e9a4911ad19e57965"},"cell_type":"code","source":"data = X_train.copy()\ndata['target'] = train_df['target']\n\n_ = scatter_matrix(np.log1p(data), alpha=0.2, diagonal='kde', figsize=(13, 13))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8ffa89c0438f3673b056f65e207364e926ed939"},"cell_type":"code","source":"(data['target'] < data['min']).sum()","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e308de61b2d0af8237e6455ecc426bfe1c829a5e"},"cell_type":"code","source":"(data['target'] > data['max']).sum()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"c418fa64f69d948f3dd44e48ca683373ab20dd06"},"cell_type":"markdown","source":"### Target value almost always lays between min and max of other values in the row (should we try LSTM here?)"},{"metadata":{"trusted":true,"_uuid":"5c75b78d3a512568e0f29f6a725e7d641aa8ff61"},"cell_type":"code","source":"_ = scatter_matrix(np.log1p(X_test), alpha=0.2, diagonal='kde', figsize=(13, 13))","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"a0e13139d9e663edd0479601b523f4cca4012d69"},"cell_type":"markdown","source":"### That is all for now. Have fun making more conspiracy theories :)"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b29e905bc3732af55c4b1594650fc308c3231f09"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"480c18125d7f65197122e43f7f049056b79a495a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c945c78fe744ca27d547ddce8db88e85ec729205"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5c6d7f827b31b835979d9abdcf1776fede1f697a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}