{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "version": "3.6.1", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}}}, "cells": [{"metadata": {"_cell_guid": "0c87cab5-4879-4fee-a48d-afdce13edf90", "_uuid": "e1efb3fac70706ff317444ebdc7d7c4e98b23a0d"}, "source": ["# Safe Driver Prediction Explotory Data Analysis\n", "\n", "`Kueip- Sept 2017`\n", "\n", "---\n", "## Outline:\n", "-   ** Intoduction** ([completed]())\n", "    - Packages Loading\n", "    - Check Memory Usage\n", "-  ** Multii-Variables Analysis**  ([non-complete]())\n", "-  ** Bi-Variables Analysis**  ([non-complete]())\n", "        - Feature Values Distribution\n", "-  ** Target Value Analysis** ([completed]())\n", "-  ** Missing Values Analysis** ([completed]())\n", "    - Matrix\n", "    - HeatMap\n", "    - Bar\n", "\n", "-  ** Feature Important** ([non-complete]())\n", "        - Decision Tree\n", "        - RandomForest\n", "        - XGB\n", "        - LGB"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "6f1fd4f8-dc7b-4951-9261-bd1ca8000a4b", "_uuid": "879e34e35f14a76b8691d6b541a43dfcc2282e43"}, "source": [], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "340fc40a-da02-4a21-9467-32e92365d6aa", "_uuid": "05046c6cca457d3f88d223cd72f33218b765dd9d"}, "source": ["# Introduction\n", "Porto Seguro, one of Brazil\u2019s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company\u2019s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "5c26f9b0-8e85-4610-9873-cf06e6b8e2e7", "_uuid": "ebc6e09fabb5c68e806b4b1457eb2ef20a6d321a"}, "source": ["### Packages Loading"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "b84eac91-4c52-430c-8dd5-c3ef4326a86c", "_uuid": "fe991bf2237c7c9058e2293e707eb9c9a8c8155b", "_kg_hide-input": true}, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "import matplotlib\n", "%matplotlib inline\n", "import seaborn as sns # visualization\n", "from subprocess import check_output\n", "import missingno as msno\n", "\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier \n", "\n", "import xgboost as xgb # Gradeint Boosting\n", "from xgboost import XGBClassifier # Gradeint Boosting\n", "import lightgbm as lgb # Gradeint Boosting\n", "import gc\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "# Any results you write to the current directory are saved as output."], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "7814780a-1d28-4648-80f3-18f5b1ee9611", "_uuid": "97af5403e5e75c5b8735a72fc744e7c7af3385d0", "_kg_hide-input": true}, "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "print(\"Train shape : \", train.shape)\n", "print(\"Test shape : \", test.shape)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "c5183e70-469b-4148-b353-52ee0590c3b0", "_uuid": "3ecb0ab00f831f00eb959e80de479355ae3f60ac"}, "source": ["- No. of rows are large with 58 columns. \n", "\n", "From VC dimension theory, we dont worry about overfitting too much, if we could cover the function set, choose the proper number of features.\n"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "78b66aec-7eab-48aa-8d18-ccfdd259cb3d", "_uuid": "2b0d517f560f9dee276ce51790948ebb7ae84bd6", "_kg_hide-input": true}, "source": ["train.head()"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "fcec6130-8453-48cb-85b2-f836961b7530", "_uuid": "ac8084920f5ff6b13b4f273f3558e3208754f2f5"}, "source": ["### Check Memory Usage"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "66cfb59b-f8c5-43c3-b1be-6017c9cb57ed", "_uuid": "8adebbf6d6d663c98a4454dae53f26827a3bc8ed", "_kg_hide-input": true}, "source": ["train.info(verbose=False),test.info(verbose=False)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "f90f61ce-ec97-4659-9307-02ec60fbc4ad", "_uuid": "6828288dc95e45ec87b88cbb5d3045ea3bdd2946"}, "source": ["### Convert Type"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "e1a64d00-c742-43aa-a15e-19bc4b819bdc", "_uuid": "b3772cc9cc75e6cf164a31020973ee6e8f6cffb1", "_kg_hide-input": true}, "source": ["for c, dtype in zip(train.columns, train.dtypes):\n", "    if dtype == np.float64:\n", "        train[c] = train[c].astype(np.float32) \n", "    elif dtype == np.int64:\n", "        train[c] = train[c].astype(np.int32) \n", "gc.collect()\n", "for c, dtype in zip(test.columns, test.dtypes):\n", "    if dtype == np.float64:\n", "        test[c] = test[c].astype(np.float32) \n", "    elif dtype == np.int64:\n", "        test[c] = test[c].astype(np.int32) "], "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["## Multi-Variable Analysis"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_kg_hide-input": true}, "source": ["from collections import Counter\n", "count = Counter()\n", "unique_values_dict = {}\n", "for col in train.columns:\n", "    unique_values_dict[col] = np.sort(train[col].unique())\n", "    count[col] = len(np.sort(train[col].unique()))   "], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_kg_hide-input": true}, "source": ["cat_cols = [ col for col , val in count.items() if(val==10)]\n", "plt.figure(figsize=(20,70))\n", "for i in range(len(cat_cols)):\n", "    c = cat_cols[i]\n", "    \n", "    means = train.groupby(c).target.mean()\n", "    stds = train.groupby(c).target.std()#.fillna(0)\n", "    means_astds = train.groupby(c).target.mean() + train.groupby(c).target.std()\n", "    means_sstds = train.groupby(c).target.mean() - train.groupby(c).target.std()\n", "    \n", "    ddd = pd.concat([means, stds, means_astds, means_sstds], axis=1); \n", "    ddd.columns = ['means', 'stds', 'means + stds', 'means - stds']\n", "    ddd.sort_values('means', inplace=True)\n", "    \n", "    plt.subplot(len(cat_cols), 2, 2*i+1)\n", "    ax = sns.countplot(train[c], order=ddd.index.values)\n", "    plt.xticks(rotation=90)\n", "    for p in ax.patches:\n", "        x=p.get_bbox().get_points()[:,0]\n", "        y=p.get_bbox().get_points()[1,1]\n", "        ax.annotate('{:.0f}'.format(y), (x.mean(), y), ha='center', va='bottom')\n", "    \n", "    plt.subplot(len(cat_cols ), 2, 2*i+2)\n", "    plt.fill_between(range(len(train[c].unique())), \n", "                     ddd.means.values - ddd.stds.values,\n", "                     ddd.means.values + ddd.stds.values,\n", "                     alpha=0.3\n", "                    )\n", "    plt.xticks(range(len(train[c].unique())), ddd.index.values, rotation=90,fontsize=18)\n", "    plt.plot(ddd.means.values, color='b', marker='.', linestyle='dashed', linewidth=0.7)\n", "    plt.plot(ddd['means + stds'].values, color='g', linestyle='dashed', linewidth=0.7)\n", "    plt.plot(ddd['means - stds'].values, color='r', linestyle='dashed', linewidth=0.7)\n", "    plt.xlabel(c + ': Means, STDs and +- STDs',fontsize=18)\n", "    #plt.ylim(80, 270)\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_kg_hide-input": true}, "source": ["import plotly.offline as py\n", "py.init_notebook_mode(connected=True)\n", "import plotly.graph_objs as go\n", "\n", "a=[column for column in train]\n", "trace = go.Heatmap(z=train.corr().values,\n", "                   x=a,\n", "                   y=a)\n", "data=[trace]\n", "py.iplot(data, filename='backorders heatmap')"], "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["## Binary Variables:\n"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_kg_hide-input": true}, "source": ["cat_cols = [ col for col , val in count.items() if(val==2)]\n", "plt.figure(figsize=(25,100))\n", "for i in range(len(cat_cols)):\n", "    c = cat_cols[i]\n", "    \n", "    means = train.groupby(c).target.mean()\n", "    stds = train.groupby(c).target.std()#.fillna(0)\n", "    means_astds = train.groupby(c).target.mean() + train.groupby(c).target.std()\n", "    means_sstds = train.groupby(c).target.mean() - train.groupby(c).target.std()\n", "    \n", "    ddd = pd.concat([means, stds, means_astds, means_sstds], axis=1); \n", "    ddd.columns = ['means', 'stds', 'means + stds', 'means - stds']\n", "    ddd.sort_values('means', inplace=True)\n", "    \n", "    plt.subplot(len(cat_cols), 2, 2*i+1)\n", "    ax = sns.countplot(train[c], order=ddd.index.values)\n", "    plt.xticks(rotation=90)\n", "    for p in ax.patches:\n", "        x=p.get_bbox().get_points()[:,0]\n", "        y=p.get_bbox().get_points()[1,1]\n", "        ax.annotate('{:.0f}'.format(y), (x.mean(), y), ha='center', va='bottom')\n", "    \n", "    plt.subplot(len(cat_cols ), 2, 2*i+2)\n", "    plt.fill_between(range(len(train[c].unique())), \n", "                     ddd.means.values - ddd.stds.values,\n", "                     ddd.means.values + ddd.stds.values,\n", "                     alpha=0.3\n", "                    )\n", "    plt.xticks(range(len(train[c].unique())), ddd.index.values, rotation=90,fontsize=18)\n", "    plt.yticks(fontsize=18)\n", "    plt.plot(ddd.means.values, color='b', marker='.', linestyle='dashed', linewidth=0.7)\n", "    plt.plot(ddd['means + stds'].values, color='g', linestyle='dashed', linewidth=0.7)\n", "    plt.plot(ddd['means - stds'].values, color='r', linestyle='dashed', linewidth=0.7)\n", "    plt.xlabel(c + ': Means, STDs and +- STDs',fontsize=16)\n", "    #plt.ylim(80, 270)\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["- **ps_ind_10_bin**\n", "- **ps_ind_11_bin**\n", "- **ps_ind_12_bin**\n", "- **ps_ind_13_bin** \n", "\n", "have obvious imbalanced distribution."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "0930e350-acdc-473e-8e26-80d42da52f18", "_uuid": "d15f3cce659c8c9fb260c628d615d98a71770ce8"}, "source": ["## Target Variable Analysis"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "21aad605-ea42-4fa3-8566-c66273760e9e", "_uuid": "662f07d35023e38312329ee65ad9dcfabd41c211", "_kg_hide-input": true}, "source": ["labels = '1', '0'\n", "sizes = [train[train.target==1].shape[0],train[train.target==0].shape[0]]\n", "colors = ['gold', 'lightskyblue']\n", "explode = (0.1, 0)  # explode 1st slice\n", "# Plot\n", "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n", "        autopct='%1.2f%%', shadow=True, startangle=140)\n", "\n", "plt.axis('equal')\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "59f7cf18-7179-46db-85ec-27b91083a2c2", "_uuid": "7d4e9f6bbf56e3e007864ab60944c1a564bde9b1", "_kg_hide-input": true}, "source": ["- imblalanced data\n", "- Scikit-Learn provide [StratifiedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit) API by preserving the percentage of samples for each class."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "d72a1c09-3839-4d64-bcaa-7112c5fabcbe", "_uuid": "b898d432403276f33a4ded5c2e380e601fa4a089"}, "source": ["### A Review on Imbalanced Learning Methods\n", "\n", "Imbalanced classification is a supervised learning problem where one class outnumbers other class by a large proportion. This problem is faced more frequently in binary classification problems than multi-level classification problems. The reasons which leads to reduction in accuracy of ML algorithms on imbalanced data sets:\n", "    1. ML algorithms struggle with accuracy because of the unequal distribution in dependent variable.\n", "    2. This causes the performance of existing classifiers to get biased towards majority class.\n", "    3. The algorithms are accuracy driven i.e. they aim to minimize the overall error to which the minority class contributes very little.\n", "    4. ML algorithms assume that the data set has balanced class distributions.\n", "    5. They also assume that errors obtained from different classes have same cost"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "8ea75f0d-51ab-42d9-ba57-e165cca78e37", "_uuid": "99df4d771b21b50604e010b5834d21cadd7b4385"}, "source": ["### How to use imbalanced data to cheat your boss ? Let's conduct an experiment!"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "f514df6a-624d-43f1-a711-78f9117e0b06", "_uuid": "e1ad178dcab8f84d356cbc10ed39a3324b2a4ad7", "_kg_hide-input": false}, "source": ["from sklearn.model_selection import StratifiedShuffleSplit\n", "X = train.drop(['id','target'], axis=1).values\n", "y = train.target.values\n", "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n", "for train_index, test_index in sss.split(X, y):\n", "    X_train, X_test = X[train_index], X[test_index]\n", "    y_train, y_test = y[train_index], y[test_index]\n", "    break\n", "    \n", "from sklearn.dummy import DummyClassifier\n", "# Negative class (0) is most frequent\n", "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n", "# Therefore the dummy 'most_frequent' classifier always predicts class 0\n", "dummy_majority.score(X_test, y_test)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "9a1213f5-bbe6-4c03-bd86-7b762555d31c", "_uuid": "4feba81a90595f0950b5c8c796cf9a040d83b0cd"}, "source": ["\n", "Hey Boss, I design a bullshit classifier with accuracy 96.35%.\n", "\n", ".\n", "\n", ".\n", "\n", "\n", "Now, you should know why **Normalized Gini** is the metric in this case, instead of **accuracy**. \n", "(If  we just used a majority class to assign values to all records, we will still be having a high accuracy.)\n", "\n", "One Specific example, if this bullshiter recognize a terrorist isnt a terroist, it will become a disaster."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "37f14b57-b78b-41b6-90d2-7586c0ad6885", "_uuid": "f592b4c99ec9bf5cef0bbd153d1377bd136955aa"}, "source": ["## Missing Values Analysis\n", "- Thanks **Pedro Schoen** for pointing missing are encoded as **-1**\n", "- Let's encode **-1** as `np.nan`"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "2b19bfdc-cd97-47cd-bada-658ec373132d", "_uuid": "a4bc90dd571f97ee3924cde257b591a63d2ae237"}, "source": ["for col in np.intersect1d(train.columns,test.columns):\n", "    train.loc[train[col]==-1,col] = np.nan\n", "    test.loc[test[col]==-1,col] = np.nan"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "68ca9e0c-ffd2-42f0-932d-e0186caec653", "_uuid": "67f2c5e32cd150749fb75dec3568819d602e30b2"}, "source": ["## Train Set Missing Values"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "0c686bbc-16db-4978-8556-ef8aa803a2fc", "_uuid": "7b5178389b7839ff3a28ef4d2a384bb2dfc38485", "_kg_hide-input": true}, "source": ["missing_df = train.isnull().sum(axis=0).reset_index()\n", "missing_df.columns = ['column_name', 'missing_count']\n", "missing_df['ratio'] = round(missing_df['missing_count'] / train.shape[0],4)\n", "missing_df[missing_df['ratio']>0][['column_name', 'missing_count','ratio']].sort_values(by='ratio',ascending=False)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "4fb765a0-d495-4e00-b9c6-2d5fec05fdb3", "_uuid": "f036353f874ab74b9b759933194e1f0cec492626", "_kg_hide-input": true}, "source": ["def missingno_matrix(df):\n", "    missingValueColumns = df.columns[df.isnull().any()].tolist()\n", "    msno.matrix(df[missingValueColumns],width_ratios=(10,1),\\\n", "            figsize=(20,8),color=(0,0, 0),fontsize=12,sparkline=True,labels=True)\n", "    plt.show()\n", "missingno_matrix(train)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "ce1df280-c7c3-4971-a360-e7f956623f19", "_uuid": "6dbbf91dcc265e3303a2f8c593b863a94e7f80da", "_kg_hide-input": true}, "source": ["def missingno_heatmap(df):\n", "    missingValueColumns = df.columns[df.isnull().any()].tolist()\n", "    msno.heatmap(df[missingValueColumns],figsize=(20,20))\n", "    plt.show()\n", "missingno_heatmap(train)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "4fdb88a0-ce6c-4b95-8531-fc46cc7d5484", "_uuid": "34c2d2bb2bd5891ec397f39ca2c7784ad63afa11", "_kg_hide-input": true}, "source": ["def missing_bar(df):\n", "\n", "    missingValueColumns = df.columns[df.isnull().any()].tolist()\n", "    msno.bar(df[missingValueColumns],figsize=(20,8),color=\"#34495e\",fontsize=12,labels=True)\n", "    plt.show()\n", "missing_bar(train)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "f3656b4a-0d08-446f-810d-36f29b933a57", "_uuid": "c2ee2a92e3a468b0c7bdc7714619514e53ac8f00"}, "source": ["## Test Set Missing Values"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "46b79a5f-9368-4302-a1be-6c4cb9dde4b0", "_uuid": "b0eb83cda124947b8d8cc62743efd14d3f4200ea", "_kg_hide-input": true}, "source": ["missing_df = test.isnull().sum(axis=0).reset_index()\n", "missing_df.columns = ['column_name', 'missing_count']\n", "missing_df['ratio'] = round(missing_df['missing_count'] / test.shape[0],4)\n", "missing_df[missing_df['ratio']>0][['column_name', 'missing_count','ratio']].sort_values(by='ratio',ascending=False)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "e86686ec-b0dc-4f05-bd32-855b79e33bc2", "_uuid": "2a13117e1370fdf04b2c33e07a5a9d15fb1a3ead"}, "source": ["missingno_matrix(test)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "95a08c3b-d6d7-4823-9a5d-19b694d3c7ea", "_uuid": "4cbd929f34f40018d33d1129c6cfa393056f05b6"}, "source": ["missingno_heatmap(test)"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "2db271e9-43be-4b35-811f-ecb7f78cd300", "_uuid": "c20e23581e540f5bef5d6fd8901c840f88a6f6c6"}, "source": ["missing_bar(test)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "b1a7d469-1fbe-4484-9267-c39dd95af71c", "_uuid": "a22ffc2dd4b7cb3b3647a7ba7252f798757017aa"}, "source": ["- Good News. Both dataset get the same NaN ratio/distribution"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "b65682ec-6fc3-4fc8-ab62-4d7e596e6fcb", "_uuid": "0c373ad876a1eddb6393a4bbb0d1814c3030734f"}, "source": ["## Feature Importance"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "57e64a35-5d5a-427e-aa78-08cc856811f4", "_uuid": "54bceb8a14ddf503ad8dd3499ce940b203c7d7c5"}, "source": ["- **Decision Tree Classifier**\n", "\n", "A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "6b6d7ed7-da7f-44f2-893a-602fe3969bcc", "_uuid": "1bbc6fdd6154f4dacb777a060aec53d6b7065388", "_kg_hide-input": true}, "source": ["matplotlib.style.use('fivethirtyeight')\n", "matplotlib.rcParams['figure.figsize'] = (12,6)\n", "model = DecisionTreeClassifier(max_depth=6 ,random_state=87)\n", "model.fit(X_train, y_train)\n", "feat_names = train.drop(['id','target'],axis=1).columns\n", "## plot the importances ##\n", "importances = model.feature_importances_\n", "\n", "indices = np.argsort(importances)[::-1]\n", "plt.figure(figsize=(12,6))\n", "plt.title(\"Feature importances by DecisionTreeClassifier\")\n", "plt.bar(range(len(indices)), importances[indices], color='lightblue',  align=\"center\")\n", "plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n", "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\n", "plt.xlim([-1, len(indices)])\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"_cell_guid": "81041894-9e01-4ca1-beed-3a8f6ecdf4aa", "_uuid": "440d02c85abf13fcc419d971c0f046836ceead2e", "_kg_hide-input": true}, "source": ["from sklearn.tree import export_graphviz\n", "import graphviz\n", "treegraph = export_graphviz(model, out_file=None, \n", "                         feature_names=train.drop(['id','target'],axis=1).columns,  \n", "                         filled=True, rounded=True,  \n", "                         special_characters=True)  \n", "graph = graphviz.Source(treegraph)  \n", "graph"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "897a3f10-b87c-40a2-bfdb-ce3488e63b4b", "_uuid": "53963883ce0bfd3915d99aacfd1f8bf9e27ed6cd"}, "source": ["- **RandomForest Classifier**\n", "\n", "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_cell_guid": "0eb5cbba-92b4-4c35-baa0-89c3ada4f600", "_uuid": "acae970b3f0b87fd53d2a964b187e008fe4ae880", "_kg_hide-input": true}, "source": ["model = RandomForestClassifier(max_depth=8)\n", "model.fit(X_train, y_train)\n", "feat_names = train.drop(['id','target'],axis=1).columns\n", "## plot the importances ##\n", "importances = model.feature_importances_\n", "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n", "indices = np.argsort(importances)[::-1]\n", "\n", "plt.figure(figsize=(12,6))\n", "plt.title(\"Feature importances by Random Forest\")\n", "plt.bar(range(len(indices)), importances[indices], color='lightblue', yerr=std[indices], align=\"center\")\n", "plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n", "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\n", "plt.xlim([-1, len(indices)])\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "6dd36016-7311-4dd8-97b9-2ba88fe1ec8b", "_uuid": "5c3e8f7316577a806a67d1d7dd4caf129dd7f88c"}, "source": ["- **XGB Classifier**\n", "\n", "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "4d768caa-5dd7-41f2-9df7-364546cb1385", "_uuid": "2914411ac5b4f76c83b2c3bbfac4ea6eeef90714", "_kg_hide-input": true}, "source": ["model = XGBClassifier(eta = 0.01, max_depth = 8, subsample = 0.8, colsample_bytree= 0.8)\n", "model.fit(X_train, y_train)\n", "importances = model.feature_importances_\n", "indices = np.argsort(importances)[::-1]\n", "plt.figure(figsize=(12,6))\n", "plt.title(\"Feature importances by XGB\") # Thanks Oscar Takeshita's kindly remind\n", "plt.bar(range(len(indices)), importances[indices], color='lightblue', align=\"center\")\n", "plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n", "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\n", "plt.xlim([-1, len(indices)])\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "ddf25b5c-0188-4e8d-9408-70b63a5a66ee", "_uuid": "1991cddd99e3be7d225d586ff6ca34198d6700a7"}, "source": ["- Create Tree digraph by using \n", "\n", "`xgb.to_graphviz`"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "b67ab82a-69cf-4f62-94be-b7d9e9dfdde1", "_uuid": "eff6d0dfbe5d6d72dceb8502999752580dc0271d", "_kg_hide-input": true}, "source": ["xgb.to_graphviz(model, fmap='', rankdir='UT', num_trees=6,\n", "                yes_color='#0000FF', no_color='#FF0000')"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "56078f62-2373-4a7f-809b-3d46318b8afd", "_uuid": "6bcb30cc1999bb898ca1125bf4a3cf81af8278e2"}, "source": ["- **LightGBM Classifier**\n", "\n", "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "f303b6e9-6bd9-4324-a961-347bc4384b09", "_uuid": "24abc8159e2f59f18505a66622cbc21c287da509", "_kg_hide-input": true}, "source": ["lgb_params = {}\n", "lgb_params['objective'] = 'binary'\n", "lgb_params['sub_feature'] = 0.80 \n", "lgb_params['max_depth'] = 7\n", "lgb_params['feature_fraction'] = 0.7\n", "lgb_params['bagging_fraction'] = 0.7\n", "lgb_params['bagging_freq'] = 10\n", "lgb_params['learning_rate'] = 0.01\n", "\n", "lgb_train = lgb.Dataset(X_train, y_train)\n", "lightgbm = lgb.train(lgb_params, lgb_train, feature_name=[ i for i in feat_names])"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "74d0a9dc-7269-49da-bff3-e5ee10cf37c6", "_uuid": "40b09ec29a1cecfafc8cccd2a11d0d6eaf91b0d7", "_kg_hide-input": true}, "source": ["plt.figure(figsize=(12,6))\n", "lgb.plot_importance(lightgbm,max_num_features=30)\n", "plt.title(\"Feature importances by LightGBM\")\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "a21f7dcc-082c-4189-81b5-c387b2139c86", "_uuid": "b8a7c3385f1eaa1063d87c947b6a5a376b682c28", "_kg_hide-input": true}, "source": ["ax = lgb.plot_tree(lightgbm, tree_index=83, figsize=(20, 8), show_info=['split_gain'])\n", "plt.show()"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "46a6d860-4f99-4f24-a6fe-c5b71fa6d497", "_uuid": "6e6d627a9762373b16e7704542e6dd171104ce72"}, "source": ["# Acknowledgement:\n", "1. Pedro Schoen\n", "\n", "\n", "## Stay Tuned\n"], "cell_type": "markdown"}], "nbformat_minor": 1, "nbformat": 4}