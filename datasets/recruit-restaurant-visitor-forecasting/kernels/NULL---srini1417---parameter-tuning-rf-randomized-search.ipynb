{"cells": [{"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_cell_guid": "032c2608-54da-4e8b-a19b-93e51600d03c", "_uuid": "a58c3f19d00c154bea0556b29dc38c8254db24c2", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["import glob, re\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn import *\n", "from datetime import datetime\n", "from xgboost import XGBRegressor\n", "\n", "from keras.layers import Embedding, Input, Dense\n", "from keras.models import Model\n", "import keras\n", "import keras.backend as K\n", "\n", "import matplotlib.pyplot as plt\n", "from sklearn.externals import joblib"], "metadata": {"_cell_guid": "13c6d4a8-37b4-4f0d-9c09-04805ec58690", "_uuid": "619372f7448401c01c225011d905098217edf6ca", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["data = {\n", "    'tra': pd.read_csv('../input/air_visit_data.csv'),\n", "    'as': pd.read_csv('../input/air_store_info.csv'),\n", "    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n", "    'ar': pd.read_csv('../input/air_reserve.csv'),\n", "    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n", "    'id': pd.read_csv('../input/store_id_relation.csv'),\n", "    'tes': pd.read_csv('../input/sample_submission.csv'),\n", "    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n", "    }\n", "\n", "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n", "for df in ['ar','hr']:\n", "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n", "    data[df]['visit_dow'] = data[df]['visit_datetime'].dt.dayofweek\n", "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n", "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n", "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n", "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n", "    # Exclude same-week reservations - from aharless kernel\n", "    data[df] = data[df][data[df]['reserve_datetime_diff'] > data[df]['visit_dow']]\n", "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n", "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n", "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n", "\n", "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n", "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n", "data['tra']['year'] = data['tra']['visit_date'].dt.year\n", "data['tra']['month'] = data['tra']['visit_date'].dt.month\n", "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n", "\n", "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n", "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n", "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n", "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n", "data['tes']['year'] = data['tes']['visit_date'].dt.year\n", "data['tes']['month'] = data['tes']['visit_date'].dt.month\n", "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n", "\n", "unique_stores = data['tes']['air_store_id'].unique()\n", "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n", "\n", "#sure it can be compressed...\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n", "\n", "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n", "# NEW FEATURES FROM Georgii Vyshnia\n", "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n", "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n", "lbl = preprocessing.LabelEncoder()\n", "for i in range(10):\n", "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n", "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n", "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n", "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n", "\n", "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n", "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n", "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n", "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n", "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n", "\n", "train = pd.merge(train, stores, how='inner', on=['air_store_id','dow']) \n", "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n", "\n", "for df in ['ar','hr']:\n", "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n", "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n", "\n", "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n", "\n", "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n", "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n", "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n", "\n", "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n", "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n", "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n", "\n", "# NEW FEATURES FROM JMBULL\n", "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n", "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n", "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n", "train['var_max_long'] = train['longitude'].max() - train['longitude']\n", "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n", "test['var_max_long'] = test['longitude'].max() - test['longitude']\n", "\n", "# NEW FEATURES FROM Georgii Vyshnia\n", "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n", "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n", "\n", "lbl = preprocessing.LabelEncoder()\n", "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n", "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n", "\n", "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n", "train = train.fillna(-1)\n", "test = test.fillna(-1)\n", "print(\"Done\")"], "metadata": {"_cell_guid": "1231e6d7-ce8e-4a35-9f7e-cd984c67c945", "_uuid": "ffaa28a4de7ad2f24e2c29dcf08d2bd18fbc7f46", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def RMSLE(y, pred):\n", "    return metrics.mean_squared_error(y, pred)**0.5"], "metadata": {"_cell_guid": "bec06905-e88c-4f0e-9b79-c2aa30480c6f", "_uuid": "0267aad049dd1e56b2721c1a0251ed5affd04089", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(\"Start of Data Load\")\n", "value_col = ['holiday_flg','min_visitors','mean_visitors','median_visitors','max_visitors','count_observations',\n", "'rs1_x','rv1_x','rs2_x','rv2_x','rs1_y','rv1_y','rs2_y','rv2_y','total_reserv_sum','total_reserv_mean',\n", "'total_reserv_dt_diff_mean','date_int','var_max_lat','var_max_long','lon_plus_lat']\n", "\n", "nn_col = value_col + ['dow', 'year', 'month', 'air_store_id2', 'air_area_name', 'air_genre_name',\n", "'air_area_name0', 'air_area_name1', 'air_area_name2', 'air_area_name3', 'air_area_name4',\n", "'air_area_name5', 'air_area_name6', 'air_genre_name0', 'air_genre_name1',\n", "'air_genre_name2', 'air_genre_name3', 'air_genre_name4']\n", "\n", "\n", "X = train.copy()\n", "X_test = test[nn_col].copy()\n", "\n", "value_scaler = preprocessing.MinMaxScaler()\n", "for vcol in value_col:\n", "    X[vcol] = value_scaler.fit_transform(X[vcol].values.astype(np.float64).reshape(-1, 1))\n", "    X_test[vcol] = value_scaler.transform(X_test[vcol].values.astype(np.float64).reshape(-1, 1))\n", "\n", "X_train = list(X[nn_col].T.as_matrix())\n", "Y_train = np.log1p(X['visitors']).values\n", "nn_train = [X_train, Y_train]\n", "nn_test = [list(X_test[nn_col].T.as_matrix())]\n", "print(\"Train and test data prepared\")"], "metadata": {"_cell_guid": "e01ddd7d-a4f5-4158-8b86-a708ec43697a", "_uuid": "b4de4420aca3558a8be00e08dc58e22d8bb93e6d", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["#***************************************Random Forest\n", "model2 = ensemble.RandomForestRegressor(n_estimators=13, random_state=3, max_depth=18,\n", "                                        min_weight_fraction_leaf=0.0002)"], "metadata": {"_cell_guid": "12cf0a93-3cd9-4971-9765-f43c67a59e98", "_uuid": "cb05a1bb76fd181beb26a1b91ad189e14491bbda", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# *********************** Training and Validation Data *************************#\n", "train_X = train[col]\n", "train_X = train_X[train_X['year'] == 2016]\n", "train_y = train[train['year'] == 2016]\n", "test_X =  train[col]\n", "test_X = test_X[test_X['year'] == 2017]\n", "test_y = train[train['year'] == 2017]"], "metadata": {"_cell_guid": "d4484df0-fac5-4880-93de-b76b89736242", "_uuid": "966b0a96de8530a764017c990220d385efed0f68", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# *********************** Training and Validation Data *************************#\n", "#**********************************************************\n", "model2.fit(train_X, np.log1p(train_y['visitors'].values))\n", "print(\"Model2 trained\")\n", "preds2 = model2.predict(test_X)\n", "print('RMSE RandomForestRegressor: ', RMSLE(np.log1p(test_y['visitors'].values), preds2))"], "metadata": {"_cell_guid": "ae660114-c5cd-48f1-a439-02f3a8b6164a", "_uuid": "1fe7f0f89dfbaba950a79bc37f768060a663d71e", "collapsed": true}}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0e79f47a-a853-43c6-9106-89ea22cac7b9", "_uuid": "0a6c3951aa9b99b5827a2dd1254529bc160c0ae0"}, "source": ["**Points to remember**\n", "- We have used 2016 data for training and validating on 2017 data\n", "- We got a RMSE of 0.548500079658"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Commented after running on a local computer\n", "# takes a longer time\n", "'''\n", "from sklearn.model_selection import GridSearchCV\n", "\n", "# parameters for GridSearchCV\n", "param_grid2 = {\"n_estimators\": [10, 18, 22],\n", "              \"max_depth\": [3, 5],\n", "              \"min_samples_split\": [15, 20],\n", "              \"min_samples_leaf\": [5, 10, 20],\n", "              \"max_leaf_nodes\": [20, 40],\n", "              \"min_weight_fraction_leaf\": [0.1]}\n", "grid_search = GridSearchCV(model2, param_grid=param_grid2)\n", "grid_search.fit(train_X, np.log1p(train_y['visitors'].values))\n", "'''\n", "from operator import itemgetter"], "metadata": {"_cell_guid": "192953ae-00f0-4f67-8a93-dadcb6f1513f", "_uuid": "af6694b643e1fa94a0ada54355b5ac3feb965780", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Utility function to report best scores\n", "def report(results, n_top=3):\n", "    for i in range(1, n_top + 1):\n", "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n", "        for candidate in candidates:\n", "            print(\"Model with rank: {0}\".format(i))\n", "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n", "                  results['mean_test_score'][candidate],\n", "                  results['std_test_score'][candidate]))\n", "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n", "            print(\"\")"], "metadata": {"_cell_guid": "6c238016-75b5-4278-a022-8d16629b0a6e", "_uuid": "58d05d463c2f3aeab4d066deb9dc73b89c17ad77", "collapsed": true}}, {"cell_type": "markdown", "metadata": {"_cell_guid": "3709a71f-3a73-4c93-934c-9a525e8fc407", "_uuid": "66f3b5e852698876d6baa4516cb9dc349fb3a4cf"}, "source": ["**Output from the local computer for the report**\n", "Model with rank: 1  \n", "Mean validation score: 0.5483)  \n", "Parameters: {'n_estimators': 18, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.1, 'max_depth': 3, 'max_leaf_nodes': 20, 'min_samples_split': 15}  \n", "\n", "Model with rank: 2  \n", "Mean validation score: 0.5483)  \n", "Parameters: {'n_estimators': 18, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.1, 'max_depth': 3, 'max_leaf_nodes': 20, 'min_samples_split': 20}  \n", "\n", "Model with rank: 3  \n", "Mean validation score: 0.5483)  \n", "Parameters: {'n_estimators': 18, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'max_depth': 3, 'max_leaf_nodes': 20, 'min_samples_split': 15}  \n", "\n", "Model with rank: 4  \n", "Mean validation score: 0.5483)  \n", "Parameters: {'n_estimators': 18, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'max_depth': 3, 'max_leaf_nodes': 20, 'min_samples_split': 20}  "]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["#Change parameters and test if it performs better than the prior model\n", "#***************************************Random Forest\n", "model2 = ensemble.RandomForestRegressor(n_estimators=18, random_state=3, max_depth=3,\n", "                                        min_weight_fraction_leaf=0.1,max_leaf_nodes = 20,\n", "                                       min_samples_split = 20)\n", "#**********************************************************\n", "model2.fit(train_X, np.log1p(train_y['visitors'].values))\n", "print(\"Model2 trained\")\n", "preds2 = model2.predict(test_X)\n", "print('RMSE RandomForestRegressor: ', RMSLE(np.log1p(test_y['visitors'].values), preds2))"], "metadata": {"_cell_guid": "4db82ef8-b5de-4a66-a493-b77db37f7c7f", "_uuid": "0299620956620697d6d0b943e4be1cd4069624c2", "collapsed": true}}, {"cell_type": "markdown", "metadata": {"_cell_guid": "f126001a-5a51-4ec1-b4fa-e72785d51b5e", "_uuid": "bb601b149dde533a590d9b8d7450a371936de58e"}, "source": ["**RMSE RandomForestRegressor:  0.529405212091**\n", "- clearly it performs better than the prior model but needs a lot of tuning\n", "**Happy Tuning the models  -  you can use the gridCV for tuning parameters for XGB, GBM etc...**"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d0a4fd9e-37bf-409f-b30f-3ca714e3f81e", "_uuid": "37becb7c9d63c9865f570af8da43728a771b7fe7", "collapsed": true}, "source": ["**Randomized Search**\n", "- Search for the Parameters consists of following:\n", "    1. an estimator - in our case it is RandomForestRegressor\n", "    2. a parameter space -  this the grid we have passed onto the GridSearchCV\n", "    3. a method for searching or sampling candidates  - we have seen GridsearchCV and other one is RandomizedSearchCV\n", "    4. a cross-validation scheme\n", "    5. a scoring function -  evaluates the parameters\n", "- GridSearchCV is an exhaustive search or say it is a brute force technique.\n", "- RandomizedSearchCV as the name mentions does a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. Which is far more efficient in searching the parameter values for fine tuning\n", "# Randomized Search"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.model_selection import RandomizedSearchCV\n", "from scipy.stats import randint as sp_randint\n", "# parameters for GridSearchCV\n", "# specify parameters and distributions to sample from\n", "param_dist = {\"max_depth\": [3, 5],\n", "              \"max_features\": sp_randint(1, 11),\n", "              \"min_samples_split\": sp_randint(2, 11),\n", "              \"min_samples_leaf\": sp_randint(1, 11),\n", "              \"bootstrap\": [True, False]\n", "             }\n", "# run randomized search\n", "n_iter_search = 20\n", "random_search = RandomizedSearchCV(model2, param_distributions=param_dist,\n", "                                   n_iter=n_iter_search)\n", "\n"], "metadata": {"_cell_guid": "1c110eab-0687-4cde-ad22-632bb5458f61", "_uuid": "1570d4cb5720aeef5c4bc3ff9a20cb8cd75000d0", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["random_search.fit(train_X, np.log1p(train_y['visitors'].values))"], "metadata": {"_cell_guid": "e838c443-ed76-419f-9fbe-0b083e115dd0", "_uuid": "ace2303bf6ed1ae9e7008190c9bb69647073a8ab", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["report(random_search.cv_results_)"], "metadata": {"_cell_guid": "03c141fa-0e5f-4348-bf24-bab00c57f448", "_uuid": "c94d54b2a589cbbc8d0ef3e5192411fa7a94273b", "collapsed": true}}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d65a241f-5ec9-484a-8aa0-7c26b582e035", "_uuid": "558d0b5d67d69938cf75f3f72927907a543d46ce"}, "source": ["It is evident that Randomized search has searched quite efficiently and in less time!!"]}], "metadata": {"language_info": {"mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "version": "3.6.4", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1, "nbformat": 4}