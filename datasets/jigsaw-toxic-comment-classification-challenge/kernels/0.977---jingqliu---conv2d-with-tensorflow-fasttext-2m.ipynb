{"cells":[{"metadata":{"collapsed":true,"_cell_guid":"77ac16e1-91b9-4fce-9405-347b77fcbe1c","_uuid":"05392a0d30eb4ada9984b17a9d02fbfa5f1c42f3","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport csv\nimport tensorflow as tf\nimport nltk\nimport gc\nfrom gensim.models import Word2Vec\nfrom keras.preprocessing import text, sequence\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8f929dc0-71f2-4769-89e2-ae326f81a28f","_uuid":"bf9fb2885cf386fbdd5854a03212f7d597f3125f"},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"collapsed":true,"_cell_guid":"0a6a8cd7-576e-4de2-a29f-ee943cd85ef0","_uuid":"76fca824894e72606c91a1190f0dbba126dfdfe2","trusted":false},"cell_type":"code","source":"#This dataset is from Kaggle Competition, Toxic Comment Classification Challenge, \n#that train dataset contains 159571 rows and 8 columns, which are id, comment_text, \n#toxic, sever_toxic, obscene, threat, insult and identity_hate.\n#The test dataset has over 150000 records.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"eee3cb40-ca06-48f0-9660-0d658a9e0ac8","_uuid":"57a2ae2b43e28fcdea0e83f9f379026a51ac548c","trusted":false},"cell_type":"code","source":"df_train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv') \ndf_test = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\ntrain_input = df_train['comment_text']\ntest_input = df_test['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4aa5ebb4-567c-44fe-9a21-1a8979824318","_uuid":"819e98594e0c114289cb061ed7e95f4be327fe42","trusted":false},"cell_type":"code","source":"# Define a function to read the FastText Pre-trained Word Embedding in to a dictionary.\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'))\ndel embeddings_index['2000000'] # The first row of the file is useless, so delete it.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"cd160fbc-a010-4112-b9aa-b406b4a9a108","_uuid":"d4d185b148d2c8c0970340afab3c2982ee6b116d","trusted":false},"cell_type":"code","source":"len(embeddings_index) \n#FastText Word Embedding file contains 2500000 words including punctuations.\n#It doesn't contains 0-9 and words like I'm, can't and etc.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c28af8be-3e50-4e0a-9314-e562f41e933e","_uuid":"9308c041571f31e0da7dd3f34906bef89711ed8f","trusted":false},"cell_type":"code","source":"max_features = 100000\nmaxlen = 170 \n#Set the max length of each comment. If it is longer than 150 then cut if off,\n#if it is shorter than 150 then pad it up to 150.\n#This max length can be choosen in different ways. \n#Here it is a number that near 80 percentile of all comment length in training dataset.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"62af182f-cba7-4ff4-929d-4fa57a47b08f","_uuid":"f54c8d3d8acd3f9d1236ead211a2d668f2d8ff92","trusted":false},"cell_type":"code","source":"# Define data cleaning function\ndef clean(string):\n    string = re.sub(r'\\n', ' ', string)\n    string = re.sub(r'\\t', ' ', string)\n    string = re.sub(\"[^A-Za-z\\(\\)\\,\\.\\?\\'\\!]\", \" \", string)\n    string = re.sub(\"\\'m\", ' am ', string)\n    string = re.sub(\"\\'s\", ' is ', string)\n    string = re.sub(\"can\\'t\", 'cannot ', string)\n    string = re.sub(\"n\\'t\", ' not ', string)\n    string = re.sub(\"\\'ve\", ' have ', string)\n    string = re.sub(\"\\'re\", ' are ', string)\n    string = re.sub(\"\\'d\", \" would \", string)\n    string = re.sub(\"\\'ll\", \" will \", string)\n    string = re.sub(\"\\,\", \" , \", string)\n    string = re.sub(\"\\'\", \" ' \", string)\n    string = re.sub(\"\\.\", \" . \", string)\n    string = re.sub(\"\\!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" ( \", string)\n    string = re.sub(r\"\\)\", \" ) \", string)\n    string = re.sub(r\"\\?\", \" ? \", string)\n    string = re.sub(r'\\s{2,}', ' ', string.lower())\n    return string","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f5cd3efe-55a8-4705-9eb9-7ad5c0f64f69","_uuid":"b7e56a87ea8bb17b818c171ed547012249e83961","trusted":false},"cell_type":"code","source":"x_train = train_input.apply(clean)\ny_train = df_train[['toxic','severe_toxic',\"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\nx_test = test_input.apply(clean)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ef8a4081-7627-4cf6-af0e-12c308ab868c","_uuid":"3258bf6e292eca4a0746f212a9a2b8cc14791197","trusted":false},"cell_type":"code","source":"#After data clean there might be some record have nothing in comment_text, fill with a word.\nx_train = x_train.fillna('fillna')\nx_test = x_test.fillna('fillna')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f5a389d2-1ca7-4f6c-b89c-0d643ceda5b9","_uuid":"9b33b2095ecdebdb81cf020e5c596dafb31ed1ba","trusted":false},"cell_type":"code","source":"#Create the dictionary whose keys contains all words in train dataset that also shown \n#in FastText word embeddings.\nlst = []\nfor line in x_train:\n    lst += line.split()\n    \ncount = Counter(lst)\nfor k in list(count.keys()):\n    if k not in embeddings_index:\n        del count[k]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c66d0916-b122-4534-aa2f-353165110ce2","_uuid":"55b9c49258a0f464c3272a1aaa27e3fae016f42a","trusted":false},"cell_type":"code","source":"len(count)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4d91ff5c-27a7-4d71-848f-cd8250edad29","_uuid":"c7f35ba956c0bc9a81e6db0e1babdbf40b255bd7","trusted":false},"cell_type":"code","source":"count = dict(sorted(count.items(), key=lambda x: -x[1]))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c8efb7cc-0a07-4f3d-8a1e-feee7a4ea66f","_uuid":"8eb3a2b55009157ad59e1e71c9db80019bf59fc2","trusted":false},"cell_type":"code","source":"count = {k:v for (k,v) in count.items() if v >= 2}","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"bd683af4-0236-4ddf-9c5e-1c1d704d97ed","_uuid":"8551e078fb215a5245d6d9ad834c3131d90ba367","trusted":false},"cell_type":"code","source":"len(count)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6df05e41-41cb-45dc-9bb8-dd2a6a46d13f","_uuid":"5eca3f249b6cd79a219342d9f0c7dc347d0c33a5","trusted":false},"cell_type":"code","source":"count = dict(zip(list(count.keys()),range(1,64349 + 1)))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2e222e84-9713-4062-9308-7fa799570843","_uuid":"48a4c42e1b05b0528eee8f0da182f1676a23b402","trusted":false},"cell_type":"code","source":"embedding_matrix = {}\nfor key in count:\n    embedding_matrix[key] = embeddings_index[key]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c0b7c697-74e4-42f0-a289-168555d5f0d0","_uuid":"02b161a87a1bd27ca801f7aa1d70aa5c36ebbe22","trusted":false},"cell_type":"code","source":"#Create teh word embedding matrix where the first element is all zeros which is for word\n#that is not shown and the padding elements.\nW = np.zeros((1,300))\nW = np.append(W, np.array(list(embedding_matrix.values())),axis=0)\nW = W.astype(np.float32, copy=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"3ef8bae9-669d-460d-8202-6ecc3cbf5998","_uuid":"63b635950b1e01caceca77b31db2bdbbe48a10e7","trusted":false},"cell_type":"code","source":"W.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"625c5b8c-5e79-4c4e-a641-6999cdee5d6e","_uuid":"249812b97d0e11c263ec18b51af00bc9ea924137","trusted":false},"cell_type":"code","source":"#Same Step for text dataset.\nlst = []\nfor line in x_test:\n    lst += line.split()\n    \ncount_test = Counter(lst)\nfor k in list(count_test.keys()):\n    if k not in embedding_matrix:\n        del count_test[k]\n    else:\n        count_test[k] = count[k]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c8c67ff2-3cfe-472f-a2f5-2d42699c502f","_uuid":"26dd78cbde6c6076ac31831c4659bb55530efca9","trusted":false},"cell_type":"code","source":"len(count_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9ae19a9d-8822-4e52-8162-8ea7962af5a5","_uuid":"9ccf59d4d0d798ecbe6c82d8a45f3ac87ce79dbe","trusted":false},"cell_type":"code","source":"#Release memory.\ndel lst\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a5cd736e-d770-4f81-a2b0-8bcf90c52f5b","_uuid":"a8ab60289c7a79663821f68b2e362dce62ba1ca1","trusted":false},"cell_type":"code","source":"#Make the train dataset to be a sequence of ids of words.\nfor i in range(len(x_train)):\n    temp = x_train[i].split()\n    for word in temp[:]:\n        if word not in count:\n            temp.remove(word)\n    for j in range(len(temp)):\n        temp[j] = count[temp[j]]\n    x_train[i] = temp","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6f34b8a1-6291-41c5-8e9b-48c5e7627d9f","_uuid":"d77a146faa5d210034f1a0e99fb32a2a66b1091e","trusted":false},"cell_type":"code","source":"for i in range(len(x_test)):\n    temp = x_test[i].split()\n    for word in temp[:]:\n        if word not in count_test:\n            temp.remove(word)\n    for j in range(len(temp)):\n        temp[j] = count_test[temp[j]]\n    x_test[i] = temp","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"56add1ad-c5ac-4f33-a664-da243f29a85b","_uuid":"9525a71524a665343e8b81a68dcda14e2af598be","trusted":false},"cell_type":"code","source":"#Create evaluation dataset.\n#Xtrain, Xval, ytrain, yval = train_test_split(x_train, y_train, train_size=0.96, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c97be199-7825-4062-936a-03e19912f03c","_uuid":"afaeeb5d6117e682afeb5ff3c2822b80185d5a00","trusted":false},"cell_type":"code","source":"#Pad sequence to 170 length.\ntrain_x = sequence.pad_sequences(list(x_train), maxlen = maxlen)\ntest_x = sequence.pad_sequences(list(x_test), maxlen = maxlen)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8c242aa7-5384-4ee6-b35a-a46336588890","_uuid":"5fb54a3b41a78d11f7a297aee2bcef8643477627","trusted":false},"cell_type":"code","source":"del embeddings_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4bad4fd0-cafa-4077-80c5-1415b1e2369f","_uuid":"dd50dd50843fd2c0e68aa13e6214e3755dd402df"},"cell_type":"markdown","source":"# Placeholders and CNN construction"},{"metadata":{"collapsed":true,"_cell_guid":"98439fcf-01f4-4575-a7b9-84e2ff327c3c","_uuid":"b6efb2baf4461a4b5d18845c5331733c662a76b7","trusted":false},"cell_type":"code","source":"filter_sizes = [1,2,3,4,5]\nnum_filters = 32\nbatch_size = 256\n#This large batch_size is specially for this case. Usually it is between 64-128.\nnum_filters_total = num_filters * len(filter_sizes)\nembedding_size = 300\nsequence_length = 170\nnum_epochs = 3 #Depends on your choice.\ndropout_keep_prob = 0.9","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b19f08f0-bba8-44de-a724-5bd8abdee16d","_uuid":"9e6371867a08bbd017ba6ff9ad75dd3b6cbd2a38","trusted":false},"cell_type":"code","source":"input_x = tf.placeholder(tf.int32, [None, sequence_length], name = \"input_x\")\ninput_y = tf.placeholder(tf.float32, [None,6], name = \"input_y\") ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c0d68110-ba35-4a05-8710-7a2e2a1ef837","_uuid":"2a764bf6fb2cd6ebab14d8352b8319f0576518f9","trusted":false},"cell_type":"code","source":"embedded_chars = tf.nn.embedding_lookup(W, input_x)\nembedded_chars_expanded = tf.expand_dims(embedded_chars, -1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"db1e495c-f7fc-4fc0-9c7e-891837c8cec8","_uuid":"e7cc3d01cad9cb8bc5f45cf6c9c0872d5f1065b5","trusted":false},"cell_type":"code","source":"def CNN(data):\n    pooled_outputs = []\n    \n    for i, filter_size in enumerate(filter_sizes):\n        \n        filter_shape = [filter_size, embedding_size, 1, num_filters]\n        \n        w = tf.Variable(tf.truncated_normal(filter_shape,stddev = 0.05), name = \"w\")\n        b = tf.Variable(tf.truncated_normal([num_filters], stddev = 0.05), name = \"b\")\n            \n        conv = tf.nn.conv2d(\n            data,\n            w,\n            strides = [1,1,1,1],\n            padding = \"VALID\",\n            name = \"conv\"\n        )\n        h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")\n        pooled = tf.nn.max_pool(\n            h,\n            ksize = [1, sequence_length - filter_size + 1, 1, 1],\n            strides = [1,1,1,1],\n            padding = \"VALID\",\n            name = \"pool\"\n        )\n        \n        pooled_outputs.append(pooled)\n    \n    #return pooled_outputs\n    h_pool = tf.concat(pooled_outputs, 3)\n    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n    return h_pool_flat","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2abf2571-3fd3-453d-aa6b-0f15e979cecf","_uuid":"882af7a113a83b5ebae906c737b01a7dc6f5e364","trusted":false},"cell_type":"code","source":"h_pool_flat = CNN(embedded_chars_expanded)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d640c13e-2e00-47d4-bf0d-461c3f3947f5","_uuid":"6588f82c724ebb388fd5534aeddf19ed3f1f696d","trusted":false},"cell_type":"code","source":"h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1526f114-ad3a-43e2-8fb3-54e48739d711","_uuid":"f0f28ced420037a746abfbce5c31b23e179d42a0","trusted":false},"cell_type":"code","source":"#In the first dense layer, reduce the node to half.\nwd1 = tf.Variable(tf.truncated_normal([num_filters_total, int(num_filters_total/2)], stddev=0.05), name = \"wd1\")\nbd1 = tf.Variable(tf.truncated_normal([int(num_filters_total/2)], stddev = 0.05), name = \"bd1\")\nlayer1 = tf.nn.xw_plus_b(h_drop, wd1, bd1, name = 'layer1') # Do wd1*h_drop + bd1\nlayer1 = tf.nn.relu(layer1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d4a2d738-1460-4d95-9f66-d7275d1cf85d","_uuid":"450e798684011859199010f97e99fb369160d55f","trusted":false},"cell_type":"code","source":"#Second dense layer, reduce the outputs to 6.\nwd2 = tf.Variable(tf.truncated_normal([int(num_filters_total/2),6], stddev = 0.05), name = 'wd2')\nbd2 = tf.Variable(tf.truncated_normal([6], stddev = 0.05), name = \"bd2\")\nlayer2 = tf.nn.xw_plus_b(layer1, wd2, bd2, name = 'layer2') \nprediction = tf.nn.sigmoid(layer2)# Make it to be 0-1.\n#pred_clipped = tf.clip_by_value(prediction, 1e-10, 0.9999999) \n#For some special loss function clip is necessary. Like log(x).","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"efca9e18-a913-454f-9f34-ff516306dad9","_uuid":"0c47cc6c827cbc97c0a566577b6f5df491595054","trusted":false},"cell_type":"code","source":"loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = layer2, labels = input_y))\noptimizer = tf.train.AdamOptimizer(learning_rate = 0.0007).minimize(loss)\n#when learning rate set to 0.0007, the mean of threat is not 0, but when it is 0.001, it becomes 0 again.\n#Learning rates usually is small for CNN compared with pure neural network. \n#Need to define a approriate learning rate before you run on the whole dataset.\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(prediction), input_y), tf.float32))\n#correct_prediction = tf.equal(tf.argmax(input_y, 1), tf.argmax(prediction, 1))\n#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70e1ed4c-b967-48b4-9f5b-d25b916e46b2","_uuid":"53f1321130f643c9740cfa4397123ca3558cc37c"},"cell_type":"markdown","source":"# Blocks and Batches"},{"metadata":{"collapsed":true,"_cell_guid":"80520cd6-a396-49fe-8937-7c1825c5b09c","_uuid":"5271c7238659c5ad96493b41c4454391d7dd22af","trusted":false},"cell_type":"code","source":"#Define batch generation function.\ndef generate_batch(data, batch_size, num_epochs, shuffle=True):\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n    l = 0\n    for epoch in range(num_epochs):\n        l += 1\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8bb56ca1-0d1a-4221-a733-41fae1dac9c3","_uuid":"284d85cc8dd602552f5924d7b63faac078515d36","trusted":false},"cell_type":"code","source":"#For Test data. Can use generate_batch function.\ndef blocks(data, block_size):\n    data = np.array(data)\n    data_size = len(data)\n    nums = int((data_size-1)/block_size) + 1\n    for block_num in range(nums):\n        if block_num == 0:\n            print(\"prediction start!\")\n        start_index = block_num * block_size\n        end_index = min((block_num + 1) * block_size, data_size)\n        yield data[start_index:end_index]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"433e2a2d-1707-449d-9c13-e3ea9426c1aa","_uuid":"5460dbfb91444000c94f5860d42e4d540fc09b64"},"cell_type":"markdown","source":"# Training and evaluate model"},{"metadata":{"collapsed":true,"_cell_guid":"b76e5e7b-8678-4bf7-b429-e82fb661dfdd","_uuid":"8414b175c462902eaeec231912a2c6827518976e","trusted":false},"cell_type":"code","source":"# The reason to create 7 different batches here is because \n#I want to make the data totally shuffled to reduce the risk that one batch have all 0.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d4d8b0ad-a7db-4f17-bd8c-67082d9d84ea","_uuid":"22ea5e8a9c06cd5e26c88345ddfffa60262e1766","trusted":false},"cell_type":"code","source":"batch1 = generate_batch(list(zip(np.array(train_x), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a8ea50dd-6769-41f5-a2d0-b772431423e7","_uuid":"fd994f1e2dd5c7d2546834180bd1489c47dc7f80","trusted":false},"cell_type":"code","source":"batch2 = generate_batch(list(zip(np.array(train_x), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7cb42ad4-ec8a-4ca4-9793-f7d1c44e4759","_uuid":"554f6d90a3f4e887b4ca24626b4148ed4d97f73d","trusted":false},"cell_type":"code","source":"batch3 = generate_batch(list(zip(np.array(train_x), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"47d16e23-1674-4bc0-b646-b8668f34809a","_uuid":"46d9f387ac2426ba6c5fbd558f98d15db96c4908","trusted":false},"cell_type":"code","source":"batch4 = generate_batch(list(zip(np.array(train_x), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0f5b726f-70d3-431d-83a0-81bf81c30bbf","_uuid":"26e2574282c4201f7e1775b1cdac649fcd1b4916","trusted":false},"cell_type":"code","source":"batch5 = generate_batch(list(zip(np.array(train_x), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"49ccadbcc2ba61b6884b4b2224b3c1194d568074"},"cell_type":"code","source":"batch6 = generate_batch(list(zip(np.array(train_x), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c1588123e0c031cb23d7f7e88b132f9a9271d1ea"},"cell_type":"code","source":"batch7 = generate_batch(list(zip(np.array(train_x), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0c7140c0-0796-43a8-ad35-fa53ca73f357","_uuid":"5c20c2d2042afec54edb57d7893dd672e1401996","trusted":false},"cell_type":"code","source":"batch_bag = [batch1,batch2,batch3,batch4,batch5,batch6,batch7]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"74b4ae55-87d1-457e-b843-d5e0ba41cf4c","_uuid":"298e8d58841abe3c0f8e25e0a6a68c45c2018316","trusted":false},"cell_type":"code","source":"test_blocks = blocks(list(np.array(test_x)), 1000)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4d8f892b-d972-4840-8289-2933c2f7d326","_uuid":"8b2bf15fc8da16c8a99de7740ca51a6ffb8f3cc1","trusted":false},"cell_type":"code","source":"train_x.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"00b755a0-abd9-47ae-b18f-897d4468ff9f","_uuid":"9c68a3f848b0163673b686c97b331ab262bebc5b","trusted":false},"cell_type":"code","source":"int((159571-1)/256)+1","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"scrolled":true,"_cell_guid":"01fe2f0d-04cc-47a2-b8d0-ef2c66d4a346","_uuid":"9da1c929a791065c3cf19288e2497d42dce1d6fd","trusted":false},"cell_type":"code","source":"init_op = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    \n    sess.run(init_op)\n    i = 0\n    for batches in batch_bag:\n        i += 1\n        print('Epoch: ' + str(i) + ' start!')\n        avg_acc = 0\n        avg_loss = 0\n        for batch in batches:\n            batch = pd.DataFrame(batch, columns = ['a','b','c','d','e','g','f'])\n            x_batch = pd.DataFrame(list(batch['a']))\n            y_batch = batch.loc[:, batch.columns != 'a']\n            _,c, acc = sess.run([optimizer, loss, accuracy],feed_dict = {input_x: x_batch, input_y: y_batch})\n            avg_loss += c\n            avg_acc += acc\n        avg_loss = avg_loss/624\n        avg_acc = avg_acc/624\n        print('Epoch:' + str(i) + ' loss is ' + str(avg_loss) + ', train accuracy is ' + str(avg_acc))\n        #print('Evaluation Accuracy: ')\n        #print(accuracy.eval({input_x: val_x, input_y: yval}))\n    \n    print('Training Finish!')\n    \n    df = pd.DataFrame()\n    for block in test_blocks:\n        block = pd.DataFrame(block)\n        pred = sess.run(prediction, feed_dict = {input_x: block})\n        df = df.append(pd.DataFrame(pred))\n    \n    print('Prediction Finish!')\n ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"980ff05d-a873-47e7-9d1e-73e50e89596e","_uuid":"b8cff91abf377e0d021fd96a1a7e4e4b8e8dbd4b","trusted":false},"cell_type":"code","source":"df.round().mean()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"12a58c07-78aa-4429-9859-598bdab29d20","_uuid":"c6e31063b3156f2a78cece7529606e276117631b","trusted":false},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = np.array(df)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}