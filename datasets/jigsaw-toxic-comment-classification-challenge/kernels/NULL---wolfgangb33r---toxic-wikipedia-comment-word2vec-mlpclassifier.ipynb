{"cells": [{"cell_type": "markdown", "source": ["# Toxic Wikipedia comment analysis\n", "\n", "Some first impressions about the given toxic comments data set that was taken from Wikipedia. \n", "\n", "### Contents: \n", "1. Import libraries\n", "2. Read the data\n", "3. Data analysis\n", "5. Fill missing data\n", "6. Split comments into words\n", "7. Train a word2vec model\n", "8. Choose a classifier\n", "9. Creating submission file\n", "\n", "Feedback and hints for improvement is greatly appreciated!\n"], "metadata": {"_cell_guid": "9a2b4532-f663-45f1-8662-49b12014e427", "_uuid": "ee1597248380caa30f6a8b84ff3c598b681cb82f"}}, {"cell_type": "markdown", "source": ["## 1. Import libraries\n", "We will need numpy for linear algebra matrix handling and pandas for convenient data import and cleaning purposes. "], "metadata": {"_cell_guid": "479a5530-a94e-4dfa-8535-ec3ff6472098", "_uuid": "955051436aa21367b9119cb53cefdd6ec478d3e8"}}, {"source": ["import time\n", "\n", "# import necessary libraries\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "#text libraries\n", "import re\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from gensim.models import word2vec\n", "\n", "# classifier imports\n", "from sklearn.neural_network import MLPClassifier"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "01725041-76de-43bc-9dee-19f834b9a13e", "_uuid": "bb5babb160cba46f2cae7506601dc6767de6ba01", "collapsed": true}}, {"cell_type": "markdown", "source": ["## 2. Read the data\n", "Use pandas to read the complete training and test data set. "], "metadata": {"_cell_guid": "43a086b8-fb13-4181-a551-1d6e2118b242", "_uuid": "e8bea99ed48619c883e906469bfee5747bf3edc3"}}, {"source": ["# Read data\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "# copy test id column for later submission\n", "result = test[['id']].copy() \n", "# show first 3 rows of the training set to get a first impression about the data\n", "print(train.head(3))"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "25e597c5-0332-40e6-9e10-e1778a29ace1", "_uuid": "bff633fa4139b21c5e59d7369e3d39bed7842e01", "collapsed": true}}, {"cell_type": "markdown", "source": ["## 3. Data analysis\n", "Take a closer look at the given data. \n", "\n", "There are no additional feature columns beside the **comment_text** column. The training data comes with six labels defining the incremental severity of toxity of each comment row. "], "metadata": {"_cell_guid": "46b2aee3-72d8-4f5d-b5bb-4536a7c6ca53", "_uuid": "a73dd93c3c8157f710eac25a935f40e65bd0558d"}}, {"source": ["# count each severity \n", "print('toxic: %d' % train[train['toxic'] > 0]['toxic'].count())\n", "print('severe_toxic: %d' % train[train['severe_toxic'] > 0]['severe_toxic'].count())\n", "print('obscene: %d' % train[train['obscene'] > 0]['obscene'].count())\n", "print('threat: %d' % train[train['threat'] > 0]['threat'].count())\n", "print('insult: %d' % train[train['insult'] > 0]['insult'].count())\n", "print('identity_hate: %d' % train[train['identity_hate'] > 0]['identity_hate'].count())"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "a35acf8f-3088-4211-99dc-2d5e0a791143", "_uuid": "7045fa17d84bb66cfa0a10db14ae78bd05f354ca", "collapsed": true}}, {"cell_type": "markdown", "source": ["I am curious if the label 'toxic' is the precondition for the label 'severe_toxic'?"], "metadata": {"_cell_guid": "b564684a-a769-4b94-b542-1eeab6586296", "_uuid": "86aafbf596d83b0ef3e60579f6d6ade702dcc531"}}, {"source": ["print('Severe toxic but NOT toxic?: %d' % train[(train['severe_toxic'] > 0) & (train['toxic'] == 0)]['id'].count())\n", "print('Insult but NOT toxic?: %d' % train[(train['insult'] > 0) & (train['toxic'] == 0)]['id'].count())\n", "print('Obscene but NOT toxic?: %d' % train[(train['obscene'] > 0) & (train['toxic'] == 0)]['id'].count())\n", "print('Threat but NOT insult?: %d' % train[(train['threat'] > 0) & (train['insult'] == 0)]['id'].count())"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "f16e2bdf-8105-444a-b1dd-522fec6225be", "_uuid": "f67d349e756ae7beba9d031d053a8086ac76311a", "collapsed": true, "scrolled": true, "_kg_hide-input": true}}, {"cell_type": "markdown", "source": ["Check the typical length of a comment."], "metadata": {"_cell_guid": "a5546b36-dacc-425b-890c-008f52f44d16", "_uuid": "09552bc7f82d55ede4e178d948f1c532d01a9095"}}, {"source": ["train['len'] = train['comment_text'].str.len()\n", "print('Average comment length: %d' % train['len'].mean())\n", "print('Median comment length: %d' % train['len'].quantile(.5))\n", "print('90th percentile comment length: %d' % train['len'].quantile(.9))"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "9ae88b5d-80be-46f0-b7b4-f71da2958ac8", "_uuid": "3cb52de743e1ef3a9addbec7c27ba75df2cb10fb", "collapsed": true, "scrolled": true}}, {"source": ["print(train[train['comment_text'].isnull()])\n", "print(test[test['comment_text'].isnull()])"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "2fcfce4c-3bd4-4887-a8c4-bd239452d3df", "_uuid": "8e2974f014f982e4879ab88a14211452f779b9b2", "collapsed": true}}, {"cell_type": "markdown", "source": ["### Some observations\n", "\n", "1. Most frequent labels are toxic, obscene and insult. They seem to be the major categories.\n", "2. Toxic and severe_toxic label are related while all other labels seem to be independent "], "metadata": {"_cell_guid": "9819d081-e8f7-4ea2-bb1b-230f9e7092c1", "_uuid": "1ab6ea7c3f7e57514b0f24b25cac8c267d0c16ce"}}, {"cell_type": "markdown", "source": ["## 4. Fill missing data\n", "We have to make sure that there are no null values within the training and test data sets, otherwise our algorithms might fail."], "metadata": {"_cell_guid": "45bfeb28-c122-4469-b6c9-a613f12e24cc", "_uuid": "a0f5c483256815dae78f78a81f77061183974e37"}}, {"source": ["test['comment_text'].fillna(value='none', inplace=True) # there is one \n", "train['comment_text'].fillna(value='none', inplace=True) "], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "b129bbfa-508a-45b0-8f99-453a6ddc71ca", "_uuid": "00978273239c254c9bfe713f50c0b1022c783876", "collapsed": true}}, {"cell_type": "markdown", "source": ["## 5. Split comments into array of words\n", "We will split the given comments into arrays of single words. We will also remove non letter/number characters.  "], "metadata": {"_cell_guid": "5f54e6be-35ca-4f99-be74-79b70d41a910", "_uuid": "28a68eba3749dadb64d55ae67f00bbc31dccb3c9"}}, {"source": ["def text_to_words(raw_text, remove_stopwords=False):\n", "    # 1. Remove non-letters, but including numbers\n", "    letters_only = re.sub(\"[^0-9a-zA-Z]\", \" \", raw_text)\n", "    # 2. Convert to lower case, split into individual words\n", "    words = letters_only.lower().split()\n", "    if remove_stopwords:\n", "        stops = set(stopwords.words(\"english\")) # In Python, searching a set is much faster than searching\n", "        meaningful_words = [w for w in words if not w in stops] # Remove stop words\n", "        words = meaningful_words\n", "    return words \n", "\n", "sentences_train = train['comment_text'].apply(text_to_words, remove_stopwords=False)\n", "sentences_test = test['comment_text'].apply(text_to_words, remove_stopwords=False)\n", "# show first three arrays as sample\n", "print(sentences_train[:3])"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "7b05dc35-b0e4-489c-85c4-9a415c0a1826", "_uuid": "a29ff3dc0fef06e988e6f54b86fdd3a7dadd1fab", "collapsed": true}}, {"cell_type": "markdown", "source": ["## 6. Train a word2vec model\n", "Train a word2vec model with the given training sentences. "], "metadata": {"_cell_guid": "37919469-2780-427b-b9e2-114fbc2de777", "_uuid": "426af7816cd3b429677c4545e69a4145c01d1dcc"}}, {"source": ["# Set values for various parameters\n", "num_features = 300    # Word vector dimensionality                      \n", "min_word_count = 40   # Minimum word count                        \n", "num_workers = 4       # Number of threads to run in parallel\n", "context = 10          # Context window size                                                                                    \n", "downsampling = 1e-3   # Downsample setting for frequent words\n", "# Initialize and train the model (this will take some time)\n", "model = word2vec.Word2Vec(sentences_train, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n", "model.init_sims(replace=True) # marks the end of training to speed up the use of the model"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "f459fa67-e33a-4c8e-a3c6-0a3248a660bc", "_uuid": "6c503a77f930e3f35ebf8dbf8e448e792b419053", "collapsed": true}}, {"cell_type": "markdown", "source": ["## 7. Choose and train a classifier\n", "In this example we will use our trained word2vec model to get avarage word vectors from each sentence and train a neural network."], "metadata": {"_cell_guid": "e39bfd9e-c9a6-4b9e-b394-0172544134f4", "_uuid": "6a868d434c2888650079ea659fe1e98e54b14b7f"}}, {"source": ["def makeFeatureVec(words, model, num_features):\n", "    # Pre-initialize an empty numpy array (for speed)\n", "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n", "    #\n", "    nwords = 0\n", "    # \n", "    # Index2word is a list that contains the names of the words in \n", "    # the model's vocabulary. Convert it to a set, for speed \n", "    index2word_set = set(model.wv.index2word)\n", "    #\n", "    # Loop over each word in the review and, if it is in the model's\n", "    # vocaublary, add its feature vector to the total\n", "    for word in words:\n", "        if word in index2word_set: \n", "            nwords = nwords + 1\n", "            featureVec = np.add(featureVec, model[word])\n", "    # Divide the result by the number of words to get the average\n", "    if nwords == 0:\n", "        nwords = 1\n", "    featureVec = np.divide(featureVec, nwords)\n", "    return featureVec\n", "\n", "def getAvgFeatureVecs(reviews, model, num_features):\n", "    # Given a set of reviews (each one a list of words), calculate \n", "    # the average feature vector for each one and return a 2D numpy array \n", "    # Preallocate a 2D numpy array, for speed\n", "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n", "    counter = 0\n", "    # Loop through the reviews\n", "    for review in reviews:\n", "        # Call the function (defined above) that makes average feature vectors\n", "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n", "        counter = counter + 1\n", "    return reviewFeatureVecs\n", "\n", "f_matrix_train = getAvgFeatureVecs(sentences_train, model, num_features)\n", "f_matrix_test = getAvgFeatureVecs(sentences_test, model, num_features)\n", "# we have to train 6 different models with 6 different Y labels\n", "y = [train['toxic'], train['severe_toxic'], train['obscene'], train['threat'], train['insult'], train['identity_hate']]\n"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "c28384f6-7536-4299-8627-bb48a1e2944c", "_uuid": "5e7d762b95fc896e90b582010629e88285cea0ce", "collapsed": true}}, {"cell_type": "markdown", "source": ["We create 6 models, one for each toxic level each."], "metadata": {"_cell_guid": "93309f79-5eaf-4413-84ff-de139f0152f2", "_uuid": "19d631489bd05081e531bfb6b09e49f6fd831a0c"}}, {"source": ["# create 6 MLP models\n", "model = []\n", "for i in range(0, 6):\n", "    m = MLPClassifier(solver='adam', hidden_layer_sizes=(30,30,30), random_state=1)\n", "    model.append(m)\n", "print(model)\n"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "0ec61191-0271-4ef9-863f-db19477780a4", "_uuid": "fbff5122e065a2f93838df68e8c4c8e9186e060b", "collapsed": true}}, {"cell_type": "markdown", "source": ["Now train the models with a partial fit approach"], "metadata": {"_cell_guid": "dcea7134-bb2b-4cc5-b513-0630e4a307e7", "_uuid": "aa8970521313b043256e48d001178f71fff4f1f0"}}, {"source": ["batch_size = 10000\n", "total_rows = f_matrix_train.shape[0]\n", "duration = 0\n", "start_train = time.time()\n", "pos = 0\n", "classes = [0,1]\n", "# we use a partial fit approach\n", "while duration < 2500 and pos < total_rows:\n", "    for i in range(0, 6):\n", "        if pos+batch_size > total_rows:\n", "            batch_size = total_rows-pos\n", "        X_p = f_matrix_train[pos:pos+batch_size]\n", "        y_p = y[i][pos:pos+batch_size]\n", "        model[i].partial_fit(X_p, y_p, classes)\n", "    pos = pos + batch_size\n", "    duration = time.time() - start_train # how long did we train so far?\n", "    print(\"Pos %d/%d duration %d\" % (pos, total_rows, duration))\n", "    # end test partial fit  "], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "334f80f9-e073-4cd5-acac-eec2a224ba23", "_uuid": "3acf97a8e33feec66af9a70ca1446b933bbfc944", "collapsed": true}}, {"cell_type": "markdown", "source": ["Now predict the result for each toxic level"], "metadata": {"_cell_guid": "81f3cc04-2cd0-4704-bb8a-a9c97beb32cb", "_uuid": "206326029ddae3637fd4265deec94a750c24edf0"}}, {"source": ["result['toxic'] = model[0].predict_proba(f_matrix_test)[:,1]\n", "result['severe_toxic'] = model[1].predict_proba(f_matrix_test)[:,1]\n", "result['obscene'] = model[2].predict_proba(f_matrix_test)[:,1]\n", "result['threat'] = model[3].predict_proba(f_matrix_test)[:,1]\n", "result['insult'] = model[4].predict_proba(f_matrix_test)[:,1]\n", "result['identity_hate'] = model[5].predict_proba(f_matrix_test)[:,1]"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "b986d3f0-bf81-4f58-810c-06fae9a532fa", "_uuid": "54713cf6986129d050924e88f162a86772961888", "collapsed": true}}, {"cell_type": "markdown", "source": ["## 8. Write prediction into submission file\n", "Save the predicted values of all our 6 models into a submission csv file."], "metadata": {"_cell_guid": "3683d90b-ebe8-4dcc-9f7f-6515a84ba396", "_uuid": "e0472bd53dd5b0f8baadd668d58588f6d633a071"}}, {"source": ["result.to_csv('submission.csv', encoding='utf-8', index=False)"], "cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "b7be5e1c-e7b3-4138-acde-cc42628adc0b", "_uuid": "88b32565becb2c37660fdb5833631af2fcc78ec9", "collapsed": true, "_kg_hide-input": false, "_kg_hide-output": false}}, {"cell_type": "markdown", "source": ["Wow, you really read my notebook till the last line, congratulation :)\n", "\n", "My prediction is by far not a top ranking one but if you use any part of this notebook in a published kernel, credit (you can simply link back here) would be greatly appreciated.\n", "\n", "Sources:\n", "[Word2Vec introduction](https://www.kaggle.com/c/word2vec-nlp-tutorial), Great word2vec tutorial by [Angela Chapman](http://www.linkedin.com/pub/angela-chapman/5/330/b97)\n"], "metadata": {"_cell_guid": "a887bdfb-2412-44ac-b38d-db902667c590", "_uuid": "767602cca818bfd6ea1fa25bea7e24a3d7faafb6"}}], "nbformat_minor": 1, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}}, "nbformat": 4}