{"cells":[{"metadata":{"collapsed":true,"_uuid":"e3a9c13d6ec84f0efb4b7ccb8927e0776cf13024","_cell_guid":"0cd2d3b2-ef58-447b-b079-41c69855e603","trusted":false},"cell_type":"code","source":"# ideas and implementation are taken from tensorflow tutorial on word2vec at https://www.tensorflow.org/tutorials/word2vec","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os, re\nimport pickle\nimport collections\nimport random\nfrom time import time\nimport math\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.tsv\", sep = '\\t')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5c443459ef9a3df3d9414180fb9180ec3987052c","_cell_guid":"03537009-e464-41f6-b76d-ad7d9138d249","trusted":false},"cell_type":"code","source":"# perform some cleaning of the text fields: remove non-characters, make lower cases, splitting item category into main and sub categories\ndef clean(text):\n    return re.sub(r'[^\\w\\s]','',text)\ndef lower(text):\n    return text.lower()\n# general categories\ndef split_cat(text): # credit to https://www.kaggle.com/thykhuely\n    cats = text.split(\"/\")\n    if len(cats) >=3:\n        return cats[0:3]\n    else: return (\"No Label\", \"No Label\", \"No Label\") \n\nfor column in ['name', 'brand_name', 'item_description']:\n    df[column] = df[column].astype(str) \n    df[column] = df[column].apply(clean).apply(lower)\ndf['category_name'] = df['category_name'].astype(str).apply(lower)\ndf['general_cat'], df['subcat_1'], df['subcat_2'] = zip(*df['category_name'].apply(lambda x: split_cat(x)))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0fcba58d16b48932ccb1a4bf2d462a14e8efc569","_cell_guid":"2647001d-0538-49dc-b630-e64b688d2d01","trusted":false},"cell_type":"code","source":"# build a corpus from the text fields\n# the skip-gram model will be applied to this corpus\ncorpus = []\nfor row in range(len(df)):\n    for column in ['name', 'general_cat', 'subcat_1', 'subcat_2', 'brand_name', 'item_description']:\n        corpus += (df.loc[row, column].split())\nprint(*corpus[:150], sep = ' ')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"47f6e57726a928f1cc39551f99579d3c42047bc7","_cell_guid":"fc9f9a74-b163-4656-a178-558947627f81","trusted":false},"cell_type":"code","source":"# get the most frequent 50,000 words used in the corpus\n# map these words to integer indices\nvocabulary_size = 50000\ndef build_dataset(corpus, vocabulary_size):\n    count = [['UNK', -1]]\n    count.extend(collections.Counter(corpus).most_common(vocabulary_size - 1))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n        data = list()\n        unk_count = 0\n    for word in corpus:\n        index = dictionary.get(word, 0)\n        if index == 0:  # dictionary['UNK']\n            unk_count += 1\n        data.append(index)\n    count[0][1] = unk_count\n    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, dictionary, reversed_dictionary\ndata, count, dictionary, reverse_dictionary = build_dataset(corpus, vocabulary_size)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c025e9f2e49fd1d7a2f8478ef03e91c2dd2587fc","_cell_guid":"e411b16b-e2a0-498d-8f71-222d78bee8e3","trusted":false},"cell_type":"code","source":"# gauge the coverage of the corpus by the vocabulary\nsum = 0\nfor _, freq in count[1:]:\n    sum += freq\nprint(\"using the most frequent %5d words captures %2.2f percent of tokens in item descriptions\" \n      %(vocabulary_size,sum/len(corpus)*100))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f2dfa55e34624d6a1de4874f925c1b8c95541fb9","_cell_guid":"85bff97c-631b-47d3-9c14-badfc031af05","trusted":false},"cell_type":"code","source":"print('Most common words (+UNK)', count[:10])\nprint('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b7c4ead0ba39fdf495342a69d738ed31cc9224d9","_cell_guid":"8f225210-2416-41a0-8e0c-cf1aed94bf65","trusted":false},"cell_type":"code","source":"# function to generate a training batch for the skip-gram model.\ndata_index = 0\ndef generate_batch(batch_size, num_skips, skip_window):\n  global data_index\n  assert batch_size % num_skips == 0\n  assert num_skips <= 2 * skip_window\n  if data_index == len(data):\n      data_index = 0\n  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n  buffer = collections.deque(maxlen=span)\n  if data_index + span > len(data):\n    data_index = 0\n  buffer.extend(data[data_index:data_index + span])\n  data_index += span\n  for i in range(batch_size // num_skips):\n    context_words = [w for w in range(span) if w != skip_window]\n    words_to_use = random.sample(context_words, num_skips)\n    for j, context_word in enumerate(words_to_use):\n      batch[i * num_skips + j] = buffer[skip_window]\n      labels[i * num_skips + j, 0] = buffer[context_word]\n    if data_index == len(data):\n        print(\"exceeded\")\n        print(data_index)\n        for word in data[:span]:\n            buffer.append(word)\n        data_index = span\n        print(\"new data_index is set to: \", data_index)\n    else:\n      buffer.append(data[data_index])\n      data_index += 1\n  # Backtrack a little bit to avoid skipping words in the end of a batch\n  data_index = (data_index + len(data) - span) % len(data)\n  return batch, labels","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"54ec888e416379e396be195802a7e379dec1526b","_cell_guid":"b2a462c7-a51d-46a0-9f30-42a433e33080","trusted":false},"cell_type":"code","source":"batch, labels = generate_batch(batch_size=128, num_skips=2, skip_window=2)\nfor i in range(10):\n  print(batch[i], reverse_dictionary[batch[i]],\n        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\nprint(data_index, len(data))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"fbed688cd462a3366241338f24f056f6338871be","_cell_guid":"a9f3a032-35b0-4c43-af4c-72f91544ce37","trusted":false},"cell_type":"code","source":"# build and train a skip-gram model.\n\nbatch_size = 128\nembedding_size = 50  # Dimension of the embedding vector.\nskip_window = 5       # How many words to consider left and right.\nnum_skips = 8         # How many times to reuse an input to generate a label.\nnum_sampled = 64      # Number of negative examples to sample.\n\n# We pick a random validation set to sample nearest neighbors. Here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent. These 3 variables are used only for\n# displaying model accuracy, they don't affect calculation.\nvalid_size = 10    # Random set of words to evaluate similarity on.\nvalid_window = 500  # Only pick dev samples in the head of the distribution.\nvalid_examples = np.random.choice(valid_window, valid_size, replace=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5e17be2a6e2b2408520c7cf6bc5562a17e334e72","_cell_guid":"194f10b4-cf74-423f-9e49-87dfcdf2e5a4","trusted":false},"cell_type":"code","source":"graph = tf.Graph()\nwith graph.as_default():\n    with tf.device('/gpu:0'):\n        # Input data.\n        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n        # Construct the variables for the NCE loss\n        nce_weights = tf.Variable(\n            tf.truncated_normal([vocabulary_size, embedding_size],\n                                stddev=1.0 / math.sqrt(embedding_size)))\n        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n        # Compute the average NCE loss for the batch.\n        loss = tf.reduce_mean(\n          tf.nn.nce_loss(weights=nce_weights,\n                         biases=nce_biases,\n                         labels=train_labels,\n                         inputs=embed,\n                         num_sampled=num_sampled,\n                         num_classes=vocabulary_size))\n\n        # Construct the SGD optimizer using a learning rate of 1.0.\n        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n        # Compute the cosine similarity between minibatch examples and all embeddings.\n        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n        normalized_embeddings = embeddings / norm\n        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n        similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n        init = tf.global_variables_initializer()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"84c1780d575be87f09d31fda3102eef8c27155a9","_cell_guid":"2364ab0a-a942-4efb-94a8-7bae5d8a2428","trusted":false},"cell_type":"code","source":"num_steps = 10**6\ncheck_N = 10**4\nwith tf.Session(graph=graph) as session:\n    init.run()\n    average_loss = 0\n    time_0 = time()\n    for step in range(num_steps):\n        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += loss_val\n        if step % check_N == 0:\n            if step > 0:\n                average_loss /= check_N\n            # The average loss is an estimate of the loss over the last check_N batches.\n            print('Average loss at step ', step, ': ', average_loss, 'time: %2.2f' %(time()-time_0))\n            average_loss = 0\n            time_0 = time()\n\n        if step % 10**4 == 0 and step > 0:\n            sim = similarity.eval()\n            for i in range(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8  # number of nearest neighbors\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = 'Nearest to %s:' % valid_word\n                for k in range(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = '%s %s,' % (log_str, close_word)\n                print(log_str)\n    print('done training')\n    final_embeddings = normalized_embeddings.eval()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5a3767ab41787ac4ac181f08677d9f413c123489","_cell_guid":"ee6c1795-da85-4c26-b0ba-a9e113e0ba02","trusted":false},"cell_type":"code","source":"# visualize the embeddings, looking at the most frequent 500 words\nfrom sklearn.manifold import TSNE\ntsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\nplot_only = 500\nlow_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0db83ecc30c8cb6334513ccfebaea546867e3b5c","_cell_guid":"adb4a220-aab5-4811-bf35-2ca4b78dc584","trusted":false},"cell_type":"code","source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nlabels = [reverse_dictionary[i] for i in range(plot_only)]\nx_plot = [low_dim_embs[i, :][0] for i, label in enumerate(labels)]\ny_plot = [low_dim_embs[i, :][1] for i, label in enumerate(labels)]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"fb9717a8dfd203ed5f397bd0eb2090f70daecc2d","_cell_guid":"4264eb5e-1230-4eb7-9a0d-7c927dec672b","trusted":false},"cell_type":"code","source":"trace1 = go.Scatter(\n    x = x_plot,\n    y = y_plot,\n    mode='markers+text',\n    name='Markers and Text',\n    text=labels,\n    textposition='top'\n)\ndata = [trace1]\nlayout = go.Layout(\n    showlegend=False,\n    title = \"visualizing word embeddings\",\n    xaxis = {\"visible\": False},\n    yaxis = {\"visible\": False}\n)\nfig = go.Figure(data=data, layout=layout)\nplot = py.iplot(fig)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}