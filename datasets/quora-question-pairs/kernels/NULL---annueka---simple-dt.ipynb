{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "13d3633c-304b-73b6-42ed-095405821ce4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import  display\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "pd.set_option('display.max_colwidth',-1)\n",
        "import textblob as tb\n",
        "import nltk as nl\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import ensemble\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e9536a7c-d257-f015-b53b-da52114e6018"
      },
      "outputs": [],
      "source": [
        "# let's define union-find function\n",
        "def indices_dict(lis):\n",
        "    d = defaultdict(list)\n",
        "    for i,(a,b) in enumerate(lis):\n",
        "        d[a].append(i)\n",
        "        d[b].append(i)\n",
        "    return d\n",
        "\n",
        "def disjoint_indices(lis):\n",
        "    d = indices_dict(lis)\n",
        "    sets = []\n",
        "    while len(d):\n",
        "        que = set(d.popitem()[1])\n",
        "        ind = set()\n",
        "        while len(que):\n",
        "            ind |= que \n",
        "            que = set([y for i in que \n",
        "                         for x in lis[i] \n",
        "                         for y in d.pop(x, [])]) - ind\n",
        "        sets += [ind]\n",
        "    return sets\n",
        "\n",
        "def disjoint_sets(lis):\n",
        "    return [set([x for i in s for x in lis[i]]) for s in disjoint_indices(lis)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cba2337c-bd72-a19b-a04c-0b8d39aafb68"
      },
      "outputs": [],
      "source": [
        "train_df=pd.read_csv('../input/train.csv')\n",
        "# only duplicated questions\n",
        "ddf=train_df[train_df.is_duplicate==1]\n",
        "print('Duplicates questions shape:',ddf.shape)\n",
        "# get all duplicated questions\n",
        "clean_ddf1=ddf[['qid1','question1']].drop_duplicates()\n",
        "clean_ddf1.columns=['qid','question']\n",
        "clean_ddf2=ddf[['qid2','question2']].drop_duplicates()\n",
        "clean_ddf2.columns=['qid','question']\n",
        "all_dqdf=clean_ddf1.append(clean_ddf2,ignore_index=True)\n",
        "print(all_dqdf.shape)\n",
        "# groupby qid1, and then we get all the combinations of id in each group\n",
        "dqids12=ddf[['qid1','qid2']]\n",
        "df12list=dqids12.groupby('qid1', as_index=False)['qid2'].agg({'dlist':(lambda x: list(x))})\n",
        "print(len(df12list))\n",
        "d12list=df12list.values\n",
        "d12list=[[i]+j for i,j in d12list]\n",
        "# get all the combinations of id, like (id1,id2)...\n",
        "d12ids=set()\n",
        "for ids in d12list:\n",
        "    ids_len=len(ids)\n",
        "    for i in range(ids_len):\n",
        "        for j in range(i+1,ids_len):\n",
        "            d12ids.add((ids[i],ids[j]))\n",
        "print(len(d12ids))\n",
        "# the same operation of qid2\n",
        "dqids21=ddf[['qid2','qid1']]\n",
        "display(dqids21.head(2))\n",
        "df21list=dqids21.groupby('qid2', as_index=False)['qid1'].agg({'dlist':(lambda x: list(x))})\n",
        "print(len(df21list))\n",
        "ids2=df21list.qid2.values\n",
        "d21list=df21list.values\n",
        "d21list=[[i]+j for i,j in d21list]\n",
        "d21ids=set()\n",
        "for ids in d21list:\n",
        "    ids_len=len(ids)\n",
        "    for i in range(ids_len):\n",
        "        for j in range(i+1,ids_len):\n",
        "            d21ids.add((ids[i],ids[j]))\n",
        "len(d21ids)\n",
        "# merge two set\n",
        "dids=list(d12ids | d21ids)\n",
        "len(dids)\n",
        "# split data into groups, so that each question in each group are duplicated\n",
        "did_u=disjoint_sets(dids)\n",
        "new_dids=[]\n",
        "for u in did_u:\n",
        "    new_dids.extend(list(combinations(u,2)))\n",
        "len(new_dids)\n",
        "new_ddf=pd.DataFrame(new_dids,columns=['qid1','qid2'])\n",
        "print('New duplicated shape:',new_ddf.shape)\n",
        "display(new_ddf.head(2))\n",
        "# merge with all_dqdf to get question1 description\n",
        "new_ddf=new_ddf.merge(all_dqdf,left_on='qid1',right_on='qid',how='left')\n",
        "new_ddf.drop('qid',inplace=True,axis=1)\n",
        "new_ddf.columns=['qid1','qid2','question1']\n",
        "new_ddf.drop_duplicates(inplace=True)\n",
        "print(new_ddf.shape)\n",
        "new_ddf.head(2)\n",
        "# the same operation with qid2\n",
        "new_ddf=new_ddf.merge(all_dqdf,left_on='qid2',right_on='qid',how='left')\n",
        "new_ddf.drop('qid',inplace=True,axis=1)\n",
        "new_ddf.columns=['qid1','qid2','question1','question2']\n",
        "new_ddf.drop_duplicates(inplace=True)\n",
        "print(new_ddf.shape)\n",
        "new_ddf.head(2)\n",
        "# is_duplicate flag\n",
        "new_ddf['is_duplicate']=1\n",
        "new_ddf.head(2)\n",
        "# let random select 10 rows to check the result\n",
        "new_ddf.sample(10)\n",
        "# the orininal duplicated pairs count:\n",
        "print(len(all_dqdf))\n",
        "# after we generate more data, then the duplicated pairs count:\n",
        "print(len(new_ddf))\n",
        "trainDF = new_ddf.append(train_df[train_df.is_duplicate==0].drop('id', 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b3433b11-3bee-4a22-9085-cccc8caee2f7"
      },
      "outputs": [],
      "source": [
        "#len(trainDF['question2'])\n",
        "len(trainDF['question1'].dropna())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "426d4f11-5a6d-9fe5-69f8-3f2f2a8dfb82"
      },
      "outputs": [],
      "source": [
        "#building a vocabulary\n",
        "vect = CountVectorizer()\n",
        "questions = trainDF['question1'].append(trainDF['question2'])\n",
        "q = vect.fit_transform(questions.values.astype('U'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e052b9a-05a0-d54e-7244-b8d4cd7f23e0"
      },
      "outputs": [],
      "source": [
        "#bulding training data\n",
        "q1 = vect.transform(trainDF['question1'].values.astype('U'))\n",
        "q2 = vect.transform(trainDF['question2'].values.astype('U'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dde6fa3f-ba4c-54c0-4556-feb760b27ff3"
      },
      "outputs": [],
      "source": [
        "#Training a decision tree on distance of q1-q2 matrices\n",
        "X_train = q1-q2\n",
        "Y_train = trainDF['is_duplicate'].values\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
        "dtree = tree.DecisionTreeClassifier(max_depth = 10)\n",
        "dtree = dtree.fit(x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2cfd8f94-9a7b-c76a-7635-a28f994a820c"
      },
      "outputs": [],
      "source": [
        "dtree.score(x_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "745957db-e2e1-c483-6273-51045c2e91d8"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "vect = CountVectorizer(ngram_range = (1,2), token_pattern=r'\\b\\w+\\b', \\\n",
        "                       analyzer = 'word', encoding='ascii',strip_accents = 'ascii',\\\n",
        "                       tokenizer=LemmaTokenizer()) \n",
        "\n",
        "c = vect.fit_transform(['Bi-grams are cool?', 'more hi.', 'kake metros'])\n",
        "c.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7ef12061-a544-c04c-cc89-d23f3fe70083"
      },
      "outputs": [],
      "source": [
        "\n",
        "len(['bi-grams',\n",
        " 'are',\n",
        " 'cool',\n",
        " '?',\n",
        " 'more',\n",
        " 'hi',\n",
        " 'kake',\n",
        " 'metro',\n",
        " 'bi-grams are',\n",
        " 'are cool',\n",
        " 'cool ?',\n",
        " '? more',\n",
        " 'more hi',\n",
        " 'hi kake',\n",
        " 'kake metro'])"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}