{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport nltk\nimport nltk.sentiment\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, cohen_kappa_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.svm import LinearSVC\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.tsv', sep='\\t')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"146bc300d7f801504162864436a35131de257ae2"},"cell_type":"markdown","source":"## EDA"},{"metadata":{"_uuid":"790076c242213e3d2fedd9da769f42ab3a83314f"},"cell_type":"markdown","source":"** The Target Class Distribution **\n"},{"metadata":{"trusted":true,"_uuid":"c18c8e8388e8fb033f57d711564030d95de17195"},"cell_type":"code","source":"sns.countplot(x='Sentiment', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f097950933573e1123569cbbfdd2c9fa58e071b7"},"cell_type":"markdown","source":"**  Word Count against Sentiment **"},{"metadata":{"trusted":true,"_uuid":"e6f48329f6bb8710a972651974c529bb21d1278e"},"cell_type":"code","source":"train['word_count'] = train['Phrase'].apply(lambda x: len(x.split()))\nsns.boxplot(x='Sentiment', y='word_count', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39602b84395a4cb5abbe9c553c063b1ab68d3a0d"},"cell_type":"markdown","source":"** Average Word Length versus Sentiment **"},{"metadata":{"trusted":true,"_uuid":"a9e7caf4b13bcbd4d14667be1e27a9ab46b7cf5b"},"cell_type":"code","source":"train['avg_word_lenght'] = train['Phrase'].apply(lambda x:np.mean([len(word) for word in x.split()]))\nsns.boxplot(x='Sentiment', y='avg_word_lenght', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1ce1151c08426628356f49bb22ea928755eefbb"},"cell_type":"markdown","source":"## Base Model"},{"metadata":{"_uuid":"80f7e3fd23ec6828c4251e78175984da2e9abb76"},"cell_type":"markdown","source":"\n**Splitting data into train, validation sets** using GroupShuffleSplit using sentieceId to split on it  instead of  train_test_split as the sntence is split into different phrases and phrases sentiment may vary so I think it is better to to evaluate the model using complete sentences in the train or the validation."},{"metadata":{"trusted":true,"_uuid":"a56cc3de86e8ac7b7d8257b6ef3f646e2eaef51b"},"cell_type":"code","source":"group_split = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\ntrain_index, validation_index = list(group_split.split(train['PhraseId'],\n                                    y=train['Sentiment'], groups=train['SentenceId']))[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac417b198baa81c7e3e0f39d72f5593b5be00099"},"cell_type":"code","source":"train, validation = train.iloc[train_index], train.iloc[validation_index]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c660dc03e41b3246e417bbdc9505f18786e30ad"},"cell_type":"markdown","source":"**Logistic Regression Model**"},{"metadata":{"_uuid":"6d11cfc24f5dee230d70a5fd4121fd6d77a37e69"},"cell_type":"markdown","source":"* **using count features**"},{"metadata":{"trusted":true,"_uuid":"137762a722a7f91e91680c799ab5067fbf97ce2b"},"cell_type":"code","source":"log_reg_countvec_pl = Pipeline([\n        ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3], stop_words=nltk.corpus.stopwords.words('english'))),\n        ('clf',LogisticRegression())\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be904dcbe0dc2f6dfc1fb28e7898d6b73e88b55f"},"cell_type":"code","source":"log_reg_countvec_pl.fit(train['Phrase'], train['Sentiment'])\naccuracy_score(validation['Sentiment'], log_reg_countvec_pl.predict(validation['Phrase']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a99635781e5bbfe2c6236f6b2d238fb27ad95feb"},"cell_type":"markdown","source":" ** Adding negation Tag to the words **"},{"metadata":{"trusted":true,"_uuid":"fa1cdb78c90b0f8a762bb75428dfbc15705932fb"},"cell_type":"code","source":"#concating tran and validation to calculate the features only once\ndata = pd.concat([train, validation], keys=['train', 'validation'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"626d16f5659a6b8efefc6925cf2a4d7870820c51"},"cell_type":"code","source":"#tokenizing the phrases to use it as input to calculate another features\ndata['tokenized_words'] = data['Phrase'].apply(nltk.word_tokenize)\n# mark words after negation word with _NEG tag\ndata['negated_phrase_tokenized'] = data['tokenized_words'].apply(nltk.sentiment.util.mark_negation)\ndata['negated_phrase'] = data['negated_phrase_tokenized'].apply(lambda x: \" \".join(x))\n#returns 1 if the text contains negation word\ndata['negated_flag'] = (data['tokenized_words'].apply(nltk.sentiment.vader.negated)).astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bba19a17f62c9660ce3df54064c10c7f34d72676"},"cell_type":"code","source":"# get_numeric_data = preprocessing.FunctionTransformer(lambda a: a[['negated_flag']], validate=False)\nget_text_data = preprocessing.FunctionTransformer(lambda a: a['negated_phrase'], validate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50d038b44fb4a8d76644a05b7d1ea1a95d34b831"},"cell_type":"markdown","source":"** Logistic Regression with Negation Marked Text **"},{"metadata":{"trusted":true,"_uuid":"167a53434cc79f110ccf369682d535fe48ddcf01"},"cell_type":"code","source":"lg_pl = Pipeline([\n        ('union', FeatureUnion( #unites both text and numeric arrays into one array\n            transformer_list = [\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n             ]\n        )), \n        ('clf',LogisticRegression(penalty ='l1'))\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c1250a74c5347c4bf4d6048e5d80f9d02b250d5"},"cell_type":"code","source":"lg_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])\naccuracy_score(data.loc['validation']['Sentiment'], lg_pl.predict(data.loc['validation']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b45f2aae4820bb06d62a41dffda4df2a6d230bd"},"cell_type":"markdown","source":"** Adding Character N-Gram **"},{"metadata":{"trusted":true,"_uuid":"41e80602d62fd164cab635ef884bec14a8b6c9b1"},"cell_type":"code","source":"lg_pl = Pipeline([\n        ('union', FeatureUnion( #unites both text and numeric arrays into one array\n            transformer_list = [\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('tfidf', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('char_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='char',ngram_range=[3,5], \\\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n             ]\n        )), \n        ('clf',LogisticRegression(penalty ='l1'))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb9a8e0d3688c51d740c4a1b8c83ada8932be8b3"},"cell_type":"code","source":"lg_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])\naccuracy_score(data.loc['validation']['Sentiment'], lg_pl.predict(data.loc['validation']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9d73d3799114a77b9e7d94ec9692bd0be544754"},"cell_type":"markdown","source":"## Stacking Different Models"},{"metadata":{"trusted":true,"_uuid":"678278758848d078d5e0eed50eb324f9007c3adc"},"cell_type":"code","source":"train_data = data.loc['train']\ntrain_index, validation_index = list(group_split.split(train_data['PhraseId'],\n                                    y=train_data['Sentiment'], groups=train_data['SentenceId']))[0]\ntrain, validation_stack = train_data.iloc[train_index], train_data.iloc[validation_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2938f9be72c51eeab37ddf4da8e860588b923e1"},"cell_type":"code","source":"nb_pl = Pipeline([\n        ('union', FeatureUnion( #unites both text and numeric arrays into one array\n            transformer_list = [\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('tfidf', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('char_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='char',ngram_range=[3,5], \\\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n             ]\n        )), \n        ('clf',MultinomialNB())\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"777a93df2ef58169abc31408d688dc52b6c34ed1"},"cell_type":"code","source":"nb_pl.fit(train, train['Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a78c607d30b0b0a5bddf875ca18ef788d35b4124"},"cell_type":"code","source":"svc_pl = Pipeline([\n        ('union', FeatureUnion( #unites both text and numeric arrays into one array\n            transformer_list = [\n                ('text_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', CountVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('tfidf', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='word',ngram_range=[1,3],\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n                ('char_features', Pipeline([\n                    ('selector', get_text_data),\n                    ('vectorizer', TfidfVectorizer(analyzer='char',ngram_range=[3,5], \\\n                                                   stop_words=nltk.corpus.stopwords.words('english')))\n                ])),\n             ]\n        )), \n        ('clf',LinearSVC())\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70c5a266ffe6ea7e5e9eee85cd30355e9341d574"},"cell_type":"code","source":"svc_pl.fit(train, train['Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e004470bd482bf353c61410d07583dbd8f084a6"},"cell_type":"markdown","source":"**Combinng the propabilities of the three models**"},{"metadata":{"trusted":true,"_uuid":"ea89807c787c61061d111767a8fae70784008fdf"},"cell_type":"code","source":"lg_stack = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d6cb246f90298851a348fb959acf73da277a380"},"cell_type":"code","source":"lg_stack.fit(\n    np.column_stack(\n        (\n            nb_pl.predict_proba(validation_stack),\n            lg_pl.predict_proba(validation_stack),\n            svc_pl.decision_function(validation_stack)\n        )\n    )\n    ,validation_stack['Sentiment']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"613858e88f09c8e1dd03bf920f9411acd4ed6add"},"cell_type":"code","source":"accuracy_score(data.loc['validation']['Sentiment'], lg_stack.predict(\n    np.column_stack(\n        (\n            nb_pl.predict_proba(data.loc['validation']),\n            lg_pl.predict_proba(data.loc['validation']),\n            svc_pl.decision_function(data.loc['validation'])\n\n            \n        )\n    )\n))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"818c7d407ead0b9446d9264d84272f036a3e0155"},"cell_type":"markdown","source":"## Test Predictions "},{"metadata":{"trusted":true,"_uuid":"88b9f8f5603fa2af5df4e0f540b602057a27535f"},"cell_type":"code","source":"test = pd.read_csv('../input/test.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92fab935e6b0f1566aa7cba6bf5b76288cc797b8"},"cell_type":"code","source":"#tokenizing the phrases to use it as input to calculate another features\ntest['tokenized_words'] = test['Phrase'].apply(nltk.word_tokenize)\n# mark words after negation word with _NEG tag\ntest['negated_phrase_tokenized'] = test['tokenized_words'].apply(nltk.sentiment.util.mark_negation)\ntest['negated_phrase'] = test['negated_phrase_tokenized'].apply(lambda x: \" \".join(x))\n#returns 1 if the text contains negation word\ntest['negated_flag'] = (test['tokenized_words'].apply(nltk.sentiment.vader.negated)).astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdcf970472e04d9d8e7f1b5063fe419d7f4c200a"},"cell_type":"markdown","source":"** Retraining with the complete train data **"},{"metadata":{"trusted":true,"_uuid":"8e2f787ec370289ff92159026faaeab4c4388b29"},"cell_type":"code","source":"lg_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])\nnb_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])\nsvc_pl.fit(data.loc['train'], data.loc['train']['Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"518daa7ee6fde14d84621ba6579de7d93e06a513"},"cell_type":"code","source":"lg_stack.fit(\n    np.column_stack(\n        (\n            nb_pl.predict_proba(data.loc['validation']),\n            lg_pl.predict_proba(data.loc['validation']),\n            svc_pl.decision_function(data.loc['validation'])\n        )\n    )\n    ,data.loc['validation']['Sentiment']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"279f998802691d8ecc935fae54bac9b15daa9ee5"},"cell_type":"code","source":"test['Sentiment'] = lg_stack.predict(\n                        np.column_stack(\n                            (\n                                nb_pl.predict_proba(test),\n                                lg_pl.predict_proba(test),\n                                svc_pl.decision_function(test)\n                            )\n                        )\n                    )","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"trusted":true,"_uuid":"4715d972c333a6ac6f4f5866fcd105e8881cb838"},"cell_type":"code","source":"test[['PhraseId', \"Sentiment\"]].to_csv('test_predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfbef45ce2a6292e1804f8a834d198d1e8b861be"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}