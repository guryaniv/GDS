{"cells":[{"metadata":{"_uuid":"55f65da35225cce58f6caaaa654a11eb70f354c1"},"cell_type":"markdown","source":"# Analyzing Home Credit Default Risk"},{"metadata":{"_uuid":"2a25ebc8a5dc27d81b9e2e21c6cd6e66dbc99791"},"cell_type":"markdown","source":"This notebook briefly walks through my process of analyzing Home Credit Default Risk to build a model to predict whether a borrower will face repayment difficulties in a given loan."},{"metadata":{"_uuid":"269ef37077f459baf21d64faffb53ca6c03ac3eb"},"cell_type":"markdown","source":"## Loading Packages and Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load data\n# submit = pd.read_csv('../input/sample_submission.csv')\ncash = pd.read_csv('../input/POS_CASH_balance.csv')\nbureau_bal = pd.read_csv('../input/bureau_balance.csv')\ncard = pd.read_csv('../input/credit_card_balance.csv')\nbureau = pd.read_csv('../input/bureau.csv')\ntrain = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')\nprevious = pd.read_csv('../input/previous_application.csv')\ninstallment = pd.read_csv('../input/installments_payments.csv')\nprint ('Done!')\nprint (cash.shape, bureau_bal.shape, card.shape, bureau.shape, train.shape, test.shape, previous.shape, installment.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c7b4d63bd788bbc8365d57cc7c123459b507f60"},"cell_type":"markdown","source":"## Inspecting the Data\n\n### Train and Test Datasets"},{"metadata":{"trusted":true,"_uuid":"ff3deb3fa5c9d785afb9c10eea0319520216c2d6"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"356677575129f24b92d7f0d7216daac60aab024e"},"cell_type":"code","source":"mean_def = train['TARGET'].mean()\nprint (\"%.2f%% of the loans in the training data have repayment difficulties.\" %(mean_def*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5eb2758d427ba824980aca6266de50401488c263"},"cell_type":"code","source":"na = train.isnull().sum()/len(train)\nna.sort_values(ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caf88fc27057023f67c81700e80cb63fc70b90ef"},"cell_type":"markdown","source":"The variables with most missing values have almost 70% missing.\n\nHere we make some plots to inspect the the proportion of repayment difficulties among different variables. Firstly the bar charts plotting average proportion of loans with payment difficulties across classes of certain variables:"},{"metadata":{"trusted":true,"_uuid":"61bfd263ac8949b2d822d07dff0844bf6a088b2f"},"cell_type":"code","source":"plt.figure(figsize=(6,4))\n\ndef plot_bar(var):\n    train.groupby([var]).mean()['TARGET'].sort_values().plot.barh(color='blue', alpha=0.7)\n    plt.axvline(mean_def, color='orange', linewidth=4)\n        \nplot_bar('CODE_GENDER')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a31e68134c13dd5b1ba6bcd19ac5bdbf9b71e3fd"},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nplot_bar('OCCUPATION_TYPE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d82749549dc2c5343ae1f1e1d19d7b1308d35d34"},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nplot_bar('NAME_EDUCATION_TYPE')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d82a824da596418ab964c082c0730087f716c492"},"cell_type":"markdown","source":"For numerical variables, we plot histograms with kde grouped by payments with or without difficulties. For some variables in days, we can transform into years:"},{"metadata":{"trusted":true,"_uuid":"dc293ba54c3be860f6cfd5a06e5b496eab8d5233"},"cell_type":"code","source":"def plot_kde(var, annualize=False):\n    sns.kdeplot(train.loc[train['TARGET'] == 0, var] / (-365 if annualize else 1), label = 'No repayment difficulties')\n    sns.kdeplot(train.loc[train['TARGET'] == 1, var] / (-365 if annualize else 1), label = 'With repayment difficulties')\n\nplot_kde('DAYS_BIRTH', annualize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"689675ea8fb3253b1eec117a326a9e077f1f0cad"},"cell_type":"code","source":"plt.figure(figsize=(13,10))\nplt.subplot(221)\nplot_kde('AMT_INCOME_TOTAL')\nplt.title('Income')\nplt.subplot(222)\nplot_kde('AMT_CREDIT')\nplt.title('Credit')\nplt.subplot(223)\nplot_kde('AMT_ANNUITY')\nplt.title('Annuity')\nplt.subplot(224)\nplot_kde('AMT_GOODS_PRICE')\nplt.title('Price of Goods')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a50018206259a481a5ee125cafaf0eb3ff21233"},"cell_type":"markdown","source":"Let's look at the top values of income:"},{"metadata":{"trusted":true,"_uuid":"764078fe8b5e9ba33fbe603b6642b63c1960eb29"},"cell_type":"code","source":"train.loc[:,['SK_ID_CURR','TARGET','AMT_INCOME_TOTAL']].sort_values('AMT_INCOME_TOTAL', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"124c09db89f73d48a103b7bf0c2b4b0cb8fbb557"},"cell_type":"markdown","source":"It is quite unimaginable that a borrower have 117 million of income AND defaulted on his/her loan. It is better to treat it as data error and regarded as NA."},{"metadata":{"trusted":true,"_uuid":"016d44903910c345e66e35c67513ed4f969c57a1"},"cell_type":"code","source":"train['AMT_INCOME_TOTAL'].replace({117000000: train['AMT_INCOME_TOTAL'].median()}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06910c286b73214cc8b4f963187717aa09678112"},"cell_type":"code","source":"train.loc[train.TARGET==1, ['SK_ID_CURR','TARGET','AMT_INCOME_TOTAL']].sort_values('AMT_INCOME_TOTAL').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cc225907d4f70d6e770a338b8117ecf5027b1b2"},"cell_type":"markdown","source":"Looking at distribution, these variables should be transformed to log before going to machine learning models."},{"metadata":{"trusted":true,"_uuid":"0ed5f1d7f9daeb0a11ca88b7c2b4479c9a57258f"},"cell_type":"code","source":"plt.figure(figsize=(13,10))\nplt.subplot(221)\nplot_kde('DAYS_BIRTH', annualize = True)\nplt.title('Birth')\nplt.subplot(222)\nplot_kde('DAYS_EMPLOYED', annualize = True)\nplt.title('Employed')\nplt.subplot(223)\nplot_kde('DAYS_REGISTRATION', annualize = True)\nplt.title('Registration')\nplt.subplot(224)\nplot_kde('DAYS_ID_PUBLISH', annualize = True)\nplt.title('ID Publish')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"452b461a88aee671712a43ba1cf63d879fb4b334"},"cell_type":"code","source":"train['DAYS_EMPLOYED'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5aed4cd060f66730a1d0dde984c839cd86641d92"},"cell_type":"markdown","source":"55374 records in 'DAYS_EMPLOYED' have value 365243, which means they are employed by nearly 1000 years? Not possible. They need to be transformed. I choose to replace it with 0."},{"metadata":{"trusted":true,"_uuid":"086e5dd0d7099fc8bb44fdd25acfce8c2d16bbde"},"cell_type":"code","source":"train['DAYS_EMPLOYED'].replace({365243: 0}, inplace = True)\ntest['DAYS_EMPLOYED'].replace({365243: 0}, inplace = True)\nplot_kde('DAYS_EMPLOYED', annualize = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58516684e3e97c66f0d6dce79b4ba565016a2663"},"cell_type":"markdown","source":"### Other Datasets\nBureau Data:"},{"metadata":{"trusted":true,"_uuid":"936950c65705a209ba2bade23447202b7f8ba779"},"cell_type":"code","source":"bureau.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b36fb75f6b573333253a56d6ab8a9a56024eaf9"},"cell_type":"markdown","source":"It looks like that CREDIT_DAYS_OVERDUE and CNT_CREDIT_PROLONG are indications the the loans are problematic. They should be good input features."},{"metadata":{"_uuid":"fd96de0037f972ef0f2fa2611eb03d53e3c11477"},"cell_type":"markdown","source":"Bureau Balance:"},{"metadata":{"trusted":true,"_uuid":"87bc4db7cba4031a2d379ba14730e89fb31a9c13"},"cell_type":"code","source":"bureau_bal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2847f2b624caf39d8836e2d798247c7b18965c57"},"cell_type":"code","source":"bureau_bal.STATUS.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"649f46018b0e99d8c4f1acec0101681b88a1dfbd"},"cell_type":"markdown","source":"Status 1-5 refers to DPD (days past due), 1 means 1-30 days, 2 means 31-60 days and so on. 5 means DPD 120+ or sold or written off."},{"metadata":{"_uuid":"7971c4d89d28bf50159fe013fc8ed55c87cedec0"},"cell_type":"markdown","source":"POS Cash Balance:"},{"metadata":{"trusted":true,"_uuid":"d6d8057d84b09d401b805ae8388eb54a253f6426"},"cell_type":"code","source":"cash.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6513044edd207aeb21cc3e6515d6f23f6ead50a9"},"cell_type":"markdown","source":"We will use SK_DPD_DEF (day past due with tolerance) for features"},{"metadata":{"_uuid":"d97e246c152560da37c39f2d9acb6b976db2a834"},"cell_type":"markdown","source":"Credit Card Balance"},{"metadata":{"trusted":true,"_uuid":"5d16079b8b9fa144845110d85824103a41125449"},"cell_type":"code","source":"card.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"444ee6986fdb274750fcfbb9963be4d561e63c8c"},"cell_type":"markdown","source":"Similarly, SK_DPD_DEF will be used."},{"metadata":{"_uuid":"31f16fd7c3e0f932999decbf5056348bcbce8192"},"cell_type":"markdown","source":"Previous Applications:"},{"metadata":{"trusted":true,"_uuid":"6c69be9c7a1d9a1888960024f4d98a68f5e2693d"},"cell_type":"code","source":"previous.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f95aa63265273d71d54f4751438aa632eea91a15"},"cell_type":"markdown","source":"It is the detailed information of previous loans. We will use NAME_CONTRACT_STATUS (whether the loan application is approved or refused and so on) and amount of loans to create features."},{"metadata":{"_uuid":"6d1d3c17886c0833eaa37bceb634987eeec2580c"},"cell_type":"markdown","source":"Installment:"},{"metadata":{"trusted":true,"_uuid":"3bc88d7fec70cde6a9942035fce8440e07583bb0"},"cell_type":"code","source":"installment.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"101d972bf4a7f9e6cbf2446435e9287337cf73df"},"cell_type":"markdown","source":"We can create some features by comparing AMT_INSTALMENT (amount to be paid) and AMT_PAYMENT (amount actually paid)"},{"metadata":{"_uuid":"848f05d5232a06dd74cdd9069a663e58dd0404bb"},"cell_type":"markdown","source":"## Feature Engineering\n\n### PCA of Home Related Variables\n\nThere are variables (from APARTMENTS_AVG to EMERGENCYSTATE_MODE) that are related to the building where the borrowers live. We are going to perform a principal component analysis (PCA) to reduce dimension of those 47 variables to make the machine learning algorithm more efficient:"},{"metadata":{"trusted":true,"_uuid":"09a658d749fd2887b62e8003b50cb9886a0c15b7"},"cell_type":"code","source":"house_train = train.loc[:,'APARTMENTS_AVG':'EMERGENCYSTATE_MODE']\nhouse_test = test.loc[:,'APARTMENTS_AVG':'EMERGENCYSTATE_MODE']\nhouse_var = house_train.columns.tolist()\nhouse_train = pd.get_dummies(house_train)\nhouse_test = pd.get_dummies(house_test)\nhouse_train.fillna(0, inplace=True)\nhouse_test.fillna(0, inplace=True)\nhouse_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed3e79be5bbe895138d9a6945598ccac0934c664"},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\nhouse_train2 = pca.fit_transform(house_train)\nhouse_test2 = pca.transform(house_test)\nhouse_train2.shape, house_test2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25eb127853fafbc79f7b12ab5de73f7755846187"},"cell_type":"code","source":"house_train2 = pd.DataFrame(house_train2, columns=['house_pc1','house_pc2','house_pc3'])\nhouse_test2 = pd.DataFrame(house_test, columns=['house_pc1','house_pc2','house_pc3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f3bae35f795c80081ef4e2f8d50a2f8ff6aecbd"},"cell_type":"code","source":"# Merge back to trian and test datasets\ntrain = pd.concat([train.drop(columns=house_var), house_train2], axis=1)\ntest = pd.concat([test.drop(columns=house_var), house_test2], axis=1)\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0db58e4c6f466c560d8cfe356a15966b1fb0e11b"},"cell_type":"markdown","source":"### Feature Transformation for Train and Test Data"},{"metadata":{"trusted":true,"_uuid":"7e2828cf2560e416a8b23392bd095fd2d0712ae9"},"cell_type":"code","source":"# Add a few domain variables\ntrain['CREDIT_INCOME_PERCENT'] = train['AMT_CREDIT']/train['AMT_INCOME_TOTAL']\ntrain['ANNUITY_INCOME_PERCENT'] = train['AMT_ANNUITY']/train['AMT_INCOME_TOTAL']\ntrain['DAYS_EMPLOYED_PERCENT'] = train['DAYS_EMPLOYED']/train['DAYS_BIRTH']\ntrain['INCOME_PER_PERSON'] = train['AMT_INCOME_TOTAL'] / train['CNT_FAM_MEMBERS']\ntrain['PAYMENT_RATE'] = train['AMT_ANNUITY'] / train['AMT_CREDIT']\ntest['CREDIT_INCOME_PERCENT'] = test['AMT_CREDIT']/test['AMT_INCOME_TOTAL']\ntest['ANNUITY_INCOME_PERCENT'] = test['AMT_ANNUITY']/test['AMT_INCOME_TOTAL']\ntest['DAYS_EMPLOYED_PERCENT'] = test['DAYS_EMPLOYED']/test['DAYS_BIRTH']\ntest['INCOME_PER_PERSON'] = test['AMT_INCOME_TOTAL'] / test['CNT_FAM_MEMBERS']\ntest['PAYMENT_RATE'] = test['AMT_ANNUITY'] / test['AMT_CREDIT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c87a118f55b70e15405413548da8c4e8fabfe192"},"cell_type":"code","source":"# Features to be log-transformed\nlog_features = ['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION']\n# For days, they need to be changed back to positive. For the sake of consistency, all days columns are transformed\ndays_features = ['DAYS_BIRTH', 'DAYS_EMPLOYED','DAYS_REGISTRATION', 'DAYS_ID_PUBLISH']\ntrain[days_features]=-train[days_features]\ntest[days_features]=-test[days_features]\ntrain[log_features]=np.log1p(train[log_features])\ntest[log_features]=np.log1p(test[log_features])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"966f8695a8cc943ee1fbe2838a01842defa419ec"},"cell_type":"markdown","source":"For all null values, it is likely that they do not provide information and is regarded as less reliable. We add a column recording how many columns are missing for every row, then the null values are filled with 0."},{"metadata":{"trusted":true,"_uuid":"4338e548e98444d28f3323d04f71705c21780361"},"cell_type":"code","source":"train['na_col'] = train.isnull().sum(axis=1)\ntest['na_col'] = test.isnull().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fc326a679cc34c58aa7c562af9dbeae1f82aab1"},"cell_type":"markdown","source":"We convert text columns into dummy variables, then align train and test columns before calculating correlations."},{"metadata":{"trusted":true,"_uuid":"9a08bf4d24a4f3f95a6335a68e54e1a38ce6c504"},"cell_type":"code","source":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)\ntrain_labels = train['TARGET']\ntrain, test = train.align(test, join = 'inner', axis = 1)\ntrain['TARGET'] = train_labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fffbf8d7d1178c306aa8620693f8a112d7477859"},"cell_type":"markdown","source":"### Feature Creation From Other Datasets"},{"metadata":{"trusted":true,"_uuid":"e4bfbd92fab839c7b9fde0a2042768013291f0ce"},"cell_type":"code","source":"# Create features from bureau \nsafe = bureau.loc[(bureau.CREDIT_DAY_OVERDUE==0)&(bureau.CNT_CREDIT_PROLONG==0),['SK_ID_CURR','SK_ID_BUREAU']]\nsafe = safe.groupby('SK_ID_CURR').count().reset_index()\nsafe.columns = ['SK_ID_CURR','past_loan_np']\nprob = bureau[['SK_ID_CURR','CREDIT_DAY_OVERDUE','CNT_CREDIT_PROLONG']].groupby('SK_ID_CURR').max()\nprob.columns=['overdue_max','prolong_max']\nprob=prob.reset_index()\nloan_type=pd.get_dummies(bureau.CREDIT_TYPE)\nloan_type['SK_ID_CURR'] = bureau['SK_ID_CURR']\nloan_type = loan_type.groupby('SK_ID_CURR').mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2753e208b3c69232db59f6ea730ec1cf35cf545e"},"cell_type":"code","source":"# Create features from bureau balance data; They need to be merged with bureau data first\n_ = bureau_bal.drop(columns=['STATUS']).groupby('SK_ID_BUREAU').max().reset_index()\nfinal_status = _.merge(bureau_bal, on=['SK_ID_BUREAU','MONTHS_BALANCE'], how='left')\nfinal_status = pd.get_dummies(final_status).drop(columns='MONTHS_BALANCE')\nfinal_status2 = bureau[['SK_ID_CURR','SK_ID_BUREAU']].merge(final_status, on='SK_ID_BUREAU', how='left')\nfinal_status2.STATUS_X.fillna(1, inplace=True) # Mark all cases without final status as unknown\nfinal_status2.fillna(0, inplace=True)\nfinal_status2 = final_status2.drop(columns='SK_ID_BUREAU').groupby('SK_ID_CURR').mean().reset_index()\n# Keep only status 1-5 and X\nfinal_status2 = final_status2.drop(columns=['STATUS_0','STATUS_C'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdcb1bf88b10e846e8a0d236a24eaf74dac42a21"},"cell_type":"code","source":"# Free up memory\ndel bureau, bureau_bal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6396dc111653d81b284a738fdb6707ede2e0b9a6"},"cell_type":"code","source":"# Create features from cash and card balance data;\ncash_dpd = cash.loc[:,['SK_ID_CURR','SK_DPD_DEF']].groupby('SK_ID_CURR').agg(['max','median']).reset_index()\ncash_dpd.columns = ['SK_ID_CURR','Cash_SK_DPD_DEF_max', 'Cash_SK_DPD_DEF_median']\ncard_dpd = card.loc[:,['SK_ID_CURR','SK_DPD_DEF']].groupby('SK_ID_CURR').agg(['max','median']).reset_index()\ncard_dpd.columns = ['SK_ID_CURR','Card_SK_DPD_DEF_max', 'Card_SK_DPD_DEF_median']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8585d1b955804f17fab5220ab42f0aa7e378631d"},"cell_type":"code","source":"del cash, card","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c2554d21a8b0b1e227f6a250f161a1bf3c32950"},"cell_type":"code","source":"# Create features from previous loans data\np_good = previous.loc[(previous.NAME_CONTRACT_STATUS=='Approved') | (previous.NAME_CONTRACT_STATUS=='Unused offer'), ['SK_ID_CURR','AMT_CREDIT']]\np_bad = previous.loc[(previous.NAME_CONTRACT_STATUS=='Canceled') | (previous.NAME_CONTRACT_STATUS=='Refused'), ['SK_ID_CURR','AMT_CREDIT']]\np_good = p_good.dropna()\np_good2 = p_good.groupby('SK_ID_CURR').sum().reset_index()\np_good2.columns = ['SK_ID_CURR','good_credit']\np_bad2 = p_bad.groupby('SK_ID_CURR').sum().reset_index()\np_bad2.columns = ['SK_ID_CURR','bad_credit']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a19404987adaf69c07d866e64dabe5ef247bcd27"},"cell_type":"code","source":"installment['diff'] = installment.AMT_PAYMENT - installment.AMT_INSTALMENT\nins_comp = installment.dropna(subset=['AMT_PAYMENT']).loc[:,['SK_ID_CURR','AMT_INSTALMENT','AMT_PAYMENT','diff']].groupby('SK_ID_CURR').sum().reset_index()\nins_comp['repay']=ins_comp.AMT_PAYMENT / ins_comp.AMT_INSTALMENT\nins_comp = ins_comp.loc[ins_comp.AMT_INSTALMENT!=0,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef73180a26d22e0b29ba35da0a7264e8487f8a19"},"cell_type":"code","source":"del previous, installment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d41d4fc6f9b4a88eff732cff1a039072b74d30b4"},"cell_type":"code","source":"# Function to merge dataframes\ndef merge_data(train, test, dfs):\n    for df in dfs:\n        train = train.merge(df, on='SK_ID_CURR', how='left')\n        test = test.merge(df, on='SK_ID_CURR', how='left')\n    return train, test\ntrain, test = merge_data(train, test, [safe, prob, loan_type, final_status2, cash_dpd, card_dpd, p_good2, p_bad2, ins_comp])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df5b0d840725aefd987e7a2b06d061bcaa87f399"},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98fb9720af2af24c5acabbe45dfd88642ea09e52"},"cell_type":"markdown","source":"Then we fill the N/A values. For repay (ratio of amount actually paid vs amount to be paid, from installment dataset) we will default as 1; others we fill zero."},{"metadata":{"trusted":true,"_uuid":"6bdb57d0856f67ef766a89387240286a7a7570fb"},"cell_type":"code","source":"train.repay.fillna(1, inplace=True)\ntest.repay.fillna(1, inplace=True)\ntrain.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05c1e2c9e68306fdb69dfaaea2eb03f6d20eb5ff"},"cell_type":"markdown","source":"### Remove Collinear Variables"},{"metadata":{"_uuid":"4f245f8cc49b4f2dde83a117c5ef54352d3a7a0b"},"cell_type":"markdown","source":"Variables highly collinear, when put together, may caused undesirable behavior in models (especially linear models). Here we are going to remove features whose correlation with another feature is above 0.9."},{"metadata":{"trusted":true,"_uuid":"96f0c42ae602a331e524ad827d32a2aba398543c"},"cell_type":"code","source":"# Remove collinear variables\ndef rm_collinear(train, test):\n    threshold = 0.9\n    corr_matrix = train.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > threshold) and column != 'SK_ID_CURR']\n    train = train.drop(columns = to_drop)\n    test = test.drop(columns = to_drop)\n    return train, test, to_drop\n\ntrain, test, collin_feat = rm_collinear(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93cae86bb5e29838a3ceb51945624d39d64562fc"},"cell_type":"code","source":"collin_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6599fe7571cda889c223ffe87b300af67bfe06de"},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bbbcdbfb21320b6ffbf3404edef0b14c452190a"},"cell_type":"markdown","source":"## Building Prediction Models"},{"metadata":{"trusted":true,"_uuid":"9094799a7388b4aab71f1c913d624d5d9f298d6b"},"cell_type":"code","source":"# Test of models\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom lightgbm import LGBMClassifier\nfrom time import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b1a217922aeccdaa0342304907d230fa288cd3b"},"cell_type":"code","source":"# Min-max scaling and imputing\nx_train = train.drop(columns = ['TARGET','SK_ID_CURR'])\nx_test = test.drop(columns=['SK_ID_CURR'])\n\nfeatures = list(x_train.columns)\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Min-max scale\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6913120c5801856e1bd7c33e8160c57f15b152c5"},"cell_type":"code","source":"# Defining candidate models\ngb = GradientBoostingClassifier(n_estimators=30, learning_rate=0.1, min_samples_split=4, random_state=10, verbose=1)\nab = AdaBoostClassifier(n_estimators=300, learning_rate=1, random_state=123)\nrf0 = RandomForestClassifier(n_estimators=49, max_depth = 5, random_state=100)\nlr = LogisticRegression(C=1, random_state=100)\nnn = MLPClassifier(hidden_layer_sizes=(32,), alpha=0.01, learning_rate_init=0.01, max_iter=100, batch_size=4082)\ndt = DecisionTreeClassifier(max_features=100)\nlgb = LGBMClassifier(n_estimators=2000, learning_rate=0.02, objective='binary', reg_lambda=0.1, num_leaves=34)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2621025dd90a945b8fb4016f321ce4a208528726"},"cell_type":"markdown","source":"We split the original training data into two sets, leaving 10% of data for validation purpose. Two functions are written: run_model_try() is for evaluating model by testing them in validation set, while run_model_real() is for running the model using all training data to submit for competition."},{"metadata":{"trusted":true,"_uuid":"5962d76fb4e0aa606602667a90280e213ab76942"},"cell_type":"code","source":"x_train_x, x_train_v, y_train_x, y_train_v = train_test_split(x_train, train_labels, test_size=0.1, random_state=168)\n\ndef run_model_try(ml):\n    start = time()\n    model = ml\n    model.fit(x_train_x, y_train_x)\n    train_pred = model.predict_proba(x_train_x)[:,1]\n    v_pred = model.predict_proba(x_train_v)[:,1]\n    train_auc = roc_auc_score(y_train_x, train_pred)\n    v_auc = roc_auc_score(y_train_v, v_pred)\n    train_acc = accuracy_score(y_train_x, model.predict(x_train_x))\n    v_acc = accuracy_score(y_train_v, model.predict(x_train_v))\n    end = time()\n    print (\"Training and validation auc: %.4f, %.4f\" %(train_auc, v_auc))\n    print (\"Training and validation accuracy: %.4f, %.4f\" %(train_acc, v_acc))\n    print (\"Time used: %.2f\" %(end-start))\n    return model\n\ndef run_model_real(ml):\n    model = ml\n    model.fit(x_train, train_labels)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14a4555b8fb33a5a426b77b0e4a0e9c8d1b0749e"},"cell_type":"markdown","source":"We finally select an LGB model:"},{"metadata":{"trusted":true,"_uuid":"19390314e09cf845d617530b2f2a3864dd2207e7"},"cell_type":"code","source":"# model = run_model_try(lgb)\n# The two lines below are for running models with all training data to submit for competition\nmodel = run_model_real(lgb)\ntest_pred = model.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a86fe8cf60120ed8f98159fc719e293c756386b6"},"cell_type":"markdown","source":"Let's look at the feature importance of our model:"},{"metadata":{"trusted":true,"_uuid":"2ebd9866cc054f5a63202840df6b0f746e536121"},"cell_type":"code","source":"plt.figure(figsize=(8,8))\n# feats = np.array(features)[cols].tolist()\nfeats = np.array(features).tolist()\nfi = pd.DataFrame()\nfi['feature'] = feats\nfi['importance'] = model.feature_importances_\nsns.barplot(x=\"importance\", y=\"feature\", data=fi.sort_values(by=\"importance\", ascending=False).head(50));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2df90191a148898a482853234fcc0bf356c461b"},"cell_type":"code","source":"submit = test[['SK_ID_CURR']]\nsubmit['TARGET'] = test_pred\nsubmit.to_csv('lgb0825.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}