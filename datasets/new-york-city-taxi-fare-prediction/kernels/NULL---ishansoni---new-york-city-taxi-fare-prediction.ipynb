{"cells":[{"metadata":{"_uuid":"0495c6988ab69d8fedbbc0c8449863a72a3c8fd7"},"cell_type":"markdown","source":"In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Basic EDA libraries\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('dark_background')\nsns.set_style(\"whitegrid\")\nfrom IPython.display import display\n%matplotlib inline\n\n# Basic ML Libraries\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ShuffleSplit\n\nimport xgboost as xgb\n\n# Deep Learning Libraries\n\n# some other libraries\nimport geopy.distance\nfrom geopy.geocoders import Nominatim","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fdc2d778aa2399664e0077bb9fbb754d6434e67"},"cell_type":"markdown","source":"### Dataset Import"},{"metadata":{"trusted":true,"_uuid":"ab7279a6d8019c2e1a31b912010305a8dac786b4"},"cell_type":"code","source":"# Since the train data is pretty large, we will import only a random subset of rows to do our analysis.\n\n# The data to load\ntrain_file = \"../input/train.csv\"\n\n# Take every N-th (in this case 10th) row\nn = 10\n\n# Count the lines or use an upper bound\nnum_lines = sum(1 for l in open(train_file))\n\n# The row indices to skip - make sure 0 is not included to keep the header!\nskip_idx = [x for x in range(1, num_lines) if x % n != 0]\n\n# Read the data\n# train = pd.read_csv(train_file, dtype={'fare_amount': 'float32', 'pickup_longitude' : 'float32', 'pickup_longitude' : 'float32', 'dropoff_longitude' : 'float32', 'dropoff_latitude' : 'float32', 'passenger_count' : 'int32'},\n#                    skiprows = skip_idx, parse_dates = ['pickup_datetime']).drop(columns = 'key')\n\ntrain = pd.read_csv(train_file, dtype={'fare_amount': 'float32', 'pickup_longitude' : 'float32', 'pickup_longitude' : 'float32', 'dropoff_longitude' : 'float32', 'dropoff_latitude' : 'float32', 'passenger_count' : 'int32'},\n                    nrows = 2_000_000, parse_dates = ['pickup_datetime']).drop(columns = 'key')\n\n\ntest = pd.read_csv(\"../input/test.csv\", dtype={'fare_amount': 'float32', 'pickup_longitude' : 'float32', 'pickup_longitude' : 'float32', 'dropoff_longitude' : 'float32', 'dropoff_latitude' : 'float32', 'passenger_count' : 'int32'},\n                   parse_dates = ['pickup_datetime'])\n\n# To be used for creating the subission csv\ntest_id = list(test.pop('key'))\n\ndisplay(train.sample(n = 5))\n\ndisplay(test.sample(n = 5))\n\ndisplay(train.info())\n\ndisplay(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"015abc8d5086a691fa9ba732d0d52ae91113e6d9"},"cell_type":"markdown","source":"### Cleaning Data"},{"metadata":{"trusted":true,"_uuid":"5f40ff8c2eeda2cd26bf33c1480e9b650603d217"},"cell_type":"code","source":"# Original Shape\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"209aa3cb59924c7b9976d33c8f12296bf4976b12"},"cell_type":"code","source":"# Lets have a peek at our data's descriptive statistics \ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d663637f7985fa0a1cac501ed7d78012a21de01"},"cell_type":"markdown","source":"Looking at the above information, there are a lot of bad data values :\n* The min fare amount is -ve, which can not happen. The maximum fare amount is also ridiculous. \n*   Latitude/ Longitude ranges are also incorrect. (Since we are dealing with the NY region, we will limit ourselves to Lat/Long ranges in this region and drop other data points that are potentially noise/ bad data values)\n* The min passenger count is 0 and the max is 208. We will have to handle these values as well\n\n"},{"metadata":{"trusted":true,"_uuid":"b866d3a04d42bc68b4275d6dc5669110081d3b0d"},"cell_type":"code","source":"# Check for nulls in our dataset.\nprint(\"Training data has nulls? :\", train.isnull().values.any())\nprint(\"Testing data has nulls? :\", test.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"810b678deae3fe297d444a029a9bcce283954ffc"},"cell_type":"code","source":"print(train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"746e28c50f0ee101c7c9e11986f3d648e93399e2"},"cell_type":"code","source":"# Since there aren't a lot of rows with nulls, we will drop them all\ntrain = train.dropna(how = 'any', axis = 'rows')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56a175558058cfe44ae5207682717bedc304d1c5"},"cell_type":"markdown","source":"##### Cleaning Fares"},{"metadata":{"trusted":true,"_uuid":"133a8d4f7383137d02ae07eb5e90fb0652ef841a"},"cell_type":"code","source":"# Let's have a look at the fare's distribution\n\nsns.distplot(train.sample(n = 20000)[\"fare_amount\"], hist = True, kde = True)\nfig = plt.gcf()\nfig.set_size_inches(20, 8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"998c529fab618dc6198d4ef8087b33d8134fd84b"},"cell_type":"code","source":"# Most of the fare amount lies b/w 0 and 60. Lets remove some extreme outliers\n# Base Fare for NYC Cab : 2.5$ (http://nymag.com/nymetro/urban/features/taxi/n_20286/)\n\nq75, q25 = np.percentile(train[\"fare_amount\"], [75 ,25])\niqr = q75 - q25\n\nprint(\"Fare Iqr\", iqr)\n\ntrain = train[train[\"fare_amount\"].between(left = 2.5, right = (q75 + 10 * iqr))]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3ace50b5dae31d11b400e723dba962eda77fae3"},"cell_type":"markdown","source":"##### Cleaning Latitude/ Longitudes"},{"metadata":{"trusted":true,"_uuid":"fe79c8212bd512fc27a822f49d7a811157b276a3"},"cell_type":"code","source":"# Cleaning Latitudes & Longitudes\n# Latitudes range from -90 to 90.\n# Longitudes range from -180 to 180.\n# New york lat/long =>40.730610, -73.935242\n\n# Lets have a look at all Invalid lat/long ranges\n\ndisplay(train[np.logical_or(train[\"pickup_latitude\"] < -90, train[\"pickup_latitude\"] > 90)])\ndisplay(train[np.logical_or(train[\"dropoff_latitude\"] < -90, train[\"dropoff_latitude\"] > 90)])\n\ndisplay(train[np.logical_or(train[\"pickup_longitude\"] < -180, train[\"pickup_longitude\"] > 180)])\ndisplay(train[np.logical_or(train[\"dropoff_longitude\"] < -180, train[\"dropoff_longitude\"] > 180)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b224122a949dcf92da19a646e1a5343b104f4436"},"cell_type":"code","source":"#There are a few rows that have invalid Latitudes & Longitudes. Will will discard them. \n#Plus, we will also discard rows that have lat/long ranges not possible for the NY region (40.730610, -73.935242)\n\ntrain = train[np.logical_and(train[\"pickup_latitude\"] >= 40, train[\"pickup_latitude\"] <= 42)]\ntrain = train[np.logical_and(train[\"dropoff_latitude\"] >= 40, train[\"dropoff_latitude\"] <= 42)]\ntrain = train[np.logical_and(train[\"pickup_longitude\"] >= -75, train[\"pickup_longitude\"] <= -73)]\ntrain = train[np.logical_and(train[\"dropoff_longitude\"] >= -75, train[\"dropoff_longitude\"] <= -73)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff6527aac12ded318d025cd961639783df917a11"},"cell_type":"markdown","source":"##### Cleaning Passenger Count"},{"metadata":{"trusted":true,"_uuid":"080a47ce0adc178d77a1a98c242c05936498dfcc"},"cell_type":"code","source":"# Lets have a look at the Passenger Count distribution\n\nsns.distplot(train.sample(n = 20000)[\"passenger_count\"], hist = True, kde = True)\nfig = plt.gcf()\nfig.set_size_inches(20, 8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1f673010d8cd807b983e7670f1251ea9513ec81"},"cell_type":"code","source":"# Most of the passenger counts are between 0 and 6. We will remove all others.\n\ntrain = train[train[\"passenger_count\"].between(left = 0, right = 6)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e1bc40ca28ddce6322d492e4c279ccb21735b31"},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6558f73e027629e7a9fef9a29ed87d5122bce5fa"},"cell_type":"markdown","source":"### Feature Engg."},{"metadata":{"_uuid":"ba10f12d5e9e91a4873299521d8e21d3ea1041e0"},"cell_type":"markdown","source":"##### Lets create a trip distance feature"},{"metadata":{"trusted":true,"_uuid":"effed8e79ac2ca5775c0cdbec024f8a3e0dc9514"},"cell_type":"code","source":"# Example\ncoords_1 = (52.2296756, 21.0122287)\ncoords_2 = (52.406374, 16.9251681)\n\nprint(geopy.distance.vincenty(coords_1, coords_2).km)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cefa374c74005349610c90eb543764c04c4c5a7"},"cell_type":"code","source":"def distanceCalculator(row) :\n    c1 = (row[\"pickup_latitude\"], row[\"pickup_longitude\"])\n    c2 = (row[\"dropoff_latitude\"], row[\"dropoff_longitude\"])\n    \n    return geopy.distance.vincenty(c1, c2).km\n\ntrain[\"distance\"] = train.apply(distanceCalculator, axis = 1)\ntest[\"distance\"] = test.apply(distanceCalculator, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a39a3862942a29bfdb2a863ae260b6fabb22b9f1"},"cell_type":"code","source":"train.sample(n = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f8157b7534bb6c682484a494e7cad4da541c3c8","collapsed":true},"cell_type":"markdown","source":"##### Lets create new TIME features using the pick up datetime"},{"metadata":{"trusted":true,"_uuid":"2a8ac599d984647614e2a107caff7c48540069d1"},"cell_type":"code","source":"train[\"hour\"] = train[\"pickup_datetime\"].dt.hour\ntest[\"hour\"] = test[\"pickup_datetime\"].dt.hour\n\ntrain[\"dayOfWeek\"] = train[\"pickup_datetime\"].dt.dayofweek\ntest[\"dayOfWeek\"] = test[\"pickup_datetime\"].dt.dayofweek\n\ntrain['day'] = train['pickup_datetime'].dt.day\ntest['day'] = test['pickup_datetime'].dt.day\n\ntrain['month'] = train['pickup_datetime'].dt.month\ntest['month'] = test['pickup_datetime'].dt.month\n\ntrain[\"year\"] = train[\"pickup_datetime\"].dt.year\ntest[\"year\"] = test[\"pickup_datetime\"].dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6589f6c5554c6f4c9e5e1b68b5809f35f39faecf"},"cell_type":"code","source":"train.sample(n = 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62ab907e26035497a49cdccc3dcd0ac9c2e7e1ce"},"cell_type":"markdown","source":"##### Creating extra features based on Lat/ Long ranges"},{"metadata":{"trusted":true,"_uuid":"f7267821eb02ae426139fb615524acc4bddb36bc"},"cell_type":"code","source":"def dist(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  \n    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n    \n    return distance\n\ndef transform(data):\n    # Distances to nearby airports, and city center\n    # By reporting distances to these points, the model can somewhat triangulate other locations of interest\n    nyc = (-74.0063889, 40.7141667)\n    jfk = (-73.7822222222, 40.6441666667)\n    ewr = (-74.175, 40.69)\n    lgr = (-73.87, 40.77)\n   \n    data['distance_to_center'] = dist(nyc[1], nyc[0],\n                                      data['pickup_latitude'], data['pickup_longitude'])\n    data['pickup_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                         data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    data['pickup_distance_to_ewr'] = dist(ewr[1], ewr[0], \n                                          data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_ewr'] = dist(ewr[1], ewr[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    data['pickup_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                          data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    data['long_dist'] = data['pickup_longitude'] - data['dropoff_longitude']\n    data['lat_dist'] = data['pickup_latitude'] - data['dropoff_latitude']\n    \n    return data\n\n\ntrain = transform(train)\ntest = transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a577c43437d975fdbd79a608193c989177b29258"},"cell_type":"code","source":"train.sample(n = 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6c7d8737ed3e3b59e8a41ce1c51cef327c4205f"},"cell_type":"markdown","source":"### Exploratory data analysis"},{"metadata":{"_uuid":"fe1b0a114e33c1b99f4b2ccdd994fdb749b7fc9d"},"cell_type":"markdown","source":"*  Does Hour of the day have any effect on the fare?"},{"metadata":{"trusted":true,"_uuid":"1fd1d3d9e2df407e186bfebf219938d702d1a886"},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (20, 8))\n\nsns.countplot(x = \"hour\", data = train.sample(n = 20000), ax = ax[0])\nplt.xlabel(\"Hour of the day\")\nplt.ylabel(\"Cab frequency\")\n\nsns.barplot(x = \"hour\", y = \"fare_amount\", data = train.sample(n = 20000), ax = ax[1])\nplt.xlabel(\"Hour of the day\")\nplt.ylabel(\"Average Fare amount\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd8a3d83320251dc388c4301be7fb39a6f52bea6"},"cell_type":"markdown","source":"Even though its not a result of direct causation, fares are a little higher when the usage is low ie. Odd hours (eg night 3- 6 in the morning) or 2 - 4 in the evening"},{"metadata":{"_uuid":"23a197762290c52793df45cda0786d3d59f4d444"},"cell_type":"markdown","source":"*  Does Day of the week have any effect on the fare?"},{"metadata":{"trusted":true,"_uuid":"6e2f886663a729792643e02e186d6dd36981ead9"},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize = (20, 8))\n\nsns.countplot(x = \"dayOfWeek\", data = train.sample(n = 20000), ax = ax[0])\nplt.xlabel(\"Day of the week\")\nplt.ylabel(\"Cab frequency\")\n\nsns.barplot(x = \"dayOfWeek\", y = \"fare_amount\", data = train.sample(n = 20000), ax = ax[1])\nplt.xlabel(\"Day of the week\")\nplt.ylabel(\"Average Fare amount\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3de5aa4881284e1b61222bfffc1d61eedc0d5f88"},"cell_type":"markdown","source":"Looks like there are more rides on Friday and Saturday but the fares are not affected much by day of the week"},{"metadata":{"trusted":true,"_uuid":"ad3dadccabc40b83a586dd8fb981936e487d2ed5"},"cell_type":"code","source":"sns.factorplot(x = \"hour\", y = \"fare_amount\", hue = \"dayOfWeek\", data = train.sample(n = 20000))\nfig = plt.gcf()\nfig.set_size_inches(20, 12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40cd5e4585fd3bc15d7fa9e0f3fcf86867d2530f"},"cell_type":"code","source":"train[\"year\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9881d0949c3e9075774c36fc36fbad3220cecd8c"},"cell_type":"code","source":"# The fare data contains data points for several years, The fares shoule depend on it(Inflation!)\n\nsns.barplot(x = \"year\", y = \"fare_amount\", data = train.sample(n = 20000))\nfig = plt.gcf()\nfig.set_size_inches(12, 8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b844b55caa8638e3b649ac887194e70037c29854"},"cell_type":"markdown","source":"And it obvious that year should affect the fare price as well."},{"metadata":{"trusted":true,"_uuid":"3e00897b9959fb1b3257b8cd080ac942ba6c6b22"},"cell_type":"code","source":"# Passenger Count\nsns.barplot(x = \"passenger_count\", y = \"fare_amount\", data = train.sample(n = 20000))\nfig = plt.gcf()\nfig.set_size_inches(12, 8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0e9071da99fae5d0d5f4ed7ee0b94935889ac59"},"cell_type":"markdown","source":"Not much information"},{"metadata":{"_uuid":"e37d13e52cb9cc6933e28ba89286930bc629e847"},"cell_type":"markdown","source":"### Model Application\n"},{"metadata":{"trusted":true,"_uuid":"6620cb1a4e6281784f9d64a4454ded6da97d112e"},"cell_type":"code","source":"display(train.sample(n = 5))\ndisplay(test.sample(n = 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7eedf995badb9e19fb7a57b5c70ee381445f868"},"cell_type":"code","source":"target = train[\"fare_amount\"].values\ntrain = train.drop(columns = [\"fare_amount\", \"pickup_datetime\"], axis = 1)\ntest = test.drop(columns = [\"pickup_datetime\"], axis = 1)\ndisplay(train.sample(n = 2))\ndisplay(test.sample(n = 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3849800eb6e447a777d36bfefd19aeda4f07c32e"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train, target, test_size = 0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dba823a9d08b85fbb761d85b4168ca9ee2408c1c"},"cell_type":"code","source":"# Even though I have created the pipeline to optimise hyperparameters and save a list of model, I will not be using it\n# due to the sheer data size. \n\nmodelResults = pd.DataFrame(columns = ['Model_Name', 'Model', 'Params', 'Test_Score', 'CV_Mean', 'CV_STD'])\n\ndef save(grid, modelName, calFI):\n    global modelResults\n    cv_scores = cross_val_score(grid.best_estimator_, X_train, y_train, cv = 10, scoring = 'neg_mean_squared_error')\n    cv_mean = cv_scores.mean()\n    cv_std = cv_scores.std()\n    test_score = grid.score(X_test, y_test)\n    \n    print(\"Best model parameter are\\n\", grid.best_estimator_)\n    print(\"Saving model {}\\n\".format(modelName))\n    print(\"Mean Cross validation score is {} with a Standard deviation of {}\\n\".format(cv_mean, cv_std))\n    print(\"Test Score for the model is {}\\n\".format(test_score))\n    \n    if calFI:\n        pd.Series(grid.best_estimator_.feature_importances_, train.columns).sort_values(ascending = True).plot.barh(width = 0.6)\n        fig = plt.gcf()\n        fig.set_size_inches(12, 12)\n        plt.title(\"{} Feature Importance\".format(modelName))\n        plt.show()\n    \n    \n    modelResults = modelResults.append({'Model_Name' : modelName, 'Model' : grid.best_estimator_, 'Params' : grid.best_params_, 'Test_Score' : test_score, 'CV_Mean' : cv_mean, 'CV_STD' : cv_std}\n                                       , ignore_index=True)\n    \n    \ndef doGridSearch(classifier, params):\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n    score_fn = make_scorer(mean_squared_error)\n    grid = GridSearchCV(classifier, params, scoring = score_fn, cv = cv)\n    grid = grid.fit(X_train, y_train)\n    \n    return grid    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cb742faf2724ac5792f00f47c900b741e176ceb"},"cell_type":"markdown","source":"##### Lets use a simple Out of the box Random Forest Regressor"},{"metadata":{"trusted":true,"_uuid":"ea78c97723120598e312d2ef111c7c8d05827f07"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nrandom_forest = RandomForestRegressor(max_features = None, oob_score = True, \n                                      bootstrap = True, verbose = 1, n_jobs = -1)\n\nrandom_forest.fit(X_train, y_train)\ny_test_preds = random_forest.predict(X_test)\n\nprint(\"RMSE score :\", np.sqrt(mean_squared_error(y_test, y_test_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7107ab1fa75c157a5b18ac91828d370003df97dd"},"cell_type":"code","source":"random_forest_preditions = random_forest.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b59187f6587cf598625fa48f1d9b7bd7504f99c3"},"cell_type":"code","source":"sub_rf = pd.DataFrame({'key': test_id, 'fare_amount': random_forest_preditions})\nsub_rf.to_csv('rf_nyc.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba531c4f59fbb52ba7e7fbc3d7eb2056b0ad9063"},"cell_type":"markdown","source":"##### Lets use XGBoost."},{"metadata":{"trusted":true,"_uuid":"0e296e1f8d9eb0273c706978d9f56cead1d1e7cc"},"cell_type":"code","source":"#Cross-validation\n\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth': 8, #Result of tuning with CV\n    'eta':.03, #Result of tuning with CV\n    'subsample': 0.8, #Result of tuning with CV\n    'colsample_bytree': 0.8, #Result of tuning with CV\n    # Other parameters\n    'objective':'reg:linear',\n    'eval_metric':'rmse',\n    'silent': 1\n}\n\n#Block of code used for hypertuning parameters. Adapt to each round of parameter tuning.\n#Turn off CV in submission\n\nCV = False\nif CV:\n    dtrain = xgb.DMatrix(train,label=y)\n    gridsearch_params = [\n        (eta)\n        for eta in np.arange(.04, 0.12, .02)\n    ]\n\n    # Define initial best params and RMSE\n    min_rmse = float(\"Inf\")\n    best_params = None\n    for (eta) in gridsearch_params:\n        print(\"CV with eta={} \".format(\n                                 eta))\n\n        # Update our parameters\n        params['eta'] = eta\n\n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=1000,\n            nfold=3,\n            metrics={'rmse'},\n            early_stopping_rounds=10\n        )\n\n        # Update best RMSE\n        mean_rmse = cv_results['test-rmse-mean'].min()\n        boost_rounds = cv_results['test-rmse-mean'].argmin()\n        print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n        if mean_rmse < min_rmse:\n            min_rmse = mean_rmse\n            best_params = (eta)\n\n    print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))\nelse:\n    #Print final params to use for the model\n    params['silent'] = 0 #Turn on output\n    print(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34c4186b4921ea834c7a1a6062758e9f4cfbc878"},"cell_type":"code","source":"def XGBmodel(x_train,x_test,y_train,y_test,params):\n    matrix_train = xgb.DMatrix(x_train, label = y_train)\n    matrix_test = xgb.DMatrix(x_test, label = y_test)\n    model = xgb.train(params = params,\n                    dtrain = matrix_train,num_boost_round = 5000, \n                    early_stopping_rounds = 10,evals = [(matrix_test,'test')])\n    return model\n\nmodel = XGBmodel(X_train, X_test, y_train, y_test, params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1653cec6529ed3e1cf5dcd08ff8bdb95b665432d"},"cell_type":"code","source":"xgbpredictions = model.predict(xgb.DMatrix(test), ntree_limit = model.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15291b2dec94ed0cc07b137c948bf0e8281f851c"},"cell_type":"code","source":"sub_xgb = pd.DataFrame({'key': test_id, 'fare_amount': xgbpredictions})\nsub_xgb.to_csv('xgboost_nyc.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54f04544d67a4a6b67e3ac55edc5b7e19ccb2556"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}