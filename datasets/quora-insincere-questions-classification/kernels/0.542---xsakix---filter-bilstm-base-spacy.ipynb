{"cells":[{"metadata":{"_uuid":"38e7d605b52588dfa82fb54def70d25e511df5bd"},"cell_type":"markdown","source":"## Inspired by:\n* https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings\n* https://www.kaggle.com/shujian/single-rnn-with-4-folds-v1-9\n* http://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/\n* https://arxiv.org/abs/1607.06450\n* https://github.com/keras-team/keras/issues/3878\n* https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n* https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout\n* https://www.kaggle.com/aquatic/entity-embedding-neural-net\n* https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate\n* https://ai.google/research/pubs/pub46697\n* https://blog.openai.com/quantifying-generalization-in-reinforcement-learning/\n* https://www.goodreads.com/book/show/33986067-deep-learning-with-python\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nnp.set_printoptions(threshold=np.nan)\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/embeddings\"))\nprint(os.listdir(\"../input/embeddings/GoogleNews-vectors-negative300\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import Callback\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,f1_score,precision_recall_fscore_support,recall_score,precision_score\nfrom keras import backend as K\nfrom sklearn.utils import class_weight\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\n\nSEED = 2018\n\nnp.random.seed(SEED)\ntf.set_random_seed(SEED)\n\n#https://www.kaggle.com/shujian/single-rnn-with-4-folds-v1-9\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in [i * 0.01 for i in range(100)]:\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        print('\\rthreshold = %f | score = %f'%(threshold,score),end='')\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    print('\\nbest threshold is % f with score %f'%(best_threshold,best_score))\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77b7c5199c59943744495e62d7c0f73f68769e17","scrolled":true},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\ndf[\"question_text\"].fillna(\"_##_\",inplace=True)\nmax_len = df['question_text'].apply(lambda x:len(x)).max()\nprint('max length of sequences:',max_len)\n# df = df.sample(frac=0.1)\n\nprint('columns:',df.columns)\npd.set_option('display.max_columns',None)\nprint('df head:',df.head())\nprint('example of the question text values:',df['question_text'].head().values)\nprint('what values contains target:',df.target.unique())\n\nprint('Computing class weights....')\n#https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(df.target.values),\n                                                 df.target.values)\nprint('class_weights:',class_weights)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e125a9cc3158e582fe4e5318faf65bd7750e367"},"cell_type":"code","source":"import spacy\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nimport time\n\nregex_tokenizer = RegexpTokenizer(r'\\w+')\nnlp = spacy.load('en_core_web_sm')\n\npunctuations = string.punctuation\nstops=set(stopwords.words('english'))\n\n\n# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\ndef cleanup_text(doc):\n    doc = nlp(doc, disable=['parser', 'ner','tagger'])\n    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n    tokens = [tok for tok in tokens if tok not in stops and tok not in punctuations]\n    return ' '.join(tokens)\n\ndef delstopwords(sentence):\n    words = [word.lower().strip() for word in regex_tokenizer.tokenize(sentence)]\n    return \" \".join([ word for word in words if word not in stops and word and word not in punctuations])\n\n# if you wanna see what it does\na = df[\"question_text\"].head(5).apply( lambda doc : cleanup_text(doc))\nb = df[\"question_text\"].head(5).apply( lambda doc : delstopwords(doc))\nprint('ORIGINAL:\\n',df[\"question_text\"].head(5).values)\nprint('SPACY:\\n',a.values)\nprint('NTLK:\\n',b.values)\n\nprint('Spacy conversion....')\nt_start = time.time()\ndf[\"question_text\"].head(1000).apply( lambda doc : cleanup_text(doc))\ntook = time.time()-t_start\nprint('Time it took:',took)\nest = len(df) * took/1000.\nprint('Estimated time:',est)\n\nprint('NTLK conversion...')\nt_start = time.time()\ndf[\"question_text\"].head(1000).apply( lambda doc : delstopwords(doc))\ntook = time.time()-t_start\nprint('Time it took:',took)\nest = len(df) * took/1000.\nprint('Estimated time:',est)\n\n\nprint('Using spacy for conversion....')\nt_start = time.time()\n\ndf[\"question_text\"] = df[\"question_text\"].apply( lambda doc : cleanup_text(doc))\nprint('example of the question text values:',df['question_text'].head().values)\n\ntook = time.time()-t_start\nprint('Time it took:',took)\n\nlen_series = df['question_text'].apply(lambda x:len(x))\nmax_len = len_series.max()\navg_len = len_series.quantile(0.89)\nprint('max length of sequences:',max_len)\nprint('avg length of sequences:',avg_len)\n\nsns.set(color_codes=True)\nplt.figure(figsize=(20, 8))\nsns.kdeplot(len_series)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f41b23c1f3f4eed0d8d419974fe795b63f3df50b"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n#dim of vectors\ndim = 100\n# max words in vocab\nnum_words = 50000\n# max number in questions\nmax_len = int(avg_len)\n\nprint('Fiting tokenizer')\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(df['question_text'])\n\nprint('spliting data')\ndf_train,df_test = train_test_split(df, random_state=1)\n\nprint('text to sequence')\nx_train = tokenizer.texts_to_sequences(df_train['question_text'])\nx_test = tokenizer.texts_to_sequences(df_test['question_text'])\n\nprint('pad sequence')\n## Pad the sentences \nx_train = pad_sequences(x_train,maxlen=max_len)\nx_test = pad_sequences(x_test, maxlen=max_len)\n\n## Get the target values\ny_train = df_train['target'].values\ny_test = df_test['target'].values\n\nprint(x_train.shape)\nprint(y_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55cb2ac1dca7de9fba51e8a7e5dba402159be302","scrolled":true},"cell_type":"code","source":"from keras.layers import Dense, Input,Embedding, Dropout, Activation, CuDNNLSTM,BatchNormalization,concatenate,SpatialDropout1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Concatenate, GlobalAveragePooling1D,Average,Conv1D,GlobalMaxPooling1D,AlphaDropout\nfrom keras.layers import MaxPooling1D,UpSampling1D,RepeatVector,LSTM,TimeDistributed,Flatten,Add, Lambda, Dot\nfrom keras.models import Model\nfrom keras.callbacks import Callback,EarlyStopping\nfrom keras.engine import Layer\nfrom keras.initializers import Ones, Zeros\nimport keras.backend as K\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras import optimizers\n\ndef f1(y_true, y_pred):\n    '''\n    metric from here \n    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n    '''\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n# https://ai.google/research/pubs/pub46697\nadam = optimizers.Adam()\nprint('LR:',K.eval(adam.lr))\n# 0.001 = learning rate in adam\n# optimal batch size ~ eps *N, where eps = learning rate and N = training size\nbatch_size = int(x_train.shape[0]*K.eval(adam.lr))\nprint('Batch size = ',batch_size)\n\n# https://blog.openai.com/quantifying-generalization-in-reinforcement-learning/\ninp1 = Input(shape=(max_len,))\nx = Embedding(num_words, dim,trainable = True,embeddings_regularizer=regularizers.l2(0.0001))(inp1)\nx = SpatialDropout1D(0.1)(x)\n# filter\nx = Conv1D(max_len,kernel_size=5,kernel_regularizer=regularizers.l2(0.0001))(x)\nx = MaxPooling1D()(x)\nx = SpatialDropout1D(0.1)(x)\n# memory\nx = Bidirectional(CuDNNLSTM(int(max_len/2), return_sequences=True,kernel_regularizer=regularizers.l2(0.0001)))(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = BatchNormalization()(x)\nx = Dense(max_len, activation='relu',kernel_regularizer=regularizers.l2(0.0001))(x)\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\nx = Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.0001))(x)\n\nmodel = Model(inputs=inp1, outputs=x)\n\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy',f1])\n\nprint(model.summary())\npatience = 10\n# if you have a lot of data, you can use task specific embeddings\nhistory = model.fit(x_train,y_train, \n                      batch_size=batch_size, \n                      validation_split=0.2,\n                      epochs=100,                      \n                      #overfits rather soon\n                      callbacks=[EarlyStopping(patience=patience)])\n\nprint('training finished...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da921fc2d216b1e2db6819ea13ab0b0c4a0d731d","scrolled":true},"cell_type":"code","source":"_, ax = plt.subplots(1, 3, figsize=(20, 8))\nax[0].plot(history.history['loss'], label='loss')\nax[0].plot(history.history['val_loss'], label='val_loss')\nax[0].legend()\nax[0].set_title('loss')\n\nax[1].plot(history.history['acc'], label='acc')\nax[1].plot(history.history['val_acc'], label='val_acc')\nax[1].legend()\nax[1].set_title('acc')\n\n\nax[2].plot(history.history['f1'], label='f1')\nax[2].plot(history.history['val_f1'], label='val_f1')\nax[2].legend()\nax[2].set_title('f1')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cde59508feb4a5c5f398128f9a5176e9a1912236","scrolled":true},"cell_type":"code","source":"import datetime\nprint(datetime.datetime.now())\n\n#for train set\ny_pred = model.predict(x_train,batch_size=batch_size, verbose=1)\nsearch_result = threshold_search(y_train, y_pred)\nprint(search_result)\ny_pred = y_pred>search_result['threshold']\ny_pred = y_pred.astype(int)\n\nprint('RESULTS ON TRAINING SET:\\n',classification_report(y_train,y_pred))\n\n\n#for test set\ny_pred = model.predict(x_test,batch_size=batch_size, verbose=1)\nsearch_result = threshold_search(y_test, y_pred)\nprint(search_result)\ny_pred = y_pred>search_result['threshold']\ny_pred = y_pred.astype(int)\n\nprint('RESULTS ON TEST SET:\\n',classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3beb75a2101c603f182934220444bf573647220c"},"cell_type":"code","source":"print('fiting final model...')\nn_epochs = len(history.history['loss']) - patience\nhistory = model.fit(x_train,y_train, batch_size=batch_size, epochs=n_epochs)\n\nprint('fitting on full data done...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49e1bd5b69b428e57ad8248868d4b23c9c6f9d84"},"cell_type":"code","source":"_, ax = plt.subplots(1, 2, figsize=(12, 6))\nax[0].plot(history.history['loss'], label='loss')\nax[0].legend()\nax[0].set_title('loss')\n\nax[1].plot(history.history['f1'], label='f1')\nax[1].legend()\nax[1].set_title('f1')\n\nplt.show()\n\ny_pred = model.predict(x_test,batch_size=batch_size, verbose=1)\nsearch_result = threshold_search(y_test, y_pred)\ny_pred = y_pred>search_result['threshold']\ny_pred = y_pred.astype(int)\n\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ad34df4ae095e1de08c64079fa6b0ecbc944423"},"cell_type":"code","source":"#submission\nprint('Loading test data...')\ndf_final = pd.read_csv('../input/test.csv')\ndf_final[\"question_text\"].fillna(\"_##_\", inplace=True)\n\nx_final=tokenizer.texts_to_sequences(df_final['question_text'])\nx_final = pad_sequences(x_final,maxlen=max_len)\n\ny_pred = model.predict(x_final,batch_size=batch_size,verbose=1)\ny_pred = y_pred > search_result['threshold']\ny_pred = y_pred.astype(int)\nprint(y_pred[:5])\n\ndf_subm = pd.DataFrame()\ndf_subm['qid'] = df_final.qid\ndf_subm['prediction'] = y_pred\nprint(df_subm.head())\ndf_subm.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}