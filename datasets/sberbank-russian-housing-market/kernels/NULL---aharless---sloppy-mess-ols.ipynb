{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "194c905a-580a-fb74-c861-d0bebbffb2f0"
      },
      "source": [
        "### Sloppy, Incomplete OLS with Some Basic Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4f961693-04de-28ea-73fc-9f7cf2940b25"
      },
      "source": [
        "This script is a dreadfully sloppy mess, it doesn't perform well (see cell below this, once I write it), and even for what it is, it's grossly incomplete.  It's as if you started painting a house but didn't even finish the first side, and the color is ugly, and the job is badly executed, and the coverage is incomplete for even the sections where the paint has been applied.  Nonetheless, I'm setting this as a sort of very early checkpoint in my modeling and programming progress in this competition.  Parts of the code are lifted from [Mark Waddoups](https://www.kaggle.com/mwaddoups/i-regression-workflow-various-models) (though he might prefer not to be associated with this) and probably from others I don't remember.  Also, it's undocumented.  Right now I'm not going to explain what I did or why, but at least I mostly know myself.  Since I haven't set up a repository yet, my version of pushing this to the cloud is to upload it to Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f5146e43-d085-efd3-647b-50626b298537"
      },
      "source": [
        "Note on performance:  I'm not going to waste a submission on this version of the output, but an earlier version that I did submit got 0.3659.  I have reason to believe this one would do a little better, since its resutls are a little closer to those of [Reynaldo's Naive XGB](https://www.kaggle.com/aharless/adding-my-dva-kopeky-to-reynaldo-s-naive-xgb/output) that got 0.3134.  I did submit a 8%/92% weighted (logarithmic) average of these results and the Reynaldo results, and it got 0.3144.  The fact that adding 8/100 portions of this output (which, I remind, is just a very sloppy OLS) only slighly damaged Reynaldo's near-state-of-the-art results tells me that I'm on to something.  (FWIW, my current best submission, at 0.3140, is a 5%/95% weighted average of an earlier version of ths with the Reynaldo result.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f39aa024-1639-1e60-7096-136b897d75a3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "macro = pd.read_csv('../input/macro.csv')\n",
        "test = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "04daf19b-afe8-fee2-636d-88513cff7e5a"
      },
      "outputs": [],
      "source": [
        "dfa = pd.concat([train, test])\n",
        "dfa.loc[:,\"sub_area\"] = dfa.sub_area.str.replace(\" \",\"\").str.replace(\"\\'\",\"\").str.replace(\"-\",\"\")\n",
        "dfa = dfa.merge(macro, on='timestamp', suffixes=['','_macro'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7605d720-32b6-3dbe-4a57-1aa2022a0493"
      },
      "outputs": [],
      "source": [
        "dfa.lnfloor = np.log(dfa.floor+1)\n",
        "dfa.build_year.isnull().mean()\n",
        "dfa.build_year[dfa.build_year<4965].max()\n",
        "dfa.num_room.isnull().mean()\n",
        "print( train.product_type.isnull().mean() )\n",
        "print( test.product_type.isnull().mean() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "575aa175-4be2-6d41-0c56-cabfccb91059"
      },
      "outputs": [],
      "source": [
        "var = (dfa.product_type=='OwnerOccupier')\n",
        "var[dfa.product_type.isnull()] = var.mean()\n",
        "print( var.isnull().mean() )\n",
        "print( var.unique() )\n",
        "var.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e15920e0-7dcd-dd0c-45d0-64fc8ccd1eee"
      },
      "outputs": [],
      "source": [
        "dfa.full_all.head()\n",
        "dfa.sub_area.head()\n",
        "b = dfa[dfa.sub_area==\"Bibirevo\"]\n",
        "b.green_part_3000.unique()\n",
        "# b.product_type.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5f75c751-e530-ceb6-442e-ef16eb09ee31"
      },
      "source": [
        "Note:  As a general principle, when there are values that seem invalid but are \"special\", such as zeros and ones in numeric fields that shouldn't be zero or one, or values that seem out of the reasonable range but aren't entirely impossible, my approach is to treat them as special cases rather than either missing or valid.  Dummy variables will take care of those special cases in OLS, though the situation may get complicated when fancy methods are involved.  One possibility is to recode them using OLS coefficients to replace the ugly values (perhaps coefficients from a baseline OLS with a sparse set of obvious features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "59b66410-2971-a2f7-23da-b023ef0e8abd"
      },
      "outputs": [],
      "source": [
        "dfa[\"fullzero\"] = (dfa.full_sq==0)\n",
        "dfa[\"fulltiny\"] = (dfa.full_sq<4)\n",
        "dfa[\"fullhuge\"] = (dfa.full_sq>2000)\n",
        "dfa[\"lnfull\"] = np.log(dfa.full_sq+1)\n",
        "\n",
        "dfa[\"nolife\"] = dfa.life_sq.isnull()\n",
        "dfa.life_sq = dfa.life_sq.fillna(dfa.life_sq.median())\n",
        "dfa[\"lifezero\"] = (dfa.life_sq==0)\n",
        "dfa[\"lifetiny\"] = (dfa.life_sq<4)\n",
        "dfa[\"lifehuge\"] = (dfa.life_sq>2000)\n",
        "dfa[\"lnlife\"] = np.log( dfa.life_sq + 1 )\n",
        "\n",
        "dfa[\"nofloor\"] = dfa.floor.isnull()\n",
        "dfa.floor = dfa.floor.fillna(dfa.floor.median())\n",
        "dfa[\"floor1\"] = (dfa.floor==1)\n",
        "dfa[\"floor0\"] = (dfa.floor==0)\n",
        "dfa[\"floorhuge\"] = (dfa.floor>50)\n",
        "dfa[\"lnfloor\"] = np.log(dfa.floor+1)\n",
        "\n",
        "dfa[\"nomax\"] = dfa.max_floor.isnull()\n",
        "dfa.max_floor = dfa.max_floor.fillna(dfa.max_floor.median())\n",
        "dfa[\"max1\"] = (dfa.max_floor==1)\n",
        "dfa[\"max0\"] = (dfa.max_floor==0)\n",
        "dfa[\"maxhuge\"] = (dfa.max_floor>80)\n",
        "dfa[\"lnmax\"] = np.log(dfa.floor+1)\n",
        "\n",
        "dfa[\"norooms\"] = dfa.num_room.isnull()\n",
        "dfa.num_room = dfa.num_room.fillna(dfa.num_room.median())\n",
        "dfa[\"zerorooms\"] = (dfa.num_room==0)\n",
        "dfa[\"lnrooms\"] = np.log( dfa.num_room + 1 )\n",
        "\n",
        "dfa[\"nokitch\"] = dfa.kitch_sq.isnull()\n",
        "dfa.kitch_sq = dfa.kitch_sq.fillna(dfa.kitch_sq.median())\n",
        "dfa[\"kitch1\"] = (dfa.kitch_sq==1)\n",
        "dfa[\"kitch0\"] = (dfa.kitch_sq==0)\n",
        "dfa[\"kitchhuge\"] = (dfa.kitch_sq>400)\n",
        "dfa[\"lnkitch\"] = np.log(dfa.kitch_sq+1)\n",
        "\n",
        "\n",
        "# Perhaps drop full_sq, life_sq, floor, max_floor, num_rooms, kitch_sq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8b496c7e-c438-4a48-1723-c9288b9da10c"
      },
      "outputs": [],
      "source": [
        "dfa[\"material0\"] = dfa.material.isnull()\n",
        "dfa[\"material1\"] = (dfa.material==1)\n",
        "dfa[\"material2\"] = (dfa.material==2)\n",
        "dfa[\"material3\"] = (dfa.material==3)\n",
        "dfa[\"material4\"] = (dfa.material==4)\n",
        "dfa[\"material5\"] = (dfa.material==5)\n",
        "dfa[\"material6\"] = (dfa.material==6)\n",
        "\n",
        "# \"state\" isn't explained but it looks like an ordinal number, so for now keep numberic\n",
        "dfa.loc[dfa.build_year>5,\"build_year\"] = np.NaN  # Value 33 seems to be invalid; others all 1-4\n",
        "dfa.state = dfa.state.fillna(dfa.state.median())\n",
        "\n",
        "# product_type gonna be ugly because there are missing values in the test set but not training\n",
        "# Check for the same problem with other variables\n",
        "dfa[\"owner_occ\"] = (dfa.product_type=='OwnerOccupier')\n",
        "dfa.owner_occ.fillna(dfa.owner_occ.mean())\n",
        "\n",
        "dfa = pd.get_dummies(dfa, dummy_na=True, columns=['sub_area'], drop_first=True)\n",
        "\n",
        "# Perhaps drop material, product_type\n",
        "# Perhaps drop all neighborhood variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e96e10d4-90e6-5287-d316-cf50ea4810be"
      },
      "outputs": [],
      "source": [
        "# Build year is ugly\n",
        "# Can be missing\n",
        "# Can be zero\n",
        "# Can be one\n",
        "# Can be some ridiculous pre-Medieval number\n",
        "# Can be some invalid huge number like 20052009\n",
        "# Can be some other invalid huge number like 4965\n",
        "# Can be a reasonable number but later than purchase year\n",
        "# Can be equal to purchase year\n",
        "# Can be a reasonable nubmer before purchase year\n",
        "\n",
        "dfa.loc[dfa.build_year>2030,\"build_year\"] = np.NaN\n",
        "dfa[\"nobuild\"] = dfa.build_year.isnull()\n",
        "dfa[\"sincebuild\"] = pd.to_datetime(dfa.timestamp).dt.year - dfa.build_year\n",
        "dfa.sincebuild.fillna(dfa.sincebuild.median(),inplace=True)\n",
        "dfa[\"futurebuild\"] = (dfa.sincebuild < 0)\n",
        "dfa[\"newhouse\"] = (dfa.sincebuild==0)\n",
        "dfa[\"tooold\"] = (dfa.sincebuild>1000)\n",
        "dfa[\"build0\"] = (dfa.build_year==0)\n",
        "dfa[\"build1\"] = (dfa.build_year==1)\n",
        "dfa[\"untilbuild\"] = -dfa.sincebuild.apply(np.min, args=[0]) # How many years until planned build\n",
        "dfa[\"lnsince\"] = dfa.sincebuild.apply( np.max, args=[0] ).add(1).apply(np.log)\n",
        "\n",
        "# Perhaps drop build_year, sincebuild"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "55332fff-fd53-5f9b-53b5-766228a6a151"
      },
      "outputs": [],
      "source": [
        "print('build0' in dfa.columns.values)\n",
        "dfa.build0.head()\n",
        "dfa.material.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "11c4e013-51cb-ba1b-c3d1-cf6eb81772ac"
      },
      "outputs": [],
      "source": [
        "# Want to check for valididty of relationships, e.g. kitch_sq < life_sq < full_sq\n",
        "# But this interacts with how variables are already processed, so that may have to be changed\n",
        "# For example, if kitch_sq is sometimes huge and there is a dummy to identify those huge cases,\n",
        "#  do we want a separate dummy to identify which of those cases are internally consistent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b68868b9-edb1-bdc6-a0e7-c3b95d26e367"
      },
      "outputs": [],
      "source": [
        "training = dfa[dfa.price_doc.notnull()]\n",
        "training.lnrp = training.price_doc.div(training.cpi).apply(np.log)\n",
        "y = training.lnrp\n",
        "X = training[[\"state\", \"owner_occ\",\n",
        "              \"fullzero\", \"fulltiny\", \"fullhuge\", \"lnfull\", \n",
        "              \"nolife\", \"lifezero\", \"lifetiny\", \"lifehuge\", \"lnlife\",\n",
        "              \"nofloor\", \"floor1\", \"floor0\", \"floorhuge\", \"lnfloor\",\n",
        "              \"nomax\", \"max1\", \"max0\", \"maxhuge\", \"lnmax\", \n",
        "              \"norooms\", \"zerorooms\", \"lnrooms\",\n",
        "              \"nokitch\", \"kitch1\", \"kitch0\", \"kitchhuge\", \"lnkitch\",\n",
        "#              \"material1\", \"material2\", \"material3\", \"material4\", \"material5\", \"material6\",\n",
        "              \"nobuild\", \"futurebuild\", \"newhouse\", \"tooold\", \n",
        "              \"build0\", \"build1\", \"untilbuild\", \"lnsince\",\n",
        "              'sub_area_Akademicheskoe',\n",
        "       'sub_area_Alekseevskoe', 'sub_area_Altufevskoe', 'sub_area_Arbat',\n",
        "       'sub_area_Babushkinskoe', 'sub_area_Basmannoe', 'sub_area_Begovoe',\n",
        "       'sub_area_Beskudnikovskoe', 'sub_area_Bibirevo',\n",
        "       'sub_area_BirjulevoVostochnoe', 'sub_area_BirjulevoZapadnoe',\n",
        "       'sub_area_Bogorodskoe', 'sub_area_Brateevo', 'sub_area_Butyrskoe',\n",
        "       'sub_area_Caricyno', 'sub_area_Cheremushki',\n",
        "       'sub_area_ChertanovoCentralnoe', 'sub_area_ChertanovoJuzhnoe',\n",
        "       'sub_area_ChertanovoSevernoe', 'sub_area_Danilovskoe',\n",
        "       'sub_area_Dmitrovskoe', 'sub_area_Donskoe', 'sub_area_Dorogomilovo',\n",
        "       'sub_area_FilevskijPark', 'sub_area_FiliDavydkovo',\n",
        "       'sub_area_Gagarinskoe', 'sub_area_Goljanovo',\n",
        "       'sub_area_Golovinskoe', 'sub_area_Hamovniki',\n",
        "       'sub_area_HoroshevoMnevniki', 'sub_area_Horoshevskoe',\n",
        "       'sub_area_Hovrino', 'sub_area_Ivanovskoe', 'sub_area_Izmajlovo',\n",
        "       'sub_area_Jakimanka', 'sub_area_Jaroslavskoe', 'sub_area_Jasenevo',\n",
        "       'sub_area_JuzhnoeButovo', 'sub_area_JuzhnoeMedvedkovo',\n",
        "       'sub_area_JuzhnoeTushino', 'sub_area_Juzhnoportovoe',\n",
        "       'sub_area_Kapotnja', 'sub_area_Konkovo', 'sub_area_Koptevo',\n",
        "       'sub_area_KosinoUhtomskoe', 'sub_area_Kotlovka',\n",
        "       'sub_area_Krasnoselskoe', 'sub_area_Krjukovo',\n",
        "       'sub_area_Krylatskoe', 'sub_area_Kuncevo', 'sub_area_Kurkino',\n",
        "       'sub_area_Kuzminki', 'sub_area_Lefortovo', 'sub_area_Levoberezhnoe',\n",
        "       'sub_area_Lianozovo', 'sub_area_Ljublino', 'sub_area_Lomonosovskoe',\n",
        "       'sub_area_Losinoostrovskoe', 'sub_area_Marfino',\n",
        "       'sub_area_MarinaRoshha', 'sub_area_Marino', 'sub_area_Matushkino',\n",
        "       'sub_area_Meshhanskoe', 'sub_area_Metrogorodok', 'sub_area_Mitino',\n",
        "       'sub_area_Molzhaninovskoe', 'sub_area_MoskvorecheSaburovo',\n",
        "       'sub_area_Mozhajskoe', 'sub_area_NagatinoSadovniki',\n",
        "       'sub_area_NagatinskijZaton', 'sub_area_Nagornoe',\n",
        "       'sub_area_Nekrasovka', 'sub_area_Nizhegorodskoe',\n",
        "       'sub_area_NovoPeredelkino', 'sub_area_Novogireevo',\n",
        "       'sub_area_Novokosino', 'sub_area_Obruchevskoe',\n",
        "       'sub_area_OchakovoMatveevskoe', 'sub_area_OrehovoBorisovoJuzhnoe',\n",
        "       'sub_area_OrehovoBorisovoSevernoe', 'sub_area_Ostankinskoe',\n",
        "       'sub_area_Otradnoe', 'sub_area_Pechatniki', 'sub_area_Perovo',\n",
        "       'sub_area_PokrovskoeStreshnevo', 'sub_area_PoselenieDesjonovskoe',\n",
        "       'sub_area_PoselenieFilimonkovskoe', 'sub_area_PoselenieKievskij',\n",
        "       'sub_area_PoselenieKlenovskoe', 'sub_area_PoselenieKokoshkino',\n",
        "       'sub_area_PoselenieKrasnopahorskoe',\n",
        "       'sub_area_PoselenieMarushkinskoe',\n",
        "       'sub_area_PoselenieMihajlovoJarcevskoe',\n",
        "       'sub_area_PoselenieMoskovskij', 'sub_area_PoselenieMosrentgen',\n",
        "       'sub_area_PoselenieNovofedorovskoe',\n",
        "       'sub_area_PoseleniePervomajskoe', 'sub_area_PoselenieRjazanovskoe',\n",
        "       'sub_area_PoselenieRogovskoe', 'sub_area_PoselenieShhapovskoe',\n",
        "       'sub_area_PoselenieShherbinka', 'sub_area_PoselenieSosenskoe',\n",
        "       'sub_area_PoselenieVnukovskoe', 'sub_area_PoselenieVoronovskoe',\n",
        "       'sub_area_PoselenieVoskresenskoe', 'sub_area_Preobrazhenskoe',\n",
        "       'sub_area_Presnenskoe', 'sub_area_ProspektVernadskogo',\n",
        "       'sub_area_Ramenki', 'sub_area_Rjazanskij', 'sub_area_Rostokino',\n",
        "       'sub_area_Savelki', 'sub_area_Savelovskoe', 'sub_area_Severnoe',\n",
        "       'sub_area_SevernoeButovo', 'sub_area_SevernoeIzmajlovo',\n",
        "       'sub_area_SevernoeMedvedkovo', 'sub_area_SevernoeTushino',\n",
        "       'sub_area_Shhukino', 'sub_area_Silino', 'sub_area_Sokol',\n",
        "       'sub_area_SokolinajaGora', 'sub_area_Sokolniki',\n",
        "       'sub_area_Solncevo', 'sub_area_StaroeKrjukovo', 'sub_area_Strogino',\n",
        "       'sub_area_Sviblovo', 'sub_area_Taganskoe', 'sub_area_Tekstilshhiki',\n",
        "       'sub_area_TeplyjStan', 'sub_area_Timirjazevskoe',\n",
        "       'sub_area_Troickijokrug', 'sub_area_TroparevoNikulino',\n",
        "       'sub_area_Tverskoe', 'sub_area_Veshnjaki', 'sub_area_Vnukovo',\n",
        "       'sub_area_Vojkovskoe', 'sub_area_Vostochnoe',\n",
        "       'sub_area_VostochnoeDegunino', 'sub_area_VostochnoeIzmajlovo',\n",
        "       'sub_area_VyhinoZhulebino', 'sub_area_Zamoskvoreche',\n",
        "       'sub_area_ZapadnoeDegunino', 'sub_area_Zjablikovo',\n",
        "       'sub_area_Zjuzino', 'sub_area_nan'\n",
        "             ]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5b59dd9-aea7-f9c2-e115-d2d0f28aeb59"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cf74aee5-9327-dac6-44e3-94efabb1041f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Imputer, StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Make a pipeline that transforms X\n",
        "pipe = make_pipeline(Imputer(), StandardScaler())\n",
        "pipe.fit(X_train)\n",
        "pipe.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "df881f79-1779-91c3-8088-5eb50661971e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "def rmsle_exp(y_true_log, y_pred_log):\n",
        "    y_true = np.exp(y_true_log)\n",
        "    y_pred = np.exp(y_pred_log)\n",
        "    return np.sqrt(np.mean(np.power(np.log(y_true + 1) - np.log(y_pred + 1), 2)))\n",
        "\n",
        "def score_model(model, pipe):\n",
        "    train_error = rmsle_exp(y_train, model.predict(pipe.transform(X_train)))\n",
        "    test_error = rmsle_exp(y_test, model.predict(pipe.transform(X_test)))\n",
        "    return train_error, test_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7f134699-2900-000f-27c6-e48e0feff7d8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression(fit_intercept=True)\n",
        "lr.fit(pipe.transform(X_train), y_train)\n",
        "\n",
        "print(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(lr, pipe)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "753e259b-a1c2-132b-3b00-28c64d755315"
      },
      "outputs": [],
      "source": [
        "pipe.fit(X)\n",
        "pipe.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b54bf33b-ec7e-f7c6-389c-d7e63b0c37be"
      },
      "outputs": [],
      "source": [
        "lr.fit(pipe.transform(X), y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0a673646-881b-fafc-5fd8-dfaba3d707f5"
      },
      "outputs": [],
      "source": [
        "testing = dfa[dfa.price_doc.isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2a3aa6b-9c69-486a-236a-81ba52903aea"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(columns=X.columns)\n",
        "for column in df_test.columns:\n",
        "        df_test[column] = testing[column]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c6bdb7d-5de1-c222-a9c2-47536d5db583"
      },
      "outputs": [],
      "source": [
        "# Make the predictions\n",
        "pi = pipe.transform(df_test)\n",
        "pred = lr.predict(pi)\n",
        "predictions = np.exp(pred)*testing.cpi\n",
        "\n",
        "# And put this in a dataframe\n",
        "predictions_df = pd.DataFrame()\n",
        "predictions_df['id'] = testing['id']\n",
        "predictions_df['price_doc'] = predictions\n",
        "predictions_df.head()\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "16ba45bc-296e-c680-7a51-f50f29c0a467"
      },
      "outputs": [],
      "source": [
        "predictions_df.to_csv('dumb_ols_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f8612645-bed7-3cfb-15dd-782b65080fb6"
      },
      "outputs": [],
      "source": [
        "np.std(pipe.transform(X)[:,15])\n",
        "print( X.iloc[:,15].mean() )\n",
        "print( X.iloc[:,15].std() )\n",
        "X.columns.values[15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "51a08093-bfe5-33b8-64b5-d1348474b541"
      },
      "outputs": [],
      "source": [
        "co = lr.coef_\n",
        "ra = range(len(co))\n",
        "mask = np.abs(co)>1e9\n",
        "ra*mask\n",
        "X.columns[mask].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ccd07865-a806-cb75-9119-74dbf75b8f36"
      },
      "outputs": [],
      "source": [
        "fo = \"y ~ \"\n",
        "i = 0\n",
        "for col in X.columns.values:\n",
        "    fo += col\n",
        "    i += 1\n",
        "    if (i<len(X.columns.values)):\n",
        "        fo += \"+\"\n",
        "fo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "70f6a331-c137-7951-aa5c-8e1da8d29904"
      },
      "outputs": [],
      "source": [
        "import statsmodels.formula.api as sm\n",
        "dfols = X.copy()\n",
        "dfols[\"y\"] = y \n",
        "# How did y get to be a column of X?\n",
        "# And now it's gone again. WTF?\n",
        "result = sm.ols(formula=fo, data=dfols).fit()\n",
        "#dfo = X.drop('y',axis=1)\n",
        "#sm.ols(y, X).fit()\n",
        "#result.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d416891e-cad1-fbc3-5636-4ce15aa170ac"
      },
      "outputs": [],
      "source": [
        "result.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "56de1ec7-81a9-3ea2-a0b6-2d62d61781a5"
      },
      "outputs": [],
      "source": [
        "\"y\" in X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d8dd0ff1-126d-f850-3969-e78ada5c2a1b"
      },
      "outputs": [],
      "source": [
        "mask = (X.isnull().sum().values>0)\n",
        "X.columns[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6359e452-1120-5177-336c-57b695c4425a"
      },
      "outputs": [],
      "source": [
        "print( X.shape )\n",
        "print( y.shape )\n",
        "print( y.isnull().sum() )\n",
        "print( np.sum(X.isnull().sum().values) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79fd4e24-232f-9304-970c-fb2fda7dc1ed"
      },
      "outputs": [],
      "source": [
        "X.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e8ec6b34-51b4-d4df-9880-ac227a638501"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3908ad8f-b070-257e-70f9-89c5d28bc24c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6544264f-07ff-baa2-b517-4730da887b40"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "957df104-446e-c4ae-1794-56eeea768c3d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3ec51749-299a-bc14-6566-a4be546f7596",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c06ce00c-8dbb-b114-e4b6-2b95c08f3513",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8edd7d43-0d95-ba98-a7bf-68caee17eda9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}