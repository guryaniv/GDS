{"cells": [{"source": ["**Introduction**\n", "\n", "I previously built a XGboost to label data without context. Thought the results are good, but it's not satisfying.\n", "\n", "And later I built a shallow RNN(due to the limitations of hardwares), and I found the RNN to be so disappoint.\n", "\n", "Therefore, I built a new XGboost model to use the context to label data.\n", "\n", "And most importantly, unlike the previous notebook, **this one is trained with all data, including PLAIN VERBATIM LETTERS etc.**\n", "\n", "The major difference in the XGboost without context and with context is their dataset preparation.\n", "\n", "XGboost with context(we will use XGBwithC for short) need the previous words(regardless of whether it comes from the same sentence or not). So we define a padding and 2 special symbols to let XGBoost understand, what's is the boundary of words and what are spaces that can be safely ignored.\n", "\n", "**Results**\n", "\n", "Both trained on 960,000 samples(words, not sentences) from en_train.csv and both have exactly the same parameters for training.\n", "\n", "After 60 epochs, here are the total different results:\n", "\n", "*XGBoost without context:\n", "\n", "valid-merror:**0.005521**\ttrain-merror:**0.004168***\n", "\n", "*XGBoost with context:\n", "\n", "valid-merror:**0.003729**\ttrain-merror:**0.000811***\n", "\n", "**Analysis**\n", "\n", "I noticed astonishingly, XGBoost with context performs so well that it outperforms XGboost without context by a great margin. And what's more, **XGboost with context nearly perfectly fits to the training dataset it has seen.** I believe, if built larger and fed more data, it might be able to challenge deep RNN.\n", "\n", "**Notebook explaintion**\n", "\n", "Due to the time limit on notebook, I choose 320,000 samples instead of 960,000 to save time.\n", "\n", "You can view outputs in this notebook.\n", "\n", "**Final Words**\n", "\n", "I have really limited time and I lack powerful computers(I only have my faithful alienware 15 r2 by my side)\n", "\n", "Therefore I won't be participating in this competition any more.\n", "\n", "And **I hope this script can help you in your research**."], "metadata": {"_uuid": "6eb071bde88ed08cb1074b68ec11f1fdb8be3e9d", "collapsed": true, "_cell_guid": "b4d0b03a-adea-4e73-ab2b-a97667efb4a7"}, "cell_type": "markdown"}, {"source": ["import pandas as pd\n", "import numpy as np\n", "import os\n", "import pickle\n", "import gc\n", "import xgboost as xgb\n", "import numpy as np\n", "import re\n", "import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "\n", "max_num_features = 10\n", "pad_size = 1\n", "boundary_letter = -1\n", "space_letter = 0\n", "max_data_size = 320000\n", "\n", "out_path = r'.'\n", "df = pd.read_csv(r'../input/en_train.csv')\n", "\n", "x_data = []\n", "y_data =  pd.factorize(df['class'])\n", "labels = y_data[1]\n", "y_data = y_data[0]\n", "gc.collect()\n", "for x in df['before'].values:\n", "    x_row = np.ones(max_num_features, dtype=int) * space_letter\n", "    for xi, i in zip(list(str(x)), np.arange(max_num_features)):\n", "        x_row[i] = ord(xi)\n", "    x_data.append(x_row)\n", "\n", "def context_window_transform(data, pad_size):\n", "    pre = np.zeros(max_num_features)\n", "    pre = [pre for x in np.arange(pad_size)]\n", "    data = pre + data + pre\n", "    neo_data = []\n", "    for i in np.arange(len(data) - pad_size * 2):\n", "        row = []\n", "        for x in data[i : i + pad_size * 2 + 1]:\n", "            row.append([boundary_letter])\n", "            row.append(x)\n", "        row.append([boundary_letter])\n", "        neo_data.append([int(x) for y in row for x in y])\n", "    return neo_data\n", "\n", "x_data = x_data[:max_data_size]\n", "y_data = y_data[:max_data_size]\n", "x_data = np.array(context_window_transform(x_data, pad_size))\n", "gc.collect()\n", "x_data = np.array(x_data)\n", "y_data = np.array(y_data)\n", "\n", "print('Total number of samples:', len(x_data))\n", "print('Use: ', max_data_size)\n", "#x_data = np.array(x_data)\n", "#y_data = np.array(y_data)\n", "\n", "print('x_data sample:')\n", "print(x_data[0])\n", "print('y_data sample:')\n", "print(y_data[0])\n", "print('labels:')\n", "print(labels)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "507c9117084d895edf7823bc21e512a299000220", "_cell_guid": "996fd5b5-2565-4d26-a7c2-49935e2a908e"}, "outputs": []}, {"source": ["x_train = x_data\n", "y_train = y_data\n", "gc.collect()\n", "\n", "x_train, x_valid, y_train, y_valid= train_test_split(x_train, y_train,\n", "                                                      test_size=0.1, random_state=2017)\n", "gc.collect()\n", "num_class = len(labels)\n", "dtrain = xgb.DMatrix(x_train, label=y_train)\n", "dvalid = xgb.DMatrix(x_valid, label=y_valid)\n", "watchlist = [(dvalid, 'valid'), (dtrain, 'train')]\n", "\n", "param = {'objective':'multi:softmax',\n", "         'eta':'0.3', 'max_depth':10,\n", "         'silent':1, 'nthread':-1,\n", "         'num_class':num_class,\n", "         'eval_metric':'merror'}\n", "model = xgb.train(param, dtrain, 50, watchlist, early_stopping_rounds=20,\n", "                  verbose_eval=10)\n", "gc.collect()\n", "\n", "pred = model.predict(dvalid)\n", "pred = [labels[int(x)] for x in pred]\n", "y_valid = [labels[x] for x in y_valid]\n", "x_valid = [ [ chr(x) for x in y[2 + max_num_features: 2 + max_num_features * 2]] for y in x_valid]\n", "x_valid = [''.join(x) for x in x_valid]\n", "x_valid = [re.sub('a+$', '', x) for x in x_valid]\n", "\n", "gc.collect()\n", "\n", "df_pred = pd.DataFrame(columns=['data', 'predict', 'target'])\n", "df_pred['data'] = x_valid\n", "df_pred['predict'] = pred\n", "df_pred['target'] = y_valid\n", "df_pred.to_csv(os.path.join(out_path, 'pred.csv'))\n", "\n", "df_erros = df_pred.loc[df_pred['predict'] != df_pred['target']]\n", "df_erros.to_csv(os.path.join(out_path, 'errors.csv'), index=False)\n", "\n", "model.save_model(os.path.join(out_path, 'xgb_model'))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "3565df1fd23bbd6d11adee7660c6837318d4a07d", "_cell_guid": "ecef095f-4641-4dd3-8d07-71eaed4bd95f"}, "outputs": []}], "metadata": {"language_info": {"mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 1}