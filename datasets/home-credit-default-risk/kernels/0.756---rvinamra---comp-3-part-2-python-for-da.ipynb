{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Continuation of WILL KOEHRSEN'S notebook used to learn analysis \n# using Python\n\n# here we will be using bureau and bureau_balance data apart from the \n# training data from the application zip fle\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# list of panda operations which will be used extensively:\n# groupby: group a dataframe by column (here, SK_ID_CURR)\n# agg: perform a calculation on the grouped data like mean, max, min etc.\n# merge: match the aggregated statistics to the appropriate client\n\n# reading the bureau file\nbureau = pd.read_csv('../input/bureau.csv')\nbureau.head()\n#print('Shape of the bureau file', bureau.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99f089b7809976896ae29be8be938a5b0084926f"},"cell_type":"code","source":"# groupby the client id, counting the previous loans and renaming the column\n\nprevious_loan_counts = bureau.groupby('SK_ID_CURR', as_index = False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f47b074403c99bd3869b28d52d64d8f79772ec84"},"cell_type":"code","source":"# joining with the training dataframe\ntrain = pd.read_csv('../input/application_train.csv')\ntrain = train.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n\n# filling the missing values with 0\ntrain['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6178dd4e74e3c6f3e25c6772c6f1313b69b1fa85"},"cell_type":"code","source":"# using the r-value and kde plots to find the usefulness of a variable\n\ndef kde_target(var_name, df):\n    \n    # Calculating the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # Calculating medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plotting the distribution for target = 0 and target = 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # printing out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    \n    # Printing out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8151af0718bad59cd9fc0047905d1eafdf024d3d"},"cell_type":"code","source":"# testing this with EXT_SOURCE_3 as it was found one of the most important \n# variable in Random Forest and Gradient Boosting method\n\nkde_target('EXT_SOURCE_3', train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e772fb72d40d8fc7da727c072fb4211d43f5e45"},"cell_type":"code","source":"# testing with the new variable created - previous_loan_counts\nkde_target('previous_loan_counts', train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6620f79944ee4c90fd8231bb659894a7b17d3d5"},"cell_type":"code","source":"# from the above plot, we can't be very certain about the impact of\n# previous_loan_count on TARGET \n\n# creating new variables from the bureau file\n\nbureau_agg = bureau.drop(columns = ['SK_ID_BUREAU']).groupby('SK_ID_CURR', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8af5ac5c85959d950003a5f08fbc1837f245f22"},"cell_type":"code","source":"# creating new variable names for each column\n# list of column names\n\ncolumns = ['SK_ID_CURR']\n\nfor var in bureau_agg.columns.levels[0]:\n    # skiping the id name as it is already added\n    if var != 'SK_ID_CURR':\n            for stat in bureau_agg.columns.levels[1][:-1]:\n                # making a new column for the variable and stat\n                columns.append('bureau_%s_%s' % (var, stat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddfa199db2f3df9b517a01bcbe36cf2bf7d6c3cb"},"cell_type":"code","source":"# assigning the list of column names as dataframe column names\nbureau_agg.columns = columns\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35518c46452fb7cb1e16aa312dcbfc74f00961bb"},"cell_type":"code","source":"# merging the new columns with the training data\n\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8a604c8eebd18d218215d5893081faf23807867"},"cell_type":"code","source":"# correaltion of the new variables with the target\n\n# list of new correlations\nnew_corrs = []\n\nfor col in columns:\n    # calculation correlation with the target\n    corr = train['TARGET'].corr(train[col])\n    \n    # appending the list as a tuple\n    new_corrs.append((col, corr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ea971993cb06b3cea5c25480f4e401d06667d1a"},"cell_type":"code","source":"# sorting and displaying the correlations\n\nnew_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)\nnew_corrs[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24f2bdd06d2fdf3a666f8e4ec7a3f9f4c5c5983e"},"cell_type":"code","source":"# none of the new variables have a significant correlation with the \n# TARGET variable\n# looking at the kde plot of the highest correlated variable\n\n# bureau_DAYS_CREDIT_mean:- How many days before current application\n# did client apply for Credit Bureau credit\n\nkde_target('bureau_DAYS_CREDIT_mean', train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f81ead828a3a9fc29db1447f4d8016a09150a60"},"cell_type":"code","source":"# encapsulating all the work related to numeric aggregation into \n# a function so as to use them with other dataframes in the future\n\ndef agg_numeric(df, group_var, df_name):\n    \"\"\" Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # removing the id variable other than the grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n        \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n    \n    # grouping by the specified variable and calculating the stats\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n    \n    # creating new column names\n    columns = [group_var]\n    \n    for var in agg.columns.levels[0]:\n        # skipping the grouping variable\n        if var != group_var:\n            for stat in agg.columns.levels[1][:-1]:\n                # making a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n            \n    agg.columns = columns\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ac3cfa0af3a25f3add6c3a136661c813594c5b4"},"cell_type":"code","source":"# checking whether the  function returns the same dataframe as before\n\nbureau_agg_new = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72709d52b2e6b772218547d1224aba0556319dbe"},"cell_type":"code","source":"bureau_agg.head()\n# the values match perfectly\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8dce0c1d3bfdde875dc58f7ca0f97dd50e06d36"},"cell_type":"code","source":"# another function to calculate correaltion of a variable with the 'TARGET'\n\ndef target_corrs(df):\n    \n    # list of correlations\n    corrs = []\n    for col in df.columns:\n        print(col)\n        #skipping the target columns as the r = 1 \n        if col != 'TARGET':\n            corr = df['TARGET'].corr(df[col])\n            \n            # appending the list as a tuple\n            corrs.append((col, corr))\n            \n    # sorting by their magnitude of correlations\n    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n    \n    return corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd482438588b7bf26a4171603fd5ab12ad767f69"},"cell_type":"code","source":"# checking the categorical columns\n# one-hot encoding for categorical columns\n\ncategorical = pd.get_dummies(bureau.select_dtypes('object'))\ncategorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\ncategorical.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2ddeb5e5c1f4281537433c1f4981a58d6af9975"},"cell_type":"code","source":"categorical_grouped = categorical.groupby('SK_ID_CURR').agg(['sum', 'mean'])\ncategorical_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61876c7dd0efed31595f54586e8a25f6d7ac58a7"},"cell_type":"code","source":"# sum represents the count for an individual and mean represents normaized count\n# renaming columns\n\ngroup_var = 'SK_ID_CURR'\ncolumns = []\n\nfor var in categorical_grouped.columns.levels[0]:\n    if var != group_var:\n        for stat in ['count', 'count_norm']:\n            columns.append('%s_%s' %(var, stat))\n            \ncategorical_grouped.columns = columns\ncategorical_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e91dda94e4e3ac0d8032a8f22f5c3d967dbdbb0"},"cell_type":"code","source":"# merging this dataframe into the training data\ntrain = train.merge(categorical_grouped, left_on = 'SK_ID_CURR', right_index = True, how = 'left')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4a1a43d5020746207d766ddac5c86b2a3f0b4ea"},"cell_type":"code","source":"train.iloc[:10, 123:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01c8dd9bdf44fde414b4646454addbcd08c31bed"},"cell_type":"code","source":"# function to handle categorical variables\n# it will calculate the counts and normalized counts\n\ndef count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    # selecting the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n    \n    # putting identifying id on the column\n    categorical[group_var] = df[group_var]\n    \n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    column_names = []\n    \n    for var in categorical.columns.levels[0]:\n        for stat in ['count', 'count_norm']:\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n            \n    categorical.columns = column_names\n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d737fafdee631dcf231cda2ac709033db10c435a"},"cell_type":"code","source":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e9134aa0b0e7bb66905f0ba1ad6d676b9d06ad2"},"cell_type":"code","source":"# using the bureau balance dataframe\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\nbureau_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a71e3afffca9e13c2e0e2bcae6e84f7f46618278"},"cell_type":"code","source":"# counting each type of status for each previous loan\n\nbureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a67e80737381143d9feadb6d4188e538a0f09a6e"},"cell_type":"code","source":"# calculating aggregation stats using a previously defined function -- agg_numeric\nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49934c72307d95894dccfc84a7d463b8469e2e27"},"cell_type":"code","source":"# aggregating dataframes for each client\n\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# including SK_ID_CURR\nbureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_by_loan.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e1beb065d9a15039621047d70e7fa3058dff96d"},"cell_type":"code","source":"bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\nbureau_balance_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"599f16e4c36c1a0c816c984924cfb6ee084d074a"},"cell_type":"code","source":"# Recap of all the things doen with bureau_balance dataframe:\n# Calculated numeric stats grouping by each loan\n# Made value counts of each categorical variable grouping by loan\n# Merged the stats and the value counts on the loans\n# Calculated numeric stats for the resulting dataframe grouping by the client id\n\n# the final dataframe has one row for each client\n# stats has been calculated for all of their loans with monthly information\n# before putting all these information in the main dataframe, resetting all the dataframes\n# using the function to build all the variables from the grounds up\n\n# freeing up memory by deleting old objects\nimport gc\ngc.enable()\ndel train, bureau, bureau_balance, bureau_agg, bureau_agg_new, bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbddc53a69f8982cbfca18d6437851c2cf983489"},"cell_type":"code","source":"# rereading new copies of all the dataframes\n\ntrain = pd.read_csv('../input/application_train.csv')\nbureau = pd.read_csv('../input/bureau.csv')\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fd529968d47abee2b68eb96e9d7121b56d233fa"},"cell_type":"code","source":"# counts in bureau dataframe\nbureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26d58f1fe6ef3186190498b4ac632f025e873b03"},"cell_type":"code","source":"# aggregated stats of bureau dataframe\nbureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7621c900625e7f111ca96102c626004b16e7cfa"},"cell_type":"code","source":"# counting values of bureau_balance df by loan\n\nbureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eca906506e16ebd2359636db290c7d1778c0c3cf"},"cell_type":"code","source":"# aggregated stats of bureau_balance by loan\n\nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6635eb834fdaeb3beb100549bcf82ed514748b00"},"cell_type":"code","source":"# aggregated stats of bureau_balance by client\n# grouping dataframe by loan\n\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# merging to include the SK_ID_CURR\n\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how  ='left')\n\n# aggregating stats for each client\n\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebb4649ddcd9ef39455bd7cc290b4f605b649ffb"},"cell_type":"code","source":"# inserting computed features in the training data\n\noriginal_features = list(train.columns)\nprint('Original Number of Features: ', len(original_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24376e1a1e7d59d84233fbf559d774bb49675bf2"},"cell_type":"code","source":"# merging with the value counts of bureau\n\ntrain = train.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# merging with the stats of bureau\n\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# merging merging with the monthly info grouped by client\n\ntrain = train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f7ccdb6105f02756d427452bc9e9f80483906f0"},"cell_type":"code","source":"new_features = list(train.columns)\nprint('Number of features using previous loans from other insti data: ', len(new_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4b0e047d97916486e437286be3b624c2e689524"},"cell_type":"code","source":"# dealing with the missing values\n# function to calculate missing values by column\n\ndef missing_values_table(df):\n    #total missing values\n    mis_val = df.isnull().sum()\n    \n    # percentage of missing values\n    mis_val_percent = 100*df.isnull().sum() / len(df)\n    \n    # making a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis = 1)\n    \n    # renaming the columns\n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0: 'Missing Values', 1: '% of Total Values'})\n    \n    # sorting the table by percentage of missing in descending order\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values('% of Total Values', ascending=False).round(1)\n    \n    # printing summary info\n    print (\"Your selected dataframe has \"+ str(df.shape[1]) + \" columns.\\n\" \"There are \" + str(mis_val_table_ren_columns.shape[0]) + \" columns that have missing values\")\n    \n    # returning the dataframe with missing info\n    return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a3b07256daac7b496461332aa601442c40c2868"},"cell_type":"code","source":"missing_train = missing_values_table(train)\nmissing_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f0a32fc18b99708c26b05b41998bd188eb39477"},"cell_type":"code","source":"# check to see whether any columns have more than 90% missing values (I don't there ain't any but just for future references and maintaining an order)\nmissing_train_vars = list(missing_train.index[missing_train['% of Total Values'] > 90])\nlen(missing_train_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1de6378407f37db4568d31627047c8fe9b5239c"},"cell_type":"code","source":"# merging test data with the appropriate data\n\ntest = pd.read_csv('../input/application_test.csv')\n\n# merging with the value counts of bureau, stats of bureau, and value counts of bureau_balance\n\ntest = test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01d81fad0fcbe5950e2e825bdf9662a52b420d59"},"cell_type":"code","source":"print('Shape of Test data: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f83d6c3af0cb46dd5e0f695d18c2ab8677bd3496"},"cell_type":"code","source":"# aligning test and train datasets\n# matching up their column number for compatibility\n\ntrain_labels = train['TARGET']\n\ntrain, test = train.align(test, join = 'inner', axis = 1)\ntrain['TARGET'] = train_labels\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32b2d68ee4ad8a49f859068267dd6156cafa319e"},"cell_type":"code","source":"print('Training data shape:', train.shape)\nprint('Testing data shape:', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dac4de71fae782ed5aece9cc21d6ba49d83bf55f"},"cell_type":"code","source":"# checking the %age of missing values in testing data\n\nmissing_test = missing_values_table(test)\nmissing_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59cbaa706f80011f827f4a9a08370998a9bc6aa1"},"cell_type":"code","source":"# none of them have missing values > 90%, might have to use another feature selection to reduce the dimensionality\n# saving the changed train and test data\n\n# CORRELATIONS\n\ncorrs = train.corr()\ncorrs = corrs.sort_values('TARGET', ascending = False)\n\n# ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50e0851f3ca13dafe6f94b29b802fc68418f970b"},"cell_type":"code","source":"# ten most negative correlations\npd.DataFrame(corrs['TARGET'].dropna().tail(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfc72ba027b446a1e270bad3d5ad88da9c720288"},"cell_type":"code","source":"kde_target(var_name='bureau_CREDIT_ACTIVE_Active_count_norm', df = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0e83565ae4ad7833e39058c9085d8912d3657d7"},"cell_type":"code","source":"# due to weak correlation the plot looks completely randoma and it won't be wise to draw any conclusions from it \n\n# to check collinearity in the data, finding correlations of one variable with another which might help in reducing dimensionality\n\n# checking variable pairs with correlation greater 0.8\nthreshold = 0.8\n\n# empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df827af2ed789181075ffa6abb7b1b9a1b8b1066"},"cell_type":"code","source":"# for the pairs of highly correlated variables we need to only one out of them\n# tracking columns to remove and columns already examined\n\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# iterating through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # removing only one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n                \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec3c8a75ea9d801063a21bc783f46a9dfc86fb5c"},"cell_type":"code","source":"# removing these variables from both test and training datasets\n\ntrain_corrs_removed = train.drop(columns = cols_to_remove)\ntest_corrs_removed = test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0c07396860799400c5d2b6149461d0c1fec48b0"},"cell_type":"code","source":"# saving new datasets created\n\ntrain_corrs_removed.to_csv('train_bureau_corrs_removed.csv', index = False)\ntest_corrs_removed.to_csv('test_bureau_corrs_removed.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"290c94f630b465fe39e4cf1ff529d35f6c4cf1db"},"cell_type":"code","source":"# MODELLING\n# control: only the data in application files\n# test one: data in application files along all the data recorded from bureau and bureau_balance\n# test two: data in the application files with all the data recorded from the bureau and bureau_balance files with highly correlated variables removed \n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc904ba878b9d42f5baffdcaf42b22a00e0006a4"},"cell_type":"code","source":"# comparing the raw version and the version with high corelations removed\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9af1d49cae4b2acaf697f6b3e066b6e9ff05e918"},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \n    \"\"\"\n    # sorting feature according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76b741cdb53ec1ffa4f03d36731cc4b2e2f6718a"},"cell_type":"code","source":"# establishing a control\n# using function described above that used GBM with the single main source application data \ntrain_control = pd.read_csv('../input/application_train.csv')\ntest_control = pd.read_csv('../input/application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b10b954beea256f832c56eff30bb31ed1157e591"},"cell_type":"code","source":"# the GBM functions returns a submission df which ca be uploaded to the competition, a f1 dataframe of feature importances adn metrics df with validation and test performance\n\nsubmission, f1, metrics = model(train_control, test_control)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"948f38b68145dbc798d3e117338e5d8452f651e3"},"cell_type":"code","source":"metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"452cec9d821955bc38240fe7bda226a507485ff6"},"cell_type":"code","source":"# extra steps are required for regularization will help to remove this slight overfitting \n# visualizing feature importance \nfi_sorted = plot_feature_importances(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbda68225743775291ea6bef64582d0359a20231"},"cell_type":"code","source":"submission.to_csv('control.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbfe13c7d0a556392d3027fabc12281af45c6490"},"cell_type":"code","source":"# running model on test one\n\nsubmission_raw, fi_raw, metrics_raw = model(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e2165c9cf490d1e79b1bd7fa651ece1af224323"},"cell_type":"code","source":"metrics_raw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e70574b298f72f3c3566b6894901b6a831d8358b"},"cell_type":"code","source":"# engineered df perform better than control ones\nfi_raw_sorted = plot_feature_importances(fi_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b33b92ad7505ca79142451d7174743db73d3931b"},"cell_type":"code","source":"# some of the engineered features make it to the most important list\n\ntop_100 = list(fi_raw_sorted['feature'])[:100]\nnew_features = [x for x in top_100 if x not in list(f1['feature'])]\n\nprint('%% of Top 100 Features created from the bureau data = %d.00' %len(new_features))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df7b1929b5ec386cfb4cffbf04d126dd020bd0c5"},"cell_type":"code","source":"submission_raw.to_csv('test_one.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"869bc91540f3fe45b3d97f1fc084deb99cc9c81a"},"cell_type":"code","source":"# test_two, highly correlated variables removed \n\nsubmission_corrs, fi_corrs, metrics_corr = model(train_corrs_removed, test_corrs_removed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd1966756cb975f858925cc3d122eed3ec83f87d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}