{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95c8728e8c14644f5afc58aa13fae26a93dac850"},"cell_type":"code","source":"# reading all the datasets\n\napplication_train = pd.read_csv('../input/application_train.csv')\napplication_test = pd.read_csv('../input/application_test.csv')\nbureau = pd.read_csv('../input/bureau.csv')\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\ncredit_card_balance = pd.read_csv('../input/credit_card_balance.csv')\ninstallments_payments = pd.read_csv('../input/installments_payments.csv')\npos_cash_balance = pd.read_csv('../input/POS_CASH_balance.csv')\nprevious_application = pd.read_csv('../input/previous_application.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4982b980fd12e4bea3ed99f207831efbc8710b33"},"cell_type":"code","source":"print(application_train.shape)\nprint(application_test.shape)\nprint(bureau.shape)\nprint(bureau_balance.shape)\nprint(credit_card_balance.shape)\nprint(installments_payments.shape)\nprint(pos_cash_balance.shape)\nprint(previous_application.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"998f948f6fb37635f0bf05c8d224f0c87fad8caa"},"cell_type":"code","source":"# Lets see the dataset\napplication_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5d4bb57cb2e2277d097ab6ed6a41b3b974bcba9"},"cell_type":"code","source":"missing_data =application_train.isnull().sum()\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7af62ae7b0e6d9b4a8d184ea5291d39e05d76990"},"cell_type":"code","source":"# it might be useful to see the percentage of data is missing\n\ntotal_cells = np.product(application_train.shape)\ntotal_missing_values = missing_data.sum()\n\n# Now calculate the percentage\n\n(total_missing_values/total_cells)*100 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0269ef67698a064b7b7836f70d75d9a69f695459"},"cell_type":"code","source":"# rows drop\nrows_with_na_dropped = application_train.dropna()\nrows_with_na_dropped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd305232b7d3f526cadd4a25f147e81cf472b221"},"cell_type":"code","source":"# just how much data did we lose?\nprint(\"Rows in Original dataset: %d \\n\" % application_train.shape[0])\nprint(\"Rows with na's dropped: %d\" % rows_with_na_dropped.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46928eeea3305b644d62f36a923c23d49d30b19a"},"cell_type":"code","source":"# column drop\ncolumns_with_na_dropped = application_train.dropna(axis =1)\ncolumns_with_na_dropped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a41e1b85aeb30293178b492672617f550aa0aff4"},"cell_type":"code","source":"# just how much data did we lose?\nprint(\"Columns in original dataset: %d \\n\" % application_train.shape[1])\nprint(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffacb804c122d5d29f11ec2c5194fcd38f9300ca"},"cell_type":"code","source":"missing_data1 =application_test.isnull().sum()\nmissing_data1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0104aa3d4a12c61a5f02f0d1119d12d79e3d18c"},"cell_type":"code","source":"# it might be useful to see the percentage of data is missing\n\ntotal_cells = np.product(application_test.shape)\ntotal_missing_values = missing_data1.sum()\n\n# Now calculate the percentage\n\n(total_missing_values/total_cells)*100 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9aded6152784549500384dc16c5559657270a98"},"cell_type":"markdown","source":"#### Exploratory Data Analysis\n\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.\n"},{"metadata":{"trusted":true,"_uuid":"7dd47374f0c8381fcab6a0eddd6b6696fff2c4e4"},"cell_type":"markdown","source":"Examine the Distribution of the Target Column\n\nThe target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.\n"},{"metadata":{"trusted":true,"_uuid":"ccc5dce1b3b456bf65960102387a011ee4b154b3"},"cell_type":"code","source":"application_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8ea0e53097bbeb734a9b5954389b2dba9789a67"},"cell_type":"code","source":"application_train['TARGET'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c599f82ac9b2e588d05a0e1acbc47754b6a2150"},"cell_type":"markdown","source":"When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can handle missing values with no need for imputation. Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now.\n"},{"metadata":{"trusted":true,"_uuid":"9947f1578266316af13a9282c830770fe0fb271f"},"cell_type":"code","source":"# Number of each type of column\napplication_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1f3b405dddb0ed8595382fce3bf52275b932e55"},"cell_type":"code","source":"application_train.select_dtypes('object').nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5c4a8093a627e778cb209a2ddd3e39e947617fb"},"cell_type":"code","source":"# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in application_train:\n    if application_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(application_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(application_train[col])\n            # Transform both training and testing data\n            application_train[col] = le.transform(application_train[col])\n            application_test[col] = le.transform(application_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cdcf19ff8c7d86944a6cac85e1b7031006b8e0e"},"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(application_train)\napp_test = pd.get_dummies(application_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b39f73848b20f745bf754100e8c1fce2ebad1fe"},"cell_type":"markdown","source":"Aligning Training and Testing Data\n\nThere need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!"},{"metadata":{"trusted":true,"_uuid":"0f0830d1668bab08a2602b24bea50956b246872c"},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b5d4958682f58e596259360c8306e6717429c25"},"cell_type":"markdown","source":"The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try dimensionality reduction (removing features that are not relevant) to reduce the size of the datasets.\n"},{"metadata":{"_uuid":"d4fec5b115f595c2b3e51c81530017935fd2d34b"},"cell_type":"markdown","source":"Anomalies\n\nOne problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method. The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:\n"},{"metadata":{"trusted":true,"_uuid":"fc6555a67a030a594491426dc0b9133e59bf5951"},"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa4bd4f123f50cc0a4b4c71ecd1baafb25b71e0e"},"cell_type":"markdown","source":"Those ages look reasonable. There are no outliers for the age on either the high or low end. How about the days of employment?\n"},{"metadata":{"trusted":true,"_uuid":"81bf765cee8161b5671be2fadc2d8b91e694b5ba"},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6c54fe22a2fcf5fde93c0f5301eb95891d506dd"},"cell_type":"code","source":"# That doesn't look right! The maximum value (besides being positive) is about 1000 years!\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e52d9c264327f727088dbaca71eb5817990f092f"},"cell_type":"code","source":"# Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of \n# default than the rest of the clients.\n\nanom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80414a0b8c213700a3e2355fc3434100d435071d"},"cell_type":"markdown","source":"Well that is extremely interesting! It turns out that the anomalies have a lower rate of default.\n\nHandling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous."},{"metadata":{"trusted":true,"_uuid":"fe9f05753c92a18e01732b0795d9d240f254be5c"},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6d1a4f38f4871f872cab83f70f066b8ee3a4b62"},"cell_type":"markdown","source":"The distribution looks to be much more in line with what we would expect, and we also have created a new column to tell the model that these values were originally anomalous (becuase we will have to fill in the nans with some value, probably the median of the column). The other columns with DAYS in the dataframe look to be about what we expect with no obvious outliers.\n\nAs an extremely important note, anything we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with np.nan in the testing data."},{"metadata":{"trusted":true,"_uuid":"b088a2076f2933808341785488cb0d2144d46bfe"},"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16fcf089ce6172199ccb5df35cf8d336932edaba"},"cell_type":"markdown","source":"\n\n\n\n\n\n\n\n\nCorrelations\n\nNow that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method.\n\nThe correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n•.00-.19 “very weak”\n•.20-.39 “weak”\n•.40-.59 “moderate”\n•.60-.79 “strong”\n•.80-1.0 “very strong”"},{"metadata":{"trusted":true,"_uuid":"14806dbcd2eccd8243a96ae7ca7af070806994ce"},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84705e5630f3892d9d214bfc00dd71af2385a0f2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}