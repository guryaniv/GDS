{"cells":[{"metadata":{"_uuid":"63ead0525bfccba8005636a9bbdf7009db3ef389"},"cell_type":"markdown","source":"# PUBG: PyTorch"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom copy import deepcopy\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\n\nimport gc, sys\ngc.enable()\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0bffc9af1f00c088f80d27174b1a3206f748bcb"},"cell_type":"markdown","source":"## Framework"},{"metadata":{"_uuid":"9dab86d68fca059efa941bee23b5782d0816d2ad"},"cell_type":"markdown","source":"### Common functions"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"405544b512bd3e3d02f40599342cfb4dcd27bbd3"},"cell_type":"markdown","source":"### Data preparation"},{"metadata":{"trusted":true,"_uuid":"3ab128856bd13b595315a4b5f8f3e07655c7c030"},"cell_type":"code","source":"def take_part_of_data(df, part):\n    \n    match_ids = df['matchId'].unique()\n    match_ids_part = np.random.choice(match_ids, int(part * len(match_ids)))\n    \n    df = df[df['matchId'].isin(match_ids_part)]\n    \n    del match_ids\n    del match_ids_part","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f0d4e4fbc5780116c5c220844d7641da175c858"},"cell_type":"markdown","source":"### Feature engineering"},{"metadata":{"trusted":true,"_uuid":"6547db0628265bc1b402ba7ed5563f8df0ce3382"},"cell_type":"code","source":"def add_new_features_1(df):\n    \n    # calculate total distance\n    df['totalDistance'] = df['rideDistance'] + df['walkDistance'] + df['swimDistance']\n    \n    # sum heals and boosts\n    df['healsAndBoosts'] = df['heals'] + df['boosts']\n    \n    # headshot rate\n    df['headshotKillsOverKills'] = df['headshotKills'] / df['kills']\n    df['headshotKillsOverKills'].fillna(0, inplace=True)\n    \n    # kill streake rate\n    df['killStreaksOverKills'] = df['killStreaks'] / df['kills']\n    df['killStreaksOverKills'].fillna(0, inplace=True)\n    \n    # kills and assists\n    df['killsAndAssists'] = df['kills'] + df['assists']\n    \n    # teamwork\n    df['assistsAndRevives'] = df['assists'] + df['revives']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c0d8350cd1f98447a7140627ea38043b2034143"},"cell_type":"code","source":"def add_new_features_2(df):\n    \n    # number of players joined\n    df['playersJoined'] = df.groupby('matchId')['matchId'].transform('count')\n    \n    # normalize features by number of players joined\n    df['killsAndAssistsOverPlayersJoined'] = df['killsAndAssists'] * ((100 - df['playersJoined']) / 100 + 1)\n    df['matchDurationOverPlayersJoined'] = df['matchDuration'] * ((100 - df['playersJoined']) / 100 + 1)\n    df['damageDealtOverPlayersJoined'] = df['damageDealt'] * ((100 - df['playersJoined']) / 100 + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad8b5633b6713f7e7354299cc21d5aea7f904751"},"cell_type":"code","source":"def add_new_features_3(df):\n    \n    # total distance over kills and assists\n    df['totalDistanceOverKillsAndAssists'] = df['totalDistance'] / df['killsAndAssists']\n    df['totalDistanceOverKillsAndAssists'].fillna(0, inplace=True)\n    df['totalDistanceOverKillsAndAssists'].replace(np.inf, 0, inplace=True)\n    \n    # total distance over heals and boosts\n    df['totalDistanceOverHealsAndBoosts'] = df['totalDistance'] / df['healsAndBoosts']\n    df['totalDistanceOverHealsAndBoosts'].fillna(0, inplace=True)\n    df['totalDistanceOverHealsAndBoosts'].replace(np.inf, 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11d4e7a15d6302aa112451689344679e112f089c"},"cell_type":"code","source":"def add_new_features_4(df):\n    \n    df['headshotRate'] = df['kills'] / df['headshotKills']\n    df['killStreakRate'] = df['killStreaks'] / df['kills']\n    df['healsAndBoosts'] = df['heals'] + df['boosts']\n    df['totalDistance'] = df['rideDistance'] + df['walkDistance'] + df['swimDistance']\n    df['killPlaceOverMaxPlace'] = df['killPlace'] / df['maxPlace']\n    df['headshotKillsOverKills'] = df['headshotKills'] / df['kills']\n    df['distanceOverWeapons'] = df['totalDistance'] / df['weaponsAcquired']\n    df['walkDistanceOverHeals'] = df['walkDistance'] / df['heals']\n    df['walkDistanceOverKills'] = df['walkDistance'] / df['kills']\n    df['killsPerWalkDistance'] = df['kills'] / df['walkDistance']\n    df[\"skill\"] = df['headshotKills'] + df['roadKills']\n    \n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    \n    df.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c4af3fc227c11763012e7520c47bcf608d91923"},"cell_type":"code","source":"def feature_engineering(df, is_train=True):\n    \n    # fix rank points\n    df['rankPoints'] = np.where(df['rankPoints'] <= 0, 0, df['rankPoints'])\n    \n    features = list(df.columns)\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    features.remove(\"matchDuration\")\n    features.remove(\"matchType\")\n    if 'winPlacePerc' in features:\n        features.remove('winPlacePerc')\n    \n    y = None\n    \n    # average y for training dataset\n    if is_train:\n        y = df.groupby(['matchId','groupId'])['winPlacePerc'].agg('mean')\n    elif 'winPlacePerc' in df.columns:\n        y = df['winPlacePerc']\n    \n    # mean by match and group\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    if is_train:\n        df_out = agg.reset_index()[['matchId','groupId']]\n    else:\n        df_out = df[['matchId','groupId']]\n    \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # max by match and group\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # max by match and group\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # number of players in group\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n    \n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    # mean by match\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    \n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    # number of groups in match\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    \n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    # drop match id and group id\n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n    \n    del agg, agg_rank\n    \n    return df_out, y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c40e27705574c955035ea1fd14270ad70da7c4c"},"cell_type":"markdown","source":"### Machine learning"},{"metadata":{"trusted":true,"_uuid":"d55b06b68252eaf889144db7f81bf3827f80c772"},"cell_type":"code","source":"class Estimator(object):\n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        raise NotImplementedException\n    \n    def predict(self, x):\n        raise NotImplementedException","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddd0403c8918043e99b2230a3423caebba37643f"},"cell_type":"code","source":"class ScikitLearnEstimator(Estimator):\n    \n    def __init__(self, estimator):\n        self.estimator = estimator\n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        self.estimator.fit(x_train, y_train)\n    \n    def predict(self, x):\n        return self.estimator.predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c4771d7271e62d2e7c94fced64cb2df83129f4d"},"cell_type":"code","source":"def fit_predict_step(estimator, x_train, y_train, train_idx, valid_idx, x_test, oof):\n    \n    # prepare train and validation data\n    x_train_train = x_train[train_idx]\n    y_train_train = y_train[train_idx]\n    x_train_valid = x_train[valid_idx]\n    y_train_valid = y_train[valid_idx]\n    \n    # fit estimator\n    estimator.fit(x_train_train, y_train_train, x_train_valid, y_train_valid)\n    \n    # collect OOF\n    oof_part = estimator.predict(x_train_valid)\n    \n    print('MAE:', mean_absolute_error(y_train_valid, oof_part))\n    \n    oof[valid_idx] = oof_part\n    \n    # make predictions for test data\n    y_part = estimator.predict(x_test)\n    \n    return y_part","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15979e3e270158f5378898802076ae1409778295"},"cell_type":"code","source":"def fit_predict(estimator, x_train, y_train, x_test):\n    \n    oof = np.zeros(x_train.shape[0])\n    \n    y = np.zeros(x_test.shape[0])\n    \n    kf = KFold(n_splits=5, random_state=42)\n    \n    for train_idx, valid_idx in kf.split(x_train):\n        \n        y_part = fit_predict_step(estimator, x_train, y_train, train_idx, valid_idx, x_test, oof)\n        \n        # average predictions for test data\n        y += y_part / kf.n_splits\n    \n    print('Final MAE:', mean_absolute_error(y_train, oof))\n    \n    return oof, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67fcee981ddaca32a8bb81b6f5b7609799f7025a"},"cell_type":"code","source":"def fit_step(estimator, x_train, y_train, train_idx, valid_idx, oof):\n    \n    # prepare train and validation data\n    x_train_train = x_train[train_idx]\n    y_train_train = y_train[train_idx]\n    x_train_valid = x_train[valid_idx]\n    y_train_valid = y_train[valid_idx]\n    \n    # fit estimator\n    estimator.fit(x_train_train, y_train_train, x_train_valid, y_train_valid)\n    \n    # collect OOF\n    oof_part = estimator.predict(x_train_valid)\n    \n    mae = mean_absolute_error(y_train_valid, oof_part)\n    print('MAE:', mae)\n    \n    oof[valid_idx] = oof_part\n    \n    return estimator, mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"954f90a79005aaea2c3eb2816c70e8639787e08f"},"cell_type":"code","source":"def fit(estimator, x_train, y_train):\n    \n    oof = np.zeros(x_train.shape[0])\n    \n    kf = KFold(n_splits=5, random_state=42)\n    \n    trained_estimators = []\n    \n    for train_idx, valid_idx in kf.split(x_train):\n        \n        e, mae = fit_step(estimator, x_train, y_train, train_idx, valid_idx, oof)\n        \n        trained_estimators.append(deepcopy(e))\n    \n    print('Final MAE:', mean_absolute_error(y_train, oof))\n    \n    return oof, trained_estimators","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"669933565c3461fcf750a414f9dc34c8a2f98322"},"cell_type":"code","source":"def predict(trained_estimators, x_test):\n    \n    y = np.zeros(x_test.shape[0])\n    \n    for estimator in trained_estimators:\n        \n        y_part = estimator.predict(x_test)\n        \n        # average predictions for test data\n        y += y_part / len(trained_estimators)\n    \n    return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf61ed8a245157bf8192ee66d5e29ba69ad4e25e"},"cell_type":"code","source":"def pipeline_fit(estimator, df_train, scaler=None):\n    \n    # add new features\n    add_new_features_4(df_train)\n    \n    # feature engineering\n    x_train, y_train = feature_engineering(df_train, is_train=True)\n    x_train = reduce_mem_usage(x_train)\n    gc.collect()\n    \n    # scale\n    if not (scaler is None):\n        scaler.fit(x_train)\n        scaled_x_train = scaler.transform(x_train)\n    else:\n        scaled_x_train = x_train.values\n    \n    # fit\n    oof, trained_estimators = fit(estimator, scaled_x_train, y_train.values)\n    \n    del x_train\n    del scaled_x_train\n    del y_train\n    gc.collect()\n    \n    return oof, trained_estimators","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65d241d0630684b5a067f3133810f1fc48679717"},"cell_type":"code","source":"def pipeline_predict(trained_estimators, df_test, scaler=None):\n    \n    # add new features\n    add_new_features_4(df_test)\n    \n    # feature engineering\n    x_test, _ = feature_engineering(df_test, is_train=False)\n    x_test = reduce_mem_usage(x_test)\n    gc.collect()\n    \n    # scale\n    if not (scaler is None):\n        scaled_x_test = scaler.transform(x_test)\n    else:\n        scaled_x_test = x_test.values\n    \n    # predict\n    y = predict(trained_estimators, scaled_x_test)\n    \n    del x_test\n    del scaled_x_test\n    gc.collect()\n    \n    return y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd2a0f1f94da41a5435e96c870178851e9ce8aa"},"cell_type":"markdown","source":"## Load train data"},{"metadata":{"_uuid":"8dcf02134aeb11d024f71f3ffbfff7de0050d1b6","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train_V2.csv', index_col='Id')\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fadd8e0fb1cd32eb1b216351dc7cd0b4f3497dec","trusted":true},"cell_type":"code","source":"df_train = reduce_mem_usage(df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f291168dc2a98bc7d1da94ac7685c9249758f7b0","trusted":true},"cell_type":"code","source":"df_train.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2221e28a1fba7ff3e05fc95184fdc74078d1be4"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84ed416c46b4b7f4fe7cf46e325759d6dfbba38f"},"cell_type":"markdown","source":"## Remove bad row"},{"metadata":{"_uuid":"0626ac7f1e3b0bef491d806699ebd0d44148fc84","trusted":true},"cell_type":"code","source":"df_train.drop(df_train[df_train['winPlacePerc'].isnull()].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91c8912fc72ae57c3693baa463deb91ad688f1b7"},"cell_type":"markdown","source":"## PyTorch"},{"metadata":{"trusted":true,"_uuid":"d8013f3136a52b5348055d98cde5bf55b0442297"},"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nfrom torch.nn.utils.weight_norm import weight_norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74846cb739e57844fb56d67f61cec5969c86e479"},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a586af03583ab010b2e29f61488490115b014eca"},"cell_type":"code","source":"class PyTorch(Estimator):\n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        \n        train_tensor = TensorDataset(\n            torch.from_numpy(x_train.astype('float32')),\n            torch.from_numpy(y_train.astype('float32')))\n        train_loader = DataLoader(train_tensor, batch_size=256, shuffle=True)\n        \n        self.model = nn.Sequential(\n            weight_norm(nn.Linear(x_train.shape[1], 128)),\n            nn.ReLU(),\n            weight_norm(nn.Linear(128, 128)),\n            nn.ReLU(),\n            weight_norm(nn.Linear(128, 128)),\n            nn.ReLU(),\n            weight_norm(nn.Linear(128, 128)),\n            nn.ReLU(),\n            weight_norm(nn.Linear(128, 1))).to(device)\n        \n        for m in self.model:\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight_v)\n                nn.init.kaiming_normal_(m.weight_g)\n                nn.init.constant_(m.bias, 0)\n        \n        criterion = nn.L1Loss()\n        optimizer = torch.optim.Adam(self.model.parameters(), betas=(0.9, 0.999), lr=1e-3)\n\n        self.model.train()\n        n_epochs = 50\n        for epoch in range(n_epochs):\n            epoch_loss = 0.0\n            for train_part, y_part in train_loader:\n                optimizer.zero_grad()\n                y_pred_part = self.model(train_part.to(device))\n                loss = criterion(y_pred_part.reshape(-1), y_part.to(device))\n                loss.backward()\n                optimizer.step()\n                epoch_loss += y_pred_part.shape[0] * loss.item()\n            print('Epoch %3d / %3d. Loss = %.5f' % (epoch + 1, n_epochs, epoch_loss / x_train.shape[0]))\n    \n    def predict(self, x):\n        self.model.eval()\n        x_tensor = torch.from_numpy(x.astype('float32'))\n        x_loader = DataLoader(x_tensor, batch_size=256, shuffle=False)\n        y_pred = np.empty(0)\n        with torch.no_grad():\n            for x_part in x_loader:\n                y_pred_part = self.model(x_part.to(device)).data.cpu().numpy().reshape(-1)\n                y_pred = np.append(y_pred, y_pred_part)\n        return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddbe1123947740e29df386759d3bf11fd08e9714"},"cell_type":"code","source":"%%time\n\nscaler = StandardScaler()\noof, trained_estimators = pipeline_fit(PyTorch(), df_train, scaler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89c77edd347bffde01f93667498b3b69b7dc57c7"},"cell_type":"code","source":"del df_train\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef83b3664766ed832082e6d04b0525df61acefd6"},"cell_type":"markdown","source":"## Load test data"},{"metadata":{"trusted":true,"_uuid":"ea99cfa7d72583071916d398b950246b7d6534a4"},"cell_type":"code","source":"df_test = pd.read_csv('../input/test_V2.csv', index_col = 'Id')\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d8c767a6c47e5e8a56be6e07c10baa6add0899c"},"cell_type":"code","source":"df_test = reduce_mem_usage(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a9d5dce92f279a6ff2cb51735bed863267f8216"},"cell_type":"code","source":"df_test_id = pd.DataFrame(index=df_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a60c44a96d09eedda65f742c0261562b5451f1b"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90eb05023537baac52fab54a896705c903354010"},"cell_type":"markdown","source":"## PyTorch"},{"metadata":{"trusted":true,"_uuid":"bb3a4c6bf7396bec812658888b6223d96cd67065"},"cell_type":"code","source":"y = pipeline_predict(trained_estimators, df_test, scaler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37e0213066501b2bbaa1f77de9adc7cb0c80b175"},"cell_type":"code","source":"del df_test\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf2e72f157fc61658b2bd8301fd4e5612dac9022"},"cell_type":"markdown","source":"## Save OOF"},{"metadata":{"trusted":true,"_uuid":"a9a11e401c048cd2c9f4bdeb2e91a85478b28bd8"},"cell_type":"code","source":"df_oof = pd.DataFrame()\ndf_oof['pytorch_oof'] = oof\ndf_oof.to_csv('pytorch_oof.csv', index_label='id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b75a27aa546a89e286c5880bfe77c16870a7ed58"},"cell_type":"markdown","source":"## Submission"},{"metadata":{"_uuid":"c464dad858f0459d7d6fe51ebb6183241f3f4685","trusted":true},"cell_type":"code","source":"df_submission = pd.DataFrame(index=df_test_id.index)\ndf_submission['winPlacePerc'] = y\ndf_submission.to_csv('pytorch_raw.csv', index_label='Id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"310047027ad879009596fc0a84d7b24593d925cc"},"cell_type":"markdown","source":"## Adjust predictions"},{"metadata":{"trusted":true,"_uuid":"7d90d99a8a412a6bf01147bdb066f77362e1e33b"},"cell_type":"code","source":"df_test = pd.read_csv('../input/test_V2.csv')\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"524d02f7822faf364db5cc7683ab13b900c3b086"},"cell_type":"code","source":"df_submission = df_submission.merge(df_test[['Id', 'matchId', 'groupId', 'maxPlace', 'numGroups']], on='Id', how='left')\ndf_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07c43541ee0c0fba99f319412dd2eaaa5986777c"},"cell_type":"code","source":"df_submission_group = df_submission.groupby(['matchId', 'groupId']).first().reset_index()\n\ndf_submission_group['rank'] = df_submission_group.groupby(['matchId'])['winPlacePerc'].rank()\n\ndf_submission_group = df_submission_group.merge(df_submission_group.groupby('matchId')['rank'].max().to_frame('max_rank').reset_index(), on='matchId', how='left')\n\ndf_submission_group['adjusted_perc'] = (df_submission_group['rank'] - 1) / (df_submission_group['numGroups'] - 1)\n\ndf_submission = df_submission.merge(df_submission_group[['adjusted_perc', 'matchId', 'groupId']], on=['matchId', 'groupId'], how='left')\n\ndf_submission['winPlacePerc'] = df_submission['adjusted_perc']\n\ndf_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59307cd5b3e11de317b1f0204c598062fa5cc3b1"},"cell_type":"code","source":"df_submission.loc[df_submission.maxPlace == 0, 'winPlacePerc'] = 0\ndf_submission.loc[df_submission.maxPlace == 1, 'winPlacePerc'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95bb1520f4adbe5d1b85906fb57944bb51ab4d4b"},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/anycode/simple-nn-baseline-4\nt = df_submission.loc[df_submission.maxPlace > 1]\ngap = 1.0 / (t.maxPlace.values - 1)\nfixed_perc = np.around(t.winPlacePerc.values / gap) * gap\ndf_submission.loc[df_submission.maxPlace > 1, 'winPlacePerc'] = fixed_perc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbb4614de7985abefb546aa1841ba6adb79d1e8a"},"cell_type":"code","source":"df_submission.loc[(df_submission.maxPlace > 1) & (df_submission.numGroups == 1), 'winPlacePerc'] = 0\n\nassert df_submission['winPlacePerc'].isnull().sum() == 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9257914773108e990428a8d51eb195c45739203"},"cell_type":"code","source":"df_submission[['Id', 'winPlacePerc']].to_csv('pytorch_adjusted.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}