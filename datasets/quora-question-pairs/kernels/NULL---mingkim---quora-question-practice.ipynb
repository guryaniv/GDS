{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "548b9ee4-34b0-c9d6-d898-cd04bea118f5",
        "_active": false,
        "collapsed": false
      },
      "source": "practice with the notebook of anokas",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd5df4a4-836f-a7be-64bf-9ed3a608894c",
        "_active": false
      },
      "outputs": [],
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "812b31af-a0df-e252-72ba-af7dd45bcd1e",
        "_active": false,
        "collapsed": false
      },
      "source": "import numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\n",
      "execution_count": null,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "348bec2a-8544-962b-ac0e-a5c355868b32",
        "_active": false,
        "collapsed": false
      },
      "source": "df_train = pd.read_csv(\"../input/train.csv\")\ndf_train.head()",
      "execution_count": null,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "118f96cc-c702-6e2a-683a-003f7a827151",
        "_active": false,
        "collapsed": false
      },
      "source": "print(\"Total number of question pairs for training : {}\".format(len(df_train)))\nprint('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nprint('Total number of questions in the training data: {}'.format(len(np.unique(qids))))\nprint('Number of questions that appear multiple times : {}'.format(np.sum(qids.value_counts() > 1)))\n\nplt.figure(figsize=(12, 5))\nplt.hist(qids.value_counts(), bins=50)\nplt.yscale('log', nonposy='clip')\nplt.title('log-histogram of question appearance counts')\nplt.xlabel('number of occurences of question')\nplt.ylabel('number of quesitons')\nprint()",
      "execution_count": null,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "b464449c-2f17-984a-1cc9-fd5552df36b4",
        "_active": false,
        "collapsed": false
      },
      "source": "from sklearn.metrics import log_loss\n\np = df_train['is_duplicate'].mean()\nprint('Prediected score : ', log_loss(df_train['is_duplicate'], np.zeros_like(df_train['is_duplicate']) + p ))\n\ndf_test = pd.read_csv('../input/test.csv')\nsub = pd.DataFrame({'test_id': df_test['test_id'], 'is_duplicate': p})\nsub.to_csv('naive_submission.csv', index=False)\nsub.head()\n",
      "execution_count": null,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "f6fb926c-cfa0-d69e-c80f-436d3f3b612d",
        "_active": false,
        "collapsed": false
      },
      "source": "df_test.head()",
      "execution_count": null,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "6308679b-bdf0-0334-06d0-94b5b12db701",
        "_active": false,
        "collapsed": false
      },
      "source": "print('Total number of question pairs for testing: {}'.format(len(df_test)))",
      "execution_count": null,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "d92dd58d-8fcc-4f57-cace-54425cdeb54c",
        "_active": false,
        "collapsed": false
      },
      "source": "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n\ndist_train = train_qs.apply(len)\ndist_test = test_qs.apply(len)\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=200, range=[0, 200], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=200, range=[0, 200], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of character count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of characters', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(),\n                                                                                                                    dist_train.std(),\n                                                                                                                    dist_test.mean(),\n                                                                                                                    dist_test.std(),\n                                                                                                                    dist_train.max(),\n                                                                                                                    dist_test.max()))",
      "execution_count": null,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "3c1a6f76-c83b-b593-c97f-7d4bda652c86",
        "_active": false,
        "collapsed": false
      },
      "source": "dist_train = train_qs.apply(lambda x : len(x.split(' ')))\ndist_test = test_qs.apply(lambda x : len(x.split(' ')))\n\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=50, range=[0, 50], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=50, range=[0, 50], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of word count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))",
      "execution_count": null,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "2c05de3e-519d-5ab2-bc1f-f576c1329e5c",
        "_active": false,
        "collapsed": false
      },
      "source": "from wordcloud import WordCloud\ncloud = WordCloud(width = 1440, height=1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')",
      "execution_count": 38,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "f3bf7390-446f-1990-9120-3b453eb9069b",
        "_active": false,
        "collapsed": false
      },
      "source": "qmarks = np.mean(train_qs.apply(lambda x : '?' in x))\nmath = np.mean(train_qs.apply(lambda x : '[math]' in x))\nfullstop = np.mean(train_qs.apply(lambda x : '.' in x))\ncapital_fist = np.mean(train_qs.apply(lambda x : x[0].isupper()))\ncapitals = np.mean(train_qs.apply(lambda x : max([y.isupper() for y in x])))\nnumbers = np.mean(train_qs.apply(lambda x : max([y.isdigit() for y in x])))\n",
      "execution_count": 41,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "3e582008-514c-28f8-2ae3-3d4125884cd7",
        "_active": false,
        "collapsed": false
      },
      "source": "# np.mean([True, True, False])",
      "execution_count": 44,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "101b26b2-5576-b492-47fa-cb72cb3c24e1",
        "_active": false,
        "collapsed": false
      },
      "source": "from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words={}\n    q2words={}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    \n    return R\n\nplt.figure(figsize=(15, 5))\ntrain_word_match = df_train.apply(word_match_share, axis=1, raw=True)\n\nplt.hist(train_word_match[df_train['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_word_match[df_train['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)",
      "execution_count": 52,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "55c30ed4-1382-4f0d-e2b7-e3a855d68228",
        "_active": true,
        "collapsed": false
      },
      "source": "# find conditional raw with specific index\ntrain_word_match[df_train['is_duplicate'] == 1]",
      "execution_count": 55,
      "cell_type": "code",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "26ae4719-c5b2-48c5-6fd1-fbf3991a9912",
        "_active": false,
        "collapsed": false
      },
      "source": "# TFIDF",
      "execution_count": null,
      "cell_type": "code",
      "outputs": []
    }
  ]
}