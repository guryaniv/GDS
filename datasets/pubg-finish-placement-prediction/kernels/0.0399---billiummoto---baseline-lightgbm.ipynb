{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\nimport xgboost\nfrom lightgbm import LGBMRegressor\npd.set_option('display.max_columns', 500)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfolder = \"../input/\"\n# folder = \"./input/\"\nprint(os.listdir(folder))\n\nNORMFACTOR = 1000\nTRAIN_SIZE = 0.9\nRANDOM_STATE = 212\nEARLY_STOP_ROUNDS = 10\nDEBUG = False\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"644a117a9978bbcb3aef3d3296da1a600a83ee77","trusted":true},"cell_type":"code","source":"cID = ['Id', 'groupId', 'matchId']\ncOrigGP= ['assists', 'boosts', 'damageDealt',\n       'DBNOs', 'headshotKills', 'heals', \n       'kills', 'killStreaks', 'longestKill', 'revives',\n       'rideDistance', 'roadKills', 'swimDistance', 'teamKills',\n       'vehicleDestroys', 'walkDistance', 'weaponsAcquired', 'killPlace'] # Gameplay information\ncRKAll = ['matchDuration', 'killPoints', 'maxPlace', 'numGroups','rankPoints','winPoints'] # Match derived information\ncRKPop =['matchDuration', 'maxPlace', 'numGroups']\ncMT = ['matchType'] # the only categorical variable\ncY = ['winPlacePerc']\n\ncAgg = ['totalKills','netKills', 'totalDistance','teamwork','itemUse','netKillsPerDist','netItemsPerDist','headsPerKill','netWeaponsPerDist']\ncGPAggMOnly = ['teamSize']\ncGP = cOrigGP+cAgg\ncGPAggM = cGP+cGPAggMOnly\n\ncGPNZip = [(col, col+\"N\") for col in cGP]\ncGPN = [col+\"N\" for col in cGP] # Normalized for match then team\ncGPT = [col+\"T\" for col in cGP]\ncGPM = [col+\"M\" for col in cGPAggM]\n\ncGPMInherit = [col+\"M\" for col in cGP]\n\ncGPAll = cGPAggM+cGPT+cGPM+cGPN\n\ndef preprocess(train, cOrigFeat):\n    train = train.copy()\n    cOrig = train.columns.values\n    # Teamkills not included in kills, roadKills is included\n    train['teamSizeFeat'] = train.groupby('groupId')['groupId'].transform('count')\n    train['totalKills'] = train['DBNOs'] + train['kills'] + train['teamKills']\n    train['totalDistance'] = train['rideDistance'] + train['walkDistance'] + train['walkDistance']\n    train['netKills'] = train['DBNOs'] + train['kills'] - train['teamKills']\n    train['teamwork'] = train['assists'] - train['teamKills']\n    train['itemUse'] = train['boosts'] + train['heals']\n    train['netKillsPerDist'] = train['netKills']/(train['totalDistance']+1)\n    train['netItemsPerDist'] = train['itemUse']/(train['totalDistance']+1)\n    train['headsPerKill'] = train['headshotKills']/(train['totalKills']+1)\n    train['netWeaponsPerDist'] = train['weaponsAcquired']/(train['totalDistance']+1)\n    train['totalMatchPlayers'] = train.groupby('matchId')['Id'].transform('count')\n    train['killsInvolved'] = train['DBNOs'] + train['kills'] + train['teamKills'] + train['assists']\n    train['pctMatchKills'] = train['totalKills']/train['totalMatchPlayers']\n    train['pctMatchKillInvolvement'] = train['killsInvolved']/train['totalMatchPlayers']\n    cEng = [col for col in train.columns.values if col not in cOrig]\n    cBase = list(cOrigFeat) + list(cEng)\n    \n    # Below are utility features which should not be trained on or modified\n    train['teamSize'] = train['teamSizeFeat']\n    train['sampleWeight'] = 1/train['teamSize']\n    cNotFeatures = [col for col in train.columns.values if col not in cBase]\n    \n    print('features done, scaling by match')\n    train[cBase] = minMidMaxGroupScale(train.groupby('matchId'), 0.75, base=train, col=cBase)\n    \n    print('match aggregation done, aggregating teams')\n    # The only thing that matters is teamwork. The features will be replace by sum by team, and min/max/mean by team will be added as well\n    train = train.merge(train.groupby('groupId')[cBase].transform(np.min), how='left', suffixes=[\"\",\"_min\"], left_index=True, right_index=True)\n    train = train.merge(train.groupby('groupId')[cBase].transform(np.max), how='left', suffixes=[\"\",\"_max\"], left_index=True, right_index=True)\n    train = train.merge(train.groupby('groupId')[cBase].transform(np.mean), how='left', suffixes=[\"\",\"_mean\"], left_index=True, right_index=True)\n    train[cBase] = train.groupby('groupId')[cBase].transform(np.sum)\n    \n    cDerived = [col for col in train.columns.values if col not in cBase+cNotFeatures]\n    cAll = cBase+cDerived\n    return (train, cAll)\n    \ndef minMidMaxGroupScale(g, midQuant, base, col=cGPAggM):\n    gMax = g[col].transform(np.max)\n    print(\"max done, quantiles\")\n    g75 = g[col].transform(lambda x: np.quantile(x, midQuant))\n    print(\"quantiles done, mins\")\n    gMin = g[col].transform(np.min)\n#     outdf = base[col]\n    print(\"mins done, time for big calc\")\n    outdf = (base[col] <= g75[col])*(base[col]-gMin[col])/(g75[col]-gMin[col]).replace(to_replace=0,value=1)*midQuant+(base[col] > g75[col])*((base[col]-g75[col])/(gMax[col]-g75[col]).replace(to_replace=0,value=1)*(1-midQuant) + midQuant) \n    print(\"big calc done\")\n    return outdf   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1e0ae37d95277bdc5ce18dff968985abe5f491a"},"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9621a1912f06293c23272108d1108ea748e9739e"},"cell_type":"code","source":"def shape(df):\n    return '{:,} rows - {:,} columns'.format(df.shape[0], df.shape[1])\n\ndef train_validation(df, train_size=TRAIN_SIZE):\n    \n    unique_games = df.matchId.unique()\n    train_index = round(int(unique_games.shape[0]*train_size))\n    \n    np.random.shuffle(unique_games)\n    \n    train_id = unique_games[:train_index]\n    validation_id = unique_games[train_index:]\n    \n    train = df[df.matchId.isin(train_id)]\n    validation = df[df.matchId.isin(validation_id)]\n    \n    return train, validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"960a603bb5d525ab0a23bb8ca5b5b6bc39f7cace"},"cell_type":"code","source":"df = pd.read_csv(folder+'train_V2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2761993e0c75e9072ec32975d538ffbad92c723"},"cell_type":"code","source":"if DEBUG:\n    df = df.head(100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffb11b884988d1ab8d5a2f0703d148d3ee5568a7"},"cell_type":"code","source":"time_0  = datetime.datetime.now()\n\ndf = reduce_mem_usage(df)\ndf, cAll = preprocess(df, cOrigGP)\nprint('Feature engineering done')\ndf = reduce_mem_usage(df)\n\ntime_1  = datetime.datetime.now()\nprint('Preprocessing took {} seconds'.format((time_1 - time_0).seconds))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"840bfa86f21e25d91a6a2eb7068f715a411ddf1a"},"cell_type":"code","source":"# lgbm code stolen from https://www.kaggle.com/mm5631/ml-workflow-data-science-approach\ntrain, validation = train_validation(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cf83d74efc994df0a562c544aa524734bae9d37"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b8960d0b69e8dc03ade3c72e5f906a01a24d68c"},"cell_type":"code","source":"train_weights = (1/train['teamSize'])\nvalidation_weights = (1/validation['teamSize'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed0c059a04689bba13852ed9df5a2093e132590e"},"cell_type":"code","source":"X_train = train[cAll]\nX_test = validation[cAll]\n\ny_train = train[cY]\ny_test = validation[cY]\n\nshape(X_train), shape(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a90ad476a1b32436da6b28ab0c98d2b03f3b39e7"},"cell_type":"code","source":"time_0 = datetime.datetime.now()\n\nlgbm = LGBMRegressor(objective='mae', n_estimators=250,  \n                     learning_rate=0.3, num_leaves=200, \n                     n_jobs=-1,  random_state=RANDOM_STATE, verbose=0)\n\nlgbm.fit(X_train, y_train, sample_weight=train_weights,\n         eval_set=[(X_test, y_test)], eval_sample_weight=[validation_weights], \n         eval_metric='mae', early_stopping_rounds=EARLY_STOP_ROUNDS, \n         verbose=0)\n\ntime_1  = datetime.datetime.now()\n\nprint('Training took {} seconds. Best iteration is {}'.format((time_1 - time_0).seconds, lgbm.best_iteration_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"259a0a75a883f536a16bf1950b95e304a2b8bbca"},"cell_type":"code","source":"print('Mean Absolute Error is {:.5f}'.format(mean_absolute_error(y_test, lgbm.predict(X_test, num_iteration=lgbm.best_iteration_), sample_weight=validation_weights)))\nprint('R2 score is {:.2%}'.format(r2_score(y_test, lgbm.predict(X_test, num_iteration=lgbm.best_iteration_), sample_weight=validation_weights)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c30529d21de37f108cf5fb4a515e5fa8a0697a31"},"cell_type":"code","source":"def plot_training(lgbm):\n    \n    fig, ax = plt.subplots(figsize=(13,7))\n    losses = lgbm.evals_result_['valid_0']['l1']\n    ax.set_ylim(np.nanmax(losses), 0)\n    ax.set_xlim(0,100)\n    ax.set_xlabel('n_estimators')\n    ax.set_ylabel('Mean Asbolute Error')\n    ax.set_title('Evolution of MAE over training iterations')\n    ax.plot(losses, color='grey');\n    \nplot_training(lgbm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e30685e16951f84cc789063810ae067e3a91cef"},"cell_type":"markdown","source":"Cool, it all works, let's train on entire training set and predict for submission. And make another submission using the current trained model just to see if there's any overfitting."},{"metadata":{"trusted":true,"_uuid":"187f7e5e2d2a1141b8049565e4d6b5d316c42e07"},"cell_type":"code","source":"test = reduce_mem_usage(pd.read_csv(folder+'test_V2.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f02ec1fd6e62a925ffc478569d97c0fd6ce994b5"},"cell_type":"code","source":"if DEBUG:\n    test = test.head(10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2fbe13366c0187c6a1a854c66035ae6d0a31da8"},"cell_type":"code","source":"time_0  = datetime.datetime.now()\ntest, cAll = preprocess(test, cOrigGP)\ntime_1  = datetime.datetime.now()\nprint('Preprocessing took {} seconds'.format((time_1 - time_0).seconds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06200037a01d722ca0ec00024b02ecb467774280"},"cell_type":"code","source":"test = reduce_mem_usage(test)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"260d89d03c5b24260d971efa562f8593c42c9c43"},"cell_type":"code","source":"X_sub = test[cAll]\nshape(X_sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9747a2b09dff263c759e7e7c7e6adef0a19c0104"},"cell_type":"code","source":"y_sub_unfull = lgbm.predict(X_sub, num_iteration=lgbm.best_iteration_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6d95cd8bbd2781570249c4e7540b375d42ddc73"},"cell_type":"markdown","source":"Create submisison file"},{"metadata":{"trusted":true,"_uuid":"471a504e2524032239fcaa165b4cc9defcc7951a"},"cell_type":"code","source":"sub = test[['Id']]\nsub['winPlacePerc'] = y_sub_unfull","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cb08447a0a4e8f07122001dffa65c7e72a2c817"},"cell_type":"code","source":"sub.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad981b2bcb5491f5768e3c4588b26328a92952c5"},"cell_type":"code","source":"sub.to_csv('sub_LGBM_unfull.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40c23f8e0e4a6472429b901d7294145bea9bb4c3"},"cell_type":"markdown","source":"Do same for full training"},{"metadata":{"trusted":true,"_uuid":"bfa5b7b1fc498835ba019a008eed7003a49a3c31"},"cell_type":"code","source":"X_train = df[cAll]\ny_train = df[cY]\ntrain_weights = (1/df['teamSize'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ef88360e959575b90dd3de7f9bf292f5269ad5e"},"cell_type":"code","source":"time_0 = datetime.datetime.now()\n\nlgbm = LGBMRegressor(objective='mae', n_estimators=250,  \n                     learning_rate=0.3, num_leaves=200, \n                     n_jobs=-1,  random_state=RANDOM_STATE, verbose=1)\n\nlgbm.fit(X_train, y_train, sample_weight=train_weights)\n\ntime_1  = datetime.datetime.now()\n\nprint('Training took {} seconds. Best iteration is {}'.format((time_1 - time_0).seconds, lgbm.best_iteration_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d6a01f7e89610e40f6c98e46b6216dce192b14d"},"cell_type":"code","source":"y_sub_full = lgbm.predict(X_sub, num_iteration=lgbm.best_iteration_)\nsub2 = test[['Id']]\nsub2['winPlacePerc'] = y_sub_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0febc1d04de73a243ecfa8ff013f22d3cd61f533"},"cell_type":"code","source":"sub2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"665d26ca4d51be0e6862956b758db73327a5c7f5"},"cell_type":"code","source":"sub2.to_csv('sub_LGBM_full.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}