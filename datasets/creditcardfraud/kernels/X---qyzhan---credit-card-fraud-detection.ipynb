{"cells":[{"metadata":{"_cell_guid":"71cf9d69-032c-4e3c-aae4-fff3f2c4be81","_uuid":"6793167d7f8c9745505b6b4c0fc537e26f0c95cb","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# Import librairies\nimport math\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\"../input/creditcard.csv\")\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ce2f144b82a15d2f214af1d81c59458a272c6fd6"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd43bf1ac4cb01067fb9404d18b728f52fad0073"},"cell_type":"code","source":"# EDA\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19294bd82e1f36c48369b4f0a96097ff3b475e8b"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e61b5260bafcfbbc3a90b4704fb21c9ba3500642"},"cell_type":"code","source":"# data dictionary: positive class is fraud, negative is not; only 0 and 1\nfraud_ind=df[df.Class==1].index\nnofraud_ind=df[df.Class==0].index\nfraud_num=len(fraud_ind)\nnofraud_num=len(nofraud_ind)\nfraud_perc=round(fraud_num/(fraud_num+nofraud_num),5)*100\nprint(\"% of fraud of all transactions is \", fraud_perc, \"%\")\n# imbalanced dataset\n# fraudulent transactions make up 0.173% portion of the data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"02f299e179461304a0642850b501374aa9926a02"},"cell_type":"code","source":"# Feature Selection\n# Correlation\nxx=df.drop(['Class','Time'],axis=1)\nplt.figure(figsize=(20,20))\nsns.heatmap(xx.corr(),annot=True,fmt= '.1f')\nplt.title('Heatmap correlation')\nplt.show()\n# Most features have no or low correlations\n# Selected features: V1-28, Amount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0fd71daae0ef1f27401b7df8f25e41f1ecc29b4a"},"cell_type":"code","source":"# Features: Amount, Time\nf, (ax_fraud, ax_not) = plt.subplots(2, 1, sharex=True, figsize=(16,6))\nbins = 30\n\nax_fraud.hist(df.Amount[df.Class==1],bins=bins,normed=True,color='coral')\nax_fraud.set_title('Fraud Transactions over Amount')\n\nax_not.hist(df.Amount[df.Class==0],bins=bins,normed=True,color='seagreen')\nax_not.set_title('Non-Fraud Transactions over Amount')\n\nplt.xlabel('Amount')\nplt.ylabel('% of Transactions')\nplt.yscale('log')\nplt.show()\n# fraudulent transactions have a higher average amount per transaction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4518cd0d6ebbc8b9354c78a6c2c0ebc8cfcaaf74"},"cell_type":"code","source":"bins=80\nplt.figure(figsize=(8,6))\nplt.hist(df.Time[df.Class==1],bins=bins,normed=True,alpha=0.9,label='Fraud',color='red')\nplt.hist(df.Time[df.Class==0],bins=bins,normed=True,alpha=0.9,label='Not Fraud',color='lightblue')\nplt.legend(loc='upper right')\nplt.xlabel('Time in Sec')\nplt.ylabel('% of Transactions')\nplt.title('Transactions over Time')\nplt.show()\n# Inverse trends between fraudulent activity and nonfraudulent activity.\n# Downturn in regular transactions, fraudulent activity increase. eg: 100,000 sec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4bcff2e425e371b8c29d1a391f0cbfbe182249d6"},"cell_type":"code","source":"# Check features V1-V28 at once\ny=df.Class\nx=df.drop(['Class','Time','Amount'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1575a990ab1d799c4a6d0b39057914279f859d3c"},"cell_type":"code","source":"import seaborn as sns\n# choose V1-V10\nsub_df_a=pd.concat([y,x.iloc[:,0:10]],axis=1)\nsub_df_aa=pd.melt(sub_df_a,id_vars=\"Class\",var_name=\"Feature\",value_name='Value')\nplt.figure(figsize=(20,8))\nsns.violinplot(x=\"Feature\",y=\"Value\",hue=\"Class\",data=sub_df_aa, split=True)\n# need to scale x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"01bf9272f87921906896df56ad38e1d4701e5be9"},"cell_type":"code","source":"# scale x\nx_scaled=(x-x.min())/(x.max()-x.min()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"afbb66ca8519cca52d3623e9925de0d749bf5528"},"cell_type":"code","source":"# Easiness to see graph: make 3 sub dfs to better v\nsub_df1=pd.concat([y,x_scaled.iloc[:,0:10]],axis=1)\nsub_df2=pd.concat([y,x_scaled.iloc[:,10:19]],axis=1)\nsub_df3=pd.concat([y,x_scaled.iloc[:,19:28]],axis=1)\nsub_df11=pd.melt(sub_df1,id_vars=\"Class\",var_name=\"Feature\",value_name='Value')\nsub_df22=pd.melt(sub_df2,id_vars=\"Class\",var_name=\"Feature\",value_name='Value')\nsub_df33=pd.melt(sub_df3,id_vars=\"Class\",var_name=\"Feature\",value_name='Value')\n\nplt.figure(figsize=(20,8))\nsns.violinplot(x=\"Feature\",y=\"Value\",hue=\"Class\",data=sub_df11, split=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"14281708fd78279135588217b36ffc93ab363da1"},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.violinplot(x=\"Feature\",y=\"Value\",hue=\"Class\",data=sub_df22, split=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"40cab482c333b0f3982647e018b274d7cf6cf7f8"},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.violinplot(x=\"Feature\",y=\"Value\",hue=\"Class\",data=sub_df33, split=True)\n# Most of the features have difference between frauds and non-frauds\n# But eg: V20 and 22 are very symmetric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b9999ae70ac7232b8e6edd9c66f0a1f70396cf50"},"cell_type":"code","source":"# Drop all of the features that have very similar distributions between the two types of transactions.\ndf_features=df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8','Time','Class'],axis=1)\n# Normalize Amount\ndf_features[\"Amount\"]=(df_features[\"Amount\"]-df_features[\"Amount\"].mean())/df_features[\"Amount\"].std()\n# Create train and test datasets\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(df_features,y,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3d9139b9e3bd311f27678b7f4baac1a74f703fc3"},"cell_type":"code","source":"# Linear classifier in tensorflow\nnV01 = tf.feature_column.numeric_column('V1')\nnV02 = tf.feature_column.numeric_column('V2')\nnV03 = tf.feature_column.numeric_column('V3')\nnV04 = tf.feature_column.numeric_column('V4')\nnV05 = tf.feature_column.numeric_column('V5')\nnV06 = tf.feature_column.numeric_column('V6')\nnV07 = tf.feature_column.numeric_column('V7')\nnV09 = tf.feature_column.numeric_column('V9')\nnV10 = tf.feature_column.numeric_column('V10')\nnV11 = tf.feature_column.numeric_column('V11')\nnV12 = tf.feature_column.numeric_column('V12')\nnV14 = tf.feature_column.numeric_column('V14')\nnV16 = tf.feature_column.numeric_column('V16')\nnV17 = tf.feature_column.numeric_column('V17')\nnV18 = tf.feature_column.numeric_column('V18')\nnV19 = tf.feature_column.numeric_column('V19')\nnV21 = tf.feature_column.numeric_column('V21')\nnV22 = tf.feature_column.numeric_column('V22')\nnV30 = tf.feature_column.numeric_column('Amount')\n\nfeatures=[nV01,nV02,nV03,nV04,nV05,nV06,nV07,nV09,nV10,nV11,nV12,nV14,nV16,nV17,nV18,nV19,nV21,nV30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"088edbd1f19bcd6809d1151fbd6bbe7b082cc36f"},"cell_type":"code","source":"# Classification\n# Tensorflow: Linear Classifier\ninput_func=tf.estimator.inputs.pandas_input_fn(x=x_train,y=y_train,batch_size=100,num_epochs=1000,shuffle=True) \nmodel=tf.estimator.LinearClassifier(feature_columns=features,n_classes=2)\nmodel.train(input_fn=input_func,steps=1000)\n\nresult1=model.evaluate(tf.estimator.inputs.pandas_input_fn(x=x_train,y=y_train,batch_size=10, num_epochs=1, shuffle=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"155e2cdc7340eed8c7e12c698e8ad1a21f0d711e"},"cell_type":"code","source":"# Result of linear classification\nprint(result1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bb8ce29122dc0530e65019a256da256435151e8e"},"cell_type":"code","source":"# Test Linear classification\neval_input_func = tf.estimator.inputs.pandas_input_fn(x=x_test,y=y_test,batch_size=10, num_epochs=1, shuffle=False)\nresults2=model.evaluate(eval_input_func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"05bac422fede1b62135e21304a3c610331e860ed"},"cell_type":"code","source":"print(results2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"077987eb7d08877ba6fd56d59a156ec3da83cab4"},"cell_type":"code","source":"# Prediction:\nfrom sklearn import metrics\npred_input_func= tf.estimator.inputs.pandas_input_fn(x=x_test,batch_size=10,num_epochs=1,shuffle=False)\npredictions = model.predict(pred_input_func)\n\ny_pred=[d['logits'] for d in predictions]\nfpr,tpr,thresholds=metrics.roc_curve(y_test, y_pred)\nroc_auc=metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(10,10))\nplt.title('ROC-Tensorflow')\nplt.plot(fpr, tpr,'b', label='Area under curve = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n# AUC=0.97 good to use the model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"727a1be0-dbe8-4215-9dae-8573576e51c4","_uuid":"feb14571ba4e146289d119f295678e829880f15a","trusted":true},"cell_type":"code","source":"# Neural network: use all the attributes\nx_neural=df.drop(['Class'],axis=1)\nx_scaled_neural=(x_neural-x_neural.min())/(x_neural.max()-x_neural.min())\ny_neural=df.Class\nfrom sklearn.model_selection import train_test_split\nx_train_neural,x_test_neural,y_train_neural,y_test_neural=train_test_split(x_scaled_neural,y_neural,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3e05bec8-06eb-4063-b1bb-8ab30e071b2a","_uuid":"fbd37c2c99d12b6d11e61500e9d12a818ae2491e","collapsed":true,"trusted":true},"cell_type":"code","source":"# Feed data into the network\ndef to_one_hot(c, depth):\n    i=np.identity(depth)\n    return i[c,:]\n\ndef train_batch(batch_size):\n    for j in range(int(len(x_train_neural)/batch_size)):\n        start=batch_size*j\n        end=start+batch_size\n        \n        train_x_batch=x_train_neural[start:end]\n        train_y_batch=y_train_neural[start:end]\n        \n        train_y_batch=np.apply_along_axis(lambda x:to_one_hot(x,depth=2),0,train_y_batch)\n        \n        yield train_x_batch, train_y_batch\n        \ndef get_test_data():\n    return x_test_neural, np.apply_along_axis(lambda x: to_one_hot(x, depth=2), 0, y_test_neural)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bff6ddea556418a75cc9594158c81e753519bc86"},"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\n\nX= tf.placeholder(tf.float32, [None, 30]) # inputs\nY= tf.placeholder(tf.float32, [None, 2]) # targets\n\ndef forward_propogation(X): # model\n    num_neural= 10\n    weights={\"lvl_1\": tf.Variable(tf.random_normal([30,num_neural])), \n               \"output\": tf.Variable(tf.random_normal([num_neural,2]))}\n    \n    biases={\"lvl_1\": tf.Variable(tf.random_normal([num_neural])), \n            \"output\": tf.Variable(tf.random_normal([2]))}\n    \n    h1=tf.add(tf.matmul(X, weights[\"lvl_1\"]), biases[\"lvl_1\"])\n    h1=tf.nn.relu(h1)\n    \n    h2=tf.add(tf.matmul(h1, weights[\"output\"]), biases[\"output\"])\n    output=tf.nn.relu(h2)\n    \n    return output\n\n# Minimize loss\nlogits=forward_propogation(X)\nloss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\noptimizer=tf.train.AdamOptimizer(0.01).minimize(loss) \n\n# Confusion matrix accuracy\ncorrect = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1)) \naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) \n\n# Precision-recall curve\ndecision_variable = tf.nn.softmax(logits) \n\nwith tf.Session() as sess: \n    sess.run(tf.global_variables_initializer())\n    \n    # train phase\n    batch_size = 128\n    n_epochs = 3\n    for epoch in range(n_epochs):\n        epoch_loss = 0\n        batch_generator = train_batch(batch_size)\n        for batch in batch_generator:\n            batch_x, batch_t = batch \n            _, curr_loss = sess.run([optimizer, loss], feed_dict={Y: batch_t, X: batch_x})\n            epoch_loss += curr_loss\n            \n        print(\"Epoch \" + str(epoch+1) + \" loss: \" + str(epoch_loss))\n        \n    # test phase\n    test_x, test_t = get_test_data()\n    test_y = sess.run(decision_variable, feed_dict={X: test_x})\n    \n    auprc = average_precision_score(test_t[:,0], test_y[:,0])\n    precision, recall, _ = precision_recall_curve(test_t[:,0], test_y[:,0])\n\n    # plot pr curve\n    plt.plot(recall, precision)\n    plt.grid()\n    plt.show()\n    \n    # print average precision\n    print(auprc)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}