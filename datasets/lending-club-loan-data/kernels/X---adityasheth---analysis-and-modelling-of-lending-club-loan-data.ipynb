{"cells":[{"metadata":{"_cell_guid":"3080aa94-09db-471c-a260-5e32188d2b7a","_uuid":"5268cb7ff02f7943ddb617087fadbe5895ac3539"},"cell_type":"markdown","source":"***Visualization(Exploratory data analysis) - Phase 1 ***\n* ***Major questions to answer(A/B Testing):***\n1. Does the installment amount affect loan status ?\n2. Does the installment grade affect loan status ?\n3. Which grade has highest default rate ? \n4. Does annual income/home-ownership affect default rate ?\n5. Which state has highest default rate ?\n* ***Text Analysis - Phase 2 ***\n6. Is it that a people with a certain empoyee title are taking up more loans as compared to others ? \n7. Does a specific purpose affect loan status ?\n* ***Model Building - Phase 3***\n8. Trying various models and comparing them"},{"metadata":{"_uuid":"1a4e039510b0aa2f88a78be4aadf43221d055a5f"},"cell_type":"markdown","source":"***Visualization(Exploratory data analysis) - Phase 1 ***"},{"metadata":{"_cell_guid":"2410ea3d-9502-4dbf-b781-ecf60c802d5b","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8d47f3fd23eee3a43e0a1759f692d518a51abfbe","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Importing the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n# Reading the dataset\ndata = pd.read_csv(\"../input/loan.csv\")\ndata_1 = pd.DataFrame(data) # Creating a copy\n\n# Checking the dataset\ndata.head()\ndata.tail()\ndata.describe()\ndata = data.iloc[:,2:-30].values","execution_count":50,"outputs":[]},{"metadata":{"_cell_guid":"416b6c52-fba0-4194-9c38-a1205aa2ebdf","_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"71239f65e413a6a97bb3e995de1ab31818c57679","trusted":true},"cell_type":"code","source":"# Setting the target vector\nstatus = data[:,14]\nunique_labels = np.unique(status, return_counts = True)\n# print(unique_labels)\n\nplt.figure()\nplt.bar(unique_labels[0],unique_labels[1])\nplt.xlabel('Type of label')\nplt.ylabel('Frequency')\nplt.title('Status categories')\nplt.show()\n\ncategory = unique_labels[0]\nfrequency = unique_labels[1]\ncategory_count = np.vstack((category,frequency))\ncategory_list = np.array(category_count.T).tolist()\ncategory_list_1 = pd.DataFrame(category_list)\nprint(category_list_1)\n","execution_count":51,"outputs":[]},{"metadata":{"_cell_guid":"79c2ef0f-6b90-40ee-be4f-3966288b6b74","_uuid":"6504104960b77ab209510f3547f7d5ec507d9ba5"},"cell_type":"markdown","source":"Let us consider only 2 major categories \"Charged off\" and \"Fully Paid\". A few reasons to do this:\n1.  To convert it into a binary cassification problem, and to analyze in detail the effect of important variables on the loan status.\n2. A lot of observations show status \"Current\", so we do not know whether it will be \"Charged Off\", \"Fully Paid\" or \"Default\".\n3. The observations for \"Default\" are too less as compared to \"Fully Paid\" or \"Charged Off\", to thoughroly investigate those observations with loan status as \"Default\". \n4. The remaining categories of \"loan status\" are not of prime importance for this analysis. \n"},{"metadata":{"_cell_guid":"c9446c3d-90b8-49a8-b43b-6171b1143028","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"2247df4be434dc9966a667fd07f3e6ff97f21bb8","collapsed":true,"trusted":true},"cell_type":"code","source":"category_one_data = data_1[data_1.loan_status == \"Fully Paid\"]\ncategory_two_data = data_1[data_1.loan_status == \"Charged Off\"]\nnew_data = np.vstack((category_one_data,category_two_data))\n# new_data_copy = pd.DataFrame(new_data)\nnew_data = new_data[:,2:-30]\nnew_data_df = pd.DataFrame(new_data)","execution_count":52,"outputs":[]},{"metadata":{"_cell_guid":"f1a5db59-5455-432d-b473-8c8c575c9d8b","_uuid":"c8ab43eddd13c37ee118296919ffe78aed5f4a2d"},"cell_type":"markdown","source":"**Exploratory Data Analysis**\n1. Variable under inspection:Installment amount\nWhether there is any trend with respect to Installment amount.\nFor eg: Higher the installment amount higher the number of \"Charged Off\" observations ?\n"},{"metadata":{"_cell_guid":"3fccd230-a86c-4c03-b42c-ad3e934cc3af","_kg_hide-input":true,"_uuid":"084110bcffe1813bfff26546699fc3cc9bd5c07c","trusted":true},"cell_type":"code","source":"# Creating bins for various installment amounts\ninstallment_amt = new_data[:,5]\nbins = np.linspace(installment_amt.min(), installment_amt.max(), 10)\ninstallment_amt = installment_amt.astype(float).reshape(installment_amt.size,1)\nbinned_installment_amt = pd.DataFrame(np.digitize(installment_amt, bins))                  \ninstallment_groups = (np.array(np.unique(binned_installment_amt, return_counts = True))).T\n\n# A bar plot to figure out the distribution of installment amount\nplt.figure()\nplt.bar(installment_groups[:,0],installment_groups[:,1])\nplt.xlabel('Installment_amt_grp')\nplt.ylabel('Frequency')\nplt.title('Distribution of Installment amount categories')\nplt.show()\n\n# Appending the installment_groups to status\nstatus_new = new_data_df[14]\nfactored_status = np.array(pd.factorize(status_new)) # 0's = Fully Paid, 1's = Charged Off\nstatus_labels = pd.DataFrame(factored_status[0])\nstatus_installment_groups = pd.DataFrame(np.hstack((binned_installment_amt,status_labels)))\nstatus_installment_groups.columns = ['Installment_amt_grp','status_labels'] \n\n# Looking for a trend in the defaulted observations\nCharged_off = status_installment_groups[status_installment_groups.status_labels == 1]\ntemp_1 = Charged_off.iloc[:,0].values\nplot_var_1 = np.array(np.unique(temp_1, return_counts = True))\nplot_var_1 = plot_var_1[:,:-1]\nplot_var_11 = plot_var_1.T # Eliminating the 10th, since as only one reading\n\n# Looking for a trend in the successful observations\nFully_paid = status_installment_groups[status_installment_groups.status_labels == 0]\ntemp_2 = Fully_paid.iloc[:,0].values\nplot_var_2 = np.array(np.unique(temp_2, return_counts = True))\nplot_var_22 = plot_var_2.T\n\n# Concatenating the two variables\nplot_var_stack = np.hstack((plot_var_11,plot_var_22))\nplot_var_stack = pd.DataFrame(plot_var_stack)\nplot_var_stack = plot_var_stack.drop(plot_var_stack.columns[2], axis=1)\nplot_var_stack.columns = ['Installment_amt_grp','Charged Off','Fully Paid']\n\n# Percent stacked\n# From raw value to percentage\ntotals = [i+j for i,j in zip(plot_var_stack['Charged Off'], plot_var_stack['Fully Paid'])]\nC_Off = [i / j * 100 for i,j in zip(plot_var_stack['Charged Off'], totals)]\nmean_C_Off = np.mean(C_Off)\nF_Paid = [i / j * 100 for i,j in zip(plot_var_stack['Fully Paid'], totals)]\nplot_var_stack = np.array(plot_var_stack)\ngroup_number = plot_var_stack[:,0]\np1 = plt.bar(group_number, C_Off, color='#7f6d5f', edgecolor='white', width=0.5)\np2 = plt.bar(group_number, F_Paid, bottom=C_Off, color='#557f2d', edgecolor='white', width=0.5)\n#Axes.axhline(y=mean_C_Off)\nplt.xlabel('Installment_amt_grp')\nplt.ylabel('Percent loan status')\nplt.title('Installment amount categories')\nplt.legend((p1, p2), ('Charged Off', 'Fully Paid'), loc = 'upper right')\nplt.show()\n","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"ac9f563e-e03f-42fe-92b2-6dbf7e6afb5e","_uuid":"0265b436e4828eb40b63c98c2c92645552cf4db1","collapsed":true},"cell_type":"markdown","source":"Though we can observe a slight variation in the \"% Charged Off\" values, overall we can say that the installment amount does not seem to affect the loan status.\n\n2) Variable under inspection:Grade.\nWhether the grade affects the Installment amount ?"},{"metadata":{"_cell_guid":"57cf9d94-c06e-4620-b818-ef4f0b33d873","_kg_hide-input":true,"_uuid":"8759054d101604c220bb511d53abfe8827eb1240","trusted":true},"cell_type":"code","source":"installment_grade = new_data[:,6]\n# print(np.unique(installment_grade, return_counts = True))\ninstallment_grade_list = np.array(np.unique(installment_grade, return_counts = True))\ninstallment_grade_df = pd.DataFrame(installment_grade_list.T)\nprint(installment_grade_df)\n\n# Distribution of Installment grade\nplt.figure()\nplt.bar(installment_grade_df[0],installment_grade_df[1])\nplt.xlabel('Installment_grade')\nplt.ylabel('Frequency')\nplt.title('Distribution of Installment grade categories')\nplt.show()\n\ninstallment_grade = pd.DataFrame(installment_grade)\nstatus_installment_grade = pd.DataFrame(np.hstack((installment_grade,status_labels)))\nstatus_installment_grade.columns = ['Installment_grade','status_labels']\n\n# Looking for a trend in the defaulted observations\nCharged_off_grade = status_installment_grade[status_installment_grade.status_labels == 1]\ntemp_11 = Charged_off_grade.iloc[:,0].values\nplot_var_grade = np.array(np.unique(temp_11, return_counts = True))\nplot_var_grade_11 = plot_var_grade.T \n\n# Looking for a trend in the successful observations\nFully_Paid_grade = status_installment_grade[status_installment_grade.status_labels == 0]\ntemp_22 = Fully_Paid_grade.iloc[:,0].values\nplot_var_grade_2 = np.array(np.unique(temp_22, return_counts = True))\nplot_var_grade_22 = plot_var_grade_2.T # Eliminating the 10th, since as only one reading\n\n# Concatenating the two variables\nplot_var_stack_1 = np.hstack((plot_var_grade_11,plot_var_grade_22))\nplot_var_stack_1 = pd.DataFrame(plot_var_stack_1)\nplot_var_stack_1 = plot_var_stack_1.drop(plot_var_stack_1.columns[2], axis=1)\nplot_var_stack_1.columns = ['Installment_grade_grp','Charged Off','Fully Paid']\n\n# Percent stacked\n# From raw value to percentage\ntotals = [i+j for i,j in zip(plot_var_stack_1['Charged Off'], plot_var_stack_1['Fully Paid'])]\nC_Off = [i / j * 100 for i,j in zip(plot_var_stack_1['Charged Off'], totals)]\nmean_C_Off = np.mean(C_Off)\nF_Paid = [i / j * 100 for i,j in zip(plot_var_stack_1['Fully Paid'], totals)]\n# plot_var_stack_1 = np.array(plot_var_stack_1)\ngroup_number = plot_var_stack_1['Installment_grade_grp']\np1 = plt.bar(group_number, C_Off, color='#7f6d5f', edgecolor='white', width=0.5)\np2 = plt.bar(group_number, F_Paid, bottom=C_Off, color='#557f2d', edgecolor='white', width=0.5)\n#Axes.axhline(y=mean_C_Off)\nplt.xlabel('Installment_grade')\nplt.ylabel('Percent loan status')\nplt.title('Installment grade categories')\nplt.legend((p1, p2), ('Charged Off', 'Fully Paid'), loc = 'upper right')\nplt.show()\n","execution_count":54,"outputs":[]},{"metadata":{"_cell_guid":"b464d5ec-2d02-434e-899c-baa0bdb4e983","_uuid":"df5fed58b703343092a63627b5b814b19211b029"},"cell_type":"markdown","source":"1. The grade does seem to affect the default rate: Higher the grade higher the percentage of \"Charged Off\" loans.\n2. Also from the plot we can conclude that Grade G has the highest \"% Charged Off\"\n3. To further investigate this we need to know what the Grade refers to, does it represent risk factor in lending the money ?\nIf yes, then the results make sense: Higher the grade higher the risk factor.\n4. Also, from the distribution plot we can see that they are already lending only a handful amount of loans to people classified in \"Grade G\". They should be more precautious in their approach to lending money to customers who are classified to be in higher grades.\n"},{"metadata":{"_uuid":"fc67596bb2fac713d4b468ab1a1f7b1f416395f9"},"cell_type":"markdown","source":"3) Variable under inspection:Home Status"},{"metadata":{"_cell_guid":"cb2323e2-e5de-49c0-9987-1ba8b6aa0c7c","_kg_hide-input":true,"_uuid":"532e5f805ce20ec00cc2591f77d4c0818c4e8ea1","trusted":true},"cell_type":"code","source":"home_status = new_data_df[10]\n# print(np.unique(home_status, return_counts = True))\nhome_status_list = np.array(np.unique(home_status, return_counts = True))\nhome_status_df = pd.DataFrame(home_status_list.T)\nprint(home_status_df)\n\n# Distribution of Emp_length\nplt.figure()\nplt.bar(home_status_df[0],home_status_df[1])\nplt.xlabel('Home Status')\nplt.ylabel('Frequency')\nplt.title('Home Status categories')\nplt.show()\n\nhome_status = pd.DataFrame(home_status)\nstatus_home_status = pd.DataFrame(np.hstack((home_status,status_labels)))\nstatus_home_status.columns = ['Home Status','status_labels']\n\n# Looking for a trend in the defaulted observations\nCharged_off_home_status = status_home_status[status_home_status.status_labels == 1]\ntemp_41 = Charged_off_home_status.iloc[:,0].values\nplot_var_home_status = np.array(np.unique(temp_41, return_counts = True))\nplot_var_home_status_44 = pd.DataFrame(plot_var_home_status.T) \n\n# Looking for a trend in the successful observations\nFully_Paid_home_status = status_home_status[status_home_status.status_labels == 0]\ntemp_42 = Fully_Paid_home_status.iloc[:,0].values\nplot_var_home_status_2 = np.array(np.unique(temp_42, return_counts = True))\nplot_var_home_status_55 = pd.DataFrame(plot_var_home_status_2.T) # Eliminating the 10th, since as only one reading\nplot_var_home_status_55 = plot_var_home_status_55.drop(0) # Eliminating the home status = \"any\", since as only one reading\n\n# Concatenating the two variables\nplot_var_stack_3 = np.hstack((plot_var_home_status_44,plot_var_home_status_55))\nplot_var_stack_3 = pd.DataFrame(plot_var_stack_3)\nplot_var_stack_3 = plot_var_stack_3.drop(plot_var_stack_3.columns[2], axis=1)\nplot_var_stack_3.columns = ['Home Status','Charged Off','Fully Paid']\n\n# Percent stacked\n# From raw value to percentage\ntotals = [i+j for i,j in zip(plot_var_stack_3['Charged Off'], plot_var_stack_3['Fully Paid'])]\nC_Off = [i / j * 100 for i,j in zip(plot_var_stack_3['Charged Off'], totals)]\nmean_C_Off = np.mean(C_Off)\nF_Paid = [i / j * 100 for i,j in zip(plot_var_stack_3['Fully Paid'], totals)]\n#plot_var_stack_3 = np.array(plot_var_stack_3)\ngroup_number = plot_var_stack_3['Home Status']\np1 = plt.bar(group_number, C_Off, color='#7f6d5f', edgecolor='white', width=0.5)\np2 = plt.bar(group_number, F_Paid, bottom=C_Off, color='#557f2d', edgecolor='white', width=0.5)\n#Axes.axhline(y=mean_C_Off)\nplt.xlabel('Home Status')\nplt.ylabel('Percent loan status')\nplt.title('Home Status categories')\nplt.legend((p1, p2), ('Charged Off', 'Fully Paid'), loc = 'upper right')\nplt.show()\n","execution_count":55,"outputs":[]},{"metadata":{"_cell_guid":"326e82bd-605e-4d2e-bf6e-6960b41b870d","_uuid":"4f9e3fe657841d00d4f5f9b9fb00f195122e1335"},"cell_type":"markdown","source":"From the stacked percentage plot, we can observe that the feature \"Home Status\" has no potential effect on our target variable \"loan status\""},{"metadata":{"_cell_guid":"7685d186-f963-4481-90a1-ac39bdc6baec","_uuid":"396f72431a82cf652b34594f214309213526473f","collapsed":true},"cell_type":"markdown","source":"4) Variable under inspection:Annual Income\n\nTo investigate this variable I create four bins to classify the annual income:\n1. People earning less than USD 40,000.\n2. People earning between USD 40,000 to USD 70,000.\n3. People earning between USD 70,000 to USD 100,000.\n4. People earning more than USD 100,000,\n\n"},{"metadata":{"_cell_guid":"a3210eef-ab77-49ab-b443-497685f6d58b","_kg_hide-input":true,"_uuid":"21a81eb86736db06ac2f5b9991857f78bb0c976c","trusted":true},"cell_type":"code","source":"## Now checking the effect of annual income on loan status\n# Creating bins for various income amounts\nannual_income = new_data[:,11]\n#bins_2 = np.linspace(annual_income.min(), annual_income.max(), 3)\nbins_2 = np.array([40000,70000,100000,150000])\nannual_income = annual_income.astype(float).reshape(annual_income.size,1)\nbinned_annual_income = pd.DataFrame(np.digitize(annual_income, bins_2))                  \nannual_groups = (np.array(np.unique(binned_annual_income, return_counts = True))).T\n\n# A bar plot to figure out the distribution of income amount\nplt.figure()\nplt.bar(annual_groups[:,0],annual_groups[:,1])\nplt.xlabel('Annual income amount group')\nplt.ylabel('Frequency')\nplt.title('Annual income amount categories')\nplt.legend(loc=\"upper right\")\nplt.show()\n\n# Appending the income_groups to status\nstatus_annual_groups = pd.DataFrame(np.hstack((binned_annual_income,status_labels)))\nstatus_annual_groups.columns = ['Annual_income_grp','status_labels'] \n\n# Looking for a trend in the defaulted observations\nCharged_off_annual_income = status_annual_groups[status_annual_groups.status_labels == 1]\ntemp_51 = Charged_off_annual_income.iloc[:,0].values\nplot_var_annual_income = np.array(np.unique(temp_51, return_counts = True))\nplot_var_annual_income_66 = pd.DataFrame(plot_var_annual_income.T) \n\n# Looking for a trend in the successful observations\nFully_Paid_annual_income = status_annual_groups[status_annual_groups.status_labels == 0]\ntemp_52 = Fully_Paid_annual_income.iloc[:,0].values\nplot_var_annual_income_2 = np.array(np.unique(temp_52, return_counts = True))\nplot_var_annual_income_77 = pd.DataFrame(plot_var_annual_income_2.T) # Eliminating the 10th, since as only one reading\n#plot_var_annual_income_55 = plot_var_home_status_55.drop(0) # Eliminating the home status = \"any\", since as only one reading\n\n# Concatenating the two variables\nplot_var_stack_4 = np.hstack((plot_var_annual_income_66,plot_var_annual_income_77))\nplot_var_stack_4 = pd.DataFrame(plot_var_stack_4)\nplot_var_stack_4 = plot_var_stack_4.drop(plot_var_stack_4.columns[2], axis=1)\nplot_var_stack_4.columns = ['Annual Income Group','Charged Off','Fully Paid']\n\n# Percent stacked\n# From raw value to percentage\ntotals = [i+j for i,j in zip(plot_var_stack_4['Charged Off'], plot_var_stack_4['Fully Paid'])]\nC_Off = [i / j * 100 for i,j in zip(plot_var_stack_4['Charged Off'], totals)]\nmean_C_Off = np.mean(C_Off)\nF_Paid = [i / j * 100 for i,j in zip(plot_var_stack_4['Fully Paid'], totals)]\n#plot_var_stack_4 = np.array(plot_var_stack_4)\ngroup_number = plot_var_stack_4['Annual Income Group']\np1 = plt.bar(group_number, C_Off, color='#7f6d5f', edgecolor='white', width=0.5)\np2 = plt.bar(group_number, F_Paid, bottom=C_Off, color='#557f2d', edgecolor='white', width=0.5)\n#Axes.axhline(y=mean_C_Off)\nplt.xlabel('Annual income amount group')\nplt.ylabel('Percent loan status')\nplt.title('Annual income amount categories')\nplt.legend((p1, p2), ('Charged Off', 'Fully Paid'), loc = 'upper right')\nplt.show()\n","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"1628328e4cd61c5b4926bc414d656611c6359869"},"cell_type":"markdown","source":"We can observe a slight downword trend, which suggests that people with higher income are less likely to get \"Charged Off\".\n"},{"metadata":{"_uuid":"e77db23d795945e99ae30be9bde9cc63871c1fee"},"cell_type":"markdown","source":"5) Variable under inspection:State\nAn important question here would be to check whether the state affects the loan status. Also, to find out which state has highest \"% Charged Off\". "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e65e04b163a64f3c5f88b8ed42a389132055db0a"},"cell_type":"code","source":"# Separating the variable under investigation\nstate = new_data_df[21]\n#print(np.unique(state, return_counts = True))\nstate_list = np.array(np.unique(state, return_counts = True))\nstate_df = pd.DataFrame(state_list.T)\nprint(state_df)\n\n# Distribution of Emp_length\nplt.figure()\nplt.bar(state_df[0],state_df[1])\nplt.xlabel('State')\nplt.ylabel('Frequency')\nplt.title('State')\nplt.show()\n\nstate = pd.DataFrame(state)\nstatus_state = pd.DataFrame(np.hstack((state,status_labels)))\nstatus_state.columns = ['State','status_labels']\n\n# Looking for a trend in the defaulted observations\nCharged_off_state = status_state[status_state.status_labels == 1]\ntemp_61 = Charged_off_state.iloc[:,0].values\nplot_var_state = np.array(np.unique(temp_61, return_counts = True))\nplot_var_state_88 = pd.DataFrame(plot_var_state.T) \n\n# Looking for a trend in the successful observations\nFully_Paid_state = status_state[status_state.status_labels == 0]\ntemp_62 = Fully_Paid_state.iloc[:,0].values\nplot_var_state_2 = np.array(np.unique(temp_62, return_counts = True))\nplot_var_state_99 = pd.DataFrame(plot_var_state_2.T) \n","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"8468a3bad2c616c488e20ca2033b595c5cce5e7d"},"cell_type":"markdown","source":"* We know US has only 50 States, but we have a list of 51 states. On investigation we can see that DC is added as a state even when it isn't a state.\n* We also notice that its present in both the cases, charged off as well as in fully paid observations.\n* So I decide on just eliminating DC from the list (Keep this in mind) .\n* Also, states like ME and ND have no people with \"Charged Off\" observations, so we will just take them off the list as well and check for any trends in the state variable. \n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"085dd3ded55c11475f8255a79b634a9da8f1ca6e"},"cell_type":"code","source":"plot_var_state_88 = plot_var_state_88.drop(7)\nplot_var_state_99 = plot_var_state_99.drop([7,21,28]) # Eliminating the home status = \"any\", since as only one reading\n\n# Concatenating the two variables\nplot_var_stack_5 = np.hstack((plot_var_state_88,plot_var_state_99))\nplot_var_stack_5 = pd.DataFrame(plot_var_stack_5)\nplot_var_stack_5 = plot_var_stack_5.drop(plot_var_stack_5.columns[2], axis=1)\nplot_var_stack_5.columns = ['state','Charged Off','Fully Paid']\n\n# Percent stacked\n# From raw value to percentage\ntotals = [i+j for i,j in zip(plot_var_stack_5['Charged Off'], plot_var_stack_5['Fully Paid'])]\nC_Off = [i / j * 100 for i,j in zip(plot_var_stack_5['Charged Off'], totals)]\nmean_C_Off = np.mean(C_Off)\nF_Paid = [i / j * 100 for i,j in zip(plot_var_stack_5['Fully Paid'], totals)]\n#plot_var_stack_5 = np.array(plot_var_stack_5)\ngroup_number = plot_var_stack_5['state']\np1 = plt.bar(group_number, C_Off, color='#7f6d5f', edgecolor='white', width=0.5)\np2 = plt.bar(group_number, F_Paid, bottom=C_Off, color='#557f2d', edgecolor='white', width=0.5)\nplt.xlabel('State')\nplt.ylabel('Percent loan status')\nplt.title('State')\nplt.legend((p1, p2), ('Charged Off', 'Fully Paid'), loc = 'upper right')\nplt.show()\n\n###### Sort in order and print top 5 states with max default % ########\n# Concatenating C_Off and state\nC_Off = pd.DataFrame(C_Off)\ntemp_plot = np.hstack((plot_var_stack_5, C_Off))\ntemp_plot = pd.DataFrame(temp_plot)\ntemp_plot.columns = ['state','Charged Off','Fully Paid','% Charged Off']\ntemp_plot = np.array(temp_plot.sort_values(by = '% Charged Off',ascending = False))\nprint(temp_plot[0:5,(0,3)])\n\ntemp_plot = pd.DataFrame(temp_plot)\ntemp_plot.columns = ['state','Charged Off','Fully Paid','% Charged Off']\ntemp_plot = temp_plot.drop(['Charged Off', 'Fully Paid'], axis = 1)\n","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"1e93f786eb27e8ef98f4e665b63c37d6e94c4ba8"},"cell_type":"markdown","source":"* We can observe that there is variation of \"% Chharged Off\" in the percent stacked plot. \n* Though we cannot draw any strong conclusions of whether the \"% Charged Off\" is affected by the \"State\" variable, we can answer our question of which state has the highest \"% Charged Off\"? \n* We could see state of Tennessee has the highest \"% Charged Off\" of 23.21%"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7403f1cb852619b260c66b9a75ad1391f0730bc6"},"cell_type":"code","source":"# Chloropleth map for better visualization\nimport plotly\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\n \nfor col in temp_plot.columns:\n    temp_plot[col] = temp_plot[col].astype(str)\n\nscl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\n#temp_plot['text'] = temp_plot['state'] + '<br>' +\\\n#    'Default rate '+ temp_plot['% Charged Off']\n\ndata_chloropleth = [ dict(\n        type ='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = temp_plot['state'],\n        z = temp_plot['% Charged Off'].astype(float),\n        locationmode = 'USA-states',\n        #text = temp_plot['text'],\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"Default rate\")\n        ) ]\n\nlayout = dict(\n        title = 'State-wise % Charged Off<br>(Hover for breakdown)',\n        geo = dict(\n            scope='usa',\n            projection=dict(type='albers usa'),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\n    \n#fig = dict(data=data_chloropleth, layout=layout)\n#py.iplot(fig, image = 'png', filename = 'test-image-2', show_link = False)\n# Change to py.iplot\nchloromap = go.Figure(data = data_chloropleth, layout = layout)\niplot(chloromap, validate=False)\n","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"5fcc2b2f127350e6c92e71129c6101b43d56b338"},"cell_type":"markdown","source":"6) Variable under inspection:Verification Status"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"85392da568ab3a14180f63d1a35a25c65cb025da"},"cell_type":"code","source":"# Separating the variable under investigation\nver_stat = new_data_df[12]\n#print(np.unique(ver_stat, return_counts = True))\nver_stat_list = np.array(np.unique(ver_stat, return_counts = True))\nver_stat_df = pd.DataFrame(ver_stat_list.T)\nprint(ver_stat_df)\n\n# Distribution of Emp_length\nplt.figure()\nplt.bar(ver_stat_df[0],ver_stat_df[1])\nplt.xlabel('Verification Status')\nplt.ylabel('Frequency')\nplt.title('Verification Status')\nplt.show()\n\nver_stat = pd.DataFrame(ver_stat)\nstatus_ver_stat = pd.DataFrame(np.hstack((ver_stat,status_labels)))\nstatus_ver_stat.columns = ['Verification Status','status_labels']\n\n# Looking for a trend in the defaulted observations\nCharged_off_ver_stat = status_ver_stat[status_ver_stat.status_labels == 1]\ntemp_71 = Charged_off_ver_stat.iloc[:,0].values\nplot_var_ver_stat = np.array(np.unique(temp_71, return_counts = True))\nplot_var_ver_stat_101 = pd.DataFrame(plot_var_ver_stat.T) \n\n# Looking for a trend in the successful observations\nFully_Paid_ver_stat = status_ver_stat[status_ver_stat.status_labels == 0]\ntemp_72 = Fully_Paid_ver_stat.iloc[:,0].values\nplot_var_ver_stat_2 = np.array(np.unique(temp_72, return_counts = True))\nplot_var_ver_stat_111 = pd.DataFrame(plot_var_ver_stat_2.T) \n\n# Concatenating the two variables\nplot_var_stack_6 = np.hstack((plot_var_ver_stat_101,plot_var_ver_stat_111))\nplot_var_stack_6 = pd.DataFrame(plot_var_stack_6)\nplot_var_stack_6 = plot_var_stack_6.drop(plot_var_stack_6.columns[2], axis=1)\nplot_var_stack_6.columns = ['Verification Status','Charged Off','Fully Paid']\n\n# Percent stacked\n# From raw value to percentage\ntotals = [i+j for i,j in zip(plot_var_stack_6['Charged Off'], plot_var_stack_6['Fully Paid'])]\nC_Off = [i / j * 100 for i,j in zip(plot_var_stack_6['Charged Off'], totals)]\nmean_C_Off = np.mean(C_Off)\nF_Paid = [i / j * 100 for i,j in zip(plot_var_stack_6['Fully Paid'], totals)]\n#plot_var_stack_5 = np.array(plot_var_stack_5)\ngroup_number = plot_var_stack_6['Verification Status']\np1 = plt.bar(group_number, C_Off, color='#7f6d5f', edgecolor='white', width=0.5)\np2 = plt.bar(group_number, F_Paid, bottom=C_Off, color='#557f2d', edgecolor='white', width=0.5)\nplt.xlabel('Verification Status')\nplt.ylabel('Percent loan status')\nplt.title('Verification Status')\nplt.legend((p1, p2), ('Charged Off', 'Fully Paid'), loc = 'upper right')\nplt.show()","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"ee5369a0beef61df8b0bea12b6b397a1e7ab5446"},"cell_type":"markdown","source":"The result is slightly unexpected, as we would think that loan given after thorough verification would result in lesser percentage of \"Charged Off\" loans, but turns out that loans given off to people without verfication show a lesser \"% Charged Off\" loans."},{"metadata":{"_uuid":"ed963f615ee654ad844a032a079a2b19106b29f3"},"cell_type":"markdown","source":"* ***Text Analysis - Phase 2 ***"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"dd307d0bb8b4170b703f15b3fc9174bbcd4ecd1d"},"cell_type":"code","source":"from wordcloud import WordCloud\n\n# Employee Title\nemp_title = new_data_df[8]\nemp_title = pd.DataFrame(emp_title)\nemp_title.columns = ['Employee Title']\nemp_title = emp_title.dropna(axis=0, how='all')\nwordcloud = WordCloud().generate(' '.join(emp_title['Employee Title']))\n\n# Generate plot\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":62,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"478521a0b4534b5740e91fe12f6e6eb5e979f635"},"cell_type":"code","source":"# Title \ntitle = new_data_df[19]\ntitle = pd.DataFrame(title)\ntitle.columns = ['Title']\ntitle = title.dropna(axis=0, how='all')\nwordcloud3 = WordCloud().generate(' '.join(title['Title']))\n\n# Generate plot\nplt.imshow(wordcloud3)\nplt.axis(\"off\")\nplt.show()","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"d877d3b7c0e87cca77ef93a56be159be3e902c1b"},"cell_type":"markdown","source":"From the word-cloud we can notice that majority of the people took a loan for debt consolidation, refinancing debt, credit card payment, or home improvement."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"291007a95b56f86a028e7e32f2d4cdd9a42c08d2"},"cell_type":"code","source":"# Description\nDescription = new_data_df[17]\nDescription = Description.dropna(axis=0, how='all')\nDescription = list(Description)\nDescription_1 = []\ni = 0 \nfor i in range(0,len(Description)):\n    s = Description[i]\n    s = s.replace(\"Borrower added on \", \"\")\n    s = s.replace(\"<br>\", \"\")\n    Description_1.append(s)\n    i = i+1\nDescription_1 = pd.DataFrame(Description_1)\nDescription_1.columns = ['Description']\nwordcloud4 = WordCloud().generate(' '.join(Description_1['Description']))\n\n# Generate plot\nplt.imshow(wordcloud4)\nplt.axis(\"off\")\nplt.show()\n","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"5096632758468b5cb72258143986e37f9ae610d2"},"cell_type":"markdown","source":"Again from the word-cloud we can notice that majority of the people described their reason for taking loan  to pay off high interest credit card loan pay."},{"metadata":{"_cell_guid":"3d4aa5de-9d48-46dd-a071-63f1480bc138","_uuid":"e300b77503e7978f2c4484e04a7f18922a72e614"},"cell_type":"markdown","source":" ***Model Building - Phase 3***\n* Data pre-processing:\n* Cleaning the data ---\n1) Selecting necessary features \n2) Taking care of nan values"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"dd4de6f105d96eb6bbe07d9d7a00d48ee4619811"},"cell_type":"code","source":"# Data pre-processing\n\n# Dealing with na values\nnew_data_copy = np.vstack((category_one_data,category_two_data))\nnew_data_copy = pd.DataFrame(new_data_copy)\n#print(np.shape(new_data_copy)) # Dimensions of the dataset\n#print(new_data_copy.isnull().sum()) # Printing number of na values in each column \n#data_2 = new_data_copy.dropna(axis = 1, how = 'all') # Dropping columns where all values are na\ndata_2 = new_data_copy\n#print(np.shape(data_2)) # Dimensions of new dataset\n# We can observe that one of the column was removed since it was completely empty\ndata_dim = np.shape(data_2)\n\n# We can see that a lot of columns contain 70% na values, which is no good for us\n# Columns having more than 20-30% na values would not be of much help, thus eliminating them\ncol_nos = []\ni = 0\nfor i in range (0,data_dim[1]):\n    num_na_val = data_2[i].isnull().sum()\n    if (num_na_val/len(data_2)) > 0.2:\n        col_nos.append(i)\n    i = i+1\n\ndata_2 = data_2.drop(data_2.columns[col_nos], axis = 1)\n#print(data_2.isnull().sum())\nnp.shape(data_2)\n\n# Now lets drop the columns like id, employee title, description,etc. which cannot be taken into consideration while modelling \nrename_var_1 = range(0,49)\ndata_2.columns = rename_var_1\ncols_remove = [0,10,11,17,18,19,20,21]\ndata_2 = data_2.drop(data_2.columns[cols_remove], axis = 1)\nnp.shape(data_2)\n\nrename_var_2 = range(0,41)\ndata_2.columns = rename_var_2\ntime_series_var = [12,17,34,36]\ncat_var_cols = [4,7,8,9,11,14,16,18,19,20,24,25,32,33,37,38,39]\ncat_plus_time_cols = [4,7,8,9,11,12,14,16,17,18,19,20,24,25,32,33,34,36,37,38,39]\ncat_var_df = data_2.iloc[:,cat_var_cols].values\ncat_var_df = pd.DataFrame(cat_var_df)\n#cat_var_df.describe(include=['category'])\ni = 0\nunique_categories = []\nfor i in cat_var_df:\n    un_cat = np.unique(cat_var_df[i])\n    unique_categories.append(un_cat)\n    i = i+1\n    \n# Removing more columns based on the above result\n#print(unique_categories)\nc = [11,12,13,15]\ncat_var_df = cat_var_df.drop(cat_var_df.columns[c], axis = 1)\nnp.shape(cat_var_df)\nr_var = range(0,13)\n# We can observe that column 16 has 56 null values, let us replace them with the most frequent value of the column\ncat_var_df.columns = r_var\n#print(cat_var_df.isnull().sum())\n# Taking care of missing values for categorical features\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)\nimputer = imputer.fit(cat_var_df[[11]])\ncat_var_df[11] = imputer.transform(cat_var_df[[11]])\n#print(cat_var_df[11].isnull().sum())\n#print(cat_var_df.isnull().sum())\nrenaming_df = range(0,13)\ncat_var_df.columns = renaming_df\n# We can clearly see that now there are no more na values\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_X = LabelEncoder()\ni = 0\nfor i in range(0,13):\n    cat_var_df[i] = labelencoder_X.fit_transform(cat_var_df[i])\n    #onehotencoder = OneHotEncoder(categorical_features = [i])\n    i = i+1\n\n# Taking care of missing values for remaining features\n#print(data_2.isnull().sum())\ndata_2_copy = data_2\nnon_cat_var = data_2_copy.drop(data_2_copy.columns[cat_plus_time_cols], axis = 1)\nrename_var = range(0,20)\nnon_cat_var.columns = rename_var\n\n\n# Also dropping the target variable\nY = non_cat_var[[7]]\nnon_cat_var = non_cat_var.drop(non_cat_var.columns[7], axis = 1) \n#non_cat_var = non_cat_var.drop(non_cat_var.columns[0], axis = 1) # Dropping the variable id\nrenaming_df = range(0,19)\nnon_cat_var.columns = renaming_df\nimputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\ni = 0\nfor i in non_cat_var:\n    imputer = imputer.fit(non_cat_var[[i]])\n    non_cat_var[i] = imputer.transform(non_cat_var[[i]])\n    i = i+1\n    \n#print(non_cat_var.isnull().sum())   \n\n# We have no nan values in our non_cat_var now\n# Let us now concatenate the categrical variables and non_categorical variables and form ourfeature matrix.\n# Checking the dimensions\nprint(np.shape(non_cat_var))\nprint(np.shape(cat_var_df))\nprint(np.shape(Y))\n\nX = np.hstack((non_cat_var,cat_var_df)) # Concatenating","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"1266337fe349d7e6817b3f737020af59a85c9543"},"cell_type":"markdown","source":"Awesome ! The boring task of cleaning the data is successfully completed. Now, \n1. Label Encoding the target variable \"Loan status\". \n2. Splitting the dataset.\n3. Feature scaling"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"aa33c88028b3140216b4acf53473a7f44b8e10ee"},"cell_type":"code","source":"# Label encoding the target variable \nlabelencoder_Y = LabelEncoder()\nY = labelencoder_Y.fit_transform(Y)\n\n#Splitting the dataset in training and testing set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"1b4b2bfbe1b2aec2054e647d3eb26e76834581bf"},"cell_type":"markdown","source":"So for model comparison, I have selected 6 Models for this analysis, and they are as follows:\n* Model 1 - XGBoostClassifier\n* Model 2 - Support Vector Classifier(SCV)\n* Model 3 - RandomForestClassifier\n* Model 4 - Logistic \n* Model 5 - BalancedBaggingClassifier\n* Model 6 - Decision Tree\n"},{"metadata":{"_cell_guid":"5e516e3f-b30a-4a5e-8e61-021c3f8e9a0a","_kg_hide-input":true,"_uuid":"9f9b43253b85f1882a186246fcc3754d0c141b43","trusted":false,"_kg_hide-output":true},"cell_type":"code","source":"# Fitting XGBClassifier to the training data: Model_1\nfrom xgboost import XGBClassifier\nclassifier_1 = XGBClassifier()\nclassifier_1.fit(X_train,Y_train)\n\n# Fitting SVM to the training data: Model 2\nfrom sklearn.svm import SVC\nclassifier_2 = SVC(kernel = 'linear', C = 1, probability = True, random_state = 0) # poly, sigmoid\nclassifier_2.fit(X_train,Y_train)\n\n# Creating and Fitting Random Forest Classifier to the training data: Model 3\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_3 = RandomForestClassifier(n_estimators = 5, criterion = 'entropy')\nclassifier_3.fit(X_train,Y_train)\n\n# Fitting classifier to the training data: Model 4 \nfrom sklearn.linear_model import LogisticRegression\nclassifier_4 = LogisticRegression(penalty = 'l1', random_state = 0)\nclassifier_4.fit(X_train,Y_train)\n\n# Fitting Balanced Bagging Classifier to the training data: Model 5\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_5 = BalancedBaggingClassifier(base_estimator = RandomForestClassifier(criterion='entropy'),\n                                       n_estimators = 5, bootstrap = True)\nclassifier_5.fit(X_train,Y_train)\n\n# Fitting Decision Tree to the training data: Model 6\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_6 = DecisionTreeClassifier()\nclassifier_6.fit(X_train,Y_train)\n","execution_count":49,"outputs":[]},{"metadata":{"_cell_guid":"b515f6a5-4c6f-4ed1-a1fa-e22cbee9e803","_kg_hide-input":true,"_uuid":"072f3870b2f54986c4ca345453dd3fa0adbad86c","collapsed":true,"trusted":false},"cell_type":"code","source":"# Predicting the results\ny_pred_1 = classifier_1.predict(X_test)\ny_pred_2 = classifier_2.predict(X_test)\ny_pred_3 = classifier_3.predict(X_test)\ny_pred_4 = classifier_4.predict(X_test)\ny_pred_5 = classifier_5.predict(X_test)\ny_pred_6 = classifier_6.predict(X_test)\n\n# Creating the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_1 = confusion_matrix(Y_test,y_pred_1)\naccuracy_1 = (cm_1[0,0]+cm_1[1,1])/len(Y_test)\n\ncm_2 = confusion_matrix(Y_test,y_pred_2)\naccuracy_2 = (cm_2[0,0]+cm_2[1,1])/len(Y_test)\n\ncm_3 = confusion_matrix(Y_test,y_pred_3)\naccuracy_3 = (cm_3[0,0]+cm_3[1,1])/len(Y_test)\n\ncm_4 = confusion_matrix(Y_test,y_pred_4)\naccuracy_4 = (cm_4[0,0]+cm_4[1,1])/len(Y_test)\n\ncm_5 = confusion_matrix(Y_test,y_pred_5)\naccuracy_5 = (cm_5[0,0]+cm_5[1,1])/len(Y_test)\n\ncm_6 = confusion_matrix(Y_test,y_pred_6)\naccuracy_6 = (cm_6[0,0]+cm_6[1,1])/len(Y_test)\n\n\nprint(\"Accuracy_XGBoost:\",accuracy_1*100,'%',\"\\nAccuracy_SVC:\",accuracy_2*100,'%',\"\\nAccuracy_RF:\",accuracy_3*100,'%',\"\\nAccuracy_Logistic:\",accuracy_4*100,'%',\n      \"\\nAccuracy_BalancedBagging:\",accuracy_5*100,'%',\"\\nAccuracy_DecisionTree:\",accuracy_6*100,'%')\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ec1fb258-8062-4753-a62d-55c7397a99aa","_uuid":"4edb1db03ad3ddb2c300f56538354c22428062b3"},"cell_type":"markdown","source":"We can see that the accuracy for all the models is very high and pretty much the same. Thus we could prefereably use a simpler model like Decision Tree to classify our data.\n\nFor future work, since as the dataset is huge, one can try classify this dataset using Artificial Neural Networks Networks. Also instead of binary classification one can try multi-class classification.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}