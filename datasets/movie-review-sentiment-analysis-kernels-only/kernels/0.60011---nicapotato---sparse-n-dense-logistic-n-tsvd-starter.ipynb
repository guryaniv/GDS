{"cells":[{"metadata":{"_uuid":"8f58b0337bfd6e4669bc1b3c07b9ec3f9f75ad1f"},"cell_type":"markdown","source":"# Logistic Regression Starter\n_By Nick Brooks_\n\n**Content:** <br>\n- Bag of Words: Term-Frequency Inverse Document Frequency Method\n- Sparse Matrix with \"Dense Features\"\n- Logistic Regression\n- Logistic Regression and Truncated Singular Value Decomposition"},{"metadata":{"_uuid":"b724ac32625e055248301bb2d5e4d06f9da6aa1a"},"cell_type":"markdown","source":"## Packages and Load"},{"metadata":{"trusted":true,"_uuid":"571f6fb1edd97d02f3ea137e7aa9dcb4f96f4ca4","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nprint(\"Data:\\n\",os.listdir(\"../input\"))\n\n# Models Packages\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n# Dimensionality Reduction\nfrom sklearn.decomposition import TruncatedSVD\n\n# Tf-Idf\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix, vstack\nfrom nltk.corpus import stopwords\n\n# Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\ndf = pd.read_csv(\"../input/train.tsv\", sep=\"\\t\", index_col = [\"PhraseId\"])#.sample(500) # Debugging..\ntrainlen = df.shape[0]\ntest_df = pd.read_csv(\"../input/test.tsv\", sep=\"\\t\", index_col = [\"PhraseId\"])#.sample(500)\ntestdex = test_df.index\nprint(\"\\nTrain Shape: \",df.shape)\nprint(\"Test Shape: \",test_df.shape)\n\ny = df.Sentiment.copy()\ndf = pd.concat([df.drop(\"Sentiment\",axis=1),test_df], axis=0)\nprint(\"All Data Shape: {} Rows, {} Columns\".format(*df.shape))\ndel test_df\n\n# Glimpse at Dataset \nprint(\"Dataset Glimpse\")\ndisplay(df.head(5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8afc09d56e87969b83afb49976a1a099bdf8717d"},"cell_type":"markdown","source":"**Dependent Variable Class Distribution:** <br>"},{"metadata":{"trusted":true,"_uuid":"2e7ae9dc09a2aa7615cebe3359328d3845123651"},"cell_type":"code","source":"print(\"Percent Representation by Sentiment Level\")\nprint(y.value_counts(normalize=True)*100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0308b62480d9d72f38674874e43a94474691747"},"cell_type":"markdown","source":"These classes are imbalanced. Approaches such as stratification and class weights are routes to improve the model."},{"metadata":{"_uuid":"6ed599d14501cf200ba6603cd5a985c57a63c1c8"},"cell_type":"markdown","source":"## Text Features\nHere we have some meta text features, what I characterize as the higher level imformation about the language."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cd2c4650b0876e09ca48e5b5698d7810154e4d0e"},"cell_type":"code","source":"# Meta Text Features\ndf[\"Phrase\"] = df[\"Phrase\"].astype(str) \ndf[\"Phrase\"] = df[\"Phrase\"].astype(str).fillna('missing') # FILL NA\ndf[\"Phrase\"] = df[\"Phrase\"].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\ndf[\"Phrase\" + '_num_words'] = df[\"Phrase\"].apply(lambda comment: len(comment.split())) # Count number of Words\ndf[\"Phrase\" + '_num_unique_words'] = df[\"Phrase\"].apply(lambda comment: len(set(w for w in comment.split())))\ndf[\"Phrase\" + '_words_vs_unique'] = df[\"Phrase\"+'_num_unique_words'] / df[\"Phrase\"+'_num_words'] * 100 # Count Unique Words","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d914a8a257585a3ddd42adfbf4ba5e3cb89db98"},"cell_type":"markdown","source":"## Term Frequence - Inverse Document Frequency\nWhile a simple bag of words method might just count the occurence of each word in each sample and one hot encode, TF-IDF weights how defining (or rare) a certain word in a sample is, in comparison with the rest of the samples."},{"metadata":{"trusted":true,"_uuid":"6c106c5160de9d32e158481a0cf293ff5f916057","collapsed":true},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    dtype = np.float32,\n    norm='l2',\n    min_df=0,\n    smooth_idf=False,\n    max_features=15000)\n# Fit and Transform\nword_vectorizer.fit(df.iloc[0:trainlen,:][\"Phrase\"])\ntrain_word_features = word_vectorizer.transform(df.iloc[0:trainlen,:][\"Phrase\"])\ntest_word_features = word_vectorizer.transform(df.iloc[trainlen:,:][\"Phrase\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1702d82610c0f09c06d53587d91b48dcb141d6b"},"cell_type":"markdown","source":"**Dummy Variable the Sentence Id** <br>\nWhile some models can get away with interger encoding categorical variables, logistic regression would not pick up on the category without dummy encoding."},{"metadata":{"trusted":true,"_uuid":"b8c9dc3fe5dddbfea4f90564f2391f66f6b09ac9","collapsed":true},"cell_type":"code","source":"sent_dummy = pd.get_dummies(df[\"SentenceId\"])\ndf.drop(\"SentenceId\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64b3c869bd1fa3f24548253ac2d0a16251c0b1cc"},"cell_type":"markdown","source":"**Scale Data Down:** <br>\nBelow there is a table with the descriptive statistics for my variables. Since most of my other features are on around 0 and 1, I will reduce the magnitude of these features so that the logistic regression will converge better and faster."},{"metadata":{"trusted":true,"_uuid":"a5f7e7439598b9e85344bc4ab917bf1efad32ebd"},"cell_type":"code","source":"print(\"Before..\")\ndisplay(df.describe())\ndense_variables = [x for x in df.columns if x not in [\"PhraseId\",\"SentenceId\",\"Phrase\"]]\nscaler = MinMaxScaler()\ndf[dense_variables] = scaler.fit_transform(df[dense_variables])\nprint(\"After..\")\ndisplay(df.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f89390308f26ceb1fe6671430d503c4d73c86475"},"cell_type":"markdown","source":"**Fill Missing Values with 0**"},{"metadata":{"trusted":true,"_uuid":"ce1d1235dafd53769c5028ba684aeba194aa3cb2"},"cell_type":"code","source":"# Fill Missing Values with 0\nprint(\"Missing Values Before:\\n\", df.isnull().sum())\ndf.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ccbdd14e3b6ff7695f12ace58cd2ccdddbd73f9"},"cell_type":"markdown","source":"## Sparse Matrix for Modeling\nSparcity refers to the data storage structure. Instead of having to explicitly use memory to assign each cell of a value, the sparse matrix assumes a matrix of zeros so that it only needs to declare where there is a none-zero value, which only represents 0.00029 % for my processed data for modeling."},{"metadata":{"trusted":true,"_uuid":"e4c9b56600f343d0f02561bf6e1a4eada20eeea7"},"cell_type":"code","source":"# Sparse Matrix\ndense_vars = [x for x in df.columns if x not in [\"PhraseId\",\"SentenceId\",\"Phrase\"]]\nX = hstack([csr_matrix(df.iloc[0:trainlen,:][dense_vars].values), csr_matrix(sent_dummy.iloc[0:trainlen,:]),train_word_features])\ntest_df = hstack([csr_matrix(df.iloc[trainlen:,:][dense_vars].values),csr_matrix(sent_dummy.iloc[trainlen:,:]), test_word_features])\n# del df, sent_dummy, train_word_features, test_word_features; gc.collect();\n# Zero Proportion\nzero_proportion = ((X.toarray() != 0).sum() / (X.shape[0]*X.shape[1]))\nprint(\"Portion of Data that has an value other than 0: {}%\".format(round(zero_proportion, 5)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2da33c46b6175f7aae02712d6191b06a7fac1eb0"},"cell_type":"markdown","source":"## Data Ready for Supervised Learning!\nNotice the amount of features! "},{"metadata":{"trusted":true,"_uuid":"092a94df86e2425b1396d7e774144cf5cbf0e4ad","collapsed":true},"cell_type":"code","source":"print(\"Train Shape: {} Rows and {} Cols\".format(*X.shape))\nprint(\"Test Shape: {} Rows and {} Cols\".format(*test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cc54d32712366a22f93faa9e284f95541191c92"},"cell_type":"markdown","source":"## Logistic Regression <br>\nA more traditional classification model in supervised learning. Serves as a great baseline model because of its simplicity, interpretability, and our experience with it as a human race."},{"metadata":{"trusted":true,"_uuid":"ad02bdc16dd9e2a7d8e067973a1d0f763703be6f","collapsed":true},"cell_type":"code","source":"# Define and Fit\nmodel = LogisticRegression(multi_class = 'ovr', C=1, solver='sag')\nmodel.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9d91188697f1e657e80d16da9cf22f2384ee0b8"},"cell_type":"markdown","source":"**Predict and Submit:**"},{"metadata":{"trusted":true,"_uuid":"8f884d7bdd852ae2d1f570ce29b7a4e152155580","collapsed":true},"cell_type":"code","source":"# Predict and Submit\nsubmission = model.predict(test_df)\nsubmission_df = pd.Series(submission).rename(\"Sentiment\")\nsubmission_df.index = testdex\nsubmission_df.to_csv(\"Logistic_sub.csv\",index=True,header=True)\ndisplay(submission_df.head())\n\ndel model, submission, submission_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79003f276ffae21a7a8caf643124ed3b46bb21f0"},"cell_type":"markdown","source":"## Add Dimensionality Reduction - Truncated Singular Value Decomposition"},{"metadata":{"trusted":true,"_uuid":"01d314ff2b9fcd66a59852efa85ad776ba6a0afe","collapsed":true},"cell_type":"code","source":"# Define and Fit TruncatedSVD Dimensionality Reduction Model\nsvd = TruncatedSVD(n_components=50, n_iter=20, random_state=42)\nsvd.fit(X) \n\n# Transform\nX = svd.transform(X)\ntest_df = svd.transform(test_df)\n\n# Logistic\nmodel = LogisticRegression(multi_class = 'ovr', C=1)\nmodel.fit(X,y)\n\n# Submit\nsubmission = model.predict(test_df)\nsubmission_df = pd.Series(submission).rename(\"Sentiment\")\nsubmission_df.index = testdex\nsubmission_df.to_csv(\"TSVD_n_Logistic_sub.csv\",index=True,header=True)\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"910dbccc5c66e3a752c4032c1d5db54c550173cf","collapsed":true},"cell_type":"code","source":"print(\"Notebook Runtime: %0.0f seconds\"%((time.time() - notebookstart)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}