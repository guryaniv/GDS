{"cells":[{"metadata":{"_uuid":"44bbae45bfef6ef3547b66e18166d9b99a2f4462"},"cell_type":"markdown","source":"# Introduction: Manual Feature Engineering (part two)\n\nIn this notebook we will expand on the [Introduction to Manual Feature Engineering](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/output) notebook. We will use the aggregation and value counting functions developed in that notebook in order to incorporate information from the `previous_application`, `POS_CASH_balance`, `installments_payments`, and `credit_card_balance` data files. We already used the information from the `bureau` and `bureau_balance` in the previous notebook and were able to improve our competition score compared to using only the `application` data. After running a model with the features included here, performance does increase, but we run into issues with an explosion in the number of features! I'm working on a notebook of feature selection, but for this notebook we will continue building up a rich set of data for our model. \n\nThe definitions of the four additional data files are:\n\n* previous_application (called `previous`): previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n* POS_CASH_BALANCE (called `cash`): monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n* credit_card_balance (called `credit`): monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n* installments_payment (called `installments`): payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment."},{"metadata":{"_uuid":"04c8826306314ba274e409667aa58a806c6f462c"},"cell_type":"markdown","source":"# Functions \n\nWe spent quite a bit of time developing two functions in the previous notebook:\n\n* `agg_numeric`: calculate aggregation statistics (`mean`, `count`, `max`, `min`) for numeric variables.\n* `agg_categorical`: compute counts and normalized counts of each category in a categorical variable.\n\nTogether, these two functions can extract information about both the numeric and categorical data in a dataframe. Our general approach will be to apply both of these functions to the dataframes, grouping by the client id, `SK_ID_CURR`. For the `POS_CASH_balance`, `credit_card_balance`, and `installment_payments`, we can first group by the `SK_ID_PREV`, the unique id for the previous loan. Then we will group the resulting dataframe by the `SK_ID_CURR` to calculate the aggregation statistics for each client across all of their previous loans. If that's a little confusing, I'd suggest heading back to the [first feature engineering notebook](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/output).**"},{"metadata":{"trusted":true,"_uuid":"9b3c521b3cb7916e7473668356a6348d272c0655","collapsed":true},"cell_type":"code","source":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\n\n# Memory management\nimport gc ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa91c4f6d654dea9c61780fae217acac0b322835"},"cell_type":"markdown","source":"## Function to Aggregate Numeric Data\n\nThis groups data by the `group_var` and calculates `mean`, `max`, `min`, and `sum`. It will only be applied to numeric data by default in pandas."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c5a7850897a9f7ea20be066f379b29d46e670584"},"cell_type":"code","source":"def agg_numeric(df, parent_var, df_name):\n    \"\"\"\n    Groups and aggregates the numeric values in a child dataframe\n    by the parent variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the child dataframe to calculate the statistics on\n        parent_var (string): \n            the parent variable used for grouping and aggregating\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated by the `parent_var` for \n            all numeric columns. Each observation of the parent variable will have \n            one row in the dataframe with the parent variable as the index. \n            The columns are also renamed using the `df_name`. Columns with all duplicate\n            values are removed. \n    \n    \"\"\"\n    \n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != parent_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    # Only want the numeric variables\n    parent_ids = df[parent_var].copy()\n    numeric_df = df.select_dtypes('number').copy()\n    numeric_df[parent_var] = parent_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n\n    # Need to create new column names\n    columns = []\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        if var != parent_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n    \n    agg.columns = columns\n    \n    # Remove the columns with all redundant values\n    _, idx = np.unique(agg, axis = 1, return_index=True)\n    agg = agg.iloc[:, idx]\n    \n    return agg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c99bb22a98377fda42944fbd04ad688cb53e3177"},"cell_type":"markdown","source":"### Function to Calculate Categorical Counts\n\nThis function calculates the occurrences (counts) of each category in a categorical variable for each client. It also calculates the normed count, which is the count for a category divided by the total counts for all categories in a categorical variable. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6899ea01794c3c3e44c7bbc6bde3613202d545d4"},"cell_type":"code","source":"def agg_categorical(df, parent_var, df_name):\n    \"\"\"\n    Aggregates the categorical features in a child dataframe\n    for each observation of the parent variable.\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    parent_var : string\n        The variable by which to group and aggregate the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with aggregated statistics for each observation of the parent_var\n        The columns are also renamed and columns with duplicate values are removed.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('category'))\n\n    # Make sure to put the identifying id on the column\n    categorical[parent_var] = df[parent_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['sum', 'count', 'mean']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    # Remove duplicate columns by values\n    _, idx = np.unique(categorical, axis = 1, return_index = True)\n    categorical = categorical.iloc[:, idx]\n    \n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eda8a6b2429cde1fcbaf5f5beec0707f739b9e73"},"cell_type":"markdown","source":"### Function for KDE Plots of Variable\n\nWe also made a function that plots the distribution of variable colored by the value of `TARGET` (either 1 for did not repay the loan or 0 for did repay the loan). We can use this function to visually examine any new variables we create. This also calculates the correlation cofficient of the variable with the target which can be used as an approximation of whether or not the created variable will be useful. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"557b945ebd7663bee123b9074389141d38c18b6b"},"cell_type":"code","source":"# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36367125eae236c36e171748513f429240772c8d"},"cell_type":"markdown","source":"# Function to Convert Data Types\n\nThis will help reduce memory usage by using more efficient types for the variables. For example `category` is often a better type than `object` (unless the number of unique categories is close to the number of rows in the dataframe)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"43ef53cbbe79461bb7d4bb1785384026cc1986e3"},"cell_type":"code","source":"import sys\n\ndef return_size(df):\n    \"\"\"Return size of dataframe in gigabytes\"\"\"\n    return round(sys.getsizeof(df) / 1e9, 2)\n\ndef convert_types(df, print_info = False):\n    \n    original_memory = df.memory_usage().sum()\n    \n    # Iterate through each column\n    for c in df:\n        \n        # Convert ids and booleans to integers\n        if ('SK_ID' in c):\n            df[c] = df[c].fillna(0).astype(np.int32)\n            \n        # Convert objects to category\n        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n            df[c] = df[c].astype('category')\n        \n        # Booleans mapped to integers\n        elif list(df[c].unique()) == [1, 0]:\n            df[c] = df[c].astype(bool)\n        \n        # Float64 to float32\n        elif df[c].dtype == float:\n            df[c] = df[c].astype(np.float32)\n            \n        # Int64 to int32\n        elif df[c].dtype == int:\n            df[c] = df[c].astype(np.int32)\n        \n    new_memory = df.memory_usage().sum()\n    \n    if print_info:\n        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ed0bafec723c5206f130895838245fcc8d91b26"},"cell_type":"markdown","source":"Let's deal with one dataframe at a time. First up is the `previous_applications`. This has one row for every previous loan a client had at Home Credit. A client can have multiple previous loans which is why we need to aggregate statistics for each client."},{"metadata":{"_uuid":"063a93d01f0afaebd41f256c13cbecd67d177f19"},"cell_type":"markdown","source":"### previous_application"},{"metadata":{"trusted":true,"_uuid":"4f81a6dce526a8627039e58d1f7025941fde2798","collapsed":true},"cell_type":"code","source":"previous = pd.read_csv('../input/previous_application.csv')\nprevious = convert_types(previous, print_info=True)\nprevious.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f70f561a3075d5c0f68c31cf33449d9db8583ae9","collapsed":true},"cell_type":"code","source":"# Calculate aggregate statistics for each numeric column\nprevious_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\nprint('Previous aggregation shape: ', previous_agg.shape)\nprevious_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"267a4f0a8097c4b13bbcb67faa25fafb7dee0af5","collapsed":true},"cell_type":"code","source":"# Calculate value counts for each categorical column\nprevious_counts = agg_categorical(previous, 'SK_ID_CURR', 'previous')\nprint('Previous counts shape: ', previous_counts.shape)\nprevious_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc0032e7c0f6f591854d31d484bd14f709f4ff13"},"cell_type":"markdown","source":"We can join the calculated dataframe to the main training dataframe using a merge. Then we should delete the calculated dataframes to avoid using too much of the kernel memory."},{"metadata":{"trusted":true,"_uuid":"7b7651f8ad471a23e8dac04c5ee8ee3c42b214ea","collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/application_train.csv')\ntrain = convert_types(train)\ntest = pd.read_csv('../input/application_test.csv')\ntest = convert_types(test)\n\n# Merge in the previous information\ntrain = train.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\ntrain = train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n\ntest = test.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\ntest = test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Remove variables to free memory\ngc.enable()\ndel previous, previous_agg, previous_counts\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cec09d0de0aeaed65b63ed59889b3c121cade929"},"cell_type":"markdown","source":"We are going to have to be careful about calculating too many features. We don't want to overwhelm the model with too many irrelevant features or features with too many missing values. In the previous notebook, we removed any features with more than 75% missing values. To be consistent, we will apply that same logic here. "},{"metadata":{"_uuid":"d4c38cd12402823c091e81dbcf42ae56e108c055"},"cell_type":"markdown","source":"## Function to Calculate Missing Values"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"836b21cd4edfa344efc9bed804bfb889fc33b5ff"},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df, print_info = False):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        if print_info:\n            # Print some summary information\n            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n                \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n                  \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d6a1c60994bd2be69b842c5393dbe2799c4ff74c"},"cell_type":"code","source":"def remove_missing_columns(train, test, threshold = 90):\n    # Calculate missing stats for train and test (remember to calculate a percent!)\n    train_miss = pd.DataFrame(train.isnull().sum())\n    train_miss['percent'] = 100 * train_miss[0] / len(train)\n    \n    test_miss = pd.DataFrame(test.isnull().sum())\n    test_miss['percent'] = 100 * test_miss[0] / len(test)\n    \n    # list of missing columns for train and test\n    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n    \n    # Combine the two lists together\n    missing_columns = list(set(missing_train_columns + missing_test_columns))\n    \n    # Print information\n    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n    \n    # Drop the missing columns and return\n    train = train.drop(columns = missing_columns)\n    test = test.drop(columns = missing_columns)\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d90c583647b95c11fa72273931ef51d584e9795","collapsed":true},"cell_type":"code","source":"train, test = remove_missing_columns(train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8b6b99fcfc81e3965213e2e98480365a9c25db1"},"cell_type":"markdown","source":"# Applying to More Data"},{"metadata":{"_uuid":"c9ff11c7e3fbd97f328c40217a902f2fb3dcad73"},"cell_type":"markdown","source":"### Function to Aggregate Stats at the Client Level"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2041ca7741921a3508258b616db39101e1e4d722"},"cell_type":"code","source":"def aggregate_client(df, group_vars, df_names):\n    \"\"\"Aggregate a dataframe with data at the loan level \n    at the client level\n    \n    Args:\n        df (dataframe): data at the loan level\n        group_vars (list of two strings): grouping variables for the loan \n        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n        names (list of two strings): names to call the resulting columns\n        (example ['cash', 'client'])\n        \n    Returns:\n        df_client (dataframe): aggregated numeric stats at the client level. \n        Each client will have a single row with all the numeric data aggregated\n    \"\"\"\n    \n    # Aggregate the numeric columns\n    df_agg = agg_numeric(df, parent_var = group_vars[0], df_name = df_names[0])\n    \n    # If there are categorical variables\n    if any(df.dtypes == 'category'):\n    \n        # Count the categorical columns\n        df_counts = agg_categorical(df, parent_var = group_vars[0], df_name = df_names[0])\n\n        # Merge the numeric and categorical\n        df_by_loan = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n\n        gc.enable()\n        del df_agg, df_counts\n        gc.collect()\n\n        # Merge to get the client id in dataframe\n        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n\n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n\n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n\n        \n    # No categorical variables\n    else:\n        # Merge to get the client id in dataframe\n        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n        \n        gc.enable()\n        del df_agg\n        gc.collect()\n        \n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n        \n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n        \n    # Memory management\n    gc.enable()\n    del df, df_by_loan\n    gc.collect()\n\n    return df_by_client","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65c0196b37488c09ec7205edcb6726d365b5a96a"},"cell_type":"markdown","source":"## Monthly Cash Data"},{"metadata":{"trusted":true,"_uuid":"e2e13040abf2af99ef9dcaea9b29f8128d2e4f07","collapsed":true},"cell_type":"code","source":"cash = pd.read_csv('../input/POS_CASH_balance.csv')\ncash = convert_types(cash, print_info=True)\ncash.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e261b4a55167cf017eb5a190b8c83571b5e1398d","collapsed":true},"cell_type":"code","source":"cash_by_client = aggregate_client(cash, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['cash', 'client'])\ncash_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa1c17e2ea0742c9bb3acdf7e7e218b0a8fc4fea","collapsed":true},"cell_type":"code","source":"print('Cash by Client Shape: ', cash_by_client.shape)\ntrain = train.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n\ngc.enable()\ndel cash, cash_by_client\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03b30744261f48c0515e1d1601654a780eaf1d51","collapsed":true},"cell_type":"code","source":"train, test = remove_missing_columns(train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f15d0b5811e29892393f643cba60f407a24084a4"},"cell_type":"markdown","source":"## Monthly Credit Data"},{"metadata":{"trusted":true,"_uuid":"0bb1f02376642b2bb50ff35620a482312ecbf258","collapsed":true},"cell_type":"code","source":"credit = pd.read_csv('../input/credit_card_balance.csv')\ncredit = convert_types(credit, print_info = True)\ncredit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0a8e270271e7037d893386550a091e24f10a5cd","collapsed":true},"cell_type":"code","source":"credit_by_client = aggregate_client(credit, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['credit', 'client'])\ncredit_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a6501414046b9c6c1b7b72c13d3227efde93435","collapsed":true},"cell_type":"code","source":"print('Credit by client shape: ', credit_by_client.shape)\n\ntrain = train.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n\ngc.enable()\ndel credit, credit_by_client\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c4d9012f8c6d426a40949c2f96883480193d680","collapsed":true},"cell_type":"code","source":"train, test = remove_missing_columns(train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13a46dfc971d3010b340636879697b5bba0f2255"},"cell_type":"markdown","source":"### Installment Payments"},{"metadata":{"trusted":true,"_uuid":"fe3bef4aaa73791c63e02ec2435a884984408ace","collapsed":true},"cell_type":"code","source":"installments = pd.read_csv('../input/installments_payments.csv')\ninstallments = convert_types(installments, print_info = True)\ninstallments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e68d671fc3fe08df21b81404ee1c3c4030ebafff","collapsed":true},"cell_type":"code","source":"installments_by_client = aggregate_client(installments, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\ninstallments_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e70b057d5f1ec2ba66738ec1ef260e02133a5de9","collapsed":true},"cell_type":"code","source":"print('Installments by client shape: ', installments_by_client.shape)\n\ntrain = train.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n\ngc.enable()\ndel installments, installments_by_client\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"478cdd9f3387297c88cca15fe725425d4910b776","collapsed":true},"cell_type":"code","source":"train, test = remove_missing_columns(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bfb6d762bbdbfadf2ac7aac8da7d835e53f5679","collapsed":true},"cell_type":"code","source":"print('Final Training Shape: ', train.shape)\nprint('Final Testing Shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"76fc295b60d6956feaf40fc10172aa81823f0066"},"cell_type":"code","source":"print(f'Final training size: {return_size(train)}')\nprint(f'Final testing size: {return_size(test)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b91556063fd2fc8b53ed8b7e674822f96e65d6cd"},"cell_type":"markdown","source":" #### Save All Newly Calculated Features\n \n Unfortunately, saving all the created features does not work in a Kaggle notebook. You will have to run the code on your personal machine. I have run the code and uploaded the [entire datasets here](https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features). I plan on doing some feature selection and uploading reduced versions of the datasets. Right now, they are slightly to big to handle in Kaggle notebooks or scripts. ."},{"metadata":{"trusted":true,"_uuid":"33e37b48ab79b6773cbbb4331aedc6117fecef23","collapsed":true},"cell_type":"code","source":"# train.to_csv('train_previous_raw.csv', index = False, chunksize = 500)\n# test.to_csv('test_previous_raw.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2f5fd2a66a1e0c58cb9f8d8e4ee3e9febc6eae0e"},"cell_type":"code","source":"if 'SK_ID_CURR' not in train.columns:\n    train = train.reset_index()\nif 'SK_ID_CURR' not in test.columns:\n    test = test.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7bd7f24dabe54d068e48b90ac1e42291bed6bf4d"},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a14ed2eaa10323c839fb37c0137fb69979578b35"},"cell_type":"code","source":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a8fe284ddf221e6482cfeac410e5f614957f904a"},"cell_type":"code","source":"submission, fi, metrics = model(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aa8eb4ce19d30fba740a03f8f2402739df5656fa"},"cell_type":"code","source":"metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a60d52e91181744d1d0e54182865b465411c7c96"},"cell_type":"code","source":"submission.to_csv('submission_manualp2.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3c4c3794bb075804b8488cef34716361c3e6b04e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}