{"cells":[{"metadata":{"_uuid":"177d2cabf507be0b6f229661443ef039ef874c48"},"cell_type":"markdown","source":"# Titanic Competition - An Introduction to Data Analytics\n## By Gabriele Sarti\n\n* URL of the Kaggle competition: [https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)\n\n* Find it here on Github: **_TODO_**\n\n## Table of Contents\n\n0. [Defining the problem](#problem)\n1.  [Preliminary steps](#first-steps)\n2. [Data cleaning](#clean)\n3. [Data exploration](#explore)\n4. [Data visualization](#viz)\n5. [Data modeling](#model)\n6. [Parameter tuning](#tune)\n7. [Final results](#end)"},{"metadata":{"_uuid":"3011c1dafce71f1101900e680c040c6bed29ecd0"},"cell_type":"markdown","source":"## 1 - Preliminary steps<a name=\"first-steps\"></a>\n\n### Load data analysis libraries\n\n\nI will use the standard tools for performing data analysis in Python, namely **numpy** for linear algebra and arrays,  **pandas** for managing the dataset and I/O data operations, **scipy** for advanced mathematics operations and **matplotlib** for plotting the results and gaining insights.\n\nI will also use **sklearn** for data cleaning and data modeling, along with **seaborn** for additional visualization purposes."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General libraries for scientific purposes\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport matplotlib\nimport sklearn\nimport os\nimport sys\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n# Plotting and visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data models\nfrom sklearn.linear_model import LinearRegression, LogisticRegressionCV, Perceptron\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.model_selection import cross_validate, ShuffleSplit, GridSearchCV\nfrom xgboost import XGBClassifier\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Libraries versions\nprint('Python: {}'.format(sys.version))\nprint('numpy: {}'.format(np.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('scipy: {}'.format(sp.__version__))\nprint('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('-'*30)\n\n# Print local folder content\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f20f060e345cc79e35074baf9d2d87900e1a8cbc"},"cell_type":"markdown","source":"### Load data\n\nBoth the datasets are found inside the **input** folder, accessible as it follows.\n\nI create a copy of the training set in order to set up a data cleaning pipeline and try different transformations."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Importing local datasets\ntrain = pd.read_csv('../input/train.csv')\ntest  = pd.read_csv('../input/test.csv')\n\n# Reference of both train and test, for cleaning purposes\ndatasets = [train, test]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f06d5f5509da7ed479522bb5aea224b5f9069252"},"cell_type":"markdown","source":"### First observations\n\nI use the functions `head`, `info` and `describe` to acquire some basic useful information about the training set.\n"},{"metadata":{"trusted":true,"_uuid":"5c95c0fa0f2e44e26a1a8ed7ce5d389c3acd30b3"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c95c0fa0f2e44e26a1a8ed7ce5d389c3acd30b3"},"cell_type":"code","source":"train.info()\nprint('-'*30)\ntest.info()\ntrain.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f04ad806de39c2a6a24a99dd58a7f29f0ddc09b6"},"cell_type":"markdown","source":"We are interested in particular in the null values present inside both the train and the test dataset, since we will have to adjust them in order to perform our analysis."},{"metadata":{"trusted":true,"_uuid":"f6ccd3ba4373843a93689da6167dba7e61b02740"},"cell_type":"code","source":"print('Null training values:\\n', train.isnull().sum())\nprint(\"-\"*30)\nprint('Test/Validation columns with null values:\\n', test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41c9fd1cc1e08d723f82a7d3a3bd37e28512f1bf"},"cell_type":"markdown","source":"We can recap the data dictionary by adding this new information:\n\n| Type    | Variable | Definition                                  | Key                                            | # train values | # train null values | # test values | # test null values |\n|---------|----------|---------------------------------------------|------------------------------------------------|----------------|---------------------|---------------|--------------------|\n| Categorical   | survived | Survival                                    | 0 = No, 1 = Yes                                | 891            | 0                   | 418           | 0                  |\n| Ordinal   | pclass   | Ticket class                                | 1 = 1st, 2 = 2nd, 3 = 3rd                      | 891            | 0                   | 418           | 0                  |\n| String  | name     | Name of the passenger                       |                                                | 891            | 0                   | 418           | 0                  |\n| Categorical  | sex      | Sex                                         |                                                | 891            | 0                   | 418           | 0                  |\n| Continuous | age      | Age in years                                |                                                | 712            | 177                 | 332           | 86                 |\n| Categorical  | sibsp    | # of siblings / spouses aboard the  Titanic |                                                | 891            | 0                   | 418           | 0                  |\n| Categorical  | parch    | # of parents / children aboard the  Titanic |                                                | 891            | 0                   | 418           | 0                  |\n| String  | ticket   | Ticket number                               |                                                | 891            | 0                   | 418           | 0                  |\n| Continuous | fare     | Passenger fare                              |                                                | 891            | 0                   | 417           | 1                  |\n| String  | cabin    | Cabin number                                |                                                | 204            | 687                 | 91            | 327                |\n| Categorical  | embarked | Port of embarkation                         | C = Cherbourg, Q = Queenstown, S = Southampton | 889            | 2                   | 418           | 0                  |\n\n#### Variable notes:\n\n\n* **pclass:** A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n* **age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* **sibsp:** The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancés were ignored)\n\n* **parch:** The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them."},{"metadata":{"_uuid":"8f25156ff8dd58ecbb68da0db5b5ab99fddd6ae2"},"cell_type":"markdown","source":"### Some considerations from preliminary phase\n\nSince some of the models I will benchmark for this competition require the entries to be non-null, I must take care of null values. \n\nThe most prominent categories having null values are `age` and `cabin`, with a consistent number of null values, followed by `embarked` (only in the train set) and `fare` (only in the test set), having just a couple of nulls.\n\n"},{"metadata":{"_uuid":"d0211b340824a40218e707704c76f1b57c8db443"},"cell_type":"markdown","source":"## 2 - Data Cleaning <a name=\"clean\"></a>\n\n### Removing null values\n\nThe approach I choose for dealing with those variables is the following:\n* Since age and fare are a float values, I will fill null entries with the respective medians across the dataset.\n* Since embarked is categorical, I will fill null embarked entries with the mode of the variable across the dataset.\n* Since cabin is not relevant for my analysis, I will drop it along with `PassengerId` and `Ticket` to reduce the noise in my training set."},{"metadata":{"trusted":true,"_uuid":"604f4493a6e8df63e628a31fa54d189a241a6734"},"cell_type":"code","source":"drop_column = ['Cabin', 'Ticket']\n\nfor d in datasets:    \n    d['Age'].fillna(d['Age'].median(), inplace = True)\n    d['Fare'].fillna(d['Fare'].median(), inplace = True)\n    d['Embarked'].fillna(d['Embarked'].mode()[0], inplace = True)\n    d.drop(drop_column, axis=1, inplace = True)    \n\nprint(train.isnull().sum())\nprint(\"-\"*30)\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0211b340824a40218e707704c76f1b57c8db443"},"cell_type":"markdown","source":"### Experimenting with feature combinations\n\nBy observing the data and other people kernels, I got some ideas for performing feature combination:\n\n* Merging `Sibsp` and `Parch` in a single numerical discrete attribute, called `FamilySize`.\n* Creating a `IsAlone` categorical attribute that is true when `FamilySize = 0`.\n* Creating a `Title` categorical attribute, with titles extracted from passengers' names.\n* Converting `Fare` and `Age` attributes into ordinal ones by grouping values into categories."},{"metadata":{"trusted":true,"_uuid":"58151b28f63637a4e8212dfe97dd843623bec2b6"},"cell_type":"code","source":"for d in datasets:\n    d['FamilySize'] = d['SibSp'] + d['Parch'] + 1\n\n    d['IsAlone'] = 1\n    d['IsAlone'].loc[d['FamilySize'] > 1] = 0\n    \n    # Extract titles from names\n    d['Title'] = d['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \n    # Group uncommon titles under \"other\" label\n    d['Title'] = d['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n    \n    # Group synonyms together \n    d['Title'] = d['Title'].replace('Mlle', 'Miss')\n    d['Title'] = d['Title'].replace('Ms', 'Miss')\n    d['Title'] = d['Title'].replace('Mme', 'Mrs')\n\n    # Grouping continuous values into categories\n    d['FareBand'] = pd.qcut(d['Fare'], 4)\n    d['AgeBand'] = pd.cut(d['Age'].astype(int), 5)\n\ntrain.info()\ntest.info()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d810596ff987b975e30f6196f33ca48c2d78a0f"},"cell_type":"markdown","source":"### Converting formats\n\nWe can now convert all the categorical literal values in numerical values thanks to the `LabelEncoder` class.\n\nWe create two clones before applying the transformation for plotting and understanding purposes."},{"metadata":{"trusted":true,"_uuid":"aa41218eb68bb3d2e7e4606d2b498ec45643220f"},"cell_type":"code","source":"le = LabelEncoder()\n\ntitanic_train = train.copy(deep = 'True')\ntitanic_test = test.copy(deep = 'True')\ntitanic_train['Age'] = titanic_train['AgeBand']\ntitanic_train['Fare'] = titanic_train['FareBand']\ntitanic_test['Age'] = titanic_test['AgeBand']\ntitanic_test['Fare'] = titanic_test['FareBand']\n\ncolumn_transformed = ['Sex', 'Embarked', 'Title', 'AgeBand', 'FareBand']\ncolumn_transform = ['Sex', 'Embarked', 'Title', 'Age', 'Fare']\n\nfor d in datasets:\n    for i in range(len(column_transform)):\n        d[column_transform[i]] = le.fit_transform(d[column_transformed[i]])\n\ndatasets.append(titanic_train)\ndatasets.append(titanic_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d10d335c7ae7d19ac6cbe12624cf35b3275cc6fa"},"cell_type":"markdown","source":"### Remove unnecessary columns\n\nNow that we have engineered our new columns and converted the old ones, we can finally drop unnecessary columns\n* `Name`, since it is a simple string from which we already extracted the title\n* `SibSp` and `Parch` since they have been merged in `FamilySize`, and they don't seem very correlated with the survival when taken alone.\n* `FareBand` and `AgeBand`, since we used them to change `Age` and `Fare` values.\n* For the training set, we also want to drop the `PassengerId`"},{"metadata":{"trusted":true,"_uuid":"a1a5f09dfc14dc824bcfbaaa09083a8a8eb5a54d"},"cell_type":"code","source":"drop_column = ['Name', 'SibSp', 'Parch', 'FareBand', 'AgeBand']\nfor d in datasets:\n        d.drop(drop_column, axis=1, inplace = True)\ntrain.drop('PassengerId', axis=1, inplace = True)\ntitanic_train.drop('PassengerId', axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"effb1a1210af7c4d2d48387c1773ee44c7588cf8"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"393fd14974eea7f1b0060900e04bb148e20789fe"},"cell_type":"code","source":"titanic_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0211b340824a40218e707704c76f1b57c8db443"},"cell_type":"markdown","source":"## 3 - Data exploration <a name=\"explore\"></a>\n\n### Looking for correlations\n\nWe'll use the `corr` method to compute the standard correlation coefficient between the numerical features and the survival of our passengers.\n\nFor categorical values, we'll simply observe the survival rate compared to each category."},{"metadata":{"trusted":true,"_uuid":"d2adc09afeb79f912bf50e96698730f02dbe4ed3"},"cell_type":"code","source":"feature_names = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'FamilySize', 'IsAlone', 'Title']\ncorr_matrix = train.corr()\n\nprint(corr_matrix[\"Survived\"].sort_values(ascending=False))\nprint(\"-\"*30)\n\nfor feature in feature_names:\n    print('Correlation between Survived and', feature)\n    print(titanic_train[[feature, 'Survived']].groupby([feature], as_index=False).mean())\n    print(\"-\"*30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b93dd3a1ca3f9696b8878af3ebdff5f22b410ce"},"cell_type":"markdown","source":"Thoughts on the correlation patterns:\n* Being female is definitely a significant factor, shown both by `Sex` and `Title` attributes.\n* Being married, or at least in company of someone, seems relevant, as both shown by `FamiliSize` and `Title` attributes\n* Social rank and financial capability seems to play a role, as we can see in `Title` (Master), `Pclass` and `Fare`. \n\nAll those points are quite intuitive.\n"},{"metadata":{"trusted":true,"_uuid":"c8ab53b3f6a3edf05ae55eb36f646a9ea59f6477"},"cell_type":"markdown","source":"## 4 - Data Visualization <a name=\"viz\"></a>\n\n# TODO"},{"metadata":{"_uuid":"4442d65cba9369a62cae3aef3600bd4217b66e0f"},"cell_type":"markdown","source":"## 5 - Data Modeling <a name=\"model\"></a>\n\n### Creating train and test copies for models"},{"metadata":{"trusted":true,"_uuid":"9e4e4802f903e216c457440e91ab7ffebe002e51"},"cell_type":"code","source":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ac689844dfe26cb50ed77844ee99099fe0854d7"},"cell_type":"code","source":"# The set of models I am going to compare\nmodels = [\n            LinearRegression(),\n            LogisticRegressionCV(),\n            Perceptron(),\n            GaussianNB(),\n            KNeighborsClassifier(),\n            SVC(probability=True),\n            DecisionTreeClassifier(),\n            AdaBoostClassifier(),\n            RandomForestClassifier(),\n            XGBClassifier()    \n        ]\n\n# Create a table of comparison for models\nmodels_columns = ['Name', 'Parameters','Train Accuracy', 'Validation Accuracy', 'Execution Time']\nmodels_df = pd.DataFrame(columns = models_columns)\npredictions = pd.DataFrame(columns = ['Survived'])\n\ncv_split = ShuffleSplit(n_splits = 10, test_size = .2, train_size = .8, random_state = 0 )\n\nindex = 0\nfor model in models:\n    models_df.loc[index, 'Name'] = model.__class__.__name__\n    models_df.loc[index, 'Parameters'] = str(model.get_params())\n    \n    scores = cross_validate(model, X_train, Y_train, cv= cv_split)\n\n    models_df.loc[index, 'Execution Time'] = scores['fit_time'].mean()\n    models_df.loc[index, 'Train Accuracy'] = scores['train_score'].mean()\n    models_df.loc[index, 'Validation Accuracy'] = scores['test_score'].mean()   \n    \n    index += 1\n\nmodels_df.sort_values(by = ['Validation Accuracy'], ascending = False, inplace = True)\nmodels_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b649fda676a1be1dd36e03930c6ce578923f8b31"},"cell_type":"markdown","source":"## 6 - Parameter Tuning <a name=\"tune\"></a>\n\n### Grid Search Tuning"},{"metadata":{"trusted":true,"_uuid":"e65e7485b2ca2e364e8d40074afed394f43bfd8f"},"cell_type":"code","source":"param_grid = {\n              'criterion': ['gini', 'entropy'],\n              'max_depth': [2,4,6,8,10,None],\n              'random_state': [0]\n             }\n\ntree = DecisionTreeClassifier(random_state = 0)\nscore = cross_validate(tree, X_train, Y_train, cv  = cv_split)\ntree.fit(X_train, Y_train)\n\ngrid_search = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\ngrid_search.fit(X_train, Y_train)\n\nprint('Before GridSearch:')\nprint('Parameters:', tree.get_params())\nprint(\"Training score:\", score['train_score'].mean()) \nprint(\"Validation score\", score['test_score'].mean())\nprint('-'*30)\nprint('After GridSearch:')\nprint('Parameters:', grid_search.best_params_)\nprint(\"Training score:\", grid_search.cv_results_['mean_train_score'][grid_search.best_index_]) \nprint(\"Validation score\", grid_search.cv_results_['mean_test_score'][grid_search.best_index_])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a373e8c59476d57010feb06eb14d343754296bc9"},"cell_type":"markdown","source":"## 7 - Final Results <a name=\"end\"></a>\n\nThe final achieved result using a `DecisionTreeClassifier` is 79.425 % of accuracy on the test set."},{"metadata":{"trusted":true,"_uuid":"e45967c8b1a06887e05705829bf20887a6e5791f"},"cell_type":"code","source":"final_tree = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\nfinal_tree.fit(X_train, Y_train)\ntest['Survived'] = final_tree.predict(X_test)\n\nsubmission = test[['PassengerId','Survived']]\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9904a27e0bb1ab50b2d17f10e8793e86407cfa74"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}