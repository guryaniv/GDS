{"cells":[{"metadata":{"_cell_guid":"e73842e2-c530-467d-b3cc-26b85183fa34","_uuid":"10074bd89683f855c563e32f54147b354f756ce7"},"cell_type":"markdown","source":"Based on the [GLRC kernel](https://www.kaggle.com/the1owl/google-landmark-random-choice-glrc) by [the1owl](https://www.kaggle.com/the1owl), I have tested some other strategies for drawing random choices from the distribution of landmark frequencies in the train set:"},{"metadata":{"collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport pylab as pl\n\ntrain = pd.read_csv('../input/train.csv')\n# test = pd.read_csv('../input/test.csv')\nsub = pd.read_csv('../input/sample_submission.csv')\npl.seed(0)\n","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"b8172129-ae78-4a82-9d8c-5055cba695ce","_uuid":"eb02026502a586e42c488ed143030786d4e7fc1a"},"cell_type":"markdown","source":"# version 1\nDraw landmakrd_id and probability values from the distribution in train set:"},{"metadata":{"collapsed":true,"_uuid":"b751607cde2293ade6e743a75ca6eb59712d1e49","_cell_guid":"862a5a54-414d-4cd8-8211-fe83f704e2c3","trusted":false},"cell_type":"code","source":"probs = train.landmark_id.value_counts() / train.shape[0]\nsub['landmark_id'] = train.landmark_id.iloc[pl.randint(0, train.shape[0], sub.shape[0])].values\nsub['prob'] = probs[sub.landmark_id].values\nsub['landmarks'] = sub.landmark_id.astype(str) + ' ' + sub.prob.astype(str)\nsub[['id','landmarks']].to_csv('submission_1.csv', index=False)\nsub[['id','landmarks']].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bdf5b908-6028-4f09-9017-f978cf42ab42","_uuid":"750949b23b7ea650c3a619fd49d3612160c489f3"},"cell_type":"markdown","source":"# version 2\nUse the most frequent landmark and its probability for all test locations:"},{"metadata":{"collapsed":true,"_uuid":"fe6841450637cd3d5ff5220f226a3687fd69870f","_cell_guid":"d23bec9d-5878-4f12-8d33-5a4db8606386","trusted":false},"cell_type":"code","source":"sub['landmarks'] = '%d %g' % (probs.index[0], probs[probs.index[0]])\nsub[['id','landmarks']].to_csv('submission_2.csv', index=False)\nsub[['id','landmarks']].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04c28c1a-c17a-4c3c-8322-decbd81c76e9","_uuid":"8aaa40347ad52df81bd49ba1be3c2d7c0a32f701"},"cell_type":"markdown","source":"# version 3\nUse only the first 1000 most frequent landmarks in train set for building the test set values:"},{"metadata":{"collapsed":true,"_uuid":"11d49d63ede522dbf0e1ea9d8df4dea134766397","_cell_guid":"029554fe-3b68-4354-a301-008c48fc071c","trusted":false},"cell_type":"code","source":"N = 1000\nprobs = train.landmark_id.value_counts() / train.shape[0]\nprobs = probs.iloc[:N]\nprobs = pd.DataFrame({'landmark_id': probs.index,\n                      'probability': probs.values}, index=pl.arange(N))\nT = pd.merge(train, probs, on='landmark_id', how='inner')\ninx = pl.randint(0, T.shape[0], sub.shape[0])\nsub['landmark_id'] = T.landmark_id.iloc[inx].values\nsub['prob'] = T.probability.iloc[inx].values\nsub['landmarks'] = sub.landmark_id.astype(str) + ' ' + sub.prob.astype(str)\nsub[['id','landmarks']].to_csv('submission_3.csv', index=False)\nsub[['id','landmarks']].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae4fb92b-e19e-4782-b2a8-7a8df033e32f","_uuid":"e29257aa407b34f67c51b6947774091af7c429e6"},"cell_type":"markdown","source":"# version 4\n\nThe same as version 3, but with unique confidence values:\n\n(it might help hacking GAP)"},{"metadata":{"collapsed":true,"_uuid":"83df8d0f8727772546926a68bc01cf5c63627cfa","_cell_guid":"b7a1ed2a-5f74-4c5b-8779-ae0eb460ebf8","trusted":true},"cell_type":"code","source":"N = 1000\nprobs = train.landmark_id.value_counts() / train.shape[0]\nprobs = probs.iloc[:N]\nprobs = pd.DataFrame({'landmark_id': probs.index,\n                      'probability': 0.1}, index=pl.arange(N))\nT = pd.merge(train, probs, on='landmark_id', how='inner')\ninx = pl.randint(0, T.shape[0], sub.shape[0])\nsub['landmark_id'] = T.landmark_id.iloc[inx].values\nsub['prob'] = T.probability.iloc[inx].values\nsub['landmarks'] = sub.landmark_id.astype(str) + ' ' + sub.prob.astype(str)\nsub[['id','landmarks']].to_csv('submission_4.csv', index=False)\nsub[['id','landmarks']].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ee83b619da599351c66899b5e4a8690405ccc7d"},"cell_type":"markdown","source":"# summary\n\nto summarize that, I show here how the GAP changes with number of landmarks from train set used in randomly gueesing the landmakrks in the test set:\n\n(I use the GAP function defined by [David](https://www.kaggle.com/davidthaler) in [https://www.kaggle.com/davidthaler/gap-metric](https://www.kaggle.com/davidthaler/gap-metric)"},{"metadata":{"trusted":true,"_uuid":"f200a56f3d222d20e220cd6d8653449bcf0f835e"},"cell_type":"code","source":"import seaborn as sns\n\ndef GAP_vector(pred, conf, true):\n    '''\n    Compute Global Average Precision (aka micro AP), the metric for the\n    Google Landmark Recognition competition.\n    This function takes predictions, labels and confidence scores as vectors.\n    In both predictions and ground-truth, use None/np.nan for \"no label\".\n\n    Args:\n        pred: vector of integer-coded predictions\n        conf: vector of probability or confidence scores for pred\n        true: vector of integer-coded labels for ground truth\n        return_x: also return the data frame used in the calculation\n\n    Returns:\n        GAP score\n    (https://www.kaggle.com/davidthaler/gap-metric)\n    '''\n    x = pd.DataFrame({'pred': pred, 'conf': conf, 'true': true})\n    x.sort_values('conf', ascending=False, inplace=True, na_position='last')\n    x['correct'] = (x.true == x.pred).astype(int)\n    x['prec_k'] = x.correct.cumsum() / (pl.arange(len(x)) + 1)\n    x['term'] = x.prec_k * x.correct\n    gap = x.term.sum() / x.true.count()\n    return gap\n\n\nNs = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 15000]\nvlSize = 20000\nTtr = train.copy()\nres = pd.DataFrame([])\nrow = 0\nfor r in range(30):\n    vlInd = pl.randint(0, Ttr.shape[0], vlSize)\n    val = Ttr.iloc[vlInd, :]\n    train = Ttr.iloc[pl.setdiff1d(pl.arange(Ttr.shape[0]), vlInd)]\n\n    for ni, N in enumerate(Ns):\n        probs = train.landmark_id.value_counts() / train.shape[0]\n        probs = probs.iloc[:N]\n        probs = pd.DataFrame({'landmark_id': probs.index,\n                              'probability': probs.values}, index=pl.arange(probs.shape[0]))\n        T = pd.merge(train, probs, on='landmark_id', how='inner')\n        inx = pl.randint(0, T.shape[0], val.shape[0])\n        val['landmark_pred'] = T.landmark_id.iloc[inx].values\n        val['prob'] = T.probability.iloc[inx].values\n        GAP1 = GAP_vector(val.landmark_pred, val.prob, val.landmark_id)\n        GAP2 = GAP_vector(val.landmark_pred, [0.1 for i in range(len(val))], val.landmark_id)\n        res = res.append(pd.DataFrame({'n':[N, N], 'run':[r, r], 'GAP':[GAP1, GAP2], 'conf':['prob', 'fixed']}))\n\npl.figure(figsize=(15,7))\nax = sns.boxplot(x='n', y='GAP', hue='conf', data=res)\nax.set_yscale('log')","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"783cb378da1106448c30153e6b9f8db582f8d51d"},"cell_type":"markdown","source":"so, the GAP score goes down by increasing the number of used highly frequent landmarks, in some cases, by using 2 landmarks and having a fixed \"confidence\" value, we get a higher GAP score than even when N is 1. That is because the probabilities in the train set might not match the probabilities in the test set. So, my next output file will be based on that:"},{"metadata":{"trusted":true,"_uuid":"5407cd5abf21c02f849bd73cc063a528dd96d8f9"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\nsub = pd.read_csv('../input/sample_submission.csv')\n\nN = 2\nprobs = train.landmark_id.value_counts() / train.shape[0]\nprobs = probs.iloc[:N]\nprobs = pd.DataFrame({'landmark_id': probs.index,\n                      'probability': 0.1}, index=pl.arange(N))\nT = pd.merge(train, probs, on='landmark_id', how='inner')\ninx = pl.randint(0, T.shape[0], sub.shape[0])\nsub['landmark_id'] = T.landmark_id.iloc[inx].values\nsub['prob'] = T.probability.iloc[inx].values\nsub['landmarks'] = sub.landmark_id.astype(str) + ' ' + sub.prob.astype(str)\nsub[['id','landmarks']].to_csv('submission_5.csv', index=False)\nsub[['id','landmarks']].head()","execution_count":7,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}