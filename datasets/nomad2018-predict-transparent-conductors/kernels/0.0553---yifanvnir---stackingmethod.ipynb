{"cells":[{"metadata":{"_cell_guid":"3be99c66-9385-408f-a8bd-972544c32d13","_uuid":"5b5f9f7ab7aa5b072a236f77413c04b8435a013f","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom subprocess import check_output\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.grid_search import GridSearchCV\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor,BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.linear_model import SGDRegressor,LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection\nfrom catboost import CatBoostRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import make_scorer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nfrom sklearn.decomposition import PCA\nfrom sklearn import model_selection\n\n\n\n\n\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ntest_id=test.id\n\n\nvector = np.vstack((train[['lattice_vector_1_ang', 'lattice_vector_2_ang','lattice_vector_3_ang']].values,\n                    test[['lattice_vector_1_ang', 'lattice_vector_2_ang','lattice_vector_3_ang']].values))\n\npca = PCA().fit(vector)\ntrain['vector_pca0'] = pca.transform(train[['lattice_vector_1_ang', 'lattice_vector_2_ang','lattice_vector_3_ang']])[:, 0]\ntest['vector_pca0'] = pca.transform(test[['lattice_vector_1_ang', 'lattice_vector_2_ang','lattice_vector_3_ang']])[:, 0]\n\n\nX = train.drop(['id','bandgap_energy_ev','formation_energy_ev_natom'], axis=1)\nY_feen = train['formation_energy_ev_natom']\nY_bee = train['bandgap_energy_ev']\n\ntest = test.drop(['id'], axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e53412b82c73a1070961ac9c11802507fa02af2"},"cell_type":"code","source":"def rmsle(h, y): \n    \"\"\"\n    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n\n    Args:\n        h - numpy array containing predictions with shape (n_samples, n_targets)\n        y - numpy array containing targets with shape (n_samples, n_targets)\n    \"\"\"\n    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())\n\nrmsleS=make_scorer(rmsle)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"d735d02760d5d1e3f5b63b2e5e5c1ae3cfcba808"},"cell_type":"code","source":"class Ensemble(object):\n    def __init__(self, n_splits, stacker, base_models):\n        self.n_splits = n_splits\n        self.stacker = stacker\n        self.base_models = base_models\n\n    def fit_predict(self, X, y, T):\n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n\n        folds = list(KFold(n_splits=self.n_splits, shuffle=True).split(X, y))\n\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n        for i, clf in enumerate(self.base_models):\n\n            S_test_i = np.zeros((T.shape[0], self.n_splits))\n            print('********:',i)\n            for j, (train_idx, test_idx) in enumerate(folds):\n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                X_holdout = X[test_idx]\n                y_holdout = y[test_idx]\n                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n                clf.fit(X_train, y_train)\n                # cross_score = cross_val_score(clf, X_train, y_train, cv=5, scoring=rmsleS)\n                # print(\"    cross_score: %.5f\" % (cross_score.mean()))\n                y_pred = clf.predict(X_holdout)               \n                print('errorrmlse:',rmsle(y_pred,y_holdout))\n                S_train[test_idx, i] = y_pred\n                S_test_i[:, j] = clf.predict(T)\n            S_test[:, i] = S_test_i.mean(axis=1)\n        #print(S_train)\n        results = cross_val_score(self.stacker, S_train, y, cv=5, scoring=rmsleS)\n        print(\"Stacker score: %.5f\" % (results.mean()))\n\n        self.stacker.fit(S_train, y)\n        res = self.stacker.predict(S_test)\n        return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de598260e5c8ba17248040e837874ddca50bef1c"},"cell_type":"code","source":"# LightGBM params\nlgb_params = {\n    'num_leaves': 8,\n    'objective': 'regression',\n    'min_data_in_leaf': 18,\n    'learning_rate': 0.04,\n    'n_estimators':500,\n    'feature_fraction': 0.7,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n    'min_child_weight':35,\n    'metric': 'l2',\n    'max_depth':8,\n    'reg_alpha':0.1,\n    'reg_lambda':1,\n    'seed':1000,\n}\n\n\n# RandomForest params\nrf_params = {\n    'max_features':'auto',\n    'n_estimators':200,\n    'n_jobs':1,\n    'max_depth':12,\n}\n\n# rf_params['n_estimators'] = 200\n# rf_params['max_depth'] = 6\n# rf_params['min_samples_split'] = 70\n# rf_params['min_samples_leaf'] = 30\n\n\n# ExtraTrees params\net_params = {}\n# et_params['n_estimators'] = 155\n# et_params['max_features'] = 0.3\n# et_params['max_depth'] = 6\n# et_params['min_samples_split'] = 40\n# et_params['min_samples_leaf'] = 18\n\n\n# XGBoost params\nxgb_params = {\n    'silent':1,\n    'learning_rate':0.01,\n    'n_estimators':1000,\n    'max_depth':4,\n    'min_child_weight':6,\n    'gamma':0,\n    'subsample':0.7,\n    'colsample_bylevel':1,\n    'scale_pos_weight':1,\n    'seed':27,\n    'n_jobs':1,\n    'reg_alpha':0.002,\n}\n\n\n\n\n# CatBoost params\ncat_params = {\n    'verbose':False,\n    'iterations':1000\n}\n\n#cat_params['iterations'] = 900\n# cat_params['depth'] = 8\n# cat_params['rsm'] = 0.95\n# cat_params['learning_rate'] = 0.03\n# cat_params['l2_leaf_reg'] = 3.5\n# cat_params['border_count'] = 8\n\n\n\n#BaggingRegressor\nbag_params={}\n\n\n\n\n# Regularized Greedy Forest params\nrgf_params = {}\n# rgf_params['max_leaf'] = 2000\n# rgf_params['learning_rate'] = 0.5\n# rgf_params['algorithm'] = \"RGF_Sib\"\n# rgf_params['test_interval'] = 100\n# rgf_params['min_samples_leaf'] = 3\n# rgf_params['reg_depth'] = 1.0\n# rgf_params['l2'] = 0.5\n# rgf_params['sl2'] = 0.005\nlgb_model = LGBMRegressor(**lgb_params)\n\nrf_model = RandomForestRegressor(**rf_params)\n\net_model = ExtraTreesRegressor(**et_params)\n\nxgb_model = XGBRegressor(**xgb_params)\n\n\n\ncat_model = CatBoostRegressor(**cat_params)\n\n#rgf_model = RGFRegressor(**rgf_params)\n\ngb_model = GradientBoostingRegressor(max_depth=5)\n\n\nknn = KNeighborsRegressor()\nada_model = AdaBoostRegressor()\nbag = BaggingRegressor()\n\nln = LinearRegression()\n\nstack = Ensemble(n_splits=5,\n        stacker = ln,\n        base_models = (knn,gb_model,bag,cat_model,xgb_model,rf_model))\n\ny_pred1 = stack.fit_predict(X, Y_feen, test)\n\n\n######################################################################\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74ed6b1ed9f887d8ca21ca796403123fc0212f80"},"cell_type":"code","source":"# LightGBM params\nlgb2_params = {\n        #'num_leaves': 10,\n        'objective': 'regression',\n        'min_data_in_leaf': 18,\n        'learning_rate': 0.04,\n        'n_estimators':100,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 1,\n        'min_child_weight':4,\n        'metric': 'l2',\n        'max_depth':8,\n        'reg_alpha':0.1,\n        'reg_lambda':0.1,\n        'seed':1000,\n}\n\n# RandomForest params\nrf2_params = {\n    'max_features':'auto',\n    'n_estimators':200,\n    'n_jobs':1,\n    'max_depth':12,\n}\n\n\n# ExtraTrees params\net2_params = {}\n# et_params['n_estimators'] = 155\n# et_params['max_features'] = 0.3\n# et_params['max_depth'] = 6\n# et_params['min_samples_split'] = 40\n# et_params['min_samples_leaf'] = 18\n\n\n# XGBoost params\nxgb2_params = {\n    'silent':1,\n    'learning_rate':0.1,\n    'n_estimators':100,\n    'max_depth':4,\n    'min_child_weight':6,\n    'gamma':0,\n    'subsample':0.7,\n    'colsample_bylevel':1,\n    'scale_pos_weight':1,\n    'seed':27,\n    'n_jobs':1,\n    'reg_alpha':0.002,\n}\n\n\n\n\n\n# CatBoost params\ncat2_params = {\n    'verbose':False,\n    'iterations':1000\n}\n\n\n\n# BaggingRegressor\nbag2_params = {}\n\n# Regularized Greedy Forest params\nrgf2_params = {}\n# rgf_params['max_leaf'] = 2000\n# rgf_params['learning_rate'] = 0.5\n# rgf_params['algorithm'] = \"RGF_Sib\"\n# rgf_params['test_interval'] = 100\n# rgf_params['min_samples_leaf'] = 3\n# rgf_params['reg_depth'] = 1.0\n# rgf_params['l2'] = 0.5\n# rgf_params['sl2'] = 0.005\nlgb2_model = LGBMRegressor(**lgb2_params)\n\nrf2_model = RandomForestRegressor(**rf2_params)\n\net2_model = ExtraTreesRegressor(**et2_params)\n\nxgb2_model = XGBRegressor(**xgb2_params)\n\ncat2_model = CatBoostRegressor(**cat2_params)\n\n# rgf_model = RGFRegressor(**rgf_params)\n\ngb2_model = GradientBoostingRegressor()\n\n\nknn2 = KNeighborsRegressor()\nada2_model = AdaBoostRegressor()\nbag2 = BaggingRegressor()\n\nln2 = LinearRegression()\n\nstack2 = Ensemble(n_splits=5,\n        stacker = ln2,\n        base_models = (knn2,gb2_model,bag2,rf2_model,cat2_model,xgb2_model))\n\ny_pred2 = stack2.fit_predict(X, Y_bee, test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"dac0145cd262cb96bb114bf10eb3794bde90e28b"},"cell_type":"code","source":"sample = pd.read_csv(\"../input/sample_submission.csv\")\n\nsample[\"formation_energy_ev_natom\"] = y_pred1\nsample[\"bandgap_energy_ev\"] = y_pred2\n\nsample.to_csv(\"subtest.csv\",index = False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5ec54f7ceae793335ba3be084d322d0d5cf5a509"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}