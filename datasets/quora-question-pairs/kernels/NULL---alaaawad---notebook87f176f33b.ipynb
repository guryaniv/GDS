{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cef907a7-4698-47c8-fe5a-c8392dc30fe1"
      },
      "source": [
        "I plan to use Word2Vec to convert each question into a word vector. Then I will use a Siamese neural network to detect if the pair is duplicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e94f9c4-1d65-c360-0dac-36c835df7444"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "pal = sns.color_palette()\n",
        "\n",
        "print('# File sizes')\n",
        "for f in os.listdir('../input'):\n",
        "    if 'zip' not in f:\n",
        "        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3d42e5d7-3234-3f1c-f3dc-8f7b24360017"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('../input/train.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0d83befc-fc2c-01b1-3126-82a8759e4cec"
      },
      "source": [
        "We are given a minimal number of data fields here, consisting of:\n",
        "\n",
        "* **id:** Looks like a simple rowID\n",
        "* **qid{1, 2}:** The unique ID of each question in the pair\n",
        "* **question{1, 2}:** The actual textual contents of the questions.\n",
        "* **is_duplicate:** The label that we are trying to predict - whether the two questions are duplicates of each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03bfcee8-ec94-0928-c471-5f2b6fc9915d"
      },
      "outputs": [],
      "source": [
        "print('Total number of question pairs for training: {}'.format(len(df_train)))\n",
        "print('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\n",
        "qids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\n",
        "print('Total number of questions in the training data: {}'.format(len(\n",
        "    np.unique(qids))))\n",
        "print('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4310c5a5-4faa-ac08-3dd2-e94c074f3bf0"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('../input/test.csv')\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1fa11637-27cf-b4d2-b3f2-a39cc1c6457f"
      },
      "source": [
        "Encode questions to unicode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad0f965c-eb63-93b3-29dd-e42397b702b5"
      },
      "outputs": [],
      "source": [
        "# encode questions to unicode\n",
        "df_train['question1'] = df_train['question1'].apply(lambda x: str(x).encode(\"utf-8\"))\n",
        "df_train['question2'] = df_train['question2'].apply(lambda x: str(x).encode(\"utf-8\"))\n",
        "df_test['question1'] = df_test['question1'].apply(lambda x: str(x).encode(\"utf-8\"))\n",
        "df_test['question2'] = df_test['question2'].apply(lambda x: str(x).encode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e0dd8608-a0c5-80bb-ef3c-e335882cf1d0"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import sys\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "193b9bd9-1e99-4429-ae8c-c4e1057ad089"
      },
      "outputs": [],
      "source": [
        "### Train a GLOVE using Gensim\n",
        "\n",
        "\n",
        "questions = list(df_train['question1']) + list(df_train['question2'])\n",
        "\n",
        "# tokenize\n",
        "c = 0\n",
        "for question in tqdm(questions):\n",
        "    questions[c] = list(gensim.utils.tokenize(question, deacc=True, lower=True))\n",
        "    c += 1\n",
        "\n",
        "# train model\n",
        "model = gensim.models.Word2Vec(questions, size=300, workers=16, iter=10, negative=20)\n",
        "\n",
        "# trim memory\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# creta a dict \n",
        "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
        "print(\"Number of tokens in Word2Vec:\", len(w2v.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0b469433-a2e7-6c5b-171d-7967621b319c"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "model.save('3_word2vec.mdl')\n",
        "model.wv.save_word2vec_format('3_word2vec.bin', binary=True)\n",
        "model = gensim.models.Word2Vec.load('3_word2vec.mdl')  # you can continue training with the loaded model!\n",
        "model.wv['computer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dc831c21-150b-5dc5-c74c-ceba3eabbd9e"
      },
      "outputs": [],
      "source": [
        "def make_question_vectors(model, sentence): \n",
        "    # return numpy document vector by averaging constituent word vectors\n",
        "    # sentence is a list of words in same style as iterator makes for entering into word2vec\n",
        "    word_vecs = []\n",
        "    for word in sentence: \n",
        "        try: \n",
        "            new_word = model[word]\n",
        "        except KeyError:\n",
        "            continue\n",
        "        # check whether array has nan before appending\n",
        "        if not np.isnan(np.sum(new_word)):\n",
        "            word_vecs.append(new_word)\n",
        "    # if no appropriate word vectors found, return array of zeros\n",
        "    if not word_vecs:\n",
        "        return np.zeros(model.layer1_size)\n",
        "    word_vecs = np.array(word_vecs)\n",
        "    return word_vecs.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6399fac-4870-8195-f8e1-f17b772def74"
      },
      "outputs": [],
      "source": [
        "vec1 = df_train['question1'].apply(lambda x: make_question_vectors(model,x.split()))\n",
        "vec2 = df_train['question2'].apply(lambda x: make_question_vectors(model,x.split()))\n",
        "df_train['q1_feats'] = list(vec1)\n",
        "df_train['q2_feats'] = list(vec2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "601362cf-28a3-6004-ddd5-c343314a98b3"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6a7bfdd5-4420-872b-44f9-e008cc81721f"
      },
      "source": [
        "TIME TO TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bdde5cd3-128e-fb96-8638-859a8d4767f6"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Lambda, merge, BatchNormalization, Activation, Input, Merge\n",
        "from keras import backend as K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6abca1ce-434f-9218-79dd-86da2f7d7f5a"
      },
      "outputs": [],
      "source": [
        "q1_branch = Sequential()\n",
        "q1_branch.add(Dense(1000, input_shape=(300,), activation='relu'))\n",
        "q1_branch.add(Dropout(0.2))\n",
        "\n",
        "q2_branch = Sequential()\n",
        "q2_branch.add(Dense(1000, input_shape=(300,), activation='relu'))\n",
        "q2_branch.add(Dropout(0.2))\n",
        "\n",
        "merged = Merge([q1_branch, q2_branch], mode='concat')\n",
        "\n",
        "final_model = Sequential()\n",
        "final_model.add(merged)\n",
        "final_model.add(Dense(500, activation='relu'))\n",
        "final_model.add(Dropout(0.2))\n",
        "final_model.add(Dense(500, activation='relu'))\n",
        "final_model.add(Dropout(0.2))\n",
        "final_model.add(Dense(2, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69ace45b-d818-91a3-3b5b-c19399d091e0"
      },
      "outputs": [],
      "source": [
        "# compile model - accuracy will be metrix we optimize for\n",
        "final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7a4e49e-9075-75df-8b32-00564fd92448"
      },
      "outputs": [],
      "source": [
        "# shuffle df_train\n",
        "df_train = df_train.reindex(np.random.permutation(df_train.index))\n",
        "\n",
        "# set number of train and test instances\n",
        "num_train = int(df_train.shape[0] * 0.88)\n",
        "num_test = df_train.shape[0] - num_train                 \n",
        "print(\"Number of training pairs: %i\"%(num_train))\n",
        "print(\"Number of testing pairs: %i\"%(num_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02348000-1f9b-6ea7-292e-f68aadf4afa9"
      },
      "outputs": [],
      "source": [
        "# init data data arrays\n",
        "X_train = np.zeros([num_train, 2, 300])\n",
        "X_test  = np.zeros([num_test, 2, 300])\n",
        "Y_train = np.zeros([num_train]) \n",
        "Y_test = np.zeros([num_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "688b22c0-a9fd-b366-26f7-69bf9be9710c"
      },
      "outputs": [],
      "source": [
        "# format data \n",
        "b = [a[None,:] for a in list(df_train['q1_feats'].values)]\n",
        "q1_feats = np.concatenate(b, axis=0)\n",
        "\n",
        "b = [a[None,:] for a in list(df_train['q2_feats'].values)]\n",
        "q2_feats = np.concatenate(b, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a1ded9c5-38ef-1923-0310-21dd80477860"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "af441e6c-9182-9754-d6c0-fa2ddd34b93f"
      },
      "outputs": [],
      "source": [
        "null"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}