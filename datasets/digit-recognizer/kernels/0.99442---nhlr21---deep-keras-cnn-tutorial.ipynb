{"cells":[{"metadata":{"_uuid":"1f7d6d20a77d913be56453413013675fed76d33d"},"cell_type":"markdown","source":"# **Deep keras CNN as digit-recognizer : complete tutorial**"},{"metadata":{"_uuid":"1c01ab59e5476149b450df906c2e6717cf0d74bf"},"cell_type":"markdown","source":"### Welcome on this tutorial ! In this notebook, we'll see how to build a deep convolutionnal neural network. We'll then train this CNN on the MNIST dataset to turn it into an incredibly effective digit-recognizer with a near 100% accuracy !"},{"metadata":{"_uuid":"c89f7addd41c9b1601bdc5c7b7f8d7b6c12606aa"},"cell_type":"markdown","source":"![](http://petr-marek.com/wp-content/uploads/2017/07/mnist.png)"},{"metadata":{"_uuid":"fd783336a1fc6ed0525649afaa6b25303c6d18a3"},"cell_type":"markdown","source":"### We are going to predict which number is drawn on different images. By doing this, we will go through several topics and fundamental techniques of deep learning. Here is a list of these techniques and some additional resources that you can consult to find out more:¶"},{"metadata":{"_uuid":"22f957e4d196be9b9d37d6bfb76287921246102a"},"cell_type":"markdown","source":"[*Overfitting*](https://elitedatascience.com/overfitting-in-machine-learning)   \n[*Artificial neural networks*](https://www.superdatascience.com/blogs/the-ultimate-guide-to-artificial-neural-networks-ann)  \n[*Convolutionnal neural networks*](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)  \n[*Activation functions in neural networks*](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)  \n[*Deep Learning with Python*](https://www.manning.com/books/deep-learning-with-python)"},{"metadata":{"_uuid":"0f491e8f37d604eaaff3cc50bd046f5a96a4fde2"},"cell_type":"markdown","source":"## **Table of Contents**"},{"metadata":{"_uuid":"f4127e902030b2b8fe329bc40b762d73fb287710"},"cell_type":"markdown","source":"1. [**Data exploration & preparation**](#data_exploration)\n2. [**CNN Theory**](#theory)\n3. [**Building model with Keras**](#model)\n4. [**Make prediction**](#prediction)"},{"metadata":{"_uuid":"4b0dc576d9b658e121083b4120e00134841475cd"},"cell_type":"markdown","source":"## **Imports and useful functions**"},{"metadata":{"_uuid":"b3a3347c489f16262b8bc58ab1cc27afb4c92650","trusted":false},"cell_type":"code","source":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D\nfrom keras.utils import np_utils","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a899970b187422f23e79c6991b15d52b9ff0634","trusted":false},"cell_type":"code","source":"#path of datasets\npath_train = '../input/train.csv'\npath_test = '../input/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81529b4e3913323b171b65222c0468dadfd5e03d"},"cell_type":"markdown","source":"## **1. Data exploration** <a id=\"data_exploration\"></a>"},{"metadata":{"_uuid":"24bfb229f61081d0b44a11cbfa9fd2867da01ede"},"cell_type":"markdown","source":"### Let's begin by printing a preview of the dataset and look at its size and what it contains:"},{"metadata":{"_uuid":"db7dc2eee58f446f31b35112689ed5e7bafb50de","trusted":false},"cell_type":"code","source":"#create dataframe for training dataset and print 5 first rows as preview\ntrain_df_raw = pd.read_csv(path_train)\ntrain_df_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b7e355fea95fc45233a4f949f091a431978be16","trusted":false},"cell_type":"code","source":"# print infos about the dataset\ntrain_df_raw.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1cbad3408546cc85b88943d1197842a703c5259","trusted":false},"cell_type":"code","source":"train_df_raw.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25b36332b7d352e6706dad130788a138f45d35d8","trusted":false},"cell_type":"code","source":"# Check if there are missing datas\ntrain_df_raw.isnull().values.any()\ntrain_df_raw.isna().values.any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"055c3fd5becd1405f5c2c92c4f9b9af0df9eaec2"},"cell_type":"markdown","source":"### From this first preview and according to information from kaggle, it seems that each image contains 784 pixels (28x28 pixels), each pixel contain a value between 0 and 254 reprensenting the gray level of this pixel. There is no missing values. Let's try to display the first images from the values of the dataset:"},{"metadata":{"_uuid":"e0c515cfa08404bf6a95aec07204b2b69734915c","trusted":false},"cell_type":"code","source":"def dislay_images_from_pixels(nb): \n    images = [np.array(train_df_raw.drop(['label'], 1).iloc[i].tolist()).reshape(28, 28) for i in range (nb)]\n    rows = int(nb/15) if int(nb/15) != 0 else int(nb/15) + 1\n    plt.figure(figsize=(16, rows))\n    for n in range(1, nb + 1):\n        plt.subplot(rows, 15, n)\n        plt.imshow(images[n-1])\n        plt.axis('off')\n        \ndislay_images_from_pixels(120)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"890dbf882351364d584c7dfecec51c506e4f5e04"},"cell_type":"markdown","source":"### Finally, let's check at the occurence of each class, to be sure that there is no asymmetry in our data that can skew the algorithm :"},{"metadata":{"_uuid":"8868d9313ddba7eaad19d52e5c19ecde8d0f96d2","trusted":false},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(20, 8))\nsns.distplot(train_df_raw.label)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d16e922e796d1e0685652516dd7b9d62ada8952"},"cell_type":"markdown","source":"### Nice, the distribution of the labels seems to be sufficiently uniform to solve properly the multiclass classification problem. \n### Now, we have to prepare our data in order to pass it to the CNN. This preparation involves reshaping and normalize our observations since the CNN needs a 4D tensor as input. Normalizing the data is necessary when working with neural networks because feeding a NN with large or heterogeneous values can trigger large gradient updates that will prevent the network from converging. Finally, we need to hot-one encode the target, to make it match with the shape of the output of our CNN, which is a 10 length vector for a 10-class classification problem."},{"metadata":{"_uuid":"53553cc1ac7662143b7ef9cc627f558277ee1785","trusted":false},"cell_type":"code","source":"# Prepare data in order to pass it to a CNN\ntrain_df = train_df_raw.copy()\n\n# Separate target from images\nX_train = train_df.drop(['label'], 1)\nY_train = train_df['label']\n\n# Add 2 dimensions to pass a 4D tensor to the CNN and normalize values\nX_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255 \n# One-hot encoding of target\nY_train = np_utils.to_categorical(Y_train, 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f055779a0de50d1e3e36a4259368e586470f479"},"cell_type":"markdown","source":"## **2. CNN Theory** <a id=\"theory\"></a>"},{"metadata":{"_uuid":"cfa5973d2a3cec676d17fb0600b66cc807a21fb1"},"cell_type":"markdown","source":"### We are going to build a convolutionnal neural network. A CNN is a neural network componed of two parts : features learning and classification. The first part consists of basically applying several operations on images before passing the result to a standard ANN performing the second part. These operations contained in the features learning are convolution, application of an activation function, pooling (which we'll not use in this architecture), and flattening. Convolution + activation function and pooling may be repeated several times to build deeper networks and improve performances.\n### Convolution is used to detect features in the images to make it more recognizable, it creates features maps containing the important features of the images. The activation function will then be applied on the features maps to break linearity of the images and make features even more recognizable. Pooling is used to downsize the result and make cnn faster. Flattening will transform the feature maps matrix from the convolution operation into a 1D vector to pass it to a standard ANN.\n### Here is a standard architecture of a CNN, we can see that if is just a succession of convolution + activation function operations (here the function used is a relu, i.e. rectifier function) before an artificial neural network:"},{"metadata":{"_uuid":"c93c8ac2e2e928a26abfd658b6dddbefe6081229"},"cell_type":"markdown","source":"![](https://cdn-images-1.medium.com/max/2400/1*vkQ0hXDaQv57sALXAJquxA.jpeg)"},{"metadata":{"_uuid":"f2f486bcf701805179b18f9c7f1e45cce64abeb8"},"cell_type":"markdown","source":"### Here is a convolutionnal neural network detailled when applied to the digit recognition task. Our model will be deeper (with more convolutionnal layers) and will not contain pooling steps but will basically be built on the same architecture :"},{"metadata":{"_uuid":"8c0ac8bf2bb48468b49b500daf84e81391801282"},"cell_type":"markdown","source":"![](https://cdn-images-1.medium.com/max/1600/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)"},{"metadata":{"_uuid":"13e9b551894e70f02f06431e2ebf8d0990a5221e"},"cell_type":"markdown","source":"## **3. Building model with Keras** <a id=\"model\"></a>"},{"metadata":{"_uuid":"808a22296daaf67f3ace9a979cb9cfc65d287f99"},"cell_type":"markdown","source":"### Now, let's implement our model ! We'll use Keras, a framefork serving as high level API for Tensorflow, which is a tensor-manipulation framework made by google. Keras allows you to build neural networks by assembling blocks (which are the layers of your neural network). For more details, [here](https://elitedatascience.com/keras-tutorial-deep-learning-in-python) is a great keras tutorial."},{"metadata":{"_uuid":"af753db7e8dd720411adb52950a44166bc562c45"},"cell_type":"markdown","source":"![](https://cdn.actuia.com/wp-content/uploads/2018/05/keras.png)"},{"metadata":{"_uuid":"0cd8c84832b40f8bbc23cf3ddcfc67cfddf5a4a5","trusted":false},"cell_type":"code","source":"def build_cnn():\n    \n    model = Sequential()\n    \n    # Multiple convolution operations to detect features in the images\n    model.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32,kernel_size=3,activation='relu')) # no need to specify shape as there is a layer before\n    model.add(BatchNormalization())\n    model.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4)) # reduce overfitting\n\n    model.add(Conv2D(64,kernel_size=3,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64,kernel_size=3,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4)) # reduce overfitting\n    \n    # Flattening and classification by standard ANN\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    model.add(Dense(10, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ea6b3a8be6e8d4d668019d8f77adb80fbe54fee"},"cell_type":"markdown","source":"### Now, let's explain the above keras model and try to link it to the representation of a convolutionnal neural network studied before. In the convolutionnal neural network architecture implemented above, we can observe several types of blocks :\n\n- Convolutionnal layers :\n        \n      model.add(Conv2D(32,kernel_size=3,activation='relu'))\n        \n### These layers **only** performs the two first steps seen above (convolution and applying activation function). It first applies convolution on the images to detect features and create features maps. Then, it applies a relu activation function on the result of the convolution to supress linearity in the image to improve features detection. After this step (repeated several times to improve features detection), we are technically ready for passing the result to flattening and to the simple artificial neural network !  \n        \n- Batch normalization  :\n\n      model.add(BatchNormalization())\n        \n### The batch normalisation is here to normalize the data after it goes out the convolutionnal layer, helping gradient propagation and allowing to build deeper models by stacking convolutional layers.\n        \n- Dropout\n\n      model.add(Dropout(0.4))\n        \n### Dropout disables a fraction of all neurons of the previous densely connected or convolutionnal layer to reduce overfitting.\n        \n- Flattening\n \n      model.add(Flatten())\n         \n### This just proceed the flattening operation we've seen before, in order to pass the features maps resulting of convolution operation to the standard artificial neural network.\n\n![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png)\n         \n- Fully connected layer\n\n      model.add(Dense(128, activation='relu'))\n\n### This layer (called fully connected layer) is the main component of the ANN which will classify each digit to it's belonging class (i.e. the digit recognized). The softmax activation function is used in the output layer to convert output from 10 output nodes in probabilities of each digit to belong in each class."},{"metadata":{"_uuid":"b1e4acdbf88de3f8209d172cbd100994e9693f48"},"cell_type":"markdown","source":"### Do not hesitate to print a summary of the model to check the total number of parameter which gives you a overview of the depth and the complexity of your model (see below). We can clearly see here that the first convolutionnal layer, for exemple, creates 32 features maps (each one corresponding to one particular feature of the image) of size 26x26 from a single 28x28 image. Moreover, we can also observe that flattening transforms a 3D tensor into a 1D tensor (or 4D into 2D):"},{"metadata":{"_uuid":"3d11a5c53216994d0843fbda758173fac1fce28e","trusted":false},"cell_type":"code","source":"build_cnn().summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dc561734cbf5c4f4757a0c7b8b8fbbcb818a3c5"},"cell_type":"markdown","source":"### These explanations are absolutely not complete at all and there is so much to say about neural networks that it is impossible to cover it in a notebook. To understand better these architecture and dive deeply (that's the case to say !)  in this exciting field, I highly recommend you to read the great book [**Deep Learning with Python**](https://www.manning.com/books/deep-learning-with-python) written by François Chollet, the creator of the keras framework. An other great ressource is accessible at the beginning of the notebook (convolutionnal neural networks) and may allows you to really understand how the features learning part works in a detailled way.\n\n### Finally, I encourage you to check [**this link**](http://scs.ryerson.ca/~aharley/vis/conv/flat.html) to vizualise all these operations in practice, applied to the MNIST dataset ! Just draw the number in the square on the left of your screen and look at the result and the prediction made by the CNN. You can observe what the features maps looks like after the convolutions operation, and what it looks like after the pooling and flattening."},{"metadata":{"_uuid":"75c77d72962949560ddf7e0a3ae70359102294ed"},"cell_type":"markdown","source":"## **4. Make prediction** <a id=\"prediction\"></a>"},{"metadata":{"_uuid":"416812634bc2209e545bdeb11e1ed21da82ae111","trusted":false},"cell_type":"code","source":"X_test = pd.read_csv(path_test)\nX_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_uuid":"03fc126ca2452bc0de304f5b8d805973ec4c49fd","trusted":false},"cell_type":"code","source":"model = build_cnn()\nmodel.fit(X_train, Y_train, batch_size=64, epochs=16)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cb9dcab1cbd60208a00d2b606e284fe44725e72","trusted":false},"cell_type":"code","source":"prediction = model.predict_classes(X_test, verbose=0)\nsubmission = pd.DataFrame({\"ImageId\": list(range(1,len(prediction)+1)),\n                         \"Label\": prediction})\nsubmission.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}