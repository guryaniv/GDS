{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"def en_model (X,y):\n\n    # get the packages \n    import numpy as np\n    import pandas as pd\n\n    \n    # splitting in train and test \n    from sklearn.model_selection import train_test_split\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\n\n# import for linear regression\n\n    from sklearn.linear_model import LinearRegression\n    import matplotlib.pyplot as plt\n    model = LinearRegression()\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n        \n    import scipy.stats as stats\n    import sklearn\n    from sklearn.metrics import r2_score\n    from math import sqrt\n    mse = sklearn.metrics.mean_squared_error(y_test, y_pred)\n    rmse=sqrt(mse)\n    Rsquare=r2_score(y_test, y_pred)\n  \n\n\n\n# import decision tree and predict \n    from sklearn.tree import DecisionTreeRegressor\n    tree_regressor = DecisionTreeRegressor(criterion=\"mse\",max_features=\"auto\")\n\n    tree_regressor.fit(x_train,y_train)\n    y_tree_pred = tree_regressor.predict(x_test)\n    mse_tree = sklearn.metrics.mean_squared_error(y_test, y_tree_pred)\n    rmse_tree=sqrt(mse_tree)\n    Rsquare_tree=r2_score(y_test, y_tree_pred)\n    \n    \n        \n    # variable importance : \n        \n\n\n\n# Random Forest \n    from sklearn.ensemble import RandomForestRegressor\n\n    rf = RandomForestRegressor(n_estimators=500, oob_score=True, random_state=0)\n    rf.fit(x_train, y_train)\n    \n    y_rf_pred = rf.predict(x_test)\n    mse_rf = sklearn.metrics.mean_squared_error(y_test, y_rf_pred)\n    rmse_rf=sqrt(mse_rf)\n    Rsquare_rf=r2_score(y_test, y_rf_pred)\n\n\n\n\n\n# GBM\n    from sklearn.ensemble import GradientBoostingRegressor\n\n    params = {'n_estimators': 500, 'max_depth': 6,\n        'learning_rate': 0.1, 'loss': 'huber','alpha':0.95}\n    gbm = GradientBoostingRegressor(**params).fit(x_train, y_train)    \n\n\n    y_gbm_pred = gbm.predict(x_test)\n    mse_gbm = sklearn.metrics.mean_squared_error(y_test, y_gbm_pred)\n    rmse_gbm=sqrt(mse_gbm)\n    Rsquare_gbm=r2_score(y_test, y_gbm_pred)\n    \n    \n    \n    \n# XGB\n        # Let's try XGboost algorithm to see if we can get better results\n    import xgboost \n    \n    xgb = xgboost.XGBRegressor(n_estimators=500, learning_rate=0.08, gamma=0, subsample=0.75,\n                               colsample_bytree=1, max_depth=7)\n    model=xgb.fit(x_train,y_train)    \n    \n    y_xgb_pred = xgb.predict(x_test)\n    mse_xgb = sklearn.metrics.mean_squared_error(y_test, y_xgb_pred)\n    rmse_xgb=sqrt(mse_xgb)\n    Rsquare_xgb=r2_score(y_test, y_xgb_pred)\n        \n\n    # getting variable importance plots \n\n\n    imp_df=pd.DataFrame(columns=['Variable','Relative_importance'])\n    imp_df['Variable']=x_train.columns     \n    imp_df['Relative_importance']=tree_regressor.feature_importances_\n    imp_df=imp_df.sort_values(['Relative_importance'], ascending=[False])\n    imp_df=imp_df.iloc[:5]\n          \n          \n    imp_df_rf=pd.DataFrame(columns=['Variable','Relative_importance'])\n    imp_df_rf['Variable']=x_train.columns     \n    imp_df_rf['Relative_importance']=rf.feature_importances_\n    imp_df_rf=imp_df_rf.sort_values(['Relative_importance'], ascending=[False])      \n    imp_df_rf=imp_df_rf.iloc[:5]\n          \n    \n    imp_df_gbm=pd.DataFrame(columns=['Variable','Relative_importance'])\n    imp_df_gbm['Variable']=x_train.columns     \n    imp_df_gbm['Relative_importance']=gbm.feature_importances_\n    imp_df_gbm=imp_df_gbm.sort_values(['Relative_importance'], ascending=[False])      \n    imp_df_gbm=imp_df_gbm.iloc[:5]\n \n    imp_df=imp_df.sort_values(['Relative_importance'],ascending=[True])\n    imp_df_rf=imp_df_rf.sort_values(['Relative_importance'],ascending=[True])\n    imp_df_gbm=imp_df_gbm.sort_values(['Relative_importance'],ascending=[True])\n    \n    \n    # plot all variable importances                    \n                       \n    import matplotlib.pyplot as plt\n    \n    %matplotlib inline\n    # Set the style\n    plt.style.use('fast')    \n    fig, axes = plt.subplots(nrows=2, ncols=2)\n    imp_df.plot(x='Variable',y='Relative_importance',kind='barh',legend=False,title='DecisionTree',ax=axes[0,0])\n    imp_df_rf.plot(x='Variable',y='Relative_importance',kind='barh',legend=False,title='RandomForest',ax=axes[0,1])\n    imp_df_gbm.plot(x='Variable',y='Relative_importance',kind='barh',legend=False,title='GBM',ax=axes[1,0])\n    fig.tight_layout()\n \n    \n    \n    en_pred=(y_rf_pred+y_tree_pred+y_gbm_pred+y_xgb_pred)/4\n    mse_ens = sklearn.metrics.mean_squared_error(y_test, en_pred)\n    rmse_ens=sqrt(mse_ens)\n    Rsquare_ens=r2_score(y_test, en_pred)\n    \n    import pandas as pd\n    list=[[\"Linear Regression\",round(Rsquare,2),round(rmse,2),round(mse,2)]]\n    list.append([\"Decision Tree\",round(Rsquare_tree,2),round(rmse_tree,2),round(mse_tree,2)])\n    list.append([\"Random Forest\",round(Rsquare_rf,2),round(rmse_rf,2),round(mse_rf,2)])\n    list.append([\"GBM\",round(Rsquare_gbm,2),round(rmse_gbm,2),round(mse_gbm,2)])\n    list.append([\"XGB\",round(Rsquare_xgb,2),round(rmse_xgb,2),round(mse_xgb,2)])\n    list.append([\"StackedEnsemble\",round(Rsquare_ens,2),round(rmse_ens,2),round(mse_ens,2)])\n\n    \n    df=pd.DataFrame(list,columns=['Model','RSQUARE','RMSE','MSE'])\n    print(df)\n    \n    \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"11099b2292f5e4dd9ea9674368b9a3c4869274eb"},"cell_type":"code","source":"house = pd.read_csv(\"../input/kc_house_data.csv\")\nhouse.head()\ndel house['id']\ndel house['date']\n\n\n","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a7019f789e96db5ee90dba7005c03188c5a2f903"},"cell_type":"code","source":"X=house.iloc[:,0:19]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"fc71b5818d777ca30b58cb0ee59b608b025db384"},"cell_type":"code","source":"y=X.pop('price')","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffa42b2d0b7e1113a340eb4816e46e3b1b8d1613"},"cell_type":"code","source":"#launch function\nen_model(X,y)","execution_count":26,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}