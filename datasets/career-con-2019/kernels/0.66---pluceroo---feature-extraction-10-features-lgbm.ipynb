{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nimport lightgbm as lgb\nsns.set()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# PATH=\"../input/Santander/\" \nPATH=\"../input/\" \nprint(os.listdir(PATH))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train_df = pd.read_csv(PATH+\"X_train.csv\")\nX_test_df = pd.read_csv(PATH+\"X_test.csv\")\nY_train_df = pd.read_csv(PATH+\"y_train.csv\")\nsub = pd.read_csv(PATH+\"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Credits: \n- https://www.kaggle.com/jsaguiar/surface-recognition-baseline\n- https://www.kaggle.com/gpreda/santander-eda-and-prediction"},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{},"cell_type":"markdown","source":"Let's check the train and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_df.shape, Y_train_df.shape , X_test_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### X_train_df have 487680 rows with 13 columns. Y_train_df have 3810 rows with 3 columns. X_test_df have 488448 rows with 13 columns. \n\nX_train_df and X_test_df contain:\n- series_id\n- measurement_number\n- 10 numerical variables named: orientation_X,\torientation_Y,\torientation_Z,\torientation_W,\tangular_velocity_X,\tangular_velocity_Y,\tangular_velocity_Z,\tlinear_acceleration_X,\tlinear_acceleration_Y and linear_acceleration_Z\n\nY_train_df contain:\n- series_id\t\n- group_id\t\n- surface\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(X_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(X_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(Y_train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing data train and test  datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's check the distribution of surface(target) value in train(Y_train_df) dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.title(\"Target labels\")\nsns.countplot(y='surface', data = Y_train_df, order = Y_train_df['surface'].value_counts().index,  palette=\"Set2\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is unbalanced! "},{"metadata":{},"cell_type":"markdown","source":"# Feature extraction"},{"metadata":{},"cell_type":"markdown","source":"Let's extract 10 features from time series\n\nWe will use these papers  [link 1](https://ieeexplore.ieee.org/document/8181558) [link 2](https://www.sciencedirect.com/science/article/pii/S2405896318323127). We will use the top 10 features to vibration signal. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import kurtosis\nfrom scipy.stats import skew\n\ndef _kurtosis(x):\n    return kurtosis(x)\n\ndef CPT5(x):\n    den = len(x)*np.exp(np.std(x))\n    return sum(np.exp(x))/den\n\ndef skewness(x):\n    return skew(x)\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 3\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))/len(x)\n\ndef zero_crossing(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1\n    return sum(np.heaviside(-xn*xn_i2,0))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_extraction(raw_frame):\n    frame = pd.DataFrame()\n    raw_frame['angular_velocity'] = raw_frame['angular_velocity_X'] + raw_frame['angular_velocity_Y'] + raw_frame['angular_velocity_Z']\n    raw_frame['linear_acceleration'] = raw_frame['linear_acceleration_X'] + raw_frame['linear_acceleration_Y'] + raw_frame['linear_acceleration_Z']\n    raw_frame['velocity_to_acceleration'] = raw_frame['angular_velocity'] / raw_frame['linear_acceleration']\n    \n    for col in raw_frame.columns[3:]:\n        frame[col + '_mean'] = raw_frame.groupby(['series_id'])[col].mean()        \n        frame[col + '_CPT5'] = raw_frame.groupby(['series_id'])[col].apply(CPT5) \n        frame[col + '_SSC'] = raw_frame.groupby(['series_id'])[col].apply(SSC) \n        frame[col + '_skewness'] = raw_frame.groupby(['series_id'])[col].apply(skewness)\n        frame[col + '_wave_lenght'] = raw_frame.groupby(['series_id'])[col].apply(wave_length)\n        frame[col + '_norm_entropy'] = raw_frame.groupby(['series_id'])[col].apply(norm_entropy)\n        frame[col + '_SRAV'] = raw_frame.groupby(['series_id'])[col].apply(SRAV)\n        frame[col + '_kurtosis'] = raw_frame.groupby(['series_id'])[col].apply(_kurtosis) \n        frame[col + '_mean_abs'] = raw_frame.groupby(['series_id'])[col].apply(mean_abs) \n        frame[col + '_zero_crossing'] = raw_frame.groupby(['series_id'])[col].apply(zero_crossing) \n    return frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = feature_extraction(X_train_df)\ntest_df = feature_extraction(X_test_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ntarget = le.fit_transform(Y_train_df['surface'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'num_leaves': 54,\n    'min_data_in_leaf': 40,\n    'objective': 'multiclass',\n    'max_depth': 7,\n    'learning_rate': 0.01,\n    \"boosting\": \"gbdt\",\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.8126672064208567,\n    \"bagging_seed\": 11,\n    \"verbosity\": -1,\n    'reg_alpha': 0.1302650970728192,\n    'reg_lambda': 0.3603427518866501,\n    \"num_class\": 9,\n    'nthread': -1\n}\n\ndef multiclass_accuracy(preds, train_data):\n    labels = train_data.get_label()\n    pred_class = np.argmax(preds.reshape(9, -1).T, axis=1)\n    return 'multi_accuracy', np.mean(labels == pred_class), True\n\nt0 = time.time()\ntrain_set = lgb.Dataset(train_df, label=target)\neval_hist = lgb.cv(params, train_set, nfold=10, num_boost_round=9999,\n                   early_stopping_rounds=100, seed=19, feval=multiclass_accuracy)\nnum_rounds = len(eval_hist['multi_logloss-mean'])\n# retrain the model and make predictions for test set\nclf = lgb.train(params, train_set, num_boost_round=num_rounds)\npredictions = clf.predict(test_df, num_iteration=None)\nprint(\"Timer: {:.1f}s\".format(time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following plots show the mean logloss and accuracy at each iteration (blue line). The red lines are the standard deviation between folds."},{"metadata":{"trusted":true},"cell_type":"code","source":"v1, v2 = eval_hist['multi_logloss-mean'][-1], eval_hist['multi_accuracy-mean'][-1]\nprint(\"Validation logloss: {:.4f}, accuracy: {:.4f}\".format(v1, v2))\nplt.figure(figsize=(10, 4))\nplt.title(\"CV multiclass logloss\")\nnum_rounds = len(eval_hist['multi_logloss-mean'])\nax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_logloss-mean'])\nax2 = ax.twinx()\np = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_logloss-stdv'], ax=ax2, color='r')\n\nplt.figure(figsize=(10, 4))\nplt.title(\"CV multiclass accuracy\")\nnum_rounds = len(eval_hist['multi_accuracy-mean'])\nax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_accuracy-mean'])\nax2 = ax.twinx()\np = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_accuracy-stdv'], ax=ax2, color='r') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = pd.DataFrame({'gain': clf.feature_importance(importance_type='gain'),\n                           'feature': clf.feature_name()})\nimportance.sort_values(by='gain', ascending=False, inplace=True)\nplt.figure(figsize=(10, 20))\nax = sns.barplot(x='gain', y='feature', data=importance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['surface'] = le.inverse_transform(predictions.argmax(axis=1))\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}