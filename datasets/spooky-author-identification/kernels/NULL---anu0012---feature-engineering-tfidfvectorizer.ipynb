{"cells": [{"source": ["import numpy as np\n", "import os\n", "import pandas as pd\n", "import sys\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.svm import LinearSVC\n", "from sklearn.ensemble import RandomForestClassifier\n", "from nltk.corpus import wordnet as wn\n", "from nltk.corpus import stopwords\n", "from nltk.stem.snowball import SnowballStemmer\n", "from nltk.stem import PorterStemmer\n", "import nltk\n", "from nltk import word_tokenize, ngrams\n", "from nltk.classify import SklearnClassifier\n", "from wordcloud import WordCloud,STOPWORDS\n", "np.random.seed(25)"], "cell_type": "code", "metadata": {"_uuid": "09c13dcbcf37d42b7ddff577ebca12ebe3c5048c", "_cell_guid": "f3fa79a9-3350-46a9-995c-16c7c9792840", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.utils.np_utils import to_categorical\n", "from keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\n", "from keras.layers import Conv1D, MaxPooling1D, Embedding\n", "from keras.models import Model\n", "from keras.layers.wrappers import TimeDistributed, Bidirectional\n", "from keras.layers.normalization import BatchNormalization\n", "from keras import backend as K\n", "import codecs"], "cell_type": "code", "metadata": {"_uuid": "7a18143c2d2c89e2c4a764e9c1f8f700335bb6f9", "_cell_guid": "9e84ca73-0915-4bdf-a02b-a3fcbaaed930", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["train = pd.read_csv(\"../input/hotel-review/train.csv\")\n", "test = pd.read_csv(\"../input/hotel-review/test.csv\")"], "cell_type": "code", "metadata": {"_uuid": "ce6631f5a3daf80925cc8ba32773fbe955de7ebc", "_cell_guid": "92831422-0dc4-4278-bce6-a497c41da0a9", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["# Target Mapping\n", "mapping_target = {'happy':0, 'not happy':1}\n", "train = train.replace({'Is_Response':mapping_target})\n", "\n", "# Browser Mapping\n", "mapping_browser = {'Firefox':0, 'Mozilla':0, 'Mozilla Firefox':0,\n", "                  'Edge': 1, 'Internet Explorer': 1 , 'InternetExplorer': 1, 'IE':1,\n", "                   'Google Chrome':2, 'Chrome':2,\n", "                   'Safari': 3, 'Opera': 4\n", "                  }\n", "train = train.replace({'Browser_Used':mapping_browser})\n", "test = test.replace({'Browser_Used':mapping_browser})\n", "# Device mapping\n", "mapping_device = {'Desktop':0, 'Mobile':1, 'Tablet':2}\n", "train = train.replace({'Device_Used':mapping_device})\n", "test = test.replace({'Device_Used':mapping_device})"], "cell_type": "code", "metadata": {"_uuid": "cc1a3dc4180aaddc41fc75c8048799fc630a6039", "_cell_guid": "9ab34b6b-2731-4884-914c-81e2ed792ea7", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["GLOVE_DIR = '../input/glove-global-vectors-for-word-representation/'\n", "MAX_SEQUENCE_LENGTH = 300\n", "MAX_NB_WORDS = 200000\n", "EMBEDDING_DIM = 200\n", "VALIDATION_SPLIT = 0.01"], "cell_type": "code", "metadata": {"_uuid": "6b545e1090771d7582ede1f73713f898d15de42f", "_cell_guid": "f74d4820-c800-445a-9322-2fda8101c1a0", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["test_id = test['User_ID']\n", "target = train['Is_Response']"], "cell_type": "code", "metadata": {"_uuid": "e01bd7c899e2da37ebcb2651909e97b3d3a27bda", "_cell_guid": "7c079219-1497-4519-9cf1-37451ce0300b", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["# function to clean data\n", "import string\n", "import itertools \n", "import re\n", "from nltk.stem import WordNetLemmatizer\n", "from string import punctuation\n", "\n", "stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n", "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n", "              'Is','If','While','This']\n", "# punct = list(string.punctuation)\n", "# punct.append(\"''\")\n", "# punct.append(\":\")\n", "# punct.append(\"...\")\n", "# punct.append(\"@\")\n", "# punct.append('\"\"')\n", "def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n", "    txt = str(text)\n", "    \n", "    # Replace apostrophes with standard lexicons\n", "    txt = txt.replace(\"isn't\", \"is not\")\n", "    txt = txt.replace(\"aren't\", \"are not\")\n", "    txt = txt.replace(\"ain't\", \"am not\")\n", "    txt = txt.replace(\"won't\", \"will not\")\n", "    txt = txt.replace(\"didn't\", \"did not\")\n", "    txt = txt.replace(\"shan't\", \"shall not\")\n", "    txt = txt.replace(\"haven't\", \"have not\")\n", "    txt = txt.replace(\"hadn't\", \"had not\")\n", "    txt = txt.replace(\"hasn't\", \"has not\")\n", "    txt = txt.replace(\"don't\", \"do not\")\n", "    txt = txt.replace(\"wasn't\", \"was not\")\n", "    txt = txt.replace(\"weren't\", \"were not\")\n", "    txt = txt.replace(\"doesn't\", \"does not\")\n", "    txt = txt.replace(\"'s\", \" is\")\n", "    txt = txt.replace(\"'re\", \" are\")\n", "    txt = txt.replace(\"'m\", \" am\")\n", "    txt = txt.replace(\"'d\", \" would\")\n", "    txt = txt.replace(\"'ll\", \" will\")\n", "    \n", "    # More cleaning\n", "    txt = re.sub(r\"review\", \"\", txt)\n", "    txt = re.sub(r\"Review\", \"\", txt)\n", "    txt = re.sub(r\"TripAdvisor\", \"\", txt)\n", "    txt = re.sub(r\"reviews\", \"\", txt)\n", "    txt = re.sub(r\"Hotel\", \"\", txt)\n", "    txt = re.sub(r\"what's\", \"\", txt)\n", "    txt = re.sub(r\"What's\", \"\", txt)\n", "    txt = re.sub(r\"\\'s\", \" \", txt)\n", "    txt = txt.replace(\"pic\", \"picture\")\n", "    txt = re.sub(r\"\\'ve\", \" have \", txt)\n", "    txt = re.sub(r\"can't\", \"cannot \", txt)\n", "    txt = re.sub(r\"n't\", \" not \", txt)\n", "    txt = re.sub(r\"I'm\", \"I am\", txt)\n", "    txt = re.sub(r\" m \", \" am \", txt)\n", "    txt = re.sub(r\"\\'re\", \" are \", txt)\n", "    txt = re.sub(r\"\\'d\", \" would \", txt)\n", "    txt = re.sub(r\"\\'ll\", \" will \", txt)\n", "    txt = re.sub(r\"60k\", \" 60000 \", txt)\n", "    txt = re.sub(r\" e g \", \" eg \", txt)\n", "    txt = re.sub(r\" b g \", \" bg \", txt)\n", "    txt = re.sub(r\"\\0s\", \"0\", txt)\n", "    txt = re.sub(r\" 9 11 \", \"911\", txt)\n", "    txt = re.sub(r\"e-mail\", \"email\", txt)\n", "    txt = re.sub(r\"\\s{2,}\", \" \", txt)\n", "    txt = re.sub(r\"quikly\", \"quickly\", txt)\n", "    txt = re.sub(r\" usa \", \" America \", txt)\n", "    txt = re.sub(r\" USA \", \" America \", txt)\n", "    txt = re.sub(r\" u s \", \" America \", txt)\n", "    txt = re.sub(r\" uk \", \" England \", txt)\n", "    txt = re.sub(r\" UK \", \" England \", txt)\n", "    txt = re.sub(r\"india\", \"India\", txt)\n", "    txt = re.sub(r\"switzerland\", \"Switzerland\", txt)\n", "    txt = re.sub(r\"china\", \"China\", txt)\n", "    txt = re.sub(r\"chinese\", \"Chinese\", txt) \n", "    txt = re.sub(r\"imrovement\", \"improvement\", txt)\n", "    txt = re.sub(r\"intially\", \"initially\", txt)\n", "    txt = re.sub(r\"quora\", \"Quora\", txt)\n", "    txt = re.sub(r\" dms \", \"direct messages \", txt)  \n", "    txt = re.sub(r\"demonitization\", \"demonetization\", txt) \n", "    txt = re.sub(r\"actived\", \"active\", txt)\n", "    txt = re.sub(r\"kms\", \" kilometers \", txt)\n", "    txt = re.sub(r\"KMs\", \" kilometers \", txt)\n", "    txt = re.sub(r\" cs \", \" computer science \", txt) \n", "    txt = re.sub(r\" upvotes \", \" up votes \", txt)\n", "    txt = re.sub(r\" iPhone \", \" phone \", txt)\n", "    txt = re.sub(r\"\\0rs \", \" rs \", txt) \n", "    txt = re.sub(r\"calender\", \"calendar\", txt)\n", "    txt = re.sub(r\"ios\", \"operating system\", txt)\n", "    txt = re.sub(r\"gps\", \"GPS\", txt)\n", "    txt = re.sub(r\"gst\", \"GST\", txt)\n", "    txt = re.sub(r\"programing\", \"programming\", txt)\n", "    txt = re.sub(r\"bestfriend\", \"best friend\", txt)\n", "    txt = re.sub(r\"dna\", \"DNA\", txt)\n", "    txt = re.sub(r\"III\", \"3\", txt) \n", "    txt = re.sub(r\"the US\", \"America\", txt)\n", "    txt = re.sub(r\"Astrology\", \"astrology\", txt)\n", "    txt = re.sub(r\"Method\", \"method\", txt)\n", "    txt = re.sub(r\"Find\", \"find\", txt) \n", "    txt = re.sub(r\"banglore\", \"Banglore\", txt)\n", "    txt = re.sub(r\" J K \", \" JK \", txt)\n", "\n", "    # Emoji replacement\n", "    txt = re.sub(r':\\)',r' Happy ',txt)\n", "    txt = re.sub(r':D',r' Happy ',txt)\n", "    txt = re.sub(r':P',r' Happy ',txt)\n", "    txt = re.sub(r':\\(',r' Sad ',txt)\n", "    \n", "    # Remove urls and emails\n", "    txt = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', txt, flags=re.MULTILINE)\n", "    txt = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', txt, flags=re.MULTILINE)\n", "    \n", "    # Remove punctuation from text\n", "    txt = ''.join([c for c in text if c not in punctuation])\n", "#     txt = txt.replace(\".\", \" \")\n", "#     txt = txt.replace(\":\", \" \")\n", "#     txt = txt.replace(\"!\", \" \")\n", "#     txt = txt.replace(\"&\", \" \")\n", "#     txt = txt.replace(\"#\", \" \")\n", "    \n", "    # Remove all symbols\n", "    txt = re.sub(r'[^A-Za-z0-9\\s]',r' ',txt)\n", "    txt = re.sub(r'\\n',r' ',txt)\n", "    \n", "    txt = re.sub(r'[0-9]',r' ',txt)\n", "    \n", "    # Replace words like sooooooo with so\n", "    txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n", "    \n", "    # Split attached words\n", "    #txt = \" \".join(re.findall('[A-Z][^A-Z]*', txt))   \n", "    \n", "    if lowercase:\n", "        txt = \" \".join([w.lower() for w in txt.split()])\n", "        \n", "    if remove_stops:\n", "        txt = \" \".join([w for w in txt.split() if w not in stops])\n", "    if stemming:\n", "        st = PorterStemmer()\n", "#         print (len(txt.split()))\n", "#         print (txt)\n", "        txt = \" \".join([st.stem(w) for w in txt.split()])\n", "    \n", "    if lemmatization:\n", "        wordnet_lemmatizer = WordNetLemmatizer()\n", "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n", "\n", "    return txt"], "cell_type": "code", "metadata": {"_uuid": "7a8db0dce6abd3f2191911e09aeac01b7dea8602", "_cell_guid": "544cc63c-c8c4-4378-9899-006fc9caac11", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["# clean description\n", "train['Description'] = train['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = True))\n", "test['Description'] = test['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = True))"], "cell_type": "code", "metadata": {"_uuid": "90911e4c16ea8148206eda7cffbeb413292e1f4c", "_cell_guid": "d9dc880e-c6f3-4057-afd0-0a4dc63a10ea", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["test['Is_Response'] = np.nan\n", "alldata = pd.concat([train, test]).reset_index(drop=True)"], "cell_type": "code", "metadata": {"_uuid": "111ff92a819b477289039b89b8e7100044fa9526", "_cell_guid": "10125f89-3daa-4fb0-b58e-c0e466598207", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["tfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,5), max_features=8000,sublinear_tf=True,\n", "                             use_idf=True)\n", "tfidfdata = tfidfvec.fit_transform(alldata['Description'])"], "cell_type": "code", "metadata": {"_uuid": "af1a7a898517000bc344755bf2c18fc7995fa44e", "_cell_guid": "62f6c755-9732-4ce5-a48a-ddbd2ff79e01", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["# create dataframe for features\n", "tfidf_df = pd.DataFrame(tfidfdata.todense())"], "cell_type": "code", "metadata": {"_uuid": "bc01300b8f5fb499fc0b89ad2e8622b37aaf4c3f", "_cell_guid": "1117cdcb-76cc-4531-bd55-de269e75629b", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]"], "cell_type": "code", "metadata": {"_uuid": "4addab62fcc75d4bd5e23900916f9e8947fc383c", "_cell_guid": "85ee1dc9-73ca-4321-836b-ca2cf3ff9f58", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["tfid_df_train = tfidf_df[:len(train)]\n", "tfid_df_test = tfidf_df[len(train):]"], "cell_type": "code", "metadata": {"_uuid": "fff08bc1e0a3042f122af7c6a0bd3c75e57f631e", "_cell_guid": "a14a125c-95aa-476e-90f2-c00b6a22cf0f", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["# split the merged data file into train and test respectively\n", "train_feats = alldata[~pd.isnull(alldata.Is_Response)]\n", "test_feats = alldata[pd.isnull(alldata.Is_Response)]"], "cell_type": "code", "metadata": {"_uuid": "af4f93a445fc4dd949b7f7ab6c84127ea79467ca", "_cell_guid": "7e994804-e238-4d95-b881-ec626a1e9897", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["### set target variable\n", "\n", "train_feats['Is_Response'] = [1 if x == 'happy' else 0 for x in train_feats['Is_Response']]"], "cell_type": "code", "metadata": {"_uuid": "f3571a0d3da337faff7cc507b41ee4b5dc950ae5", "_cell_guid": "ae52ab9a-7add-48d2-933c-45eb6e46f854", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["# merge into a new data frame with tf-idf features\n", "cols = ['Browser_Used','Device_Used']\n", "train_feats2 = pd.concat([train_feats[cols], tfid_df_train], axis=1)\n", "test_feats2 = pd.concat([test_feats[cols], tfid_df_test], axis=1)"], "cell_type": "code", "metadata": {"_uuid": "7508e84c1f9dd2a5ab7c215db415ffeb9bf1594d", "_cell_guid": "f04d7900-7ea8-4db5-9d01-a5021af58ce1", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["from sklearn.linear_model import LogisticRegression\n", "\n", "mod1 = LogisticRegression()\n", "#mod1 = LinearSVC()\n", "target = train['Is_Response']"], "cell_type": "code", "metadata": {"_uuid": "34e77075c461fa7366379caf388cfc8560497aa4", "_cell_guid": "4dc15d57-40a9-499a-bf41-325268c27409", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["## Naive Bayes 2 - tfidf is giving higher CV score\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.metrics import accuracy_score, make_scorer\n", "#print(cross_val_score(mod1, train_feats2, target, cv=5, scoring=make_scorer(accuracy_score)))"], "cell_type": "code", "metadata": {"_uuid": "99efe1b02067e79726a6644422a550ca705b510e", "_cell_guid": "bd798b10-8436-4aaa-964f-1ec7934d2850", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["mod1.fit(train_feats2, target)"], "cell_type": "code", "metadata": {"_uuid": "50ac7a4ecaef4ab0a2eb30ce6a3ebec521ca9ae5", "_cell_guid": "a28ca5a8-6efe-48fe-8921-26a18dff55df", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["preds = mod1.predict(test_feats2)"], "cell_type": "code", "metadata": {"_uuid": "d3af9fb25d059f65956ec0a0c903f3ba5cb34d48", "_cell_guid": "bbee9e05-b4d3-41d6-bd7a-337cdebe0fb2", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["result = pd.DataFrame()\n", "result['User_ID'] = test_id\n", "result['Is_Response'] = preds\n", "mapping = {0:'happy', 1:'not_happy'}\n", "result = result.replace({'Is_Response':mapping})\n", "\n", "result.to_csv(\"lr_predicted_result_1.csv\", index=False)"], "cell_type": "code", "metadata": {"_uuid": "a3ada5bfdbc3f8ad00925402e1ae6ea0a6b1aca3", "_cell_guid": "ffdf85a8-7d83-402b-be9c-cb8abc50d7e1", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": [], "cell_type": "code", "metadata": {"_uuid": "57d4e28385f699cf9d4d37d9b2473e310f204bfd", "_cell_guid": "2b81dfd4-cc2b-480b-bdd7-dcd403f7ca77", "collapsed": true}, "execution_count": null, "outputs": []}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.3", "nbconvert_exporter": "python"}}}