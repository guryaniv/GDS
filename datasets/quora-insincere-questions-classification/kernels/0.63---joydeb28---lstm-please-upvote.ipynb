{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input , LSTM , Embedding , Conv1D , Bidirectional , GRU , Dropout, CuDNNGRU\nfrom keras.layers import GlobalMaxPool1D, Dropout, Activation,CuDNNLSTM\nfrom keras.layers import MaxPooling1D, BatchNormalization,Conv2D,Flatten\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Training and Test set processing\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\ntrain_X = train[\"question_text\"].fillna(\"_na_\").values\n\ntest_X = test[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\n\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\n\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train['target'].values\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e8b8d60c8b66b26789cea7dc8396daf9619584c"},"cell_type":"code","source":"\n#Using Embeddings\nembedding_index = dict()\n#f = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt',encoding='utf8')\n#f = open('../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec')\nf = open('../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt')\nfor line in f:\n    values = line.split(\" \")\n    words = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embedding_index[words]= coefs\n    \nf.close()\nembedding_matrix = np.zeros((max_features, 300))\nfor word, index in tokenizer.word_index.items():\n    if index > max_features - 1:\n        break\n    else:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"521d4ada2ea566c1da844f1c6b80b5939cfacd41"},"cell_type":"code","source":"# model\n'''\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max_features, output_dim= embed_size , input_length=maxlen,weights=[embedding_matrix], trainable=False))\nmodel.add(Conv1D(64,3,strides=2,padding='same',activation='relu'))\nmodel.add(Bidirectional(GRU(128,activation='relu',dropout=0.25,recurrent_dropout=0.25)))\nmodel.add(Dropout(0.45))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n'''\n'''\nmodel=Sequential()\nmodel.add(Embedding(max_features, embed_size, weights=[embedding_matrix],input_length=maxlen,trainable = False))\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(32, activation=\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n'''\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size)(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8440d94e2d2aa3d238bc8d757e24c0a28f157c74"},"cell_type":"code","source":"#training\nmodel.fit(train_X,train_y,epochs=2,batch_size=512)\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e22d8a6f6f2088867380d570b72d62bf28201e46"},"cell_type":"code","source":"pred_test_y = model.predict([test_X], batch_size=1024, verbose=1)\npred_test_y = np.where(pred_test_y>0.5,1,0)                                    #changing the threshold in this version\nout_df = pd.DataFrame({\"qid\":test[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}