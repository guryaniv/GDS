{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.1"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"metadata": {"_uuid": "6edf05b1e0d0a077062c6e6ded151db90ff4433d", "_cell_guid": "4d35a60d-edde-4662-8fd9-52d266837472"}, "source": "### Big Idea: Learned embeddings for categorical features and the datetime component. \n\n* I show here how to get embeddings from datetime components. \n\nBetter ones can doubtless be extracted: e.g. cyclical components, or the daily time-elapsed [code included]), and latLong's rounded then embedded. \n\n* Based on:\nhttps://github.com/minimaxir/predict-reddit-submission-success/blob/master/predict_askreddit_submission_success_timing.ipynb\n\n * Rossman categorical embeddings idea:  https://www.kaggle.com/c/rossmann-store-sales/discussion/17974\n\nThe approach mentioned by taxi trajectory winners:\nhttp://blog.kaggle.com/2015/07/27/taxi-trajectory-winners-interview-1st-place-team-%F0%9F%9A%95/\n ", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "6b4fcecb194cd3159d467ce55dd387b621cec99f", "collapsed": true, "trusted": false, "_cell_guid": "d9f6da27-1814-453f-b3f3-7257c6bbfa44", "_execution_state": "idle"}, "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "from random import random, sample, seed"], "outputs": [], "cell_type": "code", "execution_count": 12}, {"metadata": {"_uuid": "b0f2a40c64e4b3eca19e994a818f8ca6c2c27b53", "trusted": false, "_cell_guid": "b1d3c1ed-e309-476d-9f27-aae52cd56297", "_execution_state": "idle"}, "source": "train = pd.read_csv('../input/train.csv',infer_datetime_format=True,parse_dates=[\"pickup_datetime\"])\nprint(train.shape)", "outputs": [], "cell_type": "code", "execution_count": 13}, {"metadata": {"_uuid": "4e4e213dc3ca3e32682d261b3fb8c08c4fc3e3e6", "trusted": false, "_cell_guid": "ba33b8f3-81a9-4bf8-a75d-aa0bff9ee051", "_execution_state": "idle"}, "source": "## drop outlier duration trips. I leave in 0 passenger trips and the like, so you may want to clean differently\n\nduration_mask = ((train.trip_duration < 70) | # < 1.1 min\n             (train.trip_duration > 3600*4)) # > 4 hours # orig: 3,600 = 1 hours\nprint('Anomalies in trip duration, %: {:.2f}'.format(\n    train[duration_mask].shape[0] / train.shape[0] * 100\n))\ntrain = train[~duration_mask] # drop 10k anomalies\nprint(train.shape)", "outputs": [], "cell_type": "code", "execution_count": 14}, {"metadata": {"_uuid": "e881c7a3d69cb387cea94d86e1a4031da7613b65", "trusted": false, "_cell_guid": "73664a49-fcd7-4bd5-9047-689d11a28a7b", "_execution_state": "idle"}, "source": "train.head()", "outputs": [], "cell_type": "code", "execution_count": 15}, {"metadata": {"_uuid": "d3dbf1062400621e08dd59261dd61c10ff9e016c", "trusted": false, "_cell_guid": "9e675a44-3057-4640-88ce-82d8f00b6736", "_execution_state": "idle"}, "source": "train.head().pickup_datetime", "outputs": [], "cell_type": "code", "execution_count": 16}, {"metadata": {"_uuid": "50dadad72a23ad29f485026308e8d81926d3694f", "_cell_guid": "ad38718a-c8b7-49fb-a326-c3794835d5d9"}, "source": "# Add seconds since start of day\n* Done with dt subtraction and times set to midnight..\n    * https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.normalize.html#pandas.Series.dt.normalize\n    \n    * currently not used.\n    ### Could improve by adding cyclic datetime components (sin(hour))", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "391fc9a04cfb4db533e1a85ca68b2fa10f95f50c", "trusted": false, "_cell_guid": "5b4d8ca1-28bc-40b9-86e4-c24d3f89d567", "_execution_state": "idle"}, "source": "# seconds since start of day\ntrain[\"seconds_elapsed\"] = (train.pickup_datetime - train.pickup_datetime.dt.normalize()).dt.seconds\n\n## Add cyclical time features \n\n# train['week_delta_sin'] = np.sin((train[\"pickup_datetime\"].dt.dayofweek / 7) * np.pi)**2\n# train['hour_sin'] = np.sin((train[\"pickup_datetime\"].dt.hour / 24) * np.pi)**2", "outputs": [], "cell_type": "code", "execution_count": 17}, {"metadata": {"_uuid": "e275c6893ac65fe0aaef6d3abc00ae1b0de3f309", "collapsed": true, "trusted": false, "_cell_guid": "913708c4-c359-4a8c-9dd6-234ba96811bb", "_execution_state": "idle"}, "source": "hours = np.array(train[\"pickup_datetime\"].dt.hour, dtype=int)\nminutes = np.array(train[\"pickup_datetime\"].dt.minute, dtype=int)\ndayofweeks = np.array(train[\"pickup_datetime\"].dt.dayofweek, dtype=int)\ndayofyear = np.array(train[\"pickup_datetime\"].dt.dayofyear, dtype=int)", "outputs": [], "cell_type": "code", "execution_count": 18}, {"metadata": {"_uuid": "15d5141a00e55307b5330354e71e0ecc3c0162da", "trusted": false, "_cell_guid": "2adef02a-7370-49e1-a304-475531dad49f", "_execution_state": "idle"}, "source": "print(hours[0:2])\nprint(minutes[0:2])\nprint(dayofweeks[0:2])\nprint(dayofyear[0:2])", "outputs": [], "cell_type": "code", "execution_count": 19}, {"metadata": {"_uuid": "5d366dea09a3d69d7c990f0971ab9b5bae7c8066", "_cell_guid": "29dc5628-35ab-4e32-8059-0d708e8d473e"}, "source": "## Process Categoricals \n* All features must be zero-indexed integers.\n* hours is in the correct format. (0 = 12AM EST, 23 = 11PM EST)\n* dayofweeks is in the correct format (0 = Sunday, 6 = Saturday)\n* minutes is in the correct format verbatim.\n* dayofyears is 1-indexed, so  subtract 1.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "d5fd18d40ef9dd53cc4c899692a749f4d1a98231", "trusted": false, "_cell_guid": "3349ece0-dce6-418e-af49-c774ce07afa1", "_execution_state": "idle"}, "source": "dayofyears_tf = dayofyear - 1\n\nprint(dayofyears_tf[0:10])", "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "ce83ba5c07820d2fb8e0b737a887c6dea93c7c31", "trusted": false, "_cell_guid": "8236ba7d-b73f-467c-b073-bbdabb59cc63", "_execution_state": "busy"}, "source": "from keras.models import Input, Model\nfrom keras.layers import Dense, Embedding, GlobalAveragePooling1D, concatenate, Activation\nfrom keras.layers.core import Masking, Dropout, Reshape\nfrom keras.layers.normalization import BatchNormalization\n\nbatch_size = 64\nembedding_dims = 64\nepochs = 20", "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "8238cf6eea673f4cd17a6f33731738adc9af85a0", "_cell_guid": "066c3ed4-31d4-4e97-926f-17d4422b37bd"}, "source": "# Categoricals' Embedding Branch\nEach variable gets its own input and Embeddings. (size of each Embedding is already known by construction of the variables).\n\nReshape is necessary to convert from 2D to 1D.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "6d4e2a7ce6c1d42442cc1b7c82b75ec9b877b307", "collapsed": true, "trusted": false, "_cell_guid": "f0b689c8-5f5f-4a4c-9190-94f6711b4096", "_execution_state": "busy"}, "source": "meta_embedding_dims = 64\n\nhours_input = Input(shape=(1,), name='hours_input')\nhours_embedding = Embedding(24, meta_embedding_dims)(hours_input)\nhours_reshape = Reshape((meta_embedding_dims,))(hours_embedding)\n\ndayofweeks_input = Input(shape=(1,), name='dayofweeks_input')\ndayofweeks_embedding = Embedding(7, meta_embedding_dims)(dayofweeks_input)\ndayofweeks_reshape = Reshape((meta_embedding_dims,))(dayofweeks_embedding)\n\nminutes_input = Input(shape=(1,), name='minutes_input')\nminutes_embedding = Embedding(60, meta_embedding_dims)(minutes_input)\nminutes_reshape = Reshape((meta_embedding_dims,))(minutes_embedding)\n\ndayofyears_input = Input(shape=(1,), name='dayofyears_input')\ndayofyears_embedding = Embedding(366, meta_embedding_dims)(dayofyears_input)\ndayofyears_reshape = Reshape((meta_embedding_dims,))(dayofyears_embedding)", "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "65c8656e36db4cc3eca4591874e7204d9a5ca3a8", "_cell_guid": "84a00ee6-4296-486c-9b82-273480b6b93e"}, "source": "## following this, combine with other feature layers then run learning\n\n* remainder of code to be filled in ; e.g. with all numeric features (after 0-1/normalization)", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "c9b63ebe54def14d34936d4820dfac218c2d3baf", "trusted": false, "_cell_guid": "75a989e8-3255-4ed8-be09-63eee66b7e9c", "_execution_state": "busy"}, "source": "merged = concatenate([ hours_reshape, dayofweeks_reshape, minutes_reshape, dayofyears_reshape])\n\nhidden_1 = Dense(256, activation='relu')(merged)\nhidden_1 = BatchNormalization()(hidden_1)\n\nmain_output = Dense(1, activation='sigmoid', name='main_out')(hidden_1)\n\n\nmodel = Model(inputs=[hours_input,\n                      dayofweeks_input,\n                      minutes_input,\n                      dayofyears_input], outputs=[main_output])\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nmodel.summary()", "outputs": [], "cell_type": "code", "execution_count": null}]}