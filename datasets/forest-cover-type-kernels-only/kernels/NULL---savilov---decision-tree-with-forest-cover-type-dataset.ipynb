{"cells":[{"metadata":{"_uuid":"2b8e3b937865c8e3a65038086068fad56d773d1e"},"cell_type":"markdown","source":"In this kernel I will check how works decision tree classifier with Forest Cover Type dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport datetime \nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"pandas_train = pd.read_csv(\"../input/train.csv\")\nX_pandas = pandas_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ncolumns = pandas_train.axes[1]\nX = np.array(X_pandas)\nY = np.array(pandas_train)[:,-1]\nY = Y.reshape([Y.shape[0], 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1b9dd600afaf293311234f474136f5f61cff00f"},"cell_type":"code","source":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15c32a2e8ac34138592f6f078f88f8ad1ffffbe6"},"cell_type":"markdown","source":"Choosing the best depth for criterion='entropy':"},{"metadata":{"trusted":true,"_uuid":"3f9c998caaa72506083408ae91e68bf5b9e67658"},"cell_type":"code","source":"acc_train, acc_valid = [], []\nfor i in range(3, 35):  \n    clf = DecisionTreeClassifier(criterion='entropy', max_depth=i, random_state=1)\n    clf.fit(X_train, Y_train)\n    prediction = clf.predict(X_train)\n    acc_score_train = accuracy_score(Y_train, prediction)\n    acc_train.append(acc_score_train)\n    prediction = clf.predict(X_valid)\n    acc_score_valid = accuracy_score(Y_valid, prediction)\n    acc_valid.append(acc_score_valid)\n    print(\"\"\"Depth = {0}, Train accuracy = {1},\n          Valid accuracy = {2}\"\"\".format(i, acc_score_train, acc_score_valid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a36a087a01949775390476afbb96e63ecc54fc29"},"cell_type":"markdown","source":"The best depth is 26"},{"metadata":{"trusted":true,"_uuid":"a34bb4901c1ecaffbbd6e195f2a78ac02291c3ee"},"cell_type":"code","source":"plt.plot(range(3, 35), acc_train)\nplt.plot(range(3, 35), acc_valid)\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('depth')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fafbf01079773d6439631ad4dd1bf7300a193ccb"},"cell_type":"markdown","source":"Choosing the best depth for criterion='gini':"},{"metadata":{"trusted":true,"_uuid":"eff20b5effc65ddf9a2448a77a274f09eb63dfec"},"cell_type":"code","source":"acc_train, acc_valid = [], []    \nfor i in range(3, 35):  \n    clf = DecisionTreeClassifier(criterion='gini', max_depth=i, random_state=1)\n    clf.fit(X_train, Y_train)\n    prediction = clf.predict(X_train)\n    acc_score_train = accuracy_score(Y_train, prediction)\n    acc_train.append(acc_score_train)\n    prediction = clf.predict(X_valid)\n    acc_score_valid = accuracy_score(Y_valid, prediction)\n    acc_valid.append(acc_score_valid)\n    print(\"\"\"Depth = {0}, Train accuracy = {1}\",\n          Valid accuracy = {2}\"\"\".format(i, acc_score_train, acc_score_valid))  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a916d3e2139ace234a0386d5c2fe43a3f20122bb"},"cell_type":"markdown","source":"The best depth is 31"},{"metadata":{"trusted":true,"_uuid":"ae78741d7cacc8795a03332b7cd26ef8ff7c3ac6"},"cell_type":"code","source":"plt.plot(range(3, 35), acc_train)\nplt.plot(range(3, 35), acc_valid)\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('depth')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d5539ecf5403f01b78cb3dcbab8a87ebcfa7521"},"cell_type":"markdown","source":"Extracting not important features:"},{"metadata":{"trusted":true,"_uuid":"2eeb9896c1cc5aac5d81c99eeabfdec67fc83221"},"cell_type":"code","source":"importances = clf.feature_importances_\nzero_imp_columns = []\nprint(\"\\nNot important features:\")\nfor i, name in zip(importances, columns):\n    if i == 0.0:\n        print(\"{0}:{1}\".format(name, i))\n        zero_imp_columns.append(name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f1adec93c9b86baf97dac8fd79fb5825caa395c"},"cell_type":"markdown","source":"Deleting not important features from data to estimate execution time:"},{"metadata":{"trusted":true,"_uuid":"34013887f9164a125f926918073545f37eab703a"},"cell_type":"code","source":"for f in zero_imp_columns:\n    pandas_train = pandas_train.drop([f], axis=1)\n\nX_pandas = pandas_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ncolumns = pandas_train.axes[1]\nX = np.array(X_pandas)\nY = np.array(pandas_train)[:,-1]\nY = Y.reshape([Y.shape[0], 1])\nX_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"265ebbe7fb271423d7230677ef9b3a4351664e8a"},"cell_type":"markdown","source":"Time execution for criterion='entropy' and depth=26:"},{"metadata":{"trusted":true,"_uuid":"40a51b2d174b6544432f5d8ca4e8d70b62ec9543"},"cell_type":"code","source":"start_time = datetime.datetime.now()\nclf = DecisionTreeClassifier(criterion='entropy', max_depth=26, random_state=1)\nclf.fit(X_train, Y_train)\nprediction = clf.predict(X_train)\nacc_score_train = accuracy_score(Y_train, prediction)\nprediction = clf.predict(X_valid)\nacc_score_valid = accuracy_score(Y_valid, prediction)\nprint(\"\"\"\\nDecision tree time execution(criterion='entropy' and depth=26 ): {0},\n      Train accuracy = {1},\n      Valid accuracy = {2}\"\"\".format(datetime.datetime.now() - start_time, acc_score_train, acc_score_valid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91677602fbbce06162f01fc19ca97c314c7c30d7"},"cell_type":"markdown","source":"Time execution for criterion='gini' and depth=31 :"},{"metadata":{"trusted":true,"_uuid":"66b1b71f8cd3150df59d750938fdf06359d0fb44"},"cell_type":"code","source":"start_time = datetime.datetime.now()\nclf = DecisionTreeClassifier(criterion='gini', max_depth=31, random_state=1)\nclf.fit(X_train, Y_train)\nprint(\"\"\"\\nDecision tree time execution(criterion='gini' and depth=31 ): {0},\n      Train accuracy = {1},\n      Valid accuracy = {2}\"\"\".format(datetime.datetime.now() - start_time, acc_score_train, acc_score_valid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"febb0c5447b91533fe3937dca764198c344864ab"},"cell_type":"markdown","source":"Results:<br>\nthe best depth for criterion='entropy' = 26 <br>\nTrain accuracy = 1.0,<br>\nValid accuracy = 0.7939814814814815<br>\nDecision tree time execution(criterion='entropy' and depth=26 ): 0:00:00.235269<br>\nthe best depth for criterion='gini' = 31<br>\nTrain accuracy = 1.0,<br>\nValid accuracy = 0.7939814814814815<br>\nDecision tree time execution(criterion='gini' and depth=31 ): 0:00:00.149738<br>\n<br>\nWe see high variance on this model that says about overfitting. However, we can notice that model with Gini impurity works twice faster than with Information Gain Entropy.<br>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}