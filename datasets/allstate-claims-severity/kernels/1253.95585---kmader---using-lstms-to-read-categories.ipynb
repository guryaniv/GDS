{"metadata": {"language_info": {"name": "python", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1, "cells": [{"source": ["# Overview\n", "The goal of this notebook is to make a simple neural network which uses a standard embedding followed by an LSTM for all the categorical variables and fully connected layers for the continuous ones (after a batch normalization step). The idea is a little absurd since the categories likely refer to very different things (A in cat1 is probably not the same as A in cat2), but the approach lets us see how much of this information can be captured by a single LSTM (bidirectional) and if it could be viable approach for unknown categorical data"], "metadata": {"_uuid": "db03d4ed38e4488b9910e356ea79810ca9c075a3", "_cell_guid": "d4e0e354-61a7-45ff-86b0-ab1060f33e64"}, "cell_type": "markdown"}, {"source": ["import os\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "import keras\n", "from keras.layers import Embedding, Dense, Input, MaxPooling1D, concatenate, Flatten, Dropout, BatchNormalization\n", "from keras.layers import LSTM, Bidirectional, TimeDistributed\n", "from keras.models import Model"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "547d64ad5e01739133d7d1876ca42ae408d19d13", "_cell_guid": "7ce2d1c3-6d8a-48f7-b010-f648bb921c5a"}, "cell_type": "code"}, {"source": ["base_path = os.path.join('..', 'input')\n", "train_path = os.path.join(base_path, 'train.csv')\n", "test_path = os.path.join(base_path, 'test.csv')\n", "train_df = pd.read_csv(train_path)\n", "test_df = pd.read_csv(test_path)\n", "train_df.sample(3)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "4499e2b0b4f1e408bb67adb16c2c0da389fc2dea", "_cell_guid": "37bd9a89-52b1-4800-8160-38c51597a0db"}, "cell_type": "code"}, {"source": ["from sklearn.preprocessing import LabelEncoder\n", "cat_cols = [x for x in train_df.columns if 'cat' in x]\n", "cont_cols = [x for x in train_df.columns if 'cont' in x]\n", "le_cat_encoder = LabelEncoder()\n", "# fit the encoder based on training and test datasets\n", "le_cat_encoder.fit(np.concatenate([train_df[x] for x in cat_cols]+\n", "                                 [test_df[x] for x in cat_cols]))\n", "print(len(le_cat_encoder.classes_), 'categories')\n", "y_col = 'loss'"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "06a1343abdc2d2ffe8c910ff13fe609b6c14a470", "_cell_guid": "3d15adc4-62ea-40bd-8a64-01efe6070484"}, "cell_type": "code"}, {"source": ["all_emb_chan, all_inputs = [], []\n", "for k in cat_cols:\n", "    in_val = Input(shape = (1,), name = k)\n", "    all_emb_chan +=[Embedding(len(le_cat_encoder.classes_)+1, 64)(in_val)]\n", "    all_inputs += [in_val]\n", "concat_layer = concatenate(all_emb_chan, axis = 1) # concatenate all of the columns together\n", "\n", "norm_concat_emb = BatchNormalization()(concat_layer)\n", "feature_layer = TimeDistributed(Dense(32))(Dropout(0.5)(norm_concat_emb))\n", "feature_lstm = Bidirectional(LSTM(16))(feature_layer)\n", "\n", "cont_input = Input(shape = (len(cont_cols),), name = 'continuous')\n", "bn_cont = BatchNormalization()(cont_input)\n", "cont_feature_layer = Dense(16)(Dropout(0.5)(bn_cont))\n", "full_concat_layer = concatenate([feature_lstm, cont_feature_layer])\n", "full_reduction = Dense(16)(full_concat_layer)\n", "\n", "out_layer = Dense(1, activation = 'tanh')(full_reduction)\n", "full_model = Model(inputs = all_inputs+[cont_input], \n", "                   outputs = [out_layer], name = 'FullModel')\n", "full_model.compile(optimizer = 'adam', loss = 'mae')\n", "print('Using a model with:', full_model.count_params(), 'parameters, in', len(full_model.layers), 'layers')"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "48fdfdbb09c366e442aa2175bdb99c672f1e369a", "_cell_guid": "6cd262e6-bc2f-46ed-922e-d0c42a437805"}, "cell_type": "code"}, {"source": ["y_vec = train_df[y_col].copy().values\n", "loss_mean, loss_std = y_vec.mean(), 3*y_vec.std()\n", "y_vec -= loss_mean\n", "y_vec /= loss_std\n", "train_df['loss_norm'] = y_vec.clip(-1,1)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "31df6e1f383d24a7070c12c5e3a200bfdd345a86", "collapsed": true, "_cell_guid": "00bedcdc-d7ad-4796-a7a2-3a6eb563f7e4"}, "cell_type": "code"}, {"source": ["from sklearn.model_selection import train_test_split\n", "t_split_df, v_split_df = train_test_split(train_df, \n", "                 test_size = 0.2,\n", "                 stratify = pd.qcut(train_df['loss'], 10),\n", "                                         random_state = 2017)\n", "print(t_split_df.shape, v_split_df.shape)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "d210e349c6193a7bf6a6db4f4fec6a0d646e2585", "_cell_guid": "09004ba0-bc9c-40c1-a9b0-079c0b48114d"}, "cell_type": "code"}, {"source": ["def gen_samples(in_df, batch_size = None, loss_name = 'loss_norm'):\n", "    while True:\n", "        out_df = in_df if batch_size is None else in_df.sample(batch_size)\n", "        feed_dict = {c_name: le_cat_encoder.transform(out_df[c_name].values) for c_name in cat_cols}\n", "        feed_dict['continuous'] = out_df[cont_cols].values\n", "        yield feed_dict, out_df[loss_name].values"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "dcc7c50fa339e41a37d184602ba4ffff79452a3f", "collapsed": true, "_cell_guid": "acb7e243-2380-47a0-8445-0e7ae0b3d096"}, "cell_type": "code"}, {"source": ["loss_history = []"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "acbcc25ee9920500ca75ee257713a00b9ee4a53a", "collapsed": true, "_cell_guid": "fa64be5f-9ac1-4f31-ad09-dc0439d4854d"}, "cell_type": "code"}, {"source": ["for i in range(10):\n", "    loss_history += [full_model.fit_generator(gen_samples(t_split_df, 32), \n", "                         steps_per_epoch = 500,\n", "                         epochs = 1,\n", "                         validation_data = next(gen_samples(v_split_df))\n", "                         )]"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "67f7c60db0a8b0694d18fc6378857cbd501f63d8", "_cell_guid": "455b3968-b357-4d69-a366-8298843c5435"}, "cell_type": "code"}, {"source": ["%%time\n", "valid_vars, valid_loss = next(gen_samples(v_split_df, loss_name = 'loss'))\n", "pred_loss = full_model.predict(valid_vars).ravel()*loss_std+loss_mean"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "5bde5218de9c1a69c4bb9c88cef0f494bb3c5e91", "collapsed": true, "_cell_guid": "f214b359-268d-4f55-be4e-d540a582c86c"}, "cell_type": "code"}, {"source": ["fig, ax1 = plt.subplots(1,1)\n", "ax1.hist(valid_loss-pred_loss)\n", "ax1.set_title('Loss Error: MAE-%2.2f' % (np.mean(np.abs(valid_loss-pred_loss))))\n", "ax1.set_xlabel('Actual - Predicted Loss')"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "03a1d747104230b154fb79a617ddffb750a7bbe3", "_cell_guid": "0e98aa60-69fe-457d-8cac-d35c2fc08dfe"}, "cell_type": "code"}, {"source": ["# Make Prediction\n", "Here we load the test data and make a prediction"], "metadata": {}, "cell_type": "markdown"}, {"source": ["%%time\n", "test_vars, test_id = next(gen_samples(test_df, loss_name = 'id'))\n", "pred_test_loss = full_model.predict(test_vars, verbose = 1).ravel()*loss_std+loss_mean"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "d15955692371d00fef19fcda657f38085a7c6522", "_cell_guid": "ad27e8e6-5940-419f-8801-462a4e3aae39"}, "cell_type": "code"}, {"source": ["pd.DataFrame(dict(id = test_id, loss = pred_test_loss)).to_csv('prediction.csv', index = False)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "545f5f31444e6de2fb6566328692571806e661b7", "collapsed": true, "_cell_guid": "ceab3def-44ea-4c96-bd3e-c2726e16c388"}, "cell_type": "code"}, {"source": [], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}], "nbformat": 4}