{"cells":[{"metadata":{"_uuid":"c0cd2a1fd3c9b2e6a6e9f61c8d8b36e46f1682ab"},"cell_type":"markdown","source":"## LSTM for time-series\n\nIn this competition we basically have time-series, so it makes sense to solve it as such.\n\nIn this kernel I build an LSTM neural net in Pytorch."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time\nfrom sklearn.metrics import accuracy_score\nimport time\nimport altair as alt\nfrom altair.vega import v3\nfrom IPython.display import HTML\nimport json\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch.utils.data\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split, GroupKFold, GroupShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"712895963a877392ba25583198111d80876a30a6"},"cell_type":"code","source":"# Preparing altair. I use code from this great kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n\nvega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\nvega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\nvega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n    paths: {}\n}});\n\"\"\"\n\n#------------------------------------------------ Defs for future rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"</script>\",\n)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee31cdd4e99d65a1472fb29518a00589471f01e9"},"cell_type":"markdown","source":"## Loading and preparing data\n\nWe need to convert data from csv into 3D array containing series"},{"metadata":{"trusted":true,"_uuid":"c3738f8d34b447d1391bb13385516bc1447e1b04"},"cell_type":"code","source":"train = pd.read_csv('../input/X_train.csv')\ntest = pd.read_csv('../input/X_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9719cce4f4c7cee587d79cf0b09fd57730e7c6f1"},"cell_type":"code","source":"for col in train.columns:\n    if 'orient' in col:\n        scaler = StandardScaler()\n        train[col] = scaler.fit_transform(train[col].values.reshape(-1, 1))\n        test[col] = scaler.transform(test[col].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"339c7f432d20238ed75397a331dc55f3ffda65c4"},"cell_type":"code","source":"train = np.array([x.values[:,3:].T for group,x in train.groupby('series_id')], dtype='float32')\ntest = np.array([x.values[:,3:].T for group,x in test.groupby('series_id')], dtype='float32')\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20f5d8d8c2dbce02dcc516bdcf4be13c9d8c7fcd"},"cell_type":"markdown","source":"### Encoding labels"},{"metadata":{"trusted":true,"_uuid":"7f7cc7959dd775cb15f2111b74bff0487a4693e5"},"cell_type":"code","source":"y = pd.read_csv('../input/y_train.csv')\nle = LabelEncoder()\ny = le.fit_transform(y['surface'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09ce140968d03b8ab46b848b97bd764f350224eb"},"cell_type":"markdown","source":"### LSTM Model"},{"metadata":{"trusted":true,"_uuid":"a19b36d2d433f67cec005279a06be20af4b6d5cc"},"cell_type":"code","source":"class LSTMClassifier(nn.Module):\n\n    def __init__(self, in_dim, hidden_dim, num_layers, dropout, bidirectional, num_classes, batch_size):\n        super(LSTMClassifier, self).__init__()\n        self.in_dim = in_dim\n        self.hidden_dim = hidden_dim\n        self.batch_size = batch_size\n        self.bidirectional = bidirectional\n        self.num_dir = 2 if bidirectional else 1\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.lstm = nn.LSTM(input_size=self.in_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, dropout=self.dropout/2, bidirectional=self.bidirectional,\n                            batch_first=True)\n        self.gru = nn.GRU(self.hidden_dim * 2, self.hidden_dim, bidirectional=self.bidirectional, batch_first=True)\n\n\n        self.fc = nn.Sequential(\n            nn.Linear(2048, int(hidden_dim)),\n            nn.SELU(True),\n            nn.Dropout(p=dropout),\n            nn.Linear(int(hidden_dim), num_classes),\n        )\n\n    def forward(self, x):\n\n        x = x.permute(2, 0, 1)\n        lstm_out, _ = self.lstm(x)\n        gru_out, _ = self.gru(lstm_out)\n        avg_pool_l = torch.mean(lstm_out.permute(1, 0, 2), 1)\n        max_pool_l, _ = torch.max(lstm_out.permute(1, 0, 2), 1)\n        \n        avg_pool_g = torch.mean(gru_out.permute(1, 0, 2), 1)\n        max_pool_g, _ = torch.max(gru_out.permute(1, 0, 2), 1)\n\n        x = torch.cat((avg_pool_g, max_pool_g, avg_pool_l, max_pool_l), 1)\n        y = self.fc(x)\n        \n        return y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85ce154b9996b27f937f88943604649e6b905632"},"cell_type":"markdown","source":"### Functions"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"46f988015e0c7769fd7ffd76760cd8ed56a5fff3"},"cell_type":"code","source":"def plot_training_process(df: pd.DataFrame(), epoch_col: str, value_columns: list, y_axis_name: str, title: str):\n    \n    # code mostly based on: https://altair-viz.github.io/gallery/multiline_tooltip.html\n    plot_df = df.melt(id_vars=epoch_col, value_vars=value_columns, var_name='group', value_name=y_axis_name)\n    plot_df[y_axis_name] = plot_df[y_axis_name].round(4)\n    nearest = alt.selection(type='single', nearest=True, on='mouseover', fields=[epoch_col], empty='none')\n    line = alt.Chart().mark_line(interpolate='basis').encode(\n        x=f'{epoch_col}:Q',\n        y=f'{y_axis_name}:Q',\n        color='group:N',\n    ).properties(\n        title=title\n    )\n\n    # Transparent selectors across the chart. This is what tells us\n    # the x-value of the cursor\n    selectors = alt.Chart().mark_point().encode(\n        x=f'{epoch_col}:Q',\n        opacity=alt.value(0),\n    ).add_selection(\n        nearest\n    )\n\n    # Draw points on the line, and highlight based on selection\n    points = line.mark_point().encode(\n        opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n    )\n\n    # Draw text labels near the points, and highlight based on selection\n    text = line.mark_text(align='left', dx=5, dy=-5).encode(\n        text=alt.condition(nearest, f'{y_axis_name}:Q', alt.value(' '))\n    )\n\n    # Draw a rule at the location of the selection\n    rules = alt.Chart().mark_rule(color='gray').encode(\n        x=f'{epoch_col}:Q',\n    ).transform_filter(\n        nearest\n    )\n\n    # Put the five layers into a chart and bind the data\n    return alt.layer(line, selectors, points, rules, text,\n              data=plot_df, width=600, height=300).interactive()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"281f2723df6f32decfe2175e92c156aec3359d8f"},"cell_type":"code","source":"def train_net(train_loader, val_loader, patience, model, criterion, optimizer, scheduler, verbose, plot_training):\n    valid_loss_min = np.Inf\n    patience = patience\n    # current number of epochs, where validation loss didn't increase\n    p = 0\n    # whether training should be stopped\n    stop = False\n\n    epochs = 20000\n    training_logs = []\n\n    for e in range(1, epochs + 1):\n        # print(time.ctime(), 'Epoch:', e)\n        train_loss = []\n        train_acc = []\n        model.train()\n        for batch_i, (data, target) in enumerate(train_loader):\n            data, target = data.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            train_loss.append(loss.item())\n\n            a = target.data.cpu().numpy()\n            b = output.detach().cpu().numpy().argmax(1)\n            train_acc.append(accuracy_score(a, b))\n\n            loss.backward()\n            optimizer.step()\n\n        val_loss = []\n        val_acc = []\n        for batch_i, (data, target) in enumerate(val_loader):\n            data, target = data.cuda(), target.cuda()\n            output = model(data)\n\n            loss = criterion(output, target)\n            val_loss.append(loss.item()) \n            a = target.data.cpu().numpy()\n            b = output.detach().cpu().numpy().argmax(1)\n            val_acc.append(accuracy_score(a, b))\n\n        if e % 100 == 0 and verbose:\n            print(f'Epoch {e}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train acc: {np.mean(train_acc):.4f}, valid acc: {np.mean(val_acc):.4f}')\n\n        training_logs.append([e, np.mean(train_loss), np.mean(val_loss), np.mean(train_acc), np.mean(val_acc)])\n\n        scheduler.step(np.mean(val_loss))\n\n        valid_loss = np.mean(val_loss)\n        if valid_loss <= valid_loss_min:\n            # print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n            torch.save(model.state_dict(), 'model.pt')\n            valid_loss_min = valid_loss\n            p = 0\n\n        # check if validation loss didn't improve\n        if valid_loss > valid_loss_min:\n            p += 1\n            # print(f'{p} epochs of increasing val loss')\n            if p > patience:\n                print('Stopping training')\n                stop = True\n                break        \n\n        if stop:\n            break\n\n    checkpoint = torch.load('model.pt')      \n    model.load_state_dict(checkpoint)\n    \n    if plot_training:\n        training_logs = pd.DataFrame(training_logs, columns=['Epoch', 'Train loss', 'Valid loss', 'Train accuracy', 'Validation accuracy'])\n        loss_plot = plot_training_process(df=training_logs, epoch_col='Epoch', value_columns=['Train loss', 'Valid loss'], y_axis_name='loss', title='Loss progress')\n        acc_plot = plot_training_process(df=training_logs, epoch_col='Epoch', value_columns=['Train accuracy', 'Validation accuracy'], y_axis_name='accuracy', title='Accuracy progress')\n        render(loss_plot & acc_plot)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"89a0776d59505a320ffee1fb505c97f21f91b5d6"},"cell_type":"code","source":"def initialize_model():\n    torch.manual_seed(42)\n    model = LSTMClassifier(10, 256, 3, 0.1, True, 9, 64)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=.5)\n    model.cuda()\n    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=8, factor=0.5, verbose=True)\n    return model, criterion, optimizer, scheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"255d52ae34b4a0471912bcc15423445367834545","_kg_hide-input":true},"cell_type":"code","source":"def train_net_folds(X, X_test, y, folds, plot_training, batch_size, patience, verbose, le):\n\n    oof = np.zeros((len(X), 9))\n    prediction = np.zeros((len(X_test), 9))\n    scores = []\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X[train_index], X[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        train_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n        val_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_valid), torch.LongTensor(y_valid))\n\n        train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size, shuffle=True)\n        val_loader = torch.utils.data.DataLoader(val_set,batch_size=batch_size)\n        \n        model, criterion, optimizer, scheduler = initialize_model()\n        model = train_net(train_loader, val_loader, patience, model, criterion, optimizer, scheduler, verbose, plot_training)\n        \n        y_pred_valid = []\n        for batch_i, (data, target) in enumerate(val_loader):\n            data, target = data.cuda(), target.cuda()\n            p = model(data)\n            pred = p.cpu().detach().numpy()\n            y_pred_valid.extend(pred)\n            \n        y_pred = []\n        for i, data in enumerate(test):\n            p = model(torch.FloatTensor(data).unsqueeze(0).cuda())\n            y_pred.append(p.cpu().detach().numpy().flatten())\n            \n        oof[valid_index] = np.array(y_pred_valid)\n        scores.append(accuracy_score(y_valid, np.array(y_pred_valid).argmax(1)))\n\n        prediction += y_pred\n\n    prediction /= n_fold\n    \n    prediction = np.array(prediction).argmax(1)\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    print('--' * 50)\n    \n    return oof, prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93bfcc903b1ef04a18d0032bea9cf831f257aead","scrolled":true},"cell_type":"code","source":"n_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)\noof, prediction = train_net_folds(train, test, y, folds, True, 64, 40, True, le)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ba4ee95fc74550eefea186f54a0b97562b2d2f8"},"cell_type":"code","source":"sub = pd.DataFrame(le.inverse_transform(prediction), columns=['surface'])\nsub.to_csv('submission.csv', index_label='series_id')\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d34d7bd376ed7fb894de80052091ff4d15ddc202"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}