{"cells":[{"metadata":{"_uuid":"32c0ca07f711743e7a175640c10db9cf3c958117"},"cell_type":"markdown","source":"Thanks to Kritika who's LGB model I am forking to quickly get started on this competition. Also, thanks to Swarnim Kumar whose simple neural network I will be using to get started.\nIdea is to use LGB with increased features (can use automated feature engineering since features are masked so no point in performing domain feature engineering).\n\nAnother trick can be to use entity embeddings for these features, however will have to explore if that can go well with this."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"../input\"))\n\n%matplotlib inline\n\nimport gc\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdb2711f64b2dfe743bddd2bfb32a46591f125f2"},"cell_type":"markdown","source":"### Import Datasets"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"828e4b1012e234ba7f682b1e387c2d05fbc4b859"},"cell_type":"markdown","source":"The dataset consists of an ID_code, 200 input variables (all numeric) and a binary target variable representing the transaction-happened. Since the entire dataset is masked, cannot do much of exploratory data analysis"},{"metadata":{"trusted":true,"_uuid":"a0e0b8790bfad2971ded1cc49ac797a2e7ce2dd9"},"cell_type":"code","source":"# Look at first 10 records of the train dataset\ntrain.head(n=10).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b691360673acbc5e7941e6b2f81f40823e949499"},"cell_type":"code","source":"# Check out the shape of the train and test sets\nprint('Train:', train.shape)\nprint('Test:', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"576fc096a0cbe69c8e263c24157f31cfe4f60cf6"},"cell_type":"markdown","source":"This is an unbalanced classification problem with only 10% records having target variable = 1. "},{"metadata":{"trusted":true,"_uuid":"de522dc421e5e45302fdc6e86b0c065ed0a24282","scrolled":true},"cell_type":"code","source":"# Check the target variable distribution\ntrain['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c60ef4717b02f9d06c9c1247f3b2f68f14024d59"},"cell_type":"code","source":"# Imports for Modeling\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc, confusion_matrix, classification_report\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d5e8adf058a442a6f0f367d168708f9919eae27"},"cell_type":"markdown","source":"Let's separate input variables and target variable. "},{"metadata":{"trusted":true,"_uuid":"9d8b05895c7b688096dbc47e1cd7859c5339d252"},"cell_type":"code","source":"# Target variable from the Training Set\nTarget = train['target']\n\n# Input dataset for Train and Test \ntrain_inp = train.drop(columns = ['target', 'ID_code','var_10','var_124','var_185','var_103','var_7','var_129','var_17','var_16',\n                                  'var_117','var_161','var_100','var_96','var_30','var_136','var_27','var_98','var_29','var_38','var_183','var_182',\n                                 'var_158','var_41','var_126','var_73','var_160','var_46','var_189','var_39','var_79','var_47',\n                                 'var_69','var_176','var_42','var_101','var_84','var_3','var_61','var_19','var_59','var_37'])\nX_test = test.drop(columns = ['ID_code','var_10','var_124','var_185','var_103','var_7','var_129','var_17','var_16',\n                                  'var_117','var_161','var_100','var_96','var_30','var_136','var_27','var_98','var_29','var_38','var_183','var_182',\n                             'var_158','var_41','var_126','var_73','var_160','var_46','var_189','var_39','var_79','var_47',\n                                 'var_69','var_176','var_42','var_101','var_84','var_3','var_61','var_19','var_59','var_37'])\n\ntrain= train.drop(columns=['ID_code','var_10','var_124','var_185','var_103','var_7','var_129','var_17','var_16',\n                                  'var_117','var_161','var_100','var_96','var_30','var_136','var_27','var_98','var_29','var_38','var_183','var_182',\n                          'var_158','var_41','var_126','var_73','var_160','var_46','var_189','var_39','var_79','var_47',\n                                 'var_69','var_176','var_42','var_101','var_84','var_3','var_61','var_19','var_59','var_37'])\ntest= test.drop(columns=['ID_code','var_10','var_124','var_185','var_103','var_7','var_129','var_17','var_16',\n                                  'var_117','var_161','var_100','var_96','var_30','var_136','var_27','var_98','var_29','var_38','var_183','var_182',\n                        'var_158','var_41','var_126','var_73','var_160','var_46','var_189','var_39','var_79','var_47',\n                                 'var_69','var_176','var_42','var_101','var_84','var_3','var_61','var_19','var_59','var_37'])\n\n# List of feature names\nfeatures = list(train_inp.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd69e3ded9bea8731bcc019488d29c2d12fce9b9"},"cell_type":"code","source":"# Split the Train Dataset into training and validation sets for model building. \n\nX_train, X_val, Y_train, Y_val = train_test_split(train_inp, Target, test_size= 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb75d6bdcd781ec10e2ccabeb1fbe01e0c7902dd"},"cell_type":"code","source":"# check the split of train and validation\nprint('Train:',X_train.shape)\nprint('Validation:',X_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8ffe9ec80776236932ec936b5b57f253a4f1bb6"},"cell_type":"markdown","source":"Using **Light GBM ** for modeling this prediction problem."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"70d1ec7daaaa24a657a35bd36cc4fd59667dde1c"},"cell_type":"code","source":"#custom function to build the LightGBM model.\ndef run_lgb(X_train, Y_train, X_val, Y_val, X_test):\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"auc\",\n        \"num_leaves\" : 600,\n        \"learning_rate\" : 0.01,\n        \"verbosity\" : -1,\n        \"boosting\":\"gbdt\",\n        \"max_depth\":-1,\n        \"scale_pos_weight\":2\n    }\n    \n    lgtrain = lgb.Dataset(X_train, label=Y_train)\n    lgval = lgb.Dataset(X_val, label=Y_val)\n    evals_result = {}\n    model_lgb = lgb.train(params, lgtrain, 2800 , valid_sets=[lgval], \n                      early_stopping_rounds=100, verbose_eval=100, evals_result=evals_result)\n    \n    Y_pred_lgb = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration)\n    return Y_pred_lgb, model_lgb, evals_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9a58431c69fd3b5ba66c1352f15bc547c04aa43"},"cell_type":"code","source":"# Training the model \nY_pred_lgb, model_lgb, evals_result = run_lgb(X_train, Y_train, X_val, Y_val, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"888f234059d5404ce96c07f1a3ee12ad021b270b"},"cell_type":"code","source":"# Extract feature importances\nfeature_importance_values = model_lgb.feature_importance()\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\nfeature_importances.sort_values(by='importance', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c00112868c0db89586186f7e9e13a9257cfb2961"},"cell_type":"code","source":"# Submission dataframe\n\nY_pred_lgb= model_lgb.predict(X_test)\n\nsubmit_file_lgb = pd.read_csv('../input/sample_submission.csv')\nsubmit_file_lgb['target'] = Y_pred_lgb\nsubmit_file_lgb.to_csv('Light GBM.csv', index=False)\n\nprint (\"Light GBM prediction file successfully generated.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d53e84de4c88ea6396c8d124d49e9f870c0acf49"},"cell_type":"markdown","source":"Using **Neural Network** to predict the customer challenge. As usual giving rubbish results in vanilla form- need to normalize the data and then explore entity embeddings. Can also try Fast AI or PyTorch to use Neural Networks for this, they might be fine tuning parameters and performing  normalization in the code itself."},{"metadata":{"_uuid":"0cf2cdec004d06e5b6e1c125b5dce995ebd7a2e1"},"cell_type":"markdown","source":"Normalizing the data to feed into Neural Network. Tree based models like Light GBM or XGBoost work fine without normalization but Neural Network doesn't work properly, so normalizing."},{"metadata":{"trusted":true,"_uuid":"3bce322629adb4f8af35a4d980ae71000e7f4080"},"cell_type":"code","source":"from fastai.tabular import *\n\nprocs = [Normalize]\nvalid_idx = range(len(train)- 10000, len(train))\ndata = TabularDataBunch.from_df(path = '.',df=train,dep_var='target',valid_idx = valid_idx, procs = procs,test_df=test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6a42b18672650fe0added2296657c475fd7d3cd"},"cell_type":"code","source":"learn = tabular_learner(data,layers=[150,100],metrics=accuracy)\nlearn.fit_one_cycle(16,0.001)\n\n#learn.lr_find()\n#learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fa6eaa8cbafff7181dc18cb0280dc6db5cba1fc"},"cell_type":"code","source":"test_predicts = learn.get_preds(ds_type=DatasetType.Test)\nY_pred_nn = to_np(test_predicts[0])\nY_pred_nn = Y_pred_nn[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2316a7b6f81de15974d7c878f63f2eadd90d1afc"},"cell_type":"code","source":"submit_file_nn = pd.read_csv('../input/sample_submission.csv')\nsubmit_file_nn['target'] = Y_pred_nn\nsubmit_file_nn.to_csv('Neural Network.csv', index=False)\n\nprint (\"Neural Network prediction file successfully generated.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a65832dc0e103c827f4a8891c87341dc4d02c4bc"},"cell_type":"code","source":"Y_pred = (0.5 * Y_pred_nn) + (0.5 * Y_pred_lgb)\n\nsubmit_file = pd.read_csv('../input/sample_submission.csv')\nsubmit_file['target'] = Y_pred\nsubmit_file.to_csv('LGB and NN.csv', index=False)\n\nprint (\"Combination of LGB and Neural Network prediction file successfully generated.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c21af4a5b038a1fc4c437ec5cdd56aceb7f396f"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}