{"cells":[{"metadata":{"_cell_guid":"e42ca4de-111e-4272-a27f-1af3ec16b0e6","_uuid":"a3cf743f95457f4d45b869cd8643fd0b629e0532"},"cell_type":"markdown","source":"# Detecting Credit Card Fraud with Neural Networks"},{"metadata":{"_cell_guid":"2298d680-d5bb-41b8-8ade-1a3ed2efbdfd","_uuid":"ea03b308b0a32bf9278cb9ea56d63a2089952308"},"cell_type":"markdown","source":"Let's see if we can predict whether or not a given credit card transacation is fraudulent using a neural network."},{"metadata":{"_cell_guid":"bd6424c8-8e0c-4fe3-ac87-f46a7a835458","_uuid":"0d2d4e2c7ef85595e694a0ef24fcb31b0f844c9b"},"cell_type":"markdown","source":"## Loading and Observing the Data"},{"metadata":{"_uuid":"4700267f0ac789885eeb63cd54565989c8d47e11"},"cell_type":"markdown","source":"Let's load the necessary packages and view the data file's location."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.metrics import average_precision_score, confusion_matrix\nimport matplotlib.pyplot as plt\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"Let's now load the data into a pandas dataframe and take a peek at its contents."},{"metadata":{"_cell_guid":"9d86ee8c-ccc2-491a-9687-793654d57714","_uuid":"f6636c1dad5a8d11cc736b4bd7e34af7c1a3f1c1","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcard.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e5c31c29-8c96-41a5-9a8b-ad02581eb440","_uuid":"8190f409afcbfef31d3af56ba06501d6d3da0b88","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8dceef40-a65e-4fea-a609-a35e3ff65a61","_uuid":"90dd0b749a9f7ccbd94ed0a1940cb3057b2d9b9b","trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cb9598298b483e259af5c90a8e0d8215191589d"},"cell_type":"markdown","source":"Looking at the above data description, we can see that the 'Time' feature has an extremely large standard deviation, and viewing some of the dataset above, it seems to be different for almost every data instance. Since training a machine learning model on this feature will likely lead to overfitting it,  we shall drop 'Time' from our dataset."},{"metadata":{"trusted":true,"_uuid":"baf84836ba0cfbffdddf52a8299f1907b56b93d1"},"cell_type":"code","source":"data.drop(\"Time\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79d1d044-b24c-4714-bb83-73ab44bd6935","_uuid":"e9cf42dc880efe0d7358d73835c2b334395976cc"},"cell_type":"markdown","source":"There is apparently a huge class imbalance in this dataset. We can confirm this by plotting a seaborn countplot of the different class labels."},{"metadata":{"_cell_guid":"76c48348-35a4-4d8f-aed2-5d3ad8cf8c4d","_uuid":"1754462d4ac4687c3031533283064edc0936add8","trusted":true},"cell_type":"code","source":"sns.countplot(data[\"Class\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a1902e3d-fef4-44f0-b052-bc362b762b1f","_uuid":"a5eb06883de1752bdc420a62d9a8b64401ba344b"},"cell_type":"markdown","source":"As we can see from the above plot, fraudulent activities make up a very small fraction of this dataset. Let's further check to see if there are any null values in the data."},{"metadata":{"_cell_guid":"7143ba4d-b836-47db-9838-8e326b9d88d7","_uuid":"0caebc6c43ba2fa73ec6b34e5079296dd19ee8fd","trusted":true},"cell_type":"code","source":"data.isnull().any().describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3965cdf-0d82-45e2-82c9-455a01ee09f6","_uuid":"e0c98c84fbfdf0fde8b29c6278c025fe02879bb3"},"cell_type":"markdown","source":"Fortunately, the dataset is not missing any values."},{"metadata":{"_cell_guid":"dc5fc052-8cf7-4b49-b04d-b11876033be8","_uuid":"9c7e665fc72a9e9c63f42f2ec863f479c1d69e32"},"cell_type":"markdown","source":"## Training a Model"},{"metadata":{"_cell_guid":"f4eb5053-37fe-4650-ad27-ea8a65e37faa","_uuid":"32faec95f57fcbbb8b05ada2034c367c70f101e2"},"cell_type":"markdown","source":"Let's partition our dataset into a train, validation, and test set, where 90% of the data will be a part of the training set and 5% will be allocated for each of the validation and test set."},{"metadata":{"_cell_guid":"beec7e9e-a893-4658-929b-7ffa7abd989d","_uuid":"cce2cb78a20aacf9c408d6d819f13ad1f9d2f337","trusted":true},"cell_type":"code","source":"limit = int(0.9*len(data))\ntrain = data.loc[:limit]\nval_test = data.loc[limit:]\nval_test.reset_index(drop=True, inplace=True)\nval_test_limit = int(0.5*len(val_test))\nval = val_test.loc[:val_test_limit]\ntest = val_test.loc[val_test_limit:]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a11fff9-91b6-4402-bb51-c9b15d4e1acc","_uuid":"56d704338f75039e4fd6f382f513765542a8cd3f"},"cell_type":"markdown","source":"Let's check to see that the validation and test set include a fair amount of fraudulent activites before going any further."},{"metadata":{"_cell_guid":"47ae2345-b6bf-4053-b339-b3639cc7fc40","_uuid":"3bd9317a824f9726cea8081af5ee3978eeae2aa9","trusted":true},"cell_type":"code","source":"print(\"Number of fraudulent transactions in the validation set: {}\"\\\n      .format(val[\"Class\"].value_counts()[1]))\nprint(\"Number of fraudulent transactions in the test set: {}\"\\\n      .format(test[\"Class\"].value_counts()[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db649f192ec18826568bb807ff09c095371bde3d"},"cell_type":"markdown","source":"Now we can focus on developing a model to accurately detect fraudulent activity. Due to the huge class imbalance in our dataset, a model that simply identifies all transactions as not being fraudulent would score high accuracy. There also would not be many fraudulent samples for the model to learn from to be able to accurately identify what a fraulent transaction is. Therefore, we should find a way to balance out the number of positive and negatives instances in our training set. This can be done by either oversampling the positive instances, or undersampling the negative instances. Undersampling the negative instances would involve reducing the number of non-fraudulent transactions until the ratio between positive and negative instances was approximately 1-to-1. Since we don't have that many data samples, I fear doing so would severely limit our model's performance, since it would have much less data to train on. We shall therefore oversample the positive instances in our training data.\n\nTo oversample the positive instances in our training set, we will add copies of them to it, but with their feature values slightly tweaked. This slight manipulation of the data is done so as to have the copies being different from their original counterparts, but not too different as to end up teaching our model false information. This tweaking will be done by multiplying each positive sample copy's feature values by a number between the uniform distribution of 0.9 and 1.1."},{"metadata":{"trusted":true,"_uuid":"97b6ebaa22f54c671e6eec39a7fccd29459fd129"},"cell_type":"code","source":"train_positive = train[train[\"Class\"] == 1]\ntrain_positive = pd.concat([train_positive] * int(len(train) / len(train_positive)), ignore_index=True)\nnoise = np.random.uniform(0.9, 1.1, train_positive.shape)\ntrain_positive = train_positive.multiply(noise)\ntrain_positive[\"Class\"] = 1\ntrain_extended = train.append(train_positive, ignore_index=True)\ntrain_shuffled = train_extended.sample(frac=1, random_state=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92daf2d4d7602f02258f68ef9fe3a1eb4cc29bf5"},"cell_type":"markdown","source":"The ratio of positive to negative instances in our training set should now be much more balanced."},{"metadata":{"trusted":true,"_uuid":"869bb9e409b35d647c646ef5757f5f70df5c205c"},"cell_type":"code","source":"sns.countplot(train_shuffled[\"Class\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00a07abbbb77c145d2d45d19c790d04cceab7e9e"},"cell_type":"markdown","source":"Let's now separate our train, validation, and test data into their predictors and labels."},{"metadata":{"trusted":true,"_uuid":"47465503e5a212151f3b72963f3db2114f120f9b"},"cell_type":"code","source":"X_train = train_shuffled.drop(labels=[\"Class\"], axis=1)\nY_train = train_shuffled[\"Class\"]\nX_val = val.drop(labels=[\"Class\"], axis=1)\nY_val = val[\"Class\"]\nX_test = test.drop(labels=[\"Class\"], axis=1)\nY_test = test[\"Class\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51f968925be4dea8ff06e245617d12f8096d5426"},"cell_type":"markdown","source":"Let's also standardize the values in our dataset so that when building a machine learning model we don't unintentionally lend more weight to some features over others. We will fit a standardizer to the training data, and transform the training, validation, and test data based on this scaler."},{"metadata":{"trusted":true,"_uuid":"b28e94f07ad8586e63fca639ee99018415e10c52"},"cell_type":"code","source":"scaler = StandardScaler()\nX_train[X_train.columns] = scaler.fit_transform(X_train)\nX_val[X_val.columns] = scaler.transform(X_val)\nX_test[X_test.columns] = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e45e4124350450462602f10d58569756d2f89c8"},"cell_type":"markdown","source":"Let's now build and train a feedforward neural network to detect fraudulent activity. The neural network will contain several layers of hidden units with ReLU activation functions, and a sigmoid output unit to output the probability of a given transaction being fraudulent. The Adam optimizer will be used with an initial learning rate of 1e-4 and a binary cross-entropy loss function. The model will be trained for a maximum of 50 epochs, though the learning rate of the Adam optimizer will be reduced by a factor of 0.1 if the validation loss does not improve after 3 epochs of training. This will continue until a minimum learning rate of 1e-6 is reached. If the validation loss does not improve after 5 epochs of training, we will halt training of the neural network."},{"metadata":{"trusted":true,"_uuid":"958e4606a9601333d42bff70f6d61c187c5607be"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(64, activation=\"relu\", input_dim=(X_train.shape[1])))\nmodel.add(Dense(32, activation=\"relu\"))\nmodel.add(Dense(16, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(4, activation=\"relu\"))\nmodel.add(Dense(2, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(optimizer=Adam(lr=1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=50, \n                    validation_data=(X_val, Y_val), \n                    callbacks=[ReduceLROnPlateau(patience=3, verbose=1, min_lr=1e-6), \n                               EarlyStopping(patience=5, verbose=1)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33bc61d6c211782c15f891bbcb9d4b6cf7169630"},"cell_type":"markdown","source":"## Analyzing our Model"},{"metadata":{"_uuid":"1fcb0cbaf42346cbc4a77d4a22d8d506ef98adb6"},"cell_type":"markdown","source":"With training ceased for our neural network, let's observe how the loss and accuracy evolved during training for both the training and validation set."},{"metadata":{"trusted":true,"_uuid":"0db5d7d9ce62f6bf2edb9d4cf5d12cf3d0539c55"},"cell_type":"code","source":"num_epochs = len(history.history[\"loss\"])\nfig, axarr = plt.subplots(1, 2, figsize=(24, 8))\naxarr[0].set_xlabel(\"Number of Epochs\")\naxarr[0].set_ylabel(\"Loss\")\nsns.lineplot(x=range(1, num_epochs+1), y=history.history[\"loss\"], label=\"Train\", ax=axarr[0])\nsns.lineplot(x=range(1, num_epochs+1), y=history.history[\"val_loss\"], label=\"Validation\", ax=axarr[0])\naxarr[1].set_xlabel(\"Number of Epochs\")\naxarr[1].set_ylabel(\"Accuracy\")\naxarr[1].set_ylim(0, 1)\nsns.lineplot(x=range(1, num_epochs+1), y=history.history[\"acc\"], label=\"Train\", ax=axarr[1])\nsns.lineplot(x=range(1, num_epochs+1), y=history.history[\"val_acc\"], label=\"Validation\", ax=axarr[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c4014768562c765f0cd694a3fd4c39c9f3ec44d"},"cell_type":"markdown","source":"Viewing the loss and accuracy graphs, we can see that after some initial improvements, we seemed to reach convergence after only a few epochs. Let's view the accuracy achieved by this neural network on the test set."},{"metadata":{"trusted":true,"_uuid":"ce0fc1bdd8ed4fedb2cdfc60f66c26af889a6208"},"cell_type":"code","source":"test_results = model.evaluate(X_test, Y_test)\nprint(\"The model test accuracy is {}.\".format(test_results[1]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d42ddd9-89f8-4265-ad54-2296d075faec","_uuid":"c9a5de313016cca0c74e899ffb71ad7f59c051c4"},"cell_type":"markdown","source":"Achieving almost 100% accuracy on the test set is of course thrilling, but one must not forget about the huge class imbalance still present in the test set. A model that simply outputted that there were no fraudulent transactions would achieve high accuracy as well. The dataset info recommends using the AUPRC as an evaluation metric instead, to gather a better understanding of how well the model truly performed. We will use the average precision score instead, which is an evaluation metric available in scikit-learn that is sometimes used as an alternative to AUPRC. The average precision score will have a value between 0 and 1, with a better model having a greater score. Let's view the test average precision score for our model."},{"metadata":{"trusted":true,"_uuid":"a543d8df194c3797ebe5f5dec7e1188f175b4d0f"},"cell_type":"code","source":"predictions = model.predict_classes(X_test)\nap_score = average_precision_score(Y_test, predictions)\nprint(\"The model test average precision score is {}.\".format(ap_score))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9fe3b764-684f-4112-aa1e-62c138172547","_uuid":"58ba0e4597b527f5f7422ccfb9cfc51be81abe17"},"cell_type":"markdown","source":"As we can see, the model didn't perform quite as well as we initially thought. Let's view a confusion matrix of the predictions made by the model on the test set, to gain a better idea of what type of errors it is making."},{"metadata":{"_cell_guid":"7d0d4ac8-4c75-4233-85d4-87bf9ac23670","_uuid":"58979d66e795d61af15489e86fcd0e2b724650ae","trusted":true},"cell_type":"code","source":"confusion = pd.DataFrame(confusion_matrix(Y_test, predictions))\nconfusion.columns = [\"Predicted Negative\", \"Predicted Positive\"]\nconfusion.index = [\"Actual Negative\", \"Actual Positive\"]\nsns.heatmap(confusion, annot=True)\nplt.yticks(rotation=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d4724810-1c81-4548-93a5-fcb2f5838a6d","_uuid":"a345b2351e60079c43bf152ace84e26faa8fcbee"},"cell_type":"markdown","source":"Viewing the confusion matrix, we can see that the neural network detected around half of the fraudulent transactions, but misidentifed a few non-fraudulent transactions as fraudulent."},{"metadata":{"trusted":true,"_uuid":"8908f8f1e35dd4191da961bf0a2e2dcafde7e3e1"},"cell_type":"markdown","source":"## Final Remarks"},{"metadata":{"_uuid":"0a583ef2408a7493743ae594222acd05c388deb1"},"cell_type":"markdown","source":"Training a feedforward neural network, we were able to build a machine learning model that detected credit card fraud in around half of the guilty transactions in the test set without overly missclassifying non-fraudulent transaction as fraudulent. Spending more time fine-tuning the neural network architecture, as well as manipulating the features in the dataset, could perhaps further improve upon these results. Regardless, I hope this kernel serves as a satisfactory example for others on how to approach training a neural network for a binary classification problem with a very imbalanced dataset."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}