{"cells":[{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T02:45:48.506025Z","start_time":"2018-09-25T02:45:48.474111Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.optimizers import SGD\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\n\nfrom tqdm import tqdm\n\nimport os\nimport gc\nfrom itertools import combinations, chain\nfrom datetime import datetime\nprint(os.listdir(\"../input\"))\n\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fc04f3cb613ee725c681a18f8bbcbcdb88af57d"},"cell_type":"markdown","source":"# summary"},{"metadata":{"_uuid":"968a5603b4bf10a83bc0054c21465141c8a9fcce"},"cell_type":"markdown","source":"## model summary\nWe created a total of 10 learning models and stacked their predicted by LightGBM.\n\ntable of contents\n\n"},{"metadata":{"_uuid":"fc395abfff45e2d6f729efe6ac68a8262aa5e863"},"cell_type":"markdown","source":"# nadare's kernel"},{"metadata":{"ExecuteTime":{"end_time":"2018-09-24T23:30:05.797806Z","start_time":"2018-09-24T23:30:04.059462Z"},"trusted":false,"_uuid":"9e91ca19c0142cb111e1acabaeb5a27c22eef5a4"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nsmpsb = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"885a2a8875b9a7f134c3cb64673fe32d1bb1d7c2"},"cell_type":"markdown","source":"## preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"### EDA & leader board hacking"},{"metadata":{"ExecuteTime":{"end_time":"2018-09-24T23:34:14.130390Z","start_time":"2018-09-24T23:34:12.528688Z"},"trusted":false,"_uuid":"bdbb097f03fc4d277079c5fccc01fd9574728ece"},"cell_type":"code","source":"# First of all, let's see the distribution of each variable.\n# You can see that there is a big difference in distribution between training data and test data.\n\nfrom scipy.stats import gaussian_kde\n\ndef compare_dist(ax, feature, i=0):\n    sns.kdeplot(train_df[feature], label=\"train\", ax=ax)\n    sns.kdeplot(test_df[feature], label=\"test\", ax=ax)\n\n\ndef numeric_tile(plot_func):\n    fig, axs = plt.subplots(2, 5, figsize=(24, 6))\n    axs = axs.flatten()\n    \n    for i, (ax, col) in enumerate(zip(axs, train_df.columns.tolist()[1:11])):\n        plot_func(ax, col, i)\n        ax.set_title(col)\n    plt.tight_layout()\n    \nnumeric_tile(compare_dist)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-24T23:43:39.706816Z","start_time":"2018-09-24T23:43:37.597405Z"},"trusted":false,"_uuid":"3ed05251dda674f52765dfb71298c93c2ee979f6"},"cell_type":"code","source":"# For the training data, display the distribution of variables for each target.\n\n# Please pay attention to \"Elevation\". The difference between the training data and the test data distribution is\n# thought to be due to the difference between the proportion of the target variables in the training data and the test data.\n\ndef compare_target(ax, feature, i=0):\n    sns.kdeplot(train_df.loc[:, feature], label=\"train\", ax=ax)\n    sns.kdeplot(test_df.loc[:, feature], label=\"test\", ax=ax)\n    for target in range(1, 8):\n        sns.kdeplot(train_df.loc[train_df[\"Cover_Type\"] == target, feature], label=target, alpha=0.5, lw=1, ax=ax)\n\nnumeric_tile(compare_target)        ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T00:15:51.003553Z","start_time":"2018-09-25T00:15:46.599325Z"},"trusted":false,"_uuid":"b3f75b110dc9d589866d8d6de4e0a1d6fcb25316"},"cell_type":"code","source":"# I was able to obtain the distribution of the test data by submitting prediction data with all the same purpose variables.\n\n\"\"\"\nsmpsb = pd.read_csv(\"../input/sample_submission.csv\")\nfor i in range(1, 8):\n    smpsb[\"Cover_Type\"] = i\n    smpsb.to_csv(\"all_{}.csv\".format(i), index=None)\"\"\"\n\n# and this is the magic number of this competition.\ntype_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\nclass_weight = {k: v for k, v in enumerate(type_ratio, start=1)}\n\n# By using these numbers, you can mimic the distribution of the test data from the training data.\ndef compare_balanced_dist(ax, feature, i=0):\n    min_ = min(train_df[feature].min(), test_df[feature].min())\n    max_ = max(train_df[feature].max(), test_df[feature].max())\n    X = np.linspace(min_, max_, 1000)\n\n    sns.kdeplot(train_df[feature], label=\"train\", ax=ax)\n    sns.kdeplot(test_df[feature], label=\"test\", ax=ax)\n    btest = np.zeros(1000)\n    \n    for target in range(1, 8):\n        btest += gaussian_kde(train_df.loc[train_df[\"Cover_Type\"] == target, feature])(X) * type_ratio[target-1]\n    \n    ax.plot(X, btest, label=\"balanced\")\n    ax.legend()\n\nnumeric_tile(compare_balanced_dist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e00a36ba7782b587a997871baf314f3d934fdd78"},"cell_type":"code","source":"# By using the following functions, it is possible to perform almost the same evaluation\n# as the leader board even in the local environment.\n\ndef balanced_accuracy_score(y_true, y_pred):\n    return accuracy_score(y_true, y_pred, sample_weight=np.apply_along_axis(lambda x: type_ratio[x], 0, y_true-1))\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-24T23:32:11.119296Z","start_time":"2018-09-24T23:32:10.796827Z"},"_uuid":"11d9cb5a1a6a43a5e8e7b7a3e1b777822de29b12"},"cell_type":"markdown","source":"### feature engineering 1"},{"metadata":{"_uuid":"6e90c46e4445d14719f145e7a2cea75270424680"},"cell_type":"markdown","source":"I will explain some of the features I consider important or unique."},{"metadata":{"_uuid":"8c2d853cbc8c1e198bb5c5c009bc6cb44652bc1e"},"cell_type":"markdown","source":"#### Aspect"},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T00:54:04.818905Z","start_time":"2018-09-25T00:54:04.553586Z"},"trusted":false,"_uuid":"b595095fcd707c0a30ff4e3234c7f5bc5ad9c934"},"cell_type":"code","source":"# The angle can be divided into sine and cosine\nsin_ = np.sin(np.pi*train_df[\"Aspect\"]/180)\ncos_ = np.cos(np.pi*train_df[\"Aspect\"]/180)\n\n# However, if this feature quantity alone, the effect seems to be light.\nplt.figure(figsize=(5, 4))\nfor i in range(1, 8):\n    cat = np.where(train_df[\"Cover_Type\"] == i)[0]\n    r = (.5+0.2*i)\n    plt.scatter(cos_[cat]*(r), sin_[cat]*(r), alpha=0.02*r, s=6, label=i)\nplt.xlim(-2, 3)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T00:51:00.805728Z","start_time":"2018-09-25T00:51:00.802709Z"},"_uuid":"a1199fd2d205a22f5d91bc988b2591de4e1922c5"},"cell_type":"markdown","source":"#### degree to hydrology"},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T00:52:23.588848Z","start_time":"2018-09-25T00:52:23.585856Z"},"trusted":false,"_uuid":"933e53d39a16bab8ce40efe36e5a909211f4e6d4"},"cell_type":"code","source":"# this may be good feature but unfortunally i forgot to add my data\nhydro_h = train_df[\"Vertical_Distance_To_Hydrology\"]\nhydro_v = train_df[\"Horizontal_Distance_To_Hydrology\"]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T00:52:24.960833Z","start_time":"2018-09-25T00:52:24.433217Z"},"trusted":false,"_uuid":"7e7c1fc8875a1b0ac9e70dc60a15cb60509543f2"},"cell_type":"code","source":"plt.scatter(hydro_h, hydro_v, s=1, c=train_df[\"Cover_Type\"], cmap=\"Set1\", alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T00:54:20.502007Z","start_time":"2018-09-25T00:54:20.386289Z"},"trusted":false,"_uuid":"b48277e4eb95cdd55c415a4d6e25dc687447b0de"},"cell_type":"code","source":"hydro_arctan = np.arctan((hydro_h+0.0001) / (hydro_v+0.0001))\nfor i in range(1, 8):\n    cat = np.where(train_df[\"Cover_Type\"] == i)[0]\n    sns.kdeplot(hydro_arctan[cat])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T00:54:57.520582Z","start_time":"2018-09-25T00:54:56.978022Z"},"trusted":false,"_uuid":"64ffc12e5f344534a878b0aea2e2c99bacde16ae"},"cell_type":"code","source":"plt.scatter(hydro_arctan, np.pi*train_df[\"Slope\"]/180, c=train_df[\"Cover_Type\"], cmap=\"Set1\", s=1.5, alpha=0.7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da688e40e90ed0f0b0a3bc2abd96a444d64a3318"},"cell_type":"markdown","source":"#### target_encoding "},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T02:22:22.284518Z","start_time":"2018-09-25T02:22:22.201740Z"},"trusted":false,"_uuid":"ea912ece3fa7e6022d5202a54c727ed3e9aaae4f"},"cell_type":"code","source":"# this is the ratio of Wilderness_Area\nplt.figure(figsize=(6, 6))\ntrain_df.filter(regex=\"Wilder\").sum(axis=0).plot(\"pie\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T02:25:22.054405Z","start_time":"2018-09-25T02:25:21.866933Z"},"trusted":false,"_uuid":"51deabb9bcfa6bc33ca903deeafa9f75ce1b9d8f"},"cell_type":"code","source":"# and this is ratio of \"over_Type\" in each \"Wildereness_area\"\nwilder = (train_df.filter(regex=\"Wilder\") * np.array([1, 2, 3, 4])).sum(axis=1)\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\naxs = axs.flatten()\nfor i, ax in enumerate(axs, start=1):\n    train_df.loc[wilder==i, \"Cover_Type\"].value_counts().sort_index().plot(\"pie\", ax=ax)\n    ax.set_title(i)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T02:30:23.381172Z","start_time":"2018-09-25T02:30:21.736568Z"},"trusted":false,"_uuid":"e9de8a38df1a7a5206740188264757cc53ec70bf"},"cell_type":"code","source":"# This shows the expression of Soil_Type for the objective variable.\nplt.figure(figsize=(12, 4))\nsns.heatmap(train_df.iloc[:, -41:].sort_values(by=\"Cover_Type\").iloc[:, :-1].T, cmap=\"Greys_r\")\nfor i in np.linspace(0, train_df.shape[0], 8)[1:]:\n    plt.axvline(i, c=\"r\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0459bec5c4616fa7cffaaa6ca41800962e9fd976"},"cell_type":"markdown","source":"As indicated above, category values are considered to have a major role in classification.\n\nTherefore, in order to handle category values effectively, the ratio of object variables in each category value is added as a feature quantity.\n\nIn order to prevent data leakage and not to excessively trust category values which have only a small number, we added values for 10 data as prior distribution to each category."},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T02:39:16.296728Z","start_time":"2018-09-25T02:39:16.291741Z"},"trusted":false,"_uuid":"89a5c509e52da8f96bf25b285472e86f579682af"},"cell_type":"code","source":"# this is the code\ndef categorical_post_mean(x):\n    p = (x.values)*type_ratio\n    p = p/p.sum()*x.sum() + 10*type_ratio\n    return p/p.sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85cf81e78ae32a4c0b98caf4262c3b67ad4c70c9"},"cell_type":"markdown","source":"#### summarizes preprocessing"},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T02:43:59.392818Z","start_time":"2018-09-25T02:43:45.346194Z"},"trusted":false,"_uuid":"31f82acb720296e835f66fc6d3cb9dc7abd2cc27"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nsmpsb = pd.read_csv(\"../input/sample_submission.csv\")\n\ndef main(train_df, test_df):\n    # this is public leaderboard ratio\n    start = datetime.now()\n    type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\n    \n    total_df = pd.concat([train_df.iloc[:, :-1], test_df])\n    \n    # Aspect\n    total_df[\"Aspect_Sin\"] = np.sin(np.pi*total_df[\"Aspect\"]/180)\n    total_df[\"Aspect_Cos\"] = np.cos(np.pi*total_df[\"Aspect\"]/180)\n    print(\"Aspect\", (datetime.now() - start).seconds)\n    \n    # Hillshade\n    hillshade_col = [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    \n    total_df[\"Hillshade_mean\"] = total_df[hillshade_col].mean(axis=1)\n    total_df[\"Hillshade_std\"] = total_df[hillshade_col].std(axis=1)\n    total_df[\"Hillshade_max\"] = total_df[hillshade_col].max(axis=1)\n    total_df[\"Hillshade_min\"] = total_df[hillshade_col].min(axis=1)\n    print(\"Hillshade\", (datetime.now() - start).seconds)\n    \n    # Hydrology ** I forgot to add arctan\n    total_df[\"Degree_to_Hydrology\"] = ((total_df[\"Vertical_Distance_To_Hydrology\"] + 0.001) /\n                                       (total_df[\"Horizontal_Distance_To_Hydrology\"] + 0.01))\n    \n    # Holizontal\n    horizontal_col = [\"Horizontal_Distance_To_Hydrology\",\n                      \"Horizontal_Distance_To_Roadways\",\n                      \"Horizontal_Distance_To_Fire_Points\"]\n    \n    \n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    print(\"Holizontal\", (datetime.now() - start).seconds)\n    \n    \n    def categorical_post_mean(x):\n        p = (x.values)*type_ratio\n        p = p/p.sum()*x.sum() + 10*type_ratio\n        return p/p.sum()\n    \n    # Wilder\n    wilder = pd.DataFrame([(train_df.iloc[:, 11:15] * np.arange(1, 5)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    wilder.columns = [\"Wilder_Type\", \"Cover_Type\"]\n    wilder[\"one\"] = 1\n    piv = wilder.pivot_table(values=\"one\",\n                             index=\"Wilder_Type\",\n                             columns=\"Cover_Type\",\n                             aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Wilder_Type\"] + [\"Wilder_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Wilder_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Wilder_Type\"] = (total_df.filter(regex=\"Wilder\") * np.arange(1, 5)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Wilder_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Wilder_Type_count\"] = total_df.loc[:, \"Wilder_Type_count\"].fillna(0)\n    print(\"Wilder_type\", (datetime.now() - start).seconds)\n    \n    \n    # Soil type\n    soil = pd.DataFrame([(train_df.iloc[:, -41:-1] * np.arange(1, 41)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    soil.columns = [\"Soil_Type\", \"Cover_Type\"]\n    soil[\"one\"] = 1\n    piv = soil.pivot_table(values=\"one\",\n                           index=\"Soil_Type\",\n                           columns=\"Cover_Type\",\n                           aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Soil_Type\"] + [\"Soil_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Soil_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Soil_Type\"] = (total_df.filter(regex=\"Soil\") * np.arange(1, 41)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Soil_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Soil_Type_count\"] = total_df.loc[:, \"Soil_Type_count\"].fillna(0)\n    print(\"Soil_type\", (datetime.now() - start).seconds)\n    \n    icol = total_df.select_dtypes(np.int64).columns\n    fcol = total_df.select_dtypes(np.float64).columns\n    total_df.loc[:, icol] = total_df.loc[:, icol].astype(np.int32)\n    total_df.loc[:, fcol] = total_df.loc[:, fcol].astype(np.float32)\n    return total_df\n\ntotal_df = main(train_df, test_df)\none_col = total_df.filter(regex=\"(Type\\d+)|(Area\\d+)\").columns\ntotal_df = total_df.drop(one_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T02:44:12.546743Z","start_time":"2018-09-25T02:44:12.435041Z"},"trusted":false,"_uuid":"e4fc31a4e0bf23728d886147a3430ff10de919b7"},"cell_type":"code","source":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T02:46:06.547203Z","start_time":"2018-09-25T02:46:06.154254Z"},"trusted":false,"_uuid":"0504ef9435ceaba8b37bfbde45ec416ac7e7ff34"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02527a3340e8079cdb7a7f978d06ffe36feacba4"},"cell_type":"markdown","source":"### KNN features and Decision tree feature"},{"metadata":{"_uuid":"4db26e4628d5a1933c745629c2f1cd1a38499708"},"cell_type":"markdown","source":"For the variable created up to the above, the decision tree and the k-nearest neighbor method are applied after narrowing down the number of variables and adding the prediction probability as the feature amount. \n\nI decided the combination of variables to be used last and the setting of parameters based on Multi-class logarithmic loss while considering diversity."},{"metadata":{"_uuid":"5b664180bc7f55a5f05226caf1956bb50b3a5f98"},"cell_type":"markdown","source":"#### KNN_feature"},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T02:58:54.238533Z","start_time":"2018-09-25T02:58:54.172710Z"},"_uuid":"eaf1fe074875abe7c8d997aa2c3b3355825f7d27","trusted":false},"cell_type":"code","source":"all_set =  [['Elevation', 500],\n            ['Horizontal_Distance_To_Roadways', 500],\n            ['Horizontal_Distance_To_Fire_Points', 500],\n            ['Horizontal_Distance_To_Hydrology', 500],\n            ['Hillshade_9am', 500],\n            ['Aspect', 500],\n            ['Hillshade_3pm', 500],\n            ['Slope', 500],\n            ['Hillshade_Noon', 500],\n            ['Vertical_Distance_To_Hydrology', 500],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 200],\n            ['Elevation_PLUS_Aspect', 200],\n            ['Elevation_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_9am', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Roadways', 100],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Wilder_Type_PLUS_Elevation', 500],\n            ['Wilder_Type_PLUS_Hillshade_Noon_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Degree_to_Hydrology', 200],\n            ['Wilder_Type_PLUS_Hillshade_9am_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Aspect_Cos', 500],\n            ['Hillshade_9am_dif_Hillshade_Noon_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Hillshade_Noon_PLUS_Hillshade_3pm', 200],\n            ['Hillshade_Noon_add_Hillshade_3pm_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200]]\n\n\ndef simple_feature_scores2(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    scl = StandardScaler().fit(X.loc[:, cols])\n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = scl.transform(X.loc[train, cols])\n        X_val = scl.transform(X.loc[val, cols])\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = scl.transform(X_test.loc[:, cols])\n        C = clf(**params)\n        C.fit(scl.transform(X.loc[:, cols]), y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5cc637ad06e0e1d3fe5924b5dbf15f1ee3d2a38","trusted":false},"cell_type":"code","source":"import warnings\nimport gc\nfrom multiprocessing import Pool\n\nwarnings.filterwarnings(\"ignore\")\n\npreds = []\ntest_preds = []\nfor colname, neighbor in tqdm(all_set):\n    gc.collect()\n    #print(colname, depth)\n    ts, tbs, ls, pred, test_pred = simple_feature_scores2(KNeighborsClassifier,\n                                                          colname.split(\"_PLUS_\"),\n                                                          test=True,\n                                                          n_neighbors=neighbor)\n    preds.append(pred)\n    test_preds.append(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c301a98315266944aae3498c863a9005623ec87","trusted":false},"cell_type":"code","source":"cols = list(chain.from_iterable([[col[0] + \"_KNN_{}\".format(i) for i in range(1, 8)] for col in all_set]))\nknn_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\nknn_train_df.columns = cols\nknn_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\nknn_test_df.columns = cols\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc362ea934b2b07aaa900a3f1510118aecd3eecc"},"cell_type":"markdown","source":"#### DT_features"},{"metadata":{"_uuid":"00347f67de008070fcdc4caf7d3e8bace8ba2dbd","trusted":false},"cell_type":"code","source":"all_set = [['Elevation', 4],\n           ['Horizontal_Distance_To_Roadways', 4],\n           ['Horizontal_Distance_To_Fire_Points', 3],\n           ['Horizontal_Distance_To_Hydrology', 4],\n           ['Hillshade_9am', 3],\n           ['Vertical_Distance_To_Hydrology', 3],\n           ['Slope', 4],\n           ['Aspect', 4],\n           ['Hillshade_3pm', 3],\n           ['Hillshade_Noon', 3],\n           ['Degree_to_Hydrology', 3],\n           ['Hillshade_Noon_dif_Hillshade_3pm', 3],\n           ['Hillshade_Noon_abs_Hillshade_3pm', 3],\n           ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 5],\n           ['Elevation_PLUS_Hillshade_max', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Aspect_Sin_PLUS_Aspect_Cos_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Fire_Points', 5],\n           ['Wilder_Type_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Hillshade_9am', 5],\n           ['Elevation_PLUS_Degree_to_Hydrology', 5],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Roadways', 5],\n           ['Wilder_Type_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Wilder_Type_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_std', 4],\n           ['Hillshade_9am_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_Noon_add_Hillshade_3pm', 5]]\n\ndef simple_feature_scores(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    \n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = X.loc[train, cols]\n        X_val = X.loc[val, cols]\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = X_test.loc[:, cols]\n        C = clf(**params)\n        C.fit(X.loc[:, cols], y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"882ab485d7b1be411b67d0d244cee7e9e078d226","trusted":false},"cell_type":"code","source":"preds = []\ntest_preds = []\nfor colname, depth in tqdm(all_set):\n    #print(colname, depth)\n    ts, tbs, ls, pred, test_pred = simple_feature_scores(DecisionTreeClassifier,\n                                                         colname.split(\"_PLUS_\"),\n                                                         test=True,\n                                                         max_depth=depth)\n    preds.append(pred)\n    test_preds.append(test_pred)\n\ncols = list(chain.from_iterable([[col[0] + \"_DT_{}\".format(i) for i in range(1, 8)] for col in all_set]))\ndt_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\ndt_train_df.columns = cols\n\ndt_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\ndt_test_df.columns = cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"650ae398572fa740879dc785a5b461317086ef8c","trusted":false},"cell_type":"code","source":"# target encoding features(1.2.3)\nte_train_df = total_df.filter(regex=\"ctype\").iloc[:len(train_df)]\nte_test_df = total_df.filter(regex=\"ctype\").iloc[len(train_df):]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4c32b082b39c89180db5b547ecf2b0571aaf861","trusted":false},"cell_type":"code","source":"train_level2 = train_df[[\"Id\"]]\ntest_level2 = test_df[[\"Id\"]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f349e101f81a8f3d4af953d227943254a760413e"},"cell_type":"markdown","source":"## modeling"},{"metadata":{"_uuid":"095ef00c2099aba8fd04b9e9d1694249d2feb2c5"},"cell_type":"markdown","source":"I have created 6 models\n\nwithout KNN&DT features\n* Random Forest Classifier\n* PCA & K-nearest Neighbors Classifier\n* LightGBM\n\nwith KNN & DT features\n* Random Forest Classifier\n* Logistic Regression\n* LightGBM\n\nUsing these learning machines, data for stacking was created using 10-fold cross validation."},{"metadata":{"_uuid":"f0d1dd30a7e2af14a04f8250b92cb14eda93de18"},"cell_type":"markdown","source":"### without KNN&DT feature"},{"metadata":{"trusted":false,"_uuid":"ca42a3cd6f0f284ed94472a66e0610801e909552"},"cell_type":"code","source":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)\ntype_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\nclass_weight = {k: v for k, v in enumerate(type_ratio, start=1)}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efd524163b32998c065c279e8cf10cd48b9c2142"},"cell_type":"markdown","source":"#### Random forest classifier"},{"metadata":{"trusted":false,"_uuid":"719e2a31d0667c8dfd2b332349a8367bbb57691c"},"cell_type":"code","source":"RFC1_col = [\"RFC1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in RFC1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5bf8085715c17f5117f867d2df9cab077b167714"},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=150,\n                             max_depth=12,\n                             class_weight=class_weight,\n                             n_jobs=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X.iloc[train, :]\n    X_val = X.iloc[val, :]\n\n    y_train = y[train]\n    y_val = y[val]\n    rfc.fit(X_train, y_train)\n    y_val_pred = rfc.predict(X_val)\n    y_val_proba = rfc.predict_proba(X_val)\n    \n    confusion += confusion_matrix(y_val, y_val_pred)    \n    train_level2.loc[val, RFC1_col] = y_val_proba\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n\nrfc.fit(X, y)\ntest_level2.loc[:, RFC1_col] = rfc.predict_proba(X_test)\n#smpsb.loc[:, \"Cover_Type\"] = rfc.predict(X_test)\n#smpsb.to_csv(\"RFC1.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"040b3893919ef650c0be96b562393fdff12ed226"},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bbae34ea9b2bf5a479c0e9aae0d7a13dcafd168"},"cell_type":"markdown","source":"#### PCA & KNN"},{"metadata":{"trusted":false,"_uuid":"524d158a396206814880ee7d682406dd8eb2aa37"},"cell_type":"code","source":"KNN1_col = [\"KNN1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNN1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"65537269007f3bee16e8079db738e065e704e2ef"},"cell_type":"code","source":"cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"32e8c05574e03cc14beed390ac5b1c520163c126"},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=2, n_jobs=-1)\n\nscl = StandardScaler().fit(X_test.drop(cat_col, axis=1))\nX_scl = scl.transform(X.drop(cat_col, axis=1))\nX_test_scl = scl.transform(X_test.drop(cat_col, axis=1))\npca = PCA(n_components=23).fit(X_test_scl)\nX_pca = pca.transform(X_scl)\nX_test_pca = pca.transform(X_test_scl)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X_pca[train]\n    X_val = X_pca[val]\n\n    y_train = y[train]\n    y_val = y[val]\n    knn.fit(X_train, y_train)\n    y_val_pred = knn.predict(X_val)\n    y_val_proba = knn.predict_proba(X_val)\n    \n    confusion += confusion_matrix(y_val, y_val_pred)    \n    train_level2.loc[val, KNN1_col] = y_val_proba\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n\nknn.fit(X_pca, y)\ntest_level2.loc[:, KNN1_col] = knn.predict_proba(X_test_pca)\n#smpsb.loc[:, \"Cover_Type\"] = knn.predict(X_test_pca)\n#smpsb.to_csv(\"KNN1.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1e1987f27e1b1a580c146796a16b56921ee16ce6"},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ef7c413eab43090eaa2ac07a6bb3b46c2e25ed3"},"cell_type":"markdown","source":"#### LightGBM"},{"metadata":{"trusted":false,"_uuid":"bd9a9af0ce09640f0b90e349d1d69539651fd873"},"cell_type":"code","source":"LGBM1_col = [\"LGBM1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in LGBM1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c35f7a3011697bbf95e34193c1f26c1a755c9448"},"cell_type":"code","source":"cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]\ncategorical_feature = [29, 38]\nlgbm_col = X.drop(cat_col[:-2], axis=1).columns.tolist()\nclass_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2134226ea0195d71d61f10d53b00a325bb23dbee"},"cell_type":"code","source":"gbm = lgb.LGBMClassifier(n_estimators=15,\n                         num_class=7,\n                         learning_rate=0.1,\n                         bagging_fraction=0.6,\n                         num_boost_round=370,\n                         max_depth=8,\n                         max_cat_to_onehot=40,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X.loc[train, lgbm_col]\n    X_val = X.loc[val, lgbm_col]\n\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)],\n            verbose=50, categorical_feature=categorical_feature)\n\n    y_val_pred = gbm.predict(X_val)\n    y_val_proba = gbm.predict_proba(X_val)\n    \n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n    confusion += confusion_matrix(y_val, y_val_pred)\n    train_level2.loc[val, LGBM1_col] = y_val_proba\n\n\nX_all = X.loc[:, lgbm_col]\nX_test_lgbm = X_test.loc[:, lgbm_col]\ngbm.fit(X_all, y, verbose=50, categorical_feature=categorical_feature)\ntest_level2.loc[:, LGBM1_col] = gbm.predict_proba(X_test_lgbm)\n#smpsb[\"Cover_Type\"] = gbm.predict(X_test_lgbm)\n#smpsb.to_csv(\"LGBM1.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a97ac783c210d78101df455ade63265a17379134"},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa06d3aff18a8f46592c976296539f14b380f148"},"cell_type":"markdown","source":"### with KNN & DT features"},{"metadata":{"trusted":false,"_uuid":"25308199c65d017343ea96fb02f668e6cb426dd8"},"cell_type":"code","source":"X_p = pd.concat([knn_train_df, dt_train_df, te_train_df], axis=1).astype(np.float32)\nX_test_p = pd.concat([knn_test_df, dt_test_df, te_test_df.reset_index(drop=True)], axis=1).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4e05bc017ab0deaf36bec4d86840b5f2b12366c"},"cell_type":"markdown","source":"#### RandomForestClassifier"},{"metadata":{"trusted":false,"_uuid":"e5bfffb307a40b0c67c31d53a8a08033e5b91381"},"cell_type":"code","source":"KNNDT_RF_col = [\"KNNDT_RF_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_RF_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b624a15e4a0c11612205ead52e2c8711951ca31b"},"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs=-1,\n                             n_estimators=200,\n                             max_depth=None,\n                             max_features=.7,\n                             max_leaf_nodes=220,\n                             class_weight=class_weight)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n    X_train = X_p.iloc[train, :]\n    y_train = y[train]\n    X_val = X_p.iloc[val, :]\n    y_val = y[val]\n    rfc.fit(X_train, y_train)\n\n    y_pred = rfc.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n    confusion += confusion_matrix(y_val, y_pred)\n    train_level2.loc[val, KNNDT_RF_col] = rfc.predict_proba(X_val)\n\nrfc.fit(X_p, y)\ntest_level2.loc[:, KNNDT_RF_col] = rfc.predict_proba(X_test_p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bcd8c782f669a2ef57cf14b0ac1d3f5079d3dde0"},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c283aadde8a935479ca9b3efa466d14e3abd4050"},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":false,"_uuid":"15db3d3366a7a6f6fe9bf02aec83782b748cb2eb"},"cell_type":"code","source":"KNNDT_LR_col = [\"KNNDT_LR_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_LR_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"72ad8f5bb755924ea308a365f701ad86b1f09597"},"cell_type":"code","source":"confusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X, y)):\n    X_train = X_p.iloc[train, :]\n    y_train = y[train]\n    X_val = X_p.iloc[val, :]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=-1, multi_class=\"multinomial\", C=10**9, solver=\"saga\", class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_val_pred = lr.predict(X_val)\n    train_level2.loc[val, KNNDT_LR_col] = lr.predict_proba(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n    confusion += confusion_matrix(y_val, y_val_pred)\n\nlr.fit(X_p, y)\ntest_level2.loc[:, KNNDT_LR_col] = lr.predict_proba(X_test_p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8aeac651853a98927d9d3a5c255e83d5d8dfc37f"},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91077b39bcbb10180ef2a0424b19c1791db9b776"},"cell_type":"markdown","source":"#### LightGBM"},{"metadata":{"_uuid":"d09a32de613b485bef804a563d082e3dbe1ff787","trusted":false},"cell_type":"code","source":"KNNDT_LGB_col = [\"KNNDT_LGB_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_LGB_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb284b9b2d18193fab68f211ea66f7994696d200","trusted":false},"cell_type":"code","source":"X = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1).reset_index(drop=True)\n\nX_d = pd.concat([X.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n                 knn_train_df,\n                 dt_train_df], axis=1)\n\nX_test_d = pd.concat([X_test.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n                 knn_test_df,\n                 dt_test_df], axis=1)\n\nfcol = X_d.select_dtypes(np.float64).columns\nX_d.loc[:, fcol] = X_d.loc[:, fcol].astype(np.float32)\nX_d = X_d.values.astype(np.float32)\nX_test_d.loc[:, fcol] = X_test_d.loc[:, fcol].astype(np.float32)\nX_test_d = X_test_d.values.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"742567953206af0aeae3a560941b9a5687737514","trusted":false},"cell_type":"code","source":"class_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}\n\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=32,\n                         feature_fraction=0.3,\n                         min_child_samples=20,\n                         learning_rate=0.05,\n                         num_boost_round=430,\n                         max_depth=-1,                         \n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n    X_train = X_d[train]\n    X_val = X_d[val]\n\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, categorical_feature=[33, 42])\n\n    y_pred = gbm.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n    confusion += confusion_matrix(y_val, y_pred)\n    train_level2.loc[val, KNNDT_LGB_col] = gbm.predict_proba(X_val)\n    \ngbm.fit(X_d, y, categorical_feature=[33, 42])\ntest_level2.loc[:, KNNDT_LGB_col] = gbm.predict_proba(X_test_d)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b496508a7b0b892ed6e33011e2c9d1a922fef50","trusted":false},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acfd60853a6f5c166f59a29e1bc66a345ba4126e"},"cell_type":"markdown","source":"# ykskks's kernel"},{"metadata":{"_uuid":"6c91ea9786eebe34c0dc9e772139dd12cfb81cc9","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e48eb826e4756b0dde940764177f5ada1169ba97","trusted":false},"cell_type":"code","source":"train=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1efc5e6e71606f2bcb164f8fc77fd8374b340c02"},"cell_type":"markdown","source":"First, I dropped columns that have the same value in every row in the training set."},{"metadata":{"_uuid":"e1a71c69e138722e7bd744a940ee5acf811286a4","trusted":false},"cell_type":"code","source":"train.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\ntest.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5783672c5d017b070d04c386904b20c6a00adb07"},"cell_type":"markdown","source":"## Feature engineering\n\nThe feature enginnering ideas I used here are based on [Lathwal's amazing kernel ](https://www.kaggle.com/codename007/forest-cover-type-eda-baseline-model).\nPlease go check it out if you're interested.\n\nI removed 'slope_hyd' feature from the original one beacause it did'nt seem to be that useful for prediction."},{"metadata":{"_uuid":"90502edbe566c397c1b998e62d4d72c4e82cf915","trusted":false},"cell_type":"code","source":"train['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\ntrain['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\ntrain['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\ntrain['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) / 3  \ntrain['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) / 2 ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cea9c1dfad87d4d961fbfed2c124824f485c185","trusted":false},"cell_type":"code","source":"test['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\ntest['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\ntest['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology \ntest['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) / 3  \ntest['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) / 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"483b4ae89f0770f861bea38823276331e0f692c6","trusted":false},"cell_type":"code","source":"#Id for later use\nId_train=train['Id']\nId_test=test['Id']\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa8c2be8de8ddbbc41de1a52b558dfa9d6c44fc7","trusted":false},"cell_type":"code","source":"x_train=train.drop('Cover_Type', axis=1)\ny_train=train['Cover_Type']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cb0bc06ea06f7cd01023d76c17a18e81df0d61a"},"cell_type":"markdown","source":"## Modeling\nUsing features I created above, I built four different models(RandomForest, LightGBM, LogisticRegression, and SuportVectorMachine).\n\n## randomforest"},{"metadata":{"_uuid":"1ebfc5f7a08fde1f4746234c72511194af5d265f","trusted":false},"cell_type":"code","source":"#prepare df to store pred proba\nx_train_L2=pd.DataFrame(Id_train)\nx_test_L2=pd.DataFrame(Id_test)\nrf_cul=['rf'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in rf_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n\nrf=RandomForestClassifier(max_depth=None, max_features=20,n_estimators=500, random_state=1)\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n    x_train_L1=x_train.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    rf.fit(x_train_L1, y_train_L1)\n    y_val_proba=rf.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, rf_cul]=y_val_proba\n\nrf.fit(x_train, y_train)\nx_test_L2.loc[:, rf_cul]=rf.predict_proba(test)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(rf.predict(test))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('rf.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"293cb1e3271a078108834aed9118fc18ff31672a"},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"_uuid":"ceef4c46d42a9701b28ea63fe8dd4bf8fa64cbf1","trusted":false},"cell_type":"code","source":"#prepare df to store pred proba\n#x_train_L2=pd.DataFrame(Id_train)\n#x_test_L2=pd.DataFrame(Id_test)\nlgbm_cul=['lgbm'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in lgbm_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n\nlgbm=lgb.LGBMClassifier(learning_rate=0.3, max_depth=-1, min_child_samples=20, n_estimators=300, num_leaves=200, random_state=1, n_jobs=4)\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n    x_train_L1=x_train.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    lgbm.fit(x_train_L1, y_train_L1)\n    y_val_proba=lgbm.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, lgbm_cul]=y_val_proba\n\nlgbm.fit(x_train, y_train)\nx_test_L2.loc[:, lgbm_cul]=lgbm.predict_proba(test)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(lgbm.predict(test))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('lgbm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6cfa93e9d087406d2331e830e63ef877312893a"},"cell_type":"markdown","source":"## LR"},{"metadata":{"_uuid":"f9ce60a286c0998db755fa87c58c14a715c4524d","trusted":false},"cell_type":"code","source":"lr_cul=['lr'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in lr_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n    \npca=PCA(n_components=40)\nx_train_pca=pd.DataFrame(pca.fit_transform(x_train))\ntest_pca=pd.DataFrame(pca.transform(test))\n\npipeline=Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(C=10, solver='newton-cg', multi_class='multinomial',max_iter=500))])\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n    x_train_L1=x_train_pca.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train_pca.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    pipeline.fit(x_train_L1, y_train_L1)\n    y_val_proba=pipeline.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, lr_cul]=y_val_proba\n\npipeline.fit(x_train_pca, y_train)\nx_test_L2.loc[:, lr_cul]=pipeline.predict_proba(test_pca)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(pipeline.predict(test_pca))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('lr.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39561fd3170fdc43962d89ec5d552d188e181134"},"cell_type":"markdown","source":"## SVM"},{"metadata":{"_uuid":"cfa2a2e75d833bff82cd51fb73ee38ddba7e3205","trusted":false},"cell_type":"code","source":"svm_cul=['svm'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in svm_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n    \n#pca=PCA(n_components=40)\n#x_train_pca=pca.fit_transform(x_train)\n#test_pca=pca.transform(test)\n\npipeline=Pipeline([('scaler', StandardScaler()), ('svm', SVC(C=10, gamma=0.1, probability=True))])\n\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n    x_train_L1=x_train_pca.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train_pca.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    pipeline.fit(x_train_L1, y_train_L1)\n    y_val_proba=pipeline.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, svm_cul]=y_val_proba\n\npipeline.fit(x_train_pca, y_train)\nx_test_L2.loc[:, svm_cul]=pipeline.predict_proba(test_pca)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(pipeline.predict(test_pca))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('svm.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc4dd658ece053364a668653ab3c35a5e48f83b6"},"cell_type":"markdown","source":"# stacking"},{"metadata":{"_uuid":"d42deaa383b95ea9fadc4e05bbb908fc702d7caa"},"cell_type":"markdown","source":"## Level1 summary"},{"metadata":{"trusted":false,"_uuid":"310a00ca38bfc3d2149b14620203dd8b83cad06d"},"cell_type":"code","source":"# concatenate two data\ntrain_L2 = pd.concat([x_train_L2.iloc[:, 1:].reset_index(drop=True), train_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\ntest_L2 = pd.concat([x_test_L2.iloc[:, 1:].reset_index(drop=True), test_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\ntrain_L2.to_csv(\"Wtrain_L2.csv\", index=False)\ntest_L2.to_csv(\"Wtest_L2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"28b75db2a27aa91fc6da91cd65b32fb614b5be3e"},"cell_type":"code","source":"# each models score\n\ny = pd.read_csv(\"../input/train.csv\")[\"Cover_Type\"].values\nmodel_scores = {}\ntext = []\n\nfor i in range(10):\n    y_pred = np.argmax(train_L2.iloc[:, 7*i:7*(i+1)].values, axis=1) + 1\n    score = balanced_accuracy_score(y, y_pred)\n    model_scores[cols[i*7]] = score\n    text.append(\"{}\\t{:<.5}\".format(cols[i*7], score))\n\nprint(*text[::-1], sep=\"\\n\")\npd.Series(model_scores).plot(kind=\"barh\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3fd4028181ba8d2e302bbf57cd203e004fef5e7"},"cell_type":"markdown","source":"## stacking with Logistic Regression"},{"metadata":{"ExecuteTime":{"end_time":"2018-09-25T04:55:54.024497Z","start_time":"2018-09-25T04:55:54.021506Z"},"_uuid":"a46e27fd2856f948dbb7bf9b0767e1f53b129b51"},"cell_type":"markdown","source":"### nadare's simple stacking"},{"metadata":{"trusted":false,"_uuid":"b4093457aacc4a3ac1589b20e256435e7dc8bebc"},"cell_type":"code","source":"score = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = train_level2.iloc[train, 1:]\n    X_val = train_level2.iloc[val, 1:]\n    y_train = y[train]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_val)\n    score.append(balanced_accuracy_score(y_val, y_pred))\n    #print(score[-1])\nprint(np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a073df808a00458e3a6bdfbe74fdc68bcd1649b"},"cell_type":"markdown","source":"### ykskks's simple stacking"},{"metadata":{"trusted":false,"_uuid":"f8823485458b15b4a030f0b6cb3e74597005139a"},"cell_type":"code","source":"score = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = x_train_L2.iloc[train, 1:]\n    X_val = x_train_L2.iloc[val, 1:]\n    y_train = y[train]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_val)\n    score.append(balanced_accuracy_score(y_val, y_pred))\nprint(np.mean(score))\n\nlr = LogisticRegression(n_jobs=1, class_weight=class_weight)\nlr.fit(x_train_L2, y)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdfd0730d524342031c365cd24a5934c94d199ae"},"cell_type":"markdown","source":"### double simple stacking"},{"metadata":{"trusted":false,"_uuid":"b9520a86d6a7b24591596457b6530328baa9c3b2"},"cell_type":"code","source":"score = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = train_L2.iloc[train, 1:]\n    X_val = train_L2.iloc[val, 1:]\n    y_train = y[train]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_val)\n    score.append(balanced_accuracy_score(y_val, y_pred))\nprint(np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a6116b70210f4b0be69389d95af93e3a9172dd8e"},"cell_type":"code","source":"# this is 0.83266 on public LB\n\"\"\"\nsmpsb = pd.read_csv(\"../input/sample_submission.csv\")\nlr = LogisticRegression(n_jobs=1, class_weight=class_weight)\nlr.fit(train_L2, y)\nsmpsb[\"Cover_Type\"] = lr.predict(test_L2)\nsmpsb.to_csv(\"W_ensemble_LR.csv\", index=False)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b2b9381ffef1bc1b8c25758ad17723a379b879e"},"cell_type":"markdown","source":"## stacking with LightGBM"},{"metadata":{"_uuid":"a72184de5e684ade61bb85d60c137f6994bab595","trusted":false},"cell_type":"code","source":"wtrain = train_L2.values.astype(np.float32)\nwtest = test_L2.values.astype(np.float32)\ny = pd.read_csv(\"../input/train.csv\")[\"Cover_Type\"].values\nsmpsb = pd.read_csv(\"../input/sample_submission.csv\")\ncols = train_L2.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"042ee7b70f3ba6e537f5957582214d6dc2241254","trusted":false},"cell_type":"code","source":"# this is our final submission which is 0.84806 on Public LB\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=25,\n                         learning_rate=5,\n                         min_child_samples=20,\n                         bagging_fraction=.3,\n                         bagging_freq=1,\n                         reg_lambda = 10**4.5,\n                         reg_alpha = 1,\n                         feature_fraction=.2,\n                         num_boost_round=4000,\n                         max_depth=-1,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\ngbm.fit(wtrain, y, verbose=-1)\nsmpsb[\"Cover_Type\"] = gbm.predict(wtest)\nsmpsb.to_csv(\"final_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b36ef78a85b570c684b9301a5deb79920f7caa9","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(6, 12))\nplt.barh(cols, gbm.feature_importances_)\nplt.savefig(\"feature_importances.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"351fb3443797feddd01aa91cb8833852754e0bba","trusted":false},"cell_type":"code","source":"# bagging with k-fold\nscores = []\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=25,\n                         learning_rate=5,\n                         min_child_samples=20,\n                         bagging_fraction=.3,\n                         bagging_freq=1,\n                         reg_lambda = 10**4.5,\n                         reg_alpha = 1,\n                         feature_fraction=.2,\n                         num_boost_round=8000,\n                         max_depth=-1,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=-1,\n                         silent=-1,\n                         verbose=-1)\n\nproba = np.zeros((wtest.shape[0], 7))\nfor train, val in tqdm(StratifiedKFold(n_splits=5, shuffle=True, random_state=2434).split(wtrain, y)):\n    X_train = wtrain[train]\n    X_val = wtrain[val]\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, verbose=-1, \n            eval_set=[(X_train, y_train), (X_val, y_val)], early_stopping_rounds=20)\n    proba += gbm.predict_proba(wtest) / 10\n    y_pred = gbm.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n\nprint(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d36a5aa6bcd558bd50f184ec4f93c7973e0ba9a","trusted":false},"cell_type":"code","source":"smpsb[\"Cover_Type\"] = np.argmax(proba, axis=1) + 1\nsmpsb.to_csv(\"final_submission_bagging.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}