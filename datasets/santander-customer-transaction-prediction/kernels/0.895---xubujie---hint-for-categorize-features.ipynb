{"cells":[{"metadata":{"_uuid":"bc3de9f0d22dc4202063c1aec1afc610161a4571"},"cell_type":"markdown","source":"### This kernel is trying to use shap value to evaluate each feature's impact to model output. In the last part, all features impact to model output are plotted, we can see clear patterns. And some features also show clear different pattern which can give us hint to dig furthur. For explanation of shap, please have a look at [shap](https://github.com/slundberg/shap). If you find this kernel helpful, please upvote. Honestly, I am struggling in explaining the result, I really wish someone could find something useful from here."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport shap\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.options.display.max_columns=999","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08c689bb4e0486f47395f5e6d092a13f33caeddf"},"cell_type":"code","source":"# load data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n(df_train.shape, df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ec960632573e5f8a5d166cec9030c7bb69f73a7"},"cell_type":"code","source":"df_train.drop('ID_code', axis=1, inplace=True)\ndf_test.drop('ID_code', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea67a0b10b15dd1d86e4c7f35e40bd39dff74b9c"},"cell_type":"code","source":"var_cols = df_train.columns.drop('target')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0535549988f3e26ef2f57dc065e15ba7e4afb37"},"cell_type":"markdown","source":"#  1. First, Let's try a simple lightgbm classifier"},{"metadata":{"trusted":true,"_uuid":"54065d0b2632eac2e02e35371a69fc15390e3f67"},"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'num_leaves': 13,\n    'learning_rate': 0.01,\n    'feature_fraction': 0.1,\n    'bagging_fraction': 0.3,\n    'bagging_freq': 5,\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf':10.0,\n    'num_boost_round':999999,\n    'early_stopping_rounds':500,\n    'random_state':2019\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"301f472435f14b8c6c35fd0209986ce914f00d64"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\ndf_train[var_cols] = mms.fit_transform(df_train[var_cols])\ndf_test[var_cols] = mms.fit_transform(df_test[var_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34c72dc6c6457903c204fe015b0014d3fd1583fb"},"cell_type":"code","source":"X, y = df_train.drop(['target'], axis=1), df_train.target.values\n\nX_trn, X_val, y_trn, y_val = train_test_split(X, y, test_size=0.2, random_state=2019)\n\nlgb_trn = lgb.Dataset(X_trn, y_trn)\nlgb_val = lgb.Dataset(X_val, y_val)\nmodel = lgb.train(params, lgb_trn, valid_sets=[lgb_trn, lgb_val], valid_names=['train','valid'],verbose_eval=2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a9258b943dd57dbdafad6ad84688ab93cbbaaab"},"cell_type":"code","source":"p = model.predict(df_test)\nsub = pd.read_csv('../input/sample_submission.csv')\nsub.target = p\nsub.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a890a14c4396cbe2f147b0d517b0f6d2dd5cf58"},"cell_type":"markdown","source":"# 2. Machine Learning Driven EDA"},{"metadata":{"_uuid":"e16094bf3c4f2e8d92fff067212ebea5ef844241"},"cell_type":"markdown","source":"### Importance analysis\n\n- **There are no clear difference in importance among features**\n- **If the random seed changes, the feature importance will also change** (The top importance feature always change when I change the random seed)"},{"metadata":{"trusted":true,"_uuid":"7bc107ee3c538da64a456d852c0f66ce56d852c6"},"cell_type":"code","source":"# plot feature importance\nfeature_importance = pd.DataFrame(columns=['feature','importance'])\nfeature_importance.feature = X.columns.values\nfeature_importance.importance = model.feature_importance()\nfeature_importance.sort_values(by='importance', ascending=False, inplace=True)\n\nplt.figure(figsize=(10,50))\nsns.barplot('importance', 'feature', data=feature_importance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8160088f918f8bfdfdcc72d996f797f2bf1ac79b"},"cell_type":"code","source":"# shap\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fba2123b90e1ca3ce8008011566303c29b62664a"},"cell_type":"code","source":"# summarize the effects of all the features\nshap.summary_plot(shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11adf19664c4cabb46f640639f5419c8d45823e7"},"cell_type":"markdown","source":"### This plot shows summarized information about feature impact against shap output. \n\nThere are two type of features \n1. Lower the value, higher impact to the output\n2. Higher the value, higher impact to the output"},{"metadata":{"trusted":true,"_uuid":"0fcef3ee943445ac6e8f3e5aa3d670cfed6ea3e9"},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=10, ncols=10, figsize=(50,50))\nfor i in range(10):\n    for j in range(10):\n        ids = i*10+j\n        sns.scatterplot(X['var_'+str(ids)], shap_values[:,ids], ax=ax[i,j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e78fbe133489e49bc70384b5af4211b7f5aa1222"},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=10, ncols=10, figsize=(50,50))\nfor i in range(10,20):\n    for j in range(10):\n        ids = i*10+j\n        sns.scatterplot(X['var_'+str(ids)], shap_values[:,ids], ax=ax[i-10,j])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"928f9006e0fc0ae82b1606aeeb636e6794ada17c"},"cell_type":"markdown","source":"### We can see, there are some different pattern regarding between shap value and features. Here I pick up some interesting findings:\n1. var_10 show a clear turning point at 0.8 (scaled), which suggest var_10 may be a category feature??\n2. Many feature are monotone decreasing or monotone increasing with shap output. Maybe we can shuffle value between them??"},{"metadata":{"trusted":true,"_uuid":"6c5a820b13ce46bdbafa79471397398db205e2d6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}