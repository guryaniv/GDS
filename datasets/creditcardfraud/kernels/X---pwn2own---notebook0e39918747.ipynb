{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "be835af9-e1e2-f419-269b-2cc4e8b49795"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6a9c281-e5b3-268c-35ff-0d8cfeaf7b49"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"../input/creditcard.csv\")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0074f69d-843b-19b5-ff9f-0eae11240592"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "train.shape\n",
        "credit_class = train['Class']\n",
        "count_classes = pd.value_counts(credit_class,sort=True).sort_index()\n",
        "count_classes.plot(kind='bar')\n",
        "plt.title(\"Fraud class histogram\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7edd958e-a2d8-5b5a-c4d2-4b9757b68908"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train['normAmount'] = scaler.fit_transform(train['Amount'].values.reshape(-1,1))\n",
        "train = train.drop(['Amount','Time'],axis=1)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2d02de20-7e18-bd90-4e3c-3d717a8ec9d5"
      },
      "outputs": [],
      "source": [
        "X = train.ix[:, train.columns != 'Class']\n",
        "y = train.ix[:, train.columns == 'Class']\n",
        "X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "de967eae-7ed9-2b2a-8cfc-7ac890fe7396"
      },
      "outputs": [],
      "source": [
        "number_records_fraud = len(train[train.Class == 1])\n",
        "fraud_indices = np.array(train[train.Class == 1].index)\n",
        "\n",
        "# Picking the indices of the normal classes\n",
        "normal_indices = train[train.Class == 0].index\n",
        "\n",
        "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
        "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
        "random_normal_indices = np.array(random_normal_indices)\n",
        "random_normal_indices\n",
        "random_normal_indices.shape\n",
        "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
        "under_sample_indices \n",
        "under_sample_data = train.iloc[under_sample_indices,:]\n",
        "under_sample_data\n",
        "X_undersample = under_sample_data.ix[:, under_sample_data.columns != 'Class']\n",
        "y_undersample = under_sample_data.ix[:, under_sample_data.columns == 'Class']\n",
        "\n",
        "# Showing ratio\n",
        "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
        "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
        "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8d7aaef5-477c-714f-1746-8b8c321be907"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "# Whole dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
        "\n",
        "print(\"Number transactions train dataset: \", len(X_train))\n",
        "print(\"Number transactions test dataset: \", len(X_test))\n",
        "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
        "\n",
        "# Undersampled dataset\n",
        "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
        "                                                                                                   ,y_undersample\n",
        "                                                                                                   ,test_size = 0.3\n",
        "                                                                                                   ,random_state = 0)\n",
        "print(\"\")\n",
        "print(\"Number transactions train dataset: \", len(X_train_undersample))\n",
        "print(\"Number transactions test dataset: \", len(X_test_undersample))\n",
        "print(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "32bb1df7-52ee-634b-5f7b-d605952bf508"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cross_validation import KFold, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3e6c004b-e10e-ebe5-d1a9-cd958993600f"
      },
      "outputs": [],
      "source": [
        "def printing_Kfold_scores(x_train_data,y_train_data):\n",
        "    fold = KFold(len(y_train_data),5,shuffle=False) \n",
        "    print(fold)\n",
        "\n",
        "    # Different C parameters\n",
        "    c_param_range = [0.01,0.1,1,10,100]\n",
        "\n",
        "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
        "    results_table['C_parameter'] = c_param_range\n",
        "\n",
        "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
        "    j = 0\n",
        "    for c_param in c_param_range:\n",
        "        print('-------------------------------------------')\n",
        "        print('C parameter: ', c_param)\n",
        "        print('-------------------------------------------')\n",
        "        print('')\n",
        "\n",
        "        recall_accs = []\n",
        "        for iteration, indices in enumerate(fold,start=1):\n",
        "\n",
        "            # Call the logistic regression model with a certain C parameter\n",
        "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n",
        "\n",
        "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
        "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
        "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
        "\n",
        "            # Predict values using the test indices in the training data\n",
        "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
        "\n",
        "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n",
        "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
        "            recall_accs.append(recall_acc)\n",
        "            print('Iteration ', iteration,': recall score = ', recall_acc)\n",
        "\n",
        "        # The mean value of those recall scores is the metric we want to save and get hold of.\n",
        "        results_table.ix[j,'Mean recall score'] = np.mean(recall_accs)\n",
        "        j += 1\n",
        "        print('')\n",
        "        print('Mean recall score ', np.mean(recall_accs))\n",
        "        print('')\n",
        "\n",
        "    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n",
        "    \n",
        "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
        "    print('*********************************************************************************')\n",
        "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
        "    print('*********************************************************************************')\n",
        "    \n",
        "    return best_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "67d70f0f-9366-c058-73f5-c58cec313f7a"
      },
      "outputs": [],
      "source": [
        "best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample)\n",
        "best_c = printing_Kfold_scores(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8ad06235-c97b-d018-d423-b77a14df5490"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        1#print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "956f8761-15bf-7daf-390d-1b27c2df8636"
      },
      "outputs": [],
      "source": [
        "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
        "# dataset\n",
        "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
        "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
        "y_pred_undersample = lr.predict(X_test_undersample.values)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}