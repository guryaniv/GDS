{"cells":[{"metadata":{"_uuid":"aaf56504186223b4d8e3f15a00ab7d813fe84cbd"},"cell_type":"markdown","source":"In this kernel, the rows of train data is **10000**.\n\nIt is interesting to find the smaller train data has better performance on LB.\n\nif the rows of train data exceed 40w, LB would be less than 0.1 in this kernel."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport gc\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom kaggle.competitions import twosigmanews\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom sys import getsizeof\nprint(os.listdir(\"../input\"))\nenv = twosigmanews.make_env()\nprint('Done!')\n\nisTestCode = True\n\nprint('Is a test?\\t', isTestCode)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8474caf1a3da779dd03861dc2a164267552b99ef"},"cell_type":"markdown","source":"In order to reduce memory usage, the market data and the news data are **downsampled**. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"sample_frac = 0.1\nmarket_data, news_data = env.get_training_data()\nmarket_data = market_data.sample(frac=sample_frac, random_state=2018)\nnews_data = news_data.sample(frac=sample_frac, random_state=2018)\ngc.collect()\nprint('shape of market_data:\\t', market_data.shape)\nprint('shape of news_data:\\t', news_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c795d1ed92c3b19c07834c759fb27d08040adfa"},"cell_type":"code","source":"print('memory(market_data):\\t', getsizeof(market_data))\nprint('memory(news_data):\\t', getsizeof(news_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"869ca6d856ef7bb6e130f27bf5315bc764c13fc1"},"cell_type":"markdown","source":"Only **1w** rows are selected."},{"metadata":{"trusted":true,"_uuid":"1a4f7f4e987a5fe063fe5ff6f9a0a844899ceab6"},"cell_type":"code","source":"if isTestCode:\n    market_data = market_data.tail(10000)\n    news_data = news_data.tail(50000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c927e04a5a886c9931cfba005ddf4dc7cd6d1a47"},"cell_type":"markdown","source":"Agg features\nrefer to **A simple model - using the market and news data**\n\n[https://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data](http://)"},{"metadata":{"trusted":true,"_uuid":"3fc15cd52e863c038c9375aed67b2f5ac0802af7"},"cell_type":"code","source":"news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"091958372c82ee3f1900ed610c798619a0ae6292"},"cell_type":"markdown","source":"A function to join market data and news data\nparam:\n\n    market_data: data of market\n    news_data: data of news\n    news_cols_agg: the features \n    agg_day: the size of window\n    "},{"metadata":{"trusted":true,"_uuid":"26df3407e0d586ff2d6d0904ea9f2fe2912e3dea"},"cell_type":"code","source":"\ndef market_news_join(market_data, news_data, news_cols_agg=news_cols_agg, agg_day=7):\n    news_cols = ['time', 'assetName'] + sorted(news_cols_agg.keys())\n    \n    # delete the unnecessary data\n    news_data = news_data[news_data.time<=market_data.time.max()]\n    news_data = news_data[news_data.time>=(market_data.time.min()-np.timedelta64(agg_day,'D'))]\n    \n#     print('shape(news in joinFun):\\t', news_data.shape)\n#     print('memory(news in joinFun):\\t', getsizeof(news_data))\n    \n#     print('max_time:\\t', news_data.time.max())\n#     print('min_time:\\t', news_data.time.min())\n    \n    merge_data = pd.merge(market_data, news_data[news_cols], how='inner', on='assetName', suffixes=(['_market','_news']))\n    \n    # window limit\n    merge_data = merge_data[np.array((merge_data.time_market-merge_data.time_news).dt.days<agg_day)&np.array((merge_data.time_market-merge_data.time_news).dt.days>=0)]\n    group_data = merge_data.groupby(by=['assetName']).agg(news_cols_agg)\n    \n    group_data.columns = ['_'.join(col).strip() for col in group_data.columns.values]\n    \n    result = market_data.join(group_data, on=['assetName'])\n    \n    # release memory\n    del merge_data\n    del group_data\n#     print(gc.collect())\n    \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c57148b45fdf06a237aaee90b6c3f08c37daf223"},"cell_type":"code","source":"# sorted by time\nnews_data = news_data[['time', 'assetName']+sorted(news_cols_agg.keys())]\nnews_data = news_data.sort_values(by=['time'])\nmarket_data = market_data.sort_values(by=['time'])\nprint(gc.collect())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc0dab07cdb63034a92a7339a4d478d139be7a77"},"cell_type":"markdown","source":"The raw data are divided into some slices to reduce memory usage"},{"metadata":{"trusted":true,"_uuid":"2b812bbc98af7e2c8143a376a405bac06e163dd9"},"cell_type":"code","source":"slice_list = []\nslice_size = 100000\nfor i in range(int(market_data.shape[0]/slice_size)+1):\n    slice_data = market_news_join(market_data.iloc[(i*slice_size):((i+1)*slice_size)], news_data)\n    slice_list.append(slice_data)\n    \ndel market_data\ndel news_data\nprint('release memory:\\t', gc.collect())\ndataTrain = pd.concat(slice_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baf76713306e6fc032b7287e334c3d8e63852554"},"cell_type":"markdown","source":"delete some features"},{"metadata":{"trusted":true,"_uuid":"b7d41a24019abf1ea951aa276de38aa8b8e0f2c9"},"cell_type":"code","source":"feature_used_to_train = dataTrain.columns.tolist()\nfeature_used_to_train.remove('time')\nfeature_used_to_train.remove('assetName')\nfeature_used_to_train.remove('assetCode')\nfeature_used_to_train.remove('returnsOpenNextMktres10')\nfeature_used_to_train.remove('universe')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11a562b385821102f03698a6f0fa13d2811b3d8a"},"cell_type":"markdown","source":"A metric function\nparam:\n\n    test_pre: array\n    test_data: pd.DataFrame"},{"metadata":{"trusted":true,"_uuid":"435fdf2e6b96a2b8a8e0b7aa25bd0b5d5b760d58"},"cell_type":"code","source":"def metricFun(test_pre, test_data):\n    data_score = pd.DataFrame({'time':test_data.time, 'val':test_data.returnsOpenNextMktres10 * test_pre})\n    day_sum = data_score.groupby(by=['time']).sum()\n    score = day_sum.val.mean()/day_sum.val.std()\n    return score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4407345859eeb6e83c5cce46ec3b2c08882c5ea6"},"cell_type":"markdown","source":"5-fold and trainning\n\nlocal cv is very bad"},{"metadata":{"trusted":true,"_uuid":"ed37164b9e3366042e73d8ce63e442d4caa32afe"},"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=2018)\nscorelist = []\nmodellist = []\nfor train_index, test_index in kf.split(dataTrain):\n    train_x = dataTrain.iloc[train_index][feature_used_to_train]\n    train_y = dataTrain.iloc[train_index].returnsOpenNextMktres10\n    \n    test_x = dataTrain.iloc[test_index][feature_used_to_train]\n    test_y = dataTrain.iloc[test_index]\n    \n    # the rows whose universe==1 are selected\n    test_x = test_x[test_y.universe==1.0]\n    test_y = test_y[test_y.universe==1.0]\n    \n    model = lgb.LGBMClassifier(num_leaves=127, random_state=2018)\n    train_y = 2*(train_y>0).astype(int)-1\n    model.fit(train_x, train_y)\n    \n    test_pre = model.predict_proba(test_x)[:, 1]\n    test_pre = 2*test_pre-1\n    \n    score = metricFun(test_pre, test_y)\n    \n    print('-'*50)\n    print('accuracy:\\t', (np.sum((test_pre>0) == (test_y.returnsOpenNextMktres10>0)))/test_y.shape[0] )\n    print('score:\\t', score)\n    scorelist.append(score)\n    modellist.append(model)\n    \nprint('mean of scores:\\t', np.array(scorelist).mean())\nprint('std of scores:\\t', np.array(scorelist).std())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48978909a6db66593041aabd7412ce130d8521cd"},"cell_type":"markdown","source":"plot feature importances"},{"metadata":{"trusted":true,"_uuid":"19d77944066268b110c4d6a85e98edb05842a7e0"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfeature_importances = np.sum([model.feature_importances_ for model in modellist], axis=0)\nfeature_importances = pd.Series(feature_importances)\nfeature_importances.index = feature_used_to_train\nplt.figure(figsize=(20, 5))\nfeature_importances.sort_values(ascending=False).plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36b091558c90afea7e467fe077c501652d0b3398"},"cell_type":"markdown","source":"reduce memory usage"},{"metadata":{"trusted":true,"_uuid":"6259995af2df6fe511a735e7abebd9ebc54ff6e4"},"cell_type":"code","source":"del train_x\ndel train_y\n\ndel test_x\ndel test_y\n\ndel dataTrain\n\nprint('release:\\t', gc.collect())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e17474dc130bcc462b9a0efd12ee6ea606ae183"},"cell_type":"code","source":"days = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51e8e84033b8c8b3d78fba4dc3dc2cd79af6e623"},"cell_type":"markdown","source":"generate output file"},{"metadata":{"trusted":true,"_uuid":"756ad87829b0e84e8a0889319d64639ad18361aa"},"cell_type":"code","source":"count=0\nfor (market_data_pre, news_data_pre, template) in days:\n    prelist = []\n    \n    data_pre = market_news_join(market_data_pre, news_data_pre)[feature_used_to_train]\n    \n    for model in modellist:\n        out_pre = model.predict_proba(data_pre.apply(np.float64))[:, 1]\n        out_pre = 2*out_pre-1\n        \n#         out_pre = model.predict(test_x)\n        \n        \n        prelist.append(out_pre)\n    \n    template.confidenceValue=np.clip(np.mean(np.array(prelist), axis=0), -1, 1)\n    env.predict(template)\n    count += 1\n    print(count, end=' ')\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56af81132d17ae3f3c7382a081b51e144f41418c"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}