{"nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["# **Character-Level Classification using CNNs**\n", "\n", "For general background on this topic, check out the following link.\n", "\n", "[Best Practices for Document Classifcation with Deep Learning](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/)\n", "\n", "I am implementing the network described in this [paper](https://arxiv.org/pdf/1606.01781.pdf) which was also done by someone else in [this kernel](https://www.kaggle.com/robwec/character-level-author-identification-with-cnns) which I suggest you also check out. I found [this kernel](https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights) utilizing a CNN approach helpful as well. \n", "\n", "Overall, this is most likely not the best approach for this particular dataset but may be of use for others in the future tackling larger datasets. The authors in the paper linked above describe how this method does better on larger datasets. "], "metadata": {}, "cell_type": "markdown"}, {"source": ["import numpy as np\n", "import pandas as pd"], "outputs": [], "metadata": {"_cell_guid": "1a332ad8-f695-48b8-af44-e494ca305864", "collapsed": true, "_uuid": "9599317bad9e8c308ebe26e0ea1949ca2c0a12f2"}, "execution_count": null, "cell_type": "code"}, {"source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["train.head()"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["test.head()"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["x_train = train.iloc[:,1].values\n", "y_train = train.iloc[:,2].values"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["The following block of code is adapted from [this repository](https://github.com/johnb30/py_crepe).\n", "\n", "This will take the sentences as input and turn each of them into a sparse character array. \n", "\n", "The max length is equivalent to the length of the sentence. 250 should be more than enough and you most likely can go even lower. "], "metadata": {}, "cell_type": "markdown"}, {"source": ["import string\n", "\n", "maxlen = 250\n", "alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n", "                list(string.punctuation) + ['\\n'])\n", "vocab_size = len(alphabet)\n", "check = set(alphabet)\n", "\n", "vocab = {}\n", "reverse_vocab = {}\n", "for ix, t in enumerate(alphabet):\n", "    vocab[t] = ix\n", "    reverse_vocab[ix] = t\n", "\n", "input_array = np.zeros((len(x_train), maxlen, vocab_size))\n", "for i, sentence in enumerate(x_train):\n", "    counter = 0\n", "    sentence_array = np.zeros((maxlen, vocab_size))\n", "    chars = list(sentence.lower().replace(' ', ''))\n", "    for c in chars:\n", "        if counter >= maxlen:\n", "            pass\n", "        else:\n", "            char_array = np.zeros(vocab_size, dtype=np.int)\n", "            if c in check:\n", "                ix = vocab[c]\n", "                char_array[ix] = 1\n", "            sentence_array[counter, :] = char_array\n", "            counter +=1\n", "    input_array[i, :, :] = sentence_array"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["print(np.shape(input_array))"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["Following is a One Hot Encoding of the labels (the authors)"], "metadata": {}, "cell_type": "markdown"}, {"source": ["from sklearn.preprocessing import LabelBinarizer\n", "\n", "one_hot = LabelBinarizer()\n", "y_train = one_hot.fit_transform(y_train)\n", "y_train"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["The following is the Keras architecture described in the paper linked to in the beginning. They describe 9-layer, 17-layer, 29-layer, and 49-layer variations but they all proceed in the same general manner as below. \n", "\n", "Do note that this method utilizes batch normalization instead of dropout. "], "metadata": {}, "cell_type": "markdown"}, {"source": ["from keras.models import Sequential\n", "from keras.layers import Dropout, Flatten, Dense, BatchNormalization, Activation\n", "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n", "\n", "model = Sequential()\n", "model.add(Conv1D(filters=64, kernel_size=3, padding='same', input_shape=(250, 69)))\n", "model.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n", "model.add(BatchNormalization())\n", "model.add(Activation('relu'))\n", "model.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n", "model.add(BatchNormalization())\n", "model.add(Activation('relu'))\n", "model.add(MaxPooling1D(pool_size=3, strides=2))\n", "model.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n", "model.add(BatchNormalization())\n", "model.add(Activation('relu'))\n", "model.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n", "model.add(BatchNormalization())\n", "model.add(Activation('relu'))\n", "model.add(MaxPooling1D(pool_size=3, strides=2))\n", "model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n", "model.add(BatchNormalization())\n", "model.add(Activation('relu'))\n", "model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n", "model.add(BatchNormalization())\n", "model.add(Activation('relu'))\n", "model.add(MaxPooling1D(pool_size=3, strides=2))\n", "model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n", "model.add(BatchNormalization())\n", "model.add(Activation('relu'))\n", "model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n", "model.add(BatchNormalization())\n", "model.add(Activation('relu'))\n", "model.add(GlobalMaxPooling1D())\n", "model.add(Dense(2048, activation='relu'))\n", "model.add(Dense(2048, activation='relu'))\n", "model.add(Dense(3, activation='softmax'))\n", "\n", "model.summary()"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["model.fit(input_array, y_train, validation_split=0.2, epochs=40, batch_size=64, verbose=2)"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["Data Preparation of the Test Set"], "metadata": {}, "cell_type": "markdown"}, {"source": ["x_test = test.iloc[:,1].values"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["test_array = np.zeros((len(x_test), maxlen, vocab_size))\n", "for i, sentence in enumerate(x_test):\n", "    counter = 0\n", "    sentence_array = np.zeros((maxlen, vocab_size))\n", "    chars = list(sentence.lower().replace(' ', ''))\n", "    for c in chars:\n", "        if counter >= maxlen:\n", "            pass\n", "        else:\n", "            char_array = np.zeros(vocab_size, dtype=np.int)\n", "            if c in check:\n", "                ix = vocab[c]\n", "                char_array[ix] = 1\n", "            sentence_array[counter, :] = char_array\n", "            counter +=1\n", "    test_array[i, :, :] = sentence_array"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["print(np.shape(test_array))"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["y_test = model.predict_proba(test_array)"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["ids = test['id']"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": ["submission = pd.DataFrame(y_test, columns=['EAP', 'HPL', 'MWS'])\n", "submission.insert(0, \"id\", ids)\n", "submission.to_csv(\"submission.csv\", index=False)"], "outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code"}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "file_extension": ".py", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3"}}}