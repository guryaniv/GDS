{"cells":[{"metadata":{"_uuid":"5adee266ed7114e032a06426d0a89d3c02ba6601"},"cell_type":"markdown","source":"# Some 2 cents of mine"},{"metadata":{"_uuid":"f3ce49ff95787d2f0246419dfb514cfe1392ee75"},"cell_type":"markdown","source":"I did some feature engineering and correlation analyasis on news data compared to the target 'returnsOpenNextMktres10', but didn't find strong correlations.\n\nNow, I believe that the values from market_train_df already incorporate news."},{"metadata":{"_uuid":"101a4cc2960efae072e5237982a5265c7c1e2aa9"},"cell_type":"markdown","source":"# Import Environment"},{"metadata":{"trusted":true,"_uuid":"3c0a5b5fddb1b2b068a1af2eae570a7068806825","scrolled":true},"cell_type":"code","source":"# Import some libraries\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # graphing\nimport os\nfrom datetime import datetime, timedelta # Used to subtract days from a date\nimport seaborn as sb\n\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.\n\n# Import environment\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64111417bbfb09179e3790b3a3bae7b76571107f"},"cell_type":"markdown","source":"# Load Initial Market & News Train Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import training dataset\n(market_train_df, _) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0875f55ff3996de29c62e3fbb758f567095ee37d"},"cell_type":"markdown","source":"# View Correlation Heatmap Pre-processing"},{"metadata":{"trusted":true,"_uuid":"25894b74197f92de31ca00c8563b237c44abc5e9","scrolled":false},"cell_type":"code","source":"# Heat map with market_train_df\nC_mat = market_train_df.corr()\nfig = plt.figure(figsize=(15,15))\nsb.heatmap(C_mat,vmax=0.5,square=True,annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb7901eb58195ccb5e42379265d1f6ea682c05d4"},"cell_type":"markdown","source":"# View Correlation Heatmap after removing outliers"},{"metadata":{"trusted":true,"_uuid":"e6509dde08f51ea90c9475976f4dc31355a7a9cf","scrolled":false},"cell_type":"code","source":"def remove_outlier(df,column_list,lower_percentile,upper_percentile):\n    for i in range(len(column_list)):\n        #upper_bound = np.percentile(df[column_list[i]],upper_percentile)\n        #lower_bound = np.percentile(df[column_list[i]],lower_percentile)\n        df = (df[(df[column_list[i]]<np.percentile(df[column_list[i]],upper_percentile)) & (df[column_list[i]]>np.percentile(df[column_list[i]],lower_percentile))])\n    return df\n#outlier_removal_list = ['returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevRaw10','returnsOpenPrevRaw10','returnsClosePrevMktres10','returnsOpenPrevMktres10','returnsOpenNextMktres10']\noutlier_removal_list = [ 'returnsClosePrevRaw1',\n                         'returnsOpenPrevRaw1',\n                         'returnsClosePrevRaw10',\n                         'returnsOpenPrevRaw10',\n                         'returnsOpenNextMktres10']\n\nmarket_data_no_outlier = remove_outlier(market_train_df,outlier_removal_list,2,98)\nprint(\"Number of data decreased from \",len(market_train_df['returnsOpenNextMktres10']),\" to \",len(market_data_no_outlier['returnsOpenNextMktres10']))\n\nC_mat = market_data_no_outlier.corr()\nfig = plt.figure(figsize=(15,15))\nsb.heatmap(C_mat,vmax=0.5,square=True,annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4edb2cd6da094c701dd57ca8ec05a8abc4631a6a"},"cell_type":"markdown","source":"# Process Market Data - Drop Rows with NaN Values"},{"metadata":{"trusted":true,"_uuid":"f7525fb117b423cd847886c769a6b78af55ad873"},"cell_type":"code","source":"# proces data\ndef process_merged_data(df):\n    # Drop rows with NaN values\n    df = df.dropna()\n    # Let's choose our features#\n    features = ['time','returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevRaw10','returnsOpenPrevRaw10','returnsClosePrevMktres10','returnsOpenPrevMktres10']\n    x = df[features]\n    y = df[['time','returnsOpenNextMktres10']]\n    return x,y\n\nmarket_data_no_outlier,market_data_no_outlier_target = process_merged_data(market_data_no_outlier)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e31ea33455f1e3730de6c7dd3725a7f662840da1"},"cell_type":"markdown","source":"# Standardize Data"},{"metadata":{"trusted":true,"_uuid":"6c0faa7bc74162a70c300a7638d228f4ef284ca5"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ndef scale_data(df,features):\n    scaler = StandardScaler()\n    df[features]=scaler.fit_transform(df[features])\n    return df\nfeatures = ['returnsClosePrevRaw1',\n         'returnsOpenPrevRaw1',\n         'returnsClosePrevMktres1',\n         'returnsOpenPrevMktres1',\n         'returnsClosePrevRaw10',\n         'returnsOpenPrevRaw10',\n         'returnsClosePrevMktres10',\n         'returnsOpenPrevMktres10']    \nmarket_data_no_outlier_scaled = scale_data(market_data_no_outlier,features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f297a03d63461df968a52f1b2a5873af04e11b3"},"cell_type":"markdown","source":"# View Final Correlation Heatmap before Training"},{"metadata":{"trusted":true,"_uuid":"a5c76ea56f67aee5c5ab6daf40092fda106e0636","scrolled":false},"cell_type":"code","source":"# Heat map with merged_data\nfeatures = ['returnsClosePrevRaw1',\n         'returnsOpenPrevRaw1',\n         'returnsClosePrevMktres1',\n         'returnsOpenPrevMktres1',\n         'returnsClosePrevRaw10',\n         'returnsOpenPrevRaw10',\n         'returnsClosePrevMktres10',\n         'returnsOpenPrevMktres10']\ntemp_show = market_data_no_outlier_scaled[features]\ntemp_show['target']=market_data_no_outlier_target['returnsOpenNextMktres10']\nC_mat = temp_show.corr()\nfig = plt.figure(figsize=(15,15))\nsb.heatmap(C_mat,vmax=0.5,square=True,annot=True)\nplt.show()\ndel temp_show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9248d94520abe1e1db7994c31264b64804315ed2"},"cell_type":"markdown","source":"# Split Data for Training and Validation(Testing) & Choose Final Features"},{"metadata":{"trusted":true,"_uuid":"9ca1d4ec3e6d3e0157ac93bd1ba7b430844c0ad9","scrolled":true},"cell_type":"code","source":"# Splits data for training. Takes out 30 days worth of data between training and validation set to prevent data leakage\ndef split_train_test(x,y,test_size):    \n    # Splits data as specified test_size and creates a gap of 30 days between train and test. This helps data leakage so that the model doesn't know the future when training\n    X_train = x[x['time']<(x['time'][int(len(x)*(1-test_size))]-timedelta(days=30))]\n    y_train = y[y['time']<(y['time'][int(len(x)*(1-test_size))]-timedelta(days=30))]\n    X_test = x[x['time']>x['time'][int(len(x)*(1-test_size))]]\n    y_test = y[y['time']>y['time'][int(len(y)*(1-test_size))]]   \n    # Final Features to be used\n    #features = ['returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevRaw10','returnsOpenPrevRaw10','returnsClosePrevMktres10','returnsOpenPrevMktres10'] \n    features = ['returnsClosePrevRaw10','returnsOpenPrevRaw10','returnsClosePrevMktres10']\n    \n    X_train1 = X_train[features].copy()\n    y_train1 = y_train['returnsOpenNextMktres10'].copy()\n    train_time = y_train['time']\n    \n    X_test1 = X_test[features].copy()\n    y_test1 = y_test['returnsOpenNextMktres10'].copy()\n    test_time = y_test['time']\n    return X_train1,X_test1,y_train1,y_test1,train_time,test_time\n\nX_train,X_test,y_train,y_test,train_time,test_time = split_train_test(market_data_no_outlier_scaled,market_data_no_outlier_target,0.1)\nprint(\"Test data percentage : {} %\".format(len(X_test)/(len(X_train)+len(X_test))*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3952045a856f6498a14ece73eb7ca8ed0477cb3c","scrolled":true},"cell_type":"markdown","source":"# Deep Neural Network"},{"metadata":{"_uuid":"49f523059817aee3b55d40c3ae58a1179f7d2a37"},"cell_type":"markdown","source":"### Define Model"},{"metadata":{"trusted":true,"_uuid":"99ff1e2b8892f41280057f16bf31041d0b286b4a","scrolled":true},"cell_type":"code","source":"# Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,BatchNormalization,Input\nfrom keras.optimizers import Adam\n\n# Initialize Model\nmodel = Sequential()\n# Input layer & hidden layer\nmodel.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\nmodel.add(Dense(32,activation='relu'))\n# Output layer\nmodel.add(Dense(1))\n# Compile the architecture and view summary\noptimizer = Adam(lr=0.001)\nmodel.compile(optimizer=optimizer, loss='mean_squared_error')\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83cc01ab400a45ccad5cc5273f5e173aa64f823c"},"cell_type":"markdown","source":"### Early stopping callback :"},{"metadata":{"trusted":true,"_uuid":"1ddf6c720121b5d7da38cea583939800bcd1f67a"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint,EarlyStopping\n\n# checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n# checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_acc', verbose = 1, save_best_only = True, mode ='auto')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto',restore_best_weights=True)\ncallbacks_list = [early_stopping]\n#callbacks_list = [checkpoint,early_stopping]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fcd84a1693df89bb041dcdbdea6a5451e8fa012"},"cell_type":"markdown","source":"### Train the model"},{"metadata":{"trusted":true,"_uuid":"8b317e8bf7e362fb0a07282ea4648c4eca2306ff","scrolled":true},"cell_type":"code","source":"model.fit(x=X_train.values,y=y_train.values, epochs=20,shuffle=True,validation_data=(X_test.values, y_test.values),callbacks=callbacks_list)# validation_split=0.2)#) #, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a9fda4f8ee2f88644fc212b15c7a94790664012"},"cell_type":"markdown","source":"### Sanity Check. Real vs Pred values"},{"metadata":{"trusted":true,"_uuid":"688285bf8e7a563406cda52fc0411ca879dfee1b"},"cell_type":"code","source":"data = {'y_real':y_test[:20],'y_pred':(model.predict(X_test.values[:20])).reshape(1,-1)[0]}\npd.DataFrame(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"446fecbd32ac9d01de1f9da1d6270366ac8fa8bc"},"cell_type":"markdown","source":"### Simple ConfidenceValue Creation Function from Prediction Values"},{"metadata":{"trusted":true,"_uuid":"65f99f9dbb7fdbb8135600ae195a1ca899a0710f","scrolled":true},"cell_type":"code","source":"def make_my_prediction(x):\n    my_pred = (model.predict(x)).reshape(1,-1)[0]\n    my_pred[my_pred>0]=1\n    my_pred[my_pred<0]=-1\n    return my_pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0320f6022662cdeb72ca58effa592a57893b3cf4"},"cell_type":"markdown","source":"### Sigma Score"},{"metadata":{"trusted":true,"_uuid":"f36104488d9ee0519b80095d3d2e716b54f5365f"},"cell_type":"code","source":"# sigma_score function is considered as a custom evaluation metric for xgboost\n# example of how custom evaluation function is incorporated into xgboost's training can be found here : https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py\ndef sigma_score(preds,dval,df):\n    \n    # get y_target values\n    labels = dval\n    # call time parameter to be used for grouping, so that we can add x_t values for each day\n    df_time = df\n    \n    #calculate x_t and score as specified by the competition\n    x_t = pd.Series(preds*labels)\n    x_t_sum = x_t.groupby(df_time).sum()    \n    score = (x_t_sum.mean())/(x_t_sum.std())\n    return 'sigma_score', round(score,5)\n\nmy_pred_test = make_my_prediction(X_test.values)\nprint(\"test : \",sigma_score(my_pred_test,y_test,test_time))\n\nmy_pred_train = make_my_prediction(X_train.values)\nprint(\"train : \",sigma_score(my_pred_train,y_train,train_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"027f5e56c82218897f686314ab504cc7fbcbebd4"},"cell_type":"markdown","source":"|# 10.) For Final Submission"},{"metadata":{"trusted":true,"_uuid":"c4d492fc6fc9cd71610738e5a70b7b1728622fdf"},"cell_type":"code","source":"for (market_obs_df, _, predictions_template_df) in env.get_prediction_days():  \n    features = ['returnsClosePrevRaw10','returnsOpenPrevRaw10','returnsClosePrevMktres10']\n    market_obs_df_scaled = scale_data(market_obs_df,features)    \n    x_submission = market_obs_df_scaled[features].copy()\n    # fill in NaN values with mean of rest of the values\n    for i in range(len(features)):\n         x_submission[features[i]]= x_submission[features[i]].fillna(x_submission[features[i]].mean())\n    predictions_template_df['confidenceValue'] = make_my_prediction(x_submission)\n    env.predict(predictions_template_df)\n    del x_submission\nprint('Done!')\n# Write submission file    \nenv.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}