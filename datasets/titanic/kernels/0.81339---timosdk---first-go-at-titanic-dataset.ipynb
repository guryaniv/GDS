{"cells":[{"metadata":{"_uuid":"44836da7ba2591b0e6eec15439475e2cfb31b097","_cell_guid":"79920262-b0fa-4034-aab5-8490cf594c87"},"cell_type":"markdown","source":"# Analysis of the Titanic data set with Python\n\nAfter completing the excellent intro to machine learning course on Udacity (https://classroom.udacity.com/courses/ud120) I decided to test the teachings on some introductory datasets. This Titanic dataset contains a lot of interesting data - but can get quite morbid as we are trying to predict who lives and who dies. \n\nThe following feature engineering borrows heavily from many of the other great kernels on this data set and I try to reference througout whenever I borrow something directly - but I also try to advance and go beyond the work in these other kernels. \n\n# Python Setup\n\n**Load the libraries and the data**\n\nLoad the libraries that we need and the data that we need.\n\nI smash together the training and the test dataset into one big frame so I only have to do the transformations and feature engineering once. This also gets around the issue of creating training and test set specific means and averages - when really both datasets should be transformed in the same way.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"30247518bbd41bea08d0cc31f491c2a92b110131","_cell_guid":"57ef30cf-5db3-4d98-9523-7332d02e3d39","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport re\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import svm, neighbors, naive_bayes\nfrom sklearn import model_selection\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn import cross_validation\n\n# plotting setup\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nplt.rcParams['figure.figsize'] = [14.0, 6.0]\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# training data\ndata_train = pd.read_csv(\"../input/train.csv\")\n\n# test data\ndata_test = pd.read_csv(\"../input/test.csv\")\n\n# combine test and train into one dataframe - can always split again by ['Survived']\ndata_full = pd.concat([data_train, data_test])\nprint('Full dataset columns with null values: \\n', data_full.isnull().sum())\n\n# show us some representative data.\ndata_full.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae19f076cce99e3ceed8e7f29f1e4d6ad1af7104","_cell_guid":"4397bca0-56ec-4645-a48d-9d9c8c7c6d93"},"cell_type":"markdown","source":"# Analyze the data and features\n\nData and feature analysis will be performed on the trainig data set when we need to see patterns in survival since survival data is missing in the test set. For general range analysis of the data I use the full dataset.\n\n### Quick analysis of data\n\nTo see what we are up against here are the probablities of surviving in general as well as split out for men and women. Based on this it seems prudent that the first engineered feature should hold information about gender.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"87d3d57dd0758fcc78cad879dcc7c66b19358fc2","scrolled":true,"_cell_guid":"f59128fb-6bfa-47c9-a59d-29c732dfa769","trusted":true},"cell_type":"code","source":"print(\"Chance of surviving in training set:\", data_train['Survived'].sum()/len(data_train['Survived']))\nprint(\"Chance of surviving if you are a male:\",data_train.loc[data_train['Sex'] == 'male']['Survived'].sum()/data_train['Sex'].value_counts()['male'])\nprint(\"Chance of surviving if you are a female:\",data_train.loc[data_train['Sex'] == 'female']['Survived'].sum()/data_train['Sex'].value_counts()['female'])\n\ndata_full['IsFemale'] = 1\ndata_full.loc[data_full['Sex'] == 'male', 'IsFemale'] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98c1b6034687bb7110cfc1737e4ba45f2709a03a","_cell_guid":"0f42b04e-315c-46c1-98d4-53c761cf49d9"},"cell_type":"markdown","source":"### Name length\n\nCrazy as it sounds name length seems to correlate with survival. This is partially due to women being listed as Mrs. Husband with their names in parentheses. And we know that women had better chances of survival.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1d46ac7b0d018a3f986ff11171b9c97f32e34c2d","_cell_guid":"f016e30e-bc28-4642-9fcb-edb85722dffa","trusted":true},"cell_type":"code","source":"data_full[\"NameLength\"] = data_full[\"Name\"].apply(lambda x: len(x))\n\nfig = plt.figure(figsize=(15,8))\nax=sns.kdeplot(data_full.loc[(data_train['Survived'] == 0), 'NameLength'], color='gray',shade=True,label='dead', bw=3)\nax=sns.kdeplot(data_full.loc[(data_train['Survived'] == 1), 'NameLength'], color='g',shade=True,label='alive', bw=3)\nplt.title('Name Length Distribution - Survived vs Non Survived', fontsize = 25)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15)\nplt.xlabel(\"Name Length\", fontsize = 15)\n\nprint(\"Longest Name:\", data_full.loc[data_full['NameLength']==data_full['NameLength'].max()].Name.values[0])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76888e5f53e1a696611abe3de733977b271ed590","_cell_guid":"e6a950db-ce1d-456d-b0e2-cf1d036063f6"},"cell_type":"markdown","source":"### Titles\n\nSome people have used titles to get information about social status. There is some information encoded in the title for sure. Probably not much for Mr - the only information there is that you are a male and maybe lower social status. For females there is Miss and Mrs - this encodes marital status so this offers information above just gender. Then there is the long tail of other titles. The ones that occur only twice or less it seems pointless to try and train on those titles. However, it does look like Master, Dr, and Rev have interesting survival distributions so these titles are probably valuable for creating predictions. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"8e676e4f78c8fb2b7945ae278a1230fb9f202e77","_cell_guid":"231b6b6d-95b2-4b94-8462-e2bea68c31c5","trusted":true},"cell_type":"code","source":"test_titles = data_train['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nprint(test_titles.value_counts())\n\nprint(\"-\"*10)\n\n# The most common ones are tied to gender and age and the less common ones are so uncommon that they make \n# little sense to encode - could encode as common vs fancy - but likely the fancy ones are also in high\n# class and or expensive ticket.\n\ntitle = 'Mr'\nprint(\"Chance of surviving if you are a \", title, \":\", data_train.loc[test_titles == title]['Survived'].sum()/test_titles.value_counts()[title])\n\ntitle = 'Miss'\nprint(\"Chance of surviving if you are a \", title, \":\", data_train.loc[test_titles == title]['Survived'].sum()/test_titles.value_counts()[title])\n\ntitle = 'Mrs'\nprint(\"Chance of surviving if you are a \", title, \":\", data_train.loc[test_titles == title]['Survived'].sum()/test_titles.value_counts()[title])\n\ntitle = 'Master'\nprint(\"Chance of surviving if you are a \", title, \":\", data_train.loc[test_titles == title]['Survived'].sum()/test_titles.value_counts()[title])\n\ntitle = 'Dr'\nprint(\"Chance of surviving if you are a \", title, \":\", data_train.loc[test_titles == title]['Survived'].sum()/test_titles.value_counts()[title])\n\ntitle = 'Rev'\nprint(\"Chance of surviving if you are a \", title, \":\", data_train.loc[test_titles == title]['Survived'].sum()/test_titles.value_counts()[title])\n\nprint(\"-\"*10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7123f3b7ec57c4afe6db232e90679bbfa7880fa3","_cell_guid":"b93ffa2f-7002-426a-a835-ee9dea044dc7"},"cell_type":"markdown","source":"We will encode the titles in the full dataset and just keep the 6 most frequent tiltles - the rest will be named fancy.  Titles will be used for imputing age later.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e364b6b645f1fe959b11317f3a541a2d697a8a21","_cell_guid":"f98e74aa-4d0d-48e4-9ee0-e3e75501f303","trusted":true},"cell_type":"code","source":"# get title from name\ndata_full['Titles'] = data_full['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n# fix these simple ones\ndata_full['Titles'] = data_full['Titles'].replace('Mlle', 'Miss')\ndata_full['Titles'] = data_full['Titles'].replace('Ms', 'Miss')\ndata_full['Titles'] = data_full['Titles'].replace('Mme', 'Mrs')\n\n# get a list of the 6 most frequent titles\nmostfrequenttitles = data_full['Titles'].value_counts().nlargest(6).keys()\n\n# if your title is not in the top 6 you are a fancy person\ndata_full.loc[(data_full['Titles'].isin(mostfrequenttitles)==False), 'Titles'] = \"Fancy\"\n\n# here is the value counts for the full dataset\nprint (\"Title frequencies\\n\", data_full['Titles'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7569b3f6e08395ab7d942ef6da455aecc12f48ab","_cell_guid":"51be5401-488b-4a64-ac20-ad75d5449d01","trusted":true},"cell_type":"code","source":"# create dummies from titles\ndummies = pd.get_dummies(data_full['Titles'])\ndata_full = pd.concat([data_full, dummies], axis = 1)\n\ndata_full.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4b29c7d80e684091b458151c5457bb507920163","_cell_guid":"a3e3c344-309d-47df-89bf-708754f22af9"},"cell_type":"markdown","source":"### Age\n\nFrom the age vs survival kde plot below it is looks like having age bins that are around 15 years in size would adequately divide the various ages into bins that have different chances of survival. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"9dcd9b51c87d7c31caac2d39890b114a698e46bc","_kg_hide-output":false,"_cell_guid":"af338a5f-a377-4196-be30-51b0df91d5dc","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,8))\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 0) & (data_train['Age'].isnull() == False), 'Age'], color='gray',shade=True,label='dead', bw=3)\nax=sns.kdeplot(data_train.loc[(data_train['Survived'] == 1) & (data_train['Age'].isnull() == False), 'Age'], color='g',shade=True, label='survived', bw=3)\nax=sns.kdeplot(data_full.loc[(data_train['Age'].isnull() == False), 'Age'], color='b',shade=False, label='full dataset', bw=3)\nplt.title('Age Distribution Survived vs Non Survived', fontsize = 25)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15)\nplt.xlabel(\"Age\", fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48fef52c210f783e1efd2dd20963834b3e722890","_cell_guid":"3c44eacc-e589-4b6e-b5b3-49fcde6a5dac"},"cell_type":"markdown","source":"People have been creating bins to avoid having age as a continuous variable and instead have it as a discrete variable. \n\nHowever, people have been using pd.cut for this and doing the cuts on the respective datasets. This would be fine if the max and min ages were the same for the test and trainig sets - but they are not. As can be seen below the cut creates different boundaries for these two datasets.\n\nFor my implementation I force a min and a max by inserting these at the end before I start pd.cutting. I also use labels so we can get these age bins as int values in one go. I set max age to 120 and use a 8 total bins to create 15 year bins.\n\nInterestingly it looks like the main effect of age is in combination with gender - since young males are the only ones that appear to do better than the general average of their gender. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"734f1bcedda101bbf68b8a02210ea1a48ccc0809","_cell_guid":"e1688bac-d6ea-48fd-98ef-09a0544d1ce2","trusted":true},"cell_type":"code","source":"# For demonstration purposes only - cutting on the separated set gives rise to different and nonsensical cutoffs:\npd.cut(data_train['Age'], bins = 5)\n#[(0.34, 16.336] < (16.336, 32.252] < (32.252, 48.168] < (48.168, 64.084] < (64.084, 80.0]]\npd.cut(data_test['Age'], bins = 5)\n#[(0.0942, 15.336] < (15.336, 30.502] < (30.502, 45.668] < (45.668, 60.834] < (60.834, 76.0]]\n\n# Now we will create age bins for the full dataset\nages = data_full['Age']\nages = ages.append(pd.Series([0, 80]))\nbins = pd.cut(ages, bins = 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\n\ndata_full['AgeBin'] = bins[:-2].astype(float)\n\nfig = plt.figure(figsize=(15,8))\nsns.pointplot(x='AgeBin', y='Survived', ci=95.0, hue = 'Sex', data = data_full, dodge=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"febfdb8a4c4ce8462729f0a21f758aa4b4be08ca","_cell_guid":"7ca0ba35-5f56-40f6-877e-0365a4633302"},"cell_type":"markdown","source":"From all this it looks like it would be good to create a dummy for kids - since agebin does not really provide a whole lot of differentiation except in the agebin=1 category","outputs":[],"execution_count":null},{"metadata":{"_uuid":"acd0296e231fe4c2fc1d3b9b7279f1c6a091e7f1","_cell_guid":"a4a008a1-5567-4254-8082-befbd6d9a363","collapsed":true,"trusted":true},"cell_type":"code","source":"data_full['IsKid'] = 0\ndata_full.loc[data_full['AgeBin'] == 1, 'IsKid'] = 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f049ea9eef3efd0c04696b1fd274f892e37e47a2","_cell_guid":"d9f5e837-8c95-43c4-ac13-d8c7d3a565bc"},"cell_type":"markdown","source":"** Dealing with missing values in the age feature **\n\nAge is the feature that has the most missing values after the cabin feature. In this case I want to fill the agebin with the mode of the agebin based on the title of the person. This especially seems to make sense for Master/Mr and Mrs/Miss where the title allows us to extract information about age based instead of just picking at random.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"be0c63c70929645daff331d6ea9ecd87f03a520a","_cell_guid":"41c872b1-111d-4a9e-8414-9b14004ac80b","trusted":true},"cell_type":"code","source":"# iterate through titles and fill in missing values\ntitles = data_full['Titles'].value_counts().keys()\nfor title in titles:\n    age_mode = data_full.loc[data_full['Titles']==title, 'AgeBin'].mode().values[0]\n    data_full.loc[(data_full['Titles']==title) & (data_full['AgeBin'].isnull()), 'AgeBin'] = age_mode\n\n# now convert agebin to int\ndata_full['AgeBin'] = data_full['AgeBin'].astype(int)\n\nprint('Full dataset columns with null values: \\n', data_full.isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a5a37823b8ca94ec8c41816f6a1c5f16073f48a","_cell_guid":"07107dbd-6a4b-4249-b2bc-119db79bbcd9"},"cell_type":"markdown","source":"### Family\n\nThe creation of family size variable based on sibsp & parch has been discussed elsewhere. I will use the same methods and also create an is alone feature.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"4265aba41e8f22d720dc71f040a88bd268ab73ff","_cell_guid":"35f1fc71-06d0-4fdd-b27b-4706f8b523a7","collapsed":true,"trusted":true},"cell_type":"code","source":"# Family size\ndata_full['FamilySize'] = data_full['SibSp'] + data_full['Parch'] + 1\n\n# Is this person alone on the ship\ndata_full['IsAlone'] = 0\ndata_full.loc[data_full['FamilySize'] == 1, 'IsAlone'] = 1\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1529000539bc37402821c82d400fca51d5622933","_cell_guid":"1c211720-93d2-436f-8ce5-1f40f6f8e755"},"cell_type":"markdown","source":"### Fare\n\nFirst thing is to fill in the missing value for Mr. Storey. Will do this based on median fare for people travelling on the same class as Mr. Storey (3rd class).","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1b4c3ab92882490e055498fcfadc71d8e0edff39","_cell_guid":"9d314a45-4612-407c-abc5-5f12f87bd47a","trusted":true},"cell_type":"code","source":"# the guy is on 3rd class so lets use the median fare of 3rd class to fill this value\ndata_full.loc[data_full['Fare'].isnull(), 'Fare'] = data_full.loc[data_full['Pclass'] == 3, 'Fare'].median()\n\ndata_full.loc[data_full['Ticket']=='3701']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46989b5e068a669b310f511f5017625062720782","_cell_guid":"171a727e-983b-4540-a0a8-9350460d256b"},"cell_type":"markdown","source":"Again we will create fare bins based on the data. Interestingly there is a huge number of unique fares for this dataset - it seems like prices were more fluid back in the day!","outputs":[],"execution_count":null},{"metadata":{"_uuid":"900c187a618d692f145acf61a02ef6b572376f56","_cell_guid":"e1ba0ce1-5adc-4ada-a166-a14f75068537","trusted":true},"cell_type":"code","source":"print(\"Number of unique fares:\", data_full['Fare'].nunique())\n\nplt.figure(figsize=[15,6])\n\nplt.subplot(121)\nplt.boxplot(x=data_full['Fare'], showmeans = True, meanline = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(122)\nplt.hist(x = data_full['Fare'], color = ['g'], bins = 8)\nplt.title('Fare Histogram')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers - log scale')\nplt.yscale('log', nonposy='clip')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"604975bd31f97869af8db26049f07856e5ea1d72","_cell_guid":"13121a35-27d8-4613-a056-37a942bb10c0","trusted":true},"cell_type":"code","source":"# add fare bins to the full dataset\ndata_full['FareBin'] = pd.qcut(data_full['Fare'], 6, labels = [1, 2, 3, 4, 5, 6]).astype(int)\n\nfig = plt.figure(figsize=(15,8))\nsns.pointplot(x='FareBin', y='Survived', ci=95.0, hue = 'Sex', data = data_full, dodge=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1764b41bf3ee5a61e45e676d80c59f44119bdfcc","_cell_guid":"ca6f83df-45c4-46aa-8759-38b6be9b07a6"},"cell_type":"markdown","source":"### Tickets\n\nWe will look at tickets and the survival of people booked on the same tickets. There is a grave danger of over fitting here - if we look at dead men and alive females - these are the most common cases - we can easily overfit on the data. So instead we will concentrate on the less common cases and see how these play out: dead women and kids and alive males. \n\nThis analysis is heavily borrowed from: https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster/code\n\nThe graphs below visualize what happens here:\n* Dead female on ticket - then all the men on ticket dies also\n* Man survives - then all females survive\n* Kid dies - both men and women on ticket die","outputs":[],"execution_count":null},{"metadata":{"_uuid":"943bbdc05a79543cb1c7fa7a6f34d7be79bf250a","_cell_guid":"2d25b4bd-ecdb-4b45-bbb6-087fe6859e1d","collapsed":true,"trusted":true},"cell_type":"code","source":"# create table with counts of people per ticket\nticket_table = pd.DataFrame(data_full[\"Ticket\"].value_counts())\nticket_table.rename(columns={'Ticket': 'People_on_ticket'}, inplace = True)\n\nticket_table['Dead_female_on_ticket'] = data_full.Ticket[(data_full.AgeBin > 1) & (data_full.Survived < 1) & (data_full.IsFemale)].value_counts()\nticket_table['Dead_female_on_ticket'].fillna(0, inplace=True)\nticket_table.loc[ticket_table['Dead_female_on_ticket'] > 0, 'Dead_female_on_ticket'] = 1\nticket_table['Dead_female_on_ticket'] = ticket_table['Dead_female_on_ticket'].astype(int)\n\nticket_table['Dead_kid_on_ticket'] = data_full.Ticket[(data_full.AgeBin == 1) & (data_full.Survived < 1)].value_counts()\nticket_table['Dead_kid_on_ticket'].fillna(0, inplace=True)\nticket_table.loc[ticket_table['Dead_kid_on_ticket'] > 0, 'Dead_kid_on_ticket'] = 1\nticket_table['Dead_kid_on_ticket'] = ticket_table['Dead_kid_on_ticket'].astype(int)\n\nticket_table['Alive_male_on_ticket'] = data_full.Ticket[(data_full.AgeBin > 1) & (data_full.Survived > 0) & (data_full.IsFemale == False)].value_counts()\nticket_table['Alive_male_on_ticket'].fillna(0, inplace=True)\nticket_table.loc[ticket_table['Alive_male_on_ticket'] > 0, 'Alive_male_on_ticket'] = 1\nticket_table['Alive_male_on_ticket'] = ticket_table['Alive_male_on_ticket'].astype(int)\n\n# unique identifiers for tickets with more than 2 people\nticket_table[\"Ticket_id\"]= pd.Categorical(ticket_table.index).codes\nticket_table.loc[ticket_table[\"People_on_ticket\"] < 3, 'Ticket_id' ] = -1\n\n# merge with the data_full\ndata_full = pd.merge(data_full, ticket_table, left_on=\"Ticket\",right_index=True,how='left', sort=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e98ed3ee2bcd9634487c488e12d090c703cdb82","_cell_guid":"36e0e450-88fb-426c-b7ad-9372a1125cc7","trusted":true},"cell_type":"code","source":"fig, (maxis1, maxis2, maxis3) = plt.subplots(1, 3,figsize=(15,6))\nsns.pointplot(x='Dead_female_on_ticket', y='Survived', ci=95.0, hue = 'Sex', data = data_full, dodge=True, ax = maxis1)\nsns.pointplot(x='Alive_male_on_ticket', y='Survived', ci=95.0, hue = 'Sex', data = data_full, dodge=True, ax = maxis2)\nsns.pointplot(x='Dead_kid_on_ticket', y='Survived', ci=95.0, hue = 'Sex', data = data_full, dodge=True, ax = maxis3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77ecab06ca67b67a319059158adf921f8b1f8a29","_cell_guid":"fa5b45c7-fe3d-46bf-8eea-7037d5e93023"},"cell_type":"markdown","source":"### Last names\nSimilar to tickets - last names can be used to group people. For this one we want to make sure that we only count dead or alive if this person has family size > 1.\n\nVisualize: This is similar to tickets - but since it is possible for two Andersons and Smiths to not be related the correlations here are not as strong as they were for people on the same tickets.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e9bccc9b5699ec8e5cc5459eed46d9eb142f8b4d","_cell_guid":"a73e5bc3-ca4b-43c1-a5c8-1e8f85451854","collapsed":true,"trusted":true},"cell_type":"code","source":"data_full['Lastname'] = data_full[\"Name\"].apply(lambda x: x.split(',')[0].lower())\n\nlastname_table = pd.DataFrame(data_full[\"Lastname\"].value_counts())\nlastname_table.rename(columns={'Lastname': 'People_w_lastname'}, inplace = True)\n\nlastname_table['Dead_mom_w_lastname'] = data_full.Lastname[(data_full.AgeBin > 1) & (data_full.Survived < 1) & (data_full.FamilySize > 1) & (data_full.IsFemale)].value_counts()\nlastname_table['Dead_mom_w_lastname'].fillna(0, inplace=True)\nlastname_table.loc[lastname_table['Dead_mom_w_lastname'] > 0, 'Dead_mom_w_lastname'] = 1\nlastname_table['Dead_mom_w_lastname'] = lastname_table['Dead_mom_w_lastname'].astype(int)\n\nlastname_table['Dead_kid_w_lastname'] = data_full.Lastname[(data_full.AgeBin == 1) & (data_full.Survived < 1) & (data_full.FamilySize > 1)].value_counts()\nlastname_table['Dead_kid_w_lastname'].fillna(0, inplace=True)\nlastname_table.loc[lastname_table['Dead_kid_w_lastname'] > 0, 'Dead_kid_w_lastname'] = 1\nlastname_table['Dead_kid_w_lastname'] = lastname_table['Dead_kid_w_lastname'].astype(int)\n\nlastname_table['Alive_dad_w_lastname'] = data_full.Lastname[(data_full.AgeBin > 1) & (data_full.Survived > 0) & (data_full.IsFemale==False) & (data_full.FamilySize > 1)].value_counts()\nlastname_table['Alive_dad_w_lastname'].fillna(0, inplace=True)\nlastname_table.loc[lastname_table['Alive_dad_w_lastname'] > 0, 'Alive_dad_w_lastname'] = 1\nlastname_table['Alive_dad_w_lastname'] = lastname_table['Alive_dad_w_lastname'].astype(int)\n\n# unique identifiers for lastname with more than 2 people\nlastname_table[\"Lastname_id\"]= pd.Categorical(lastname_table.index).codes\nlastname_table.loc[lastname_table[\"People_w_lastname\"] < 3, 'Lastname_id' ] = -1\n\n# merge with the data_full table\ndata_full = pd.merge(data_full, lastname_table, left_on=\"Lastname\",right_index=True,how='left', sort=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2ee6959a1e77d65525717bba98f1dab92df3ebd","_cell_guid":"24d2f3cf-94f1-4261-a4ca-6fdcf4d08c61","trusted":true},"cell_type":"code","source":"fig, (maxis1, maxis2, maxis3) = plt.subplots(1, 3,figsize=(15,6))\nsns.pointplot(x='Dead_mom_w_lastname', y='Survived', ci=95.0, hue = 'Sex', data = data_full, dodge=True, ax = maxis1)\nsns.pointplot(x='Alive_dad_w_lastname', y='Survived', ci=95.0, hue = 'Sex', data = data_full, dodge=True, ax = maxis2)\nsns.pointplot(x='Dead_kid_w_lastname', y='Survived', ci=95.0, hue = 'Sex', data = data_full, dodge=True, ax = maxis3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4553b8b495401105b9de0efe905e7d3193616c50","_cell_guid":"475f226f-660a-4224-b21f-7120195425de"},"cell_type":"markdown","source":"### Embarkment\n\nThis visualization was borrowed from https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy. However, in that kernel the author did not specify the hue_order and was suppressing warnings so the middle plot endded up with male and female colors reversed. With the hue_color specified it is clear that men do not do well regardless of their port of origin. However, the patterns are different based on port so there is some interesting information here!","outputs":[],"execution_count":null},{"metadata":{"_uuid":"af074ff87ea989cd45175d7c2cb491df8c2f90ca","_cell_guid":"2c30cbd3-c3be-4b03-ae3b-6d921198b502","trusted":true},"cell_type":"code","source":"e = sns.FacetGrid(data_train, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep', order = [1, 2, 3], hue_order = ['male', 'female'])\ne.add_legend()\n\n# fill embared with mode\ndata_full['Embarked'] = data_full['Embarked'].fillna(data_full['Embarked'].mode().values[0])\n\n# add dummies for the embarked as there is no linear relationship here\ndummies = pd.get_dummies(data_full['Embarked'], prefix = 'embrk')\ndata_full = pd.concat([data_full, dummies], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80ea8b5a4251133fd1763b9951453a3005685df8","_cell_guid":"f002756a-0abb-417d-ba31-0021138f45d9"},"cell_type":"markdown","source":"### Decks and cabins\n\nThe Cabin feature seems like it may have some interesting information - like maybe people on certain decks could get to the lifeboats faster or there may be some social class information encoded in the Cabin feature. Cabin consist of a letter followed by a number - on in the case of the $$500 ticket consist of 3 letter/number separated by space. The letter is presumably the deck and the number the cabin number on that deck. Also, there is a large proportion of people where we have no cabin information.\n\nInterestingly it does not look like the deck significanlty affects survival - but the survival rate of people with an assigned cabin certainly is better than the average - so we will just encode a 'has cabin' feature.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"24a8d8e119d5b992db8cef02a56eada6acc80264","_cell_guid":"36c85486-8ef0-4471-9441-608e9635a46d","trusted":true},"cell_type":"code","source":"data_full['Deck'] = data_full['Cabin'].str[:1]\n\n# setup has cabin\ndata_full['HasCabin'] = 1\ndata_full.loc[data_full['Cabin'].isnull(), 'HasCabin'] = 0\n\nfig = plt.figure(figsize=(15,8))\nsns.pointplot(x='Deck', y='Survived', ci=95.0, hue = 'Sex', data = data_full, dodge=True, order = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'])\n\n# transform decks to integers\ndata_full['Deck'] = pd.Categorical(data_full['Deck'].fillna('N')).codes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19eab41df0e8226e796d3d0481b0fedf95cdabec","_cell_guid":"4b2b5e8b-988d-4b48-8597-7d08d101aee7"},"cell_type":"markdown","source":"### Pclass\n\nWe will just make dummies from this. There is relationship between class and social status but usually 1st class is not just one better than 2nd class - they are usually in totally different stratospheres - so I do not want to force a linear relationship on this data.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"8666aa870d52da810312d7e18d2e3210305cb0f4","_cell_guid":"0ede643e-312f-483c-bdb6-138af7d0306b","trusted":true},"cell_type":"code","source":"# create dummies from Pclass\n\ndummies = pd.get_dummies(data_full['Pclass'], prefix = 'class')\ndata_full = pd.concat([data_full, dummies], axis = 1)\ndata_full.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d90ef98b1585cc960d5216e7d62db2209d4e89e7","_cell_guid":"faed05e6-2062-424c-bdcc-08a3892053ea"},"cell_type":"markdown","source":"# Prepare data for machine learning\n\n### Get the NumPy arrays\n\nNow we are ready to extract the np arrays from the data frames. First we need to select what deatures to use for the modeling and then we are going to grab features from train and test data and setup our labels.\n\nI end up dropping most of the lastname features - I found these to lead to overfitting and not be as informative as wether people were travelling on the same ticket. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"437f065ecc17b647c5641f7171daecdd9dfab733","_cell_guid":"8e189814-11b2-43b5-b3b5-7dc13f042bee","trusted":true},"cell_type":"code","source":"# get rid of superfluous columns\nfinal_full_fram = data_full.drop(['Name', 'Sex', 'SibSp', 'Parch', 'Ticket', \n                                  'Embarked', 'Titles', 'Cabin',\n                                  'Fare', 'Age', 'Ticket_id', 'Lastname',\n                                  'Lastname_id', 'People_w_lastname', \n                                  'Alive_dad_w_lastname', \n                                  'Rev', 'People_on_ticket', 'Fancy',\n                                  'Dr', \n                                 ], axis = 1)\n\n\n# Split the data back to test and train\ndata_train1 = final_full_fram[final_full_fram['Survived'].isnull() == False]\nprint('Train columns (', data_train1.shape[1], ') with null values:\\n', data_train1.isnull().sum())\n\nprint(\" -\"*20)\n\ndata_test1 = final_full_fram[final_full_fram['Survived'].isnull() == True]\nprint('Train columns (', data_test1.shape[1], ') with null values:\\n', data_test1.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"934b770026667a04561e2c19edd894ea4f406312","_cell_guid":"51e064b7-5569-481d-b174-7215e6e0e3f6","trusted":true},"cell_type":"code","source":"# make sure we only have numerical values in our dataframe\n#data_test1.info()\n\n#check ranges\ndata_test1.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10a65dbe0eae5681fa1750345fd7508e0f260a3b","_cell_guid":"a0524a3e-03f8-4556-ac84-0c6ba0eaf0c4","trusted":true},"cell_type":"code","source":"features = data_train1.columns\nfeatures = features.drop(['PassengerId', 'Survived'])\nprint (\"These are the features we will use for modeling:\\n\", features)\n\n# create the np arrays that we need for training and testing\nnp_train_features = data_train1.as_matrix(columns = features)\nprint (\"training features shape\", np_train_features.shape)\n\nnp_train_labels = data_train1['Survived']\nprint (\"training labels shape\", np_train_labels.shape)\n\nnp_test_features = data_test1.as_matrix(columns = features)\nprint (\"testing features shape\", np_test_features.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e52037d532d0c285ca50ff910764d548e4dec11c","_cell_guid":"0bea4405-05b6-4e4b-9cea-337202b45bd1"},"cell_type":"markdown","source":"### Feature scaling\n\nIt is always good to do feature scaling. This is not super necessary in this case since most of these are dummies or have low ranges (we binned the fares and the ages) ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"380fb29389ef30d7ad7a31bec7075563c6d4688a","_cell_guid":"272d7aab-34c0-4e5a-be3e-547a0e187c52","trusted":true},"cell_type":"code","source":"# fit scaler on full dataset\nscaler = StandardScaler()\nscaler.fit(np.concatenate((np_train_features, np_test_features), axis=0))\n\nprint(scaler.mean_)\n\nnp_train_features = scaler.transform(np_train_features)\nnp_test_features  = scaler.transform(np_test_features) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5405fac92acd312a98f8a5f3703c1e15d5eeb556","_cell_guid":"af12a685-0bc6-43df-9932-c204e02c41d3"},"cell_type":"markdown","source":"### Check on feature importances\n\nFor this I am getting the p-values for a general f_classifier using SelectKBest. The list of features that we drop earlier is based on p-values above 0.05 and also the check of feature importances done for the random forest regressor.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"9d8ad3008e0ddb5db45bec041a413eb26146784b","_cell_guid":"6a8ff60c-2612-4e23-bee4-b51b41818b42","trusted":true},"cell_type":"code","source":"selector = SelectKBest(f_classif, k=len(features))\nselector.fit(data_train1[features], np_train_labels)\n\nscores = selector.pvalues_\nindices = np.argsort(scores)[::1]\nprint(\"Features p-values :\")\nfor f in range(len(scores)):\n    print(\"%.3e %s\" % (scores[indices[f]],features[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ed0c67a477c37a75fa71c5240df22c794946c19","_cell_guid":"35cb8c34-5915-4000-aa98-e6fb307f755c","collapsed":true},"cell_type":"markdown","source":"# Learning using cross validation and grid search\n\nI use GridSearchCV to optimize hyperparameters. I use the default 3 fold cross validation (the CV part of GridSearchCV) since on the training data set (891 samples) that will lead to around 300 samples per fold - so it will train on 600 samples and test on 300. \n\n+---test---+----------+----------+\n\n+----------+---test---+----------+\n\n+----------+----------+---test---+\n\n## Support Vector Machine\n\nFirst algorithm is support vector machine. \n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"ab416fd4c298b2fb8ccb8a6541d015f0e54232d7","_cell_guid":"082262c6-301f-4a95-96a3-76b6343d8fba","trusted":true},"cell_type":"code","source":"svc = svm.SVC()\nparameters = {'kernel': ['linear', 'rbf'],\n              'C':[1, 2, 4, 8]}\n\nclf_svm = model_selection.GridSearchCV(svc, parameters, n_jobs = 2)\nclf_svm.fit(np_train_features, np_train_labels)\n\nprint(\"Best score {} came from {}\".format(clf_svm.best_score_, clf_svm.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fae04a2e94400901f925b0dc8c7eda8dc40dc13","_cell_guid":"1c71c6e7-206d-4d03-819a-b255f66abe6f"},"cell_type":"markdown","source":"## Random Forest\n\nNext we will try a random forest classifier. This one will generate slightly different scores on consecutive runs due to the random part of the random forest. \n\nEven the optimal hyperparameters are sightly different on consecutive runs.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c99c1dc88a7077d3dc0c783ac3494dd35b3dda34","_cell_guid":"4e63d80f-a673-4577-a9da-c83b31807bc7","trusted":true},"cell_type":"code","source":"rf_regr = RandomForestClassifier()\nparameters = {\"min_samples_split\" :[4]\n            ,\"n_estimators\" : [50, 100]\n            ,\"criterion\": ('gini','entropy')\n             }\n\nclf_rf = model_selection.GridSearchCV(rf_regr, parameters, n_jobs = 2)\nclf_rf.fit(np_train_features, np_train_labels)\n\nprint(\"Best score {} came from {}\".format(clf_rf.best_score_, clf_rf.best_params_))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbfcbe8bd99d69ab475c37a44344630749646b47","_cell_guid":"1a210b67-8e64-4319-9b85-deffef4d7658"},"cell_type":"markdown","source":"## eXtreme Gradient Boosting\n\nXGBoost is a fast and gives quite good results. http://xgboost.readthedocs.io/en/latest/model.html\n","outputs":[],"execution_count":null},{"metadata":{"_kg_hide-input":false,"_uuid":"fc3d4777e8ffef7ae43039551ade13df173bd6b8","_kg_hide-output":false,"_cell_guid":"7aea1633-381b-470b-b3fa-2ccc8d55574a","trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\nparameters = {'learning_rate': [0.05, 0.1, .25, 0.5], \n              'max_depth': [1,2,4,8], \n              'n_estimators': [50, 100]\n             }\n\nclf_xgb = model_selection.GridSearchCV(xgb, parameters, n_jobs = 2)\nclf_xgb.fit(np_train_features, np_train_labels)\n\nprint(\"Best score {} came from {}\".format(clf_xgb.best_score_, clf_xgb.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dcc7022c5522d64c3ef3ffb6edcaefed8a4edae","_cell_guid":"8bb27a05-95d1-4ef5-aa8f-9e920d4fd6eb"},"cell_type":"markdown","source":"## Naive Bayes\n\nFor some reason this one gives real poor results.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e851a1aefa7f7425b307c3c8052e6f5d20e21e1f","_cell_guid":"1014c6f6-aa73-4d3d-b4c0-f64afd6330d7","trusted":true},"cell_type":"code","source":"nb = naive_bayes.GaussianNB()\nparameters = {'priors': [None]}\n\nclf_nb = model_selection.GridSearchCV(nb, parameters)\nclf_nb.fit(np_train_features, np_train_labels)\n\nprint(\"Best score {} came from {}\".format(clf_nb.best_score_, clf_nb.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a27fdcab6c1464b937d76055b2d16296cc06bd3","_cell_guid":"ee0cd31d-d279-4ae2-a9e5-e62f33a284fa"},"cell_type":"markdown","source":"## K-Nearest Neighbors\n\nThis is a fast algorithm especially on a small dataset like this. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"4a25cccc276b4d8aee17b4a7f4523f09ecbecc40","_cell_guid":"20d6c4f9-2dc2-48b6-8db2-d86fc4d782ca","trusted":true},"cell_type":"code","source":"knn = neighbors.KNeighborsClassifier()\nparameters = {'n_neighbors': [1,2,3,4,5,6,7],\n              'weights': ['uniform', 'distance'], \n              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n             }\n\nclf_knn = model_selection.GridSearchCV(knn, parameters, n_jobs = 2)\nclf_knn.fit(np_train_features, np_train_labels)\n\nprint(\"Best score {} came from {}\".format(clf_knn.best_score_, clf_knn.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b88e206afbe1348bf71b673252c3dcd98fb40c17","_cell_guid":"e18a68ab-5d63-4da1-a1af-de293a227300"},"cell_type":"markdown","source":"## Gradient Boosting\nAnother algorithm","outputs":[],"execution_count":null},{"metadata":{"_uuid":"7bfa5cbafdde6060765b57951a8e77cf2dde743b","_cell_guid":"4e9161d1-089e-4a5e-bbb0-da397d6016ef","trusted":true},"cell_type":"code","source":"gbk = GradientBoostingClassifier()\n\nparameters = {\n            'learning_rate': [0.05, 0.1],\n            'n_estimators': [50, 100], \n            'max_depth': [2,3,4,5]   \n             }\n\nclf_gbk = model_selection.GridSearchCV(gbk, parameters, n_jobs = 2)\nclf_gbk.fit(np_train_features, np_train_labels)\n\nprint(\"Best score {} came from {}\".format(clf_gbk.best_score_, clf_gbk.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff04337689900115cced5cd00e94f699ac5eb1ec","_cell_guid":"e72bbcbd-843f-46ba-8626-7e600b3166ea"},"cell_type":"markdown","source":"## Another random forest\n\nThis time we will run with class weights - we know that 1502 out of 2224 died - so survived weight should be 0.325 and dead weight should be 0.675.\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"63f67ec0d7ebf0f396e6194a45d12c248996dfa3","_cell_guid":"a240b86d-ed76-4a78-8e98-c142b25f35a8","trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=50, min_samples_split=4, class_weight={0:0.675,1:0.325})\n\n# for this one we will use kfold validation since we already have hyper parameters specified\nkf = cross_validation.KFold(np_train_labels.shape[0], n_folds=3, random_state=42)\n\nscores = cross_validation.cross_val_score(rfc, np_train_features, np_train_labels, cv=kf)\nprint(\"Accuracy on 3-fold XV: %0.3f (+/- %0.2f)\" % (scores.mean()*100, scores.std()*100))\n\nrfc.fit(np_train_features, np_train_labels)\nscore = rfc.score(np_train_features, np_train_labels)\nprint(\"Accuracy on full set: %0.3f\" % (score*100))\n\nprint(\" *\"*15)\nprint(\"Feature importances in this model:\")\n\nimportances = rfc.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(len(features)):\n    print(\"%0.2f%% %s\" % (importances[indices[f]]*100, features[indices[f]]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"852f6ff14b213b3fa36d875fe197ba8d8150cd82","_cell_guid":"1873ea91-27ed-42f1-a4db-ffbd06a3b3f9"},"cell_type":"markdown","source":"## Note on Accuracy\n\nWhoooo those are some nice accuracies! 98% accuracy from XGB! We nailed it. \n\nBut not so fast - since we have coded in dead and alive males, females, and kids we have coded the labels into the features! Thus the within sample accuracy on crossvalidation is nolonger going to be reflective of out of sample accuracy - the test data will not perform as well since these features will only help people that are on tickets are also present in the training set. \n\nBy definition the training set people are all in the training set so if we really wanted to calculate accuracy for the algorithms we would need to partition the training set _before_ feature engineering and make sure that the partition of tickets and lastnames matches the partition seen in the train/test set. This could be done with an elaborate pipeline if someone is up for it.\n\nAlso -  we are most likely in overfitting territory here...","outputs":[],"execution_count":null},{"metadata":{"_uuid":"4f8e1eed3d75eb7080d70e4218c0384407e64a9a","_cell_guid":"22396578-ef73-48a0-96a3-31083433c366"},"cell_type":"markdown","source":"# Submission\n\nIn this example it looked like our support vector machine was doing the best job on the cross validated data so that is what we will use for the first submission.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"38cd0d0452739e7a6306c6c5b93c9f98e042095e","_cell_guid":"5c85724c-e8a0-4307-a297-c83dee863007","trusted":true},"cell_type":"code","source":"data_test1.loc[:, 'Survived-SVM'] = clf_svm.best_estimator_.predict(np_test_features)\nprint(\"SVM predicted number of survivors:\", data_test1['Survived-SVM'].sum())\n\ndata_test1.loc[:, 'Survived-RF'] = clf_rf.best_estimator_.predict(np_test_features)\nprint(\"RF predicted number of survivors:\", data_test1['Survived-RF'].sum())\n\ndata_test1.loc[:, 'Survived-RFC'] = rfc.predict(np_test_features)\nprint(\"RFC predicted number of survivors:\", data_test1['Survived-RFC'].sum())\n\ndata_test1.loc[:, 'Survived-XGB'] = clf_xgb.best_estimator_.predict(np_test_features)\nprint(\"XGB predicted number of survivors:\", data_test1['Survived-XGB'].sum())\n\n# use the RFC data\ndata_test1.loc[:, 'Survived'] = data_test1['Survived-RFC'].astype(int)\n\nsubmit = data_test1[['PassengerId', 'Survived']]\nsubmit.to_csv(\"submit.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7606d687dc5c41c213d1a373a414e56beea9e93e","_cell_guid":"b43c0bb1-f1a1-4752-a824-3c9ee119b9de"},"cell_type":"markdown","source":"## Future work\n\nWe have some nice models here and some nice results - it may be possible to do some voting on these results to get the best prediction.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"930e64878f0ad0abdc221826fb9a23c353ce6352","_cell_guid":"43646ac5-f754-4364-a405-c76248e58b44","trusted":true},"cell_type":"code","source":"data_test1[['Survived-SVM', 'Survived-RF', 'Survived-RFC', 'Survived-XGB', 'Survived']]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69ab0d3297fa793bf31f2570b618df9497cd3e31","_cell_guid":"3973a32b-62e2-4851-9ada-ee2c3b55d717","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}