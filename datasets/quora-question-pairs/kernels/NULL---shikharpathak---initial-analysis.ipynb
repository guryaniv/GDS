{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "339acabe-1d23-8782-87d1-07eda2f5049d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4846c062-7350-566d-0693-181f5c6925d1"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bd6bb321-860d-d0fe-059d-622034a0f835"
      },
      "source": [
        "### Import plotting libraries ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "68fe4408-e57e-15c1-4a88-27d9ae7ee7e1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(color_codes=True)\n",
        "sns.set_style(\"white\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7bf75bf2-242a-7d19-53bc-1c1fad387ae2"
      },
      "source": [
        "### Import modeling libraries ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9271cd55-625c-6e72-9152-bf0a64b8b4d3"
      },
      "outputs": [],
      "source": [
        "import sklearn.ensemble\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, log_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a2e8c695-e12e-4697-c89c-af2354285ea5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e9425502-b733-74a2-da78-769cf1e31917"
      },
      "outputs": [],
      "source": [
        "train_set = pd.read_csv('../input/train.csv')\n",
        "\n",
        "test_set = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ef49c77b-34b5-bd65-fb48-79f539b693ff"
      },
      "outputs": [],
      "source": [
        "metrics = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "20af0d11-5ec0-f98f-f132-b7a25bdf4fa6"
      },
      "outputs": [],
      "source": [
        "print('There are {} records in train'.format(train_set.shape[0]))\n",
        "print('There are {} records in train'.format(test_set.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8adc5737-61ba-2855-ba75-2fdd6378e667"
      },
      "outputs": [],
      "source": [
        "target = 'is_duplicate'\n",
        "ID = 'id'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4f5d9248-83b9-2ec5-ce33-c66c9ecb7362"
      },
      "source": [
        "### Find nulls in questions ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "961af815-d390-2657-d6e2-c9ed12695cf4"
      },
      "outputs": [],
      "source": [
        "def find_nulls(df, column):\n",
        "    res = df.ix[pd.isnull(df[column])]\n",
        "    percentages = []\n",
        "    percentages.append(len(res[column]))\n",
        "    percentages.append(df.shape[0] - res.shape[0])\n",
        "    percentages = pd.DataFrame(percentages, columns=[column], index=['Nulls', 'Non nulls'])\n",
        "    return percentages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60512980-382a-760e-4009-4c907070c30d"
      },
      "outputs": [],
      "source": [
        "def plot_bar_chart(data, column):\n",
        "    N = data.shape[0]\n",
        "    ind = np.arange(N)\n",
        "    width = 0.35\n",
        "    fig, ax = plt.subplots(figsize=(8,3))\n",
        "    rects = ax.bar(ind, data[column])\n",
        "    ax.set_xticks(ind + width / 2)\n",
        "    ax.set_xticklabels(data.index)\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
        "                '%d' % int(height),\n",
        "                ha='center', va='bottom')\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "20fa2ce5-ffb3-d531-3779-a109bfd5ed13"
      },
      "source": [
        "### Find nulls in question1 ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a583f5b-5318-0595-0fee-841fe864f631"
      },
      "outputs": [],
      "source": [
        "res = find_nulls(train_set, 'question1')\n",
        "res = pd.concat([res, find_nulls(train_set, 'question2')], axis=1)\n",
        "res = res.rename(columns={\n",
        "    'question1': 'train_q1',\n",
        "    'question2': 'train_q2'\n",
        "})\n",
        "res = pd.concat([res, find_nulls(test_set, 'question1')], axis=1)\n",
        "res = pd.concat([res, find_nulls(test_set, 'question2')], axis=1)\n",
        "res = res.rename(columns={\n",
        "    'question1': 'test_q1',\n",
        "    'question2': 'test_q2'\n",
        "})\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9ddcb242-6c74-19ee-2b3d-bfaa96cb9552"
      },
      "source": [
        "### Fill the NA's ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bfccc082-ab20-afb7-93da-23cdf861d9a1"
      },
      "outputs": [],
      "source": [
        "train_set['question1'] = train_set['question1'].fillna('')\n",
        "train_set['question2'] = train_set['question2'].fillna('')\n",
        "\n",
        "test_set['question1'] = test_set['question1'].fillna('')\n",
        "test_set['question2'] = test_set['question2'].fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "acb87ec2-70f3-7c55-1051-3fb52455917e"
      },
      "outputs": [],
      "source": [
        "word_counts = {}\n",
        "\n",
        "def find_word_counts(word_counts, tokenlist):\n",
        "    for token in tokenlist:\n",
        "        if token in word_counts:\n",
        "            word_counts[token] = word_counts[token] + 1\n",
        "        else:\n",
        "            word_counts[token] = 1\n",
        "    return word_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "647bf46d-d36b-11f2-c73f-343fec0bbede"
      },
      "source": [
        "### Use regex to find quoted words that can be converted to complete words  ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "711cc203-8720-a2b1-f9e3-038bd2b82a85"
      },
      "outputs": [],
      "source": [
        "train_set['q1_quotes'] = train_set['question1'].str.extract('(\\w+\\'\\w+)\\s.*')\n",
        "train_set['q2_quotes'] = train_set['question2'].str.extract('(\\w+\\'\\w+)\\s.*')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "337813be-a188-4789-5816-1914cf7d4892"
      },
      "source": [
        "### Pie chart to represent distributions of  sentence with quotes against unquotes ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2d3dfb19-af4c-8082-201e-aa4462b7964f"
      },
      "outputs": [],
      "source": [
        "quote_counts = []\n",
        "quote_counts.append(train_set.ix[pd.notnull(train_set['q1_quotes'])].shape[0])\n",
        "quote_counts.append(train_set.ix[pd.isnull(train_set['q1_quotes'])].shape[0])\n",
        "quote_counts = pd.DataFrame(quote_counts, columns=['counts'], \n",
        "                            index=['Quotes', 'Unquotes'])\n",
        "\n",
        "colors = ['gold', 'yellowgreen']\n",
        "explode = (0.1, 0)\n",
        " \n",
        "plt.pie(quote_counts['counts'], labels=quote_counts.index, colors=colors, \n",
        "        explode=(1, 0), autopct='%.1f%%',)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2f128cc-f893-cad8-63fc-9edb43d22b09"
      },
      "outputs": [],
      "source": [
        "quote_counts = []\n",
        "quote_counts.append(train_set.ix[pd.notnull(train_set['q2_quotes'])].shape[0])\n",
        "quote_counts.append(train_set.ix[pd.isnull(train_set['q2_quotes'])].shape[0])\n",
        "quote_counts = pd.DataFrame(quote_counts, columns=['counts'], \n",
        "                            index=['Quotes', 'Unquotes'])\n",
        "\n",
        "colors = ['lightblue', 'coral']\n",
        "explode = (0.1, 0)\n",
        " \n",
        "plt.pie(quote_counts['counts'], labels=quote_counts.index, colors=colors, \n",
        "        explode=(1, 0), autopct='%.1f%%',)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "85debfc6-7c16-124c-15b8-dd8de28ee75c"
      },
      "outputs": [],
      "source": [
        "re.match('^(\\w+\\'\\w+)\\s.*', \"What's causing someone to be jealous?\").group(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f923ac8-eeed-1e8d-38b3-966a68f240b2"
      },
      "outputs": [],
      "source": [
        "duplicates = train_set.ix[train_set['is_duplicate'] == 1]\n",
        "non_duplicates = train_set.ix[train_set['is_duplicate'] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9e507be-bc20-3d53-53e8-96916b26be04"
      },
      "outputs": [],
      "source": [
        "def find_matching_words(x):\n",
        "    wq1 = str(x['question1']).lower().split(' ')\n",
        "    wq2 = str(x['question2']).lower().split(' ')\n",
        "    matches = set(wq1).intersection(set(wq2))\n",
        "    return len(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1de480b0-6ca3-5a0b-7777-e6da0ed74903"
      },
      "outputs": [],
      "source": [
        "re.match('.*(\\?\\s).*', \"Is Kristen Stewart a bad actress?  Why or why not?\").group(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da534062-4a02-ea81-ed21-42a49b9109f8"
      },
      "outputs": [],
      "source": [
        "def find_bi_grams(question):\n",
        "    question = question.replace\n",
        "    tokens = question.lower().split(' ')\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    bigrams = []\n",
        "    \n",
        "    print(len(pos_tags))\n",
        "    for i in range(len(pos_tags)):\n",
        "        if i == len(pos_tags) - 1:\n",
        "            continue\n",
        "        bigrams.append((pos_tags[i][0], pos_tags[i+1][0]))\n",
        "        #if pos_tags[i][1] == 'JJ' and pos_tags[i+1][1] == 'NN':\n",
        "        #    bigrams.append((pos_tags[i][0], pos_tags[i+1][0]))\n",
        "    print('Done')\n",
        "    return bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "85509f42-afb3-d5cd-4139-e2a5f8cdac9d"
      },
      "outputs": [],
      "source": [
        "train_set['has_questionmark'] = train_set['question1'].str.extract('.*(\\?\\s).*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4bbb5b21-e629-9640-2534-5bac560b9f0b"
      },
      "outputs": [],
      "source": [
        "train_set.ix[pd.notnull(train_set['has_questionmark'])]['question1'].iloc[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c91714dc-b278-d6c0-7889-044c4a09b610"
      },
      "outputs": [],
      "source": [
        "train_set.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1138a6aa-fb84-da8e-2905-0d7c2882a3c0"
      },
      "outputs": [],
      "source": [
        "403717 / 404290"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ce8fc82b-b26d-4ba0-be59-d9a6d98bd121"
      },
      "outputs": [],
      "source": [
        "train_set['q1_bigrams'] = train_set['question1'].map(find_bi_grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1010c0b0-29ce-a913-03a8-6085d26e4ce2"
      },
      "outputs": [],
      "source": [
        "copy_frame = train_set.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "72c58922-4c67-dcf7-7ea6-14ada44e53a7"
      },
      "outputs": [],
      "source": [
        "def replace_q1_puncts(sentence):\n",
        "    sentence = sentence.replace('What\\'s', 'What is')\n",
        "    sentence = sentence.replace('What\\'re', 'What are')\n",
        "    sentence = sentence.replace('Who\\'s', 'Who is')\n",
        "    sentence = sentence.replace('who\\'re', 'who are')\n",
        "    sentence = sentence.replace('How\\'s', 'How is')\n",
        "    sentence = sentence.replace('don\\'t', 'do not')\n",
        "    sentence = sentence.replace('Don\\'t', 'Do not')\n",
        "    sentence = sentence.replace('can\\'t', 'can not')\n",
        "    sentence = sentence.replace('doesn\\'t', 'does not')\n",
        "    sentence = sentence.replace('does\\'t', 'does not')\n",
        "    sentence = sentence.replace('didn\\'t', 'did not')\n",
        "    sentence = sentence.replace('isn\\'t', 'is not')\n",
        "    sentence = sentence.replace('Isn\\'t', 'Is not')\n",
        "    sentence = sentence.replace('won\\'t', 'will not')\n",
        "    sentence = sentence.replace('haven\\'t', 'have not')\n",
        "    sentence = sentence.replace('aren\\'t', 'are not')\n",
        "    sentence = sentence.replace('hasn\\'t', 'has not')\n",
        "    sentence = sentence.replace('shouldn\\'t', 'should not')\n",
        "    sentence = sentence.replace('Shouldn\\'t', 'Should not')\n",
        "    sentence = sentence.replace('wouldn\\'t', 'would not')\n",
        "    sentence = sentence.replace('wasn\\'t', 'was not')\n",
        "    sentence = sentence.replace('couldn\\'t', 'could not')\n",
        "    sentence = sentence.replace('It\\'s', 'It is')\n",
        "    sentence = sentence.replace('that\\'s', 'that is')\n",
        "    sentence = sentence.replace('I\\'m', 'I am')\n",
        "    sentence = sentence.replace('I\\'ve', 'I have')\n",
        "    sentence = sentence.replace('I\\'ll', 'I will')\n",
        "    sentence = sentence.replace('you\\'ve', 'you have')\n",
        "    sentence = sentence.replace('you\\'re', 'you are')\n",
        "    sentence = sentence.replace('there\\'s', 'there is')\n",
        "    sentence = sentence.replace('they\\'re', 'they are')\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "43db6b84-013c-5301-4007-90978011f53f"
      },
      "outputs": [],
      "source": [
        "def replace_q2_puncts(sentence):\n",
        "    sentence = sentence.replace('What\\'s', 'What is')\n",
        "    sentence = sentence.replace('don\\'t', 'do not')\n",
        "    sentence = sentence.replace('Don\\'t', 'Do not')\n",
        "    sentence = sentence.replace('can\\'t', 'can not')\n",
        "    sentence = sentence.replace('I\\'m', 'I am')\n",
        "    sentence = sentence.replace('doesn\\'t', 'does not')\n",
        "    sentence = sentence.replace('you\\'ve', 'you have')\n",
        "    sentence = sentence.replace('didn\\'t', 'did not')\n",
        "    sentence = sentence.replace('I\\'ve', 'I have')\n",
        "    sentence = sentence.replace('isn\\'t', 'is not')\n",
        "    sentence = sentence.replace('you\\'re', 'you are')\n",
        "    sentence = sentence.replace('won\\'t', 'will not')\n",
        "    sentence = sentence.replace('they\\'re', 'they are')\n",
        "    sentence = sentence.replace('haven\\'t', 'have not')\n",
        "    sentence = sentence.replace('aren\\'t', 'are not')\n",
        "    sentence = sentence.replace('hasn\\'t', 'has not')\n",
        "    sentence = sentence.replace('shouldn\\'t', 'should not')\n",
        "    sentence = sentence.replace('wouldn\\'t', 'would not')\n",
        "    sentence = sentence.replace('wasn\\'t', 'was not')\n",
        "    sentence = sentence.replace('couldn\\'t', 'could not')\n",
        "    \n",
        "    \n",
        "    sentence = sentence.replace('Doesn\\'t', 'Does not')\n",
        "    sentence = sentence.replace('Wouldn\\'t', 'Would not')\n",
        "    sentence = sentence.replace('weren\\'t', 'were not')\n",
        "    sentence = sentence.replace('where\\'s', 'where is')\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "366e693f-acba-7855-54bd-30c0bd31435c"
      },
      "outputs": [],
      "source": [
        "copy_frame['question1'] = copy_frame['question1'].map(replace_q1_puncts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e5ae55f8-8ed4-1cef-9c4b-dc7c756d248e"
      },
      "outputs": [],
      "source": [
        "copy_frame['question2'] = copy_frame['question2'].map(replace_q2_puncts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "38266b40-04da-f572-6cac-fe76c74cf48f"
      },
      "outputs": [],
      "source": [
        "copy_frame['q1_quotes'] = copy_frame['question1'].str.extract('(\\w+\\'\\w+)\\s.*')\n",
        "copy_frame['q2_quotes'] = copy_frame['question2'].str.extract('(\\w+\\'\\w+)\\s.*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b4f1a055-b6bd-6069-976b-a869e2139168"
      },
      "outputs": [],
      "source": [
        "copy_frame['q1_length'] = copy_frame['question1'].apply(lambda x: len(x.split(' ')))\n",
        "copy_frame['q2_length'] = copy_frame['question2'].apply(lambda x: len(x.split(' ')))\n",
        "copy_frame['words_shared'] = copy_frame.apply(lambda x: find_matching_words(x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "070bf978-0c08-89c2-4e5f-6ff93aa99a4b"
      },
      "outputs": [],
      "source": [
        "def find_words_not_in_stops(x):\n",
        "    q1_not_stops = set(x['question1'].split(' ')).difference(stops)\n",
        "    q2_not_stops = set(x['question2'].split(' ')).difference(stops)\n",
        "    commons = q1_not_stops.intersection(q2_not_stops)\n",
        "    return commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "311d4769-8efa-bde6-bc37-a2015a18809d"
      },
      "outputs": [],
      "source": [
        "copy_frame['words_shared_without_stops'] = copy_frame.apply(lambda x:find_words_not_in_stops(x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a06f62a2-6ad9-788b-e894-00821b22615e"
      },
      "outputs": [],
      "source": [
        "copy_frame['words_shared_without_stops_len'] = copy_frame['words_shared_without_stops'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "27390a30-9513-59d9-c724-edb078a1fdb9"
      },
      "outputs": [],
      "source": [
        "quote_counts = []\n",
        "quote_counts.append(copy_frame.ix[pd.notnull(copy_frame['q1_quotes'])].shape[0])\n",
        "quote_counts.append(copy_frame.ix[pd.isnull(copy_frame['q1_quotes'])].shape[0])\n",
        "quote_counts = pd.DataFrame(quote_counts, columns=['counts'], \n",
        "                            index=['Quotes', 'Unquotes'])\n",
        "\n",
        "colors = ['gold', 'yellowgreen']\n",
        "explode = (0.1, 0)\n",
        " \n",
        "plt.pie(quote_counts['counts'], labels=quote_counts.index, colors=colors, \n",
        "        explode=(1, 0), autopct='%.1f%%',)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "84b6d96c-58ef-aa4a-69a5-99fb27a759f9"
      },
      "outputs": [],
      "source": [
        "quote_counts = []\n",
        "quote_counts.append(copy_frame.ix[pd.notnull(copy_frame['q2_quotes'])].shape[0])\n",
        "quote_counts.append(copy_frame.ix[pd.isnull(copy_frame['q2_quotes'])].shape[0])\n",
        "quote_counts = pd.DataFrame(quote_counts, columns=['counts'], \n",
        "                            index=['Quotes', 'Unquotes'])\n",
        "\n",
        "colors = ['gold', 'yellowgreen']\n",
        "explode = (0.1, 0)\n",
        " \n",
        "plt.pie(quote_counts['counts'], labels=quote_counts.index, colors=colors, \n",
        "        explode=(1, 0), autopct='%.1f%%',)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9410e58-245f-ab28-76ca-2520db61b848"
      },
      "outputs": [],
      "source": [
        "train_features = ['q1_length', 'q2_length', 'words_shared']\n",
        "X = copy_frame[train_features]\n",
        "y = copy_frame[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
        "clf = RandomForestClassifier()\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "y_proba = clf.predict_proba(X_test)\n",
        "log_loss_score = log_loss(y_test, y_proba)\n",
        "metrics.append(log_loss_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "86bf4262-08f4-a4c6-334a-30d1068c6bff"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(metrics, columns=['logloss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "80681035-bde6-016b-6676-c63f01851c99"
      },
      "outputs": [],
      "source": [
        "def preprocess():\n",
        "    test_set['question1'] = test_set['question1'].map(replace_q1_puncts)\n",
        "    test_set['question1'] = test_set['question1'].map(replace_q2_puncts)\n",
        "    test_set['q1_length'] = test_set['question1'].apply(lambda x: len(x.split(' ')))\n",
        "    test_set['q2_length'] = test_set['question2'].apply(lambda x: len(x.split(' ')))\n",
        "    test_set['words_shared'] = test_set.apply(lambda x: find_matching_words(x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "29f67751-1f9e-76a7-a05b-185427931800"
      },
      "outputs": [],
      "source": [
        "#preprocess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1bf53b5-aac3-511c-283d-96aa27e2e5d9"
      },
      "outputs": [],
      "source": [
        "def generate_predictions():\n",
        "    test_ids = test_set['test_id']\n",
        "    predictions = clf.predict_proba(test_set[train_features])\n",
        "\n",
        "    submission = pd.DataFrame(test_ids)\n",
        "\n",
        "    prediction_set = []\n",
        "    for i in range(len(predictions)):\n",
        "        prediction_set.append(predictions[i][1])\n",
        "    \n",
        "    prediction_set = pd.DataFrame(prediction_set, columns=[target])\n",
        "    submission = pd.concat([submission, prediction_set], axis=1)\n",
        "    return submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a4544ae-63ff-40e5-926b-813d00f667d1"
      },
      "outputs": [],
      "source": [
        "#submission = generate_predictions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9071a76d-599a-f34d-69b4-4f092eb9c850"
      },
      "outputs": [],
      "source": [
        "#print(set(pd.isnull(submission[target])))\n",
        "#submission.to_csv(\"submission_quotes_replaced.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}