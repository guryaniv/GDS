{"cells":[
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "from __future__ import print_function\n\nimport pandas as pd \nimport numpy as np \nfrom ml_metrics import quadratic_weighted_kappa\nimport random\n\nrandom.seed(23)\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.utils import np_utils\nfrom keras import optimizers\nfrom scipy.optimize import fmin_powell\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\ndef preprocess_data(X, scaler=None):\n    if not scaler:\n        scaler = StandardScaler()\n        scaler.fit(X)\n    X = scaler.transform(X)\n    return X, scaler\n\ndef eval_wrapper(y, yhat):  \n    y = np.array(y)\n    y = y.astype(int)\n    yhat = np.array(yhat)\n    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n    return quadratic_weighted_kappa(yhat, y)\n    \ndef preprocess_labels(labels, encoder=None, categorical=True):\n    if not encoder:\n        encoder = LabelEncoder()\n        encoder.fit(labels)\n    y = encoder.transform(labels).astype(np.int32)\n    if categorical:\n        y = np_utils.to_categorical(y)\n    return y, encoder\n    \ndef apply_offset(data, bin_offset, sv, scorer=eval_wrapper):\n    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\n    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\n    score = scorer(data[1], data[2])\n    return score\n    \n# global variables\ncolumns_to_drop = ['Id', 'Response', 'Medical_History_10', 'Medical_History_24', 'Medical_History_32']\nnum_classes = 8\n\nprint(\"Load the data using pandas\")\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n\n# combine train and test\nall_data = train.append(test)\n\n# create any new variables    \nall_data['Product_Info_2_char'] = all_data.Product_Info_2.str[1]\nall_data['Product_Info_2_num'] = all_data.Product_Info_2.str[2]\n\n# factorize categorical variables\nall_data['Product_Info_2'] = pd.factorize(all_data['Product_Info_2'])[0]\nall_data['Product_Info_2_char'] = pd.factorize(all_data['Product_Info_2_char'])[0]\nall_data['Product_Info_2_num'] = pd.factorize(all_data['Product_Info_2_num'])[0]\nall_data['BMI_Age'] = all_data['BMI'] * all_data['Ins_Age']\nmed_keyword_columns = all_data.columns[all_data.columns.str.startswith('Medical_Keyword_')]\nall_data['Med_Keywords_Count'] = all_data[med_keyword_columns].sum(axis=1)\n\nprint('Eliminate missing values')    \n# Use -1 for any others\nall_data.fillna(-1, inplace=True)\n\n# fix the dtype on the label column\nall_data['Response'] = all_data['Response'].astype(int)\n\n# Provide split column\nall_data['Split'] = np.random.randint(5, size=all_data.shape[0])\n\n#to-do: Add one-hot encoding for all categorical variables\n\n# split train and test\ntrain = all_data[all_data['Response']>0].copy()\ntest = all_data[all_data['Response']<1].copy()\n\n\n#### CODE GOES HERE ########\ntestId = test[\"Id\"]\nlabels = train[\"Response\"]\n\nlabels_test = test[\"Response\"]\ny, encoder = preprocess_labels(labels)\ntrain = train.drop(columns_to_drop, axis = 1)\ntest = test.drop(columns_to_drop, axis = 1)\n\ntrain, scaler = preprocess_data(train)\ntest, _ = preprocess_data(test, scaler)\n\ndims = train.shape[1]\n\n\nprint(dims, 'dims')\n\nprint('Building model...')\n\nmodel = Sequential()\nmodel.add(Dense(output_dim=32, init='glorot_uniform', input_dim=dims, activation='tanh'))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(32, init='glorot_uniform'))\nmodel.add(PReLU())\n\nmodel.add(Dense(num_classes, init='glorot_uniform'))\nmodel.add(Activation('softmax'))\nSGDopt = optimizers.SGD(lr=0.01, momentum=0.001, decay=0.0, nesterov=False)\nmodel.compile(loss='categorical_crossentropy', optimizer=SGDopt)\n\ntrain = np.array(train, dtype=np.float32)\ny = np.array(y, dtype=np.int32)\n\nprint('Training model...')\nmodel.fit(train, y, nb_epoch = 10, batch_size = 16, validation_split = 0.4, verbose = 0)\n\nprint('Generating submission...')\ntest = np.array(test, dtype=np.float32)\ntest_preds = model.predict_classes(test, batch_size = 50, verbose = 0)\ntest_preds += 1\n\ntrain_preds = model.predict_classes(train, batch_size = 50, verbose = 0)\ntrain_preds += 1\n\ntrain_probs = model.predict_proba(train, batch_size = 50, verbose = 0)\ntest_probs = model.predict_proba(test, batch_size = 50, verbose = 0)\n\nclasses = np.array(range(num_classes))+1\nexpected_value_train = np.dot(train_probs, classes)\nexpected_value_test = np.dot(test_probs, classes)\n\ntrain_preds = expected_value_train\ntest_preds = expected_value_test\n\nexpected_value_train = np.clip(expected_value_train, 1, 8)\nexpected_value_test = np.clip(expected_value_test, 1, 8)\n\nexpected_value_train = np.round(expected_value_train).astype(int)\nexpected_value_test = np.round(expected_value_test).astype(int)\n\nprint('Train score: ',eval_wrapper(expected_value_train, labels))\n\ntrain_preds = np.clip(train_preds, -0.99, 8.99)\ntest_preds = np.clip(test_preds, -0.99, 8.99)\n\n# train offsets \noffsets = np.array([-1, -1, -1, -1, -1, -1, -1, -1])\noffset_train_preds = np.vstack((train_preds, train_preds, labels))\nfor j in range(num_classes):\n    train_offset = lambda x: -apply_offset(offset_train_preds, x, j)\n    offsets[j] = fmin_powell(train_offset, offsets[j])  \n\n# apply offsets to test\ndata = np.vstack((test_preds, test_preds, labels_test.values))\nfor j in range(num_classes):\n    data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j] \n\nfinal_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\n\npreds_out = pd.DataFrame({\"Id\": testId, \"Response\": final_test_preds})\npreds_out = preds_out.set_index('Id')\npreds_out.to_csv('keras_expv.csv')"
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}