{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "613c7edb-fa0a-fc62-4822-690183dc4f14"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b7ea3ed0-05f4-a3d4-7bbb-cebf910a0779"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('../input/train.csv')\n",
        "question1 = []\n",
        "question2 = []\n",
        "is_duplicate = []\n",
        "question1 = data[\"question1\"].astype('str') \n",
        "question2 = data[\"question2\"].astype('str') \n",
        "is_duplicate = data[\"is_duplicate\"]\n",
        "\n",
        "print (len(question1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad9561d0-a49d-d157-1671-8c6f486ce24c"
      },
      "outputs": [],
      "source": [
        "MAX_NB_WORDS = 200000\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "EMBEDDING_DIM = 300\n",
        "questions = question1 + question2\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(questions)\n",
        "question1_word_sequences = tokenizer.texts_to_sequences(question1)\n",
        "question2_word_sequences = tokenizer.texts_to_sequences(question2)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(\"Words in index: %d\" % len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "beeb6673-724c-00dc-b6cd-d34f07caa449"
      },
      "outputs": [],
      "source": [
        "from os.path import expanduser, exists\n",
        "from zipfile import ZipFile\n",
        "KERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\n",
        "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
        "GLOVE_ZIP_FILE = 'glove.840B.300d.zip'\n",
        "GLOVE_FILE = 'glove.840B.300d.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f9b44f5-c1c7-beba-b2fc-ad4b767c9a88"
      },
      "outputs": [],
      "source": [
        "# download \n",
        "if not exists(KERAS_DATASETS_DIR + GLOVE_ZIP_FILE):    \n",
        "    zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n",
        "    zipfile.extract(GLOVE_FILE, path=KERAS_DATASETS_DIR)\n",
        "    \n",
        "print(\"Processing\", GLOVE_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fa0cd943-89b7-f9a1-fed4-df7746da16e7"
      },
      "outputs": [],
      "source": [
        "from os.path import expanduser, exists\n",
        "from zipfile import ZipFile\n",
        "KERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\n",
        "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
        "GLOVE_ZIP_FILE = 'glove.840B.300d.zip'\n",
        "GLOVE_FILE = 'glove.840B.300d.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58d1c28b-d99f-3d37-3a84-efb4374dfb80"
      },
      "outputs": [],
      "source": [
        "#Not possible to download on kaggle\n",
        "#if not exists(KERAS_DATASETS_DIR + GLOVE_ZIP_FILE):    \n",
        "#    zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n",
        "#    zipfile.extract(GLOVE_FILE, path=KERAS_DATASETS_DIR)\n",
        "    \n",
        "print(\"Processing\", GLOVE_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e563e55e-20cc-acf8-9992-7fbdf82027ad"
      },
      "outputs": [],
      "source": [
        "embeddings_index = {}\n",
        "'''with open(KERAS_DATASETS_DIR + GLOVE_FILE, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding'''\n",
        "\n",
        "print('Word embeddings: %d' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "35b15c79-7246-5ecf-328b-ecf6a7107060"
      },
      "outputs": [],
      "source": [
        "q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "q2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = np.array(is_duplicate, dtype=int)\n",
        "print('Shape of question1 data tensor:', q1_data.shape)\n",
        "print('Shape of question2 data tensor:', q2_data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "25e664e9-11c7-830f-21b4-a3918cd936d5"
      },
      "outputs": [],
      "source": [
        "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NB_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        word_embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "98740ac4-dadc-3e68-7c0e-b53d3b4c7358"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = np.stack((q1_data, q2_data), axis=1)\n",
        "y = labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
        "Q1_train = X_train[:,0]\n",
        "Q2_train = X_train[:,1]\n",
        "Q1_test = X_test[:,0]\n",
        "Q2_test = X_test[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d4579ea9-57cd-2ad4-313a-eb6a2a2810ca"
      },
      "outputs": [],
      "source": [
        "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ce3fc7c8-3980-b220-565d-b85ea43f3f01"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 25\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1\n",
        "TEST_SPLIT = 0.1\n",
        "RNG_SEED = 13371447\n",
        "NB_EPOCHS = 25\n",
        "DROPOUT = 0.1\n",
        "BATCH_SIZE = 32\n",
        "q1 = Embedding(nb_words + 1, \n",
        "                 EMBEDDING_DIM, \n",
        "                 weights=[word_embedding_matrix], \n",
        "                 input_length=MAX_SEQUENCE_LENGTH, \n",
        "                 trainable=False)(question1)\n",
        "q1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1)\n",
        "q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1)\n",
        "\n",
        "q2 = Embedding(nb_words + 1, \n",
        "                 EMBEDDING_DIM, \n",
        "                 weights=[word_embedding_matrix], \n",
        "                 input_length=MAX_SEQUENCE_LENGTH, \n",
        "                 trainable=False)(question2)\n",
        "q2 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q2)\n",
        "q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q2)\n",
        "\n",
        "merged = concatenate([q1,q2])\n",
        "merged = Dense(200, activation='relu')(merged)\n",
        "merged = Dropout(DROPOUT)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "merged = Dense(200, activation='relu')(merged)\n",
        "merged = Dropout(DROPOUT)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "merged = Dense(200, activation='relu')(merged)\n",
        "merged = Dropout(DROPOUT)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "merged = Dense(200, activation='relu')(merged)\n",
        "merged = Dropout(DROPOUT)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "\n",
        "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "20fe4c5c-039e-f6b1-55dd-1d87228b6300"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "098b77db-8013-c97f-688d-bb057cf50a28"
      },
      "outputs": [],
      "source": [
        "MODEL_WEIGHTS = 'question_pairs_weights.h5'\n",
        "callbacks = [ModelCheckpoint(MODEL_WEIGHTS, monitor='val_acc', save_best_only=True)]\n",
        "history = model.fit([Q1_train, Q2_train],\n",
        "                    y_train,\n",
        "                    epochs=NB_EPOCHS,\n",
        "                    validation_split=VALIDATION_SPLIT,\n",
        "                    verbose=2,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6c52c2b9-4629-0b99-df62-418c335276e2"
      },
      "outputs": [],
      "source": [
        "acc = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
        "                    'training': history.history['acc'],\n",
        "                    'validation': history.history['val_acc']})\n",
        "ax = acc.iloc[:,:].plot(x='epoch', figsize={5,8}, grid=True)\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_ylim([0.0,1.0]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2263318f-9919-63e5-fa4b-37f893e9e7b9"
      },
      "outputs": [],
      "source": [
        "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc']))\n",
        "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aded5002-8dc7-0b9f-4844-8ab66a5a8186"
      },
      "outputs": [],
      "source": [
        "model.load_weights(MODEL_WEIGHTS)\n",
        "predictions = model.predict([test_q1, test_q2, test_q1, test_q2], verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c2120609-1f89-ce0d-d9fe-c24a3050e238"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame(predictions, columns=['is_duplicate'])\n",
        "submission.insert(0, 'test_id', test.test_id)\n",
        "file_name = 'submission_{}.csv'.format(min_loss)\n",
        "submission.to_csv(file_name, index=False)\n",
        "\n",
        "submission.head(10)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}