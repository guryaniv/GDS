{"nbformat": 4, "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "version": "3.6.3", "file_extension": ".py"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"source": ["\n", "**Here is my  approach :**\n", "\n", "1. .OpenCv get body coordinates ( x-coordinate, y-coordinate, width and height of image). Model used \u201chaarcascade_fullbody.xml\u201d\n", "1.Use OpenCv to get face coordinate (model used \u201clbpcascade_frontalcatface.xml\u201d)\n", "1. Use OpenCv to get upper body (haarcascade_upperbody) and lower body (haarcascade_lowerbody.xml)\n", "1. Next get coordinates for legs (which can be derived from the full body and lower body means end of lower body to end of upper body, then bisect for left and right leg and further bisect each to get top and bottom part of leg)\n", "1. Similarly for hands get the coordinates between top of full body and top of upper body and then bisect vertically for left and right hand and then bisect horizontally for top and lower portion of hands.\n", "1. Next use Keras to move forward with prediction \n", "\n", "So far I have worked to get full body coordinates for all 1247 .aps files and still working on the face , lower and upper body coordinates to overcome the 10% to 15% error I have \n", "\n", "read_header(infile) and read_data(infile) has been leveraged from existing kernels:\n", "\n", "*I joined this competition in mid-November, which is almost  end period of this competition,  trying to see how far I can reach...*"], "metadata": {"_uuid": "d3210a5fef5e686004d66731eb0b8cd1ca489f75", "_cell_guid": "1c87aede-6af6-427b-b9ee-46d111bef6a7"}, "cell_type": "markdown"}, {"source": ["from __future__ import print_function\n", "from __future__ import division\n", "import numpy as np\n", "import os\n", "from matplotlib import pyplot as plt\n", "import cv2\n", "import pandas as pd\n", "import seaborn as sns\n", "import scipy.stats as stats\n", "import matplotlib\n", "import datetime\n", "import glob\n", "import csv, io \n", "%matplotlib inline\n", "print(\"Package Imported..\")\n"], "outputs": [], "metadata": {"_uuid": "ce979f6210aef593395d26ef8fcfed405ad02e4a", "_cell_guid": "a412925d-48e9-4e22-bd0e-f3460c3c79c3", "collapsed": true}, "cell_type": "code", "execution_count": null}, {"source": ["**read_header(infile) Leverage from existing kernel ...**"], "metadata": {"_uuid": "9b4558f6f9137df1f6cceeb1a5553f2638f2866e", "_cell_guid": "6e37bda2-1cec-47a0-b6c1-34da561f561b"}, "cell_type": "markdown"}, {"source": ["def read_header(infile):\n", "    \"\"\"Read image header (first 512 bytes)\n", "    \"\"\"\n", "    h = dict()\n", "    fid = open(infile, 'r+b')\n", "    h['filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n", "    h['parent_filename'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 20))\n", "    h['comments1'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n", "    h['comments2'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 80))\n", "    h['energy_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['config_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['file_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['trans_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['scan_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['data_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['date_modified'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 16))\n", "    h['frequency'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['mat_velocity'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['num_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n", "    h['num_polarization_channels'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['spare00'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['adc_min_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['adc_max_voltage'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['band_width'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['spare01'] = np.fromfile(fid, dtype = np.int16, count = 5)\n", "    h['polarization_type'] = np.fromfile(fid, dtype = np.int16, count = 4)\n", "    h['record_header_size'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['word_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['word_precision'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['min_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['max_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['avg_data_value'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['data_scale_factor'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['data_units'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['surf_removal'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n", "    h['edge_weighting'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n", "    h['x_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n", "    h['y_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n", "    h['z_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n", "    h['t_units'] = np.fromfile(fid, dtype = np.uint16, count = 1)\n", "    h['spare02'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['x_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['y_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['z_return_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['scan_orientation'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['scan_direction'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['data_storage_order'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['scanner_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['x_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['y_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['z_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['t_inc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['num_x_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n", "    h['num_y_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n", "    h['num_z_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n", "    h['num_t_pts'] = np.fromfile(fid, dtype = np.int32, count = 1)\n", "    h['x_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['y_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['z_speed'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['x_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['y_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['z_acc'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['x_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['y_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['z_motor_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['x_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['y_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['z_encoder_res'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['date_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n", "    h['time_processed'] = b''.join(np.fromfile(fid, dtype = 'S1', count = 8))\n", "    h['depth_recon'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['x_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['y_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['elevation_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['roll_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['z_max_travel'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['azimuth_offset_angle'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['adc_type'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['spare06'] = np.fromfile(fid, dtype = np.int16, count = 1)\n", "    h['scanner_radius'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['x_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['y_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['z_offset'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['t_delay'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['range_gate_start'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['range_gate_end'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['ahis_software_version'] = np.fromfile(fid, dtype = np.float32, count = 1)\n", "    h['spare_end'] = np.fromfile(fid, dtype = np.float32, count = 10)\n", "    return h\n", "print(\"Read Header Completed\")"], "outputs": [], "metadata": {"_uuid": "3be63afb955d3ebba02ecb799c906b429449cb9d", "_cell_guid": "4fc91904-d1cb-463d-a62b-19aeda5cb805", "collapsed": true}, "cell_type": "code", "execution_count": null}, {"source": ["**read_data(infile) Leverage from existing kernel ...**"], "metadata": {"_uuid": "c8f3cccde3c925be6c4d2e82e11a362f8b53a368", "_cell_guid": "29c7b00d-5f61-41d4-80c1-bacba12474b0"}, "cell_type": "markdown"}, {"source": ["def read_data(infile):\n", "    \"\"\"Read any of the 4 types of image files, returns a numpy array of the image contents\n", "    \"\"\"\n", "    extension = os.path.splitext(infile)[1]\n", "    h = read_header(infile)\n", "    nx = int(h['num_x_pts'])\n", "    ny = int(h['num_y_pts'])\n", "    nt = int(h['num_t_pts'])\n", "    fid = open(infile, 'rb')\n", "    fid.seek(512) #skip header\n", "    if extension == '.aps' or extension == '.a3daps':\n", "        if(h['word_type']==7): #float32\n", "            data = np.fromfile(fid, dtype = np.float32, count = nx * ny * nt)\n", "        elif(h['word_type']==4): #uint16\n", "            data = np.fromfile(fid, dtype = np.uint16, count = nx * ny * nt)\n", "#        data = data * h['data_scale_factor'] #scaling factor\n", "        data = data.reshape(nx, ny, nt, order='F').copy() #make N-d image\n", "    elif extension == '.a3d':\n", "        if(h['word_type']==7): #float32\n", "            data = np.fromfile(fid, dtype = np.float32, count = nx * ny * nt)\n", "        elif(h['word_type']==4): #uint16\n", "            data = np.fromfile(fid, dtype = np.uint16, count = nx * ny * nt)\n", "#        data = data * h['data_scale_factor'] #scaling factor\n", "        data = data.reshape(nx, nt, ny, order='F').copy() #make N-d image\n", "    elif extension == '.ahi':\n", "        data = np.fromfile(fid, dtype = np.float32, count = 2* nx * ny * nt)\n", "        data = data.reshape(2, ny, nx, nt, order='F').copy()\n", "        real = data[0,:,:,:].copy()\n", "        imag = data[1,:,:,:].copy()\n", "    fid.close()\n", "    if extension != '.ahi':\n", "        return data\n", "    else:\n", "        return real, imag"], "outputs": [], "metadata": {"_uuid": "03e506c5ad79bddf71530a768df41869fa22d5eb", "_cell_guid": "b92f420b-d768-4e1c-98cc-5558400d5fb8", "collapsed": true}, "cell_type": "code", "execution_count": null}, {"source": ["1. This notebook runs from current directory . \n", "1.  Under the current directory I have \"output\" and \"stage1_aps\" directories\n", "1.  under \"stage1_aps\" I have all the 1247 .aps files \n", "1.  under \"output\" directory a file named \"Full_Body_coordinates.csv\" will be generated \n", "1.  File Full_Body_coordinates.csv will store Image File Name, File Extension, X Coordinate, Y Coordinate , width , and height.\n", "\n", "Below mentioned has been tested for all 1247 .aps images\n"], "metadata": {"_uuid": "7bb1e5d692d76a74068c112a69dc5c30680c9cb0", "_cell_guid": "6ce32e44-cb6b-4ba5-a4e7-bb0d8edd6902"}, "cell_type": "markdown"}, {"source": ["def aps_full_body_coord():\n", "    \n", "    csvfilename='Full_Body_coordinates.csv'\n", "    #outdirname='output' # Commented outputdirname as the dir doesn't exists for this notebook\n", "    outdirname='../working'\n", "    with io.open(outdirname + \"/\" + csvfilename,'w',encoding='ascii',errors='replace') as out_file:   \n", "        writer = csv.writer(out_file)\n", "        writer.writerow(('Image_Name','File_Extension','x','y','w','h')) \n", "        \n", "        i =0\n", "        for path in glob.glob(\"stage1_aps/*.aps\"):\n", "            ff=path.split(os.sep)[1]\n", "            img_name = os.path.splitext(ff)[0]\n", "            img_file_extn = os.path.splitext(ff)[1]\n", "            \"\"\"\n", "            Comment the below mentioned 3 lines to get coordinates of all 1247 files\n", "            \"\"\"\n", "            i +=1 # Pl comment for all 1247 .aps files\n", "            if i ==20: # Pl comment for all 1247 .aps files\n", "                break # Pl comment for all 1247 .aps files\n", "            image = read_data(path)\n", "\n", "            image = image.transpose(1,0,2)\n", "            #print(\"Image \",image.shape,image[:,:,0].shape, image[:,:,0].dtype, type(image[:,:,0]))\n", "\n", "\n", "            test1 = (image / 255).round().astype(np.uint8)\n", "            test1 = np.array(test1)\n", "\n", "            test_c = np.copy(cv2.cvtColor(test1[:,:,0], cv2.COLOR_GRAY2RGB))\n", "            test_c = cv2.cvtColor(test_c, cv2.COLOR_RGB2GRAY)\n", "\n", "            test_d = np.copy(cv2.cvtColor(test1[:,:,0], cv2.COLOR_GRAY2RGB))\n", "            test_d = cv2.cvtColor(test_d, cv2.COLOR_RGB2GRAY)\n", "\n", "            test_e = np.copy(test_d)\n", "\n", "            face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_fullbody.xml')\n", "            faces1 = face_cascade.detectMultiScale(test_c,scaleFactor=1.001)#,minNeighbors=1,minSize=(100,100),maxSize=(600,600),flags=cv2.CASCADE_SCALE_IMAGE)\n", "\n", "            if len(faces1)==0:\n", "                faces1=[[160,100,200,550]]\n", "\n", "            elif  len(faces1)>= 2:\n", "                faces1 = [faces1[0]]\n", "            else:\n", "                faces1 = faces1\n", "\n", "            for (x,y,w,h) in faces1:\n", "                #print(\"x,y,w,h \",x,y,w,h)\n", "                if x > 150 or x < 100 :\n", "                    x = 160\n", "                if y > 200   :\n", "                    y = 100\n", "                if w < 150  :\n", "                    w =200\n", "                if h < 400  :\n", "                    h = 550\n", "\n", "                cv2.rectangle(test_e, (x-70,0), (x+w+70,y+h), (255,0,0), 3)\n", "                (x,y,w,h)= (x-70,0,w+70,h) \n", "\n", "            writer.writerow((img_name,img_file_extn,x,y,w,h))\n", "            plt.imshow(np.flipud(test_e),cmap='gray')\n", "            #plt.imshow(test_e,cmap='gray')\n", "            plt.show()\n", "            #print(\"Face \",faces1)\n", "    out_file.close()\n", "    \n", "\"\"\" TEST\"\"\"   \n", "aps_full_body_coord()"], "outputs": [], "metadata": {"_uuid": "259399777c1ce73291bb4439ed43cbdb47ab54dd", "_cell_guid": "ac6346cc-ccaa-4107-8d6e-9a2567422038", "collapsed": true}, "cell_type": "code", "execution_count": null}, {"source": ["from subprocess import check_output\n", "#print(check_output([\"ls\", \"-l\", \"../../kaggle/input\"]).decode(\"utf8\"))\n", "#print(check_output([\"ls\", \"/\"]).decode(\"utf8\"))\n", "#print(check_output([\"ls\",\"-l\",  \"../working\"]).decode(\"utf8\"))\n", "\n", "\"\"\"\n", "Check if the Full Body coordinate has been created and how many rows it has ....\n", "\"\"\"\n", "print(check_output([\"head\",\"-2\",  \"../working/Full_Body_coordinates.csv\"]).decode(\"utf8\"))\n", "print(check_output([\"wc\",\"-l\",  \"../working/Full_Body_coordinates.csv\"]).decode(\"utf8\"))"], "outputs": [], "metadata": {"_uuid": "a0f7f4da4401d412b75338cd4d9dd52347d37f7c", "_cell_guid": "b7b5c8f1-c158-48ca-a8ac-7e5e7c93f103", "collapsed": true}, "cell_type": "code", "execution_count": null}, {"source": [], "metadata": {"_uuid": "edb018a792ead121872be5a151dafa8d425d8a68", "_cell_guid": "7af15e78-2bcc-4500-9151-10d2f8958327"}, "cell_type": "markdown"}, {"source": [], "metadata": {"_uuid": "24153ce2b014a3ea72bbf8a46c7e397f6b2d9807", "_cell_guid": "f040c8cc-ddad-4c36-9d87-ff269bb5699a"}, "cell_type": "markdown"}, {"source": [], "metadata": {"_uuid": "dbb325f8a3e85362a10223497c09bed35029e074", "_cell_guid": "65c7d05b-23e8-4153-8e4b-4490beaa5e34"}, "cell_type": "markdown"}, {"source": [], "metadata": {"_uuid": "4a9975c4528969f0bb903dc7554de2ba25ce8202", "_cell_guid": "8127d832-1d78-4440-9dfc-1eb7bc35880b"}, "cell_type": "markdown"}]}