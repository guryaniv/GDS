{"nbformat_minor": 1, "cells": [{"source": ["# How many wrong predictions on my submission?\n", "\n", "This is a way to find out **how many of my submitted predictions are wrong** using the evaluation obtained with a submission.  \n", "I'm quite new in the use of machine learning techniques, so probably there was already a function in every ML library to do this, but I wasn't able to find it in sklearn.\n", "\n", "## Log Loss\n", "Submissions for the Spooky Author Identification Challange are evaluated using multi-class lagaritmic loss which has the following formula:\n", "\n", "$$log loss = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^My_{ij}\\log(p_{ij}),$$\n", "\n", "$$y_{ij} = 1\\ if\\ observation\\ i\\ belongs\\ to\\ class\\ j\\ and\\ 0\\ otherwise$$\n", "$$p_{ij} = my\\ predicted\\ probability\\ that\\ observation\\ i\\ belongs\\ to\\ class\\ j$$\n", "$$M = number\\ of\\ classes$$\n", "$$N = number\\ of\\ observations$$\n", "\n", "\n", "Moreover, to avoid undetermined values, all predicted probabilities are clipped to a maximum of 1-epsilon and a minimum of epsilon, with epsilon=1e-15.  \n", "\n", "## The Idea\n", "We can make an \"extremization\" process over our predictions setting our probabilities directly to the maximum and minimum clipping values.  \n", "Example: [0.99199, 0.00811, 0.0000] -> [1-epsilon, epsilon, epsilon] with epsilon = 1e-15  \n", "  \n", "Submitting predictions in this form will give us the best possible result if all our predictions are right but will also penalize us a lot for each single wrong prediction. As an example, a submission that scores 0.44097 will score 3.93369 if \"extremized\" and resubmitted.  \n", "\n", "The useful thing of this process is that when submitting these extreme predictions we know exactly how much a wrong prediction contributes to our score, so we can easily find the number of wrong predictions.\n", "\n", "## Calculating the number of wrong predictions\n", "\n", "In logistic loss both our right predictions and wrong predictions contribute to the loss score in reason of their distance from the real thing. (i.e. we also get penalized if we make a right prediction with 0.9 probability instead of 1).  \n", "  \n", "When submitting extremized predictions we have fixed contributions from right and wrong assignments as follows:  \n", "  \n", "$$log1 = log(1 - 1e-15)\\ for\\ right\\ predictions$$  \n", "  \n", "$$log2 = log(1e-15)\\ for\\ right\\ predictions$$  \n", "  \n", "Having determined these quantities we can now express our logloss score as follows:  \n", "\n", "$$ logloss= -\\frac{1}{N}[(N-w)log1 + wlog2] $$\n", "  \n", "and so we have\n", "  \n", "$$w = N  \\frac{log1-logloss}{log2-log1} $$\n", "\n", "where w is the number of wrong predictions.\n", "\n", "## Let's put it in code!"], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["import pandas as pd\n", "import numpy as np"]}, {"source": ["### First thing, we need to \"extremize\" our predictions as described above."], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["def boolToExtreme(b):\n", "    \"\"\"Service function for predToExtremeValues\"\"\"\n", "    if b:\n", "        return np.float128(1 - np.float128(1e-15))\n", "    return np.float128(1e-15)\n", "\n", "def predToExtremeValues(predictions):\n", "    \"\"\"Get the predictions in the form required for submission\n", "    as a pandas DataFrame and edit the probability to be exactly\n", "    the extreme values 10^15 and 1-10^15\"\"\"\n", "    \n", "    predictions = predictions.apply(lambda el: [p==el.max() for p in el], axis=1)\n", "    predictions = predictions.applymap(boolToExtreme)\n", "    return predictions"]}, {"source": ["### Then we build our calculating function"], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["def howManyWrong(logloss, N):\n", "    \"\"\"Returns the estimate number of wrong predictions based on the\n", "    Spooky Author Challenge evaluations system. logloss is the score,\n", "    N is the number of observations\"\"\"\n", "    \n", "    log1 = np.float128(np.log(np.float128(1 - 1e-15)))\n", "    log2 = np.float128(np.log(np.float128(1e-15)))\n", "    \n", "    w = N * ((log1 - logloss) / (log2 - log1))\n", "    return w\n"]}, {"source": ["## How to use it\n", "So here is the scenario I have in mind:  \n", "suppose we have a prediction csv files as required by the challange and you want to know how many wrong predictions there were. Following these steps you can find out:\n", "\n", "1. Load your predictions as a pandas dataframe;\n", "2. \"Extremize\" them using ext_pred = predToExtremeValues(your_predictions)\n", "3. Feed your \"extremized\" predictions to kaggle and get your \"extremized\" evaluation\n", "4. call howManyWrong(logloss, n_of_observations) to get the number of wrong predictions\n", "\n", "### Test\n", "The following code is a test using the training data so that we know the true labels for our predictions"], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# For test purposes let's use sklearn's log_loss evaluator\n", "from sklearn.metrics import log_loss\n", "\n", "dataset = pd.read_csv('../input/train.csv')\n", "\n", "def yToInt(author):\n", "    if author == \"EAP\":\n", "        return 0\n", "    elif author == \"HPL\":\n", "        return 1\n", "    else:\n", "        return 2\n", "\n", "# True labels from the training set\n", "y_true = [yToInt(a) for a in dataset.author]\n", "\n", "# Fake predictions for test purposes (always predict EAP)\n", "fakePred = pd.DataFrame([[\"id1212\", 1, 0, 0]]*len(y_true), columns=(\"id\", \"EAP\", \"HPL\", \"MWS\"))\n", "\n", "# Fake prediction values: leave out the id column\n", "fpv = fakePred[[\"EAP\", \"HPL\", \"MWS\"]]\n", "\n", "# Count the actual number of wrong predictions\n", "aw = 0\n", "for i in range(len(y_true)):\n", "    if not fpv.loc[i][y_true[i]] == 1:\n", "        aw += 1\n", "\n", "print(\"Actual wrong predictions:\", aw)\n", "\n", "# logloss estimate\n", "logloss = log_loss(y_true, fpv.as_matrix(), eps=1e-15)\n", "print(\"Logloss:\", logloss)\n", "print(\"Calculated number of wrong predictions:\", howManyWrong(logloss, len(y_true)))\n", "print(\"Accuracy:\", (len(y_true)-howManyWrong(logloss, len(y_true)))/len(y_true))"]}, {"source": ["### Actual application"], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# On one submission I had a score of 0.44097\n", "# Submitting the same predictions in the \"extremized\" form\n", "# I got the following logloss value\n", "logloss = 3.93369\n", "\n", "# Number of predictions in the submission:\n", "N = 8392\n", "\n", "w = howManyWrong(logloss, N)\n", "print(\"Calculated number of wrong predictions:\", w)\n", "print(\"Accuracy:\", (N-w)/N)\n"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": []}], "metadata": {"kernelspec": {"name": "conda-root-py", "display_name": "Python [conda root]", "language": "python"}, "anaconda-cloud": {}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "version": "3.6.3", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "nbformat": 4}