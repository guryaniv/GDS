{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Oversampling by shuffling\n\nAs has been pointed out by Branden Murray [here](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/83882) and demonstrated [here](https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works) random shuffling of the numerical values of the features does not seem to hurt the CV and LB scores as long as the shuffling is performed within the same target class (0 or 1). This opens up a possibility to generate a large number of training data sets equivalent to the original one. This might be useful, for example, for balancing the data set -- now we can easily add to the original data set 9 shuffled copies containing only the data corresponding to target = 1. The purpose of this notebook is to see how this idea works in practice.  \n\nLet's take the LightGBM parameters from one of a [high scoring public kernel](https://www.kaggle.com/marcospcsj/kernel-cod-valid-cruzada-lgbm) and test this idea (spoiler: no, we are not going to break the leaderboard, sorry).  "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"###############################################################\n# Loading libraries\n###############################################################\n\nimport os\nimport shutil\nimport feather\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scikitplot.metrics import plot_confusion_matrix, plot_roc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n###############################################################\n# Setting parameters\n###############################################################\nEARLY_STOPPING=4000\nNFOLDS=15\nNSHUFFLES_1=2\nNSHUFFLES_0=1\n\ntrain_df = pd.read_csv('../input/train.csv') \ntest_df = pd.read_csv('../input/test.csv') \nfeatures = [c for c in train_df.columns if c not in ['ID_code', 'target']]\ntarget = train_df['target']\n\n########################################################################\n#  Making folds    \n########################################################################\n\nnum_folds = NFOLDS\nfolds = StratifiedKFold(n_splits=num_folds, shuffle=False, random_state=2319)\n\noof_preds = np.zeros((len(train_df), 1))\ntest_preds = np.zeros((len(test_df), 1))\nroc_cv =[]\n\n########################################################################\n#  LightGBM parameters    \n########################################################################\n\nparam = {\n    'bagging_freq': 5,          \n    'bagging_fraction': 0.335,   \n    'boost_from_average':'false',   \n    'boost': 'gbdt',\n    'feature_fraction': 0.041,   \n    'learning_rate': 0.0083,     \n    'max_depth': -1,                \n    'metric':'auc',\n    'min_data_in_leaf': 80,     \n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,           \n    'num_threads': 8,\n    'tree_learner': 'serial',   \n    'objective': 'binary',      \n    'verbosity': 1\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19fabb383fbc5e6e345f631a2c68f750eb57ed43"},"cell_type":"markdown","source":"Next, we need to define a function to do oversampling by shuffling (\"overshuffling\" :-)):"},{"metadata":{"trusted":true,"_uuid":"65223aaf8a33fba5758ab7bf97188c7e700e80bc"},"cell_type":"code","source":"def overshuff(df, y, n=2, m=1): \n    \"\"\"\n    df - the data frame to process \n    y - target\n    n - the number of shuffled copies of the positive class to add\n    m - the number of shuffled copies of the negative class to add\n    \"\"\"\n    \n    df_1 = df[y==1] # Selecting the observations with target=1\n    y_1 = y[y==1]  # Target for this observations \n    \n    randoms=np.random.randint(0, 999999, size=n) # Random seeds to be used for shuffling\n    \n    for i, rand in enumerate(randoms): # n shufflings\n        # shuffle:\n        df_sh = df_1.apply(lambda x: x.sample(n=len(x), random_state=rand).values) \n        df = pd.concat([df, df_sh]) # add to the original data frame\n        y = pd.concat([y, y_1]) # add to the target\n        print(\"Step {} of {}. The random state used is {}\".format(i+1, n, rand))\n    \n    #rand = np.random.randint(0, 999999, size=1)[0] # one more random seed \n    #df, y = shuffle(df, y, random_state=rand) # one last shuffling (just in case!)\n    \n    print(\"The random state used for the final shuffling is {}\".format(rand))\n    \n    #########################################\n    \n    df_1 = df[y==0] # Selecting the observations with target=0\n    y_1 = y[y==0]  # Target for this observations \n    \n    randoms=np.random.randint(0, 999999, size=m) # Random seeds to be used for shuffling\n    \n    for i, rand in enumerate(randoms): # m shufflings\n        # shuffle:\n        df_sh = df_1.apply(lambda x: x.sample(n=len(x), random_state=rand).values) \n        df = pd.concat([df, df_sh]) # add to the original data frame\n        y = pd.concat([y, y_1]) # add to the target\n        print(\"Step {} of {}. The random state used is {}\".format(i+1, n, rand))\n    \n    rand = np.random.randint(0, 999999, size=1)[0] # one more random seed \n    df, y = shuffle(df, y, random_state=rand) # one last shuffling (just in case!)\n    \n    print(\"The random state used for the final shuffling is {}\".format(rand))\n    \n    #########################################\n    \n    return df, y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"deb9cfb98e115b02e5084ad61c6d99c75bc5787d"},"cell_type":"markdown","source":"Here is a handy function that we will use to plot the distribution of predictions."},{"metadata":{"trusted":true,"_uuid":"7dedd9d1dc873d81752f7a315ce03b545d1a7560"},"cell_type":"code","source":"########################################################################\n# Function for plotting the distribution of predictions   \n########################################################################\n    \ndef plot_prediction_distribution(y_true, y_pred, ax):\n    df = pd.DataFrame({'prediction': y_pred, 'ground_truth': y_true})\n    \n    sns.distplot(df[df['ground_truth'] == 0]['prediction'], \n                 label='negative', ax=ax)\n    sns.distplot(df[df['ground_truth'] == 1]['prediction'], \n                 label='positive', ax=ax)\n\n    ax.legend(prop={'size': 16}, title = 'Labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd0da371d65e83e6f53ec67debc6367cc5543263"},"cell_type":"code","source":"########################################################################\n#  Training the model    \n########################################################################\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold idx:{}\".format(fold_ + 1))\n    \n    trn_x = train_df.iloc[trn_idx][features]\n    trn_y = target.iloc[trn_idx]\n    \n    # Apply the overshuffling function to the training data \n    # (excluding the validation set!)\n    \n    trn_x, trn_y = overshuff(trn_x, trn_y, NSHUFFLES_1, NSHUFFLES_0)\n    \n    print(\"Converting the data to lgbm format\")\n    trn_data = lgb.Dataset(trn_x, label=trn_y)\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n    \n    print(\"Training the classifier\")\n    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], \n                    verbose_eval=5000, early_stopping_rounds = EARLY_STOPPING)\n    \n    print(\"Making predictions for the validation data\")\n    val_pred = clf.predict(train_df.iloc[val_idx][features], \n                           num_iteration=clf.best_iteration)\n    \n########################################################################\n    \n    print(\"Computing the AUC score\")\n    roc_cv.append(roc_auc_score(target.iloc[val_idx], val_pred))\n    \n    print(\"AUC = {}\".format(roc_auc_score(target.iloc[val_idx], val_pred)))\n    oof_preds[val_idx, :] = val_pred.reshape((-1, 1))\n    \n    print(\"Making predictions for the test data\")\n    test_fold_pred = clf.predict(test_df[features], \n                                 num_iteration=clf.best_iteration).\\\n                                 reshape((-1, 1))\n        \n    test_preds += test_fold_pred\n    \n   # preds = pd.DataFrame(oof_preds[val_idx, :], columns=['pos_preds'])\n   # preds['neg_preds'] = 1.0 - preds['pos_preds']\n   # fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(24, 6))\n   # plot_prediction_distribution(target.iloc[val_idx], preds['pos_preds'], ax=ax1);\n   # plot_roc(target.iloc[val_idx], preds[['neg_preds','pos_preds']], ax=ax2);\n   # plot_confusion_matrix(target.iloc[val_idx], oof_preds[val_idx, :]>0.5, ax=ax3);\n   # path_fig_full = 'Fold_{}_'.format(fold_ + 1)+'model_diagnostics.png'\n   # fig.savefig(path_fig_full) \n    \n########################################################################\n#  Computing statistics   \n########################################################################\n\ntest_preds /= num_folds\n\nroc_score_1 = round(roc_auc_score(target.ravel(), oof_preds.ravel()), 5)\nroc_cv = np.array(roc_cv)\nroc_score = round(sum(roc_cv)/len(roc_cv), 5)\nst_dev = round(np.array(roc_cv).std(), 5)\n\nprint(\"Average of the folds' AUCs = {}\".format(roc_score))\n\nprint(\"Combined folds' AUC = {}\".format(roc_score_1))\n\nprint(\"The standard deviation = {}\".format(st_dev))\n\nprint(\"Saving OOF predictions\")\noof_preds = pd.DataFrame(np.column_stack((train_df.ID_code.values, \n                                          oof_preds.ravel())), \n                        columns=['ID_code', 'target'])\n    \noof_preds.to_csv((\"LGBM_{}.csv\").format(str(roc_score)), index=False)\n\nprint(\"Saving submission file\")\nsub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub[\"target\"] = test_preds\nsub.to_csv('submission_{}.csv'.format(str(roc_score)), index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}