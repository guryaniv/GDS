{"metadata": {"language_info": {"version": "3.6.1", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "cells": [{"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "daedaff6-1aa2-4394-85f7-636d86783b98", "_uuid": "33eacb1f572e8a9da2857623809ffde1587b7935"}, "source": "In this kernel i want to compare performance (score) of various models and also see how the models do when stacking/ensembling them in different approaches. Goal for me is to understand how the various models do, how various types of models do and what benefits the stacking/ensembling brings..\n\nThe feature engineering and preprocessing is quite basic still - so don't expect top scores. Focus here is mainly on comparing the models.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"trusted": false, "_execution_state": "idle", "_cell_guid": "eb77c446-dbbc-4970-aa15-96a15729cac9", "_uuid": "6f9d28d7dab49ee4e30a8588c5e54c5b39f261db"}, "source": "import numpy as np\nimport pandas as pd\nimport haversine\n\n# Read training data (first 100k rows only for speed..)\ntrain = pd.read_csv('../input/train.csv', parse_dates=['pickup_datetime'], nrows=100000)\n\n# Log transform the Y values\ntrain_y = np.log1p(train['trip_duration'])\n\n# Add some features..\ntrain['distance'] = train.apply(lambda r: haversine.haversine((r['pickup_latitude'], r['pickup_longitude']), (r['dropoff_latitude'], r['dropoff_longitude'])), axis=1)\ntrain['month'] = train.pickup_datetime.dt.month\ntrain['day'] = train.pickup_datetime.dt.day\ntrain['dw'] = train.pickup_datetime.dt.dayofweek\ntrain['h'] = train.pickup_datetime.dt.hour\ntrain['store_and_fwd_flag'] = train['store_and_fwd_flag'].map(lambda x: 0 if x == 'N' else 1)\ntrain = train.drop(['id', 'pickup_datetime', 'dropoff_datetime', 'trip_duration'], axis=1)", "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "0985fc2f-618f-4352-a5c9-21daf9d94adf", "_uuid": "6233ad937c782deb8552f5787fbe1a5e80d8da5d"}, "source": "Next we'll define some models...", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_cell_guid": "6878678d-0548-4710-bd35-8d991fd6b9da", "_uuid": "4cec006839aaf8fd5f1046e00eded582669f602d"}, "source": "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge, Lasso\nfrom lightgbm.sklearn import LGBMRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nlgbm_model = LGBMRegressor(\n    n_estimators=150,\n    subsample=0.85,\n    subsample_freq=5,\n    learning_rate=0.05\n)\n\ncatboost_model = CatBoostRegressor(iterations=150)\nxgb_model = XGBRegressor(objective='reg:linear', n_estimators=150, subsample=0.75)\nrf_model = RandomForestRegressor(n_estimators=25, min_samples_leaf=25, min_samples_split=25)\ntree_model = DecisionTreeRegressor(min_samples_leaf=25, min_samples_split=25)\nknn_model = KNeighborsRegressor(n_neighbors=25, weights='distance')\nridge_model = Ridge(alpha=75.0)\nlasso_model = Lasso(alpha=0.75)", "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "adfb60f6-8a40-4f2b-b77a-4199c121e2e0", "_uuid": "6488efecab16051d009204f374c9ef3382795684"}, "source": "We also want some ensembles to compare to... First we make a averaged ensemble, which simply takes the mean of the sub-models.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_cell_guid": "90615934-73fc-4df5-91de-d414de89ead6", "_uuid": "0211248593ec866b0686ebe4f78060e2df85e9bc"}, "source": "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n\n\nclass AveragingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, regressors):\n        self.regressors = regressors\n        self.predictions = None\n\n    def fit(self, X, y):\n        for regr in self.regressors:\n            regr.fit(X, y)\n        return self\n\n    def predict(self, X):\n        self.predictions = np.column_stack([regr.predict(X) for regr in self.regressors])\n        return np.mean(self.predictions, axis=1)\n    \n    \naveraged_model = AveragingRegressor([catboost_model, xgb_model, rf_model, lgbm_model])", "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_uuid": "f051fc5494bca926018aca37d38b7572328f9fb4"}, "source": "Second ensemble is a stacked model. Mlxtend library includes utility code for this that does the out-of-fold predictions and fitting the second-level model.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_cell_guid": "a5d7faca-422f-4aec-976c-c222a3185ba0", "_uuid": "1607e7fec3f20de19bb495987e6b0ef7b45ff824"}, "source": "from mlxtend.regressor import StackingCVRegressor\n\nstacked_model = StackingCVRegressor(\n    regressors=[catboost_model, xgb_model, rf_model, lgbm_model],\n    meta_regressor=Ridge()\n)", "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "fd96fead-75df-4fd1-b08a-24bbe572af36", "_uuid": "7c8cb131ce7891ae0496cb65da57a90ed3ba6ec5"}, "source": "Now .. let's go and compare how the models do!", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_cell_guid": "48d8a629-8c72-4324-a871-3158066d68e3", "_uuid": "2c5636954b31e2146fdb66e8b099f99ff7a51733"}, "source": "from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\ndef rmse_fun(predicted, actual):\n    return np.sqrt(np.mean(np.square(predicted - actual)))\n\nrmse = make_scorer(rmse_fun, greater_is_better=False)\n\nmodels = [\n     ('CatBoost', catboost_model),\n     ('XGBoost', xgb_model),\n     ('LightGBM', lgbm_model),\n     ('DecisionTree', tree_model),\n     ('RandomForest', rf_model),\n     ('Ridge', ridge_model),\n     ('Lasso', lasso_model),\n     ('KNN', knn_model),\n     ('Averaged', averaged_model),\n     ('Stacked', stacked_model),\n]\n\nscores = [\n    -1.0 * cross_val_score(model, train.values, train_y.values, scoring=rmse).mean()\n    for _,model in models\n]\n\ndataz = pd.DataFrame({ 'Model': [name for name, _ in models], 'Error (RMSE)': scores })\ndataz.plot(x='Model', kind='bar')", "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "fc9908dd-646e-4a18-bfd0-5bf86333f713", "_uuid": "e9cf7360499ff3e7e4618918d9ad475316af2926"}, "source": "Note in above chart, we're plotting the error (RMSE). So lower means better!\n\nWhat we can see is, the linear models (Lasso & Ridge) do particularly bad. I'm assuming this is because the pickup/dropoff coordinates and date/time are important features with a non-linear relation to the ride time, which the linear models can't model.\n\nThe tree based models (XGBoost, CatBoost, LightGBM, RF and DecisionTree) all do pretty good, with RF, XGBoost and LightGBM really shining. (Side note, i didn't tune the catboost model yet...)\n\nAlso interesting is, the ensembled models do even slightly better than the single models. So there is some (minimal) gain from combining models still!", "outputs": [], "cell_type": "markdown", "execution_count": null}], "nbformat": 4, "nbformat_minor": 0}