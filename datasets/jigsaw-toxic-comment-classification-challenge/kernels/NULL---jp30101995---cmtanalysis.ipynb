{"cells":[{"metadata":{"_cell_guid":"62e10414-535f-4622-aad6-3443ae6fc56c","_uuid":"284c9e0874e028db7fdc89244c56c73c8465d5d9","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"97367743-5965-4fec-a30b-d23e8916ed4b","_uuid":"7637a250bf6c26a2ad7fe276e67d85279fc3061e","trusted":false},"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"68f75898-c839-48c3-adf8-459bfa08338e","_uuid":"ffbaa33ebcb61760da5151f2e7e636e56232a728","trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubm = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5244a2b7-ae54-4cf6-b1eb-55cde5877c1c","_uuid":"09e6bc3c36a3ec6a5b5febfe40fd80513e85a713","trusted":false,"collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97355432-9b98-4703-ba48-ca086bfcbfe8","collapsed":true,"_uuid":"b087d70dfcb92e834d443802af997c239b863afe","trusted":false},"cell_type":"code","source":"pd.options.mode.chained_assignment = None\n# nltk for nlp\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\n# list of stopwords like articles, preposition\nstop = set(stopwords.words('english'))\nfrom string import punctuation\nfrom collections import Counter\nimport re\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d3569aa6-8466-46f0-bdfc-052a45d8ca74","collapsed":true,"_uuid":"f3d754d9a0bcd26a8d610794c70f172fb3ffbf38","trusted":false},"cell_type":"code","source":"def tokenizer(text):\n    try:\n        tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text)]\n        \n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n\n        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n        tokens = list(filter(lambda t: t not in punctuation, tokens))\n        tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', \n                                            u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n        filtered_tokens = []\n        for token in tokens:\n            if re.search('[a-zA-Z]', token):\n                filtered_tokens.append(token)\n\n        filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n\n        return filtered_tokens\n    except Error as e:\n        print(e)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ff6d9e6d-b65b-4ef3-a628-914db18428c5","_uuid":"c6cf0c171b75c51e4b57f1bc244c0eec773a9fcc","trusted":false},"cell_type":"code","source":"train['tokens'] = train['comment_text'].map(tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"93168830-9146-4092-b5ef-df7f4c2880e9","_uuid":"d18e7f2653aef814d73ce4648e6916021f8f4372","trusted":false,"collapsed":true},"cell_type":"code","source":"for descripition, tokens in zip(train['comment_text'].head(5), train['tokens'].head(5)):\n    print('description:', descripition)\n    print('tokens:', tokens)\n    print() ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b14d92ca-ae29-4c60-bf82-f61297d86b52","collapsed":true,"_uuid":"a2a9497847c574ca39b124566370d81e0bc48b78","trusted":false},"cell_type":"code","source":"def keywords(category):\n    tokens = train[train[category] == 1]['tokens']\n    alltokens = []\n    for token_list in tokens:\n        alltokens += token_list\n    counter = Counter(alltokens)\n    return counter.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76ffe825-8af5-4c59-a39f-209d10c94144","_uuid":"be07e7c520d82c4ac3de3985f342832e84e2cded","trusted":false,"collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b88a24ad-f36c-4289-b714-98a4cca15a49","_uuid":"3785d28bc2022650e6130c0bb5e606f7f9613694","trusted":false,"collapsed":true},"cell_type":"code","source":"for category in set(['toxic','severe_toxic','obscene', 'threat', 'insult','identity_hate','nill']):\n    print('category :', category)\n    print('top 10 keywords:', keywords(category))\n    print('---')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a345bc6f-c62b-4a64-8a11-6275fa7ec21d","_uuid":"1aeae7d8ab56274a34e7b8b0a2dff788f9f29b18","trusted":false,"collapsed":true},"cell_type":"code","source":"cols = ['toxic', 'severe_toxic', 'obscene', 'insult', 'threat' , 'identity_hate']\ntrain['nill'] = 1 - train[cols].max(axis = 1)\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"10fd2fe1-597f-4190-8deb-ee541bbba53b","_uuid":"3cc05fac69750d3e6c52e6a0bb22695628755caf","trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# min_df is minimum number of documents that contain a term t\n# max_features is maximum number of unique tokens (across documents) that we'd consider\n# TfidfVectorizer preprocesses the descriptions using the tokenizer we defined above\n\nvectorizer = TfidfVectorizer(min_df=10, max_features=100000, tokenizer=tokenizer, ngram_range=(1, 2))\nvz = vectorizer.fit_transform(list(train['comment_text']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7c7ee551-2080-43ef-8133-e97ccd5e3001","_uuid":"08f2e5aeade5beae76e5298e26fdc8abf49cc86a","trusted":false,"collapsed":true},"cell_type":"code","source":"vz.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dfbf9c48-b5c4-4f23-ad6f-24a6ca1eb03e","collapsed":true,"_uuid":"daf55376a11b77a4a25352ffdf6b2e516b8db864","trusted":false},"cell_type":"code","source":"tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\ntfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')\ntfidf.columns = ['tfidf']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de5a66fe-48ff-4d7e-98e3-42b622410d5c","_uuid":"1db39408528c4801973778c4816ef74a5a110d88","trusted":false,"collapsed":true},"cell_type":"code","source":"tfidf.tfidf.hist(bins=50, figsize=(15,7))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"00ad6da7-a8a7-4ed6-9393-2633b322001a","_uuid":"0e7414c7bbc703a188bbfcdc5e30ebb01fbc89de","trusted":false,"collapsed":true},"cell_type":"code","source":"tfidf.sort_values(by=['tfidf'], ascending=True).head(30)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1cf51183-4a26-4e52-bc89-f208fe24d172","_uuid":"0322049e0a9a78847a5c27dbe0c072dcc3df6c6e","trusted":false,"collapsed":true},"cell_type":"code","source":"tfidf.sort_values(by=['tfidf'], ascending=False).head(30)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"039f1ed0-285b-46c5-a240-70695e673192","collapsed":true,"_uuid":"354c1d62ec0d757aaa6307e9328b4adf7c67ccd5","trusted":false},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=50, random_state=0)\nsvd_tfidf = svd.fit_transform(vz)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f86f431-d219-45f0-ad52-2220145d2c9c","_uuid":"d48d1d6705a5d7977177ee8c06d0f1c3144ed7bb","trusted":false,"collapsed":true},"cell_type":"code","source":"svd_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9254e48f-e7bf-4318-8221-05dbc05e0cdc","collapsed":true,"_uuid":"bd44cf4598a623ef21f9cbd4bbb21f0b43add123","trusted":false},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0)\ntsne_tfidf = tsne_model.fit_transform(svd_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce260064-e3eb-4bf2-a828-0a657070a688","collapsed":true,"_uuid":"979f9b52311a7c749b0e7cb19be3703bf01e5e1e","trusted":false},"cell_type":"code","source":"tsne_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bfcef8be-498f-420a-8a70-70923c94bdf4","collapsed":true,"_uuid":"973b7032fe01d9952377fab3c49f100d738ab414","trusted":false},"cell_type":"code","source":"import bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.plotting import figure, show, output_notebook","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"136e1b08-d04d-43ef-90bd-3b9b07fed938","collapsed":true,"_uuid":"088ece86fa84e571b285cc65c53be06cde86899a","trusted":false},"cell_type":"code","source":"output_notebook()\nplot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"tf-idf clustering of the comments\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n    x_axis_type=None, y_axis_type=None, min_border=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"18f5feb7-98a9-4b4b-93a4-691dd64c3c02","collapsed":true,"_uuid":"5142a64f79cf66a7dab66691573ed640d90b15f3","trusted":false},"cell_type":"code","source":"tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\ntfidf_df['comment_text'] = train['comment_text']\ntfidf_df['toxic'] = data['toxic']\ntfidf_df['severe_toxic'] = data['severe_toxic']\ntfidf_df['obscene'] = data['obscene']\ntfidf_df['insult'] = data['insult']\ntfidf_df['threat'] = data['threat']\ntfidf_df['identity_hate'] = data['indentity_hate']\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c77b5c84-5744-438e-beaa-80126f1effcb","collapsed":true,"_uuid":"482e5e5ca590074083a3df74091be19616803c0f","trusted":false},"cell_type":"code","source":"plot_tfidf.scatter(x='x', y='y', source=tfidf_df)\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips={\"description\": \"@description\", \"category\":\"@toxic @severe_toxic @insult @obscene @threat @idenity_hate\" }\nshow(plot_tfidf)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","version":"3.6.4","nbconvert_exporter":"python","name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}