{"cells":[{"metadata":{"_cell_guid":"0d97c7c2-aedf-4554-8758-db261341ade6","_uuid":"3b977605d317553e606751047328df50c1ca3fc0"},"cell_type":"markdown","source":"To Spellcheck or Not?\n===\n\n**Pros: **\n* Fewer unknown tokens\n* Reduced noise\n* More uniform train and test sets (maybe)\n\n**Cons: **\n* Misspellings are a potential feature, by correcting them we could be *losing* information!\n* Many bad spelling corrections could add noise\n\nHow much spelling correction helps (or hurts) probably depends somewhat on your model and what other pre-processing you're doing. For me, training on spell checked inputs didn't make any noticable difference with a single model. But, I saw a decent improvement when ensembling the results of spellchecked models with non-spellchecked models.\n\nThe Spellchecker...\n===================\nI found a nice python package that does a solid job of spellchecking individual words: [Autocorrect](https://github.com/phatpiglet/autocorrect/). Huge thanks to [phatpiglet](http://phatpiglet.com/) and the rest of the contributors. Given a word, it returns its best guess at a spelling correction. If the word isn't misspelled or it's so badly misspelled that the module can't come up with any suggestions, it just returns the input word unmodified.\n\nUnfortunately, it's not installed on Kaggle and I can't seem to install it. So I added the stub below to get it working here. If you want to run it yourself locally just install autocorrect and change `RUNNING_ON_KAGGLE` to `False`"},{"metadata":{"_cell_guid":"e60f9dc2-b021-4750-a37b-97a92a421b5d","_uuid":"c2f8966971f12aa96e30d9b77c57f409fe1d23fd","trusted":false,"collapsed":true},"cell_type":"code","source":"RUNNING_ON_KAGGLE = True\nif RUNNING_ON_KAGGLE:\n    def spell(word):\n        return word\nelse: \n    from autocorrect import spell\n    \nspell('horse')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a7ab95e4-e7f7-4eb6-9a33-73fe7bc6c690","_uuid":"62444269c3f33403668d309e2bd475895bbd8f7c"},"cell_type":"markdown","source":"... is slow\n===================\nRunning on my machine it takes around 0.001 second to correct one, 5-letter word, 0.17 seconds to correct an 8-letter word and 0.26 seconds for a 10-letter word. If we were to try correcting every word in every comment in both the training and test sets it would take *roughly*: \n\n~0.01 sec/word \\* ~100 words/comment \\* 320,000 comments = 320,000 seconds or **88 hours!**\n\nBrute force is pretty obviously a bad idea. Fortunatly with a few tweaks we can get it running much faster:\n* Only try to spellcheck words that are missing form the word embedding\n* Skip words that are >24 characters long, they take *forever* to correct and are usually beyond repair anyway\n* Multiple processes\n\nThe result:"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"from multiprocessing import Pool, cpu_count\nimport pandas as pd\nimport re\n\n\ndef get_known_words(word_embeddings_file):\n    words = set()\n    with open(word_embeddings_file,encoding='utf8') as f:\n        for line in f:\n            values = line.rstrip().rsplit(' ')\n            words.add(values[0].lower())\n    return words\n\n\nEMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\nif RUNNING_ON_KAGGLE:\n    words = set()\nelse:\n    words = get_known_words(EMBEDDING_FILE)\n\n\ndef chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\n\ndef spell_check(chunk):\n    fixed_rows = []\n    for i,row in chunk.iterrows():\n        fxd_words = []\n        comment = row['comment_text'].lower()\n        comment = re.sub('[^a-zA-Z ]+', '', comment)\n        for w in comment.split():\n            if w is None:\n                continue\n            if w in words or len(w) > 24:\n                fxd_words.append(w)\n            else:\n                fxd_words.append(spell(w).lower())\n        sp_comment = ' '.join(fxd_words)\n        fixed_rows.append((row[0],sp_comment))\n    return fixed_rows\n\n\nPROC_COUNT = cpu_count()\nCHUNK_SIZE = 1024\npool = Pool(PROC_COUNT)\n\n# Uncomment line below to run\nfor set_name in [] #['train', 'test']:\n    source = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/'+set_name+'.csv') # remove [:100] to proces all examples\n    source['comment_text'] = source['comment_text'].astype(str)\n    result = source.copy()\n\n    fixed_rows = pool.map(spell_check,chunker(source,CHUNK_SIZE))\n    for fxd_row in fixed_rows:\n        for index,fixed_comment in fxd_row:\n            result.set_value(index,'comment_text',fixed_comment)\n\n    if RUNNING_ON_KAGGLE:\n        print(result)\n    else:\n        result.to_csv('sp_check_'+set_name+'.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"On my machine it took just **a little over 2 hours **to run, much better than 88. \n\nI included my outputs in the data section of this kernal feel free to use them with your own models. Note: I also used 's [Prashant Kikani](https://www.kaggle.com/prashantkikani)'s [super-awesome preprocessing code](https://www.kaggle.com/prashantkikani/pooled-gru-glove-with-preprocessing) on my inputs first. Thanks Prashant!\n\nLet me know if you have any questions, comments, suggestions or improvements! I'd love to get your feedback. Thanks for reading."},{"metadata":{"_cell_guid":"0f3a6214-3300-439c-90d4-47a00c8a419a","collapsed":true,"_uuid":"0690c57a9e41b943e9929735287819c30850e0a8","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","version":"3.6.4","name":"python"}},"nbformat":4,"nbformat_minor":1}