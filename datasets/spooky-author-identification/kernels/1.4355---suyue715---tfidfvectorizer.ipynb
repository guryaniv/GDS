{"cells": [{"source": ["## Technical report\n", " a detailed account of your final model \n", " \n", " assumptions that you made during your modeling \n", " \n", " \n", " \n", " \n", " "], "cell_type": "markdown", "metadata": {"_uuid": "9a194413742870233b45243cf59ef4f2df80aee6", "_cell_guid": "ddbc96ba-790e-40e1-a0a0-7d1b977e70a2"}}, {"source": ["How you acquired your data (including any sampling that you did)\n", "\n", "\n", "\n", "How the data should be transformed, including justifying your choices\n", "\n", "How you operationalized your outcome variable, including your justification\n", "\n", "Your choice of model and any hyperparameters, including what metric or metrics you use to determine a successful model\n", "\n", "Any future deployment strategies, additions of data, or modeling techniques you have yet to try"], "cell_type": "markdown", "metadata": {"_uuid": "d5c730dd237abbaa474e475149aea6fba4ef6fe1", "_cell_guid": "44bfe7ca-d682-4476-b0a1-30009bfae3a4"}}, {"source": ["1. Project objective: Identify the author of the sentences in the test set.\n", "\n", "2. Dataset: Works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley\n", "\n", "3. Initial EDA: Variable distributions, correlations, etc\n", "\n", "4.Train/Test Split\n", "\n", "5.Preprocessing performed on Training set: data types converted, missing data handled, dummy variables created, data parsed for errors\n", "\n", "6.-Initial model created\n", "\n", "7.-Initial model evaluated appropriately (R^2, RMSE, MAE, or Precision/Recall/F1)\n", "\n", "some reference: https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb"], "cell_type": "markdown", "metadata": {"_uuid": "65afb590a5cae2d0da62fdadc5363e33fc15af2a", "_cell_guid": "420f16dd-4b8a-480a-b0ff-394a6d149711"}}, {"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "\n", "from nltk.corpus import stopwords\n", "from nltk.tokenize import word_tokenize \n", "from nltk.stem import SnowballStemmer\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "import spacy\n", "from textblob import TextBlob\n", "\n", "# Any results you write to the current directory are saved as output.\n", "from keras.preprocessing import sequence\n", "from keras.utils import np_utils\n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Activation, Embedding\n", "from keras.layers import LSTM\n", "\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import confusion_matrix, classification_report\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.decomposition import LatentDirichletAllocation\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.decomposition import TruncatedSVD"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "35619e3e5c15f15b3d0fcbee4d59cc00bf237438", "_cell_guid": "4d627dd6-eac2-40c7-a0f0-111c77ea0c27"}}, {"source": ["#import the datasets\n", "train=pd.read_csv('../input/train.csv')\n", "test=pd.read_csv('../input/test.csv')\n", "sample=pd.read_csv('../input/sample_submission.csv')"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "8e8d87623b9b9a89150425a43ffe2cbc4ab0b364", "_cell_guid": "f2b503ab-fec4-4776-a1b0-dffc833154df", "collapsed": true}}, {"source": ["## ** Preprocessing**"], "cell_type": "markdown", "metadata": {"_uuid": "47928c1e1e11ef06fe30d98489c36772a411f896", "_cell_guid": "65b75403-a5b0-42d7-aac6-a9bca2cf086b"}}, {"source": ["**1.Assign the authors with numbers to a new column 'author_num' to the original dataframe**"], "cell_type": "markdown", "metadata": {"_uuid": "f150ecf0825e14013e3703d7811d4b60a5487652", "_cell_guid": "5aba8a13-6229-44b2-9461-13f9ba5a469e"}}, {"source": ["#Assign the authors with numbers to a new column 'author_num'\n", "#0 for 'EAP'\n", "#1 for 'HPL'\n", "#2 for 'MWS'\n", "train['author_num']=train['author'].apply({'EAP':0,  'HPL':1,'MWS':2}.get)\n", "train.head()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "680e34cf8436fe7df9b693e47c228b861c46e809", "_cell_guid": "dd7d3b28-86e0-4fde-9fe3-3fd276cfd415"}}, {"source": ["#Assign the features and target\n", "X_text_train=train['text'].values\n", "X_text_test=test['text'].values\n", "y=train['author_num'].values\n", "num_labels = len(np.unique(train['author_num']))"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "2e5b50c8aa4042b401e11ee304213b9dfc32c77e", "_cell_guid": "34c1d63f-2fd7-4496-aa59-8b7170b514a9", "collapsed": true}}, {"source": ["**2. Removing the stopwords, punctuations and stemming the words**"], "cell_type": "markdown", "metadata": {"_uuid": "1fd03387235f8e9ca5dbda9159ab138c37b82551", "_cell_guid": "01e532cc-5cd0-4199-91d4-bc996abd49d1"}}, {"source": ["#Define the stopwords to remove and the stemming tool\n", "stop_words = set(stopwords.words('english'))\n", "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n", "stemmer = SnowballStemmer('english')"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0108a1847fd5e0837dd33a37d59abd6774fc8bf2", "_cell_guid": "d152b67f-ee66-4045-83ab-96fdae5547cd", "collapsed": true}}, {"source": ["## NLTK tokenize package: http://www.nltk.org/api/nltk.tokenize.html\n", "\n", "**word_tokenize** "], "cell_type": "markdown", "metadata": {"_uuid": "81063047cce94a177e4996c402eb561ef2ccaa39", "_cell_guid": "697d7832-b0a0-48d7-acbd-0705e64207d4"}}, {"source": ["#Preprocess the text in training and testing\n", "processed_train = []\n", "for doc in X_text_train:\n", "    tokens = word_tokenize(doc)\n", "    filtered = [word for word in tokens if word not in stop_words]\n", "    stemmed = [stemmer.stem(word) for word in filtered]\n", "    processed_train.append(stemmed)\n", "    \n", "processed_test = []\n", "for doc in X_text_test:\n", "    tokens = word_tokenize(doc)\n", "    filtered = [word for word in tokens if word not in stop_words]\n", "    stemmed = [stemmer.stem(word) for word in filtered]\n", "    processed_test.append(stemmed)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "70d0b6c9f650ed423ba9cd31e06ec79ab06783e3", "_cell_guid": "e0d83d27-812e-402a-a6ee-2c82a5bfb35e", "collapsed": true}}, {"source": [], "cell_type": "markdown", "metadata": {"_uuid": "3471e59535200af3bb661f769178bd3e3f3f983a", "_cell_guid": "d19f9556-8c3a-4ddc-9e30-0a82b0eb7737"}}, {"source": ["X_text_train[1]"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "a117aeffd262580f6d9d9c4a3e9517df7b87a682", "_cell_guid": "14396488-e309-4da3-bbc3-f2a13ea0545f"}}, {"source": ["processed_train[1]"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "979d8d4cd76d26ea636057ee482e304653ba87fe", "_cell_guid": "160cbe3a-5076-4011-b9ee-dbb4b6ac037a", "scrolled": true}}, {"source": ["train['processed_train']=processed_train"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "960d0b53ff94ba511c0817dcd0699900cc1c9354", "_cell_guid": "fe331b06-1dc3-4f99-93f1-8824a8052934", "collapsed": true}}, {"source": ["train.head()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "d63ef2096d49cb19a665a40a0fca232eca2ba610", "_cell_guid": "129c5a23-e90d-47cc-80d6-2b88c64ed63d"}}, {"source": ["train.columns"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "545e6dca2762f784134051588050c1b14f18f34f", "_cell_guid": "ac9823cc-ea50-4be2-b496-dceab2ca15cb"}}, {"source": ["row_lst = []\n", "for lst in train.loc[:,'processed_train']:\n", "    text = ''\n", "    for word in lst:\n", "        text = text + ' ' + word\n", "    row_lst.append(text)\n", "\n", "train['final_processed_text'] = row_lst"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "a2ae9da7f953584692d02ab55c8b2dea7a2d741a", "_cell_guid": "754563cd-63c0-41fe-b719-09ef89d51bc4", "collapsed": true}}, {"source": ["test['processed_test']=processed_test\n", "test.head()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0c392bcca2c9a4acc85f28473f5b9f54998647b4", "_cell_guid": "1ecf4f2a-12a8-4efb-895d-6516f934f2e5"}}, {"source": ["row_lst = []\n", "for lst in test.loc[:,'processed_test']:\n", "    text = ''\n", "    for word in lst:\n", "        text = text + ' ' + word\n", "    row_lst.append(text)\n", "\n", "test['final_processed_test'] = row_lst\n", "\n", "test.head()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0fb8e0d6997d7dfaa9d00e1853aafd38145600e6", "_cell_guid": "864da92b-0437-4f57-bf7f-c402320895bc"}}, {"source": ["train.head()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "2ebb5cfe73ea32f1aa83c6c5b6d7cec247bfe3a8", "_cell_guid": "a4f1645e-43b2-467c-9fe2-3839d3c38e2c"}}, {"source": ["## **Named Entities**"], "cell_type": "markdown", "metadata": {"_uuid": "4ec26611fa00d711ebfce208883a90c8253e6168", "_cell_guid": "fc898a26-48e9-4e21-af55-8ea410afee44"}}, {"source": ["Named entities are business, people, countries, or other things that refer to a specific person, place, or thing (think Apple, computer manufacturer versus apple, delicious crunchy fruit). spaCy can identify named entities for us, which we can either highlight or drop from our analyses.\n"], "cell_type": "markdown", "metadata": {"_uuid": "e992b9232fc44f17d27ba98290712026255f0bd5", "_cell_guid": "282a0a67-6ff1-42d4-8300-ba5f7c881f9b"}}, {"source": ["nlp = spacy.load('en')\n", "content=[]\n", "for i in train['processed_train']:\n", "    content.append(i)\n", "\n", "# for named_entity in content.ents:\n", "#     print(named_entity.text, named_entity.label_)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "26d9df7fe077c0f4e04e6a5ecc8fe1d348e89077", "_cell_guid": "b4490d9f-8a9c-47f8-a577-28d973041462", "collapsed": true}}, {"source": ["\n", "##  Use LDA to identify topic"], "cell_type": "markdown", "metadata": {"_uuid": "c73f59ffd8e7cfc1fccb5dde98954f9d78370257", "_cell_guid": "9a53f4a8-fddf-46ec-b04d-7136f66945ae"}}, {"source": ["from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n", "cv = CountVectorizer(stop_words='english')\n", "cv.fit(train['text'])\n", "X = cv.transform(train['text'])\n", "feature_names = cv.get_feature_names()\n", "\n", "lda = LatentDirichletAllocation(n_components=10)\n", "lda.fit(X)\n", "\n", "results = pd.DataFrame(lda.components_,\n", "                      columns=feature_names)\n", "\n", "for topic in range(10):\n", "    print('Topic', topic)\n", "    word_list = results.T[topic].sort_values(ascending=False).index\n", "    print(' '.join(word_list[0:25]), '\\n')"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "9e053ec7205255844e3d5f442f8df157970464eb", "_cell_guid": "75d5e233-3b82-4a00-aaef-61cfd24a4f30"}}, {"source": ["## RandomForestClassifier\n", "\n"], "cell_type": "markdown", "metadata": {"_uuid": "44704d12f21dcf40190c9d52b0321cb7441399a9", "_cell_guid": "433c825f-a899-4722-bbc8-9250b5d4ca75"}}, {"source": ["X_train, X_test, y_train, y_test = train_test_split(train['final_processed_text'],\n", "                                                   train['author_num'],\n", "                                                   test_size=0.33,\n", "                                                   random_state=8675309)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0d3d009d95f08b780fec3d1f55d6402c45d7a08d", "_cell_guid": "32f35225-ed7c-4c8e-9784-4878076fa7ad", "collapsed": true}}, {"source": ["\n", "cv = CountVectorizer(stop_words='english')\n", "cv.fit(X_train)\n", "\n", "X_train_cv = cv.transform(X_train)\n", "X_test_cv = cv.transform(X_test)\n", "\n", "rf = RandomForestClassifier()\n", "rf.fit(X_train_cv, y_train)\n", "print(rf.score(X_test_cv, y_test))\n", "predictions = rf.predict(X_test_cv)\n", "print(confusion_matrix(y_test, predictions))\n", "print(classification_report(y_test, predictions))"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "8c5b12f0a6c8019fc3a978cb601e1cfe045121fe", "_cell_guid": "0dbede8f-09cb-45a8-97f2-05091d71354b", "scrolled": true}}, {"source": ["## TfidfVectorizer() with English stop words"], "cell_type": "markdown", "metadata": {"_uuid": "ad06dd7594854738a29fd78db77353d0a542fdb1", "_cell_guid": "8d1aaafb-ad95-4aa3-8bb5-eefccd763fa5"}}, {"source": [], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "f1349738c034ef49fd9fca9b131866f853cf5a57", "_cell_guid": "ec655b82-a4ac-4359-9b45-59d3f23e1c97", "collapsed": true}}, {"source": ["\n", "tfidf = TfidfVectorizer(stop_words='english')\n", "tfidf.fit(X_train)\n", "\n", "X_train_tfidf = tfidf.transform(X_train)\n", "X_test_tfidf = tfidf.transform(X_test)\n", "test_tfidf = tfidf.transform(test['final_processed_test'])\n", "\n", "rf = RandomForestClassifier()\n", "rf.fit(X_train_tfidf, y_train)\n", "print(rf.score(X_test_tfidf, y_test))\n", "predictions = rf.predict(X_test_tfidf)\n", "print(confusion_matrix(y_test, predictions))\n", "print(classification_report(y_test, predictions))"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "12e0b979937c3f760df53f740f885a3e23c2fdae", "_cell_guid": "734a694c-9549-4ab8-b1ab-f02205ffc1d3"}}, {"source": [], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "b10ec966f8c3ff9a0dd0e1385ff47c6d075a7478", "_cell_guid": "13f16290-b9b8-4279-848b-f93b6cc49997", "collapsed": true}}, {"source": ["pred=rf.predict_proba(test_tfidf)\n", "prob=pd.DataFrame(pred,columns=['EAP','HPL','MWS'])\n", "submit1=pd.concat([test, prob], axis=1)\n", "del submit1['text']"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "6cdbe42781abdb3bcfb4c1954293e9595db8f2fa", "_cell_guid": "f8fefb76-89c9-4c52-8061-ec74b2100e03", "collapsed": true}}, {"source": [], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "3840ae27057ea73fb9e037b7cb9f552b8c9b4022", "_cell_guid": "561ad4d3-d4d2-4a8d-b89b-67eab539f2f5", "collapsed": true}}, {"source": ["del submit1['processed_test']"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "8bbd8d9c2ad88b70f98bf4fd523d651c625fdd43", "_cell_guid": "30323bd3-5ac7-4fbb-a589-2576d4b928b2", "collapsed": true}}, {"source": ["del submit1['final_processed_test']"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "8f36f92713a9ded3566670703753ae4bffc98e19", "_cell_guid": "59ffa1e9-2675-4c92-a320-9b674b23743f", "collapsed": true}}, {"source": ["submit1"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "754cae13033aff334c1465c3119bbc4b0eb2faea", "_cell_guid": "7423e10f-e379-4edc-94ad-b0c1a070530c"}}, {"source": ["submit1.to_csv('./TfidfVectorizer.csv', index=False, header=True)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "546d336bcbc9bcb5b95e9e14b453d58c30e4d3e6", "_cell_guid": "8f818bad-7846-469a-93f7-47114e34bb0b", "collapsed": true}}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "version": "3.6.3", "nbconvert_exporter": "python"}}}