{"cells":[{"metadata":{"_uuid":"2477a29951c9fed6f676e8681f876c026b317a6c"},"cell_type":"markdown","source":"# Table of Content\n1. [Introduction](#introduction)\n2. [Preparation](#preparation)\n3. [Feature Extraction](#feature_extraction)   \n4. [EDA](#eda)\n    1. [Time Series](#eda_time_series)\n    2. [Missing Values (Professionals)](#eda_missing_values_professionals)\n    3. [Missing Values (Students)](#eda_missing_values_students)\n    4. [Tags Matching](#eda_tags_matching)\n    5. [First Activity](#eda_first_activity)\n    6. [Last Activity](#eda_last_activity)\n    7. [First Answer](#eda_first_answer)\n    8. [Email Response Time](#eda_email_response)\n    9. [Word Count](#eda_wordcound)\n5. [Ad Hoc Analysis](#ad_hoc)\n    1. [User Activities](#ad_hoc_user_activites)\n    2. [Dependency Graph](#ad_hoc_dependency)\n6. [Recommendation](#recommendation)\n    1. [Example 1](#recommender_example1)\n    2. [Example 2](#recommender_example2)\n7. [Topic Model (LDA)](#topic_model)\n    1. [Model](#lda_model)\n    2. [Topics](#lda_topics)\n    3. [Document-Topic Probabilities](#lda_doc_topic_prob)\n    4. [Example 1](#lda_example1)\n    5. [Example 2](#lda_example2)"},{"metadata":{"_uuid":"6b382d6f7f23a78d528ddcc4891b336fc5edce52"},"cell_type":"markdown","source":"# 1. Introduction <a id=\"introduction\"></a>  \nWith this notebook I would like to show some approaches for a new recommendation engine at [www.careervillage.org](http://www.careervillage.org).  \nThe figure below shows the data available to us and their dependencies. The most important datasets are \"Questions\", \"Answers\", \"Professionals\" and \"Tags\". Users can join a school oder group membership, which is currently not used by many.  \nIn Section [Preparation](#preparation) we load all necesary libraries, available csv files and set some global parameters. After this we use some [Feature extraction](#features_extraction) to create some new data columns for the analysis and natural language processing. We continue with some [Exploratory Data Analysis (EDA)](#eda), to get some insight into the current data.  \nIn addition, there is a section for [ad hoc analyses](#ad_hoc). These should make it possible to analyse the [activities of users](#ad_hoc_user_acitivies) and to check the [dependency](#ad_hoc_dependency) between questions, answers,  users and tags.  \nWith a [Topic Model (LDA)](#topic_model) we would like to identify different topics proportion of the questions. The advantage of LDA is that a question can have several topics and not only one. We can take advantage of this by identifying which topics are preferred by professionals when they answering the questions and use this for new recommendations.  \nThe [Recommendation Engine](#recommendation) itself should be answer the following questions (not yet finished):  \n* When a student asks a question, are there already any similar questions that might help him?  \n* Which professionals are most expected to answer the new question?  \n* Which questions should be forwarded to a professional to answer?  \n* Which hashtags might interest a user because of his previous activities?\n\n\n![workflow_diagram](https://i.imgur.com/zzAo1JD.jpg)"},{"metadata":{"_uuid":"09bdf3f10f1df59055536781ff5a2a01a747ba10"},"cell_type":"markdown","source":"# 2. Preparation <a id=\"preparation\"></a>"},{"metadata":{"_uuid":"e2d5c571db0b74aaa1feb51cd0940bf1493136a8"},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"trusted":true,"_uuid":"6d4be46cd101250d3bbdeb85f8654c69e0e26a0a","_kg_hide-input":false},"cell_type":"code","source":"import os\n#import datetime\nimport math\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.dates as mdates\nfrom datetime import datetime\nimport networkx as nx\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport spacy\nnlp = spacy.load('en')\nnlp.remove_pipe('parser')\nnlp.remove_pipe('ner')\n#nlp.remove_pipe('tagger')\n\nfrom wordcloud import WordCloud\n\nimport gensim\nimport pyLDAvis\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()\n\n#import plotly.offline as py\n#py.init_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"913c99f441849620c3dd45533c91568a9cb27957"},"cell_type":"markdown","source":"## Read CSV Files"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"input_dir = '../input'\n#print(os.listdir(input_dir))\n\nprofessionals = pd.read_csv(os.path.join(input_dir, 'professionals.csv'))\ngroups = pd.read_csv(os.path.join(input_dir, 'groups.csv'))\ncomments = pd.read_csv(os.path.join(input_dir, 'comments.csv'))\nschool_memberships = pd.read_csv(os.path.join(input_dir, 'school_memberships.csv'))\ntags = pd.read_csv(os.path.join(input_dir, 'tags.csv'))\nemails = pd.read_csv(os.path.join(input_dir, 'emails.csv'))\ngroup_memberships = pd.read_csv(os.path.join(input_dir, 'group_memberships.csv'))\nanswers = pd.read_csv(os.path.join(input_dir, 'answers.csv'))\nstudents = pd.read_csv(os.path.join(input_dir, 'students.csv'))\nmatches = pd.read_csv(os.path.join(input_dir, 'matches.csv'))\nquestions = pd.read_csv(os.path.join(input_dir, 'questions.csv'))\ntag_users = pd.read_csv(os.path.join(input_dir, 'tag_users.csv'))\ntag_questions = pd.read_csv(os.path.join(input_dir, 'tag_questions.csv'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6e7d123adb9929ad477099c0e7d6e635924a18b"},"cell_type":"markdown","source":"## Global Parameters"},{"metadata":{"trusted":true,"_uuid":"7f8541c637620924140dad5ddc739a175067af3b"},"cell_type":"code","source":"warnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', -1)\n\nseed = 13\nrandom.seed(seed)\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3eae14cbe5d63c41176b6af25ab66dd57943d16"},"cell_type":"markdown","source":"# 3. Features extraction <a id=\"feature_extraction\"></a> "},{"metadata":{"_uuid":"be3907611ab722da9836e599c5f46155a537844d"},"cell_type":"markdown","source":"## Parameters"},{"metadata":{"trusted":true,"_uuid":"3b22dc7ef369701cd50a599d4d7898ad0de9acbb"},"cell_type":"code","source":"# Spacy Tokenfilter for part-of-speech tagging\ntoken_pos = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'INTJ', 'X']\n\n# The data export was from 1. February 2019. For Production use datetime.now()\nactual_date = datetime(2019, 2 ,1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"381f5a983596fa8a3ab585a168b6b9b3aaac09b7"},"cell_type":"markdown","source":"## Functions"},{"metadata":{"trusted":true,"_uuid":"cfc55238256ffcb2125573fcecff97a0b5c4ef0f","_kg_hide-input":true},"cell_type":"code","source":"def nlp_preprocessing(data):\n    \"\"\" Use NLP to transform the text corpus to cleaned sentences and word tokens\n\n    \"\"\"    \n    def token_filter(token):\n        \"\"\" Keep tokens who are alphapetic, in the pos (part-of-speech) list and not in stop list\n\n        \"\"\"    \n        return not token.is_stop and token.is_alpha and token.pos_ in token_pos\n    \n    processed_tokens = []\n    data_pipe = nlp.pipe(data)\n    for doc in data_pipe:\n        filtered_tokens = [token.lemma_.lower() for token in doc if token_filter(token)]\n        processed_tokens.append(filtered_tokens)\n    return processed_tokens","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86ca1667419efbd80c2e81940fc98a9c5d779c10"},"cell_type":"markdown","source":"## Features\nFirst we transform the datetime columns (*date_added and date_joined*), so that we can work with time delta functions.  \nAfter this we create the following new columns:  \n\n**DataFrame Questions:**  \n* questions_full_text: Merge the questions title with the body for later use of NLP.  \n* questions_answers_count: How many answers a question has.  \n* questions_first_answers: The timestamp for the first answer of the question.  \n* questions_last_answers: The timestamp for the last answer of the question.  \n* nlp_tokens: Extract relevant tokens from the question full text.  \n\n**DataFrame Answers:**  \n* time_delta_answert: Time delta from question to answer.  \n\n**DataFrame Professionals:**  \n* professionals_time_delta_joined: Time since creating the account.  \n* professionals_answers_count: Number of written answers.  \n* professionals_comments_count: Number of written comments.  \n* date_last_answer: Date last answer.  \n* date_first_answer: Date first answer.  \n* date_last_comment: Date last comment.  \n* date_first_comment: Date first comment.  \n* date_last_activity: Date last activity (answer or comment).  \n* date_first_activity: Date first activity (answer or comment).  \n\n**DataFrame Students:**  \n* students_time_delta_joined: Time since creating the account.  \n* students_questions_count: Number of written questions.  \n* students_comments_count: Number of written comments.  \n* date_last_questions: Date last question.  \n* date_first_questions: Date first question.  \n* date_last_comment: Date last comment.  \n* date_first_comment: Date first comment.  \n* date_last_activity: Date last activity (question or comment).  \n* date_first_activity: Date first activity (question or comment).  \n\n**New DataFrame emails_response:**  \nHas the response activity from professionals to emails and additional informations about the questions behind.\n* time_delta_email_answer: Time needed the question was answered after the email was send.  \n* time_delta_question_email: Time needed the email was send after the questions was written."},{"metadata":{"trusted":true,"_uuid":"f38d504ab3c3bb63c9592760203d36f37e476ed1","_kg_hide-input":true,"_kg_hide-output":false,"scrolled":true},"cell_type":"code","source":"%%time\n# Transform datatypes\nquestions['questions_date_added'] = pd.to_datetime(questions['questions_date_added'], infer_datetime_format=True)\nanswers['answers_date_added'] = pd.to_datetime(answers['answers_date_added'], infer_datetime_format=True)\nprofessionals['professionals_date_joined'] = pd.to_datetime(professionals['professionals_date_joined'], infer_datetime_format=True)\nstudents['students_date_joined'] = pd.to_datetime(students['students_date_joined'], infer_datetime_format=True)\nemails['emails_date_sent'] = pd.to_datetime(emails['emails_date_sent'], infer_datetime_format=True)\ncomments['comments_date_added'] = pd.to_datetime(comments['comments_date_added'], infer_datetime_format=True)\n\n### Questions\n# Merge Question Title and Body\nquestions['questions_full_text'] = questions['questions_title'] +'\\r\\n\\r\\n'+ questions['questions_body']\n# Count of answers\ntemp = answers.groupby('answers_question_id').size()\nquestions['questions_answers_count'] = pd.merge(questions, pd.DataFrame(temp.rename('count')), left_on='questions_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# First answer for questions\ntemp = answers[['answers_question_id', 'answers_date_added']].groupby('answers_question_id').min()\nquestions['questions_first_answers'] = pd.merge(questions, pd.DataFrame(temp), left_on='questions_id', right_index=True, how='left')['answers_date_added']\n# Last answer for questions\ntemp = answers[['answers_question_id', 'answers_date_added']].groupby('answers_question_id').max()\nquestions['questions_last_answers'] = pd.merge(questions, pd.DataFrame(temp), left_on='questions_id', right_index=True, how='left')['answers_date_added']\n# Get NLP Tokens\nquestions['nlp_tokens'] = nlp_preprocessing(questions['questions_full_text'])\n\n### Answers\n# Days required to answer the question\ntemp = pd.merge(questions, answers, left_on='questions_id', right_on='answers_question_id')\nanswers['time_delta_answer'] = (temp['answers_date_added'] - temp['questions_date_added'])\n\n\n### Professionals\n# Time since joining\nprofessionals['professionals_time_delta_joined'] = actual_date - professionals['professionals_date_joined']\n# Number of answers\ntemp = answers.groupby('answers_author_id').size()\nprofessionals['professionals_answers_count'] = pd.merge(professionals, pd.DataFrame(temp.rename('count')), left_on='professionals_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# Number of comments\ntemp = comments.groupby('comments_author_id').size()\nprofessionals['professionals_comments_count'] = pd.merge(professionals, pd.DataFrame(temp.rename('count')), left_on='professionals_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# Last activity (Answer)\ntemp = answers.groupby('answers_author_id')['answers_date_added'].max()\nprofessionals['date_last_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_answer')), left_on='professionals_id', right_index=True, how='left')['last_answer']\n# First activity (Answer)\ntemp = answers.groupby('answers_author_id')['answers_date_added'].min()\nprofessionals['date_first_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_answer')), left_on='professionals_id', right_index=True, how='left')['first_answer']\n# Last activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].max()\nprofessionals['date_last_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_comment')), left_on='professionals_id', right_index=True, how='left')['last_comment']\n# First activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].min()\nprofessionals['date_first_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_comment')), left_on='professionals_id', right_index=True, how='left')['first_comment']\n# Last activity (Total)\nprofessionals['date_last_activity'] = professionals[['date_last_answer', 'date_last_comment']].max(axis=1)\n# First activity (Total)\nprofessionals['date_first_activity'] = professionals[['date_first_answer', 'date_first_comment']].min(axis=1)\n\n### Students\n# Time since joining\nstudents['students_time_delta_joined'] = actual_date - students['students_date_joined']\n# Number of answers\ntemp = questions.groupby('questions_author_id').size()\nstudents['students_questions_count'] = pd.merge(students, pd.DataFrame(temp.rename('count')), left_on='students_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# Number of comments\ntemp = comments.groupby('comments_author_id').size()\nstudents['students_comments_count'] = pd.merge(students, pd.DataFrame(temp.rename('count')), left_on='students_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# Last activity (Question)\ntemp = questions.groupby('questions_author_id')['questions_date_added'].max()\nstudents['date_last_question'] = pd.merge(students, pd.DataFrame(temp.rename('last_question')), left_on='students_id', right_index=True, how='left')['last_question']\n# First activity (Question)\ntemp = questions.groupby('questions_author_id')['questions_date_added'].min()\nstudents['date_first_question'] = pd.merge(students, pd.DataFrame(temp.rename('first_question')), left_on='students_id', right_index=True, how='left')['first_question']\n# Last activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].max()\nstudents['date_last_comment'] = pd.merge(students, pd.DataFrame(temp.rename('last_comment')), left_on='students_id', right_index=True, how='left')['last_comment']\n# First activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].min()\nstudents['date_first_comment'] = pd.merge(students, pd.DataFrame(temp.rename('first_comment')), left_on='students_id', right_index=True, how='left')['first_comment']\n# Last activity (Total)\nstudents['date_last_activity'] = students[['date_last_question', 'date_last_comment']].max(axis=1)\n# First activity (Total)\nstudents['date_first_activity'] = students[['date_first_question', 'date_first_comment']].min(axis=1)\n\n### Emails Response\nemails_response = pd.merge(emails, matches, left_on='emails_id', right_on='matches_email_id', how='inner')\nemails_response = pd.merge(emails_response, questions, left_on='matches_question_id', right_on='questions_id', how='inner')\nemails_response = pd.merge(emails_response, answers, left_on=['emails_recipient_id', 'matches_question_id'], right_on=['answers_author_id', 'answers_question_id'], how='left')\nemails_response = emails_response.drop(['matches_email_id', 'matches_question_id', 'answers_id', 'answers_author_id', 'answers_body', 'answers_question_id'], axis=1)\nemails_response = emails_response.drop(['questions_author_id', 'questions_title', 'questions_body', 'questions_full_text'], axis=1)\nemails_response['time_delta_email_answer'] = (emails_response['answers_date_added'] - emails_response['emails_date_sent'])\nemails_response['time_delta_question_email'] = (emails_response['emails_date_sent'] - emails_response['questions_date_added'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4624b3acfa13e3d1c66b2ece9c87fade4649f04c"},"cell_type":"markdown","source":"# 3. EDA <a id=\"eda\"></a>"},{"metadata":{"_uuid":"0cb336f34e36ed9819641cf0bd2cf0774fc78afe"},"cell_type":"markdown","source":"## Time Series <a id=\"eda_time_series\"></a> \nHere we can see in which year the most user activity was. There was a large increase in 2016. 40% of all questions and comments were in this year.  \nIn some future analyses, we will limit the data to 2016 and onwards. This is to prevent possible noise from the Career Village start time."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ae23204d5a87d74acc3787addcd14fd0162339a9"},"cell_type":"code","source":"plt_questions = (questions.groupby([questions['questions_date_added'].dt.year]).size()/len(questions.index))\nplt_answers = (answers.groupby([answers['answers_date_added'].dt.year]).size()/len(answers.index))\nplt_emails = (emails.groupby([emails['emails_date_sent'].dt.year]).size()/len(emails.index))\nplt_comments = (comments.groupby([comments['comments_date_added'].dt.year]).size()/len(comments.index))\nplt_data = pd.DataFrame({'questions': plt_questions,\n                        'answers':plt_answers,\n                        'emails':plt_emails,\n                        'comments':plt_comments})\nplt_data.plot(kind='bar', figsize=(15, 5))\nplt.xlabel('Year')\nplt.ylabel('Proportion')\nplt.title('Distribution over time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33a461245133937a2294f994b05888300089ee92"},"cell_type":"markdown","source":"## Missing values (Professionals) <a id=\"eda_missing_values_professionals\"></a> \nThe *Location, Industry and Headline* were specified by the most professionals. Even hashtags are used by most. The specification of the school is made however only by the fewest.  \nThe industry could therefore be a good feature for the recommendation. Especially for new authors who have not written any answer or comments. If an professional is in the medical field, he should not necessarily get questions about a career as a lawyer.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c7be50b6aef76c7bf3080cf773ce65e7bc2758b2"},"cell_type":"code","source":"temp = professionals[['professionals_location', 'professionals_industry', 'professionals_headline']].fillna('Missing')\ntemp = temp.applymap(lambda x: x if x == 'Missing' else 'Available')\nplt_professionals_location = temp.groupby('professionals_location').size()/len(temp.index)\nplt_professionals_industry = temp.groupby('professionals_industry').size()/len(temp.index)\nplt_professionals_headline = temp.groupby('professionals_headline').size()/len(temp.index)\nplt_professionals_tags = tag_users['tag_users_user_id'].unique()\nplt_professionals_tags = professionals['professionals_id'].apply(lambda x: 'Available' if x in plt_professionals_tags else 'Missing').rename('professionals_tags')\nplt_professionals_tags = plt_professionals_tags.groupby(plt_professionals_tags).size()/len(plt_professionals_tags.index)\nplt_professionals_group = group_memberships['group_memberships_user_id'].unique()\nplt_professionals_group = professionals['professionals_id'].apply(lambda x: 'Available' if x in plt_professionals_group else 'Missing').rename('professionals_groups')\nplt_professionals_group = plt_professionals_group.groupby(plt_professionals_group).size()/len(plt_professionals_group.index)\nplt_professionals_school = school_memberships['school_memberships_user_id'].unique()\nplt_professionals_school = professionals['professionals_id'].apply(lambda x: 'Available' if x in plt_professionals_school else 'Missing').rename('professionals_schools')\nplt_professionals_school = plt_professionals_school.groupby(plt_professionals_school).size()/len(plt_professionals_school.index)\ntemp = professionals[['professionals_answers_count', 'professionals_comments_count']]\ntemp = temp.applymap(lambda x: 'Available' if x > 0 else 'Missing')\nplt_professionals_answers = temp.groupby('professionals_answers_count').size()/len(temp.index)\nplt_professionals_comments = temp.groupby('professionals_comments_count').size()/len(temp.index)\n\nplt_data = pd.DataFrame({'Location': plt_professionals_location,\n                        'Industry': plt_professionals_industry,\n                        'Headline': plt_professionals_headline,\n                        'Tags': plt_professionals_tags,\n                        'Groups': plt_professionals_group,\n                        'Schools': plt_professionals_school,\n                        'Answers': plt_professionals_answers,\n                        'Comments': plt_professionals_comments,})\n\nplt_data.T.plot(kind='bar', stacked=True, figsize=(15, 5))\nplt.ylabel('Proportion')\nplt.title('Missing values for professionals')\nplt.yticks(np.arange(0, 1.05, 0.1))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\nplt_data.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"655ae5d2b23a72b55571f435228c4ba5c5df7b8c"},"cell_type":"markdown","source":"## Missing values (Students) <a id=\"eda_missing_values_students\"></a> \nIt's a little different with the students. Only the location is specified by most students, while the rest is rather not used."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b85a09d4019aa961572afcd6cce21a81b86dba35"},"cell_type":"code","source":"temp = students[['students_location']].fillna('Missing')\ntemp = temp.applymap(lambda x: x if x == 'Missing' else 'Available')\nplt_students_location = temp.groupby('students_location').size()/len(temp.index)\nplt_students_tags = tag_users['tag_users_user_id'].unique()\nplt_students_tags = students['students_id'].apply(lambda x: 'Available' if x in plt_students_tags else 'Missing').rename('students_tags')\nplt_students_tags = plt_students_tags.groupby(plt_students_tags).size()/len(plt_students_tags.index)\nplt_students_group = group_memberships['group_memberships_user_id'].unique()\nplt_students_group = students['students_id'].apply(lambda x: 'Available' if x in plt_students_group else 'Missing').rename('students_groups')\nplt_students_group = plt_students_group.groupby(plt_students_group).size()/len(plt_students_group.index)\nplt_students_school = school_memberships['school_memberships_user_id'].unique()\nplt_students_school = students['students_id'].apply(lambda x: 'Available' if x in plt_students_school else 'Missing').rename('students_schools')\nplt_students_school = plt_students_school.groupby(plt_students_school).size()/len(plt_students_school.index)\ntemp = students[['students_questions_count', 'students_comments_count']]\ntemp = temp.applymap(lambda x: 'Available' if x > 0 else 'Missing')\nplt_students_questions = temp.groupby('students_questions_count').size()/len(temp.index)\nplt_students_comments = temp.groupby('students_comments_count').size()/len(temp.index)\n\nplt_data = pd.DataFrame({'Location': plt_students_location,\n                        'Tags': plt_students_tags,\n                        'Groups': plt_students_group,\n                        'Schools': plt_students_school,\n                        'Answers': plt_students_questions,\n                        'Comments': plt_students_comments,})\n\nplt_data.T.plot(kind='bar', stacked=True, figsize=(15, 5))\nplt.ylabel('Proportion')\nplt.title('Missing values for students')\nplt.yticks(np.arange(0, 1.05, 0.1))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\nplt_data.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38dc013010142d51d6c35b1eb3541301dc182a9d"},"cell_type":"markdown","source":"## Tags matching <a id=\"eda_tags_matching\"></a> \nThe size of the bubbles depends on how many questions the tag is used. The x-axis is how many professionals have subscribe the tag and the y-axis is how many students have subscribe the tag.  \nThe top tag for professionals ist *telecommunications* on the right site with about 11% but the tag doesn't appear in many questions or students subscribtion.  \nThe top tags for questions is *college* with 15.6% and *carrer* with 6.5%. The other top tags are carrer specific (*medicine, engineering, business, ...*).  \nThe top tag for students is *college* but only 1.5% of the students have subscribe this tag.  "},{"metadata":{"trusted":true,"_uuid":"ec9c393467a825686fe813daf0d6737048b9949f","_kg_hide-input":true},"cell_type":"code","source":"students_tags = tag_users[tag_users['tag_users_user_id'].isin(students['students_id'])]\nstudents_tags = pd.merge(students_tags, tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\nstudents_tags['user_type'] = 'student'\nprofessionals_tags = tag_users[tag_users['tag_users_user_id'].isin(professionals['professionals_id'])]\nprofessionals_tags = pd.merge(professionals_tags, tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\nprofessionals_tags['user_type'] = 'professional'\nquestions_tags = tag_questions\nquestions_tags = pd.merge(questions_tags, tags, left_on='tag_questions_tag_id', right_on='tags_tag_id')\nquestions_tags['user_type'] = 'question'\nplt_data = pd.concat([students_tags, professionals_tags, questions_tags])\n\nplt_data = plt_data[['tags_tag_name', 'user_type']].pivot_table(index='tags_tag_name', columns='user_type', aggfunc=len, fill_value=0)\nplt_data['professional'] = plt_data['professional'] / professionals.shape[0]\nplt_data['student'] = plt_data['student'] / students.shape[0]\nplt_data['question'] = plt_data['question'] / questions.shape[0]\nplt_data['sum'] = (plt_data['professional'] + plt_data['student'] + plt_data['question'])\nplt_data = plt_data.sort_values(by='sum', ascending=False).drop(['sum'], axis=1).head(100)\n\n# Bubble chart\nfig, ax = plt.subplots(facecolor='w',figsize=(15, 15))\nax.set_xlabel('Professionals')\nax.set_ylabel('Students')\nax.set_title('Tags Matching')\nax.set_xlim([0, max(plt_data['professional'])+0.001])\nax.set_ylim([0, max(plt_data['student'])+0.001])\nimport matplotlib.ticker as mtick\nax.xaxis.set_major_formatter(mtick.FuncFormatter(\"{:.2%}\".format))\nax.yaxis.set_major_formatter(mtick.FuncFormatter(\"{:.2%}\".format))\nax.grid(True)\ni = 0\nfor key, row in plt_data.iterrows():\n    ax.scatter(row['professional'], row['student'], s=row['question']*10**4, alpha=.5)\n    if i < 25:\n        ax.annotate('{}: {:.2%}'.format(key, row['question']), xy=(row['professional'], row['student']))\n    i += 1\nplt.show()\n\n# Wordcloud\nplt.figure(figsize=(20, 20))\nwordloud_values = ['student', 'professional', 'question']\naxisNum = 1\nfor wordcloud_value in wordloud_values:\n    wordcloud = WordCloud(margin=0, max_words=20, random_state=seed).generate_from_frequencies(plt_data[wordcloud_value])\n    ax = plt.subplot(1, 3, axisNum)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(wordcloud_value)\n    plt.axis(\"off\")\n    axisNum += 1\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d940e8543d132fad208e365fb6ebc823d444507d"},"cell_type":"markdown","source":"## First activity after registration  <a id=\"eda_first_activity\"></a>\nHere we can see how long it took, that an professional makes his first answer or a student his first question after the registration.  \nThe most of them write the first answer/question within in the first day or haven't write any yet and use the account for other activities."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"48b8ee7f0bb62ccc31ac35593ecb8e2d79046d89"},"cell_type":"code","source":"plt_professionals = professionals\nplt_professionals = plt_professionals[(plt_professionals['professionals_date_joined'] >= '01-01-2016') & (plt_professionals['professionals_date_joined'] <= '30-06-2018')]\nplt_professionals = (plt_professionals['date_first_activity'] - plt_professionals['professionals_date_joined']).dt.days.fillna(9999).astype(int)\nplt_professionals = plt_professionals.groupby(plt_professionals).size()/len(plt_professionals.index)\nplt_professionals = plt_professionals.rename(lambda x: 0 if x < 0.0 else x)\nplt_professionals = plt_professionals.rename(lambda x: x if x <= 7.0 or x == 9999 else '> 7')\nplt_professionals = plt_professionals.rename({9999:'NaN'})\nplt_professionals = plt_professionals.groupby(level=0).sum()\n\nplt_students = students\nplt_students = plt_students[(plt_students['students_date_joined'] >= '01-01-2016') & (plt_students['students_date_joined'] <= '30-06-2018')]\nplt_students = (plt_students['date_first_activity'] - plt_students['students_date_joined']).dt.days.fillna(9999).astype(int)\nplt_students = plt_students.groupby(plt_students).size()/len(plt_students.index)\nplt_students = plt_students.rename(lambda x: 0 if x < 0.0 else x)\nplt_students = plt_students.rename(lambda x: x if x <= 7.0 or x == 9999 else '> 7')\nplt_students = plt_students.rename({9999:'NaN'})\nplt_students = plt_students.groupby(level=0).sum()\n\nplt_data = pd.DataFrame({'Professionals': plt_professionals,\n                        'Students': plt_students})\n\nplt_data.plot(kind='bar', figsize=(15, 5))\nplt.xlabel('Days')\nplt.ylabel('Proportion')\nplt.title('Days for first activity after registration')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cfa8ba5fcd8bb8eeb335bc599b4541893112327"},"cell_type":"markdown","source":"## Last activity  <a id=\"eda_last_activity\"></a>  \nDepending on the last comment, question or answer of a user, we have extract the last activity date. On the previously plot we have seen, that many users haven't done any activity yet. For the 'last activity' plot we take a look only on users with already have one activity (*dropna*).  \nOn the cumulative histogram we can see, that in the last 12 months only 39% of professionals and 24% of students have written a comment, question or answer.  \n50% of the professionals haven't done any activity for 17 months."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"944beaefb0ddafeadfb85f0b8a45d5fcddb6233c"},"cell_type":"code","source":"plt_professionals = ((actual_date - professionals['date_last_activity']).dt.days/30).dropna().apply(lambda x: math.ceil(x)).astype(int)\nplt_professionals = plt_professionals.groupby(plt_professionals).size()/len(plt_professionals.index)\nplt_professionals = plt_professionals.rename(lambda x: 0 if x < 0.0 else x)\nplt_professionals = plt_professionals.rename(lambda x: x if x <= 36.0 or x == 9999 else '> 36')\nplt_professionals = plt_professionals.rename({9999:'NaN'})\nplt_professionals = plt_professionals.groupby(level=0).sum()\n\nplt_students = ((actual_date - students['date_last_activity']).dt.days/30).dropna().apply(lambda x: math.ceil(x)).astype(int)\nplt_students = plt_students.groupby(plt_students).size()/len(plt_students.index)\nplt_students = plt_students.rename(lambda x: 0 if x < 0.0 else x)\nplt_students = plt_students.rename(lambda x: x if x <= 36.0 or x == 9999 else '> 36')\nplt_students = plt_students.rename({9999:'NaN'})\nplt_students = plt_students.groupby(level=0).sum()\n\nplt_data = pd.DataFrame({'Professionals': plt_professionals,\n                        'Students': plt_students})\n\nplt_data.plot(kind='bar', figsize=(15, 5))\nplt.xlabel('Months')\nplt.ylabel('Proportion')\nplt.title('Months for last activity')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"84ab3817d4c61eda72e0644e25c60757e05ba33e"},"cell_type":"code","source":"plt_professionals = ((actual_date - professionals['date_last_activity']).dt.days/30).dropna().apply(lambda x: math.ceil(x)).astype(int)\nplt_students = ((actual_date - students['date_last_activity']).dt.days/30).dropna().apply(lambda x: math.ceil(x)).astype(int)\nplt_data = pd.DataFrame({'Professionals': plt_professionals,\n                        'Students': plt_students})\nplt_total = pd.concat([plt_data['Professionals'], plt_data['Students']]).rename('All Users')\nplt_data.plot(kind='hist', bins=1000, density=True, histtype='step', cumulative=True, figsize=(15, 7), lw=2, grid=True)\nplt_total.plot(kind='hist', bins=1000, density=True, histtype='step', cumulative=True, figsize=(15, 7), lw=2, grid=True)\nplt.xlabel('Months')\nplt.ylabel('Cumulative')\nplt.title('Cumulative histogram for last activity')\nplt.legend(loc='upper left')\nplt.xlim([0, 50])\nplt.xticks(range(0, 51, 1))\nplt.yticks(np.arange(0, 1.05, 0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb576dc5e80d47966cc8ec3ec252df77d351d2f2"},"cell_type":"markdown","source":"## First answer for questions  <a id=\"eda_first_answer\"></a>\nHere we can see how long it takes for a question to get the first answer.  \nThe most questions get answered within the first two weeks. But 15% need more than 30 weeks to get there answer. And there are still some unanswered questions.  \nThe questions are filtered until 30-06-2018 to ignore new unanswered questions."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"89d53edcd27be5bd558bf878f1b6391aa58bb108"},"cell_type":"code","source":"plt_questions = questions\nplt_questions = plt_questions[(plt_questions['questions_date_added'] >= '01-01-2016') & (plt_questions['questions_date_added'] <= '30-06-2018')]\nplt_questions = ((plt_questions['questions_first_answers'] - plt_questions['questions_date_added']).dt.days/7).fillna(9999).apply(lambda x: math.ceil(x)).astype(int)\nplt_questions = plt_questions.groupby(plt_questions).size()/len(plt_questions.index)\nplt_questions = plt_questions.rename(lambda x: 0 if x < 0.0 else x)\nplt_questions = plt_questions.rename(lambda x: x if x <= 30.0 or x == 9999 else '> 30')\nplt_questions = plt_questions.rename({9999:'NaN'})\nplt_questions = plt_questions.groupby(level=0).sum()\n\nplt_data = pd.DataFrame({'Questions': plt_questions})\n\nplt_data.plot(kind='bar', figsize=(15, 5))\nplt.xlabel('Weeks')\nplt.ylabel('Frequency')\nplt.title('Weeks for first answer')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b00afecb66a087bd64c0936f83c1999b07702d81"},"cell_type":"markdown","source":"## Email response time  <a id=\"eda_email_response\"></a>  \nThe first plot shows how long it takes for professionals to answer a question after they receive an email notification of a recommended question. This is limited to emails where the specified question was also answered (*dropna*).  When users want to answer a question from an email, they usually use the most recent email. The response time is within the first few days.  \n\nOn the second plot with the response time for questions, we can see the emails focusing on older unanswered questions. the immediate emails are focusing on new questions (makes sense)."},{"metadata":{"trusted":true,"_uuid":"5e28e610fae672a59b1e2ce7864798b5ab98031f","_kg_hide-input":true},"cell_type":"code","source":"plt_email_response = emails_response[(emails_response['emails_date_sent'] >= '01-01-2016')].dropna()\n\nplt_data = pd.DataFrame()\ntitle_mapping = {'time_delta_email_answer':'Answers', 'time_delta_question_email':'Questions'}\nfor qa in ['time_delta_email_answer', 'time_delta_question_email']:\n    plt_data = pd.DataFrame()\n    for fl in ['email_notification_daily', 'email_notification_weekly', 'email_notification_immediate']:\n        temp = plt_email_response[plt_email_response['emails_frequency_level'] == fl]\n        temp = temp[qa].dt.days.astype(int)\n        temp = temp.groupby(temp).size()/len(temp.index)\n        temp = temp.rename(lambda x: 0 if x < 0.0 else x)\n        temp = temp.rename(lambda x: x if x <= 14.0 else '> 14')\n        temp = temp.groupby(level=0).sum() \n        plt_data = pd.concat([plt_data, temp], axis=1, sort=False)\n    plt_data.columns = ['Daily', 'Weekly', 'Immediate']\n\n    plt_data.plot(kind='bar', figsize=(15, 5))\n    plt.xlabel('Days')\n    plt.ylabel('Frequency')\n    plt.title('Email response time ({})'.format(title_mapping[qa]))\n    plt.legend(loc='upper center')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6d4e7f060204eafa95b5b4eec7d9fe87da26aa2"},"cell_type":"markdown","source":"## Word count <a id=\"eda_wordcound\"></a>\nHere we can see how many words are used for the questions and answers.  \nThe professionals write very detailed answers for the students questions."},{"metadata":{"trusted":true,"_uuid":"a0e10c3d4ca8e10fe10bedc14389a484fd4a6179","_kg_hide-input":true},"cell_type":"code","source":"plt_data_questions = questions['questions_full_text'].apply(lambda x: len(x.split())).rename(\"Questions\")\nplt_data_answers = answers['answers_body'].astype(str).apply(lambda x: len(x.split())).rename(\"Answers\")\nplt_data = pd.DataFrame([plt_data_questions, plt_data_answers]).T\nprint(plt_data.describe())\nplt_data.plot(kind='box', showfliers=False, vert=False, figsize=(15, 5), grid=True)\nplt.xticks(range(0, 400, 25))\nplt.xlabel('Words')\nplt.title('Word count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81c3919679f0588339176410b0b31b326ae251c4"},"cell_type":"markdown","source":"# 5. Ad Hoc Analysis <a id=\"ad_hoc\"></a>  "},{"metadata":{"_uuid":"009defebce03fbacc27c5fce8e387f401ebec3bb"},"cell_type":"markdown","source":"## User Activities <a id=\"ad_hoc_user_activities\"></a>  "},{"metadata":{"trusted":true,"_uuid":"230848fde1cef154f87cc0a51f57793bd631b241","_kg_hide-input":true},"cell_type":"code","source":"def plot_user_activity(user_id=seed, xticks_type='Month', xticks_interval=3):\n    \"\"\" Merges all relevant data for a user together and builds a timeline chart.\n        \n        :param user_id: Index of the 'students' dataframe (default: seed)\n        :param xticks_type: Grouping x axis by 'Month', 'Day' or 'Year' (default: 'Month')\n        :param xticks_interval: Integer to plot every n ticks (default: 3)\n    \"\"\"  \n    graph_student = students[students['students_id'] == user_id]\n    graph_student = pd.DataFrame({'type':'Registration', \n                               'id':graph_student['students_id'], \n                               'date':graph_student['students_date_joined'],\n                              'color':'green'})\n    graph_professional = professionals[professionals['professionals_id'] == user_id]\n    graph_professional = pd.DataFrame({'type':'Registration', \n                               'id':graph_professional['professionals_id'], \n                               'date':graph_professional['professionals_date_joined'],\n                              'color':'green'})\n    graph_questions = questions[questions['questions_author_id'] == user_id]\n    graph_questions = pd.DataFrame({'type':'Questions',\n                                   'id':graph_questions['questions_id'], \n                                   'date':graph_questions['questions_date_added'],\n                                  'color':'blue'})\n    graph_answers = answers[answers['answers_author_id'] == user_id]\n    graph_answers = pd.DataFrame({'type':'Answers',\n                                   'id':graph_answers['answers_id'], \n                                   'date':graph_answers['answers_date_added'],\n                                  'color':'red'})\n    graph_comments = comments[comments['comments_author_id'] == user_id]\n    graph_comments = pd.DataFrame({'type':'Comments',\n                                   'id':graph_comments['comments_id'], \n                                   'date':graph_comments['comments_date_added'],\n                                  'color':'orange'})\n\n    graph_data = pd.concat([graph_student, graph_professional, graph_questions, graph_comments, graph_answers])\n    # Group data by date\n    if xticks_type=='Day':\n        graph_data['date'] = graph_data['date'].dt.strftime('%Y-%m-%d')\n        graph_data['date'] = graph_data['date'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n    else:\n        graph_data['date'] = graph_data['date'].dt.strftime('%B %Y')\n        graph_data['date'] = graph_data['date'].apply(lambda x: datetime.strptime(x, \"%B %Y\"))     \n    \n    graph_data = graph_data.groupby(['type', 'date', 'color']).size().rename('count').reset_index().sort_values('date')\n    graph_data['name'] = graph_data['count'].map(str)+ ' '+graph_data['type']\n    names = graph_data['name'].tolist()\n    colors = graph_data['color'].tolist()\n    dates = graph_data['date'].tolist()\n\n    # Plot\n    levels = np.array([-7, 7, -5, 5, -3, 3, -1, 1])\n    fig, ax = plt.subplots(figsize=(15, 8))\n\n    # Create the base line\n    start = min(dates)\n    stop = max(dates)\n    ax.plot((start, stop), (0, 0), 'k', alpha=.5)\n\n    # Create annotations\n    for ii, (iname, idate, icol) in enumerate(zip(names, dates, colors)):\n        level = levels[ii % len(levels)]\n        vert = 'top' if level < 0 else 'bottom'\n        ax.scatter(idate, 0, s=100, facecolor=icol, edgecolor='k', zorder=9999)\n        ax.plot((idate, idate), (0, level), c=icol, alpha=1.0, lw=2)\n        ax.text(idate, level, iname, horizontalalignment='center', verticalalignment=vert, fontsize=12, backgroundcolor=icol)\n    ax.set(title=\"Timeline for user: {}\".format(user_id))\n    # Set the xticks formatting\n    if xticks_type=='Month':\n        ax.get_xaxis().set_major_locator(mdates.MonthLocator(interval=xticks_interval))\n        ax.get_xaxis().set_major_formatter(mdates.DateFormatter(\"%B %Y\"))\n    elif xticks_type=='Day':\n        ax.get_xaxis().set_major_locator(mdates.DayLocator(interval=xticks_interval))\n        ax.get_xaxis().set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n    elif xticks_type=='Year':\n        ax.get_xaxis().set_major_locator(mdates.YearLocator())\n        ax.get_xaxis().set_major_formatter(mdates.DateFormatter(\"%Y\"))        \n    fig.autofmt_xdate()\n    #Legend\n    legend = []\n    for index, row in graph_data[['type', 'color']].drop_duplicates().iterrows():\n        legend += [mpatches.Patch(color=row['color'], label=row['type'])]\n    plt.legend(handles=legend, loc='center left', bbox_to_anchor=(1, 0.5))\n    \n    # Remove components for a cleaner look\n    plt.setp((ax.get_yticklabels() + ax.get_yticklines() + list(ax.spines.values())), visible=False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e01eaa3fc86cd3bea411d13cc315402d6464a9b"},"cell_type":"markdown","source":"## Example 1 (Professionals)"},{"metadata":{"trusted":true,"_uuid":"071f7daa7ad18c4e02a7a630b55226dcae8ec241","_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"user_id = '977428d851b24183b223be0eb8619a8c'\nplot_user_activity(user_id=user_id, xticks_type='Month', xticks_interval=1)\nuser_id = 'e1d39b665987455fbcfbec3fc6df6056'\nplot_user_activity(user_id=user_id, xticks_type='Month', xticks_interval=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce699331d69e6d6771399010e7aac0b494ccafc5"},"cell_type":"markdown","source":"## Example 2 (Students)"},{"metadata":{"trusted":true,"_uuid":"9d34f50c23dcb1001105aceb34e91b3eb3f760f1"},"cell_type":"code","source":"user_id = '16908136951a48ed942738822cedd5c2'\nplot_user_activity(user_id=user_id, xticks_type='Month', xticks_interval=1)\nuser_id = 'e5c389a88c884e13ac828dd22628acc8'\nplot_user_activity(user_id=user_id, xticks_type='Day', xticks_interval=7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4939f36064ad2b2a24e80dd58f8e1165e0590baa"},"cell_type":"markdown","source":"## Dependency Graph <a id=\"ad_hoc_dependency\"></a>  "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"10642249fbb697cdfd6e9d7632f5646db55414a0"},"cell_type":"code","source":"def plot_dependecy_graph(emails_id=[], details=['tag', 'group', 'school'], details_min_edges=1):\n    \"\"\" Merges all relevant data for a given email together and builds a dependency graph and report.\n        \n        :param emails_id: 'email_id' of the 'emails' dataframe\n        :param details: List which details should be ploted (default: ['tags', 'groups', 'schools'])\n    \"\"\"  \n    graph_edges = pd.DataFrame()\n    graph_nodes = pd.DataFrame()\n    #email\n    graph_emails = emails[emails['emails_id'].isin(emails_id)]\n    graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_emails['emails_id'], 'type':'email', 'color':'grey', 'size':1})])\n    #questions\n    graph_matches = matches[matches['matches_email_id'].isin(graph_emails['emails_id'])]\n    graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_matches['matches_question_id'], 'type':'question', 'color':'blue', 'size':1})])\n    graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_matches['matches_email_id'], 'target':graph_matches['matches_question_id']})])\n    #answers\n    graph_answers = answers[answers['answers_question_id'].isin(graph_matches['matches_question_id'])]\n    graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_answers['answers_id'], 'type':'answer', 'color':'red', 'size':1})])\n    graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_answers['answers_question_id'], 'target':graph_answers['answers_id']})])\n    #professionals\n    graph_professionals = answers[answers['answers_question_id'].isin(graph_matches['matches_question_id'])]\n    graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_professionals['answers_author_id'], 'type':'professional', 'color':'cyan', 'size':1})])\n    graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_professionals['answers_id'], 'target':graph_professionals['answers_author_id']})])\n    #students\n    graph_students = questions[questions['questions_id'].isin(graph_matches['matches_question_id'])]\n    graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_students['questions_author_id'], 'type':'student', 'color':'green', 'size':1})])\n    graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_students['questions_id'], 'target':graph_students['questions_author_id']})])\n    if 'tag' in details:\n        #question tags\n        graph_questions_tags = tag_questions[tag_questions['tag_questions_question_id'].isin(graph_matches['matches_question_id'])]\n        graph_questions_tags = pd.merge(graph_questions_tags, tags, left_on='tag_questions_tag_id', right_on='tags_tag_id')\n        graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_questions_tags['tags_tag_name'], 'type':'tag', 'color':'yellow', 'size':1})])\n        graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_questions_tags['tag_questions_question_id'], 'target':graph_questions_tags['tags_tag_name']})])  \n        #professional tags\n        graph_professionals_tags = tag_users[tag_users['tag_users_user_id'].isin(graph_professionals['answers_author_id'])]\n        graph_professionals_tags = pd.merge(graph_professionals_tags, tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\n        graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_professionals_tags['tags_tag_name'], 'type':'tag', 'color':'yellow', 'size':1})])\n        graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_professionals_tags['tag_users_user_id'], 'target':graph_professionals_tags['tags_tag_name']})])     \n        #students tags\n        graph_students_tags = tag_users[tag_users['tag_users_user_id'].isin(graph_students['questions_author_id'])]\n        graph_students_tags = pd.merge(graph_students_tags, tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\n        graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_students_tags['tags_tag_name'], 'type':'tag', 'color':'yellow', 'size':1})])\n        graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_students_tags['tag_users_user_id'], 'target':graph_students_tags['tags_tag_name']})]) \n    if 'group' in details:\n        #professional group\n        graph_professionals_group = group_memberships[group_memberships['group_memberships_user_id'].isin(graph_professionals['answers_author_id'])]\n        graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_professionals_group['group_memberships_group_id'], 'type':'group', 'color':'orange', 'size':1})])\n        graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_professionals_group['group_memberships_user_id'], 'target':graph_professionals_group['group_memberships_group_id']})]) \n        #students group\n        graph_students_group = group_memberships[group_memberships['group_memberships_user_id'].isin(graph_students['questions_author_id'])]\n        graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_students_group['group_memberships_group_id'], 'type':'group', 'color':'orange', 'size':1})])\n        graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_students_group['group_memberships_user_id'], 'target':graph_students_group['group_memberships_group_id']})]) \n    if 'school' in details:\n        #professional school\n        graph_professionals_school = school_memberships[school_memberships['school_memberships_user_id'].isin(graph_professionals['answers_author_id'])]\n        graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_professionals_school['school_memberships_school_id'], 'type':'school', 'color':'purple', 'size':1})])\n        graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_professionals_school['school_memberships_user_id'], 'target':graph_professionals_school['school_memberships_school_id']})])\n        #students school\n        graph_students_school = school_memberships[school_memberships['school_memberships_user_id'].isin(graph_students['questions_author_id'])]\n        graph_nodes = pd.concat([graph_nodes, pd.DataFrame({'node':graph_students_school['school_memberships_school_id'], 'type':'school', 'color':'purple', 'size':1})])\n        graph_edges = pd.concat([graph_edges, pd.DataFrame({'source':graph_students_school['school_memberships_user_id'], 'target':graph_students_school['school_memberships_school_id']})])\n    \n    # check min count of edges for details\n    graph_nodes = graph_nodes.drop_duplicates()\n    temp = pd.concat([graph_edges[['source', 'target']], graph_edges[['target', 'source']].rename(columns={'target':'source', 'source':'target'})])\n    temp = temp[temp['source'].isin(graph_nodes[graph_nodes['type'].isin(details)]['node'])]\n    temp = temp.drop_duplicates().groupby('source').size()\n    graph_nodes = graph_nodes[~graph_nodes['node'].isin(temp[temp<details_min_edges].index.values)]\n    graph_edges = graph_edges[(~graph_edges['source'].isin(temp[temp<details_min_edges].index.values)) & (~graph_edges['target'].isin(temp[temp<details_min_edges].index.values))]\n    graph_nodes_color = graph_nodes['color']\n    \n    plt.figure(figsize=(15, 15)) \n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes['node'])\n    G.add_edges_from({tuple(row) for i,row in graph_edges[['source', 'target']].iterrows()})\n    nx.draw_networkx(G, with_labels=True, node_color=graph_nodes_color, font_size=8, node_size=900/len(emails_id))\n    plt.title('Dependency graph for email {}'.format(emails_id))\n    plt.axis('off')\n\n    legend = []\n    for index, row in graph_nodes[['type', 'color']].drop_duplicates().iterrows():\n        legend += [mpatches.Patch(color=row['color'], label=row['type'])]\n    plt.legend(handles=legend)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb3c952348cf4add6c9dcc2f7a9ed0de3b7d6a86"},"cell_type":"markdown","source":"## Example 1  <a id=\"dependency_graph_example1\"></a> \nIn example 1 we have <span style=\"background-color:gray\">one email</span> with <span style=\"background-color:blue\">two questions</span> with a few <span style=\"background-color:yellow\">questions tags</span>. There are <span style=\"background-color:red\">several answers</span> for the question. The <span style=\"background-color:green\">students</span> from the questions haven't any <span style=\"background-color:orange\">group</span> or <span style=\"background-color:purple\">school</span> membership. The <span style=\"background-color:cyan\">professionals</span> have more motivation to subscribe some <span style=\"background-color:yellow\">tags</span> and specify there <span style=\"background-color:purple\">school</span> membership. But only <span style=\"background-color:cyan\">two professionals</span> have joined a <span style=\"background-color:orange\">group</span>.<span style=\"background-color:blue\"></span>  \n\nIn the second plot of this example we increase the parameter *details_min_edges* to 2. This provides a better overview of which tags are used by multiple nodes. For example, the 'financial-planing' node is used in two questions and is subscribed by the professional and student at the same time."},{"metadata":{"trusted":true,"_uuid":"3d92917fa79830687e90c33f572090f16d6d89ee"},"cell_type":"code","source":"emails_id = emails.loc[seed, 'emails_id']\nplot_dependecy_graph(emails_id=[emails_id])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e38f4dc765a33e164b11574fbf699942e1aa974"},"cell_type":"code","source":"emails_id = emails.loc[seed, 'emails_id']\nplot_dependecy_graph(emails_id=[emails_id], details_min_edges=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7708ae7bddac1b907b5e5e1b38c75b1d833b37f"},"cell_type":"markdown","source":"## Example 1  <a id=\"dependency_graph_example2\"></a> "},{"metadata":{"trusted":true,"_uuid":"7a78322321b5425803a1876bb2a8b8e75f96d815"},"cell_type":"code","source":"emails_id = emails.loc[seed*2, 'emails_id']\nplot_dependecy_graph(emails_id=[emails_id])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d616e7970cb60c43aef6217b0264ec1507ae7b6"},"cell_type":"markdown","source":"# 5. Recommendation <a id=\"recommendation\"></a>  \nWith the preprocessed data I build a tf-idf corpus and can use this, to calculate the (cosine) similarity between a new question and the given questions.  \nHere are the detailed steps:  \n1. Use NLP on the Questions corpus.  \n    a. Use part-of-speech tagging to filter words.  \n    b. Calculate the tf-idf for a better Information Retrieval.  \n2. Use NLP on the Query text.  \n    a. Use part-of-speech tagging to filter words.  \n    b. Calculate the tf-idf for a better Information Retrieval.  \n3. Use the cosine similiarty to get similiar questions for the query text.  \n4. Get the answers and professionals for the similar questions.  \n5. Make a recommendation to fit the best professionals to answer the new question.  \n\nI use the similar questions and the professionals who answered the question to calculate a *recommendation score*. On the basis of this, professionals will be recommended who have already answered many similar questions which have not been answered by many others. The first draft of this formula is as follows  \n$Professional_{score} = \\sum\\limits_{q}^{Q}(q_{sim}*\\dfrac{1}{q_{answers}}*p_{answers})$  \n$Q$ = Similar  questions  answered  by  professional  p  \n$q_{sim}$ = Similarity of the question q  \n$q_{answers}$ = Total answers for question q  \n$p_{answers}$ = Total answers of professional p\n    "},{"metadata":{"_uuid":"748fb724c1017c58a471fe84d100d9e002a2104b"},"cell_type":"markdown","source":"## Functions"},{"metadata":{"trusted":true,"_uuid":"24c22356a4d59ae213915b74f4141b565f9e5bab","_kg_hide-input":true},"cell_type":"code","source":"def get_similar_docs(corpus, query_text, threshold=0.0, top=5):\n    \"\"\" Calculates the tfidf of the corpus and returns similiar questions, matching the query text.\n\n    \"\"\"  \n    #nlp_corpus = [' '.join(x) for x in nlp_preprocessing(corpus)]\n    nlp_corpus = [' '.join(x) for x in questions['nlp_tokens']]\n    nlp_text = [' '.join(nlp_preprocessing([query_text])[0])]\n    vectorizer = TfidfVectorizer(lowercase = True, stop_words = 'english')\n    vectorizer.fit(nlp_corpus)\n    corpus_tfidf = vectorizer.transform(nlp_corpus)\n    \n    text_tfidf = vectorizer.transform(nlp_text)\n    sim = cosine_similarity(corpus_tfidf, text_tfidf)\n    sim_idx = (sim >= threshold).nonzero()[0]\n    result = pd.DataFrame({'similarity':sim[sim_idx].reshape(-1,),\n                          'text':corpus[sim_idx]},\n                          index=sim_idx)\n    result = result.sort_values(by=['similarity'], ascending=False).head(top)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da4aa6676852abbaf59f0e82b0a707032068d6a3","_kg_hide-input":true},"cell_type":"code","source":"def get_questions_answers(sim_questions):\n    \"\"\" Merges the questions with the corresponding answers\n\n    \"\"\"  \n    sim_question_answers = pd.merge(sim_questions, questions, left_index=True, right_index=True)\n    sim_question_answers = pd.merge(sim_question_answers, answers, left_on='questions_id', right_on='answers_question_id')\n    sim_question_answers = sim_question_answers[['questions_id', 'similarity', 'questions_title', 'questions_body', 'answers_body']]\n    return sim_question_answers","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d548d2fd6c53e04da7408195e6ea10adc15ced89"},"cell_type":"code","source":"def get_recommendation(sim_questions, plot_graph=True, print_report=True, top_n=5):\n    \"\"\" Get the top recommended professionals based on questions\n\n    \"\"\"    \n    df = pd.merge(sim_questions, questions, left_index=True, right_index=True)\n    df = pd.merge(df, answers, left_on='questions_id', right_on='answers_question_id')\n    df = df[['questions_id', 'similarity', 'answers_author_id']]\n    plot_data = pd.DataFrame(columns=['source', 'target', 'value'])\n    plot_data = plot_data.append(pd.DataFrame({'source':['question'] * len(df['questions_id'].drop_duplicates()),\n                                                   'target':df['questions_id'].drop_duplicates(),\n                                                  'value':df['similarity'].drop_duplicates()}), ignore_index=True)\n    temp_values = df['similarity']/df['questions_id'].apply(lambda x: df.groupby('questions_id').size()[x])\n    temp_values = temp_values * df['answers_author_id'].apply(lambda x: df.groupby('answers_author_id').size()[x])\n    plot_data = plot_data.append(pd.DataFrame({'source':df['questions_id'],\n                                                   'target':df['answers_author_id'],\n                                                  'value':temp_values}), ignore_index=True)  \n    \n    if plot_graph:\n        plt.figure(figsize=(15, 15)) \n        G = nx.Graph()\n        \n        node_color = []\n        node_size = []\n        G.add_nodes_from(plot_data[plot_data['source'] == 'question']['source'].dropna().unique())\n        node_color += (['grey']*len(plot_data[plot_data['source'] == 'question']['source'].dropna().unique()))\n        node_size += ([3]*len(plot_data[plot_data['source'] == 'question']['source'].dropna().unique()))\n        G.add_nodes_from(plot_data[plot_data['source'] == 'question']['target'].dropna().sort_values().unique())\n        node_color += (['blue']*len(plot_data[plot_data['source'] == 'question']['target'].dropna().unique()))\n        node_size += (plot_data[plot_data['source'] == 'question']['value'].sort_values().tolist())\n        G.add_nodes_from(plot_data[~plot_data['target'].isin(plot_data['source'])]['target'].dropna().sort_values().unique())\n        node_color += (['cyan']*len(plot_data[~plot_data['target'].isin(plot_data['source'])]['target'].dropna().unique()))\n        node_size += (plot_data[~plot_data['target'].isin(plot_data['source'])].groupby('target')['value'].sum().tolist())\n        node_size = [s*800 for s in node_size]\n\n        G.add_edges_from({tuple(row) for i,row in plot_data[['source', 'target']].dropna().iterrows()})\n        nx.draw_networkx(G, with_labels=True, node_color=node_color, font_size=8, node_size=node_size)\n        plt.title('Recommendation graph from new question to professionals')\n        plt.axis('off')\n        \n        legend_email = mpatches.Patch(color='grey', label='New Question')\n        legend_question = mpatches.Patch(color='blue', label='Similar Question')\n        legend_student = mpatches.Patch(color='cyan', label='Professional')\n        plt.legend(handles=[legend_email, legend_question, legend_student])\n        plt.show()\n\n    if print_report:\n        top_professionals = plot_data[~plot_data['target'].isin(plot_data['source'])][['target', 'value']].groupby('target').sum()\n        top_professionals = top_professionals.reset_index().sort_values('value', ascending = False).head(top_n)\n        top_professionals.columns = ['professional', 'recommendation_score']\n        print(top_professionals)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44cd0775fc6edb552e97b1a3c75baa6bc2bab6b1"},"cell_type":"markdown","source":"## Example 1  <a id=\"recommender_example1\"></a> \nI use a already existing question to get a recommendation. It's a question about the process of becoming a lawyer and how hard it will be."},{"metadata":{"trusted":true,"_uuid":"3bf8644af977e2957e757923158de8021c246d83","_kg_hide-input":true},"cell_type":"code","source":"sim_corpus = questions['questions_full_text']\nsim_text = sim_corpus[seed]\nprint('Example 1 Question:\\n', sim_text)\nsim_questions = get_similar_docs(sim_corpus, sim_text, top=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35c19e3b411406894e10b9e1daa237ac533b0b9a"},"cell_type":"markdown","source":"**Similar Questions:  **  \nIn this example the first recommendation is the question itself.  \nBut a look on the other recommendation seems to be a good match too. They are about *lawyer* and how *hard* it will be."},{"metadata":{"trusted":true,"_uuid":"4a515bef34bc323eb6ab88fccbf88a7828046e67"},"cell_type":"code","source":"sim_questions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0069e530fb8d4fb014d394476b67343876795b0c"},"cell_type":"markdown","source":"**Answers to similar questions**  \nNow I can merge the recommended questions with the answers for these questions. This can be used to give the student who asked the question a first recommendation of his question. Maybe these answers are already an answer to his question."},{"metadata":{"trusted":true,"_uuid":"3983c59ab8010c4a1da331b3cfb4ba67905442ea","scrolled":false},"cell_type":"code","source":"get_questions_answers(sim_questions).head().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed49e33489786cf3038bb3b088b077ce7ef83768"},"cell_type":"markdown","source":"**Recommended Professionals**  \nIn the middle we have the <span style=\"background-color:gray\">new question</span> with edges to <span style=\"background-color:blue\">similar questions</span>. The size for the nodes of the similar questions dependence  on the similarity.  \nThen we can see what <span style=\"background-color:blue\">similar questions</span> have answered by <span style=\"background-color:cyan\">professionals</span>. The size for the nodes of the professionals dependence on the recommendation score.  \nWe search a professionals with a big node, because they have a higher recommendation.\n\nThe Professional **c5c2ca95fcd3463a8852b8bc9d636313** has the highest score with 2.27. This is because he answered three questions with a high similarity."},{"metadata":{"trusted":true,"_uuid":"70cd0c6dedfcd71f0c47dfe81d547bdbb036246f"},"cell_type":"code","source":"get_recommendation(sim_questions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37b7481d137de7300591d6b451f855eed7f25218"},"cell_type":"markdown","source":"## Example 2  <a id=\"recommender_example2\"></a> \nExample 2 use a new defined question about the carrer as a data scientist."},{"metadata":{"trusted":true,"_uuid":"307c556cddee20bfc8eea53ee854dc3d2426e1fd","_kg_hide-input":true},"cell_type":"code","source":"query_text = 'I will finish my college next year and would like to start a career as a data scientist. \\n'\\\n            +'What is the best way to become a good data scientist? #data-science'\nprint('Example 2 Question:\\n', query_text)\nsim_questions = get_similar_docs(sim_corpus, query_text, top=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a427117c0b76d9e3c246c4ba5e3d35729f09bd9"},"cell_type":"markdown","source":"**Similar Questions:  **  \nThe recommended questions are also about the carrer and preparation of become a data scientist."},{"metadata":{"trusted":true,"_uuid":"bbb41eca9ee678e9915059231aee52a36faa355e"},"cell_type":"code","source":"sim_questions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52f0cdc2c7e8df24cf683cf0f2811c9c523d1f95"},"cell_type":"markdown","source":"**Answers to similar questions**  \nHere we have several answers to the recommended similar questions and can use this to forward the question to a professional."},{"metadata":{"trusted":true,"_uuid":"6f4888f57cd5ac700e7305180c5078b53d2c8e79"},"cell_type":"code","source":"get_questions_answers(sim_questions).head(5).T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed565266b26d9915b56361c0e98c2eddd2d5a830"},"cell_type":"markdown","source":"**Recommended Professionals**  \nIn this example we have increased the the size to the top 10 similar questions. As we can see there is one professional, who has answered three of this similar questions and seems to be a good recommendation."},{"metadata":{"trusted":true,"_uuid":"108ec6effaca5756b3285b29afb656817e643009"},"cell_type":"code","source":"get_recommendation(sim_questions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56c74acb932eecf777a377b0f44483384e4da7ee"},"cell_type":"markdown","source":"# 6. Topic Model (LDA) <a id=\"lda\"></a>  \nIn this section I will implement a LDA Model to get topic probabilities for the questions. We can use this to see how topics are distributed across questions and which words characterize them.  \nNew questions can be allocated to topics and forwarded to professional who are familiar with these topics.\n\n1. Use NLP on the Questions corpus.  \n    a. Use part-of-speech tagging to filter words.  \n    b. Filter extrem values from corpus.  \n    c. Calculate the tf-idf. \n2. Train a LDA Model.  \n3. Give the topics names.\n3. Get the topic probability of a query text."},{"metadata":{"_uuid":"9b77a0e8cc9215d0f019847f2990b28ba7465e1b"},"cell_type":"markdown","source":"## Parameters"},{"metadata":{"trusted":true,"_uuid":"ea3ae0f16c1ea1b7c1350685f8011c597242509c"},"cell_type":"code","source":"# Gensim Dictionary\nextremes_no_below = 10\nextremes_no_above = 0.6\nextremes_keep_n = 8000\n\n# LDA\nnum_topics = 18\npasses = 20\nchunksize = 1000\nalpha = 1/50","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ea6ca7808f15a1c59df417acc998323117820ba"},"cell_type":"markdown","source":"## Functions"},{"metadata":{"trusted":true,"_uuid":"832000e03ca48de376e76633d0071abd61e523f3","_kg_hide-input":true},"cell_type":"code","source":"def get_model_results(ldamodel, corpus, dictionary):\n    \"\"\" Create doc-topic probabilities table and visualization for the LDA model\n\n    \"\"\"  \n    vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n    transformed = ldamodel.get_document_topics(corpus)\n    df = pd.DataFrame.from_records([{v:k for v, k in row} for row in transformed])\n    return vis, df  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"118275936bf3d97327624189cea18b8b5d3ac4e2","_kg_hide-input":true},"cell_type":"code","source":"def get_model_wordcloud(ldamodel):\n    \"\"\" Create a Word Cloud for each topic of the LDA model\n\n    \"\"\"  \n    plot_cols = 3\n    plot_rows = math.ceil(num_topics / 3)\n    axisNum = 0\n    plt.figure(figsize=(5*plot_cols, 3*plot_rows))\n    for topicID in range(ldamodel.state.get_lambda().shape[0]):\n        #gather most relevant terms for the given topic\n        topics_terms = ldamodel.state.get_lambda()\n        tmpDict = {}\n        for i in range(1, len(topics_terms[0])):\n            tmpDict[ldamodel.id2word[i]]=topics_terms[topicID,i]\n\n        # draw the wordcloud\n        wordcloud = WordCloud( margin=0,max_words=20 ).generate_from_frequencies(tmpDict)\n        axisNum += 1\n        ax = plt.subplot(plot_rows, plot_cols, axisNum)\n\n        plt.imshow(wordcloud, interpolation='bilinear')\n        title = topicID\n        plt.title(title)\n        plt.axis(\"off\")\n        plt.margins(x=0, y=0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb9b52b789b3a47d5acf04bfa4f9831809ec1c04","_kg_hide-input":true},"cell_type":"code","source":"def topic_query(data, query):\n    \"\"\" Get Documents matching the query with the doc-topic probabilities\n\n    \"\"\"  \n    result = data\n    result['sort'] = 0\n    for topic in query:\n        result = result[result[topic] >= query[topic]]\n        result['sort'] += result[topic]\n    result = result.sort_values(['sort'], ascending=False)\n    result = result.drop('sort', axis=1)\n    result = result.head(5)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3b3157ba4d863b69eae25ef5fe0894ab9695845","_kg_hide-input":true},"cell_type":"code","source":"def get_text_topics(text, top=20):\n    \"\"\" Get the topics probabilities for a text and highlight relevant words\n\n    \"\"\"    \n    def token_topic(token):\n        return topic_words.get(token, -1)\n    \n    colors = ['\\033[46m', '\\033[45m', '\\033[44m', '\\033[43m', '\\033[42m', '\\033[41m', '\\033[47m']    \n    nlp_tokens = nlp_preprocessing([text])\n\n    bow_text = [lda_dic.doc2bow(doc) for doc in nlp_tokens]\n    bow_text = lda_tfidf[bow_text]\n    topic_text = lda_model.get_document_topics(bow_text)\n    topic_text = pd.DataFrame.from_records([{v:k for v, k in row} for row in topic_text])\n    \n\n    print('Question:')\n    topic_words = []\n    topic_labeled = 0\n    for topic in topic_text.columns.values:\n        topic_terms = lda_model.get_topic_terms(topic, top)\n        topic_words = topic_words+[[topic_labeled, lda_dic[pair[0]], pair[1]] for pair in topic_terms]\n        topic_labeled += 1\n    topic_words = pd.DataFrame(topic_words, columns=['topic', 'word', 'value']).pivot(index='word', columns='topic', values='value').idxmax(axis=1)\n    nlp_doc = nlp(text)\n    text_highlight = ''.join([x.string if token_topic(x.lemma_.lower()) <0  else colors[token_topic(x.lemma_.lower()) % len(colors)] + x.string + '\\033[0m' for x in nlp_doc])\n    print(text_highlight) \n    \n    print('\\nTopics:')\n    topic_labeled = 0\n    for topic in topic_text:\n        print(colors[topic_labeled % len(colors)]+'Topic '+str(topic)+':', '{0:.2%}'.format(topic_text[topic].values[0])+'\\033[0m')\n        topic_labeled += 1\n\n    # Plot Pie chart\n    plt_data = topic_text\n    plt_data.columns = ['Topic '+str(c) for c in plt_data.columns]\n    plt_data['Others'] = 1-plt_data.sum(axis=1)\n    plt_data = plt_data.T\n    plt_data.plot(kind='pie', y=0, autopct='%.2f')\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.title('Topics Probabilities')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a15892694b985e8d7859a5984ecfe2a91634b6c1"},"cell_type":"markdown","source":"## Create Model <a id=\"lda_model\"></a> "},{"metadata":{"trusted":true,"_uuid":"90bb54123aea8ef9384f9b0664afd4a56a0d6478","_kg_hide-input":true},"cell_type":"code","source":"%%time\nlda_tokens = questions['nlp_tokens']\n# Gensim Dictionary\nlda_dic = gensim.corpora.Dictionary(lda_tokens)\nlda_dic.filter_extremes(no_below=extremes_no_below, no_above=extremes_no_above, keep_n=extremes_keep_n)\nlda_corpus = [lda_dic.doc2bow(doc) for doc in lda_tokens]\n\nlda_tfidf = gensim.models.TfidfModel(lda_corpus)\nlda_corpus = lda_tfidf[lda_corpus]\n\n# Create LDA Model\nlda_model = gensim.models.ldamodel.LdaModel(lda_corpus, num_topics=num_topics, \n                                            id2word = lda_dic, passes=passes,\n                                            chunksize=chunksize,update_every=0, \n                                            alpha=alpha, random_state=seed)\n\n# Create Visualization and Doc-Topic Probapilities\nlda_vis, lda_result = get_model_results(lda_model, lda_corpus, lda_dic)\nlda_questions = questions[['questions_id', 'questions_title', 'questions_body']]\nlda_questions = pd.concat([lda_questions, lda_result.add_prefix('Topic_')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b153a6d1aab72ceea7cd059ea2dd8656f154839b"},"cell_type":"markdown","source":"## Topics  <a id=\"lda_topics\"></a> \nEach wordcloud shows a topic and the top words who define the topic. \nHere some examples:    \nTopic 0 is for teacher (*teacher, teaching, education, ...*)  \nTopic 1 is for designer (*design, video, graphic, art, ...*)  \nTopic 4 is for veterinary (*veterainary, vet, animal, ...*) but seems to be for actors to (*film, theatre, music, singer, ...*)  \nTopic 9 is for health (*medicine, doctor, dental, ...*)  \nTopic 13 is for engineers (*engineering, mechanical, aerospace, electrical, ...*)  \nTopic 17 is for sport (*sport, athlet, basketball, ...*)"},{"metadata":{"trusted":true,"_uuid":"72d42361a13378fadfaccb523c299495c56a8aef"},"cell_type":"code","source":"get_model_wordcloud(lda_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f13e31b8e45d9a1382394856cfc7a8e7278285aa"},"cell_type":"markdown","source":"## Interactive Visualization  \n*lda_vis* is a interactive visualization for topic model. But it makes some problems with the sceen width on kaggle, so I commented it out."},{"metadata":{"trusted":true,"_uuid":"e3859ee0e992c02ac6f672e9ac0213add425fc9c"},"cell_type":"code","source":"#lda_vis","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b90d4f83a55c10f4d1b688b674a183c212783ba"},"cell_type":"markdown","source":"## Document-Topic Probabilities <a id=\"lda_doc_topic_prob\"></a> \nHere are the topic probabilites for the first five questions.  \nTopics with *NaN* values for these five question were deleted.  \nIf a topic probabilites is under a give threshold it gets automaticaly a *NaN* value"},{"metadata":{"trusted":true,"_uuid":"318227c84748729db71a7a3e9a07edc2e1c98497"},"cell_type":"code","source":"lda_questions.head(5).dropna(axis=1, how='all').T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"057926656658ac7d8fce6e80fb9ca81be162f660"},"cell_type":"markdown","source":"## Example 1  <a id=\"lda_example1\"></a> \nThe example with the data science text was assigned to topic 3 with 87%.  \nThe highlighted text are words, who define the topic.  \nA look at the previously created wordcloud shows that topic 3 is a mix of *math* and *computer science*."},{"metadata":{"trusted":true,"_uuid":"3c1d7c9bc7eeceda27ab5fbf16637a4082dc46ba","_kg_hide-input":true},"cell_type":"code","source":"query_text = 'I will finish my college next year and would like to start a career as a Data Scientist. \\n\\n'\\\n            +'What is the best way to become a good Data Scientist? #data-science'\nget_text_topics(query_text, 80)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66c068009b4908e435604b437da6f9dbe23ed845"},"cell_type":"markdown","source":"## Example 2  <a id=\"lda_example2\"></a>\nNow I would like to make a query, which gives me back documents with the topic *veterinary (**Topic 4**)* and *health (**Topic 9**)*.  \nThe first two questions are about descision to begin the career as a veterinarian (*Topic 4*) or in another medical field (*Topic 9*)."},{"metadata":{"trusted":true,"_uuid":"d3b1ba5573567fdfac96cb0bd0fe28702602d2fa","scrolled":false},"cell_type":"code","source":"query = {'Topic_4':0.4, 'Topic_9':0.4}\ntopic_query(lda_questions, query).dropna(axis=1, how='all').head(2).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66ab0c9fafa3ff8fa938e82a96a1c50ce28e2ef7","_kg_hide-input":true},"cell_type":"code","source":"get_text_topics(questions['questions_full_text'][20658], 50)\nprint()\nget_text_topics(questions['questions_full_text'][3075], 50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0137dbd02f26ba096e7b0a7d0105d005ae9fb3c8"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}