{"cells":[{"metadata":{"_cell_guid":"9bdf9e9e-24e0-423d-be7a-bce5bc7ec504","_uuid":"57162a5dcf04379cf4daad672d92a4f6bf58bea6"},"cell_type":"markdown","source":"**Author:** Raoul Malm\n\n**Abstract:** \n\nGiven are 4750 labeled images (1.73GB) showing plants of 12 different types, the goal is to classify correctly the species shown on the 794 images (91MB) of the test set. All images are quadratic but vary in size. We resize them such that each image has the shape (299,299,3). Next, we detect and segment the plant parts of the images, then normalize them such that each pixel is defined on the range [-1,1]. An optional step is to generate new images through rotations, translations and axis flippings, augmenting the original data. All images are then fed into a pretrained Xception model provided by keras and we extract 2048 bottleneck features for each image. Having computed these features once, we train and validate a basic logistic regression, random forest and fully connected neural network model. Finally, we predict the species classes of the test images and write the submission file. Note, that this work demonstrates just a quick and basic implementation and is not fine-tuned.\n\n**Results:**  \n\n- Using a subset of 200 samples of each species for training and validation with a 90%/10% split, we can achieve an accuracy of 88.33% on the validation set. This takes roughly 30 minutes within the kaggle environment.\n- Using all labeled data for training and making use of little data augmentation, we can obtain an accuracy of 90.06% on the test set. This takes several hours on my average laptop CPU. If you have more computer power you can easily improve these results.\n\n**Outline:**\n\n1. [Libraries and settings](#1-bullet)\n2. [Analyze data](#2-bullet)\n3. [Manipulate data](#3-bullet)\n4. [Extract bottleneck features from Xception](#4-bullet)\n5. [Train and validate models](#5-bullet)\n6. [Predict species and submit test results](#6-bullet)\n\n**References:** \n\n[Seedlings - Pretrained keras models by beluga](https://www.kaggle.com/gaborfodor/seedlings-pretrained-keras-models)  \n[Plant Seedlings Fun with Computer Vision by Gábor Vecsei](https://www.kaggle.com/gaborvecsei/plant-seedlings-fun-with-computer-vision)\n\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"403d2601-16bb-4513-8485-b2dba467e6f6","_uuid":"f2c38a3c76bd841196928dd13bfa34a349bfb28a"},"cell_type":"markdown","source":"# 1. Libraries and settings <a class=\"anchor\" id=\"1-bullet\"></a> ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"fcc18b58-8124-45ed-a625-a77a3a161bba","_uuid":"537d1cb91cd3de22ae42873b9840b44b0e5b38df","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.applications import xception\nfrom keras.preprocessing import image \nimport keras.preprocessing.image\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport sklearn.ensemble\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nfrom tqdm import tqdm\nfrom glob import glob\nimport cv2\nimport os\nimport seaborn as sns\nimport mpl_toolkits.axes_grid1\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport datetime\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 16\n\n# start timer\nglobal_start = datetime.datetime.now();\n\n# validation set size\nvalid_set_size_percentage = 10; # default = 10%\n\n# kaggle\nrunning_on_kaggle = True # set True when running on kaggle\n\n# train data\ntake_only_samples_of_train_data = True; # set False to train on all train data\nnum_samples_of_train_data_per_species = 200 # < 221, ignored if take_only_samples_of_train_data = True\nload_bf_of_train_data = False # set True to load bottleneck features from file\n\n# test data\ntake_only_samples_of_test_data = False; # set False to predict on all test data\nnum_samples_of_test_data = 200 # < 794, ignored if take_only_samples_of_test_data = True\nload_bf_of_test_data = False # set True to load bottleneck features from file\n\n# augmented images\nuse_aug_data = False # set True to use augmented images\nload_bf_of_aug_data = True # set True to load bottleneck features from file\n\n# show plots\nshow_plots = True # set False to reduce notebook running time\n\n# overview of directories\nprint('current directory:')\n!ls  \nprint('\\nparent directory:')\n!ls ..  ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a1b730c7-e2c0-4dc9-8b72-6307c5d0d61e","_uuid":"e3e5041c946170c3a6ca28c08434a944827d05a9"},"cell_type":"markdown","source":"# 2. Analyze Data <a class=\"anchor\" id=\"2-bullet\"></a> \n- 12 different plant species\n- at least 211 images of each species => in total 4750 in training set\n- image sizes vary strongly => resize images to (299,299,3) for Xception input","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"85b4de29-9400-4173-afd0-7a5d2614f347","scrolled":true,"_uuid":"b31caf199b7c2e460cae43e85a72e3c572a1053d","collapsed":true,"trusted":true},"cell_type":"code","source":"# input directory\nprint('input directory:')\n!ls ../input/plant-seedlings-classification\nprint('\\nfolders containing images of the corresponding species:')\n!ls ../input/plant-seedlings-classification/train","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db062349-6665-439d-ac84-703826272a8a","_uuid":"2dfbe152f6e86444eb104f4de783d9905eecd97e","collapsed":true,"trusted":false},"cell_type":"code","source":"## read train and test data\n\n# directories\ncw_dir = os.getcwd()\ndata_dir = '../input/plant-seedlings-classification/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')\n\n# different species in the data set\nspecies = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat',\n           'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed',\n           'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\nnum_species = len(species)\n\n# print number of images of each species in the training data\nfor sp in species:\n    print('{} images of {}'.format(len(os.listdir(os.path.join(train_dir, sp))),sp))\n    \n# read all train data\ntrain = []\nfor species_id, sp in enumerate(species):\n    for file in os.listdir(os.path.join(train_dir, sp)):\n        train.append(['train/{}/{}'.format(sp, file), file, species_id, sp])\ntrain_df = pd.DataFrame(train, columns=['filepath', 'file', 'species_id', 'species'])\nprint('')\nprint('train_df.shape = ', train_df.shape)\n\n# read all test data\ntest = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file), file])\ntest_df = pd.DataFrame(test, columns=['filepath', 'file'])\nprint('test_df.shape = ', test_df.shape)\n\n# function to read an image \ndef read_image(filepath, target_size=None):\n    img = cv2.imread(os.path.join(data_dir, filepath), cv2.IMREAD_COLOR)\n    img = cv2.resize(img.copy(), target_size, interpolation = cv2.INTER_AREA)\n    #img = image.load_img(os.path.join(data_dir, filepath),target_size=target_size)\n    #img = image.img_to_array(img)\n    return img\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2780b250-0fe1-474a-9db2-a670159aef1f","_uuid":"0df363b8f3a0c632bfa8f9905e54619743a876ae","collapsed":true,"trusted":false},"cell_type":"code","source":"## get all image shapes: this is time-consuming, therefore I have deactivated\n## the code. The results are included below and show that all\n## images are quadratic, but their size can be very different. \n\nif False:\n    train_df['image_heigth'] = 0\n    train_df['image_width'] = 0\n    train_df['image_channel'] = 0\n\n    #get all image shapes\n    for i in range(len(train_df)):\n        img = read_img(train_df.filepath.values[i])\n        train_df.loc[i,'image_heigth'] = img.shape[0]\n        train_df.loc[i,'image_width'] = img.shape[1]\n        train_df.loc[i,'image_channel'] = img.shape[2]\n\n    test_df['image_heigth'] = 0\n    test_df['image_width'] = 0\n    test_df['image_channel'] = 0\n\n    # get all image shapes\n    for i in range(len(test_df)):\n        img = read_img(test_df.filepath.values[i])\n        test_df.loc[i,'image_heigth'] = img.shape[0]\n        test_df.loc[i,'image_width'] = img.shape[1]\n        test_df.loc[i,'image_channel'] = img.shape[2]\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1685c861-967e-453c-af73-6a99d2767109","_uuid":"2a0a0d0cb9d56caa1daa9c8fa2a1450ca535b5e6","collapsed":true,"trusted":false},"cell_type":"code","source":"# train data\n#print(train_df.describe())\ntrain_df.head()\n\n#         species_id  image_heigth  image_width  image_channel\n# count  4750.000000   4750.000000  4750.000000         4750.0\n# mean      5.669263    370.730105   371.465895            3.0\n# std       3.311364    318.649607   323.275012            0.0\n# min       0.000000     49.000000    49.000000            3.0\n# 25%       3.000000    140.000000   140.000000            3.0\n# 50%       6.000000    266.500000   266.500000            3.0\n# 75%       8.000000    507.000000   507.750000            3.0\n# max      11.000000   3457.000000  3991.000000            3.0","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d509f770-63dc-4123-b955-55a910e9c28e","_uuid":"f7fc7f98a049fb56775db644572cd0f7203ed74d","collapsed":true,"trusted":false},"cell_type":"code","source":"# test data\n#print(test_df.describe())\ntest_df.head()\n\n#        image_heigth  image_width  image_channel\n# count    794.000000   794.000000          794.0\n# mean     269.492443   269.492443            3.0\n# std       43.109211    43.109211            0.0\n# min      200.000000   200.000000            3.0\n# 25%      232.000000   232.000000            3.0\n# 50%      267.000000   267.000000            3.0\n# 75%      307.000000   307.000000            3.0\n# max      349.000000   349.000000            3.0","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d3d0a954-ff10-4258-9b0e-1c92d92a8076","_uuid":"369c067e8630b553008047f83bb76a7b4731be27","collapsed":true,"trusted":false},"cell_type":"code","source":"## show 12 images of all species of train data\n\nif show_plots:\n\n    fig = plt.figure(1, figsize=(num_species, num_species))\n    grid = mpl_toolkits.axes_grid1.ImageGrid(fig, 111, nrows_ncols=(num_species, num_species), \n                                             axes_pad=0.05)\n    i = 0\n    for species_id, sp in enumerate(species):\n        for filepath in train_df[train_df['species'] == sp]['filepath'].values[:num_species]:\n            ax = grid[i]\n            img = read_image(filepath, (224, 224))\n            ax.imshow(img.astype(np.uint8))\n            ax.axis('off')\n            if i % num_species == num_species - 1:\n                ax.text(250, 112, sp, verticalalignment='center')\n            i += 1\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"43e1fe30-394e-452e-8f54-4ebfd870b153","_uuid":"8b1818931f12e4d8f3a5df7c912e09bbbfd705c8","collapsed":true,"trusted":false},"cell_type":"code","source":"## show some test images \n\nif show_plots:\n\n    fig = plt.figure(1, figsize=(10, 10))\n    grid = mpl_toolkits.axes_grid1.ImageGrid(fig, 111, nrows_ncols=(5, 10), \n                                             axes_pad=0.05)\n    i = 0\n    for j in range(5):\n        for filepath in test_df['filepath'].values[j*5:j*5+10]:\n            ax = grid[i]\n            img = read_image(filepath, (224, 224))\n            ax.imshow(img.astype(np.uint8))\n            ax.axis('off')\n            i += 1\n    plt.show();\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"78e7a6ca-6dc2-4843-9688-0e9ff92097bf","_uuid":"ba116f3b99d968a057910f5a55f9c608e24c1d43"},"cell_type":"markdown","source":"# 3. Manipulate data <a class=\"anchor\" id=\"3-bullet\"></a> \n- detect and segment the plants in the images\n- preprocess images to shape (299,299,3) with values in the range [-1,1]\n- create numpy arrays with training/validation/test images","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2651ed73-7df2-49e4-aabf-32a727f7dbbf","_uuid":"87a14b1a0398b51d46ebcf12b859da21f83e91d2","collapsed":true,"trusted":false},"cell_type":"code","source":"## take a fixed number of samples for testing purpose\n\nif take_only_samples_of_train_data:\n    train_df = pd.concat([train_df[train_df['species'] == sp][:num_samples_of_train_data_per_species] for sp in species])\n    train_df.index = np.arange(len(train_df))\n\nif take_only_samples_of_test_data:\n    test_df = test_df[:num_samples_of_test_data]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6149b2d5-51ad-478b-8247-0cec9fcfa4a4","_uuid":"0eb54d623fafbf903e34b2363ffbed6117133d8b","collapsed":true,"trusted":false},"cell_type":"code","source":"## detect and segment plants in the image \n\ndef create_mask_for_plant(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    \n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n\n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return mask\n\ndef segment_plant(image):\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\n\ndef sharpen_image(image):\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n    return image_sharp\n\ndef read_segmented_image(filepath, img_size):\n    img = cv2.imread(os.path.join(data_dir, filepath), cv2.IMREAD_COLOR)\n    img = cv2.resize(img.copy(), img_size, interpolation = cv2.INTER_AREA)\n\n    image_mask = create_mask_for_plant(img)\n    image_segmented = segment_plant(img)\n    image_sharpen = sharpen_image(image_segmented)\n    return img, image_mask, image_segmented, image_sharpen\n     \n\n# show some images\nif show_plots:\n    for i in range(4):\n \n        img, image_mask, image_segmented, image_sharpen = read_segmented_image(\n            train_df.loc[i,'filepath'],(224,224))\n        \n        fig, axs = plt.subplots(1, 4, figsize=(20, 20))\n        axs[0].imshow(img.astype(np.uint8))\n        axs[1].imshow(image_mask.astype(np.uint8))\n        axs[2].imshow(image_segmented.astype(np.uint8))\n        axs[3].imshow(image_sharpen.astype(np.uint8))\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"329bb2af-ab4b-4565-a81e-397681701291","scrolled":true,"_uuid":"d9e21f0bbebc3e387ba20333dbb25191c3ea7137","collapsed":true,"trusted":false},"cell_type":"code","source":"## read and preprocess all training/validation/test images and labels\n\ndef preprocess_image(img):\n    img /= 255.\n    img -= 0.5\n    img *= 2\n    return img\n\ntarget_image_size = 299\n\nprint('read and preprocess training and validation images')\n\n# read, preprocess training and validation images  \nx_train_valid = np.zeros((len(train_df), target_image_size, target_image_size, 3),\n                         dtype='float32')\ny_train_valid = train_df.loc[:, 'species_id'].values \nfor i, filepath in tqdm(enumerate(train_df['filepath'])):\n    \n    # read original images\n    #img = read_image(filepath, (target_image_size, target_image_size))\n    \n    # read segmented image\n    _,_,_,img = read_segmented_image(filepath, (target_image_size, target_image_size))\n    \n    # all pixel values are now between -1 and 1\n    x_train_valid[i] = preprocess_image(np.expand_dims(img.copy().astype(np.float), axis=0)) \n\nprint('read and preprocess test images')\n\n# read, preprocess test images  \nx_test = np.zeros((len(test_df), target_image_size, target_image_size, 3), dtype='float32')\nfor i, filepath in tqdm(enumerate(test_df['filepath'])):\n    \n    # read original image\n    #img = read_image(filepath, (target_image_size, target_image_size))\n    \n    # read segmented image\n    _,_,_,img = read_segmented_image(filepath, (target_image_size, target_image_size))\n    \n    # all pixel values are now between -1 and 1\n    x_test[i] = preprocess_image(np.expand_dims(img.copy().astype(np.float), axis=0)) \n    \nprint('x_train_valid.shape = ', x_train_valid.shape)\nprint('x_test.shape = ', x_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e1fb7d69-668a-4aef-995f-30bf48ac0d51","_uuid":"73d9e6bef2ffb4c4671c660f4c514242dd91b613","collapsed":true,"trusted":false},"cell_type":"code","source":"## data augmentation\n\n# generate new images via rotations, translations, flippings\ndef generate_images(imgs):\n    imgs_len = len(imgs)\n    image_generator = keras.preprocessing.image.ImageDataGenerator(\n        rotation_range = 10, width_shift_range = 0.1 , height_shift_range = 0.1,\n        horizontal_flip = False, vertical_flip = False, zoom_range = 0.1)\n\n    imgs = image_generator.flow(imgs.copy(), np.zeros(imgs_len), batch_size=imgs_len, shuffle = False).next()    \n  \n    # return transformed images in the same order as the original ones\n    return imgs[0]\n\n\n# show some examples\nif show_plots:\n    imgs = (((x_train_valid[0:4]+1.)/2.)*255.) # transform pixels into range [0,255]\n    imgs_generated = imgs\n\n    fig, axs = plt.subplots(4, 8, figsize=(20, 10))\n    for i in range(8):\n        axs[0,i].imshow(imgs_generated[0].astype(np.uint8))\n        axs[1,i].imshow(imgs_generated[1].astype(np.uint8))\n        axs[2,i].imshow(imgs_generated[2].astype(np.uint8))\n        axs[3,i].imshow(imgs_generated[3].astype(np.uint8))   \n        imgs_generated = generate_images(imgs)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f0f35011-d29f-45a8-a649-e37ce0ec7da9","_uuid":"f26c233eb60f06ae02729e77bf37b83aaa3a3509"},"cell_type":"markdown","source":"\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9a2df7dc-836b-465b-8a67-246468a02c72","_uuid":"9b28a885f91aefc363fa893440ec26cb05f8b91b"},"cell_type":"markdown","source":"# 4. Extract bottleneck features from Xception <a class=\"anchor\" id=\"4-bullet\"></a> \n- compute or load the 2048 bottleneck features for each image","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ff39473f-09f2-4ec8-8b01-72b65f5d3293","_uuid":"3f851cfd9587aa1d043d5f3e398932dde47b1082","collapsed":true,"trusted":false},"cell_type":"code","source":"## copy xception models into ~.keras directory\n\n# set True when running on kaggle\nif running_on_kaggle:\n    # create cache and models directory (only on kaggle)\n    cache_dir = os.path.expanduser(os.path.join('~', '.keras'))\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    models_dir = os.path.join(cache_dir, 'models')\n    if not os.path.exists(models_dir):\n        os.makedirs(models_dir)\n\n    # show available pretrained keras models\n    !ls ../input/keras-pretrained-models/\n\n    # copy xception models to models directory\n    print('')\n    print('use xception models')\n    !cp ../input/keras-pretrained-models/xception* ~/.keras/models/\n    !ls ~/.keras/models\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"da3fd809-29a0-4c20-8d62-171c56adc88d","_uuid":"57279e8c9b35c22f1f14ed78af810fe47789427e","collapsed":true,"trusted":false},"cell_type":"code","source":"## compute or load bottleneck features from xception model\n\n# train and validation images\nif not load_bf_of_train_data:\n    \n    print('x_train_valid.shape = ', x_train_valid.shape)\n    print('y_train_valid.shape = ', y_train_valid.shape)\n    print('')\n\n    print('compute bottleneck features from Xception network')\n\n    local_start = datetime.datetime.now()\n    \n    # load xception base model and predict the last layer comprising 2048 neurons per image\n    base_model = xception.Xception(weights='imagenet', include_top=False, pooling='avg')\n    x_train_valid_bf = base_model.predict(x_train_valid, batch_size=32, verbose=1)\n\n    print('running time: ', datetime.datetime.now()-local_start)    \n    print('')\n    print('x_train_valid_bf.shape = ', x_train_valid_bf.shape)\n\n    print('')\n    print('save bottleneck features and labels for later ')\n    np.save(os.path.join(os.getcwd(),'x_train_valid_bf.npy'), x_train_valid_bf)\n    np.save(os.path.join(os.getcwd(),'y_train_valid.npy'), y_train_valid)\n\nelse:    \n    # load bottleneck features and labels\n    \n    print('load bottleneck features and labels')\n    \n    x_train_valid_bf = np.load(os.path.join(os.getcwd(),'x_train_valid_bf_of_segmented_images.npy'))\n    y_train_valid = np.load(os.path.join(os.getcwd(),'y_train_valid_of_segmented_images.npy'))\n\n    print('x_train_valid_bf.shape = ', x_train_valid_bf.shape)\n    print('y_train_valid.shape = ', y_train_valid.shape)\n    \n# test images\nif not load_bf_of_test_data:\n    # compute bottleneck features from xception model\n    \n    local_start = datetime.datetime.now()\n    \n    # load xception base model and predict the last layer comprising 2048 neurons per image\n    base_model = xception.Xception(weights='imagenet', include_top=False, pooling='avg')\n    x_test_bf = base_model.predict(x_test, batch_size=32, verbose=1)\n    \n    print('running time: ', datetime.datetime.now()-local_start)    \n    print('')\n    print('x_test_bf = ',x_test_bf.shape)\n\n    print('save bottleneck features ')\n    np.save(os.path.join(os.getcwd(),'x_test_bf.npy'), x_test_bf)\n\nelse:\n    # load bottleneck features and compute the predictions\n    print('load bottleneck features')\n    x_test_bf = np.load(os.path.join(os.getcwd(),'x_test_bf_of_segmented_images.npy'))\n    print('x_test_bf.shape = ', x_test_bf.shape)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0997b477-ab23-45a4-a74b-1436ca053711","_uuid":"7914ce7f12fcbb2ab6a8db4a2672aa5e2f5eed83","collapsed":true,"trusted":false},"cell_type":"code","source":"## compute or load bottleneck features for augmented data\n\nif use_aug_data:\n\n    if not load_bf_of_aug_data:\n\n        for i in range(2):\n            x_aug_tmp = generate_images(x_train_valid)\n            y_aug_tmp = y_train_valid\n\n            print('compute bottleneck features from Xception network')\n            print('x_aug_tmp.shape = ', x_aug_tmp)\n            print('y_aug_tmp.shape = ', y_aug_tmp)\n\n            local_start = datetime.datetime.now()\n\n            # load xception model and predict the last layer having 2048 neurons per image\n            base_model = xception.Xception(weights='imagenet', include_top=False, pooling='avg')\n            x_aug_tmp_bf = base_model.predict(x_aug_tmp, batch_size=32, verbose=1)\n            \n            print('running time: ', datetime.datetime.now()-local_start)    \n            print('')\n            print('x_aug_tmp_bf.shape = ', x_aug_tmp_bf.shape)\n            \n            if i==0:\n                x_aug = x_aug_tmp_bf\n                y_aug = y_aug_tmp\n            else:\n                x_aug = np.concatenate([x_aug,x_aug_tmp_bf])\n                y_aug = np.concatenate([y_aug,y_aug_tmp])\n            \n            print('')\n            print('save bottleneck features and labels for later ')\n            np.save(os.path.join(os.getcwd(),'x_aug.npy'), x_aug)\n            np.save(os.path.join(os.getcwd(),'y_aug.npy'), y_aug)\n\n     \n    else:\n        # load bottleneck features and compute the predictions\n\n        print('load bottleneck features')\n        x_aug_bf = np.load(os.path.join(os.getcwd(), 'x_aug_bf_of_segmented_images.npy'))\n        y_aug = np.load(os.path.join(os.getcwd(), 'y_aug_of_segmented_images.npy'))\n\n        print('x_aug_bf.shape = ', x_aug_bf.shape)\n        print('y_aug.shape = ', y_aug.shape)\n   ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fed6f54-f6b9-4f8e-a290-53fbbfc5430f","_uuid":"06feb75f1da04e6439094775585f8f101f70f5a5","collapsed":true,"trusted":false},"cell_type":"code","source":"## combine files\nif False:\n    x_aug_bf_1 = np.load(os.path.join(os.getcwd(), 'x_aug_bf_of_segmented_images.npy'))\n    y_aug_1 = np.load(os.path.join(os.getcwd(), 'y_aug_of_segmented_images.npy'))\n    print(x_aug_bf_1.shape, y_aug_1.shape)\n    x_aug_bf_2 = np.load(os.path.join(os.getcwd(), 'x_aug_bf.npy'))\n    y_aug_2 = np.load(os.path.join(os.getcwd(), 'y_aug.npy'))\n    print(x_aug_bf_2.shape, y_aug_2.shape)\n    x_aug_bf = np.concatenate([x_aug_bf_1,x_aug_bf_2])\n    y_aug = np.concatenate([y_aug_1,y_aug_2])\n    print(x_aug_bf.shape,y_aug.shape)\n    #np.save(os.path.join(os.getcwd(),'x_aug_bf.npy'), x_aug_bf)\n    #np.save(os.path.join(os.getcwd(),'y_aug.npy'), y_aug)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"946dfce3-180d-435e-b3eb-2b8bb0e87a3e","_uuid":"9f5339438fbfabbfe417be9cebcd97c87121a8a5"},"cell_type":"markdown","source":"# 5. Train and validate models <a class=\"anchor\" id=\"5-bullet\"></a> \n- try out logistic regression, random forest, a neural network\n- validate the models and choose the best predictions","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1a07c90d-2248-4ac1-8ede-f170003f350d","_uuid":"d90634f5d903c97f617ce3d42b3b6df0bdac23f4","collapsed":true,"trusted":false},"cell_type":"code","source":"# split data into training and validation sets\n#set_seed = 123\n\n# use one-hot encoding for categorial labels\ndef dense_to_one_hot(labels_dense, num_classes):\n    num_labels = labels_dense.shape[0]\n    index_offset = np.arange(num_labels) * num_classes\n    labels_one_hot = np.zeros((num_labels, num_classes))\n    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n    return labels_one_hot\n\ndef one_hot_to_dense(labels_one_hot):\n    num_labels = labels_one_hot.shape[0]\n    num_classes = labels_one_hot.shape[1]\n    labels_dense = np.where(labels_one_hot == 1)[1]      \n    return labels_dense\n\n# function to shuffle randomly train and validation data\ndef shuffle_train_valid_data():\n    \n    print('shuffle train and validation data')\n    \n    # shuffle train and validation data of original data\n    perm_array = np.arange(len(x_train_valid_bf)) \n    np.random.shuffle(perm_array)\n    \n    # split train and validation sets based on original data\n    x_train_bf = x_train_valid_bf[perm_array[:train_set_size]]\n    y_train = dense_to_one_hot(y_train_valid[perm_array[:train_set_size]], num_species)\n    x_valid_bf = x_train_valid_bf[perm_array[-valid_set_size:]]\n    y_valid = dense_to_one_hot(y_train_valid[perm_array[-valid_set_size:]], num_species)\n    \n    # augment train data by generated images\n    if use_aug_data:\n        \n        x_train_bf = np.concatenate([x_train_bf, x_aug_bf])\n        y_train = np.concatenate([y_train, dense_to_one_hot(y_aug, num_species)])\n        \n        # shuffle data\n        perm_array = np.arange(len(x_train_bf)) \n        np.random.shuffle(perm_array)\n        \n        x_train_bf = x_train_bf[perm_array]\n        y_train = y_train[perm_array]\n         \n    return x_train_bf, y_train, x_valid_bf, y_valid \n\nif valid_set_size_percentage > 0:\n    # split into train and validation sets\n    valid_set_size = int(len(x_train_valid_bf) * valid_set_size_percentage/100);\n    train_set_size = len(x_train_valid_bf) - valid_set_size;\nelse:\n    # train on all available data\n    valid_set_size = int(len(x_train_valid_bf) * 0.1);\n    train_set_size = len(x_train_valid_bf)\n\n# split into train and validation sets including shuffling\nx_train_bf, y_train, x_valid_bf, y_valid = shuffle_train_valid_data() \n\nprint('x_train_bf.shape = ', x_train_bf.shape)\nprint('y_train.shape = ', y_train.shape)\nprint('x_valid_bf.shape = ', x_valid_bf.shape)\nprint('y_valid.shape = ', y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"881d363b-813e-47c2-a6c0-6a32673fc114","scrolled":true,"_uuid":"e074e8f6359165b5fc605449df6981dcbb8887a3","collapsed":true,"trusted":false},"cell_type":"code","source":"## logistic regression\n\ncv_num = 1\n\nacc_logreg_train = acc_logreg_valid = 0\ny_test_pred_proba_logreg = 0\n\nfor i in range(cv_num):\n    \n    shuffle_train_valid_data() # shuffle data\n    \n    logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n    logreg.fit(x_train_bf, one_hot_to_dense(y_train))\n   \n    acc_logreg_train += logreg.score(x_train_bf, one_hot_to_dense(y_train))\n    acc_logreg_valid += logreg.score(x_valid_bf, one_hot_to_dense(y_valid))\n\n    y_test_pred_proba_logreg += logreg.predict_proba(x_test_bf)\n    \nacc_logreg_train /= cv_num\nacc_logreg_valid /= cv_num\ny_test_pred_proba_logreg /= cv_num\ny_test_pred_class_logreg = np.argmax(y_test_pred_proba_logreg, axis = 1)\n\nprint('Logistic Regression')\nprint('Accuracy train/valid = %.4f/%.4f'%(acc_logreg_train, acc_logreg_valid))\nprint('y_test_pred_class_logreg.shape = ', y_test_pred_class_logreg.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b5dcd6e-62a2-4fa5-b80f-8d319ec33e1d","_uuid":"30e2b906f8833ee857d3fe98d11c1a93be51f19a","collapsed":true,"trusted":false},"cell_type":"code","source":"## random forest\n\nrandom_forest = sklearn.ensemble.RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(x_train_bf, one_hot_to_dense(y_train))\n\nacc_train_random_forest = random_forest.score(x_train_bf, one_hot_to_dense(y_train))\nacc_valid_random_forest = random_forest.score(x_valid_bf, one_hot_to_dense(y_valid))\n\nprint('Random Forest')\nprint('Accuracy train/valid = %.4f/%.4f'%(acc_train_random_forest, acc_valid_random_forest))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"148d9c30-8d64-4c89-abcc-8d1e621c0fa0","_uuid":"0dbea7a9b2f45111aaa862d0d1329799dd949dc5","collapsed":true,"trusted":false},"cell_type":"code","source":"## neural network with tensorflow\n\n# permutation array for shuffling train data\nperm_array_train = np.arange(len(x_train_bf)) \nindex_in_epoch = 0\n\n# function: to get the next mini batch\ndef get_next_batch(batch_size):\n    \n    global index_in_epoch, perm_array_train\n  \n    start = index_in_epoch\n    index_in_epoch += batch_size\n    \n    if index_in_epoch > train_set_size:\n        np.random.shuffle(perm_array_train) # shuffle data\n        start = 0 # start next epoch\n        index_in_epoch = batch_size\n              \n    end = index_in_epoch\n    \n    return x_train_bf[perm_array_train[start:end]], y_train[perm_array_train[start:end]]\n\nx_size = x_train_bf.shape[1] # number of features\ny_size = num_species # binary variable\nn_n_fc1 = 1024 # number of neurons of first layer\nn_n_fc2 = num_species # number of neurons of second layer\n\n# variables for input and output \nx_data = tf.placeholder('float', shape=[None, x_size])\ny_data = tf.placeholder('float', shape=[None, y_size])\n\n# 1.layer: fully connected\nW_fc1 = tf.Variable(tf.truncated_normal(shape = [x_size, n_n_fc1], stddev = 0.1))\nb_fc1 = tf.Variable(tf.constant(0.1, shape = [n_n_fc1]))  \nh_fc1 = tf.nn.relu(tf.matmul(x_data, W_fc1) + b_fc1)\n\n# add dropout\ntf_keep_prob = tf.placeholder('float')\nh_fc1_drop = tf.nn.dropout(h_fc1, tf_keep_prob)\n\n# 3.layer: fully connected\nW_fc2 = tf.Variable(tf.truncated_normal(shape = [n_n_fc1, n_n_fc2], stddev = 0.1)) \nb_fc2 = tf.Variable(tf.constant(0.1, shape = [n_n_fc2]))  \nz_pred = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n# cost function\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_data, \n                                                                       logits=z_pred));\n\n# optimisation function\ntf_learn_rate = tf.placeholder(dtype='float', name=\"tf_learn_rate\")\ntrain_step = tf.train.AdamOptimizer(tf_learn_rate).minimize(cross_entropy)\n\n# evaluation\ny_pred = tf.cast(tf.nn.softmax(z_pred), dtype = tf.float32);\ny_pred_class = tf.cast(tf.argmax(y_pred,1), tf.int32)\ny_data_class = tf.cast(tf.argmax(y_data,1), tf.int32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_class, y_data_class), tf.float32))\n\n# parameters\ncv_num = 1 # number of cross validations\nn_epoch = 15 # number of epochs\nbatch_size = 50 \nkeep_prob = 0.33 # dropout regularization with keeping probability\nlearn_rate_range = [0.01,0.005,0.0025,0.001,0.001,0.001,0.00075,0.0005,0.00025,0.0001,\n                   0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001];\nlearn_rate_step = 3 # in terms of epochs\n\nacc_train_DNN = 0\nacc_valid_DNN = 0\nloss_train_DNN = 0\nloss_valid_DNN = 0\ny_test_pred_proba_DNN = 0\ny_valid_pred_proba = 0\n\n# use cross validation\nfor j in range(cv_num):\n    \n    # start TensorFlow session and initialize global variables\n    sess = tf.InteractiveSession() \n    sess.run(tf.global_variables_initializer())  \n\n    # shuffle train/validation splits\n    shuffle_train_valid_data() \n    n_step = -1;\n\n    # training model\n    for i in range(int(n_epoch*train_set_size/batch_size)):\n\n        if i%int(learn_rate_step*train_set_size/batch_size) == 0:\n            n_step += 1;\n            learn_rate = learn_rate_range[n_step];\n            print('learnrate = ', learn_rate)\n        \n        x_batch, y_batch = get_next_batch(batch_size)\n        \n        sess.run(train_step, feed_dict={x_data: x_batch, y_data: y_batch, \n                                        tf_keep_prob: keep_prob, \n                                        tf_learn_rate: learn_rate})\n\n        if i%int(0.25*train_set_size/batch_size) == 0:\n            \n            train_loss = sess.run(cross_entropy,\n                                  feed_dict={x_data: x_train_bf[:valid_set_size], \n                                             y_data: y_train[:valid_set_size], \n                                             tf_keep_prob: 1.0})\n\n            \n            train_acc = accuracy.eval(feed_dict={x_data: x_train_bf[:valid_set_size], \n                                                 y_data: y_train[:valid_set_size], \n                                                 tf_keep_prob: 1.0})    \n\n            valid_loss = sess.run(cross_entropy,feed_dict={x_data: x_valid_bf, \n                                                           y_data: y_valid, \n                                                           tf_keep_prob: 1.0})\n\n           \n            valid_acc = accuracy.eval(feed_dict={x_data: x_valid_bf, \n                                                 y_data: y_valid, \n                                                 tf_keep_prob: 1.0})      \n\n            print('%.2f epoch: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(\n                (i+1)*batch_size/train_set_size, train_loss, valid_loss, train_acc, valid_acc))\n\n    \n    acc_train_DNN += train_acc\n    acc_valid_DNN += valid_acc\n    loss_train_DNN += train_loss\n    loss_valid_DNN += valid_loss\n    \n    y_valid_pred_proba += y_pred.eval(feed_dict={x_data: x_valid_bf, tf_keep_prob: 1.0}) \n    y_test_pred_proba_DNN += y_pred.eval(feed_dict={x_data: x_test_bf, tf_keep_prob: 1.0}) \n\n    sess.close()\n        \nacc_train_DNN /= float(cv_num)\nacc_valid_DNN /= float(cv_num)\nloss_train_DNN /= float(cv_num)\nloss_valid_DNN /= float(cv_num)\n\n# final validation prediction\ny_valid_pred_proba /= float(cv_num)\ny_valid_pred_class = np.argmax(y_valid_pred_proba, axis = 1)\n\n# final test prediction\ny_test_pred_proba_DNN /= float(cv_num)\ny_test_pred_class_DNN = np.argmax(y_test_pred_proba_DNN, axis = 1)\n\n# final loss and accuracy\nprint('')\nprint('final: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(loss_train_DNN, \n                                                                      loss_valid_DNN, \n                                                                      acc_train_DNN, \n                                                                      acc_valid_DNN))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73d5be5b-a47f-4b80-93a2-2f8fdf0ce4f2","_uuid":"1f2f35ad061ed265d10b4bedfb81dbfc47861d24","collapsed":true,"trusted":false},"cell_type":"code","source":"## show confusion matrix\n\nif show_plots:\n    \n    cnf_matrix = confusion_matrix(one_hot_to_dense(y_valid), y_valid_pred_class)\n\n    abbreviation = ['BG', 'Ch', 'Cl', 'CC', 'CW', 'FH', 'LSB', 'M', 'SM', 'SP', 'SFC', 'SB']\n    pd.DataFrame({'class': species, 'abbreviation': abbreviation})\n\n    fig, ax = plt.subplots(1)\n    ax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\n    ax.set_xticklabels(abbreviation)\n    ax.set_yticklabels(abbreviation)\n    plt.title('Confusion matrix of validation set')\n    plt.ylabel('True species')\n    plt.xlabel('Predicted species')\n    plt.show();\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f1a6a5d-fa59-4a73-b26d-28aaa5df630b","_uuid":"9013a99c0f8c7f92d7325dd4570c9f9c9035ed9a"},"cell_type":"markdown","source":"# 6. Predict species and submit test results <a class=\"anchor\" id=\"6-bullet\"></a> \n- take the results from the best model or a combination\n- write the submission file","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"99321744-1f3e-4064-890c-c4159f67057c","_uuid":"02e19fa2de1c9e79e1bed539af73b7f584e77c5f","collapsed":true,"trusted":false},"cell_type":"code","source":"## choose prediction\n#y_test_pred_class = y_test_pred_class_DNN\ny_test_pred_class = y_test_pred_class_logreg","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f971cfca-82cf-4d09-9f45-0bbd646acc42","_uuid":"4325434464254b903e308f33c77fa0dd22fa21f5","collapsed":true,"trusted":false},"cell_type":"code","source":"## submit test results\ntest_df['species_id'] = y_test_pred_class\ntest_df['species'] = [species[sp] for sp in y_test_pred_class]\ntest_df[['file', 'species']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15a3db81-f79e-4747-acf5-3145759c09bf","_uuid":"e5abf75b812f38fa99aa463654ad476e19044273","collapsed":true,"trusted":false},"cell_type":"code","source":"print('total running time: ', datetime.datetime.now()-global_start)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}