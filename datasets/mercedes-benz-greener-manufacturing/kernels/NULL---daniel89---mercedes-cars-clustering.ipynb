{"nbformat_minor": 0, "cells": [{"source": "I used this approach in my solution which is the 11th on the Leaderboard. Here I'm infering 4 components in the target's value distribution and I'm showing how did I identify to which of these components does the particular object belong.\nOf course I should mention this forum thread as the main source of the idea: https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/35382.\nMy solution itself is here: https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/36242", "metadata": {"_execution_state": "idle", "_cell_guid": "1b0b761f-404e-4ca4-9f78-ba00ea2e4d51", "collapsed": false, "_uuid": "f42978276b86e73957bcbe12452d418f4f883e85"}, "execution_count": null, "cell_type": "markdown", "outputs": []}, {"outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "4afa49f4-659c-47a2-8a61-4c38001e3439", "_uuid": "359a6be2f7966898f1ef0a36d0eae1f7514ba618"}, "execution_count": null, "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\ntrain = pd.read_csv('../input/train.csv')\nX_train = train.drop(['y'],axis=1)\ny_train = train['y']\nX_test = pd.read_csv('../input/test.csv')\n\n#\n#   Here we drop columns with zero std\n#\n\nzero_std = X_train.std()[X_train.std()==0].index\nX_train = X_train.drop(zero_std,axis=1)\nX_test = X_test.drop(zero_std,axis=1)", "cell_type": "code"}, {"source": "The four components I've mentioned are clearly observable on the distplot", "metadata": {"_execution_state": "idle", "_cell_guid": "0231afca-407d-4f28-a36e-e8ac591c8654", "collapsed": false, "_uuid": "c1a691b2373f10e0670dcdfa515ba14842e39a68"}, "execution_count": null, "cell_type": "markdown", "outputs": []}, {"source": "sns.distplot(y_train[y_train<170],bins=100,kde=False)", "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "acd3edb4-3295-4003-a030-4bcf0149123c", "collapsed": false, "_uuid": "bb05c8c7bc6bced4fd3b088f97ed33c23060f435"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "So we are trying using our features to figure out where (to what component) does the particular object belongs. \nHere we build a kind of \"cluster encoder\". It takes categorical feature, computes group means and clusters them to four groups.", "metadata": {"_execution_state": "idle", "_cell_guid": "4bebc7ad-ef80-4275-b25e-e0a9b46a34a4", "collapsed": false, "_uuid": "24af68d8f061c81a68aebfd343913b4c42920958"}, "execution_count": null, "cell_type": "markdown", "outputs": []}, {"source": "class cluster_target_encoder:\n    def make_encoding(self,df):\n        self.encoding = df.groupby('X')['y'].mean()\n    def fit(self,X,y):\n        df = pd.DataFrame(columns=['X','y'],index=X.index)\n        df['X'] = X\n        df['y'] = y\n        self.make_encoding(df)\n        clust = KMeans(4,random_state=0)\n        labels = clust.fit_predict(self.encoding[df['X'].values].values.reshape(-1,1))\n        df['labels'] = labels\n        self.clust_encoding = df.groupby('X')['labels'].median()\n    def transform(self,X):\n        res = X.map(self.clust_encoding).astype(float)\n        return res\n    def fit_transform(self,X,y):\n        self.fit(X,y)\n        return self.transform(X)", "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "fdbfa000-0320-4b27-9f46-795418435146", "collapsed": false, "_uuid": "cb0c2a77439a1668a662c34c8ff4584476e2a95a"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Now as mentioned on forum we use X0 to split the components", "metadata": {"_execution_state": "idle", "_cell_guid": "38f92210-fd95-48c9-b95b-dbfd51d0d9ac", "collapsed": false, "_uuid": "b07f4461915565943019b01f161e5e9f87f22f37"}, "execution_count": null, "cell_type": "markdown", "outputs": []}, {"source": "enc1 = cluster_target_encoder()\nlabels_train = enc1.fit_transform(X_train['X0'],train['y'])\nlabels_test = enc1.transform(X_test['X0'])\n%pylab inline\nplt.figure(figsize(10,5))\nplt.hist(y_train.values[labels_train==0],bins=70,label='cluster 0')\nplt.hist(y_train.values[labels_train==1],bins=100,label='cluster 1')\nplt.hist(y_train.values[labels_train==2],bins=70,label='cluster 2')\nplt.hist(y_train.values[labels_train==3],bins=70,label='cluster 3')\nplt.legend()\nplt.title('Train targets distribution for all clusters')\nplt.xlim((60,170))\nplt.show()", "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "b826be21-f086-4cb3-a46d-8cb34caa8950", "collapsed": false, "_uuid": "7f53fd80f30aac3f19fa90e5f2919d118a3235ab"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "Brilliant, isn't it? But we have a problem. We have some values of X0 in test which we don't have in train, so we have some NaNs in labels_test", "metadata": {"_execution_state": "idle", "_cell_guid": "f678a632-52a7-4a2d-9289-7558dc55e826", "collapsed": false, "_uuid": "9abb6d41d12804384f143ac734c06731c889cd7a"}, "execution_count": null, "cell_type": "markdown", "outputs": []}, {"source": "labels_test[np.isnan(labels_test)].shape", "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "9a32c10f-7201-44c3-9a68-69970eee7e4f", "collapsed": false, "_uuid": "a7b621c39002809eda5455721390d82babf08655"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "In fact we can just do nothing. 6 objects is to few to worry about. But instead we can predict these labels using other features. Actually this global four \"car clusters\" are obvious for most of the algorithms and it's really not a problem to predict them. The problem is to predict what's happening inside the cluster (you can read the Discussions thread for more).\nLet's ensure that we can predict the labels well.", "metadata": {"_execution_state": "idle", "_cell_guid": "9f8c5d25-5476-4828-b36a-093d221231d6", "collapsed": false, "_uuid": "abcfceed5975dad0f9ee8632c23dde01952e5168"}, "execution_count": null, "cell_type": "markdown", "outputs": []}, {"source": "cross_val_score(\n    X = X_train.select_dtypes(include=[np.number]),\n    y = labels_train,\n    estimator = xgb.XGBClassifier(),\n    cv = 5,\n    scoring = 'accuracy')", "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "e039f9e1-8a2c-4c16-a866-33417dcdfa92", "collapsed": false, "_uuid": "d2e726910c8863aa3015010ab0b218c2a9cc11d2"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "As you see the accuracy is super. The last thing we have to do is to predict NaNs in labels_test and we have almost perfect split of these four parts of the mixture", "metadata": {"_execution_state": "idle", "_cell_guid": "9085aa60-fdf3-4d5d-bb90-60c29540f4d6", "collapsed": false, "_uuid": "cd56d0e1b5bc3c8c28239c314cbb3568bde4b9aa"}, "execution_count": null, "cell_type": "markdown", "outputs": []}, {"source": "est = xgb.XGBClassifier()\nest.fit(X_train.select_dtypes(include=[np.number]),labels_train)\nlabels_test[np.isnan(labels_test)] = est.predict(\n    X_test.select_dtypes(include=[np.number]))[np.isnan(labels_test)]\nnp.isnan(labels_test).any()", "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "a57678c8-f58a-4623-963d-e17f471d61cc", "collapsed": false, "_uuid": "051e5c98511be9e5d549c8d63d94ff611a86bf3d"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "And the last note here. This feature is not the silver bullet. In fact we did not add much information, machine learning algorithms are capable to understand this structure without our help. Take a look how well does xgboost separate these clusters when predicting the y.", "metadata": {"_execution_state": "idle", "_cell_guid": "8dcdb35c-293a-4d2d-91f5-ad79b14a441c", "collapsed": false, "_uuid": "77093c8592e68a0b85c12d76a966fba6e82565fb"}, "execution_count": null, "cell_type": "markdown", "outputs": []}, {"source": "y_pred = cross_val_predict(\n    X = X_train.select_dtypes(include=[np.number]),\n    y = y_train,\n    estimator = xgb.XGBRegressor(),\n    cv = 5)\nplt.figure(figsize(10,5))\nplt.hist(y_pred[labels_train==0],bins=70,label='cluster 0')\nplt.hist(y_pred[labels_train==1],bins=100,label='cluster 1')\nplt.hist(y_pred[labels_train==2],bins=70,label='cluster 2')\nplt.hist(y_pred[labels_train==3],bins=70,label='cluster 3')\nplt.legend()\nplt.title('Cross_val_predict distribution for all clusters')\nplt.show()", "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "f6123f2b-8a77-42df-be0c-bb353d735c0e", "collapsed": false, "_uuid": "4c3ac990a7739707c5118a1a353f76d4b05de248"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": "But this new feature was really very useful for me, you can check my solution to figure out how.", "metadata": {"_execution_state": "idle", "_cell_guid": "3dedaf4f-43e8-476d-b958-02b1ae886dd4", "collapsed": false, "_uuid": "a139bcb0b7ab33c139cb22cc3ae25cbd571f9b0d"}, "execution_count": null, "cell_type": "markdown", "outputs": []}], "metadata": {"language_info": {"name": "python", "version": "3.6.1", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat": 4}