{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score\nfrom matplotlib import pyplot\nimport xgboost as xgb\nfrom scipy import stats\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"df_application = pd.read_csv('../input/application_train.csv')\ndf_application_test = pd.read_csv('../input/application_test.csv')\ndf_application.head()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7929b41c0c2cb6dd78d0aab8e2c32d936874c059","collapsed":true},"cell_type":"code","source":"df_application['Source'] = 'Train'\ndf_application_test['Source'] = 'Test'    \ndf = pd.concat((df_application,df_application_test),axis = 0,sort = False)\ncat_cols = [col for col in df.columns if (df[col].dtype == object) & (col != 'Source' )]\nle = preprocessing.LabelEncoder()\nfor col in cat_cols:\n    df[col] = le.fit_transform(df[col].fillna(\"Missing\"))\ndf.head()\ndf_train = df[df['Source'] == \"Train\"].drop('Source', axis =1)\ndf_test = df[df['Source'] == \"Test\"].drop('Source', axis =1)\ndel df","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4e753b3a05ef661e415a9d1add2d804b9e0d1c5e","collapsed":true},"cell_type":"code","source":"df_bureau = pd.read_csv(\"../input/bureau.csv\")\ndf_bureau_balance = pd.read_csv(\"../input/bureau_balance.csv\")\n##Create simple feature\ndf_bureau_balance[\"MONTHS_BALANCE\"]= np.abs(df_bureau_balance[\"MONTHS_BALANCE\"])\ndf_bureau_balance[\"Period\"] = np.where((df_bureau_balance[\"MONTHS_BALANCE\"] < 7),\"short\",np.where((df_bureau_balance[\"MONTHS_BALANCE\"] < 13),\"medium\",\"long\"))\n#df_bureau_balance = pd.get_dummies(df_bureau_balance,prefix = \"STATUS\",columns = \"STATUS\",dummy_na = True)\ndf_bureau_balance[\"Period_status\"] = df_bureau_balance[\"Period\"].astype(str) + \"_\" + df_bureau_balance[\"STATUS\"]\ndf_bureau_balance.head(5)\n#df_bureau_balance = pd.get_dummies(df_bureau_balance,prefix = \"Period_status\",columns = \"Period_status\")","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3d62dc58bf1d85d8c10a30e061fcbca6ca253546","collapsed":true},"cell_type":"code","source":"df_bureau_balance = df_bureau_balance.groupby([\"SK_ID_BUREAU\",\"Period_status\"]) \\\n                                     .agg({\"MONTHS_BALANCE\" : [\"count\",\"min\",\"max\",\"mean\"]}) \\\n                                     .reset_index()\ndf_bureau_balance.columns =  [''.join(col).strip() for col in df_bureau_balance.columns.values]\ndf_bureau_balance.head()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6522f7b12cc9a02e6dae5bef01133af0ffcff651","collapsed":true},"cell_type":"code","source":"df_bureau_balance = df_bureau_balance.pivot_table(index = 'SK_ID_BUREAU',columns = 'Period_status',values = ['MONTHS_BALANCEcount','MONTHS_BALANCEmin','MONTHS_BALANCEmax','MONTHS_BALANCEmean']).reset_index()\ndf_bureau_balance.columns =  [''.join(col).strip() for col in df_bureau_balance.columns.values]\ndf_bureau = pd.merge(df_bureau,df_bureau_balance, how=\"left\", on = \"SK_ID_BUREAU\")\ndf_bureau=pd.get_dummies(df_bureau,prefix =['CREDIT_ACTIVE','CREDIT_CURRENCY','CREDIT_TYPE'] ,columns = ['CREDIT_ACTIVE','CREDIT_CURRENCY','CREDIT_TYPE'],dummy_na = True)\ndf_bureau.head(5)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3d8baaffced0b80dd4d71291d0ec51050774ca5","collapsed":true},"cell_type":"code","source":"ohe_cols = df_bureau.columns[14:].tolist()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42fb48133c782d4f24b06681c6a71188e66b06f8","collapsed":true},"cell_type":"code","source":"df_bureau_features = df_bureau.groupby(['SK_ID_CURR']) \\\n                        .agg({'SK_ID_BUREAU' : ['nunique'],\n                              'DAYS_CREDIT'  : ['min','max','mean','std'],\n                              'CREDIT_DAY_OVERDUE' :['min','max','mean','std'],\n                              'DAYS_CREDIT_ENDDATE':['min','max','mean','std'],\n                              'DAYS_ENDDATE_FACT': ['min','max','mean','std'],\n                              'AMT_CREDIT_MAX_OVERDUE' : ['mean','min','max'],\n                              'CNT_CREDIT_PROLONG' : ['mean','min','max'],\n                              'AMT_CREDIT_SUM' : ['min','max','mean','std'],\n                              'AMT_CREDIT_SUM_DEBT' : ['sum','min','max'],\n                              'AMT_CREDIT_SUM_LIMIT': ['sum','min','max'],\n                              'AMT_CREDIT_SUM_OVERDUE':['sum','min','max'],\n                              'DAYS_CREDIT_UPDATE' : ['sum','min','max'],\n                              'AMT_ANNUITY' : ['sum','min','max','mean']\n                             })\ndf_bureau_features = df_bureau_features.reset_index()\ndf_bureau_features.columns =  [''.join(col).strip() for col in df_bureau_features.columns.values]\ndf_bureau_features_ohe = df_bureau[['SK_ID_CURR'] + ohe_cols].groupby(['SK_ID_CURR']).mean().reset_index()\ndf_bureau_features = pd.merge(df_bureau_features,df_bureau_features_ohe,how = 'left',on = 'SK_ID_CURR')\ndel df_bureau_features_ohe\ndf_train = pd.merge(df_train,df_bureau_features,how = 'left',on = 'SK_ID_CURR')\ndf_test = pd.merge(df_test,df_bureau_features,how = 'left',on = 'SK_ID_CURR')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"566ba435a4195f2094f2b0b6f37d390a379e5d0b","scrolled":true,"collapsed":true},"cell_type":"code","source":"###previous applicatin data\ndf_previous_application = pd.read_csv(\"../input/previous_application.csv\")\ncat_cols = [col for col in df_previous_application.columns if (df_previous_application[col].dtype == object) & ((col != 'SK_ID_CURR' ) | (col != 'SK_ID_PREV'))]\ndf_previous_application = pd.get_dummies(df_previous_application,prefix = cat_cols,columns = cat_cols)\ndf_previous_application.head()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9ce4c7160d67e251479825dd453938e9bad5999","collapsed":true},"cell_type":"code","source":"##Create current application ->  credit card balance, installment monthly balancemapping\n##Current Application features\ndf_POS_CASH_balance = pd.read_csv(\"../input/POS_CASH_balance.csv\")\ndf_POS_CASH_balance = pd.get_dummies(df_POS_CASH_balance, columns= [\"NAME_CONTRACT_STATUS\"])\ndf_POS_CASH_balance_current = df_POS_CASH_balance.drop('SK_ID_PREV',axis = 1).groupby('SK_ID_CURR').mean().reset_index()\ndf_POS_CASH_balance_previous = df_POS_CASH_balance.drop('SK_ID_CURR',axis = 1).groupby('SK_ID_PREV').mean().reset_index()\ndel df_POS_CASH_balance\ndf_POS_CASH_balance_current.head()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d299900595100622f7e74d506d49df76668e08c7","collapsed":true},"cell_type":"code","source":"##installment history\ndf_installments_payments = pd.read_csv(\"../input/installments_payments.csv\")\ndf_installments_payments_current = df_installments_payments.drop('SK_ID_PREV',axis= 1).groupby('SK_ID_CURR').mean().reset_index()\ndf_installments_payments_previous = df_installments_payments.drop('SK_ID_CURR',axis= 1).groupby('SK_ID_PREV').mean().reset_index()\ndel df_installments_payments\ndf_installments_payments_current.head()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1cdadb8d025f81a69598822261f47cfd897bc1a","collapsed":true},"cell_type":"code","source":"##Credit card history\ndf_credit_card_balance = pd.read_csv(\"../input/credit_card_balance.csv\")\ndf_credit_card_balance = pd.get_dummies(df_credit_card_balance, columns= ['NAME_CONTRACT_STATUS'])\ndf_credit_card_balance_current = df_credit_card_balance.drop('SK_ID_PREV',axis = 1).groupby('SK_ID_CURR').mean().reset_index()\ndf_credit_card_balance_previous = df_credit_card_balance.drop('SK_ID_CURR',axis = 1).groupby('SK_ID_PREV').mean().reset_index()\ndel df_credit_card_balance\ndf_credit_card_balance_current.head()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69e221ed4e52181139e01f25358f04439fb5a56b","collapsed":true},"cell_type":"code","source":"###Append to train and test sets\ndf_train = df_train.merge(df_POS_CASH_balance_current, on = 'SK_ID_CURR',how = 'left',suffixes=['','_POS_bal_curr']) \\\n                   .merge(df_installments_payments_current, on = 'SK_ID_CURR',how = 'left', suffixes = ['','_installments_curr']) \\\n                   .merge(df_credit_card_balance_current, on = 'SK_ID_CURR',how = 'left',suffixes=['','_credit_card_bal_curr'])\n        \ndf_test =  df_test.merge(df_POS_CASH_balance_current, on = 'SK_ID_CURR',how = 'left',suffixes=['','_POS_bal_curr']) \\\n                  .merge(df_installments_payments_current, on = 'SK_ID_CURR',how = 'left',suffixes = ['','_installments_curr']) \\\n                  .merge(df_credit_card_balance_current, on = 'SK_ID_CURR',how = 'left',suffixes=['','_credit_card_bal_curr']) \\\n\ndf_train.head()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2686e3128fdcad7116a7ca06600c8dbca92ac9e3"},"cell_type":"code","source":"###Leavin out SK_ID_PREV for now. Will use it as a temporal variable later on\ndf_previous_application = df_previous_application.merge(df_POS_CASH_balance_previous, on = 'SK_ID_PREV',how = 'left',suffixes=['','_POS_bal_past']) \\\n                                                 .merge(df_installments_payments_previous, on = 'SK_ID_PREV',how = 'left',suffixes = ['','_installments_past']) \\\n                                                 .merge(df_credit_card_balance_previous, on = 'SK_ID_PREV',how = 'left',suffixes=['','_credit_card_bal_past'])\n        \ndf_previous_application = df_previous_application.drop(\"SK_ID_PREV\",axis= 1).groupby(['SK_ID_CURR']).mean().reset_index()\n\ndf_train = df_train.merge(df_previous_application, on = 'SK_ID_CURR',how = 'left',suffixes= ('','_past_appl'))\ndf_test = df_test.merge(df_previous_application, on = 'SK_ID_CURR',how = 'left',suffixes= ('','_past_appl'))\ndf_previous_application.head()\n\ndel df_previous_application","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a7925d65e731b69d766ef3bb7f878b07c9a7bce","collapsed":true},"cell_type":"code","source":"\n##Boosting trial\ndf1 = df_train.sample(frac = 1)\nmsk = np.random.rand(len(df1))\neval_set = df1[msk >= 0.95]\ntrain = df1[msk < 0.95]\n\n\n# In[101]:\n#train_cols = cat_var+numeric_var\n#Train matrices\nX_train = train.drop(['TARGET','SK_ID_CURR'],axis = 1)\nY_train = train['TARGET']\n#Eval matrices\nX_eval = eval_set.drop(['TARGET','SK_ID_CURR'],axis = 1)\nY_eval = eval_set['TARGET']\n#Y_eval.shape = (len(Y_eval),1)\n\n##Try model , eval_metric = 'accuracy', eval_set = eval_set\n# 1. XGB eval_set = [(X_train, Y_train), (X_eval, Y_eval)]\ndef xgb_(X_train = X_train, Y_train = Y_train,\n         params = {\n                 \"objective\"        :['multi:softmax']\n                 ,\"max_depth\"        :[2]\n                 ,'eta'              :[0.1]\n        },\n        fit_params = {\n                'eval_metric'      :['mlogloss']\n                ,'eval_set'         :[(X_eval,Y_eval)]},\n        X_eval = X_eval, Y_eval = Y_eval):\n\n    \n    ######### Apply xgb\n    #d_train = xgb.DMatrix(X_train , label = Y_train)\n    #d_eval =  xgb.DMatrix(X_eval , label = Y_eval)\n    print(\"# Tuning hyper-parameters for accuracy\" )\n    print()\n    model_xgb = xgb.XGBClassifier()\n   \n    clf = GridSearchCV(model_xgb, param_grid = params,\n                       fit_params = fit_params,cv =3, scoring =\"roc_auc\")\n    clf.fit(X_train, Y_train)\n    print(\"\\nBest parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n    \n    print(\"Detailed classification report:\")\n    print()\n    print(\"The model is trained on the full development set.\")\n    print(\"The scores are computed on the full evaluation set.\")\n    print()\n    y_true, y_pred = Y_eval, clf.predict(X_eval)\n    print(classification_report(y_true, y_pred))\n    print()\n    print(confusion_matrix(y_true, y_pred))\n    print()\n    model_xgb = clf.best_estimator_\n    xgb.plot_importance(model_xgb,max_num_features = 20)\n    \n    #Eval model\n    Y_dev_pred = model_xgb.predict_proba(X_eval)[:,1]\n    score = roc_auc_score(Y_eval,Y_dev_pred)\n    # retrieve performance metrics\n    results = model_xgb.evals_result()\n    metrics = fit_params['eval_metric']\n    epochs = len(results['validation_0'][metrics[0]])\n    x_axis = range(0, epochs)\n    # plot log loss\n    \n    for metric in metrics:\n        fig, ax = pyplot.subplots()\n        ax.plot(x_axis, results['validation_0'][metric], label='Train')\n        ax.plot(x_axis, results['validation_1'][metric], label='Validation/Hold out set')\n        ax.legend()\n        pyplot.ylabel('%s' %(metric) )\n        pyplot.title('XGBoost %s' %(metric))\n        \n        pyplot.show()\n\n    return model_xgb,score,clf,results,model_xgb.feature_importances_\n\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ca36351c38b8da04fd6f399a75d1a4127ee2c33","scrolled":true,"collapsed":true},"cell_type":"code","source":"xgb_params = {\n     'learning_rate'    :[0.02]\n    ,'reg_lambda'       :[16]\n    ,\"max_depth\"        :[9]\n    ,'silent'           :[False]\n    ,'n_estimators'     :[1000]\n    ,'colsample_bytree' :[0.5]\n    ,'nthread'          :[-1]\n    ,'subsample'        :[0.5]\n    ,'objective'        :[\"binary:logistic\"]\n    ,'scale_pos_weight' :[2]\n    #,'tree_method'      :['gpu_hist']\n    #,'min_child_weight' :[10]\n    }\nfit_params = {\n    'eval_metric'       :['auc']\n    ,'eval_set'         :[(X_train,Y_train),(X_eval,Y_eval)]\n    ,'early_stopping_rounds' : 30\n  #  ,'early_stopping_rounds' :[5]\n}\n\nmodel_xgb,score_xgb,xgb_gridsearch,results,feats = xgb_(X_train = X_train,X_eval = X_eval,params = xgb_params,fit_params = fit_params)\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9be75267d7fff0d3fa480b45bd592ecc397abac","collapsed":true},"cell_type":"code","source":"X_test = df_test.drop(['TARGET','SK_ID_CURR'],axis = 1)\nY_test = model_xgb.predict_proba(X_test)[:,1]","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2a007b460fdf8e7868840d620b5795ba335a404","collapsed":true},"cell_type":"code","source":"df_application_test[\"TARGET\"] = Y_test\ndf_submit = df_application_test[[\"SK_ID_CURR\",\"TARGET\"]]\ndf_submit.to_csv('submission_appl_bureau.csv', index=False)","execution_count":18,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}