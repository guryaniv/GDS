{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Exercício PMR 3508 - Costa Rican Household\nNeste exercício, faremos a análise da base adult, gerando um classificador **KNN** o nível de vulnerabilidade ou nível de carência de domicílios/famílias na Costa Rica. Utilizaremos a biblioteca pandas, bem como a sci-kit learn. A **validação cruzada** será utilizada como forma de comparar os resultados entre classificadores KNN que usam diferentes valores de K. \n\n**Autor:** Gabriel Augusto Bianchi Azevedo Ferreira - **NUSP :** 8989404"},{"metadata":{"trusted":true,"_uuid":"2f08d616860b8822bf44dd6cce42905048dfe33a"},"cell_type":"code","source":"#Carregando a base de dados\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"../input/train.csv\",\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89a51e64454e4df22b8d983e66b184fb49a6cc4a"},"cell_type":"code","source":"data.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ad4324ea5bef711859694bf131d8de1796befcb"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(15,5))\nplt.subplot(121)\ndata.Target.value_counts().plot(kind=\"bar\")\nplt.ylabel(\"Número de famílias\")\nplt.xlabel(\"Classes\")\nt=plt.title(\"Distribuição das classes na base de dados\")\nplt.subplot(122)\n(data.Target.value_counts()*100/data.Target.value_counts().sum()).plot(kind=\"bar\")\nplt.ylabel(\"%\")\nplt.xlabel(\"Classes\")\nt=plt.title(\"Distribuição porcentual das classes na base de dados\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee717927632d5bc753e22be43fd3958cc4890ba8"},"cell_type":"markdown","source":"** Como existe uma classe que responde por 62.7% dos dados, esse deveria ser nosso baseline para acurácia **"},{"metadata":{"trusted":true,"_uuid":"12369bf7287a23c312a7c94c52e35e9ac607e599"},"cell_type":"code","source":"ndata = data.dropna()\nndata.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e9f377e47dfb1d22c77ac5d0e55ddda269443fd"},"cell_type":"markdown","source":"** Percebemos que a quantidade de dados baixou de 9557 para 156 após removermos as linhas com Missing Data. Algo não parece bom... Vamos olhar para as colunas com dados faltantes...**"},{"metadata":{"trusted":true,"_uuid":"2a3992a01a9282a07366dd303cac74133e08ae82"},"cell_type":"code","source":"print(\"Colunas que possuem missing data:\")\nprint(data.columns[data.isnull().any()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ac282f157723afea44f73948732994407aae95d"},"cell_type":"code","source":"#Removendo essas colunas\ndropped_data = data.drop(labels=['v2a1', 'v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5e585aea9e3ec6e8d2468f5ccf0b7bbc7a6875e"},"cell_type":"code","source":"dropped_data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"505eab9f4b74b5596c95ebcf5b1bb2759ff56566"},"cell_type":"markdown","source":"** Agora temos 5 features a menos, mas ainda temos 9557 linhas de dados !! ** <br> **Vamos selecionar apenas as colunas de dados numéricos:**"},{"metadata":{"trusted":true,"_uuid":"5977462f5e503e4403aad0bbeb5259c2cdc6881c"},"cell_type":"code","source":"numeric_data = dropped_data.select_dtypes(include=[np.number])\nprint(\"(Linhas, Nro. Features) = \", numeric_data.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71181dfb81813f22377508975c77670169e029be"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nknn = KNeighborsClassifier(n_neighbors=100)\nscores = cross_val_score(knn, numeric_data, numeric_data.Target, cv=5)\nprint(\"O resultado da validacao cruzada foi (media): \", scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d101580acac07b3a8539cb6a0e253d43e2d03f6"},"cell_type":"markdown","source":"** Agora usando apenas os dados numéricos, vamos escolher um K, para o KNN **"},{"metadata":{"trusted":true,"_uuid":"3a10fb3c3c46aaadc9b5256a747c860e4896c9d7"},"cell_type":"code","source":"score_list = []\nk_list = np.arange(1,501,10)\nfor k in k_list:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, numeric_data, numeric_data.Target, cv=5)\n    score_list.append(scores.mean())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c4b2b7244620ceceb71815a57b443a0489be360"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nk_list = np.arange(1,501,10)\nplt.plot(k_list,score_list)\nplt.xlabel(\"num_neighbors\")\nplt.title(\"Score X num_neighbors\")\nprint(\"O valor máximo de score é: \", np.stack(score_list).max(), \" e o valor de K correspondente é :\", k_list[np.argmax(np.stack(score_list))])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27641bf38be62eefea824b4cef5f27a62b74d385"},"cell_type":"markdown","source":"** Agora vamos usar todos os dados, incluindo os não-numéricos **"},{"metadata":{"trusted":true,"_uuid":"b80e0a16780fb3e51603fb75ccc2ae0eec2047e5"},"cell_type":"code","source":"from sklearn import preprocessing\nencoded_data = dropped_data.apply(preprocessing.LabelEncoder().fit_transform)\nknn = KNeighborsClassifier(n_neighbors=131)\nscores = cross_val_score(knn, encoded_data, encoded_data.Target, cv=5)\nprint(\"O resultado da validacao cruzada foi (media): \", scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f42b074d6e6e471f1c5a462f859d19368db81407"},"cell_type":"markdown","source":"** O uso dos dados não-numéricos, à primeira vista não trouxe melhorais. Precisamos escolher um subconjunto das features. Como as features são muitas , vamos utilizar um método do sklearn para escolher as K melhores colunas, mais representativas**"},{"metadata":{"trusted":true,"_uuid":"08ab5da7885220d50936657394d7121255436d9b"},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nsel = SelectKBest(k=50)\nselected = sel.fit_transform(encoded_data.loc[:,encoded_data.columns != \"Target\"], encoded_data.Target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5220822e3ac61156af5120f1c7b7cac4af17b5f5"},"cell_type":"markdown","source":"** Selecionamos assim, as 50 features mais significativas para o problema. Utilizaremos um K=131, como obtido anteriormente **"},{"metadata":{"trusted":true,"_uuid":"176f67d3b1f285840d9a7d151c8a1d9c91385321"},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=131)\nscores = cross_val_score(knn, selected, encoded_data.Target, cv=5)\nprint(\"O resultado da validacao cruzada foi (media): \", scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5792f5bd52883b43c8a87d6d6df1d541bf9eb1f4"},"cell_type":"markdown","source":"**Nesse caso, o resultado foi de 65% de acurácia, utilizando-se o o classificador KNN. Essa acurácia está, portanto superior ao baseline de 62.7% **"},{"metadata":{"trusted":true,"_uuid":"a5c2f44d7a2fe57a3be71b4d6cac2eaab9da93fb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}