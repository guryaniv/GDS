{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport gensim\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/embeddings/GoogleNews-vectors-negative300/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path = \"../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\"\nembeddings = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n## Collection of all these word vectorings is embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a88c5a8df3ddd5c42389199a66ce7d578591614"},"cell_type":"code","source":"len(embeddings['modi'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96fde44e1420fbcb9194479b9a7b677ca18cf80a"},"cell_type":"code","source":"embeddings.most_similar('rahul', topn=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4e93ea5d37058ed402a2e873aa91f2928a1ed65"},"cell_type":"code","source":"embeddings.most_similar('hyundai', topn=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61a5b19d1fe1d46a163fdbb51487bae211aeac7e"},"cell_type":"code","source":"embeddings.doesnt_match(['football','basketball','cricket','apple'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6172fad11759d1a05e5207104a91075134873dd7"},"cell_type":"code","source":"url = 'https://bit.ly/2S2yXEd'\nimdb = pd.read_csv(url)\nimdb['review'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2910802d5590d49fe0b73178de5165cc34fce1a9"},"cell_type":"code","source":"import nltk\ndoc = imdb.loc[0, 'review']\nwords = nltk.word_tokenize(doc.lower())\ntemp =pd.DataFrame()\nfor word in words:\n    try:\n        print(word, embeddings[word][:5])\n        temp = temp.append(pd.Series(embeddings[word]), ignore_index = True)\n    except:\n        print(word, 'is not there')\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"147f7322f157c8f075bb3f867b619e17d242f4e9"},"cell_type":"code","source":"docs = imdb['review'].str.replace('-',' ').str.lower().str.replace('[^a-z ]', '')\nstopwords = nltk.corpus.stopwords.words('english')\nclean_sentence = lambda doc: ' '.join([word for word in nltk.word_tokenize(doc) if word not in stopwords])\ndocs_clean = docs.apply(clean_sentence)\ndocs_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffe041e77a84160c54f5e25a3e7770b3a7551d0b"},"cell_type":"code","source":"docs_vectors = pd.DataFrame()\nfor doc in docs_clean:\n    words = nltk.word_tokenize(doc)\n    temp = pd.DataFrame()\n    for word in words:\n        try:\n            word_vec = embeddings[word]\n            temp = temp.append(pd.Series(word_vec), ignore_index=True)\n        except:\n            pass\n    docs_vectors = docs_vectors.append(temp.mean(), ignore_index = True)\ndocs_vectors.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e88d1cd0781ef9c6c0d6c12db7772f7536c6a490"},"cell_type":"code","source":"imdb.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1145ec0596ddb524cb2f2307b973a7aeae7e752"},"cell_type":"code","source":"imdb.loc[64, 'review']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4bff68f61bdbdfbc84ca80126a68f47b6aa0c9b"},"cell_type":"code","source":"imdb.loc[590, 'review']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e73bc0cc0436ff1ad87595446d3d63dc05ff765"},"cell_type":"code","source":"X = docs_vectors.drop([64, 590])\ny = imdb['sentiment'].drop([64, 590])\nfrom sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y = train_test_split(X,y, test_size = 0.2, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5a07529159019d47d829521587bce4c52afba4d"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = RandomForestClassifier(n_estimators=800).fit(train_x, train_y)\ntest_pred = model.predict(test_x)\naccuracy_score(test_y, test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d460c1763b069554ed4d2d03ebec581a75b0676"},"cell_type":"code","source":"model = AdaBoostClassifier(n_estimators=800).fit(train_x, train_y)\ntest_pred = model.predict(test_x)\naccuracy_score(test_y, test_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}