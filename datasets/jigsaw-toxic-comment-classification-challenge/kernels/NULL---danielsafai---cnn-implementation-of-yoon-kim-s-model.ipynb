{"metadata": {"language_info": {"version": "3.6.4", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat": 4, "cells": [{"metadata": {"_uuid": "7b486ed1856a061e75f27c240c22886c565efa63", "_cell_guid": "d2b7f058-8cf5-4599-b099-f58a17b27f91"}, "source": ["## Keras implementation of Yoon Kim's model for sentence classification"], "cell_type": "markdown"}, {"metadata": {"_uuid": "71345317674baa7f0ab5e029cb2a07df9262fe6b", "_cell_guid": "c2b62e7b-a49b-4501-b303-ef7328fa208d"}, "source": ["##### The following is a keras implementation of Yoon Kim's convolutional neural network model for sentence classification from the paper: https://arxiv.org/abs/1408.5882"], "cell_type": "markdown"}, {"metadata": {"_uuid": "673c19d49285841534961478fec2dce4d1394cfd", "_cell_guid": "ff691dcb-4d7e-481d-8a63-244571012e9c"}, "source": ["##### The basic idea is to use a convolutional neural network where different convolutions are used to produce different n-gram-like filters to determine the sentiment of a given text. Using the \"glove\" pre-trained embeddings and manually setting different kernels sizes, the hope is to capture strong single phrases in sentences that results in a particular outcome."], "cell_type": "markdown"}, {"metadata": {"_uuid": "5d4c457e9946e88f79fe3efd29cf601ebe9987db", "collapsed": true, "_cell_guid": "44eb8bb2-979b-4dc9-8f36-0c6a7a74823a", "scrolled": true}, "source": ["import pandas as pd\n", "import re\n", "import numpy as np\n", "from keras.preprocessing import sequence\n", "from keras.regularizers import l2\n", "from keras.models import Model\n", "from keras.layers.merge import concatenate\n", "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n", "from keras.layers import Dense, GlobalMaxPooling1D, Activation, Dropout, GaussianNoise\n", "from keras.layers import Embedding, Input, BatchNormalization, SpatialDropout1D, Conv1D\n", "from keras.optimizers import Adam\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "from pandas_summary import DataFrameSummary \n", "from IPython.display import display\n", "import itertools\n", "from nltk.corpus import words\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "7394571847983f631cf44419e010532f1e7a525c", "_cell_guid": "a37ed914-3787-4bbe-847c-3ba7a3c93b39"}, "source": ["##### Determine dimension of embedding vector, max size of vocabulary and max length of sentence (crop the rest)"], "cell_type": "markdown"}, {"metadata": {"_uuid": "c21b046383b3dcdcdd8319a3ba4f51896a5ac385", "collapsed": true, "_cell_guid": "76e9a099-026b-4632-b0ff-29329df05499"}, "source": ["# Set parameters\n", "embed_size   = 50    # how big is each word vector\n", "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n", "maxlen       = 100   # max number of words in a comment to use "], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "0ccdb2895aa8e190f9f427290912282f6cb6baea", "_cell_guid": "cbda94d6-ebdf-46fa-824d-22c23dbbaf36"}, "source": ["##### Load data..."], "cell_type": "markdown"}, {"metadata": {"_uuid": "dca42dbb8cad2faa1b4527dedaa031c8939aa523", "_cell_guid": "f66337db-af9a-4604-b675-f8c229084e85"}, "source": ["# Load data\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "\n", "list_sentences_train = train[\"comment_text\"].fillna(\"_NaN_\").values\n", "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n", "y = train[list_classes].values\n", "list_sentences_test = test[\"comment_text\"].fillna(\"_NaN_\").values"], "outputs": [], "cell_type": "code", "execution_count": 1}, {"metadata": {"_uuid": "214b0c3803ce23163787c559bf8c103271cacc9b", "_cell_guid": "f7e84b2b-0b6b-4d26-96de-93714744992b"}, "source": ["##### Tokenize sentences, convert to integers and pad sentences < 100 words"], "cell_type": "markdown"}, {"metadata": {"_uuid": "06ec3d6800e124b935178c6b981cfc5e27a3c7ad", "collapsed": true, "_cell_guid": "09b529ed-38f0-44db-a97d-ca725d1c1f4f"}, "source": ["# Pad sentences and convert to integers\n", "tokenizer = Tokenizer(num_words=max_features)\n", "tokenizer.fit_on_texts(list(list_sentences_train))\n", "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n", "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n", "\n", "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post')\n", "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post')"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "699edb3012216da4bf9754b706da58fc4fd10912", "_cell_guid": "901cbce3-4e2d-48a5-80de-48366ced45a4"}, "source": ["##### Load \"glove\" pre-trained embeddings and construct vocabulary dictionary"], "cell_type": "markdown"}, {"metadata": {"_uuid": "0ba1b991235f978f3648532f2a4a93af2ec0bfd3", "collapsed": true, "_cell_guid": "8a8fccae-6501-432f-8db4-2ae4e7847c40"}, "source": ["# Read the glove word vectors (space delimited strings) into a dictionary from word->vector\n", "def get_coefs(word,*arr): \n", "    return word, np.asarray(arr, dtype='float32')\n", "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('./data/glove.6B.50d.txt'))"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "febf2cd76cf3a6a6895a331c8e900f9df382bbb2", "_cell_guid": "ce1655d7-b5dd-4f48-a3ef-e4b02bb5159f"}, "source": ["##### Create embedding matrix and initialize space for new words not present in \"glove\""], "cell_type": "markdown"}, {"metadata": {"_uuid": "8d96bbe653615bca5b5e0239e7a0adc337b69085", "collapsed": true, "_cell_guid": "c4c3a7b3-6ad3-4cc3-acc4-8aeb18b30856"}, "source": ["# Create embeddings matrix\n", "all_embs = np.stack(embeddings_index.values())\n", "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n", "\n", "# Create embedding matrix using our vocabulary\n", "word_index = tokenizer.word_index\n", "nb_words = min(max_features, len(word_index))\n", "\n", "# Initialize embedding matrix\n", "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n", "\n", "# Loop through each word and get its embedding vector\n", "for word, i in word_index.items():\n", "    if i >= max_features: \n", "        continue # Skip words appearing less than the minimum allowed\n", "    embedding_vector = embeddings_index.get(word)\n", "    if embedding_vector is not None: \n", "        embedding_matrix[i] = embedding_vector"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "e9999f863832b0790840cdc5cc1296eca67c41bd", "_cell_guid": "71d0d9ae-0024-418d-92aa-8d91f10c78e3"}, "source": ["##### Set no. of convolution filters and weigh the outcome variable in order to balance.\n", "- 128 filters are used for each convolution. I.e. with a kernel size of 3, 128 tri grams are constructed each representing a specific feature. With a kernel size of 4, 128 4-grams are constructed and so on.."], "cell_type": "markdown"}, {"metadata": {"_uuid": "53ed470e95c215e0ecc5be9df9f66402fb08726b", "collapsed": true, "_cell_guid": "38d9b786-e556-4e92-ba9c-7b47bab9bc58"}, "source": ["# Initialize parameters\n", "conv_filters = 128 # No. filters to use for each convolution\n", "weight_vec = list(np.max(np.sum(y, axis=0))/np.sum(y, axis=0))\n", "class_weight = {i: weight_vec[i] for i in range(6)}"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "570facca18c7ca89ef615c47ce562811b620c431", "_cell_guid": "910d419d-7767-40fd-af9f-085e7e687b12"}, "source": ["##### Construct Convolutional Neural Network"], "cell_type": "markdown"}, {"metadata": {"_uuid": "bc4b69a2db4bf1e4262d6aec4e7671d9353f084c", "collapsed": true, "_cell_guid": "eaf137d0-229e-4130-a938-e30b7c8f8f66"}, "source": ["inp = Input(shape=(X_train.shape[1],), dtype='int64')\n", "emb = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n", "\n", "# Specify each convolution layer and their kernel siz i.e. n-grams \n", "conv1_1 = Conv1D(filters=conv_filters, kernel_size=3)(emb)\n", "btch1_1 = BatchNormalization()(conv1_1)\n", "drp1_1  = Dropout(0.2)(btch1_1)\n", "actv1_1 = Activation('relu')(drp1_1)\n", "glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n", "\n", "conv1_2 = Conv1D(filters=conv_filters, kernel_size=4)(emb)\n", "btch1_2 = BatchNormalization()(conv1_2)\n", "drp1_2  = Dropout(0.2)(btch1_2)\n", "actv1_2 = Activation('relu')(drp1_2)\n", "glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n", "\n", "conv1_3 = Conv1D(filters=conv_filters, kernel_size=5)(emb)\n", "btch1_3 = BatchNormalization()(conv1_3)\n", "drp1_3  = Dropout(0.2)(btch1_3)\n", "actv1_3 = Activation('relu')(drp1_3)\n", "glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n", "\n", "conv1_4 = Conv1D(filters=conv_filters, kernel_size=6)(emb)\n", "btch1_4 = BatchNormalization()(conv1_4)\n", "drp1_4  = Dropout(0.2)(btch1_4)\n", "actv1_4 = Activation('relu')(drp1_4)\n", "glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n", "\n", "# Gather all convolution layers\n", "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n", "drp1 = Dropout(0.2)(cnct)\n", "\n", "dns1  = Dense(32, activation='relu')(drp1)\n", "btch1 = BatchNormalization()(dns1)\n", "drp2  = Dropout(0.2)(btch1)\n", "\n", "out = Dense(y.shape[1], activation='sigmoid')(drp2)"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "1bfa4501e43e8b0784b7bafcac9e2d47a5043d9c", "collapsed": true, "_cell_guid": "dd176424-156f-4e7e-8160-b3cae7a1a342", "scrolled": true}, "source": ["# Compile\n", "model = Model(inputs=inp, outputs=out)\n", "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n", "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "7d8a9ad0b0b2b5aafb6d697a78380a53e3efe77e", "collapsed": true, "_cell_guid": "18960cdb-ab02-4052-8f05-0c794047c8c6", "scrolled": true}, "source": ["# Estimate model\n", "model.fit(X_train, y, validation_split=0.1, epochs=2, batch_size=32, shuffle=True, class_weight=class_weight)"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "f744a0429fd2706808016b917f2194ac5bbe6f84", "_cell_guid": "6e1d85ed-c594-4134-a5e8-00f046dbb1ab"}, "source": ["##### Predict and finally submit"], "cell_type": "markdown"}, {"metadata": {"_uuid": "a445e8f05353f391ab56ab5f50248e6417751190", "collapsed": true, "_cell_guid": "82c412ed-e24d-46bf-9912-99c255eb1ca9"}, "source": ["# Predict\n", "preds = model.predict(X_test)"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "b6e0af2175dccc76623c108305592ac209780e51", "collapsed": true, "_cell_guid": "628ae9f9-5dee-4b74-bdcd-b1e2a16c2fbb"}, "source": ["# Create submission\n", "submid = pd.DataFrame({'id': test[\"id\"]})\n", "submission = pd.concat([submid, pd.DataFrame(preds, columns = list_classes)], axis=1)\n", "submission.to_csv('conv_glove_simple_sub.csv', index=False)"], "outputs": [], "cell_type": "code", "execution_count": null}], "nbformat_minor": 1}