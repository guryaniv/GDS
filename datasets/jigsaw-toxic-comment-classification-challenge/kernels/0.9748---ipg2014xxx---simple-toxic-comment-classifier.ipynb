{"cells":[{"metadata":{"_uuid":"c89a281ffa048f167c3407d8ac1af965d66a2714","_cell_guid":"9561611d-7907-4501-a587-a0536ec98c1d"},"cell_type":"markdown","source":"# Getting Started\nFirst off, we import the necessary packages and set the random seed in order to get consistent results"},{"metadata":{"_uuid":"4198b79f00882af3c1de3470a67f3ae8e2585200","_cell_guid":"95dd1390-8d11-4852-bfec-cbf060b4214c","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib notebook\n\nnp.random.seed(19)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"5cb822a542916ee1dce17c0e86657f725b3f13f4","_cell_guid":"a24acbe5-c1eb-460a-86f2-36dce29c9147","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(\"Training samples = \", train.shape[0])\nprint(\"Test samples = \", test.shape[0])","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"ebe3b7dc8b4bdce8353b474d36886ca7b78ee3d2","_cell_guid":"6138ebc3-e824-47c9-8297-92c679aeda55","trusted":true},"cell_type":"code","source":"train_rows =train.shape[0]\nlabels = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] \ntrain.drop(['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\ntest_id = test.pop('id')\ntrain.head()","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"35e151afd596f8d9d00e0060dc94af1928e5b4e2","_cell_guid":"d3359498-7766-433b-a028-930d2980a467"},"cell_type":"markdown","source":"# Check for the null values\n\nNow we check for the presences of null values in the training and testing data. Then we'll combine both using pd.concat"},{"metadata":{"_uuid":"12ede281d2613003381ee8753622c6dfd04194be","_cell_guid":"82f4da1b-8461-49f8-ab03-ff42719c4031","trusted":true},"cell_type":"code","source":"print(\"Null values in training data\", train.isnull().sum(), sep=\"\\n\")\nprint(\"Null values in testing data\", test.isnull().sum(), sep=\"\\n\")","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"d8937f9ab46c99cfdbfa318c0b987866be11515d","_cell_guid":"1d29fc81-0dc0-485b-aac1-81a942e8b6f0","trusted":true},"cell_type":"code","source":"data = pd.concat([train, test])\ndel train\ndel test\ndata.shape","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"21b9af5e491c26189de73378e6a8d8aef20aa63f","_cell_guid":"62e2c2ef-56c3-4b4d-955d-6cf2ac8db514"},"cell_type":"markdown","source":"# Data Preprocessing\n\nWe can see that the comments contain various special characters as well as escape sequences (e.g '\\n'). <br>\nHere we are removing the escape sequences, digits and the commonly used words (.i.e. stop words) using regular expressions.  We are interested in keeping the special characters as they are used in emoticons or some other expressions."},{"metadata":{"_uuid":"4bc35fbb93b6e4f788a78c120a987210f92b7693","_cell_guid":"da65e5aa-fe23-4ac8-a623-0822b190f576","collapsed":true,"trusted":true},"cell_type":"code","source":"import re\nimport nltk\n\nstop_words = set(nltk.corpus.stopwords.words('english'))\n\ndef preprocess_input(comment):\n# remove the extra spaces at the end.\n    comment = comment.strip()\n# lowercase to avoid difference between 'hate', 'HaTe'\n    comment = comment.lower()\n# remove the escape sequences. \n    comment = re.sub('[\\s0-9]',' ', comment)\n# Use nltk's word tokenizer to split the sentence into words. It is better than the 'split' method.\n    words = nltk.word_tokenize(comment)\n# removing the commonly used words.\n    #words = [word for word in words if not word in stop_words and len(word) > 2]\n    words = [word for word in words if len(word) > 2]\n    comment = ' '.join(words)\n    return comment","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"b366343f8c12fd20673624df3c8a494befe39dae","_cell_guid":"54ac822d-ebf3-4f30-8b7a-a217a542a0c3","trusted":true},"cell_type":"code","source":"print(\"SAMPLE PREPROCESSING\")\nprint(\"\\nOriginal comment: \", data.comment_text.iloc[0], sep='\\n')\nprint(\"\\nProcessed comment: \", preprocess_input(data.comment_text.iloc[0]), sep='\\n')","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"a58ca429-e3c3-4ab5-9eee-f235cf608f1f","_uuid":"b247a42d12585ce0d3b234030fc7b2dd388d2c4d"},"cell_type":"markdown","source":"Applying the preprocessing on whole input data. This will take sometime as the dataset is large."},{"metadata":{"_uuid":"f6c9976c4ce1ef161d87c2abbd56fc5484b32fa3","_cell_guid":"32de7e8f-72fb-4abf-8a5a-754526595580","trusted":true},"cell_type":"code","source":"data.comment_text = data.comment_text.apply(lambda row: preprocess_input(row))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"688548c442868a0c928d45d2688f3cfbd6b98208","_cell_guid":"b1012591-d16b-43e3-b87e-454da453d5bb","trusted":true},"cell_type":"code","source":"data.head()","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"a8a5b385-bcb6-49fc-8078-6b6703d885a1","_uuid":"f8b6c0c8e3fa3c6ccbbfff5aa6a07208bf9e7653"},"cell_type":"markdown","source":"# Feature Extraction\n\nWe use Tfidf Vectorizer to convert the collection of raw documents to a matrix of TF-IDF features. Here the feature will be made of character n-grams rather than word. Because the comments contain various characters which may not be defined in ASCII so we will use Unicode."},{"metadata":{"_uuid":"5d39d86c0a79f879134e100ced3ba98f0061750f","_cell_guid":"0c2f39c5-3bbf-4b65-ab55-56d27799a081","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer(min_df=0.1, max_df=0.7, \n                       analyzer='char',\n                       ngram_range=(1, 3),\n                       strip_accents='unicode',\n                       sublinear_tf=True,\n                       max_features=30000\n                      )","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9db54d5cc0b08a7dd037fc518f196ae793d9c7e2"},"cell_type":"code","source":"test = data[train_rows:]\ntrain = data[:train_rows]\ndel data","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"a640aeebfba836465f9a1a54248e58de6b820aeb","_cell_guid":"cfc6fe76-1e39-4bd8-8e3c-e4c4aee013f9","collapsed":true,"trusted":true},"cell_type":"code","source":"vect = vect.fit(train.comment_text)\ntrain = vect.transform(train.comment_text)\ntest = vect.transform(test.comment_text)","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"8a0d938f78d1612990f15203ed20a592df62fc46","_cell_guid":"df9ae18a-064a-41d4-8556-e6685174c1ac","trusted":true},"cell_type":"code","source":"print('Training feature set = ', train.shape)\nprint('Testing feature set = ', test.shape)","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"8d0760af-60e9-4025-83f4-170d3953162a","_uuid":"adf7809f538662c6a835496bc1769f98f2215607"},"cell_type":"markdown","source":"# Applying machine learning\n\nWe have to predict the probability associated with each label.  I am using Logistic Regression for simplicity."},{"metadata":{"_uuid":"c5989564052abd8e84b091edad0a024127728398","_cell_guid":"3c832f33-abbf-408e-8a48-55e396dc81e3","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\ncols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ny_pred = pd.read_csv('../input/sample_submission.csv')\n\nfor c in cols:\n    clf = LogisticRegression(C=4, solver='sag')\n    clf.fit(train, labels[c])\n    y_pred[c] = clf.predict_proba(test)[:,1]\n    score = np.mean(cross_val_score(clf, train, labels[c], scoring='roc_auc', cv=5))\n    print(\"ROC_AUC score for\", c, \"=\",  score)","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"6625be4463dbe7367426768275aad45a32369276","_cell_guid":"6f47d4b6-f2c3-4687-ad37-c76330148ebc","trusted":true},"cell_type":"code","source":"y_pred.head()","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"8a543002d5b54866c8f7344be9cf60f46709d6d9","_cell_guid":"ef203c28-e8f5-4855-b5a1-148670ca966a","collapsed":true,"trusted":true},"cell_type":"code","source":"y_pred.to_csv('my_submission.csv', index=False)","execution_count":32,"outputs":[]}],"metadata":{"anaconda-cloud":{},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}