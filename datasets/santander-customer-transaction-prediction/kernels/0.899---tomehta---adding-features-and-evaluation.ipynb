{"cells":[{"metadata":{"_uuid":"045c17c9b0212a76c13c84015f83b7ff69663926"},"cell_type":"markdown","source":"I am trying to analyze data thru diff views to get a clue which features may be impacting target. My intenetion is to keep things simple and easily comprehendable. I myself get lost sometimes in good kernels which are bit low on structure part.  I have tried to keep it structured and scalable for new features and models. "},{"metadata":{"_uuid":"a1ac7e184fb43ff589e96361b88e2eeeaef12a08"},"cell_type":"markdown","source":"-  [Import and Read](#LibLink)\n-  [Basic EDA](#EDALink)\n-  [Functions](#FuncLink)\n-  [Plotting](#PlotLink)\n-  [Corr and Bin](#CorLink)\n-  [Features](#FeatLink)\n-  [Model](#ModLink)"},{"metadata":{"_uuid":"20dc019a068ecac80acbf9e73a6b7db23351a7f3"},"cell_type":"markdown","source":"<div id=\"LibLink\">\n**Import libraries**\n</div>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import rankdata\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport itertools\nfrom sklearn import metrics\nfrom scipy.stats import norm, rankdata\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\npd.set_option('display.max_columns', 200)\n# below is to have multiple outputs from same Jupyter cells\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nwarnings.filterwarnings('ignore')\nfrom feature_selector import FeatureSelector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ee018ed8963c8f89080775233e577c00097bb66"},"cell_type":"markdown","source":"Read files "},{"metadata":{"trusted":true,"_uuid":"c31a7a77a67dc8fc1a31ecded7604644eba9c191","_kg_hide-input":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67253860f039b7077471613e09ad8b9365b5e0ed"},"cell_type":"code","source":"train_df = train_df.sample(n = 20000, random_state = 42)\ntest_df = test_df.sample(n = 20000, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53d8c7a7be2b176e9a3d787bd033d01ad3365153"},"cell_type":"markdown","source":"<div id=\"EDALink\">\n **Basic EDA**\n </div>"},{"metadata":{"_uuid":"e5b9dbbf245c752967f3c54e71cd7445cc1d693e"},"cell_type":"markdown","source":"Number of rows and columns in Dataset"},{"metadata":{"trusted":true,"_uuid":"cb4b634debc40b073a0111b82e5c98384b059b32","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(\"Train Shape\\n\")\nprint(\"*\"*80)\ntrain_df.shape\nprint(\"*\"*80)\nprint(\"Test Shape\\n\")\nprint(\"*\"*80)\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48ddf52efdf7079e7152df8d1d6b92cf1396c3ab"},"cell_type":"markdown","source":"Basic statistics for datasets"},{"metadata":{"trusted":true,"_uuid":"9a4ce748d907ff3dc4c95c3edfc42545038a6b73","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(\"Train Describe\\n\")\ntrain_df.describe() \nprint(\"Test Describe\\n\")\ntest_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83090337f70e68708e47bee88a3872b628fce67d"},"cell_type":"markdown","source":"Distribution of target in training dataset. This shows its a imbalance data set, with 90% of data being 0 and 10% as 1."},{"metadata":{"trusted":true,"_uuid":"b7c82e3e72b2be025166f0419ce802e19819b9e5","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df[\"target\"].value_counts()/train_df.shape[0]*100\nfig,ax= plt.subplots()\nsns.countplot(data=train_df,x=\"target\",ax=ax)\nax.set(xlabel=\"Target\",\n       ylabel=\"Count\", \n       Title = \"Target Distribution\"\n       )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6c5c1d194bca8fd8ae5f74dad231ce59157d3c2"},"cell_type":"markdown","source":"Looking at output of describe for both df, data seems to be similar in both the datasets (test and train).  Another point is test is of same size as train. we need to find a way to extract some info from test data."},{"metadata":{"_uuid":"e21fbc5df4c5f168562906c60d878ff26292e6a1"},"cell_type":"markdown","source":"### Missing values"},{"metadata":{"_uuid":"fc56f602f4ff55a327dcbb4af51ae6052f3096e4"},"cell_type":"markdown","source":"None of dataset has any missing values. "},{"metadata":{"trusted":true,"_uuid":"b9c2610042de4c2e31e689003ca2917dd3fd62e3","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df.isnull().sum().sum()\ntest_df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c856c7871e285eb6c473b6b7784e28100a51aa7"},"cell_type":"markdown","source":"<div id=\"FuncLink\">\n** Utility Functions for EDA and Feature Engineering **\n    </div>"},{"metadata":{"_uuid":"3e27ccbe3369a3f40960fa3d468d0413df2c2dac"},"cell_type":"markdown","source":"To reduce memory footprint"},{"metadata":{"trusted":true,"_uuid":"86d7c05f2da55920c9603c711854ca698f30935a","_kg_hide-input":true},"cell_type":"code","source":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()/2.0\n            c_max = df[col].max()/2.0\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c64b5059a1512621727341669618b0647a8f4a63"},"cell_type":"markdown","source":"To plot distributions features of two datasets  **plot_feature_distribution**"},{"metadata":{"trusted":true,"_uuid":"b35042d04d478d00964be01da3d5a63e20bfdc9e","_kg_hide-input":true},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad43773a0231bd5562529ad89c08624bbbc0f110"},"cell_type":"markdown","source":"To plot boxplot of features of two datasets, along with class split  **plot_feature_boxplot**"},{"metadata":{"trusted":true,"_uuid":"f560708977ce27991e066efff9a6d10835335ddc","_kg_hide-input":true},"cell_type":"code","source":"def plot_feature_boxplot(df1,df2,label1,label2,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(100,2,figsize=(10,180))\n\n    for feature in features:\n        i += 1\n        plt.subplot(100,2,i)\n        sns.boxplot(y=df1[feature], x=target, showfliers=False)\n        plt.title(feature+'_train', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n        i += 1\n        plt.subplot(100,2,i)\n        sns.boxplot(df2[feature],orient='v',color='r')\n        plt.title(feature+'_test', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n\n        #locs, labels = plt.xticks()\n        #plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        #plt.tick_params(axis='y', which='major', labelsize=6)\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc348fe279443864f77f9e8cfcfc5f8e5ce58895"},"cell_type":"markdown","source":"To plot violinplot of features of two datasets, along with class split  **plot_feature_violinplot**"},{"metadata":{"trusted":true,"_uuid":"ff1eef6b1cb7df1ea2ab7e2f7011ebff11cc90ef","_kg_hide-input":true},"cell_type":"code","source":"def plot_feature_violinplot(df1,df2,label1,label2,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(100,2,figsize=(10,180))\n\n    for feature in features:\n        i += 1\n        plt.subplot(100,2,i)\n        sns.violinplot(y=df1[feature], x=target, showfliers=False)\n        plt.title(feature+'_train', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n        i += 1\n        plt.subplot(100,2,i)\n        sns.violinplot(df2[feature],orient='v',color='r')\n        plt.title(feature+'_test', fontsize=10)\n        plt.ylabel('')\n        plt.xlabel('')\n\n        #locs, labels = plt.xticks()\n        #plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        #plt.tick_params(axis='y', which='major', labelsize=6)\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51ce65e4db9056894eb80316cf18a473e718408a"},"cell_type":"markdown","source":"To plot violinplot of binned features for training along with class split  **plot_binned_feature_target_violinplot**"},{"metadata":{"trusted":true,"_uuid":"8eb06c6dfe19a3831ed18366a3b025305796dcdf","_kg_hide-input":true},"cell_type":"code","source":"def plot_binned_feature_target_violinplot(df,features,target):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(200,1,figsize=(8,380))\n\n    for feature in features:\n        bins = np.nanpercentile(df[feature], range(0,101,10))\n        df[feature+\"_binned\"] = pd.cut(df[feature],bins=bins)\n        i += 1\n        plt.subplot(200,1,i)\n        sns.violinplot(y=df[feature+\"_binned\"], x=target, showfliers=False)\n        plt.title(feature+'_binned & Target', fontsize=12)\n        plt.ylabel('')\n        plt.xlabel('')\n       \n        locs, labels = plt.xticks()\n        plt.xticks([0.0,1.0])\n        plt.tick_params(axis='y', which='major', labelsize=8)\n        #ax.set_xticks([0.15, 0.68, 0.97])\n        #plt.gca().axes.get_xaxis().set_visible(False)\n        #plt.gca().axes.get_yaxis().set_visible(False)\n    plt.tight_layout()  \n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64bf6241ffe7401d3f42b713a06560f5ed16a0c5"},"cell_type":"markdown","source":"To add new features row wise  **add_new_feature_row**"},{"metadata":{"trusted":true,"_uuid":"9edf44dad8b57bbe0f8cba463a46494ca43d7f5c","_kg_hide-input":true},"cell_type":"code","source":"def add_new_feature_row(df,features):\n    for feature in features:\n        df[feature+\"_pct\"] = df[feature].pct_change()\n        df[feature+\"_diff\"] = df[feature].diff()\n        df.drop(feature,axis=1)\n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acfc7046d668f5837b9580242596ddb6bc1f6a52"},"cell_type":"markdown","source":"To normailize features using combined dataset **add_feature_df**"},{"metadata":{"trusted":true,"_uuid":"e9fd60e00e06680009907a79a1b81b68ee673632","_kg_hide-input":true},"cell_type":"code","source":"def add_feature_df(df,features):\n    # count +ve and -ve\n    df['count+'] = np.array(df>0).sum(axis=1)\n    df['count-'] = np.array(df<0).sum(axis=1)\n    #sum\n    #df['sum_outside'] = df.sum(axis=1)\n        \n    for feature in features:\n        #normalize\n        #df[feature+'_norm'] = (df[feature] - df[feature].mean())/df[feature].std()\n        #percentage change row wise\n        #df[feature+\"_pct\"] = df[feature].pct_change() # didnt give boost\n        #diff change row wise\n        #df[feature+\"_diff\"] = df[feature].diff() # didnt give boost\n        # Square\n        #df[feature+'^2'] = df[feature] * df[feature]\n        # Cube\n        #df[feature+'^3'] = df[feature] * df[feature] * df[feature]\n        # 4th power\n        #df[feature+'^4'] = df[feature] * df[feature] * df[feature] * df[feature]\n        # Cumulative percentile (not normalized)\n        #df[feature+'_cp'] = rankdata(df[feature]).astype('float32')\n        # Cumulative normal percentile, probabilites\n        #df[feature+'_cnp'] = norm.cdf(df[feature]).astype('float32')\n        # sqrt\n        #df[feature+'_sqrt'] = np.sqrt(df[feature])\n        #binning\n        #bins = np.nanpercentile(df[feature], range(0,101,10))\n        #df[feature+\"_binned\"] = pd.cut(df[feature],bins=bins)\n        #rounding\n        #df[feature+'_r2'] = np.round(df[feature], 2)\n        #rounding\n        #df['r1_'+feature] = np.round(df[feature], 1)\n        #exp\n        #df['exp_'+feature] = np.exp(df[feature])\n        #exp and feature\n        #df['xintoexp_'+feature] = np.exp(df[feature])*df[feature]\n        #sum\n        #df['sum_inside'] = df[[feature]].sum(axis=1)\n        #max\n        #df['max'] = df[[feature]].max(axis=1)\n        #min\n        #df['min'] = df[[feature]].min(axis=1)\n        #max\n        #df['std'] = df[[feature]].std(axis=1)\n        #skew\n        #df['skew'] = df[[feature]].skew(axis=1)\n        #kurt\n        #df['kurt'] = df[[feature]].kurtosis(axis=1)\n        #median\n        #df['med'] = df[[feature]].median(axis=1)\n        #tanh\n        df['tanh_'+feature] = np.tanh(df[feature])\n        \n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d34774824dc25f89aa8e14ae79ff12de9e3e3ee9","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0f97c79fafccbc795d71f394e9ab73dcdc27364"},"cell_type":"markdown","source":"<div id = FeatLink>\n** Feature Engineering **\n    </div>"},{"metadata":{"trusted":true,"_uuid":"7d657777097d7b7c2a5d128bcb8e5bde25becf77","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"test_df['target']= np.nan\ncombine_df = train_df.append(test_df,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dab867b443e9b811bb170ecdecc59c530f69fd1e"},"cell_type":"code","source":"#features = train_df.columns[~train_df.columns.isin(['target','ID_code'])]\n#combine_df = add_feature_df(combine_df,features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc150e9ed25451af0a029d411d37a10db66f11a"},"cell_type":"code","source":"#combine_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f70b83874df6001fa8d3e43ea7dfdc7900f411b8"},"cell_type":"markdown","source":"Separate out train and test. Append new features created to training dataset."},{"metadata":{"trusted":true,"_uuid":"70e5e4e99e3f9c3e0d5c3fd6f4d47921cfbfc7b4","_kg_hide-input":true},"cell_type":"code","source":"train_df = combine_df[combine_df['target'].notnull()].reset_index(drop=True)\ntest_df = combine_df[combine_df['target'].isnull()].reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dee9a846251d16e30ece27c6ee041ceb84452b85"},"cell_type":"code","source":"# features will have added cols defined as part of feature engineering\nfeatures = train_df.columns[~train_df.columns.isin(['target','ID_code'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ace6071354a6adf5711ff05a3b84203f8daec87"},"cell_type":"code","source":"# Features are in train and labels are in train_labels\nfs = FeatureSelector(data = train_df[features], labels = train_df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86ccc9782c8e77240c8cb7825060c5055cf6400f"},"cell_type":"code","source":"fs.identify_all(selection_params = {'missing_threshold': 0.6,    \n                                    'correlation_threshold': 0.95, \n                                    'task': 'classification',    \n                                    'eval_metric': 'auc', \n                                    'cumulative_importance': 0.95})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e2ab9df01d4ef07423dbb7e83945eb28be2139a"},"cell_type":"code","source":"collinear_features = fs.ops['collinear']\nfs.record_collinear.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b424ec2281d71c902ca35a771f1a83bf53d1c3ff"},"cell_type":"code","source":"fs.feature_importances.head(200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6259e1820eaa47b6d30c1c075692f38b66a1d401"},"cell_type":"code","source":"# plot the feature importances\nfs.plot_feature_importances(threshold = 0.99, plot_n = 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9655d5d945ff967316672442868f6fb9ff72de7"},"cell_type":"code","source":"# list of zero importance features\nzero_importance_features = fs.ops['zero_importance']\nzero_importance_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff532dbf6fc14b31c24cff3df986252f8b9384d7"},"cell_type":"code","source":"train_removed_all = fs.remove(methods = 'all', \n                                          keep_one_hot=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e07b95305c752b4d05f8f6bd1b68d299c017e5b"},"cell_type":"code","source":"fs.ops # stats for which all are removed by what method.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c603193250a0ebf1c05df3b6838f45f2c7fc4b46"},"cell_type":"code","source":"train_removed_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ddc5c2163b4c2f9d4e9cf86d0a49e1d4fcb7cba"},"cell_type":"code","source":"feat_sel=train_removed_all.columns.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6d466e34d297f62d3dc71d22e49f79740ce7664"},"cell_type":"code","source":"feat_sel\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95ee1696c955a4926e55cc3f9a6db7457ba597d4"},"cell_type":"code","source":"with open (\"feat_sel.csv\",\"w\")as fp:\n   for line in feat_sel:\n       fp.write(str(line)+\"\\n\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcfec46a2841a8977d79d2684612686780fa368e"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ntest_df['target']= np.nan\ncombine_df = train_df.append(test_df,ignore_index=True)\nfeatures = train_df.columns[~train_df.columns.isin(['target','ID_code'])]\ncombine_df = add_feature_df(combine_df,features)\ntrain_df = combine_df[combine_df['target'].notnull()].reset_index(drop=True)\ntest_df = combine_df[combine_df['target'].isnull()].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f78b966da8865bb2fa0c54c3a7f87671d15743fb"},"cell_type":"markdown","source":"<div id = ModLink>\n** Modeling **\n    </div>"},{"metadata":{"_uuid":"b1cfdf207e434c21a34727c39adb0714cb614bfa"},"cell_type":"markdown","source":"This is model lifted and shifted from [Fayaz's](https://www.kaggle.com/fayzur/lightgbm-customer-transaction-prediction) kernel."},{"metadata":{"trusted":true,"_uuid":"19087a7d6d9c3f1d85fcb01f3ec1736b5b6902aa","_kg_hide-input":true},"cell_type":"code","source":"#test_df = test_df.drop(\"target\",axis=1)\npredictors = train_removed_all.columns\nnfold = 5\ntarget = 'target'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"118e7f6d0b3247ce06fd3c371ddd4c2cafb709e6","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"'''param = {\n     'num_leaves': 18,\n     'max_bin': 63,\n     'min_data_in_leaf': 5,\n     'learning_rate': 0.010614430970330217,\n     'min_sum_hessian_in_leaf': 0.0093586657313989123,\n     'feature_fraction': 0.056701788569420042,\n     'lambda_l1': 0.060222413158420585,\n     'lambda_l2': 4.6580550589317573,\n     'min_gain_to_split': 0.29588543202055562,\n     'max_depth': 49,\n     'save_binary': True,\n     'seed': 1337,\n     'feature_fraction_seed': 1337,\n     'bagging_seed': 1337,\n     'drop_seed': 1337,\n     'data_random_seed': 1337,\n     'objective': 'binary',\n     'boosting_type': 'gbdt',\n     'verbose': 1,\n     'metric': 'auc',\n     'is_unbalance': True,\n     'boost_from_average': False\n}\n'''\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}\n\nnfold = 10\n\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name='auto',\n                           categorical_feature = 'auto',\n                           free_raw_data = False\n                           )\n    print(\"after lgb train\")\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name='auto',\n                           categorical_feature = 'auto',\n                           free_raw_data = False\n                           )   \n    print(\"after lgb test\")\n    nround = 1000000\n    clf = lgb.train(param, \n                    xg_train, \n                    nround, \n                    valid_sets = [xg_train,xg_valid], \n                    early_stopping_rounds=3000,\n                    verbose_eval=1000)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=nround) \n    print(\"after lgb fit\")\n    predictions += clf.predict(test_df[predictors], num_iteration=nround) / nfold\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = predictors\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] =  i = i + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n   \n\n\nprint(\"\\n\\nCV AUC: {:<0.4f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f571b0b795d4399afff15f79dcc4333f6cc44212"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f278c7024610bf96fd6e8481d0ad7fbbefcace3"},"cell_type":"markdown","source":"Feature Importance as per model"},{"metadata":{"trusted":true,"_uuid":"25ff5a766870570c636c7f050fa691e9827e12a9"},"cell_type":"code","source":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28b221e14ad4c4550815f34c4ed7dc9f4552aed7"},"cell_type":"code","source":" # Get feature importances\nimp_df = pd.DataFrame()\nimp_df[\"feature\"] = predictors\nimp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\nimp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\nimp_df['trn_score'] = roc_auc_score(train_df['target'], clf.predict(train_df.loc[:,predictors]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16df1df83061cadbae47d1660e57df5c3dd1d09f"},"cell_type":"code","source":"imp_df.sort_values(by=\"importance_gain\",ascending=False).to_csv(\"imp_lgb.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8d92649aec55cdc32727b33401415b85b249f1e"},"cell_type":"code","source":"imp_df.sort_values(by=\"importance_gain\",ascending=False)[:150]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"206bc988ebf49ece7fcc0e6b3731316c94d46cef"},"cell_type":"markdown","source":"Submission file"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"ab96b4788012a836de70b91402fc95cc03cc94f7"},"cell_type":"code","source":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"sant_lgb.csv\", index=False)\nsub_df[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c521e8afe487dc1c9c061db92064c17b7d34a030"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f11b1d72c6e587a0b9c87fc4676e4281e0ec5c48"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84bca05e6b9193993898cbcb7b627ff7d8b8be4a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7657a65cfe0970cdec440a423a79712653001e3a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e243cf51b59b67cda1968b2ac0553eaca68db5cf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}