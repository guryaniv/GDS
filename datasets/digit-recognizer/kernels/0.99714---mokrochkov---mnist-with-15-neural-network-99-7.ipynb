{"cells":[{"metadata":{},"cell_type":"markdown","source":"A single properly trained convolutional neural network is extremely good at classifying hand written digits. \nI have achieved accuracy of 99.5% with the network that I used below. However going beyond 99.5% requires multiple networks and even then you are gaining .2% or so accuracy at the expensve of n times longer training.\nThis particular kernal will use 15 networks and take the ensemble of the highest confidence prediction from each.\n\nThis is my first real attempt at using Pytorch, so a bit of the code comes from pytorch tutorial and some other parts can surely be done better as well."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# usual imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\n# torch and torchvision in order to build the neural network\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as Func\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Right away we define a neural network architecture. A simple neural network here with 1 or 2 hidden layers will easily achieve results in the high 90% range, but we are interested in getting as close to 100% as possible, so we are using multiple convolutions layers with batch normalization followed with multiple fully connected layers with batch normalization and dropout. This network is somewhat similar in structure and was inspired by VGG-16/19 networks.\nThe training is done on the GPU which is easily done with PyTorch in a few easy lines.\nCross entropy loss and Adam optimizer with default parameters is used."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# define neural network architecture\n# based on VGG style neural network\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        \n        # N x N x 1 --> N x N x 64\n        self.conv_1_64 = nn.Conv2d(1, 64, 3, padding=1)\n        \n        # N x N x 64 --> N x N x 64\n        self.conv_64_64 = nn.Conv2d(64,64,3,padding = 1) \n        \n        # N x N x 64 --> N x N x 128\n        self.conv_64_128 = nn.Conv2d(64, 128, 3, padding = 1) \n        \n        # N x N x 128 --> N x N x 128\n        self.conv_128_128 = nn.Conv2d(128,128,3,padding = 1)\n        \n        # N in case of MNIST is 7, so we go from 7 x 7 x 128 to a 1024, with further layers of 512, 256, 128 and finally 10\n        self.fc1 = nn.Linear(128 * 7 * 7, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc4 = nn.Linear(256, 128)\n        self.fc5 = nn.Linear(128, 10)\n        \n        # initiate dropout layer with a given probability\n        self.dropout = nn.Dropout(p=0.5);\n        \n        # initiate batch normalizations\n        self.batch_norm_64 = nn.BatchNorm2d(64)\n        self.batch_norm_128_2d = nn.BatchNorm2d(128)\n        self.batch_norm_128_1d = nn.BatchNorm1d(128)\n        self.batch_norm_256 = nn.BatchNorm1d(256)\n        self.batch_norm_512 = nn.BatchNorm1d(512)\n        self.batch_norm_1024 = nn.BatchNorm1d(1024)\n\n    def forward(self, x):\n        # input --> CNN 1, final dimensions 28 x 28 x 64\n        x = F.relu(self.conv_1_64(x))\n        x = self.batch_norm_64(x)\n        \n        # CNN 1 --> CNN 2 --> CNN 3, final dimensions 28 x 28 x 64\n        x = self.batch_norm_64(F.relu(self.conv_64_64(x)))\n        x = self.batch_norm_64(F.relu(self.conv_64_64(x)))\n        \n        # max pool, final dimensions 14 x 14 x 64\n        x = F.max_pool2d(x, (2, 2))\n        \n        # CNN 3 --> CNN 4 --> CNN 5 --> CNN 6, final dimensions 14 x 14 x 128\n        x = self.batch_norm_128_2d(F.relu(self.conv_64_128(x)))\n        x = self.batch_norm_128_2d(F.relu(self.conv_128_128(x)))\n        x = self.batch_norm_128_2d(F.relu(self.conv_128_128(x)))\n        \n        # max pool, final dimensions 7 x 7 x 128\n        x = F.max_pool2d(x, (2, 2))\n        \n        # convert to a vector\n        x = x.view(-1, self.num_flat_features(x))\n        \n        # fully connected layers\n        x = self.dropout(self.batch_norm_1024(F.relu(self.fc1(x))))\n        x = self.dropout(self.batch_norm_512(F.relu(self.fc2(x))))\n        x = self.dropout(self.batch_norm_256(F.relu(self.fc3(x))))\n        x = self.dropout(self.batch_norm_128_1d(F.relu(self.fc4(x))))\n        x = self.fc5(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n\nnetCNN = Net()\n\n# select device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(netCNN.parameters(), lr=0.001)\nnetCNN.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading data in with pytorch is a bit of a mess without the dataloader. Chances are I probably don't know how to do it properly yet, so I'm doing it in a way that works.\n\nHere I also define the hyperparameters: number of networks, batch size, number of epochs.\nAfter reading in the data we need to make sure that it is normalized to between 0 and 1, hence the division by 255."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_networks = 15\nbatchsize = 500\nnum_epochs = 100\n\nimg_rows = 28\nimg_cols = 28\n# for convolutional neural network\nx_train = pd.read_csv('../input/train.csv')\nx_test = pd.read_csv('../input/test.csv')\ny_train = x_train['label'].values\nx_train = x_train.drop(['label'],1).values\nx_test = x_test.values\n\nx_train = x_train.reshape(np.shape(x_train)[0],1,img_rows,img_cols)\nx_test = x_test.reshape(np.shape(x_test)[0],1,img_rows,img_cols)\n\nx_train = x_train.astype('float32')/255\nx_test = x_test.astype('float32')/255\n\ntrain = torch.from_numpy(x_train)\ntest = torch.from_numpy(x_test)\ntrain_label = torch.from_numpy(y_train)\ntrain_label = train_label.long()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At each epoch we apply a random augmentation to each image: 0-10 degree rotation, 0%-20% up and down translation and 90% to 110% scaling\n\nAfter each network has finished training we immediately use it to predict the labels for the test set and store the values in a 28000x10x15 (15 being the number of neworks) array. At the end of training for the final network we will pick the best prediction in order to form the final 28000 entry output."},{"metadata":{"trusted":true},"cell_type":"code","source":"affine = transforms.RandomAffine(degrees=10, translate=(.20,.20), scale=(.9,1.1))\noutput_all = torch.empty(test.size(0),10,num_networks)\nnum_iterations_train = int(train.size(0)/batchsize)\nnum_iterations_test = int(test.size(0)/batchsize)\n\ntime_start = time.time()\n\nfor n_network in range(num_networks):\n    train_temp = torch.empty(np.shape(x_train)[0],1,img_rows,img_cols)\n    netCNN = Net()\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(netCNN.parameters(), lr=0.001)\n    netCNN.to(device)\n    for epoch in range(num_epochs):\n        # it seems that these have to be done one at a time\n        for n_input, input_data in enumerate(train):\n            train_pil = transforms.ToPILImage(mode=None)(train[n_input])\n            train_temp[n_input,:,:,:] = Func.to_tensor(affine(train_pil))\n\n        running_loss = 0.0\n        for i in range(num_iterations_train):\n\n            inputs = train_temp[i*batchsize:(i+1)*batchsize]\n            labels = train_label[i*batchsize:(i+1)*batchsize]\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = netCNN(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n        #print('Epoch %d loss: %.3f' % (epoch + 1, running_loss/batchsize*1000))\n        running_loss = 0.0\n\n    print('Finished Training Network %i' % (n_network+1))\n\n    prediction_all = torch.empty(train.size(0),1)\n    for i in range(num_iterations_train):\n        test_input = train[i*batchsize:(i+1)*batchsize]\n        output = netCNN(test_input.to(device))\n        _, prediction = torch.max(output,1)\n        prediction_all[i*batchsize:(i+1)*batchsize,0] = prediction.cpu()\n\n    match = train_label.int().reshape(train.size(0),1) == prediction_all.int()\n    fraction = int(match.sum())/int(prediction_all.shape[0])\n    print(fraction)\n    \n    #prediction_all = torch.empty(test.size(0),1)\n    for i in range(num_iterations_test):\n        test_input = test[i*batchsize:(i+1)*batchsize]\n        output = netCNN(test_input.to(device))\n        output_all[i*batchsize:(i+1)*batchsize,:,n_network] = output.data\n        del output\n        del test_input\n        torch.cuda.empty_cache()\n\na,_ = torch.max(output_all,2)\n_,predicted_final = torch.max(a,1)\n\nruntime = time.time() - time_start\nprint('Elapsed Time: %.3f seconds' % runtime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally write and submit our prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"output_file = 'submission.csv'\npred_final_numpy = predicted_final.numpy()\nwith open(output_file, 'w') as f :\n    f.write('ImageId,Label\\n')\n    for i in range(len(predicted_final)) :\n        f.write(\"\".join([str(i+1),',',str(pred_final_numpy[i]),'\\n']))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}