{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport gc\nimport numpy as np\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold, RepeatedKFold, GroupKFold\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import ADASYN\nimport category_encoders as ce\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da5389e9b074699fbe332541dc7cd896592e861a"},"cell_type":"markdown","source":"## Challenge\n* The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?\n\n* Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify."},{"metadata":{"_uuid":"96bee1bcb1120c05574a0cd1eff76875e41e70e0"},"cell_type":"markdown","source":"## Road Map of Clean Data / Feature Engineering / Encoding / Remove Null Value\n1.  Load Data.....\n1.  Clean features...\n1.  Extracting features...\n1.  Encoding Data....\n1.  Fill NA value.....\n1.  Prepared Model.....\n1.  Predict Value on test Dataset......\n1.  Submit your result ......."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"def dprint(*args, **kwargs):\n    print(\"[{}] \".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")) + \\\n        \" \".join(map(str,args)), **kwargs)\n\nid_name = 'Id'\ntarget_name = 'Target'\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3c7b697fabc249fb1b80db62758dcaad3b09615","collapsed":true},"cell_type":"code","source":"# Load data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abdb900b639f0bf5350cfd1bbd220b8448241a6c","collapsed":true},"cell_type":"code","source":"train['is_test'] = 0\ntest['is_test'] = 1\ndf_all = pd.concat([train, test], axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"268e840f90ff2eae91262abffc69207066bcb01a"},"cell_type":"code","source":"dprint('Clean features...')\ncols = ['dependency']\nfor c in tqdm(cols):\n    x = df_all[c].values\n    strs = []\n    for i, v in enumerate(x):\n        try:\n            val = float(v)\n        except:\n            strs.append(v)\n            val = np.nan\n        x[i] = val\n    strs = np.unique(strs)\n\n    for s in strs:\n        df_all[c + '_' + s] = df_all[c].apply(lambda x: 1 if x == s else 0)\n\n    df_all[c] = x\n    df_all[c] = df_all[c].astype(float)\ndprint(\"Done.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c013c1840d2126c11411a01394ab92b68e69765"},"cell_type":"code","source":"dprint(\"Extracting features...\")\ndef extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n    df['rent_to_bedrooms'] = df['v2a1']/df['bedrooms']\n    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n    df['tamhog_to_bedrooms'] = df['tamhog']/df['bedrooms']\n    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n    df['r4t3_to_bedrooms'] = df['r4t3']/df['bedrooms']\n    df['rent_to_r4t3'] = df['v2a1']/df['r4t3']\n    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1'])\n    df['hhsize_to_rooms'] = df['hhsize']/df['rooms']\n    df['hhsize_to_bedrooms'] = df['hhsize']/df['bedrooms']\n    df['rent_to_hhsize'] = df['v2a1']/df['hhsize']\n    df['qmobilephone_to_r4t3'] = df['qmobilephone']/df['r4t3']\n    df['qmobilephone_to_v18q1'] = df['qmobilephone']/df['v18q1']\n    \n\nextract_features(train)\nextract_features(test)\ndprint(\"Done.\")         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ce14b2913f725f6cd6e1a33fe3d3064d6cfecdc9"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n   \n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ace74699d67c4ddb2a369436d55edff82578eb81"},"cell_type":"code","source":"dprint(\"Encoding Data....\")\nencode_data(train)\nencode_data(test)\ndprint(\"Done...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"998e033149ff68cae360d05ed3838cc745d954e1","collapsed":true},"cell_type":"code","source":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n    # do something advanced above...\n    \n    # Drop SQB variables, as they are just squres of other vars \n    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id', 'idhogar'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdac22f68e9c37ee48fda8546c1c312e80f8e78c"},"cell_type":"code","source":"dprint(\"Do_feature Engineering....\")\ntrain = do_features(train)\ntest = do_features(test)\ndprint(\"Done....\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"240428d3240eadbf1bdd532274cd3163cdb23798"},"cell_type":"code","source":"dprint(\"Fill Na value....\")\ntrain = train.fillna(0)\ntest = test.fillna(0)\ndprint(\"Done....\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"371391febda7b17f921b7c73a0c1a49d311636d4"},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7048f296ca5aa3a617a130e76ce25c72de2609bf","collapsed":true},"cell_type":"code","source":"cols_to_drop = [\n    id_name, \n    target_name,\n]\nX = train.drop(cols_to_drop, axis=1, errors='ignore')\ny = train[target_name].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e35b0b18947dd64de17635f53d7d37f816f19a9","collapsed":true},"cell_type":"code","source":"y = pd.get_dummies(y).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"010bba56172de98eaaf8cdd94fc35b0ccfacfae9"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed5e04e2617a95957f9313c97bfbb2ac62d3d8b1"},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import Adam\n\nfrom keras import regularizers\ninput_dim = len(X_train.columns) \ninput_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bea499a1a006a964000490453a75038dddfe8dc"},"cell_type":"code","source":"input_dim = len(X_train.columns) \nbatch_size=32\nmodel = Sequential()\n\n# first input layer with first hidden layer in a single statement\nmodel.add( Dense(120, input_shape=(input_dim,), activation='tanh') )\n# 10 is the size(no. of neurons) of first hidden layer, 4 is the no. of features in the input layer\n# input_shape=(4,)  can also be written as   input_dim=4\n\n# second hiden layer\nmodel.add(Dense(64,activation='relu')) # 8 = no. of neurons in second hidden layer\n\n# third hiden layer\nmodel.add(Dense(32,activation='relu')) # 6 = no. of neurons in third hidden layer\n\n# ouput layer\nmodel.add(Dense(4,activation='softmax')) # 3 = no. of neurons in output layer as three categories of labels are there\n\n# compile method receives three arguments: \"an optimizer\", \"a loss function\" and \"a list of metrics\"\nmodel.compile(Adam(lr=0.02),'mse', ['accuracy'])\n# we use \"binary_crossentropy\" for binary classification problems and\n# \"categorical_crossentropy\" for multiclass classification problems\n# the compile statement can also be written as:-\n# model.compile(optimizer=Adam(lr=0.04), loss='categorical_crossentropy',metrics=['accuracy'])\n# we can give more than one metrics like ['accuracy', 'mae', 'mape']\n\nmodel.summary()\nmodel.fit(X_train, y_train, steps_per_epoch=850 ,epochs = 10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"335bd89a2d4c5175834b4f3bf4f62d8cbc00e9d5"},"cell_type":"code","source":"scores = model.evaluate(X_test, y_test)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e931d17632a1224d184dcabf7a803b9b4dc618ae","collapsed":true},"cell_type":"code","source":"predictions = model.predict_classes(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b1e0b3daf28fd415e7a3da010dcff880f1ccfc4"},"cell_type":"code","source":"list(set(predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e69b52288067587f818729d5e0faf6a329630885"},"cell_type":"code","source":"sub = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f91042d5e8f98039384ce27c9cf829ce4b7ad016"},"cell_type":"code","source":"sub['Target'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0eebcaa05b723e2e6a435923ad7b322856bf12cc"},"cell_type":"code","source":"sub.to_csv(\"keras.csv\", index= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"695ec059281d97ddf3703136788b501328a4cd57","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}