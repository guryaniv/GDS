{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# import Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\n% matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6589d664a793304aadbb196011d5ad99c314ac4d"},"cell_type":"code","source":"# Utilities from kaggle kernels\n# Instead of data = pd.read_csv(\"../input/train_V2.csv\")\n# We use : data = read_fast(\"../input/train_V2.csv\")\nimport random\nimport time\n\ndef reduce_mem_usage_func(df):\n    \"\"\" Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n        iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef get_sampled_data(filename, sample_size):\n    n = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)\n    skip = sorted(random.sample(range(1,n+1),n-sample_size)) #the 0-indexed header will not be included in the skip list\n    df = pd.read_csv(filename, skiprows=skip)\n    return df\n\n\ndef read_fast(filename, sample=True, sample_size=2500000, reduce_mem_usage=True):\n    start_time = time.time()\n    df = get_sampled_data(filename, sample_size) if sample else pd.read_csv(filename)\n    new_df = reduce_mem_usage_func(df) if reduce_mem_usage else df\n    elapsed_time = int(time.time() - start_time)\n    print('Time to get data frame: {:02d}:{:02d}:{:02d}'.format(\n               elapsed_time // 3600,\n               (elapsed_time % 3600 // 60),\n               elapsed_time % 60))\n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67ee14274dafe4876b1f5297fe3ff56a831b0aa1"},"cell_type":"code","source":"train = read_fast(\"../input/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d42584d6b6af785305688cbf056e1600c75010a"},"cell_type":"code","source":"test = read_fast(\"../input/test.csv\", sample = False)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"064c08a799edec4811d6dfb22cfcb3e3693c0633"},"cell_type":"code","source":"# check index of dataframe\ntrain.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28321142648f2b97b723863e62fe6c7bf6d5ab66"},"cell_type":"code","source":"train = train.dropna(thresh=0.70*len(train), axis=1)\n\ntest = test.dropna(thresh=0.70*len(test), axis=1)\ntrain = train.drop(['SMode'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39b261d1504008ba5ab0b689cc7b2841f1fe3806"},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dec99c5b3e30083af50dc3683f58309718b7ac3f"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = ('EngineVersion', 'AppVersion','AvSigVersion','Census_OSVersion')\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train[c].values)) \n    train[c] = lbl.transform(list(train[c].values))\n    lbl.fit(list(test[c].values)) \n    test[c] = lbl.transform(list(test[c].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5419a0ea7a9643b6719d55dd246272013f5caae"},"cell_type":"code","source":"train = train.select_dtypes(include=[np.number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69adc65ad0f7e6b749076b3ddae2ae57c29b4604"},"cell_type":"code","source":"train = train.fillna(train.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f978fb7c1b945edd74af7943faf367b778c2745a"},"cell_type":"code","source":"y = train[\"HasDetections\"]\n\nX = train.drop(labels = [\"HasDetections\"],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a9626eb2ba920bb52bc4c533c5f37e76c4533c3","scrolled":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\n\ndef identify_zero_importance_features(X, y, iterations = 1):\n    \"\"\"\n    Identify zero importance features in a training dataset based on the \n    feature importances from a gradient boosting model. \n    \n    Parameters\n    --------\n    train : dataframe\n        Training features\n        \n    train_labels : np.array\n        Labels for training data\n        \n    iterations : integer, default = 2\n        Number of cross validation splits to use for determining feature importances\n    \"\"\"\n    \n    # Initialize an empty array to hold feature importances\n    feature_importances = np.zeros(X.shape[1])\n\n    # Create the model with several hyperparameters\n\n    model = LGBMClassifier(objective = 'binary',\n                           num_leaves=60,\n                        learning_rate=0.1,\n                        n_estimators=700,\n                        max_depth =  -1,\n                      boosting = 'gbdt',\n                              bagging_fraction=0.8,\n                              bagging_freq=1, \n                              feature_fraction=0.8,\n                              bagging_seed=11,\n                           metric = 'auc',\n                             lambda_l1 = 0.1,\n                         random_state = 133,\n                         verbosity =  -1)\n    # Fit the model multiple times to avoid overfitting\n    for i in range(iterations):\n\n        # Split into training and validation set\n        train_features, valid_features, train_y, valid_y = train_test_split(X, y, \n                                                                            test_size = 0.25, \n                                                                            random_state = i)\n\n        # Train using early stopping\n        model.fit(train_features, train_y, early_stopping_rounds=100, \n                  eval_set = [(valid_features, valid_y)])\n\n        # Record the feature importances\n        feature_importances += model.feature_importances_ / iterations\n    \n    feature_importances = pd.DataFrame({'feature': list(X.columns), \n                            'importance': feature_importances}).sort_values('importance', \n                                                                            ascending = False)\n    \n    # Find the features with zero importance\n    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n    \n    return zero_features, feature_importances\nzero_features, feature_importances = identify_zero_importance_features(X, y, iterations = 1)\nprint('zero_features:',zero_features)\nprint('feature_importances : ', feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c7b5307e2a5f90423870878263c8a0d424d909b"},"cell_type":"code","source":"feature_importances.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9ba3af55bae44811519aec3700addbe04613d19"},"cell_type":"code","source":"pp =np.percentile(feature_importances['importance'], 20) \nprint(pp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8b970ff2bbbb4e62c05f5f9e10af12425942d99"},"cell_type":"code","source":"to_drop = feature_importances[feature_importances['importance'] <= pp]['feature']\nX = X.drop(columns = to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ff82f95301eaf7091d8548ba2b2c337dd824f50"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80c690ebd072098fbf9f44f3112ba7a7f9e64682"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\n    # Cross validate model with Kfold stratified cross val\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\nkfold = StratifiedKFold(n_splits=2, shuffle=False, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8e8cb3ae2ece1273b19d74e198c0c9aaaf35508"},"cell_type":"code","source":"#lgbm \nimport lightgbm as lgb\nlbm = lgb.LGBMClassifier()\n\n\n## Search grid for optimal parameters\nlbm_param_grid = {'num_leaves': [2000],\n         'min_data_in_leaf': [50],\n         'objective': ['binary'],\n         'max_depth': [-1],\n         'learning_rate': [0.05],\n         \"boosting\": [\"gbdt\"],\n         \"feature_fraction\": [0.8],\n         \"bagging_freq\": [5],\n         \"bagging_fraction\": [0.8],\n         \"bagging_seed\": [11],\n         \"lambda_l1\": [0.1],\n         \"lambda_l2\": [0.1],\n         \"random_state\": [42],          \n         \"verbosity\": [-1]}\ngsExtC = GridSearchCV(lbm,param_grid = lbm_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4)\n\ngsExtC.fit(X_train,y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b2fcafdb0fd38f0dc6b8d565587efbfead4c843"},"cell_type":"code","source":"test_id = test['MachineIdentifier']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4eb2714b29e2509e6d36005d3fcee23b0abab80a"},"cell_type":"code","source":"feats = test.drop(['MachineIdentifier'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5715e3d9066af1825810c0c00b84d44b90851511"},"cell_type":"code","source":"feats = feats.select_dtypes(include=[np.number])\n\nfeats = feats[X_train.columns]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c79fe1a1536d6585512cbcf8a12d6e95bc791b0e"},"cell_type":"code","source":"feats = feats.fillna(feats.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"286406a200298f0d58771ac511be1ea6eb47c356"},"cell_type":"code","source":"predictions = gsExtC.predict(feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c42df1b542ae6e57537524f3fa3e84b48131a16a"},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['MachineIdentifier'] = test_id\nsubmission['HasDetections'] = predictions \nsubmission.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01b29b1e2aa25b6182fe759a28c6e29173da3b04"},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}