{"cells": [{"source": ["# Spatial Autocorrelation In Zillow Home Price Residuals\n", "\n", "This kernel checks for spatial autocorrelation in the home price residuals for the Zillow dataset. It does so by computing the \"gamma index\" and  checking for its statistical significance. For an overvidew of the gamma index (and other spatial autocorrelation concepts), check out this brief:\n", "\n", "http://www.dpi.inpe.br/gilberto/tutorials/software/geoda/tutorials/w9_spauto3_slides.pdf\n", "\n", "Evidence of spatial autocorrelation can provide guidance for better predicting the residuals for the competition. Strong spatial autocorrelation means that homes that are close together tend to have similar residuals. That would then suggest that we should look for \"localized\" factors when trying to predict those residuals."], "metadata": {"_cell_guid": "f283bc2a-bf4b-4827-abff-696fc779e27a", "_uuid": "2cf6df0b9e69fa88d5cadf4e333e64f1dd664343"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "6e17f8e7-272f-37e7-4e03-9ef3b2695f3e", "_uuid": "71000d7c77fac73510e133e9be14ab0a4e8a2d1d"}, "execution_count": null, "cell_type": "code", "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"]}, {"outputs": [], "metadata": {"_cell_guid": "65f852d8-1d7a-35df-eff8-092f64231f97", "_uuid": "350138328c8309d47c6b42f5a3d83a62e20a9392", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "color = sns.color_palette()\n", "\n", "%matplotlib inline"]}, {"source": ["## Define a map projection\n", "In the Zillow dataset, parcel locations are given as geographic (longitude, latitude) coordinates. That's great, but if we want to compute distances between parcels (which we do), it's a lot easier to work in projected (x, y) coordinates. \n", "\n", "We can use `pyproj` to define a function that remaps from (lon, lat) to (x, y) according to a specified map projection. The string below defines a Transverse Mercator projection with a standard meridian at 118.5 degrees west longitude, which crosses the Los Angeles area. So our projected coordinates will have a minimal amount of spatial distortion. \n", "\n", "Projected coordinates will be in meters."], "metadata": {"_cell_guid": "aa476b47-ebf2-43fe-b063-d2e01957b93e", "_uuid": "dc376ccb69b719639dc9ec9215ca4b20b58c7ba1"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "72a0bd2e-6b99-40ca-81a8-b6e11aa729d6", "_uuid": "78f188c1fbcab974c82878245cdebdcf1302a81f", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["import pyproj\n", "center_lon = -118.5  # A reasonable value for the Los Angeles area.\n", "ps0 = '+proj=tmerc +lat_0=0.0 +lon_0=%.1f +y_0=0 +x_00 +k_0=0.9996 +units=m +ellps=WGS84' % center_lon\n", "remap = pyproj.Proj(ps0)"]}, {"source": ["## Get all parcel locations\n", "Read the file that lists all properties. Then build a lookup table that associates each parcel ID with its projected coordinates."], "metadata": {"_cell_guid": "2c42857b-0ba3-4755-b965-4ecf0b1b8e45", "_uuid": "1a46d6d9f5f7ea184e4c97a276c4f209458ef829"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "0d326510-932c-4930-a704-5475fa6dbf69", "_uuid": "b6844fbb6daee4bf89f7c96a5a9d27d7a1db6c36"}, "execution_count": null, "cell_type": "code", "source": ["props = pd.read_csv('../input/properties_2016.csv')"]}, {"outputs": [], "metadata": {"_cell_guid": "32059ac9-6234-4f52-aa32-145d5017e295", "_uuid": "907f7faefa76cbca9e5bcb2894b519b99bfdc26b"}, "execution_count": null, "cell_type": "code", "source": ["# Build a new dataframe that just includes the lon and lat, indexed by parcel ID.\n", "lon = props['longitude'] / 1.0e6\n", "lat = props['latitude'] / 1.0e6\n", "parcelid = props['parcelid']\n", "parcel_coords = pd.DataFrame({'parcelid': parcelid, 'lon': lon, 'lat': lat})\n", "parcel_coords = parcel_coords.set_index('parcelid').dropna(axis=0)\n", "parcel_coords.head()"]}, {"outputs": [], "metadata": {"_cell_guid": "3a2d4ade-fcb1-4dcb-998f-94bd86dc787e", "_uuid": "ace1ac45d96bc96ab3fce56f9d31f6209cd7ed56"}, "execution_count": null, "cell_type": "code", "source": ["# Make a plot to get a sense of the overall spatial distribution of the data we're \n", "# dealing with. Here we're just plotting a random sample of the parcel lon/lat coordinates.\n", "dfs = parcel_coords.sample(n=5000)\n", "plt.figure(figsize=(12, 12))\n", "plt.plot(dfs['lon'], dfs['lat'], '.')"]}, {"source": ["That's LA alright. \n", "\n", "For the analysis below, we're just going to focus on a subset of the area.\n", "So here we define a lon/lat bounding box for the area of interest, which pretty much covers the San Fernando Valley. This bounding box will be applied to the training data when we read it a couple of cells down.\n"], "metadata": {"_cell_guid": "d6608965-a97e-4954-a6b1-23ec4afb4b89", "_uuid": "ce3d67d045d0419ce4bfb787e0de1414b05cbc27"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "bc527a18-dd21-4ea3-a3d7-ff9968f52b6c", "_uuid": "27d7c4aa4d47e70017edcdd669667f577ea8a5d3", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["lon_min = -118.70\n", "lon_max = -118.25\n", "lat_min = 34.10\n", "lat_max = 34.35"]}, {"source": ["## Get sale price residuals\n", "The training dataset consists of parcels with a known residual error. We read this list, associating each one with its \n", "latitude and longitude using the look-up table that we just built above. Then we apply the map projection that we defined above, giving projected (x, y) coordiantes for each parcel. \n", "\n"], "metadata": {"_cell_guid": "7e679840-bf41-4fbf-92d4-c8cec6bd18cf", "_uuid": "fbef2642aa21ad8f21392a6b0236379fe430c25e"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "e444fb46-fae0-405b-9149-e8c4fb61225e", "_uuid": "0d911dce5255bfc4787ba2531d818237dbad6edf"}, "execution_count": null, "cell_type": "code", "source": ["import csv\n", "parcel_list = []\n", "with open('../input/train_2016_v2.csv') as source:\n", "\n", "    reader = csv.DictReader(source, delimiter=',')\n", "    k = 0\n", "    for rec in reader:\n", "    \n", "#         k += 1\n", "#         if k % 10000 == 0:\n", "#             print('Handling record %d' % k)\n", "        \n", "        pid = int(rec['parcelid'])\n", "        lon = parcel_coords.loc[pid]['lon']\n", "        lat = parcel_coords.loc[pid]['lat']\n", "        \n", "        # Only keep records that fall within our area of interest.\n", "        if lon_min < lon < lon_max and lat_min < lat < lat_max:\n", "            \n", "            # Also, subsample the records a bit. A full sample is more than we need to make \n", "            # our point, and just slows things down.\n", "            if np.random.random() < 0.1:\n", "                \n", "                # Get the projected coordinates.\n", "                (xx, yy) = remap(lon, lat)\n", "                \n", "                # Add the relevant information to the parcel list.\n", "                parcel_list.append({'xx': xx, 'yy': yy, 'resid': float(rec['logerror'])})\n", "            \n", "print('Keeping information for %d parcels' % len(parcel_list))"]}, {"outputs": [], "metadata": {"_cell_guid": "bdccc6af-6d95-4bf8-a54e-298a776c472c", "_uuid": "09c05be86d40c0bd96ad7e53cb2571332d6db777"}, "execution_count": null, "cell_type": "code", "source": ["# Take another look at the spatial distribution of the parcels. This time we plot them\n", "# in projected (x, y) coordinates.\n", "df = pd.DataFrame.from_dict(parcel_list)\n", "df.head()\n", "plt.figure(figsize=(10, 10))\n", "plt.plot(df['xx'], df['yy'], '.')"]}, {"source": ["## Spatial autocorrelation \n", "In this section we compute the gamma index as a measure of spatial autocorrelation. The gamma index is based on the cross-product of two measures of the similarity between pairs of parcels: \"value similarity\" and \"spatial similarity\". There are a lot of different ways these quantities can be defined.\n", "\n", "Here we will define the \"value similarity\" for a pair of parcels as a function of the difference of their log error values. We take this difference and apply a negative exponential scaling, so that larger values (near 1.0) represent greater similarity of residuals.\n", "\n", "The \"spatial similarity\" will be defined similarly as a negative exponential function of the linear distance between a pair of parcels. Speficially the spatial similarity function looks like this."], "metadata": {"_cell_guid": "a46aac37-6466-4a39-ace0-4f40c94a7f97", "_uuid": "be0e348050c05088998882eee9a8c1874488e39e"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "d22b06b0-82f2-496f-a4ff-e0b0745b12fc", "_uuid": "4badc9aacb5571fd5abe5cc6680106964de3ff10"}, "execution_count": null, "cell_type": "code", "source": ["range_parameter = 300.0  # meters\n", "distance = np.arange(0.0, 2000.0, 10.0)\n", "spatial_similarity = np.exp(-distance / range_parameter)\n", "\n", "plt.figure(figsize=(8, 4))\n", "plt.plot(distance, spatial_similarity, '-')\n", "plt.xlabel('parcel-to-parcel distance [meters]')\n", "plt.ylabel('spatial similarity')"]}, {"source": ["We have used a range parameter of 300 meters. That scales the function so that inter-parcel distances larger than about 1500 meters are assigned a spatial similarity close to zero. So in the calculations below, we can essentially ignore pairs of parcels that are separated by distances greater than that.     \n", "\n", "Next we look at pairs of parcels from the list that we created above, and compute their spatial and value similarity measures."], "metadata": {"_cell_guid": "13196e9e-2ef8-46cc-b183-7702b94401ef", "_uuid": "fc462d83e3ea57a216764164325258699c088c1c"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "f97895e1-65e9-4d90-9c7c-d3f3e7dbf712", "_uuid": "ed74142a0b450d75744e5d35d63dc0dab7fd0e9d", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["parcel_count = len(parcel_list)\n", "distance_threshold = 1500.0  # meters\n", "range_parameter = 300.0  # meters \n", "spatial_similarity = []\n", "value_similarity = []\n", "sample_count = 0\n", "\n", "for ii in range(parcel_count):\n", "    \n", "#     if ii % 100 == 0:\n", "#         print('%d / %d' % (ii, parcel_count))\n", "        \n", "    for jj in range(ii+1, parcel_count):\n", "        dx = parcel_list[ii]['xx'] - parcel_list[jj]['xx']\n", "        dy = parcel_list[ii]['yy'] - parcel_list[jj]['yy']\n", "        dd = np.sqrt(dx**2 + dy **2)\n", "        if dd < distance_threshold:\n", "            dv = np.abs(parcel_list[ii]['resid'] - parcel_list[jj]['resid'])\n", "            value_similarity.append(np.exp(-dv / 0.5))\n", "            spatial_similarity.append(np.exp(-dd / range_parameter))\n", "            sample_count += 1"]}, {"source": ["Compare the spatial similarity and value similarity values that we just found."], "metadata": {"_cell_guid": "f1b39c2b-4e6f-4f4f-a8bb-7ab5c4a8bf70", "_uuid": "ba48f3e7f2dd7d9a91b4ee6dac034d16fbfd306d"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "0d2306a2-9e27-404e-87fa-20b6fe7947eb", "_uuid": "4d76c6cc0a628e56c04704b482557d752131af1e"}, "execution_count": null, "cell_type": "code", "source": ["plt.figure(figsize=(8,6))\n", "plt.plot(spatial_similarity, value_similarity, '.')\n", "plt.xlabel('spatial similarity')\n", "plt.ylabel('value similarity')\n", "plt.ylim(0, 1.02)"]}, {"source": ["That certainly looks like spatial autocorrelation. That is, parcel pairs that have a high \"spatial similarity\" (i.e. that are close to \n", "one another) tend to have high \"value similarity\" as well (i.e. they have similar errors in estimated home sale prices). Parcels that are further apart begin to lose that tendency. \n", "\n", "That assertion can be made more rigorous by computing the gamma index and checking its statistical significance."], "metadata": {"_cell_guid": "c8ee8b36-b276-45b7-8707-730f3eb16751", "_uuid": "a3c7f098424454884afaa9a44a4364ad9a2aae27"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "6f05d9fd-19e2-4c02-91de-8e000e713ad8", "_uuid": "b001e15f0d7b8254a387c3c75fc9a5533779fec2"}, "execution_count": null, "cell_type": "code", "source": ["gamma = np.dot(spatial_similarity, value_similarity)\n", "print('gamma = %f' % gamma)"]}, {"source": ["So we know the gamma value. Great. But what does it mean? Is that value high or low?\n", "\n", "The gamma value itself doesn't tell us anything because it depends on the units and the scale of the values that we are comparing. So to determine whether this value is evidence of spatial autocorrelation, we need to compare it to values that you would get if no autocorrelation existed. That is, we need to do your basic statistical hypothesis test.\n", "\n", "We can manufacture a set of \"null hypothesis\" gamma values by keeping the spatial similarity values that we computed above, but replacing the value similarities with values computed from randomly selected parcel pairs. Doing this a bunch of times gives us a distributon of what gamma would tend to be in the absence of autocorrelation. "], "metadata": {"_cell_guid": "a1f8c89c-e92a-456a-ba41-56c0f6322ede", "_uuid": "38d8bfea27f2a4e8ff38603ea007e0156c9926c1"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "e9b061d1-a355-4860-93e1-e38710d24609", "_uuid": "e809f9ae823fb0bfe534a040433c0c0ba3bc4bf9"}, "execution_count": null, "cell_type": "code", "source": ["nnn = 100\n", "gamma_null = np.zeros(nnn)\n", "for k in range(nnn):\n", "    for i in range(len(spatial_similarity)):\n", "        zz = np.random.choice(len(parcel_list), 2, replace=False)\n", "        v0 = parcel_list[zz[0]]['resid']\n", "        v1 = parcel_list[zz[1]]['resid']\n", "        dv = np.abs(v0 - v1)\n", "        value_similarity = np.exp(-dv /0.5)\n", "        gamma_null[k] += value_similarity * spatial_similarity[i]\n", "    print('%d / %d: null gamma: %.1f [vs. observed gamma %.1f]' % (k, nnn, gamma_null[k], gamma))"]}, {"outputs": [], "metadata": {"_cell_guid": "6d206530-c409-4769-939e-63116bc17766", "_uuid": "938032112f762405b9894d639ecf59680ab80d2f"}, "execution_count": null, "cell_type": "code", "source": ["# Plot the distribution of the gamma values that we woudl get under the null \n", "# hypothesis of no spatial autocorrelation.\n", "plt.figure(figsize=(10, 10))\n", "plt.hist(gamma_null, bins=30)\n", "plt.xlabel('Gamma Value')\n", "plt.ylabel('Relative Frequency')\n", "plt.title('Gamma Values In Absence Of Autocorrelation (Versus Observed Value %.1f)' % gamma)"]}, {"source": ["So in the absence of spatial autocorrelation, the gamma index never gets as large as the one that we \n", "observed in our dataset (out of the 100 cases that we generated). This amounts to very strng evidence that Zillow home price estimates have spatially correlated errors.\n"], "metadata": {"_cell_guid": "dd6bc801-e8bd-4671-ba25-277380a12424", "_uuid": "3784d6ccc40448ef7b08f47a47cedfbb39e1a2e9"}, "cell_type": "markdown"}, {"outputs": [], "metadata": {"_cell_guid": "bb6031e5-d109-4e47-949b-0201369b5871", "_uuid": "2d98ff1a253e05da5e7f30b31ce05224d3d1c7d0", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": []}], "metadata": {"language_info": {"nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "file_extension": ".py", "version": "3.6.1", "pygments_lexer": "ipython3"}, "_change_revision": 0, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "_is_fork": false}, "nbformat_minor": 1, "nbformat": 4}