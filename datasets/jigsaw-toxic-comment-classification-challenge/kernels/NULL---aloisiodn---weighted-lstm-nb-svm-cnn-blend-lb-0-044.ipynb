{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "version": "3.6.3", "mimetype": "text/x-python", "file_extension": ".py"}}, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "d1aed5c1-2284-4aa0-9bdd-8b3a87f10455", "_uuid": "6c82c5ade1ea2ebe3115b7acdbcb5eb01b8dc604"}, "cell_type": "markdown", "source": ["## Introduction"]}, {"metadata": {"_cell_guid": "5a738023-69cc-4acc-a92b-53c338dd1ab3", "_uuid": "c8b9bb50ea3e069843370be618e835cd53a2dc5e"}, "cell_type": "markdown", "source": ["This is just a weighted version of [Minimal LSTM + NB-SVM baseline ensemble (lb 0.044)](https://www.kaggle.com/jhoward/minimal-lstm-nb-svm-baseline-ensemble-lb-0-044) from [Jeremy Howard](https://www.kaggle.com/jhoward) with one more model. It scores just a little better.\n", "\n", "There are 3 very different strong baselines currently in the kernels for this competition:\n", "    \n", "- An *LSTM* model, which uses a recurrent neural network to model state across each text, with no feature engineering\n", "- An *NB-SVM* inspired model, which uses a simple linear approach on top of naive bayes features\n", "\n", "- An cnn based model\n", "\n", "In theory, an ensemble works best when the individual models are as different as possible. Therefore, we should see that even a simple average of these two models gets a good result. Let's try it! First, we'll load the outputs of the models (in the Kaggle Kernels environment you can add these as input files directly from the UI; otherwise you'll need to download them first)."]}, {"metadata": {"_cell_guid": "4f49e4ea-a226-4d61-921b-db33c41453a8", "collapsed": true, "_uuid": "d3acc5945691f3ac94b41a99efd00d2f794b28d9"}, "execution_count": null, "source": ["import numpy as np, pandas as pd\n", "\n", "f_lstm = '../input/improved-lstm-baseline-glove-dropout-lb-0-048/submission.csv'\n", "f_nbsvm = '../input/nb-svm-strong-linear-baseline-eda-0-052-lb/submission.csv'\n", "f_cnnrnn = '../input/keras-cnn-rnn-0-051-lb/baseline.csv'"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "55a8085b-7a52-4c20-975d-4f6fe0dc7202", "collapsed": true, "_uuid": "12511977319d644ed8a07af913b0f8e00b14f22f"}, "execution_count": null, "source": ["p_lstm = pd.read_csv(f_lstm)\n", "p_nbsvm = pd.read_csv(f_nbsvm)\n", "p_cnnrnn = pd.read_csv(f_cnnrnn)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "74113048-4055-4ead-98b8-a003b6b075bd", "_uuid": "756837877eacd2ab5312623640c66e156a75c1fe"}, "cell_type": "markdown", "source": ["Now we can take the weighted average of the label columns."]}, {"metadata": {"_cell_guid": "c736d49f-de3b-46f5-b4c3-04a26ac98d31", "collapsed": true, "_uuid": "d3a16c6bb660eb600e68c11c03bd2d841b7515f4"}, "execution_count": null, "source": ["label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "p_res = p_lstm.copy()\n", "p_res[label_cols] = (p_cnnrnn[label_cols]*3 + p_nbsvm[label_cols]*2 + p_lstm[label_cols]*5) / 10"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "d4359384-5657-4660-8fd9-6b395bb7c2a6", "_uuid": "807d5f936cc49ccff2e4edc28287be06b008beb1"}, "cell_type": "markdown", "source": ["And finally, create our CSV."]}, {"metadata": {"_cell_guid": "5f73aafe-81a9-4149-b879-717fee5f1814", "_uuid": "30905be21b812c74c8bb1e3469134b226cbcfc78"}, "execution_count": null, "source": ["p_res.to_csv('submission.csv', index=False)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "d66ecdcc-6339-48d5-94ad-519187078972", "_uuid": "648c18c7829885667bbb79a4ed95b82cb5cc620d"}, "cell_type": "markdown", "source": ["As we hoped, when we submit this to Kaggle, we get a great result - a 0.44 score, compared to the individual scores of 0.48 and 0.52. This is currently the best Kaggle kernel submission that runs within the kernels sandbox!"]}], "nbformat_minor": 1}