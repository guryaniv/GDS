{"metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "name": "python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_uuid": "9ac53d679466b9a912834f35996cb272cb9dae0a", "_cell_guid": "65a056b8-5212-48a6-a8ce-487ff54e404b"}, "cell_type": "markdown", "source": ["## Notes\n", "<p>\n", "In this notebook you will learn how to train a simple LSTM RNN (Long-Short Term Model, Recurrent Neural Network) to generate text in the style of Edgar Allen Poe.  \n", "<br>\n", "I myself am actually quite new to NLP and deep learning, and this notebook was created mostly on information found in this tutorial:  \n", "https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/  \n", "<br>\n", "This was also a helpful resource for me when getting introduced to the subject:  \n", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/  \n", "<br>\n", "And before that, it would be best to know the basics of neural networks, such as forward/back prop, layers, activation, etc.\n", "1. I would recommend Andrew Ng's machine learning course on coursera for a good starting point (Rated 4.9 out of 5 of 52,993 ratings):  \n", "https://www.coursera.org/learn/machine-learning  \n", "He's actually recently released a 5-course specialization solely focused on neural networks; the 5th course titled 'Sequential Model' will cover RNNs and is expected to be available within the month.  \n", "<br>\n", "I won't be explaining the theory as to why LSTMs work, but hopefully you will gain a better understanding of what format the input data needs to be in, and how to assemble a simple RNN.\n", "</p>\n"]}, {"metadata": {"_uuid": "8f0e9b77d46ff9cbe609b207918efd961b7b389f", "_cell_guid": "46de4a33-1cfd-44d6-a070-0a15a33e81f9"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import re # regex package\n", "from keras.models import Sequential\n", "from keras.layers import Dense, Activation, TimeDistributed, LSTM, Dropout\n", "from keras import optimizers\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "def load_data(file, nrows = 100):\n", "    df = pd.read_csv('../input/' + file + '.csv', nrows = nrows)\n", "    return(df)\n", "\n", "def shape_data(df):\n", "    df = df.groupby('author', as_index=True).agg([lambda x: ' '.join(x)])\n", "    return(df)\n", "\n", "def to_char_list(string):\n", "    char_list = []\n", "    for char in string:\n", "        char_list.append(char)\n", "    return(char_list)\n", "\n", "## replace multiple whitespaces with single ' ' character\n", "## also trim the start and end of the string, because it will come from a list\n", "## so has leading and trailing '\"[ ' and ']\"' characters respectively\n", "def clean_text(s):\n", "    replaced = re.sub('\\\\s+', ' ', s).strip() # replace multiple whitespaces with single ' ' character\n", "    replaced = replaced[3:-2] # remove leading and trailing '\"[ ' and ']\"' characters respectively\n", "    return(replaced)\n", "\n", "## Model\n", "HIDDEN_DIM = 256\n", "LAYER_NUM = 2\n", "def get_model(learning_rate = 0.009):\n", "    model = Sequential()\n", "    model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n", "    model.add(Dropout(0.3))\n", "    for i in range(LAYER_NUM - 1):\n", "        model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n", "        model.add(Dropout(0.3))\n", "    model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n", "    model.add(Activation('softmax'))\n", "    optimizer = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n", "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n", "    return(model)\n", "\n", "## Train model\n", "BATCH_SIZE = 64\n", "GENERATE_LENGTH = 150\n", "def train_model(model, X, y, epochs, every=2):\n", "    nb_epoch = 0\n", "    while True:\n", "        #print('\\n\\n')\n", "        if not nb_epoch % every == 0:\n", "            model.fit(X, y, batch_size=BATCH_SIZE, verbose=0, epochs=1)\n", "            nb_epoch += 1\n", "        if nb_epoch % every == 0:\n", "            model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n", "            print('nb_epoch = %i'%nb_epoch)\n", "            print(generate_text(model, GENERATE_LENGTH))\n", "            model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, nb_epoch))\n", "            if nb_epoch % epochs == 0 and not nb_epoch == 0:\n", "                break\n", "            nb_epoch += 1\n", "            \n", "## Text Generation function\n", "def generate_text(model, length):\n", "    ix = [np.random.randint(VOCAB_SIZE)]\n", "    #ix = [char_to_ix['.']]\n", "    y_char = [ix_to_char[ix[-1]]]\n", "    X = np.zeros((1, length, VOCAB_SIZE))\n", "    for i in range(length):\n", "        X[0, i, :][ix[-1]] = 1\n", "        #print(ix_to_char[ix[-1]], end=\"\")\n", "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n", "        y_char.append(ix_to_char[ix[-1]])\n", "    return ('').join(y_char)"]}, {"metadata": {"_uuid": "10d542861ba6c0b80aaa3f93dacf3aefed27bb3a", "_cell_guid": "af83d14e-7215-4215-9e7b-3b226a4a3dd3"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["## Main ## \n", "train_sample = load_data('train', nrows=5000) # read first 5000 rows of the dataset\n", "print(train_sample.shape)\n", "#test = load_data('test')\n", "train = shape_data(train_sample) # aggregate text column, groupped by author\n", "print(train.shape)"]}, {"metadata": {"_uuid": "f7f7c1a4c18489a09e505581e7e4a3578bd401f0", "_cell_guid": "bc431ee8-3642-413d-92ff-295c48384e82"}, "cell_type": "markdown", "source": ["## store the text of each author in 1 combined string"]}, {"metadata": {"_uuid": "a8918f89c0b43b747cfa12ccbd262740645b9c59", "_cell_guid": "70a96cda-4778-4ceb-beea-2bb73ed3daaa", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["eap_text = clean_text(str(train['text'].loc['EAP',:].values))\n", "mws_text = clean_text(str(train['text'].loc['MWS',:].values))\n", "hpl_text = clean_text(str(train['text'].loc['HPL',:].values))"]}, {"metadata": {"_uuid": "1f76f99c60bfdb8f413916b28c78091633eae7c8", "_cell_guid": "d6237379-9c34-481a-9997-1429eaf1148b"}, "cell_type": "markdown", "source": ["## convert that string into a list of characters"]}, {"metadata": {"_uuid": "e89484bb35d7418dd70465199e6c5afbd276031b", "_cell_guid": "547aa1df-028a-40cf-8e17-11bda8ff285b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["eap_char_list = to_char_list(eap_text)\n", "print('num characters in EAP text: %i'%len(eap_char_list))\n", "mws_char_list = to_char_list(mws_text)\n", "print('num characters in MWS text: %i'%len(mws_char_list))\n", "hpl_char_list = to_char_list(hpl_text)\n", "print('num characters in HPL text: %i'%len(hpl_char_list))"]}, {"metadata": {"_uuid": "4af3b7f49d2791b40df1b01c0723d8a916cddf33", "_cell_guid": "56c41763-8d16-4bc4-8ead-e4f1a0195b93"}, "cell_type": "markdown", "source": ["## Create mappings from characters to numeric"]}, {"metadata": {"_uuid": "7811af5c725be635ac8f66d4f3a087be5e168b3e", "_cell_guid": "223b3275-d139-4c1a-bab7-5a81dece85d4"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["eap_char_to_ix = {x:y for x,y in zip(set(eap_char_list),range(len(set(eap_char_list))))}\n", "mws_char_to_ix = {x:y for x,y in zip(set(mws_char_list),range(len(set(mws_char_list))))}\n", "hpl_char_to_ix = {x:y for x,y in zip(set(hpl_char_list),range(len(set(hpl_char_list))))}\n", "print('num unique chars eap: {}\\nnum unique chars mws: {}\\nnum unique chars hpl: {}'.format(len(eap_char_to_ix),len(mws_char_to_ix),len(hpl_char_to_ix)))"]}, {"metadata": {"_uuid": "5e995ce32c08f62239efb9d9eafcf896c5516861", "_kg_hide-input": true, "_cell_guid": "cb14b64d-1d35-4ac6-b8e8-03b1213a002b", "_kg_hide-output": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["## Quick Question: which characters does Edgar Allen Poe use that Mary Shelley does not?\n", "## (same question for H.P. Lovecraft and all 6 combinations).\n", "## List Comprehensions\n", "## Letters Edgar Allen Poe Uses\n", "eap_not_mws = [x for x in list(eap_char_to_ix.keys()) if x not in list(mws_char_to_ix.keys())]\n", "eap_not_hpl = [x for x in list(eap_char_to_ix.keys()) if x not in list(hpl_char_to_ix.keys())]\n", "## Letters Mary Shelley Uses\n", "mws_not_eap = [x for x in list(mws_char_to_ix.keys()) if x not in list(eap_char_to_ix.keys())]\n", "mws_not_hpl = [x for x in list(mws_char_to_ix.keys()) if x not in list(hpl_char_to_ix.keys())]\n", "## Letters Lovecraft Uses\n", "hpl_not_eap = [x for x in list(hpl_char_to_ix.keys()) if x not in list(eap_char_to_ix.keys())]\n", "hpl_not_mws = [x for x in list(hpl_char_to_ix.keys()) if x not in list(mws_char_to_ix.keys())]\n", "\n", "# Even when loading the full training set, this didn't really lead to any obvious character\n", "# abuses, except for some EAP and HPL text having non-english characters."]}, {"metadata": {"_uuid": "31a548426f6feead94c7205827c6f046d71969ca", "_cell_guid": "9268ed07-2eb0-4405-bc9c-817bd1ea6c7a"}, "cell_type": "markdown", "source": ["## More Data Prepping"]}, {"metadata": {"_uuid": "edc4bb94728cd2ae9fefa5482f60c332159b25c5", "_cell_guid": "1bef63f4-77b7-4e83-b96a-4d5716aa218e"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["AUTHOR = 'EAP'\n", "if AUTHOR == 'EAP':\n", "    data = eap_char_list # our text data in 'by character' format\n", "    char_to_ix = eap_char_to_ix # our dictionary\n", "\n", "ix_to_char = dict (zip(char_to_ix.values(),char_to_ix.keys())) # reverse of our dictionary\n", "VOCAB_SIZE = len(char_to_ix) # length of our dictionary\n", "SEQ_LENGTH = 150\n", "#SEQ_LENGTH = (len(eap_char_list) + len(mws_char_list) + len(hpl_char_list)) // train_sample.shape[0] # how long to make a sentence\n", "NUM_SEQUENCES = len(data)//SEQ_LENGTH # number of sequences we're training on\n", "\n", "## Making a dataset to train on from what we have so far\n", "X = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, VOCAB_SIZE))\n", "y = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, VOCAB_SIZE))\n", "for i in range(0, NUM_SEQUENCES):\n", "    X_sequence = data[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n", "    X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n", "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n", "    for j in range(SEQ_LENGTH):\n", "        input_sequence[j][X_sequence_ix[j]] = 1.\n", "    X[i] = input_sequence\n", "\n", "    y_sequence = data[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n", "    y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n", "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n", "    for j in range(SEQ_LENGTH):\n", "        target_sequence[j][y_sequence_ix[j]] = 1.\n", "    y[i] = target_sequence\n", "    \n", "print('AUTHOR: {}'.format(AUTHOR))\n", "print('SEQUENCE LENGTH: {}'.format(SEQ_LENGTH))\n", "print('NUM SEQUENCES: {}'.format(NUM_SEQUENCES))\n", "print('VOCAB SIZE: {}'.format(VOCAB_SIZE))"]}, {"metadata": {"_uuid": "d28bfa5c92f0c1539e636d3e88b44d45ad5f59ca", "_cell_guid": "12b664ed-ec7b-4c96-8428-143f42b0cc64"}, "cell_type": "markdown", "source": ["[](http://)## What does our dataset look like now that it's ready for training?"]}, {"metadata": {"_uuid": "5f6f8b07ee90c2f11549ae4c971c2839fbb37eb0", "_cell_guid": "d14415f8-6627-4b78-a254-7cd35fcf28c3"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print('X: type-{}, shape-{}'.format(type(X), X.shape))\n", "print('y: type-{}, shape-{}'.format(type(y), y.shape))"]}, {"metadata": {"_uuid": "3d6fb8e6b7762ccc09df54e00747ddc70c281219", "_cell_guid": "ed46eec9-ad6a-413d-b121-47da83a7da71"}, "cell_type": "markdown", "source": ["## What does our dataset look like now that it's ready for training? (continued)\n", "<p> There are 3 dimensions: (num_sequences, seq_length, num_unique_char)  \n", "In the first 5000 rows of this dataset, the text with author Edgar Allen Poe has 55,474 characters.  \n", "We chose to look at sequences of length 150, so our num_sequences = 55,474//150 = 369  \n", "num_unique_char is the number of unique characters in our dicitonary  \n", "**X and y are are are collection of 1908 arrays of shape (150, 70), where each array represents a 150 character sequence.  \n", "Each row is a character, and the columns are binary flags representing the index of that character in our dicitonary.**\n", "</p>"]}, {"metadata": {"_uuid": "8db4043a188c5d51ea779c7a5c22a8d14756126c", "_cell_guid": "78894da3-cfc5-41a5-94e4-4c6d2a32df6c"}, "cell_type": "markdown", "source": ["## Here's a better view of one of what X and y are"]}, {"metadata": {"_uuid": "148655a2b0e821fed09304d9a57a05b162d46611", "_cell_guid": "ccefaec4-5688-485a-b67d-cad95684fdb8"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(\"X shape - \", X[0,:,:].shape, 'getting col sums...')\n", "print(X[0,:,:].sum(0), '\\ngetting row sums...\\n', X[0,:,:].sum(1))\n", "\n", "print(\"y shape - \", y[0,:,:].shape, 'getting col sums...')\n", "print(y[0,:,:].sum(0), '\\ngetting row sums...\\n', y[0,:,:].sum(1))"]}, {"metadata": {"_uuid": "8436c516380aa3c97492b24cdbeeeef274b39071", "_cell_guid": "3b35ec4e-e0f6-40d1-832c-a93fc2ce4966"}, "cell_type": "markdown", "source": ["## Here's a better view of one of what X and y are (continued)\n", "<p>\n", "If you look through the two arrays carefully, you will see one index in X decreases by 1, and one index in y increases by 1.\n", "This is because the target sequence (the y sequence) is just the X sequence shifted forward by 1 character.  \n", "By setting up our data in this way, we train the model to predict the next character in the sequence.  \n", "This is visualized below:\n", "</p>"]}, {"metadata": {"_uuid": "8f2d3e3936cdfe25e144dbf7efb57ec984cfe571", "_kg_hide-input": true, "_cell_guid": "49d694aa-08c2-4af7-b93c-ab0a1ec2574d"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["X_seq = []\n", "for row in X[0,:,:]: # iterate over the 150 rows in this sequence\n", "    ix = np.argmax(row) # gets the index of the max value in the row (will be 1 and the rest 0)\n", "    char = ix_to_char[ix] # looks in our dictionary to switch from index to character\n", "    X_seq.append(char) # appends the character to our list\n", "X_seq = ''.join(X_seq) # turns our list into a string\n", "\n", "y_seq = []\n", "for row in y[0,:,:]: # iterate over the 150 rows in this sequence\n", "    ix = np.argmax(row) # gets the index of the max value in the row (will be 1 and the rest 0)\n", "    char = ix_to_char[ix] # looks in our dictionary to switch from index to character\n", "    y_seq.append(char) # appends the character to our list\n", "y_seq = ''.join(y_seq) # turns our list into a string\n", "print('X_seq: %s'%X_seq)\n", "print('\\ny_seq: %s'%y_seq)\n", "print('\\nCapital T appears 1 time in X and 0 times in y')\n", "print('Capital I appears 1 time in X and 2 times in y')"]}, {"metadata": {"_uuid": "6281fee6e2ce9f616212b3db516c443de06b11d7", "_cell_guid": "faeb4e2c-92c4-4e8f-bcc3-a91f6649258d"}, "cell_type": "markdown", "source": ["## Model Summary"]}, {"metadata": {"_uuid": "53b9484685c6fd103a2f5994ff546e02c382bede", "_cell_guid": "531ff93e-ace5-4fa4-859f-a0f815ff3acb"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["model = get_model(learning_rate = .003)\n", "model.summary()"]}, {"metadata": {"_uuid": "93d9a5ed12d534ece3497d2c23965e9af1a8f420", "_cell_guid": "1d27728b-e8b1-43bb-8355-de51683674ff"}, "cell_type": "markdown", "source": ["## Training\n", "<p> see the code for train_model at the top of this notebook if you'd like to edit it.  It will print results and save model weights every 2 epochs.  If you stop the kernel while it's training, you can run it again and it will run additional epochs from where the model left off.\n", "</p>"]}, {"metadata": {"_uuid": "350e57667c2c9a68e906818ba824666fc2e7c574", "_cell_guid": "6c0ec8cf-c402-45e1-999a-b0444ab0bd34"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(\"Building Edgar Allen Poe model, taking into account previous {} characters...\".format(SEQ_LENGTH))\n", "train_model(model, X, y, 30, 5) # train 30 epochs, will print/save every 5 epochs"]}, {"metadata": {"_uuid": "8ce275d5806f1efe6ad9b5fe4b103195390f1545", "_cell_guid": "a0bc1fe2-e0b9-46ce-824b-214698af9961", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["generate_text(model, 150) # Generate random text of length 150"]}, {"metadata": {"_uuid": "71d87fb7cbd05b8d3318dc96f9a1182f2fe771e1", "_cell_guid": "6d2b4ff1-5454-47a4-8b5c-32ec3523a0a5", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["generate_text(model, 150) # Generate random text of length 150"]}, {"metadata": {"_uuid": "1061d5b4211a91dd705ceb74d949c936126000d7", "_cell_guid": "0fd091dd-3ad6-4038-80a2-67c8446ea63d"}, "cell_type": "markdown", "source": ["To get 'good' results, you should probably train on the full dataset, lower the learning_rate, and run a higher number of epochs & hidden nodes...  this may take several hours/days even on a GPU."]}]}