{"cells":[{"metadata":{"trusted":true,"_uuid":"731fa6947453de68ba06d846122bdd1d177fe3a9"},"cell_type":"code","source":"# Initial solution for the Store Sale Forecasting challange: \n# Approach:\n#    A simple 4-layered feed-forward neural network that predicts the sales of a given item in a store at a given date.\n#    Use this notebook just for practice because this solution is not suitable for forecasting problems","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom datetime import datetime, timedelta\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport time\n\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# First let us load the datasets into different Dataframes\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsample_submission_df = pd.read_csv('../input/sample_submission.csv')\n\n# Dimensions\nprint('Train shape:', train_df.shape)\nprint('Test shape:', test_df.shape)\nprint('Sample submission shape:', sample_submission_df.shape)\n# Set of features we have are: date, store, and item\ndisplay(train_df.sample(10))\ndisplay(test_df.sample(10))\ndisplay(sample_submission_df.sample(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9939e4dec1bb134d637672061f3d5dcbda642db","collapsed":true},"cell_type":"code","source":"def process_date(d):\n    \"\"\"\n    Process the given date string and return a vector of various features.\n    The vector contains [year,month,day,weekday] fields\n    \"\"\"\n    dp = datetime.strptime(d, \"%Y-%m-%d\")\n    date_fields = []\n    date_fields.append(dp.year)\n    date_fields.append(dp.month)\n    date_fields.append(dp.day)\n    date_fields.append(dp.weekday())\n    return date_fields\n\n# Sample date parsing\ndf = process_date('2018-09-8')\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2db7d02cd15214db502c34becb884824fe5760d","collapsed":true},"cell_type":"code","source":"# Process the training data\nvec_train_df = train_df.copy(deep=True)\n\n# Vectorize the date field\nvec_train_df['year'],vec_train_df['month'],vec_train_df['day'],vec_train_df['weekday'] = zip(*train_df.date.apply(lambda d: process_date(d)).values)\nvec_train_df = vec_train_df.drop(columns='date')\ndisplay(vec_train_df.sample(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"793ca86e4e5cb7f841518bb34416beb07d93210e","collapsed":true},"cell_type":"code","source":"# Process the test data\nvec_test_df = test_df.copy(deep=True)\n\n# Vectorize the date field\nvec_test_df['year'],vec_test_df['month'],vec_test_df['day'],vec_test_df['weekday'] = zip(*test_df.date.apply(lambda d: process_date(d)).values)\nvec_test_df = vec_test_df.drop(columns='date')\ndisplay(vec_test_df.sample(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8bac6def10ac0b74b6fd37e733a355137938942c"},"cell_type":"code","source":"def vectorized_train_and_test():\n    \"\"\"\n        Returns the final train and test dataframes, ready for training and predicting.\n    \"\"\"\n    return vec_train_df,vec_test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"40a09007de38682b17a2d012dd12f82289ffb03c"},"cell_type":"code","source":"### Below are related DNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3eb529f9fd94530ec9749a2804d59e96f5689351","collapsed":true},"cell_type":"code","source":"class FFNet:\n    \"\"\"\n    A Feed-forward network with 4 hidden layers\n    \"\"\"\n    def __init__(self, n, num_classes, hidden_neurons=[1024,512,128,64], learning_rate=0.01, \n                 l2_lambda = 0.01, use_regularization=False,\n                 data_type = tf.float32):\n        # Hidden units\n        (n1, n2, n3, n4) = hidden_neurons\n        self.n_neurons_1 = n1\n        self.n_neurons_2 = n2\n        self.n_neurons_3 = n3\n        self.n_neurons_4 = n4\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n\n        # Placeholders\n        self.X = tf.placeholder(dtype=tf.float32, shape=[None, n], name=\"input_x\")\n        self.Y = tf.placeholder(dtype=tf.float32, shape=[None, self.num_classes], name=\"input_y\")\n\n        print('Hidden neurons:', str(hidden_neurons))\n        print('Number of hidden layers:', str(len(hidden_neurons)), ' + 1')\n        print('Learning rate:', self.learning_rate)\n        print('Use regularization:', use_regularization)\n        print('l2_lambda:', l2_lambda)\n        # Initializers\n        sigma = 1\n        weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n        bias_initializer = tf.zeros_initializer()\n        \n        # Weights and biases initialization\n        self.weights = {\n            'w1': tf.Variable(weight_initializer([n, self.n_neurons_1])),\n            'w2': tf.Variable(weight_initializer([self.n_neurons_1, self.n_neurons_2])),\n            'w3': tf.Variable(weight_initializer([self.n_neurons_2, self.n_neurons_3])),\n            'w4': tf.Variable(weight_initializer([self.n_neurons_3, self.n_neurons_4])),\n            'w_out': tf.Variable(weight_initializer([self.n_neurons_4, self.num_classes]))\n        }\n        self.biases = {\n            'b1': tf.Variable(bias_initializer([self.n_neurons_1])),\n            'b2': tf.Variable(bias_initializer([self.n_neurons_2])),\n            'b3': tf.Variable(bias_initializer([self.n_neurons_3])),\n            'b4': tf.Variable(bias_initializer([self.n_neurons_4])),\n            'b_out': tf.Variable(bias_initializer([self.num_classes]))\n        }\n        \n        # Hidden layers\n        with tf.name_scope(\"hidden_layers\"):\n            # Then fully connected layers\n            a1 = tf.nn.relu(tf.add(tf.matmul(self.X, self.weights['w1']), self.biases['b1']), name=\"hidden_1\")\n            a2 = tf.nn.relu(tf.add(tf.matmul(a1, self.weights['w2']), self.biases['b2']), name=\"hidden_2\")\n            a3 = tf.nn.relu(tf.add(tf.matmul(a2, self.weights['w3']), self.biases['b3']), name=\"hidden_3\")\n            a4 = tf.nn.relu(tf.add(tf.matmul(a3, self.weights['w4']), self.biases['b4']), name=\"hidden_4\")\n            \n        # Output layer with softmax\n        with tf.name_scope(\"output_layer\"):\n            self.y_out = tf.add(tf.matmul(a4, self.weights['w_out']), self.biases['b_out'], name=\"predictions\")\n            self.a_out = tf.nn.softmax(self.y_out, name=\"output_activations\")\n            print('Output layer shape', self.y_out.shape)\n        \n        # Calculate mean cross-entropy loss\n        with tf.name_scope(\"loss\"):\n            all_losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.y_out, labels=self.Y)\n            self.loss = tf.reduce_mean(all_losses)\n            \n            # Regularization, if required\n            if use_regularization:\n                print('USING L2 Regularization')\n                l2_regularizers = tf.nn.l2_loss(self.weights['w1']) + tf.nn.l2_loss(self.weights['w2']) + tf.nn.l2_loss(self.weights['w3']) + tf.nn.l2_loss(self.weights['w4']) + tf.nn.l2_loss(self.weights['w_out'])\n                self.loss = tf.reduce_mean(self.loss + l2_lambda * l2_regularizers)\n            \n        # Optimizer\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n        \n        # Accuracy\n        with tf.name_scope(\"accuracy\"):\n            correct_predictions = tf.equal(tf.argmax(self.a_out, 1) , tf.argmax(self.Y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n        \n        # Save the model\n        self.model_saver = tf.train.Saver()      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a75ff91e533d402ffab1c9d00b918d35b904945c"},"cell_type":"code","source":"#-----------Training and saving the model-----------------\ndef train(ff_net, data_x, data_y, num_epoches=100, batch_size=1000, epoch_display_step=10, model_path = None):\n    \"\"\"\n        Train and save the model at the given model_path. Only the model with best score will be saved.\n    \"\"\"\n    # Number of features\n    n = data_x.shape[1]\n    # Number of classes\n    num_classes = ff_net.num_classes\n    \n    # Prepare the train, test and train batches\n    X_train, X_test, Y_train, Y_test = train_test_split(data_x, data_y, test_size=0.10, random_state=26) #24\n    print('After data split:')\n    print('Train X shape', X_train.shape)\n    print('Train Y shape', Y_train.shape)\n    print('Test X shape', X_test.shape)\n    print('Test Y shape', Y_test.shape)\n    \n    # Process the test data\n    X_test, Y_test = get_test_data(X_test, Y_test, num_classes)\n    \n    # Training data size after the split\n    m = X_train.shape[0]\n    num_batches = int(m / batch_size)\n    print('Num of batches per epoch', num_batches)\n        \n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n        start_time = time.time()\n        if model_path:\n            # Summary writer\n            summary_file_writer = tf.summary.FileWriter(model_path + '/summary', session.graph)\n            # Summary merger tensor\n            merged_summary = tf.summary.merge_all()\n        \n        # Model with best accuracy found so far\n        best_validation_accuracy = 0.0\n        \n        # One complete cycle of training data\n        for epoch in range(num_epoches):\n            avg_cost = 0.\n            avg_training_accuracy = 0.\n            \n            # Run all batches\n            for i_batch in range(num_batches):\n                batch_X, batch_Y = get_batch(X_train, Y_train, i_batch, batch_size, num_classes)\n                \n                # Run the optimizer\n                train_dict = {ff_net.X:batch_X, ff_net.Y:batch_Y}\n                session.run([ff_net.loss, ff_net.optimizer], feed_dict=train_dict)\n                \n                # Get the current accuracy\n                cst, acc = session.run([ff_net.loss, ff_net.accuracy], feed_dict=train_dict)\n                avg_cost += cst\n                avg_training_accuracy += acc\n                            \n            # Display test accuracy occasionally\n            if epoch % epoch_display_step == 0:\n                # Test the model with test data\n                test_accuracy = ff_net.accuracy.eval({ff_net.X:X_test, ff_net.Y:Y_test})\n                print('Epoch:', '%04d' % epoch, 'loss=', \"{:.9f}\".format(avg_cost / num_batches),\n                     'Training accuracy=', (avg_training_accuracy / num_batches),\n                     'Test/validation accuracy=', test_accuracy)\n                # Save the model as MAX-model with highest validation accuracy found so far\n                if model_path:\n                    if test_accuracy > best_validation_accuracy:\n                        best_validation_accuracy = test_accuracy\n                        max_model_path = model_path + '_MAX'\n                        # Save the model\n                        ff_net.model_saver.save(sess=session, save_path=max_model_path)\n                        print('Best validation accuracy found so far:', best_validation_accuracy)\n                        print(\"Model saved in file: %s\" % max_model_path)\n        \n        end_time = time.time()\n        time_taken = end_time - start_time\n        \n        # Save all variables of the TensorFlow graph to file.\n        if model_path:\n            # Save the model\n            ff_net.model_saver.save(sess=session, save_path=model_path)\n            print(\"Model saved in file: %s\" % model_path)\n        else:\n            print('No model_path specified! Not saving the model')\n\n        # Print the time-usage.\n        print(\"Time taken: \" + str(timedelta(seconds=int(round(time_taken)))))\n        print(\"Training Finished!\")\n        \n#---------Restore the model and run predictions---------------\ndef predict(ff_net, model_path, scalar_dump_dir, predict_x):\n    \"\"\"\n        Run the prediction on batch of documents in predict_x using the \n        saved model at model_path and the scalar dump at scalar_dump_dir.\n    \"\"\"\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n        # Restore the model\n        ff_net.model_saver.restore(sess=session, save_path=model_path)\n        \n        # Stop if atleast one NaN value is found [No need of data_y here, so passing dummy array]\n        if contains_NaNs(predict_x, np.zeros((1,1))):\n            print('Invalid data found in the test data!!')\n            return\n\n        # Scale input data\n        predict_x = scale_data_x(predict_x, scalar_dump_dir=scalar_dump_dir, use_saved_scalar=True)\n        print('Scaled the predict x using saved scalar')\n        \n        # Convert one-hot vector to actual class indeces\n        preds = ff_net.y_out.eval({ff_net.X:predict_x})\n        preds = np.argmax(preds, axis=1)\n        return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bb60971f744b0f62fdbf1759bdf5a5da56fe369","collapsed":true},"cell_type":"code","source":"#---------Helper methods used for training and testnig-----------\ndef get_batch(x_train, y_train, batch_num, batch_size, num_classes):\n    \"\"\"\n        Pre-process the training batch data.\n        Splits the training data into small batches of size batch_size. Also, converts the \n        class into a one-hot vector in the batch. Returns the x and y for a batch, separately.\n    \"\"\"\n    batch_start = batch_num * batch_size\n    \n    # Prepare one-hot-vector of y_train\n    y_one_hot = []\n    for l in y_train[batch_start: batch_start + batch_size]:\n        ohv = np.zeros(num_classes, dtype=float)\n        ohv[l] = 1\n        y_one_hot.append(ohv)\n    # Conver to np array\n    y_one_hot = np.array(y_one_hot)\n    \n    #print('Batch x and y shapes ', x_train[batch_start: batch_start + batch_size].shape, y_one_hot.shape)\n    return x_train[batch_start: batch_start + batch_size], y_one_hot\n\ndef get_test_data(x_test, y_test, num_classes):\n    \"\"\"\n        Pre-process the test data. In case of test data, \n        only y needs to be converted into one-hot vector.\n    \"\"\"\n    y_one_hot = []\n    for l in y_test:\n        ohv = np.zeros(num_classes, dtype=float)\n        ohv[l] = 1\n        y_one_hot.append(ohv)\n    \n    # Conver to np array\n    y_one_hot = np.array(y_one_hot)\n    \n    return x_test, y_one_hot\n\n#----------Data Scaling----------------\nfrom sklearn import preprocessing\nimport _pickle as cPickle\ndef scale_data_x(data_x, scalar_dump_dir, use_saved_scalar=False):\n    \"\"\" \n    Scale the feature data.\n    If use_saved_scalar is True, then the given data_x will be scaled using the scalar \n    saved at scalar_dump_dir. Otherwise, given data_x will be used to fit the scalar and the scalar will be saved at scalar_dump_dir.\n    Finally, scaled data_x will be returned.\n    \"\"\"\n    if not use_saved_scalar:\n        scaler = preprocessing.StandardScaler().fit(data_x)\n        print('Created a new scalar')\n    else:\n        scaler = cPickle.load(open(scalar_dump_dir + 'scalar.pickle', \"rb\"))\n        print('Using the existing scalar at', scalar_dump_dir)\n    \n    scaled_data_x = scaler.transform(data_x)\n    if scalar_dump_dir:\n        cPickle.dump(scaler, open(scalar_dump_dir + 'scalar.pickle', \"wb\"))\n    print('Scaled data x to shape', scaled_data_x.shape)\n    return scaled_data_x\n\n#--------Check for NaNs----------\ndef contains_NaNs(X_data, Y_data):\n    \"\"\"\n        Checks if the given X_data and Y_data has any NaNs. \n        Returns True, if there is atleast one NaN in the data.\n    \"\"\"\n    if np.isfinite(X_data).any() and np.isfinite(Y_data).any():\n        return False\n    # Then there is some invalid data\n    return True\n\na = np.empty((2,2))\nb = np.empty((2,2))\na[:] = np.nan\nb[:] = 0\ncontains_NaNs(a, b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"388a803a93f7d88cc69ba4b3353a98c736de731a","collapsed":true},"cell_type":"code","source":"#-----Create the required directories------\ntmp_scalar_dump_dir = 'tmp/scale/'\ntmp_model_p = 'tmp/model'\nsales_scalar_dump_dir = 'sales/scale/'\nsales_model_p = 'sales/model'\n\ndef create_dir():\n    dir_lst = [tmp_scalar_dump_dir, tmp_model_p, sales_scalar_dump_dir, sales_model_p]\n    for d in dir_lst:\n        if not os.path.exists(d):\n            os.makedirs(d)\n            print('Created', d)\n            \ndef delete_dir():\n    import shutil\n    dir_lst = [tmp_scalar_dump_dir, tmp_model_p, sales_scalar_dump_dir, sales_model_p]\n    for d in dir_lst:\n        if os.path.exists(d):\n            shutil.rmtree(d)\n            print('Deleted', d)\n            \n# Delete existing and create empty directories\n#ONE_TIME delete_dir()\ncreate_dir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2aee579a757b024c09d0df8dd7fae575e938bc3a","collapsed":true},"cell_type":"code","source":"#-------Test the FFNet-------\ndef try_net():\n    \"\"\"\n        Sample training and prediction flow with random data\n    \"\"\"\n    import random\n    x = np.random.rand(1000,10)\n    scale_data_x(x, tmp_scalar_dump_dir, use_saved_scalar=False)\n    y = np.array([random.randint(0, 4) for i in range(1000)])\n    \n    # Network\n    n = x.shape[1]\n    ff_net = FFNet(n, num_classes = 5, hidden_neurons=[1024, 512, 128, 64], learning_rate=0.01, l2_lambda = 0.01, use_regularization=True)\n    train(ff_net, x, y, num_epoches=5, batch_size=200, epoch_display_step=2, model_path=tmp_model_p)\n    \n    print('Predictions:')\n    pred_x = np.random.rand(1,10)\n    preds = predict(ff_net, tmp_model_p, tmp_scalar_dump_dir, pred_x)\n    print('x', pred_x)\n    print('y', preds)\n\ntry_net()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ee3d2edbe14edcf9b61c508b6ea30841d6828e8d","collapsed":true},"cell_type":"code","source":"#---Training and Submission-----\ny_column = 'sales'\nMAX_SALE_VALUE = 500\n# Number of features\nn = 6\n# Create the network\nff_net = FFNet(n, num_classes = MAX_SALE_VALUE, \n                   hidden_neurons=[1024, 1024, 768, 512], learning_rate=0.0015, \n                   l2_lambda = 0.005, use_regularization=False)\ndef train_sales_data():\n    \"\"\"\n        Train the model on the actual sales train data and save the model.\n    \"\"\"\n    train_data_df, _ = vectorized_train_and_test()\n    #Separate the label y from the training data\n    df_y_data = train_data_df[y_column]\n    # Drop the y_column from the training data\n    df_x_data = train_data_df.drop(y_column, axis=1)\n    \n    # Convert into numpy matrix\n    X_data = df_x_data.values\n    Y_data = df_y_data.values\n    \n    # Stop if atleast one NaN value is found\n    if contains_NaNs(X_data, Y_data):\n        print('Invalid data found in the training data!!')\n        return\n    \n    # Scale input data x\n    X_data = scale_data_x(X_data, scalar_dump_dir=sales_scalar_dump_dir, use_saved_scalar=False)\n    #print('>>>>>[TODO] Training on small data!!')\n    #X_data = X_data[:5000, :]\n    #Y_data = Y_data[:5000]\n    \n    print('Training data X shape', X_data.shape)\n    print('Training data Y shape', Y_data.shape)\n    #np.set_printoptions(suppress=True)\n    #print(X_data[0])    \n    \n    print('Starting the training process...')\n    # Network\n    n = X_data.shape[1]\n    print('#>>> Value of n to re-create FFNet during prediction is:', n)\n    train(ff_net, X_data, Y_data, num_epoches=120, batch_size=10000, epoch_display_step=1, model_path=sales_model_p)\n    return ff_net\n    \n#------Tests----------    \ndef predict_sales_data(ff_net):\n    \"\"\"\n    Generates the sample predictions\n    \"\"\"\n    train_data_df, _ = vectorized_train_and_test()\n    train_data_df = train_data_df.sample(10)\n    \n    # Drop the y_column from the training data\n    df_x_data = train_data_df.drop(y_column, axis=1)\n    \n    # Convert into numpy matrix\n    predict_x = df_x_data.values\n    predictions = predict(model_path=sales_model_p, ff_net=ff_net, scalar_dump_dir=sales_scalar_dump_dir, predict_x=predict_x)\n    \n    # Add the predictions to final submission df\n    df_x_data['predictions'] = predictions\n    return df_x_data\n\n# Train and predict\ntrain_sales_data()\ndf_predictions = predict_sales_data(ff_net)\ndf_predictions['sales'] = train_df.iloc[df_predictions.index.values, :].sales.values\ndisplay(df_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30f09610f371416a0be62aeba167863cffd468b6","scrolled":true,"collapsed":true},"cell_type":"code","source":"def submission_data(ff_net):\n    \"\"\"\n    Generates the final predictions and creates the submission file\n    \"\"\"\n    _, test_data_df = vectorized_train_and_test()\n    \n    # Drop the y_column from the training data\n    df_x_data = test_data_df.drop('id', axis=1)\n    \n    # Convert into numpy matrix\n    predict_x = df_x_data.values\n    \n    predictions = predict(model_path=sales_model_p, ff_net=ff_net, scalar_dump_dir=sales_scalar_dump_dir, predict_x=predict_x)\n    \n    # Add the predictions to final submission df\n    df_x_data['predictions'] = predictions\n    return df_x_data\n\nsubmission_df = submission_data(ff_net)\n# Rename the columns and sort by id\nsubmission_df['sales'] = submission_df.loc[:,['predictions']]\nfinal_submission_df = submission_df.loc[:,['sales']]\ndisplay(final_submission_df)\nprint('Saved the sumbision results')\nfinal_submission_df.to_csv('final_submission_result.csv', index_label='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8c2844f56c7b13868e0d6d03fe278290ad80420"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}