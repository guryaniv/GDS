{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport glob\nimport zipfile\nimport functools\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['axes.grid'] = False\nmpl.rcParams['figure.figsize'] = (12,12)\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.image as mpimg\nimport pandas as pd\nfrom PIL import Image\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport tensorflow as tf\nimport tensorflow.contrib as tfcontrib\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import losses\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras import backend as K\n\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"markdown","source":"print(os.listdir(\"../input/train\"))"},{"metadata":{"trusted":true,"_uuid":"c683877ce95a512be102e6219aef589c1bda090c"},"cell_type":"code","source":"competition_name = \"../input\"\nimg_dir = os.path.join(competition_name, \"train\")\nlabel_dir = os.path.join(competition_name, \"train_masks\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fc1519aa898f653ad595224d0e0dcec74e09d68"},"cell_type":"code","source":"df_train = pd.read_csv(os.path.join(competition_name, 'train_masks.csv'))\nids_train = df_train['img'].map(lambda s: s.split('.')[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f5000c49a7b13c35a9f48586ff6939cc41ff8ea1"},"cell_type":"code","source":"ids_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a3a2bfa1393f66348130486f45f1dd131db8c92"},"cell_type":"code","source":"x_train_filenames = []\ny_train_filenames = []\nfor img_id in ids_train:\n    x_train_filenames.append(os.path.join(img_dir, \"{}.jpg\".format(img_id)))\n    y_train_filenames.append(os.path.join(label_dir, \"{}_mask.gif\".format(img_id)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1256d980bdc206615d5845026eb5521e573bc412"},"cell_type":"code","source":"x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = \\\n                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17b6e0c7cf10d1caeb78beb39231af45484a41df"},"cell_type":"code","source":"num_train_examples = len(x_train_filenames)\nnum_val_examples = len(x_val_filenames)\n\nprint(\"Number of training examples: {}\".format(num_train_examples))\nprint(\"Number of validation examples: {}\".format(num_val_examples))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffb3e5ff59367b9895cd7f9b690560a226b3c0b0"},"cell_type":"code","source":"y_train_filenames[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f2389e8e438550aa38716339d0525c0edd0318a"},"cell_type":"code","source":"display_num = 5\n\nr_choices = np.random.choice(num_train_examples, display_num)\n\nplt.figure(figsize=(10, 15))\nfor i in range(0, display_num * 2, 2):\n    img_num = r_choices[i // 2]\n    x_pathname = x_train_filenames[img_num]\n    y_pathname = y_train_filenames[img_num]\n    plt.subplot(display_num, 2, i + 1)\n    plt.imshow(mpimg.imread(x_pathname))\n    plt.title(\"Original Image\")\n    \n    example_labels = Image.open(y_pathname)\n    label_vals = np.unique(example_labels)\n    \n    plt.subplot(display_num, 2, i + 2)\n    plt.imshow(example_labels)\n    plt.title(\"Masked Image\")  \n    \nplt.suptitle(\"Examples of Images and their Masks\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90dd64b6974ce24ffe11e982ebd09af7158cf9ea"},"cell_type":"code","source":"img_shape = (256, 256, 3)\nbatch_size = 3\nepochs = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de8881762f13eef58765bdc96d019d72c970fd2f"},"cell_type":"code","source":"def _process_pathnames(fname, label_path):\n    # We map this function onto each pathname pair  \n    img_str = tf.read_file(fname)\n    img = tf.image.decode_jpeg(img_str, channels=3)\n    label_img_str = tf.read_file(label_path)\n      # These are gif images so they return as (num_frames, h, w, c)\n    label_img = tf.image.decode_gif(label_img_str)[0]\n      # The label image should only have values of 1 or 0, indicating pixel wise\n      # object (car) or not (background). We take the first channel only. \n    label_img = label_img[:, :, 0]\n    label_img = tf.expand_dims(label_img, axis=-1)\n    return img, label_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6df21343d8195ff9ba3f71ee157ca869f8ae4499"},"cell_type":"code","source":"def shift_img(output_img, label_img, width_shift_range, height_shift_range):\n    \"\"\"This fn will perform the horizontal or vertical shift\"\"\"\n    if width_shift_range:\n        width_shift_range = tf.random_uniform([], -width_shift_range * img_shape[1],\n                                                  width_shift_range * img_shape[1])\n    if height_shift_range:\n        height_shift_range = tf.random_uniform([],-height_shift_range * img_shape[0],\n                                                   height_shift_range * img_shape[0])\n      # Translate both \n    output_img = tfcontrib.image.translate(output_img,[width_shift_range, height_shift_range])\n    label_img = tfcontrib.image.translate(label_img,[width_shift_range, height_shift_range])\n    return output_img, label_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fd1b68a78f778a7b641e9cd1ea2e81ce9906ade"},"cell_type":"code","source":"def flip_img(horizontal_flip, tr_img, label_img):\n    if horizontal_flip:\n        flip_prob = tf.random_uniform([], 0.0, 1.0)\n        tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n                                    lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n                                    lambda: (tr_img, label_img))\n    return tr_img, label_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58043e273f9aa35253fbca7d8e3559a19a9fe88e"},"cell_type":"code","source":"def _augment(img,\n             label_img,\n             resize=None,  # Resize the image to some size e.g. [256, 256]\n             scale=1,  # Scale image e.g. 1 / 255.\n             hue_delta=0,  # Adjust the hue of an RGB image by random factor\n             horizontal_flip=False,  # Random left right flip,\n             width_shift_range=0,  # Randomly translate the image horizontally\n             height_shift_range=0):  # Randomly translate the image vertically \n    if resize is not None:\n        # Resize both images\n        label_img = tf.image.resize_images(label_img, resize)\n        img = tf.image.resize_images(img, resize)\n\n    if hue_delta:\n        img = tf.image.random_hue(img, hue_delta)\n\n    img, label_img = flip_img(horizontal_flip, img, label_img)\n    img, label_img = shift_img(img, label_img, width_shift_range, height_shift_range)\n    label_img = tf.to_float(label_img) * scale\n    img = tf.to_float(img) * scale \n    return img, label_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c876f9bf5d46d8b32ee468fba50c725155cbb0f"},"cell_type":"code","source":"def get_baseline_dataset(filenames, \n                         labels,\n                         preproc_fn=functools.partial(_augment),\n                         threads=5, \n                         batch_size=batch_size,\n                         shuffle=True):           \n    num_x = len(filenames)\n    # Create a dataset from the filenames and labels\n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n    # Map our preprocessing function to every element in our dataset, taking\n    # advantage of multithreading\n    dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n    if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n        assert batch_size == 1, \"Batching images must be of the same size\"\n        \n    dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n\n    if shuffle:\n        dataset = dataset.shuffle(num_x)\n\n    # It's necessary to repeat our data for all epochs \n    dataset = dataset.repeat().batch(batch_size)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49360b06059397f94c1fd0285c85cef0f79330b7"},"cell_type":"code","source":"tr_cfg = {\n    'resize': [img_shape[0], img_shape[1]],\n    'scale': 1 / 255.,\n    'hue_delta': 0.1,\n    'horizontal_flip': True,\n    'width_shift_range': 0.1,\n    'height_shift_range': 0.1\n}\ntr_preprocessing_fn = functools.partial(_augment, **tr_cfg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dcb5923d85f8c9ddb3c2b7bb2dab848326940e0"},"cell_type":"code","source":"val_cfg = {\n    'resize': [img_shape[0], img_shape[1]],\n    'scale': 1 / 255.,\n}\nval_preprocessing_fn = functools.partial(_augment, **val_cfg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2750f0346b121a60b937bc940b1eb0ac85368c24"},"cell_type":"code","source":"train_ds = get_baseline_dataset(x_train_filenames,\n                                y_train_filenames,\n                                preproc_fn=tr_preprocessing_fn,\n                                batch_size=batch_size)\nval_ds = get_baseline_dataset(x_val_filenames,\n                              y_val_filenames, \n                              preproc_fn=val_preprocessing_fn,\n                              batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b167a6b4cdef989a747f47a87eca4945b8eaa36a"},"cell_type":"code","source":"temp_ds = get_baseline_dataset(x_train_filenames, \n                               y_train_filenames,\n                               preproc_fn=tr_preprocessing_fn,\n                               batch_size=1,\n                               shuffle=False)\n# Let's examine some of these augmented images\ndata_aug_iter = temp_ds.make_one_shot_iterator()\nnext_element = data_aug_iter.get_next()\nwith tf.Session() as sess: \n    batch_of_imgs, label = sess.run(next_element)\n\n  # Running next element in our graph will produce a batch of images\n    plt.figure(figsize=(10, 10))\n    img = batch_of_imgs[0]\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(img)\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(label[0, :, :, 0])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7c247261448d2e95892c1a192f978c900c6e731"},"cell_type":"code","source":"def conv_block(input_tensor, num_filters):\n    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n    encoder = layers.BatchNormalization()(encoder)\n    encoder = layers.Activation('relu')(encoder)\n    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n    encoder = layers.BatchNormalization()(encoder)\n    encoder = layers.Activation('relu')(encoder)\n    return encoder\n\ndef encoder_block(input_tensor, num_filters):\n    encoder = conv_block(input_tensor, num_filters)\n    encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n    \n    return encoder_pool, encoder\n\ndef decoder_block(input_tensor, concat_tensor, num_filters):\n    decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n    decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n    decoder = layers.BatchNormalization()(decoder)\n    decoder = layers.Activation('relu')(decoder)\n    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n    decoder = layers.BatchNormalization()(decoder)\n    decoder = layers.Activation('relu')(decoder)\n    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n    decoder = layers.BatchNormalization()(decoder)\n    decoder = layers.Activation('relu')(decoder)\n    return decoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75356679d74b639aaac13a58a4e3cbfc1b32dada"},"cell_type":"code","source":"inputs = layers.Input(shape=img_shape)\n# 256\n\nencoder0_pool, encoder0 = encoder_block(inputs, 32)\n# 128\n\nencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n# 64\n\nencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n# 32\n\nencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n# 16\n\nencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n# 8\n\ncenter = conv_block(encoder4_pool, 1024)\n# center\n\ndecoder4 = decoder_block(center, encoder4, 512)\n# 16\n\ndecoder3 = decoder_block(decoder4, encoder3, 256)\n# 32\n\ndecoder2 = decoder_block(decoder3, encoder2, 128)\n# 64\n\ndecoder1 = decoder_block(decoder2, encoder1, 64)\n# 128\n\ndecoder0 = decoder_block(decoder1, encoder0, 32)\n# 256\n\noutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n\nmodel = models.Model(inputs=[inputs], outputs=[outputs])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"101c05f0da83f64f0cf968d701fd1b21769a4b0d"},"cell_type":"code","source":"def dice_coeff(y_true, y_pred):\n    smooth = 1.\n    # Flatten\n    y_true_f = tf.reshape(y_true, [-1])\n    y_pred_f = tf.reshape(y_pred, [-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85d3929dfa5fc035335deabf2014b26a8c97a1b2"},"cell_type":"code","source":"def dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"492b916f5c9d248b2432c2cd1ad38597defa38e5"},"cell_type":"code","source":"def bce_dice_loss(y_true, y_pred):\n    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6779692104d2cf588cc1fa2aaa1f9e976f60810f"},"cell_type":"code","source":"model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bb5f55e6dfb23c694ea0b3c3cc0ad6b7bd6bb86"},"cell_type":"markdown","source":"save_model_path = '../weights.hdf5'\ncp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_dice_loss', save_best_only=True, verbose=1)"},{"metadata":{"trusted":true,"_uuid":"38a43c83981016d8613922a64dab51ee5201a5f3"},"cell_type":"code","source":"history = model.fit(train_ds, \n                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n                   epochs=epochs,\n                   validation_data=val_ds,\n                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7e6ce7240efe86e14431713d68850c355e6f986"},"cell_type":"code","source":"dice = history.history['dice_loss']\nval_dice = history.history['val_dice_loss']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, dice, label='Training Dice Loss')\nplt.plot(epochs_range, val_dice, label='Validation Dice Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Dice Loss')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7142430f0a00f06ae67e6fd2272487b3aa7796b"},"cell_type":"code","source":"model = models.load_model(save_model_path, custom_objects={'bce_dice_loss': bce_dice_loss,\n                                                           'dice_loss': dice_loss})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c5803e8ba1d10b057511003eeaae7e68bb48960"},"cell_type":"code","source":"data_aug_iter = val_ds.make_one_shot_iterator()\nnext_element = data_aug_iter.get_next()\n\nplt.figure(figsize=(10, 20))\nfor i in range(5):\n    batch_of_imgs, label = tf.keras.backend.get_session().run(next_element)\n    img = batch_of_imgs[0]\n    predicted_label = model.predict(batch_of_imgs)[0]\n\n    plt.subplot(5, 3, 3 * i + 1)\n    plt.imshow(img)\n    plt.title(\"Input image\")\n\n    plt.subplot(5, 3, 3 * i + 2)\n    plt.imshow(label[0, :, :, 0])\n    plt.title(\"Actual Mask\")\n    plt.subplot(5, 3, 3 * i + 3)\n    plt.imshow(predicted_label[:, :, 0])\n    plt.title(\"Predicted Mask\")\nplt.suptitle(\"Examples of Input Image, Label, and Prediction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bd2d15568dbc684f11f36d7c78061442ca588fb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}