{"cells":[{"metadata":{"_uuid":"8c72a5021e037c721a6c8ad23a3607af55d5c0a9"},"cell_type":"markdown","source":"# Table of contents\n*  [Introduction](#section1) \n*  [Libraries](#section2) \n*  [EDA](#section4)\n      - [Read the data in](#section5)\n          - [Description](#section6)\n          - [Target column](#section7)\n      - [Cleaning](#section8)\n          - [Missing values](#section9)\n              - [Train/Test](#section10)\n              - [Historical transactions](#section12)\n              - [New transactions](#section13)\n      - [Feature engineering](#section14)\n          - [Train/Test](#section15)\n          - [Historical + New transactions](#section18)\n          - [Aggregate function](#section40)   \n* [Data preparation](#section20)     \n   - [Merge](#section56)\n   - [Final Train/Test](#section57)\n* [Modeling and testing](#section21)\n    - [LightGBM](#section22)\n    - [Xgboost](#section23)\n    - [Summary of results](#section24)\n    - [Feature importance](#section25)\n    - [Ensembled model: averaged and stacked](#section26)\n        - [Model definition](#section27)\n        - [Predictions](#section28)\n        - [Results](#section29)\n* [Submission](#section30)\n* [References](#section31)\n\nby @samaxtech\n\nIf you found this kernel helpful or would like to share your thoughts feel free to upvote and leave a comment! :)"},{"metadata":{"_uuid":"bc4c357422d83e54ff6a3a5bd365a0ea83293597"},"cell_type":"markdown","source":"---\n<a id='section1'></a>\n# Introduction\nAs part of the Elo Merchant Category Recommendation Kaggle competition, this kernel aims to predict a merchant loyalty score for a certain credict card holder."},{"metadata":{"_uuid":"fce441d178384af65e37edccd98eff7a5eace1ee"},"cell_type":"markdown","source":"<a id='section2'></a>\n# Import libraries"},{"metadata":{"trusted":true,"_uuid":"7fd9dbaacddeea6781a4dba565f1b8f9a6809855"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport datetime\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge, Lasso\nfrom lightgbm.sklearn import LGBMRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nimport warnings\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nsns.set(style='white', context='notebook', palette='deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c4735eef6c9fa72c60731b87a6c842096c87e5d","_kg_hide-input":true},"cell_type":"code","source":"Image(url= \"https://upload.wikimedia.org/wikipedia/commons/1/10/Logo-ELO-NEG-Black.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f7a441d365fddc8b65fe7dc16873e20b4966c60"},"cell_type":"markdown","source":"---\n<a id='section4'></a>\n# EDA "},{"metadata":{"_uuid":"da1a7ff853f16bae6e9b64fd95e549c839bfe03d"},"cell_type":"markdown","source":"<a id='section5'></a>\n## Read in the data"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3823241830309287b76ca513ed0b60540924c256"},"cell_type":"code","source":"# Historical and new transactions data\nhist_trans = pd.read_csv('../input/historical_transactions.csv')\nnew_trans = pd.read_csv('../input/new_merchant_transactions.csv')\n\n# Train and Test data\ntrain = pd.read_csv('../input/train.csv', parse_dates=['first_active_month'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['first_active_month'])\n\ntrain_idx = train.shape[0]\ntest_idx = test.shape[0]\n\nprint(\"--------------------------\")\nprint(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)\nprint(\"--------------------------\")\nprint(\"Historical transactions shape: \", hist_trans.shape)\nprint(\"New transactions shape: \", new_trans.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5636c2dd01f8ad96e9fe1ca33023dcf2f21c3ca"},"cell_type":"markdown","source":"<a id='section6'></a>\n### Description"},{"metadata":{"trusted":true,"_uuid":"5b789a7fc633ce9f60a2cf162bd40c0740b6c433","_kg_hide-input":true},"cell_type":"code","source":"print(\"----------------------------------------------------------------\")\nprint(\"Train\")\nprint(\"----------------------------------------------------------------\")\nprint(train.info())\nprint(\"\\n----------------------------------------------------------------\")\nprint(\"Test\")\nprint(\"----------------------------------------------------------------\")\nprint(train.info())\nprint(\"\\n----------------------------------------------------------------\")\nprint(\"Historical transactions\")\nprint(\"----------------------------------------------------------------\")\nprint(hist_trans.info())\nprint(\"\\n----------------------------------------------------------------\")\nprint(\"New transactions\")\nprint(\"----------------------------------------------------------------\")\nprint(new_trans.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8989d7be6fd5381bc1a32cb02692ed49538ef599"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26402c76a8abcc55275ee1556bdb6eb218b3f24c"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"a4cd541e6087eb643a88b3c6ee2c3202e35270bc"},"cell_type":"code","source":"hist_trans.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ba2ec54556b5bb3af862f82ddc6f7bee13a9860"},"cell_type":"code","source":"new_trans.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ee70e910d3d114edc24608d8f2abc0136fc41df"},"cell_type":"markdown","source":"<a id='section7'></a>\n### Target column"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"1d4a0dc19490e62d7633ddf53eb63a534c636a78"},"cell_type":"code","source":"print(\"Target description:\\n\\n\", train['target'].describe())\nprint(\"\\n--------------------------------------------------------------------------------------------\")\nprint(\"\\nTarget values:\\n\\n\", train['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a9e9200759c54ff3bb5f93695bb66e5e6582563"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\nax1, ax2 = axes.flatten()\n\n# Distribution\nsns.distplot(train['target'], ax=ax1, color='Green')\n\n# Sorted correlations with target\nsorted_corrs = train.corr()['target'].sort_values(ascending=False)\nsns.heatmap(train[sorted_corrs.index].corr(), ax=ax2)\n\nax1.set_title('Target Distribution')\nax2.set_title('Correlations')\nplt.show()\ndel sorted_corrs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e840b09a6664d64ecd5d4b0d8ec0a1587546d348"},"cell_type":"markdown","source":"There seem to be 2207 values around -33 for the target column, which follows a normal distribution. Let's take that into account. Also, 'feature_3' correlates better than 'feature_2' and 'feature_1' with 'target'.\n\nLet's confirm the number of values under -30."},{"metadata":{"trusted":true,"_uuid":"797fad254aef2122a61e3becb1150bcc39d337cf"},"cell_type":"code","source":"under_30 = train.loc[train['target'] < -30, 'target'].count()\nprint(\"Under -30:\", under_30, \"values.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf72d2b62c1957a9123400c4c536999e71d975eb"},"cell_type":"markdown","source":"<a id='section8'></a>\n# Cleaning\n<a id='section9'></a>\n## Missing"},{"metadata":{"trusted":true,"_uuid":"f4dde287b21391b070cc054974b080289679a478","_kg_hide-input":true},"cell_type":"code","source":"print(\"MISSING VALUES BEFORE CLEANING\\n\")\nprint(\"--------------------------------------------------\\nTrain:\\n--------------------------------------------------\\n\", train.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nTest:\\n--------------------------------------------------\\n\", test.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nHistorical transactions:\\n--------------------------------------------------\\n\", hist_trans.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nNew transactions:\\n--------------------------------------------------\\n\", new_trans.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1b5b610a4bdd5eeadd0166e02a87e07df2da11d"},"cell_type":"markdown","source":"<a id='section10'></a>\n### Train/Test\nThere's no null values for train. However, there seem to be one observation with a missing 'first_active_month' in test."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d7bc10e6a05c27a9711e8f5742d0370dd70228e6"},"cell_type":"code","source":"test_missing = test[test.isnull()['first_active_month']]\nidx_test_missing = test_missing.index\ntest_missing","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59c9245da5c9dfd2126564d248c17c78412c8157"},"cell_type":"markdown","source":"We can look for all the observations that match the same 'feature_1', 'feature_2' and 'feature_3' values as that, and replace it with the 'first_active_month' that corresponds to their mode."},{"metadata":{"trusted":true,"_uuid":"c2043796522cec743879176f48e783a87b25f280"},"cell_type":"code","source":"same_category = test[(test['feature_1'] == 5) & (test['feature_2'] == 2) & (test['feature_3'] == 1)]\ntest.loc[idx_test_missing, 'first_active_month'] = same_category['first_active_month'].mode()[0]\n\ndel same_category\ntest.iloc[11578]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c9106aa1013b24d882d5eaafdb4a9ec521aaeaa"},"cell_type":"markdown","source":"<a id='section12'></a>\n### Historical transactions"},{"metadata":{"_uuid":"73a733b89c4fe8d932988790d9f20d8edbf57bfd"},"cell_type":"markdown","source":"Similarly, in this case let's drop the missing rows for 'category_3' and 'merchant_id' (less than 1% of total)."},{"metadata":{"trusted":true,"_uuid":"0b856b50ebee191791a9d7209850c30a9fb26b1b"},"cell_type":"code","source":"hist_trans.dropna(subset=['category_3', 'merchant_id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b9bf60a72fe44a06efee1c4f508794d03bfdcdc"},"cell_type":"markdown","source":"For 'category_2', since it has about 10% of missing values, let's replace them with the rounded average value (since values for this column include [1.0, 2.0, 3.0, 4.0, 5.0]), as seen below."},{"metadata":{"trusted":true,"_uuid":"5b28cc963cb4cb3c9beb77cc290369a842cb2956"},"cell_type":"code","source":"hist_trans['category_2'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05298d903433f659549f04357e3a53e44e2232ee"},"cell_type":"code","source":"hist_trans['category_2'].fillna((math.floor(hist_trans['category_2'].mean())), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfa81652acc852727daf924d9e773ac6c8a7ac20"},"cell_type":"markdown","source":"<a id='section13'></a>\n### New transactions\nOnce more, since there are missing values in 'category_3', 'merchant_id', 'category_2' and they add up to no more than about 5%, let's  drop the corresponding rows."},{"metadata":{"trusted":true,"_uuid":"e0a68cdb0c324ed0c7c38c884a197c7dd2d4fd67"},"cell_type":"code","source":"new_trans.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b088973b84f6188fa80c3bff937da6ad7a8a980f"},"cell_type":"markdown","source":"Lastly, let's confirm no null values are present after cleaning."},{"metadata":{"trusted":true,"_uuid":"5e8e90e505d9f7bb5bde5892d7beae0b32d1d2fc","_kg_hide-input":true},"cell_type":"code","source":"print(\"MISSING VALUES AFTER CLEANING\\n\")\nprint(\"--------------------------------------------------\\nTrain:\\n--------------------------------------------------\\n\", train.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nTest:\\n--------------------------------------------------\\n\", test.isnull().sum())\n#print(\"\\n--------------------------------------------------\\nMerchant:\\n--------------------------------------------------\\n\", merchants.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nHistorical transactions:\\n--------------------------------------------------\\n\", hist_trans.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nNew transactions:\\n--------------------------------------------------\\n\", new_trans.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"508ee325bf8080cafeda0aaa84be12312d868cdd"},"cell_type":"markdown","source":"<a id='section14'></a>\n# Feature engineering"},{"metadata":{"trusted":true,"_uuid":"1f65b7975b3560416afea49f9e9dc8027a928425"},"cell_type":"code","source":"# Merge train and test for data processing\ndata = pd.concat([train, test], ignore_index=True)\n\n# Check shapes match\nprint(\"Train ({}) + Test ({}) observations: {}\".format(train.shape[0], test.shape[0], train.shape[0] + test.shape[0]))\nprint(\"Merged shape:\", data.shape)\n\ndel train\ndel test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88819bc2106733914565af8f2dbd34b92e4836d5"},"cell_type":"markdown","source":"<a id='section15'></a>\n## Train/Test"},{"metadata":{"trusted":true,"_uuid":"5203ec3e8ddff1cc528a3daf4904790049acc45e"},"cell_type":"code","source":"# Year and month, separately\ndata['year'] = data['first_active_month'].dt.year\ndata['month'] = data['first_active_month'].dt.month\n\n# Elapsed time, until the latest date on the dataset\ndata['elapsed_time'] = (datetime.date(2018, 2, 1) - data['first_active_month'].dt.date).dt.days\n\n# Categorical features: 'feature_1', 'feature_2' and 'feature_3'\ncont = 1\nfor col in ['feature_1', 'feature_2', 'feature_3']:\n    dummy_col = pd.get_dummies(data[col], prefix='f{}'.format(cont))\n    data = pd.concat([data, dummy_col], axis=1)\n    data.drop(col, axis=1, inplace=True)\n    cont += 1\n    \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd7839d5879c7b295ce9cc5adb5d629582bb96b1"},"cell_type":"markdown","source":"<a id='section18'></a>\n### Historical + New transactions\nLet's create a column called 'new' on 'hist_trans' and 'new_trans' such that, before concatening them, they have the age reference:\n\n- 1: New\n- 0: Historical"},{"metadata":{"trusted":true,"_uuid":"a96aad5d6ee63045ed506278cf813fc04f132ba0"},"cell_type":"code","source":"new_trans['new'] = 1\nhist_trans['new'] = 0\n\n# Concatenate new_trans and hist_trans\ntrans_data = pd.concat([new_trans, hist_trans])\n\ndel new_trans\ndel hist_trans","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25ffde1a3370c7b22206f99105753fd78ceed7d6"},"cell_type":"markdown","source":"More preprocessing: 'category_1', 'category_2' and 'category_3'."},{"metadata":{"trusted":true,"_uuid":"d8f3d085b5b71f796caf03ea2f1d3b7664437870"},"cell_type":"code","source":"# Change Yes/No for 0/1 in 'authorized_flag' and 'category_1'\nyes_no_dict = {'Y':1, 'N':0}\ntrans_data['authorized_flag'] = trans_data['authorized_flag'].map(yes_no_dict)\ntrans_data['category_1'] = trans_data['category_1'].map(yes_no_dict)\n\n# Create five different cols for 'category_2'\ndummy_col = pd.get_dummies(trans_data['category_2'], prefix='category_2')\ntrans_data = pd.concat([trans_data, dummy_col], axis=1)\ntrans_data.drop('category_2', axis=1, inplace=True)\n    \n# Create three different cols for categorical A/B/C in 'category_3'\ndummy_col = pd.get_dummies(trans_data['category_3'], prefix='cat3')\ntrans_data = pd.concat([trans_data, dummy_col], axis=1)\ntrans_data.drop('category_3', axis=1, inplace=True)\n\ntrans_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff3c2a444f70196b03a528454155084eac64bc2d"},"cell_type":"markdown","source":"---\n<a id='section40'></a>\n### Aggregate function\nAggregate function, grouped by 'card_id': min, max, mean, median, std, sum, nunique, range. \n\nAdded:\n- Count on 'installments' and 'purchase_amount'.\n- Mode() on 'new' column (previously created).\n- Mean on new trans_data's category_2 dummy columns.\n- Mean on trans_data's category_4.\n- Mean on 'cat3_A', 'cat3_B' and 'cat3_C' (old 'category_3').\n- Mean on merchants' new dummy columns."},{"metadata":{"trusted":true,"_uuid":"3d8b855c737d0f49354bbb1c903b0c280aed37b7"},"cell_type":"code","source":"def aggregate_historical_transactions(trans_data):\n    \n    trans_data.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans_data['purchase_date']).astype(np.int64)*1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'cat3_A': ['mean'],\n        'cat3_B': ['mean'],\n        'cat3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'city_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'purchase_amount': ['count', 'sum', 'median', 'max', 'min', 'std'],\n        'installments': ['count', 'sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max'],\n        'new':[lambda x:x.value_counts().index[0]] # Mode\n        }\n    \n    agg_history = trans_data.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n\n    df = (trans_data.groupby('card_id').size().reset_index(name='transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\ntrans_data = aggregate_historical_transactions(trans_data)\ntrans_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66e57d7dc83fbfdfd4916957d82f8f7e7ab635d7"},"cell_type":"markdown","source":"---\n<a id='section20'></a>\n# Data preparation\n<a id='section56'></a>\n## Merge"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"53d7e3aee837b0b4224d2550ace7e663c37c74c1"},"cell_type":"code","source":"# Merch data (train + test) with trans_data (historical + new transactions)\nprocessed_data = pd.merge(data, trans_data, on='card_id', how='left')\ndel data\ndel trans_data\nprint(processed_data.shape)\nprocessed_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"076ddd56d65cb691157172cf6f7cdf7fb64ffac2"},"cell_type":"markdown","source":"<a id='section57'></a>\n## Final Train/Test "},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"893ad6cdc077fa30b5c2a9899c38b52ca03e2dd0"},"cell_type":"code","source":"# Train and Test\ntrain = processed_data[:train_idx]\ntest = processed_data[train_idx:]\n\ndel processed_data\n\n# There are some nan values after feature eng in 'purchase_amount_std' and 'installments_std'\ncols = ['purchase_amount_std', 'installments_std']\n\nfor col in cols:\n    train[col].fillna((train[col].value_counts().index[0]), inplace=True)\n    test[col].fillna((test[col].value_counts().index[0]), inplace=True)\n\ntarget = train['target']\n\ncols_2_remove = ['target', 'card_id', 'first_active_month']\nfor col in cols_2_remove:  \n    del train[col]\n    del test[col] \n\n# Check on shapes\nprint(\"--------------------------\")\nprint(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)\nprint(\"--------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4dbc2b41f56ccded5728760c3ec76ecdb208765"},"cell_type":"markdown","source":"---\n<a id='section21'></a>\n# Modeling and testing"},{"metadata":{"_uuid":"8c1ffc386ef21ced8f296b15389464965c9bf633"},"cell_type":"markdown","source":"<a id='section22'></a>\n## LightGBM"},{"metadata":{"trusted":true,"_uuid":"89fae7f72e712b834a128c86e9cb504078b78c54"},"cell_type":"code","source":"lgb_params = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) / FOLDs.n_splits\n    \n\ndel fold_importance_df_lgb\ndel trn_data\ndel val_data\n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d4703c8666ebcca9eccd0c42e11f3a01a03e98d"},"cell_type":"markdown","source":"<a id='section23'></a>\n## Xgboost"},{"metadata":{"trusted":true,"_uuid":"7163cd80fe006b4b8c2611197953eeddf99c2844"},"cell_type":"code","source":"train.rename(index=str, columns={\"new_<lambda>\": \"new_mode\"}, inplace=True)\ntest.rename(index=str, columns={\"new_<lambda>\": \"new_mode\"}, inplace=True)\n\nxgb_params = {'eta': 0.001, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=100, verbose_eval=200)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit=xgb_model.best_ntree_limit+50) / FOLDs.n_splits\n\ndel trn_data\ndel val_data\ndel watchlist\n\nnp.sqrt(mean_squared_error(oof_xgb, target))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2397101139ce7f24ef63e902b2673e487e65d8a"},"cell_type":"markdown","source":"<a id='section24'></a>\n## Summary of results"},{"metadata":{"trusted":true,"_uuid":"05575aedc8bb430e1cd5ad74535faf9e0bbd6edd"},"cell_type":"code","source":"print(\"-----------------\\nScores on train\\n-----------------\")\nprint('lgb:', np.sqrt(mean_squared_error(oof_lgb, target)))\nprint('xgb:', np.sqrt(mean_squared_error(oof_xgb, target)))\n\ntotal_sum = 0.5*oof_lgb + 0.5*oof_xgb\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(total_sum, target)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5648b79387996410a4f131f581f3cef38771f03"},"cell_type":"markdown","source":"<a id='section25'></a>\n## Feature importance"},{"metadata":{"trusted":true,"_uuid":"0ad08e5c46bdc980e34d0cb828e36bb7f27a487c"},"cell_type":"code","source":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')\ndel feature_importance_df_lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97dde43a8c3a54264477e1594f81285b408440a0"},"cell_type":"markdown","source":"---\n<a id='section26'></a>\n# Ensembled model: averaged and stacked\nThe follwing tests lgbm, xgb, catboost, random forest, decision tree, knn, ridge and lasso models individual performance, and compared for averaged and stacked models.\n<a id='section27'></a>\n## Model definition"},{"metadata":{"trusted":true,"_uuid":"ca94b185e51e01335ba971aca895ffb3894000ce"},"cell_type":"code","source":"# Model definition\ntrain_y = target\n\n# Same lgbm and xgb models as before\nlgbm_model = LGBMRegressor(\n                objective=\"regression\", metric=\"rmse\", \n                max_depth=7, min_child_samples=20, \n                reg_alpha= 1, reg_lambda=1,\n                num_leaves=64, learning_rate=0.001, \n                subsample=0.8, colsample_bytree=0.8, \n                verbosity=-1\n)\n\nxgb_model = XGBRegressor(\n                eta=0.001, max_depth=7, \n                subsample=0.8, colsample_bytree=0.8, \n                objective='reg:linear', eval_metric='rmse', \n                silent=True\n)\n\n\n# Test catboost, random forest, decision tree, knn, ridge and lasso models individual performance, for averaged and stacked model\ncatboost_model = CatBoostRegressor(iterations=150)\nrf_model = RandomForestRegressor(n_estimators=25, min_samples_leaf=25, min_samples_split=25)\ntree_model = DecisionTreeRegressor(min_samples_leaf=25, min_samples_split=25)\nknn_model = KNeighborsRegressor(n_neighbors=25, weights='distance')\nridge_model = Ridge(alpha=75.0)\nlasso_model = Lasso(alpha=0.75)\n\n# ------------------------------------------------------------------------------------------------\n# Average regressor\nclass AveragingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, regressors):\n        self.regressors = regressors\n        self.predictions = None\n\n    def fit(self, X, y):\n        for regr in self.regressors:\n            regr.fit(X, y)\n        return self\n\n    def predict(self, X):\n        self.predictions = np.column_stack([regr.predict(X) for regr in self.regressors])\n        return np.mean(self.predictions, axis=1)\n    \n# Averaged & stacked models \naveraged_model = AveragingRegressor([catboost_model, xgb_model, rf_model, lgbm_model])\n\n\nstacked_model = StackingCVRegressor(\n    regressors=[catboost_model, xgb_model, rf_model, lgbm_model],\n    meta_regressor=Ridge()\n)\n\n# Test performance\ndef rmse_fun(predicted, actual):\n    return np.sqrt(np.mean(np.square(predicted - actual)))\n\nrmse = make_scorer(rmse_fun, greater_is_better=False)\n\nmodels = [\n     ('CatBoost', catboost_model),\n     ('XGBoost', xgb_model),\n     ('LightGBM', lgbm_model),\n     ('DecisionTree', tree_model),\n     ('RandomForest', rf_model),\n     ('Ridge', ridge_model),\n     ('Lasso', lasso_model),\n     ('KNN', knn_model),\n     ('Averaged', averaged_model),\n     ('Stacked', stacked_model),\n]\n\n\nscores = [\n    -1.0 * cross_val_score(model, train.values, train_y.values, scoring=rmse).mean()\n    for _,model in models\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa4f39a32a51f607880d23cc2d22e5776ecdb7c5"},"cell_type":"code","source":"dataz = pd.DataFrame({ 'Model': [name for name, _ in models], 'Error (RMSE)': scores })\ndataz.plot(x='Model', kind='bar')\nplt.savefig('stacked_scores.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6587ebbe1aebf1419a981dde40a4633eb44c8fe"},"cell_type":"markdown","source":"<a id='section28'></a>\n## Results"},{"metadata":{"trusted":true,"_uuid":"126eafdbd0e620dd2e472ef161944af6f5c178b1"},"cell_type":"code","source":"dataz","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc288a32957c411e8b1efa3a437b3651050bd7ca"},"cell_type":"markdown","source":"<a id='section29'></a>\n## Predictions"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"8f2cae45ef645912354cb61c99d7cc1eea8e2931"},"cell_type":"code","source":"# Stacked model predictions (best score)\nstacked_model.fit(train.values, target.values)    \npredictions_stacked = stacked_model.predict(test.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56e8b0f399c3853bfb591e253b86dd111c5a3365"},"cell_type":"markdown","source":"<a id='section30'></a>\n# Submission"},{"metadata":{"trusted":true,"_uuid":"344cfaf4f3a8281e53801e59e77be27608fff179"},"cell_type":"code","source":"# LightGBM/Xgboost\nsub_df = pd.read_csv('../input/sample_submission.csv')\nsub_df[\"target\"] = 0.5 * predictions_lgb + 0.5 * predictions_xgb\nsub_df.to_csv(\"submission_lgbxgboost.csv\", index=False)\n\n# Stacked\nsub_df = pd.read_csv('../input/sample_submission.csv')\nsub_df[\"target\"] = predictions_stacked\nsub_df.to_csv(\"submission_stacked.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13edc68a019c79670308e087ee7516259ea58274"},"cell_type":"markdown","source":"---\n<a id='section31'></a>\n# References\nSpecial thanks to the following references:\n- https://www.kaggle.com/mjbahmani/a-data-science-framework-for-elo (@mjbahmani)\n- https://www.kaggle.com/youhanlee/hello-elo-ensemble-will-help-you (@youhanlee)\n- https://www.kaggle.com/peterhurford/you-re-going-to-want-more-categories-lb-3-737 (@peterhurford)\n- https://www.kaggle.com/eikedehling/comparing-models-xgb-lgb-rf-and-stacking (@eikedehling)"},{"metadata":{"trusted":true,"_uuid":"e5498c6e3bbdf5b5f4adf4923b887d48b61ee577"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}