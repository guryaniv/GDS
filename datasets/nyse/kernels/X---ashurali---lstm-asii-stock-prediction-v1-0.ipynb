{"metadata": {"_change_revision": 0, "_is_fork": false, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.1"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"_cell_guid": "179c2ccd-823a-9576-f8d4-552dbbde3483", "_uuid": "daf656a3cba54099b5419b910c4afb51ed403c29"}, "execution_count": null, "outputs": [], "source": "# 07/05/2017 Update\n\nThis project is based on my [GitHub link][1] and my research is based on  [this paper][2]. \n\nInstead of using Echo state network which was used in the Stanford research paper, we are going to use LSTM which is more advanced in training the neural network.\n\nMore updates will be provided to accommodate the dataset in this Kaggle challenge.  You can simply adjust it to choose your features and window for data.\n\nThank you all!\n\n# Import module first\n\n\n  [1]: https://github.com/BenjiKCF/Neural-Network-with-Financial-Time-Series-Data\n  [2]: http://cs229.stanford.edu/proj2012/BernalFokPidaparthi-FinancialMarketTimeSeriesPredictionwithRecurrentNeural.pdf", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "78d1f84f-aed9-747b-154a-f7fe6c7bb8c9", "_uuid": "e8b3da179b9613732a07c5c72a925215fd36f78d", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pandas import datetime\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import load_model\nimport keras\nimport h5py\nimport requests\nimport os", "cell_type": "code"}, {"metadata": {"_cell_guid": "02cc6c64-3161-cf2d-4298-c700c2eee594", "_uuid": "f5c12e68609f79265d7e6a2850694179a75043c4"}, "execution_count": null, "outputs": [], "source": "# Read data and transform them to pandas dataframe", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "e10138dc-b1a1-fd9e-64c8-3bbf3b1844a6", "_uuid": "55fef02f94dacb66bbdb1df6401f3e5b2576cfb3", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "df = pd.read_csv(\"../input/ASIISahamDataset/ASII-JK(10).CSV\", index_col = 0)\ndf[\"Adj Close\"] = df.Close # Moving close to the last column\ndf.drop(['Close'], 1, inplace=True) # Moving close to the last column\ndf.head()", "cell_type": "code"}, {"metadata": {"_cell_guid": "7f9cab92-a31c-034a-df07-7bc7d6591668", "_uuid": "972242c647d53fb277ab34fe4060f47b92300d6e"}, "execution_count": null, "outputs": [], "source": "# Normalize the data", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "a6052d2e-f656-543b-90cc-9f85b9837bac", "_uuid": "8f9f3950a6d09fcc79bd1cc00cd7e800f3c0c313", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "def normalize_data(df):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    df['Open'] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))\n    df['High'] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))\n    df['Low'] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))\n    df['Volume'] = min_max_scaler.fit_transform(df.Volume.values.reshape(-1,1))\n    df['Adj Close'] = min_max_scaler.fit_transform(df['Adj Close'].values.reshape(-1,1))\n    return df\ndf = normalize_data(df)\ndf.head()", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "8512f8df-a065-42d3-bba6-4bfd99d133d1", "_uuid": "bc77a5f3396cd76b84fb30eaa03b61309369e19d", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "df['Close'] = df['Adj Close']\ndf.drop('Adj Close',axis = 1, inplace = True)", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "7d47cae5-c676-478c-8aae-24514d7ff4fb", "_uuid": "d74ee70e2079d0d1a76abedccadd53d5c89f14e6", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "df", "cell_type": "code"}, {"metadata": {"_cell_guid": "cda2ae07-6030-3cfc-7786-8557891c0fb7", "_uuid": "dfdb44304917afef742cf682fde4b0e459fce404"}, "execution_count": null, "outputs": [], "source": "# Create training set and testing set", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "27bca4c4-cd5f-23a3-2e3f-53433cf2dd96", "_uuid": "9524b5a2a7011f55152fb002f1573d13b286f310", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "def load_data(stock, seq_len):\n    amount_of_features = len(stock.columns) # 5\n    data = stock.as_matrix() \n    sequence_length = seq_len + 1 # index starting from 0\n    result = []\n    \n    for index in range(len(data) - sequence_length): # maxmimum date = latest date - sequence length\n        result.append(data[index: index + sequence_length]) # index : index + 22days\n    \n    result = np.array(result)\n    row = round(0.9 * result.shape[0]) # 90% split\n    train = result[:int(row), :] # 90% date, all features \n    \n    x_train = train[:, :-1] \n    y_train = train[:, -1][:,-1]\n    \n    x_test = result[int(row):, :-1] \n    y_test = result[int(row):, -1][:,-1]\n\n    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n\n    return [x_train, y_train, x_test, y_test]", "cell_type": "code"}, {"metadata": {"_cell_guid": "9cbdcc33-fb39-0ee8-2604-d47c6d2e1b92", "_uuid": "c2e831f0977baf0f65d2be96071a857bff77d97f"}, "execution_count": null, "outputs": [], "source": "# Build the structure of model\n\nBased on my hyperparameter testing on [here][1]. I found that these parameters are the most suitable for this task.\n\n![dropout = 0.3][2]\n![epochs = 90][3]\n![LSTM 256 > LSTM 256 > Relu 32 > Linear 1][4]\n\n\n\n  [1]: https://github.com/BenjiKCF/Neural-Network-with-Financial-Time-Series-Data\n  [2]: https://github.com/BenjiKCF/Neural-Network-with-Financial-Time-Series-Data/blob/master/dropout.png?raw=true\n  [3]: https://github.com/BenjiKCF/Neural-Network-with-Financial-Time-Series-Data/blob/master/epochs2.png?raw=true\n  [4]: https://github.com/BenjiKCF/Neural-Network-with-Financial-Time-Series-Data/blob/master/neurons.png?raw=true", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "17b0bf0c-5b76-9052-ecd3-b9f99ba3da94", "_uuid": "80a007774b7f1604c2eba4a31704e19e53ccdd3a", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "def build_model(layers):\n    d = 0.3\n    model = Sequential()\n    \n    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=True))\n    model.add(Dropout(d))\n        \n    model.add(LSTM(256, input_shape=(layers[1], layers[0]), return_sequences=False))\n    model.add(Dropout(d))\n        \n    model.add(Dense(32,kernel_initializer=\"uniform\",activation='relu'))        \n    model.add(Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n    \n    # adam = keras.optimizers.Adam(decay=0.2)\n        \n    start = time.time()\n    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n    print(\"Compilation Time : \", time.time() - start)\n    return model", "cell_type": "code"}, {"metadata": {"_cell_guid": "ca28b9e0-390f-5368-b043-00d0fc4f1b2a", "_uuid": "813c7703d8c6c38d2ad402d97933b6b4d4dc7c1b"}, "execution_count": null, "outputs": [], "source": "# Train the model", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "c3c292d2-bd7b-6e7f-bbba-39b64860ba20", "_uuid": "df156d8459d89fbd20ef82d39bd5a3d8fb6c92f7", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "window = 22\nX_train, y_train, X_test, y_test = load_data(df, window)\nprint (X_train[0], y_train[0])", "cell_type": "code"}, {"metadata": {"_cell_guid": "212f77cd-fdeb-40df-595b-d6de775610f8", "_uuid": "9b273b99c6d9c8eeba9689eb24a4175f0060ec9a", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "model = build_model([5,window,1])", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "7fca42d0-3dd6-4317-b0df-31bfad78b915", "_uuid": "3a20d0ddac77ff1791b051efc94b0ba8a60e3e4a", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "model.fit(X_train,y_train,batch_size=512,epochs=90,validation_split=0.1,verbose=1)", "cell_type": "code"}, {"metadata": {"_cell_guid": "52177800-ffc4-dfd5-c915-46d98f941ca7", "_uuid": "c7cdb28a945b18e6b543f0021f14d24911615e23", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "# print(X_test[-1])\ndiff=[]\nratio=[]\np = model.predict(X_test)\nprint (p.shape)\n# for each data index in test data\nfor u in range(len(y_test)):\n    # pr = prediction day u\n    pr = p[u][0]\n    # (y_test day u / pr) - 1\n    ratio.append((y_test[u]/pr)-1)\n    diff.append(abs(y_test[u]- pr))\n    # print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr))\n    # Last day prediction\n    # print(p[-1]) ", "cell_type": "code"}, {"metadata": {"_cell_guid": "fc1b3c69-d033-e7c1-9ff3-04cefd5d9940", "_uuid": "22ad98471c6831c08c7ab90a41bf7186d00c527a"}, "execution_count": null, "outputs": [], "source": "# Denormalize the data", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "fb935328-2516-89a0-e555-bdc5b289dac9", "_uuid": "7b4cee95815658f6bccb29e31bcd44de2576cb35", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "df = pd.read_csv(\"../input/ASIISahamDataset/ASII-JK(10).CSV\", index_col = 0)\ndf[\"Adj Close\"] = df.Close # Moving close to the last column\ndf.drop(['Close'], 1, inplace=True) # Moving close to the last column\n\ndf['Close'] = df['Adj Close']\ndf.drop('Adj Close',axis = 1, inplace = True)\n\n# Bug fixed at here, please update the denormalize function to this one\ndef denormalize(df, normalized_value): \n    df = df['Close'].values.reshape(-1,1)\n    normalized_value = normalized_value.reshape(-1,1)\n    \n    #return df.shape, p.shape\n    min_max_scaler = preprocessing.MinMaxScaler()\n    a = min_max_scaler.fit_transform(df)\n    new = min_max_scaler.inverse_transform(normalized_value)\n    return new\n\nnewp = denormalize(df, p)\nnewy_test = denormalize(df, y_test)", "cell_type": "code"}, {"metadata": {"_cell_guid": "2f638a96-8f36-7dc9-4118-f97e3df06705", "_uuid": "aeedd5a377e2aa074bbb3fd8b24761e3bc304d77", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "def model_score(model, X_train, y_train, X_test, y_test):\n    trainScore = model.evaluate(X_train, y_train, verbose=0)\n    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n\n    testScore = model.evaluate(X_test, y_test, verbose=0)\n    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n    return trainScore[0], testScore[0]\n\n\nmodel_score(model, X_train, y_train, X_test, y_test)", "cell_type": "code"}, {"metadata": {"_cell_guid": "335d3b00-b700-a97f-c2ad-938949618ac7", "_uuid": "fb3da7bd64036c64fe56e0e4715b486ad78ad2ff"}, "execution_count": null, "outputs": [], "source": "# Since the Kaggle dataset only contains a few years, the mean square error is not as small as my original model on GitHub.\n\nWith more than 40 years of data, we will get:\n\nTrain Score: 0.00006 MSE (0.01 RMSE)\n\nTest Score: 0.00029 MSE (0.02 RMSE)", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "99be25aa-e2c3-35fd-64a5-988200295018", "_uuid": "ae4d660d25adc92ba249e6c54f5e76f4d652995d", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "import matplotlib.pyplot as plt2\n\nplt2.plot(newp,color='red', label='Prediction')\nplt2.plot(newy_test,color='blue', label='Actual')\nplt2.legend(loc='best')\nplt2.show()", "cell_type": "code"}, {"metadata": {"collapsed": true, "_cell_guid": "73092acc-eb54-aeb8-6616-8b1f16b0569d", "_uuid": "a36f1c8942b61e21ab79d5b42bbc8b12ee27d873"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "9b09a45c-0595-bc94-2ca6-d6930181b109", "_uuid": "ce4523cdbdb6c7a51ba55ff4347b72050f332638", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}, {"metadata": {"_cell_guid": "16fb7e45-13e5-7fd6-dd63-2b18f0fdac30", "_uuid": "97d8f0cb7878aaf757935b86357ec5a96026a388", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}, {"metadata": {"_cell_guid": "ce0ba2d6-3f74-c2da-3363-d46d38fcdc77", "_uuid": "333a8e6f8cd7b9f82ac890b07008ee839f8d18bb"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "6e18acc0-de77-7a74-a857-bedf2c19d9fe", "_uuid": "be0cfc55d1258df3e1a73684b750199bb013cbb7", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}, {"metadata": {"_cell_guid": "14d5f557-3767-7a6e-1992-d8a9ff78ee87", "_uuid": "6e04a3942e85e7324a6263a0e9bef6022c1ba6c9", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}, {"metadata": {"_cell_guid": "c9bf36cd-1c14-e608-103d-05ec02f6f68f", "_uuid": "26bb3aa3d2fddc22ba5f9fd4a18103294792504d", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}, {"metadata": {"collapsed": true, "_cell_guid": "bac5d910-ca0d-0e21-4f88-a8c9365dfa9b", "_uuid": "1ffddc90eebcc92fa43faa7114b548af932dfd38", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}]}