{"cells":[{"metadata":{"_uuid":"2ce78599f3e78b5a40c4d2e44ad29fa29c4d978b"},"cell_type":"markdown","source":"<h1>TGS - Simple U-Net</h1>\n\nIn this Kernel I'll create a simple U-Net to create a baseline for future improvements.\n\nThanks to all these participants for their great kernels, on which this is based:\n<ul>\n<li>Jack (Jiaxin) Shao: <a href=\"https://www.kaggle.com/shaojiaxin/u-net-with-simple-resnet-blocks-v2-new-loss\">https://www.kaggle.com/shaojiaxin/u-net-with-simple-resnet-blocks-v2-new-loss</a></li>\n<li>Jesper: <a href=\"https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics\">https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics</a></li>\n<li>Nikhil Tomar: <a href=\"https://www.kaggle.com/nikhilroxtomar/u-net-with-image-augmentation\">https://www.kaggle.com/nikhilroxtomar/u-net-with-image-augmentation\"</a></li>\n<li>Bruno G. do Amaral: <a href=\"https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation\">https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation</a></li>\n<li>Peter Hönigschmid: <a href=\"https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification\">https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification</a></li>\n<li>NPHard: <a href=\"https://www.kaggle.com/meaninglesslives/apply-crf\">https://www.kaggle.com/meaninglesslives/apply-crf</a></li>\n<li>Peter: <a href=\"https://www.kaggle.com/pestipeti/explanation-of-scoring-metric\">https://www.kaggle.com/pestipeti/explanation-of-scoring-metric</a></hi>\n</ul>\n\nSome links for beginners I've found also useful:\n<ul>\n<li><a href=\"https://machinelearningmastery.com/check-point-deep-learning-models-keras/\">https://machinelearningmastery.com/check-point-deep-learning-models-keras/</a></li>\n<li><a href=\"https://machinelearningmastery.com/save-load-keras-deep-learning-models/\">https://machinelearningmastery.com/save-load-keras-deep-learning-models/</a></li>\n</ul>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"89ed0ca48b40aa1af74f38625236c00b56ef9a76"},"cell_type":"code","source":"# Version of the notebook (used for output file names)\nversion = 9","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59c3e3f268d54f9ce376889a47aa6a0de564283d"},"cell_type":"markdown","source":"<h3>Needed imports</h3>"},{"metadata":{"_uuid":"c902dc643c15e1dba0b3245aa2fd19ffd5a0b0ce","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Lambda, Conv2D, SpatialDropout2D, BatchNormalization,Activation\nfrom keras.layers import MaxPooling2D, Conv2DTranspose, concatenate\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.models import Model, load_model, model_from_json\nfrom keras.optimizers import Adam, SGD\nimport keras.backend as K\nfrom keras import losses\nimport tensorflow as tf\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b022bfb3b9aada9f16da2f0b2db7b9cc823a488","trusted":true,"collapsed":true},"cell_type":"markdown","source":"<h2>Convenience functions<h2>\n\nSupport functions used in the kernel."},{"metadata":{"_uuid":"655aafd6d3f7ef7b4a2415c258ef34122faa9e26","trusted":true,"collapsed":true},"cell_type":"code","source":"# Function to upsize images\ndef upsize(img):\n    return resize(img, (128, 128, 1), mode='constant', preserve_range=True)\n\n# Function to downsize images\ndef downsize(img):\n    return resize(img, (101, 101, 1), mode='constant', preserve_range=True)\n\n# Downsize predictions to size (101,101,1)\ndef downsize_preds(preds):\n    preds_resized = []\n    for i in range(len(preds)):\n        preds_resized.append(np.squeeze(downsize(preds[i])))\n    return np.array(preds_resized)\n\n# Works much faster than my previous implementation RLenc\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    np.where: locates the positions in the array where a given condition holds true.\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"183dcf65960c19ec8a6f69d10144f35d902ee2c5"},"cell_type":"markdown","source":"<h3>Metrics used<h3>\n\n<code>competition_metric</code> is based on kernel <a href=\"https://www.kaggle.com/pestipeti/explanation-of-scoring-metric\">https://www.kaggle.com/pestipeti/explanation-of-scoring-metric</a>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dda2ff0a0cd55b0ef22b51708c8bd2115ff59faf"},"cell_type":"code","source":"def castF(x):\n    return K.cast(x, K.floatx())\n\ndef castB(x):\n    return K.cast(x, bool)\n\n#def iou_loss_core(y_true,y_pred):\n#    intersection = y_true * y_pred\n#    notTrue = 1 - y_true\n#    union = y_true + (notTrue * y_pred)\n#    return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n\ndef iou_loss_core(y_true, y_pred, smooth=1):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n    iou = (intersection + smooth) / ( union + smooth)\n    return iou\n\n\ndef iou_loss(y_true, y_pred):\n    return 1 - iou_loss_core(y_true, y_pred)\n\ndef iou_bce_loss(y_true, y_pred):\n    return losses.binary_crossentropy(y_true, y_pred) + 3 * iou_loss(y_true, y_pred)\n\ndef competition_metric(true, pred): #any shape can go\n\n    tresholds = [0.5 + (i*.05)  for i in range(10)]\n\n    #flattened images (batch, pixels)\n    true = K.batch_flatten(true)\n    pred = K.batch_flatten(pred)\n    pred = castF(K.greater(pred, 0.5))\n\n    #total white pixels - (batch,)\n    trueSum = K.sum(true, axis=-1)\n    predSum = K.sum(pred, axis=-1)\n\n    #has mask or not per image - (batch,)\n    true1 = castF(K.greater(trueSum, 1))    \n    pred1 = castF(K.greater(predSum, 1))\n\n    #to get images that have mask in both true and pred\n    truePositiveMask = castB(true1 * pred1)\n\n    #separating only the possible true positives to check iou\n    testTrue = tf.boolean_mask(true, truePositiveMask)\n    testPred = tf.boolean_mask(pred, truePositiveMask)\n\n    #getting iou and threshold comparisons\n    iou = iou_loss_core(testTrue,testPred) \n    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n\n    #mean of thressholds for true positives and total sum\n    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n    truePositives = K.sum(truePositives)\n\n    #to get images that don't have mask in both true and pred\n    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n    trueNegatives = K.sum(trueNegatives) \n\n    return (truePositives + trueNegatives) / castF(K.shape(true)[0])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d0d9ef8b6f82b3e5667bf4f0fbadad2ac46f55d"},"cell_type":"markdown","source":"<h3>Network architecture</h3>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c3c1c11b7b27ae872ea9e0d29fe4ded27498d37d"},"cell_type":"code","source":"def conv_block(neurons, block_input, bn=False, dropout=None):\n    conv1 = Conv2D(neurons, (3,3), padding='same', kernel_initializer='glorot_normal')(block_input)\n    if bn:\n        conv1 = BatchNormalization()(conv1)\n    conv1 = Activation('relu')(conv1)\n    if dropout is not None:\n        conv1 = SpatialDropout2D(dropout)(conv1)\n    conv2 = Conv2D(neurons, (3,3), padding='same', kernel_initializer='glorot_normal')(conv1)\n    if bn:\n        conv2 = BatchNormalization()(conv2)\n    conv2 = Activation('relu')(conv2)\n    if dropout is not None:\n        conv2 = SpatialDropout2D(dropout)(conv2)\n    pool = MaxPooling2D((2,2))(conv2)\n    return pool, conv2  # returns the block output and the shortcut to use in the uppooling blocks\n\ndef middle_block(neurons, block_input, bn=False, dropout=None):\n    conv1 = Conv2D(neurons, (3,3), padding='same', kernel_initializer='glorot_normal')(block_input)\n    if bn:\n        conv1 = BatchNormalization()(conv1)\n    conv1 = Activation('relu')(conv1)\n    if dropout is not None:\n        conv1 = SpatialDropout2D(dropout)(conv1)\n    conv2 = Conv2D(neurons, (3,3), padding='same', kernel_initializer='glorot_normal')(conv1)\n    if bn:\n        conv2 = BatchNormalization()(conv2)\n    conv2 = Activation('relu')(conv2)\n    if dropout is not None:\n        conv2 = SpatialDropout2D(dropout)(conv2)\n    \n    return conv2\n\ndef deconv_block(neurons, block_input, shortcut, bn=False, dropout=None):\n    deconv = Conv2DTranspose(neurons, (3, 3), strides=(2, 2), padding=\"same\")(block_input)\n    uconv = concatenate([deconv, shortcut])\n    uconv = Conv2D(neurons, (3, 3), padding=\"same\", kernel_initializer='glorot_normal')(uconv)\n    if bn:\n        uconv = BatchNormalization()(uconv)\n    uconv = Activation('relu')(uconv)\n    if dropout is not None:\n        uconv = SpatialDropout2D(dropout)(uconv)\n    uconv = Conv2D(neurons, (3, 3), padding=\"same\", kernel_initializer='glorot_normal')(uconv)\n    if bn:\n        uconv = BatchNormalization()(uconv)\n    uconv = Activation('relu')(uconv)\n    if dropout is not None:\n        uconv = SpatialDropout2D(dropout)(uconv)\n        \n    return uconv\n    \ndef build_model(start_neurons, bn=False, dropout=None):\n    \n    input_layer = Input((128, 128, 1))\n    \n    # 128 -> 64\n    conv1, shortcut1 = conv_block(start_neurons, input_layer, bn, dropout)\n\n    # 64 -> 32\n    conv2, shortcut2 = conv_block(start_neurons * 2, conv1, bn, dropout)\n    \n    # 32 -> 16\n    conv3, shortcut3 = conv_block(start_neurons * 4, conv2, bn, dropout)\n    \n    # 16 -> 8\n    conv4, shortcut4 = conv_block(start_neurons * 8, conv3, bn, dropout)\n    \n    # Middle\n    convm = middle_block(start_neurons * 16, conv4, bn, dropout)\n    \n    # 8 -> 16\n    deconv4 = deconv_block(start_neurons * 8, convm, shortcut4, bn, dropout)\n    \n    # 16 -> 32\n    deconv3 = deconv_block(start_neurons * 4, deconv4, shortcut3, bn, dropout)\n    \n    # 32 -> 64\n    deconv2 = deconv_block(start_neurons * 2, deconv3, shortcut2, bn, dropout)\n    \n    # 64 -> 128\n    deconv1 = deconv_block(start_neurons, deconv2, shortcut1, bn, dropout)\n    \n    #uconv1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(deconv1)\n    \n    model = Model(input_layer, output_layer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bdfe0f4aa08acb4632954915876e4fbcfe7c895"},"cell_type":"markdown","source":"<h2>Data loading and preprocessing<h2>\n\nI set as data directory the output directory of another kernel of mine in which I store for convenience all data including images as HF5 files. Loading this files just takes a few seconds (5 s on average), instead of several minutes to load the CSV and images every time. <a href=\"https://www.kaggle.com/jcesquiveld/tgs-reading-data-and-storing-in-hf5\">https://www.kaggle.com/jcesquiveld/tgs-reading-data-and-storing-in-hf5</a>.\n\nTo see how to access data generated as output in another kernel see: <a href=\"https://www.kaggle.com/product-feedback/45472\">https://www.kaggle.com/product-feedback/45472</a>."},{"metadata":{"trusted":true,"_uuid":"07c6e3c94c1d1191eab7a895ec9157b8b986ea32"},"cell_type":"code","source":"%%time\nDATA_DIR = '../input/tgs-reading-data-and-storing-in-hf5/'\ntrain = pd.read_hdf(DATA_DIR + 'tgs_salt.h5', key='train')\ntest = pd.read_hdf(DATA_DIR + 'tgs_salt.h5', key='test')\nsubmission = pd.read_hdf(DATA_DIR + 'tgs_salt.h5', key='submission')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37eea132a0b3e6742731bade4af7682e61b0fd5b"},"cell_type":"markdown","source":"The preprocessing step is pretty sloww, so I might include it in the HF5 data file."},{"metadata":{"_uuid":"e0b3efdc92aaa971c723ffeb55157d8d17d6b936","trusted":true,"collapsed":true},"cell_type":"code","source":"# Resize images to (128,128,1) and clip pixel values to [0,1]\nimages_resized = train.images.map(upsize)\nmasks_resized = train.masks.map(upsize)\nX = np.stack(images_resized) / 255\ny = np.stack(masks_resized) / 255","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"563890e367bb9bfd7c856dcb97eeac9e299485ef"},"cell_type":"markdown","source":"<h2>Basic data augmentation</h2>\n\nI'll try basic data augmentation by just flipping the images horizontally."},{"metadata":{"trusted":true,"_uuid":"cfecc7bcf4f8923a4a56ebf302630072fa3d1050","collapsed":true},"cell_type":"code","source":"X_aug = np.concatenate((X, [np.fliplr(img) for img in X]), axis=0)\ny_aug = np.concatenate((y, [np.fliplr(img) for img in y]), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"418ebc55b0f2b651f24b6b7833c679fb9c86cb0d","collapsed":true},"cell_type":"code","source":"# Split the train data into actual train data and validation data\n# train_test_split already shuffles data by default, so no need to do it\n\nX_train, X_val, y_train, y_val = train_test_split(X_aug, y_aug, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89435685f6d52abd0c1426965053c04a9309c10a"},"cell_type":"markdown","source":"<h2>Build and save the model</h2>"},{"metadata":{"trusted":true,"_uuid":"8f5236510f4333a334c9afc85db594e3d20ded84","collapsed":true},"cell_type":"code","source":"# Build and save model in JSON format\n\njson_filename = 'unet_salt_{}.json'.format(version)\n\nmodel = build_model(start_neurons=8, bn=True, dropout=0.05)\nmodel_json = model.to_json()\nwith open(json_filename, 'w') as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91b774b12d5d7460e30b2b56a9412997561db8b9"},"cell_type":"markdown","source":"<h2>Training</h2>\n\nDuring training, the best model will be saved with the callback <code>ModelCheckpoint</code>.  To pass what metric to the checker, pass its name to the <code>monitor</code> attribute (you can see the metrics used by the model with <code>model.metrics__names</code>. If it is from the validation set, prefix the name with <code>val_</code>. If you want to maximize or minimize this metric, use <code>mode='min'</code> or <code>mode='max'</code>."},{"metadata":{"_uuid":"c2e681b1382f69951821c9939b59f1f896cf33df"},"cell_type":"markdown","source":"<h2>Loading previous weights (optional)</h2>\nIf we want our model to load weights from a previous training (best model), set <code>load_previous_weights = True</code> and execute the next cell."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"db264274a76432f7d9cb92eb2d03a0c3500ef88f"},"cell_type":"code","source":"load_previous_weights = False\nif (load_previous_weights):\n    # Restore the model \n    weights_filename = 'unet_salt_weights_{}.h5'.format(version)\n    model.load_weights(weights_filename)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"291e7e8c99f2404c807b63fc26dec159e5297b64","scrolled":false,"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='min', verbose=1)\nweights_filename = 'unet_salt_weights_{}.h5'.format(version)\ncheckpoint = ModelCheckpoint(weights_filename, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\noptimizer = SGD(lr=0.2, momentum=0.8, decay=0.001, nesterov=False)\n#optimizer = Adam(lr=0.01)\n\nmodel.compile(optimizer=optimizer, loss=iou_loss, metrics=['accuracy', competition_metric])\nhistory = model.fit(X_train, y_train, batch_size=16, validation_data = [X_val, y_val], \n                    epochs=1, callbacks=[checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d01afe96ab7609b584d59e0612d143c68f95080"},"cell_type":"code","source":"# Let's see how the model performs (last model after training, not the saved best one)\n\n# On the train set\nprint('*** Last model on train set ***')\nmodel.evaluate(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c64c6b57ec55f87f0333e2a4ec9d04f5c0779994"},"cell_type":"code","source":"# On the validation set\nprint('*** Last model on val set ***')  \nmodel.evaluate(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc9fd2a522c538dbe2a4bfebc4719c446710be9d","trusted":true},"cell_type":"code","source":"print(model.metrics_names)\n\nfigure, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,5))\n\n# summarize history for loss\nax1.plot(history.history['loss'])\nax1.plot(history.history['val_loss'])\nax1.grid(True)\nax1.set_title('LOSS')\nax1.set_ylabel('loss')\nax1.set_xlabel('epoch')\nax1.legend(['train', 'validation'], loc='upper left')\n\n# summarize history for accuracy\nax2.plot(history.history['acc'])\nax2.plot(history.history['val_acc'])\nax2.grid(True)\nax2.set_title('ACCURACY')\nax2.set_ylabel('accuracy')\nax2.set_xlabel('epoch')\nax2.legend(['train', 'validation'], loc='upper left')\n\n# summarize history for competition metric\nax3.plot(history.history['competition_metric'])\nax3.plot(history.history['val_competition_metric'])\nax3.grid(True)\nax3.set_title('COMPETITION METRIC')\nax3.set_ylabel('competition metric')\nax3.set_xlabel('epoch')\nax3.legend(['train', 'validation'], loc='upper left')\n\n# Save image for reports\nplt.savefig('history_unet_salt_{}.png'.format(version))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}