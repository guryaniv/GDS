{"cells": [{"source": ["# **0.29 LB Score + Beginner NLP Tutorial**"], "cell_type": "markdown", "metadata": {"_cell_guid": "f5d98a99-0ce2-4c2c-90db-276451d24dd1", "_uuid": "b0f30f1df9d61f870994c75a8b28b85b67d35611"}}, {"source": ["> ## Introduction\n", "\n", "This is my first Kaggle Competition on NLP. This notebook is heavily inspired from the kernels of \n", "1. abhishek : https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n", "2. nzw0301 : https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31\n", "3. sudalairajkumar : https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author\n", "Thank you for your brilliant kernels and tutorials.\n", "\n", "Approach followed :\n", "1. Simple Feature Engineering - Punctuation,Stop Words,Glove Sentence vectors\n", "2. Creating stack features from\n", " - Simple Features such as  tfidf and count vectors for words and chars and applying multinomial naive bayes(mnb)- tfidf+words+mnb,tfidf+chars+mnb,count+words+mnb,count+chars+mnb\n", " - Conv Nets on keras texttosequence, NNs on glove sentence vectors and Fast Text\n", "3. XGBoost, which is the final model which will use the simple features and stack features as input\n", "\n", "Five Fold Crossvalidation is used in all the models along with early stopping for those involving epochs.\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "767f48ca-574b-4db6-91b4-9812599d5f40", "_uuid": "bcdfdea94c61aea5d1078e00e7062683e8761e33"}}, {"source": ["> ## Imports\n", "\n", "In particular, the important ones are scikit and keras which will deal with the data modelling and feature extraction"], "cell_type": "markdown", "metadata": {"_cell_guid": "1d7a3e12-6514-4169-9c26-28b0b3baf847", "_uuid": "6d0924303ccb6cdfc67fd97652288f6ceb8b7b26"}}, {"source": ["# Imports\n", "import pandas as pd\n", "import sys\n", "import glob\n", "import errno\n", "import csv\n", "import numpy as np\n", "from nltk.corpus import stopwords\n", "import re\n", "import nltk.data\n", "import nltk\n", "import os\n", "from collections import OrderedDict\n", "from subprocess import check_call\n", "from shutil import copyfile\n", "from sklearn.metrics import log_loss\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import mpld3\n", "mpld3.enable_notebook()\n", "import seaborn as sns\n", "from collections import Counter\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn import ensemble, metrics, model_selection, naive_bayes\n", "from sklearn.preprocessing import LabelEncoder\n", "import xgboost as xgb\n", "from tqdm import tqdm\n", "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n", "from keras.layers import GlobalAveragePooling1D,Merge,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\n", "from keras.preprocessing import sequence, text\n", "from keras.callbacks import EarlyStopping\n", "from nltk import word_tokenize\n", "from keras.layers.merge import concatenate\n", "from keras.utils import np_utils\n", "from keras.models import Sequential\n", "from keras.layers.recurrent import LSTM, GRU\n", "from keras.layers.core import Dense, Activation, Dropout\n", "from keras.layers.embeddings import Embedding\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras import initializers\n", "from keras import backend as K\n", "from sklearn.linear_model import SGDClassifier as sgd\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.callbacks import EarlyStopping"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "dc5b9252-7d0b-49b7-819b-bd323d20043b", "_uuid": "ed5c80c84c14e92ff4de5e4f3f0211778a04e299", "collapsed": true}, "execution_count": null}, {"source": ["> ## Read data"], "cell_type": "markdown", "metadata": {"_cell_guid": "d213e432-a134-4d85-8b88-16fed3db8a26", "_uuid": "6fa28aaff53727a340d68331a4d267c0b3c98e53"}}, {"source": ["# Read data\n", "train = \"../input/spooky-author-identification/train.csv\"\n", "test = \"../input/spooky-author-identification/test.csv\"\n", "wv = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\n", "X_train = pd.read_csv( train, header=0,delimiter=\",\" )\n", "X_test = pd.read_csv( test, header=0,delimiter=\",\" )\n", "\n", "authors = ['EAP','MWS','HPL']\n", "Y_train = LabelEncoder().fit_transform(X_train['author'])"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "b81d83fe-af27-45e4-a877-88701472233f", "_uuid": "ae0cc89a2c64db19c2526960a9e7ea5b2f3e207d", "collapsed": true}, "execution_count": null}, {"source": [">## Clean Data\n", "\n", "This is to extract the pure words from the texts"], "cell_type": "markdown", "metadata": {"_cell_guid": "57b0752e-87d9-4753-bcae-e3d490ea724a", "_uuid": "160fb283cd791532c99c571f719536eed5a44ddb"}}, {"source": ["# Clean data\n", "def clean(X_train,X_test):\n", "    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n", "    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n", "    return X_train,X_test\n", "X_train,X_test = clean(X_train,X_test)\n"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "8a57b8ce-f62f-4384-9ede-9b36d5bd6f3a", "_uuid": "5823c0a3cb183422e3babf23204c3440ef8e9aed", "collapsed": true}, "execution_count": null}, {"source": ["> # Simple Feature Engineering\n", "\n", "I am using viz. three types of features\n", "1. Punctuations - Each author favours a particular type(s) of punctuation. Hence I've taken sets of punctuations which are similar in behaviour. They are \";:\" , \",.\" , \"?\" , \"'\" , \"\"\" and the combined set of all of these.\n", "2. Stop word percentage- Stop word percentage of each text \n", "\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "206c8c20-4b47-406c-b58b-6d8cd0214d02", "_uuid": "c1001d9ef26eeb9863b26e21cb910e886baa702c"}}, {"source": ["# Feature Engineering\n", "# Punctuation\n", "punctuations = [{\"id\":1,\"p\":\"[;:]\"},{\"id\":2,\"p\":\"[,.]\"},{\"id\":3,\"p\":\"[?]\"},{\"id\":4,\"p\":\"[\\']\"},{\"id\":5,\"p\":\"[\\\"]\"},{\"id\":6,\"p\":\"[;:,.?\\'\\\"]\"}]\n", "for p in punctuations:\n", "    punctuation = p[\"p\"]\n", "    _train =  [ sentence.split() for sentence in X_train['text'] ]\n", "    X_train['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _train]    \n", "\n", "    _test =  [ sentence.split() for sentence in X_test['text'] ]\n", "    X_test['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _test]    \n", "\n"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "b75a91dd-84e8-432b-96d2-8fd440eeda58", "_uuid": "d0dcf1ed3522a379795823c1d04aa3ed1c15c8a3", "collapsed": true}, "execution_count": null}, {"source": ["# Feature Engineering\n", "# Stop Words\n", "_dist_train = [x for x in X_train['words']]\n", "X_train['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_train]\n", "\n", "_dist_test = [x for x in X_test['words']]\n", "X_test['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_test]    "], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "d4683ee4-4d6c-4579-a114-1a2506466ba5", "_uuid": "a146530189fa92a757865581cd2fbbe4d25e4ae6", "collapsed": true}, "execution_count": null}, {"source": ["> ## Stack Features\n", "\n", "Thanks sudalairajkumar, for explaining this technique.\n", "\n", "Stack features are features which are in itself predictions from existing features. Existing features which have many dimensions are good usecases to try out feature stacking on.\n", "\n", "The motivation for using stack features is:\n", "1. To create features with respect to documents such as tfidf and counts, both word based and character based.\n", "2. To reduce the dimensionality of the huge feature vectors of the above so as to fit the data better.\n", "3. Multinomial Bayes works well due to the multinomial distribution and strong independence assumptions in the model.\n", "\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "20413657-fe05-4eea-b4b5-1aac0728eb63", "_uuid": "1cb4be139a9c38085f96f6b2aaaa15dd974589d2"}}, {"source": ["# Feature Engineering\n", "# tfidf - words - nb\n", "def tfidfWords(X_train,X_test):\n", "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n", "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n", "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n", "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n", "    return train_tfidf,test_tfidf,full_tfidf\n", "    \n", "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n", "    model = naive_bayes.MultinomialNB()\n", "    model.fit(train_X, train_y)\n", "    pred_test_y = model.predict_proba(test_X)\n", "    pred_test_y2 = model.predict_proba(test_X2)\n", "    return pred_test_y, pred_test_y2, model\n", "\n", "def do_tfidf_MNB(X_train,X_test,Y_train):\n", "    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([X_train.shape[0], 3])\n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "    for dev_index, val_index in kf.split(X_train):\n", "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n", "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n", "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    print(\"Mean cv score : \", np.mean(cv_scores))\n", "    pred_full_test = pred_full_test / 5.\n", "    return pred_train,pred_full_test\n", "\n", "pred_train,pred_test = do_tfidf_MNB(X_train,X_test,Y_train)\n", "X_train[\"tfidf_words_nb_eap\"] = pred_train[:,0]\n", "X_train[\"tfidf_words_nb_hpl\"] = pred_train[:,1]\n", "X_train[\"tfidf_words_nb_mws\"] = pred_train[:,2]\n", "X_test[\"tfidf_words_nb_eap\"] = pred_test[:,0]\n", "X_test[\"tfidf_words_nb_hpl\"] = pred_test[:,1]\n", "X_test[\"tfidf_words_nb_mws\"] = pred_test[:,2]"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "fd55f994-9a63-42ef-bf22-329ac518aaba", "_uuid": "dc67057a457e2ce9fbc5d63881039616164362ac", "collapsed": true}, "execution_count": null}, {"source": ["# Feature Engineering\n", "# tfidf - chars - nb\n", "def tfidfWords(X_train,X_test):\n", "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n", "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n", "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n", "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n", "    return train_tfidf,test_tfidf\n", "    \n", "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n", "    model = naive_bayes.MultinomialNB()\n", "    model.fit(train_X, train_y)\n", "    pred_test_y = model.predict_proba(test_X)\n", "    pred_test_y2 = model.predict_proba(test_X2)\n", "    return pred_test_y, pred_test_y2, model\n", "\n", "def do(X_train,X_test,Y_train):\n", "    train_tfidf,test_tfidf = tfidfWords(X_train,X_test)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([X_train.shape[0], 3])\n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "    for dev_index, val_index in kf.split(X_train):\n", "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n", "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n", "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    print(\"Mean cv score : \", np.mean(cv_scores))\n", "    pred_full_test = pred_full_test / 5.\n", "    return pred_train,pred_full_test\n", "pred_train,pred_test = do(X_train,X_test,Y_train)\n", "X_train[\"tfidf_chars_nb_eap\"] = pred_train[:,0]\n", "X_train[\"tfidf_chars_nb_hpl\"] = pred_train[:,1]\n", "X_train[\"tfidf_chars_nb_mws\"] = pred_train[:,2]\n", "X_test[\"tfidf_chars_nb_eap\"] = pred_test[:,0]\n", "X_test[\"tfidf_chars_nb_hpl\"] = pred_test[:,1]\n", "X_test[\"tfidf_chars_nb_mws\"] = pred_test[:,2]"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "de63c172-df39-4bb9-a453-d0dcf3095977", "_uuid": "544570b7813f55003e70baa51e31ad53cfa83a51", "collapsed": true}, "execution_count": null}, {"source": ["# Feature Engineering\n", "# count - words - nb\n", "def countWords(X_train,X_test):\n", "    count_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n", "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n", "    train_count = count_vec.transform(X_train['text'].values.tolist())\n", "    test_count = count_vec.transform(X_test['text'].values.tolist())\n", "    return train_count,test_count\n", "    \n", "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n", "    model = naive_bayes.MultinomialNB()\n", "    model.fit(train_X, train_y)\n", "    pred_test_y = model.predict_proba(test_X)\n", "    pred_test_y2 = model.predict_proba(test_X2)\n", "    return pred_test_y, pred_test_y2, model\n", "\n", "def do_count_MNB(X_train,X_test,Y_train):\n", "    train_count,test_count=countWords(X_train,X_test)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([X_train.shape[0], 3])\n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "    for dev_index, val_index in kf.split(X_train):\n", "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n", "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n", "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    print(\"Mean cv score : \", np.mean(cv_scores))\n", "    pred_full_test = pred_full_test / 5.\n", "    return pred_train,pred_full_test\n", "\n", "pred_train,pred_test = do_count_MNB(X_train,X_test,Y_train)\n", "X_train[\"count_words_nb_eap\"] = pred_train[:,0]\n", "X_train[\"count_words_nb_hpl\"] = pred_train[:,1]\n", "X_train[\"count_words_nb_mws\"] = pred_train[:,2]\n", "X_test[\"count_words_nb_eap\"] = pred_test[:,0]\n", "X_test[\"count_words_nb_hpl\"] = pred_test[:,1]\n", "X_test[\"count_words_nb_mws\"] = pred_test[:,2]"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "22686d4f-f176-48a4-bdf3-ccc48c97c2ae", "_uuid": "0a0b350127e8095b863d4d498bf83a930ec77a03", "collapsed": true}, "execution_count": null}, {"source": ["# Feature Engineering\n", "# count - chars - nb\n", "def countChars(X_train,X_test):\n", "    count_vec = CountVectorizer(ngram_range=(1,7),analyzer='char')\n", "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n", "    train_count = count_vec.transform(X_train['text'].values.tolist())\n", "    test_count = count_vec.transform(X_test['text'].values.tolist())\n", "    return train_count,test_count\n", "    \n", "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n", "    model = naive_bayes.MultinomialNB()\n", "    model.fit(train_X, train_y)\n", "    pred_test_y = model.predict_proba(test_X)\n", "    pred_test_y2 = model.predict_proba(test_X2)\n", "    return pred_test_y, pred_test_y2, model\n", "\n", "def do_count_chars_MNB(X_train,X_test,Y_train):\n", "    train_count,test_count=countChars(X_train,X_test)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([X_train.shape[0], 3])\n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "    for dev_index, val_index in kf.split(X_train):\n", "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n", "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n", "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    print(\"Mean cv score : \", np.mean(cv_scores))\n", "    pred_full_test = pred_full_test / 5.\n", "    return pred_train,pred_full_test\n", "\n", "pred_train,pred_test = do_count_chars_MNB(X_train,X_test,Y_train)\n", "X_train[\"count_chars_nb_eap\"] = pred_train[:,0]\n", "X_train[\"count_chars_nb_hpl\"] = pred_train[:,1]\n", "X_train[\"count_chars_nb_mws\"] = pred_train[:,2]\n", "X_test[\"count_chars_nb_eap\"] = pred_test[:,0]\n", "X_test[\"count_chars_nb_hpl\"] = pred_test[:,1]\n", "X_test[\"count_chars_nb_mws\"] = pred_test[:,2]"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "75994cc6-655d-46fd-a519-5ae41577792e", "_uuid": "a19ae104c208f554c167ec2d01cf94b037a3ecfd", "collapsed": true}, "execution_count": null}, {"source": ["> ## Creating sentence vectors from individual word vectors\n", "\n", "Thanks abhishek, for explaining this technique.\n", "sent2vec creates a normalized vector for the whole sentence from Glove embeddings. This works better than simply averaging or summing up the individual word vectors to obtain sentence vectors.\n", "\n", "These are simple features."], "cell_type": "markdown", "metadata": {"_cell_guid": "02c65896-4c23-4e32-b7f7-970fcf5fbdb1", "_uuid": "bb5335590476c9eba3a29719be5ff4ddb5fed5e8"}}, {"source": ["# load the GloVe vectors in a dictionary:\n", "\n", "def loadWordVecs():\n", "    embeddings_index = {}\n", "    f = open(wv)\n", "    for line in f:\n", "        values = line.split()\n", "        word = values[0]\n", "        coefs = np.asarray(values[1:], dtype='float32')\n", "        embeddings_index[word] = coefs\n", "    f.close()\n", "    print('Found %s word vectors.' % len(embeddings_index))\n", "    return embeddings_index\n", "\n", "def sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n", "    words = str(s).lower()\n", "    words = word_tokenize(words)\n", "    words = [w for w in words if not w in stopwords.words('english')]\n", "    words = [w for w in words if w.isalpha()]\n", "    M = []\n", "    for w in words:\n", "        try:\n", "            M.append(embeddings_index[w])\n", "        except:\n", "            continue\n", "    M = np.array(M)\n", "    v = M.sum(axis=0)\n", "    if type(v) != np.ndarray:\n", "        return np.zeros(100)\n", "    return v / np.sqrt((v ** 2).sum())\n", "\n", "def doGlove(x_train,x_test):\n", "    embeddings_index = loadWordVecs()\n", "    # create sentence vectors using the above function for training and validation set\n", "    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n", "    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n", "    xtrain_glove = np.array(xtrain_glove)\n", "    xtest_glove = np.array(xtest_glove)\n", "    return xtrain_glove,xtest_glove,embeddings_index\n", "\n", "glove_vecs_train,glove_vecs_test,embeddings_index = doGlove(X_train['text'],X_test['text'])\n", "X_train[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_train.tolist())\n", "X_test[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_test.tolist())\n", "\n"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "6d4fd041-8719-4924-b1da-0a1bd51456e4", "_uuid": "30874cc591fe99222b9aa1b4afaaef9bb7359d7d", "collapsed": true}, "execution_count": null}, {"source": ["> ## Using Neural Networks and Facebook's Fasttext\n", "\n", "1. NN => Conv Net which uses keras text to sequences to obtain input features\n", "2. NN_Glove =>  Simple Neural Net which uses sentence glove features\n", "3. Fasttext => Uses fasttext implementation via keras. Thanks nzw0301, for explaining this technique\n", "\n", "All of the above are used to obtain stack features."], "cell_type": "markdown", "metadata": {"_cell_guid": "ca087c6a-98b7-45b8-a6dd-be44e8a98e53", "_uuid": "f505cda13922f67ca448492497a18d7bbde3ef79"}}, {"source": ["# Using Neural Networks and Facebook's Fasttext\n", "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n", "\n", "# NN\n", "def doAddNN(X_train,X_test,pred_train,pred_test):\n", "    X_train[\"nn_eap\"] = pred_train[:,0]\n", "    X_train[\"nn_hpl\"] = pred_train[:,1]\n", "    X_train[\"nn_mws\"] = pred_train[:,2]\n", "    X_test[\"nn_eap\"] = pred_test[:,0]\n", "    X_test[\"nn_hpl\"] = pred_test[:,1]\n", "    X_test[\"nn_mws\"] = pred_test[:,2]\n", "    return X_train,X_test\n", "\n", "def initNN(nb_words_cnt,max_len):\n", "    model = Sequential()\n", "    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n", "    model.add(Dropout(0.3))\n", "    model.add(Conv1D(64,\n", "                     5,\n", "                     padding='valid',\n", "                     activation='relu'))\n", "    model.add(Dropout(0.3))\n", "    model.add(MaxPooling1D())\n", "    model.add(Flatten())\n", "    model.add(Dense(800, activation='relu'))\n", "    model.add(Dropout(0.5))\n", "    model.add(Dense(3, activation='softmax'))\n", "\n", "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n", "    return model\n", "\n", "def doNN(X_train,X_test,Y_train):\n", "    max_len = 70\n", "    nb_words = 10000\n", "    \n", "    print('Processing text dataset')\n", "    texts_1 = []\n", "    for text in X_train['text']:\n", "        texts_1.append(text)\n", "\n", "    print('Found %s texts.' % len(texts_1))\n", "    test_texts_1 = []\n", "    for text in X_test['text']:\n", "        test_texts_1.append(text)\n", "    print('Found %s texts.' % len(test_texts_1))\n", "    \n", "    tokenizer = Tokenizer(num_words=nb_words)\n", "    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n", "    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n", "    word_index = tokenizer.word_index\n", "    print('Found %s unique tokens.' % len(word_index))\n", "\n", "    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n", "\n", "    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n", "    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n", "    del test_sequences_1\n", "    del sequences_1\n", "    nb_words_cnt = min(nb_words, len(word_index)) + 1\n", "\n", "    # we need to binarize the labels for the neural net\n", "    ytrain_enc = np_utils.to_categorical(Y_train)\n", "    \n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n", "    for dev_index, val_index in kf.split(xtrain_pad):\n", "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n", "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n", "        model = initNN(nb_words_cnt,max_len)\n", "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n", "        pred_val_y = model.predict(val_X)\n", "        pred_test_y = model.predict(xtest_pad)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n", "\n", "## NN Glove\n", "\n", "def doAddNN_glove(X_train,X_test,pred_train,pred_test):\n", "    X_train[\"nn_glove_eap\"] = pred_train[:,0]\n", "    X_train[\"nn_glove_hpl\"] = pred_train[:,1]\n", "    X_train[\"nn_glove_mws\"] = pred_train[:,2]\n", "    X_test[\"nn_glove_eap\"] = pred_test[:,0]\n", "    X_test[\"nn_glove_hpl\"] = pred_test[:,1]\n", "    X_test[\"nn_glove_mws\"] = pred_test[:,2]\n", "    return X_train,X_test\n", "\n", "def initNN_glove():\n", "    # create a simple 3 layer sequential neural net\n", "    model = Sequential()\n", "\n", "    model.add(Dense(128, input_dim=100, activation='relu'))\n", "    model.add(Dropout(0.3))\n", "    model.add(BatchNormalization())\n", "\n", "    model.add(Dense(128, activation='relu'))\n", "    model.add(Dropout(0.3))\n", "    model.add(BatchNormalization())\n", "\n", "    model.add(Dense(3))\n", "    model.add(Activation('softmax'))\n", "\n", "    # compile the model\n", "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n", "    return model\n", "\n", "def doNN_glove(X_train,X_test,Y_train,xtrain_glove,xtest_glove):\n", "    # scale the data before any neural net:\n", "    scl = preprocessing.StandardScaler()\n", "    ytrain_enc = np_utils.to_categorical(Y_train)\n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    xtrain_glove = scl.fit_transform(xtrain_glove)\n", "    xtest_glove = scl.fit_transform(xtest_glove)\n", "    pred_train = np.zeros([xtrain_glove.shape[0], 3])\n", "    \n", "    for dev_index, val_index in kf.split(xtrain_glove):\n", "        dev_X, val_X = xtrain_glove[dev_index], xtrain_glove[val_index]\n", "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n", "        model = initNN_glove()\n", "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=10, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n", "        pred_val_y = model.predict(val_X)\n", "        pred_test_y = model.predict(xtest_glove)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "    return doAddNN_glove(X_train,X_test,pred_train,pred_full_test/5)\n", "\n", "# Fast Text\n", "\n", "def doAddFastText(X_train,X_test,pred_train,pred_test):\n", "    X_train[\"ff_eap\"] = pred_train[:,0]\n", "    X_train[\"ff_hpl\"] = pred_train[:,1]\n", "    X_train[\"ff_mws\"] = pred_train[:,2]\n", "    X_test[\"ff_eap\"] = pred_test[:,0]\n", "    X_test[\"ff_hpl\"] = pred_test[:,1]\n", "    X_test[\"ff_mws\"] = pred_test[:,2]\n", "    return X_train,X_test\n", "\n", "\n", "def initFastText(embedding_dims,input_dim):\n", "    model = Sequential()\n", "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n", "    model.add(GlobalAveragePooling1D())\n", "    model.add(Dense(3, activation='softmax'))\n", "\n", "    model.compile(loss='categorical_crossentropy',\n", "                  optimizer='adam',\n", "                  metrics=['accuracy'])\n", "    return model\n", "\n", "def preprocessFastText(text):\n", "    text = text.replace(\"' \", \" ' \")\n", "    signs = set(',.:;\"?!')\n", "    prods = set(text) & signs\n", "    if not prods:\n", "        return text\n", "\n", "    for sign in prods:\n", "        text = text.replace(sign, ' {} '.format(sign) )\n", "    return text\n", "\n", "def create_docs(df, n_gram_max=2):\n", "    def add_ngram(q, n_gram_max):\n", "            ngrams = []\n", "            for n in range(2, n_gram_max+1):\n", "                for w_index in range(len(q)-n+1):\n", "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n", "            return q + ngrams\n", "        \n", "    docs = []\n", "    for doc in df.text:\n", "        doc = preprocessFastText(doc).split()\n", "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n", "    \n", "    return docs\n", "\n", "def doFastText(X_train,X_test,Y_train):\n", "    min_count = 2\n", "\n", "    docs = create_docs(X_train)\n", "    tokenizer = Tokenizer(lower=False, filters='')\n", "    tokenizer.fit_on_texts(docs)\n", "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n", "\n", "    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n", "    tokenizer.fit_on_texts(docs)\n", "    docs = tokenizer.texts_to_sequences(docs)\n", "\n", "    maxlen = 300\n", "\n", "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n", "    input_dim = np.max(docs) + 1\n", "    embedding_dims = 20\n", "\n", "    # we need to binarize the labels for the neural net\n", "    ytrain_enc = np_utils.to_categorical(Y_train)\n", "\n", "    docs_test = create_docs(X_test)\n", "    docs_test = tokenizer.texts_to_sequences(docs_test)\n", "    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n", "    xtrain_pad = docs\n", "    xtest_pad = docs_test\n", "    \n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n", "    for dev_index, val_index in kf.split(xtrain_pad):\n", "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n", "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n", "        model = initFastText(embedding_dims,input_dim)\n", "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=25, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n", "        pred_val_y = model.predict(val_X)\n", "        pred_test_y = model.predict(docs_test)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "    return doAddFastText(X_train,X_test,pred_train,pred_full_test/5)\n", "\n", "X_train,X_test = doFastText(X_train,X_test,Y_train)\n", "X_train,X_test = doNN(X_train,X_test,Y_train)\n", "X_train,X_test = doNN_glove(X_train,X_test,Y_train,glove_vecs_train,glove_vecs_test)\n"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "3da63bdc-d29e-4e29-b687-8c2090ec4206", "_uuid": "f3155dca771fadc880140e85756cc2bef4a70099", "collapsed": true}, "execution_count": null}, {"source": ["> ## Final model \n", "\n", "Uses XGBoost which contains all the simple and stack features as input"], "cell_type": "markdown", "metadata": {"_cell_guid": "729ee8c9-0cbf-4aba-9937-e0820afe502a", "_uuid": "3013562a3a0ad8e8981a0c9f4f1761acd748696b"}}, {"source": ["# Final Model\n", "# XGBoost\n", "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n", "    param = {}\n", "    param['objective'] = 'multi:softprob'\n", "    param['eta'] = 0.1\n", "    param['max_depth'] = 3\n", "    param['silent'] = 1\n", "    param['num_class'] = 3\n", "    param['eval_metric'] = \"mlogloss\"\n", "    param['min_child_weight'] = child\n", "    param['subsample'] = 0.8\n", "    param['colsample_bytree'] = colsample\n", "    param['seed'] = seed_val\n", "    num_rounds = 2000\n", "\n", "    plst = list(param.items())\n", "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n", "\n", "    if test_y is not None:\n", "        xgtest = xgb.DMatrix(test_X, label=test_y)\n", "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n", "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n", "    else:\n", "        xgtest = xgb.DMatrix(test_X)\n", "        model = xgb.train(plst, xgtrain, num_rounds)\n", "\n", "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n", "    if test_X2 is not None:\n", "        xgtest2 = xgb.DMatrix(test_X2)\n", "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n", "    return pred_test_y, pred_test_y2, model\n", "\n", "def do(X_train,X_test,Y_train):\n", "    drop_columns=[\"id\",\"text\",\"words\"]\n", "    x_train = X_train.drop(drop_columns+['author'],axis=1)\n", "    x_test = X_test.drop(drop_columns,axis=1)\n", "    y_train = Y_train\n", "    \n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([x_train.shape[0], 3])\n", "    for dev_index, val_index in kf.split(x_train):\n", "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n", "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n", "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    print(\"cv scores : \", cv_scores)\n", "    return pred_full_test/5\n", "result = do(X_train,X_test,Y_train)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "dee100c6-9d47-4919-9385-ca66ab7b5e87", "_uuid": "598a66e5c0c4d9cf3b303cf54bfca2dcf08a30c3", "collapsed": true}, "execution_count": null}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "mimetype": "text/x-python", "version": "3.6.3", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}, "anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1}