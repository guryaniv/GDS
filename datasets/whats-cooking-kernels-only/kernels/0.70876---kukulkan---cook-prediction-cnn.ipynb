{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"train_df = pd.read_json('../input/train.json')\ndisplay(train_df)\ntest_df = pd.read_json('../input/test.json')\ndisplay(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8505900a06dc37351aa72cd438b9c4b103ea1c45"},"cell_type":"code","source":"cuisine_list = []\ningredient_list = []\ningredient_list2 = []\nvocabulary = {}\nvocabulary_inv = {}\ningredient_num_list = []\ningredient_len_list = []\nvocabulary[\"PADDING\"] = 0\nvocabulary_inv[0] = \"PADDING\"\nfor i,row in train_df.iterrows():\n    cuisine = row[0]\n    if(not cuisine in cuisine_list):\n        cuisine_list.append(cuisine)\n    ingredient_list = row[2]\n    ingredient_num_list.append(len(ingredient_list))\n    for ingredient in ingredient_list:\n        ingredient_len_list.append(len(ingredient.split()))\n        if(not ingredient.lower() in ingredient_list):\n            ingredient_list.append(ingredient.lower())\n        for ingredient_part in ingredient.split():\n            if(not ingredient_part.lower() in ingredient_list2):\n                ingredient_list2.append(ingredient_part.lower())\n                vocabulary[ingredient_part.lower()] = len(vocabulary)\n                vocabulary_inv[len(vocabulary_inv)] = ingredient_part.lower()\n#===\nfor i,row in test_df.iterrows():\n    ingredient_list = row[1]\n    ingredient_num_list.append(len(ingredient_list))\n    for ingredient in ingredient_list:\n        ingredient_len_list.append(len(ingredient.split()))\n        if(not ingredient in ingredient_list):\n            print(\"total =\",ingredient_list)\n            print(ingredient,\" is not contained!\")\n            ingredient_list.append(ingredient)\n        for ingredient_part in ingredient.split():\n                if(not ingredient_part.lower() in ingredient_list2):\n                    print(ingredient_part,\" is not contained as part!\")\n                    ingredient_list2.append(ingredient_part.lower())\n                    vocabulary[ingredient_part.lower()] = len(vocabulary)\n                    vocabulary_inv[len(vocabulary_inv)] = ingredient_part.lower()\nassert len(cuisine_list)==len(set(cuisine_list)),\"cuisine_list duplicated!\"\nassert len(ingredient_list)==len(set(ingredient_list)),\"ingredient_list duplicated!\"\nassert len(ingredient_list2)==len(set(ingredient_list2)),\"ingredient_list2 duplicated!\"\nprint(\"cuisine_set =\",cuisine_list)\nprint(\"max ingredient num =\",max(ingredient_num_list))\nmax_ingredient_num = max(ingredient_num_list)\nprint(\"max ingredient len =\",max(ingredient_len_list))\nmax_ingredient_len = max(ingredient_len_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bd163ce179c9644f239545f139cc9158893ebec"},"cell_type":"code","source":"trainX = []\ntrainY = []\nfor i,row in train_df.iterrows():\n    cuisine = row[0]\n    cuisine_label = np.zeros(len(cuisine_list))\n    cuisine_label[cuisine_list.index(cuisine)] = 1\n    trainY.append(cuisine_label)\n    #===\n    ingredient_list = row[2]\n    ingredient_token_list = []\n    for j in range(max_ingredient_num):\n        ingredient_part_token_list = []\n        if(j<len(ingredient_list)):\n            ingredient = ingredient_list[j]\n            ingredient_part_split = ingredient.split()\n            for k in range(max_ingredient_len):\n                if(k<len(ingredient_part_split)):\n                    ingredient_part_token_list.append(vocabulary[ingredient_part_split[k].lower()])\n                else:\n                    ingredient_part_token_list.append(0)\n        else:\n            for k in range(max_ingredient_len):\n                ingredient_part_token_list.append(0)\n        ingredient_token_list.append(ingredient_part_token_list)\n    #==\n    trainX.append(ingredient_token_list)\n#==\ntestX = []\nfor i,row in test_df.iterrows():\n    ingredient_list = row[1]\n    ingredient_token_list = []\n    for j in range(max_ingredient_num):\n        ingredient_part_token_list = []\n        if(j<len(ingredient_list)):\n            ingredient = ingredient_list[j]\n            ingredient_part_split = ingredient.split()\n            for k in range(max_ingredient_len):\n                if(k<len(ingredient_part_split)):\n                    ingredient_part_token_list.append(vocabulary[ingredient_part_split[k].lower()])\n                else:\n                    ingredient_part_token_list.append(0)\n        else:\n            for k in range(max_ingredient_len):\n                ingredient_part_token_list.append(0)\n        ingredient_token_list.append(ingredient_part_token_list)\n    #==\n    testX.append(ingredient_token_list)\n#==\ntrainX = np.array(trainX)\nprint(np.shape(trainX))\ntrainY = np.array(trainY)\nprint(np.shape(trainY))\ntestX = np.array(testX)\nprint(np.shape(testX))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebb5205dd838fe2c2d57f272eeceef33838c9b13"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#(trainX, evalX, trainY, evalY) = train_test_split(trainX,trainY, test_size=0.1, shuffle=False)\ntrainX2 = np.split(trainX, max_ingredient_num, axis=1)\ntrainX2 =  np.reshape(trainX2, (len(trainX2),len(trainX2[0]),max_ingredient_len))\n#evalX2 = np.split(evalX, max_ingredient_num, axis=1)\n#evalX2 =  np.reshape(evalX2, (len(evalX2),len(evalX2[0]),max_ingredient_len))\ntestX2 = np.split(testX, max_ingredient_num, axis=1)\ntestX2 =  np.reshape(testX2, (len(testX2),len(testX2[0]),max_ingredient_len))\nprint(np.shape(trainX),np.shape(testX),np.shape(trainX2),np.shape(testX2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b82db1f2de2c44d262dd9c82b59d092df7b27dc"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding, LSTM,Bidirectional\nfrom keras.layers.merge import Concatenate\n#==\nembedding_dim = 400\nfilter_sizes = (4, 8)\nnum_filters = 100\nhidden_dims = 300\n#==\ninput_shape = (max_ingredient_len,)\ninput_blocks = []\nz_blocks = []\nConverter = Embedding(len(vocabulary), embedding_dim, input_length=max_ingredient_len,mask_zero=True)\nConvo = Convolution1D(filters=num_filters*2,kernel_size=filter_sizes[0],padding=\"valid\",activation=\"relu\",strides=1)\nPool = MaxPooling1D(pool_size=2)\nDrop1 = Dropout(0.5)\nBi_LSTM = Bidirectional(LSTM(num_filters,activation='softmax',recurrent_activation='tanh',use_bias=True,recurrent_dropout=0.0,return_sequences=False))\nDense1 = Dense(20)\nfor i in range(max_ingredient_num):\n    model_input = Input(shape=input_shape)\n    input_blocks.append(model_input)\n    z = Converter(model_input)\n    z = Drop1(z)\n    #conv = Convo(z)\n    #conv = Pool(conv)\n    pressed = Bi_LSTM(z)\n    dense = Dense1(pressed)\n    z_blocks.append(dense)\nconcatted = Concatenate()(z_blocks)\nconcatted = Dropout(0.5)(concatted)\nconcatted = Dense(hidden_dims)(concatted)\nmodel_output = Dense(len(cuisine_list),activation=\"softmax\")(concatted)\nmodel = Model(input_blocks,model_output)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8491a80cb3f4e03dd13795c62dcfd08cec2c5374","collapsed":true},"cell_type":"code","source":"import keras\nbatch_size = 256\nnum_epochs = 50\ncallbacks = []\ncallbacks.append(keras.callbacks.EarlyStopping('loss', min_delta=1e-3, patience=1))\ntrainX2_list = []\nfor train in trainX2:\n    trainX2_list.append(train)\nmodel.fit(trainX2_list,trainY,batch_size=batch_size, epochs=num_epochs,validation_split=0.25, verbose=1,callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd54f1a2c4e9cc6903dad7475edcfcd1f75b4fae","collapsed":true},"cell_type":"code","source":"from datetime import datetime\nnow_time = '{0:%Y_%m_%d_%H_%M_%S}'.format(datetime.now())\n#\ntestX2_list = []\nfor test in testX2:\n    testX2_list.append(test)\npredicted = model.predict(testX2_list)\ntest_id = test_df.id\nassert len(predicted)==len(test_id),\"Prediction num does not match!\"\nid_list = []\npred_cuisine_list = []\nfor food_id,pred in zip(test_id,predicted):\n    id_list.append(food_id)\n    max_index = pred.argmax()\n    pred_cuisine = cuisine_list[max_index]\n    pred_cuisine_list.append(pred_cuisine)\nfrom collections import OrderedDict\n#pred_dict = {\"id\":id_list,\"cuisine\":pred_cuisine_list}\npred_dict = OrderedDict([(\"id\",id_list), (\"cuisine\",pred_cuisine_list)])\npred_df = pd.DataFrame.from_dict(pred_dict)\npred_df.to_csv(\"submission\"+now_time+\".csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}