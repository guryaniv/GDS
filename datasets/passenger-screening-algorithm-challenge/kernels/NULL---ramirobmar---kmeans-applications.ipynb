{"cells": [{"outputs": [], "metadata": {"_uuid": "49dcf09247abfef871c28bb767936b60baa3036f", "collapsed": true, "_cell_guid": "5dcc3732-bef3-44a9-a9f7-c0031bdf493c"}, "execution_count": null, "cell_type": "code", "source": ["import sys\n", "import os\n", "import numpy as np\n", "from numpy import array, asarray, ma, zeros, sum\n", "from matplotlib import pyplot as plt\n", "import cv2\n", "import pandas as pd\n", "import seaborn as sns\n", "import scipy.stats as stats\n", "from scipy.cluster.vq import vq, kmeans, whiten, kmeans2\n", "from sklearn.cluster import DBSCAN\n", "from sklearn import metrics\n", "from sklearn.datasets.samples_generator import make_blobs\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.svm import LinearSVC\n", "from sklearn.datasets import make_classification"]}, {"outputs": [], "metadata": {"_uuid": "4979653104b147671f429590b937b1c5309abd9a", "_cell_guid": "1ddb84fa-49c6-4b2e-ab83-5b9fa9be0ac8"}, "execution_count": null, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import os\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "os.system('mkdir ./output')\n", "os.system('mkdir ./output/Threats')\n", "os.system('mkdir ./output/NoThreats')\n", "os.system('mkdir ./training')\n", "os.system('mkdir ./results')\n", "os.system('mkdir ./models')\n", "os.system('mkdir ./input')\n", "print(\"#################################\")\n", "print(\"Directory application structure\")\n", "print(\"#################################\")\n", "print(check_output([\"ls\", \"../input/datainput\"]).decode(\"utf8\"))\n", "print(check_output([\"ls\", \"../input/modelstraining\"]).decode(\"utf8\"))\n", "\n", "\n", "# Any results you write to the current directory are saved as output."]}, {"outputs": [], "metadata": {"_uuid": "9b4ace7f17f9d1471b08539b68bc5a940cdefba1", "collapsed": true, "_cell_guid": "cc483fe8-8fe8-46ab-a997-133808119ba2"}, "execution_count": null, "cell_type": "code", "source": ["############################################################################################################\n", "# Name: cluster_analisys\n", "# Autor: Ramiro Bueno Mart\u00ednez\n", "# Date: 27/11/2017\n", "##########################################################################################################\n", "def cluster_analisys(subset):\n", "\ttry:\n", "\t\twhitened = whiten(subset)\n", "\t\tcodebook,distortion = kmeans(whitened,5)\n", "\t\treturn distortion,whitened,codebook\n", "\texcept Exception as exception:\n", "\t\tprint (\"cluster_analisys: Excepcion {0}\".format(exception))"]}, {"outputs": [], "metadata": {"_uuid": "f3c1d4092c804a3b9df630e893a88933662779ec", "collapsed": true, "_cell_guid": "54930a93-06c9-466c-831e-b6d2e0951261"}, "execution_count": null, "cell_type": "code", "source": ["PATH_PHOTO_FILES = '.'\n", "NAME_FILE_MODEL_KMEANS = '../input/modelstraining/model_kmeans.csv'\n", "DATA_FOR_PREDICTION_FILE = '../input/datainput/0a83698bce92a6824dcc37c1d7fc31f5.csv'"]}, {"outputs": [], "metadata": {"_uuid": "cc11b623c77913c54873338c39b5608d678ac7d2", "collapsed": true, "_cell_guid": "ff3d8bf0-34f0-4900-a7ad-4e2f262735da"}, "execution_count": null, "cell_type": "code", "source": ["############################################################################################################\n", "# Name: creation_dataset_threats\n", "# Autor: Ramiro Bueno Mart\u00ednez\n", "# Date: 27/11/2017\n", "# Description: The goal of this function is the finding of possible anomalies in the different kind of\n", "# images generated with the body scannes, using clustering techniques based in the analysis of different\n", "# part of the body and different possition imagees\n", "############################################################################################################\n", "def creation_dataset_threats(dataset, FILENAME, highcontrast=False):\n", "\n", "\t\t# We have an image with a matrix structure of 660 rows by 512 colums\n", "\t\t# This function send this matrix to a algorithm to obtain different centroids and the distorsion\n", "\t\t# values that we will use to find possible anomalies \n", "\t\t#Initialization variables\n", "\t\tm_distortion = 0\n", "\t\tcol = 0\n", "\t\trow = 0\n", "\t\tnth = 0\n", "\t\t\n", "\t\t\n", "\t\toutput_dir = PATH_PHOTO_FILES + '/' + 'output'\n", "\t\tinput_dir = PATH_PHOTO_FILES + '/' + 'input'\n", "\t\t\n", "\t\t\n", "\t\tname_of_photo_file = input_dir + '/' + FILENAME\n", "\t\t\n", "\t\t\n", "\t\tprint (\"Starting clustering Analisys: {0}\".format(FILENAME))\n", "\t\t\n", "\t\t#Only they are going to analyse 4 Image Front-Behind-Left-Right\n", "\t\trange_images = [0,4,8,12]\n", "\t\ttry:\n", "\t\t\t\n", "\t\t\t#for nth in range(16):\n", "\t\t\tfor nth in range_images:\n", "\t\t\t\n", "\t\t\t\tan_img = get_single_image(name_of_photo_file, nth)  \t\t\t\t#returns the nth=3 image from the image stack\n", "\t\t\t\t\n", "\t\t\t\t#Prueba mejorando la resoluci\u00f3n de la imagen \n", "\t\t\t\tif highcontrast == True:\n", "\t\t\t\t\timg_rescaled = convert_to_grayscale(an_img)\n", "\t\t\t\t\tan_img = spread_spectrum(img_rescaled)\n", "\t\t\n", "\t\t\t\tdata_array = np.array(an_img)\n", "\t\t\t\tdistortion_array[nth,1],whitened,codebook= cluster_analisys(data_array)\n", "\t\t\t\tm_distortion = m_distortion + distortion_array[nth,1]\n", "\t\t\t\trecord = pd.DataFrame([[FILENAME, nth, distortion_array[nth,1],'nothreat']],columns=['Name','Nth','Distortion','Threat'])\n", "\t\t\t\tdataset = dataset.append(record,ignore_index=True)\n", "\t\t\t\tcol = col + 1\n", "\t\t\t\tif col % 4 == 0:\n", "\t\t\t\t\trow = row + 1\n", "\t\t\t\t\tcol = 0\n", "\t\t\t\tprint(\".\")\n", "\t\t\n", "\t\t\n", "\t\t\n", "\t\t\t#plt.show()\n", "\t\t\t#print(distortion_array)\n", "\t\t\t####################################################\n", "\t\t\t#Esta parte se puede optimizar con las funciones DataFrame.mean()\n", "\t\t\tmedia = 0\n", "\t\t\tmedia = m_distortion / 4\n", "\t\t\t#print (\"Mean Value of Distortion: {0} \\n\".format(media))\n", "\t\t\t############################################################################################\n", "\t\t\t# At this point we are finding possible anomalies of different distortion measurement with\n", "\t\t\t# respect to a threshold fixed before\n", "\t\t\t############################################################################################\n", "\t\t\tpercent = 17  # 17% of the mean value we suppose that its a wrong value \n", "\t\t\t############################################################################################\n", "\t\t\tthreshold_thread = media * percent / 100\n", "\t\t\tfor i in range(len(dataset)):\n", "\t\t\t\tif abs(media - dataset.loc[i,('Distortion')]) > threshold_thread: \n", "\t\t\t\t\tdataset.loc[i,('Threat')] = 'threat'\n", "\t\t\t\tif abs(media - dataset.loc[i,('Distortion')]) <= threshold_thread:\n", "\t\t\t\t\tdataset.loc[i,('Threat')] = 'nothreat'\n", "\t\t\t\t\t\n", "\t\texcept Exception as exception:\n", "\t\t\tprint (\"Function [creation_dataset_threats]: Excepcion {0}\".format(exception))\n", "\t\t\tsys.exit(1)\n", "\t\tfinally:\n", "\t\t\treturn dataset"]}, {"outputs": [], "metadata": {"_uuid": "591e1a1258f7a1700a99059875ffd1f5f721fa3f", "_cell_guid": "4d228864-dabb-43dd-9c1e-2297e5d15614"}, "execution_count": null, "cell_type": "code", "source": ["if __name__ == \"__main__\":\n", "\n", "\tprint (\"DATA_ANALISIS_TOOL: Initialization of the system...\")\n", "\ttry:\n", "\t\tfile_models = os.listdir(\"../input/modelstraining\")\n", "        if 'model_kmeans.csv' in file_models:\n", "\t\t\tprint('The training data has been created before .........')\n", "\t\telse:\n", "\t\t\tfiles = os.listdir('./training')\n", "############################################################################################\n", "#TRAINING MODEL SECTION\n", "############################################################################################\n", "\t\t\n", "\t\t\n", "\t\t\n", "\t\t\tdataset_kmeans = pd.DataFrame(columns=['Id','Name','Nth','Distortion','Difference','Mean','Threat'])\n", "\t\t\t\n", "\t\t\tfor name_file in files:\n", "\t\t\t\tdataset_kmeans = creation_dataset_threats(dataset_kmeans, name_file, highcontrast=True)\n", "\t\t\n", "\t\t\n", "\t\t\tdistorsion_media = dataset_kmeans.mean(axis=None)['Distortion']\n", "\t\t\tfor index in range(len(dataset_kmeans)):\n", "\t\t\t\tdistortion = dataset_kmeans.iloc[index]['Distortion']\n", "\t\t\t\tdataset_kmeans.loc[index,('Difference')] = abs(distorsion_media - distortion)\n", "\t\t\t\tdataset_kmeans.loc[index,('Mean')] = distorsion_media\n", "\t\t\t\n", "\t\t\tsave_model(dataset_kmeans,NAME_FILE_MODEL_KMEANS) \n", "\t\t\tprint(\"Created csv training data ....................\")\n", "\t\t\n", "\t\t##############################################################################################\n", "\t\t#APPLICATION OF DIFFERENT KIND OF MODELS TO DETECT POSSIBLE THREATS\n", "\t\t##############################################################################################\n", "\t\tprint(\"Starting model creation based in Support Vector Machine Algorithm ......\")\n", "\t\tdataset = pd.read_csv(NAME_FILE_MODEL_KMEANS, sep=',')\n", "\t\tn_samp = len(dataset)\n", "\t\tX, y = make_classification(n_samples=n_samp, n_features=4, random_state=0)\n", "        row = 0\n", "\t\tfor index in range(len(dataset)):\n", "\t\t\tX[row][0]=dataset.loc[index,'Nth']\n", "\t\t\tX[row][1]=dataset.loc[index,'Distortion']\n", "\t\t\tX[row][2]=dataset.loc[index,'Difference']\n", "\t\t\tX[row][3]=dataset.loc[index,'Mean']\n", "\t\t\tif dataset.loc[index,'Threat'] == 'threat':\n", "\t\t\t\ty[index]=np.int32(1)\n", "\t\t\telse:\n", "\t\t\t\ty[index]=np.int32(0)\n", "\t\t\t\n", "\t\t\trow = row + 1\n", "\t\tclf = LinearSVC(random_state=0)\n", "\t\tclf.fit(X, y)\n", "\t\tprint(\"Created model ..........\")\n", "\t\tprint(\"Starting the process of detection threats .....\")\n", "\t\t\n", "        DATA_FOR_PREDICTION_FILE\n", "\t\tdataset = pd.read_csv(DATA_FOR_PREDICTION_FILE, sep=',')\n", "\t\tdata_array = np.array(dataset)\n", "        distortion,whitened,codebook = cluster_analisys(data_array)\n", "\t\t#Generation of the record data\n", "\t\tmean_value = dataset['Distortion'].mean()\n", "\t\tdifference = abs(mean_value - distortion)\n", "\t\t\n", "        result_of_prediction = clf.predict([[nth,distortion,difference,mean_value]]) \t\n", "\t\tname_file = DATA_FOR_PREDICTION_FILE\n", "        if result_of_prediction == True:\n", "\t\t\tprint(\"Passenger {0} it could suppose a thread at Body Scanner {1}\".format(name_file,3))\n", "\t\t\tthreat_result = True\n", "\t\t\t\t\t\n", "\t\telse:\n", "\t\t\tprint(\"Passenger {0} it could suppose a thread at Body Scanner {1}\".format(name_file,3))\n", "\t\t\t\t\t\t\t\n", "        \n", "        sys.exit(0)  #At the demo only the programme do a unique prediction\n", "  \t\n", "\texcept Exception as exception:\n", "\t\tprint (\"DATA_ANALISIS_TOOL: Excepcion {0}\".format(exception))\n", "\t\tsys.exit(1)\n", "\tfinally:\n", "\t\tprint(\"DATA_ANALISIS_TOOL: That's all\")"]}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "version": "3.6.3", "file_extension": ".py", "mimetype": "text/x-python"}}}