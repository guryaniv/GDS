{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom hyperopt import STATUS_OK\nfrom hyperopt import fmin, tpe, hp, Trials\nfrom datetime import datetime\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Bayesian Optimzation code copied here for display purposes"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4f697040daeb14b2ccf2a8ed04f639e351ac6efe"},"cell_type":"code","source":"# Color codes used by Bayesian Optimization\nclass BColours(object):\n    BLUE = '\\033[94m'\n    CYAN = '\\033[36m'\n    GREEN = '\\033[32m'\n    MAGENTA = '\\033[35m'\n    RED = '\\033[31m'\n    ENDC = '\\033[0m'\n\n# Print Log Class used by Bayesian Optimization\nclass PrintLog(object):\n    def __init__(self, params):\n        self.ymax = None\n        self.xmax = None\n        self.params = params\n        self.ite = 1\n        self.start_time = datetime.now()\n        self.last_round = datetime.now()\n        # sizes of parameters name and all\n        self.sizes = [max(len(ps), 7) for ps in params]\n        # Sorted indexes to access parameters\n        self.sorti = sorted(range(len(self.params)),\n                            key=self.params.__getitem__)\n\n    def reset_timer(self):\n        self.start_time = datetime.now()\n        self.last_round = datetime.now()\n\n    def print_header(self, initialization=False):\n        if initialization:\n            print(\"{}Initialization{}\".format(BColours.RED,\n                                              BColours.ENDC))\n        else:\n            print(\"{}Bayesian Optimization{}\".format(BColours.RED,\n                                                     BColours.ENDC))\n        print(BColours.BLUE + \"-\" * (29 + sum([s + 5 for s in self.sizes])) +\n            BColours.ENDC)\n        print(\"{0:>{1}}\".format(\"Step\", 5), end=\" | \")\n        print(\"{0:>{1}}\".format(\"Time\", 6), end=\" | \")\n        print(\"{0:>{1}}\".format(\"Value\", 10), end=\" | \")\n        for index in self.sorti:\n            print(\"{0:>{1}}\".format(self.params[index],\n                                    self.sizes[index] + 2),\n                  end=\" | \")\n        print('')\n\n    def print_step(self, x, y, warning=False):\n        print(\"{:>5d}\".format(self.ite), end=\" | \")\n        m, s = divmod((datetime.now() - self.last_round).total_seconds(), 60)\n        print(\"{:>02d}m{:>02d}s\".format(int(m), int(s)), end=\" | \")\n\n        if self.ymax is None or self.ymax < y:\n            self.ymax = y\n            self.xmax = x\n            print(\"{0}{2: >10.5f}{1}\".format(BColours.MAGENTA,\n                                             BColours.ENDC,\n                                             y),\n                  end=\" | \")\n            for index in self.sorti:\n                print(\"{0}{2: >{3}.{4}f}{1}\".format(\n                            BColours.GREEN, BColours.ENDC,\n                            x[self.params[index]],\n                            self.sizes[index] + 2,\n                            min(self.sizes[index] - 3, 6 - 2)\n                        ),\n                      end=\" | \")\n        else:\n            print(\"{: >10.5f}\".format(y), end=\" | \")\n            for index in self.sorti:\n                print(\"{0: >{1}.{2}f}\".format(x[self.params[index]],\n                                              self.sizes[index] + 2,\n                                              min(self.sizes[index] - 3, 6 - 2)),\n                      end=\" | \")\n        if warning:\n            print(\"{}Warning: Test point chose at \"\n                  \"random due to repeated sample.{}\".format(BColours.RED,\n                                                            BColours.ENDC))\n        print()\n        self.last_round = datetime.now()\n        self.ite += 1\n\n    def print_summary(self):\n        pass","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3532796f456a4ee2f0044e56f541976a7ca7f37d"},"cell_type":"markdown","source":"CSV files readers and enhancers"},{"metadata":{"trusted":true,"_uuid":"8a430b5f2c1e5e29b64375557d7f5f5d5b4d28ce","collapsed":true},"cell_type":"code","source":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df=None, nan_as_category=True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\n# Preprocess application_train.csv and application_test.csv\ndef application_train_test(num_rows=None, nan_as_category=False):\n    # Read data and merge\n    df = pd.read_csv('../input/application_train.csv', nrows= num_rows)\n    test_df = pd.read_csv('../input/application_test.csv', nrows= num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n    df = df[df['CODE_GENDER'] != 'XNA']\n    \n    docs = [_f for _f in df.columns if 'FLAG_DOC' in _f]\n    live = [_f for _f in df.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n    \n    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n\n    inc_by_org = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n\n    df['NEW_CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n    df['NEW_CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n    df['NEW_DOC_IND_AVG'] = df[docs].mean(axis=1)\n    df['NEW_DOC_IND_STD'] = df[docs].std(axis=1)\n    df['NEW_DOC_IND_KURT'] = df[docs].kurtosis(axis=1)\n    df['NEW_LIVE_IND_SUM'] = df[live].sum(axis=1)\n    df['NEW_LIVE_IND_STD'] = df[live].std(axis=1)\n    df['NEW_LIVE_IND_KURT'] = df[live].kurtosis(axis=1)\n    df['NEW_INC_PER_CHLD'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n    df['NEW_INC_BY_ORG'] = df['ORGANIZATION_TYPE'].map(inc_by_org)\n    df['NEW_EMPLOY_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    df['NEW_ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / (1 + df['AMT_INCOME_TOTAL'])\n    df['NEW_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['NEW_EXT_SOURCES_MEAN'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n    df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n    df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n    df['NEW_CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n    df['NEW_CAR_TO_EMPLOY_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n    df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n    df['NEW_PHONE_TO_EMPLOY_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n    df['NEW_CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n    \n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    # Categorical features with One-Hot encode\n    cat_feats = [f for f in df.columns if df[f].dtype == 'object']\n    for cat_f in cat_feats:\n        # Build indexer from value counts\n        counts = df[cat_f].value_counts()\n        # Make sure counts are always ordered the same way\n        counts = counts.reset_index().sort_values([cat_f, 'index'], ascending=False).set_index('index')\n        indexer = counts.loc[counts[cat_f] >= 5].index\n        # unknown values will be automatically assigned -1\n        df[cat_f] = indexer.get_indexer(df[cat_f])\n    \n    # df, cat_cols = one_hot_encoder(df, nan_as_category)\n    \n    del test_df\n    gc.collect()\n    return df\n\n# Preprocess bureau.csv and bureau_balance.csv\ndef bureau_and_balance(num_rows = None, nan_as_category = True):\n    bureau = pd.read_csv('../input/bureau.csv', nrows = num_rows)\n    bb = pd.read_csv('../input/bureau_balance.csv', nrows = num_rows)\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n    \n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n    del bb, bb_agg\n    gc.collect()\n    \n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n    }\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    \n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    cols = active_agg.columns.tolist()\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n    del active, active_agg\n    gc.collect()\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n    \n    for e in cols:\n        bureau_agg['NEW_RATIO_BURO_' + e[0] + \"_\" + e[1].upper()] = bureau_agg['ACTIVE_' + e[0] + \"_\" + e[1].upper()] / bureau_agg['CLOSED_' + e[0] + \"_\" + e[1].upper()]\n    \n    del closed, closed_agg, bureau\n    gc.collect()\n    return bureau_agg\n\n# Preprocess previous_applications.csv\ndef previous_applications(num_rows = None, nan_as_category = True):\n    prev = pd.read_csv('../input/previous_application.csv', nrows = num_rows)\n    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n    # Days 365.243 values -> nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n    # Add feature: value ask / value received percentage\n    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n    # Previous applications numeric features\n    num_aggregations = {\n        'AMT_ANNUITY': ['min', 'max', 'mean'],\n        'AMT_APPLICATION': ['min', 'max', 'mean'],\n        'AMT_CREDIT': ['min', 'max', 'mean'],\n        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'DAYS_DECISION': ['min', 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n    }\n    # Previous applications categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n    \n    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n    # Previous Applications: Approved Applications - only numerical features\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n    cols = approved_agg.columns.tolist()\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n    # Previous Applications: Refused Applications - only numerical features\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n    del refused, refused_agg, approved, approved_agg, prev\n    \n    for e in cols:\n        prev_agg['NEW_RATIO_PREV_' + e[0] + \"_\" + e[1].upper()] = prev_agg['APPROVED_' + e[0] + \"_\" + e[1].upper()] / prev_agg['REFUSED_' + e[0] + \"_\" + e[1].upper()]\n    \n    gc.collect()\n    return prev_agg\n\n# Preprocess POS_CASH_balance.csv\ndef pos_cash(num_rows = None, nan_as_category = True):\n    pos = pd.read_csv('../input/POS_CASH_balance.csv', nrows = num_rows)\n    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n    # Features\n    aggregations = {\n        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n        'SK_DPD': ['max', 'mean'],\n        'SK_DPD_DEF': ['max', 'mean']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    \n    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n    del pos\n    gc.collect()\n    return pos_agg\n    \n# Preprocess installments_payments.csv\ndef installments_payments(num_rows = None, nan_as_category = True):\n    ins = pd.read_csv('../input/installments_payments.csv', nrows = num_rows)\n    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n    # Percentage and difference paid in each installment (amount paid and installment value)\n    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n    # Days past due and days before due (no negative values)\n    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n    # Features: Perform aggregations\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum'],\n        'DBD': ['max', 'mean', 'sum'],\n        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n    del ins\n    gc.collect()\n    return ins_agg\n\n# Preprocess credit_card_balance.csv\ndef credit_card_balance(num_rows = None, nan_as_category = True):\n    cc = pd.read_csv('../input/credit_card_balance.csv', nrows = num_rows)\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n    # General aggregations\n    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n    del cc\n    gc.collect()\n    return cc_agg\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34c08c3cfed34e2d14a9bbc69400e8178820b37a"},"cell_type":"markdown","source":"Hyperopt helper object"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1c416e34d0206e70451fad6c00bd7ab6b80eaea9"},"cell_type":"code","source":"# Class used to optimize LightGBM\nclass HyperOptHelper(object):\n    def __init__(self, df, maximize=True, params_=None):\n        self.maximize = True\n        self.first_run = True\n        self.best_score = None\n        self.plog = None\n        self.df = df\n        self.plog = PrintLog(sorted(params_.keys()))\n    \n    def display_header(self, params_):\n        print('   SCORE | LEAVES | SUBSAMPLE | COLSAMPLE | MIN_SPLIT_GAIN | REG_ALPHA | REG_LAMBDA | LEAF_WEIGHT')\n    \n    def display_current_run(self, params_=None, score_=None):\n        if self.first_run:\n            self.plog.print_header()\n            # self.display_header(params_)\n            self.first_run = False\n        \n        # self.display_params(params_, score_)\n        self.plog.print_step(x=params_, y=score_, warning=False)\n    \n    def is_best_score(self, score_):\n        if self.best_score is None:\n            self.best_score = score_\n            return True\n        else:\n            if self.maximize:\n                if score_ > self.best_score:\n                    self.best_score = score_\n                    return True\n            else:\n                if score_ < self.best_score:\n                    self.best_score = score_\n                    return True\n    \n    def score_params(self, params=None):\n        lgb_params = {\n            'objective': 'binary',\n            'min_split_gain': np.power(10, params['p4_gain']),\n            'subsample': params['p2_subsamp'],\n            'colsample_bytree': params['p3_colsamp'],\n            'reg_alpha': np.power(10, params['p5_alph']),\n            'reg_lambda': np.power(10, params['p6_lamb']),\n            'num_leaves': int(params['p1_leaf']),\n            'verbose': -1,\n            'min_child_weight': np.power(10, params['p7_weight']),\n            'seed': 3,\n            'boosting_type': 'gbdt',\n            'max_depth': -1,\n            'learning_rate': 0.1\n        }\n\n        eval_hist = lgb.cv(\n            params=lgb_params, \n            train_set=self.df, \n            num_boost_round=300, \n            folds=None, \n            nfold=5, \n            stratified=True, \n            shuffle=True, \n            metrics='auc',  \n            early_stopping_rounds=50, \n            fpreproc=None, \n            verbose_eval=0, \n            show_stdv=True, \n            seed=0, \n            callbacks=None\n        )\n        \n        self.display_current_run(params, eval_hist['auc-mean'][-1])\n        return {'loss': -eval_hist['auc-mean'][-1], 'status': STATUS_OK}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28d5ad20e973ed2adcd1c8ab6996ba9aacf0019a"},"cell_type":"markdown","source":"Read files, create full dataset and optimize"},{"metadata":{"trusted":true,"_uuid":"a86aff4e91bd59201e474095f403c2a7ec5b70c7","collapsed":true},"cell_type":"code","source":"df = application_train_test(num_rows=None)\nwith timer(\"Process bureau and bureau_balance\"):\n    bureau = bureau_and_balance(num_rows=None)\n    print(\"Bureau df shape:\", bureau.shape)\n    df = df.join(bureau, how='left', on='SK_ID_CURR')\n    del bureau\n    gc.collect()\nwith timer(\"Process previous_applications\"):\n    prev = previous_applications(num_rows=None)\n    print(\"Previous applications df shape:\", prev.shape)\n    df = df.join(prev, how='left', on='SK_ID_CURR')\n    del prev\n    gc.collect()\nwith timer(\"Process POS-CASH balance\"):\n    pos = pos_cash(num_rows=None)\n    print(\"Pos-cash balance df shape:\", pos.shape)\n    df = df.join(pos, how='left', on='SK_ID_CURR')\n    del pos\n    gc.collect()\nwith timer(\"Process installments payments\"):\n    ins = installments_payments(num_rows=None)\n    print(\"Installments payments df shape:\", ins.shape)\n    df = df.join(ins, how='left', on='SK_ID_CURR')\n    del ins\n    gc.collect()\nwith timer(\"Process credit card balance\"):\n    cc = credit_card_balance(num_rows=None)\n    print(\"Credit card balance df shape:\", cc.shape)\n    df = df.join(cc, how='left', on='SK_ID_CURR')\n    del cc\n    gc.collect()\nwith timer(\"Run LightGBM with kfold\"):\n    gc.collect()\n    print(df.shape)\n    features = [f for f in df.columns \n                if f not in ['TARGET', 'SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    dtrain = lgb.Dataset(\n        data=df.loc[df['TARGET'].notnull(), features],\n        label=df.loc[df['TARGET'].notnull(), 'TARGET'],\n        free_raw_data=False\n    )\n    del df\n    gc.collect()\n    \n    param_space = {\n        'p1_leaf': hp.uniform('p1_leaf', 5, 80),\n        'p2_subsamp': hp.uniform('p2_subsamp', 0.6, 0.95),\n        'p3_colsamp': hp.uniform('p3_colsamp', 0.6, 0.95),\n        'p7_weight': hp.uniform('min_child_weight', -5, 2),\n        'p4_gain': hp.uniform('p4_gain', -5, 2),\n        'p5_alph': hp.uniform('reg_alph', -5, 2),\n        'p6_lamb': hp.uniform('reg_lamb', -5, 2)\n    }\n    np.random.seed(100)\n    trials = Trials()\n    best = fmin(\n        HyperOptHelper(df=dtrain, maximize=True, params_=param_space).score_params,\n        param_space,\n        algo=tpe.suggest,\n        max_evals=15,\n        trials=trials,\n        return_argmin=True\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2645f2ffc6da72368bee4d503bbb09191b78e090"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}