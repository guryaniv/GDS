{"cells":[{"metadata":{"_uuid":"476881c12a166d6162b2d1323839e80d1cc80eba"},"cell_type":"markdown","source":"<h1 style=\"font-size:30px\">Predict Future Sales - EDA</h1>\n<br>\nThe problem is based on a time-series dataset consisting of daily sales data provided by one of the largest Russian software firms - 1C Company, which is available as a Kaggle competition.<br>\n<hr>\n<h1 style=\"font-size:18px\">Objective</h1>\n<br>\nInitially we aim to understand and analyze the data, and then to predict total sales for every product and store in the next month.<br>\nThis kernel is the first part of the scope.\n<hr>\n<h1 style=\"font-size:18px\">Content</h1>\n<br>\nThis kernel will be divided into:\n1. <a href=\"#basic\">Basic Information</a>\n2. <a href=\"#engineering\">Feature Engineering</a>\n3. <a href=\"#numeric\">Numeric Features</a>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"<h1 style=\"font-size:18px\">File description</h1>\n* sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n* test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n* sample_submission.csv - a sample submission file in the correct format.\n* items.csv - supplemental information about the items/products.\n* item_categories.csv  - supplemental information about the items categories.\n* shops.csv- supplemental information about the shops."},{"metadata":{"_uuid":"fb3281ea60568ecd586afc8d8d1141b1ff0c292b"},"cell_type":"markdown","source":"<h1 style=\"font-size:18px\">About the data</h1>\n* ID - an Id that represents a (Shop, Item) tuple within the test set\n* shop_id - unique identifier of a shop\n* item_id - unique identifier of a product\n* item_category_id - unique identifier of item category\n* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n* item_price - current price of an item\n* date - date in format dd/mm/yyyy\n* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* item_name - name of item\n* shop_name - name of shop\n* item_category_name - name of item category"},{"metadata":{"_uuid":"bd968d3f7a6ca9fbcc0ba950cdda84862fa6ebb2"},"cell_type":"markdown","source":"<h1 style=\"font-size:18px\">Import libraries</h1>"},{"metadata":{"trusted":true,"_uuid":"24dcb140f6f6b07f0f8968f4e7590dc530e9eff0"},"cell_type":"code","source":"# Numpy for numerical computing\nimport numpy as np\n\n# Pandas for Dataframes\nimport pandas as pd\npd.set_option('display.max_columns',100)\n\n# Matplolib for visualization\nfrom matplotlib import pyplot as plt\n# display plots in the notebook\n%matplotlib inline\n\n# Seaborn for easier visualization\nimport seaborn as sns\n\n# Datetime deal with dates formats\nimport datetime as dt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b237b4ca25dc1a9e27768c21ce79fe388a734f6"},"cell_type":"markdown","source":"<span style=\"font-size:18px\">**Load files**</span>"},{"metadata":{"trusted":true,"_uuid":"fa666ac4831c3a220e440ae43d68196e9a5bef2f"},"cell_type":"code","source":"train = pd.read_csv('../input/sales_train.csv')\ntest = pd.read_csv('../input/test.csv')\nitems = pd.read_csv('../input/items.csv')\nitems_categories = pd.read_csv('../input/item_categories.csv')\nshops = pd.read_csv('../input/shops.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2bfb9c8dfee1cb821a8ce58675da8665cc46a53"},"cell_type":"markdown","source":"<br id=\"basic\">\n# 1. Basic Information\nLet's first check some informations about the dataset for each loaded file, as:\n* Dimension\n* Features type\n* Number of missing values\n* View the first 3 rows"},{"metadata":{"trusted":true,"_uuid":"a1068881cf6469a556c75cffe835efca368dba0d"},"cell_type":"code","source":"# Dataframe dimensions\nprint('The dimension of the training set is:',train.shape,'\\n')\nprint('The feature types are:\\n', train.dtypes,'\\n')\nprint('Number of missing values:\\n',train.isnull().sum())\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d12d9598c7b8ac9b2c4b345d56987506241ee63f"},"cell_type":"code","source":"print('The dimension of the test set is:',test.shape,'\\n')\nprint('The feature types are:\\n', test.dtypes,'\\n')\nprint('Number of missing values:\\n',test.isnull().sum())\ntest.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6217f84aa7c48771215956a25fcf322ad32fa69e"},"cell_type":"code","source":"print('The dimension of the items set is:',items.shape,'\\n')\nprint('The feature types are:\\n', items.dtypes,'\\n')\nprint('Number of missing values:\\n',items.isnull().sum())\nitems.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e08444b732deefdc8a080e676f891f9296e8835"},"cell_type":"code","source":"print('The dimension of the items categories is:',items_categories.shape,'\\n')\nprint('The feature types are:\\n', items_categories.dtypes,'\\n')\nprint('Number of missing values:\\n', items_categories.isnull().sum())\nitems_categories.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7197b0a2618ee9f0cbdc2287032f1d1067440d3e"},"cell_type":"code","source":"print('The dimension of the shops set is:',shops.shape,'\\n')\nprint('The feature types are:\\n', shops.dtypes,'\\n')\nprint('Number of missing values:\\n', shops.isnull().sum())\nshops.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"498f9cd6ef86d81355bcbd17036a005c59396ed2"},"cell_type":"markdown","source":"So far the data looks pretty good. There is no missing values, and the features seems to have the correct type."},{"metadata":{"_uuid":"1a7dfb8c7e2fbf9b80b428e97f10bff76a37e7bc"},"cell_type":"markdown","source":"<br id=\"engineering\">\n# 2. Feature Engineering\n\nBefore we continue, let's do some preliminary feature engineering to make the data easier to deal with.<br>\n<br>\nFirstly, let's change the date type, from object to datetime. Then divide de **date** feature to create 3 new columns for **year**, **month** and **day**."},{"metadata":{"trusted":true,"_uuid":"76dae23b6e2091176aee053ed7285feff4cee995"},"cell_type":"code","source":"# Change the date type\ndate = train.date.apply(lambda x:dt.datetime.strptime(x, '%d.%m.%Y'))\n\n# Create 3 new features for year, month and day\ntrain['year'] = date.dt.year\ntrain['month'] = date.dt.month\ntrain['day'] = date.dt.day\ntrain.head()\n\n# Remove the \"date\" feature\ntrain = train.drop('date', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ba7dad70573de5c9a4057b944dcb965a65e2d0b"},"cell_type":"markdown","source":"Checking the dataset we can see that there is a feature that might be missing in the **train set**: item_category_id.<br> \nThe **item_category_id** and the **item_id** are related at **items set**. Thus, we can create another feature for these categories."},{"metadata":{"trusted":true,"_uuid":"e278372a53a75e240c66245845eff9cabb483671"},"cell_type":"code","source":"# Add the \"item_category_id\" to the dataset\ntrain = pd.merge(train, items.drop('item_name', axis=1), on='item_id')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7d279fdbce260015c24904b96fda7f2b41e3522"},"cell_type":"markdown","source":"As we have the current price of the item (item_price) and the number of items sold (item_cnt_day) features, we can create another feature called \"revenue\" by their dot multiplication."},{"metadata":{"trusted":true,"_uuid":"c50a3d63b496bec3fe8cc271a0959d3b5afd240c"},"cell_type":"code","source":"# Create \"revenue\" feature\ntrain['revenue'] = train.item_price*train.item_cnt_day\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"227e027c7b441c046f0474e0168e4f800ac33c2a"},"cell_type":"markdown","source":"<br id=\"numeric\">\n# 3. Numeric features\nTo ease up the study of the numeric features, we will look the data throught grouping it by year and month to see if there is any possible seasonability.<br>\n<br>\nFirstly we will analyze the feature item_cnt_day, which gives the number of products sold."},{"metadata":{"trusted":true,"_uuid":"d4c88465dde2494abc44152ae45ed02b61eb6d9c"},"cell_type":"code","source":"# Plot the total number of products sold by year\ntrain.groupby('year').item_cnt_day.sum().plot()\nplt.xticks(np.arange(2013, 2016, 1))\nplt.xlabel('Year')\nplt.ylabel('Total number of products sold')\nplt.show()\n\n# Plot the total number of products sold by month for each year\ntrain.groupby(['month','year']).sum()['item_cnt_day'].unstack().plot()\nplt.xlabel('Month')\nplt.ylabel('Total number of products sold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efa872fc447f83a0213058c30cfb3b993a02610d"},"cell_type":"markdown","source":"We can see that the number of sold products are decreasing over the years.<br>\nLooking at the months, the sales seems to vary in a certain range until October, and then the sales start to increase greatly.<br>\n<br>\nNow let's check the revenue behavior."},{"metadata":{"trusted":true,"_uuid":"d4649605a75e5e5315d7fdfa680512cad05ee3ca"},"cell_type":"code","source":"# Plot the total revenue by year\ntrain.groupby('year').revenue.sum().plot()\nplt.xticks(np.arange(2013, 2016, 1))\nplt.xlabel('Year')\nplt.ylabel('Total revenue')\nplt.show()\n\n# Plot the total revenue by month for each year\ntrain.groupby(['month','year']).sum()['revenue'].unstack().plot()\nplt.xlabel('Month')\nplt.ylabel('Total revenue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10c84a02909b4abacd5dfdfd3034601e3bf4b2b9"},"cell_type":"markdown","source":"The revenue behavior is a little different from the number of total sales.<br>\nIn 2014 the total revenue increased, even though the number of total sales decreased from 2013. This is due to the \"item_price\" variable, which can fluctuate by the time.<br>\nWe can also observe that, over the months, even though the number of sales product decreased, the revenue seems similar for the three years.<br>\n<br>\nLet's look at the top 10 items and the top 10 shops."},{"metadata":{"trusted":true,"_uuid":"dad2180bf4d6a95bb6152b4c49dbe8d351983a16"},"cell_type":"code","source":"# Plot the top 10 items\nsns.countplot(y='item_id', hue='year', data=train, order = train['item_id'].value_counts().iloc[:10].index)\nplt.xlim(0,20000)\nplt.xlabel('Number of times the item was sold')\nplt.ylabel('Identifier of the item')\nplt.show()\n\n# Plot the top 10 shops\nsns.countplot(y='shop_id', hue='year', data=train, order = train['shop_id'].value_counts().iloc[:10].index)\nplt.xlabel('Number of times the shop sold')\nplt.ylabel('Identifier of the shop')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c98ba586595786995734aeaeb048b3dcc96656a"},"cell_type":"markdown","source":"The item 20949 is the sales champion over the years by far!<br>\nThe top 10 shops have similar sales behavior over the years."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}