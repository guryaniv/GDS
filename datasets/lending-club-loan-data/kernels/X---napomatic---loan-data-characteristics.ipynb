{"cells":[
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "# Loan data characteristics"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Imports and setup\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n%matplotlib inline\nplt.style.use('ggplot')\n\n# Kaggle note: Any results written to the current directory are saved as output"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Read loan data\ndate_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d']\nloans = pd.read_csv(\"../input/loan.csv\", low_memory=False, index_col='id',\n    parse_dates=date_cols, infer_datetime_format=True)\nprint(\"Dataset size: {}\".format(loans.shape))\nprint(loans.head())"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# What columns do we have?\nprint(\"{} columns: {}\".format(len(loans.columns), loans.columns))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Let's take a look at the different columns and what data they contain\n#cols = loans.columns[0:10]  # cycle through 0:10, 10:20, ...\ncols = ['loan_amnt', 'term', 'int_rate', 'installment', 'emp_length']  # or pick specific columns\nprint(cols)\nfor col in cols:\n    print(loans[col].describe())  # describe one by one in case of mixed types"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Parse term durations: ' 36 months' -> 36 (numeric)\nprint(\"term before:-\")\nprint(loans.term.head())\nloans.term = pd.to_numeric(loans.term.str[:3])\nprint(\"term after:-\")\nprint(loans.term.head())"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Parse emp_length: '< 1 year' -> 1.0, '1 year' -> 1.0, '7 year' -> 7.0, etc. (numeric)\nprint(\"emp_length before:-\")\nprint(loans.emp_length.head())\nloans.emp_length = loans.emp_length.str.extract(\"(\\d+)\", expand=False).map(float)\nprint(\"emp_length after:-\")\nprint(loans.emp_length.head())"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# What is the distribution of loans by status?\nloans_by_status = loans.groupby('loan_status')\nprint(loans_by_status['loan_status'].count())\nloans_by_status['loan_status'].count().plot(kind='bar')"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# What is the distribution of loans by purpose?\nloans_by_purpose = loans.groupby('purpose')\nprint(loans_by_purpose['purpose'].count())\nloans_by_purpose['purpose'].count().plot(kind='bar')"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# What is the distribution of loans by term?\nloans_by_term = loans.groupby('term')\nprint(loans_by_term['term'].count())\nloans_by_term['term'].count().plot(kind='bar')"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Binary Classification Task\nGoal: Predict loans at application stage that will default"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Select loans issued within desired date range\n#loans.issue_d.describe()  # dataset range: 2007-06-01 to 2015-12-01\nrange_selected = ('2007-06-01', '2010-12-31')\nloans_selected = loans.loc[(range_selected[0] <= loans.issue_d) & (loans.issue_d <= range_selected[1])]\nprint(\"{num} loans were issued from {range[0]} to {range[1]}\".format(num=len(loans_selected), range=range_selected))\n\n# What is their distribution by status?\nprint(loans_selected.groupby('loan_status')['loan_status'].count())"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Let's setup a binary classification target 'default': 0 => Fully Paid, 1 => Charged Off\nloans_subset = loans_selected.copy()\nloans_subset['default'] = None\nloans_subset.loc[(loans_subset.loan_status == 'Fully Paid') | (loans_subset.loan_status == 'Does not meet the credit policy. Status:Fully Paid'), 'default'] = 0\nloans_subset.loc[(loans_subset.loan_status == 'Charged Off') | (loans_subset.loan_status == 'Does not meet the credit policy. Status:Charged Off'), 'default'] = 1\n\n# Drop loans that haven't been terminated yet (we don't know what their final status will be)\nloans_subset = loans_subset[~loans_subset.default.isnull()]\nprint(\"Data subset size: {}\".format(loans_subset.shape))\n\n# Re-encode 'default' column as numeric (0 or 1)\nloans_subset['default'] = pd.to_numeric(loans_subset['default'])"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Drop columns that are unimportant, superfluous or leak target information\n# Note: We only want to keep information that is available at loan *application* stage\napplication_cols = [\n    # Identifiers and dates\n    #'id',  # used as index column\n    'member_id',\n    'issue_d',\n    \n    # Loan application details\n    #'application_type',  # all 'INDIVIDUAL'\n    'loan_amnt',  # $ applied for\n    'term',  # 36 or 60 months\n    'int_rate',  # % annual (?) interest rate\n    'installment',  # $ monthly payment\n    'emp_title',  # employee/employer title\n    'emp_length',  # 0-10+ years\n    'home_ownership',  # RENT, OWN, MORTGAGE, etc.\n    'verification_status',  # mostly 'Not Verified'\n    #'verification_status_joint',  # all 0\n    'purpose',  # 'debt_consolidation', 'small_business', etc.\n    'title',  # text\n    #'desc',  # text, too verbose, may contain updates after application stage\n    'zip_code',  # 100XX\n    'addr_state',  # covered by zip_code?\n    \n    # Additional loan listing details\n    #'initial_list_status',  # all 'f'\n    #'policy_code',  # all 1\n    #'url',  # unqiue per loan\n\n    # Borrower's creditworthiness\n    'annual_inc', #'annual_inc_joint',  # income ($; individual only, no joint loans)\n    'dti', #'dti_joint',  # debt-to-income ratio (%; individual only, no joint loans)\n    'revol_bal', 'revol_util',  # revolving accounts: balance ($), utilization (%)\n    #'tot_cur_bal', 'max_bal_bc',  # overall balance: total current, max; all null\n    'earliest_cr_line', 'total_acc', 'open_acc',  # credit accounts\n    'inq_last_6mths', #'inq_last_12m', 'inq_fi',  # credit inquiries (only 6 mths available)\n    'delinq_2yrs', 'mths_since_last_delinq', #'acc_now_delinq',  # delinquency (acc_now_delinq is mostly 0)\n    #'tot_coll_amt', 'collections_12_mths_ex_med',  # collections; all null or 0\n    #'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util',  # installment accounts; all null\n    #'open_acc_6m', 'open_rv_12m', 'open_rv_24m', 'total_rev_hi_lim', 'total_cu_tl', 'all_util', # revolving trading accounts; all null\n    \n    # Public records\n    'pub_rec', 'mths_since_last_record',\n    #'mths_since_last_major_derog',  # all null\n\n    # Loan rating as determined by lender (potential multi-class targets to predict?)\n    #'grade',\n    #'sub_grade',\n\n    # Desired binary target to predict\n    'default'\n]\n\nloans_small = loans_subset[application_cols]\n\n# Check selected data subset\nprint(\"Small dataset has {} rows, {} columns:\".format(len(loans_small), len(loans_small.columns)))\nprint(loans_small.head())\nprint(\"Class distribution:\")\nprint(loans_small.groupby('default')['default'].count())"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Write dataset to disk (if you want to save it)\nloans_small.to_csv(\"loans_small.csv\")\nprint(\"Dataset saved!\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Read back from disk (to skip all previous steps if you've saved it already)\nloans_small = pd.read_csv(\"loans_small.csv\", index_col=0, parse_dates=True)\nprint(\"Loaded data has {} rows, {} columns:\".format(len(loans_small), len(loans_small.columns)))\nprint(loans_small.head())\nprint(\"Class distribution:\")\nprint(loans_small.groupby('default')['default'].count())"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Specify a subset of feature columns and a target to predict ('default')\nfeature_cols = [\n    'loan_amnt', 'term', 'int_rate', 'installment', 'purpose',\n    #'emp_title', # free text\n    'emp_length', 'home_ownership',\n    #'zip_code', 'addr_state',  # categorical, but too many levels\n    'annual_inc', 'dti',\n    'revol_bal', 'revol_util',\n    'verification_status'\n]\n\ntarget_col = 'default'\n\n# Create the final dataset we'll use for classification\nkeep_cols = feature_cols + [target_col]\nloans_final = loans_small[keep_cols]\n\n# Drop samples with null values (few enough that we can ignore)\nloans_final.dropna(inplace=True)\n\nprint(\"Final dataset: {} features, {} samples\".format(len(loans_final.columns), len(loans_final)))\nprint(loans_final.head())\nprint(\"Final class distribution (after dropping nulls):\")\nclass_counts = loans_final.groupby(target_col)[target_col].agg({\n    'count': len,\n    'ratio': lambda x: float(len(x)) / len(loans_final)\n})\nprint(class_counts)\n\n# Extract desired features and target column\nX = loans_final[feature_cols]\ny = loans_final[target_col]\nprint(\"{} features: {}\".format(len(X.columns), X.columns))\nprint(\"Target: {}\".format(y.name))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Encode categorical variables among features\ncategorical_vars = ['home_ownership', 'purpose', 'verification_status']\nX = pd.get_dummies(X, columns=categorical_vars, drop_first=True)\nprint(\"{} features after encoding categorical variables: {}\".format(len(X.columns), X.columns))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Split into training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nprint(\"Training set: {} samples, test set: {} samples\".format(len(X_train), len(X_test)))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Common sklearn imports\nfrom sklearn.metrics import classification_report\n\n# Define a simple train-predict utility function\ndef train_predict(clf, X_train, X_test, y_train, y_test):\n    \"\"\"Train clf on <X_train, y_train>, predict <X_test, y_test>; return y_pred.\"\"\"\n    print(\"Training a {}...\".format(clf.__class__.__name__))\n    %time clf.fit(X_train, y_train)\n    print(clf)\n    \n    print(\"Predicting test labels...\")\n    y_pred = clf.predict(X_test)\n    return y_pred"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Classify using a Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=42)\ny_pred = train_predict(clf, X_train, X_test, y_train, y_test)\nprint(classification_report(y_test, y_pred))\n\n# Analyze feature importance\nfeature_imps = pd.DataFrame({'feature': X_train.columns, 'importance': clf.feature_importances_})\nfeature_imps.sort_values(by='importance', ascending=False, inplace=True)\nprint(\"Top 10 important features:\")\nprint(feature_imps[:10])"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Classify using a Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=10, random_state=42)\ny_pred = train_predict(clf, X_train, X_test, y_train, y_test)\nprint(classification_report(y_test, y_pred))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Classify using a Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(n_estimators=100, max_depth=1, learning_rate=1.0, random_state=42)\ny_pred = train_predict(clf, X_train, X_test, y_train, y_test)\nprint(classification_report(y_test, y_pred))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Note: The performance on the interesting class (default=1) is very low!\n# TODO: Try subsampling the other class (default=0) or other methods to mitigate class imbalance."
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}