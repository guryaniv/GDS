{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression, PLSSVD\nfrom sklearn.metrics import mean_squared_error\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib notebook\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f39711cfb449cea62117dd2cbcf51b03e9f6f40"},"cell_type":"code","source":"# Read train and test files\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(\"Train rows and columns : \", train.shape)\nprint(\"Test rows and columns : \", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Train rows and columns :  (4459, 4993) - the number of rows or instances are more compared to features\n# Test rows and columns :  (49342, 4992) - the number of rows or instances are more compared  to features and surprisingly we have more test instances than train instances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6239fbc6879fb3cb1b6c34a3feb5ffe8a096dc33"},"cell_type":"code","source":"### Below are the steps involved to understand, clean and prepare your data for building your predictive model:\n##### Variable Identification\n##### Univariate Analysis\n##### Bi-variate Analysis\n##### Missing values treatment\n##### Outlier treatment\n##### Variable transformation\n##### Variable creation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd0a9c014a8c070d5f565aa92010719a1b9e4eb4"},"cell_type":"markdown","source":"### Variable Identification"},{"metadata":{"trusted":true,"_uuid":"a238c089685c1bed2b4898b66ea55e7d0378ec30","collapsed":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c2b211326654376cb3eb26c090b81acbfd662a1a"},"cell_type":"code","source":"# target  - is a Target variable\n# Other Columns - Are predictors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ea8b4d0a689f7f31806c2f3fd64156d65880c95","collapsed":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85f90f78b7bee1d189ed8a7c019cb089e04cd7fc"},"cell_type":"markdown","source":"### Data Type of Columns and the variable category"},{"metadata":{"trusted":true,"_uuid":"0336b9dbc61669f1d8aaded6cba7568ca3c35130","collapsed":true},"cell_type":"code","source":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ef8cda9acbcecf1c8645e99c5525bd3b7da2d045"},"cell_type":"code","source":"# Majority of the columns have integer , followed by float and ID (category / string)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebdff4d40a8587d9593aea7a8da6de141c6c42ec"},"cell_type":"markdown","source":"### Univariate Analysis\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"376741376138323702151023672d2ef2255c9ec9"},"cell_type":"code","source":"### we will check for scatter plot of the target variable to see for any otuliers which are visible clearly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"399374033f192954124ecd0a7c3bbc34a2d51960","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.scatter(range(train.shape[0]), np.sort(train['target'].values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('Target', fontsize=12)\nplt.title(\"Target Distribution\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"740b6a6ebc0bdda3191704aac273beec4d44ca98"},"cell_type":"code","source":"## the above distribution shows that there are no outliers but the values are increasing from low range to very high range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"14abd41278fdd6696b10eeed44f51ef39999fa2e"},"cell_type":"code","source":"# As there are lot of variables we cannot see all the distributions at one place. But we can definitely see the distribution of Target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55a2f777053c69a86a9c6ddd19e1ac2a762a0142","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.distplot(train['target']);\nplt.title('target histogram.');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"043569953625662e0766ed1ec39cff48c3498ebc"},"cell_type":"code","source":"### The above distributionis right skewed one and hence it needs a transformation -let us try with Log transformation and see the plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"654fd1059104fea7f1d761dd3d1314736fe28d15"},"cell_type":"code","source":"log_train_target = np.log(train.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9590f1a7145bfe0afcd5b75a8491864d098c0094","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.distplot(log_train_target);\nplt.title('Logarithm transformed target histogram.');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"afdaee580f94c423bf78b9d8e13df26a2f060ec0"},"cell_type":"code","source":"# The above distribution looks better when compared to the original variable (target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67da19bffacde129ec18665d147dab4e9ffcb3b1"},"cell_type":"markdown","source":"### Bi-variate Analysis"},{"metadata":{"_uuid":"2f538aa9c54429f946530751e9c34140cd0e56ff"},"cell_type":"markdown","source":"#### Columns with constant values - Checking"},{"metadata":{"trusted":true,"_uuid":"1c9ad8051214c3be58ec3258558255f9f21c0895","collapsed":true},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"869366349d0116120710a994aae481a57a717c3e"},"cell_type":"code","source":"# Checking the columns which has constant values and remove them from our analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fb2e0773d4b289f34aadb4ed3b817a7906031de"},"cell_type":"code","source":"unique_list = train.nunique().reset_index()\nunique_list.columns = [\"col_name\", \"unique_count\"]\nconstant_list = unique_list[unique_list[\"unique_count\"]==1]\nconstant_list.shape\n\n\n## Check for test also\nunique_list_test = test.nunique().reset_index()\nunique_list_test.columns = [\"col_name\", \"unique_count\"]\nconstant_list_test = unique_list_test[unique_list_test[\"unique_count\"]==1]\nconstant_list_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f13398c01cf0797349273ea2e40c8d07bfdf1ba"},"cell_type":"code","source":"# Yes, there are 256 columns with same constant values , it is better to remove them from our analysis. Let us see what are those columns\n# Dropping id variables\nprint(\"The Original train dataset has:\", train.shape[1], 'Columns')\ntrain = train.drop(constant_list.col_name.tolist(), axis=1)\nprint(\"The Final train dataset after removing the constant columns list has :\", train.shape[1], 'Columns')\n# The test dataset also needs to be removed - all the constant columns\ntest = test.drop(constant_list.col_name.tolist(), axis=1)\nprint(\"The Final test dataset after removing the constant columns list has :\", test.shape[1], 'Columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2a0724ac9d5dcf5b82ec9695d316b106756ebacb"},"cell_type":"code","source":"# Now let us look at the correlation of all other remaining variables w.r.t target variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9a0e2de413a5daae96a86e43d0b96f1df47ba423"},"cell_type":"code","source":"#### Identify Highly Correlated Features\n\n# Create correlation matrix\ncorr_matrix = train.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dab1d96159729917fe204f0e98ffc65777f30296"},"cell_type":"code","source":"upper_pd = pd.DataFrame(upper) \nupper_first_row = upper_pd.head(1)\ndata = upper_first_row.iloc[0]\ncolumns = ['target']\nnames = upper_first_row.columns.values[1:]\ndf = pd.DataFrame(data,  index = names, columns=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc94eff65c7f8fbcd38423f8c3bd5ec4e3837d9c","collapsed":true},"cell_type":"code","source":"# identify the maximum correlation value \ns=df.max()\ns\nprint('The maximum correlation value:', s)\n# Hence filter out those attributes with > 0.2 correlation value and consider them as important features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6f885822c1ad9a5a3cf38f38137d7f40f25c960","collapsed":true},"cell_type":"code","source":"## Correlation heatmap\n## To further reduce the variables - we can put a cut off of correlation values to be 0.25 becuase we have only maximum correlation - 0.27\ndf1 = df[df['target'] > 0.25]\nprint('the features with greater than 0.25 correlation value with target column are :', df1.shape[0])\n# Let us print out all the variables with > 0.25 Correlation value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04d87fb6eb8f1c03b5fc26a18a9fbeda521ecb90","collapsed":true},"cell_type":"code","source":"pkmn_type_colors = ['#78C850',  # Grass\n                    '#F08030',  # Fire\n                    '#6890F0',  # Water\n                    '#A8B820',  # Bug\n                    '#A8A878',  # Normal\n                    '#A040A0',  # Poison\n                    '#F8D030',  # Electric\n                    '#E0C068',  # Ground\n                    '#EE99AC',  # Fairy\n                    '#C03028',  # Fighting\n                    '#F85888',  # Psychic\n                    '#B8A038',  # Rock\n                    '#705898',  # Ghost\n                    '#98D8D8',  # Ice\n                    '#7038F8',  # Dragon\n                   ]\nplt.figure(figsize=(15,12))\n# Count Plot (a.k.a. Bar Plot)\nsns.barplot( x = df1.target,  y = df1.index, data=df1, palette=pkmn_type_colors)\n\nplt.title(\" Top features with their Correlation Values\", fontsize=15)\n# Rotate x-labels\nplt.xticks(rotation=-45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c07732a3d0ae0f68dce6fc162ed38374a4958ec6"},"cell_type":"code","source":"# The features - 555f18bd3 and 9fd594eec are having better correlation values with target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd4b0394c8b3156f078f0f2175a86a73f2bdd9ec"},"cell_type":"code","source":"# Visualizing the top features based on the correlation values with all other features\nimp_corr = train[df1.index].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6da73b99e74b93c23ab2cb974aaf0c3e319ed56","collapsed":true},"cell_type":"code","source":" # Heatmap\nplt.figure(figsize=(15,12))\nsns.heatmap(imp_corr, annot = True, vmax=.8, square=True, cmap=\"BuPu\")\nplt.title(\"Important features with Correlation Map\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db7232c4a6463f88919c9b222057780417bd88e3"},"cell_type":"markdown","source":"### Missing Values Treatment"},{"metadata":{"trusted":true,"_uuid":"66ed4038d7cdc83c9716fe958fed3e7f282f4fac"},"cell_type":"code","source":"#Checking for train\nmissing_df = train.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\nmissing_df\nprint(\"The number of missing columns in train set are :\",missing_df.shape[0])\n#Checking for Test\nmissing_df = test.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\nmissing_df\nprint(\"The number of missing columns in test set are :\",missing_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"641811ff55993be45842a73a340f0a8475761a38"},"cell_type":"code","source":"### There are no missing values.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22ad6383a0c281ff46729bb6d1ec924158cf3baf"},"cell_type":"markdown","source":"### Outlier Treatment"},{"metadata":{"trusted":true,"_uuid":"9b4f60023d808792911f1df13b44a02138a7baad","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\n# fig, axs = plt.subplots()\nsns.boxplot(data=train[df1.index].iloc[:],orient='h',palette=\"Set2\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88f4044e3e7bf5be058053645a4781b3f6f98cc7","collapsed":true},"cell_type":"code","source":"## Let us check the summary of these variables\ntrain[df1.index].describe()\n# All most all the variables are looking like outliers\n# Let us not remove any of these variables outliers, let us develop a baseline model and then we will come into this.\n# Too much extreme values are found with f190486d6 and 58e2e02e6","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"252deb07a82703fa39e645ad66f3e4a02c9cdd4e"},"cell_type":"markdown","source":"### Variable Transformation & Variable Creation"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bba284e9ae9101f4afb81673f2370dfed5e2e294"},"cell_type":"code","source":"# we have already checked the target variable distribution and we have already transformed into Log \n# I do not see any variables new variables to be created (Already lot of variables )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a79bb4d04361236c7575dcc7068953e87bde370"},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"_uuid":"de65b69b34dc6809927aee93bd3ee51fa08a0ca8"},"cell_type":"markdown","source":"### PCA Analysis"},{"metadata":{"trusted":true,"_uuid":"d746b6814bdbcfee337bfb8a0d31fd65eb57b6aa","collapsed":true},"cell_type":"code","source":"## For any dimensionality reduction problem , first thing that comes to our mind is doing a Principal Component Analysis - Let us see how we can implement here!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ea803a58f39d016a185c8541b4ca35857c9c5e26"},"cell_type":"code","source":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression, PLSSVD\nfrom sklearn.metrics import mean_squared_error\n\n# To implement PCA in python, simply import PCA from sklearn library. \n# categorical variables have to be converted into numeric.  Here, we have only numeric features\n\n#convert it to numpy arrays\n\nX1=train.drop(['ID', 'target'], axis=1).values\n\n#Scaling the values\n# X1 = scale(X1)\n# pca = PCA(n_components = 800)\npca = PCA(n_components = 15)\nX_reduced = pca.fit_transform(scale(X1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a161854d3b61a1136a9e6a0da42a13730b138344"},"cell_type":"code","source":"#Cumulative Variance explains\nvar1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac910523a1430527fa588b5272a530ed18019784","collapsed":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3f39c50d061debb95724c0b6fcea96f5e6272a9f"},"cell_type":"code","source":"# 3-fold CV, with shuffle\nn = len(X_reduced)\nkf_3 = model_selection.KFold( n_splits=3, shuffle=True, random_state=1)\n\nregr = LinearRegression()\nmse = []\n\ny = log_train_target\n\n# Calculate MSE with only the intercept (no principal components in regression)\nscore = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_3, scoring='neg_mean_squared_error').mean()    \nmse.append(score)\n\n# Calculate MSE using CV for the 200 principle components, adding one component at the time.\nfor i in np.arange(1, 15):\n    score = -1*model_selection.cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_3, scoring='neg_mean_squared_error').mean()\n    mse.append(score)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"243b9237ecaa1cdd2a8b2410379098cf90f3e787","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\n# Plot results    \nplt.plot(mse, '-v')\nplt.xlabel('Number of principal components in regression')\nplt.ylabel('MSE')\nplt.title('Target')\nplt.xlim(xmin=-1);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ecadab5ca1eb355dbc87cf97643d38fd7b52032","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nX=train.drop(['ID', 'target'], axis=1).values\n\ny = log_train_target\n\n\n# As there are > 4000 attributes, I have tested above to how many components this is reaching with 85% variance ( 1000 components contribute) \npca2 = PCA(n_components = 15)\n\n# Split into training and test sets\nX_train, X_test , y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5, random_state=1)\n\n# Scale the data\nX_reduced_train = pca2.fit_transform(scale(X_train))\nn = len(X_reduced_train)\n\n\n\n# 3-fold CV, with shuffle\nkf_3 = model_selection.KFold( n_splits=3, shuffle=True, random_state=1)\n\nmse = []\n\n# Calculate MSE with only the intercept (no principal components in regression)\nscore = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_3, scoring='neg_mean_squared_error').mean()    \nmse.append(score)\n\n# Calculate MSE using CV for the 1000 principle components, adding one component at the time.\nfor i in np.arange(1, 15):\n    score = -1*model_selection.cross_val_score(regr, X_reduced_train[:,:i], y_train.ravel(), cv=kf_3, scoring='neg_mean_squared_error').mean()\n    mse.append(score)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac7fe1a685d5fde01a3cd9ad35d1a4ef62553ddb","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\nplt.plot(np.array(mse), '-v')\nplt.xlabel('Number of principal components in regression')\nplt.ylabel('MSE')\nplt.title('Target')\nplt.xlim(xmin=-1);\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"989fe9c6abb140e50dd467ddcdfae36c318e2d60","collapsed":true},"cell_type":"code","source":"# # Scale the data(Validation test data)\nX_reduced_test = pca2.transform(scale(X_test))\n\n# Train regression model on training data \nregr = LinearRegression()\nregr.fit(X_reduced_train[:,:15], y_train)\n\n# Prediction with validation_test data\n# Scale the data(Validation test data)\n\npred = regr.predict(X_reduced_test)\nrmse = sqrt(mean_squared_error(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c78d6090ba9713db604bc7720a00338fc99bea2d"},"cell_type":"code","source":"# rmse = 1.692 - Leaderboad score gave 1.73","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67d84bafd6c0a2012984443a5de9a4a25e8d2a5e","collapsed":true},"cell_type":"code","source":"## prepare this - predictions on the final test dataset\ntest_X = test.drop( [\"ID\"], axis=1)\nfinal_X_reduced_test = pca2.transform(scale(test_X))[:,:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a2209480908612c4f637b4dc42384eb56f15dbaa"},"cell_type":"code","source":"pred_final_test = regr.predict(final_X_reduced_test)\npred_final_test = np.expm1(pred_final_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"021ef339ea3e95259b4cac207f227e4a5e95757e"},"cell_type":"markdown","source":"### Partial Least Squares Regression"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"88dacf0b4457189973fc4b02db1634b74ac92843"},"cell_type":"code","source":"# n = len(X_train)\n\n# # 10-fold CV, with shuffle\n# kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n\n# mse = []\n\n# for i in np.arange(1, 100):\n#     pls = PLSRegression(n_components=i)\n#     score = model_selection.cross_val_score(pls, scale(X_train), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n#     mse.append(-score)\n\n# # Plot results\n# plt.plot(np.arange(1, 100), np.array(mse), '-v')\n# plt.xlabel('Number of principal components in regression')\n# plt.ylabel('MSE')\n# plt.title('Target')\n# plt.xlim(xmin=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58118e14a2e980dfb409dc120569f14163206368"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nX=train.drop(['ID', 'target'], axis=1).values\n\ny = log_train_target\n# Split into training and test sets\nX_train, X_test , y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28d348f2a3a5c1967c5c1f94e3d5fef39995d42d"},"cell_type":"code","source":"pls = PLSRegression(n_components=24)\npls.fit(scale(X_train), y_train)\n\nsqrt(mean_squared_error(y_test, pls.predict(scale(X_test))))\n## 2.68 - not a good score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7ba9be39ea110d76c68f369f76ce87e627fc30ae"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true,"_uuid":"d581476ea1fb850159edf92da01876b1a0d5fc26"},"cell_type":"code","source":"#Import Library\nfrom sklearn.ensemble import RandomForestRegressor #use RandomForestRegressor for regression problem\nmodel= RandomForestRegressor(n_estimators=1000,  n_jobs = -1,random_state =50, max_features = \"auto\",\n                                 min_samples_leaf = 1)\n\n# Train the model using the training sets and check score\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nX=train.drop(['ID', 'target'], axis=1).values\n\ny = log_train_target\n\n# Split into training and test sets\nX_train, X_test , y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5, random_state=1)\n\nmodel.fit(X_train, y_train)\n\n #Predict Output\nprint(\"RMSE : \", sqrt(mean_squared_error(y_test,  model.predict(X_test))))  \n\n\n\n#Predict Output (test)\ntest1=test.drop(['ID'], axis=1).values\npred_final_test= model.predict(test1) \npred_final_test = np.expm1(pred_final_test)\n\n# Making a submission file #\nsub_df = pd.DataFrame({\"ID\":test[\"ID\"].values})\nsub_df[\"target\"] = pred_final_test\nsub_df.to_csv(\"baseline_RandomForest.csv\", index=False)\n\n# RMSE :  1.4634184706155577","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e79fbdf94643d4d4a8bd499be0b35044790dd30c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ec808308702a80433fab61d856491142cd48de3d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4d1da25f6e9ead281e22597d73ef16722674a2e3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"49bf6b41d0408eb38166442ea5d2086412f30892"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"593dc08214364dfb11d761adcd28a51f9e931560"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}