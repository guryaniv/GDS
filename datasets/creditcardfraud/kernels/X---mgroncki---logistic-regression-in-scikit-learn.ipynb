{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"6d3b41843580ae037846b7079f2fd79d7add5147","_cell_guid":"41650b3a-dff8-4d14-9595-6b3fb4ccdea7"},"cell_type":"markdown","source":"## Load data and visualize the data"},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"credit_card = pd.read_csv('../input/creditcard.csv')","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"c4e55ed51d60613c4f19457af0131b4fb973fad9","_cell_guid":"fd1b18c4-d0bf-4d81-b784-3fcec2578c12","trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(7, 5))\nsns.countplot(x='Class', data=credit_card)\n_ = plt.title('# Fraud vs NonFraud')\n_ = plt.xlabel('Class (1==Fraud)')","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"5b132aff2b6d28b35e2c0ff256a3f97ff74f3cb3","_cell_guid":"19175b74-8157-418c-b6b0-6c2a1b719814"},"cell_type":"markdown","source":"As we can see we have mostly non-fraudulent transactions. Such a problem is also called inbalanced class problem.\n\n99.8% of all transactions are non-fraudulent. The easiest classifier would always predict no fraud and would be in almost all cases correct. Such classifier would have a very high accuracy but is quite useless."},{"metadata":{"_uuid":"5ee8b7fcd59def3b33733b5bbfddf39709dfd077","_cell_guid":"0c90460f-5a34-46a0-bd6f-88e699b27d2f","trusted":true},"cell_type":"code","source":"base_line_accuracy = 1-np.sum(credit_card.Class)/credit_card.shape[0]\nbase_line_accuracy","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"8ecd2b9433bc7759f08a3c5c08627a57f7a2a2e3","_cell_guid":"0f4090aa-ccfc-442f-ada5-03466fccbb59"},"cell_type":"markdown","source":"For such an inbalanced class problem we could use over or undersampling methods to try to balance the classes (see inbalance-learn for example: https://github.com/scikit-learn-contrib/imbalanced-learn), but this out of the scope of todays post. We will come back to this in a later post.\n\nAs accuracy is not very informative in this case the AUC (Aera under the curve) a better metric to assess the model quality. The AUC in a two class classification class is equal to the probability that our classifier will detect a fraudulent transaction given one fraudulent and genuiune transaction to choice from. Guessing would have a probability of 50%."},{"metadata":{"collapsed":true,"_uuid":"16728c540fc206821df972fcd071646f48b385e5","_cell_guid":"3de8033f-5ddb-4a12-9230-c1cd01ab68e2","trusted":true},"cell_type":"code","source":"X = credit_card.drop(columns='Class', axis=1)\ny = credit_card.Class.values","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"7fd243cb53c6126ef7001ac3a8c92649b6effd94","_cell_guid":"cacba1af-23cd-4a35-9603-c904387956b4"},"cell_type":"markdown","source":"Due to the construction of the dataset (PCA transformed features, which minimizes the correlation between factors), we dont have any highly correlated features. Multicolinearity could cause problems in a logisitc regression.\n\nTo test for multicolinearity one could look into the correlation matrix (works only for non categorical features) or run partial regressions and compare the standard errors or use pseudo-R^2 values and calculate Variance-Inflation-Factors.\n\n"},{"metadata":{"_uuid":"5c3a98163d12601711b33425f479edfb37a92fca","_cell_guid":"ff310572-91fe-40d7-8370-89088320d9f0","trusted":true},"cell_type":"code","source":"corr = X.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"be916ae070cd9df31f67306364b5faa00de99b79","_cell_guid":"65c81b41-418d-47fa-a484-98e643aaf208"},"cell_type":"markdown","source":"## Logisitc Regression with Sklearn\n\nShort reminder of Logistic Regression:\n\nIn Logisitic Regression the logits (logs of the odds) are assumed to be a linear function of the features\n\n$$L=\\log(\\frac{P(Y=1)}{1-P(Y=1)}) = \\beta_0 + \\sum_{i=1}^n \\beta_i X_i. $$\n\nSolving this equatation for $p=P(Y=1)$ yields to\n\n$$ p = \\frac{\\exp(L)}{1-\\exp(L)}.$$\n\nThe parameters $\\beta_i$ can be derived by Maximum Likelihood Estimation (MLE). The likelihood for a given $m$ observation $Y_j$ is\n\n$$ lkl = \\prod_{j=1}^m p^{Y_j}(1-p)^{1-Y_j}.$$\n\nTo find the maximum of the likelihood is equivalent to the minimize the negative logarithm of the likelihood (loglikelihood).\n\n$$  -llkh = -\\sum_{j=1}^m Y_j \\log(p) + (1-Y_j) \\log(1-p),$$\n\nwhich is numerical more stable. The log-likelihood function has the same form as the cross-entropy error function for a discrete case.\n\nSo finding the maximum likelihood estimator is the same problem as minimizing the average cross entropy error function.\n\nIn SciKit-Learn uses by default a coordinate descent algorithm to find the minimum of L2 regularized version of the loss function (see. http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression).\n\nThe main difference between L1 (Lasso) and L2 (Ridge) regulaziation is, that the L1 prefer a sparse solution (the higher the regulazation parameter the more parameter will be zero) while L2 enforce small parameter values."},{"metadata":{"_uuid":"67c42133bf74c0f3aaa84e49456fe04a2f5e9d96","_cell_guid":"763a278e-91af-4fe6-a255-ddaea0bbd790"},"cell_type":"markdown","source":"## Train the model\n\n### Training and test set\n\nFirst we split our data set into a train and a validation set by using the function train_test_split. The model performace "},{"metadata":{"collapsed":true,"_uuid":"d19b826169ff464f82eba4e0b349bacc4361c8a0","_cell_guid":"d54b91c2-fc78-4b24-a7c8-5b1a974cdf36","trusted":true},"cell_type":"code","source":"np.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(X, y)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"d1ea1658ffca81f4f9c83e9759b90f1311ce7a71","_cell_guid":"88a09d8f-4238-4560-a180-17d048ae5dd7"},"cell_type":"markdown","source":"### Model definition\n\nAs preperation we standardize our features to have zero mean and a unit standard deviation. The convergence of gradient descent algorithm are better. We use the class `StandardScaler`. The class *StandardScaler* has the method `fit_transform()` which learn the mean $\\mu_i$ and standard deviation $\\sigma_i$ of each feature $i$ and return a standardized version $\\frac{x_i - \\mu_i}{\\sigma}$. We learn the mean and sd on the training data. We can apply the same standardization on the test set with the function *transform()*.\n\n\nThe logistic regression is implemented in the class `LogisticRegression`, we will use for now the default parameterization. The model can be fit using the function `fit()`. After fitting the model can be used to make predicitons `predict()` or return the estimated the class probabilities `predict_proba()`.\n\nWe combine both steps into a Pipeline. The pipline performs both steps automatically. When we call the method `fit()` of the pipeline, it will invoke the method `fit_and_transform()` for all but the last step and the method `fit()` of the last step, which is equivalent to:\n\n```python\nlr.fit(scaler.fit_transform(X_train), y_train)\n```\n\nor visualized as a dataflow:\n\n```X_train => scaler.fit_transform(.) => lr.fit(., y_train)```\n\nIf we invoke the method `predict()` of the pipeline its equvivalent to\n\n\n```python\nlr.predict(scaler.transform(X_train))\n```\n\n\n\n"},{"metadata":{"collapsed":true,"_uuid":"86ecda4d1700405da9ed0d8bc6f7d8eabc0f1ddb","_cell_guid":"fb45b18e-9357-4a98-9d98-078f96c3a591","trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nlr = LogisticRegression()\nmodel1 = Pipeline([('standardize', scaler),\n                    ('log_reg', lr)])","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"7c6abdd6d0db2ffede02f01d2d3f1ff916de0eee","_cell_guid":"804537d0-b635-4e90-a88c-56fac8749e2e"},"cell_type":"markdown","source":"In the next step we fit our model to the training data"},{"metadata":{"_uuid":"bc769713962566efac8b2310f45d7ebe79dcb173","_cell_guid":"b8f46ca5-f69a-4755-abbf-9cc76723c133","trusted":true},"cell_type":"code","source":"model1.fit(X_train, y_train)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"dc612edede19c594a8f28e3c396f0797a98888ba","_cell_guid":"4c3a4e42-36ee-4814-b8b2-0513ed7404cb"},"cell_type":"markdown","source":"### Training score and Test score\n\n`confusion_matrix()` returns the confusion matrix, C where $C_{0,0}$ are the true negatives (TN) and $C_{0,1}$ the false positives (FP) and vice-versa  for the positives in the 2nd row. We use the function `accurary_score()` to calculate the accuracy our models on the train and test data. We see that the accuracy is quite high (99,9%) which is expected in such an unbalanced class problem. With the method `roc_auc_score()`can we get the area under the receiver-operator-curve (AUC) for our simple model."},{"metadata":{"_uuid":"e4f22259d9018f233874d9e4afdb42ec882bd967","_cell_guid":"f708ecc2-9114-4b5e-86fe-bd8423aa622c","trusted":true},"cell_type":"code","source":"y_train_hat = model1.predict(X_train)\ny_train_hat_probs = model1.predict_proba(X_train)[:,1]\ntrain_accuracy = accuracy_score(y_train, y_train_hat)*100\ntrain_auc_roc = roc_auc_score(y_train, y_train_hat_probs)*100\nprint('Confusion matrix:\\n', confusion_matrix(y_train, y_train_hat))\nprint('Training accuracy: %.4f %%' % train_accuracy)\nprint('Training AUC: %.4f %%' % train_auc_roc)","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"61f082bfd5aef408353ac404e2e18d0ceea4b793"},"cell_type":"markdown","source":"![](http://)Our model is able to detect 68 fraudulent transactions out of 113 (recall of 60%) and produce 12 false alarms (<0.02%) on the test data."},{"metadata":{"_uuid":"a83a2229893813ad2986910d20f562b369b76eb4","_cell_guid":"ca356e72-46ac-4766-8640-11d644c4534f","trusted":true},"cell_type":"code","source":"y_test_hat = model1.predict(X_test)\ny_test_hat_probs = model1.predict_proba(X_test)[:,1]\ntest_accuracy = accuracy_score(y_test, y_test_hat)*100\ntest_auc_roc = roc_auc_score(y_test, y_test_hat_probs)*100\nprint('Confusion matrix:\\n', confusion_matrix(y_test, y_test_hat))\nprint('Training accuracy: %.4f %%' % test_accuracy)\nprint('Training AUC: %.4f %%' % test_auc_roc)","execution_count":89,"outputs":[]},{"metadata":{"_uuid":"c729b8843460c25410c80294c91c4a7b31b70c7b"},"cell_type":"markdown","source":"With the function `classification_report()` we print get the precision, recall per each class."},{"metadata":{"_uuid":"258dda7a672ec39dd33f095dbc59cc42832f50b0","_cell_guid":"26394d30-80ec-4db0-929c-0a81585d4d66","trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_test_hat, digits=6))","execution_count":69,"outputs":[]},{"metadata":{"_uuid":"9bf995d6629899b3190446a5cb695f1b854ac935"},"cell_type":"markdown","source":"To visualize the Receiver-Operator-Curve we use the function `roc_curve`. The method returns the true positive rate (recall) and the false positive rate (probability for a false alarm) for a bunch of different thresholds. This curve shows the trade-off between recall (detect fraud) and false alarm probability.\n\nIf we classifiy all transaction as fraud, we would have a recall of 100% but also the highest false alarm rate possible (100%). The naive way to minimize the false alarm probability is to classify all transaction as legitime. **"},{"metadata":{"_uuid":"9317259c2c241aa6cb00346b5b91bd485d7ec448","_cell_guid":"d5db8aad-8866-4639-9a1c-cd32e33e11a1","trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(y_test, y_test_hat_probs, drop_intermediate=True)\n\nf, ax = plt.subplots(figsize=(9, 6))\n_ = plt.plot(fpr, tpr, [0,1], [0, 1])\n_ = plt.title('AUC ROC')\n_ = plt.xlabel('False positive rate')\n_ = plt.ylabel('True positive rate')\nplt.style.use('seaborn')\n\nplt.savefig('auc_roc.png', dpi=600)","execution_count":15,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"9e1692cf8dd385592b20e30f229ff192a30ce5c7","_cell_guid":"9d834f9c-5817-4df8-b4a4-e31103236daf","trusted":false},"cell_type":"markdown","source":"Our model classify all transaction with a fraud probability => 50% as fraud. If we choose the threshold higher, we could reach a lower false positive rate but we would also miss more fraudulent transactions. If we choose the thredhold lower we can catch more fraud but need to investigate more false positives.\n\nDepending on the costs for each error, it make sense to select another threshold.\n\nIf we set the threshold to 90% the recall decrease from 60% to 45%. while the false positve rate is the same. We can see that our model assign some non-fraudulent a very high probability to be fraud"},{"metadata":{"trusted":true,"_uuid":"301d90a2e2642de98e870bb3b478447edf77c359"},"cell_type":"code","source":"y_hat_90 = (y_test_hat_probs > 0.90 )*1\nprint('Confusion matrix:\\n', confusion_matrix(y_test, y_hat_90))\nprint(classification_report(y_test, y_hat_90, digits=6))\n","execution_count":87,"outputs":[]},{"metadata":{"_uuid":"28a041cd50ec592b6049337ea2f432d56249f04d"},"cell_type":"markdown","source":"If we set the threshold down to 10%, we can detect around 75% of all fraud case but almost double our false positive rate (now 25 false alarms)"},{"metadata":{"trusted":true,"_uuid":"81062209f3f2161bf70a48d4839da22da5d49f3d"},"cell_type":"code","source":"y_hat_10 = (y_test_hat_probs > 0.10)*1\nprint('Confusion matrix:\\n', confusion_matrix(y_test, y_hat_10))\nprint(classification_report(y_test, y_hat_10, digits=4))","execution_count":92,"outputs":[]},{"metadata":{"_uuid":"a5efd7e296f4f60621ff067abdf25ba399f0d0a5"},"cell_type":"markdown","source":"Where to go from here?\n-------------------\n\nWe just scratched the surface of sklearn and logistic regression. For example we could spent much more time with the \n\n- feature selection / engineering (which is a bit hard without any background information about the features),\n- we could try techniques to counter the data inbalance and \n- we could use cross-validation to fine tune the hyperparameters (regulaziation constant C) or\n- try a different regulization (Lasso/Elastic Net) or \n- optimizer (stochastic gradient desent instead of coordinate descnet)\n- adjust class weights to move the decision boundary (make missed frauds more expansive in the loss function)\n- and finally we could try different classifer models in sklearn like decision trees, random forrests, knn,  naive bayes or support vector machines. \n\nBut for now we will stop here and will implement a logisitc regression model with stochastic gradient descent in TensorFlow and then extend it to a neural net.\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}