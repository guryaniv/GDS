{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d9e53332-bc26-4c84-e665-c2010ecb13f3"
      },
      "source": [
        "This is a quick kernel with kNN imputation and a GradientBoostingRegressor that achieved ~.33 on the LB. \n",
        "\n",
        "NOTE: I am having some issues running this on Kaggle so I apologize if errors are currently visible. \n",
        "FancyImpute is easier to work with offline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a3b685a1-d350-5af1-cc71-816867dfe038"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.utils import shuffle\n",
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "pd.set_option(\"display.max_columns\",len(test))\n",
        "train.shape,test.shape\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "33147a90-6b7b-3ab2-fc24-0ee910f28cfe"
      },
      "outputs": [],
      "source": [
        "missing_vals = pd.concat([train.isnull().sum()/len(train),test.isnull().sum()/len(test)],axis=1, keys=['Train','Test'])\n",
        "missing_vals.sort_values(ascending=False,by=\"Train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ee75ff64-88bf-e39f-4120-07c38a93979d"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "def time_stamp(df):\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "    df[\"year\"] = df[\"timestamp\"].dt.year\n",
        "    df[\"month\"] =  df[\"timestamp\"].dt.month\n",
        "    df[\"day\"] = df[\"timestamp\"].dt.day \n",
        "    del df[\"timestamp\"]\n",
        "\n",
        "time_stamp(train)\n",
        "time_stamp(test)\n",
        "\n",
        "\n",
        "\n",
        "train_cont = [x for x in train.columns if train.dtypes[x] != 'object' and (x not in ['day','month','year'])]\n",
        "test_cont = [x for x in test.columns if test.dtypes[x] != 'object' and (x not in ['day','month','year'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ab8a9f47-ff79-dfeb-fd4e-385add113146"
      },
      "outputs": [],
      "source": [
        "\"\"\"Examining Correlations\"\"\"\n",
        "pd.DataFrame(train.corr()[\"price_doc\"].sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fcf88bdc-e953-9c15-89ca-e17032b00687"
      },
      "source": [
        "Based on missing values and correlations, the following variables were removed to reduce noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "af35edab-357a-fc54-1aed-66d913f0ba09"
      },
      "outputs": [],
      "source": [
        "del_low_corr = [\"trc_sqm_5000\",\"prom_part_500\",\"build_count_1971-1995\",\n",
        "               \"school_quota\",\"ID_railroad_station_walk\",\"cemetery_km\",\"water_km\",\n",
        "               \"big_church_count_500\",\"cafe_sum_3000_max_price_avg\",\n",
        "               \"cafe_avg_price_3000\",\"cafe_sum_3000_min_price_avg\",\n",
        "               \"build_count_1921-1945\",\"16_29_male\",\n",
        "               \"female_f\",\"full_all\",\"ID_bus_terminal\",\"ID_railroad_station_avto\",\n",
        "               \"ID_big_road1\",\"ID_big_road2\",\"trc_count_500\",\"trc_count_1000\",\n",
        "               \"trc_sqm_1500\",\"cafe_count_500_price_4000\",\"cafe_count_500_price_2500\",\n",
        "               \"market_count_5000\",\"hospital_beds_raion\",\n",
        "               \"cafe_avg_price_500\",\"cafe_sum_500_max_price_avg\",\n",
        "               \"cafe_sum_500_min_price_avg\",\"preschool_quota\",\"cafe_count_1500\"]\n",
        "for i in del_low_corr:\n",
        "    del train[i]\n",
        "    del test[i]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "68eee4d7-4731-6ca3-1f74-1f420b644462"
      },
      "outputs": [],
      "source": [
        "for i in train.skew().keys():\n",
        "    if abs(train.skew()[i]) > .5 and (i!= 'price_doc'):\n",
        "        train[i] = np.log1p(train[i])\n",
        "        test[i] = np.log1p(test[i])\n",
        "        print(\"Just finished {} with skew {}\".format(str(i), str(train.skew()[i])))\n",
        "        #Distribution plots if needed\n",
        "        #sns.distplot(train[i].dropna())\n",
        "        #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "21b07987-711e-262c-2219-bf5c8bbad48a"
      },
      "outputs": [],
      "source": [
        "train = pd.get_dummies(train)\n",
        "test = pd.get_dummies(test)\n",
        "\n",
        "sns.distplot(train.price_doc)\n",
        "plt.title(\"price\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "train.price_doc = np.log1p(train.price_doc)\n",
        "\n",
        "sns.distplot(train.price_doc)\n",
        "plt.title(\"log transformed price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1ebfbe12-5e83-123f-c357-b47649398c21"
      },
      "source": [
        "Slight improvements are seen in the distribution after the transformation from skew  > 1.7 to -.88\n",
        "This should not heavily effect GBR model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4266e8fe-8b39-ad78-87da-a44d28ee13a2"
      },
      "outputs": [],
      "source": [
        "\"\"\"from fancyimpute import  KNN\n",
        "\n",
        "train.material = train.material.fillna(1) #As none are missing in test.\n",
        "\n",
        "train_columns = list(train) #fancyimpute removes var names\n",
        "test_columns = list(test)\n",
        "\n",
        "train = pd.DataFrame(KNN(k=3).complete(train))\n",
        "test = pd.DataFrame(KNN(k=3).complete(test))\n",
        "\n",
        "train.columns = train_columns \n",
        "test.columns = test_columns\n",
        "\n",
        "#Make material an object again. \n",
        "train.material = train.material.astype(\"object\")\n",
        "test.material = test.material.astype(\"object\")\n",
        "train = pd.get_dummies(train)\n",
        "test = pd.get_dummies(test)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e85b8b7d-9aa4-d738-dd0f-53ae64fccf99"
      },
      "source": [
        "Since the data set has high multicollinearity, a PLSRegression model will be fit to the data, followed by a GBR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae8ba45f-50c6-0072-4514-c1cd71f5bead"
      },
      "outputs": [],
      "source": [
        "\"\"\"my_ids = train['id']\n",
        "test_id = test['id']\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.preprocessing import scale\n",
        "X = np.array(train.drop([\"price_doc\",\"id\"],axis=1))\n",
        "y = np.array(train[\"price_doc\"])\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .25,random_state=1)\n",
        "\n",
        "pls = PLSRegression(n_components=20,scale=False)\n",
        "pls.fit(X_train,y_train)\n",
        "y_pred = pls.predict(X_test)\n",
        "r2_score(y_test,y_pred),mean_squared_error(y_test,y_pred)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd05f53d-b287-1776-b025-24cf916e4fc6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gb = GradientBoostingRegressor(loss='huber',learning_rate=.05,n_estimators=500,\n",
        "                              max_features='sqrt',min_samples_leaf=10)\n",
        "                                #max_features = 'sqrt' for multicollinearity.\n",
        "\"\"\"gb.fit(X_train,y_train)\n",
        "gb_pred = gb.predict(X_test)\n",
        "r2_score(y_test,gb_pred),mean_squared_error(y_test,gb_pred)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ea430526-f203-df4f-0160-3e14dfdbe5c6"
      },
      "source": [
        "the fancyimpute kNN class does not seem to be working on Kaggle, but un-commenting the above code will work as a starter implementation for those wishing to implement this in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4ba2dce9-ef96-4943-bd94-1544c72d1230"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}