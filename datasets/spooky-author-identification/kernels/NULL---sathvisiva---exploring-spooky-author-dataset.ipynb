{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["Hello Everyone !! This is going to my learning from other kernels\n", "\n", "We have to identify author of the given sentence. This is multiclass classification problem\n", "\n", "\n", "The competition dataset contains text from works of fiction written by spooky authors of the public domain:\n", "\n", "* Edgar Allan Poe (EAP)\n", "* HP Lovecraft (HPL)\n", "* Mary Wollstonecraft Shelley (MWS)"], "metadata": {"_uuid": "299f38504d7b688961cc9262ec60642a11793f1d", "_cell_guid": "61b0d81d-3a42-419e-9ab3-e26265da6299"}}, {"cell_type": "markdown", "source": ["First, lets load all the libraries needed"], "metadata": {"_uuid": "5cda902fd6926cd892d625aaec4684ba098299a0", "_cell_guid": "6fee345f-b3ec-4f59-bf6a-96d0bd4f956f"}}, {"cell_type": "code", "execution_count": null, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt # plotting\n", "import seaborn as sns # plotting\n", "%matplotlib inline\n", "\n", "import string\n", "\n", "#plotly related \n", "import plotly.offline as py\n", "import plotly.graph_objs as go\n", "import plotly.tools as tls\n", "import plotly.figure_factory as ff\n", "py.init_notebook_mode(connected = True)\n", "\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from wordcloud import WordCloud, STOPWORDS\n", "from nltk.tokenize import sent_tokenize,word_tokenize\n", "from collections import Counter\n", "\n", "import scipy\n", "\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn import ensemble, metrics, model_selection, naive_bayes\n", "from sklearn.decomposition import TruncatedSVD\n", "import xgboost as xgb\n", "from sklearn.metrics import confusion_matrix\n", "\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "metadata": {"_uuid": "70c98c88939b9c9ed0d34d03c2e82942d5699f0c", "_cell_guid": "1885ff11-9bc7-4adf-8f08-aacf88439680"}}, {"cell_type": "markdown", "source": ["***Helper Functions***\n", "These function will be useful in the later steps"], "metadata": {"_uuid": "e2392ce1c2b615bc9479f02cd965b7d9cd607286", "_cell_guid": "16e35d72-e1cf-493a-ab66-b26827863d1f"}}, {"cell_type": "code", "execution_count": null, "source": ["#function to display plots\n", "def displayPlots(type='bar',x=None, y=None, data=None, hue=None, color=None, title=None, figsize = (11,8), rotate = 0):\n", "    fig, ax = plt.subplots(figsize = figsize)\n", "    if type == 'bar':\n", "        ax = displySnsBarPlot(x=x, y=y, data =data,hue=hue, color = color)\n", "    elif type == 'violin':\n", "        ax = displaySnsViolinPlot(x=x, y=y, data =data,hue=hue, color = color)\n", "    elif type == 'box':\n", "        ax = displaySnsBoxPlot(x=x, y=y, data =data,hue=hue, color = color)\n", "    elif type == 'strip':\n", "        ax = displySnsStripPlot(x=x, y=y, data =data,hue=hue, color = color)\n", "    plt.xticks(rotation = 45)\n", "    \n", "def displySnsBarPlot(x=None, y=None, data=None, hue=None, color=None):\n", "    sns.barplot(x =x, y=y, data =train, hue = hue, color=color)\n", "\n", "def displaySnsViolinPlot(x=None, y=None, data=None, hue=None, color=None):\n", "    sns.violinplot(x =x, y=y, data =train, hue = hue, color=color)\n", "    \n", "def displaySnsBoxPlot(x=None, y=None, data=None, hue=None, color=None):\n", "    sns.boxplot(x =x, y=y, data =train, hue = hue, color=color)\n", "\n", "def displySnsStripPlot(x=None, y=None, data=None, hue=None, color=None):\n", "    sns.stripplot(x =x, y=y, data =train, hue = hue, color=color, jitter =True)\n", "\n", "def displayWordCloud(data = None, backgroundcolor = 'black',  width=800, height=600 ):\n", "    wordcloud = WordCloud(stopwords = STOPWORDS, background_color = backgroundcolor, \n", "                         width = width, height = height).generate(data)\n", "    plt.figure(figsize = (15 , 10))\n", "    plt.imshow(wordcloud)\n", "    plt.axis(\"off\")\n", "    plt.show()\n", "    \n", "def runNM(train_X, train_y, test_X, test_y, test_X2 ):\n", "    print(train_X)\n", "    model = naive_bayes.MultinomialNB()\n", "    model.fit(train_X, train_y)\n", "    pred_text_y = model.predict_proba(test_X)\n", "    pred_text_y2 = model.predict_proba(test_X2)\n", "    return pred_text_y, pred_text_y2, model\n", "\n", "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n", "    param = {}\n", "    param['objective'] = 'multi:softprob'\n", "    param['eta'] = 0.1\n", "    param['max_depth'] = 3\n", "    param['silent'] = 1\n", "    param['num_class'] = 3\n", "    param['eval_metric'] = \"mlogloss\"\n", "    param['min_child_weight'] = child\n", "    param['subsample'] = 0.8\n", "    param['colsample_bytree'] = colsample\n", "    param['seed'] = seed_val\n", "    num_rounds = 2000\n", "\n", "    plst = list(param.items())\n", "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n", "\n", "    if test_y is not None:\n", "        xgtest = xgb.DMatrix(test_X, label=test_y)\n", "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n", "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n", "    else:\n", "        xgtest = xgb.DMatrix(test_X)\n", "        model = xgb.train(plst, xgtrain, num_rounds)\n", "\n", "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n", "    if test_X2 is not None:\n", "        xgtest2 = xgb.DMatrix(test_X2)\n", "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n", "    return pred_test_y, pred_test_y2, model\n", "    "], "outputs": [], "metadata": {"_uuid": "eb5390ba9673731731dc8eddf0673941c9a896d9", "_cell_guid": "f07fb88e-4f67-4a99-bd73-35d1c6cf5141", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["# loading train and test dataset\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n"], "outputs": [], "metadata": {"_uuid": "2281ccb1b48f457b4f970dede0fe32ca00f9559c", "_cell_guid": "4d457f9f-1698-416a-b2e0-ccee4f732296", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["# lets look at the few entries in train and test dataset\n", "print(train.head())\n", "print(test.head())"], "outputs": [], "metadata": {"_uuid": "66aea8438826bfc3dda53ea6e1d248c740bb2f8a", "_cell_guid": "d9d98f54-41da-4e93-8381-cd9de55ca04b"}}, {"cell_type": "code", "execution_count": null, "source": ["#Checking for missing values in train and test dataset\n", "print(train.isnull().sum())\n", "print(test.isnull().sum())"], "outputs": [], "metadata": {"_uuid": "5fecdd947d84c00868136ee08224d1d0b0e21899", "_cell_guid": "d9e32e24-3176-44d1-a9a8-e286926cec86"}}, {"cell_type": "code", "execution_count": null, "source": ["#there are no missing value in both training and test data\n", "#lets see how data are distributed among authors in training data\n", "displayPlots(type = 'bar', x =train.author.unique(), y = train.author.value_counts().values, title = \"Distribution of words among authors\" )"], "outputs": [], "metadata": {"_uuid": "8a7f5efd45e604e102be9d2eee803b2ed220d392", "_cell_guid": "fa414660-77e3-445c-aa34-cc33222c4e32"}}, {"cell_type": "code", "execution_count": null, "source": ["#given dataset is quite balanced\n", "#lets see some of the sentences writtern by each author\n", "train_eap = train[train[\"author\"] == 'EAP'][\"text\"]\n", "train_hpl = train[train[\"author\"] == 'HPL'][\"text\"]\n", "train_mws = train[train[\"author\"] == 'MWS'][\"text\"]\n", "\n", "print(\"EAP\")\n", "print(train_eap.sample(3).values,'\\n')\n", "print(\"HPL\")\n", "print(train_hpl.sample(3).values,'\\n')\n", "print(\"MWS\")\n", "print(train_mws.sample(3).values,'\\n')"], "outputs": [], "metadata": {"_uuid": "f56fb2c92e31278dc575e38c6009d41038836cdf", "_cell_guid": "52dbe560-f866-4292-adb0-00e6718d76de"}}, {"cell_type": "code", "execution_count": null, "source": ["#display all words using wordcloud\n", "displayWordCloud(' '.join(train['text']))"], "outputs": [], "metadata": {"_uuid": "56056fd1e0849feaad043b80fe9048bd5380767a", "_cell_guid": "739fda33-5215-448c-978f-40ad2d850dcb"}}, {"cell_type": "code", "execution_count": null, "source": ["#display words used by EAP using wordcloud\n", "displayWordCloud(' '.join(train_eap))"], "outputs": [], "metadata": {"_uuid": "3a31a42d3fcc3d384cd382224d8e3fbc0b23cb28", "_cell_guid": "46150542-1f3e-4ac2-8f2f-677e2d0a3c10"}}, {"cell_type": "code", "execution_count": null, "source": ["displayWordCloud(' '.join(train_hpl))"], "outputs": [], "metadata": {"_uuid": "c4dbb5c755dfcfdd57bfad78ce4c4f8370fcb40c", "_cell_guid": "63437b45-330a-404a-a05b-75e0e26d0a67"}}, {"cell_type": "code", "execution_count": null, "source": ["displayWordCloud(' '.join(train_mws))"], "outputs": [], "metadata": {"_uuid": "32fb5104bb055adcc31ca864765332480f171f7a", "scrolled": true, "_cell_guid": "373c168f-2d55-4b8a-a567-6182358ff391"}}, {"cell_type": "code", "execution_count": null, "source": ["\n", "# calculate different measures of the quantity of information\n", "train['sentences'] = train.text.transform(lambda x: len(sent_tokenize(x)))\n", "train['words'] = train.text.transform(lambda x: len(word_tokenize(x)))\n", "train['text_len'] = train.text.transform(lambda x: len(x))\n", "\n", "test['sentences'] = test.text.transform(lambda x: len(sent_tokenize(x)))\n", "test['words'] = test.text.transform(lambda x: len(word_tokenize(x)))\n", "test['text_len'] = test.text.transform(lambda x: len(x))\n", "\n", "train.groupby(\"author\")[['sentences','words','text_len']].sum().plot.bar(subplots = True, layout=(1,3), figsize=(16,8))"], "outputs": [], "metadata": {"_uuid": "972c080b2bc2b09f6ae3732f41bff94b7e795f55", "_cell_guid": "e0f6db56-8e76-4234-aa65-68775646d7a7"}}, {"cell_type": "code", "execution_count": null, "source": ["#length of unique words\n", "train[\"unique_words\"] = train[\"text\"].apply(lambda x : len(set(x.split())))\n", "test[\"unique_words\"] = test[\"text\"].apply(lambda x : len(set(x.split())))\n", "#length of stop words\n", "eng_stopwords = set(stopwords.words(\"english\"))\n", "train[\"stop_words\"] = train[\"text\"].apply(lambda x : len([w for w in str(x).split() if w in eng_stopwords]))\n", "test[\"stop_words\"] = test[\"text\"].apply(lambda x : len([w for w in str(x).split() if w in eng_stopwords]))\n", "\n", "#length of punctuations\n", "train[\"punctuations\"] = train[\"text\"].apply(lambda x : len([w for w in str(x) if w in string.punctuation]))\n", "test[\"punctuations\"] = test[\"text\"].apply(lambda x : len([w for w in str(x) if w in string.punctuation]))\n", "\n", "#upper case letters\n", "train[\"upper_case\"] = train[\"text\"].apply(lambda x : len([w for w in str(x).split() if w.isupper()]))\n", "test[\"upper_case\"] = test[\"text\"].apply(lambda x : len([w for w in str(x).split() if w.isupper()]))\n", "\n", "#title letters\n", "train[\"Title\"] = train[\"text\"].apply(lambda x : len([w for w in str(x).split() if w.istitle()]))\n", "test[\"Title\"] = test[\"text\"].apply(lambda x : len([w for w in str(x).split() if w.istitle()]))\n", "\n", "train.groupby('author')[['unique_words' , 'stop_words' , 'punctuations' , 'upper_case']].sum().plot.bar(subplots = True, layout = (1,4), figsize = (16,8))\n", "\n", "\n"], "outputs": [], "metadata": {"_uuid": "f165c0742ddef3f87bd9e8bd686fbb3193f6f6be", "_cell_guid": "7aeff343-c5ea-4353-a45e-283993afb5b9"}}, {"cell_type": "markdown", "source": ["Lets create base model and check how well it is performing using the features which we have generated in the above steps"], "metadata": {"_uuid": "555f042903f0d61eaa4efb153671f33e75c254d3", "_cell_guid": "b6a5c055-23e1-4bd5-87bd-01beb1328e4d"}}, {"cell_type": "code", "execution_count": null, "source": ["train_y = train.author.map({'EAP' : 0, 'HPL' : 1, 'MWS' : 2})\n"], "outputs": [], "metadata": {"_uuid": "35bbce0c64172305c5bcd7739468e2e0fc47a4fd", "_cell_guid": "fd9a3aac-04ac-4677-94dc-71670e6c548e", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["vect = CountVectorizer(ngram_range=(1,2),min_df = 5).fit(train['text'])\n", "train_vectorized = vect.transform(train['text'])\n", "len(vect.get_feature_names())"], "outputs": [], "metadata": {}}, {"cell_type": "code", "execution_count": null, "source": ["stack = train[['sentences' , 'words' , 'text_len' ]]\n", "train_vectorized = scipy.sparse.hstack([train_vectorized,stack])\n", "model = LogisticRegression(C=.03,n_jobs = -1)\n", "model.fit(train_vectorized , train['author'])"], "outputs": [], "metadata": {}}, {"cell_type": "code", "execution_count": null, "source": ["test_vectorized = vect.transform(test['text'])\n", "stack1 = test[['sentences' , 'words' , 'text_len' ]]\n", "test_vectorized = scipy.sparse.hstack([test_vectorized,stack1])\n"], "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["df = pd.DataFrame(model.predict_proba(test_vectorized), index= test['id'] , columns = ['EAP','HLP','MWS'])\n", "df.head()"], "outputs": [], "metadata": {}}, {"cell_type": "code", "execution_count": null, "source": ["tfidf_vec = TfidfVectorizer(stop_words = 'english', ngram_range = (1,3))\n", "full_tfidfvet = tfidf_vec.fit_transform(train[\"text\"].values.tolist() + test[\"text\"].values.tolist())\n", "train_tfidfvec = tfidf_vec.fit_transform(train[\"text\"].values.tolist())\n", "test_tfidfvec = tfidf_vec.fit_transform(test[\"text\"].values.tolist())"], "outputs": [], "metadata": {"_uuid": "30be711a1b8383d36ab163316f73fca43249e564", "_cell_guid": "99ea1f02-f7d4-4af6-843c-72b47774f2e5", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["n_comp = 20\n", "svd_obj = TruncatedSVD(n_components = n_comp, algorithm = 'arpack')\n", "svd_obj.fit(full_tfidfvet)\n", "train_svd = pd.DataFrame(svd_obj.fit_transform(train_tfidfvec))\n", "test_svd = pd.DataFrame(svd_obj.fit_transform(test_tfidfvec))\n", "\n", "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n", "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n", "train  = pd.concat([train , train_svd], axis = 1)\n", "test  = pd.concat([test , test_svd], axis = 1)\n", "\n", "del full_tfidfvet, train_tfidfvec, test_tfidfvec, train_svd, test_svd"], "outputs": [], "metadata": {"_uuid": "d98cb7d2d6b070bf181e9cbfa033671838baa7a5", "_cell_guid": "9ff3e21c-ddd6-4a6a-99e5-7df44dc0f7a7", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["cols_to_drop = ['id', 'text']\n", "\n", "train_X = train.drop(cols_to_drop + ['author'], axis = 1)\n", "test_X = test.drop(cols_to_drop, axis = 1)\n", "\n", "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train.shape[0], 3])\n", "for dev_index, val_index in kf.split(train_X):\n", "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    break\n", "print(\"cv scores : \", cv_scores)"], "outputs": [], "metadata": {"_uuid": "6d0a79d88236345c0c5abab3547a1f5162e92492", "_cell_guid": "9e38ff54-3d31-4eea-9840-f47c9f73e15e", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["fig, ax = plt.subplots(figsize = (12,12))\n", "xgb.plot_importance(model, max_num_features = 50, height = 0.8, ax=ax)\n", "plt.show()"], "outputs": [], "metadata": {"_uuid": "becd349b299839d546368d704aaddd9759beebaa", "_cell_guid": "1a7d78a8-ed94-46cf-8a40-5f0385221ff4", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 1007)"], "outputs": [], "metadata": {"_uuid": "e2769b121357a33e6f2d99f798ae4a02ea57ecfc", "_cell_guid": "607b645c-6c72-4655-81c0-2ccba32f0c0b", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["classifier.fit(train_X, train_y)\n", "y_pred = classifier.predict(test_X)"], "outputs": [], "metadata": {"_uuid": "0ea1af87e9223d3fa494fb1632af3912ff16e18a", "_cell_guid": "74accb6b-c730-4b71-859b-836859417c54", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": [], "outputs": [], "metadata": {"_uuid": "a267ac0a8d8394fd6ac8546f18d11bba4fc75219", "_cell_guid": "ef3028e3-091e-4ec5-b49a-6a6a8d3e972c", "collapsed": true}}]}