{"cells":[{"metadata":{"trusted":true,"_uuid":"91b8a000faf13b2ba6658cbf7335caf071fcd50c","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy\nimport gc\nimport progressbar\n\nfrom scipy.stats import ks_2samp, skew, kurtosis\nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler, scale\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\n\nfrom sklearn.decomposition import PCA, TruncatedSVD, FastICA, FactorAnalysis, KernelPCA\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\nfrom sklearn.manifold import TSNE\nfrom sklearn.grid_search import GridSearchCV\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30c8ad8313c629a9884c03a9871ea33e86faa6ab"},"cell_type":"markdown","source":"### **Read in the data**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"39d5aaf04dd8b91068b1307160729b1a3694e302"},"cell_type":"code","source":"def read_data():\n    print(\"############# Read the data #############\")\n    \n    train = pd.read_csv(\"../input/train.csv\", index_col = 0)\n    test = pd.read_csv(\"../input/test.csv\", index_col = 0)\n    print(\"\\nTrain shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea635a7a05af5901840c842f716f28ae74e92134"},"cell_type":"markdown","source":"### **Convert and drop target column**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"2d6000c3cb8fdf145f7c204a419bdc0cf8345734"},"cell_type":"code","source":"def convert_and_drop_target(train, test):\n    print(\"############# Convert and drop target column #############\")\n    \n    y_train = train.target\n    y_train = np.log1p(y_train)\n    test_ID = test.index\n\n    train = train.drop(['target'], 1)\n    \n    return train, test, y_train, test_ID","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c009fd6e20d3cd5360f659571619c24542d11295"},"cell_type":"markdown","source":"### **Removing duplicate columns**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"cb894fb7440210b1a887cb568bda2789feec48d7"},"cell_type":"code","source":"def remove_duplicate_columns(train, test):\n    print(\"############# Remove duplicate columns #############\")\n    \n    train = train.T.drop_duplicates().T\n    columns_not_to_be_dropped = train.columns\n    columns_to_be_dropped = [col for col in test.columns if col not in columns_not_to_be_dropped]\n    print(\"Number of columns removed - \" + str(len(columns_to_be_dropped)))\n    \n    test = test.drop(columns_to_be_dropped, 1)\n    print(\"\\nTrain shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04aec17979791bcc2d9649cb17b5fa90bd57b9eb"},"cell_type":"markdown","source":"### **Removing columns with constant values**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3c350fc723756c95390587390a42d7e50e994851"},"cell_type":"code","source":"def remove_constant_columns(train, test):\n    print(\"############# Remove constant columns #############\")\n    \n    col_with_std_zero = train.loc[:, train.std(axis = 0) == 0].columns\n    print(\"Number of columns removed - \" + str(len(col_with_std_zero)))\n    \n    train = train.loc[:, train.std(axis = 0) != 0]\n    test = test.drop(col_with_std_zero, 1)\n    print(\"\\nTrain shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"446dc98d51e1af7c8a8966fc01c6c0aecf622401"},"cell_type":"markdown","source":"### **Removing columns having low importance**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"a439d49127e5861d37a41c0cdd0e6608e9535b4b"},"cell_type":"code","source":"def remove_features_using_importance(x_train, y_train, x_test):\n    print(\"############# Remove columns with low importance #############\")\n    \n    def rmsle(actual, predicted):\n        return np.sqrt(np.mean(np.power(np.log1p(actual)-np.log1p(predicted), 2)))\n    \n    num_of_features = 1000\n    \n    print(\"Split train and test\")\n    x1, x2, y1, y2 = train_test_split(x_train, y_train, test_size = 0.20, random_state = 42)\n    model = RandomForestRegressor(n_jobs = -1, random_state = 7)\n    model.fit(x1, y1)\n    print(rmsle(np.expm1(y2), np.expm1(model.predict(x2))))\n    \n    print(\"Get columns by feature importances\")\n    col_df = pd.DataFrame({'importance': model.feature_importances_, 'feature': x_train.columns})\n    col_df_sorted = col_df.sort_values(by = ['importance'], ascending = [False])\n    columns = col_df_sorted[:num_of_features]['feature'].values\n    print(\"Number of columns removed - \" + str(len(x_train.columns) - len(columns)))\n    \n    x_train = x_train[columns]\n    x_test = x_test[columns]\n    print(\"\\nTrain shape: {}\\nTest shape: {}\".format(x_train.shape, x_test.shape))\n    \n    return x_train, x_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4354c19a90e074a862a5ac95562a9fe167b0a251"},"cell_type":"markdown","source":"### **Removing columns having different distributions in train and test set**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b72eb0611c71477ee030517994eeb1dcb065c041"},"cell_type":"code","source":"def remove_features_having_different_distributions(train, test):\n    print(\"############# Remove columns having different distributions in train and test set #############\")\n    \n    threshold_p_value = 0.01 \n    threshold_statistic = 0.3\n    \n    cols_with_different_distributions = []\n    for col in train.columns:\n        statistic, pvalue = ks_2samp(train[col].values, test[col].values)\n        if pvalue <= threshold_p_value and np.abs(statistic) > threshold_statistic:\n            cols_with_different_distributions.append(col)\n    \n    print(\"Number of columns removed - \" + str(len(cols_with_different_distributions)))\n    for col in cols_with_different_distributions:\n        if col in train.columns:\n            train = train.drop(col, axis = 1)\n            test = test.drop(col, axis = 1)\n    print(\"\\nTrain shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n    \n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10b1e36a4e44a6520da2693713acb50515dbf0df"},"cell_type":"markdown","source":"### **Adding aggregate features**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"d2feddee9aa505e7005e6d196b52e33cc8446db3"},"cell_type":"code","source":"def add_aggregate_features(train, test):\n    print(\"############# Add aggregate features #############\")\n    \n    weight = ((train != 0).sum()/len(train)).values\n    \n    tmp_train = train[train != 0]\n    tmp_test = test[test != 0]\n\n    print(\"Adding row features.....\")\n    \n    print(\"weight count\")\n    train[\"weight_count\"] = (tmp_train * weight).sum(axis = 1)\n    test[\"weight_count\"] = (tmp_test * weight).sum(axis = 1)\n    \n    print(\"number of non-zero values\")\n    train[\"count_non_0\"] = (train != 0).sum(axis = 1)\n    test[\"count_non_0\"] = (test != 0).sum(axis = 1)\n    \n    print(\"number of different\")\n    train[\"num_different\"] = tmp_train.nunique(axis = 1)\n    test[\"num_different\"] = tmp_test.nunique(axis = 1)\n    \n    print(\"sum\")\n    train[\"sum\"] = train.sum(axis=1)\n    test[\"sum\"] = test.sum(axis=1)\n\n    print(\"variance\")\n    train[\"var\"] = tmp_train.var(axis=1)\n    test[\"var\"] = tmp_test.var(axis=1)\n\n    print(\"mean\")\n    train[\"mean\"] = tmp_train.mean(axis=1)\n    test[\"mean\"] = tmp_test.mean(axis=1)\n    \n    print(\"median\")\n    train[\"median\"] = tmp_train.median(axis=1)\n    test[\"median\"] = tmp_test.median(axis=1)\n\n    print(\"std\")\n    train[\"std\"] = tmp_train.std(axis=1)\n    test[\"std\"] = tmp_test.std(axis=1)\n\n    print(\"max\")\n    train[\"max\"] = tmp_train.max(axis=1)\n    test[\"max\"] = tmp_test.max(axis=1)\n\n    print(\"min\")\n    train[\"min\"] = tmp_train.min(axis=1)\n    test[\"min\"] = tmp_test.min(axis=1)\n    \n    print(\"skew\")\n    train[\"skew\"] = tmp_train.apply(skew, axis=1)\n    test[\"skew\"] = tmp_test.apply(skew, axis=1)\n    \n    print(\"kurtosis\")\n    train[\"kurtosis\"] = tmp_train.apply(kurtosis, axis=1)\n    test[\"kurtosis\"] = tmp_test.apply(kurtosis, axis=1)\n    \n    print(\"\\nTrain shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n    \n    # Remove an NA valuess\n    train = train.fillna(0)\n    test = test.fillna(0)\n    \n    del(tmp_train)\n    del(tmp_test)\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fcc0065782a70e50522ccb3382aa09c9ef0c792"},"cell_type":"markdown","source":"### **Mean-variance scale columns**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"fbdbfeb0393e2eaa0d05a3ffbb4662d3f7a86de5"},"cell_type":"code","source":"def mean_variance_scale_columns(total):\n    print(\"############# Mean-variance scale columns #############\")\n    \n    p = progressbar.ProgressBar()\n    p.start()\n\n    # Mean-variance scale all columns excluding 0-values' \n    number_of_columns = len(total.columns)\n    for col_index, col in enumerate(total.columns):    \n        p.update(col_index/number_of_columns * 100)\n\n        # Detect outliers in this column\n        data = total[col].values\n        data_mean, data_std = np.mean(data), np.std(data)\n        cut_off = data_std * 3\n        lower, upper = data_mean - cut_off, data_mean + cut_off\n        outliers = [x for x in data if x < lower or x > upper]\n\n        # If there are crazy high values, do a log-transform\n        if len(outliers) > 0:\n            non_zero_idx = data != 0\n            total.loc[non_zero_idx, col] = np.log(data[non_zero_idx])\n\n        # Scale non-zero column values\n        nonzero_rows = (total[col] != 0)\n        if  np.isfinite(total.loc[nonzero_rows, col]).all():\n            total.loc[nonzero_rows, col] = scale(list(total.loc[nonzero_rows, col]))\n            if  np.isfinite(total[col]).all():\n                # Scale all column values\n                total[col] = scale(list(total[col]))\n        gc.collect()\n\n    p.finish()\n    \n    return total","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acc67cab0a048832739ffe4f19b3b6aaeed94061"},"cell_type":"markdown","source":"### **Add dimensional reduction features back to dataset**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c8c7411b403eb9b41d1e8cdce23d69a2f60e78a4"},"cell_type":"code","source":"def add_decomposed_features_back_to_df(train, \n                                       test,\n                                       total,\n                                       n_components,\n                                       use_pca = False,\n                                       use_tsne = False,\n                                       use_tsvd = False,\n                                       use_ica = False,\n                                       use_fa = False,\n                                       use_grp = False,\n                                       use_srp = False):\n    print(\"############# Add decomposed components back to dataframe #############\")\n    \n    N_COMP = n_components\n    ntrain = len(train)\n    sparse_matrix = scipy.sparse.csr_matrix(total.values)\n\n    print(\"\\nStart decomposition process...\")\n    \n    if use_pca:\n        print(\"PCA\")\n        pca = PCA(n_components = N_COMP, random_state = 42)\n        pca_results = pca.fit_transform(total)\n        pca_results_train = pca_results[:ntrain]\n        pca_results_test = pca_results[ntrain:]\n        \n    if use_tsne:  \n        print(\"TSNE\")\n        tsne = TSNE(n_components = 3, init = 'pca')\n        tsne_results = tsne.fit_transform(total)\n        tsne_results_train = tsne_results[:ntrain]\n        tsne_results_test = tsne_results[ntrain:]\n\n    if use_tsvd:\n        print(\"tSVD\")\n        tsvd = TruncatedSVD(n_components = N_COMP, random_state = 42)\n        tsvd_results = tsvd.fit_transform(sparse_matrix)\n        tsvd_results_train = tsvd_results[:ntrain]\n        tsvd_results_test = tsvd_results[ntrain:]\n\n    if use_ica:\n        print(\"ICA\")\n        ica = FastICA(n_components = N_COMP, random_state=42)\n        ica_results = ica.fit_transform(total)\n        ica_results_train = ica_results[:ntrain]\n        ica_results_test = ica_results[ntrain:]\n\n    if use_fa:\n        print(\"FA\")\n        fa = FactorAnalysis(n_components = N_COMP, random_state=42)\n        fa_results = fa.fit_transform(total)\n        fa_results_train = fa_results[:ntrain]\n        fa_results_test = fa_results[ntrain:]\n\n    if use_grp:\n        print(\"GRP\")\n        grp = GaussianRandomProjection(n_components = N_COMP, eps = 0.1, random_state = 42)\n        grp_results = grp.fit_transform(total)\n        grp_results_train = grp_results[:ntrain]\n        grp_results_test = grp_results[ntrain:]\n\n    if use_srp:\n        print(\"SRP\")\n        srp = SparseRandomProjection(n_components = N_COMP, dense_output=True, random_state=42)\n        srp_results = srp.fit_transform(total)\n        srp_results_train = srp_results[:ntrain]\n        srp_results_test = srp_results[ntrain:]\n\n    print(\"Append decomposition components to datasets...\")\n    for i in range(1, N_COMP + 1):\n        \n        if use_pca:\n            train['pca_' + str(i)] = pca_results_train[:, i - 1]\n            test['pca_' + str(i)] = pca_results_test[:, i - 1]\n            \n        if use_tsne:\n            train['tsne_' + str(i)] = tsne_results_train[:, i - 1]\n            test['tsne_' + str(i)] = tsne_results_test[:, i - 1]\n            \n        if use_tsvd:\n            train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n            test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n        \n        if use_ica:\n            train['ica_' + str(i)] = ica_results_train[:, i - 1]\n            test['ica_' + str(i)] = ica_results_test[:, i - 1]\n        \n        if use_fa:\n            train['fa_' + str(i)] = fa_results_train[:, i - 1]\n            test['fa_' + str(i)] = fa_results_test[:, i - 1]\n\n        if use_grp:\n            train['grp_' + str(i)] = grp_results_train[:, i - 1]\n            test['grp_' + str(i)] = grp_results_test[:, i - 1]\n        \n        if use_srp:\n            train['srp_' + str(i)] = srp_results_train[:, i - 1]\n            test['srp_' + str(i)] = srp_results_test[:, i - 1]\n    print(\"\\nTrain shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"129771d5e0d6a8b218150a9d56830926505600bd"},"cell_type":"markdown","source":"### **Building a new dataframe with the dimensional reduction rechniques**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"938b3c4c2772b81df34ad5bff95dc7ea5867624a"},"cell_type":"code","source":"def use_decomposed_features_as_new_df(train, \n                                      test,\n                                      total,\n                                      n_components,\n                                      use_pca = False,\n                                      use_tsvd = False,\n                                      use_ica = False,\n                                      use_fa = False,\n                                      use_grp = False,\n                                      use_srp = False):\n    print(\"############# Create new dataframe from decomposed components #############\")\n    \n    N_COMP = n_components\n    ntrain = len(train)\n\n    print(\"\\nStart decomposition process...\")\n    \n    if use_pca:\n        print(\"PCA\")\n        pca = PCA(n_components = N_COMP, random_state = 42)\n        pca_results = pca.fit_transform(total)\n        pca_results_train = pca_results[:ntrain]\n        pca_results_test = pca_results[ntrain:]\n\n    if use_tsvd:\n        print(\"tSVD\")\n        tsvd = TruncatedSVD(n_components = N_COMP, random_state=42)\n        tsvd_results = tsvd.fit_transform(total)\n        tsvd_results_train = tsvd_results[:ntrain]\n        tsvd_results_test = tsvd_results[ntrain:]\n\n    if use_ica:\n        print(\"ICA\")\n        ica = FastICA(n_components = N_COMP, random_state=42)\n        ica_results = ica.fit_transform(total)\n        ica_results_train = ica_results[:ntrain]\n        ica_results_test = ica_results[ntrain:]\n\n    if use_fa:\n        print(\"FA\")\n        fa = FactorAnalysis(n_components = N_COMP, random_state=42)\n        fa_results = fa.fit_transform(total)\n        fa_results_train = fa_results[:ntrain]\n        fa_results_test = fa_results[ntrain:]\n\n    if use_grp:\n        print(\"GRP\")\n        grp = GaussianRandomProjection(n_components = N_COMP, eps=0.1, random_state=42)\n        grp_results = grp.fit_transform(total)\n        grp_results_train = grp_results[:ntrain]\n        grp_results_test = grp_results[ntrain:]\n\n    if use_srp:\n        print(\"SRP\")\n        srp = SparseRandomProjection(n_components = N_COMP, dense_output=True, random_state=42)\n        srp_results = srp.fit_transform(total)\n        srp_results_train = srp_results[:ntrain]\n        srp_results_test = srp_results[ntrain:]\n        \n    print(\"Append decomposition components together...\")\n    train_decomposed = np.concatenate([srp_results_train, grp_results_train, ica_results_train, pca_results_train, tsvd_results_train], axis=1)\n    test_decomposed = np.concatenate([srp_results_test, grp_results_test, ica_results_test, pca_results_test, tsvd_results_test], axis=1)\n\n    train_with_only_decomposed_features = pd.DataFrame(train_decomposed)\n    test_with_only_decomposed_features = pd.DataFrame(test_decomposed)\n    \n    for agg_col in ['sum', 'var', 'mean', 'median', 'std', 'weight_count', 'count_non_0', 'num_different', 'max', 'min']:\n        train_with_only_decomposed_features[col] = train[col]\n        test_with_only_decomposed_features[col] = test[col]\n    \n    # Remove any NA\n    train_with_only_decomposed_features = train_with_only_decomposed_features.fillna(0)\n    test_with_only_decomposed_features = test_with_only_decomposed_features.fillna(0)\n    \n    return train_with_only_decomposed_features, test_with_only_decomposed_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a22b2880c997dbd1574fc8887e0dbb4ac91f007b"},"cell_type":"markdown","source":"### **Build validation set using adversarial validation**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"8da0ceba808c00c837256e3e01b811f8fe0fb296"},"cell_type":"code","source":"def generate_adversarial_validation_set(train, test):\n    x_test = test.drop([\"is_test\", \"target\"], 1)\n\n    train, val = train[train.predicted_probs < 0.9], train[train.predicted_probs > 0.9]\n    train = train.drop([\"is_test\", \"predicted_probs\"], 1)\n    val = val.drop([\"is_test\", \"predicted_probs\"], 1)\n\n    x_train, y_train = train.drop(\"target\", 1), train.target\n    x_val, y_val = val.drop(\"target\", 1), val.target\n    print(\"\\nTrain shape: {}\\nValidation shape: {}\\nTest shape: {}\".format(x_train.shape, x_val.shape, x_test.shape))\n    \n    return x_train, y_train, x_val, y_val, x_test\n\ndef get_training_set_with_test_set_similarity_predictions(X_train, Y_train, X_test):\n    print(\"############# Generate adversarial validation set #############\")\n    \n    print(\"Add target column\")\n    X_train['target'] = Y_train\n    X_test['target'] = 0\n    \n    X_train[\"is_test\"] = 0\n    X_test[\"is_test\"] = 1\n    assert(np.all(train.columns == test.columns))\n    \n    print(\"Concat train and test data\")\n    total = pd.concat([X_train, X_test])\n    total = total.fillna(0)\n    \n    x = total.drop([\"is_test\", \"target\"], axis = 1)\n    y = total.is_test\n    \n    print(\"Start cross-validating\")\n    n_estimators = 100\n    classifier = RandomForestClassifier(n_estimators = n_estimators, n_jobs = -1)\n    predictions = np.zeros(y.shape)\n    \n    stratified_kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 5678)\n    \n    for fold_index, (train_indices, test_indices) in enumerate(stratified_kfold.split(x, y)):\n        print(\"Fold - \" + str(fold_index))\n        \n        x_train = x.iloc[train_indices]\n        y_train = y.iloc[train_indices]\n        x_test = x.iloc[test_indices]\n        y_test = y.iloc[test_indices]\n        \n        classifier.fit(x_train, y_train)\n        \n        predicted_probabilities = classifier.predict_proba(x_test)[:, 1]\n        \n        auc = roc_auc_score(y_test, predicted_probabilities)\n        print(\"AUC Score - \" + str(auc) + \"%\")\n        \n        predictions[test_indices] = predicted_probabilities\n    total['predicted_probs'] = predictions\n    \n    print(\"Generating training set\")\n    total = total[total.is_test == 0]\n    \n    print(\"Sorting according to predictions\")\n    train_set_with_predictions_for_test_set_similarity = total.sort_values([\"predicted_probs\"], ascending = False)\n    \n    return train_set_with_predictions_for_test_set_similarity, X_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95eeaaf5f63c5b4c18d3c1785e84a8edb2272c4e"},"cell_type":"markdown","source":"### **Building the model**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19a728a085c86db1ba5c8d0297b825cef565dd1c"},"cell_type":"code","source":"def RMSLE(actual, predicted):\n    return np.sqrt(np.mean(np.power(np.log1p(actual) - np.log1p(predicted), 2)))\n\n# LightGBM Model\ndef run_lgb(x_train, y_train, x_val, y_val, x_test):\n    print(\"############# Build LightGBM model #############\")\n    model_lgb = lgb.LGBMRegressor(objective='regression',\n                                  num_leaves = 144,\n                                  learning_rate = 0.005, \n                                  n_estimators = 720, \n                                  max_depth = 13,\n                                  metric = 'rmse',\n                                  is_training_metric = True,\n                                  max_bin = 55, \n                                  bagging_fraction = 0.8,\n                                  verbose = -1,\n                                  bagging_freq = 5, \n                                  feature_fraction = 0.9)\n    \n    print(\"Validating.....\")\n    model_lgb.fit(x_train, y_train, eval_set = (x_val, y_val), early_stopping_rounds = 100, verbose = True, eval_metric = 'rmse')\n    \n    print(\"Train on full dataset\")\n    X_TRAIN = pd.concat([x_train, x_val])\n    Y_TRAIN = pd.concat([y_train, y_val])\n    \n    model_lgb.fit(X = X_TRAIN,\n                  y = Y_TRAIN)\n    \n    \n    y_pred_test = np.expm1(model_lgb.predict(X_TEST))\n    print(\"LightGBM Training Completed...\")\n    \n    return y_pred_test\n\n# XGBoost Model\ndef run_xgb(x_train, y_train, x_val, y_val, X_TEST):\n    print(\"############# Build XGBoost model #############\")\n    model_xgb = xgb.XGBRegressor(colsample_bytree = 0.055, \n                                 colsample_bylevel = 0.5, \n                                 gamma = 1.5, \n                                 learning_rate = 0.02, \n                                 max_depth = 32, \n                                 objective = 'reg:linear',\n                                 booster = 'gbtree',\n                                 min_child_weight = 57, \n                                 n_estimators = 1000, \n                                 reg_alpha = 0, \n                                 reg_lambda = 0,\n                                 eval_metric = 'rmse', \n                                 subsample = 0.7, \n                                 silent = 1, \n                                 n_jobs = -1, \n                                 early_stopping_rounds = 14,\n                                 random_state = 7, \n                                 nthread = -1)\n    \n    print(\"Validating.....\")\n    model_xgb.fit(x_train, y_train, eval_set = [(x_train, y_train), (x_val, y_val)], eval_metric = 'rmse', early_stopping_rounds = 100, verbose = True)\n    \n    print(\"Train on full dataset\")\n    X_TRAIN = pd.concat([x_train, x_val])\n    Y_TRAIN = pd.concat([y_train, y_val])\n    \n    model_xgb.fit(X = X_TRAIN, \n                  y = Y_TRAIN)\n    \n    y_pred_test = np.expm1(model_xgb.predict(X_TEST))\n    print(\"XGBoost Training Completed...\")\n    \n    return y_pred_test\n    \n# CatBoost Model\n# def run_cbm(x_train, y_train, x_val, y_val, X_TEST):\n#     print(\"############# Build CatBoost model #############\")\n    \n#     model_cb = cb.CatBoostRegressor(iterations = 500,\n#                                  learning_rate = 0.05,\n#                                  depth = 10,\n#                                  eval_metric = 'RMSE',\n#                                  random_seed = 42,\n#                                  bagging_temperature = 0.2,\n#                                  od_type = 'Iter',\n#                                  metric_period = 50,\n#                                  od_wait = 20)\n    \n#     model_cb.fit(x_train, y_train)\n#     rmsle = RMSLE(np.expm1(y_val), np.expm1(model_cb.predict(x_val)))\n#     print(\"The RMSLE score on validation set is - \" + str(rmsle))\n    \n#     X_TRAIN = pd.concat([x_train, x_val])\n#     Y_TRAIN = pd.concat([y_train, y_val])\n    \n#     model_cb.fit(X = X_TRAIN, \n#                  y = Y_TRAIN)\n    \n#     y_pred_test = np.expm1(model_cb.predict(X_TEST))\n#     print(\"CatBoost Training Completed...\")\n    \n#     return y_pred_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0537e062755acf1140654ed36d060b10383de4a9"},"cell_type":"markdown","source":"### **Ensemble predictions**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b90a728ebc5e0df0b1dd0da5dc89f18d76cf5908"},"cell_type":"code","source":"def ensemble_predictions(lgb_pred = None, xgb_pred = None, cbm_pred = None, lgb_ratio = 1/3, xgb_ratio = 1/3, cbm_ratio = 1/3):\n    print(\"############# Ensemble model predictions #############\")\n    \n    y_pred_test_final = lgb_pred * lgb_ratio + xgb_pred * xgb_ratio #+ cbm_pred * cbm_ratio\n    return y_pred_test_final","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"482e5fd413131da7a57fd0d47cd2e54a35da3065"},"cell_type":"markdown","source":"### **Save submission file**"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e3cf0c0deafc07e9bb1f1e12352dcd6c8af47cc7"},"cell_type":"code","source":"def save_submission_file(y_pred_test, test_ID, model_name):\n    print(\"############# Save submission files #############\")\n    \n    sub = pd.DataFrame(y_pred_test)\n    sub.columns = ['target']\n    sub.insert(0, 'ID', test_ID)\n    print(sub.head())\n    sub.to_csv(model_name +'_10_all_decomposition_features.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b791ba69ac32c4a8af802d3840375ee9f9f278a5"},"cell_type":"markdown","source":"### **Creating submission**"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d7bbe1c871250d867a04d28aebbd96e4b2d33795","collapsed":true},"cell_type":"code","source":"train_orig, test_orig = read_data()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"4d560b6ccaac36e6174c801e27561ceda2cc40f6","collapsed":true},"cell_type":"code","source":"train, test, y_train, test_ID = convert_and_drop_target(train = train_orig.copy(), \n                                                        test = test_orig.copy())\n\ntrain_without_duplicate_columns, test_without_duplicate_columns = remove_duplicate_columns(train = train.copy(), \n                                                                                           test = test.copy())\n\ntrain_without_constant_columns, test_without_constant_columns = remove_constant_columns(train = train_without_duplicate_columns.copy(), \n                                                                                        test = test_without_duplicate_columns.copy())\n\ntrain_with_columns_of_high_importance, test_with_columns_of_high_importance = remove_features_using_importance(x_train = train_without_constant_columns.copy(), \n                                                                                                              y_train = y_train.copy(), \n                                                                                                              x_test = test_without_constant_columns.copy())\n\ntrain_with_columns_having_same_distributions, test_with_columns_having_same_distributions = remove_features_having_different_distributions(train = train_with_columns_of_high_importance.copy(), \n                                                                                                                                           test = test_with_columns_of_high_importance.copy())\n\ntotal = pd.concat([train_with_columns_having_same_distributions, test_with_columns_having_same_distributions])\ntotal_scaled = mean_variance_scale_columns(total = total.copy())\n\ntrain_with_aggregate_features, test_with_aggregate_features = add_aggregate_features(train = train_with_columns_having_same_distributions.copy(), \n                                                                                     test = test_with_columns_having_same_distributions.copy())\n\n\ntrain_with_decomposed_features, test_with_decomposed_features = add_decomposed_features_back_to_df(train = train_with_aggregate_features.copy(),\n                                                                                                 test = test_with_aggregate_features.copy(),\n                                                                                                 total = total_scaled.copy(),\n                                                                                                 n_components = 100,\n                                                                                                 use_pca = False,\n                                                                                                 use_tsne = False,\n                                                                                                 use_tsvd = False,\n                                                                                                 use_ica = False,\n                                                                                                 use_fa = False,\n                                                                                                 use_grp = False,\n                                                                                                 use_srp = True)\n\n\ntraining_set, testing_set = get_training_set_with_test_set_similarity_predictions(train_with_decomposed_features.copy(), y_train.copy(), test_with_decomposed_features.copy())\n\nx_train, y_train, x_val, y_val, X_TEST = generate_adversarial_validation_set(training_set.copy(), testing_set.copy())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f053b9a9c882ef4f2558abc45d5d48626852cd02","collapsed":true},"cell_type":"code","source":"# y_pred_test_lbg = run_lgb(x_train, y_train, x_val, y_val, X_TEST)\ny_pred_test_xgb = run_xgb(x_train, y_train, x_val, y_val, X_TEST)\n# y_pred_test_cbm = run_cbm(x_train, y_train, x_val, y_val, X_TEST)\n\ny_pred_test_final = ensemble_predictions(lgb_pred = y_pred_test_lbg, \n                                         xgb_pred = y_pred_test_xgb, \n                                         cbm_pred = None, \n                                         lgb_ratio = 1/3, \n                                         xgb_ratio = 1/3, \n                                         cbm_ratio = 1/3)\n\nsave_submission_file(y_pred_test = y_pred_test_final, test_ID = test_ID, model_name = \"Ensemble_LGB_XGB_with_columns_scaled\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"47ebcc42768b98e5f397eedafe9c0902a16e2902"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}