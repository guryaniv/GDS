{"cells":[{"metadata":{"trusted":true,"_uuid":"8c5b0e133c17d3e7c8dee7c0d3fe864763c58c51"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB,  BernoulliNB\nfrom sklearn.metrics import accuracy_score, log_loss,jaccard_similarity_score\nimport math\nfrom sklearn.preprocessing import normalize\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport logging\nfrom heamy.dataset import Dataset\nfrom heamy.estimator import Classifier\nfrom heamy.pipeline import ModelsPipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f67cdb6dd11b518abeb21d0682c0a454425d9d9"},"cell_type":"code","source":"DATA_DIR = \"../input\"\nSUBMISSION_FILE = \"{0}/sample_submission.csv\".format(DATA_DIR)\nCACHE=False\n\nNFOLDS = 5\nSEED = 1337\nMETRIC = log_loss\n\nID = 'Id'\nTARGET = 'Cover_Type'\n\nnp.set_printoptions(precision=5)\nnp.set_printoptions(suppress=True)\n\nnp.random.seed(SEED)\n#logging.basicConfig(level=logging.DEBUG)\nlogging.basicConfig(level=logging.WARNING)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a49a8e618ddde11afa48d206959d6fb0b0ecd0ae"},"cell_type":"code","source":"def add_feats(df):\n    df['HF1'] = df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Fire_Points']\n    df['HF2'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\n    df['HR1'] = (df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Roadways'])\n    df['HR2'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\n    df['FR1'] = (df['Horizontal_Distance_To_Fire_Points']+df['Horizontal_Distance_To_Roadways'])\n    df['FR2'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\n    df['EV1'] = df.Elevation+df.Vertical_Distance_To_Hydrology\n    df['EV2'] = df.Elevation-df.Vertical_Distance_To_Hydrology\n    df['Mean_HF1'] = df.HF1/2\n    df['Mean_HF2'] = df.HF2/2\n    df['Mean_HR1'] = df.HR1/2\n    df['Mean_HR2'] = df.HR2/2\n    df['Mean_FR1'] = df.FR1/2\n    df['Mean_FR2'] = df.FR2/2\n    df['Mean_EV1'] = df.EV1/2\n    df['Mean_EV2'] = df.EV2/2    \n    df['Elevation_Vertical'] = df['Elevation']+df['Vertical_Distance_To_Hydrology']    \n    df['Neg_Elevation_Vertical'] = df['Elevation']-df['Vertical_Distance_To_Hydrology']\n    \n    # Given the horizontal & vertical distance to hydrology, \n    # it will be more intuitive to obtain the euclidean distance: sqrt{(verticaldistance)^2 + (horizontaldistance)^2}    \n    df['slope_hyd_sqrt'] = (df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)**0.5\n    df.slope_hyd_sqrt=df.slope_hyd_sqrt.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    df['slope_hyd2'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)\n    df.slope_hyd2=df.slope_hyd2.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    #Mean distance to Amenities \n    df['Mean_Amenities']=(df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology + df.Horizontal_Distance_To_Roadways) / 3 \n    #Mean Distance to Fire and Water \n    df['Mean_Fire_Hyd1']=(df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology) / 2\n    df['Mean_Fire_Hyd2']=(df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Roadways) / 2\n    \n    #Shadiness\n    df['Shadiness_morn_noon'] = df.Hillshade_9am/(df.Hillshade_Noon+1)\n    df['Shadiness_noon_3pm'] = df.Hillshade_Noon/(df.Hillshade_3pm+1)\n    df['Shadiness_morn_3'] = df.Hillshade_9am/(df.Hillshade_3pm+1)\n    df['Shadiness_morn_avg'] = (df.Hillshade_9am+df.Hillshade_Noon)/2\n    df['Shadiness_afternoon'] = (df.Hillshade_Noon+df.Hillshade_3pm)/2\n    df['Shadiness_mean_hillshade'] =  (df['Hillshade_9am']  + df['Hillshade_Noon'] + df['Hillshade_3pm'] ) / 3    \n    \n    # Shade Difference\n    df[\"Hillshade-9_Noon_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_Noon\"]\n    df[\"Hillshade-noon_3pm_diff\"] = df[\"Hillshade_Noon\"] - df[\"Hillshade_3pm\"]\n    df[\"Hillshade-9am_3pm_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_3pm\"]\n\n    # Mountain Trees\n    df[\"Slope*Elevation\"] = df[\"Slope\"] * df[\"Elevation\"]\n    # Only some trees can grow on steep montain\n    \n    ### More features\n    df['Neg_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\n    df['Neg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\n    df['Neg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\n    \n    df['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])/2\n    df['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])/2\n    df['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])/2   \n        \n    df[\"Vertical_Distance_To_Hydrology\"] = abs(df['Vertical_Distance_To_Hydrology'])\n    \n    df['Neg_Elev_Hyd'] = df.Elevation-df.Horizontal_Distance_To_Hydrology*0.2\n    \n    # Bin Features\n    bin_defs = [\n        # col name, bin size, new name\n        ('Elevation', 200, 'Binned_Elevation'), # Elevation is different in train vs. test!?\n        ('Aspect', 45, 'Binned_Aspect'),\n        ('Slope', 6, 'Binned_Slope'),\n        ('Horizontal_Distance_To_Hydrology', 140, 'Binned_Horizontal_Distance_To_Hydrology'),\n        ('Horizontal_Distance_To_Roadways', 712, 'Binned_Horizontal_Distance_To_Roadways'),\n        ('Hillshade_9am', 32, 'Binned_Hillshade_9am'),\n        ('Hillshade_Noon', 32, 'Binned_Hillshade_Noon'),\n        ('Hillshade_3pm', 32, 'Binned_Hillshade_3pm'),\n        ('Horizontal_Distance_To_Fire_Points', 717, 'Binned_Horizontal_Distance_To_Fire_Points')\n    ]\n    \n    for col_name, bin_size, new_name in bin_defs:\n        df[new_name] = np.floor(df[col_name]/bin_size)\n        \n    print('Total number of features : %d' % (df.shape)[1])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9e40c900beed2192852513d122b2a992b683bac"},"cell_type":"code","source":"def load_and_process_dataset():\n    train = pd.read_csv(\"{0}/train.csv\".format(DATA_DIR))\n    test = pd.read_csv(\"{0}/test.csv\".format(DATA_DIR))\n\n    y_train = train[TARGET].ravel() -1 # XGB needs labels starting with 0!\n    \n    classes = train.Cover_Type.unique()\n    num_classes = len(classes)\n    print(\"There are %i classes: %s \" % (num_classes, classes))        \n\n    train.drop([ID, TARGET], axis=1, inplace=True)\n    test.drop([ID], axis=1, inplace=True)\n    \n    train = add_feats(train)    \n    test = add_feats(test)    \n    \n    cols_to_normalize = [ 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n                       'Horizontal_Distance_To_Fire_Points', \n                       'Shadiness_morn_noon', 'Shadiness_noon_3pm', 'Shadiness_morn_3',\n                       'Shadiness_morn_avg', \n                       'Shadiness_afternoon', \n                       'Shadiness_mean_hillshade',\n                       'HF1', 'HF2', \n                       'HR1', 'HR2', \n                       'FR1', 'FR2'\n                       ]\n\n    train[cols_to_normalize] = normalize(train[cols_to_normalize])\n    test[cols_to_normalize] = normalize(test[cols_to_normalize])\n\n    # elevation was found to have very different distributions on test and training sets\n    # lets just drop it for now to see if we can implememnt a more robust classifier!\n    train = train.drop('Elevation', axis=1)\n    test = test.drop('Elevation', axis=1)    \n    \n    x_train = train.values\n    x_test = test.values\n\n    return {'X_train': x_train, 'X_test': x_test, 'y_train': y_train}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b62040abd6942949fe6e43f52a864f04d70c844"},"cell_type":"code","source":"dataset = Dataset(preprocessor=load_and_process_dataset, use_cache=True)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23afa4f3dccacc443a6c615d89f4df70f49f4ee5"},"cell_type":"code","source":"# Parameters for the classifiers\nrf_params = {\n    'n_estimators': 200,\n    'criterion': 'entropy',\n    'random_state': 0\n}\n\nrf1_params = {\n    'n_estimators': 200,\n    'criterion': 'gini',\n    'random_state': 0\n}\n\net1_params = {\n    'n_estimators': 200,\n    'criterion': 'gini',\n    'random_state': 0\n}\n\net_params = {\n    'n_estimators': 200,\n    'criterion': 'entropy',\n    'random_state': 0\n}\n\net1_params = {\n    'n_estimators': 200,\n    'criterion': 'gini',\n    'random_state': 0\n}\n\nlgb_params = {\n    'n_estimators': 200, \n    'learning_rate':0.1\n}\n\nlogr_params = {\n        'solver' : 'liblinear',\n        'multi_class' : 'ovr',\n        'C': 1,\n        'random_state': 0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c75a1ae641bf4fb4a2492f3c33721e82ba86a3e2"},"cell_type":"code","source":"rf = Classifier(dataset=dataset, estimator = RandomForestClassifier, use_cache=CACHE, parameters=rf_params,name='rf')\net = Classifier(dataset=dataset, estimator = ExtraTreesClassifier, use_cache=CACHE, parameters=et_params,name='et')   \nrf1 = Classifier(dataset=dataset, estimator=RandomForestClassifier, use_cache=CACHE, parameters=rf1_params,name='rf1')\net1 = Classifier(dataset=dataset, use_cache=CACHE, estimator=ExtraTreesClassifier, parameters=et1_params,name='et1')\nlgbc = Classifier(dataset=dataset, estimator=LGBMClassifier, use_cache=CACHE, parameters=lgb_params,name='lgbc')\ngnb = Classifier(dataset=dataset,estimator=GaussianNB, use_cache=CACHE, name='gnb')\nlogr = Classifier(dataset=dataset, estimator=LogisticRegression, use_cache=CACHE, parameters=logr_params,name='logr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0061caaa81fb17658054c265a942b7d8f31f35e9"},"cell_type":"code","source":"def xgb_first(X_train, y_train, X_test, y_test=None):\n    xg_params = {\n        'seed': 0,\n        'colsample_bytree': 0.7,\n        'silent': 1,\n        'subsample': 0.7,\n        'learning_rate': 0.1,\n        'objective': 'multi:softprob',   \n        'num_class': 7,\n        'max_depth': 4,\n        'min_child_weight': 1,\n        'eval_metric': 'mlogloss',\n        'nrounds': 200\n    }    \n    X_train = xgb.DMatrix(X_train, label=y_train)\n    model = xgb.train(xg_params, X_train, xg_params['nrounds'])\n    return model.predict(xgb.DMatrix(X_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3e4f49174135476ea7ec969f4c93236220aafdc"},"cell_type":"code","source":"xgb_first = Classifier(estimator=xgb_first, dataset=dataset, use_cache=CACHE, name='xgb_first')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11f353469ba0cdb87d79e9509570dcdb5ec24ea4"},"cell_type":"code","source":"pipeline = ModelsPipeline(rf, et, et1, lgbc, logr, gnb, xgb_first) \n\nstack_ds = pipeline.stack(k=NFOLDS,seed=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1c724d5ae4079efc84f389738ab188c4dc4e023"},"cell_type":"code","source":"# Train LogisticRegression on stacked data (second stage)\nlr = LogisticRegression\nlr_params = {'C': 5, 'random_state' : SEED, 'solver' : 'liblinear', 'multi_class' : 'ovr',}\nstacker = Classifier(dataset=stack_ds, estimator=lr, use_cache=False, parameters=lr_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b31d1c68871389730473a685f14202a5ad29821a"},"cell_type":"code","source":"# Validate results using k-fold cross-validation\nresults = stacker.validate(k=NFOLDS,scorer=log_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a33eb2b6d067cd121d333502d473089559e04a70"},"cell_type":"code","source":"models = [rf, et, et1, lgbc, logr, gnb, xgb_first]       \nprint(\"Log Loss\")\nfor index, element in enumerate(models):\n    print(index, element.name)\n    element.validate(k=NFOLDS,scorer=log_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a891996842d3fa34cf2a7eec3a1b39c98b524a6"},"cell_type":"code","source":"preds_proba = stacker.predict() \n# Note: labels starting with 0 in xgboost, therefore adding +1!\npredictions = np.round(np.argmax(preds_proba, axis=1)).astype(int) + 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"634d0f83eb1aa12a035ec66affd6ad181d934ac0"},"cell_type":"code","source":"submission = pd.read_csv(SUBMISSION_FILE)\nsubmission[TARGET] = predictions\nsubmission.to_csv('Stacking_with_heamy_logregr.sub.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49035feb2b54ea6e51d94b37eea4cc65f48cbf91"},"cell_type":"code","source":"# Use a xgb-model as 2nd-stage model\n\ndtrain = xgb.DMatrix(stack_ds.X_train, label=stack_ds.y_train)\ndtest = xgb.DMatrix(stack_ds.X_test)\n\nxgb_params = {\n    'seed': 0,\n    'colsample_bytree': 0.8,\n    'silent': 1,\n    'subsample': 0.6,\n    'learning_rate': 0.05,\n    'objective': 'multi:softprob',\n    'num_class': 7,        \n    'max_depth': 6,\n    'num_parallel_tree': 1,\n    'min_child_weight': 1,\n    'eval_metric': 'mlogloss',\n}\n\nres = xgb.cv(xgb_params, dtrain, num_boost_round=1000, \n             nfold=NFOLDS, seed=SEED, stratified=True,\n             early_stopping_rounds=20, verbose_eval=5, show_stdv=True)\n\nbest_nrounds = res.shape[0] - 1\ncv_mean = res.iloc[-1, 2]\ncv_std = res.iloc[-1, 3]\n\nprint('Ensemble-CV: {0}+{1}, best nrounds = {2}'.format(cv_mean, cv_std, best_nrounds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f09b50c6cb6423fe39d9bfda982e5e65b3e028c2"},"cell_type":"code","source":"# Train with best rounds\nmodel = xgb.train(xgb_params, dtrain, best_nrounds)\n\nxpreds_proba = model.predict(dtest)\n\n# Note: labels starting with 0 in xgboost, therefore adding +1!\npredictions = np.round(np.argmax(xpreds_proba, axis=1)).astype(int) + 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ca2b302aea724fc448c6d4b1981262deac5ea66"},"cell_type":"code","source":"submission = pd.read_csv(SUBMISSION_FILE)\nsubmission[TARGET] = predictions\nsubmission.to_csv('Stacking_with_heamy_cv_mlogloss_' + str(cv_mean) + '.sub.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edb84eb0a91eda10bb85c4b4867fdb0e8f6ad847"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}