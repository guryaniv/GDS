{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "171f1c05-31f1-0db5-328b-999b94f48276"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import datetime\n",
        "import operator\n",
        "from sklearn.cross_validation import train_test_split\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import plot, show, subplot, specgram, imshow, savefig\n",
        "from collections import defaultdict\n",
        "\n",
        "RS = 12357\n",
        "ROUNDS = 315\n",
        "#ROUNDS = 75\n",
        "\n",
        "print(\"Started\")\n",
        "np.random.seed(RS)\n",
        "input_folder = '../input/quora-question-pairs/'\n",
        "\n",
        "def train_xgb(X, y, params):\n",
        "\tprint(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n",
        "\tx, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n",
        "\n",
        "\txg_train = xgb.DMatrix(x, label=y_train)\n",
        "\txg_val = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "\twatchlist  = [(xg_train,'train'), (xg_val,'eval')]\n",
        "\treturn xgb.train(params, xg_train, ROUNDS, watchlist)\n",
        "\n",
        "def predict_xgb(clr, X_test):\n",
        "\treturn clr.predict(xgb.DMatrix(X_test))\n",
        "\n",
        "def create_feature_map(features):\n",
        "\toutfile = open('xgb.fmap', 'w')\n",
        "\ti = 0\n",
        "\tfor feat in features:\n",
        "\t\toutfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
        "\t\ti = i + 1\n",
        "\toutfile.close()\n",
        "\n",
        "def add_word_count(x, df, word):\n",
        "\tx['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
        "\tx['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
        "\tx[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n",
        "\n",
        "def main():\n",
        "\tparams = {}\n",
        "\tparams['objective'] = 'binary:logistic'\n",
        "\tparams['eval_metric'] = 'logloss'\n",
        "\tparams['eta'] = 0.11\n",
        "\tparams['max_depth'] = 5\n",
        "\tparams['silent'] = 1\n",
        "\tparams['seed'] = RS\n",
        "\n",
        "\tdf_train = pd.read_csv(input_folder + 'train.csv')\n",
        "\tdf_test  = pd.read_csv(input_folder + 'test.csv')\n",
        "\n",
        "\tprint(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\n",
        "\n",
        "\tprint(\"Features processing, be patient...\")\n",
        "\n",
        "\t# If a word appears only once, we ignore it completely (likely a typo)\n",
        "\t# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
        "\tdef get_weight(count, eps=10000, min_count=2):\n",
        "\t\treturn 0 if count < min_count else 1 / (count + eps)\n",
        "\n",
        "\ttrain_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
        "\twords = (\" \".join(train_qs)).lower().split()\n",
        "\tcounts = Counter(words)\n",
        "\tweights = {word: get_weight(count) for word, count in counts.items()}\n",
        "\n",
        "\tstops = set(stopwords.words(\"english\"))\n",
        "\tdef word_shares(row):\n",
        "\t\tq1_list = str(row['question1']).lower().split()\n",
        "\t\tq1 = set(q1_list)\n",
        "\t\tq1words = q1.difference(stops)\n",
        "\t\tif len(q1words) == 0:\n",
        "\t\t\treturn '0:0:0:0:0:0:0:0'\n",
        "        \n",
        "\t\tq2_list = str(row['question2']).lower().split()\n",
        "\t\tq2 = set(q2_list)\n",
        "\t\tq2words = q2.difference(stops)\n",
        "\t\tif len(q2words) == 0:\n",
        "\t\t\treturn '0:0:0:0:0:0:0:0'\n",
        "\n",
        "\t\twords_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
        "\n",
        "\t\tq1stops = q1.intersection(stops)\n",
        "\t\tq2stops = q2.intersection(stops)\n",
        "\n",
        "\t\tq1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
        "\t\tq2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
        "\n",
        "\t\tshared_2gram = q1_2gram.intersection(q2_2gram)\n",
        "\n",
        "\t\tshared_words = q1words.intersection(q2words)\n",
        "\t\tshared_weights = [weights.get(w, 0) for w in shared_words]\n",
        "\t\tq1_weights = [weights.get(w, 0) for w in q1words]\n",
        "\t\tq2_weights = [weights.get(w, 0) for w in q2words]\n",
        "\t\ttotal_weights = q1_weights + q1_weights\n",
        "\t\t\n",
        "\t\tR1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
        "\t\tR2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
        "\t\tR31 = len(q1stops) / len(q1words) #stops in q1\n",
        "\t\tR32 = len(q2stops) / len(q2words) #stops in q2\n",
        "\t\tRcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
        "\t\tRcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
        "\t\tif len(q1_2gram) + len(q2_2gram) == 0:\n",
        "\t\t\tR2gram = 0\n",
        "\t\telse:\n",
        "\t\t\tR2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
        "\t\treturn '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)\n",
        "\n",
        "\tdf = pd.concat([df_train, df_test])\n",
        "\tdf['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n",
        "\n",
        "\tx = pd.DataFrame()\n",
        "\n",
        "\tx['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
        "\tx['word_match_2root'] = np.sqrt(x['word_match'])\n",
        "\tx['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
        "\tx['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
        "\n",
        "\tx['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
        "\tx['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
        "\tx['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
        "\tx['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
        "\tx['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
        "\tx['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n",
        "\n",
        "\tx['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
        "\tx['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
        "\tx['diff_len'] = x['len_q1'] - x['len_q2']\n",
        "\t\n",
        "\tx['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
        "\tx['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
        "\tx['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n",
        "\n",
        "\tx['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
        "\tx['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
        "\tx['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n",
        "\n",
        "\tx['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
        "\tx['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
        "\tx['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n",
        "\n",
        "\tx['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n",
        "\tx['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n",
        "\tx['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n",
        "\n",
        "\tx['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
        "\tx['duplicated'] = df.duplicated(['question1','question2']).astype(int)\n",
        "\tadd_word_count(x, df,'how')\n",
        "\tadd_word_count(x, df,'what')\n",
        "\tadd_word_count(x, df,'which')\n",
        "\tadd_word_count(x, df,'who')\n",
        "\tadd_word_count(x, df,'where')\n",
        "\tadd_word_count(x, df,'when')\n",
        "\tadd_word_count(x, df,'why')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdf1 = df_train[['question1']].copy()\n",
        "\tdf2 = df_train[['question2']].copy()\n",
        "\tdf1_test = df_test[['question1']].copy()\n",
        "\tdf2_test = df_test[['question2']].copy()\n",
        "\n",
        "\tdf2.rename(columns = {'question2':'question1'},inplace=True)\n",
        "\tdf2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
        "\n",
        "\ttrain_questions = df1.append(df2)\n",
        "\ttrain_questions = train_questions.append(df1_test)\n",
        "\ttrain_questions = train_questions.append(df2_test)\n",
        "\t#train_questions.drop_duplicates(subset = ['qid1'],inplace=True)\n",
        "\ttrain_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
        "\n",
        "\ttrain_questions.reset_index(inplace=True,drop=True)\n",
        "\tquestions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
        "\ttrain_cp = df_train.copy()\n",
        "\ttest_cp = df_test.copy()\n",
        "\ttrain_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
        "\n",
        "\ttest_cp['is_duplicate'] = -1\n",
        "\ttest_cp.rename(columns={'test_id':'id'},inplace=True)\n",
        "\tcomb = pd.concat([train_cp,test_cp])\n",
        "\n",
        "\tx['q1_hash'] = comb['question1'].map(questions_dict)\n",
        "\tx['q2_hash'] = comb['question2'].map(questions_dict)\n",
        "\n",
        "\tq1_vc = x.q1_hash.value_counts().to_dict()\n",
        "\tq2_vc = x.q2_hash.value_counts().to_dict()\n",
        "\n",
        "\tdef try_apply_dict(x,dict_to_apply):\n",
        "\t\ttry:\n",
        "\t\t\treturn dict_to_apply[x]\n",
        "\t\texcept KeyError:\n",
        "\t\t\treturn 0\n",
        "\t#map to frequency space\n",
        "\tx['q1_freq'] = x['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
        "\tx['q2_freq'] = x['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
        "\tdel q1_vc, q2_vc, comb, test_cp, train_cp, df1, df2, df1_test, df2_test, train_questions, questions_dict\n",
        "\t\n",
        "\tfeature_names = list(x.columns.values)\n",
        "\tcreate_feature_map(feature_names)\n",
        "\tprint(\"Features: {}\".format(feature_names))\n",
        "\n",
        "\tx_train = x[:df_train.shape[0]]\n",
        "\tx_test  = x[df_train.shape[0]:]\n",
        "\ty_train = df_train['is_duplicate'].values\n",
        "\tdel x, df_train\n",
        "\n",
        "\tif 1: # Now we oversample the negative class - on your own risk of overfitting!\n",
        "\t\tpos_train = x_train[y_train == 1]\n",
        "\t\tneg_train = x_train[y_train == 0]\n",
        "\n",
        "\t\tprint(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
        "\t\tp = 0.165\n",
        "\t\tscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
        "\t\twhile scale > 1:\n",
        "\t\t\tneg_train = pd.concat([neg_train, neg_train])\n",
        "\t\t\tscale -=1\n",
        "\t\tneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
        "\t\tprint(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
        "\n",
        "\t\tx_train = pd.concat([pos_train, neg_train])\n",
        "\t\ty_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
        "\t\tdel pos_train, neg_train\n",
        "\t\n",
        "\tprint(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))\n",
        "\tclr = train_xgb(x_train, y_train, params)\n",
        "\tdel x_train, y_train\n",
        "\tpreds = predict_xgb(clr, x_test)\n",
        "\n",
        "\tprint(\"Writing output...\")\n",
        "\tsub = pd.DataFrame()\n",
        "\tsub['test_id'] = df_test['test_id']\n",
        "\tsub['is_duplicate'] = preds *.75\n",
        "\tsub.to_csv(\"xgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)\n",
        "\n",
        "\tprint(\"Features importances...\")\n",
        "\timportance = clr.get_fscore(fmap='xgb.fmap')\n",
        "\timportance = sorted(importance.items(), key=operator.itemgetter(1))\n",
        "\tft = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
        "\n",
        "\tft.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\n",
        "\tplt.gcf().savefig('features_importance.png')\n",
        "\n",
        "main()\n",
        "print(\"Done.\")"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}