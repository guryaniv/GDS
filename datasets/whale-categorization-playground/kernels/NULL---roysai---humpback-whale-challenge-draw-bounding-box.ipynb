{"cells":[{"metadata":{"_uuid":"991f7f2d7f796d82025d6263482ef759b8b519f0"},"cell_type":"markdown","source":"This project is part of image preprossesing for Humpback Whale Identification Challenge. The model is trained to predict bounding box for the fluke of a whale. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport glob\n\nimport matplotlib.pyplot as plt\n\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, GlobalMaxPooling2D\nfrom keras.layers.core import Flatten, Dense, Dropout\nfrom keras.optimizers import Adam\n\nimport keras\nimport tensorflow as tf\n\nimport random","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def load_img(SL, train=True):\n    \"\"\"\n    Given a list of image name, return the image array in the list\n    \"\"\"\n    \n    outlst=np.zeros((len(SL), h_size, w_size, channel))\n    ori_size=np.zeros((len(SL), 2))\n    for n, ID in enumerate(SL):\n        if train==False:\n            path=TEST_PATH+ID\n        else:\n            path=TRAIN_PATH+ID\n        img=imread(path)\n        ori_size[n, :]=img.shape[:2]   \n        if img.ndim==3:\n            img=resize(img, (h_size, w_size, 3))\n        elif img.ndim==2:\n            img=resize(img, (h_size, w_size))\n            img=np.expand_dims(img, axis=-1)\n            img[:,:,:]=img\n            \n        outlst[n]=img\n    return outlst, ori_size\n\ndef check_id(image):\n    ID=train['Id'][train['Image']==image].iloc[0]\n    return ID\n\ndef flip_image(img):\n    flipped=np.fliplr(img)\n    return flipped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d90760c2b7e74808c94a9c1f7b4f7c48de0451f","collapsed":true},"cell_type":"code","source":"#define the input size. Since we are using VGG19 as training, we will set the size to be 224x224\nh_size=224\nw_size=224\nchannel=3\n\nTRAIN_PATH=\"../input/whale-categorization-playground/train/\"\nTEST_PATH=\"../input/whale-categorization-playground/test/\"\n\n#Credit to \"Michael Chmutov\" for hand labling about 600 images that we can used to train our model\nlabel=pd.read_csv('../input/bb-kaggle/bb_kaggle.csv')\n\n#reformatting the imported table\nlabel.rename(columns={'ystart xstart yend xend': 'coordinate'}, inplace=True)\nystart=[]\nxstart=[]\nyend=[]\nxend=[]\nfor i in range (len(label)):\n    a=label['coordinate'].iloc[i]\n    a=a.split(' ')\n    ystart.append(int(a[0]))\n    xstart.append(int(a[1]))\n    yend.append(int(a[2]))\n    xend.append(int(a[3]))\nlabel['ystart']=ystart\nlabel['xstart']=xstart\nlabel['yend']=yend\nlabel['xend']=xend\nlabel.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86e8fd36f139adad9e4d7fe1c9fd6fea7d52707d","collapsed":true},"cell_type":"code","source":"#define variables to store original size\nori_h=[]\nori_w=[]\nori_channel=[]\nimg_array=np.zeros((len(label), h_size, w_size, channel))\nfor n, images in enumerate(label['fn']):\n    img=imread(TRAIN_PATH+images)\n    ori_h.append(img.shape[0])\n    ori_w.append(img.shape[1])\n    _channel=img.ndim\n    ori_channel.append(_channel)\n    \n    if _channel==3:\n        img=resize(img, (h_size, w_size, channel))\n    elif _channel==2:\n        img=resize(img, (h_size, w_size))\n        img=np.expand_dims(img, axis=-1)\n        img[:,:,:]=img\n    img_array[n]=img\n\nlabel['original height']= ori_h\nlabel['original width']= ori_w\nlabel['original channel']= ori_channel\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5557ff38748d0724f275b501ff528fe3bf2879d8","collapsed":true},"cell_type":"code","source":"#Define and store y_ratio and x_ratio. these ratios are important so that we can transform the 224x224 bounding box back to its original size\ny_ratio=label['original height']/h_size\nx_ratio=label['original width']/w_size\n\nt_ystart=np.rint(ystart/y_ratio)\nt_xstart=np.rint(xstart/x_ratio)\nt_yend=np.rint(yend/y_ratio)\nt_xend=np.rint(xend/x_ratio)\n\nlabel['t_ystart']=t_ystart\nlabel['t_xstart']=t_xstart\nlabel['t_yend']=t_yend\nlabel['t_xend']=t_xend\ny_true=np.concatenate((np.expand_dims(t_ystart.values, axis=-1), np.expand_dims(t_xstart.values, axis=-1), np.expand_dims(t_yend.values, axis=-1), np.expand_dims(t_xend.values, axis=-1)), axis=1)\n\nlabel.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69a3d0726bd758d17d1f1e6770b703607c65e426","collapsed":true},"cell_type":"code","source":"#credit to \"taindow\" for supplying additional labeled images. we will process the images as above as well.\nlabel2=pd.read_csv('../input/bb-label-extra/bb_labels (1).csv')\n\ny_ratio=label2['target_size_2']/h_size\nx_ratio=label2['target_size_1']/w_size\n\nt_ystart=np.rint(label2['ymin'].values/y_ratio)\nt_xstart=np.rint(label2['xmin'].values/x_ratio)\nt_yend=np.rint(label2['ymax'].values/y_ratio)\nt_xend=np.rint(label2['xmax'].values/x_ratio)\n\nlabel2['t_ystart']=t_ystart\nlabel2['t_xstart']=t_xstart\nlabel2['t_yend']=t_yend\nlabel2['t_xend']=t_xend\ny2_true=np.concatenate((np.expand_dims(t_ystart, axis=-1), np.expand_dims(t_xstart, axis=-1), np.expand_dims(t_yend, axis=-1), np.expand_dims(t_xend, axis=-1)), axis=1)\n\nlabel2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2f5a6a6cf946ee22398c6f4ea888242590a3168","collapsed":true},"cell_type":"code","source":"#load all images into array. All greyscale images will be converted to 3 channels as well\nimg2_array=np.zeros((len(label2), h_size, w_size, channel))\nfor n, images in enumerate(label2['image']):\n    img=imread(TRAIN_PATH+images)\n    _channel=img.ndim\n       \n    if _channel==3:\n        img=resize(img, (h_size, w_size, channel))\n    elif _channel==2:\n        img=resize(img, (h_size, w_size))\n        img=np.expand_dims(img, axis=-1)\n        img[:,:,:]=img\n    img2_array[n]=img\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f407a0db09f90f5671404ad48c0c5d3c7dc619","collapsed":true},"cell_type":"code","source":"#sanity check\nprint (img_array.shape, img2_array.shape)\nprint (y_true.shape, y2_true.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63ec61eb626881af929b8b6a413fe6dbc5b5f7fb","scrolled":true,"collapsed":true},"cell_type":"code","source":"def check_distance(y_true, y_pred):\n    \"\"\"\n    Customize loss function\n    \n    Argument:\n    y_true-- Ground thruth position of the top left and bottom right corner of the bouding box.\n    y_pred-- Predicted position of the top left and bottom right corner of the bounding box by the model.\n    \n    return-- the sum of distance between the top left and bottom right of y_true and y_pred\n    \"\"\"\n    y11, x11, y12, x12=tf.split(y_true, 4, axis=1)\n    y21, x21, y22, x22=tf.split(y_pred, 4, axis=1)\n    \n    dist_start=tf.sqrt(tf.square(y21-y11)+tf.square(x21-x11))\n    dist_end=tf.sqrt(tf.square(y22-y12)+tf.square(x22-x12))\n    sum_dist=dist_start+dist_end\n    return sum_dist\n    \ndef get_base_model():\n    base_model=keras.applications.vgg19.VGG19(include_top=False, weights=None)\n    base_model.load_weights('../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n\n    x=base_model.output\n    x=GlobalMaxPooling2D()(x)\n    x=Dropout(0.05)(x)\n    dense_1=Dense(100)(x)\n    dense_1=Dense(50)(dense_1)\n    dense_1=Dense(10)(dense_1)\n    base_output=Dense(4)(dense_1)\n    base_model=Model(base_model.input, base_output)\n    return base_model\n    \nin_dims=(h_size, w_size, channel)\ninputs=Input(in_dims)\nbase_model=get_base_model()\noutputs=base_model(inputs)\n\nmodel=Model(inputs=inputs, outputs=outputs)\nmodel.compile(loss=check_distance, optimizer=Adam(0.00001))\nprint (model.summary())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f5da474f9456d01d390ff1dc3be6272aa97d508f"},"cell_type":"code","source":"#concatenate all image array from both sources\nall_img=np.concatenate([img_array, img2_array], axis=0)\nall_y=np.concatenate([y_true, y2_true], axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ca00c32b912cc0f4630174fd90298b014c688e9","collapsed":true},"cell_type":"code","source":"\"Uncomment to load pretrained weight\"\n#model.load_weights('../input/crop-image-weight/crop_image_v0.h5')\n#model.evaluate(all_img, all_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a7dc86eae9b6c686b42bd680523107ce24139e6","collapsed":true},"cell_type":"code","source":"\"Uncomment to start training\"\nmodel.fit(all_img, all_y, batch_size=32, epochs=20, validation_split=0.2)\n#model.save_weights('crop_image_v0.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b71e1548672c52594f645d3e8ec0d53a114200e","collapsed":true},"cell_type":"code","source":"#Predicting bounding box of all train images\nall_train_img_path=glob.glob('../input/whale-categorization-playground/train/*.jpg')\nall_train_img_name=[] \nfor file in all_train_img_path: \n    all_train_img_name.append(file.split('/')[-1])\n\n_batch=2000 \nnum_batch=int(len(all_train_img_name)/_batch)\nindexes=list(range(num_batch))\nall_train_coordinateFrame=pd.DataFrame()\nfor i in indexes:\n    print (f'Cropping image batch {i+1} of {indexes[-1]+1}')\n    coordinateFrame=pd.DataFrame() \n    if i!=indexes[-1]:\n        img_array, ori_size=load_img(all_train_img_name[i*_batch:(i+1)*_batch])\n        coordinateFrame['image']=all_train_img_name[i*_batch:(i+1)*_batch]\n\n    else: \n        img_array, ori_size=load_img(all_train_img_name[i*_batch:])\n        coordinateFrame['image']=all_train_img_name[i*_batch:]\n\n    coordinate=np.rint(model.predict(img_array))\n    coordinateFrame['ori height']=ori_size[:, 0] \n    coordinateFrame['ori width']=ori_size[:, 1]\n    \n    y_ratio=coordinateFrame['ori height']/h_size\n    x_ratio=coordinateFrame['ori width']/w_size\n    \n    coordinateFrame['ystart']=coordinate[:, 0]\n    coordinateFrame['xstart']=coordinate[:, 1]\n    coordinateFrame['yend']=coordinate[:,2]\n    coordinateFrame['xend']=coordinate[:,3]\n    \n    #to make sure that no negatvie xstart and ystart\n    coordinateFrame['xstart'][coordinateFrame['xstart']<0]=0 \n    coordinateFrame['ystart'][coordinateFrame['ystart']<0]=0\n    \n    #transform the coordinate back to original shape, minus/add additional 10 pixels to avoid overcropping\n    coordinateFrame['new ystart']=np.rint(coordinateFrame['ystart']*y_ratio)\n    coordinateFrame['new xstart']=np.rint(coordinateFrame['xstart']*x_ratio)\n    coordinateFrame['new yend']=np.rint(coordinateFrame['yend']*y_ratio)\n    coordinateFrame['new xend']=np.rint(coordinateFrame['xend']*x_ratio)\n    \n    all_train_coordinateFrame=pd.concat([all_train_coordinateFrame, coordinateFrame], axis=0, ignore_index=True)\nall_train_coordinateFrame.to_csv('whale_train_crop_table.csv')\nall_train_coordinateFrame.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27f170f3b1b4e181be02248e7271ec9170fb4757","collapsed":true},"cell_type":"code","source":"#Predicting bounding box of test images\nall_test_img_path=glob.glob('../input/whale-categorization-playground/test/*.jpg')\nall_test_img_name=[]\nfor file in all_test_img_path:\n    all_test_img_name.append(file.split('/')[-1])\n\n_batch=2000\nnum_batch=int(len(all_test_img_name)/_batch)\nindexes=np.arange(num_batch+1)\nall_test_coordinateFrame=pd.DataFrame()\nfor i in indexes:\n    print (f'Cropping image batch {i+1} of {indexes[-1]+1}')\n    coordinateFrame=pd.DataFrame() \n    if i!=indexes[-1]:\n        img_array, ori_size=load_img(all_test_img_name[i*_batch:(i+1)*_batch], train=False)\n        coordinateFrame['image']=all_test_img_name[i*_batch:(i+1)*_batch]\n    else:\n        img_array, ori_size=load_img(all_test_img_name[i*_batch:], train=False)\n        coordinateFrame['image']=all_test_img_name[i*_batch:]\n\n    coordinate=np.rint(model.predict(img_array))\n    coordinateFrame['ori height']=ori_size[:, 0]\n    coordinateFrame['ori width']=ori_size[:, 1]\n    coordinateFrame['ystart']=coordinate[:, 0]\n    coordinateFrame['xstart']=coordinate[:, 1]\n    coordinateFrame['yend']=coordinate[:,2]\n    coordinateFrame['xend']=coordinate[:,3] \n    coordinateFrame['xstart'][coordinateFrame['xstart']<0]=0\n    coordinateFrame['ystart'][coordinateFrame['ystart']<0]=0\n\n    y_ratio=coordinateFrame['ori height']/h_size\n    x_ratio=coordinateFrame['ori width']/w_size\n\n    coordinateFrame['new ystart']=np.rint(coordinateFrame['ystart']*y_ratio)\n    coordinateFrame['new xstart']=np.rint(coordinateFrame['xstart']*x_ratio)\n    coordinateFrame['new yend']=np.rint(coordinateFrame['yend']*y_ratio)\n    coordinateFrame['new xend']=np.rint(coordinateFrame['xend']*x_ratio)\n    all_test_coordinateFrame=pd.concat([all_test_coordinateFrame, coordinateFrame], axis=0, ignore_index=True)\nall_test_coordinateFrame.head()\nall_test_coordinateFrame.to_csv('whale_test_crop_table.csv')                                  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5224010892ea1a9ff2e0f22b7ff7e41fa1f5b339","collapsed":true},"cell_type":"code","source":"#sanity check: randomly choose 4 images from train to display before and after cropping\nIMG_SHAPE=(224,224,3)\ndef crop_image(images, train=True):\n    \"\"\"take in images, crop according to train/test crop table, resize to IMG_SHAPE\n    \n    Arguments:\n    images-- list consist of image name to be cropped\n    train-- bool, True if the images are from train, False if test\n    \n    return:\n    output_array-- array with axis-0 as number of images, axis-1 to 3 as cropped array of the input images\n    \"\"\"\n    output_array=np.zeros((len(images), IMG_SHAPE[0], IMG_SHAPE[1], IMG_SHAPE[2]))\n    for n, image in enumerate (images):\n        if train:\n            path=TRAIN_PATH+image\n            table=all_train_coordinateFrame.copy()\n        else:\n            path=TEST_PATH+image\n            table=all_test_coordinateFrame.copy()\n            \n        ystart=table['new ystart'][table['image']==image].values\n        xstart=table['new xstart'][table['image']==image].values\n        yend=table['new yend'][table['image']==image].values\n        xend=table['new xend'][table['image']==image].values\n        \n        img_array=imread(path)\n        height, width=img_array.shape[0], img_array.shape[1]\n        \n        #provide 10 pixel of margin, so that we are sure that entire flute will be cropped.\n        ystart=int(max(ystart-10, 0))\n        xstart=int(max(xstart-10, 0))\n        yend=int(min(yend+10, height))\n        xend=int(min(xend+10, width))\n        \n        #if image is greyscale, convert it to rgb with all channel = greyscale channel\n        if img_array.ndim==3:\n            img_array=img_array[ystart:yend, xstart:xend, :]\n            img_array=resize(img_array, IMG_SHAPE)\n        else:\n            img_array=img_array[ystart:yend, xstart:xend]\n            img_array=resize(img_array, (IMG_SHAPE[0], IMG_SHAPE[1]))\n            img_array=np.expand_dims(img_array, axis=-1)\n            img_array[:,:,:]=img_array\n        output_array[n]=img_array\n    return output_array\n\nrandom_img=[random.choice(all_train_img_name) for _ in range(4)]\n\nfor n, img in enumerate(random_img):\n    cropped_img=crop_image([img])\n    \n    plt.subplot(4,2,n*2+1)\n    plt.imshow(load_img([img])[0][0])\n    \n    plt.subplot(4, 2, n*2+2)\n    plt.imshow(cropped_img[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bccb5a8786dc0737732b422ad15390786161f15d"},"cell_type":"markdown","source":"A few things to note:\n1. The fluke will be distorted. The effect of the distortion is unknow. One can fix the aspect ratio of the bounding box to prevent distortion.\n2. Overcropping tends to do more harm than good, since many of the whale features are located at the tip of the fluke which might be lost when overcropped. Hence we added additional 10 pixels to the left/right of the upper left/bottom right corner of the bounding box"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d6043b273547dd48d5bfdd754d20c8ecc6b724d0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}