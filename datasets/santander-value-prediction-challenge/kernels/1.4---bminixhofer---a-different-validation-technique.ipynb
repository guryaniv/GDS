{"cells":[{"metadata":{"_uuid":"11ffb332646f2da07df7e5e23aaf0465ed31f7f0"},"cell_type":"markdown","source":"I think its save to say that everyone taking part in this competition has had problems with validation. Local CV score does not at all represent the leaderboard score. Some people have even described a [negative correlation](https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/59307#350685).\n\nAnd how are we supposed to solve this problem without any idea if our model is good without submitting to the leaderboard first (and keep in mind that the public LB is evaluated only on 50% on the data too).\n\nWe have to find a better way of local validation. In this kernel I try a *different* technique. I wouldn't dare to call it better because I have not thoroughly evaluated it but it is certainly interesting."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"46a2f59f286ae4bc5f96bc80a3f65f919705edb2"},"cell_type":"code","source":"import lightgbm as lgb\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import KFold, BaseCrossValidator\nfrom sklearn.decomposition import TruncatedSVD, FastICA, FactorAnalysis\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import scale\nfrom scipy.stats import skew, kurtosis, gmean, ks_2samp\nimport gc\nimport psutil\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\n\ntqdm.pandas()\nsns.set(style=\"white\", color_codes=True)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96b8c3cd91fbe449b8ed3b087e21593e643789b7"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49c4762b1b0c8b6078725b63f3cb547aface3576"},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{"_uuid":"faf05b95c4a1d91284a11b1f8cf57b524971be91"},"cell_type":"markdown","source":"We will start by defining basic row aggregation features. These are used in most public kernels so I will not further elaborate on this part."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3802db031b5a49aa453dbb459bab5823cdb040e6"},"cell_type":"code","source":"def aggregate_row(row):\n    non_zero_values = row.iloc[row.nonzero()].astype(float)\n    if non_zero_values.empty:\n        aggs = {\n            'non_zero_mean': np.nan,\n            'non_zero_std': np.nan,\n            'non_zero_max': np.nan,\n            'non_zero_min': np.nan,\n            'non_zero_sum': np.nan,\n            'non_zero_skewness': np.nan,\n            'non_zero_kurtosis': np.nan,\n            'non_zero_median': np.nan,\n            'non_zero_q1': np.nan,\n            'non_zero_q3': np.nan,\n            'non_zero_gmean': np.nan,\n            'non_zero_log_mean': np.nan,\n            'non_zero_log_std': np.nan,\n            'non_zero_log_max': np.nan,\n            'non_zero_log_min': np.nan,\n            'non_zero_log_sum': np.nan,\n            'non_zero_log_skewness': np.nan,\n            'non_zero_log_kurtosis': np.nan,\n            'non_zero_log_median': np.nan,\n            'non_zero_log_q1': np.nan,\n            'non_zero_log_q3': np.nan,\n            'non_zero_log_gmean': np.nan,\n            'non_zero_count': np.nan,\n            'non_zero_fraction': np.nan\n        }\n    else:\n        aggs = {\n            'non_zero_mean': non_zero_values.mean(),\n            'non_zero_std': non_zero_values.std(),\n            'non_zero_max': non_zero_values.max(),\n            'non_zero_min': non_zero_values.min(),\n            'non_zero_sum': non_zero_values.sum(),\n            'non_zero_skewness': skew(non_zero_values),\n            'non_zero_kurtosis': kurtosis(non_zero_values),\n            'non_zero_median': non_zero_values.median(),\n            'non_zero_q1': np.percentile(non_zero_values, q=25),\n            'non_zero_q3': np.percentile(non_zero_values, q=75),\n            'non_zero_gmean': gmean(non_zero_values),\n            'non_zero_log_mean': np.log1p(non_zero_values).mean(),\n            'non_zero_log_std': np.log1p(non_zero_values).std(),\n            'non_zero_log_max': np.log1p(non_zero_values).max(),\n            'non_zero_log_min': np.log1p(non_zero_values).min(),\n            'non_zero_log_sum': np.log1p(non_zero_values).sum(),\n            'non_zero_log_skewness': skew(np.log1p(non_zero_values)),\n            'non_zero_log_kurtosis': kurtosis(np.log1p(non_zero_values)),\n            'non_zero_log_median': np.log1p(non_zero_values).median(),\n            'non_zero_log_q1': np.percentile(np.log1p(non_zero_values), q=25),\n            'non_zero_log_q3': np.percentile(np.log1p(non_zero_values), q=75),\n            'non_zero_log_gmean': gmean(np.log1p(non_zero_values)),\n            'non_zero_count': non_zero_values.count(),\n            'non_zero_fraction': non_zero_values.count() / row.count()\n        }\n    return pd.Series(aggs, index=list(aggs.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"355b68d37802cc3a7be811d524577563f745b24a"},"cell_type":"code","source":"eng_features = train.iloc[:, 2:].progress_apply(aggregate_row, axis=1)\neng_features_test = test.iloc[:, 1:].progress_apply(aggregate_row, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebdd3072d7845bd4ca6c14de95042fa3d2871028"},"cell_type":"markdown","source":"# Adversarial validation"},{"metadata":{"_uuid":"2fa63cdf95bc72ca27907cbf343afe8b611c63d7"},"cell_type":"markdown","source":"We will now perform adversarial validation using *only* the previously defined statistical features."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fcae1ff2a86fe0f82a2503959f1441b6b795fc54"},"cell_type":"code","source":"train_matrix = np.hstack([\n    eng_features.values\n])\n\ntest_matrix = np.hstack([\n    eng_features_test.values\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"131921f912a6bbf4cf16f4b6812a5a1bf99ea1f5"},"cell_type":"code","source":"lgb_params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'learning_rate': 0.1,\n    'num_leaves': 16,\n    'max_depth': -1,\n    'min_child_samples': 1,\n    'max_bin': 300,\n    'subsample': 1.0,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.5,\n    'min_child_weight': 10,\n    'reg_lambda': 0.1,\n    'reg_alpha': 0.0,\n    'scale_pos_weight': 1,\n    'zero_as_missing': False,\n    'num_threads': -1,\n}\n\nadversarial_x = np.vstack([\n    train_matrix,\n    test_matrix\n])\nadversarial_y = np.ones(len(adversarial_x))\nadversarial_y[:len(train_matrix)] = 0\n\ncv = KFold(n_splits=5, random_state=100, shuffle=True)\ntrain_preds = np.zeros((len(adversarial_x)))\n\nfor i, (train_index, valid_index) in enumerate(cv.split(adversarial_y)):\n    print(f'Fold {i}')\n    \n    dtrain = lgb.Dataset(adversarial_x[train_index], \n                         label=adversarial_y[train_index])\n    dvalid = lgb.Dataset(adversarial_x[valid_index], \n                         label=adversarial_y[valid_index])\n    \n    evals_result = {}\n    model = lgb.train(lgb_params, dtrain,\n                      num_boost_round=10000, \n                      valid_sets=[dtrain, dvalid],\n                      valid_names=['train', 'valid'],\n                      early_stopping_rounds=100, \n                      verbose_eval=2000, \n                      evals_result=evals_result)\n\n    valid_preds = model.predict(adversarial_x[valid_index])\n    train_preds[valid_index] = valid_preds\n    \nprint('Overall ROC AUC', roc_auc_score(adversarial_y, train_preds))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4da924d02a2f719c40b38fbd390ba3a5a69ba0b"},"cell_type":"markdown","source":"This yields a horrible score of 97% ROC AUC. And that is only with statistical features! But at least it can not perfectly distinguish between the sets. Let's plot the predictions of the model against the train set.\n\nIf the train set is a time series, we might see more incorrectly classified samples in the end of the train series."},{"metadata":{"trusted":true,"_uuid":"bee272c5524021968aaf719b00208d950c7f3bae"},"cell_type":"code","source":"predictions = pd.Series(train_preds[:len(train)])\npredictions_sample = predictions.sample(frac=1)\nsns.jointplot(predictions_sample.index, predictions_sample, size=10, stat_func=None,\n              marginal_kws=dict(bins=15), joint_kws=dict(s=3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"346862057164e2c8d96d91cdd6d40723ab22502d"},"cell_type":"markdown","source":"That does not seem to be the case. But there are quite some samples which the model mistakes as from the test set. With these weights we can define a weighted metric function. That is weighted RMSLE in our case. I will also save the weights so you can conveniently download them and try it in your own model."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b3da546adc821f66c99d9d3549ba089b7d7871fe"},"cell_type":"code","source":"np.save('weights.npy', predictions.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"91bf098f3eb9ce1ef73f5adee8733b30384cfd26"},"cell_type":"code","source":"weights = predictions + 0.1\nweights = weights / weights.mean()\n\ndef weighted_rmsle(y_true, y_pred, index):\n    errors = (np.log1p(y_true) - np.log1p(y_pred)) ** 2\n    errors = errors * weights[index]\n    \n    return np.sqrt(np.mean(errors))\n\ndef rmsle(y_true, y_pred):\n    errors = (np.log1p(y_true) - np.log1p(y_pred)) ** 2\n\n    return np.sqrt(np.mean(errors))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e132d4c0722fb428395433c717d9459ce390a38b"},"cell_type":"markdown","source":"What we do here is first add 0.1 to the predictions (so that samples are never completely ignored by our metric function, even if the adversarial classifier marks them as \"clearly from the train set\"). \n\nAfterwards, we divide the weights by their mean to center them around 1, so that samples are weighted by 1 on average. This is done to keep the weighted RMSLE on the same scale as regular RMSLE. \n\nFinally, we define the weighted metric by multiplying the error of each sample by the corresponding weight before averaging it. Recall that RMSLE is defined as\n\n$$\\text{RMSLE} = \\sqrt{\\frac{1}{n}\\sum_{i=0}^{n}(\\log(1+\\hat{y}_i)-\\log(1+y_i))^2}$$\n\nthen weighted RMSLE is defined as \n\n$$\\text{weighted RMSLE} = \\sqrt{\\frac{1}{n}\\sum_{i=0}^{n}(\\log(1+\\hat{y}_i)-\\log(1+y_i))^2\\cdot w_i}$$ \n\nwhere $w$ is a vector of the (processed) outputs of our adversarial validation model which should have a mean of 1."},{"metadata":{"_uuid":"077c21c17994ae7f9547eb4598cd175e58afde14"},"cell_type":"markdown","source":"# Training a regular regressor"},{"metadata":{"_uuid":"3621035bce35d8c68f43286d1b50dab1a15b9c4b"},"cell_type":"markdown","source":"We will now train a regular regressor and see how well our new validaton method fares. Regarding feature selection I did:\n- dropping of duplicate columns\n- dropping of columns with zero variance"},{"metadata":{"trusted":true,"_uuid":"5e2412c57a8b63b19159b10f721c2038f89d15a8"},"cell_type":"code","source":"x_train = train.iloc[:, 2:].values\nx_test = test.iloc[:, 1:].values\ny_train = np.log(train['target'])\n\n_, unique_indices = np.unique(x_train, return_index=True, axis=1)\nvariance_greater_zero = x_train.var(axis=0) > 0\n\nmask = np.zeros(x_train.shape[1], dtype=bool)\nmask[unique_indices] = True\nmask[variance_greater_zero] = True\n\nx_train = x_train[:, mask]\nx_test = x_test[:, mask]\n\nx_train.shape, x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88180dfb530782469f2f7976be951163930d7eeb"},"cell_type":"markdown","source":"There is also some very basic feature engineering with decompositon features using SVD and ICA to keep the regressor (more or less) competitive."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"700eee1cc02d0419ffe77028eaf9e3b8d3aaeaf0"},"cell_type":"code","source":"decomposer = FeatureUnion([\n    ('svd', TruncatedSVD(n_components=50, random_state=100)),\n    ('ica', FastICA(n_components=20, random_state=100))\n])\n\ndecomposed_train = decomposer.fit_transform(x_train)\ndecomposed_test = decomposer.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"391308d35606f0d06389baa15d889c1e7edbcc7f"},"cell_type":"code","source":"train_matrix = np.hstack([\n    eng_features.values,\n    decomposed_train\n])\ntest_matrix = np.hstack([\n    eng_features_test.values,\n    decomposed_test\n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b568c357c9c7bc963039864f973c826022b9dd98"},"cell_type":"markdown","source":"Hyperparameters are the same as with the adversarial validation model and barely tuned on this input."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1fc39d5566161522c6817a6b4adbefc5b7027b08"},"cell_type":"code","source":"lgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate': 0.01,\n    'num_leaves': 16,\n    'max_depth': -1,\n    'min_child_samples': 1,\n    'max_bin': 300,\n    'subsample': 1.0,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.5,\n    'min_child_weight': 10,\n    'reg_lambda': 0.1,\n    'reg_alpha': 0.0,\n    'scale_pos_weight': 1,\n    'zero_as_missing': False,\n    'num_threads': -1,\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3003b9f39e98ac8bcf70514a96705ba7c7b804d"},"cell_type":"markdown","source":"I also use a spin on KFold which splits the training set based on target value. This somewhat decreases the error differences between multiple folds, but I didn't notice it that much. The implementation is from [here](https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/59299)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"82690f8343845a1051a8c19b24b34e4bc31f1bb9"},"cell_type":"code","source":"class KFoldByTargetValue(BaseCrossValidator):\n    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def _iter_test_indices(self, X, y=None, groups=None):\n        n_samples = X.shape[0]\n        indices = np.arange(n_samples)\n\n        sorted_idx_vals = sorted(zip(indices, X), key=lambda x: x[1])\n        indices = [idx for idx, val in sorted_idx_vals]\n\n        for split_start in range(self.n_splits):\n            split_indeces = indices[split_start::self.n_splits]\n            yield split_indeces\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        return self.n_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbc12f9185e7ded92a2cf14daed03a143d4a465d"},"cell_type":"markdown","source":"Finally, train the model. We will measure regular RMSLE and weighted RMSLE. I will also submit the predictions to the leaderboard and compare scores.\n\nNote that you should *not* tune parameters like the 0.1 minimum prediction so that the weighted RMSLE is equal to the leaderboard score. Or if you do, be very careful with it. You will overfit to the behaviour of this specific models difference from training set to test set. Which might make the metric a bad measurement for other models.\n\nIt is only supposed to be a rough estimate of the LB score which regards which samples from the training distribution are similar to those from the test distribution and weights them accordingly higher."},{"metadata":{"trusted":true,"_uuid":"19d18e8141a063b9ee2fecf6c9931ce9122c86ab"},"cell_type":"code","source":"cv = KFoldByTargetValue(n_splits=5, shuffle=True, random_state=100)\n\ntrain_preds = np.zeros((len(train)))\ntest_preds = np.zeros((len(test)))\n\nfor i, (train_index, valid_index) in enumerate(cv.split(y_train)):\n    print(f'Fold {i}')\n    \n    dtrain = lgb.Dataset(train_matrix[train_index], \n                         label=y_train[train_index])\n    dvalid = lgb.Dataset(train_matrix[valid_index], \n                         label=y_train[valid_index])\n    \n    evals_result = {}\n    model = lgb.train(lgb_params, dtrain,\n                      num_boost_round=10000, \n                      valid_sets=[dtrain, dvalid],\n                      valid_names=['train', 'valid'],\n                      early_stopping_rounds=100, \n                      verbose_eval=2000, \n                      evals_result=evals_result)\n    \n    valid_preds = np.exp(model.predict(train_matrix[valid_index]))\n    test_preds += np.exp(model.predict(test_matrix)) / cv.n_splits\n    \n    train_preds[valid_index] = valid_preds\n    \n    print('RMSLE: ', rmsle(np.exp(y_train[valid_index]), valid_preds))\n    print('Weighted RMSLE: ', weighted_rmsle(np.exp(y_train[valid_index]), valid_preds, valid_index))\nprint()\nprint('Overall RMSLE: ', rmsle(np.exp(y_train), train_preds))\nprint('Overall Weighted RMSLE: ', weighted_rmsle(np.exp(y_train), train_preds, np.arange(len(train_preds))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c068549b49d00be387f45664aaf27fd4ffb380cf"},"cell_type":"markdown","source":"Notice that the weighted RMSLE is significantly higher than regular RMSLE. This makes sense, because samples that are similar to the distribution are obviously harder to predict than samples which have properties similar to the other samples in the train distribution."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6bd5a6da4c5c2db4543344b78593b9154317aaa"},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['ID'] = test['ID']\nsubmission['target'] = test_preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75393a37a4e54db51ebcdb998fa4b04e2657300d"},"cell_type":"markdown","source":"Finally, submit the predictions and hope the weighted RMSLE is a good approximation of the LB score."},{"metadata":{"_uuid":"305f51d79fef6be425a9b28a8645c9a820059da6"},"cell_type":"markdown","source":"# Where to go from here\n\nIn this kernel, I showed a different kind of validation. To be honest, I am not sure if this is a good way of validation at all. Please discuss with me in the comments :) I will definitely try this scheme in more elaborate models and see how it compares to the leaderboard score there. And again: *Without* tuning this score for a specific model. That defeats the purpose.\n\nOne way to probably improve the score is by weighting samples more conservatively e. g. start from 1 and add / substract some value based on the predictions of the adversarial classifier like\n\n$$\\text{weights} = 1 + (\\text{predictions} - 0.5) * \\alpha$$ \n\nwhere 0 < $\\alpha$ < 2 and the smaller $\\alpha$, the less noticeable is the adversarial weighting."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}