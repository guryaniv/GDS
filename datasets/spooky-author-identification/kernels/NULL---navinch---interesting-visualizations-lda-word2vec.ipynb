{"nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python"}}, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "34068389-5c3f-4c29-95c0-a9a7276a5d5b", "_uuid": "3fb69fa31ca1452f4d60789a0702d1f84dcec8c5"}, "cell_type": "markdown", "source": ["# **\"Boost\" your NLP skills for beginners**\n", "\n", "From this notebook you can get very good understanding of end-to-end data science & natural language processing pipeline, starting with raw data and running through preparing, modeling, visualizing, and analyzing the data.\n", "\n", "Notebook covers:\n", "* A tour of the dataset\n", "* Introduction to text processing with spaCy\n", "* Automatic phrase modeling\n", "* Topic modeling with LDA\n", "* Visualizing topic models with pyLDAvis\n", "* Word vector models with word2vec\n", "* Visualizing word2vec with t-SNE"]}, {"metadata": {"_cell_guid": "4eb002d2-f37e-426c-8b44-60e81f38d0e0", "_uuid": "70710a30f64f19d7041c875ba3cb2c1507719a71"}, "cell_type": "code", "execution_count": null, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import spacy\n", "import itertools as it\n", "from gensim.models import Phrases\n", "from gensim.models.word2vec import LineSentence\n", "from gensim.corpora import Dictionary, MmCorpus\n", "from gensim.models.ldamulticore import LdaMulticore\n", "import pyLDAvis\n", "import pyLDAvis.gensim\n", "import warnings\n", "import _pickle as pickle\n", "from gensim.models import Word2Vec\n", "from sklearn.manifold import TSNE\n", "from bokeh.plotting import figure, show, output_notebook\n", "from bokeh.models import HoverTool, ColumnDataSource, value\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "output_notebook()\n", "\n", "from subprocess import check_output"], "outputs": []}, {"metadata": {"_cell_guid": "bd8c1351-bc19-460b-b6de-4a74053ee087", "_uuid": "499a2a3869e9447653e86df68ea2be07bf399421"}, "cell_type": "markdown", "source": [" ## Spooky author dataset\n", " **19579 ** sentences<br>\n", " **3** authors\n", "* Edgar Allan Poe (EAP)\n", "* HP Lovecraft (HPL)\n", "* Mary Wollstonecraft Shelley (MWS)"]}, {"metadata": {"_cell_guid": "8cf3b0e8-e30d-4883-a664-04af7af898be", "_uuid": "9a92255aff8ade369f8a248db87dcf23a83caf6f"}, "cell_type": "code", "execution_count": null, "source": ["train = pd.read_csv('../input/train.csv')\n", "train['author'].value_counts().plot.pie(autopct='%.2f', fontsize=20, figsize=(6, 6))\n", "plt.title('Authorwise distribution')\n", "None"], "outputs": []}, {"metadata": {"_cell_guid": "dd07dc07-276d-4999-8509-ebde40bf953a", "_uuid": "cef62a4dce7b9b213d9615ef9db143c208f51625"}, "cell_type": "code", "execution_count": null, "source": ["train.isnull().any()  # sanity check for null values"], "outputs": []}, {"metadata": {"_cell_guid": "d4635350-fe8c-4efc-ac57-658e6c6f6f70", "_uuid": "abfbfd3a35bebc08fa9de55eca641f0535a9deca"}, "cell_type": "markdown", "source": ["## Spacy - Industrial strength NLP in python\n", "spaCy is an industrial-strength natural language processing (NLP) library for Python. spaCy's goal is to take recent advancements in natural language processing out of research papers and put them in the hands of users to build production software.\n", "spaCy handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n", "\n", "* Tokenization\n", "* Text normalization, such as lowercasing, stemming/lemmatization\n", "* Part-of-speech tagging\n", "* Syntactic dependency parsing\n", "* Sentence boundary detection\n", "* Named entity recognition and annotation\n", "\n", "spaCy contains built-in data and models which you can use out-of-the-box for processing general-purpose English language text:\n", "\n", "* Large English vocabulary, including stopword lists\n", "* Token \"probabilities\"\n", "* Word vectors\n", "\n", "spaCy is written in optimized Cython, which means it's fast. According to a few independent sources, it's the fastest syntactic parser available in any language. Key pieces of the spaCy parsing pipeline are written in pure C, enabling efficient multithreading (i.e., spaCy can release the GIL)."]}, {"metadata": {"_cell_guid": "29c1c817-95be-4650-9dfb-0cd02879f98c", "_uuid": "46e6fac44a8f874d9044864bd686098d1d161870", "collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["nlp = spacy.load('en')"], "outputs": []}, {"metadata": {"_cell_guid": "b51f8c55-be7c-418f-b906-a9ea830950fa", "_uuid": "7d659348d67e2bd65c27ed36341d86cfaa0b62d3"}, "cell_type": "markdown", "source": ["Let's grab a sample sentence to play with."]}, {"metadata": {"_cell_guid": "3faae455-ea2b-4524-9394-0a08cb624c8f", "_uuid": "9ed1835a032a55e7a1aca609aeaab33a2a2f293a"}, "cell_type": "code", "execution_count": null, "source": ["sample_sent = train.loc[1000, 'text']\n", "print(sample_sent)"], "outputs": []}, {"metadata": {"_cell_guid": "564ba77f-c3d2-4048-aeac-69c122bbfc09", "_uuid": "7dccb27851ab72af1c5edfd0620d0ee7639ec3e5"}, "cell_type": "code", "execution_count": null, "source": ["parsed_sent = nlp(sample_sent)\n", "print(parsed_sent)"], "outputs": []}, {"metadata": {"_cell_guid": "cf77fb8a-6bd4-4492-8340-86f689cc0162", "_uuid": "ef51dbfb13011d6af6f1fb44074db30572dadc5d"}, "cell_type": "markdown", "source": ["Spacy handed over an object on which we can get sentences, segmentation, ......."]}, {"metadata": {"_cell_guid": "6bdb5179-24cd-4867-aa70-79281ad119a9", "_uuid": "2a7c28d7b0037055ed9395cb42205dbc4c579846"}, "cell_type": "code", "execution_count": null, "source": ["for num, sentence in enumerate(parsed_sent.sents):\n", "    print('Sentence {}:'.format(num + 1))\n", "    print(sentence)\n", "    print()"], "outputs": []}, {"metadata": {"_cell_guid": "48e9e2b2-efd3-41ce-b6b5-2516377e4348", "_uuid": "4d27b7485ec19547a7b361340834712298753163"}, "cell_type": "markdown", "source": ["Named entity detection"]}, {"metadata": {"_cell_guid": "c5225670-5731-48d4-bdfb-fecddedc5144", "_uuid": "dfea13fe6bd8d67d71eb1133c60e0dd48f490a37"}, "cell_type": "code", "execution_count": null, "source": ["for num, entity in enumerate(parsed_sent.ents):\n", "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n", "    print()"], "outputs": []}, {"metadata": {"_cell_guid": "99940ab1-8bd6-488b-82c6-df56b3894aaa", "_uuid": "9ae23f7f6d1767bfc541f623ef48a0c8bd58dc27"}, "cell_type": "markdown", "source": ["Part of speech tagging"]}, {"metadata": {"_cell_guid": "63ef1ed8-59ff-4b28-a425-8bc10c48bf76", "_uuid": "c234926714166aeb13f3aaf5d6867dd5dfea42f9"}, "cell_type": "code", "execution_count": null, "source": ["token_text = [token.orth_ for token in parsed_sent]\n", "token_pos = [token.pos_ for token in parsed_sent]\n", "\n", "pd.DataFrame({'token_text': token_text, 'part_of_speech': token_pos})"], "outputs": []}, {"metadata": {"_cell_guid": "d6a0b766-c895-4a0d-adb4-2e9912fdcef7", "_uuid": "e5a6ccf4425735f5e4104269f6f622b44afcdadb"}, "cell_type": "markdown", "source": ["What about text normalization, like stemming/lemmatization and shape analysis?"]}, {"metadata": {"_cell_guid": "4249afe6-ceb8-4872-91e0-e39db231e76b", "_uuid": "df2187f4e48ad98d6a9d4a3466866347ada1482b"}, "cell_type": "code", "execution_count": null, "source": ["token_lemma = [token.lemma_ for token in parsed_sent]\n", "token_shape = [token.shape_ for token in parsed_sent]\n", "\n", "pd.DataFrame({'token_text': token_text, 'token_lemma': token_lemma, 'token_shape': token_shape})"], "outputs": []}, {"metadata": {"_cell_guid": "bc15068f-026e-4576-905e-b40e981e5776", "_uuid": "e7ba4acacd37e6ba4a4f246bfbe5dad31948d005"}, "cell_type": "markdown", "source": ["What about token-level entity analysis?"]}, {"metadata": {"_cell_guid": "49a4a358-2af9-40ea-ad67-f73bc2a1e12b", "_uuid": "f2640f5c14eaf7359dc4fbf93161ac9c245277a0"}, "cell_type": "code", "execution_count": null, "source": ["token_entity_type = [token.ent_type_ for token in parsed_sent]\n", "token_entity_iob = [token.ent_iob_ for token in parsed_sent]\n", "\n", "pd.DataFrame({'token_text': token_text, 'token_entity_type': token_entity_type,\n", "              'token_entity_iob': token_entity_iob})"], "outputs": []}, {"metadata": {"_cell_guid": "b17d4e63-939e-4d96-9dbf-4b506edce96d", "_uuid": "d02b1867b990b4d45cbcfe1ca4f8bc2cc9cc0b1c"}, "cell_type": "markdown", "source": ["In  the above dataframe B, I, O stand for Begin, Inside, Outside respectively. Example New York is described as 'New' is begin and 'York' is inside of a phrase 'New York'. Because 'New York' occurs frequently.\n", "\n", "What about a variety of other token-level attributes, such as the relative frequency of tokens, and whether or not a token matches any of these categories?\n", "* stopword\n", "* punctuation\n", "* whitespace\n", "* represents a number\n", "* whether or not the token is included in spaCy's default vocabulary?"]}, {"metadata": {"_cell_guid": "d1abdfb6-9efc-4076-b76f-1873aab3c7d5", "_uuid": "c3b8a0a2b2c38ce94cf82e52430ae41c82d25b88"}, "cell_type": "code", "execution_count": null, "source": ["token_attributes = [(token.orth_,\n", "                     token.prob,\n", "                     token.is_stop,\n", "                     token.is_punct,\n", "                     token.is_space,\n", "                     token.like_num,\n", "                     token.is_oov)\n", "                    for token in parsed_sent]\n", "\n", "df = pd.DataFrame(token_attributes,\n", "                  columns=['text',\n", "                           'log_probability',\n", "                           'stop?',\n", "                           'punctuation?',\n", "                           'whitespace?',\n", "                           'number?',\n", "                           'out of vocab.?'])\n", "\n", "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n", "                                       .applymap(lambda x: u'Yes' if x else u''))\n", "                                               \n", "df"], "outputs": []}, {"metadata": {"_cell_guid": "301177f8-6fda-456c-823b-3cb217dc7a71", "_uuid": "12e6071fba305c1c6a6c864b97e926142453e2d6"}, "cell_type": "markdown", "source": ["## Phrase Modeling\n", "Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. \n", "\n", "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n", "\n", "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york would become new_york). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n", "\n", "We turn to the indispensible gensim library to help us with phrase modeling \u2014 the Phrases class in particular."]}, {"metadata": {"_cell_guid": "c0047317-486f-4257-a9a1-76af2455c989", "_uuid": "63137228182ca2130f07cda85d33eecb3c427ccc"}, "cell_type": "markdown", "source": ["As we're performing phrase modeling, we'll be doing some iterative data transformation at the same time. Our roadmap for data preparation includes:\n", "* normalize text\n", "* First-order phrase modeling \u2192\u2192 apply first-order phrase model to transform sentences\n", "* Second-order phrase modeling \u2192\u2192 apply second-order phrase model to transform sentences\n", "* Apply text normalization and second-order phrase model to text\n", "* We'll use this transformed data as the input for some higher-level modeling approaches in the following sections.\n", "\n", "First, let's define a few helper functions that we'll use for text normalization. In particular, the lemmatized_sentence function will use spaCy to:\n", "* Iterate over all the sentences \n", "* Remove punctuation and excess whitespace\n", "* Lemmatize the text"]}, {"metadata": {"_cell_guid": "8d7f02e4-6293-4717-962a-e3e266fcd92b", "_uuid": "0a20cd2e8f4a5ceafec6fd5d641a86baf8cc6400", "collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["def punct_space(token):\n", "    \"\"\"\n", "    helper function to eliminate tokens\n", "    that are pure punctuation or whitespace\n", "    \"\"\"\n", "    \n", "    return token.is_punct or token.is_space\n", "\n", "def lemmatized_sentence(sent):\n", "    \"\"\"\n", "    helper function to use spaCy to parse sentences,\n", "    lemmatize the text\n", "    \"\"\"\n", "    return u' '.join([token.lemma_ for token in nlp(sent)\n", "                             if not punct_space(token)])"], "outputs": []}, {"metadata": {"_cell_guid": "77492ab5-d751-47dd-8ed4-ca4772c1bf40", "_uuid": "de42605b6c0d0aca0b802d93f045e1cb885f1aa9", "collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["train['unigram_text'] = train['text'].map(lambda x: lemmatized_sentence(x))"], "outputs": []}, {"metadata": {"_cell_guid": "8e350a70-9af4-4da6-946d-885fed98ba58", "_uuid": "77a6c15b6db6cc46f49b7ad880752373b4f4a40f"}, "cell_type": "code", "execution_count": null, "source": ["print(train.loc[1000, 'unigram_text'])"], "outputs": []}, {"metadata": {"_cell_guid": "65f30190-8121-4e34-969f-0cacd7a213be", "_uuid": "44c6a85d98a180fcf5c043133dabcf29b2ac0b69"}, "cell_type": "markdown", "source": ["Next, we'll learn a phrase model that will link individual words into two-word phrases. We'd expect words that together represent a specific concept, like \"new york\", to be linked together to form a new, single token: \"new_york\"."]}, {"metadata": {"_cell_guid": "19fa595c-63dc-433e-a7f7-2fa1707120dd", "_uuid": "ff2277dbe7e16b4f30ef755157f6b31217b7e464", "collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["bigram_model = Phrases(train.loc[:, 'unigram_text'])"], "outputs": []}, {"metadata": {"_cell_guid": "46be5b49-1575-4871-a2db-fe97f4bb2556", "_uuid": "e34ff408fee02797b925141f4b4c5e02016f71bd"}, "cell_type": "code", "execution_count": null, "source": ["train['bigram_text'] = train['unigram_text'].map(lambda x: u''.join(bigram_model[x]))"], "outputs": []}, {"metadata": {"_cell_guid": "b1374fdf-50b5-4925-8c17-f02099be10fd", "_uuid": "d8cc7fb80a3a56d72a783cc3eed02ce7874f345e"}, "cell_type": "code", "execution_count": null, "source": ["print(train.loc[1000, 'bigram_text'])"], "outputs": []}, {"metadata": {"_cell_guid": "de00ab19-64bf-4858-add6-d06b54a7da0b", "_uuid": "06bd08f4e5c266c163e672a9ef07608c1d176fa3"}, "cell_type": "markdown", "source": ["The text to learn is less, so model did not encounter new york many times and it could not generalize well. Which is really not necessary for this problem but might be useful in complex applications. [This link provides good examples on this.](http://nbviewer.jupyter.org/github/naveenrc/YelpChallenge/blob/e4813a131c788242c233b9e9290de0126544f3e0/Modern_NLP.ipynb#Phrase-Modeling)"]}, {"metadata": {"_cell_guid": "0371fbef-b3bb-4849-b2d6-a19236e94821", "_uuid": "62168731b15a41e107d9d540797dac1660f207a2"}, "cell_type": "markdown", "source": ["## Topic Modeling with Latent Dirichlet Allocation (LDA)\n", "Topic modeling is family of techniques that can be used to describe and summarize the documents in a corpus according to a set of latent \"topics\".\n", "\n", "In many conventional NLP applications, documents are represented a mixture of the individual tokens (words and phrases) they contain. In other words, a document is represented as a vector of token counts. There are two layers in this model \u2014 documents and tokens \u2014 and the size or dimensionality of the document vectors is the number of tokens in the corpus vocabulary. This approach has a number of disadvantages:\n", "\n", "* Document vectors tend to be large (one dimension for each token \u21d2\u21d2 lots of dimensions)\n", "* They also tend to be very sparse. Any given document only contains a small fraction of all tokens in the vocabulary, so most values in the document's token vector are 0.\n", "* The dimensions are fully indepedent from each other \u2014 there's no sense of connection between related tokens, such as knife and fork.\n", "\n", "LDA injects a third layer into this conceptual model. Documents are represented as a mixture of a pre-defined number of topics, and the topics are represented as a mixture of the individual tokens in the vocabulary. The number of topics is a model hyperparameter selected by the practitioner. LDA makes a prior assumption that the (document, topic) and (topic, token) mixtures follow Dirichlet probability distributions. This assumption encourages documents to consist mostly of a handful of topics, and topics to consist mostly of a modest set of the tokens.\n", "\n", "LDA is fully unsupervised. The topics are \"discovered\" automatically from the data by trying to maximize the likelihood of observing the documents in your corpus, given the modeling assumptions. They are expected to capture some latent structure and organization within the documents, and often have a meaningful human interpretation for people familiar with the subject material.\n", "\n", "We'll again turn to gensim to assist with data preparation and modeling. In particular, gensim offers a high-performance parallelized implementation of LDA with its LdaMulticore class."]}, {"metadata": {"_cell_guid": "ec875155-2973-4e2e-94a1-a634d1717336", "_uuid": "67256528534b38d20ffad5cbceafccbad87ffd05"}, "cell_type": "markdown", "source": ["The first step to creating an LDA model is to learn the full vocabulary of the corpus to be modeled. We'll use gensim's Dictionary class for this."]}, {"metadata": {"_cell_guid": "009fff85-2c39-4adb-8472-c2efce3f556d", "_uuid": "45f1552914a1d5d785d43e7017df23e4d810ba11", "collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["for idx, sent in train['bigram_text'].iteritems():\n", "    bigram_dictionary = Dictionary([sent.split()])\n", "    \n", "bigram_dictionary.compactify()"], "outputs": []}, {"metadata": {"_cell_guid": "65b6eda3-b850-4e89-8480-45ca1bd4f039", "_uuid": "92fe1463c6515eebb7b0afd1236854ca10a04e12"}, "cell_type": "markdown", "source": ["Like many NLP techniques, LDA uses a simplifying assumption known as the bag-of-words model. In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, is discarded.\n", "\n", "Using the gensim Dictionary we learned to generate a bag-of-words representation for each review. The bigram_bow_generator function implements this. We'll save the resulting bag-of-words reviews as a matrix.\n", "\n", "In the following code, \"bag-of-words\" is abbreviated as bow."]}, {"metadata": {"_cell_guid": "89b334bb-fe7d-4391-9bdb-240699084a94", "_uuid": "6530e1fdd1a1b7c60b4fbd296c62803cd0671bb1", "collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["bigram_bow = bigram_dictionary.doc2bow(train['bigram_text'])"], "outputs": []}, {"metadata": {"_cell_guid": "2b67df45-8013-4e2a-ac8d-acad3113939d", "_uuid": "27a079c623a2eef5ba840d298dd683d05591ea9b", "collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["lda = LdaMulticore(bigram_bow,\n", "                   num_topics=3,\n", "                   id2word=bigram_dictionary)"], "outputs": []}, {"metadata": {"_cell_guid": "ee507274-b265-49ae-85bb-83655bd80eba", "_uuid": "5e9cb7c0139c7d44157c9fd049af65fc8b21138d", "collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["def explore_topic(topic_number, topn=25):\n", "    \"\"\"\n", "    accept a user-supplied topic number and\n", "    print out a formatted list of the top terms\n", "    \"\"\"\n", "        \n", "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n", "\n", "    for term, frequency in lda.show_topic(topic_number, topn=25):\n", "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"], "outputs": []}, {"metadata": {"_cell_guid": "4652acf2-308c-4c2f-b564-177e948ad336", "_uuid": "48407377b37fe5121a84aa0ae0ace107327bc388"}, "cell_type": "code", "execution_count": null, "source": ["explore_topic(2)"], "outputs": []}, {"metadata": {"_cell_guid": "e7184b29-58c6-4af2-b795-fd860fa2d958", "_uuid": "b665bb37224297bcf716fd1340952b15156f925a"}, "cell_type": "code", "execution_count": null, "source": ["LDAvis_prepared = pyLDAvis.gensim.prepare(lda, bigram_bow, bigram_dictionary)\n", "pyLDAvis.display(LDAvis_prepared)"], "outputs": []}, {"metadata": {"_cell_guid": "a5b16724-0b97-4305-9920-435665af25f1", "_uuid": "4640dc2297492d5e77a8d029251bef5918699856"}, "cell_type": "markdown", "source": ["### Wait, what am I looking at again?\n", "There are a lot of moving parts in the visualization. Here's a brief summary:\n", "\n", "* On the left, there is a plot of the \"distance\" between all of the topics (labeled as the Intertopic Distance Map)\n", "\n", "The plot is rendered in two dimensions according a multidimensional scaling (MDS) algorithm. Topics that are generally similar should be appear close together on the plot, while dissimilar topics should appear far apart.\n", "\n", "The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n", "\n", "An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n", "\n", "* On the right, there is a bar chart showing top terms.\n", "\n", "When no topic is selected in the plot on the left, the bar chart shows the top most \"salient\" terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n", "\n", "When a particular topic is selected, the bar chart changes to show the top most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter \u03bb\u03bb, which can be adjusted with a slider above the bar chart.\n", "\n", "Setting the \u03bb\u03bb parameter close to 1.0 (the default) will rank the terms solely according to their probability within the topic.\n", "\n", "Setting \u03bb\u03bb close to 0.0 will rank the terms solely according to their \"distinctiveness\" or \"exclusivity\" within the topic \u2014 i.e., terms that occur only in this topic, and do not occur in other topics.\n", "\n", "Setting \u03bb\u03bb to values between 0.0 and 1.0 will result in an intermediate ranking, weighting term probability and exclusivity accordingly.\n", "\n", "Rolling the mouse over a term in the bar chart on the right will cause the topic circles to resize in the plot on the left, to show the strength of the relationship between the topics and the selected term.\n", "\n", "A more detailed explanation of the pyLDAvis visualization can be found here. Unfortunately, though the data used by gensim and pyLDAvis are the same, they don't use the same ID numbers for topics. If you need to match up topics in gensim's LdaMulticore object and pyLDAvis' visualization, you have to dig through the terms manually.\n", "\n", "### Analyzing our LDA model\n", "The interactive visualization pyLDAvis produces is helpful for both:\n", "\n", "Better understanding and interpreting individual topics, and Better understanding the relationships between the topics.\n", "\n", "For (1), you can manually select each topic to view its top most freqeuent and/or \"relevant\" terms, using different values of the \u03bb\u03bb parameter. This can help when you're trying to assign a human interpretable name or \"meaning\" to each topic.\n", "\n", "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.\n", "\n", "### Describing text with LDA\n", "Beyond data exploration, one of the key uses for an LDA model is providing a compact, quantitative description of natural language text. Once an LDA model has been trained, it can be used to represent free text as a mixture of the topics the model learned from the original corpus. This mixture can be interpreted as a probability distribution across the topics, so the LDA representation of a paragraph of text might look like 50% Topic A, 20% Topic B, 20% Topic C, and 10% Topic D.\n", "\n", "To use an LDA model to generate a vector representation of new text, you'll need to apply any text preprocessing steps you used on the model's training corpus to the new text, too. For our model, the preprocessing steps we used include:\n", "\n", "* Using spaCy to remove punctuation and lemmatize the text\n", "* Applying our first-order phrase model to join word pairs\n", "* Applying our second-order phrase model to join longer phrases\n", "* Removing stopwords\n", "* Creating a bag-of-words representation\n", "\n", "Once you've applied these preprocessing steps to the new text, it's ready to pass directly to the model to create an LDA representation. The lda_description(...) function will perform all these steps for us, including printing the resulting topical description of the input text."]}, {"metadata": {"_cell_guid": "131ec21b-cabc-44e0-b89a-5f1155e08f86", "_uuid": "0f386ba3e3b6ce3c2d2f0d9757834ebe687c310b", "collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["topic_names={0: 'EAP',\n", "             1: 'MWS',\n", "             2: 'HPL'}\n", "def lda_description(text, min_topic_freq=0.08):\n", "    \"\"\"\n", "    accept the original text of a review and (1) parse it with spaCy,\n", "    (2) apply text pre-proccessing steps, (3) create a bag-of-words\n", "    representation, (4) create an LDA representation, and\n", "    (5) print a sorted list of the top topics in the LDA representation\n", "    \"\"\"\n", "    \n", "    # parse the review text with spaCy\n", "    parsed_sent = nlp(text)\n", "    \n", "    # lemmatize the text and remove punctuation and whitespace\n", "    unigram_sent = [token.lemma_ for token in parsed_sent\n", "                      if not punct_space(token)]\n", "    \n", "    # apply the first-order models\n", "    bigram_sent = bigram_model[unigram_sent]\n", "    \n", "    # remove any remaining stopwords\n", "    bigram_sent = [term for term in bigram_sent\n", "                      if not term in spacy.en.English.Defaults.stop_words]\n", "    \n", "    # create a bag-of-words representation\n", "    sent_bow = bigram_dictionary.doc2bow(bigram_sent)\n", "    \n", "    # create an LDA representation\n", "    sent_lda = lda[sent_bow]\n", "    \n", "    # sort with the most highly related topics first\n", "    sent_lda = sorted(sent_lda, key=lambda x: -x[1])\n", "    \n", "    for topic_number, freq in sent_lda:\n", "        if freq < min_topic_freq:\n", "            break\n", "            \n", "        # print the most highly related topic names and frequencies\n", "        print('{:25} {}'.format(topic_names[topic_number],\n", "                                round(freq, 3)))"], "outputs": []}, {"metadata": {"_cell_guid": "02677451-d9cc-453a-bcd7-d64f0596aa48", "_uuid": "1586ca2abd484dc5acfa28bc9e9589e6e2352e7d"}, "cell_type": "code", "execution_count": null, "source": ["print('Probabilities:')\n", "print(lda_description(train.loc[5, 'text']))\n", "print('Actual:', train.loc[5, 'author'])"], "outputs": []}, {"metadata": {"_cell_guid": "6595cabc-5cb4-4126-b7b5-56eecc509303", "_uuid": "c27865077e434df5c7194b22c0a4f9801c1952d1"}, "cell_type": "code", "execution_count": null, "source": ["print('Probabilities:')\n", "print(lda_description(train.loc[1000, 'text']))\n", "print('Actual:', train.loc[1000, 'author'])"], "outputs": []}, {"metadata": {"_cell_guid": "43a6fddb-10cb-4c89-a77b-21480f22d159", "_uuid": "422f11283f12f103c82351f489d154c34e53a6ea"}, "cell_type": "markdown", "source": ["Model is trying to predict the topic probabilities as you can see above. This is an example of how LDA works. Topics are not exactly what we need. I have just mapped the names to show how its used and can be interpreted.\n", "\n", "## Word Vector Embedding with Word2Vec\n", "The goal of word vector embedding models, or word vector models for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the meaning or concept the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised \u2014 they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n", "\n", "Perhaps the best-known word vector model is word2vec, originally proposed in 2013. The general idea of word2vec is, for a given focus word, to use the context of the word \u2014 i.e., the other words immediately before and after it \u2014 to provide hints about what the focus word might mean. To do this, word2vec uses a sliding window technique, where it considers snippets of text only a few tokens long at a time.\n", "\n", "At the start of the learning process, the model initializes random vectors for all terms in the corpus vocabulary. The model then slides the window across every snippet of text in the corpus, with each word taking turns as the focus word. Each time the model considers a new snippet, it tries to learn some information about the focus word based on the surrouding context, and it \"nudges\" the words' vector representations accordingly. One complete pass sliding the window across all of the corpus text is known as a training epoch. It's common to train a word2vec model for multiple passes/epochs over the corpus. Over time, the model rearranges the terms' vector representations such that terms that frequently appear in similar contexts have vector representations that are close to each other in vector space.\n", "\n", "For a deeper dive into word2vec's machine learning process, see here.\n", "\n", "Word2vec has a number of user-defined hyperparameters, including:\n", "\n", "* The dimensionality of the vectors. Typical choices include a few dozen to several hundred.\n", "* The width of the sliding window, in tokens. Five is a common default choice, but narrower and wider windows are possible.\n", "* The number of training epochs.\n", "\n", "For using word2vec in Python, gensim comes to the rescue again! It offers a highly-optimized, parallelized implementation of the word2vec algorithm with its Word2Vec class."]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["train['vec_inp'] = train['bigram_text'].map(lambda x: x.split(' '))"], "outputs": []}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["import sys\n", "\n", "word2vec = Word2Vec(train['vec_inp'], size=20, window=5,\n", "                        min_count=5, sg=0)\n", "\n", "# perform another 100 epochs of training\n", "for i in range(1,200):\n", "    sys.stderr.write('\\rOn {}'.format(i))\n", "    word2vec.train(train['vec_inp'], total_examples=word2vec.corpus_count, \n", "                   epochs=word2vec.iter)"], "outputs": []}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["print(u'{:,} terms in the word2vec vocabulary.'.format(len(word2vec.wv.vocab)))"], "outputs": []}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["# build a list of the terms, integer indices,\n", "# and term counts from the food2vec model vocabulary\n", "ordered_vocab = [(term, voc.index, voc.count)\n", "                 for term, voc in word2vec.wv.vocab.items()]\n", "\n", "# sort by the term counts, so the most common terms appear first\n", "ordered_vocab = sorted(ordered_vocab, key=lambda x: -x[2])\n", "\n", "# unzip the terms, integer indices, and counts into separate lists\n", "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n", "# create a DataFrame with the word2vec vectors as data,\n", "# and the terms as row labels\n", "word_vectors = pd.DataFrame(word2vec.wv.syn0[:],\n", "                            index=ordered_terms)\n", "\n", "word_vectors"], "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["def get_related_terms(token, topn=10):\n", "    \"\"\"\n", "    look up the topn most similar terms to token\n", "    and print them as a formatted list\n", "    \"\"\"\n", "\n", "    for word, similarity in word2vec.most_similar(positive=[token], topn=topn):\n", "\n", "        print(u'{:20} {}'.format(word, round(similarity, 3)))"], "outputs": []}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["get_related_terms(u'owl')"], "outputs": []}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["get_related_terms(u'fear')"], "outputs": []}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["get_related_terms(u'blood')"], "outputs": []}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["get_related_terms(u'jealous')"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["### Word algebra!\n", "The core idea is that once words are represented as numerical vectors, you can do math with them. The mathematical procedure goes like this:\n", "\n", "* Provide a set of words or phrases that you'd like to add or subtract.\n", "* Look up the vectors that represent those terms in the word vector model.\n", "* Add and subtract those vectors to produce a new, combined vector.\n", "* Look up the most similar vector(s) to this new, combined vector via cosine similarity.\n", "* Return the word(s) associated with the similar vector(s).\n", "\n", "But more generally, you can think of the vectors that represent each word as encoding some information about the meaning or concepts of the word. What happens when you ask the model to combine the meaning and concepts of words in new ways? Let's see."]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["def word_algebra(add=[], subtract=[], topn=1):\n", "    \"\"\"\n", "    combine the vectors associated with the words provided\n", "    in add= and subtract=, look up the topn most similar\n", "    terms to the combined vector, and print the result(s)\n", "    \"\"\"\n", "    answers = word2vec.most_similar(positive=add, negative=subtract, topn=topn)\n", "    \n", "    for term, similarity in answers:\n", "        print(term)"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["### NIGHT + FEAR = ?"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["word_algebra(add=[u'night', u'fear'])"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["### FEAR - NIGHT = ? (something negative)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["word_algebra(add=[u'fear'], subtract=[u'night'])"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["### NIGHT - FEAR = ? (something pleasant)"]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["word_algebra(add=[u'night'], subtract=[u'fear'])"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["## Word Vector Visualization with t-SNE\n", "t-Distributed Stochastic Neighbor Embedding, or t-SNE for short, is a dimensionality reduction technique to assist with visualizing high-dimensional datasets. It attempts to map high-dimensional data onto a low two- or three-dimensional representation such that the relative distances between points are preserved as closely as possible in both high-dimensional and low-dimensional space.\n", "\n", "scikit-learn provides a convenient implementation of the t-SNE algorithm with its TSNE class."]}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["tsne_input = word_vectors.drop(spacy.en.English.Defaults.stop_words, errors=u'ignore')"], "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["tsne = TSNE()\n", "tsne_vectors = tsne.fit_transform(tsne_input.values)"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["Now we have a two-dimensional representation of our data! Let's take a look."]}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["tsne_vectors = pd.DataFrame(tsne_vectors,\n", "                            index=pd.Index(tsne_input.index),\n", "                            columns=[u'x_coord', u'y_coord'])\n", "tsne_vectors.head()"], "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": ["tsne_vectors[u'word'] = tsne_vectors.index"], "outputs": []}, {"metadata": {}, "cell_type": "code", "execution_count": null, "source": ["# add our DataFrame as a ColumnDataSource for Bokeh\n", "plot_data = ColumnDataSource(tsne_vectors)\n", "\n", "# create the plot and configure the\n", "# title, dimensions, and tools\n", "tsne_plot = figure(title=u't-SNE Word Embeddings',\n", "                   plot_width = 800,\n", "                   plot_height = 800,\n", "                   tools= (u'pan, wheel_zoom, box_zoom,'\n", "                           u'box_select, resize, reset'),\n", "                   active_scroll=u'wheel_zoom')\n", "\n", "# add a hover tool to display words on roll-over\n", "tsne_plot.add_tools( HoverTool(tooltips = u'@word') )\n", "\n", "# draw the words as circles on the plot\n", "tsne_plot.circle(u'x_coord', u'y_coord', source=plot_data,\n", "                 color=u'blue', line_alpha=0.2, fill_alpha=0.1,\n", "                 size=10, hover_line_color=u'black')\n", "\n", "# configure visual elements of the plot\n", "tsne_plot.title.text_font_size = value(u'16pt')\n", "tsne_plot.xaxis.visible = False\n", "tsne_plot.yaxis.visible = False\n", "tsne_plot.grid.grid_line_color = None\n", "tsne_plot.outline_line_color = None\n", "\n", "# engage!\n", "show(tsne_plot);"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["Conclusion\n", "Let's round up the major components that we've seen:\n", "\n", "* Text processing with spaCy\n", "* Automated phrase modeling\n", "* Topic modeling with LDA  \u27f6  \u27f6  visualization with pyLDAvis\n", "* Word vector modeling with word2vec  \u27f6  \u27f6  visualization with t-SNE\n", "\n", "Why use these models?<br>\n", "Dense vector representations for text like LDA and word2vec can greatly improve performance for a number of common, text-heavy problems like:\n", "\n", "* Text classification\n", "* Search\n", "* Recommendations\n", "* Question answering\n", "...and more generally are a powerful way machines can help humans make sense of what's in a giant pile of text. They're also often useful as a pre-processing step for many other downstream machine learning applications"]}]}