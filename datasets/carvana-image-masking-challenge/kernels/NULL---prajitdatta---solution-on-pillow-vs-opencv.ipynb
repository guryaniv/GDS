{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.1", "pygments_lexer": "ipython3", "file_extension": ".py"}}, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "10b24b50e4a18c18eba46934d8245217bb41c5b4", "_cell_guid": "aca3928b-ed3d-4a9a-88fc-02bc3eea1750"}, "source": ["**Fast benchmark: Pillow vs OpenCV**\n", "Background: when we deal with images in image-based problems and deploy a deep learning solution, it is better to have a fast image reading and transforming library. Let's compare Pillow and OpenCV python libraries on image loading and some basic transformations on source images from Carvana competition.\n", "OpenCV: C++, python-wrapper\n", "Pillow: Python, C\n", "Intuition says that Opencv should be a little faster, let's see this by examples\n", "This question I asked myself after reading the PyTorch documentation on image transformation. Most of transformations take as input a PIL image."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["import PIL\n", "import cv2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["At first, let's get packages versions, specs and some info on the machine"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {}, "source": ["print(cv2.__version__, cv2.__spec__)\n", "print(cv2.getBuildInformation())"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {}, "source": ["PIL.__version__, PIL.__spec__"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {}, "source": ["!cat /proc/cpuinfo | egrep \"model name\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Data storage info: ROTA 1 means rotational device"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {}, "source": ["!lsblk -o name,rota,type,mountpoint"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's setup the input data"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {}, "source": ["import os\n", "this_path = os.path.dirname('.')\n", "\n", "INPUT_PATH = os.path.abspath(os.path.join(this_path, '..', 'input'))\n", "TRAIN_DATA = os.path.join(INPUT_PATH, \"train\")\n", "from glob import glob\n", "filenames = glob(os.path.join(TRAIN_DATA, \"*.jpg\"))\n", "len(filenames)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["import matplotlib.pylab as plt\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["1 stage: 100 images, load image + blur + flip"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["import numpy as np\n", "from PIL import Image, ImageOps\n", "\n", "def stage_1_PIL(filename):\n", "    img_pil = Image.open(filename)\n", "    img_pil = ImageOps.box_blur(img_pil, radius=3)\n", "    img_pil = img_pil.transpose(Image.FLIP_LEFT_RIGHT)\n", "    return np.asarray(img_pil)\n", "\n", "def stage_1_cv2(filename):\n", "    img = cv2.imread(filename)\n", "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "    img = cv2.blur(img, ksize=(3, 3))\n", "    img = cv2.flip(img, flipCode=1)\n", "    return img"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's compare briefly results of transformations on the first image. Results are not perfectly the same, but it is not important for the benchmark"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {}, "source": ["f = filenames[0]\n", "r1 = stage_1_PIL(f) \n", "r2 = stage_1_cv2(f)\n", "\n", "plt.figure(figsize=(16, 16))\n", "plt.subplot(131)\n", "plt.imshow(r1)\n", "plt.subplot(132)\n", "plt.imshow(r2)\n", "plt.subplot(133)\n", "plt.imshow(np.abs(r1 - r2))"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["%timeit -n5 -r3 [stage_1_PIL(f) for f in filenames[:100]]"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["%timeit -n5 -r3 [stage_1_cv2(f) for f in filenames[:100]]"]}], "nbformat_minor": 1}