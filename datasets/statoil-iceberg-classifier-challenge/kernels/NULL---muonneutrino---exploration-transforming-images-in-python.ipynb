{"nbformat": 4, "cells": [{"cell_type": "markdown", "source": ["# Introduction\n", "\n", "In this notebook, I go over some visualizations of the iceberg data. My purpose here is not to set up a classifier, but rather to try to get some sense of what types of features might be most useful.\n", "\n", "As usual, we first have to import some packages."], "metadata": {"_cell_guid": "3f6b5382-2dca-4674-a829-4b40f745f8ab", "_uuid": "e3d807f6eceb919b81af20089a7f24a5785b270f"}}, {"cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline"], "outputs": [], "metadata": {"_cell_guid": "d3b7962e-1fd7-477c-97c3-dbe0b3e86d44", "_uuid": "d5b0a28ec5ffb7a268645d4ef068e71bb1049a2a", "collapsed": true}, "execution_count": 1}, {"cell_type": "markdown", "source": ["I'll now read in the file. It is in a .json format rather than the .csv format that seems to be most common on Kaggle. It also turns out that \"inc_angle\" has some bad values, so I will convert it to a floating point format with the NaN values left in."], "metadata": {"_cell_guid": "ef60ade4-42b5-426b-8bea-6a9bad0ae564", "_uuid": "9760980490941a23097ec0d6439bfd6550421d5c"}}, {"cell_type": "code", "source": ["train = pd.read_json('../input/train.json')\n", "train['inc_angle'] = pd.to_numeric(train['inc_angle'],errors='coerce')"], "outputs": [], "metadata": {"_cell_guid": "4c326022-e1f9-4fcc-aecc-4e49fbd0a7a7", "_uuid": "e2e838a5b59e426560dda47866dc9e1ae96c5e28", "collapsed": true}, "execution_count": 2}, {"cell_type": "markdown", "source": ["# Global Image Stats\n", "\n", "Now, I will look at some global properties of the images. These are things like the minimum and maximum values, the means, medians, and the 50% mid range of the signal values. There are two different bands used in the file, so I'll do this separately for each band."], "metadata": {"_cell_guid": "8471d14e-0f20-4133-93e9-83a45f9ca93f", "_uuid": "053364f970c04e7b7493f5f5a25ef34999eceabc"}}, {"cell_type": "code", "source": ["def get_stats(train,label=1):\n", "    train['max'+str(label)] = [np.max(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['min'+str(label)] = [np.min(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['minpos'+str(label)] = [np.argmin(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['med'+str(label)] = [np.median(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['std'+str(label)] = [np.std(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['mean'+str(label)] = [np.mean(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in train['band_'+str(label)] ]\n", "    train['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in train['band_'+str(label)] ]\n", "    train['mid50_'+str(label)] = train['p75_'+str(label)]-train['p25_'+str(label)]\n", "\n", "    return train\n", "train = get_stats(train,1)\n", "train = get_stats(train,2)"], "outputs": [], "metadata": {"_cell_guid": "d3fb61d3-4257-4087-a687-28021c5321d7", "_uuid": "52be688ca0e79a20b62d3bf4046de090e1ce4757", "collapsed": true}, "execution_count": 3}, {"cell_type": "markdown", "source": ["## Plotting the Statistics\n", "\n", "Now, we can make some histograms of these variables. I'll make histograms of both classes to see if there are any differences."], "metadata": {"_cell_guid": "cede78c3-f4f2-48b2-b7d8-445b4ac5e4e5", "_uuid": "ffbb08b2a534b16274ad6fc5e7c8d0aff149eb16"}}, {"cell_type": "code", "source": ["def plot_var(name,nbins=50):\n", "    minval = train[name].min()\n", "    maxval = train[name].max()\n", "    plt.hist(train.loc[train.is_iceberg==1,name],range=[minval,maxval],\n", "             bins=nbins,color='b',alpha=0.5,label='Boat')\n", "    plt.hist(train.loc[train.is_iceberg==0,name],range=[minval,maxval],\n", "             bins=nbins,color='r',alpha=0.5,label='Iceberg')\n", "    plt.legend()\n", "    plt.xlim([minval,maxval])\n", "    plt.xlabel(name)\n", "    plt.ylabel('Number')\n", "    plt.show()"], "outputs": [], "metadata": {"_cell_guid": "0cdbeba9-755d-4d62-b75b-cf49c6e57300", "_uuid": "185bbb1313da7669c9c88aa1f45610ed424763f6", "collapsed": true}, "execution_count": 4}, {"cell_type": "code", "source": ["for col in ['inc_angle','min1','max1','std1','med1','mean1','mid50_1']:\n", "    plot_var(col)\n"], "outputs": [], "metadata": {"_cell_guid": "55bc1eef-7116-4146-9400-021431ebceb2", "_uuid": "c6847037e3ee35c9c270d7fa90a03eb612921ae1"}, "execution_count": 5}, {"cell_type": "markdown", "source": ["For the first band, we see that there are some significant differences. The middle 50% range has around the same size for both, but the minimum, maximum, standard deviation, median, and mean all show noticeable differences in some range of the values. Evidently, these basic variables seem to have some sensitivity to what we are trying to measure. We might expect this if, for example, icebergs are much larger than ships and thus cover more pixels."], "metadata": {"_cell_guid": "9c7c0d15-a56a-4907-bcc2-90597762e867", "_uuid": "4f7c0c920d52b46535a063522b43ae42ecd48d4f"}}, {"cell_type": "code", "source": ["for col in ['min2','max2','std2','med2','mean2','mid50_2']:\n", "    plot_var(col)"], "outputs": [], "metadata": {"_cell_guid": "ff2e9811-c584-4c54-a339-0e470a582072", "_uuid": "c754292382f321f85cabeea695e42466ca3028a6"}, "execution_count": 6}, {"cell_type": "markdown", "source": ["We get similar results for the second band.\n", "\n", "## Correlations Between Features\n", "\n", "Now that we've established that these variables may have some use, we should look at the correlations between them. For this, I'll just plot the correlation matrix. It would also be good to look at scatter plots of each pair of variables (this can be done easily in Seaborn), but i won't do that here."], "metadata": {"_cell_guid": "564b16ed-29d1-4c86-8a80-36459be07a5d", "_uuid": "7786509c7a4b57f6cdfed05717aa553e87b1742d"}}, {"cell_type": "code", "source": ["train_stats = train.drop(['id','is_iceberg','band_1','band_2'],axis=1)"], "outputs": [], "metadata": {"_cell_guid": "b0bea715-5ad0-4d1e-9b8a-dbd2f054067d", "_uuid": "8d696224f078434f7a831cb8097f1f01fd267832", "collapsed": true}, "execution_count": 7}, {"cell_type": "code", "source": ["corr = train_stats.corr()\n", "fig = plt.figure(1, figsize=(10,10))\n", "plt.imshow(corr,cmap='inferno')\n", "labels = np.arange(len(train_stats.columns))\n", "plt.xticks(labels,train_stats.columns,rotation=90)\n", "plt.yticks(labels,train_stats.columns)\n", "plt.title('Correlation Matrix of Global Variables')\n", "cbar = plt.colorbar(shrink=0.85,pad=0.02)\n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "cff2d8a6-38fc-4e55-870b-e1fe9c472c66", "_uuid": "1ab9c02a97de0b16a03be96f7d8fc3e0461a6fcd"}, "execution_count": 8}, {"cell_type": "markdown", "source": ["We see that there are large correlations between some of the variables. In particular, the mean, median, 25% signal, and 75% signal are all closely related, with nearly 75% correlation. The min and max are also pretty highly correlated for band 1, as are the min and median for both bands, suggesting that the signals have maybe been scaled in some way to force this correlation. There are also some correlations between the two bands. Finally, we see an anticorrelation of around -0.5 between the mean of band 2 and the angle, with a weaker correlation for band 1.\n", "\n", "# Plotting Some Images\n", "\n", "It's good to plot some images before we do too much analysis. That way, we can get some sense of what we're looking at. The images are 75 x 75 pixels each with two bands."], "metadata": {"_cell_guid": "f0086732-c84b-47ad-9c46-b149023604ca", "_uuid": "a3b37a1d0b624a87f4100ba7632273df0ffcf99b"}}, {"cell_type": "code", "source": ["icebergs = train[train.is_iceberg==1].sample(n=9,random_state=123)\n", "ships = train[train.is_iceberg==0].sample(n=9,random_state=456)"], "outputs": [], "metadata": {"_cell_guid": "92dd2b27-7d95-473c-8010-fd94dd6909ae", "_uuid": "558e3c85afd8623a0b3a6de2fa6db17ecc477073", "collapsed": true}, "execution_count": 9}, {"cell_type": "markdown", "source": ["## Raw Images\n", "\n", "The first set show 9 random icebergs using band 1."], "metadata": {"_cell_guid": "eecc2aa9-d4dc-4e8e-9bd7-d1efa98c9185", "_uuid": "8c335063a3ee3d139a82b354127f84db927a2b6e"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = np.reshape(np.array(icebergs.iloc[i,0]),(75,75))\n", "    ax.imshow(arr,cmap='inferno')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "6aff18ab-e6fc-44f2-bd51-711e8eea0cca", "_uuid": "43c9b4538cf5f25ac2d7267d3c970b7318f4809b"}, "execution_count": 10}, {"cell_type": "markdown", "source": ["The second set shows ships in band 1."], "metadata": {"_cell_guid": "fd4f6360-41b5-46cc-9362-8c67fc2e9ef6", "_uuid": "024effe2af1f0e59f25b21be212349aa6682265a"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = np.reshape(np.array(ships.iloc[i,0]),(75,75))\n", "    ax.imshow(arr,cmap='inferno')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "467908fb-4604-43e7-abcb-3926d62d1dd7", "_uuid": "a6f119ad589daac5eeaf68c4e592593e52542d5b"}, "execution_count": 11}, {"cell_type": "markdown", "source": ["The next set show the same set of icebergs in band 2."], "metadata": {"_cell_guid": "24a8494c-d681-4a2a-8fdd-3441fbdb09e2", "_uuid": "7ca7edca88a30695eaa55525aa10e684c78a6e0a"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = np.reshape(np.array(icebergs.iloc[i,1]),(75,75))\n", "    ax.imshow(arr,cmap='inferno')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "49e1ea04-7170-49ac-8b55-0b9a0be5ba17", "_uuid": "00c96fbaaf3637ebcd13377f7fad4edef792db1e"}, "execution_count": 12}, {"cell_type": "markdown", "source": ["Finally, the last set are the same ship images as before, but for band 2."], "metadata": {"_cell_guid": "8d3d0839-04ad-49e2-9c1c-1dfebd547584", "_uuid": "785755445d5113e0959dc257d62ebf6373e90ac4"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = np.reshape(np.array(ships.iloc[i,1]),(75,75))\n", "    ax.imshow(arr,cmap='inferno')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "74672d90-558d-4852-91e8-8b8bc1ba842a", "_uuid": "34c5efefbb7f183000dcbd228f4d6220c91e6de7"}, "execution_count": 13}, {"cell_type": "markdown", "source": ["So, we see that everything looks pretty blob-like. It also appears that the background is not really random noise but rather has some spatial correlations. If the background is dominated by things like waves rather than noise, then spatial correlations would clearly be expected. The ships seem to have a more regular structure, with a pronounced skewness in the blobs for larger signals.\n", "\n", "Some of these blobs are not that high above noise, and in the last set there are even two images where the signal cannot even be seen by eye, so it may be advantageous to first transform the images in some way to enhance the contrast between the signals and the background.\n", "\n", "# Transforming the Images\n", "\n", "I'll look at a few types of basic transforms that can be easily defined by FIR filters. The scipy convolve2d function will run a convolution of two arrays, so we just need to define the kernels. I have not optimized the kernels here, and there are many other choices of types of kernels, so you should try out different options to see what they do."], "metadata": {"_cell_guid": "afebf15a-9881-4751-af55-b89885315dbc", "_uuid": "0b24f383f282d397ffd93453ca32b85635b6844b"}}, {"cell_type": "code", "source": ["from scipy import signal\n", "\n", "xder = np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n", "yder = np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n", "smooth = np.array([[1,1,1],[1,5,1],[1,1,1]])\n", "xder2 = np.array([[-1,2,-1],[-3,6,-3],[-1,2,-1]])\n", "yder2 = np.array([[-1,-3,-1],[2,6,2],[-1,-3,-1]])"], "outputs": [], "metadata": {"_cell_guid": "37f3b0c5-e21b-44ef-9449-180ea3607982", "_uuid": "8ef03db2de1537d51f2fe0fa3b81e8a69b1cd07c", "collapsed": true}, "execution_count": 14}, {"cell_type": "markdown", "source": ["### Smoothing\n", "\n", "First, let's try smoothing the images. The kernel here just has all positive values and is symmetric in both directions.\n", "\n", "I'll first plot the icebergs and then the ships. These are all for Band 1."], "metadata": {"_cell_guid": "73cc0812-2717-4bb8-89e9-fc796be52769", "_uuid": "609d788ff117b019232933a9901c7ad7b6206440"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = signal.convolve2d(np.reshape(np.array(icebergs.iloc[i,0]),(75,75)),smooth,mode='valid')\n", "    ax.imshow(arr,cmap='inferno')\n", "    ax.set_title('Smoothed')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "742f0180-8b5d-4a25-a966-bc96d2e14b20", "_uuid": "db077938970e8ce88eabf34094cdae0af34dc751"}, "execution_count": 15}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = signal.convolve2d(np.reshape(np.array(ships.iloc[i,0]),(75,75)),smooth,mode='valid')\n", "    ax.imshow(arr,cmap='inferno')\n", "    ax.set_title('Smoothed')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "f09be874-41e1-4244-a1d5-4f9991e43bef", "_uuid": "ff1a651da6875f20e9e74a822c8b2259544c080b"}, "execution_count": 16}, {"cell_type": "markdown", "source": ["As we might expect, smoothing blurs the features. However, it also enhances the contrast between bright and dark regions, so it may be quite useful if we want to use it to seed some clusters in a cluster/peak finder.\n", "\n", "### Derivative with Respect to X\n", "\n", "An X-derivative will typically be antisymmetric with respect to reversing the values around the x-axis. This will provide some level of edge detection in the x-direction. I will take the derivatives of the original images."], "metadata": {"_cell_guid": "82dad14e-6256-4a6b-8c9d-e2cba6335a86", "_uuid": "fef7273fd12217517d21d825950b0a7f7a2d33f1"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = signal.convolve2d(np.reshape(np.array(icebergs.iloc[i,0]),(75,75)),xder,mode='valid')\n", "    ax.imshow(arr,cmap='inferno')\n", "    ax.set_title('X-derivative')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "d0f5497d-446e-4639-827b-d24e8a57bb74", "_uuid": "3af43f75ce3ef45c607ced7afeb2862ace02b793"}, "execution_count": 17}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = signal.convolve2d(np.reshape(np.array(ships.iloc[i,0]),(75,75)),xder,mode='valid')\n", "    ax.imshow(arr,cmap='inferno')\n", "    ax.set_title('X-derivative')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "740c90a4-edf3-41ee-8975-f727d987f466", "_uuid": "7438c1cdb7ae5fd175abb6a4e959cbf7c3761f9f"}, "execution_count": 18}, {"cell_type": "markdown", "source": ["Note that you should see a dark region on the left side of a peak and a bright region on the right. If you look closely enough, you should see that the positions may have changed in all of these transformations. This is because I have chosen not to zero-pad the arrays. The resulting transformed arrays are slightly smaller than the input.\n", "\n", "### Gradient Magnitude\n", "\n", "It should also be trivial to see how to do a y-derivative. Rather than that, we can look at the magnitude of the gradient. That is, treat the x and y derivatives as a gradient vector at each position and then take the magnitude at each point."], "metadata": {"_cell_guid": "b8f1ccaf-431d-4f40-9c07-0e67babf52c8", "_uuid": "87be9ec6c126c20e74d9cacfca964a220daafb67"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arrx = signal.convolve2d(np.reshape(np.array(icebergs.iloc[i,0]),(75,75)),xder,mode='valid')\n", "    arry = signal.convolve2d(np.reshape(np.array(icebergs.iloc[i,0]),(75,75)),yder,mode='valid')\n", "    ax.imshow(np.hypot(arrx,arry),cmap='inferno')\n", "    ax.set_title('Gradient Magnitude')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "6deb2cd7-c00b-4a5b-a5d6-e13c3b20c302", "_uuid": "43e60815a8d8c4ecabc10dbd08c13dee598434a1"}, "execution_count": 19}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arrx = signal.convolve2d(np.reshape(np.array(ships.iloc[i,0]),(75,75)),xder,mode='valid')\n", "    arry = signal.convolve2d(np.reshape(np.array(ships.iloc[i,0]),(75,75)),yder,mode='valid')\n", "    ax.imshow(np.hypot(arrx,arry),cmap='inferno')\n", "    ax.set_title('Gradient Magnitude')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "eecde1ae-c4fa-49d2-a5ca-2bb02aea91b4", "_uuid": "d16a52de90aa156ce4f1285c9e25776ee73dc98e"}, "execution_count": 20}, {"cell_type": "markdown", "source": ["We see interesting circular shapes everywhere in these images. But, the signals look fairly strong. The ships, in particular, show fairly bright edges and most create nice loops. This sort of operator might be useful to put into a more advanced model like a neural net. At the very least, it would be good to compare the results using this against things like the raw and smoothed data. \n", "\n", "### Second Derivatives\n", "\n", "We can also define a simple second-derivative operator. A 3x3 second derivative should do less smoothing than a 3x3 first derivative, so we might see less contrast between the signal and the background."], "metadata": {"_cell_guid": "502c880e-b8e0-411a-949e-ed479eee9eb8", "_uuid": "5317b614ac932287b8fdd9d32e7e08980d3c0b15"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = signal.convolve2d(np.reshape(np.array(icebergs.iloc[i,0]),(75,75)),xder2,mode='valid')\n", "    ax.imshow(arr,cmap='inferno')\n", "    ax.set_title(r'Second X derivative')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "946ee417-ce7d-45a6-85c0-b0820073c6b0", "_uuid": "efe9b35eeb8e00abe174bc46419d5177b08158a6"}, "execution_count": 21}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arr = signal.convolve2d(np.reshape(np.array(ships.iloc[i,0]),(75,75)),xder2,mode='valid')\n", "    ax.imshow(arr,cmap='inferno')\n", "    ax.set_title(r'Second X derivative')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "27395a07-8d1f-4f15-b12f-f6402516869f", "_uuid": "8c27b04280b10019bca7d2ccb0dd246c7b9ab66e"}, "execution_count": 22}, {"cell_type": "markdown", "source": ["Here, we do see that the signals are not particularly obvious. The ships are a bit more visible, but the noise looks like it may be quite problematic here unless we do more smoothing.\n", "\n", "### Laplacian\n", "\n", "The Laplacian operator is just the sum of second derivatives, or the divergence of the gradient."], "metadata": {"_cell_guid": "dc94915a-d3f9-4a5d-a4e5-47f8f2dba5ed", "_uuid": "e4d879184549027d2335fe22620503a9b486e89a"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arrx = signal.convolve2d(np.reshape(np.array(icebergs.iloc[i,0]),(75,75)),xder2,mode='valid')\n", "    arry = signal.convolve2d(np.reshape(np.array(icebergs.iloc[i,0]),(75,75)),yder2,mode='valid')\n", "\n", "    ax.imshow(np.hypot(arrx,arry),cmap='inferno')\n", "    ax.set_title('Laplacian')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "ea9790d7-8cac-4aa0-8de9-cec50f377cc3", "_uuid": "42c25ed5570173b531e8f44a170db5fecb72928d"}, "execution_count": 23}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arrx = signal.convolve2d(np.reshape(np.array(ships.iloc[i,0]),(75,75)),xder2,mode='valid')\n", "    arry = signal.convolve2d(np.reshape(np.array(ships.iloc[i,0]),(75,75)),yder2,mode='valid')\n", "\n", "    ax.imshow(np.hypot(arrx,arry),cmap='inferno')\n", "    ax.set_title('Laplacian')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "1bdc8e29-00f6-4c37-8cbb-b2856ff63fd5", "_uuid": "9080d9cb931885f80b0c2a58981d8d3b6458138d"}, "execution_count": 24}, {"cell_type": "markdown", "source": ["We see interesting vertical and horizontal line features in the Laplacian images, but the signals are mostly difficult to see and split into many small clusters. Again, second derivatives probably require more smoothing to be useful.\n", "\n", "### Magnitude of the Curl of Gradient\n", "\n", "There are many other things that we can look at. The last one I'll do is the magnitude of the curl of the gradient. For a differentiable function, this actually is supposed to be exactly 0, but for our discrete images, this likely isn't the case. I wouldn't expect this to be useful though."], "metadata": {"_cell_guid": "ec6d7750-f3ae-4adb-8270-c0b798df7139", "_uuid": "36ae91fdb17a288619f05dfb69e89159e264806c"}}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arrx = signal.convolve2d(np.reshape(np.array(icebergs.iloc[i,0]),(75,75)),xder,mode='valid')\n", "    arry = signal.convolve2d(np.reshape(np.array(icebergs.iloc[i,0]),(75,75)),yder,mode='valid')\n", "    arrx = signal.convolve2d(arrx,yder,mode='valid')\n", "    arry = signal.convolve2d(arry,xder,mode='valid')\n", "    ax.imshow(np.hypot(arrx,arry),cmap='inferno')\n", "    ax.set_title('Curl of Gradient Magnitude')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "63c6196e-3c70-41f8-adb1-4b8efa8e02fc", "_uuid": "8cb2a44b26b67bbac16ce7ac61f76d1795be117c"}, "execution_count": 25}, {"cell_type": "code", "source": ["# Plot band_1\n", "fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3,3,i+1)\n", "    arrx = signal.convolve2d(np.reshape(np.array(ships.iloc[i,0]),(75,75)),xder,mode='valid')\n", "    arry = signal.convolve2d(np.reshape(np.array(ships.iloc[i,0]),(75,75)),yder,mode='valid')\n", "    arrx = signal.convolve2d(arrx,yder,mode='valid')\n", "    arry = signal.convolve2d(arry,xder,mode='valid')\n", "    ax.imshow(np.hypot(arrx,arry),cmap='inferno')\n", "    ax.set_title('Curl of Gradient Magnitude')\n", "    \n", "plt.show()"], "outputs": [], "metadata": {"_cell_guid": "485a3e9c-475a-40a5-9a8f-091ed5c3a766", "_uuid": "a2edc332a5ed04c05a33ffdbd445d9e035742d3e"}, "execution_count": 26}, {"cell_type": "markdown", "source": ["We can see that as expected, it will at best be of limited usefulness. However, we do get some nice looking images. we see a number of small, separated tiles. Even if it's not useful for analysis, we get some nice textures by transforming the images with this particular transformation."], "metadata": {"_cell_guid": "dbe406cf-e39f-4f81-86c6-01ce7fb8a3d8", "_uuid": "be03a83ad0346daad71e62f1b00e12c3eed41ae9"}}, {"cell_type": "markdown", "source": ["# Conclusions\n", "\n", "We've constructed some global features from the images and found that there are some noticeable differences between icebergs and ships even just from those. However, I would expect that a classifier based only on global statistics will not be very effective.\n", "\n", "We've also looked at a number of transformations of the images. Edge detection (gradient)-based methods seem to get some nice features, and smoothing may help out with images with small signal size."], "metadata": {"_cell_guid": "2ec88fec-17d6-40e9-8ef8-cde6dd79cf6a", "_uuid": "11a48ac21641ad2d1b96314407fd4257cb4168ca"}}, {"cell_type": "code", "source": [], "outputs": [], "metadata": {"_cell_guid": "4ec3d083-c3bc-4e05-bffa-d922ea7eecc3", "_uuid": "db9423337ad582cc8abc3513719c177f8caa9d73", "collapsed": true}, "execution_count": null}], "nbformat_minor": 1, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "version": "3.6.3"}}}