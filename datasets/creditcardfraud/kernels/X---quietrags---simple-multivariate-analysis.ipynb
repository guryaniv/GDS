{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "89ad7fce-37cf-445f-dcbf-0482bb8aec23"
      },
      "source": [
        "Multivariate Analysis - An Unsupervised Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8de27eed-13c6-ce31-e018-bb32bf35bc36"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "492c70b1-65f2-cb88-f871-72be7fe205c1"
      },
      "outputs": [],
      "source": [
        "#Importing the libraries\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "plt.style.use('seaborn-whitegrid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "660b1374-73fc-9df7-f80a-66db813bc427"
      },
      "outputs": [],
      "source": [
        "# Read in the csv file\n",
        "df = pd.read_csv('../input/creditcard.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c80f4ef-e4cd-c2a7-c4e3-967dd8a09f2b"
      },
      "outputs": [],
      "source": [
        "# Sort the Values by Class. \n",
        "# Results in all Negative (No Fraud Samples) up top and the Positive (Fraud Samples) after it\n",
        "# Reset the index\n",
        "\n",
        "df.sort_values(by='Class',inplace=True)\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "639ee1ec-fabc-f481-336f-535bc2a566ff"
      },
      "outputs": [],
      "source": [
        "# Store the indices to build train, validation and test sets\n",
        "negative_indices = df[df['Class'] == 0].index.tolist()\n",
        "positive_indices = df[df['Class'] == 1].index.tolist()\n",
        "\n",
        "print(min(negative_indices),max(negative_indices))\n",
        "print(min(positive_indices),max(positive_indices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f717616-4ec6-ae1e-5fa0-8b658f8196b9"
      },
      "outputs": [],
      "source": [
        "# Choosing only negative samples for training Data Set,\n",
        "# Choosing 284000 out of 284315 rows. Will use the remaining rows for validation and test sets\n",
        "training_indices = np.random.choice(negative_indices,284000,replace=False)\n",
        "remaining_negative_indices = [x for x in negative_indices if x not in training_indices]\n",
        "\n",
        "# Choose 100 out of the 315 remaining negative rows for validation set\n",
        "val_negative_indices = np.random.choice(remaining_negative_indices,100,replace=False).tolist()\n",
        "# Choose 100 out of the 491 positive samples to be a part of the validation set\n",
        "val_positive_indices = np.random.choice(positive_indices,100,replace=False).tolist()\n",
        "\n",
        "# Use the remaining to build a Test Set\n",
        "test_negative_indices = [x for x in remaining_negative_indices if x not in val_negative_indices]\n",
        "test_positive_indices = [x for x in positive_indices if x not in val_positive_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ed316896-cfeb-cbf5-ea2e-fb52c2e957de"
      },
      "outputs": [],
      "source": [
        "# Build the list of interesting columns to use as features\n",
        "collist = df.columns.tolist()\n",
        "collist.remove('index')\n",
        "collist.remove('Class')\n",
        "collist.remove('Time')\n",
        "print(collist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "10b4b0e8-8146-490a-7433-06ff81cc0206"
      },
      "outputs": [],
      "source": [
        "# Build the Data Sets - Training, Validation and Testing \n",
        "\n",
        "train_df = df.ix[training_indices,:]\n",
        "val_df = df.ix[val_negative_indices+val_positive_indices,:]\n",
        "test_df = df.ix[test_negative_indices+test_positive_indices,:]\n",
        "\n",
        "print(len(train_df),len(val_df),len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8343215f-adda-dacc-70a6-5916981f525e"
      },
      "outputs": [],
      "source": [
        "# Standardizing our DataSets\n",
        "\n",
        "from sklearn.preprocessing import  StandardScaler\n",
        "\n",
        "scaler  = StandardScaler().fit(train_df[collist])\n",
        "\n",
        "X_train_std = scaler.transform(train_df[collist])\n",
        "X_val_std = scaler.transform(val_df[collist])\n",
        "X_test_std = scaler.transform(test_df[collist])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5690779-37e0-1b86-62fe-b1a7a9260ff4"
      },
      "outputs": [],
      "source": [
        "# Compute the Per Column Mean and the Covariance Matrix\n",
        "\n",
        "sigma = np.cov(X_train_std.T)\n",
        "mean = np.mean(X_train_std,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "feda9bf4-50c2-f10d-e6e2-04c2abceeff7"
      },
      "outputs": [],
      "source": [
        "# Compute the Probability of each validation sample using the multivariate PDF\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "val_pred_probs = multivariate_normal.pdf(X_val_std,mean=mean,cov=sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "793fe3b6-a690-56e2-6f46-e86add46feeb"
      },
      "outputs": [],
      "source": [
        "# Look at the Average Proabability and Std in probability of the negative and the positive samples\n",
        "print(\"Negative Samples\", val_pred_probs[:100].mean(),val_pred_probs[:100].std())\n",
        "print(\"Positive Samples\",val_pred_probs[100:].mean(),val_pred_probs[100:].std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "66f90231-c687-79ff-ab06-e3f06e6c19af"
      },
      "outputs": [],
      "source": [
        "# Lets plot and see how the probabilties for the Negative and the Positive Samples are distributes\n",
        "\n",
        "plt.hist(val_pred_probs[:100],alpha=0.4);\n",
        "plt.hist(val_pred_probs[100:],alpha=0.7);\n",
        "\n",
        "# Looks like the Positive Samples are all within a narrow range of Probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "885efb48-e9be-a3f6-9167-f52df9b53909"
      },
      "outputs": [],
      "source": [
        "# if we set an episilon of 1e-16 then all samples with probability less than this epsilon are fraudalent (hypothesis)\n",
        "preds = (val_pred_probs < 1e-16).astype('int')\n",
        "labels = val_df.Class.values\n",
        "print(confusion_matrix(labels,preds))\n",
        "print(classification_report(labels,preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b12ff23a-854d-020f-8ea2-e747b95068a0"
      },
      "outputs": [],
      "source": [
        "# Lets try the same with our Test Set \n",
        "\n",
        "test_pred_probs = multivariate_normal.pdf(X_test_std,mean=mean,cov=sigma)\n",
        "preds = (test_pred_probs < 1e-16).astype('int')\n",
        "labels = test_df.Class.values\n",
        "\n",
        "print(confusion_matrix(labels,preds))\n",
        "print(classification_report(labels,preds))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3a4fcf57-ff75-15fe-bc66-d53df1d3cb2b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}