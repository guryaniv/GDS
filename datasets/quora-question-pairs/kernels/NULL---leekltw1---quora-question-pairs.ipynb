{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor file in os.listdir('../input/'):\n    print(file.ljust(30)+str(round(os.path.getsize('../input/'+file)/1000000,2))+'MB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"134ea36c3ff10a8f01501ff182eba741def53e9e"},"cell_type":"code","source":"ckpt = datetime.now()\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv',index_col='test_id')\nprint(f'time cost: {datetime.now() - ckpt}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a08c44163a88092352cb4caab49c52e1a12157a2"},"cell_type":"code","source":"print(len(train_data[train_data['is_duplicate']==1]))\nprint(len(train_data[train_data['is_duplicate']==0]))\nprint('The ratio:',len(train_data[train_data['is_duplicate']==1])/len(train_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5309942caa2f204f554546664a6a0a03416727b"},"cell_type":"markdown","source":"# naive guess for benchmark"},{"metadata":{"trusted":true,"_uuid":"759ef18020caba7b598b58c58cf241511a792f19"},"cell_type":"code","source":"# from sklearn.metrics import log_loss\n# probability = train_data['is_duplicate'].mean()\n# naive_loss = log_loss(train_data['is_duplicate'].astype('float'),np.zeros((len(train_data),1))+probability)\n# print(naive_loss)\n# print(train_data[train_data['is_duplicate']==1][['question1', 'question2']].head(5))\n# print(train_data[train_data['is_duplicate']==0][['question1', 'question2']].head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db572b55bac69383f4c195d39612f79df2961e24"},"cell_type":"markdown","source":"# NLP feature enginnering\n\n\nLet's try embedding * tf-idf  seems good.\n\n\n[What does word embedding weighted by tf-idf mean?](https://stats.stackexchange.com/questions/318910/what-does-word-embedding-weighted-by-tf-idf-mean)\n\n[Word2Vec embeddings with TF-IDF](https://datascience.stackexchange.com/questions/28598/word2vec-embeddings-with-tf-idf)\n\n[What does a weighted word embedding mean?](https://stackoverflow.com/questions/47727078/what-does-a-weighted-word-embedding-mean)\n\n[NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment Analysis](http://www.aclweb.org/anthology/S17-2100)\n> In this work, tweets were modeled using three types of text representation. The first one is a bag-of-words model weighted by tf-idf (term frequency - inverse document frequency) (Section 2.1.1). The second represents a sentence by averaging the word embeddings of all words (in the sentence) and the third represents a sentence by averaging the weighted word embeddings of all words, the weight of a word is given by tf-idf (Section 2.1.2)."},{"metadata":{"trusted":true,"_uuid":"f324d1c5b716b2d56be7b006de37a27ad8b824a2"},"cell_type":"code","source":"import psutil\npsutil.virtual_memory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0698e0cd2fc0fac75f34251df34da673085dca2"},"cell_type":"code","source":"print(\"Let's check Nan before processing for train data\")\nfrom IPython.display import display, HTML\nprint('In training data')\nprint('\\nquestion1 is nan:')\ndisplay(train_data[train_data['question1'].apply(lambda x:x is np.nan)])\nprint('question2 is nan:')\ndisplay(train_data[train_data['question2'].apply(lambda x:x is np.nan)])\n\n\nprint(\"Let's check Nan before processing for test data\")\nprint('In testing data')\nprint('\\nquestion1 is nan:')\ndisplay(test_data[test_data['question1'].apply(lambda x:x is np.nan)])\nprint('question2 is nan:')\ndisplay(test_data[test_data['question2'].apply(lambda x:x is np.nan)])\n\nprint('special case')\ndisplay(test_data[(test_data['question1'].apply(lambda x:x is np.nan)) & (test_data['question2'].apply(lambda x:x is np.nan))])\n# test_data.loc[['life in dublin?\"'],:]\n\nprint('Give 0 to these test rows in default.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d311ebd5b1d5cb8a6487411347cf76a7f611b1a5"},"cell_type":"markdown","source":"Legacy code, since we have to reserved  data for submission.\n\n        def delete_nan(data):\n            print('shape before delete',data.shape)\n            data.drop(data[data['question1'].apply(lambda x:x is np.nan)].index,axis=0,inplace=True)\n            data.drop(data[data['question2'].apply(lambda x:x is np.nan)].index,axis=0,inplace=True)\n            print('shape after delete',data.shape)\n            return data\n         \n        train_data = delete_nan(train_data)\n        test_data = delete_nan(test_data)\n        corpus = []\n        for data in [train_data,test_data]:\n            for column in ['question1','question2']:\n                corpus+=list(data[column])\n\nKeep them now. Give 0 to these test rows in default."},{"metadata":{"trusted":true,"_uuid":"6255c7c07ddef9985f7e11147652919e22bf5db5"},"cell_type":"code","source":"psutil.virtual_memory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"920f6de7cd1e83a1d3dcb948ddef5f184f7c2150"},"cell_type":"code","source":"corpus =list(train_data['question1'])+list(train_data['question2'])\nprint('the nan: ',list(filter(lambda x:x[0] if x[1] is np.nan else None, enumerate(corpus))))\ncorpus = list(map(lambda x:'' if x is np.nan else x, corpus))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0ba4000533424ce1a2abf227070bc2f43485d36"},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\"))\nfrom gensim.models import FastText\ncorpus_tokenized = [nltk.word_tokenize(str(sent)) for sent in corpus]\nword_embedding_model = FastText(corpus_tokenized,window=5, min_count=0,workers=1000)\nword_embedding_model.save('./fasttext.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b85ab6050e05ed162921a0912d7c8437d8ad95f"},"cell_type":"markdown","source":"# I don't think remove stopwords before embedding is a good idea, stopwords often show relation between entities, which is important for context, but you can still try it.\n\n```\n'What is the step by step guide to invest in share market in india?',\n 'What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n 'How can I increase the speed of my internet connection while using a VPN?',\n 'Why am I mentally very lonely? How can I solve it?',\n 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?'\n ```\n \n```\n[['What', 'step', 'step', 'guide', 'invest', 'share', 'market', 'india', '?'], \n['What', 'story', 'Kohinoor', '(', 'Koh-i-Noor', ')', 'Diamond', '?'], \n['How', 'I', 'increase', 'speed', 'internet', 'connection', 'using', 'VPN', '?'], \n['Why', 'I', 'mentally', 'lonely', '?', 'How', 'I', 'solve', '?'], \n['Which', 'one', 'dissolve', 'water', 'quikly', 'sugar', ',', 'salt', ',', 'methane', 'carbon', 'di', 'oxide', '?'],\n ```"},{"metadata":{"trusted":true,"_uuid":"238dc0b89a92c25d2372bd4d4f1e4b818d44e56a"},"cell_type":"code","source":"def embedding_after_filter_stopwords():\n    for idx,sent in enumerate(corpus):\n        corpus[idx] = list(filter(lambda word: word not in stop_words,sent))\n    word_embedding_model = FastText(corpus,window=5, min_count=1)\n    word_embedding_model.save('./fasttext_with_stopwords.model')\n    return word_embedding_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d455abf794d7ed737b7fe3efac460edec72ed6cf"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nckpt = datetime.now()\nvectorizer = TfidfVectorizer()\nvectorizer.fit(corpus)\nprint(f'Time cost: {datetime.now()-ckpt}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32034174f5b3cfff242991e5bafced5e922285f8"},"cell_type":"code","source":"train_data[train_data['question1'].apply(lambda x:x is np.nan)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd9d12b109ca05d0b88a883a89142a2b44778aa4"},"cell_type":"code","source":"filter_npnan(train_data.loc[363362,'question1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f907b4b1321455b6b6787fd3cb53584560f5ecc5"},"cell_type":"code","source":"ckpt = datetime.now()\ndef filter_npnan(x):\n    return '' if x is np.nan else x\nx1 = train_data['question1'].apply(filter_npnan)\nx2 = train_data['question2'].apply(filter_npnan)\nprint('Filtered')\nx1 = vectorizer.transform(x1)\nx2 = vectorizer.transform(x2)\nprint(f'Time cost: {datetime.now()-ckpt}')\nprint(type(x1))\nprint(type(x2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f87e0408320bba95dd675cead9aa5308e400f472"},"cell_type":"code","source":"y = train_data['is_duplicate']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e07760ac32a8cc264bb7530ee875dd9022f3211"},"cell_type":"markdown","source":"\n[Compressed Sparse Row matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n"},{"metadata":{"trusted":true,"_uuid":"c6165e7e5fc3baf9d9df78391947b54cd4623f97"},"cell_type":"code","source":"print('The size of bag of words after tf-ifd',len(vectorizer.get_feature_names()))\nprint('corpus length:',len(corpus))\nprint('Shape:',x1.shape)\nprint('Shape:',x2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9949820f35653393f5b14b9875478aff5f64ec5d"},"cell_type":"code","source":"print('type of tfidf vocab',type(vectorizer.vocabulary_))\nprint('length of the vocab',len(vectorizer.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3e2c961cd7f03f88585dea153142832a0848f29"},"cell_type":"code","source":"from scipy.sparse import hstack\nx = hstack([x1,x2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff739ea8cbedc2987536b6ec3c94d50028f86d12"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ff0bb7f6cf8a9abdb40d828c9836f29329af683"},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1ad3ed80768afd02c866dc0c69839cc94813dd6"},"cell_type":"code","source":"est = RandomForestRegressor()\nest.fit(x_train,y_train)\nprint(est.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2f3393fa7514ed52616929a59bd420907b0d76a"},"cell_type":"code","source":"# x_train = np.random.randn(100,6)\n# y_train = x_train[:,0]*5+x_train[:,1]*2+x_train[:,3]*4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f789fa8867419e9efcaac29784ab7ea0d8d044a"},"cell_type":"code","source":"from catboost import Pool, CatBoostRegressor, cv\nfrom catboost import CatBoostClassifier\nmodel = CatBoostClassifier(\n    iterations=5,\n    random_seed=0,\n    learning_rate=0.1\n)\nmodel.fit(\n   x_train, y_train,\n    logging_level='Verbose',\n    plot=True\n);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158e5bfb0970b7cc9f42b6ee22a1355450ff0849"},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}