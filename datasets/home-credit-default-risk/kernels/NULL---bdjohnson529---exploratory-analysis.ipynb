{"cells":[{"metadata":{"_uuid":"7c14e02dc4ffc9b4f551267a96c873ee7e00f7cc"},"cell_type":"markdown","source":"The company Home Credit provides loans to individuals with limited credit history, a community typically underserved by banks and other lenders. Home Credit helps these individuals build a credit history by initially offering Point of Sale loans for home appliances, and gradually extending other lines of credit.\n\nHome Credit has published a dataset collected from a past group of customers, collected at the time of application. This notebook will use the publically available data to train a machine learning model capable of predicting the success of future customers based on application data.\n\nLet's start by examining the dataset *application_train*. Aside from the 'TARGET' feature, which describes the success of loan repayment, all features present in the training dataset are also present in the test dataset. We will select a small group of these 121 features to train the learning model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false,"_kg_hide-input":true,"scrolled":false,"collapsed":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# read in datasets\napp_train = pd.read_csv('../input/application_train.csv')\napp_test = pd.read_csv('../input/application_test.csv')\n\nprint('Training data features: \\t\\t', app_train.shape[1])\nprint('Testing data features: \\t\\t\\t', app_test.shape[1])\nprint('\\nTraining data observations: \\t\\t', app_train.shape[0])\nprint('Testing data observations: \\t\\t', app_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6edab99505c180f3a5289a2184394697e89ce251"},"cell_type":"markdown","source":"Of the 121 features at our disposal, 67 contain some amount of null entries. Our ML model cannot interpret null observations, so if we want to use these partially null features we will need to impute the missing values using correlated features. We will start by analyzing the 55 completely full features which have an entry for every single observation."},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"4eb9077ea84cba8c8fefe15c3a6fdbc1af3494af"},"cell_type":"code","source":"# boolean function to verify whether all features are numeric\ndef TestNumericFeatures(df, feature):\n    non_numeric = []\n\n    unique_var = df[feature].unique()\n\n    # verify that every unique variable is numeric\n    for var in unique_var:\n        vartype = type(var)\n        if (vartype is not np.int64) and (vartype is not np.float64):\n            non_numeric.append(feature)\n            return False\n\n    return True\n\n# analyze features in a CSV\ndef AnalyzeFeatures(df):\n    # list all features in dataset\n    feature_list = df.columns.tolist()\n    d = {'Feature': feature_list, 'Percentage_Full': 1}\t\t# create dictionary\n    feature_data = pd.DataFrame(data=d)\n\n    # track non-numeric booleans\n    non_numeric_bools = []\n    \n    # list features with null entries\n    null_features = df.columns[df.isna().any()].tolist()\n\n    # get percentage of filled entries\n    for entry in null_features:\n        feature = str(entry)\n        x = df[feature].isnull().mean()\n        feature_data.loc[(feature_data['Feature'] == feature), 'Percentage_Full'] = x\n\n    # feature type: (0: boolean, 1: categorical, 2: continuous)\n    feature_data['Type'] = -1\n    feature_list = feature_data['Feature'].tolist()\n    for entry in feature_list:\n        feature = str(entry)\n        unique_var = df[feature].unique()\n\n        # label boolean variables\n        if len(unique_var) == 2:\n            if ( TestNumericFeatures(app_train, feature) ):\n                feature_data.loc[(feature_data['Feature'] == feature), 'Type'] = 0\n            else:\n                non_numeric_bools.append(feature)\n        else:\n            num_var = len(unique_var)\n            feature_data.loc[(feature_data['Feature'] == feature), 'Type'] = num_var\n\n    return feature_data, non_numeric_bools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f317ccfb91ffc465c010a3866e88877b24f03734","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"feature_data, non_numeric_bools = AnalyzeFeatures(app_train)\n\nfull_features = feature_data[['Feature', 'Percentage_Full']].groupby(['Percentage_Full']).get_group(1)\n\nnum_full = full_features.shape[0]\ntotal = feature_data.shape[0]\nnum_partial = total - num_full\n\ndf = pd.DataFrame({'Number of Features': [num_full, num_partial] }, index=['Full Features', 'Partially Null'])\ndf['Number of Features'].plot(kind='pie', autopct='%.2f', figsize=(5, 5))\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d5e1524a3650ab872cbd651d60ba4c319a79b32"},"cell_type":"markdown","source":"Let's start by taking a look at the entries with dates. All data which references a date is negative. Many of our algorithms cannot process negative data, so we will convert all dates to positive floating point numbers in units of years. In the kernel density plots below, it is obvious that the 'YEARS_EMPLOYED' feature is unreliable. There are applications where the customer claims to have been employed for 1000 years! While it is unclear how this data got corrupted, it is clear that we cannot use 'YEARS_EMPLOYED' or 'DAYS_EMPLOYED' feature to train our model.\n\nThe age plot reveals a shift in the probability distribution between the successful and unsuccesful observations. The distribution of successful customers is skewed towards older age, while the distribution of unsuccessful customers is concentrated at ages below 40. This could indicate a positive correlation between age and the likelihood of a customer to repay the loan.\n\n**Hypothesis:  Younger loan applicants are more likely to default on the loan. **"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c7e58ba25109a34dcc2719871380617a75fd3b78","collapsed":true},"cell_type":"code","source":"feature_list = full_features['Feature']\n\n# check for features with date\ndate_features=[]\nfor feature in feature_list:\n    if feature.find('DAY') >= 0:\n        date_features.append(feature)\n        \nprint(\"Features containing a date: \", date_features)\nprint('-'*20)\nprint(\"Sample data: \")\nprint( app_train['DAYS_BIRTH'].head() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d051dc9922126fcaa0d4b858aca25f79d434505","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"combined = [app_train, app_test]\n\nfor df in combined:\n    df['AGE'] = abs(df['DAYS_BIRTH']) / 365\n    df['YEARS_EMPLOYED'] = abs(df['DAYS_EMPLOYED']) / 365\n    df['YEARS_REGISTRATION'] = abs(df['DAYS_REGISTRATION']) / 365\n    df['YEARS_ID_PUBLISH'] = abs(app_train['DAYS_ID_PUBLISH']) / 365\n    \n    df.drop(columns=['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH'])\n    \napp_train = combined[0]\napp_test = combined[0]\n\ntarget0 = app_train[['TARGET', 'AGE', 'YEARS_EMPLOYED', 'YEARS_REGISTRATION', 'YEARS_ID_PUBLISH', 'YEARS_EMPLOYED']].groupby(['TARGET']).get_group(0)\ntarget1 = app_train[['TARGET', 'AGE', 'YEARS_EMPLOYED', 'YEARS_REGISTRATION', 'YEARS_ID_PUBLISH', 'YEARS_EMPLOYED']].groupby(['TARGET']).get_group(1)\n\n# plot histograms\nplt.figure()\nplot1 = target0['AGE'].plot(kind='kde', legend=True)\nplot1 = target1['AGE'].plot(kind='kde', legend=True)\nplot1.legend(('Succesful Repayments', 'Failed Repayments'))\nplot1.set_xlabel(\"Age\")\n\nplt.figure()\nplot2 = target0['YEARS_EMPLOYED'].plot(kind='kde', legend=True)\nplot2 = target1['YEARS_EMPLOYED'].plot(kind='kde', legend=True)\nplot2.legend(('Succesful Repayments', 'Failed Repayments'))\nplot2.set_xlabel(\"Years Employed\")\n\nplt.figure()\nplot3 = target0['YEARS_REGISTRATION'].plot(kind='kde', legend=True)\nplot3 = target1['YEARS_REGISTRATION'].plot(kind='kde', legend=True)\nplot3.legend(('Succesful Repayments', 'Failed Repayments'))\nplot3.set_xlabel(\"Years Registered\")\n\nplt.figure()\nplot4 = target0['YEARS_ID_PUBLISH'].plot(kind='kde', legend=True)\nplot4 = target1['YEARS_ID_PUBLISH'].plot(kind='kde', legend=True)\nplot4.legend(('Succesful Repayments', 'Failed Repayments'))\nplot4.set_xlabel(\"Years with Published ID\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e21c65f561e2a757f16fab38231c4a5753f80959"},"cell_type":"markdown","source":"To quantify the correlation between our features and the target, we will perform a chi-squared test. The chi-squared test uses a pivot table to calculate the probability that two variables are independent of each other. Since this test requires numeric entries, we will pivot the target against the boolean features in our dataset.\n\nThe p-value is the probability that the two variables are independent of each other. We want to trim our dataset of the variables which do not have a strong correlation with the target. By selecting for features which have a p-value of less than 0.25, we can drop 6 boolean features."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"173c90ba3e3d7136cb6d83189cd8b23b0d563f36","collapsed":true},"cell_type":"code","source":"from scipy.stats import chi2_contingency\n\ndef ChiSquared(df, feature_name, target_name):\n    contingency_table = []\n\n    feature_values = df[feature_name].unique()\n    target_values = df[target_name].unique()\n\n    for x in feature_values:\n        obs = df[[feature_name, target_name]].groupby([feature_name]).get_group(x)\n\n        # only proceed if there is more than one value\n        obs_list = obs['TARGET'].unique()\n        if ( len(obs_list) < 2 ):\n            chi2 = -1\n            p = -1\n            return chi2, p\n\n        segregated_obs = []\n        for y in target_values:\n            obs_list = obs.groupby([target_name]).get_group(y)\n            num_obs = len(obs_list)\n            segregated_obs.append(num_obs)\n\n        contingency_table.append(segregated_obs)\n\n    contingency_table = np.array(contingency_table)\n    contingency_table = np.transpose(contingency_table)\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    return chi2, p","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"13d12cba688b1b1d672d744e4c4eb655e0ea19e4","collapsed":true},"cell_type":"code","source":"full_feature_list = full_features['Feature'].tolist()\n\nbools = feature_data[['Feature', 'Type']].groupby(['Type']).get_group(0)\nbool_list = bools['Feature'].tolist()\n\nfull_bool_list = [x for x in bool_list if x in full_feature_list]\nfull_bool_list.remove('TARGET')\n\nchi2_table = []\nfor feature in full_bool_list:\n    chi2, p = ChiSquared(app_train, feature, 'TARGET')\n    chi2_table.append([feature, chi2, p])\n    \nchi2_arr = np.array(chi2_table)\n# filter non-valid entries\nfeatures = [x[0] for x in chi2_arr if x[2].astype(float)>0]\npvalues = [x[2].astype(float) for x in chi2_arr if x[2].astype(float)>0]\n\nselected_features = [x[0] for x in chi2_arr if (x[2].astype(float)>0) & (x[2].astype(float)<0.25)]\nprint(\"Number of full, boolean features: \", len(features) )\nprint(\"Selected features: \", len(selected_features) )\n\n\n# plot histogram\nfig,ax = plt.subplots(1)\nax.bar(features, pvalues)\nax.set_ylabel('P-value')\nax.set_xlabel('Features')\n# Turn off labels\nax.set_xticklabels([])\n\nplt.show()\n\nprint(\"Selected features: \", selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f5f27e3cdeabfce6cd5bcecb10c0e23341a47fa","collapsed":true},"cell_type":"markdown","source":"Now we will use the random forest classifier to predict the target of the 'test' dataset. Predictions are saved in a CSV file."},{"metadata":{"_uuid":"692d3e05d01aba49baff220c27d2cedb4e4aacac","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# initialize test and training datasets\nX_train = app_train[selected_features]\nY_train = app_train['TARGET']\nX_test = app_test[selected_features]\n\n# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint( \"random forest score:\\t\", acc_random_forest )\n\nsubmission = pd.DataFrame({\n    \"Applicant\": app_test['SK_ID_CURR'],\n    \"Success\": Y_pred\n    })\n    \nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}