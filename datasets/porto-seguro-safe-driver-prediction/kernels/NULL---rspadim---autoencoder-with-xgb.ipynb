{"cells":[{"metadata":{"_uuid":"e172d32f4703cd778258e0882c4e80743cd10849"},"cell_type":"markdown","source":"Feature encoding with xgboost https://github.com/xiaozhouwang/kaggle-porto-seguro/blob/master/code/fea_eng0.py"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport matplotlib.pyplot as plt\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b68a7e87a36d068c01f9a7d980e077ed1d9728c1"},"cell_type":"markdown","source":"Reading Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print('Reading datasets')\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint('Merging test and train')\ntest['target'] = np.nan\ntrain = train.append(test).reset_index() # merge train and test\ndel test\nprint('Done, shape=',np.shape(train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6e9096d8c77a6fab690eee73e69a3b0ff692ad1"},"cell_type":"markdown","source":"Rank Gauss transformation"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"29a7e53372569af14d1dfad6b49374e36010e4a2"},"cell_type":"code","source":"def rank_gauss(x, title=None):\n    #Trying to implement rankGauss in python, here are my steps\n    # 1) Get the index of the series\n    # 2) sort the series\n    # 3) standardize the series between -1 and 1\n    # 4) apply erfinv to the standardized series\n    # 5) create a new series using the index\n    # Am i missing something ??\n    # I subtract mean afterwards. And do not touch 1/0 (binary columns). \n    # The basic idea of this \"RankGauss\" was to apply rank trafo and them shape them like gaussians. \n    # Thats the basic idea. You can try your own variation of this.\n    \n    if(title!=None):\n        fig, axs = plt.subplots(3, 3)\n        fig.suptitle(title)\n        axs[0][0].hist(x)\n\n    from scipy.special import erfinv\n    N = x.shape[0]\n    temp = x.argsort()\n    if(title!=None):\n        print('1)', max(temp), min(temp))\n        axs[0][1].hist(temp)\n    rank_x = temp.argsort() / N\n    if(title!=None):\n        print('2)', max(rank_x), min(rank_x))\n        axs[0][2].hist(rank_x)\n    rank_x -= rank_x.mean()\n    if(title!=None): \n        print('3)', max(rank_x), min(rank_x))\n        axs[1][0].hist(rank_x)\n    rank_x *= 2\n    if(title!=None):\n        print('4)', max(rank_x), min(rank_x))\n        axs[1][1].hist(rank_x)\n    efi_x = erfinv(rank_x)\n    if(title!=None): \n        print('5)', max(efi_x), min(efi_x))\n        axs[1][2].hist(efi_x)\n    efi_x -= efi_x.mean()\n    if(title!=None):\n        print('6)', max(efi_x), min(efi_x))\n        axs[2][0].hist(efi_x)\n        plt.show()\n\n    return efi_x\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87b76a47c82f018d52a134abd9f96fc9cacc2dd"},"cell_type":"markdown","source":"Categorical to RankGauss, Binary to -1/1"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"709daa5d14be87e2d0de842f636edea2ef259754"},"cell_type":"code","source":"for i in train.columns:\n    if i.endswith('cat'): # could be train[i].dtype == 'object' + labelencode, or maybe one hot encode...\n        print('Categorical: ',i)\n        train[i] = rank_gauss(train[i].values, i) # display rank gauss tranformation\n    elif i.endswith('bin'):\n        print('Binary: ',i)\n    else:\n        print('Numeric: ',i)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ff8ef15e35a609eec68d99eb1a21ab9cdcf2de1"},"cell_type":"markdown","source":"DAE Generator for xgb"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1958ad44e654265864783cd990a3f690bccb0a78"},"cell_type":"code","source":"# TODO, use incremental learning with XGB and execute denoising autoencoder\n\nfrom math import ceil\nclass DAESequence:\n    def __init__(self, df, batch_size=128, random_cols=.15, random_rows=1, use_cache=False, use_lock=False, verbose=True):\n        self.df = df.values.copy()     # ndarray baby\n        self.batch_size = int(batch_size)\n        self.len_data = df.shape[0]\n        self.len_input_columns = df.shape[1]\n        if(random_cols <= 0):\n            self.random_cols = 0\n        elif(random_cols >= 1):\n            self.random_cols = self.len_input_columns\n        else:\n            self.random_cols = int(random_cols*self.len_input_columns)\n        if(self.random_cols > self.len_input_columns):\n            self.random_cols = self.len_input_columns\n        self.random_rows = random_rows\n        self.cache = None\n        self.use_cache = use_cache\n        self.use_lock = use_lock\n        self.verbose = verbose\n        \n        self.lock = ReadWriteLock()\n        self.on_epoch_end()\n\n    def on_epoch_end(self):\n        if(not self.use_cache):\n            return\n        if(self.use_lock):\n            self.lock.acquire_write()\n        if(self.verbose):\n            print(\"Doing Cache\")\n        self.cache = {}\n        for i in range(0, self.__len__()):\n            self.cache[i] = self.__getitem__(i, True)\n        if(self.use_lock):\n            self.lock.release_write()\n        gc.collect()\n        if(self.verbose):\n            print(\"Done\")\n\n    def __len__(self):\n        return int(ceil(self.len_data / float(self.batch_size)))\n\n    def __getitem__(self, idx, doing_cache=False):\n        if(not doing_cache and self.cache is not None and not (self.random_cols <=0 or self.random_rows<=0)):\n            if(idx in self.cache.keys()):\n                if(self.use_lock):\n                    self.lock.acquire_read()\n                ret0, ret1 = self.cache[idx][0], self.cache[idx][1]\n                if(self.use_lock):\n                    self.lock.release_read()\n                if (not doing_cache and self.verbose):\n                    print('DAESequence Cache ', idx)\n                return ret0, ret1\n        idx_end = min(idx + self.batch_size, self.len_data)\n        cur_len = idx_end - idx\n        rows_to_sample = int(self.random_rows * cur_len)\n        input_x = self.df[idx: idx_end]\n        if (self.random_cols <= 0 or self.random_rows <= 0 or rows_to_sample<=0):\n            return input_x, input_x # not dae\n        # here start the magic\n        random_rows = np.random.randint(low=0, high=self.len_data-rows_to_sample, size=rows_to_sample)\n        random_rows[random_rows>idx] += cur_len # just to don't select twice the current rows\n        cols_to_shuffle = np.random.randint(low=0, high=self.len_input_columns, size=self.random_cols)\n        noise_x = input_x.copy()\n        noise_x[0:rows_to_sample, cols_to_shuffle] = self.df[random_rows[:,None], cols_to_shuffle]\n        if(not doing_cache and self.verbose):\n            print('DAESequence ', idx)\n        return noise_x, input_x\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b58c818032fde555aaf1c8f93c2c21bb57618f48"},"cell_type":"markdown","source":"Creating Model and Fitting"},{"metadata":{"trusted":true,"_uuid":"5694b5b1c11488c53529d8b1556bd1066d346532"},"cell_type":"code","source":"print(\"Create Model\")\ndae_data = train[train.columns.drop(['id','target'])] # only get \"X\" vector\n\n# reduce data size, we are in kaggle =)\ndae_data = dae_data[0:1000]\ndae_data = dae_data.drop('index', axis=1)\nlen_input_columns, len_data = dae_data.shape[1], dae_data.shape[0]\n\neta = 0.1\nmax_depth = 6\nsubsample = 0.9\ncolsample_bytree = 0.85\nmin_child_weight = 55\nnum_boost_round = 500\n\nparams = {\"objective\": \"reg:linear\",\n          \"booster\": \"gbtree\",\n          \"eta\": eta,\n          \"max_depth\": int(max_depth),\n          \"subsample\": subsample,\n          \"colsample_bytree\": colsample_bytree,\n          \"min_child_weight\": min_child_weight,\n          \"silent\": 1\n          }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4e7f7c5768759b6dff301ea239fe4a235583d0b"},"cell_type":"markdown","source":"Fitting model with data"},{"metadata":{"trusted":true,"_uuid":"5286f08ee919696c00f529398cb8bc7f0c0bd6c0","scrolled":true},"cell_type":"code","source":"#FIRST IDEA, AE with stacking use the output (feature_Results) as input to next model\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\ntrain_rows = dae_data.shape[0]\nfeature_results = []\nglobal_min_length, global_max_length = None, None\nfor target_g in dae_data.columns.tolist():\n    features = dae_data.columns.drop(target_g).tolist()\n    target_list = [target_g]\n    train_fea = np.array(dae_data[features])\n    for target in target_list:\n        train_label = dae_data[target]\n        kfold = KFold(n_splits=5, random_state=218, shuffle=True)\n        kf = kfold.split(dae_data)\n        cv_train = np.zeros(shape=(dae_data.shape[0], 1))\n        min_length, max_length = None, None\n        for i, (train_fold, validate) in enumerate(kf):\n            X_train, X_validate, label_train, label_validate = train_fea[train_fold, :], train_fea[validate, :], train_label[train_fold], train_label[validate]\n            dtrain = xgb.DMatrix(X_train, label_train)\n            dvalid = xgb.DMatrix(X_validate, label_validate)\n            watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n            bst = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=50, verbose_eval=False)\n            min_length, max_length = min_length if min_length is not None and min_length<bst.best_ntree_limit else bst.best_ntree_limit, \\\n                                     max_length if max_length is not None and max_length>bst.best_ntree_limit else bst.best_ntree_limit\n            cv_train[validate, 0] += bst.predict(xgb.DMatrix(X_validate), ntree_limit=bst.best_ntree_limit)\n        global_min_length, global_max_length = \\\n            global_min_length if global_min_length is not None and global_min_length<min_length else min_length, \\\n            global_max_length if global_max_length is not None and global_max_length>max_length else max_length\n        print(target, 'mse: ', mean_squared_error(train_label, cv_train), ' min/max best_ntree_limit: ',min_length, max_length)\n        feature_results.append(cv_train)\n\nfeature_results = np.hstack(feature_results)\nprint(\"AE MSE from train data: \", mean_squared_error(dae_data, feature_results),\n     'min/max best_ntree_limit: ',global_min_length, global_max_length)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7d580ef207f9bc38108c4eb3549c3f567332430","scrolled":false},"cell_type":"code","source":"#SECOND IDEA, AE with CV to get highest tree size, and train a regressor with this size \n# and predict 3 'layers', each one with one size (trees/4 * 1,2,3...)\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nprint(\"CV Part\")\ntree_sizes={}\nfor target_g in dae_data.columns.tolist():\n    features = dae_data.columns.drop(target_g).tolist()\n    dtrain = xgb.DMatrix(dae_data[features].values, dae_data[target_g].values)\n    bst = xgb.cv(params, dtrain, num_boost_round, early_stopping_rounds=50, verbose_eval=False, nfold=5)\n    tree_sizes[target_g] = bst.shape[0]\n    print(target_g, tree_sizes[target_g])\nprint(tree_sizes, min(tree_sizes.values()), max(tree_sizes.values()))\nmax_trees = max(tree_sizes.values())\nprint(\"Tree size: \",max_trees)\n# we could get max size, fit models (overfit obvious), half size and 1/3 size\nfeature_results2 = []\nfeatures = []\nprint(\"CREATE FEATURE PART \", len(dae_data.columns.tolist()))\nfor target_g in dae_data.columns.tolist():\n    features = dae_data.columns.drop(target_g).tolist()\n    dtrain = xgb.DMatrix(dae_data[features].values, dae_data[target_g].values)\n    size = tree_sizes[target_g] # max_trees\n    bst = xgb.train(params, dtrain, num_boost_round=size)\n    pred = bst.predict(dtrain)\n    feature_results2.append(pred)\n    print('creating features: ',target_g, size, ' error:', mean_squared_error(dae_data[target_g], pred))\n    if(int(size/4)<=0):\n        features.append(bst.predict(dtrain, ntree_limit =size) )\n    else:\n        features.append(bst.predict(dtrain, ntree_limit =int(size/4)*1) )\n        features.append(bst.predict(dtrain, ntree_limit =int(size/4)*2) )\n        features.append(bst.predict(dtrain, ntree_limit =int(size/4)*3) )\nprint(len(feature_results2))\nfeature_results2 = np.hstack(feature_results2).reshape(-1,dae_data.shape[1])\nfeatures = np.hstack(features)\n\n\n# this error is a bit high, maybe we should do max_tree per feature? or stack too?\nprint(\"AE MSE from train data: \", mean_squared_error(dae_data, feature_results2))\n\nprint(\"Features = \",features.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7e87e87651d6e4eab8e9a0f182a3cd58fba4522"},"cell_type":"code","source":"plt.hist(dae_data)\nplt.show()\nplt.hist(feature_results, bins=100)\nplt.show()\nplt.hist(feature_results2, bins=100)\nplt.show()\nplt.hist(features)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82e496c3152a1d1052442d8cafda32e7a47bb81b"},"cell_type":"markdown","source":"# TODO IDEAS:\n\n1) DAE with tree (change input each 'epoch')\n\n2) feature extraction with stacking (get features with different n_tree at predict using stacking for loop), this give a better AE RMSE maybe a better feature representation too. must test"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd01c7e4a03209ab20e6dbea8dd410daaa59a81c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}