{"cells": [{"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "from scipy import stats\n", "import os, sys\n", "pd.options.mode.chained_assignment = None # subpress some warnning\n", "\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["from IPython.display import HTML\n", "\n", "HTML('''<script>\n", "code_show=true; \n", "function code_toggle() {\n", " if (code_show){\n", " $('div.input').hide();\n", " } else {\n", " $('div.input').show();\n", " }\n", " code_show = !code_show\n", "} \n", "$( document ).ready(code_toggle);\n", "</script>\n", "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"]}, {"metadata": {}, "cell_type": "markdown", "source": ["###  1) Load Data"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["full_data = pd.read_csv('../input/UCI_Credit_Card.csv')\n", "full_data.rename(columns={'default.payment.next.month': 'dpnm'}, inplace=True)\n", "print('data_origin.shape: ', full_data.shape)\n", "print('data_target_value_counts:')\n", "print(full_data['dpnm'].value_counts())\n", "full_data.head()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["####  a) running next code\uff0cif not sampling"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["data_origin = full_data"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### b) running next code, if sampling"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["from imblearn.over_sampling import SMOTE"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["X_sampled, Y_sampled = SMOTE().fit_sample(full_data.drop('dpnm', axis=1), full_data.dpnm)\n", "XY_sampled = np.append(X_sampled, Y_sampled.reshape(Y_sampled.shape[0], 1), axis=1)\n", "\n", "cata_variables = ['ID', 'AGE', 'EDUCATION', 'SEX', 'MARRIAGE', 'PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'dpnm']\n", "XY_sampled_m = pd.DataFrame(XY_sampled, columns=full_data.columns)\n", "XY_sampled_m[cata_variables] = XY_sampled_m[cata_variables].astype(int)\n", "data_origin = XY_sampled_m\n", "\n", "print('data_dimension after sampling: ')\n", "print(data_origin.shape)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["###  2) Data Preproessing"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["from termcolor import colored\n", "import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pylab as plt\n", "from sklearn.feature_selection import RFE\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.preprocessing import MinMaxScaler\n"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["print(colored('DATA_FEATURES: ', 'yellow'))\n", "pd.DataFrame(data_origin.columns).T"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### 1) Describe and reform category and quantitative data respectively:"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["def describe_factor(x_):\n", "    \"\"\"\n", "    describe data features, trying to find nan\n", "    :param x: dataframe\n", "    return: level conclusion\n", "    \"\"\"\n", "    level_count = dict()\n", "    \n", "    for lvl in x_.unique():\n", "        if pd.isnull(lvl):\n", "            level_count[\"NaN\"] = x_.isnull().sum()\n", "        else:\n", "            level_count[lvl] = np.sum(x_==lvl)\n", "    return level_count\n", "\n", "print('Describe and reform category data:')\n", "print('\\n')\n", "print(colored('Sex:', 'red'))\n", "print(describe_factor(data_origin['SEX']))\n", "print(colored('Education: ', 'red'))\n", "print(describe_factor(data_origin[\"EDUCATION\"]))\n", "data_origin[\"EDUCATION\"] = data_origin[\"EDUCATION\"].map({0: np.NaN, 1:1, 2:2, 3:3, 4:np.NaN, \n", "    5: np.NaN, 6: np.NaN})\n", "print(colored('For Education, (0, 5, 6) should be setted to be NA for further analysis, then;', 'yellow'))\n", "print(describe_factor(data_origin[\"EDUCATION\"]))\n", "print(colored('Marriage:', 'red'))\n", "print(describe_factor(data_origin['MARRIAGE']))\n", "data_origin.MARRIAGE = data_origin.MARRIAGE.map({0:np.NaN, 1:1, 2:2, 3:3})\n", "print(colored('For Marriage, (0) should be setted to be NA for further analysis, then;', 'yellow'))\n", "print(describe_factor(data_origin.MARRIAGE))"]}, {"execution_count": null, "outputs": [], "metadata": {"scrolled": false}, "cell_type": "code", "source": ["print(\"Others are quantitative\")\n", "print('\\n')\n", "print('#'*8, ' CHECK NULL:  ', '#'*8)\n", "print(data_origin.isnull().sum())"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### 2\uff09 Imputation\n", "##### As you can see, we have replaced several levels with 'NAN' before, and the number of such 'NAN' is not really big, therefore we simply impute it using Mode imputation."]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["data_origin[\"EDUCATION\"][data_origin[\"EDUCATION\"].isnull()] = data_origin[\"EDUCATION\"].mode().values\n", "data_origin[\"MARRIAGE\"][data_origin[\"MARRIAGE\"].isnull()] = data_origin[\"MARRIAGE\"].mode().values\n", "\n", "print('After imputation, check null: ')\n", "print('the number of \"NAN\": ', data_origin.isnull().sum().sum())"]}, {"metadata": {}, "cell_type": "markdown", "source": ["\n", "#### 3) Plot correlation between features\n", "##### From the plot below, we can notice that only 'PAY_0' has significant influence on our traget\u3002 Also, bill_amout seems not influential at all"]}, {"execution_count": null, "outputs": [], "metadata": {"scrolled": false}, "cell_type": "code", "source": ["corr = data_origin.drop(['ID'], axis=1).corr()\n", "\n", "# Generate a mask for the upper triangle\n", "mask = np.zeros_like(corr, dtype=np.bool)\n", "mask[np.triu_indices_from(mask)] = True\n", "\n", "# Set up the matplotlib figure\n", "f, ax = plt.subplots(figsize=(11, 9))\n", "\n", "# Generate a custom diverging colormap\n", "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n", "\n", "# Draw the heatmap with the mask and correct aspect ratio\n", "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n", "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n", "plt.show()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### 4) Using RFE( Recursive features elemination) to select  influential features\n", "##### After applying RFE, only one feature has popped up, PAY_0, which is consistent with the result we have obtained from correlation plot"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["lr_raw_model = LogisticRegression()\n", "mmscale = MinMaxScaler()\n", "X_scaled = mmscale.fit_transform(full_data.iloc[:,1:-1])\n", "rfe_lr = RFE(lr_raw_model, 1)\n", "fit = rfe_lr.fit(X_scaled, full_data.dpnm)\n", "print(\"Num Features:\",fit.n_features_)\n", "print(\"Selected Features:\",fit.support_)\n", "print(\"Feature Ranking: \",fit.ranking_)\n", "print('\\n')"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### 5) Discretize quantitative features using two methods: \n", "##### a) Decision tree;\n", "##### b) K-means.\n", "##### Before applying these two methods, we can think about for a minute. Decision tree classify samples based on there taget value, cause using GINI or ENTROPY, whereas K-means only cares about distances between samples without any reference to their target value.\n", "\n", "#### On the other hand, discretization may help us avoid outliers and unit-difference."]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### a) firstly, trying Decision_tree"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["from sklearn.model_selection import GridSearchCV\n", "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_kg_hide-input": true}, "cell_type": "code", "source": ["import numpy as np\n", "import math\n", "from scipy import stats\n", "from sklearn.utils.multiclass import type_of_target\n", "\n", "class WOE:\n", "    def __init__(self):\n", "        self._WOE_MIN = -20\n", "        self._WOE_MAX = 20\n", "\n", "    def woe(self, X, y, event=1):\n", "        '''\n", "        Calculate woe of each feature category and information value\n", "        :param X: 2-D numpy array explanatory features which should be discreted already\n", "        :param y: 1-D numpy array target variable which should be binary\n", "        :param event: value of binary stands for the event to predict\n", "        :return: numpy array of woe dictionaries, each dictionary contains woe values for categories of each feature\n", "                 numpy array of information value of each feature\n", "        '''\n", "        self.check_target_binary(y)\n", "\n", "        res_woe = []\n", "        res_iv = []\n", "        for i in range(0, X.shape[-1]):\n", "            x = X[:, i]\n", "            woe_dict, iv1 = self.woe_single_x(x, y, event)\n", "            res_woe.append(woe_dict)\n", "            res_iv.append(iv1)\n", "        return np.array(res_woe), np.array(res_iv)\n", "\n", "    def woe_single_x(self, x, y, event=1):\n", "        '''\n", "        calculate woe and information for a single feature\n", "        :param x: 1-D numpy starnds for single feature\n", "        :param y: 1-D numpy array target variable\n", "        :param event: value of binary stands for the event to predict\n", "        :return: dictionary contains woe values for categories of this feature\n", "                 information value of this feature\n", "        '''\n", "        self.check_target_binary(y)\n", "\n", "        event_total, non_event_total = self.count_binary(y, event=event)\n", "        x_labels = np.unique(x)\n", "        woe_dict = {}\n", "        iv = 0\n", "        for x1 in x_labels:\n", "            y1 = y[np.where(x == x1)[0]]\n", "            event_count, non_event_count = self.count_binary(y1, event=event)\n", "            rate_event = 1.0 * (event_count + 1.0) / (event_total + 2.0)\n", "            rate_non_event = 1.0 * (non_event_count + 1.0) / (non_event_total + 2.0)\n", "            woe1 = math.log(rate_event / rate_non_event)\n", "            woe_dict[x1] = woe1\n", "            iv += (rate_event - rate_non_event) * woe1\n", "        return woe_dict, iv\n", "\n", "    def woe_replace(self, X, woe_arr):\n", "        '''\n", "        replace the explanatory feature categories with its woe value\n", "        :param X: 2-D numpy array explanatory features which should be discreted already\n", "        :param woe_arr: numpy array of woe dictionaries, each dictionary contains woe values for categories of each feature\n", "        :return: the new numpy array in which woe values filled\n", "        '''\n", "        if X.shape[-1] != woe_arr.shape[-1]:\n", "            raise ValueError('WOE dict array length must be equal with features length')\n", "\n", "        res = np.copy(X).astype(float)\n", "        idx = 0\n", "        for woe_dict in woe_arr:\n", "            for k in woe_dict.keys():\n", "                woe = woe_dict[k]\n", "                res[:, idx][np.where(res[:, idx] == k)[0]] = woe * 1.0\n", "            idx += 1\n", "\n", "        return res\n", "\n", "    def combined_iv(self, X, y, masks, event=1):\n", "        '''\n", "        calcute the information vlaue of combination features\n", "        :param X: 2-D numpy array explanatory features which should be discreted already\n", "        :param y: 1-D numpy array target variable\n", "        :param masks: 1-D numpy array of masks stands for which features are included in combination,\n", "                      e.g. np.array([0,0,1,1,1,0,0,0,0,0,1]), the length should be same as features length\n", "        :param event: value of binary stands for the event to predict\n", "        :return: woe dictionary and information value of combined features\n", "        '''\n", "        if masks.shape[-1] != X.shape[-1]:\n", "            raise ValueError('Masks array length must be equal with features length')\n", "\n", "        x = X[:, np.where(masks == 1)[0]]\n", "        tmp = []\n", "        for i in range(x.shape[0]):\n", "            tmp.append(self.combine(x[i, :]))\n", "\n", "        dumy = np.array(tmp)\n", "        # dumy_labels = np.unique(dumy)\n", "        woe, iv = self.woe_single_x(dumy, y, event)\n", "        return woe, iv\n", "\n", "    def combine(self, list):\n", "        res = ''\n", "        for item in list:\n", "            res += str(item)\n", "        return res\n", "\n", "    def count_binary(self, a, event=1):\n", "        event_count = (a == event).sum()\n", "        non_event_count = a.shape[-1] - event_count\n", "        return event_count, non_event_count\n", "\n", "    def check_target_binary(self, y):\n", "        '''\n", "        check if the target variable is binary, raise error if not.\n", "        :param y:\n", "        :return:\n", "        '''\n", "        y_type = type_of_target(y)\n", "        if y_type not in ['binary']:\n", "            raise ValueError('Label type must be binary')\n", "\n", "    def feature_discretion(self, X):\n", "        '''\n", "        Discrete the continuous features of input data X, and keep other features unchanged.\n", "        :param X : numpy array\n", "        :return: the numpy array in which all continuous features are discreted\n", "        '''\n", "        temp = []\n", "        for i in range(0, X.shape[-1]):\n", "            x = X[:, i]\n", "            x_type = type_of_target(x)\n", "            if x_type == 'continuous':\n", "                x1 = self.discrete(x)\n", "                temp.append(x1)\n", "            else:\n", "                temp.append(x)\n", "        return np.array(temp).T\n", "\n", "    def discrete(self, x):\n", "        '''\n", "        Discrete the input 1-D numpy array using 5 equal percentiles\n", "        :param x: 1-D numpy array\n", "        :return: discreted 1-D numpy array\n", "        '''\n", "        res = np.array([0] * x.shape[-1], dtype=int)\n", "        for i in range(5):\n", "            point1 = stats.scoreatpercentile(x, i * 20)\n", "            point2 = stats.scoreatpercentile(x, (i + 1) * 20)\n", "            x1 = x[np.where((x >= point1) & (x <= point2))]\n", "            mask = np.in1d(x, x1)\n", "            res[mask] = (i + 1)\n", "        return res\n", "\n", "    @property\n", "    def WOE_MIN(self):\n", "        return self._WOE_MIN\n", "    @WOE_MIN.setter\n", "    def WOE_MIN(self, woe_min):\n", "        self._WOE_MIN = woe_min\n", "    @property\n", "    def WOE_MAX(self):\n", "        return self._WOE_MAX\n", "    @WOE_MAX.setter\n", "    def WOE_MAX(self, woe_max):\n", "        self._WOE_MAX = woe_max"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# using tree to discretize continous variables 'LIMIT_BAL' as an example.\n", "\n", "reg_labels = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n", "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n", "data_cut_dt = data_origin.copy()\n", "y = data_cut_dt['dpnm']  \n", "dt_model_ori = DecisionTreeClassifier(random_state=9, criterion='gini')\n", "params_to_try = {'max_leaf_nodes': [5, 10, 15]}\n", "\n", "grid_dt = GridSearchCV(dt_model_ori, params_to_try, n_jobs=-1, verbose=0)\n", "grid_dt.fit(pd.DataFrame(data_origin['LIMIT_BAL']),y)\n", "grid_dt.best_estimator_"]}, {"metadata": {}, "cell_type": "markdown", "source": ["##### After trying different values of \"max_lead_nodes\", the tree always chooses the largest one. However, it is understandable that the gini coefficient becomes smaller when tree grows.\n", "##### So we try different values of 'max_lead_nodes' for each quantative feature, store their resulted IV and select the value where the diff (or derivatives) of IV dose not change much."]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# using decision tree to discretize the continuous variables.\n", "\n", "woe_c = WOE() # function from tools\n", "\n", "\n", "max_iv = []   \n", "group_numbers_to_try = [4, 5, 8, 9, 10, 12, 15]\n", "\n", "print(colored('the resulted rolling differences or deriavtes of each IV for each label are: ', 'yellow'))\n", "\n", "for label in reg_labels:\n", "    print('\\n')\n", "    print(colored('label: {}'.format(label), 'red'))\n", "    \n", "    iv_i = []\n", "    X_to_use = pd.DataFrame(data_cut_dt[label])\n", "    \n", "    for g_n in group_numbers_to_try:\n", "        dt_model = DecisionTreeClassifier(random_state=9, max_leaf_nodes=g_n, criterion='gini')\n", "        dt_model.fit(X_to_use, y)\n", "        X_pred_group_number = dt_model.apply(X_to_use, check_input=True)\n", "        iv_i.append(woe_c.woe_single_x(x=np.array(X_pred_group_number), y=y)[1]) #calculate iv for each feature and each tried value\n", "        \n", "    max_iv_i = np.argmax(iv_i) # the position of max iv for each feature\n", "    max_iv.append(max_iv_i)\n", "    print(np.diff(iv_i) / np.diff(group_numbers_to_try))"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### from above, we choose leaf_nodes for 14 quantitative variables as  [5, 4, 10, 12, 15, 10, 10, 12, 8, 8, 12, 10, 8, 12]"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# after selecting the best cut_lines mannually.\n", "\n", "cut_bins = [5, 4, 10, 12, 15, 10, 10, 12, 8, 8, 12, 10, 8, 12]\n", "\n", "for i, label in enumerate(reg_labels):\n", "    X_to_use = pd.DataFrame(data_cut_dt[label])\n", "    \n", "    dt_model = DecisionTreeClassifier(random_state=9, max_leaf_nodes=cut_bins[i], criterion='gini')\n", "    dt_model.fit(X_to_use, y)\n", "    X_pred_group_number = dt_model.apply(X_to_use, check_input=True)\n", "    X_pred_group_number_uniq = np.unique(X_pred_group_number)\n", "    \n", "    for j in range(len(X_pred_group_number_uniq)):\n", "        data_cut_dt[label][X_pred_group_number == X_pred_group_number_uniq[j]] = j # convert label into 0,1,2,3,..."]}, {"execution_count": null, "outputs": [], "metadata": {"scrolled": true}, "cell_type": "code", "source": ["data_cut_dt.head()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["##### then, we can calculate the IV for each feature after discretization"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["woes, iv = woe_c.woe(X=np.array(data_cut_dt.iloc[:,1:-1]), y=y)\n", "\n", "print('IV for each feature after discretization: ')\n", "pd.DataFrame(iv, index=data_cut_dt.iloc[:,1:-1].columns)"]}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": ["#### b) Secondly, we try K-means for dividing"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["from sklearn.cluster import KMeans"]}, {"metadata": {}, "cell_type": "markdown", "source": ["##### here, we simply choose the same cutting group for each quantitative feature, with cut_bins = [5, 4, 10, 12, 15, 10, 10, 12, 8, 8, 12, 10, 8, 12]\n", "##### the resulted cutting points:"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["def kmeans_cut(data_origin, labels, k):\n", "    \"\"\"\n", "    Using k-means to discretize continuous features\n", "    :param data_origin: original data_set\n", "    :param labels: column names which need discretization\n", "    :param k: the number of groups to be divided\n", "    :return: centers, cutting_points and transformed data\n", "    \"\"\"\n", "    data = data_origin.copy()\n", "\n", "    for label in labels:\n", "        k_model = KMeans(n_clusters=k)\n", "        k_model.fit(pd.DataFrame(data[label]))\n", "        cut_centers = pd.DataFrame(k_model.cluster_centers_).sort_values(0)\n", "        cutting_points = cut_centers.rolling(2).mean().iloc[1:]  # \u76f8\u90bb\u4e24\u9879\u6c42\u91cd\u70b9\uff0c\u4f5c\u4e3a\u8fb9\u754c\u70b9\n", "\n", "        # get minimum point of cutting points\n", "        if data[label].min() > 0:\n", "            cutting_points = [0] + list(cutting_points[0]) + [data[label].max() + 1]\n", "        else:\n", "            cutting_points = [data[label].min()] + list(cutting_points[0]) + [data[label].max() + 10]\n", "\n", "        data_cut_group_label = pd.cut(data[label], bins=cutting_points, labels=range(k), right=False)\n", "        data[label] = data_cut_group_label\n", "    return cut_centers, cutting_points, data"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# trying k-means distreriztion , see any difference\n", "\n", "data_cut_kms = data_origin.copy()\n", "\n", "print(colored('the resulted rolling differences or deriavtes of each IV for each label are: ', 'yellow'))\n", "\n", "for i, label in enumerate(reg_labels):\n", "    c =[]\n", "    c.append(label) # convert label to be a list\n", "    \n", "    centers, cutting_points, resulted_data = kmeans_cut(data_cut_kms, c, k=cut_bins[i])\n", "    data_cut_kms = resulted_data.copy()\n", "    \n", "    print('\\n')\n", "    print('#'*10, label,':')\n", "    print(cutting_points)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["data_cut_kms.head()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["##### Again, we can calculate the IV for each feature after discretization"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["woes, iv = woe_c.woe(X=np.array(data_cut_kms.iloc[:,1:-1]), y=y)\n", "\n", "print('IV for each feature after discretization: ')\n", "pd.DataFrame(iv, index=data_cut_kms.iloc[:,1:-1].columns)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["### 3)  Now, continue to modeling"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### before modeling, you need to determine which discretization method to use and therefore the data used in model"]}, {"metadata": {}, "cell_type": "markdown", "source": ["##### if using decisition tree, use following data"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["data_cut_dt.drop(['ID','SEX', 'MARRIAGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6'],\n", "                 axis=1, inplace=True)\n", "\n", "# delete SEX, MARRIAGE, BILL_AMOUNT\n", "data_cut_X = data_cut_dt.iloc[:,:-1]\n", "data_cut_Y = data_cut_dt.iloc[:,-1]"]}, {"metadata": {}, "cell_type": "markdown", "source": ["##### if using k-means, use following data"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["data_cut_kms.drop(['ID','AGE', 'SEX', 'MARRIAGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6'], \n", "                  axis=1, inplace=True)\n", "\n", "# delete SEX, MARRIAGE, BILL_AMOUNT\n", "data_cut_X = data_cut_kms.iloc[:,:-1]\n", "data_cut_Y = data_cut_kms.iloc[:,-1]"]}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": ["#### 1) after choosing data, we need to split choosed data into training and testing data."]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["from sklearn.model_selection import StratifiedShuffleSplit"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["def split_data(size, data_x, data_y):\n", "    \"\"\"\n", "    split full data into training and testing data\n", "    :param size: percentile for testing data\n", "    :param data_x: dataframe of full data\n", "    :param data_y: target\n", "    \"\"\"\n", "    sss = StratifiedShuffleSplit(n_splits=2, test_size=size, random_state=9)\n", "    split1, split2 = sss.split(data_x, data_y)\n", "    x_train, x_test = data_x.iloc[split1[0]], data_x.iloc[split1[1]]\n", "    Y_train, Y_test = data_y[split1[0]], data_y[split1[1]]\n", "    return x_train, x_test, Y_train, Y_test"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# eliminate negative integers\n", "\n", "pay_status = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n", "\n", "for pay in pay_status:\n", "    pay_value = data_cut_X[pay]\n", "    data_cut_X[pay][pay_value == -1] = 9\n", "    data_cut_X[pay][pay_value == -2] = 10\n", "\n", "# now split\n", "X_train, X_test, y_train, y_test = split_data(0.2, data_cut_X, data_cut_Y) # use 20% percentile testing data here"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["print(colored('FULL_X_DATA: ', 'green'))\n", "print(data_cut_X.shape)\n", "print(colored('X_TRAIN.shape: ', 'green'))\n", "print(X_train.shape)\n", "print(colored('X_TEST.shape: ', 'green'))\n", "print(X_test.shape)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### 2)  applying one-hot encoder to all data_X"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["from sklearn.preprocessing import OneHotEncoder"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# one_hot encoder\n", "\n", "one_hot = OneHotEncoder(sparse=True)\n", "one_hot.fit(data_cut_X)\n", "X_train_oh = one_hot.transform(X_train)\n", "X_test_oh = one_hot.transform(X_test)\n", "\n", "#using dummy to get column names\n", "X_dummy = pd.get_dummies(X_train, columns=X_train.columns)\n", "names = X_dummy.columns\n", "X_train_oh = pd.DataFrame(X_train_oh.toarray(), columns=names, index=X_train.index)\n", "X_test_oh = pd.DataFrame(X_test_oh.toarray(), columns=names, index=X_test.index)"]}, {"execution_count": null, "outputs": [], "metadata": {"scrolled": false}, "cell_type": "code", "source": ["print(colored('After encodering: ', 'yellow'))\n", "print(colored('X_TRAIN.shape: ', 'green'))\n", "print(X_train_oh.shape)\n", "print(colored('X_TEST.shape: ', 'green'))\n", "print(X_test_oh.shape)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### 3) Let's try four models. the parameters of some models have not been optimised, but they work well."]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n", "from sklearn.svm import LinearSVC\n", "import matplotlib.pylab as plt\n", "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, accuracy_score, auc, confusion_matrix, f1_score"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# classifier to be tried.\n", "\n", "clfs = {'LogisticRegression':LogisticRegressionCV(Cs=10, scoring='recall', penalty='l1', solver='liblinear'),\n", "       'RandomForest': RandomForestClassifier(n_estimators=100),\n", "       'GradientBoosting': GradientBoostingClassifier(learning_rate= 0.05, max_depth= 6,\n", "                                                        n_estimators=200, max_features = 0.3,\n", "                                                        min_samples_leaf = 5)}"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### fit the four models, calculate the metrics in cols, plot a roc curve and create a feature_importance dataframe for tree_based models"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# original_oh = pd.read_csv(DATA_PATH + 'original_data_oh.csv').iloc[:,1:]\n", "\n", "cols = ['model', 'auc', 'precision_score', 'recall_score', 'f1_score', 'accuracy', 'train_score']\n", "models_report = pd.DataFrame(columns=cols)\n", "feature_importance = pd.DataFrame()\n", "conf_matrix = dict()\n", "\n", "for clf, clf_name in zip(clfs.values(), clfs.keys()):\n", "    # fit model\n", "    clf.fit(X_train_oh, y_train)\n", "    y_pred = clf.predict(X_test_oh)\n", "#     y_original_pred = clf.predict(original_oh)\n", "    y_score = pd.DataFrame(clf.predict_proba(X_test_oh)).iloc[:,1]\n", "#     y_original_score = pd.DataFrame(clf.predict_proba(original_oh)).iloc[:,1]\n", "\n", "    print('Computing{}'.format(clf_name))\n", "    \n", "    # add features importance\n", "    if (clf_name == 'RandomForest') | (clf_name == 'GradientBoosting'):\n", "        tmp_fi = pd.Series(clf.feature_importances_)\n", "        feature_importance[clf_name] = tmp_fi\n", "    \n", "    # calculate required metrics\n", "    tmp = pd.Series({\n", "                    'model': clf_name,\n", "                    'auc': roc_auc_score(y_test, y_score),\n", "                    'precision_score':precision_score(y_test, y_pred),\n", "                    'recall_score':recall_score(y_test, y_pred),\n", "                    'f1_score': f1_score(y_test, y_pred),\n", "                    'accuracy': accuracy_score(y_test, y_pred),\n", "                    'train_score': clf.score(X_train_oh, y_train),\n", "#                     'original_recall': recall_score(full_data.dpnm, y_original_pred),\n", "#                     'original_auc': roc_auc_score(full_data.dpnm, y_original_score),\n", "                    })\n", "    models_report = models_report.append(tmp, ignore_index = True)\n", "    conf_matrix[clf_name] = pd.crosstab(y_test, y_pred, rownames=['True'], colnames= ['Predicted'], margins=False)\n", "    \n", "    # plot roc\n", "    fpr, tpr, _ = roc_curve(y_test, y_score, drop_intermediate = False, pos_label = 1)\n", "    auc_value = auc(fpr, tpr)\n", "    \n", "    plt.figure(1, figsize = (9,9))\n", "    plt.xlabel('False Positive Rate')\n", "    plt.ylabel('True Positive Rate')\n", "    plt.title('Receiver operating characteristic example')\n", "    plt.plot(fpr, tpr, lw=2, label=clf_name + '(area = %0.2f)' % auc_value)\n", "    plt.legend(loc=\"lower right\")\n", "    \n", "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n", "plt.show()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["print('models_report: ')\n", "models_report"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### Feature importance"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["fi = feature_importance\n", "features = X_train_oh.columns\n", "fi.index = features\n", "fi = fi.head(15) # Only take the 15 most important metrics\n", "fi = fi.sort_values('RandomForest', ascending=False)\n", "fi = (fi / fi.sum(axis=0)) * 100\n", "fi.plot.barh(title = 'Feature importances for Tree algorithms', figsize = (6,9))\n", "plt.show()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["### 4) take logistic regression as example for further analysis "]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# fit logistic regression model\n", "\n", "lrcv = LogisticRegressionCV(Cs=10, class_weight='balanced', scoring='recall', penalty='l1', solver='liblinear')\n", "lrcv.fit(X_train_oh, y_train)\n", "lrcv.C_  # c selected\n", "\n", "print('scores for training data: ')\n", "print(lrcv.score(X_train_oh, y_train))\n", "print('scores for testing data: ')\n", "print(lrcv.score(X_test_oh, y_test))"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["def roc_curve_plot(classifier, data, y, event=1):\n", "    \"\"\"\n", "    plot rov curve and also calculate auc\n", "    :param classifier: defined and fitted classifier\n", "    :param data: data for calculating, dataframe\n", "    :param y: true target value for data\n", "    :return: auc value and plot\n", "    \"\"\"\n", "\n", "    y_pred = classifier.decision_function(data)\n", "\n", "    fpr, tpr, thresholds = roc_curve(y, y_pred, pos_label=event)\n", "    auc_value = auc(fpr, tpr)\n", "\n", "    # plot roc curve and caluculate auc\n", "    plt.figure(figsize=(10, 10))\n", "    lw = 2\n", "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % auc_value)\n", "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n", "    plt.xlim([0.0, 1.0])\n", "    plt.ylim([0.0, 1.05])\n", "    plt.xlabel('False Positive Rate')\n", "    plt.ylabel('True Positive Rate')\n", "    plt.title('Receiver operating characteristic example')\n", "    plt.legend(loc=\"lower right\")\n", "    plt.show()\n", "    return auc_value\n"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# metrics for model performance examnation\n", "auc_v = roc_curve_plot(lrcv, X_train_oh, y_train)"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["def ks_plot(y_true, y_prob):\n", "    \"\"\"\n", "    plot ks-curve and calculate ks value\n", "    :param y_true: true values of target y of samples\n", "    :param y_prob: predicted probability of been positive of samples\n", "    :return: ks curve plot ad ks value\n", "    \"\"\"\n", "    df = pd.concat((pd.DataFrame(np.array(y_prob[:, 0]), columns=['y_prob']),\n", "                    pd.DataFrame(np.array(y_true), columns=['y_true'])),\n", "                    axis=1)\n", "    df_sorted = df.sort_values(by='y_prob')  # sort by predicted probability of samples\n", "    y_prob_sorted = df_sorted.iloc[:, 0]\n", "    y_true_sorted = df_sorted.iloc[:, 1]\n", "    total_good_count = y_true.value_counts()[0]\n", "    total_bad_count = y_true.value_counts()[1]\n", "\n", "    cut_points = np.linspace(y_prob.min(), y_prob.max(), 31)  # thresholds\n", "    good_event_cdf = []\n", "    bad_event_cdf = []\n", "\n", "    # calculate cdf for good and bad event respectively.\n", "    for i, tr in enumerate(cut_points):\n", "        selected_data = y_true_sorted[y_prob_sorted <= tr]\n", "        good_event_count = sum(selected_data == 0)  # count good_event\n", "        bad_event_count = sum(selected_data == 1)  # count bad_event\n", "        good_event_cdf.append(good_event_count / total_good_count)\n", "        bad_event_cdf.append(bad_event_count / total_bad_count)\n", "\n", "    # calculate ks value\n", "    good_bad_diff = np.array(bad_event_cdf) - np.array(good_event_cdf)\n", "    ks_value = max(good_bad_diff)\n", "    ks_position = np.argmax(good_bad_diff).astype(int)\n", "\n", "    # plot curve\n", "    plt.figure(figsize=(10, 10))\n", "    lw = 2\n", "    plt.plot(cut_points, good_event_cdf, color='darkorange', lw=lw, label='good_event_cdf')\n", "    plt.plot(cut_points, bad_event_cdf, color='green', lw=lw, label='bad_event_cdf')\n", "    plt.plot([cut_points[ks_position], cut_points[ks_position]],\n", "             [good_event_cdf[ks_position], bad_event_cdf[ks_position]],\n", "             color='navy', lw=lw, linestyle='--')\n", "    plt.xlim([0.0, 1.3])\n", "    plt.ylim([0.0, 1.2])\n", "    plt.xlabel('thresholds')\n", "    plt.ylabel('cumulative rate')\n", "    plt.title('K-S plot')\n", "    plt.legend(loc=\"lower right\")\n", "    plt.show()\n", "\n", "    return ks_value\n"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# plot ks\n", "\n", "ks_plot(y_test, lrcv.predict_proba(X_test_oh))"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["print(colored('resultes of metrics for logistic regression: ', 'yellow'))\n", "print('accuracy_score: ')\n", "print(accuracy_score(y_test,lrcv.predict(X_test_oh)))\n", "print('recall_score: ')\n", "print(recall_score(y_test,lrcv.predict(X_test_oh), pos_label=1))  #  tp / (tp + fn)\n", "print('precision_score: ')\n", "print(precision_score(y_test,lrcv.predict(X_test_oh), pos_label=1))  #  tp / (tp + fp)\n", "print('f1_score: ')\n", "print(f1_score(y_test, lrcv.predict(X_test_oh)))\n", "print('confusion_matrix: ')\n", "confusion_matrix(y_test,lrcv.predict(X_test_oh))\n", "# tn, fp, fn, tp = confusion_matrix(y_test,lrcv.predict(X_test_oh)).ravel()\n", "# print(tn, fp, fn, tp)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# from sklearn.metrics import precision_recall_curve\n", "# from sklearn.model_selection import cross_val_predict\n", "\n", "# y_scores = pd.DataFrame(cross_val_predict(lrcv, X_train_oh, y_train, cv=3, method='decision_function'))\n", "# precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# from sklearn.metrics import precision_recall_curve\n", "# from model.tools import *\n", "# plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n", "# plt.show()"]}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": ["###  5) trying woe and iv methods"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### replace discretized data with woe and split data into training and testing"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# replace discretized data with woe\n", "\n", "woes, iv = woe_c.woe(X=np.array(data_cut_X), y=y)\n", "data_woe = pd.DataFrame(woe_c.woe_replace(X=np.array(data_cut_X), woe_arr=woes), index=data_cut_X.index, columns=data_cut_X.columns)\n", "\n", "# split data\n", "X_train, X_test, y_train, y_test = split_data(0.3, data_woe, data_cut_Y)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### fit logistic regression model"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["lrcv_woe = LogisticRegressionCV(Cs=10, class_weight='balanced', scoring='recall', penalty='l1', solver='liblinear')\n", "lrcv_woe.fit(X_train, y_train)\n", "lrcv_woe.C_  # C selected\n", "print('scores for training data: ')\n", "print(lrcv_woe.score(X_train, y_train))\n", "print('scores for testing data: ')\n", "print(lrcv_woe.score(X_test, y_test))"]}, {"metadata": {}, "cell_type": "markdown", "source": ["#### plot roc and calculate auc"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# plot ks\n", "\n", "auc_v = roc_curve_plot(lrcv_woe, X_test, y_test)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["ks_plot(y_test, lrcv_woe.predict_proba(X_test))"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["print(colored('resultes of metrics for WOE logistic regression: ', 'yellow'))\n", "print('WOE model')\n", "print('accuracy_score: ')\n", "print(accuracy_score(y_test,lrcv_woe.predict(X_test)))\n", "print('recall_score: ')\n", "print(recall_score(y_test,lrcv_woe.predict(X_test), pos_label=1))  #  tp / (tp + fn)\n", "print('precision_score: ')\n", "print(precision_score(y_test,lrcv_woe.predict(X_test), pos_label=1))  #  tp / (tp + fp)\n", "print('f1_score: ')\n", "print(f1_score(y_test, lrcv_woe.predict(X_test)))\n", "print('confusion_matrix: ')\n", "confusion_matrix(y_test,lrcv_woe.predict(X_test))"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": []}], "metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1, "nbformat": 4}