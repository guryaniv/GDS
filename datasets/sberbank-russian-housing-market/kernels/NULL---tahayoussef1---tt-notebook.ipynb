{"nbformat_minor": 0, "nbformat": 4, "cells": [{"source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\ndef corr_plot2(dataframe2, top_n2, target2, fig_x2, fig_y2):\n    corrmat2 = dataframe2.corr()\n    #top_n - top n correlations +1 since price is included\n    top_n2 = top_n2 + 1 \n    cols2 = corrmat2.nlargest(top_n2, target2)[target2].index\n    cm2 = np.corrcoef(donnees[cols2].values.T)\n    f2, ax2 = plt.subplots(figsize=(fig_x2,fig_y2))\n    sns.set(font_scale=1.25)\n    cmap2 = plt.cm.viridis\n    hm2 = sns.heatmap(cm2, cbar=False, annot=True, square=True,cmap = cmap2, fmt='.2f', annot_kws={'size': 10},\t yticklabels=cols2.values, xticklabels=cols2.values)\n    plt.show()\n    return cols2,cm2\ndef corr_plot(dataframe, top_n, target, fig_x, fig_y):\n    corrmat = dataframe.corr()\n    #top_n - top n correlations +1 since price is included\n    top_n = top_n + 1 \n    cols = corrmat.nlargest(top_n, target)[target].index\n    cm = np.corrcoef(donnees[cols].values.T)\n    \"\"\"f, ax = plt.subplots(figsize=(fig_x,fig_y))\n    sns.set(font_scale=1.25)\n    cmap = plt.cm.viridis\n    hm = sns.heatmap(cm, cbar=False, annot=True, square=True,cmap = cmap, fmt='.2f', annot_kws={'size': 10},\t yticklabels=cols.values, xticklabels=cols.values)\n    plt.show()\"\"\"\n    return cols,cm\n\n\ndonnees1 = pd.read_csv(\"train.csv\")\ndonnees2 = pd.read_csv(\"macro.csv\")\ndonnees = pd.merge(donnees1, donnees2, on='timestamp')\ndel donnees[\"timestamp\"]\n\n\n#missing data\ntotal = donnees.isnull().sum().sort_values(ascending=False) #calculer le total des valeurs manquantes\npercent = (donnees.isnull().sum()/donnees.isnull().count()).sort_values(ascending=False) #le pourcentage de valeur manquantes pour chaque variable \nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\ndonnees = donnees.drop((missing_data[missing_data['Total'] > 10000]).index,1)\n#print donnees.shape\ndonnees = donnees.dropna(thresh=donnees.shape[1])\nprint donnees.shape\ndtype_df = donnees.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()\ntab = []\nfor x in range(0,dtype_df.shape[0]):\n\tif(dtype_df[\"Column Type\"][x] == \"object\"):\n\t\ttab.append(dtype_df[\"Count\"][x])\n\t\t\nfor x in range(0,15):\n\tdonnees[tab[x]] = pd.factorize(donnees[tab[x]])[0]\ncorr_20,cm = corr_plot(donnees, 150, 'price_doc', 10,10)\n\ncorr_20 = corr_20[0:35]\nprint corr_20\ncorr_22 = corr_plot2(donnees, 10, 'price_doc', 10,10)\ndonnees = donnees[corr_20].copy()\ntest = pd.read_csv(\"test.csv\")\ntest = pd.merge(test, donnees2, on='timestamp')\ndel test[\"timestamp\"]\nprint test.shape\ntest = test[corr_20].copy()\n\n\n#missing data\ntotal = test.isnull().sum().sort_values(ascending=False) #calculer le total des valeurs manquantes\npercent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False) #le pourcentage de valeur manquantes pour chaque variable \nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\ntest  = test.fillna(test.mean())\ntotal = test.isnull().sum().sort_values(ascending=False) #calculer le total des valeurs manquantes\npercent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False) #le pourcentage de valeur manquantes pour chaque variable \nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n\ntotal = donnees.isnull().sum().sort_values(ascending=False) #calculer le total des valeurs manquantes\npercent = (donnees.isnull().sum()/donnees.isnull().count()).sort_values(ascending=False) #le pourcentage de valeur manquantes pour chaque variable \nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\ndonnees = donnees.dropna(thresh=donnees.shape[1])\ndtype_df = donnees.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()\ntab = []\nfor x in range(0,dtype_df.shape[0]):\n    if(dtype_df[\"Column Type\"][x] == \"object\"):\n        tab.append(dtype_df[\"Count\"][x])\n        #print dtype_df[\"Count\"][x]\n\nfor x in range(0,len(tab)):\n    donnees[tab[x]] = pd.factorize(donnees[tab[x]])[0]\n    test[tab[x]] = pd.factorize(test[tab[x]])[0]\n\nprice = donnees.price_doc\ntest = test.drop('price_doc', 1)\ndonnees = donnees.drop('price_doc', 1)\nprint(test.columns)\nprint('--------------------------------')\nprint(donnees.columns)\n#----------------------------------------\nmodeleReg1=LinearRegression()\nX_train, X_test, y_train, y_test = train_test_split(donnees, price, train_size=0.9)\nmodeleReg1.fit(X_train,y_train) #effectuer la regression lineaire\ny_predicted2 = modeleReg1.predict(X_test)\nR = modeleReg1.score(X_test,y_test)\nprint \"R = \",R\nms = sqrt(mean_squared_error(y_test, y_predicted2))\nprint(ms)\n\ny_predicted = modeleReg1.predict(test)\nid_test = range(30474,38136)\noutput = pd.DataFrame({'id': id_test, 'price_doc': y_predicted})\noutput.head()\noutput.to_csv('taha.csv', index=False)\n", "cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_uuid": "0f8fdf0450f6fb56cde5a52cb00d1fe04c65aeeb"}}], "metadata": {"language_info": {"name": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}}