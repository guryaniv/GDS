{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"013144a39e02ab90bb92aa6fcb926e3a7572c110"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bba79851e200261b134df97d57a930ba1836dd3"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', index_col='ID')\ntest = pd.read_csv('../input/test.csv', index_col='ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e7cfc37cc94daaef9516ed681e0791f44430c99"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebaec0e7c4054d7f4a48282f8d01af28364ebe51"},"cell_type":"markdown","source":"As we can see: our data is table of anonimus digits, we can not guess of this, so we can use only mathematics"},{"metadata":{"_uuid":"5c08e011cb49ca3d72529b3bff9a3046fde945b9"},"cell_type":"markdown","source":"## Target variable"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4725a0aa6777f0998b7c962bf7d93dbc115813fb"},"cell_type":"code","source":"sns.distplot(train.target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4f98b57703818a2aefaa8cf1353c4e18682d332"},"cell_type":"markdown","source":"This does not look like a normal distribution, so let's use the logarithm"},{"metadata":{"trusted":true,"_uuid":"5636b9e49f6e67cfbe86a1c0cecbc8a7e3b9344a"},"cell_type":"code","source":"sns.distplot(np.log1p(train.target))\ny = np.log1p(train.target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b58c77801bd06da77b3a8ce7f2fc187036842af0"},"cell_type":"markdown","source":"Function to count metrics"},{"metadata":{"trusted":true,"_uuid":"fbe87b967dc5b3ae5a1eccb67351998405f83608"},"cell_type":"code","source":"def rmsle(y, pred):\n    assert len(y) == len(pred)\n    return np.sqrt(np.mean(np.power(y-pred, 2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b50e0383c3e1bf850d0d577aea00749db9cbc253"},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"_uuid":"f0a13994e4b387a5aee39d02ab32eef2f1de9b97"},"cell_type":"markdown","source":"Drop columns with only 1 value"},{"metadata":{"trusted":true,"_uuid":"2c881b86781d2996676b44ea0a06c09e89bfffd5"},"cell_type":"code","source":"cols_with_onlyone_val = train.columns[train.nunique() == 1]\ntrain.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\ntest.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\ntrain = train.round(32)\ntest = test.round(32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81e56c9079651ae01e65a2b8bd3cdd5fc0fdbe6c"},"cell_type":"markdown","source":"Drop equal columns "},{"metadata":{"trusted":true,"_uuid":"6f8c9ecac47b2b0ac0fbeaee1fcbc03dcd69450b"},"cell_type":"code","source":"colsToRemove = []\ncolumns = train.columns\nfor i in range(len(columns)-1):\n    v = train[columns[i]].values\n    dupCols = []\n    for j in range(i + 1,len(columns)):\n        if np.array_equal(v, train[columns[j]].values):\n            colsToRemove.append(columns[j])\ntrain.drop(colsToRemove, axis=1, inplace=True)\ntest.drop(colsToRemove, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2db11023e4e2b008d09a5d1ef39283b036910255"},"cell_type":"code","source":"X = train.drop('target', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d7cb53c7dd3f7c7318c33243f453f06fb6e3943"},"cell_type":"code","source":"X['mean'] = X.mean(axis = 1)\nX['sum'] = X.sum(axis = 1)\nX['max'] = X.max(axis = 1)\nX['min'] = X.min(axis = 1)\nX['std'] = X.std(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06f64f60549de46d64241890023bfcd8f057e4b2"},"cell_type":"code","source":"test['mean'] = test.mean(axis = 1)\ntest['sum'] = test.sum(axis = 1)\ntest['max'] = test.max(axis = 1)\ntest['min'] = test.min(axis = 1)\ntest['std'] = test.std(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad9e9ff269a00ee318591f3f9276bc73329883db"},"cell_type":"markdown","source":"Some columns contain too few values. Delete them"},{"metadata":{"trusted":true,"_uuid":"c717bbb3e0b2eb76d48a289f2c4671c0b1cf9c92"},"cell_type":"code","source":"cols = X[X > 0].count()[X[X > 0].count() > 200].sort_values().index\nX_train = X[cols].copy()\nX_test = test[cols].copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7f1087ecd0350b379a7600d92901fdcc8fc0ca0"},"cell_type":"markdown","source":"Add clustering, as addition features"},{"metadata":{"trusted":true,"_uuid":"57c2d26e9d349db6ca0ee3e8c902d3903f2af343"},"cell_type":"code","source":"clust = KMeans(n_clusters=15).fit(X_train)\nX_with_clust = X_train.join(pd.DataFrame(clust.transform(X_train), columns= [ 'clust_' + str(i) for i in range(15)], index=X_train.index))\nX_with_clust['cluster_name'] = clust.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25b06bbf2cfb8b827c39f36249330316a168cf23"},"cell_type":"code","source":"test_with_clust = X_test.join(pd.DataFrame(clust.transform(X_test), columns= [ 'clust_' + str(i) for i in range(15)], index=X_test.index))\ntest_with_clust['cluster_name'] = clust.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c08b4feb99d7b7a4de1cec5cf53cbbe07128bdc"},"cell_type":"markdown","source":"Select features with Rendom Forest regressor"},{"metadata":{"trusted":true,"_uuid":"887f2b73d93303de09cc708f3a1a70403758e710"},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_with_clust, y)\nfeatures = pd.DataFrame({'importance': rf.feature_importances_, 'name': X_with_clust.columns})\nX_short = X_with_clust[features.sort_values('importance', ascending=False).name[:500]]\ntest_short = test_with_clust[features.sort_values('importance', ascending=False).name[:500]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32ed032bdb629a5d1796cca9e03061499c04512d","collapsed":true},"cell_type":"markdown","source":"## Search for lightGBM params"},{"metadata":{"trusted":true,"_uuid":"375f197c9ea11982c788aaaa5928fb7b0650589f"},"cell_type":"code","source":"import lightgbm\nimport hyperopt\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\nN_HYPEROPT_PROBES = 500\n\nHYPEROPT_ALGO = tpe.suggest  #  tpe.suggest OR hyperopt.rand.suggest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98a5bd69e776f65b74b26eaa6b5449919493f071"},"cell_type":"code","source":"def get_lgb_params(space):\n    lgb_params = dict()\n    lgb_params['boosting_type'] = space['boosting_type'] if 'boosting_type' in space else 'gbdt'\n    lgb_params['application'] = 'regression'\n    lgb_params['metric'] = 'l2_root'\n    lgb_params['learning_rate'] = space['learning_rate']\n    lgb_params['num_leaves'] = int(space['num_leaves'])\n    lgb_params['min_data_in_leaf'] = int(space['min_data_in_leaf'])\n    lgb_params['min_sum_hessian_in_leaf'] = space['min_sum_hessian_in_leaf']\n    lgb_params['max_depth'] = -1\n    lgb_params['lambda_l1'] = space['lambda_l1'] if 'lambda_l1' in space else 0.0\n    lgb_params['lambda_l2'] = space['lambda_l2'] if 'lambda_l2' in space else 0.0\n    lgb_params['max_bin'] = int(space['max_bin']) if 'max_bin' in space else 256\n    lgb_params['feature_fraction'] = space['feature_fraction']\n    lgb_params['bagging_fraction'] = space['bagging_fraction']\n    lgb_params['bagging_freq'] = int(space['bagging_freq']) if 'bagging_freq' in space else 1\n\n    return lgb_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f101a5aaadc59601ed50167a146d5f86d357f9f2"},"cell_type":"code","source":"obj_call_count = 0\ncur_best_loss = np.inf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1db0c914fc0cb5c409496e4b7a4560cb98f60f0"},"cell_type":"code","source":"X_1, X_test, y_1, y_test = train_test_split(X_short, y, test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X_1, y_1, test_size=0.3)\nD_train = lightgbm.Dataset(X_train, y_train)\nD_val = lightgbm.Dataset(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26d0854b45328b86e698718603ca01adba02e83c"},"cell_type":"code","source":"def objective(space):\n    global obj_call_count, cur_best_loss\n\n    obj_call_count += 1\n\n    #print('\\nXGB objective call #{} cur_best_loss={:7.5f}'.format(obj_call_count,cur_best_loss) )\n\n    lgb_params = get_lgb_params(space)\n\n    sorted_params = sorted(space.items(), key=lambda z: z[0])\n    params_str = str.join(' ', ['{}={}'.format(k, v) for k, v in sorted_params])\n    #print('Params: {}'.format(params_str) )\n\n    model = lightgbm.train(lgb_params,\n                           D_train,\n                           num_boost_round=10000,\n                           # metrics='mlogloss',\n                           valid_sets=D_val,\n                           # valid_names='val',\n                           # fobj=None,\n                           # feval=None,\n                           # init_model=None,\n                           # feature_name='auto',\n                           # categorical_feature='auto',\n                           early_stopping_rounds=100,\n                           # evals_result=None,\n                           verbose_eval=False,\n                           # learning_rates=None,\n                           # keep_training_booster=False,\n                           # callbacks=None\n                           )\n\n    nb_trees = model.best_iteration\n    val_loss = model.best_score\n\n    #print('nb_trees={} val_loss={}'.format(nb_trees, val_loss))\n\n    y_pred = model.predict(X_test, num_iteration=nb_trees)\n    test_loss = rmsle(y_test, y_pred)\n\n    #print('test_loss={}'.format(test_loss))\n\n    if test_loss<cur_best_loss:\n        cur_best_loss = test_loss\n        print('NEW BEST LOSS={}'.format(cur_best_loss))\n\n\n    return{'loss':test_loss, 'status': STATUS_OK }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"192f0361db5521873018ede6f1eae93dd30aeb22"},"cell_type":"code","source":"space ={\n        'num_leaves': hp.quniform ('num_leaves', 10, 200, 1),\n        'min_data_in_leaf':  hp.quniform ('min_data_in_leaf', 10, 200, 1),\n        'feature_fraction': hp.uniform('feature_fraction', 0.75, 1.0),\n        'bagging_fraction': hp.uniform('bagging_fraction', 0.75, 1.0),\n        'learning_rate': hp.loguniform('learning_rate', -5.0, -2.3),\n        'min_sum_hessian_in_leaf': hp.loguniform('min_sum_hessian_in_leaf', 0, 2.3),\n        'max_bin': hp.quniform ('max_bin', 64, 512, 1),\n        'bagging_freq': hp.quniform ('bagging_freq', 1, 5, 1),\n        'lambda_l1': hp.uniform('lambda_l1', 0, 10 ),\n        'lambda_l2': hp.uniform('lambda_l2', 0, 10 ),\n       }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfe4a4a5ec20ee476d6aa2bb20846371484314e8"},"cell_type":"code","source":"trials = Trials()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"205db1a93f9221182b4674326a9de5c8c15f31b5"},"cell_type":"code","source":"best = hyperopt.fmin(fn=objective,\n                     space=space,\n                     algo=HYPEROPT_ALGO,\n                     max_evals=N_HYPEROPT_PROBES,\n                     trials=trials,\n                     verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd82a57b771b1fab6f87b0ffd4eab74f2259e5c3"},"cell_type":"code","source":"print(get_lgb_params(best))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66835d6150134108176447446d751979ca04bc9d"},"cell_type":"markdown","source":"We've chosen the parameters and now we can train the model"},{"metadata":{"trusted":true,"_uuid":"b75a47b71dbf7295591e3d7af447a6036893de99"},"cell_type":"code","source":"def run_lgb(train_X, train_y, val_X, val_y, test_X, params):\n    \n    lgtrain = lightgbm.Dataset(train_X, label=train_y)\n    lgval = lightgbm.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lightgbm.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=False, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4884eb8851b13f7e1617840e5ae75ee251e465eb"},"cell_type":"code","source":"X_train = X_short.reset_index(drop=True)\ny_train = y.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7144517400abfc206d76d65b8d63c4cb0e5eb47"},"cell_type":"code","source":"params = get_lgb_params(best)\nkf = KFold(n_splits=5, shuffle=True, random_state=2017)\npred_test_lgb = 0\nfor dev_index, val_index in kf.split(X_train):\n    dev_X, val_X = X_train.loc[dev_index,:], X_train.loc[val_index,:]\n    dev_y, val_y = y_train[dev_index], y_train[val_index]\n    pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_short,params)\n    pred_test_lgb += pred_test\npred_test_lgb /= 5.\npred_test_lgb = np.expm1(pred_test_lgb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2d4b8d55ec7c204ebceb26340c04f13458021ce"},"cell_type":"markdown","source":"## CatBoost"},{"metadata":{"trusted":true,"_uuid":"0992e837b3baddf2e9aa34e57311545794f8b90c"},"cell_type":"code","source":"from catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1cbd2fb163d2b4daa64e21e8b92ac446f8833e6"},"cell_type":"code","source":"cb_model = CatBoostRegressor(iterations=500,\n                             learning_rate=0.05,\n                             depth=10,\n                             eval_metric='RMSE',\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5bd6d446a2e62123d0373e107012e4df2789704"},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_short, y, test_size = 0.2, random_state = 42)\ncb_model.fit(X_train, y_train,\n             eval_set=(X_val, y_val),\n             use_best_model=True,\n             verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07afd7eee05521f1bced806effb3b0e0e8252e80"},"cell_type":"code","source":"pred_test_cat = np.expm1(cb_model.predict(test_short))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"909bc0eb645728d429ef1b2da62282d0281ffcc5"},"cell_type":"markdown","source":"## Combine models"},{"metadata":{"trusted":true,"_uuid":"c0773c3335e106db72a5d699a03555157a0a7ebc"},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['ID'] = test.index\nsub['target'] = (pred_test_lgb + pred_test_cat)/2\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dca08e342b9189cf595163e9756504fe6cb3bce1"},"cell_type":"markdown","source":"Thanks to:<br>\nhttps://www.kaggle.com/the1owl/love-is-the-answer<br>\nhttps://www.kaggle.com/alexpengxiao/preprocessing-model-averaging-by-xgb-lgb-1-39<br>\nhttps://www.kaggle.com/samratp/lightgbm-xgboost-catboost"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d10ee54dd5de9d16b750e34972ff128c99a07d64"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}