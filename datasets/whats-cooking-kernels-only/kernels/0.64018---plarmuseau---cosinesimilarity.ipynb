{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# Import the required libraries \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom scipy.sparse import coo_matrix, vstack\nimport pandas as pd\nimport json\nfrom sklearn import manifold\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np # linear algebra\nfrom mpl_toolkits.mplot3d import Axes3D\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7ff50ba79a30fc19d257fdeb2ad9925264f172a1"},"cell_type":"code","source":"from sklearn.preprocessing import normalize\nfrom scipy.sparse import coo_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef cosine(plays):\n    normalized = normalize(plays)\n    return normalized.dot(normalized.T)\n\n\ndef bhattacharya(plays):\n    plays.data = np.sqrt(plays.data)\n    return cosine(plays)\n\n\ndef ochiai(plays):\n    plays = csr_matrix(plays)\n    plays.data = np.ones(len(plays.data))\n    return cosine(plays)\n\n\ndef bm25_weight(data, K1=1.2, B=0.8):\n    \"\"\" Weighs each row of the matrix data by BM25 weighting \"\"\"\n    # calculate idf per term (user)\n    N = float(data.shape[0])\n    idf = np.log(N / (1 + np.bincount(data.col)))\n\n    # calculate length_norm per document (artist)\n    row_sums = np.squeeze(np.asarray(data.sum(1)))\n    average_length = row_sums.sum() / N\n    length_norm = (1.0 - B) + B * row_sums / average_length\n\n    # weight matrix rows by bm25\n    ret = coo_matrix(data)\n    ret.data = ret.data * (K1 + 1.0) / (K1 * length_norm[ret.row] + ret.data) * idf[ret.col]\n    return ret\n\n\ndef bm25(plays):\n    plays = bm25_weight(plays)\n    return plays.dot(plays.T)\n\ndef get_largest(row, N=10):\n    if N >= row.nnz:\n        best = zip(row.data, row.indices)\n    else:\n        ind = np.argpartition(row.data, -N)[-N:]\n        best = zip(row.data[ind], row.indices[ind])\n    return sorted(best, reverse=True)\n\n\ndef calculate_similar_artists(similarity, artists, artistid):\n    neighbours = similarity[artistid]\n    top = get_largest(neighbours)\n    return [(artists[other], score, i) for i, (score, other) in enumerate(top)]\n\n\n\ndef tf_svd_vec(data,descr,k,ngram):\n    # attention: an 100.000 rows database returns a 100kx100k =10G matrix\n    #from sklearn.decomposition import TruncatedSVD\n    from sklearn.feature_extraction.text import CountVectorizer\n    from sklearn.metrics.pairwise import cosine_similarity\n    from scipy.sparse.linalg import svds\n    \n    vec = CountVectorizer(ngram_range=(1,ngram),strip_accents='ascii',min_df=0.00125 )  #,token_pattern=\"[a-zA-Z]*\")\n    # term frequency data train, data.name\n    tf= vec.fit_transform(data[descr].fillna(''))\n    print('term Frequency',tf.shape)\n    #print('words',vec.get_feature_names())\n    #svd = TruncatedSVD(n_components=k, n_iter=7, random_state=42)\n    Ur, Si, VTr = svds(bm25_weight(coo_matrix(tf)), k=k)\n    #Ur=svd.fit_transform(bm25(coo_matrix(tf)))  #all words\n    #print(svd.explained_variance_ratio_)  \n    #print('explained variance',svd.explained_variance_ratio_.sum())\n    #reduced vectorspace to k features\n    #Xrdf=pd.DataFrame(cosine_similarity(Xr,Xr[:100]))\n    #print('reduced term freq',Xrdf[:5])\n    print('Ur,Si,VTr shape',Ur.shape,Si.shape,VTr.shape)\n    VTr=pd.DataFrame(VTr,columns=vec.get_feature_names())\n    Ur=pd.DataFrame(Ur,index=data.index)\n    return Ur,VTr\n\ndef OptSim(trainm,veld1,veld2,k,ngram):\n    from scipy.sparse.linalg import svds\n    Ux,Vx=tf_svd_vec(trainm,veld1,k,ngram)\n    Uy,Vy=tf_svd_vec(trainm,veld2,k,ngram)\n    comwords=list(set(Vx.columns).intersection(Vy.columns))\n    comwords=[x for x in comwords if len(x)>2]\n    print('nr common words to align',len(comwords))    \n    Uo,So,VTo=svds( Vy[comwords].dot(Vx[comwords].T),k=int(k*.8) )\n    #\n    print('matrix alignment',Uo.shape,VTo.shape )\n    Osim=np.matmul(Uo,VTo)\n\n    print('Osim',Osim.shape)\n    return Vx,pd.DataFrame(VTo.dot(Vx),columns=Vx.columns),Vy,pd.DataFrame(Uo.T.dot(Vy),columns=Vy.columns) \n\n#Ux,Vx=tf_svd_vec(train,'naam',500)\n#Ux,Vx=tf_svd_vec(para,'klasnaam',100)\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport re\n\ndef cleanhtml(raw_html):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', raw_html)\n    return cleantext","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Dataset Preparation\nprint (\"Read Dataset ... \")\ntrain = pd.read_json('../input/train.json')\ntest = pd.read_json('../input/test.json')\nprint(train.head())\nprint(test.head())\ntrain[['id','cuisine']].groupby('cuisine').count().plot(kind='bar')\n\n# Text Data Features\n\nprint (\"Prepare text data of Train and Test ... \")\ntrain_text = train['ingredients'].map(\" \".join)\ntest_text = test['ingredients'].map(\" \".join)\ntrain_text.columns=['ingredients']\npd.DataFrame(train_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7935da04dadf0089dee0a2fa7ca8afefd251d663"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccf039227d74f9b9dfed6015c022185f1d2a7844","collapsed":true},"cell_type":"markdown","source":"#OptSim(pd.DataFrame(train_text),'ingredients','ingredients',200,1)\nTXu_,TXvh_=tf_svd_vec(pd.DataFrame(train_text.append(test_text)),'ingredients',200,1)"},{"metadata":{"trusted":true,"_uuid":"f1b9cfe6057c699e21b5ecd46a7df2e3277e8706","collapsed":true},"cell_type":"markdown","source":"from sklearn.metrics import pairwise\nTXcos=cosine_similarity( TXu_[:len(train)],TXu_[len(train):] ) "},{"metadata":{"trusted":true,"_uuid":"3e8f0ce903634d72c553a9d10b9a787ebf821e82","collapsed":true},"cell_type":"markdown","source":"TXcos=pd.DataFrame( TXcos )\nTXcos['cuisine']=train['cuisine']\n"},{"metadata":{"trusted":true,"_uuid":"97068fc5c0921d09f661a698d164cc39ad5e1297","collapsed":true},"cell_type":"markdown","source":"\ntop3=TXcos[TXcos[0]>TXcos[0].max()*0.9][[0,'cuisine']].sort_values(0,ascending=False)\ntop3.values.reshape(-1)\n"},{"metadata":{"trusted":true,"_uuid":"68b57a4e4c71b6d68238ae151e34fa21a3874e2d","scrolled":true},"cell_type":"code","source":"def zoekkeywords(padf,groepb,groeptxt,ngram):\n    para3d=padf.groupby(groepb).apply(lambda x : x[groeptxt].values)\n    para3d=pd.DataFrame(para3d,columns=[groeptxt])\n    for xi in range(0,len(para3d)):\n        tottxt=''\n        for yi in range(0,len(para3d.iloc[xi][groeptxt])):\n            tottxt=tottxt+' '+str(para3d.iloc[xi][groeptxt][yi])\n    \n        para3d.iloc[xi][groeptxt]=cleanhtml(tottxt)\n    \n    print(para3d)\n    Ux2,Vx2=tf_svd_vec(para3d,groeptxt,5,ngram)  #15 0.63\n    from sklearn.metrics.pairwise import cosine_similarity\n\n    UVklasse=pd.DataFrame(cosine_similarity(Ux2,Vx2.T),index=Ux2.index,columns=Vx2.columns)\n    print(UVklasse.head())\n    autoklas=pd.DataFrame([])\n    autoklas['klasse']=UVklasse.index\n    autoklas['woorden']='woorden'\n    for xi in range(0,len(UVklasse)):\n        klassind=UVklasse.index[xi]\n        #print(klassind)\n        tempo=pd.DataFrame(UVklasse.iloc[xi])\n        laatste=tempo.sort_values(by=klassind)[-10:]\n        #print(laatste)\n        text=laatste.index.values\n        tempo=tempo.sort_values(by=klassind,ascending=False)[:2900]\n        tempo=tempo[tempo[klassind]>tempo[klassind].max()*0.31]\n        #print(tempo)\n        autoklas.iat[xi,1]=\",\".join(tempo.index.values)\n\n    return autoklas,UVklasse\n\nklassekeywords=pd.DataFrame([])\nklassekeywords,UVsimilariteit=zoekkeywords(train,'cuisine','ingredients',1)\nklassekeywords.columns=['cuisine','ingredients']\n#klassekeywords.to_csv('d:klas_tagsb.csv',index=False)\n\n#klassekeywords=zoekkeywords(para,'klasse','titel',2)\n#klassekeywords['default']=\"Default group\"\n#klassekeywords.to_csv('d:klas_tagsc.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95d832e7e15faafe53ccd5f332a18287ebb3b816"},"cell_type":"code","source":"klassekeywords.ingredients\nTRu_,TRvh_=tf_svd_vec(pd.DataFrame((train_text.append(test_text)).append(klassekeywords.ingredients)) ,'ingredients',18,1)\nTRcos=cosine_similarity( TRu_[:len(train)+len(test)],TRu_[len(train)+len(test):] ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8446820daf18209db13eb6898ba0560e039c17d0","collapsed":true},"cell_type":"code","source":"TRcos=pd.DataFrame(TRcos,columns=klassekeywords.cuisine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c746caa1ea9b899bb8474618f73f58d07c0d156e"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(multi_class = 'ovr')\n#model.fit(TRcos[:len(train)], train.cuisine)\nmodel.fit(cosine_similarity(TRu_[:len(train)],TRvh_.T) ,train.cuisine)\n\n#(model.predict(TRcos[:len(train)])==train.cuisine).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f53589f7e576270ffda5365ca3da6e9465ea7e32","collapsed":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import classification_report\n\n\nparams = {\n    'multi_class': 'ovr',\n    'solver': 'lbfgs'\n}\n\nlgbm_params = {\n    'n_estimators': 250,\n    'max_depth': 22,\n    'learning_rate': 0.2,\n    'objective': 'multiclass',\n    'n_jobs': 7\n}\n\nmodel = LGBMClassifier(**lgbm_params)\n# model = LogisticRegression(**params)\nmodel.fit(TRcos[:len(train)], train.cuisine)\nprint(model)\n\n\ny_true =train.cuisine\ntarget_names = model.classes_\ny_pred = model.predict(TRcos[:len(train)])\nprint(classification_report(y_true, y_pred, target_names=target_names))\n\n\nsubmission = model.predict(TRcos[len(train):len(train)+len(test)])\nsubmission_df = pd.Series(submission, index=test.index).rename('cuisine')\nsubmission_df.to_csv(\"logistic_sub.csv\", index=True, header=True)\nprint(submission_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4cf3b71c01d8db2feedee17cfa48f2b223950b6","collapsed":true},"cell_type":"code","source":"submission = model.predict(TRcos[len(train):len(train)+len(test)])\n\nsubmission_df=pd.DataFrame(submission)\nsubmission_df['id']=test.id\nsubmission_df.columns=['cuisine','id']\nsubmission_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"df00429e56a6ba59f33dcdaacd50abaf66f8b252"},"cell_type":"code","source":"submission_df.to_csv(\"logistic_sub.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}