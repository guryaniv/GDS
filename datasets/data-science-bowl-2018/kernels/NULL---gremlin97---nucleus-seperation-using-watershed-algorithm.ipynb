{"cells":[{"metadata":{"_uuid":"c67c0b9050a1c55eb62fa85ffd8a680b9b3f458b","_cell_guid":"a73b9c4a-3813-47a3-a25d-6622cb49a245"},"cell_type":"markdown","source":"**Initially applying Image segmentation for one Image**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport pathlib\nimport imageio\nimport cv2\nimport skimage\nprint(os.listdir(\"../input\"))\n#GLobalising Training data and creating a single image path\ntrain_paths = pathlib.Path('../input/stage1_train').glob('*/images/*.png')\ntrain_sorted = sorted([x for x in train_paths])\nim_path = train_sorted[45]\nim = cv2.imread(str(im_path))#Read Images\n#im\n\n","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"e8c8a6c783c8cf5abdb17fc772053944e714ab32","_cell_guid":"5da2a7d8-f8fb-434b-88a4-b044ef7ef6e5","trusted":true},"cell_type":"code","source":"#First we will analyse one image \n#Image ID\nim_id = im_path.parts[-3]\nim_id","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Converting image to grayscale for better analysis\nim_gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\nim1=im","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"b29f4cc6d4921ffca8a9cc0151a82274c1bf02ad","_cell_guid":"33cec52f-ef77-4bb1-893b-3c1702696635","trusted":true},"cell_type":"code","source":"#Plotting data for comparision of grayscale versus original image\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,8))\nplt.subplot(121)\nplt.imshow(im)\nplt.axis('off')\nplt.title('Original Image')\nplt.subplot(122)\nplt.imshow(im_gray, cmap='gray')\nplt.axis('off')\nplt.title('Grayscale Image')\nplt.tight_layout()\nplt.show()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"f820e7feeb47a5e7870a2b051d713ab114dda24c","scrolled":true,"_cell_guid":"ed0779a7-6f95-4e54-a1aa-5cab56b5ff3d","trusted":true},"cell_type":"code","source":"#In image processing gaussioan blur is applied to reduce the noise in the image \n#Gaussian blur is neccesary to be applied before watershedding\nim_blur=cv2.GaussianBlur(im_gray,(5,5),0)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,8))\nplt.subplot(121)\nplt.imshow(im)\nplt.axis('off')\nplt.title('Original Image')\nplt.subplot(122)\nplt.imshow(im_blur, cmap='gray')\nplt.axis('off')\nplt.title('Blurred Grayscale Image')\nplt.tight_layout()\nplt.show()","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"0b8be111aa3251be199897669ba52ac3d0fbf0f9","_cell_guid":"35b152e4-fa9e-4450-9303-572387089767"},"cell_type":"markdown","source":"\n"},{"metadata":{"_kg_hide-input":false,"_uuid":"2a8f49d66d4df8af4f403ddf6945e6b35da2afdf","_kg_hide-output":false,"_cell_guid":"efade880-cbd6-4080-b7df-b6391272de50"},"cell_type":"markdown","source":"**APPLYING IMAGE SEGMENTATION USING WATERSHED**"},{"metadata":{"_uuid":"a5aa80ddb2ccca6cac38a6dd65422ab62c383840","_cell_guid":"9d62263c-aff6-46cd-96e8-32f0f0dfb995","trusted":true},"cell_type":"code","source":"#Using Watershed\nret,th = cv2.threshold(im_blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\nplt.figure(figsize=(10,10))\nplt.subplot(121)\nplt.imshow(th,cmap='gray')\nplt.axis(\"off\")\nplt.show()","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"9621dd33490883a03f5fbd3a59b10df17512cf83","_cell_guid":"912cbcf7-93c9-470e-9c7d-1c20b5d6cfa9","trusted":true},"cell_type":"code","source":"# noise removal\nkernel = np.ones((3,3),np.uint8)\nopening = cv2.morphologyEx(th,cv2.MORPH_OPEN,kernel, iterations = 2)\nplt.figure(figsize=(10,10))\nplt.subplot(121)\nplt.imshow(opening,cmap='gray')\nplt.axis(\"off\")\nplt.show()\n","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"cb9761bcf0be1b32ead6cda436e9c268b2d5b83d","_cell_guid":"0d201cf1-fc1b-412e-8c85-f2059bbc1fa8","trusted":true},"cell_type":"code","source":"# sure background area\nsure_bg = cv2.dilate(opening,kernel,iterations=3)\n# Finding sure foreground area\ndist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\nret, sure_fg = cv2.threshold(dist_transform,0.005*dist_transform.max(),255,0)\n# Finding unknown region\nsure_fg = np.uint8(sure_fg)\nunknown = cv2.subtract(sure_bg,sure_fg)\n# Marker labelling\nret, markers = cv2.connectedComponents(sure_fg)\n# Add one to all labels so that sure background is not 0, but 1\nmarkers = markers+1\n# Now, mark the region of unknown with zero\nmarkers[unknown==255] = 0\nplt.figure(figsize=(10,10))\nplt.subplot(121)\nplt.imshow(markers,cmap='jet')\nplt.axis(\"off\")\nplt.show()\n\n","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"ac824de11978864a2f04c2e09c1db708003d7fa1","_cell_guid":"5d2a5450-3336-4a59-b195-d6ce8354664c","trusted":true},"cell_type":"code","source":"markers = cv2.watershed(im1,markers)\nim1[markers == -1] = [255,0,0]\nplt.figure(figsize=(10,10))\nplt.subplot(121)\nplt.imshow(im1,cmap='gray')\nplt.axis(\"off\")\nplt.show()\n","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"4cb000aef40f45cecf1fb41f80bade61ae81465e","scrolled":true,"_cell_guid":"cb29d076-16d0-45ff-b4e3-392bdaf9b069","trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(20, 20), dpi= 80, facecolor='w', edgecolor='k')\nplt.axis(\"off\")\nplt.subplot(121)\nplt.imshow(im1)\nplt.axis(\"off\")\nplt.subplot(122)\nplt.imshow(markers,cmap='gray')\nplt.axis(\"off\")\nplt.show()\n\n\n","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"745f231ed616675a1951c01ce66d272dd04b33ec","_kg_hide-output":false,"_cell_guid":"d82ac467-3aa9-4bba-a764-2abbe1dee4c4"},"cell_type":"markdown","source":"**Calculating Total Number of Masks**"},{"metadata":{"_uuid":"017dfddef4a70d089f3ab3ace79d768cf98cfe9e","collapsed":true,"_cell_guid":"47adf5cb-67de-464c-854a-8ceb45d23489","trusted":true},"cell_type":"code","source":"mask = np.where(markers > sure_fg, 1, 0)\n# Make sure the larger portion of the mask is considered background\nif np.sum(mask==0) < np.sum(mask==1):\n    mask = np.where(mask, 0, 1)","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"c16e93255927089283293740b44d72ca0d08ccc3","_cell_guid":"62901e2f-ed6d-4ce4-ba6f-155131a738e3","trusted":true},"cell_type":"code","source":"from scipy import ndimage\nlabels, nlabels = ndimage.label(mask)\n# Regenerate the labels\nlabel_arrays = []\nfor label_num in range(1, nlabels+1):\n    label_mask = np.where(labels == label_num, 1, 0)\n    label_arrays.append(label_mask)\n    \nprint('There are {} separate components / objects detected.'.format(nlabels))","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"08f8e777501b1ae451e3fa828aefb9343d3c9fa9","_cell_guid":"f4702464-6c79-4afd-947d-c082459a50d4","trusted":true},"cell_type":"code","source":"for label_ind, label_coords in enumerate(ndimage.find_objects(labels)):\n    cell = markers[label_coords]\n    \n    # Check if the label size is too small\n    if np.product(cell.shape) < 10: \n        #print('Label {} is too small! Setting to 0.'.format(label_ind))\n        mask = np.where(labels==label_ind+1, 0, mask)\n\n# Regenerate the labels\nlabels, nlabels = ndimage.label(mask)\n\nlabel_arrays = []\nfor label_num in range(1, nlabels+1):\n    label_mask = np.where(labels == label_num, 1, 0)\n    label_arrays.append(label_mask)\n    \nprint('There are now {} separate components / objects detected.'.format(nlabels))","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"ca1f7f13c683123cbf21f404de1d3a7252f89d63","scrolled":true,"_cell_guid":"65b69d0e-b4c6-434b-aa09-53b64f21ddd3","trusted":true},"cell_type":"code","source":"#RLE Encoding Function\ndef rle_encoding(x):\n    '''\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    '''\n    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b+1, 0))\n        run_lengths[-1] += 1\n        \n        prev = b\n    return \" \".join([str(i) for i in run_lengths])\n\nprint('RLE Encoding for the current mask is: {}'.format(rle_encoding(label_mask)))","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"7ca5efd9c411fe2d2572cd40f94236590b7c85ac","_cell_guid":"71685cad-7622-4137-913d-c9e2465d6bb9"},"cell_type":"markdown","source":"**Now for Multiple Images **"},{"metadata":{"_uuid":"9c93c72a9b4739e14df75f9f8a7863bc473f1d78","collapsed":true,"_cell_guid":"0ef5c372-3586-46b3-aaab-d85ae8dcbf0f","trusted":true},"cell_type":"code","source":"#Writing a function to carry out all processing given above for the whole test dataset Images\nimport pandas as pd\n\n\ndef analyze_image(im_path):\n    '''\n    Take an image_path (pathlib.Path object), preprocess and label it, extract the RLE strings \n    and dump it into a Pandas DataFrame.\n    '''\n    # Read in data and convert to grayscale\n    im_id = im_path.parts[-3]\n    im = cv2.imread(str(im_path))\n    #COnverting to grayscale\n    im_gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n    #im1 = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n    im1=im\n    \n    im_blur=cv2.GaussianBlur(im_gray,(5,5),0)\n    import matplotlib.pyplot as plt\n\n    #plt.figure(figsize=(8,8))\n\n    #plt.subplot(121)\n    #plt.imshow(im)\n    #plt.axis('off')\n    #plt.title('Original Image')\n\n    #plt.subplot(122)\n    #plt.imshow(im_blur, cmap='gray')\n    #plt.axis('off')\n    #plt.title('Blurred Grayscale Image')\n\n    #plt.tight_layout()\n    #plt.show()\n    \n    ret,th = cv2.threshold(im_blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n    #plt.figure(figsize=(10,10))\n    #plt.subplot(121)\n    #plt.imshow(th,cmap='gray')\n    #plt.axis(\"off\")\n    #plt.show()\n    \n    # noise removal\n    kernel = np.ones((3,3),np.uint8)\n    opening = cv2.morphologyEx(th,cv2.MORPH_OPEN,kernel, iterations = 2)\n    #plt.figure(figsize=(10,10))\n    #plt.subplot(121)\n    #plt.imshow(opening,cmap='gray')\n    #plt.axis(\"off\")\n    #plt.show()\n    \n    # sure background area\n    sure_bg = cv2.dilate(opening,kernel,iterations=3)\n    # Finding sure foreground area\n    dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n    ret, sure_fg = cv2.threshold(dist_transform,0.005*dist_transform.max(),255,0)\n    # Finding unknown region\n    sure_fg = np.uint8(sure_fg)\n    unknown = cv2.subtract(sure_bg,sure_fg)\n    # Marker labelling\n    ret, markers = cv2.connectedComponents(sure_fg)\n    # Add one to all labels so that sure background is not 0, but 1\n    markers = markers+1\n    # Now, mark the region of unknown with zero\n    markers[unknown==255] = 0\n    #plt.figure(figsize=(10,10))\n    #plt.subplot(121)\n    #plt.imshow(markers,cmap='jet')\n    #plt.axis(\"off\")\n    #plt.show()\n    markers = cv2.watershed(im1,markers)\n    im1[markers == -1] = [255,0,0]\n    #fig=plt.figure(figsize=(20, 20), dpi= 80, facecolor='w', edgecolor='k')\n    #plt.axis(\"off\")\n    #plt.subplot(131)\n    #plt.imshow(im1)\n    #plt.axis(\"off\")\n    #plt.subplot(132)\n    #plt.imshow(markers,cmap='gray')\n    #plt.axis(\"off\")\n    #plt.show()\n    #plt.subplot(133)\n    #plt.imshow(im_gray,cmap='gray')\n    #plt.axis(\"off\")\n    #plt.show()\n    mask = np.where(markers > sure_fg, 1, 0)\n\n    # Make sure the larger portion of the mask is considered background\n    if np.sum(mask==0) < np.sum(mask==1):\n        mask = np.where(mask, 0, 1)\n        \n    from scipy import ndimage\n    labels, nlabels = ndimage.label(mask)\n    #print('There are {} separate components / objects detected.'.format(nlabels))\n    \n    for label_ind, label_coords in enumerate(ndimage.find_objects(labels)):\n         cell = markers[label_coords]\n    \n         # Check if the label size is too small\n         if np.product(cell.shape) < 10: \n             #print('Label {} is too small! Setting to 0.'.format(label_ind))\n             mask = np.where(labels==label_ind+1, 0, mask)\n\n    # Regenerate the labels\n    labels, nlabels = ndimage.label(mask)\n\n    label_arrays = []\n    for label_num in range(1, nlabels+1):\n        label_mask = np.where(labels == label_num, 1, 0)\n        label_arrays.append(label_mask)\n    \n    #print('There are now {} separate components / objects detected.'.format(nlabels))\n    \n    def rle_encoding(x):\n        dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n        run_lengths = []\n        prev = -2\n        for b in dots:\n            if (b>prev+1): run_lengths.extend((b+1, 0))\n            run_lengths[-1] += 1\n            prev = b\n        return \" \".join([str(i) for i in run_lengths])\n\n    #print('RLE Encoding for the current mask is: {}'.format(rle_encoding(label_mask)))\n    \n    # Loop through labels and add each to a DataFrame\n    im_df = pd.DataFrame()\n    for label_num in range(1, nlabels+1):\n        label_mask = np.where(labels == label_num, 1, 0)\n        if label_mask.flatten().sum() > 10:\n            rle = rle_encoding(label_mask)\n            s = pd.Series({'ImageId': im_id, 'EncodedPixels': rle})\n            im_df = im_df.append(s, ignore_index=True)\n    \n    return im_df\n\n\ndef analyze_list_of_images(im_path_list):\n    '''\n    Takes a list of image paths (pathlib.Path objects), analyzes each,\n    and returns a submission-ready DataFrame.'''\n    all_df = pd.DataFrame()\n    for im_path in im_path_list:\n        im_df = analyze_image(im_path)\n        all_df = all_df.append(im_df, ignore_index=True)\n    \n    return all_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9182d8bb6969f1002be351cc5b3c24caff56fd26","collapsed":true,"_cell_guid":"c75607c0-3b89-4c10-a564-b5fc2d11e7db","trusted":true},"cell_type":"code","source":"testing = pathlib.Path('../input/stage2_test_final/').glob('*/images/*.png')\ndf = analyze_list_of_images(list(testing))\ndf.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2c3ae79916feddf7140e11d963027c6ad2de26f","collapsed":true,"_cell_guid":"adfbec51-0e80-4774-949a-04fea450ed37","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8885b5a19b029383000a5a0e81c9a672a30760fe","collapsed":true,"_cell_guid":"19f57541-cd27-4c40-9806-8ae6d5ccff55","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17bd4790460aff866d6e0698e326bc26083da776","collapsed":true,"_cell_guid":"621c9779-e010-4797-b596-1fbfbea394e9","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01d7571552b5258ac21752156bf5d78d3f067c20","collapsed":true,"_cell_guid":"cf898044-0386-41d3-a02b-45fabbf05225","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"557707f47d12ac74130979c67a5862e1f710765c","collapsed":true,"_cell_guid":"63e0590c-215b-4ac0-97b2-d1a7a51b0dc6","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b75140a0e3c5013978a0cc2c99d3425388c93b3f","collapsed":true,"_cell_guid":"1860ed14-18f9-4ea3-96ff-2d788adcf4c4","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}