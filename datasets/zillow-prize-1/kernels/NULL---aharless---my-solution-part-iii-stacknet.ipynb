{"cells": [{"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["ACTUALLY_RUN_MAKEDATA = False\n", "ACTUALLY_RUN_MAKESUB = False"], "execution_count": 2}, {"metadata": {}, "cell_type": "markdown", "source": ["This kernel doesn't really do anything.  It just documents how I used StackNet.  Essentially, I just followed the instructions from [KazAnova's GitHub reopository](http://https://github.com/kaz-Anova/StackNet/blob/master/example/zillow_regression_sparse/README.MD) (as referenced in [this discussion topic](https://www.kaggle.com/c/zillow-prize-1/discussion/39111)), making no changes to the model, but with the data processing changes below (to include 2017 data and fix a Python version issue).  The block below is my version of KazAnova's <code>make_stacknet_data.py</code>, with the main program inside an <code>if False</code> block to prevent it from running on Kaggle."]}, {"outputs": [], "metadata": {"_uuid": "1709780cda0c8861ebbc4dd1b211d4dc2cc0874a", "collapsed": true, "_cell_guid": "7365d071-9c3a-4a6b-bcc7-70b897bf09a3"}, "cell_type": "code", "source": ["# make_stacknet_data17a.py\n", "\n", "# Based on this kaggle script : https://www.kaggle.com/danieleewww/xgboost-lightgbm-and-olsv107-w-month-features/code\n", "\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.preprocessing import LabelEncoder\n", "from scipy.sparse import csr_matrix\n", "\n", "\n", "directory=\"input/\" # hodls the data\n", "\n", "## converts arrayo to sparse svmlight format\n", "def fromsparsetofile(filename, array, deli1=\" \", deli2=\":\",ytarget=None):    \n", "    zsparse=csr_matrix(array)\n", "    indptr = zsparse.indptr\n", "    indices = zsparse.indices\n", "    data = zsparse.data\n", "    print(\" data lenth %d\" % (len(data)))\n", "    print(\" indices lenth %d\" % (len(indices)))    \n", "    print(\" indptr lenth %d\" % (len(indptr)))\n", "    \n", "    f=open(filename,\"w\")\n", "    counter_row=0\n", "    for b in range(0,len(indptr)-1):\n", "        #if there is a target, print it else , print nothing\n", "        if type(ytarget)!=type(None):\n", "             f.write(str(ytarget[b]) + deli1)     \n", "             \n", "        for k in range(indptr[b],indptr[b+1]):\n", "            if (k==indptr[b]):\n", "                if np.isnan(data[k]):\n", "                    f.write(\"%d%s%f\" % (indices[k],deli2,-1))\n", "                else :\n", "                    f.write(\"%d%s%f\" % (indices[k],deli2,data[k]))                    \n", "            else :\n", "                if np.isnan(data[k]):\n", "                     f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,-1))  \n", "                else :\n", "                    f.write(\"%s%d%s%f\" % (deli1,indices[k],deli2,data[k]))\n", "        f.write(\"\\n\")\n", "        counter_row+=1\n", "        if counter_row%10000==0:    \n", "            print(\" row : %d \" % (counter_row))    \n", "    f.close()  \n", "    \n", "#creates the main dataset abd prints 2 files to dataset2_train.txt and  dataset2_test.txt\n", "\n", "def dataset2():\n", "\n", "    ##### RE-READ PROPERTIES FILE\n", "    \n", "    print( \"\\nRe-reading properties file ...\")\n", "    properties = pd.read_csv(directory +'properties_2016.csv')\n", "\n", "    train = pd.read_csv(directory +\"train_2016_v2.csv\")      \n", "\n", "    properties2016_raw = pd.read_csv('../input/properties_2016.csv', low_memory = False)\n", "    properties2017 = pd.read_csv('../input/properties_2017.csv', low_memory = False)\n", "    taxvars = ['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']\n", "    tax2016 = properties2016_raw[['parcelid']+taxvars]\n", "    properties2016 = properties2017.drop(taxvars,axis=1).merge(tax2016, \n", "                 how='left', on='parcelid').reindex_axis(properties2017.columns, axis=1)\n", "    for c in properties2016.columns:\n", "        properties2016[c]=properties2016[c].fillna(-1)\n", "        if properties2016[c].dtype == 'object':\n", "            lbl = LabelEncoder()\n", "            lbl.fit(list(properties2016[c].values))\n", "            properties2016[c] = lbl.transform(list(properties2016[c].values))\n", "    for c in properties2017.columns:\n", "        properties2017[c]=properties2017[c].fillna(-1)\n", "        if properties2017[c].dtype == 'object':\n", "            lbl = LabelEncoder()\n", "            lbl.fit(list(properties2017[c].values))\n", "            properties2017[c] = lbl.transform(list(properties2017[c].values))\n", "    train2016 = pd.read_csv('../input/train_2016_v2.csv')\n", "    train2017 = pd.read_csv('../input/train_2017.csv')\n", "\n", "    sample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)\n", "    train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n", "    train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n", "    train_df = pd.concat([train2016, train2017], axis = 0)\n", "\n", "    test = pd.merge(sample_submission[['ParcelId']], properties2017.rename(columns = {'parcelid': 'ParcelId'}), \n", "                how = 'left', on = 'ParcelId')\n", "\n", "    ##### PROCESS DATA FOR XGBOOST\n", "        \n", "    train_df[\"transactiondate\"] = pd.to_datetime(train_df[\"transactiondate\"])\n", "    train_df[\"Month\"] = train_df[\"transactiondate\"].dt.month\n", "    \n", "    x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "    x_test = test.drop(['ParcelId'], axis=1)\n", "    \n", "    x_test[\"transactiondate\"] = '2016-07-01'\n", "    x_test[\"transactiondate\"] = pd.to_datetime(x_test[\"transactiondate\"])\n", "    x_test[\"Month\"] = x_test[\"transactiondate\"].dt.month #should use the most common training date 2016-10-01\n", "    x_test = x_test.drop(['transactiondate'], axis=1)\n", "    \n", "    # shape        \n", "    print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n", "    \n", "    # drop out ouliers\n", "    train_df=train_df[ train_df.logerror > -0.4 ]\n", "    train_df=train_df[ train_df.logerror < 0.419 ]\n", "    x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "    y_train = train_df[\"logerror\"].values.astype(np.float32)\n", "    x_train = x_train.values.astype(np.float32, copy=False)\n", "    x_test = x_test.values.astype(np.float32, copy=False)  \n", "  \n", "    print('After removing outliers:')     \n", "    print (\" shapes of dataset 2 \", x_train.shape, y_train.shape, x_test.shape)\n", "    \n", "    print (\" printing %s \" % (\"dataset2_train.txt\") )\n", "    fromsparsetofile(\"dataset2_train.txt\", x_train, deli1=\" \", deli2=\":\",ytarget=y_train)     \n", "    print (\" printing %s \" % (\"dataset2_test.txt\") )    \n", "    fromsparsetofile(\"dataset2_test.txt\", x_test, deli1=\" \", deli2=\":\",ytarget=None)         \n", "    print (\" finished with daatset2 \" )      \n", "    return\n", " \n", "\n", "\n", "def main():\n", "    \n", "    if ACTUALLY_RUN_MAKEDATA:\n", "        dataset2()\n", "\n", "    \n", "        print( \"\\nFinished ...\")\n", "    \n", "    \n", "    \n", "\n", "if __name__ == '__main__':\n", "   main()"], "execution_count": 3}, {"metadata": {}, "cell_type": "markdown", "source": ["And below is my version of KazAnova's <code>create_submission.py</code>, with the whole program inside an <code>if False</code> block.  (Note that, if you were actually running it, you would want to run it separately from the above, after running StackNet, or maybe you could put in code to wait for StackNet to complete.)"]}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# create_submission.py\n", "\n", "if ACTUALLY_RUN_MAKESUB:\n", "\n", "    # -*- coding: utf-8 -*-\n", "\n", "    #generates submission based on 1-column prediction csv\n", "\n", "    sample=\"input/sample_submission.csv\" # name of sample sybmission\n", "    prediction=\"pred2.csv\"# prediction file\n", "    output=\"output_dataset2017a.csv\"# output submission\n", "\n", "    #the predictions are copied 6 times\n", "\n", "    ff=open(sample, \"r\")\n", "    ff_pred=open(prediction, \"r\")\n", "    fs=open(output,\"w\")\n", "    fs.write(ff.readline())\n", "    s=0\n", "    for line in ff: #read sample submission file\n", "        splits=line.split(\",\")\n", "        ids=splits[0] # get id\n", "        pre_line=ff_pred.readline().replace(\"\\n\",\"\") # parse prediction file and get prediction for the row\n", "        fs.write(ids) # write id\n", "        for j in range(6): # copy the prediction 6 times\n", "            fs.write( \",\" +pre_line )\n", "        fs.write(\"\\n\")\n", "        s+=1\n", "    ff.close() \n", "    ff_pred.close()\n", "    fs.close()       \n", "    print (\"done\")"], "execution_count": 4}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": [], "execution_count": null}], "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"mimetype": "text/x-python", "version": "3.6.3", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python"}}, "nbformat": 4}