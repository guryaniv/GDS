{"cells":[{"metadata":{"_uuid":"2e738c4a0bd42d983246b810191ec4d46b73f071"},"cell_type":"markdown","source":"This is heavily based on following two kernels - \nhttps://www.kaggle.com/bguberfain/a-simple-model-using-the-market-and-news-data\nhttps://www.kaggle.com/dmitrypukhov/eda-and-nn-for-market-and-news"},{"metadata":{"trusted":true,"_uuid":"b82d44394fa7ca57c583522722f9ac59b5e554b0"},"cell_type":"code","source":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom time import time\nimport seaborn as sns\nimport os\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nfrom sklearn.metrics import accuracy_score\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, Activation\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95bb44a7f6a0a2051df16624af6b5b4b03dc105f"},"cell_type":"code","source":"toy=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e4c6d73d0bd15aa0a90ba67e81cac4f3884815c"},"cell_type":"code","source":"# This competition settings\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d04f2e202d9033a919274d68d5ba26d80528a16"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"476981b35bbd99a1e0d661ef88804c26440fcb12"},"cell_type":"code","source":"market_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b042e1701db38410e73c74a61eae2dd8a5590910"},"cell_type":"code","source":"news_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"811f0ac9edb6f166174c4b7a894cc83afa373b9d"},"cell_type":"code","source":"# We will reduce the number of samples for memory reasons\nif toy:\n    market_train_df = market_train_df.tail(100_000)\n    news_train_df = news_train_df.tail(300_000)\nelse:\n    market_train_df = market_train_df.tail(750_000)\n    news_train_df = news_train_df.tail(1500_000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21d1685c8928137fa1ddd75f69b8ceb5c2a85541"},"cell_type":"code","source":"news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2bc428dd09599f345d0334fea07936c87daf913"},"cell_type":"code","source":"def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n    gc.collect()\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n    gc.collect()\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    gc.collect()\n    \n    return market_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3db82220e8bd21e38e4aa7f40b26d7bb0051d24"},"cell_type":"code","source":"market_df = join_market_news(market_train_df, news_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4886d50f2557f2bfe41966225f6bcc6678a1bf47"},"cell_type":"code","source":"market_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d917f85d89165d791caece6172ed7847df8c1a0"},"cell_type":"code","source":"market_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f5ce905fd198c5e8f3e7c43cc3721414f2bde25"},"cell_type":"code","source":"market_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c75d3b557a2952877da7b6dcb5a0af2c0a92f219"},"cell_type":"code","source":"    # Features\n    cat_cols = ['assetCode']\n    time_cols=['year', 'week', 'day', 'dayofweek']\n    mkt_numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n    \n    news_numeric_cols = [\n#        'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n#        'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n#        'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n#        'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n#        'returnsOpenNextMktres10', 'universe', 'urgency_min', 'urgency_count',\n#        'takeSequence_max', 'bodySize_min', 'bodySize_max', 'bodySize_mean',\n#        'bodySize_std', 'wordCount_min', 'wordCount_max', 'wordCount_mean',\n#        'wordCount_std', 'sentenceCount_min', 'sentenceCount_max',\n#        'sentenceCount_mean', 'sentenceCount_std', 'companyCount_min',\n#        'companyCount_max', 'companyCount_mean', 'companyCount_std',\n#        'marketCommentary_min', 'marketCommentary_max', 'marketCommentary_mean',\n#        'marketCommentary_std', \n        'relevance_min', 'relevance_max',\n        'relevance_mean', 'relevance_std', 'sentimentNegative_min',\n        'sentimentNegative_max', 'sentimentNegative_mean',\n        'sentimentNegative_std', 'sentimentNeutral_min', 'sentimentNeutral_max',\n        'sentimentNeutral_mean', 'sentimentNeutral_std',\n        'sentimentPositive_min', 'sentimentPositive_max',\n        'sentimentPositive_mean', 'sentimentPositive_std',\n        'sentimentWordCount_min', 'sentimentWordCount_max',\n        'sentimentWordCount_mean', 'sentimentWordCount_std',\n        'noveltyCount12H_min', 'noveltyCount12H_max', 'noveltyCount12H_mean',\n        'noveltyCount12H_std', 'noveltyCount24H_min', 'noveltyCount24H_max',\n        'noveltyCount24H_mean', 'noveltyCount24H_std', 'noveltyCount3D_min',\n        'noveltyCount3D_max', 'noveltyCount3D_mean', 'noveltyCount3D_std',\n        'noveltyCount5D_min', 'noveltyCount5D_max', 'noveltyCount5D_mean',\n        'noveltyCount5D_std', 'noveltyCount7D_min', 'noveltyCount7D_max',\n        'noveltyCount7D_mean', 'noveltyCount7D_std']\n#         'volumeCounts12H_min',\n#        'volumeCounts12H_max', 'volumeCounts12H_mean', 'volumeCounts12H_std',\n#        'volumeCounts24H_min', 'volumeCounts24H_max', 'volumeCounts24H_mean',\n#        'volumeCounts24H_std', 'volumeCounts3D_min', 'volumeCounts3D_max',\n#        'volumeCounts3D_mean', 'volumeCounts3D_std', 'volumeCounts5D_min',\n#        'volumeCounts5D_max', 'volumeCounts5D_mean', 'volumeCounts5D_std',\n#        'volumeCounts7D_min', 'volumeCounts7D_max', 'volumeCounts7D_mean',\n#        'volumeCounts7D_std']\n    \n    numeric_cols = mkt_numeric_cols + news_numeric_cols\n    \n    feature_cols = cat_cols + time_cols + numeric_cols\n    \n    # Labels\n    label_cols = ['returnsOpenNextMktres10']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c0bcc679ff54bff0e87b5e98112ce5bb5c91807"},"cell_type":"code","source":"print (np.unique(feature_cols).shape)\nprint (len(feature_cols))\nprint (numeric_cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6baf8670d3ee8e0bad58992cf76c6454157aefcf"},"cell_type":"markdown","source":"## Split to train, validation and test"},{"metadata":{"trusted":true,"_uuid":"6c1c30797ef9d52c194ae89d82ad3032bb293ff3"},"cell_type":"code","source":"# Split to train, validation and test.\n# ToDo: remove shuffle, use generator.\n#market_train_df, market_test_df = train_test_split(market[market.time > '2009'].sample(100000, random_state=42), shuffle=True, random_state=24)\nmarket_train_df, market_test_df = train_test_split(market_df, shuffle=True, random_state=24)\nmarket_train_df, market_val_df = train_test_split(market_train_df, shuffle=True, random_state=24)\n\n# Look at min/max and quantiles\nmarket_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eafcd6a52d8e6375dd4d54fce2c41029d180454d"},"cell_type":"markdown","source":"## Preprocess the data\nScale numeric columns, encode categorical. Split to X, y for training."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"85edd087f2e51454ba31d540d624004af2f9e288"},"cell_type":"code","source":"class Prepro:\n    \"\"\"\n    Bring all preprocessing here: scale, encoding\n    Should be fit on train data and called on train, validation or test data\n    \"\"\"\n    \n    def __init__(self, feature_cols, cat_cols, time_cols, numeric_cols, label_cols):\n        self.feature_cols = feature_cols\n        self.cat_cols = cat_cols\n        self.time_cols = time_cols\n        self.numeric_cols = numeric_cols\n        self.label_cols = label_cols\n        self.cats={}\n    \n    def transformXy(self, df):\n        \"\"\"\n        Preprocess and return X,y\n        \"\"\"\n        df = df.copy()\n        # Scale, encode etc. features\n        X = self.transform(df)\n        # Scale labels\n        df[self.label_cols] = self.y_scaler.transform(df[self.label_cols])\n        y = df[self.label_cols]\n        return(X,y)\n    \n    def transform(self, df):\n        \"\"\"\n        Preprocess and return X\n        \"\"\"\n        # Add day, week, year\n        df = self.prepare_time_cols(df)\n        # Fill nans\n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        # Preprocess categorical features\n        for col in cat_cols:\n            df[col] = df[col].apply(lambda cat_name: self.prepare_cat_cols(cat_name, col))\n        # Scale numeric features and labels\n        df[self.numeric_cols+self.time_cols] = self.numeric_scaler.transform(df[self.numeric_cols+self.time_cols])\n        # Return X\n        return df[self.feature_cols]\n    \n    def fit(self, df):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df\n        To be called on train df only\n        \"\"\"\n        # Extract day, week, year from time\n        df = df.copy()\n        df = self.prepare_time_cols(df)\n        # Handle strange cases, impute etc.\n        df = self.prepare_train_df(df)\n        # Use QuantileTransformer to handle outliers\n        # Fit for labels\n        self.y_scaler = QuantileTransformer()\n        self.y_scaler.fit(df[self.label_cols])\n        # Fit for numeric and time\n        self.numeric_scaler = QuantileTransformer()\n        self.numeric_scaler.fit(df[self.numeric_cols + self.time_cols])\n        # Fit for categories\n        # Organize dictionary, each category column has list with values\n        self.cats=dict()\n        for col in cat_cols:\n            self.cats[col] = list(df[col].unique())\n\n    def prepare_train_df(self, train_df):\n        \"\"\"\n        Clean na, remove strange cases.\n        For train dataset only. \n        \"\"\"\n        # Handle nans\n        train_df = train_df.copy()\n        # Need better imputer\n        # for col in numeric_cols:\n        #     market_train_df[col] = market_train_df[col].fillna(market_train_df[col].mean())\n        train_df.tail()\n        train_df[self.numeric_cols] = train_df[self.numeric_cols].fillna(0)\n\n        # # Remove strange cases with close/open ratio > 2\n        max_ratio  = 2\n        train_df = train_df[np.abs(train_df['close'] / train_df['open']) <= max_ratio]\n        return(train_df)\n\n    def prepare_time_cols(self, df):\n        \"\"\" \n        Extract time parts, they are important for time series \n        \"\"\"\n        df = df.copy()\n        df['year'] = df['time'].dt.year\n        # Maybe remove month because week of year can handle the same info\n        df['day'] = df['time'].dt.day\n        # Week of year\n        df['week'] = df['time'].dt.week\n        df['dayofweek'] = df['time'].dt.dayofweek\n        return(df)\n        \n    def prepare_cat_cols(self, cat_name, col):\n        \"\"\"\n        Encode categorical features to numbers\n        \"\"\"\n        try:\n            # Transform to index of name in stored names list\n            index_value = self.cats[col].index(cat_name)\n        except ValueError:\n            # If new value, add it to the list and return new index\n            self.cats[col].append(cat_name)\n            index_value = len(self.cats[col])\n        index_value = 1.0/(index_value+1.0)\n        return(index_value)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39be518ea7b819e5898f9f5aa1b5cb156373f99b"},"cell_type":"code","source":"# Preprocess and split to X_train, X_val, X_test, y_train ...\nprepro = Prepro(feature_cols, cat_cols, time_cols, numeric_cols, label_cols)\nprepro.fit(market_train_df)\n\n# Clean train df,handle strange cases\nmarket_train_df = prepro.prepare_train_df(market_train_df)\n\nX_train, y_train = prepro.transformXy(market_train_df)\nX_val, y_val = prepro.transformXy(market_val_df)\nX_test, y_test = prepro.transformXy(market_test_df)\n\n# Display for visual check. \npd.concat([X_train,y_train], axis=1).describe()\n#X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3b7fafdd69852eac0ad94501adfe87ef4a48e7b"},"cell_type":"markdown","source":"# 5. Base model\nNN with Dense layers.\n"},{"metadata":{"_uuid":"9ae61753db796dbbbcbbce536ade22008c7e33e4"},"cell_type":"markdown","source":"## Create the model"},{"metadata":{"trusted":true,"_uuid":"7eae329a820e1614958e031fccc9e47c065d4abd"},"cell_type":"code","source":"# Initialize the constructor\nmodel = Sequential()\n\n# Add an input layer \ninput_size = X_train.shape[1]\n\n# Add layers - no worries which are.\n# ToDo: find a good architecture of NN\nmodel.add(Dense(256, input_shape=(input_size,), kernel_initializer='glorot_normal'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\n# Add an output layer \nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a06ff01bdad074a86dce8a3ef685f15eac017f4c"},"cell_type":"markdown","source":"## Train market model"},{"metadata":{"trusted":true,"_uuid":"e4e5814f7f8cbd1591c99e7580d16072ebf93961","_kg_hide-output":false},"cell_type":"code","source":"weights_file='best_weights.h5'\n\n# We'll stop training if no improvement after some epochs\nearlystopper = EarlyStopping(patience=5, verbose=1)\n\n# Low, avg and high scor training will be saved here\n# Save the best model during the traning\ncheckpointer = ModelCheckpoint(weights_file\n    ,verbose=1\n    ,save_best_only=True\n    ,save_weights_only=True)\n\nreduce_lr = ReduceLROnPlateau(factor=0.2,\n                              patience=5, min_lr=0.001)\n\n# Train\ntraining = model.fit(X_train,y_train\n                                ,batch_size=512\n                                ,epochs=100\n                                ,validation_data=[X_val, y_val]\n                                #,steps_per_epoch=100\n                                 #, validation_steps=100\n                                ,callbacks=[earlystopper, checkpointer, reduce_lr])\n# Load best weights saved\nmodel.load_weights(weights_file)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8364d16fd63e4ef983573c9f4237e02ad0d004e9"},"cell_type":"markdown","source":"## Evaluate market model\n\n### Loss function by epoch "},{"metadata":{"trusted":true,"_uuid":"cc30491137c941043e2bd82f739c4147e5c1569f"},"cell_type":"code","source":"# f, axs = plt.subplots(1,2, figsize=(12,4))\n# axs[0].plot(training.history['loss'])\n# axs[0].set_xlabel(\"Epoch\")\n# axs[0].set_ylabel(\"Loss\")\n# axs[0].set_title(\"Loss\")\n# axs[1].plot(training.history['val_loss'])\n# axs[1].set_xlabel(\"Epoch\")\n# axs[1].set_ylabel(\"val_loss\")\n# axs[1].set_title(\"Validation loss\")\n# plt.tight_layout()\n# plt.show()\nplt.plot(training.history['loss'])\nplt.plot(training.history['val_loss'])\nplt.title(\"Loss and validation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Value\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"386c1f3461d9b9b3df18856eceb53dd2ee495579"},"cell_type":"markdown","source":"### Predict on test data"},{"metadata":{"trusted":true,"_uuid":"5f843bc7b234b61837de390722af3f191737c483"},"cell_type":"code","source":"pred_size=100\nX_test2 = X_test.values[:pred_size]\ny_pred2 = model.predict(X_test2) [:,0]*2-1\ny_test2 = y_test.values[:pred_size][:,0]*2-1\n\nax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\nax1.plot(y_test2, linestyle='none', marker='.', color='darkblue')\nax1.plot(y_pred2, linestyle='none', marker='.', color='darkred')\nax1.legend([\"Ground truth\",\"Predicted\"])\nax1.set_title(\"Both\")\nax1.set_xlabel(\"Epoch\")\nax2 = plt.subplot2grid((2, 2), (0, 1), colspan=1,rowspan=1)\nax2.plot(y_test2, linestyle='none', marker='.', color='darkblue')\nax2.set_title(\"Ground truth\")\nax3 = plt.subplot2grid((2, 2), (1, 1), colspan=1,rowspan=1)\nax3.plot(y_pred2, linestyle='none', marker='.', color='darkred')\nax3.set_title(\"Predicted\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e34e3ee9448fcbc131830b3277ee830fa5e6ec1b"},"cell_type":"markdown","source":"### Predict on random asset"},{"metadata":{"trusted":true,"_uuid":"180acea47aa0580448e8790bf55b64d8e202013b"},"cell_type":"code","source":"def predict_random_asset():\n    \"\"\"\n    Get random asset from test set, predict on it, plot ground truth and predicted value\n    \"\"\"\n    # Get any asset\n    ass = market_test_df.assetName.sample(1, random_state=24).iloc[0]\n    test_ass_df = market_test_df[market_test_df['assetName'] == ass]\n    # Preprocess\n    X,y = prepro.transformXy(test_ass_df)\n    y.index = test_ass_df.time\n    # Predict\n    pred = pd.DataFrame(model.predict(X)*2 -1)\n    pred.index = test_ass_df.time\n    # Plot\n    plt.plot(y*2-1, linestyle='none', marker='.', color='darkblue')\n    plt.plot(pred, linestyle='none', marker='.', color='orange')\n    plt.title(ass)\n    plt.legend([\"Ground truth\", \"predicted\"])\n    plt.show()\n    \npredict_random_asset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46dd65f9bf556765fc0447e7f57abbbc629d1deb","scrolled":true},"cell_type":"code","source":"# Accuracy metric\nconfidence = model.predict(X_test)*2 -1\nprint(\"Accuracy: %f\" % accuracy_score(y_test > 0, confidence > 0))\n\n# Show distribution of confidence that will be used as submission\nplt.hist(confidence, bins='auto')\nplt.xlabel(\"Confidence\")\nplt.ylabel(\"Count\")\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1089c3423417fe6d568afa4a9ca6d57dd5f449d"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"43a3295c06c520b525542183b1049d0a593e0d26"},"cell_type":"code","source":"def make_predictions(market_obs_df, news_obs_df, predictions_template_df):\n    \"\"\"\n    Predict confidence for one day and update predictions_template_df['confidenceValue']\n    @param market_obs_df: market_obs_df returned from env\n    @param predictions_template_df: predictions_template_df returned from env.\n    @return: None. prediction_template_df updated instead. \n    \"\"\"\n    # Preprocess the data\n    market_obs_df = join_market_news(market_obs_df, news_obs_df)\n    X = prepro.transform(market_obs_df)\n    # Predict\n    y_pred = model.predict(X)\n    confidence_df=pd.DataFrame(y_pred*2-1, columns=['confidence'])\n\n    # Merge predicted confidence to predictions template\n    pred_df = pd.concat([predictions_template_df, confidence_df], axis=1).fillna(0)\n    predictions_template_df.confidenceValue = pred_df.confidence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79ed08e77aa14dfb7bd811d2187b0d2059a95052"},"cell_type":"code","source":"##########################\n# Submission code\n\n# Save data here for later debugging on it\ndays_saved_data = []\n\n# Store execution info for plotting later\npredicted_days=[]\npredicted_times=[]\nlast_predictions_template_df = None\n\n# Predict day by day\ndays = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    # Store the data for later debugging on it\n    days_saved_data.append((market_obs_df, news_obs_df, predictions_template_df))\n    # For later plotting\n    predicted_days.append(market_obs_df.iloc[0].time.strftime('%Y-%m-%d'))\n    time_start = time()\n\n    # Call prediction func\n    make_predictions(market_obs_df, news_obs_df, predictions_template_df)\n    #!!!\n    env.predict(predictions_template_df)\n    \n    # For later plotting\n    last_predictions_template_df = predictions_template_df\n    predicted_times.append(time()-time_start)\n    #print(\"Prediction completed for \", predicted_days[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38ce296e50b8c15c4dde4c4626e489c474b39179"},"cell_type":"code","source":"# Plot execution time \nsns.barplot(np.array(predicted_days), np.array(predicted_times))\nplt.title(\"Execution time per day\")\nplt.xlabel(\"Day\")\nplt.ylabel(\"Execution time, seconds\")\nplt.show()\n\n# Plot predicted confidence for last day\nlast_predictions_template_df.plot(linestyle='none', marker='.')\nplt.title(\"Predicted confidence for last observed day: %s\" % predicted_days[-1])\nplt.xlabel(\"Observation No.\")\nplt.ylabel(\"Confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcbe9c35e83e45f890d2450942fca4a5819b40d5"},"cell_type":"code","source":"# We've got a submission file!\n# !!! Write submission after all days are predicted\nenv.write_submission_file()\nprint([filename for filename in os.listdir('.') if '.csv' in filename])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"040f1eba8b1f0fe7fc7ebb31a400269503af3427"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}