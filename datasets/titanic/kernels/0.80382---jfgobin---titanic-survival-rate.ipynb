{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9c8137c4-c24c-9e9b-4a15-f034702cb10a"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is my first attempt at a Kaggle Competition. This work is inspired by Megan Risdal's script ([here][1]) and Ben Hammer's intro to Random Forest Benchmark in R ([here][2]).\n",
        "\n",
        "Many shouts and tips of the hat to the both of them for the scripts and notebooks, from which I learned a lot.\n",
        "\n",
        "# Missing data\n",
        "\n",
        "The dataset is not complete: for example, some passengers lacks the cabin number, which includes the deck letter. This could have been useful: the G-deck (2nd and 3rd classes) was below the water level ([Plans][3]). It is less likely that people asleep at that level would have had the time to leave their cabins and rush to the Boat deck, 20 meters above them, where the lifeboats were in time to get in a lifeboat - one of the main contributors to the tragedy was the lack of enough lifeboats compared to the number of passengers on board.\n",
        "\n",
        "Some of the values, such as the *Cabin* field won't be reconstructed. The *Embark* and *Age* fields will be reconstructed in the training set then in the test set as well.\n",
        "\n",
        "## Fixing *Embarked*\n",
        "\n",
        "Megan uses the median value to determine the port of origin, that is she computes for each class the distribution of the fare paid from each port and assigns the port that is the likeliest to be the origin, that is whose median fare is the closest to the fare paid by the traveler for his or her Pclass. As there are only two values missing in the training set and none in the test set, I won't replicate her code and simply fill in the blanks.\n",
        "\n",
        "## Fixing *Age*\n",
        "\n",
        "For this, Megan opted to use the Multiple Imputations using Chained Equations (MICE). This algorithm is provided by the Python package **fancyimpute**. However, after a few tries and some frustrations with the (in)famous \"isnan\" message, I decided to write something myself. That's not elegant but that does the job.\n",
        "\n",
        "Basically, for the each *Pclass*, *Sex* and *Title* variables, I select at random an age in the existing distribution and reassign it. As the imputation is random, there is a fairly large chance that results will change from run to run. A potential workaround is to run this a few times and select the most frequent result for the predicted *Survived* variable.\n",
        "\n",
        "## Fixing *Fare*\n",
        "\n",
        "Only one value is missing, so I took the median for people in a relatively similar situation.\n",
        "\n",
        "# Trip of the Titanic\n",
        "\n",
        "The Titanic was bound for New York, the Titanic left England from Southampton, docked in Cherbourg (France) and the next day in Queenstown (Ireland, nowadays Cobh). It sank five days in its trip, about two days away from its destination.\n",
        "\n",
        "While several questions will always remain unanswered, it is certain that a conjunction of factors led to the tragedy: the old mentality of \"having lifeboats to carry passengers from a sinking boat to a rescuing boat\", the \n",
        "\n",
        "  [1]: https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic\n",
        "  [2]: https://www.kaggle.com/benhamner/titanic/random-forest-benchmark-r/code\n",
        "  [3]: http://titanicwhitestar.e-monsite.com/pages/les-plans-les-ponts-et-interieurs-du-rms-titanic.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7d259ce2-21df-1e6f-e5d2-c89198b6c049"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np                # linear algebra\n",
        "import pandas as pd               # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "#import fancyimpute as fi         # MICE completions\n",
        "import re                         # Regular expressions\n",
        "import matplotlib.pyplot as plt   # Plot various stuff\n",
        "import sklearn                    # Learning algorithm\n",
        "import sklearn.tree               # Decision trees\n",
        "import sklearn.preprocessing      # Preprocessors\n",
        "import sklearn.neighbors          # k-Nearest Neighbours\n",
        "import sklearn.naive_bayes        # Naive Bayes\n",
        "import sklearn.svm                # C-Support SVM\n",
        "import sklearn.linear_model       # Logistic regression\n",
        "import mlxtend.classifier         # Stacking Classifier\n",
        "import sklearn.feature_extraction # Extract features through DictVectorizer\n",
        "\n",
        "# Load the files\n",
        "\n",
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "\n",
        "# The columns are\n",
        "# PassengerID  : The passenger's ID\n",
        "# Survived     : 1 if the passenger survived, 0 otherwise\n",
        "# Pclass       : Passenger class, which can be considered a proxy for social status\n",
        "# Name         : Passenger's name\n",
        "# Sex          : Passenger's sex\n",
        "# Age          : Passenger's age\n",
        "# SibSp        : Number of siblings or spouses aboard\n",
        "# Parch        : Number of parents or children aboard\n",
        "# Ticket       : Ticket number\n",
        "# Fare         : Fare paid, another variable to indicate social status\n",
        "# Cabin        : Denomination of the cabin\n",
        "# Embarked     : Port of embarkation\n",
        "\n",
        "# Some of the data is missing, for example\n",
        "\n",
        "# 10\t1\t2\tNasser, Mrs. Nicholas (Adele Achem)\tfemale\t14\t1\t0\t237736\t30.0708\t\tC\n",
        "# Which lacks the cabin.\n",
        "\n",
        "# Which data is complete and useful\n",
        "# PassengerID, unlikely to be useful (this is a unique ID), but needed to uniquely identify a passenger\n",
        "# Survived, which is what we need to predict\n",
        "# Pclass, very useful (Survival rate 1st class is +/-63%, 3rd class is about 24%)\n",
        "# Name, could be useful for the last name as it will group the families together\n",
        "# Sex, very useful as this was still a time when \"children and ladies first\" was applied during naufrage\n",
        "# Age, would be useful but a 177 rows are missing. Maybe we can reconstruct this.\n",
        "# SibSp, useful: it shows that single people (SibSp==0) had less chance of survival than couples \n",
        "#                or parent-children (53%), but then that the higher the SibSp, the lower the survival rate\n",
        "# Parch, useful, similar behaviour as SibSp\n",
        "# Ticket, could be useful, but need some cleanup. Normally numeric but some have characters attached\n",
        "# Fare, likely to be useful\n",
        "# Cabin, could have been useful, but there are too many values missing in the training set. Instead the\n",
        "# Pclass will be used.\n",
        "# Embarked, useful.\n",
        "\n",
        "# Augmentation of the dataset\n",
        "# From the data provided we will derive\n",
        "# LastName   - the element before the \",\" in the Name field\n",
        "# TicketNum  - the numeric part of the ticket\n",
        "\n",
        "# Prepare the dataset\n",
        "# Separation of the last name from the name\n",
        "train['LastName'] = train['Name'].apply(lambda x: x.split(',')[0])\n",
        "test['LastName'] = test['Name'].apply(lambda x: x.split(',')[0])\n",
        "# Cleaning of the ticket value\n",
        "train['TicketNum'] = train['Ticket'].apply(lambda x: x[x.find(' ')+1:])\n",
        "test['TicketNum'] = test['Ticket'].apply(lambda x: x[x.find(' ')+1:])\n",
        "# Separate the title from the rest, children were often called \"Miss\" or \"Master\" at the time\n",
        "title_re = re.compile(\"^.[^,]+, ([^ ]+) .*$\")\n",
        "train['Title'] = train['Name'].apply(lambda x: title_re.search(x).group(1))\n",
        "test['Title'] = test['Name'].apply(lambda x: title_re.search(x).group(1))\n",
        "\n",
        "# Check if we have missing data\n",
        "print(\"=== BEFORE IMPUTERS ===\\n\")\n",
        "print(\"=== Check for missing values in training set ===\")\n",
        "for iname in train.columns.values.tolist():\n",
        "    nna = sum(pd.isnull(train[iname]))\n",
        "    print(repr(iname).rjust(16), repr(nna).rjust(4))\n",
        "    \n",
        "print(\"\\n\\n=== Check for missing values in test set ===\")\n",
        "for iname in test.columns.values.tolist():\n",
        "    nna = sum(pd.isnull(test[iname]))\n",
        "    print(repr(iname).rjust(16), repr(nna).rjust(4))\n",
        "\n",
        "# Complete / impute the datasets\n",
        "\n",
        "# In the training set, two rows miss the Embarked value, and 177 the Age value. In the test set, 86\n",
        "# miss the Age value and 1 the Fare value.\n",
        "# Megan determined that the two missing Embarked are likely to be \"C\" (Cherbourg), which we will\n",
        "# simply fill. If this is against the rule, I will simply port the algorithm.\n",
        "train.loc[[61,829], 'Embarked']='C'\n",
        "\n",
        "# Fill the missing ages\n",
        "# For the missing ages, we will consider that the following variables are representative of the age:\n",
        "# Pclass, Sex, Title\n",
        "# Name - unlikely to be helpful as this is likely to be almost unique\n",
        "# LastName - unlikely to be helpful\n",
        "# SibSp/Parch - could be useful, but need a lot of work\n",
        "# Ticket - unlikely to be useful, likely to be unique\n",
        "# Fare - unlikely to be useful\n",
        "# Cabin - could have been useful if there wasn't so many missing values\n",
        "# Embarked - could have been useful, but this chops the dataset into too many fragments\n",
        "# For each useful variable, I will iterate through the possible values (They are all categoricals)\n",
        "# and if there are any missing value:\n",
        "# Find the min, the max and select a value at random between these.\n",
        "# Now comes the difficult choice of what distribution to use\n",
        "# Uniform? T-Student? Gamma? Something else?\n",
        "# Major drawback - as there is an element of randomness, the behavior can change from run to run\n",
        "# Some data\n",
        "# In the training set\n",
        "# Pclass  Title   Sex   Number NA  Number Entries  Percentage NA  Age Min  Age Max  Age Median\n",
        "#      3  Mr.     male         90             229          39.30       11       74          26\n",
        "#      3  Mrs.    female        9              33          27.27       15       63          31\n",
        "#      3  Miss.   female       33              69          47.83        0       45          18\n",
        "#      3  Master. male          4              24          16.67        0       12           4\n",
        "#      1  Mr.     male         20              87          22.99       17       80          40\n",
        "#      1  Mrs.    female        8              34          23.53       17       62          41.5\n",
        "#      1  Miss.   female        1              45           2.22        2       63          30\n",
        "#      1  Dr.     male          1               3          33.33       32       50          44\n",
        "#      2  Mr.     male          9              82          10.98       16       70          31\n",
        "#      2  Miss.   female        2              32           6.25        2       50          24\n",
        "# The most problematic imputation will be 3rd Class/Female/Miss which have almost half of \n",
        "# the entries missing the age. It seems the term \"Miss\" was used in a variety of circumstances across\n",
        "# the three classes. Was that intentional or by mistake?\n",
        "def age_imputer(df_to_imp, set_name, doplot=False):\n",
        "    for a1 in df_to_imp['Pclass'].unique():\n",
        "        for a2 in df_to_imp['Title'].unique():\n",
        "            for a3 in df_to_imp['Sex'].unique():\n",
        "                #query = {'Pclass': [a1],\n",
        "                #         'Sex': [a3],\n",
        "                #         'Title': [a2]}\n",
        "                #mask = train[['Pclass','Title','Sex']].isin(query).all(1)\n",
        "                v1 = df_to_imp[(df_to_imp.Pclass==a1)&(df_to_imp.Title==a2)&(df_to_imp.Sex==a3)]['Age'].count()\n",
        "                v2 = sum(np.isnan(df_to_imp[(df_to_imp.Pclass==a1)&(df_to_imp.Title==a2)&\\\n",
        "                                            (df_to_imp.Sex==a3)]['Age']))\n",
        "                if (v1 > 0) and (v2 > 0):\n",
        "                    age_dist = np.array(df_to_imp[(df_to_imp.Pclass==a1)&(df_to_imp.Title==a2)&\\\n",
        "                                                  (df_to_imp.Sex==a3)]['Age'])\n",
        "                    age_dist = age_dist[np.logical_not(np.isnan(age_dist))]\n",
        "                    age_dist_before = age_dist.copy()\n",
        "                    # To make it simple - if will select as many elements as I have NAs to replace\n",
        "                    # in the list of existing values with respect to distribution. At least the distribution\n",
        "                    # is respected\n",
        "                    new_age = np.random.choice(age_dist,v2,replace=True)\n",
        "                    # Impute the missing values\n",
        "                    df_to_imp.loc[(df_to_imp.Pclass==a1)&(df_to_imp.Title==a2)&(df_to_imp.Sex==a3)&\\\n",
        "                                  (pd.isnull(df_to_imp.Age)),'Age'] = new_age\n",
        "                    if doplot:\n",
        "                        # Plot distributions (old and new)\n",
        "                        plt.subplot(2,1,1)\n",
        "                        plt.title((\"Distribution for Pclass: {0:d}, Sex: {1}, Title: {2}\\n\" +\\\n",
        "                                   \"(imputed: {3:d} value{4})\\n(Set name: {5})\").format(\n",
        "                                  a1, a3, a2, v2, 's' if (v2>1) else '', set_name))\n",
        "                        plt.hist(age_dist_before, normed=True)\n",
        "                        plt.subplot(2,1,2)\n",
        "                        plt.hist(df_to_imp[(df_to_imp.Pclass==a1)&(df_to_imp.Title==a2)&\\\n",
        "                                           (df_to_imp.Sex==a3)]['Age'], normed=True)\n",
        "                        plt.show()\n",
        "\n",
        "age_imputer(train, \"Train\")\n",
        "age_imputer(test, \"Test\")\n",
        "\n",
        "\n",
        "print(\"=== AFTER IMPUTERS ===\\n\")\n",
        "print(\"=== Check for missing values in training set ===\")\n",
        "for iname in train.columns.values.tolist():\n",
        "    nna = sum(pd.isnull(train[iname]))\n",
        "    print(repr(iname).rjust(16), repr(nna).rjust(4))\n",
        "    \n",
        "print(\"\\n\\n=== Check for missing values in test set ===\")\n",
        "for iname in test.columns.values.tolist():\n",
        "    nna = sum(pd.isnull(test[iname]))\n",
        "    print(repr(iname).rjust(16), repr(nna).rjust(4))\n",
        "\n",
        "# At this point, there is still a row with age == NULL\n",
        "# She is the only \"Ms.\" \"female\" \"3rd Pclass\"\n",
        "# Given that she is a single lady (Parch==SibSp==0),  let's assign her\n",
        "# The median age of the single ladies after the coming of age\n",
        "\n",
        "test.loc[(test.PassengerId==980), 'Age'] = np.median(test.loc[(test.Pclass==3)&\\\n",
        "                                                              (test.Sex=='female')&\\\n",
        "                                                              (test.Parch==0)&\\\n",
        "                                                              (test.SibSp==0)&\\\n",
        "                                                              (test.Age>17),'Age'])\n",
        "\n",
        "# At this point, we are done with completing Age. Let's take care of the fare.\n",
        "# Fare is pretty easy - only one value is missing in the test dataset.\n",
        "# He is in 3rd Pclass, left from Southampton.\n",
        "# Let's assign him the median of the tickets of childless single men who embarked in Southampton\n",
        "# in third class over 40 years old.\n",
        "\n",
        "test.loc[(test.PassengerId==1044),'Fare'] = test.loc[(test.Pclass==3)&\\\n",
        "                                                     (test.Embarked=='S')&\\\n",
        "                                                     (test.Parch==0)&\\\n",
        "                                                     (test.SibSp==0)&\\\n",
        "                                                     (test.Age>40)&\\\n",
        "                                                     -(pd.isnull(test.Fare)),'Fare'].median()\n",
        "\n",
        "# Some of the entries have a Fare of 0.0. I do not know if this is normal or not, and in doubt\n",
        "# I will leave them that way. One possible explanation is they were part of the personnel \n",
        "# of either White Star Line or Harland and Wolff.\n",
        "\n",
        "# Well, that is it. The datasets are relatively complete, imputed where a value was missing \n",
        "# so we are ready to rock.\n",
        "\n",
        "# Initially, I had written my own stuff to rank the classifiers, but sklearn has\n",
        "# its own procedures.\n",
        "train_set = train.copy()\n",
        "# Keep only the relevant variables\n",
        "train_set_data = train_set[['Pclass', \n",
        "                            'Sex',\n",
        "                            'Age',\n",
        "#                            'SibSp',\n",
        "#                            'Parch',\n",
        "                            'Fare']].copy()\n",
        "train_set_target = train_set['Survived'].copy()\n",
        "# Classifiers does not use categorical but need labels\n",
        "le = sklearn.preprocessing.LabelEncoder()\n",
        "for i_label in ['Sex']:\n",
        "    train_set_data.loc[:,i_label] = le.fit_transform(train_set_data.loc[:,i_label])\n",
        "# Dict vectorizer is nice, but that does not guarantee I will have the some feature for both sets\n",
        "#dv = sklearn.feature_extraction.DictVectorizer(sparse=False)\n",
        "#train_set_data_dv = dv.fit_transform(train_set_data.to_dict(orient='records'))\n",
        "# Select and prepare the classifiers\n",
        "# Decision tree\n",
        "clf_dt = sklearn.tree.DecisionTreeClassifier()\n",
        "# k-nearest neighbours (n=5, default)\n",
        "clf_knn = sklearn.neighbors.KNeighborsClassifier()\n",
        "# k-nearest neighbours (n=3)\n",
        "clf_knn3 = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "# Naive Bayes\n",
        "clf_nb = sklearn.naive_bayes.GaussianNB()\n",
        "# SVM\n",
        "clf_svc = sklearn.svm.SVC()\n",
        "# Stacking classifier\n",
        "lr = sklearn.linear_model.LogisticRegression()\n",
        "clf_st = mlxtend.classifier.StackingClassifier(classifiers=[clf_dt,clf_knn,clf_nb,clf_svc], \n",
        "                                               meta_classifier=lr)\n",
        "# Look at the scores\n",
        "print(\"\\n\\n\")\n",
        "print(\"3-fold cross validation\")\n",
        "print(\"=======================\\n\")\n",
        "for clf,clf_name in zip([clf_dt, clf_knn, clf_knn3, clf_nb, clf_svc, clf_st],\n",
        "                        ['Decision Tree', 'k-Nearest Neighbors (5)',\n",
        "                         'k-Nearest Neighbors (3)', 'Naive Bayes',\n",
        "                         'Stacking Classifier (lr)']):\n",
        "    scores = sklearn.model_selection.cross_val_score(clf,\n",
        "                                                     train_set_data,\n",
        "                                                     train_set_target,\n",
        "                                                     cv=3, \n",
        "                                                     scoring='accuracy')\n",
        "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
        "          % (scores.mean(), scores.std(), clf_name))\n",
        "\n",
        "# Interestingly, Naive Bayes outperforms everybody else\n",
        "# Still, I choose the stacked classifier\n",
        "clf_ch = clf_st\n",
        "# Prep the test set to retain the features\n",
        "test_set = test.copy()\n",
        "test_set_data = test_set[['Pclass', \n",
        "                          'Sex',\n",
        "                          'Age',\n",
        "#                          'SibSp',\n",
        "#                          'Parch',\n",
        "                          'Fare']].copy()\n",
        "#test_set_data_dv = dv.fit_transform(test_set_data.to_dict(orient='records'))\n",
        "for i_label in ['Sex']:\n",
        "    test_set_data.loc[:,i_label] = le.fit_transform(test_set_data.loc[:,i_label])\n",
        "# And use the model\n",
        "clf_ch = clf_ch.fit(train_set_data,train_set_target)\n",
        "test_set['Survived'] = clf_ch.predict(test_set_data)\n",
        "# As a final verification, check the survival rates\n",
        "\n",
        "train_sv_rate = (100.0*sum(train['Survived'])) / (1.0*len(train))\n",
        "test_sv_rate = (100.0*sum(test_set['Survived'])) / (1.0*len(test_set))\n",
        "\n",
        "print(\"Survival rates\")\n",
        "print(\"==============\\n\")\n",
        "print(\"Train set: %2.2f\"%(train_sv_rate))\n",
        "print(\"Test set:  %2.2f\"%(test_sv_rate))\n",
        "\n",
        "submission = pd.DataFrame({ 'PassengerId': test_set.PassengerId,\n",
        "                            'Survived': test_set.Survived})\n",
        "submission.to_csv(\"titanic_prediction.csv\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3726d145-6993-8b63-f1a4-b6da9f1f3e2f"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}