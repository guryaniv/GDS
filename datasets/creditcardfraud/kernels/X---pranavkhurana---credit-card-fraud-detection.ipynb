{"cells":[{"metadata":{"_cell_guid":"cb26ecbd-0864-41cf-acbc-b8031c8c980e","_uuid":"3a0f779fde2f9a3118febb2700e374d4f20053f6"},"cell_type":"markdown","source":"# **CREDIT CARD FRAUD DETECTION** \n# (Using Gaussian Distribution Model and Logistic Regression)"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"77d1f262-a7d9-4919-ad57-5cac3f5f6c0a","_uuid":"67e324d8c2fd5a2b7a08c3f139eddf1adb7c80b6"},"cell_type":"markdown","source":"First lets import the credit card dataset into our script-"},{"metadata":{"_cell_guid":"1f5b2258-1ce4-48cb-b71b-816d27cfb406","_uuid":"5d2d5aa0b048ebadc3bfc181465435733f7e2674","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcard.csv\")\nprint(\"Shape of input data: \"+str(data.shape))\ndata.head()","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Distribution of target class"},{"metadata":{"_cell_guid":"1a563622-c981-4a47-8d70-33f06f71d706","_uuid":"3c3f8f26bac2b5f564427da1f23d7cf090116cba"},"cell_type":"markdown","source":"Now lets have a look at the distribution of target class which signifies whether the transaction is actually fraudulent."},{"metadata":{"_cell_guid":"3dbae1cf-be66-4b60-919b-ea42f914cbad","_uuid":"f915058925390d3577a6b9de241ce9707e673272","trusted":true},"cell_type":"code","source":"count_classes = pd.value_counts(data['Class'])\ncount_classes.plot(kind = 'bar')\nplt.title(\"Distribution of target classes\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"fac6b0d1-fab7-48ff-8827-48d5e82a019a","_uuid":"89960ecb6711313ff205d58935e7411f7406df78"},"cell_type":"markdown","source":"**The data is clearly imbalanced (or skewed).**\n\nTo work with such data, certain things have to be kept in mind:"},{"metadata":{"_cell_guid":"9e929268-488b-44c2-9bb5-73f3c0a679ff","_uuid":"900ca83c84fa886869ddc4ec7c324fbe1c64baf9","collapsed":true},"cell_type":"markdown","source":"# Gaussian distribution Model"},{"metadata":{"_cell_guid":"65750179-448e-4f3d-9a59-a80ab68e641a","_uuid":"893353fc80c4ced4c54a1f911dcac6b370f8be64","collapsed":true,"trusted":true},"cell_type":"code","source":"features = ['V%d' % number for number in range(1, 29)]\ntarget = 'Class'","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"ede12a08-4017-438f-8a3c-aca9f5692e06","_uuid":"6fdc7b70a1e2dcee668935562fdf8194378f301b"},"cell_type":"markdown","source":" **Identifying which features are not much of help in the algorithm**\n (This piece of code is taken from [Shelars1985](https://www.kaggle.com/shelars1985) kernel on anomaly detection)"},{"metadata":{"_cell_guid":"5a7409b2-8caa-41f2-8b76-34730a7547ef","_uuid":"f753d99129e5f1bfecd567b3e61bfb3f1156eb8e","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns \nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nv_features = data.iloc[:,1:29].columns\n\nplt.figure(figsize=(12,8*4))\ngs = gridspec.GridSpec(7, 4)\nfor i, cn in enumerate(data[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(data[cn][data.Class == 1], bins=50)\n    sns.distplot(data[cn][data.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('feature: ' + str(cn))\nplt.show()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"a937730c-8754-4fa5-9ddc-bcbe1ccc0b16","_uuid":"f3b5e7fb3ad09aab9ab8d2e532661c1a94f0300f","trusted":true},"cell_type":"code","source":"# Removing features which  do not align well with the gaussian curve\n# Also amount and time are not used for fitting the model\nX = np.matrix(data[features].drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1))\ny = np.matrix(data[target])\nX.shape","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"b78f0e48-3054-4729-a471-842f2bff7f4c","_uuid":"93db1db055c2dba78d3aab171ab595d04da70a07"},"cell_type":"markdown","source":"Normalizing the data"},{"metadata":{"_cell_guid":"58218aef-863b-4a5e-8032-a99110d9ce9f","_uuid":"af1b73dbe88c869f957cecf84bcb39f033ec13b9","trusted":true},"cell_type":"code","source":"def normalize(X):\n    \"\"\"\n    Make the distribution of the values of each variable similar by subtracting the mean and by dividing by the standard deviation.\n    \"\"\"\n    X -= X.min(axis=0)\n    X /= (X.max(axis=0)-X.min(axis=0))\n\n    # for feature in X.columns:\n    #     X[feature] -= X[feature].mean()\n    #     X[feature] /= X[feature].std()\n\n    return X\n\nX = normalize(X)\nX","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"3b766dc2-c7ba-42ac-ba5c-ae5c2d10ce0b","_uuid":"dec27bca8a18873ffb88d24ac3be9c9b9999e251"},"cell_type":"markdown","source":"Splitting the data into Test and Train data"},{"metadata":{"_cell_guid":"d619f1c6-88b2-4111-9d4a-7757faf41841","_uuid":"a6f88f12330f695b6a9b11e36ea58441601f7160","collapsed":true,"trusted":true},"cell_type":"code","source":"test_break = 140000\n\nX_test = X[test_break:,:]\ny_test = y[:,test_break:]\n\nX = X[:test_break,:]\ny = y[:,:test_break]\n","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"4aeef555-8ce9-4161-ad94-dd637994aa0c","_uuid":"9ca45d46472b2b9947b81cad3279137d03b99252"},"cell_type":"markdown","source":"Finding mean and Covariance of the obtained matrix"},{"metadata":{"_cell_guid":"c24e471b-c0a4-42a7-8883-f04262b5ac34","_uuid":"cc74e4a622d8bd812ab0bde4d75b4070ec09d342","trusted":true},"cell_type":"code","source":"mu = X.mean(axis=0)\nmu = np.squeeze(np.asarray(mu))\n\ncov = np.cov(X,rowvar=0)\nprint (np.diag(cov))\n# print (cov)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"9f4e73b6-d165-4dc2-b09e-42a5f758bbda","_uuid":"3f5fe004105cffce7e1bb40fe4cd21b7ccf50017","trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import multivariate_normal\n\np= multivariate_normal.pdf(X, mean=mu, cov=1)\n\nprint (p.shape)\nprint (p)","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"934154d7-5a8f-4df8-af7e-449e59d92e74","_uuid":"b6785ce997d15a7513ccc6c6535a415163abfa81"},"cell_type":"markdown","source":"We now have probability distribution of the training set.\nNow we find threshold (or epsilon) such that we have a balanced precision and recall values (or maximum f1 value)."},{"metadata":{"_cell_guid":"36b073db-9444-4daa-b365-ecf3da407e24","_uuid":"1d67c6e9df6e6610a5e54691608f42d2237798b1","trusted":true},"cell_type":"code","source":"def select_threshold(pval, yval):  \n    \n    best_tp=0\n    best_fp=0\n    best_fn=0\n    best_epsilon = 0\n    best_f1 = 0\n    f1 = 0\n\n    step = (pval.max() - pval.min()) / 1000\n\n    for epsilon in np.arange(pval.min(), pval.max(), step):\n        preds = pval < epsilon\n#         print \"preds->\",preds\n#         print \"pval->\",pval\n#         print epsilon\n#         print yval.shape,preds.shape\n        tp = np.sum(np.logical_and(preds == 1, yval == 1)).astype(float)\n        fp = np.sum(np.logical_and(preds == 1, yval == 0)).astype(float)\n        fn = np.sum(np.logical_and(preds == 0, yval == 1)).astype(float)\n\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        f1 = (2.0 * precision * recall) / (precision + recall)\n\n#         print (\"epsilon=\",epsilon)\n#         print (tp,fp,fn)\n#         print (\"f1=\",f1)\n#         print (\"best f1=\",best_f1)\n#         print (\"besttpfp\",best_tp,best_fp,best_fn)\n        if f1 > best_f1:\n            best_tp=tp\n            best_fp=fp\n            best_fn=fn\n#             print \"besttpfp\",tp,fp,fn\n            best_f1 = f1\n            best_epsilon = epsilon\n\n    return best_epsilon, best_f1\n\nepsilon, f1 = select_threshold(p, y)\nprint (\"epsilon and f1(for training data)=\",epsilon, f1)\n","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"47a6d2bb-8b58-4741-9772-fe16f1205e93","_uuid":"a48439a8e8180e60681106a36d8d84e9165151aa"},"cell_type":"markdown","source":"Applying this threshold to the test dataset"},{"metadata":{"_cell_guid":"b5d75c92-2179-4721-8902-ae663ec1723b","_uuid":"984a17867c279eb1fe13eb7f2f66eede8949e914","trusted":true},"cell_type":"code","source":"# # Applying the threshold to the data set\ndef test(X,yval,epsilon):\n\n    pval = multivariate_normal.pdf(X, mean=mu, cov=1)\n    \n    f1 = 0\n    \n    preds = pval < epsilon\n    tp = np.sum(np.logical_and(preds == 1, yval == 1)).astype(float)\n    fp = np.sum(np.logical_and(preds == 1, yval == 0)).astype(float)\n    fn = np.sum(np.logical_and(preds == 0, yval == 1)).astype(float)\n\n#     print tp,fp,fn\n    precision = tp*1.0 / (tp + fp)\n    recall = tp / (tp + fn)\n    f1 = (2.0 * precision * recall) / (precision + recall)\n\n    yval.shape = (yval.shape[1],1)\n#     print(yval.shape,preds.shape)\n    \n    return f1,precision,preds,recall\n\nf1,precision,preds,recall = test(X_test,y_test,epsilon)\n\nprint ('test data results: f1=',f1,' recall = ',recall,' and precision =',precision)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"81eea274-c03e-45ae-b846-d333cc29d43f","_uuid":"f4be4c72c8c25d94e3c41e25029cfd47c100244a"},"cell_type":"markdown","source":"Plotting the confusion matrix for the gaussian model"},{"metadata":{"_cell_guid":"06ae88eb-af82-41cb-8393-23bd6fa42e17","_uuid":"3245ad523816ba1b53c15b65b1fa4991cbe6ba4a","collapsed":true,"trusted":true},"cell_type":"code","source":"#function for PLOTTING CONFUSION MATRIX\n\nimport itertools\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        1#print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"afa64400-f965-46b4-b568-91ae2d849a18","_uuid":"1f8d7cd5a6f47ef441ef516c10eb3d5f26f4ffce","trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test,preds)\n\nprint ('confusion matrix of test dataset = \\n',cnf_matrix)\n\nprint(classification_report(y_test, preds))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()\n","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"9e44e568-070e-4521-a6e6-9ff70c06c6c7","_uuid":"0b69db9cdc96630bac8bde317231c290db5b2db8"},"cell_type":"markdown","source":"# Logistic Regression Model"},{"metadata":{"_cell_guid":"49045ebf-7794-4e0a-aa12-d187a8239f73","_uuid":"f94aa3104f90cbb2d1cb2763e33a0f6a2e5a0918","trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import multivariate_normal\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nX = np.matrix(data[features])\ny = np.matrix(data[target])\n\ny=np.squeeze(np.asarray(y))\n","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"02246e35-5903-4cfb-adec-b99714514894","_uuid":"99aab395ffd7ae327118fd381de717c95cc3bb1b"},"cell_type":"markdown","source":"Normalizing the data"},{"metadata":{"_cell_guid":"28df7484-af82-4955-bc6c-3c2e16ea5929","_uuid":"118dd82febb58c57ab2ec95195b8f6c5f7c1a990","collapsed":true,"trusted":true},"cell_type":"code","source":"def normalize(X):\n\n    \"\"\"\n    Make the distribution of the values of each variable similar by subtracting the mean and by dividing by the standard deviation.\n    \"\"\"\n    X -= X.min(axis=0)\n    X /= (X.max(axis=0)-X.min(axis=0))\n\n    # for feature in X.columns:\n    #     X[feature] -= X[feature].mean()\n    #     X[feature] /= X[feature].std()\n\n    return X\n\n","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"1d754d4d-fca9-4ca7-9718-57220c53e213","_uuid":"b8b0d337d2eb43b5b5564b5834f6ae02fb0c1c45","collapsed":true,"trusted":true},"cell_type":"code","source":"# Define the model\nmodel = LogisticRegression()\n\n# Define the splitter for splitting the data in a train set and a test set\nsplitter = StratifiedShuffleSplit(y,n_iter=1, test_size=0.5, random_state=0)\n\n# Loop through the splits (only one)\nfor train_indices, test_indices in splitter:\n    # Select the train and test data\n    X_train, y_train = X[train_indices], y[train_indices]\n    X_test, y_test = X[test_indices], y[test_indices]\n    \n    # Normalize the data\n    X_train = normalize(X_train)\n    X_test = normalize(X_test)\n    \n    # Fit and predict!\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"d17a08ae-b8a5-4d06-879e-5ef5f6727777","_uuid":"0689a70e5afa41bc69484648a45acfe019e57250","trusted":true},"cell_type":"code","source":"# And finally: show the results\nprint(classification_report(y_test, y_pred))","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"3d10893b-9102-4a4d-9a1d-68f7e09e7665","_uuid":"94f541fbcc4b87dc8f4ad89212c2317570e20e79","trusted":true},"cell_type":"code","source":"cnf_matrix2 = confusion_matrix(y_test,y_pred)\n\nprint ('confusion matrix of test dataset = \\n',cnf_matrix2)\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix2\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"90a613b1-9530-4082-9882-4c653f3207a8","_uuid":"e0683061356a9da71504a128b0a8dbd1c2c4c8ae"},"cell_type":"markdown","source":"**Trying to plot partial dependence of various variables to gain insight about what features to use**"},{"metadata":{"_cell_guid":"9a472da2-fb1e-4991-a097-39f04fa90df9","_uuid":"b341d32cfc1ab3a7c1428b8f42ac23367a06faa7","trusted":true,"collapsed":true},"cell_type":"code","source":"# %matplotlib inline\n# from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n# from sklearn.ensemble import GradientBoostingRegressor\n# # get_some_data is defined in hidden cell above.\n\n# # scikit-learn originally implemented partial dependence plots only for Gradient Boosting models\n# # this was due to an implementation detail, and a future release will support all model types.\n# my_model = GradientBoostingRegressor()\n# # fit the model as usual\n# my_model.fit(X_train, y_train)\n\n# # Here we make the plot\n# my_plots = plot_partial_dependence(my_model,       \n#                                    features=[0, 1], # column numbers of plots we want to show\n#                                    X=X_train,            # raw predictors data.\n#                                    feature_names=['V1', 'V2'], # labels on graphs\n#                                    grid_resolution=10)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}