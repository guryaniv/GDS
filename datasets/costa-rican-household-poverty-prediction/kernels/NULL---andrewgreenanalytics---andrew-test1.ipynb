{"cells":[{"metadata":{"trusted":true,"_uuid":"5ad53ab7fa021bdc5f2169865cd7a9c6badcefe0"},"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Sep 15 12:27:35 2018\n\n@author: mollyking\n\"\"\"\n\n\n# coding: utf-8\n\n# In[30]:\n\n\n##############################################################\n####### Costa Rican Household Poverty Level Protection #######\n##############################################################\n\n#Code By\n#Andrew Green\n#Ksenia Luu\n#Molly King\n#Zach Densmore\n\n#importing packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport statsmodels.formula.api as smf  # R-like model specification\nimport statsmodels.api as sm\nimport numpy as np\nimport os\nimport pylab\n\nimport statsmodels as sm\nfrom sklearn import linear_model, metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Andrews path\nandrew = 'C:/Users/acgre/Desktop/Personal Work/Costa Rican Welfare/'\n#Ksenia's path\nksenia = 'put path on your machine here'\n#Molly's path\nmolly = '/Users/mollyking/Documents/Kaggle/'\n#Zach's path\nzach = 'put path on your machine here'\n\n\n#Set working directory\n#os.chdir(andrew)\n\n\n#Import datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nprint(train.columns)\n\n\n\n###################################################\n########### Global Functions  #####################\n###################################################\n\n### Function to make an ROC Curve\ndef roc(fpr, tpr):\n    roc_auc = metrics.auc(fpr,tpr)\n    print('Area under ROC Curve = {}'.format(roc_auc))\n\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n\n### Confusion Matrix\ndef plot_confusion_matrix(matrix):\n    plt.figure()\n    np.fill_diagonal(matrix, 0)\n    fig = plt.figure(figsize=(8,8))\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(matrix)\n    fig.colorbar(cax)\n    plt.title('Confusion Matrix')\n    plt.savefig('Confusion Matrix.pdf')\n    plt.savefig('Confusion Matrix')\n    plt.show()\n\n\n# %%\n\n###Per the Kaggle discussions, the same household can have different values.  Examine and correct:\nd={}\nweird=[]\nfor row in train.iterrows():\n    idhogar=row[1]['idhogar']\n    target=row[1]['Target']\n    if idhogar in d:\n        if d[idhogar]!=target:\n            weird.append(idhogar)\n    else:\n        d[idhogar]=target\n\n\n\nlen(set(weird))\n\n\n# In[32]:\n\n\n#Set record so the correct target value belonging to head of household is set each time\nfor i in set(weird):\n    hhold=train[train['idhogar']==i][['idhogar', 'parentesco1', 'Target']]\n    target=hhold[hhold['parentesco1']==1]['Target'].tolist()[0]\n    for row in hhold.iterrows():\n        idx=row[0]\n        if row[1]['parentesco1']!=1:\n            train.at[idx, 'Target']=target\n\n\n\n# In[33]:\n            \n##Initial Summary\nprint('')\nprint('----- Summary of Input Data -----')\nprint('')\n\n# show the object is a DataFrame\nprint('Object type: ', type(train))\n\n\n# show number of observations in the DataFrame\nprint('Number of observations: ', len(train))\n\n\nprint(train.describe())\nprint(test.describe())\n\n\nprint(train.shape)\nprint(test.shape)\n\n\n#Changes pandas settings so all columns are visible\npd.set_option('max_rows',200)\npd.set_option('max_columns',200)\n\n\ntrain.describe()\n\n\n#Training set variables missing data:\n#v2a1\n#v18q1\n#rez_esc\n#public\n#meaneduc\n#SQBmeaned\n\n\ntest.describe()\n\n\n#Test set variables missing data:\n#v2a1\n#v18q1\n#rez_esc\n#meaneduc\n#SQBmeaned\n\n\n# In[34]:\n\n\n###Univariate EDA Round One - Initial Scan Prior to Imputation\n\nprint('')\nprint('----- Initial Univariate EDA -----')\nprint('')\n\n\n#Histograms and Boxplots\ntrain.hist(figsize=(20,20))\ntrain.plot(kind= 'box', subplots=True, layout=(30,6),  figsize=(10,10))#sharex=False, sharey=False,\n\n\n# %%\n\n\n###Impute missing values and create missingness variables\n###Opted for medians for first round, but we can test means, etc.\n\nprint('')\nprint('----- Value Imputation -----')\nprint('')\n\n\n#v2a1\ntrain['IMP_v2a1'] = train.v2a1.fillna(train.v2a1.median())\ntrain['M_v2a1'] = train.v2a1.isnull().astype(int)\n\n\ntest['IMP_v2a1'] = test.v2a1.fillna(train.v2a1.median())\ntest['M_v2a1'] = test.v2a1.isnull().astype(int)\n\n\n#v18q1\ntrain['IMP_v18q1'] = train.v18q1.fillna(train.v18q1.median())\ntrain['M_v18q1'] = train.v18q1.isnull().astype(int)\n\n\ntest['IMP_v18q1'] = test.v18q1.fillna(train.v18q1.median())\ntest['M_v18q1'] = test.v18q1.isnull().astype(int)\n\n\n#rez_esc\ntrain['IMP_rez_esc'] = train.rez_esc.fillna(train.rez_esc.median())\ntrain['M_rez_esc'] = train.rez_esc.isnull().astype(int)\n\n\ntest['IMP_rez_esc'] = test.rez_esc.fillna(train.rez_esc.median())\ntest['M_rez_esc'] = test.rez_esc.isnull().astype(int)\n\n\n#meaneduc\ntrain['IMP_meaneduc'] = train.meaneduc.fillna(train.meaneduc.median())\ntrain['M_meaneduc'] = train.meaneduc.isnull().astype(int)\n\n\ntest['IMP_meaneduc'] = test.meaneduc.fillna(train.meaneduc.median())\ntest['M_meaneduc'] = test.meaneduc.isnull().astype(int)\n\n\n#SQBmeaned\ntrain['IMP_SQBmeaned'] = train.SQBmeaned.fillna(train.SQBmeaned.median())\ntrain['M_SQBmeaned'] = train.SQBmeaned.isnull().astype(int)\n\n\ntest['IMP_SQBmeaned'] = test.SQBmeaned.fillna(train.SQBmeaned.median())\ntest['M_SQBmeaned'] = test.SQBmeaned.isnull().astype(int)\n\n\n\n\n#Find all variables with \"No\" instead of a 0\ndef no_variables(dataset,col_names):\n    for col in col_names:\n        if dataset[col] == 'No':\n            return col\n\n\n#Replace variables containing \"No\" values with zeros and return updated values\ndef replace_values(no_variables):\n    dataset[col] = [0 if dataset[col]=='No' else dataset[col] for dataset[col] in dataset[col]]\n    return dataset[col]\n\n\n#Return functions\ncol_names = list(train)\n#no_variables(train,col_names)\n#replace_values(no_variables)\n\ncol_names = list(test)\n#no_variables(test,col_names)\n#replace_values(no_variables)\n\n\n#Convert all variable names to lower case\ntrain.columns = [s.lower() for s in train.columns]\ntest.columns = [s.lower() for s in test.columns]\n\n\n# %%\n\n\n###Univariate EDA Round Two - Post-Imputation Scan\n\n\nprint('')\nprint('----- Univariate EDA - Post-Imputation Scan-----')\nprint('')\n\n#Histograms and Boxplots Post-Imputation\ntrain.hist(figsize=(20,20))\ntrain.plot(kind= 'box', subplots=True, layout=(30,6),  figsize=(10,10))#sharex=False, sharey=False,\n\n\n\n###Response variable exploration\ntrain['target'].hist()\ntrain['target'].plot(kind='box')\n\n#sm.qqplot(train['target'], line='45')\n#pylab.show()\n\n\nlist(train)\n\n\n# In[36]:\n\n\n###Heatmap to explore response correlation\n\n\n#Age\ntrain2 = train[['target','r4h1','r4h2','r4h3','r4m1','r4m2','r4m3','r4t1','r4t2','r4t3','age']]\ncorr = train2.corr()\nsns.heatmap(corr,annot=True)\n\n\n\n\n# In[37]:\n\n\n#Home Ownership / Lack Thereof\ntrain2 = train[['target','tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5']]\ncorr = train2.corr()\nsns.heatmap(corr,annot=True)\n\n\n#Living Conditions - can add more here\ntrain2 = train[['target','rooms','hacdor','hacapo','overcrowding',]]\ncorr = train2.corr()\nsns.heatmap(corr,annot=True)\n\n\n#Household\ntrain2 = train[['target','tamhog','tamviv','hhsize','parentesco1','parentesco2','parentesco3','parentesco4','hogar_nin','hogar_adul','hogar_mayor','hogar_total','dependency']]\ncorr = train2.corr()\nsns.heatmap(corr,annot=True)\n\n\n#Education\ntrain2 = train[['target','escolari','rez_esc','hacapo','edjefe','edjefa','meaneduc']]\ncorr = train2.corr()\nsns.heatmap(corr,annot=True)\n\n\n#Possessions\ntrain2 = train[['target','refrig','v18q1','hacapo','computer','television','mobilephone','qmobilephone']]\ncorr = train2.corr()\nsns.heatmap(corr,annot=True)\n\n\n\n#Location\ntrain2 = train[['target','lugar1','lugar2','lugar3','lugar4','lugar5','lugar6','area1','area2']]\ncorr = train2.corr()\nsns.heatmap(corr,annot=True)\n\n\n\n#Social Conditions / Status\ntrain2 = train[['target','estadocivil1','estadocivil2','estadocivil3','estadocivil4','estadocivil5','estadocivil6','estadocivil7']]\ncorr = train2.corr()\nsns.heatmap(corr,annot=True)\n\n\n\n# In[46]:\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87d9b737ba54f216355807fac9871e8545549f23"},"cell_type":"code","source":"train=train.fillna(train.median())\ntest=test.fillna(train.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47429648630502f79a9b3d391a9c26cadbfb1cf0"},"cell_type":"code","source":"train.replace(('yes', 'no'), (1, 0), inplace=True)\ntest.replace(('yes', 'no'), (1, 0), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea6d96b5dacb52cfa5298a33f6f3f6a6da8829d5"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, f1_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors.nearest_centroid import NearestCentroid\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4b5d5d42ee599fc62d6c6322f638737bfcd6035"},"cell_type":"code","source":"y_train_final = pd.DataFrame(train[['target','idhogar']])\nmid_train = train.copy()\n\ndel mid_train['target']; del mid_train['idhogar']\n\nmid_train.reset_index(drop=True)\n\nnew_train = pd.concat([y_train_final,mid_train],axis = 1)\nprint(len(new_train))\nprint(len(y_train_final))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08fdbc2133fe3b975dfd7475a7a0f06a0152661e"},"cell_type":"code","source":"names = ['SGD'#,'SVC'\n         ,'Nearest Centroid'\n        ,'Random Forest','Extra Trees'\n        ,'Decision Tree','Gradient Booster 1.0','Gradient Booster 0.1','Gradient Booster Plain'\n        ,'Multi-layer Perceptron'] \n\nregressors = [SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n                            eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n                            learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n                            n_jobs=-1, penalty='l2', power_t=0.5, random_state=None,\n                            shuffle=True, tol=None, verbose=0, warm_start=False),\n              #SVC(),\n              NearestCentroid(metric='euclidean', shrink_threshold=None),\n              RandomForestClassifier(n_estimators=10, max_depth=None,\n                                     min_samples_split=2, random_state=0),\n              ExtraTreesClassifier(n_estimators=10, max_depth=None,\n                                   min_samples_split=2, random_state=0),\n              DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n                                     random_state=0),\n              GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n                                         max_depth=1, random_state=0),\n              GradientBoostingClassifier(n_estimators=100, learning_rate=.1,\n                                         max_depth=1, random_state=0),\n              GradientBoostingClassifier(),\n              MLPClassifier(solver='lbfgs', alpha=1e-5,\n                            hidden_layer_sizes=(5, 2), random_state=1),\n             LogisticRegression(multi_class='multinomial', \n                                solver='saga', \n                                verbose=1, \n                                n_jobs=-1)]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"aeaa18812983c78d593b93bee77fb33fcab83178"},"cell_type":"code","source":"model_data = new_train.copy()\n\n\nfrom sklearn.model_selection import KFold\n\nN_FOLDS = 5\nRANDOM_SEED = 1\n\ncv_f1_results = np.zeros((N_FOLDS, len(names)))\ncv_acc_results = np.zeros((N_FOLDS, len(names)))\n\nkf = KFold(n_splits = N_FOLDS, shuffle=False, random_state = RANDOM_SEED)\n\nindex_for_fold = 0  # fold count initialized \nfor train_index, test_index in kf.split(model_data):\n\n    X_train = model_data.iloc[train_index, 3:model_data.shape[1]]\n    X_test = model_data.iloc[test_index, 3:model_data.shape[1]]\n    y_train = model_data.iloc[train_index, 0]\n    y_test = model_data.iloc[test_index, 0]  \n\n    index_for_method = 0  # initialize\n    for name, reg_model in zip(names, regressors):\n\n        reg_model.fit(X_train, y_train)  # fit on the train set for this fold\n\n        # evaluate on the test set for this fold\n        y_test_predict = reg_model.predict(X_test)\n        fold_method_acc_result = accuracy_score(y_test, y_test_predict)\n        cv_acc_results[index_for_fold, index_for_method] = fold_method_acc_result\n        \n        fold_method_f1_result = f1_score(y_test, y_test_predict, average='macro')\n        cv_f1_results[index_for_fold, index_for_method] = fold_method_f1_result\n        index_for_method += 1\n  \n    index_for_fold += 1\n\ncv_acc_results_df = pd.DataFrame(cv_acc_results)\ncv_acc_results_df.columns = names\n\ncv_f1_results_df = pd.DataFrame(cv_f1_results)\ncv_f1_results_df.columns = names\n\nprint('\\n----------------------------------------------')\nprint('Average results from ', N_FOLDS, '-fold cross-validation\\n',\n      '\\nMethod                        Acc Score', sep = '')     \nprint(cv_acc_results_df.mean())    \n\nprint('\\n----------------------------------------------')\nprint('Average results from ', N_FOLDS, '-fold cross-validation\\n',\n      '\\nMethod                        F1 Score', sep = '')     \nprint(cv_f1_results_df.mean())   \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01e4eec6131a2e45d4fd1e44f7802e0a892ee626"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e7e8597012d1b2724cb2fb48d7d7a2feba4c111"},"cell_type":"code","source":"X_test = test.copy()\nX_test_id = X_test['id']\n\ndel X_test['id']; del X_test['idhogar']; del X_test['target']\nlen(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49a0e6599fe971520642b4bfb1b7433bc9525cdd"},"cell_type":"code","source":"del new_train['id']; del new_train['idhogar']; del new_train['target']\nprint(new_train.shape)\nprint(y_train_final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"856708144ee8784aa4cc460a40f30647e264ee81"},"cell_type":"code","source":"########### final predictions\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n                                         max_depth=1, random_state=0)\nclf.fit(new_train, y_train_final['target'])  # fit on the train set for this fold\n# evaluate on the test set for this fold\ny_test_prob = clf.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"766bfbbba5b03ed68be5583abb7736da3b46c6bf"},"cell_type":"code","source":"your_file = pd.concat([pd.DataFrame(X_test_id),pd.DataFrame(y_test_prob)],axis = 1, join_axes=[X_test_id.index])\nyour_file.columns = ['Id','Target']\nyour_file.index.drop\nyour_file.to_csv('costarica.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}