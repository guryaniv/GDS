{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "41148523-ae1b-e054-9198-9f3bec8ec953"
      },
      "source": [
        "## Credit Card Fraud Detection\n",
        "  \n",
        "#### Author : Rahul Choudhry\n",
        "  \n",
        "#### Description:  \n",
        "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
        "\n",
        "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
        "\n",
        "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\n",
        "\n",
        "Please cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "94274070-4bab-c8e6-816c-f730e2460d8c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "#import tensorflow as tf\n",
        "from sklearn.cross_validation import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.gridspec as gridspec\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d8d59b3-d398-f7f5-f70a-1f8c7b17325a"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../input/creditcard.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "af80e997-1d8b-d487-d67a-1d89064877d8"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "print(df.describe())\n",
        "print(df.isnull().sum())\n",
        "print (df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d44b6357-20c2-919d-d8ac-86b2fef70001"
      },
      "outputs": [],
      "source": [
        "df['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6310f981-7772-1961-19a3-1bb947fd0efa"
      },
      "source": [
        "## Time:\n",
        "Time variable is the time elapsed in seconds for each transaction from the first transaction in the dataset. For now, keeping  the field until we are sure if it has/has not value. Plotting a histogram of Time in Fraudulent and Normal transactions. We see there are a couple of peaks in the fraudulent transactions. At the time of the first peak in Fraudulent transactions (elapsed time = 40K seconds), there is also a large number of normal transactions. The second peak occurs at about 90K seconds since the start of first transaction. During this time, the normal transactions are very low. \n",
        "\n",
        "We can also see that the the normal transactions show a trend. The first uptrend began at about 25K seconds and then started to decline at about 75K seconds. The delta between the two ~ 50K seconds is approximately 14 hours. This sounds intuitively correct and could be the transactions happening during the day hours. The difference between the two bottoms on the Normal transactions is ~ 82K seconds ~ 1 day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e0f80fc-8ffd-c90b-11ce-6894cc52845a"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
        "\n",
        "bins = 50\n",
        "\n",
        "ax1.hist(df.Time[df.Class == 1], bins = bins)\n",
        "ax1.set_title('Fraud')\n",
        "\n",
        "bins = 100\n",
        "ax2.hist(df.Time[df.Class == 0], bins = bins)\n",
        "ax2.set_title('Normal')\n",
        "\n",
        "plt.xlabel('Time - in Seconds')\n",
        "plt.ylabel('Number of Transactions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "13587a4f-aa5a-cb15-1bf3-80f1c720a0e8"
      },
      "source": [
        "## Amount:\n",
        "\n",
        "Next we look at the summary stats of the transaction amount field for fraudulent and normal transactions.The IQR of Fraudulent is between $1 to $105 and the median is $9. The mean is $122 and its large difference from the median is due to the outliers on the right side of the distribution. The fraudulent transactions also have a large standard deviation of $256.\n",
        "\n",
        "For normal transactions, the IQR range is between $5 to $77. The difference between Mean ($88) and Median ($22) is $66 and is tighter than the $104 difference for fraudulent transactions.\n",
        "\n",
        "The histograms below show the distributions of both the transaction types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a24f4250-8cb5-41f0-a6f8-5204b3453e7a"
      },
      "outputs": [],
      "source": [
        "print (\"Fraud\")\n",
        "print (df.Amount[df.Class == 1].describe())\n",
        "print ()\n",
        "print (\"Normal\")\n",
        "print (df.Amount[df.Class == 0].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3da9bd9b-5f77-d39d-ed94-1ada060701b7"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
        "\n",
        "bins = 30\n",
        "\n",
        "ax1.hist(df.Amount[df.Class == 1], bins = bins)\n",
        "ax1.set_title('Fraud')\n",
        "\n",
        "bins = 100\n",
        "\n",
        "ax2.hist(df.Amount[df.Class == 0], bins = bins)\n",
        "ax2.set_title('Normal')\n",
        "\n",
        "plt.xlabel('Amount ($)')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8032a9cc-2ea0-5ce1-c700-b55282ea368b"
      },
      "source": [
        "The scatterplots between the time elapsed and transaction amount have been grouped by the transaction type. We do see the some extreme outliers in the Fraud transactions happening during the periods of low volumes for normal transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "312455fe-69d8-94c5-61a7-9a9f35b25da0"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,6))\n",
        "\n",
        "ax1.scatter(df.Time[df.Class == 1], df.Amount[df.Class == 1])\n",
        "ax1.set_title('Fraud')\n",
        "\n",
        "ax2.scatter(df.Time[df.Class == 0], df.Amount[df.Class == 0])\n",
        "ax2.set_title('Normal')\n",
        "\n",
        "plt.xlabel('Time (in Seconds)')\n",
        "plt.ylabel('Amount')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8506c1fb-296a-9d2a-6472-0edd5c7538ec"
      },
      "source": [
        "## PCA Transformed features: \n",
        "\n",
        "As mentioned in the descriptions above, this dataset has 29 numerical features that we obtained as a result of PCA. We do not have any business context of what those fields imply. In the series of plots shown below, we plot the histograms overlaid with density for each of these variables. The plots are color coded by the type of transaction. **Green ~ Normal,**  **Blue ~ Fraud.** We will visually inspect these distributions and use that information to only keep the variables where we see a clear distinction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f1445b92-f239-019d-c571-a02441f0d2cd"
      },
      "outputs": [],
      "source": [
        "#Select only the anonymized features.\n",
        "v_features = df.ix[:,1:29].columns\n",
        "plt.figure(figsize=(12,28*4))\n",
        "gs = gridspec.GridSpec(28, 1)\n",
        "for i, cn in enumerate(df[v_features]):\n",
        "    ax = plt.subplot(gs[i])\n",
        "    sns.distplot(df[cn][df.Class == 1], bins=50)\n",
        "    sns.distplot(df[cn][df.Class == 0], bins=100)\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_title('histogram of feature: ' + str(cn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5f6a04f4-e1e8-2f83-48db-8d3b0591a5f0"
      },
      "source": [
        "Dropping  some variables as they have very similar distributions for both types of transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "db35e047-bc6d-ddf9-8f0f-f18abffeb4ee"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7fa5890b-f929-625b-6daf-313fe814a235"
      },
      "source": [
        "Performing scaling on Amount and Time field as a necessary data transformation step before modeling. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a4b3689-991f-c3cd-7a9a-46ad511fbc29"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df['normAmount'] = StandardScaler().fit_transform(df['Amount'].reshape(-1, 1))\n",
        "df = df.drop(['Amount'],axis=1)\n",
        "df['normTime'] = StandardScaler().fit_transform(df['Time'].reshape(-1, 1))\n",
        "df = df.drop(['Time'],axis=1)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58c48f9a-2993-70ad-454b-d50936ac69e2"
      },
      "outputs": [],
      "source": [
        "X = df.ix[:, df.columns != 'Class']\n",
        "y = df.ix[:, df.columns == 'Class']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "71cb0516-e68e-e8cb-a503-bc5da702c43e"
      },
      "source": [
        "Undersampling the normal transactions so that the number of normal transactions is 3 times the fraudulent transactions. This is to overcome the extreme imbalance between the two classes as described above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0d7c887-9fc3-f992-f1e6-94fc1492d346"
      },
      "outputs": [],
      "source": [
        "number_records_fraud = len(df[df.Class == 1])\n",
        "fraud_indices = np.array(df[df.Class == 1].index)\n",
        "\n",
        "\n",
        "normal_indices = df[df.Class == 0].index\n",
        "\n",
        "\n",
        "random_normal_indices = np.random.choice(normal_indices, number_records_fraud*3, replace = False)\n",
        "random_normal_indices = np.array(random_normal_indices)\n",
        "\n",
        "#Concatenating the indices\n",
        "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
        "\n",
        "# Create the undersampled dataset\n",
        "under_sample_data = df.iloc[under_sample_indices,:]\n",
        "\n",
        "X_undersample = under_sample_data.ix[:, under_sample_data.columns != 'Class']\n",
        "y_undersample = under_sample_data.ix[:, under_sample_data.columns == 'Class']\n",
        "\n",
        "# Printing info\n",
        "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])*1.0/len(under_sample_data))\n",
        "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])*1.0/len(under_sample_data))\n",
        "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bcd9888a-62a3-eb45-ac15-7a8f8af7d556"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "Split the data into train - test in the ratio 75:25. The 25% test is our holdout sample that we do not use for training or CV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "84d3e3fe-187c-0214-575c-2aa6060de9aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "# Entire dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 0)\n",
        "\n",
        "print(\"Number transactions train dataset: \", len(X_train))\n",
        "print(\"Number transactions test dataset: \", len(X_test))\n",
        "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
        "\n",
        "# Undersampled dataset\n",
        "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
        "                                                                                                   ,y_undersample\n",
        "                                                                                                   ,test_size = 0.25\n",
        "                                                                                                   ,random_state = 0)\n",
        "print(\"\")\n",
        "print(\"Number transactions train dataset: \", len(X_train_undersample))\n",
        "print(\"Number transactions test dataset: \", len(X_test_undersample))\n",
        "print(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "37538f04-6515-84d7-ff71-ed4b61a6a6e9"
      },
      "source": [
        "Defining some helper functions for calculating different accuracy metrics that we will use for evaluating model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f6590b64-a2d5-1eb7-10e9-693b6424b38c"
      },
      "outputs": [],
      "source": [
        "def ROC_curve_data(y_true, y_score):\n",
        "    y_true  = np.asarray(y_true,  dtype=np.bool_)\n",
        "    y_score = np.asarray(y_score, dtype=np.float_)\n",
        "    assert(y_score.size == y_true.size)\n",
        "\n",
        "    order = np.argsort(y_score) # Just ordering stuffs\n",
        "    y_true  = y_true[order]\n",
        "    # The thresholds to consider are just the values of score, and 0 (accept everything)\n",
        "    thresholds = np.insert(y_score[order],0,0)\n",
        "    TP = [sum(y_true)] # Number of True Positives (For Threshold = 0 => We accept everything => TP[0] = # of postive in true y)\n",
        "    FP = [sum(~y_true)] # Number of True Positives (For Threshold = 0 => We accept everything => TP[0] = # of postive in true y)\n",
        "    TN = [0] # Number of True Negatives (For Threshold = 0 => We accept everything => we don't have negatives !)\n",
        "    FN = [0] # Number of True Negatives (For Threshold = 0 => We accept everything => we don't have negatives !)\n",
        "\n",
        "    for i in range(1, thresholds.size) : # \"-1\" because the last threshold\n",
        "        # At this step, we stop predicting y_score[i-1] as True, but as False.... what y_true value say about it ?\n",
        "        # if y_true was True, that step was a mistake !\n",
        "        TP.append(TP[-1] - int(y_true[i-1]))\n",
        "        FN.append(FN[-1] + int(y_true[i-1]))\n",
        "        # if y_true was False, that step was good !\n",
        "        FP.append(FP[-1] - int(~y_true[i-1]))\n",
        "        TN.append(TN[-1] + int(~y_true[i-1]))\n",
        "\n",
        "    TP = np.asarray(TP, dtype=np.int_)\n",
        "    FP = np.asarray(FP, dtype=np.int_)\n",
        "    TN = np.asarray(TN, dtype=np.int_)\n",
        "    FN = np.asarray(FN, dtype=np.int_)\n",
        "\n",
        "    accuracy    = (TP + TN) / (TP + FP + TN + FN)\n",
        "    sensitivity = TP / (TP + FN)\n",
        "    specificity = TN / (FP + TN)\n",
        "    return((thresholds, TP, FP, TN, FN))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d44a781f-5c72-7539-76e4-14e7364193bb"
      },
      "source": [
        "We are now ready to start the modeling. In the first cut, we will be using Logistic regression model and train it on the X variables in the  75% of records from the dataset we created after downsampling the majority class (Normal transactions) and combining with the fraudulent transactions.\n",
        "\n",
        "Since the number of records in this 75% training sample is not that large(1476 records), I will be performimg 5 fold CV to get the optimal value of C parameter. The metric of interest is Area Under Precision Recall curve. \n",
        "\n",
        "The function below called **printing\\_Kfold\\_scores** is used for performing Cross Validation ad then choosing the best value of C parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71a2ac0c-fef4-14f8-94c7-62bfcc132f0e"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cross_validation import KFold, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d17ee8f-6eb1-7808-822a-0a548b60fa5a"
      },
      "outputs": [],
      "source": [
        "def perform_Kfold_CV(x_train_data,y_train_data):\n",
        "    fold = KFold(len(y_train_data),5,shuffle=False) \n",
        "\n",
        "    # Different C parameters\n",
        "    c_param_range = [0.001,0.01,0.1,1,10,100]\n",
        "\n",
        "    results_table = pd.DataFrame(index = range(len(c_param_range),3), columns = ['C_parameter','Mean recall score','Mean_F1'])\n",
        "    results_table['C_parameter'] = c_param_range\n",
        "\n",
        "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
        "    j = 0\n",
        "    for c_param in c_param_range:\n",
        "        print('-------------------------------------------')\n",
        "        print('C parameter: ', c_param)\n",
        "        print('-------------------------------------------')\n",
        "        print('')\n",
        "\n",
        "        recall_accs = []\n",
        "        auprc_accs = []\n",
        "        F1_accs = []\n",
        "        for iteration, indices in enumerate(fold,start=1):\n",
        "\n",
        "            # Call the logistic regression model with a certain C parameter. Using L1 penalty - Lasso\n",
        "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n",
        "\n",
        "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
        "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
        "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
        "\n",
        "            # Predict values using the test indices in the training data\n",
        "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
        "\n",
        "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n",
        "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
        "            \n",
        "            # Compute confusion matrix\n",
        "            cnf_matrix = confusion_matrix(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
        "            np.set_printoptions(precision=2)\n",
        "            recall = cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1])\n",
        "            precision = cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1])\n",
        "            F1 = 2*recall*precision/(precision+recall)\n",
        "            \n",
        "            #prg_curve = prg.create_prg_curve(y_train_data.iloc[indices[1],:].values, y_pred_undersample)\n",
        "            #auprc_acc = prg.calc_auprg(prg_curve)\n",
        "            #auprc_accs.append(auprc_acc)\n",
        "            \n",
        "            recall_accs.append(recall_acc)\n",
        "            F1_accs.append(F1)\n",
        "            \n",
        "            print('Iteration ', iteration,': recall score = ', recall_acc)\n",
        "            print('Iteration ', iteration,': F1 score = ', F1)\n",
        "            #print('Iteration ', iteration,': AUPRC score = ', auprc_acc)\n",
        "\n",
        "        # The mean value of those recall scores is the metric we want to save and get hold of.\n",
        "        results_table.ix[j,'Mean recall score'] = np.mean(recall_accs)\n",
        "        results_table.ix[j,'Mean_F1'] = np.mean(F1_accs)\n",
        "        #results_table.ix[j,'Mean_AUPRC'] = np.mean(auprc_accs)\n",
        "        j += 1\n",
        "        print('')\n",
        "        print('Mean recall score ', np.mean(recall_accs))\n",
        "        print('')\n",
        "        print('Mean F1 score ', np.mean(F1_accs))\n",
        "        #print('Mean AUPRC score ', np.mean(auprc_accs))\n",
        "        print('')\n",
        "\n",
        "    print('Best C param for recall score ', results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter'])\n",
        "    print('')\n",
        "    print('Best C param for F1 score ', results_table.loc[results_table['Mean_F1'].idxmax()]['C_parameter'])\n",
        "    print('')\n",
        "    #best_c = results_table.loc[results_table['Mean_F1'].idxmax()]['C_parameter']\n",
        "    #print('Best C param for AUPRC ', best_c)\n",
        "    #print('')\n",
        "    \n",
        "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
        "    print('*********************************************************************************')\n",
        "    #print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
        "    print('*********************************************************************************')\n",
        "    \n",
        "    return results_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3a5f9d50-77f3-5696-82a7-004a8f4acb36"
      },
      "outputs": [],
      "source": [
        "cv_results = perform_Kfold_CV(X_train_undersample,y_train_undersample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "54914048-e9e5-a4e1-f9f1-bcd36919b30f"
      },
      "outputs": [],
      "source": [
        "best_c_F1 = cv_results.loc[cv_results['Mean_F1'].idxmax()]['C_parameter']\n",
        "best_c_recall = cv_results.loc[cv_results['Mean recall score'].idxmax()]['C_parameter']\n",
        "print('Best CParameter for optimal F1 score ' , best_c_F1)\n",
        "print('Best CParameter for optimal recall score ' , best_c_recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c82da161-f8ca-3aab-df3d-4c99a90f7dae"
      },
      "source": [
        "We see that the best value of C parameter  is 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e1236e9-d80c-949d-b66f-d037bcce32b6"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        1#print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5ef2bb39-a0e0-303c-f8cd-48b43a422c56"
      },
      "outputs": [],
      "source": [
        "# Use this C_parameter for best F1 score to build the final model with the undersampled training dataset  and predict the classes in the undersampled test\n",
        "# dataset\n",
        "lr = LogisticRegression(C = best_c_F1, penalty = 'l1')\n",
        "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
        "y_pred_undersample = lr.predict(X_test_undersample.values)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "recall = cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1])\n",
        "precision = cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1])\n",
        "f1 = 2*recall*precision/(precision+recall)\n",
        "print(\"Recall metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "print(\"Precision metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1]))\n",
        "print(\"F1 metric in the undersampled testing dataset: \", f1)\n",
        "#print(\"AUPRC metric in the undersampled testing dataset: \", auprc_acc)\n",
        "\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "de6055a6-fff5-a5e9-bb68-51dc4216e54a"
      },
      "source": [
        "Viola !! So our first model is correctly classifying 114 of the 130 fraud transactions correctly, and 367 of the 372 normal transactions correctly. Combining Accuracy and Recall, we calculate another measure called the F1 score which seems pretty good so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "534ee7db-0dff-27dc-de17-e934eb5c5296"
      },
      "source": [
        "Next we try to make predictions using the same model on the overall test set where the normal transactions are much higher than the fraud transactions. It would be interesting to see how we perform now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0140e784-4473-e485-c8ab-c51dbfcd6294"
      },
      "outputs": [],
      "source": [
        "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
        "# dataset\n",
        "y_pred = lr.predict(X_test.values)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "recall = cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1])\n",
        "precision = cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1])\n",
        "f1 = 2*recall*precision/(precision+recall)\n",
        "\n",
        "#prg_curve = prg.create_prg_curve(y_test.values, y_pred)\n",
        "#auprc_acc = prg.calc_auprg(prg_curve)\n",
        "#print(\"AUPRC metric in the testing dataset: \", auprc_acc)\n",
        "#prg.plot_prg(prg_curve)\n",
        "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "print(\"Precision metric in the testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1]))\n",
        "print(\"F1 metric in the testing dataset: \", f1)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d5759cc0-5006-9f72-926c-d7ac0c95b2a8"
      },
      "source": [
        "Our recall has slightly improved slightly, however the precision has gone down from 96% to 11%. The F1 score which combines Precision and Recall has also reduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "69996897-f5ac-cabc-c02f-e0ab82a8f215"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f43fd08-c354-7482-cc48-d49ea2a21b3a"
      },
      "outputs": [],
      "source": [
        "%timeit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from collections import OrderedDict\n",
        "%pylab inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "95f544a9-d8a2-b5f7-0e42-75d47a22d5c5"
      },
      "source": [
        "After loading the required dependencies, we build our first Forest model. We set n_jobs = 3 to leverage parallelism due to multi cores and also set the number of estimators as a fixed value = 501.\n",
        " \n",
        "Next we do a prediction on the 25% Test set after downsampling and then calculate the performance metrics as calculated above for logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cbec8707-0f15-79c8-ca14-42acca144987"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators = 501, oob_score = True,n_jobs = 3, random_state =1)\n",
        "model.fit(X_train_undersample,y_train_undersample.values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "009e658c-9123-1baa-1493-8252921777e9"
      },
      "outputs": [],
      "source": [
        "y_pred_score_rf_usample = model.predict(X_test_undersample)\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_score_rf_usample)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "recall = cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1])\n",
        "precision = cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1])\n",
        "f1 = 2*recall*precision/(precision+recall)\n",
        "print(\"Recall metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "print(\"Precision metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1]))\n",
        "print(\"F1 metric in the undersampled testing dataset: \", f1)\n",
        "#print(\"AUPRC metric in the undersampled testing dataset: \", auprc_acc)\n",
        "\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d8423aa5-2ba8-f44e-8ab1-80d48a41e584"
      },
      "source": [
        "On the down sampled test data, we see that  baseline Random Forest is slightly bad in terms of Recall and F1 score but better in terms of Precision  when compared to Logistic regression on the same test data. We can now try different combinations of max_features values and vary the number of Trees(estimators) and inspect the OOB error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "44faa6ca-9fbe-faed-5db1-8aea0f67fc50"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 123\n",
        "ensemble_clfs = [\n",
        "    (\"RandomForestClassifier, max_features='sqrt'\",\n",
        "        RandomForestClassifier(warm_start=True, oob_score=True,\n",
        "                               max_features=\"sqrt\",\n",
        "                               random_state=RANDOM_STATE)),\n",
        "    (\"RandomForestClassifier, max_features='log2'\",\n",
        "        RandomForestClassifier(warm_start=True, max_features='log2',\n",
        "                               oob_score=True,\n",
        "                               random_state=RANDOM_STATE)),\n",
        "    (\"RandomForestClassifier, max_features=None\",\n",
        "        RandomForestClassifier(warm_start=True, max_features=None,\n",
        "                               oob_score=True,\n",
        "                               random_state=RANDOM_STATE))\n",
        "]\n",
        "\n",
        "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
        "\n",
        "# Range of `n_estimators` values to explore.\n",
        "min_estimators = 51\n",
        "max_estimators = 301\n",
        "\n",
        "for label, clf in ensemble_clfs:\n",
        "    for i in range(min_estimators, max_estimators + 1):\n",
        "        clf.set_params(n_estimators=i)\n",
        "        clf.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
        "\n",
        "        # Record the OOB error for each `n_estimators=i` setting.\n",
        "        oob_error = 1 - clf.oob_score_\n",
        "        error_rate[label].append((i, oob_error))\n",
        "\n",
        "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
        "pylab.rcParams['figure.figsize'] = (14, 8)\n",
        "for label, clf_err in error_rate.items():\n",
        "    xs, ys = zip(*clf_err)\n",
        "    plt.plot(xs, ys, label=label)\n",
        "\n",
        "plt.xlim(min_estimators, max_estimators)\n",
        "plt.xlabel(\"n_estimators\")\n",
        "plt.ylabel(\"OOB error rate\")\n",
        "plt.legend(loc=\"upper right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e6c16ebb-1c0f-6e9e-b54b-ef926230692d"
      },
      "source": [
        "As seen from the plot above, the red line corresponding to max_features = None is having a lower OOB error and the error almost minimizes at 225 trees. We can also perform a randomized search and include some other hyper-parameters that control tree depth while fixing the number of trees and the number of features at every split from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4bb094e4-c81e-7020-3a56-0eaab900e7e3"
      },
      "outputs": [],
      "source": [
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a4019d0c-567a-2281-fe0f-0a31c998f3bb"
      },
      "source": [
        "## Setting the hyper-prameter choices and setting the grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "31f21e5e-beb2-566c-941d-88d258ed6470"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import randint as sp_randint\n",
        "param_dist = {\"max_depth\": [3, None],\n",
        "              \"max_features\": [None],\n",
        "              \"min_samples_split\": sp_randint(2, 11),\n",
        "              \"min_samples_leaf\": sp_randint(1, 11),\n",
        "              \"criterion\": [\"gini\"]}\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "clf = RandomForestClassifier(n_estimators=201,oob_score=True)\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search)\n",
        "\n",
        "from time import time\n",
        "start = time()\n",
        "random_search.fit(X_train_undersample,y_train_undersample.values.ravel() )\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8adab810-1f94-aadc-48f8-0735e0f66aa3"
      },
      "source": [
        "Choosing the best hyper-parameter choices from model tuning and building the final random forest moel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7b5b27a4-88af-6ea3-61c2-604a7113d337"
      },
      "outputs": [],
      "source": [
        "model_final_rf = RandomForestClassifier(n_estimators = 201, oob_score = True,n_jobs = 3, random_state =1, min_samples_split = 8, min_samples_leaf = 1, max_features =None)\n",
        "model_final_rf.fit(X_train_undersample,y_train_undersample.values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f3a10384-d7d5-f232-8014-fc03e0511f44"
      },
      "outputs": [],
      "source": [
        "importances = model_final_rf.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in model_final_rf.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X.shape[1]):\n",
        "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(X.shape[1]), importances[indices],\n",
        "       color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), indices)\n",
        "plt.xlim([-1, X.shape[1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7467f0bf-d3ce-c6a9-9ac7-6339bbc2b87b"
      },
      "source": [
        "## Calculate the different performance evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "148b8f3a-8715-658f-b9e3-bcca6200b9f3"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "y_pred_score_rf_usample = model_final_rf.predict(X_test_undersample)\n",
        "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_score_rf_usample)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "recall = cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1])\n",
        "precision = cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1])\n",
        "f1 = 2*recall*precision/(precision+recall)\n",
        "print(\"Recall metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "print(\"Precision metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1]))\n",
        "print(\"F1 metric in the undersampled testing dataset: \", f1)\n",
        "#print(\"AUPRC metric in the undersampled testing dataset: \", auprc_acc)\n",
        "\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0aba5cd2-fef7-b0c0-8017-174ae84de372"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "y_pred_score_rf = model_final_rf.predict(X_test)\n",
        "cnf_matrix = confusion_matrix(y_test,y_pred_score_rf)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "recall = cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1])\n",
        "precision = cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1])\n",
        "f1 = 2*recall*precision/(precision+recall)\n",
        "print(\"Recall metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "print(\"Precision metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1]))\n",
        "print(\"F1 metric in the undersampled testing dataset: \", f1)\n",
        "#print(\"AUPRC metric in the undersampled testing dataset: \", auprc_acc)\n",
        "\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ba7d65af-8faf-4266-a20c-f47367f580b7"
      },
      "source": [
        " We see that the recall is almost 0.93 which means that we will be correctly classifying 93 out of 100 fraudulent transactions. This is what is most important. However, the precision is 0.078 which means that out of every 100 predictions that are classified, roughly 8 are fraud and the rest 92 are not fraud implying that we have a high false positive rate. Let's work on finding a compromise between precision and recall.\n",
        "\n",
        "Let us now try up-sampling by creating synthetic data using SMOTE for the Fraudulent transactions and create a 50:50 Fraud  / Normal transactions training dataset and then test against the the test set consisting of skewed transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a732120e-4ea5-dc5f-1492-55544dca01a0"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "os = SMOTE(random_state=999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d80b3b4-869f-1ad4-0067-eeb9b4845681"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 0)\n",
        "\n",
        "print(\"Number transactions train dataset: \", len(X_train))\n",
        "print(\"Number transactions test dataset: \", len(X_test))\n",
        "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
        "columns = X_train.columns\n",
        "os_data_X,os_data_y=os.fit_sample(X_train,y_train.values.ravel())\n",
        "os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\n",
        "os_data_y= pd.DataFrame(data=os_data_y,columns=[\"Class\"])\n",
        "print(\"Total number of records in oversampled data is \",len(os_data_X))\n",
        "print(\"Number of normal transcation in oversampled data\",len(os_data_y[os_data_y[\"Class\"]==0]))\n",
        "print(\"No.of fraud transcation\",len(os_data_y[os_data_y[\"Class\"]==1]))\n",
        "print(\"Proportion of Normal data in oversampled data is \",len(os_data_y[os_data_y[\"Class\"]==0])/len(os_data_X))\n",
        "print(\"Proportion of fraud data in oversampled data is \",len(os_data_y[os_data_y[\"Class\"]==1])/len(os_data_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f863554c-10a6-3874-fed7-dc31f8b0153e"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_final_rf = RandomForestClassifier(n_estimators = 201, oob_score = True,n_jobs = 8, random_state =1, max_features = 'sqrt')\n",
        "start = time()\n",
        "model_final_rf.fit(os_data_X,os_data_y.values.ravel())\n",
        "print(\"Model train took %.2f seconds \" % ((time() - start)))\n",
        "\n",
        "# Compute confusion matrix\n",
        "y_pred_score_rf = model_final_rf.predict(X_test)\n",
        "cnf_matrix = confusion_matrix(y_test,y_pred_score_rf)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "recall = cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1])\n",
        "precision = cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1])\n",
        "f1 = 2*recall*precision/(precision+recall)\n",
        "print(\"Recall metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
        "print(\"Precision metric in the undersampled testing dataset: \", cnf_matrix[1,1]*1.0/(cnf_matrix[0,1]+cnf_matrix[1,1]))\n",
        "print(\"F1 metric in the undersampled testing dataset: \", f1)\n",
        "\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix\n",
        "                      , classes=class_names\n",
        "                      , title='Confusion matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b64231b6-a6a0-9b37-86ad-953713843193"
      },
      "source": [
        "We see that the recall has gone down from 0.93  when we were downsampling the normal transactions, \n",
        " to 0.84 when doing up-sampling. However, by doing so precision has improved drastically from 0.078 to 0.88. \n",
        "\n",
        "We can also try other techniques such as varying the thresholds to find the sweet spot on the Precision Recall curve, changing the cost function to penalize based on the type of error made. \n",
        "\n",
        "Please leave suggestions/comments if you liked the analysis."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}