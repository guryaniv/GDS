{"cells":[{"metadata":{"_uuid":"7bb87592b49b3e1b9368b0749aac1f76f8d4d46b"},"cell_type":"markdown","source":"Hi guys, I'm Ivan! \n\nIn this kernel, I'm going to introduce a simple algorithm to solve a binary classification problem. **Logistic Regression!!!**\nAs this is my very first kernel regards on classification, please correct me if you spot any mistakes! :)\nThis kernel is only meant for beginners, hopefully it will be useful! Now, let's start !"},{"metadata":{"_uuid":"1ee5d475d28e98ed5f9642073eafc4022848b6f0"},"cell_type":"markdown","source":"**Main Content:**\n* Import important packages,  training & testing dataset\n* Data Cleaning\n* Find the best fit model\n* Submission"},{"metadata":{"_uuid":"388685426321ca5040f8a2c4002ea0b259bb6328"},"cell_type":"markdown","source":"**PART I: Import Packages, training & testing dataset**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"#Import basic packages\nimport numpy as np\nimport pandas as pd\n\n#Package for data visualisation\nimport matplotlib.pyplot as plt\n\n#Packages for preprocessing\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\n#Packages for modelling \nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Package for evaluation\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"#Read training & testing dataset and store it as DataFrame \ndf_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")\n\n#Check the shape of each of the dataset\nprint(df_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9551fdc2965747e8ae07d49c69819261c70f54fc"},"cell_type":"markdown","source":"Preprocess for data cleaning\n\nUsually, we will conduct data cleaning process for both training & testing dataset. To aviod repetitive work, we will first need to combine both training & testing dataset into one. \n\nps. By looking at the shape for both dataset, testing dataset is short of one column==> 'target'. We will need to assign a new column named 'target' in the testing set. As this is a binary classification problem, the values will be either 1 or 0 under column 'target' in training test. To avoid confusion, we will need to assign a different value. "},{"metadata":{"trusted":true,"_uuid":"ca6b1ca08c26508b980f0b82a7167e9fe65d6f55"},"cell_type":"code","source":"#Assign a value and create a new column in testing set.\ndf_test['target'] = 10\n#Combine both dataset and denote to (df_all)\ndf_all = df_train.append(df_test, sort = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afb876e787f09c30f16c91733b89ca13f66755c4"},"cell_type":"markdown","source":"**PART II: Data Cleaning**\n\nData cleaning is extremely important in many of the project, and it is very tedious for most of the time. To have a good picture on the data, sometimes we need to be creative! \n"},{"metadata":{"trusted":true,"_uuid":"86e6a19e64931306e23dc4afd7c1599335d775e8"},"cell_type":"code","source":"#Take a look at the summary of each column\ndf_all.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e01c25568e910304f00afd4c7f83c4dfabf0065"},"cell_type":"markdown","source":"By looking at the above output, it seems that we do not have any missing value across all the columns. Well, it is not as smooth as you think....\n\nIt is stated at the missing value is replace by -1. Therefore, we have to change it back to NaN and fill up with a value that is more reasonable! "},{"metadata":{"trusted":true,"_uuid":"f3ace4ab522020098697f5c0f8b186ebd020e0fb"},"cell_type":"code","source":"#Replace (-1) with NaN\ndf_all = df_all.replace(-1,np.nan)\n#Let's look at the summary again\ndf_all.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcdc80331a650722965f98f0a8e4a46ba06315f8"},"cell_type":"code","source":"#Let's list down all the categorical variables that contain NaN. \ncat_na = ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_05_cat', 'ps_car_07_cat', 'ps_car_09_cat']\n#Bar chart plot regards on the frequency count of each category in each of the categorical variables that contain NaN. \nfor i in cat_na:\n    my_tab = pd.crosstab(index = df_all[i],columns=\"count\")    \n    my_tab.plot.bar()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c514dfced3ac8c4f0f7b00bca8417ab3d4331227"},"cell_type":"markdown","source":"Interpretation on the plot:\n\nIn each of the bar chart, there is a significance difference in the frequency count of each of the catetory. It will be reasonable that if we fill up the missing value with the value that appears the most in the corresponding column. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"713464933784a3900dede744a237f9e4d446013d"},"cell_type":"code","source":"#Fill NaN with most frequently number\nfor i in cat_na:\n    df_all[i] = df_all[i].fillna(df_all[i].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3b1fc15dff99c74929070b595d8a24950a82b178"},"cell_type":"code","source":"#List down all the continuous variables that contain NaN\ncont_na = ['ps_reg_03', 'ps_car_11', 'ps_car_12', 'ps_car_14']\n#It is not wise to have a frequency plot for continuous variable...give it a try if you want to find out. \n#Fill NaN with mean\nfor i in cont_na:\n    df_all[i] = df_all[i].fillna(df_all[i].mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8103b935788421ce01af2841b6753ed41eaa267e"},"cell_type":"markdown","source":"Change some of the continuous variables to categorical variables. \n\nReason behind this: \nSome of the columns are classified as continuous variables, however, it makes more sense to convert it into categorical variables if the number of categories is not huge. "},{"metadata":{"trusted":true,"_uuid":"71b31f354fb593de47877590428ee773ae2819d3"},"cell_type":"code","source":"#Use nunique method to determine the number of unique values in each column\ndef count_unique_value(dataframe):\n    df = pd.DataFrame()\n    df['No. of unique value'] = dataframe.nunique()\n    df['DataType'] = dataframe.dtypes\n    return df\n\nprint(count_unique_value(df_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"117f5d96ff447c54bc7daeda8806d941abab2c81"},"cell_type":"code","source":"#Change datatype to 'Category' for the columns with number of unique value <= 20. \ndef change_datatype(dataframe):\n    col = dataframe.columns\n    for i in col:\n        if dataframe[i].nunique()<=20:\n            dataframe[i] = dataframe[i].astype('category')\n    \nchange_datatype(df_all)\n\n#Change the datatype of target to int64. \ndf_all['target'] = df_all['target'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88d400991e07792dcd70e9295de90b0dfef48139"},"cell_type":"markdown","source":"We have done most of the work in data cleaning process. Right now, let's wrap up! \n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9d0aec6340aeaae5677729914c58a2052b8a87ee"},"cell_type":"code","source":"#Convert categorical variables to dummy variables\ndf_all_dummy = pd.get_dummies(df_all, drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"009336dfd7c230a91e78958dfbc8620babf84db7"},"cell_type":"code","source":"#Split the combined dataset into training set & testing set\ndf_train_adj = df_all_dummy[df_all_dummy['target'] != 10]\ndf_test_adj = df_all_dummy[df_all_dummy['target'] == 10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fa4006a5483fdec06fbe9ca3d1e424904b057d3"},"cell_type":"markdown","source":"**PART III: Find the best fit model**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b3695ffb6b0f8fada014f104112a06f6c28c150c"},"cell_type":"code","source":"#Extract training data from training set\ndata_to_train = df_train_adj.drop(['target','id'], axis = 1)\n#Extract labels from training set\nlabels_to_use = df_train_adj['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb9ccd7103cb5abd9d171444e176af6fb2ab51de"},"cell_type":"code","source":"#Build different model\n\n#Logistic Regression\nlogreg = make_pipeline(RobustScaler(), LogisticRegression())\n\n#SGD Classifier\nsgd = make_pipeline(RobustScaler(), SGDClassifier(loss=\"log\"))\n\n#Random Forest Classifier\nrfc = make_pipeline(RobustScaler(), RandomForestClassifier(50))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf1381b0fa00e7a3bad950b698d7eabc2de0c492"},"cell_type":"markdown","source":"Of course you can build as much model as you want. It is also possible to have a more accurate result by changing the default parameters. For Random forecast classifier, the larger number of estimators, the more accurate of the model. However, the processing time is getting slower as the number of estimators increases. The maximum number of estimators should not be larger than total number of variables. "},{"metadata":{"_uuid":"3c316de7c8188f948a762243bc5584bb436d93f8"},"cell_type":"markdown","source":"**Build a evaluation model**"},{"metadata":{"trusted":true,"_uuid":"1cbc98fe403c071a3ddf2951359bd3ddaf14f1d4"},"cell_type":"code","source":"def evaluation_auc(model):\n    result= cross_val_score(model, data_to_train, labels_to_use, cv = 3, scoring = 'roc_auc')\n    return(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11d5f5baaf8642ca8ee282ef49979d94dac5ccc3"},"cell_type":"code","source":"#Score for Logistic Regression\nscore = evaluation_auc(logreg)\nprint(\"\\nLogistic Regression Score: {:.5f} ({:.5f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a3c21e8eb8de3d6e0c29b8bd5f56129c4cf73ce"},"cell_type":"code","source":"#Score for SGD Classifier\nscore = evaluation_auc(sgd)\nprint(\"\\nSGD Classifier Score: {:.5f} ({:.5f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5805d6f47fc0756b8b2c2f7ee115141575f739dc"},"cell_type":"code","source":"#Score for Random Forest Classifier\nscore = evaluation_auc(rfc)\nprint(\"\\nRandom Forest Classifier score: {:.5f} ({:.5f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78a57a0fe01fc24b56ebbfb283fb8ddb2a5a9119"},"cell_type":"markdown","source":"Logistics Regression performs the best! Our final submission will be based on that! "},{"metadata":{"_uuid":"c8e1066b95d8ff893e2d4a87506a4fde9e6a453d"},"cell_type":"markdown","source":"**PART IV: Submission**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"34da320ce0d50682363d1f8ba29a9553ba90e219"},"cell_type":"code","source":"#Submission preparation\ntest_df_id = df_test['id']\ntest_df_x = df_test_adj.drop(['target', 'id'], axis = 1)\nlogreg.fit(data_to_train, labels_to_use)\n\n#As we are predicting probability, use predict_proba instead of predict! \ntest_df_y = logreg.predict_proba(test_df_x)[:,1]\n\nsubmission = pd.DataFrame({'id': list(test_df_id), 'target': list(test_df_y)})\nsubmission.to_csv('sgd_log.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cfc21471aa6745d4f8f1deb4aa99341f2a5bc38"},"cell_type":"markdown","source":"*This is the end of the kernel, thank you! *"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}