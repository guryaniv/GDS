{"cells":[{"metadata":{"_uuid":"56dfb1f11bcce0b60d46cc0fb12c7613a0b77972"},"cell_type":"markdown","source":"## I. Introduction"},{"metadata":{"_uuid":"b851e3ff8cab8cec46f0603d907f98560ed4a02b"},"cell_type":"markdown","source":"In **YOLOV3**, **bounding boxes** for objects in training images are a key input.  **Anchor boxes** are a key part YOLOV3 model configuration, one that can sets YOLOV3 up for efficent training.  Anchor boxes are defined at the resolution chosen for training inputs.  As an example, RSNA images are of dimensions **1024x1024**, but possible YOLOV3 training dimensions are **416x416**, **512x512**, and **608x608** (notice these are all multiples of 32).   During training, YOLOV3 figures out offsets from the closest anchor box that provides the lowest loss for a ground truth bounding box.  So,  while any reasonable set of anchor boxes can be adequate for model convergence using YOLOV3, this kernel analyzes RSNA Stage 2 training inputs with the goal of potentially choosing anchor boxes that lead to more efficient training convergence.\n\nV2 was a stable version with input image size of (defined by **YOLOV3_SIZE**) of **608x608**.  V3 analyzes anchor boxes at YOLOV3_SIZE of **512**. \n\nComments welcome!\n\nReferences: [YOLOV3 Paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf), [YOLOV3 Web Site](https://pjreddie.com/darknet/yolo/)"},{"metadata":{"trusted":true,"_uuid":"2c7f4d7437d32c1fa148e66b9b54bc6a873d5c14"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.cluster import MiniBatchKMeans\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# clone darknet\n!git clone https://github.com/pjreddie/darknet","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# lets look at the default anchor boxes in yolov3-tiny.cfg (6 anchor boxes)\n# and yolov3.cfg (9 anchor boxes) and the associated input image sizes\n!cp darknet/cfg/yolov3-tiny.cfg .\n!grep -E 'width|height|anchors' yolov3-tiny.cfg\n\nprint (\"---------\")\n\n!cp darknet/cfg/yolov3.cfg .\n!grep -E 'width|height|anchors' yolov3.cfg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f41a35ab8465f8a08dbc5a988487e30e71a0e9b9"},"cell_type":"code","source":"# so the default configuration of yolov3-tiny is 416x416, and the one for yolov3 is 608x608\n# yolov3 uses three anchor boxes per 'scale.'  Predictions are first done at the input scale.\n# then the network upsamples the inputs to twice the resolution and makes predictions again.\n# upsampling helps detect smaller objects.  yolov3-tiny has 2 scales (6 anchor boxes), and\n# yolov3 has 3 scales (9 anchor boxes).\n# Analysis Choices:\n# V2: input width and height of 608 (19x19 cells) for both YOLOV3 Tiny and YOLOV3\n# V3: input width and height of 512 (16x16 cells) for both YOLOV3 Tiny and YOLOV3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9fe4babb7dea7be2313d2c485077c29a9bf9140"},"cell_type":"code","source":"# cleanup darknet download\n!rm -rf darknet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be7032fedc81f2e2cbbce65f6c35bfde5cddc1b4"},"cell_type":"code","source":"# global variables\nTRAIN_LABELS_CSV_FILE=\"../input/stage_2_train_labels.csv\"\n# pedantic nit: we are changing 'Target' to 'label' on the way in\nTRAIN_LABELS_CSV_COLUMN_NAMES=['patientId', 'x1', 'y1', 'bw', 'bh', 'label']\n\nDICOM_IMAGE_SIZE=1024\nYOLOV3_SIZE=512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cb7689732aeed55e495dad6ac1a7bedf9c05a8f"},"cell_type":"code","source":"# read RSNA TRAIN_LABELS_CSV_FILE into a pandas dataframe\nlabelsbboxdf = pd.read_csv(TRAIN_LABELS_CSV_FILE,\n                           names=TRAIN_LABELS_CSV_COLUMN_NAMES,\n                           # skip the header line\n                           header=0,\n                           # index the dataframe on patientId\n                           index_col='patientId')\n\nlabelsbboxdf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2afd3fcda32d3c9c0e9124e50290bd50d5013f30"},"cell_type":"code","source":"# drop all fields except the bounding box dimensions and\n# all row except the Lung Opacity ones\nyolov3bboxesdf=labelsbboxdf[['bw', 'bh']].dropna()\nyolov3bboxesdf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bace63b1cbc25dad5333e375d02cc93befa4f65c"},"cell_type":"code","source":"# resize bounding boxes for YOLOV3_SIZE\nyolov3bboxesdf=yolov3bboxesdf*(YOLOV3_SIZE/DICOM_IMAGE_SIZE)\nyolov3bboxesdf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8d6426035057c2155d8e8d967411dcfeda568f0"},"cell_type":"code","source":"# as reference, below are the vitals on bounding boxes at DICOM_IMAGE_SIZE\nlabelsbboxdf[['bw', 'bh']].describe(percentiles=[0.25, 0.5, 0.75, .95])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7579399ef69d6f74f13a2ab2afaf65579f43f318"},"cell_type":"code","source":"# below are the vitals on bounding boxes at chosen input size of YOLOV3_SIZE\nyolov3bboxesdf.describe(percentiles=[0.25, 0.5, 0.75, 0.85, .95])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec6b9626f1e7e54116e4f57f815204d7cee015af"},"cell_type":"code","source":"# we could hand-craft the following anchor boxes :\n# ~<min, ~<25%, ~<50%, ~<75%, ~<85%, ~<95% and have a 6 anchor box set (for yolov3 tiny)\n!printf '10,15, 75,75 100,125, 100,175 125,225, 150,275\\n' > rsna-yolov3-manual-tiny-anchors.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d790a7627c62919dc282b6277eda69d6d2c56dd4"},"cell_type":"code","source":"!cat rsna-yolov3-manual-tiny-anchors.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05cd5b9481f3b1decc0c4c6b69c970cc32c8b135"},"cell_type":"code","source":"# let's see what kmeans analysis gives us","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8aa9e19fc07febe6f05f86ad4d18cf534723dd6e"},"cell_type":"code","source":"# convert to numpy array\nbboxarray=np.array(yolov3bboxesdf)\n\nprint (bboxarray.shape)\nprint (bboxarray)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"059556cd6f2c5a270124f0b304b7da49867cb916"},"cell_type":"code","source":"# fit to 6 kmeans clusters (for yolov3 tiny)\nkmeans=MiniBatchKMeans(n_clusters=6, verbose=1)\ncolors=['b.', 'g.', 'r.', 'c.', 'm.', 'y.',  'k.']\nkmeans.fit(bboxarray)\ncentroids=kmeans.cluster_centers_\nlabels=kmeans.labels_\n\nprint (centroids.shape)\nprint (centroids)\nprint (labels.shape)\nprint (labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07992e37b782a20fadb303c5702171d43d5678d1"},"cell_type":"code","source":"# view computed centroids to bounding box dimensions' scatterplot\nplt.figure(figsize=(10,10))\nfor i in range(len(bboxarray)):\n    plt.plot(bboxarray[i][0], bboxarray[i][1], colors[labels[i]], markersize=10)   \nplt.scatter(centroids[:,0], centroids[:,1], marker=\"x\", s=150, linewidth=5, zorder=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6a02d00240f1e9ebbb402ad70efec7346004bdf"},"cell_type":"code","source":"# post process centroids\nanchors=np.around(centroids)\nprint (len(anchors))\nprint (anchors)\nprint (\"---------\")\nind = np.lexsort((anchors[:,1], anchors[:,0])) # lexsort uses the second argument first, followed by the first argument\n#print (ind)\nsortedanchors=np.array([anchors[i] for i in ind])\nprint(sortedanchors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"119b2fc9c496abba2c66f3c081ebf31f9d1394b0"},"cell_type":"code","source":"# write anchor boxes to file\n# organize anchor boxes in YOLOV3 format\nfor i in range (len(sortedanchors)):\n    anchorbox=\"{},{}\".format(int(sortedanchors[i][0]), int(sortedanchors[i][1]))\n    if i==0:\n        anchorrecord=anchorbox\n    else:\n        anchorrecord=\"{},  {}\".format(anchorrecord, anchorbox)\nanchorrecord=\"{}\\n\".format(anchorrecord)\n\nprint (anchorrecord)\n\n# save anchor box specification to file\nsavedanchorsfilename='rsna-yolov3-kmeans-tiny-anchors.txt'\nwith open(savedanchorsfilename,'w') as file:\n    file.write(anchorrecord)\nfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"866d4a727bc77451552feedca679dd8cdbb1bcb6"},"cell_type":"code","source":"!cat rsna-yolov3-kmeans-tiny-anchors.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11c66e6dcb28a1801c93775082d8829cc75f1220"},"cell_type":"code","source":"# fit to 9 kmeans clusters\nkmeans=MiniBatchKMeans(n_clusters=9, verbose=1)\ncolors=['b.', 'g.', 'r.', 'c.', 'm.', 'y.', 'k.', 'w.', 'b.']\nkmeans.fit(bboxarray)\ncentroids=kmeans.cluster_centers_\nlabels=kmeans.labels_\n\nprint (centroids.shape)\nprint (centroids)\nprint (labels.shape)\nprint (labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6014d63cdb34b3083d9fd5566faf2ace7fb2e8ac"},"cell_type":"code","source":"# view computed centroids to bounding box dimensions' scatterplot\nplt.figure(figsize=(10,10))\nfor i in range(len(bboxarray)):\n    plt.plot(bboxarray[i][0], bboxarray[i][1], colors[labels[i]], markersize=10)   \nplt.scatter(centroids[:,0], centroids[:,1], marker=\"x\", s=150, linewidth=5, zorder=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d21eef8b8f64fb8f77f06a2fa34bad8d923a8a2"},"cell_type":"code","source":"# post process centroids\nanchors=np.around(centroids)\nprint (len(anchors))\nprint (anchors)\nprint (\"---------\")\nind = np.lexsort((anchors[:,1], anchors[:,0]))\n#print (ind)\nsortedanchors=np.array([anchors[i] for i in ind])\nprint(sortedanchors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17ed91e639422e7fc943d41c5749746da5f378ad"},"cell_type":"code","source":"# write anchor boxes to file\n# organize anchor boxes in YOLOV3 format\nfor i in range (len(sortedanchors)):\n    anchorbox=\"{},{}\".format(int(sortedanchors[i][0]), int(sortedanchors[i][1]))\n    if i==0:\n        anchorrecord=anchorbox\n    else:\n        anchorrecord=\"{},  {}\".format(anchorrecord, anchorbox)\nanchorrecord=\"{}\\n\".format(anchorrecord)\n\nprint (anchorrecord)\n\n# save anchor box specification to file\nsavedanchorsfilename='rsna-yolov3-kmeans-anchors.txt'\nwith open(savedanchorsfilename,'w') as file:\n    file.write(anchorrecord)\nfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d61f7c69a3d52d7316689c4cdf579e6f702ad46"},"cell_type":"code","source":"!cat rsna-yolov3-kmeans-anchors.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aedb26a06984ca921c4afbe393b7d22dcea9583c"},"cell_type":"code","source":"# everything together\n# print default yolov3-tiny anchors\n!grep anchors yolov3-tiny.cfg\n# print hand-crafted yolov3 tiny anchors\n!cat rsna-yolov3-manual-tiny-anchors.txt\n# print kmeans suggested yolov3 tiny anchors\n!cat rsna-yolov3-kmeans-tiny-anchors.txt\n# print default yolov3 anchors\n!grep anchors yolov3.cfg\n# print kmeans suggested yolov3 anchors\n!cat rsna-yolov3-kmeans-anchors.txt\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65ffcd2817aa7c7846b9e15f41ac6184510feeab"},"cell_type":"markdown","source":"## II.  Conclusion"},{"metadata":{"_uuid":"5a63264178f9ed2cc8f3199beb67935d74c3460d"},"cell_type":"markdown","source":"1.  The default anchors will work but will cause significantly larger number of steps for training to converge.\n2.  The hand-crafted anchor boxes for YOLOV3 Tiny are better than what k-means gives us.  But we could do better with the smallest default anchor box of **10, 13** (YOLOV3 default) just in case the test cases have very small **Lung Opacity** areas.   After that, the hand-crafted anchor boxes for YOLOV3 Tiny look adequate.\n3.  For YOLOV3, again we could do better with **10, 13,** as the smallest anchor box.   We could use the rest of the k-means proposed anchor boxes, dropping the largest one and fine tuning the one before that if needed.  Between sets of three anchor boxes per scale, most bounding boxes should be converged efficiently.\n4.  **With the choice of input image sizes**, proceeding with the following anchor box sets appears to be a good choice for RSNA Stage 2 training data [**NOTE**: your results may vary slightly on subsequent runs]:\n     \n     **V3 (Input Image Size=512x512)**:\n     \n         YOLOV3-Tiny: 10,13, 75,75 100,125, 100,175 125,225, 150,275\n     \n         YOLOV3: 10,13, 74,63, 81,103, 96,146, 112,206, 122,251, 123,102, 136,165, 143,287\n     \n     **V2 (Input Image Size=608x608)**:\n     \n         YOLOV3-Tiny: 10,13, 100,100 125,175, 150,250 160,300, 175,325\n     \n         YOLOV3: 10,13, 80,76, 107,114, 114,169, 117,216, 130,267, 155,146, 157,326, 170,340\n\n5.  The analysis makes us much more familiar with the data and correlate it to training progress."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}