{"nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"version": "3.6.1", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "cells": [{"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from datetime import timedelta\n", "import datetime as dt\n", "import matplotlib.pyplot as plt\n", "plt.rcParams['figure.figsize'] = [16, 10]\n", "import seaborn as sns\n", "import xgboost as xgb\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import MiniBatchKMeans\n", "\n", "# Any results you write to the current directory are saved as output."], "cell_type": "code", "metadata": {"_uuid": "bafb9d0c9e18b2b5ce4417b7ef89f33842abe83d", "collapsed": true, "_cell_guid": "37b93f00-a837-4c5b-aadc-9a80e06a6f07"}, "execution_count": null, "outputs": []}, {"source": ["t0 = dt.datetime.now()\n", "train = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\n", "test = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\n", "sample_submission = pd.read_csv('../input/nyc-taxi-trip-duration/sample_submission.csv')\n", "test_1 = test.copy()"], "cell_type": "code", "metadata": {"_uuid": "575bba9c46eded693d0b3acb037fd60ef1ba613e", "_cell_guid": "f5cadcc6-50a2-49f1-b47c-571fc6cf8c95"}, "execution_count": null, "outputs": []}, {"source": ["## Feature Engineering"], "cell_type": "markdown", "metadata": {"_uuid": "46fb327efb75aaeae757e1c1362bb327693f3761", "_cell_guid": "92f5ad77-4c8e-4de0-b691-0ad8deb8e1c4"}}, {"source": ["A lot of the features have been extracted using existing models, especially the model of \"beluga\" (Cheers, mate). I also tried using the weather information as a variable but it seems that they do not serve so much of a useful purpose as far the accuracy of the result is concerned. Perhaps, I would use some sort of ensemble learning later to calculate feature importance of variables"], "cell_type": "markdown", "metadata": {"_uuid": "532f24122cc389cae9e6b4f2673ba122efeaf080", "_cell_guid": "f79349b8-1ca7-4b0c-ae4d-0ae55610b2d6"}}, {"source": ["### Conversion of DATETIME Features"], "cell_type": "markdown", "metadata": {"_uuid": "7e00786095d45b6d28c3170da8e20cd98bc7a6b0", "_cell_guid": "996b65f9-da95-4817-aac7-4eb537bdd8fb"}}, {"source": ["train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n", "test['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\n", "train.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\n", "test.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date\n", "train['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\n"], "cell_type": "code", "metadata": {"_uuid": "a82bdb6ca802737fca56005c531ab764535f9f70", "collapsed": true, "_cell_guid": "8fb8357c-503b-4d44-8e5a-217144994b78"}, "execution_count": null, "outputs": []}, {"source": ["### DateTime Features"], "cell_type": "markdown", "metadata": {"_uuid": "ccc62afdfdd6853b25af9ae19f085cb8253d78b8", "collapsed": true, "_cell_guid": "4b587dca-d665-4746-95d4-8d7332b5693e"}}, {"source": ["train.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday\n", "train.loc[:, 'pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear\n", "train.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\n", "train.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\n", "train.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n", "train.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n", "\n", "test.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\n", "test.loc[:, 'pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\n", "test.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\n", "test.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\n", "test.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n", "test.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']\n", "\n", "train.loc[:, 'pickup_dayofyear'] = train['pickup_datetime'].dt.dayofyear\n", "test.loc[:,'pickup_dayofyear'] = test['pickup_datetime'].dt.dayofyear"], "cell_type": "code", "metadata": {"_uuid": "5d59fd4ef4ad0bcfd26a7a1d140082b884e4d6b8", "collapsed": true, "_cell_guid": "b418a7c2-21ff-4fa5-a2e3-d09cb7a48a6a"}, "execution_count": null, "outputs": []}, {"source": ["### Bearing Feature"], "cell_type": "markdown", "metadata": {"_uuid": "ff4f1d7848f4eaee0ad6891260779e01e14c0aea", "collapsed": true, "_cell_guid": "dd811ba5-3c8e-490e-a7db-32a4a4fe4661"}}, {"source": ["def bearing_array(lat1, lng1, lat2, lng2):\n", "    AVG_EARTH_RADIUS = 6371  # in km\n", "    lng_delta_rad = np.radians(lng2 - lng1)\n", "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n", "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n", "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n", "    return np.degrees(np.arctan2(y, x))\n", "\n", "train.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, \n", "                                          train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "\n", "test.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, \n", "                                         test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n"], "cell_type": "code", "metadata": {"_uuid": "e09b4951c029f2a8296cc95d8a3b6bd5b7d3c713", "collapsed": true, "_cell_guid": "cbb02090-2ca6-4730-956b-755147d0a98e"}, "execution_count": null, "outputs": []}, {"source": ["### Distance Calculation"], "cell_type": "markdown", "metadata": {"_uuid": "80d3e8da82a961b24d1eb46ce6dc0490d176099a", "collapsed": true, "_cell_guid": "3dc2ed1c-8f70-4cb8-ad63-451dfd91d30d"}}, {"source": ["def haversine_array(lat1, lng1, lat2, lng2):\n", "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n", "    AVG_EARTH_RADIUS = 6371  # in km\n", "    lat = lat2 - lat1\n", "    lng = lng2 - lng1\n", "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n", "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n", "    return h\n", "\n", "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n", "    a = haversine_array(lat1, lng1, lat1, lng2)\n", "    b = haversine_array(lat1, lng1, lat2, lng1)\n", "    return a + b\n", "\n", "train.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "train.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "\n", "\n", "test.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n", "test.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n", "\n", "\n", "\n", "train.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2\n", "train.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2\n", "test.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2\n", "test.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2"], "cell_type": "code", "metadata": {"_uuid": "989a82f1ed353db24e11625e4060df3e8d5b621a", "_cell_guid": "97dcc6a5-b5f7-4f52-aec1-811e10463dbb"}, "execution_count": null, "outputs": []}, {"source": ["### PCA Features"], "cell_type": "markdown", "metadata": {"_uuid": "fd84ca83effba816da345af081e6d5e064d7ace5", "collapsed": true, "_cell_guid": "467037da-be53-422c-bacc-ae05b727edf8"}}, {"source": ["coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n", "                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n", "                    test[['pickup_latitude', 'pickup_longitude']].values,\n", "                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n", "\n", "pca = PCA().fit(coords)\n"], "cell_type": "code", "metadata": {"_uuid": "c20ff891223e78b7c33550ae3fdf2352912d434e", "collapsed": true, "_cell_guid": "66bc5e5d-6ddf-4aa6-8cb5-296f53e633ef"}, "execution_count": null, "outputs": []}, {"source": ["train['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\n", "train['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\n", "train['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n", "train['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n", "test['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\n", "test['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\n", "test['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n", "test['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n", "\n", "train.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])\n", "test.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])"], "cell_type": "code", "metadata": {"_uuid": "6318cede3b9a64223dcc793694944101529a58fa", "collapsed": true, "_cell_guid": "68dfc8ed-fd94-4f22-bdd1-707a210bae78"}, "execution_count": null, "outputs": []}, {"source": ["### Clustering Features"], "cell_type": "markdown", "metadata": {"_uuid": "06cc4f5439fd0a98dfab3efa64d8b83c2ca40abf", "_cell_guid": "17a2ba72-1afd-4537-aa97-1fcd050665f3"}}, {"source": ["sample_ind = np.random.permutation(len(coords))[:500000]\n", "kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])"], "cell_type": "code", "metadata": {"_uuid": "f1a232f9732a63f7fbf4ccc654246c246d6c1714", "collapsed": true, "_cell_guid": "69940065-a771-4139-8461-ce2b833541d9"}, "execution_count": null, "outputs": []}, {"source": ["train.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])\n", "train.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])\n", "test.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\n", "test.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])\n", "t1 = dt.datetime.now()"], "cell_type": "code", "metadata": {"_uuid": "d1c0f8ce7ba39ac0e3ddf13080e5852a0b5aa23a", "collapsed": true, "_cell_guid": "098c70be-6a35-4713-b069-0ed56e898b20"}, "execution_count": null, "outputs": []}, {"source": ["## OSRM Data"], "cell_type": "markdown", "metadata": {"_uuid": "c1526cbb487fe825ababc2d1385926ad4b177b7d", "_cell_guid": "b016f6d2-452d-4b31-858d-2c7bb123465c"}}, {"source": ["fr1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv', usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps', ])\n", "fr2 = pd.read_csv('../input/new-york-city-taxi-with-osrm//fastest_routes_train_part_2.csv', usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n", "test_street_info = pd.read_csv('../input/new-york-city-taxi-with-osrm//fastest_routes_test.csv',\n", "                               usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])"], "cell_type": "code", "metadata": {"_uuid": "2360f6c7276a1d340824da8ff911399fafa4d0a1", "collapsed": true, "_cell_guid": "7ef23e1a-54f2-422f-b9ff-9fc27c3c55a6"}, "execution_count": null, "outputs": []}, {"source": ["train_street_info = pd.concat((fr1, fr2))\n", "train = train.merge(train_street_info, how='left', on='id')\n", "test = test.merge(test_street_info, how='left', on='id')"], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["### Features Checking"], "cell_type": "markdown", "metadata": {}}, {"source": ["train['log_trip_duration'] = np.log(train['trip_duration'].values + 1)"], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["feature_names = list(train.columns)\n", "print(np.setdiff1d(train.columns, test.columns))\n"], "cell_type": "code", "metadata": {}, "execution_count": null, "outputs": []}, {"source": ["do_not_use_for_training = ['id', 'log_trip_duration', 'trip_duration', 'dropoff_datetime', 'pickup_date', \n", "                           'pickup_datetime', 'date']\n", "feature_names = [f for f in train.columns if f not in do_not_use_for_training]\n", "# print(feature_names)\n", "print('We have %i features.' % len(feature_names))\n", "train[feature_names].count()\n", "         "], "cell_type": "code", "metadata": {}, "execution_count": null, "outputs": []}, {"source": ["### Features Encoding "], "cell_type": "markdown", "metadata": {}}, {"source": ["train['store_and_fwd_flag'] = train['store_and_fwd_flag'].map(lambda x: 0 if x == 'N' else 1)"], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["test['store_and_fwd_flag'] = test['store_and_fwd_flag'].map(lambda x: 0 if x == 'N' else 1)"], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["### K Fold Cross Validation"], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.model_selection import KFold\n", "\n", "X = train[feature_names].values\n", "y = np.log(train['trip_duration'].values + 1)  \n", "\n", "\n", "kf = KFold(n_splits=10)\n", "kf.get_n_splits(X)\n", "\n", "print(kf)  \n", "\n", "KFold(n_splits=10, random_state=None, shuffle=False)\n", "for train_index, test_index in kf.split(X):\n", "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n", "    X_train, X_test = X[train_index], X[test_index]\n", "    y_train, y_test = y[train_index], y[test_index]\n", "    \n", "    \n", " "], "cell_type": "code", "metadata": {}, "execution_count": null, "outputs": []}, {"source": ["### XgBoost Implementation"], "cell_type": "markdown", "metadata": {}}, {"source": ["   \n", "dtrain = xgb.DMatrix(X_train, label=y_train)\n", "dvalid = xgb.DMatrix(X_test, label=y_test)\n", "dtest = xgb.DMatrix(test[feature_names].values)\n", "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n", "\n", "# Try different parameters! My favorite is random search :)\n", "xgb_pars = {'min_child_weight': 10, 'eta': 0.04, 'colsample_bytree': 0.8, 'max_depth': 15,\n", "            'subsample': 0.75, 'lambda': 2, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n", "            'eval_metric': 'rmse', 'objective': 'reg:linear'}    "], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["model = xgb.train(xgb_pars, dtrain, 500, watchlist, early_stopping_rounds=250,\n", "                  maximize=False, verbose_eval=15)"], "cell_type": "code", "metadata": {}, "execution_count": null, "outputs": []}, {"source": ["#### Continued :)"], "cell_type": "markdown", "metadata": {}}, {"source": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "outputs": []}], "nbformat": 4}