{"cells":[{"metadata":{"_uuid":"46a3c184f908d43681642a411f496b478d6ead30"},"cell_type":"markdown","source":"I am trying to understand the kernel of Abhishek in Spooky Author Competition\nThe Orginal Kernel is [Approaching (Almost) Any NLP Problem on Kaggle\n](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle). Therefore most of the code is from that Kernel\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nimport json\n\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_json('../input/whats-cooking-kernels-only/train.json')\ntest = pd.read_json('../input/whats-cooking-kernels-only/test.json')\nsub = pd.read_csv('../input/whats-cooking-kernels-only/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7346a125dcf2cf8493a2405a2d3c18b2ca595f3b","collapsed":true},"cell_type":"code","source":"type(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"487204d6cfd72396ae866a21eb0f548e00b07e5e","collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f3ffdcf4d799dfb94a5fc13abc1d9b3c26c8033","collapsed":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65aaac0e73f39257a44e07dfba23c22bca2572dd"},"cell_type":"markdown","source":"# Metric Code"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c348d9efa07cc299dbd38dc26f9f013bb7a68229"},"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a12cde244b37e8877e44304a02a4f4ba56f1793"},"cell_type":"markdown","source":"We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"96718f6e4b712fc28be1bf33fd645dab681760fe"},"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.cuisine.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90a5067c2bf6ea0b7049ca6707341259d406f405","collapsed":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"788ebef0918ed49d62fe358eb2e9aef8ecfc6ac8","collapsed":true},"cell_type":"code","source":"type(train.ingredients)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52867c438f795a2b68a338351ae49db1282e3c2f","collapsed":true},"cell_type":"code","source":"train.ingredients.str.join(' ').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"837570ef53edfbfd8d56a33c65ee21847057f388"},"cell_type":"markdown","source":"Before going further it is important that we split the data into training and validation sets. We can do it using train_test_split from the model_selection module of scikit-learn."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8de5c91d4650eef4711a22ec5ed9aeee180d7c4f"},"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.ingredients.str.join(' '), y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e273d4ec839b728a98f5fdf7e4b2d1d3578241a","collapsed":true},"cell_type":"code","source":"xtrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec9ef8ced06a6e3ba8a6224584a1e46818073337","collapsed":true},"cell_type":"code","source":"ytrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"098d91797cf1ce28916ee571292afda2c19ae59b","collapsed":true},"cell_type":"code","source":"print (xtrain.shape)\nprint (xvalid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a150c37cb342c3a999d869c9e7f4a8fd9841857"},"cell_type":"markdown","source":"# Building Basic Models"},{"metadata":{"_uuid":"f8f5f7ec624795df70ed205076d103b3a7e557e6"},"cell_type":"markdown","source":"## TF -IDF and LR"},{"metadata":{"_uuid":"52c23cd2a5404bb2047662df1d6d9796937b24a4"},"cell_type":"markdown","source":"Let's start building our very first model.\n\nOur very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b360fd75c892bb04260ed5d0defc94c42eb7f7f8"},"cell_type":"code","source":"# Always start with these features. They work (almost) everytime!\n# tfv = TfidfVectorizer(min_df=3,  max_features=None, \n#             strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n#             ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n#             stop_words = 'english')\n\ntfv = TfidfVectorizer(max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            )\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0e39ff1121e0a6801f0d5592a4ad40fb3c0d381","collapsed":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5041042b65b761d07d038a9c161d40096ecb543"},"cell_type":"markdown","source":"## Count Vectorizer and LR"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"635b919a39a6571e95e8b39bdbf5fb9716e2171d"},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"289cadda65c7e71c19dfd88642314a4888072def","collapsed":true},"cell_type":"code","source":"xtrain_ctv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"754f811c9061929cf4b43c386965c195d7e1dabd","collapsed":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba82eac7c69e44606812a238c50430fd5850dedf"},"cell_type":"markdown","source":"## Multinomial Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"9e012752835226538263f8b9fadc17de778646f7","collapsed":true},"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"987c29edb680f66b41116dc187873373ea8c1cee"},"cell_type":"markdown","source":"# Glove Vectors"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ae3faa70dc02e6b864535517db19577e5deffb21"},"cell_type":"code","source":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dd9b9a66b054031bba4b953828fe879cfd008a3","collapsed":true},"cell_type":"code","source":"f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a8e268967175acf0f25f10139197af3637829ea","collapsed":true},"cell_type":"code","source":"for line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9a6a174d3ccf90ce0e813453720caccfb3fb0968"},"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cfb172ed0f4394e3f20b6a8963a0efe3f8dedac","collapsed":true},"cell_type":"code","source":"# create sentence vectors using the above function for training and validation set\nxtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\nxvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e03b843b72b130715974e7f028f7945976de9509"},"cell_type":"code","source":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f6ba66a002ee9a169b8cc32ede8fe600527a92c","collapsed":true},"cell_type":"code","source":"xtrain_glove","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ab3001408a09e863d58db74e47c037e1ae39bbe"},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true,"_uuid":"90d475b9ef893aa231bb9103a51dd32726ec89b2","collapsed":true},"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(nthread=10, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1bf9b60055116a5e194c72fd3d34bf82d3fa03db"},"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}