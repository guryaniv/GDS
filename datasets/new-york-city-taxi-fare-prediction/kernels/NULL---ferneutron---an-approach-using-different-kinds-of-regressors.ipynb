{"cells":[{"metadata":{"_uuid":"a4bcdfedac518f77bd5f28b7aaa012c2292f9145"},"cell_type":"markdown","source":"# An approach using different kinds of Regressors"},{"metadata":{"_uuid":"3592b7ddb9f3d6a3bda1f1752116068b919b2d92"},"cell_type":"markdown","source":"The idea of this notebook is to show you an approach making use of different regressors which are:\n\n* XGBoosting\n* Random Forest\n* Gradient Boosting Tree\n* Ada Boost Regressor\n\nIn this notebook we compare the performance of each regressor making a variation in the number of estimator for each regressor."},{"metadata":{"trusted":true,"_uuid":"c4369f950bc8396973eeae16e53a9d830f633ca8"},"cell_type":"code","source":"\"\"\"Loading libraries and stuff\"\"\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\"\"\"Preprocessing and metrics\"\"\"\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n\"\"\"Regressors\"\"\"\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n% matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb98725110c29ae175c16a7cb494dff75da9ec73"},"cell_type":"markdown","source":"# 1. Preparing dataset"},{"metadata":{"_uuid":"1d437a2c9f382603c88e65a14054bd2fba368a10"},"cell_type":"markdown","source":"For this exercise, we will only load a million of samples. The original dataset is like 2GB of samples which is so much for my computer, so loading only a million is enoguh for this example."},{"metadata":{"trusted":true,"_uuid":"10ff7951b33550b5e435b69ff583c46404bbe706"},"cell_type":"code","source":"data = pd.read_csv('../input/train.csv', nrows=100000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"861439e78a72c29cbc44655b13a271c7f7bc13c0"},"cell_type":"markdown","source":"Next we will to describe the dataframe to look some interesting things."},{"metadata":{"trusted":true,"_uuid":"0b294e1b86f83d1fa1b7ff6e382e1208ad2b5ce9"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4de8cefb5cab481d1356576756b17c162af88c77"},"cell_type":"markdown","source":"Some important things we can notice is that the average fare is something like 11.3. Another important think is that the standard deviation in either pickup and dropoff longitud is similar as well as pickup and dropoff latitude.\nWe can see that there are some null values, we will proceed to fix it."},{"metadata":{"_uuid":"58b3dd293f691445766192f82e9947c4a36ba4fd"},"cell_type":"markdown","source":"## 1.1 Removing null values"},{"metadata":{"trusted":true,"_uuid":"38cdaa56503f45769b43894dfb0162dbd17c667d"},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"152fbd9ef41f92b0c7c618d2d5f90870f8de2d07"},"cell_type":"code","source":"print('Size with nulls (train): %d' % len(data))\ndata = data.dropna(how = 'any', axis = 'rows')\nprint('Size without nulls (train): %d' % len(data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93203f361b767118a5445fdd0a9d485948437f86"},"cell_type":"markdown","source":"## 1.1 Removing fake fares"},{"metadata":{"_uuid":"5b2198674a788c01acb9d00a19ca480a4b4f1f85"},"cell_type":"markdown","source":"One important thing is analyze if the dataset have noise. In this case we are goint to look if there are some \"fake\" fares. For this case we will accept as a \"fake\" value every sample minor than zero from the fares_amount column, this is because it could not be negative values for this column."},{"metadata":{"trusted":true,"_uuid":"8c78c08d576890cf3604f6a6d209a6c4a9837f44"},"cell_type":"code","source":"k = 0\nfor i in data.fare_amount:\n    if i < 0:\n        k += 1\nprint(\"Number of fake fares: \", k)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00e0b84f7e4dd6720b3bf65a64bef938f2f03a98"},"cell_type":"markdown","source":"There are 38 values which are under 0, this is not possible due to the fare must be more than zero. So, we will to proceed to remove all these values."},{"metadata":{"trusted":true,"_uuid":"045fad7bd8947ee9e73b8d9411294d9dfdf43ac3"},"cell_type":"code","source":"print('Length of original data: %d' % len(data))\ndata = data[data.fare_amount>=0]\nprint('Lengh of new data: %d' % len(data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a19cafcb5f4e0bb7750de9cea3293b49a92e8940"},"cell_type":"markdown","source":"# 2. Visualization"},{"metadata":{"_uuid":"f08248ee832af66919bc3aab72f6e20f8c61b5b1"},"cell_type":"markdown","source":"Always is important to visualizate data, so in this part we are going to go deeper inside dataset for a better understanding."},{"metadata":{"_uuid":"d98876854514c089a0ebac9b08af3165ed5fb8d5"},"cell_type":"markdown","source":"## 2.1 Histogram of fare amount"},{"metadata":{"trusted":true,"_uuid":"79c77c2758c6453a0538b07e625d7550e5a6015b"},"cell_type":"code","source":"data[data.fare_amount<70].fare_amount.hist(bins=70, figsize=(14,3))\nplt.xlabel('Fare $USD')\nplt.title(\"Histogram of fare's distribution\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"784c817ccb1fcd2f537a87948f23e06409218374"},"cell_type":"markdown","source":"It is obvious that the fare amount are centered around 8usd. Also we can see that there are some extra points with fares so expensive, this is more than 45usd."},{"metadata":{"_uuid":"300f2124b891b37d45cf740cb99f70af01ac2122"},"cell_type":"markdown","source":"# 3. Adding new features"},{"metadata":{"_uuid":"d614ee70a80c91d8b7e0cc69726a6785daa1b109"},"cell_type":"markdown","source":"The orginal dataset is composed with the next features:\n\n* fare_amount\t\n* pickup_longitude\t\n* pickup_latitude\t\n* dropoff_longitude\t\n* dropoff_latitude\t\n* passenger_count\n\nwhich are the basic atributes to describe the behavior of the pickup and dropoff process. But, are all of these values important? do we could add some more features to the dataset? which? how?.\nWell, it is important to notice that we could add more features from the dataset, for example the distance between the pickup and dropoff a customer. So, based in this idea we will proceed to add this features to the dataset."},{"metadata":{"trusted":true,"_uuid":"f049495b7bee296e04738609ea7aeeedf29856e5"},"cell_type":"code","source":"data['lon_change'] = abs(data.dropoff_longitude - data.pickup_longitude)\ndata['lat_change'] = abs(data.dropoff_latitude - data.pickup_latitude)\n\nfeatures_train  = ['pickup_longitude',\n               'pickup_latitude',\n               'dropoff_longitude',\n               'dropoff_latitude',\n               'lat_change',\n               'lon_change']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"243e032315c21d81fbfbf045f0e07eccc52ab2a5"},"cell_type":"code","source":"Y = data.fare_amount.as_matrix()\nX = data[features_train]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b924ee6a2a604c81462e04dfbe8f63b52ff030ed"},"cell_type":"code","source":"Y = np.reshape(Y,(Y.shape[0],1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7adfaca912a5c10d5998a86feef5bbd32b9af7bd"},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ffbb53233c4786c24cec99a1ad533a63ca73181"},"cell_type":"markdown","source":"## 3.1 Scaling the Y vector"},{"metadata":{"_uuid":"12e7cfba0183c3b47e786fc9d57781b036d0f508"},"cell_type":"markdown","source":"The target (Y) vector has values bigger than zero, for simplicity we will scale it in a range of 0,1."},{"metadata":{"trusted":true,"_uuid":"2e48453fddc7636cccb1daec374cc2193139400a"},"cell_type":"code","source":"min_max_scaler = preprocessing.MinMaxScaler()\nY = min_max_scaler.fit_transform(Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34254bd70444bfb34186b0dfef5e52c9669c52ce"},"cell_type":"markdown","source":"## 3.2 Splitting data"},{"metadata":{"trusted":true,"_uuid":"e0652dafa18b6a09445640031eaa31232c2a7e17"},"cell_type":"code","source":"Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c194579a6fc070aa5b99a654f7a6c1cbaa0e07d9"},"cell_type":"code","source":"#np.save('Xtrain.npy',Xtrain)\n#np.save('Xtest.npy', Xtest)\n#np.save('Ytrain.npy', Ytrain)\n#np.save('Ytest.npy', Ytest)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8df38b836cb569da8bb915890f8913bda8efe91"},"cell_type":"markdown","source":"# 4. Regression modules"},{"metadata":{"_uuid":"cef6521c993a6a45577ea06616f8d0dc5c8e106a"},"cell_type":"markdown","source":"In this section we will present a class called \"Regressors\" which has defined every algorith we will use in this notebook. "},{"metadata":{"trusted":true,"_uuid":"deb533c23579c9409e9b0b9a1ae6b0efd762cecd"},"cell_type":"code","source":"class Regressors:\n    def __init__(self, Xtrain, Xtest, Ytrain, Ytest):\n        \"\"\"Initializing\"\"\"\n        self.Xtrain = Xtrain\n        self.Xtest = Xtest\n        self.Ytrain = Ytrain\n        self.Ytest = Ytest\n\n    def def_xgboost(self, estimators):\n        \"\"\"Defining the XGBoosting regressor\"\"\"\n        xgb_ = xgb.XGBRegressor(objective ='reg:linear', learning_rate=0.01, max_depth=3, n_estimators=estimators)\n        xgb_.fit(self.Xtrain, self.Ytrain)\n        pred = xgb_.predict(self.Xtest)\n        \n        return pred\n\n    def def_RandomForestRegressor(self, estimators):\n        \"\"\"Defining the Random Forest Regeressor\"\"\"\n        rfr_ = RandomForestRegressor(n_estimators=estimators, max_depth=3)\n        rfr_.fit(self.Xtrain, self.Ytrain)\n        pred = rfr_.predict(self.Xtest)\n\n        return pred\n\n    def def_GradientBoostingRegressor(self, estimators):\n        \"\"\"Defining the Gradient Boosting Regressor\"\"\"\n        gbr_ = GradientBoostingRegressor(n_estimators=estimators, max_depth=3)\n        gbr_.fit(self.Xtrain, self.Ytrain)\n        pred = gbr_.predict(self.Xtest)\n\n        return pred\n\n    def def_AdaBoostRegressor(self, estimators):\n        \"\"\"Defining Ada Boosting Regressor\"\"\"\n        abr_ = AdaBoostRegressor(n_estimators=estimators)\n        abr_.fit(self.Xtrain, self.Ytrain)\n        pred = abr_.predict(self.Xtest)\n\n        return pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02252846850ae3f01a9a6d487654fe3ba52c3915"},"cell_type":"markdown","source":"Now we will define some functions which are \"def_metrics\" and \"plot_performance\". \nThe functionality of \"def_metrics\" is to calculate the mean absolute error and the mean squeare error for the regression algorithms.\nThe functionality of \"plot_ performance\" is to display a graph which help us to visualize in a better way the performance of every regression algorithm."},{"metadata":{"trusted":true,"_uuid":"d40f71d1163c6274df26137701fea0de92493665"},"cell_type":"code","source":"def def_metrics(ypred):\n    mae = mean_absolute_error(Ytest, ypred)\n    mse = mean_squared_error(Ytest, ypred)\n\n    return mae, mse\n\ndef plot_performance(plot_name, loss_mae, loss_mse):\n    steps = np.arange(50, 500, 50)\n    plt.style.use('ggplot')\n    plt.title(plot_name)\n    plt.plot(steps, loss_mae, linewidth=3, label=\"MAE\")\n    plt.plot(steps, loss_mse, linewidth=3, label=\"MSE\")\n    plt.legend()\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Number of estimators\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2faf21ef9d997aca200d8be800c368fe0122038"},"cell_type":"code","source":"\"\"\"Initializing the class\"\"\"\nmodel = Regressors(Xtrain, Xtest, Ytrain, Ytest)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5d011fbdac21fee8cadc6fd3de8af08176d5dd3"},"cell_type":"markdown","source":"## 4.2 XGBoost"},{"metadata":{"trusted":true,"_uuid":"daeb3843daf0399bad37c308d743d99740116251"},"cell_type":"code","source":"plot_name=\"XGBoosting Regressor\"\nloss_mae, loss_mse = [], []\nprint(plot_name)\nfor est in range(50,500,50):\n    print(\"Number of estimators: %d\" % est)\n    mae, mse = def_metrics(model.def_xgboost(estimators = est))\n    print(\"MAE: \", mae)\n    print(\"MSE: \", mse)\n    loss_mae.append(mae)\n    loss_mse.append(mse)\nplot_performance(plot_name, loss_mae, loss_mse)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b65006124e31972572a17f2907974b8b761631e"},"cell_type":"markdown","source":"## 4.2 Random Forest Regressor"},{"metadata":{"trusted":true,"_uuid":"091a1a503fb955ae6464bfc10d2783141712ffe2"},"cell_type":"code","source":"plot_name=\"Random Forest Regressor\"\nloss_mae, loss_mse = [], []\nprint(plot_name)\nfor est in range(50,500,50):\n    print(\"Number of estimators: %d\" % est)\n    mae, mse = def_metrics(model.def_xgboost(estimators = est))\n    print(\"MAE: \", mae)\n    print(\"MSE: \", mse)\n    loss_mae.append(mae)\n    loss_mse.append(mse)\nplot_performance(plot_name, loss_mae, loss_mse)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15d65a8afbc0b93936db5131af1c546a7a347dba"},"cell_type":"markdown","source":"## 4.3 Gradient Boosting Regressor"},{"metadata":{"trusted":true,"_uuid":"5bccc3acc9e665282435a15c1de2615ceaef86aa"},"cell_type":"code","source":"plot_name=\"Gradient Boosting Regressor\"\nloss_mae, loss_mse = [], []\nprint(plot_name)\nfor est in range(50,500,50):\n    print(\"Number of estimators: %d\" % est)\n    mae, mse = def_metrics(model.def_xgboost(estimators = est))\n    print(\"MAE: \", mae)\n    print(\"MSE: \", mse)\n    loss_mae.append(mae)\n    loss_mse.append(mse)\nplot_performance(plot_name, loss_mae, loss_mse)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1c6f025ec29d8980b657766f5695f76d55aaa86"},"cell_type":"markdown","source":"## 4.4 Ada Boost Regressor"},{"metadata":{"trusted":true,"_uuid":"389da28080d18bfe05d8e9d00fe97fcc83b981d1"},"cell_type":"code","source":"plot_name=\"Ada Boost Regressor\"\nloss_mae, loss_mse= [], []\nprint(plot_name)\nfor est in range(50,500,50):\n    print(\"Number of estimators: %d\" % est)\n    mae, mse = def_metrics(model.def_xgboost(estimators = est))\n    print(\"MAE: \", mae)\n    print(\"MSE: \", mse)\n    loss_mae.append(mae)\n    loss_mse.append(mse)\nplot_performance(plot_name, loss_mae, loss_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7035173668a76564fb30e1ef831fa0098c333f1"},"cell_type":"markdown","source":"# 5. Conclusion"},{"metadata":{"_uuid":"f84c3d939083491ee1e2261d564f8eb53ab33abc"},"cell_type":"markdown","source":"In this notebook we have shown how the performance is getting improved realted with the number of estimators for each regression algorithm. A future work could be focused on varying the hyperparamters for every regressor and apply cross validation."},{"metadata":{"trusted":true,"_uuid":"d2d92ecea948acf176a5c5bcbeea3ee54b3517c9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}