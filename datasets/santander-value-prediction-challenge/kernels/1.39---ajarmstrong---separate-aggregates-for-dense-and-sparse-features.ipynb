{"cells":[{"metadata":{"_uuid":"a858803d8cb1b27eb1645c6a4914cc4db8463d9d"},"cell_type":"markdown","source":"# Santander - Separate Aggregates for the dense and sparse feature clusters, applied to simple XGBoost model\n\n\nNote: most of the code in this kernel is taken from a number of other public kernels, including the pre-processing, aggregate feature types and XGBoost code & settings.  Thank you everyone for being so generous with sharing Kernels!!\n\nMy own work is primarily the creation of aggregate features for the most dense columns.  I did this because they standout so much from the other columns that they must come from a different distribution - they hold a different type of transctional or limit information.\nIn other public kernels it can be seen that there are a number of other variable clusters, which suggests 3-4 groups of variables could be created, with aggregate features for each.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.externals import joblib\n\nimport xgboost as xgb\nimport lightgbm as lgb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"796d90c4798cfeeadf86a22eaca07032cd0dd299"},"cell_type":"code","source":"train_file = '../input/train.csv'\ntest_file = '../input/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6c233514b7c047d2bb57e483b311552a9a303b4f"},"cell_type":"code","source":"train = pd.read_csv(train_file)\ntest = pd.read_csv(test_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3157b90a8bf8dd8f915dc0e90f8cbd72f270b90e"},"cell_type":"code","source":"test_ID = test['ID']\ny_train = train['target']\ny_train = np.log1p(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"50966b8bb208ef5f56fb111a9f0ef86ee9be7510"},"cell_type":"code","source":"train.drop(\"ID\", axis = 1, inplace = True)\ntrain.drop(\"target\", axis = 1, inplace = True)\ntest.drop(\"ID\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ef824e22fe82febf52abe8f855b34bee211781f7"},"cell_type":"code","source":"NUM_OF_DECIMALS = 32\ntrain = train.round(NUM_OF_DECIMALS)\ntest = test.round(NUM_OF_DECIMALS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3224a05b529249c2955fcc333f3317f6f2517317"},"cell_type":"markdown","source":"### Split out the dense/sparse clusters here"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3faeae5aa22fa30d165be44c312f942a5f48162b"},"cell_type":"code","source":"train_zeros = pd.DataFrame({'Percent_zero':((train.values)==0).mean(axis=0),\n                           'Column' : train.columns})\n\nhigh_vol_columns = train_zeros['Column'][train_zeros['Percent_zero'] < 0.70].values\nlow_vol_columns = train_zeros['Column'][train_zeros['Percent_zero'] >= 0.70].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8baf0207891e71f6572d4bf1159da7e465a2ad4"},"cell_type":"code","source":"train = train.replace({0:np.nan})\ntest = test.replace({0:np.nan})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8bef57af0f6d66c356f410993b9361bffdc1b3a","collapsed":true},"cell_type":"code","source":"cluster_sets = {\"low\":low_vol_columns, \"high\":high_vol_columns}\nfor cluster_key in cluster_sets:\n    for df in [train,test]:\n        df[\"count_not0_\"+cluster_key] = df[cluster_sets[cluster_key]].count(axis=1)\n        df[\"sum_\"+cluster_key] = df[cluster_sets[cluster_key]].sum(axis=1)\n        df[\"var_\"+cluster_key] = df[cluster_sets[cluster_key]].var(axis=1)\n        df[\"median_\"+cluster_key] = df[cluster_sets[cluster_key]].median(axis=1)\n        df[\"mean_\"+cluster_key] = df[cluster_sets[cluster_key]].mean(axis=1)\n        df[\"std_\"+cluster_key] = df[cluster_sets[cluster_key]].std(axis=1)\n        df[\"max_\"+cluster_key] = df[cluster_sets[cluster_key]].max(axis=1)\n        df[\"min_\"+cluster_key] = df[cluster_sets[cluster_key]].min(axis=1)\n        df[\"skew_\"+cluster_key] = df[cluster_sets[cluster_key]].skew(axis=1)\n        df[\"kurtosis_\"+cluster_key] = df[cluster_sets[cluster_key]].kurtosis(axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23fbd71b6360435b869e5fec0c0649d3a10ebd89"},"cell_type":"markdown","source":"#### I had another version that only dropped the sparse columns and kept the dense columns, but dropping those as well improved LB score."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"52ab62a30b7b7ef594cba8c83778973e744ffef0"},"cell_type":"code","source":"train_more_simplified = train.drop(high_vol_columns,axis=1).drop(low_vol_columns,axis=1)\ntest_more_simplified = test.drop(high_vol_columns,axis=1).drop(low_vol_columns,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d0880f34ce04a2cc31d8b6d139a223d6be85d04","collapsed":true},"cell_type":"code","source":"train_more_simplified.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fccf049e3342db359955fac53bb30486bb2f752d"},"cell_type":"markdown","source":"#### Define XGBoost model.  Note this is taken (almost) straight from another public kernel, although I think I changed the max depth to try to control overfitting. No grid-search performed, could help."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b006982776eb6aa54e24f6a464bc2481a9a25b29"},"cell_type":"code","source":"def run_xgb(train_X, train_y, val_X, val_y, test_X):\n    params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.001,\n          'max_depth': 6, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n    print(\"Load matrices\")\n    tr_data = xgb.DMatrix(train_X, train_y)\n    va_data = xgb.DMatrix(val_X, val_y)\n    \n    print(\"Set watchlist\")\n    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n\n    print(\"Train model\")\n    model_xgb = xgb.train(params, tr_data, 20000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n    \n    dtest = xgb.DMatrix(test_X)\n    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n    return xgb_pred_y, model_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e85dee48733c0cf91eff246f365b2cb104ecc5bf"},"cell_type":"code","source":"dev_X, val_X, dev_y, val_y = train_test_split(train_more_simplified, y_train, test_size = 0.2, random_state = 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"74261e6f2c204dbf44a48c40908319618bd4f4b2","collapsed":true},"cell_type":"code","source":"pred_test_xgb, model_xgb = run_xgb(dev_X, dev_y, val_X, val_y, test_more_simplified)\nprint(\"Finished!\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"243601c26fe7f56ff3dbf6f6bab75b8f253d51e8"},"cell_type":"markdown","source":"# Create submission file"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"948b48a579c2eb0165a51d85ff87f2a9f458727a"},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub[\"target\"] = pred_test_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4f881ec2f3d1612c43dc4febf76c487c7cd9db0b"},"cell_type":"code","source":"sub.to_csv('sub_XGB_Aggregate_v2.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}