{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"file_extension": ".py", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "5b89ba10-3494-4d29-9369-1a1484365c57", "_uuid": "ff141cd49a32e8d7599495f91aed84600e70e6e3", "collapsed": true}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from matplotlib import pyplot as plt\n", "import random\n", "import seaborn as sns\n", "import warnings\n", "\n", "%matplotlib inline\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "79fef0e3-00fe-4842-b61f-3c3bd782c7ea", "_uuid": "4ccbe06c77ded4f658ecd0648cbf43ea4f444123", "collapsed": true}, "outputs": [], "source": ["import plotly\n", "plotly.tools.set_credentials_file(username='....', api_key='......')\n", "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "920506bd-bea2-4621-9473-c6a4df58cb9b", "_uuid": "f18ee8abc3847e0555c6c076bbe27b7ee55d27cb", "collapsed": true}, "outputs": [], "source": ["import plotly.plotly as py\n", "import plotly.graph_objs as go"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "ae63f298-caf2-47f2-aa5f-6a9496612dcf", "_uuid": "cead3a8184f287f67bcacbbf15f325891e89b25a"}, "source": ["## Data Preprocessing\n", "First off, I will conduct a EDA to take a first look at the data and then do preprocessing"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "1585440d-7c0b-43e0-a2a7-965663d10b4b", "_uuid": "c9020124974cf5e9675ecbd2b9a1225bdd533e7b"}, "source": ["### Loading Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "81217956-026a-4996-acc6-e0d2cd8b6ba7", "_uuid": "dae0468c2f29d61e2a4514b51c9921bd308235b3"}, "outputs": [], "source": ["df = open('..../Credit Fraud Detection/creditcard.csv')\n", "df = pd.read_csv(df)\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "d037ba44-96dc-40e2-be4b-cd9f1ff60e30", "_uuid": "846fcc3b56c19150dc76032aecacdafd3174859d", "collapsed": true, "_kg_hide-input": true, "_kg_hide-output": true}, "outputs": [], "source": ["df.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "3a8202ab-6135-4d96-9936-170b2b4e4163", "_uuid": "1e2230fb570598956216f1df4933b81b35401e4a", "collapsed": true}, "outputs": [], "source": ["df.columns[df.isnull().any()].tolist() \n", "# there is no missing values in columns"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d8d14e5c-e063-47be-b6c9-2b7d61625970", "_uuid": "7bd9328297caf1f42f1138860dea06cb39eb3aca"}, "source": ["### Amount column is NOT in line with other columns, and I will standardize it so that it is compatible with other columns"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "060544ee-b22a-4024-b57b-68b1fa1db9a2", "_uuid": "f64252a98dc234c2efcdf93815b7cb72d5ffad72", "collapsed": true, "scrolled": true}, "outputs": [], "source": ["# drop the Time column.since it doesn't make sense in modeling.\n", "df = df.drop(['Time'],axis=1) "]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "fb24294b-6b57-434f-8171-7bbe51965ab2", "_uuid": "b5bcad73513e32517b716ee6c140a2720ede49d4", "collapsed": true}, "outputs": [], "source": ["sns.boxplot(df.Amount);\n", "# There are some outliers in the Amount column, however here I will leave them here "]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "0bdf42ce-cd9c-4ce5-a2f0-c31b26b8717d", "_uuid": "90b7fc67224161232f670617c1f4c4ef96dacdcd", "collapsed": true}, "outputs": [], "source": ["df[df[\"Amount\"] > 10000]\n", "# none of these 7 records is claassified as \"fraud\"."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "9724566e-e6fb-41a3-9197-4634d8d1a9fb", "_uuid": "fd8a42a43270ed8f8ac7a886f3e9ef97c1b8696a"}, "source": ["### Ckeck the feature variables, target variable + skewness of Class"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "f85550fe-8a9f-4f4c-87db-7cb7745dfadb", "_uuid": "f5f4dfcdcea625ad151783800b02374757a11b7d", "collapsed": true}, "outputs": [], "source": ["count_classes = pd.value_counts(df['Class'], sort = True).sort_index()\n", "count_classes = pd.DataFrame(count_classes)\n", "count_classes"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "3ff32c16-a2cc-4256-b005-0ebdb1fa5c08", "_uuid": "dc73c9b38f4e9ac70e185947b1d6145a93ce31f1", "collapsed": true}, "outputs": [], "source": ["labels = [\"Legitimate\", \"Fraud\"]\n", "values = count_classes[\"Class\"].tolist()\n", "\n", "trace = go.Pie(labels=labels, \n", "               values=values, \n", "               textfont=dict(size=18)\n", "              )\n", "\n", "py.image.ishow([trace])"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "84457d71-30dd-46ff-a9b6-7f6ab6305ddc", "_uuid": "3a0177a29f3523440df3223e10009817f8986abb"}, "source": ["It is obvious that this is an imbalanced dataset, the majority of which is 0 (normal transaction) and the minority is 1 (fraud transaction)<br>\n", "Considering that imbalance issues can harm the predictability of traditional models. Therefore, I will resample the training dataset for better modelling.<br>\n", "To make the dataset balanced, I have the following approaches:\n", "1. oversampling\n", "2. undersmapling\n", "3. SMOTE\n", "<br>Here, I want to cut down the majority class sample size to match the minority, ie. undersampling approach"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "defb91c2-ebcc-47d4-adfb-a87dc341162e", "_uuid": "010882d2980019b6e3bcffcec28e0e164732d4b7"}, "source": ["### Before downsampling, I need to split the data into training and test sets"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "cdc21e24-2ed6-4e96-a28d-cc15a19e450e", "_uuid": "0c732e8a577ed3529416904a643021431f0cc1d8", "collapsed": true}, "outputs": [], "source": ["from sklearn.cross_validation import train_test_split\n", "np.random.seed(37)\n", "x = df.iloc[:, df.columns != 'Class']\n", "y = df.iloc[:, df.columns == 'Class']\n", "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "72445ecf-ad69-4da9-ae85-8fdb652c217e", "_uuid": "9185a03c977cc66a92a8a23be22b07e0acc44a34"}, "source": ["#### After data split, I can downsample training data to get the dataset balanced  in terms of class (fraud vs. normal) (50-50 ratio)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "4064fdc3-173f-40b2-a779-620248d83886", "_uuid": "9ebbfb8bf7133a730d9e7aabb04d0f0755155900", "collapsed": true}, "outputs": [], "source": ["count_fraud = len(y_train.ix[y_train.Class == 1, :])\n", "count_normal = len(y_train.ix[y_train.Class == 0, :])\n", "print count_fraud, count_normal # unbalanced training data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "ddfb9858-dc8c-4fcb-a964-2cbbe79814a0", "_uuid": "c2639bd7522cb1a5541652f05b6141e5c55b3fd3", "collapsed": true}, "outputs": [], "source": ["normal_indices = y_train[y_train.Class == 0].index\n", "fraud_indices = y_train[y_train.Class == 1].index\n", "\n", "# randomly select \"normal\" records\n", "np.random.seed(307)\n", "random_normal_indices = np.random.choice(normal_indices, count_fraud, replace = False)\n", "undersample_indices = np.concatenate([fraud_indices,random_normal_indices])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "92300857-808a-43a9-8eaa-9f608760ed56", "_uuid": "12ad5d3efa236398493eba6084a9a9f4b38084e4", "collapsed": true}, "outputs": [], "source": ["train_undersampled = df.iloc[undersample_indices]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "be850b2c-b476-4aaa-92fe-ce2f40fdd9d5", "_uuid": "db3d2e6bfd6b6ade5ee5169a09a9e114be2a091e", "collapsed": true}, "outputs": [], "source": ["feature_set_1 = pd.concat([train_undersampled.iloc[:, 0:15], train_undersampled[\"Class\"]], axis=1)\n", "feature_set_2 = train_undersampled.iloc[:, 15:31]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "b87411f1-0664-4f7a-8d18-dad1d701e35e", "_uuid": "66e9773f16971cf744f935f1336d25f61c8219e9", "collapsed": true}, "outputs": [], "source": ["sns.pairplot(feature_set_1, hue = \"Class\");"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "b4425328-ab30-4d69-9bb3-6a01caabbdaa", "_uuid": "c44d9b3f6e91f790d912541573cf6bf1f8e197f6", "collapsed": true}, "outputs": [], "source": ["sns.pairplot(feature_set_2, hue = \"Class\");"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "7a2cc098-0653-46d8-8ac5-b7b7cce19457", "_uuid": "272819869a4b175b1d4bddeeb3a0b4f6e2f98072"}, "source": ["It is obvious that the two classess can't seperate from one another, so logistic regression and random forest might be good choices for modeling. (SVC and Dicriminant Analysis both are better performed when the classes can be seperated from each other pretter well)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "96f87b31-e7b3-4f62-a880-3893b516eb45", "_uuid": "57d2c65206113418bf7368838c33d0ff48be7ead"}, "source": ["# Modeling"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "d9f72587-67d5-44d4-ae97-e040593fcdd8", "_uuid": "c7fb94a63d81069f053732422c51192e5c182d62", "collapsed": true}, "outputs": [], "source": ["x_train_undersample = x_train.ix[undersample_indices, :]\n", "y_train_undersample = y_train.ix[undersample_indices, :]"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "ab237ede-2246-4c5a-8eaa-7226143ebf13", "_uuid": "57501ab997e33bc46dc0e6311d17c5e1933a944e"}, "source": ["### 1. Logistic Classifier (L1 penalty) "]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "241dac7e-cc51-450a-9295-5e122c45eb74", "_uuid": "d1b965c723af11722f84046849a5bd993ee10e09", "collapsed": true}, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "dfdbb57a-f0c9-43a5-8d46-54e434ce585b", "_uuid": "eb8067f2a33029f640cebdea7e97af3b63704777", "collapsed": true}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegressionCV"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "cadcb2a5-0c33-4921-82c8-322af80b1182", "_uuid": "2bce7f83f4263af7af8c2a82f5b29db96bf3def1", "collapsed": true}, "outputs": [], "source": ["logit = LogisticRegressionCV(penalty='l1', cv = 10, solver=\"liblinear\")\n", "logit = logit.fit(x_train_undersample, y_train_undersample.values.ravel())      \n", "print \"The best parameter C is\",logit.C_[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "f0215532-567c-4470-ab45-01069fbf8d37", "_uuid": "9b26c44ef11d56636ee6e15428546101e5dd121e", "collapsed": true}, "outputs": [], "source": ["ceof_ = logit.coef_.tolist()[0]\n", "feature_names = x_train_undersample.columns.values.tolist()\n", "pd.DataFrame({\n", "    \"feature_name\": feature_names,\n", "    \"values\": ceof_\n", "})"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "608e05a3-592b-40e1-aafa-9f01772c2e57", "_uuid": "fe4ebe08caad83b61e2072082577377432202293"}, "source": ["### 2. Random Forest Classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "ce27f1b3-2a35-485a-ac72-a33e88ed414c", "_uuid": "57c80934f396ebd83a4b618d881edee60ea565c9", "collapsed": true}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "23a7284d-f292-4275-a0ac-9b5f27851f19", "_uuid": "d1c6e25b2811ebd05af23649d76273cff8a9d3b3", "collapsed": true}, "outputs": [], "source": ["rfc = RandomForestClassifier(n_jobs = -1, bootstrap = True, oob_score = True) \n", "\n", "param_grid = { \n", "    'n_estimators': [50, 100, 200, 500, 700],\n", "    'max_features': ['sqrt', 'log2'],\n", "    'criterion': ['gini', 'entropy']\n", "}\n", "\n", "clf_rfc = GridSearchCV(rfc, param_grid=param_grid, cv= 5)\n", "clf_rfc = clf_rfc.fit(x_train_undersample, y_train_undersample.values.ravel())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "ba92e637-89d3-4460-b868-99ae1cd7cbb4", "_uuid": "9c6e35f523c7fee3e95e03168448c029c9207bb7", "collapsed": true}, "outputs": [], "source": ["print 'The best parameters are', clf_rfc.best_params_"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0a8e2d7b-c482-456b-9c62-592f48d63713", "_uuid": "dce08f0f364316f0f53c53537528a7a59de840dc"}, "source": ["### 4. Resampling + Model Selection"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "de52e917-146d-4811-b799-f6f0282fac5e", "_uuid": "0ae8e6d90b6ce8d231c0357d0316130f2a183ed3", "collapsed": true}, "source": ["#### I will regenerate data from training data to make prediction with the . Then I will make a new undersampling training dataset for model selection."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "5c554fb6-b484-4c8c-9373-7b0589b4dadd", "_uuid": "d60987a1b5051b81ac0f205953efdd220a79f388", "collapsed": true}, "outputs": [], "source": ["from sklearn.metrics import precision_recall_curve, precision_score, roc_auc_score, roc_curve, confusion_matrix, auc, classification_report, recall_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "daad546e-ca49-4540-8a76-b32a0d4f19cb", "_uuid": "df6f0dbcc1faa8159b79a849b11ec309d674c556", "collapsed": true}, "outputs": [], "source": ["#normal_indices_complement = map(lambda x: (x in normal_indices) & (x not in random_normal_indices), normal_indices)\n", "np.random.seed(370)\n", "random_normal_indices_1 = np.random.choice(normal_indices, count_fraud, replace = False)\n", "undersample_indices_1 = np.concatenate([fraud_indices,random_normal_indices_1])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "443bead6-71a4-43fd-8faf-eeaf182b4df1", "_uuid": "f520dfd354aff6f8da8ac318f1d8160a81f22580", "collapsed": true}, "outputs": [], "source": ["x_train_undersample_1 = x_train.ix[undersample_indices_1, :]\n", "y_train_undersample_1 = y_train.ix[undersample_indices_1, :]\n", "#print type(y_train_undersample_1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "aa74c176-491e-4312-ab36-7a1da7972801", "_uuid": "52d28ed0ab4dd7713380f3f921c24b430a1bc0a7", "collapsed": true}, "outputs": [], "source": ["# predict with the default threshold 0.5\n", "\n", "y_train_pred_logit = logit.predict(x_train_undersample_1.values)\n", "y_train_pred_rfc = clf_rfc.predict(x_train_undersample_1.values)\n", "#print type(y_train_pred_rfc)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e41d5f6e-a644-4fdd-93bf-25f435c8a7f2", "_uuid": "b7bf8263fb2aa7806556f9567aac02ea53a58022"}, "source": ["#### use ROC with auc score and PR Curve to evaluate model performance."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "35c1db93-fdbb-4ce8-96d0-cdbcf887251b", "_uuid": "bf8df7b07a0b29a0ddb91ee5e713fb0df6419cfc", "collapsed": true}, "outputs": [], "source": ["import itertools\n", "np.set_printoptions(precision=3)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "e6ada945-b021-4117-83b8-8dfc9540451a", "_uuid": "87d9dbc48f7808c94a1d43d71cda1da26dee74c5", "collapsed": true}, "outputs": [], "source": ["\"\"\"\n", "    This function prints and plots the confusion matrix.\n", "    Normalization can be applied by setting `normalize=True`.\n", "    \"\"\"\n", "\n", "def plot_confusion_matrix(cm, classes,\n", "                          normalize=False,\n", "                          title='Confusion matrix',\n", "                          cmap=plt.cm.Blues\n", "                         ):\n", "    \n", "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    plt.title(title)\n", "    plt.colorbar()\n", "    tick_marks = np.arange(len(classes))\n", "    plt.xticks(tick_marks, classes, rotation=0)\n", "    plt.yticks(tick_marks, classes)\n", "    plt.grid(False)\n", "\n", "    if normalize:\n", "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "        #print(\"Normalized confusion matrix\")\n", "    else:\n", "        1#print('Confusion matrix, without normalization')\n", "\n", "    #print(cm)\n", "\n", "    thresh = cm.max() / 2.0\n", "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "        plt.text(j, i, cm[i, j],\n", "                 horizontalalignment=\"center\",\n", "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n", "\n", "    plt.tight_layout()\n", "    plt.ylabel('True label')\n", "    plt.xlabel('Predicted label')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "0e79d814-b00b-4f95-87e3-dcb6603f0f06", "_uuid": "d328a1316a7cbc6b6c3dfae0f6018b45ded9bedb", "collapsed": true, "_kg_hide-input": false, "_kg_hide-output": false}, "outputs": [], "source": ["# ROC CURVE\n", "def plot_roc_curve(clf, x_train, y_train):\n", "    \n", "    y_pred_score = clf.predict_proba(x_train)[:,1]\n", "    #print \"y_pred_score: \"\n", "    #print(y_pred_score)\n", "    #print \"y_true: \"\n", "    #print(y_train.values.ravel())\n", "    fpr, tpr, thresholds = roc_curve(y_train.values.ravel(), y_pred_score)\n", "    roc_auc = auc(fpr,tpr)\n", "    # Plot ROC\n", "    plt.title('Receiver Operating Characteristic')\n", "    plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n", "    plt.legend(loc='lower right')\n", "    plt.plot([0,1],[0,1],'r--')\n", "    plt.xlim([-0.1,1.0])\n", "    plt.ylim([-0.1,1.01])\n", "    plt.ylabel('True Positive Rate')\n", "    plt.xlabel('False Positive Rate')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "82f514ca-0af9-49e2-9f44-449e5487c984", "_uuid": "e9da3000e57a8302a2dcba39dbefc9c2bef1d8b4", "collapsed": true}, "outputs": [], "source": ["# Precision Recall Curve\n", "def plot_Precision_Recall_Curve(clf, x_train, y_train):\n", "    \n", "    probas_pred = clf.predict_proba(x_train)[:,1]\n", "    precision, recall, thresholds = precision_recall_curve(y_train.values.ravel(), probas_pred)\n", "    #print 'Pricision: ', precision\n", "    pr_auc = auc(recall, precision)\n", "    # Plot Precision-Recall curve\n", "    plt.plot(precision, recall, label='AUC = %0.2f'% pr_auc)\n", "    plt.xlabel('Recall')\n", "    plt.ylabel('Precision')\n", "    plt.ylim([0.0, 1.03])\n", "    plt.xlim([0.0, 1.03])\n", "    plt.title('Precision-Recall example')\n", "    plt.legend(loc=\"lower left\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "0a94f255-cd46-4d63-ac69-9eb0836cc3b2", "_uuid": "9a55a202c9f49ff14eec1590181dd03395f534a5", "collapsed": true}, "outputs": [], "source": ["cm_logit = confusion_matrix(y_train_undersample_1,y_train_pred_logit)\n", "cm_rfc = confusion_matrix(y_train_undersample_1,y_train_pred_rfc)\n", "classes_name = ['Legitimate', 'Fraud']"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "49b58ab4-640b-4519-87cc-2fb56dac27f4", "_uuid": "96b6db0ef9b11f37208b7b1dfd44368ebbc6deb3"}, "outputs": [], "source": ["# with the default threshold 0.5, how do those three models work? show with the confusion matrix\n", "figure = plt.figure(figsize=(16, 6))\n", "\n", "ax = figure.add_subplot(1,2,1)\n", "plot_confusion_matrix(cm_logit, classes_name)\n", "plt.title(\"Confusion Matrix of Logit\", fontsize = 15)\n", "\n", "\n", "ax = figure.add_subplot(1,2,2)\n", "plot_confusion_matrix(cm_rfc, classes_name)\n", "plt.title(\"Confusion Matrix of Random Forest\", fontsize = 15);"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "433cce89-20e5-4701-9f30-4e39df9014b7", "_uuid": "95059051305d2bd7829e93daacbe94748e7c2a05", "collapsed": true}, "outputs": [], "source": ["# ROC with AUC score\n", "figure = plt.figure(figsize=(16,7))\n", "\n", "ax = figure.add_subplot(1,2,1)\n", "plot_roc_curve(clf_logit, x_train_undersample_1, y_train_undersample_1)\n", "plt.title(\"ROC of Logit\")\n", "\n", "ax = figure.add_subplot(1,2,2)\n", "plot_roc_curve(clf_rfc, x_train_undersample_1, y_train_undersample_1)\n", "plt.title(\"ROC of Random Forest\");"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "f45b9fb2-9db2-430b-9276-09044766832a", "_uuid": "64b07cc10867a4b5925563e8729db886a86365bf", "collapsed": true}, "outputs": [], "source": ["# Precison Recall curve with auc score\n", "\n", "figure = plt.figure(figsize=(16,7))\n", "\n", "ax = figure.add_subplot(1,2,1)\n", "plot_Precision_Recall_Curve(clf_logit, x_train_undersample_1, y_train_undersample_1)\n", "plt.title('PR Curve of Logit')\n", "\n", "ax = figure.add_subplot(1,2,2)\n", "plot_Precision_Recall_Curve(clf_rfc, x_train_undersample_1, y_train_undersample_1)\n", "plt.title('PR Curve of Random Forest');"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d0fc9f46-f52c-40e7-9149-17b72c398f62", "_uuid": "235576533c7ffcf9ba1ba0bd185502025ea44e1e"}, "source": ["Conclusion:<br>\n", "Random Forest gave an amazing performance! BUT it is NOT the end. In reality, financial institutes will pay financially when they misclassify the transactions, and the conventional model doesn't take this into consideration, so at this point I'd like to introduce a cost sentitive model to make classification -- Bayesian Risk Minimization Approach"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "ead88252-b9d5-4398-b83b-fc11233c3aad", "_uuid": "8e86bfbaf899ebfab4b83c084d213081b178493a"}, "source": ["[](http://)# Cost Sensitive Classfier"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "4aee1862-8196-40f0-b5d7-1e058b025a26", "_uuid": "9bb7ee3f792b90060331b7c09948ad5d8ce72fc6"}, "source": ["## Background of cost sensitive models "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "38e398e5-b211-4143-a1f2-82dcbbc3e4d6", "_uuid": "7d2d050a58c3682c4a594539bbecd433394ab3e5"}, "source": ["Either wrongly detect a fraudulent transaction or fail to detect a fraudulent transaction is costly. <br> Taking cost into consideration and thus choose a classifer that minimizes the total cost is optimal."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "09bf8cd3-2d55-49a8-89a7-6716bbaefcfc", "_uuid": "15eadd1b36c716f7de9b9169c997c1bf5776381c"}, "source": ["### Cost Matrix"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "49b40bb2-72ea-4155-ac29-585a8b0d145e", "_uuid": "2ccef30f31f8ea3725f7438639d567760d0cba0a"}, "source": ["1. (Fraud, Fraud): both the detected and true labels are \"fraud\", where financial institutes need a fixed number of cost to deal with this situation, let's say $C_a$.\n", "2. (Legitimate, Fraud): FIs detect as normal but actually is a fraud, where the financial institute would lose the total numnber spent on this transaction. \n", "3. (Fraud, Legitimate): FIs detect as a Fraud but actually a normal one, where card holders would call the FIs to repay the order with 50% probability and give up the transaction with 50% probability. Actually there are intangible losses for FIs in this situation which is the satisfaction decline of the cusotmers, but at this point I just neglect it. \n", "4. (Legitimate, Legitimate): no cost."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "8b873430-908e-4f20-822c-d4c901dcd67c", "_uuid": "5ce9c32257a00e70060c0b0f0435b9a8e97eb3fb"}, "source": ["|        | $Cost Matrix$|\n", "| ------ | ------ | ------ |\n", "|        |  True Label $$(t_i)$$ |     |\n", "| Predicted Label $$(p_i)$$| Fraud (1)| Legitimate (0)|\n", "| Fraud (1) | $$C_a$$ | $$0.5 Amt_i$$ |\n", "| Legitimate (0)| $$Amt_i$$ | 0 |\n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "415fe838-ea6f-4f27-8671-a56689ca1cc5", "_uuid": "14182c2b6b985239cf7586a7b747a4ee686575c3"}, "source": ["### Cost Function"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "cfffe5a6-642d-4a30-8683-1f747007609f", "_uuid": "af7118b24882ca85040ebdf4ed5ffee1941a9a96"}, "source": ["###  $$cost = \\sum\\limits_{i=1}^n t_i (p_i C_a + (1 - p_i) Amt_i) + (1 - t_i) (0.5 Amt_i)$$ "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "2c43f7c1-dea7-4176-a160-4ce94d564ef5", "_uuid": "371d41b1f9c99231fbd7ed995f0c58b78184bc78"}, "source": ["## A threshold based approach\n", "First I'd like to introduce the cost funciton to the Logisitc model and RF model I got above and to choose the optimal model and its corresponding threshold to minimize the total cost."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "cc409772-5766-4f60-ba7e-71745bd4b888", "_uuid": "ab42392ad6355faefff926c2aaa9f5fbe8eecc7d"}, "source": ["Suppose $C_a$ is 2 UDS."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "0cc3f85a-879e-4330-9a45-0406d19e56ef", "_uuid": "0f51452c6b536a862b4bcd4ab9ed1319362af5e4", "collapsed": true}, "outputs": [], "source": ["thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "334e5154-bf34-4c44-9a2a-d7bbea9d4b78", "_uuid": "6dafc758d69aa2c5e0f2f18f7953c1b2f6781331", "collapsed": true}, "outputs": [], "source": ["def create_cost_df(model, x, y, thresholds):\n", "    results = []\n", "    Amount = x.Amount.tolist()\n", "    for i in range(len(thresholds)):\n", "        y_pred = model.predict_proba(x)[:,1] > thresholds[i]\n", "        y_pred = 1*y_pred\n", "    \n", "        result_i = pd.DataFrame(\n", "            {\n", "                'Amt': Amount,\n", "                't': y.Class.tolist(),\n", "                'p': y_pred\n", "            }\n", "        )\n", "        \n", "\n", "        result_i[\"Ca\"] = 2\n", "        result_i[\"cost\"] = result_i.t*(result_i.p*result_i.Ca + (1 - result_i.p)*result_i.Amt) + (1 - result_i.t)*result_i.Ca\n", "        result_i[\"recall_score\"] = recall_score(y, y_pred)\n", "        result_i[\"precision_score\"] = precision_score(y, y_pred)\n", "        \n", "        result_i = result_i.groupby(['recall_score','precision_score'])['cost'].mean().reset_index()\n", "        result_i['threshold'] = thresholds[i]\n", "        \n", "        results.append(result_i)\n", "    \n", "    results = pd.concat(results, axis=0, ignore_index=True)\n", "    return results"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a96bd022-fad7-4c91-8e80-f808456bf55f", "_uuid": "3303b32feb59c9e118b72b8ef864ed3020bda51d"}, "source": ["Logistic Reg."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "c540b3aa-36db-49d6-bfd9-52e35856b840", "_uuid": "0a6d3c3c1abf9f8cc36a2882bdc3322f40d17a5e", "collapsed": true}, "outputs": [], "source": ["cost_df_logit = create_cost_df(logit, x_train_undersample_1, y_train_undersample_1, thresholds)\n", "cost_df_logit"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b0caef01-6e06-455b-bc6c-a786160b6b72", "_uuid": "6c91d82a608a31ea78b58af7c161fa6ede8346af"}, "source": ["Random Forest Classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "e21926a0-7064-4484-a943-72039e9bafda", "_uuid": "cd18bae6764306587fe8f6fa993f2e444fcfc201", "collapsed": true}, "outputs": [], "source": ["cost_df_rfc = create_cost_df(clf_rfc, x_train_undersample_1, y_train_undersample_1, thresholds)\n", "cost_df_rfc"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "4ef35be2-f5b0-48c5-9795-892117b33c01", "_uuid": "c51ee5b8e5814a301f6c8303d061a8f83f81f075", "collapsed": true}, "outputs": [], "source": ["cost_bar_logit = go.Scatter(x=cost_df_logit.threshold,\n", "                            y=cost_df_logit.cost,\n", "                            name='Toal Cost of Logistic Model',\n", "                            mode = \"lines+markers\"\n", "                           )\n", "\n", "cost_bar_rfc = go.Scatter(x=cost_df_rfc.threshold,\n", "                          y=cost_df_rfc.cost,\n", "                          name='Total Cost of Random Forest Model',\n", "                          mode = \"lines+markers\"\n", "                         )\n", "\n", "layout = go.Layout(title='Total Cost by Models',\n", "                    yaxis=dict(\n", "                        title='Cost in USD',\n", "                        showgrid = True,\n", "                        range = [0,15]\n", "                    ),\n", "                    xaxis=dict(\n", "                        title='Threshold'\n", "                    ),\n", "                    legend=dict(\n", "                       x=0,\n", "                       y=1,\n", "                       bgcolor='rgba(255, 255, 255, 0)',\n", "                       bordercolor='rgba(255, 255, 255, 0)'\n", "                    )\n", "                   )\n", "\n", "data = [cost_bar_logit, cost_bar_rfc]\n", "\n", "fig = go.Figure(data=data, layout=layout)\n", "py.image.ishow(fig)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "f7af8bc9-d4ab-480c-a195-b80f05476145", "_uuid": "680d5f51ac9cd266e70016d2b54abc357732e554", "collapsed": true}, "outputs": [], "source": ["recall_logit = go.Scatter(x=cost_df_logit.threshold,\n", "                          y=cost_df_logit.recall_score,\n", "                          name='Recall of Logit',\n", "                          mode = \"lines\",\n", "                          marker = dict(\n", "                              color = \"#E1396C\"\n", "                          )\n", "                         )\n", "\n", "recall_rfc = go.Scatter(x=cost_df_rfc.threshold,\n", "                        y=cost_df_rfc.recall_score,\n", "                        name='Recall of RF',\n", "                        mode=\"lines\",\n", "                        marker=dict(\n", "                            color = \"#FEBFB3\"\n", "                        )\n", "                       )\n", "    \n", "precision_logit = go.Scatter(x=cost_df_logit.threshold,\n", "                             y=cost_df_logit.precision_score,\n", "                             name='Precision of Logit',\n", "                             mode = \"lines\",\n", "                             marker=dict(\n", "                                 color = \"rgb(55, 83, 109)\"\n", "                             )\n", "                            )\n", "\n", "\n", "precision_rfc = go.Scatter(x=cost_df_rfc.threshold,\n", "                           y=cost_df_rfc.precision_score,\n", "                           name='Precision of RF',\n", "                           mode = \"lines\",\n", "                           marker=dict(\n", "                                 color = \"rgb(26, 118, 255)\"\n", "                             )\n", "                            )\n", "\n", "layout = go.Layout(\n", "    title='Model Performance',\n", "    yaxis=dict(\n", "        title='Score'\n", "    ),\n", "    xaxis=dict(\n", "        title='Treshold'\n", "    ),\n", "    barmode = \"group\"\n", ")\n", "    \n", "\n", "data = [recall_logit, precision_logit, recall_rfc, precision_rfc]\n", "\n", "fig = go.Figure(data=data, layout=layout)\n", "\n", "py.image.ishow(fig)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d9470e03-d861-4ec0-8baf-3462c9da48a3", "_uuid": "70dc00815db948a349cfb41b092a026d66b4749c"}, "source": ["Conclusion:<br>\n", "Combining the cost graph and model performance graph above, I can conclude that Random Forest model with threshold 0.5 is the optimal one, which can minizie the total cost while leave the model performance well."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "acba6088-e04d-4995-a39b-8acbcefb775b", "_uuid": "6b46ccebea41882d95d682bf419ea005897bc68c"}, "source": ["## Bayesian Risk Minimization Classifer"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0ebe8eda-5a6b-4dce-aece-63a4da38c341", "_uuid": "d32ef62b554bc116196eefb3539199b6bc7790a3"}, "source": ["Basic Idea:<br>\n", "a decision model based on quantifying tradeoffs between various decisions using probabilities and the costs that accompany such decisions. "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "31e81b4e-0908-4e99-abe7-7926e8cf8dae", "_uuid": "c178a51f8dac5af96ea4886f2b2900f9ac6b9bf7"}, "source": ["$$R(p_f|X) = L(p_f|t_f) P(p_f|X) + L(p_f|t_l) P(p_l|X)$$"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "40d6fd50-fe53-4dbe-8123-9b8c8f9f15dc", "_uuid": "fd5051eeaa88545a61dd217593674828f2bfe0ff"}, "source": ["$$R(p_l|X) = L(p_l|t_f) P(p_f|X) + L(p_l|t_l) P(p_l|X)$$"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "51fb402d-429a-44a1-8210-19f185341ec1", "_uuid": "f008c02a83e6c42655838c12d4cd95df2ead461f"}, "source": ["Class as fraud if $$R(p_f|X) =< R(p_l|X)$$"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "86994384-4cf2-44dc-a225-58c9f2ad3011", "_uuid": "9427be882702d40ba88fcbc2647c2b0f175d6131"}, "source": ["Computing with the cost matrix above, we can rewrite the risk function as:"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "bd0189eb-0077-4bda-800a-78cd1b00ba8b", "_uuid": "4968d5b9d5a607a30dd3275f85c1959bb9f8ec96"}, "source": ["$$ R(p_f|X) = C_a P(p_f|X) + 1/2 Amt_i P(p_l|X)$$"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b6c54942-9b1f-4aed-aa9c-18735bf0d88b", "_uuid": "9bde63eab298ae3b2dac4187eff6f395cc07b632"}, "source": ["$$R(p_l|X) = Amt_i P(p_f|X)$$"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d28ca537-0a8f-4f35-a782-c788d7a78338", "_uuid": "29162d67c3668cf7493ff1250f4e617faf042d8f"}, "source": ["$$cost = \\sum\\limits_{i=1}^n t_i (p_i C_a + (1 - p_i) Amt_i) + (1 - t_i) (0.5 Amt_i)$$ "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0ba93194-34e5-4b01-9be1-f5521a2ce493", "_uuid": "1894971c1515874050038bfe4a7908a2ab13b039"}, "source": ["Suppose $C_a$ to be 2 USD"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "2262631c-93ac-46de-b805-03a565e1677f", "_uuid": "2bd2f30b3fe6d5beafcff4447e72f78fd48a8fd0", "collapsed": true}, "outputs": [], "source": ["def Create_Cost_DF(clf, x, y):\n", "    prob_fraud = clf.predict_proba(x)[:,1]\n", "    prob_legitimate = clf.predict_proba(x)[:,0]\n", "    Amount = x.Amount.tolist()\n", "    y_true = y.Class.tolist()\n", "    \n", "    \n", "    cost_df = pd.DataFrame({\n", "        \"t\": y_true,\n", "        \"prob_fraud\": prob_fraud,\n", "        \"prob_legitimate\": prob_legitimate,\n", "        \"Amount\": Amount\n", "    }\n", "    )\n", "    \n", "    cost_df[\"Ca\"] = 2\n", "    cost_df[\"risk_fraud\"] = cost_df.Ca*cost_df.prob_fraud + cost_df.Ca*cost_df.prob_legitimate\n", "    cost_df[\"risk_legitimate\"] = cost_df.Amount*cost_df.prob_fraud\n", "    \n", "    \n", "    '''\n", "    if risk of classifying as fraud is less than the risk of classifying as legitimate, \n", "    then classify as fraud. ie. p = 1\n", "    '''\n", "    for i in range(len(y)):\n", "        if cost_df.risk_fraud[i] <= cost_df.risk_legitimate[i]:\n", "            cost_df.ix[i,\"p\"] = 1\n", "        else:\n", "            cost_df.ix[i,\"p\"] = 0\n", "    \n", "    cost_df[\"cost\"] = cost_df.t*(cost_df.p*cost_df.Ca + (1 - cost_df.p)*cost_df.Amount) + (1 - cost_df.t)*cost_df.Ca\n", "    \n", "    return cost_df"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "d9cdfe7b-6896-4532-b6ba-f7435b021ee3", "_uuid": "2cc1ccc69a0e7669fe42cd53865ccc715c086b2c", "collapsed": true}, "outputs": [], "source": ["cost_logit = Create_Cost_DF(logit, x_train_undersample_1, y_train_undersample_1)\n", "\n", "recall_logit = recall_score(cost_logit.t, cost_logit.p)\n", "precision_logit = precision_score(cost_logit.t, cost_logit.p)\n", "mean_cost_logit = cost_logit.cost.mean()\n", "\n", "print \"The recall of logit is %f\" % (recall_logit)\n", "print \"--------------------------------\"\n", "print \"The precision of logit is %f\" % (precision_logit)\n", "print \"--------------------------------\"\n", "print \"The cost of logit is %f\" % (mean_cost_logit)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "0e0d25f1-ef10-4aab-b7c7-7a964f19b8a2", "_uuid": "0b19e683921406a783c4152905e38f55c082f1a0", "collapsed": true}, "outputs": [], "source": ["cost_rfc = Create_Cost_DF(clf_rfc, x_train_undersample_1, y_train_undersample_1)\n", "\n", "recall_rfc = recall_score(cost_rfc.t, cost_rfc.p)\n", "precision_rfc = precision_score(cost_rfc.t, cost_rfc.p)\n", "mean_cost_rfc = cost_rfc.cost.mean()\n", "\n", "print \"The recall of RF is %f\" % (recall_rfc)\n", "print \"--------------------------------\"\n", "print \"The precision of RF is %f\" % (precision_rfc)\n", "print \"--------------------------------\"\n", "print \"The cost of RF is %f\" % (mean_cost_rfc)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "37203916-8823-4f71-85c4-7a5d1b535378", "_uuid": "cc34b4ff28807566016060d10abd58818ebb6406"}, "source": ["From the analysis result, I derive that the risk mimimization approach based on RF decreases the cost by 11% compared to the threshold based approach at an expense of the lower recall and precision."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "81ff9af9-5b1b-48e0-b050-623595fd1e88", "_uuid": "d0d6b47cb369c4447d39b6350fcb7b56aac84fb1"}, "source": ["## Prediction on test data"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e7e2652f-3bdd-4461-91a2-96156275c46b", "_uuid": "6cc3dcfee9f0c4adf438e4a248240ac1d144a435"}, "source": ["Finally, I am going to user the models to make predictions on the test data"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a0db13c9-1b8f-46c2-a0c6-5774a97362b9", "_uuid": "9328d3faf06d31e22b1501506355bc6b24ebfeed"}, "source": ["Model 1: Random Forest with threshold 0.5"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "f72a9cbf-3993-4892-9e7c-9c2fa6974b2c", "_uuid": "fd092c15812923075ee36e855c1a90f1fc44ade7", "collapsed": true}, "outputs": [], "source": ["'''\n", "The optimal model is rfc with threshold 0.5\n", "'''\n", "\n", "y_test_pred = clf_rfc.predict_proba(x_test)[:,1] > 0.5\n", "y_test_pred = 1*y_test_pred\n", "\n", "y_test_true = y_test\n", "cm_test = confusion_matrix(y_test_true, y_test_pred)\n", "\n", "classes_name = ['Legitimate', 'Fraud']\n", "\n", "fig = plt.Figure(figsize=(10,8))\n", "ax = fig.add_subplot(1,1,1)\n", "plot_confusion_matrix(cm_test, classes_name,\n", "                          title='Confusion matrix of Test Data'\n", "                         )\n", "\n", "print \"Recall is %.3f\" % (float(cm_test.item(3))/(cm_test.item(3)+cm_test.item(2)))\n", "print \"Precision is %.3f\" % (float(cm_test.item(3))/(cm_test.item(3)+cm_test.item(1)))\n", "\n", "cost_test_df = create_cost_df(clf_rfc, x_test, y_test, thresholds)\n", "print \"The cost is %f\" % (cost_test_df[cost_test_df.threshold == 0.5][\"cost\"])"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "94829860-6e1e-4db6-bfc9-8487e8007b80", "_uuid": "18adf1be893607de275f0f7fb5921938ef0a1c66"}, "source": ["Model 2: Random Forest Model with Risk Minimization Approach"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "9e7f1866-df7c-42d4-8005-179777ad9782", "_uuid": "6351d19fc5a555d7933a7e0b65a1056f00122e28", "collapsed": true}, "outputs": [], "source": ["cost_rf_test = Create_Cost_DF(clf_rfc, x_test, y_test)\n", "y_test_pred = cost_rf_test.p\n", "y_test_true = y_test\n", "cm_test_rf = confusion_matrix(y_test_true, y_test_pred)\n", "\n", "classes_name = ['Legitimate', 'Fraud']\n", "\n", "fig = plt.Figure(figsize=(10,8))\n", "ax = fig.add_subplot(1,1,1)\n", "plot_confusion_matrix(cm_test_rf, \n", "                      classes_name,\n", "                      title='Confusion matrix of Test Data with risk minimum minimization approach'\n", "                     )\n", "\n", "print \"Recall is %.3f\" % (float(cm_test_rf.item(3))/(cm_test_rf.item(3)+cm_test_rf.item(2)))\n", "print \"Precision is %.3f\" % (float(cm_test_rf.item(3))/(cm_test_rf.item(3)+cm_test_rf.item(1)))\n", "print \"The cost is %f\" % (cost_rf_test.cost.mean())"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d974f1e3-0a3b-427d-be14-629d28b24608", "_uuid": "97d64d12096a9a2cc0190fc4ed6848331a8f330c"}, "source": ["Takeaways:<br>\n", "1. the precision is dramatically low compared to the training data. This is understandable because I downsampled the \"normal\" class for the better performance of machine learning models. This way models will overestimate the frequency of \"fraudulent\" transactions leading to a sharp decline in precision.\n", "2. Though a lower recall and precision, risk minimization approach seems to have the lower cost than the threshold approach. But the cost estimation depends heavily on the modeling of cost matrix, whcih means the cost will change correspondingly if the cost matrix changes. \n", "<br>As a result it is really hard to say which model is better. From my perspective at this point, I prefer to model number 1 because it only has 1.5% higher cost than model 2 but increase recall and precision significantly in return. "]}]}