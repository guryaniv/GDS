{"cells":[{"metadata":{"_kg_hide-input":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_kg_hide-output":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"cell_type":"markdown","source":"### Table of Contents\n\n1. [Getting Started](#Getting-Started)\n  - [File descriptions](#File-descriptions)\n2. [Background of the problem domain](#Background)\n3. [Objective](#Objective)\n4. [Datasets](#Datasets)\n  - [About authors](#About-authors)\n  - [Import libraries](#Import-libraries)\n  - [Load datasets](#Load-datasets)\n5. [Data Exploration](#Data-Exploration)\n  - [Data fields](#Data-fields)\n  - [Quick statistics by grouping authors](#Quick-statistics-by-grouping-authors)\n  - [Data distribution by each author](#Data-distribution-by-each-author)\n  - [Distribution of semantics](#Distribution-of-semantics)\n  - [Wordcloud](#Wordcloud)\n6. [Data Preprocessing](#Data-Preprocessing)\n  - [Binarizing target variable](#Binarizing-target-variable)\n  - [Scrubbing of data](#Scrubbing-of-data)\n7. [Shuffle and split data](#Shuffle-and-split-data)\n8. [Word vectors](#Word-vectors)\n9. [Evaluating model performance](#Evaluating-model-performance)\n  - [Multinomial Naive Bayes](#Multinomial-Naive-Bayes)\n  - [Logistic Regression](#Logistic-Regression)\n  - [SVM](#SVM)\n  - [Stochastic Gradient Descent Classifier](#Stochastic-Gradient-Descent-Classifier)\n  - [XGBoost](#XGBoost)\n  - [Model performance conclusion](#Model-performance-conclusion)\n10. [Parameter tuning using GridSearchCV](#Parameter-tuning)\n11. [Model training](#Model-training)\n  - [Fit and predict the data]()\n  - [Accuracy score](#Accuracy-score)\n  - [Classification report](#Classification-report)\n  - [ROC and AUC curves](#ROC-and-AUC-curves)\n12. [Predict unexplored data](#Predict-unexplored-data)\n  - [Predict](#Predict)\n  - [Reflect author names](#Reflect-author-names)\n13. [Conclusion](#Conclusion)\n  - [Compare heatmaps from the prediction](#Compare-heatmaps-from-the-prediction)\n  - [Compare Wordclouds from the prediction](#Compare-Wordclouds-from-the-prediction)"},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"### Getting Started\n\nThe dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer, so you may notice the odd non-sentence here and there. Your objective is to accurately identify the author of the sentences in the test set.\n\n##### File descriptions\n`train.csv` - the training set\n\n`test.csv` - the test set\n\n`sample_submission.csv` - a sample submission file in the correct format"},{"metadata":{"_uuid":"04164f7b960e56e1ac3e44d8a5b2dcffff419816","_cell_guid":"26e02914-7b5f-4fc6-aebe-1b07aedc1d55"},"cell_type":"markdown","source":"### Background\n\nSince ancient days, discoveries, patents, analyses were and are often documented. It does not matter how they are documented.\n\nThey can be documented either in the form of wall paintings or wall embedded drawings or any sort of scripts.\n\nOlder documents are often referred to continue the research even now in every fields like literature, experiments, myths, etc.\n\nHowever, it is necessary for the seeker or organisation that the authenticity of these documents are met. Still, there could be lots of anonymous and non-classified documents exists. \n\nSo, how were these documents verified? In the ancient days, these were all manually verified. There are different patterns like style of writing, language of communication, etc. of understand from where these documents came in and potentially by whom. Also, do we have any similar documents identified earlier?\n\nThis is broadly known as Stylometry. Stylometry deals with the study of linguistic style and is widely adopted across academic, literary, music and fine arts. Also, known as authorship attribution.\n\nAlso, now-a-days, the anonymous nature of online-message distribution makes identity tracing a critical problem in our society where everything is online."},{"metadata":{"_uuid":"94beadce0a638ccf45821eddc14eb90a7602470c","_cell_guid":"09b6e213-d7d3-48ab-b8f8-cf560f9eb537"},"cell_type":"markdown","source":"### Objective\n\nScan through the test documents and identify the potential author of each document."},{"metadata":{"_uuid":"edc2837c52d92ac92fcade96bcafb21c67e776c4","_cell_guid":"af8847b5-b500-4a96-809e-a0daa7c7ef18"},"cell_type":"markdown","source":"### Datasets\n\nAuthorship attribution is indeed a huge field and is applicable across multiple domains.\n\nThis holds it application everywhere like forensics, mythologies, online messages, ancient belongings, etc.\n\nFor now, we shall limit our practive with the minimum dataset. The attached dataset holds for only three authors and they are:\n- Edgar Allan Poe [EAP]\n- Howard Phillips Lovecraft [HWS]\n- Mary Wollstonecraft Shelley [MWS]"},{"metadata":{"_uuid":"3b7cbd5f1a828cc8e989a359ac98c2ee4fbbfdf1","_cell_guid":"65e1827f-c53a-4f79-8ea2-3913a91ed451"},"cell_type":"markdown","source":"##### About authors\n\n_Edgar Allan Poe_ was an American writer, editor, and literary critic. He is best known for his poetry and short stories, particularly his tales of mystery and the macabre.\n\n_Howard Phillips Lovecraft_ was an American writer who achieved posthumous fame through his influential works of horror fiction.\n\n_Mary Wollstonecraft Shelley_ was an English novelist, short story writer, dramatist, essayist, biographer, and travel writer, best known for her Gothic novel Frankenstein: or, The Modern Prometheus."},{"metadata":{"_uuid":"a005cec949f8afa51eb54d10242699e268115e2e","_cell_guid":"b1ed97dd-3b10-407e-8cee-10235a6d88e3"},"cell_type":"markdown","source":"##### Import libraries"},{"metadata":{"collapsed":true,"_uuid":"020d070303b319faf2fc9b6944880539aaf13e0c","_cell_guid":"90f6d4a8-cb14-4ca7-b5e9-db3b6ef0cf48","trusted":true},"cell_type":"code","source":"# !pip install wordcloud\n# !pip install nltk\n\n# Load libraries #\n\nimport csv\nimport json\n\nimport numpy\nimport pandas\nfrom time import time\n\nfrom matplotlib import pyplot\nimport seaborn as sns\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"de92c8e5408f1cff464d28749f5e401abc020407","_cell_guid":"d372cc37-e20b-4955-ab97-e22cf644e76c"},"cell_type":"markdown","source":"##### Load datasets"},{"metadata":{"_uuid":"c7b2e0fb0802469d2be87dcd938e9d2045518cb6","_cell_guid":"54bcd26e-f704-41aa-bc0f-ee90d16f77ca","trusted":true},"cell_type":"code","source":"# Load all data #\n\nauthored_contents = pandas.read_csv(\"../input/train.csv\")\n\nunauthored_contents = pandas.read_csv(\"../input/train.csv\")","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"ca9a9c8b8fd83dd8a600685b4674b0adce9e000d","_cell_guid":"4822a3ed-324a-4673-b20a-4592fa2343cc","trusted":true},"cell_type":"code","source":"authored_contents.head()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"8130f8d954da0d301448bb26922b3ba077e7f2ef","_cell_guid":"708363a6-4db4-4e34-8edb-fa80f5265859","trusted":true},"cell_type":"code","source":"unauthored_contents.head()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"92fc33cc581cd2edf406439845d3eac3ecd54cac","_cell_guid":"9fbd3752-2fa0-43e5-a4dc-1de4e360826e","trusted":true},"cell_type":"code","source":"authored_contents.shape","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"32fe5c2941097d14ac25b67a43be1174a325c3db","_cell_guid":"2072f725-316c-471b-b8ed-d1e66f8c1c29"},"cell_type":"markdown","source":"### Data Exploration"},{"metadata":{"_uuid":"0892c91f79134d6dbfe61de11d3088ec839aabd2","_cell_guid":"9b2dfea0-f99d-4018-8560-02d0cd901903"},"cell_type":"markdown","source":"##### Data fields\n\n`id` - a unique identifier for each sentence\n\n`text` - some text written by one of the authors and is our feature.\n\n`author` - the author of the sentence (EAP: Edgar Allan Poe, HPL: HP Lovecraft; MWS: Mary Wollstonecraft Shelley). Hence, this is our target variable."},{"metadata":{"_uuid":"969d047ecc9eda99fa96221a879b55da63e2ddd4","_cell_guid":"55627c53-313f-44cc-ac5d-67eb74f954fd"},"cell_type":"markdown","source":"##### Quick statistics by grouping authors"},{"metadata":{"_uuid":"693e78065dfe71308cf9684d2f9acf61e6a59096","_cell_guid":"11f5d14b-7c2f-43f5-a3ca-87fd75408ff6","trusted":true},"cell_type":"code","source":"training_records = len(authored_contents)\n\nauthor_eap, author_mws, author_hpl = authored_contents.author.value_counts()\n\nprint(\"Total number of authored contents: \", training_records)\nprint(\"Total number of authored contents by EAP: \", author_eap)\nprint(\"Total number of authored contents by MWS: \", author_mws)\nprint(\"Total number of authored contents by HPL: \", author_hpl)","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"e8c1d8a6235926557e0e625b2b30863f4837dea2","_cell_guid":"6aac1182-92f8-41c2-ba55-1dce2844e875"},"cell_type":"markdown","source":"Below, we shall study the text length in the training dataset."},{"metadata":{"_uuid":"b8e6c80d4f4117a6d41cda4e0b1b3c71d6a059e3","_cell_guid":"4c68303f-188f-4d0e-9992-4189a4cb56e8","trusted":true,"collapsed":true},"cell_type":"code","source":"# grab text length of each contents\n\nauthored_contents['text_length'] = authored_contents['text'].str.len()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"be0bb7384a275ff1b481135ea4a69c568f36fbab","_cell_guid":"2086f644-b8b4-4da2-8120-e90c4b3c3ba2","trusted":true},"cell_type":"code","source":"authored_contents.head()","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"bb756108993748601a4dbe40f234948d36ae4ebf","_cell_guid":"ae4e58ad-9cea-4edb-88b4-c230c27b5b50"},"cell_type":"markdown","source":"##### Data distribution by each author"},{"metadata":{"_uuid":"4f2b33f01a852591561ba167ce152faa947eed63","_cell_guid":"dcfe355e-7db5-4596-ad45-884e510861a8","trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(14,5))\nsns.countplot(authored_contents['author'],)\npyplot.xlabel('Author')\npyplot.title('Target variable distribution')\npyplot.show()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"cdd95bc8561e340d964dfb66ce06d148721f17ee","_cell_guid":"5927d22d-4540-417d-bd58-c036b6fbe7a9"},"cell_type":"markdown","source":"Now, grouping all the documents by each author to better understand the count."},{"metadata":{"_uuid":"dc9f924d31452f71b8d9fa276d90f5b6b97e9581","_cell_guid":"d6680ca2-bbab-4e24-bd74-11a0ab3dac32","trusted":true},"cell_type":"code","source":"authored_contents.groupby('author').size()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"984d967449dee334feb9c0e03af67f4b19c65fca","_cell_guid":"172dad12-5225-4550-96ea-d2acb2bbc3dd"},"cell_type":"markdown","source":"Below, we shall study the text length in the testing dataset."},{"metadata":{"collapsed":true,"_uuid":"2f81e92310409c7a052bd11fd863ea03f40be2fe","_cell_guid":"61508ad1-244a-417c-b8ee-11b70dd79b45","trusted":true},"cell_type":"code","source":"# examine the same in test data\n\ntesting_records = len(unauthored_contents)\n\nunauthored_contents['text_length'] = unauthored_contents['text'].str.len()","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"cc8f95ad9496745923dbfff4aae1d0929e7d660d","_cell_guid":"b87887e3-cf70-46b6-82b1-5f7edd90d2c0","trusted":true},"cell_type":"code","source":"unauthored_contents.head()","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"a3bcb73420b8c4778107b09bf1a85732f63232d9","_cell_guid":"3772a1ae-961f-40f2-aa12-1293f1cb36f9"},"cell_type":"markdown","source":"##### Distribution of semantics"},{"metadata":{"_uuid":"04ea7c577f3148726dd93edaa62fe3d9f56b595e","_cell_guid":"6056d7f2-10ac-4c96-a909-54924ca6e497"},"cell_type":"markdown","source":"Lets study the pattern of the documents.\n\nIn the code cell below, we shall grab few statistics to understand the writing patterns of the authors. We shall evaluate following parameters for each document:\n- Number of words\n- Number of unique words\n- Number of characters\n- Number of stopwords\n- Number of punctuations\n- Number of words in uppercase\n- Number of words in titlecase\n- Mean length of each word\n\nThis is to study the pattern of writing of each given author."},{"metadata":{"collapsed":true,"_uuid":"7a4a59ba3b270b0fb7f375fc66045acfd26a3f4e","_cell_guid":"cf9cb230-8498-409b-9273-bd77c000c5a1","trusted":true},"cell_type":"code","source":"def text_len(df):\n    df['num_words'] = df['text'].apply(lambda x: len(str(x).split()))\n    df['num_uniq_words'] = df['text'].apply(lambda x: len(set(str(x).split())))\n    df['num_chars'] = df['text'].apply(lambda x: len(str(x)))\n    df['num_stopwords'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() \n                                                          if w in set(stopwords.words('english'))]))\n    df['num_punctuations'] = df['text'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]))\n    df['num_words_upper'] = df['text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n    df['num_words_title'] = df['text'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n    df['mean_word_len'] = df['text'].apply(lambda x: numpy.mean([len(w) for w in str(x).split()]))\n    df['num_character_len'] = df['text'].apply(lambda x: len(x))","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"c694dd8e5cecec8d3503a13a75548b63db2e3d16","_cell_guid":"c89ed5b3-0550-40d9-9d37-5cecc9103e58","trusted":true,"collapsed":true},"cell_type":"code","source":"text_len(authored_contents)\ntext_len(unauthored_contents)","execution_count":15,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c5daa79e2c7e91e8e481a000639e66a0026a8dee","_cell_guid":"afd2bc9a-3e3a-4960-9e8a-a21893f01c5f","trusted":true},"cell_type":"code","source":"def plot_heatmap(df):\n    pyplot.figure(figsize=(14,6))\n\n    pyplot.subplot(211)\n    sns.heatmap(pandas.crosstab(df['author'], df['num_words']), cmap='gist_earth', xticklabels=True)\n    pyplot.xlabel('Original text word count')\n    pyplot.ylabel('Author')\n    pyplot.tight_layout()\n    pyplot.show()\n\n\n    pyplot.subplot(212)\n    sns.heatmap(pandas.crosstab(df['author'], df['num_uniq_words']), cmap='gist_heat', xticklabels=True)\n    pyplot.xlabel('Unique text word count')\n    pyplot.ylabel('Author')\n    pyplot.tight_layout()\n    pyplot.show()\n\n\n    pyplot.subplot(212)\n    sns.heatmap(pandas.crosstab(df['author'], df['num_punctuations']), cmap='gist_heat', xticklabels=True)\n    pyplot.xlabel('Punctuations')\n    pyplot.ylabel('Author')\n    pyplot.tight_layout()\n    pyplot.show()\n\n\n    pyplot.subplot(212)\n    sns.heatmap(pandas.crosstab(df['author'], df['mean_word_len']), cmap='gist_heat', xticklabels=False)\n    pyplot.xlabel('Mean word length')\n    pyplot.ylabel('Author')\n    pyplot.tight_layout()\n    pyplot.show()","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"197cdccdb087e55b727fc43257bc5354fbe877a1","_cell_guid":"9e962ad0-1b1a-44fb-8dd1-840be6b958d0","trusted":true},"cell_type":"code","source":"plot_heatmap(authored_contents)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"25b3e06a6efb88045bc6c8002e08dff635ecec60","_cell_guid":"44539433-988e-4c93-89f8-817dbeba2744"},"cell_type":"markdown","source":"So far this heatmap explains what words were frequently used across the documents for each author.\n\nWe need more insights what words do they pick to better explain their interests of literature."},{"metadata":{"_uuid":"ce5f9b2e5fe42b00d5cdd41455394d6f61be8b0d","_cell_guid":"7a7b060f-0d05-4229-8d28-bf3e91ab69e7"},"cell_type":"markdown","source":"##### Wordcloud"},{"metadata":{"collapsed":true,"_uuid":"45c42a7222ba86abe500b3015a379f9f420a0007","_cell_guid":"335705f9-29af-4180-96f9-06cd667f5b37","trusted":true},"cell_type":"code","source":"eap_documents = authored_contents[authored_contents.author == 'EAP']['text'].values\nhpl_documents = authored_contents[authored_contents.author == 'HPL']['text'].values\nmws_documents = authored_contents[authored_contents.author == 'MWS']['text'].values","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"adbf827464fcd8caf665693f4c9ac0288ee3d34f","_cell_guid":"96f7a4bb-6cca-4b6a-acc8-03c6ab2c4bf8","trusted":true,"collapsed":true},"cell_type":"code","source":"eap_words = \" \".join(eap_documents)\nhpl_words = \" \".join(hpl_documents)\nmws_words = \" \".join(mws_documents)","execution_count":19,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c8f571b740c7239fb72214ed04b817eb85ae04bc","_cell_guid":"3a32b61d-1822-4f7c-8c1c-692ef3d95d6d","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"99596fca47835586a4c44709dd7bdcaa8b6d4e76","_cell_guid":"b0c8af03-6bca-4585-9bf7-4f9e9101c9dc","trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(16,13))\n\nwordcloud = WordCloud(relative_scaling = 1.0, stopwords = STOPWORDS, max_font_size= 35)\nwordcloud.generate(eap_words)\npyplot.imshow(wordcloud.recolor(colormap= 'Pastel2' , random_state=17), alpha=0.98)\npyplot.axis('off')\npyplot.show()","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"1dc5cdcb29f404e25af82352e05d6e7cfa4354f0","_cell_guid":"3a0bc1af-09d1-4061-b0a8-ecf4eb760c4e","trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(16,13))\n\nwordcloud = WordCloud(relative_scaling = 1.0, stopwords = STOPWORDS, max_font_size= 35)\nwordcloud.generate(hpl_words)\npyplot.imshow(wordcloud.recolor(colormap= 'Pastel2' , random_state=17), alpha=0.98)\npyplot.axis('off')\npyplot.show()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"c15560a0714f898d8e4891d96b63eda5090f1284","_cell_guid":"f4960a34-f9d1-478b-a970-1a547ad899c7","trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(16,13))\n\nwordcloud = WordCloud(relative_scaling = 1.0, stopwords = STOPWORDS, max_font_size= 35)\nwordcloud.generate(mws_words)\npyplot.imshow(wordcloud.recolor(colormap= 'Pastel2' , random_state=17), alpha=0.98)\npyplot.axis('off')\npyplot.show()","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"55d3d9b2293f9a9de38a7e0afe4f20ab1bddfe87","_cell_guid":"13c89904-b11b-4b80-b087-47feea89d845"},"cell_type":"markdown","source":"These wordclouds does now explains more about authors and their preferences towards their writings.\n\nAs __Edgar Allan Poe__ was particularly known for tales of mystery and the macabre. So, words like _death, soul, life, corpse, spirit and shadow_ better describes his interests.\n\nAs __Howard Phillips Lovecraft__ was famous for the work of horror fiction. So, words like _fear, horror and body_ describes his interests.\n\nAs __Mary Wollstonecraft Shelley__ was a novelist, short story writer, dramatist, essayist, biographer, and travel writer. So, words like _love, affection, happiness, misery, despair, beauty and pleasure_ best describes her interests."},{"metadata":{"_uuid":"654a9a74bf126c5e571ff5444996ea9617be9d1d","_cell_guid":"7be6b3b6-2527-4933-9e73-a4e0f16ca77f"},"cell_type":"markdown","source":"Since, these 2 authors completely shares their own areas of interests and being mutually exclusive, we can better understand the dataset now.\n\nWe can later verify the the unpredicted dataset once they are predicted."},{"metadata":{"_uuid":"c67d939dcb73a68dd4f090a4f8b4dda0c7d153d6","_cell_guid":"d51c57a7-1d4f-4333-b55d-5a98df40e023"},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{"_uuid":"93e5035e4b6188a580b7abde85d476a0cebac558","_cell_guid":"c7fafd0c-7a84-4aee-8ddc-20c3a2ed366b"},"cell_type":"markdown","source":"Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called categorical variables) be converted. \n\nAs with the non-numeric features, we need to convert the non-numeric target label, `author` to numerical values for the learning algorithm to work. Since there are only three possible categories for this label (`EAP`, `HPL` and `MWS`), we can avoid using one-hot encoding and simply encode these two categories as 0, 1 and 2 respectively.\n\nIn code cell below, you will need to implement the following:\n\n- Convert the target label `author` to numerical entries. Set records with \"EAP\" to 0, records with \"HPL\" to 1 and records with \"MWS\" to 2 and storing it in `numerical_author` column."},{"metadata":{"_uuid":"7e19f105a8f83e10e23744776036db3addc7f538","_cell_guid":"0ffd55a5-01d4-4b99-bf46-300784b94f60"},"cell_type":"markdown","source":"##### Binarizing target variable"},{"metadata":{"_uuid":"7771ce0a369b2d0a815a1bdffb591584796f55c8","_cell_guid":"99329503-eb76-401d-88fa-d6295efb8876","trusted":true,"collapsed":true},"cell_type":"code","source":"authored_contents['numerical_author'] = authored_contents.author.map({ 'EAP': 0, 'HPL': 1, 'MWS': 2 })","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"71b3afdd0105446553583bb89d8850114d2c8e67","_cell_guid":"5fb285e9-dc0c-4ebe-b57a-e0024329c6e6","trusted":true},"cell_type":"code","source":"# Quick view of preprocessing\n\nauthored_contents[['text', 'author', 'numerical_author']].head()","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"19707b63b7a982dd766c5bf45e799e9de92ed83c","_cell_guid":"3590fd4c-7fc9-4448-84fb-439fe87a2f3f","trusted":true},"cell_type":"code","source":"authored_contents.head()","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"fc5cb184d6af4acdcde7c019b9b1d0030609e99f","_cell_guid":"12e9776c-d9c5-4f96-ac84-27973ae05016"},"cell_type":"markdown","source":"We will now clean texts by tokenizing, removing punctuations and stemming. This will reduce the feature set."},{"metadata":{"_uuid":"d228b315255b54f6c1d5d7a8473120e20c485310","_cell_guid":"47796b55-2a7a-4f63-8231-1505da29e701"},"cell_type":"markdown","source":"##### Scrubbing of data"},{"metadata":{"collapsed":true,"_uuid":"1bc03d86502471cd2b0c0102214b684682e95053","_cell_guid":"1f263de0-f56b-4d4b-a550-ab6274f2fd26","trusted":true},"cell_type":"code","source":"all_stopwords = stopwords.words('english')\nps = PorterStemmer()\n\ndef scrub_text(data_frame):\n    sentences = []\n    for i in data_frame.values:\n#         sentence = unicode(i[1], 'utf-8')\n        sentence = i[1]\n\n        # remove all punctuations\n        sentence = sentence.translate(string.punctuation)\n\n        # break sentence into words\n        array_of_words = word_tokenize(sentence)\n\n        # removes all English stopwords\n        array_of_words = [word for word in array_of_words if word.lower() not in all_stopwords]\n\n        # singularise words in the array_of_words\n        array_of_words = [ps.stem(word) for word in array_of_words]\n        cleaned_sentence = ' '.join(array_of_words)\n\n        sentences.append(cleaned_sentence)\n\n    return sentences","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"50ff69438dd4e9ba2b22f1c9c59bb47b2e7c6cc1","_cell_guid":"10b7777b-1723-4f87-bd0b-b560e6652b21","trusted":true,"collapsed":true},"cell_type":"code","source":"# Run the #scrub_text over the text in the training and testing datasets.\n\ntraining_cleaned_texts = scrub_text(authored_contents)\ntesting_cleaned_texts = scrub_text(unauthored_contents)","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"89b074f95c7e60b4b43a92a3bf0a100bced9f6c2","_cell_guid":"553b02fd-f88e-4585-a143-83b8a5467066"},"cell_type":"markdown","source":"Saving the cleaned text into `scrubbed_text` for both `authored_contents` and `unauthored_contents`.\n\nHowever, we will not use this since this will defeat the purpose to analyse the semantics of texts already authored.\n\nSoon, we shall see what is the performance of classifiers on both set of data i.e. `scrubbed_text` and `text`."},{"metadata":{"_uuid":"b145cca7968030bcdbf2f6ad3b3a86e2be845b4c","_cell_guid":"767e302a-14c3-437c-8089-bc47e132e0b6","trusted":true,"collapsed":true},"cell_type":"code","source":"authored_contents['scrubbed_text'] = training_cleaned_texts\n\nunauthored_contents['scrubbed_text'] = testing_cleaned_texts","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"ca53f814dbdcf49490ea1edd05abb76c75a6bda3","_cell_guid":"16eece1b-03b9-4d11-8744-fbd237320e75","trusted":true,"collapsed":true},"cell_type":"code","source":"# Define labels and features set\n\nX = authored_contents['text']\nY = authored_contents['numerical_author']","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"517b5f1a20bf3f2dc44bedbb832ccb03443cf49c","_cell_guid":"52a1a7e4-e77d-4237-acc8-bfa0d4cbc05c"},"cell_type":"markdown","source":"### Shuffle and split data"},{"metadata":{"_uuid":"ec8a3d8e8f7adf35eadf47873dc5074b799f67b9","_cell_guid":"dcd7a946-9271-4fa1-bc13-fa78ec2ec5f0"},"cell_type":"markdown","source":"We will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing."},{"metadata":{"_uuid":"7500c56b037da18955a6ed476caa5dc924e193b8","_cell_guid":"355d4e24-32e6-4d8c-b04d-8a84b4e2eec6","trusted":true},"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,\n                                                    Y,\n                                                    test_size = 0.2,\n                                                    random_state = 0)","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"7383473dd2bd8f5416b81bbf76366fd9f68ca445","_cell_guid":"f536a3c1-e094-4c3a-9987-dc3f50260bd0","trusted":true},"cell_type":"code","source":"# Show the results of the split\n\n# Features\nprint(\"Training set has \" + str(X_train.shape[0]) + \" features.\")\nprint(\"Testing set has \" + str(X_test.shape[0]) + \" features.\")\n\n# Labels\nprint(\"Training set has \" + str(Y_train.shape[0]) + \" labels.\")\nprint(\"Testing set has \" + str(Y_test.shape[0]) + \" labels.\")\n\nprint(\"\\nPrinting labels set...\")\nprint(Y_train.value_counts())\n\nprint(Y_test.value_counts())","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"08c7caa7b6cce554427b9ac30bd8786f9d929495","_cell_guid":"7c8f34dd-c83c-450c-b1d3-2965c582e99c"},"cell_type":"markdown","source":"###### Another cross validation:\n\nWe will now cross-validate our training set using `sklearn.model_selection.StratifiedShuffleSplit` approach\n\nThis as mentioned in the training videos, this will divide our training set into N folds.\nWe will iterate over each fold acting as testing data at a single point of time and rest of them will act as training sets.\n\nWe will tweak certain parameters to learn the dataset pattern."},{"metadata":{"_uuid":"8e58aadf173e8df13795257f471e2322bbee0db4","_cell_guid":"2cdb26f3-041a-49f9-af49-2ca15e060ee3","trusted":true,"collapsed":true},"cell_type":"code","source":"X2_train, X2_test, Y2_train, Y2_test = train_test_split(X,\n                                                        Y,\n                                                        stratify = Y,\n                                                        test_size = 0.2,\n                                                        random_state = 42)","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"e92fbfb6b5010c566c01583596f9d4d1b0a70e53","_cell_guid":"97bcfa78-9854-4a4d-85ed-a9db71c01014","trusted":true},"cell_type":"code","source":"# Show the results of the split\n\n# Features\nprint(\"Training set has \" + str(X2_train.shape[0]) + \" features.\")\nprint(\"Testing set has \" + str(X2_test.shape[0]) + \" features.\")\n\n# Labels\nprint(\"\\nTraining set has \" + str(Y2_train.shape[0]) + \" labels.\")\nprint(\"Testing set has \" + str(Y2_test.shape[0]) + \" labels.\")\n\nprint(\"\\nPrinting labels set...\")\nprint(Y_train.value_counts())\n\nprint(Y_test.value_counts())","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"6d9d804458545877c1c16a6ab4fa78198906b13d","_cell_guid":"3cca4437-6bb7-4c12-976c-0482804786e4"},"cell_type":"markdown","source":"### Word vectors"},{"metadata":{"_uuid":"ba221d243952896f167dab9562a2c1a0b8043b18","_cell_guid":"65391fda-96ed-45b3-961a-3319a3171f72"},"cell_type":"markdown","source":"We shall now grab the words in a document and tokenize them and build vocabulary out of them.\n\nWe are still working on the original texts that contains punctuations and un-stemmed words."},{"metadata":{"collapsed":true,"_uuid":"16b8468c9d11e6fa850d0b955ee16ede6158d95b","_cell_guid":"60610e70-fb94-4fab-b518-c76ab892d4dc","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(X2_train)\n\ntraining_vectorizer = vectorizer.transform(X2_train)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"ece9ee633f3460a8d81e46b3130e19b83e848d61","_cell_guid":"6b57abca-dcbc-4133-89e9-44130692bb00","trusted":true},"cell_type":"code","source":"# Quick view of vectors from the texts\n\ntraining_vectorizer.toarray()","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"e580adcdcdcc8933e30dbb6f68bd8981d51c33b7","_cell_guid":"05beb078-b057-4cad-bfb1-ef5636baf668","trusted":true},"cell_type":"code","source":"print(len(vectorizer.get_feature_names()))","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"c8b2e5edafef118a338f341bc43f722de5bf0ff8","_cell_guid":"23e818c3-16ca-49a9-a9ba-c68e0cd8c326","trusted":true,"collapsed":true},"cell_type":"code","source":"# run vectorizer for X2_test\n\ntesting_vectorizer = vectorizer.transform(X2_test)","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"2f81a0ea78ce7e91dc044603a579767d9d483862","_cell_guid":"9ac02050-a834-4371-83bc-938c734dccfd"},"cell_type":"markdown","source":"As per the above transformation, `training_vectorizer` is our feature set."},{"metadata":{"_uuid":"a849ca791664894c13b7902683639b095eab57a2","_cell_guid":"89d872e5-7040-43ee-afe5-601aa9dbe5b5"},"cell_type":"markdown","source":"### Evaluating model performance"},{"metadata":{"_uuid":"d4b50b6b98aed62c503ba1f4a681b7cd5c91737c","_cell_guid":"fb51f1b5-3f19-4f25-bac9-0ce069b8dbb2"},"cell_type":"markdown","source":"The following are some of the supervised learning models that will be used to evaluate the performance of the model on this problem and the dataset.\n\n- Multinomial Naive Bayes\n- Logistic Regression\n- XGBoost from the ensemble methods\n- SVM\n- Stochastic Gradient Descent Classifier (SGDC)"},{"metadata":{"collapsed":true,"_uuid":"897ed239b62ac0f96261fdd44751d6e5584dba31","_cell_guid":"114163b2-fc68-415d-827c-0cb6e8b35152","trusted":true},"cell_type":"code","source":"# Include libraries to evaluate performances on the attached dataset\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"a9181e07bc1c528757c4dd86c272008f2ba2b9d2","_cell_guid":"03ec1875-481b-4f7b-bed8-961a0608f103"},"cell_type":"markdown","source":"##### Multinomial Naive Bayes"},{"metadata":{"_uuid":"85c45f6019936472ac12e5cfa49d6d4637a70744","_cell_guid":"31b60b00-e0fd-4207-8d36-f3cb1cfc57cb","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nkfold = KFold(n_splits = 10, random_state = 7)\n\nmodel = MultinomialNB()\nstart = time()\nresults = cross_val_score(model, training_vectorizer, Y2_train, cv=kfold)\nend = time()\n\nprint(\"Mean value: \" + str(results.mean()))\nprint(\"Training time: \" + str((end - start)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e84b4dd38210f25339249b645225555dd847545e","_cell_guid":"66afe360-1880-4e75-92e0-b5e6ef23dc29"},"cell_type":"markdown","source":"##### Logistic Regression"},{"metadata":{"_uuid":"b0a211b10c6c19d5b9cebe8e34928d4a75cec11c","_cell_guid":"399a3f92-597b-40bc-8638-72097a2972b9","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nkfold = KFold(n_splits = 10, random_state = 7)\n\nmodel = LogisticRegression()\nstart = time()\nresults = cross_val_score(model, training_vectorizer, Y2_train, cv=kfold)\nend = time()\n\nprint(\"Mean value: \" + str(results.mean()))\nprint(\"Training time: \" + str(end - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbaf96847df356c2b8fae3736ae3180b53d8d5a7","_cell_guid":"dbdeba4f-ac2f-4849-8e68-5149fa8f7c8a"},"cell_type":"markdown","source":"##### SVM"},{"metadata":{"_uuid":"6f95f897f31cfb6adac0136f72ba8c9d9df2461c","_cell_guid":"527470dc-83b3-4b6e-8182-3458ce466e47","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nkfold = KFold(n_splits = 10, random_state = 7)\n\nmodel = SVC()\nstart = time()\nresults = cross_val_score(model, training_vectorizer, Y2_train, cv=kfold)\nend = time()\n\nprint(\"Mean value: \" + str(results.mean()))\nprint(\"Training time: \" + str(end - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7adb7bece23575442406393a6bc20031a1fa7565","_cell_guid":"de4d1cff-499a-41f6-898d-20d6abdfd211"},"cell_type":"markdown","source":"##### Stochastic Gradient Descent Classifier"},{"metadata":{"_uuid":"e0ac8950442dd72acd5e40f1ff0c0f7e1fb11b71","_cell_guid":"58405688-859f-4611-8f9e-9d2cedf086b3","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nkfold = KFold(n_splits = 10, random_state = 7)\n\nmodel = SGDClassifier()\nstart = time()\nresults = cross_val_score(model, training_vectorizer, Y2_train, cv=kfold)\nend = time()\n\nprint(\"Mean value: \" + str(results.mean()))\nprint(\"Training time: \" + str(end - start))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3edaefe9b726d8bff731d719cfc9bef680ae2efa","_cell_guid":"139d02cb-8d16-461a-98c0-cc97408a1e8d"},"cell_type":"markdown","source":"##### XGBoost"},{"metadata":{"collapsed":true,"_uuid":"bce22d6cc99db4ee6dab85ea7b18fd27d162ca0a","_cell_guid":"dee7256a-7037-494c-a1b9-63fae850af20","trusted":false},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nstart = time()\nresults = cross_val_score(model, training_vectorizer, Y2_train, cv=kfold)\nend = time()\n\nprint(\"Mean value: \" + str(results.mean())).format(results.mean())\nprint(\"Training time: \" + str(end - start))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a271f8e2894fc6b2ac009619861adc9d243e453c","_cell_guid":"f5761e0f-e859-462b-be52-07db1380c639","trusted":false},"cell_type":"markdown","source":"#### Statistics:\n\n         Algorithm            Mean accuracy    Training time            O/P\n    Multinomial Naive Bayes      0.836            0.141        .83 accuracy with the least amount of training time\n     Logistic Regression         0.821            9.789        .82 accuracy with some amount of training time\n            SGDC                 0.799            0.432        .79 accuracy with the least of training time\n          XGBoost                0.635            84.103       .63 accuracy with unacceptable training time\n            SVM                  0.403            528.707      least accuracy with highest amount of training time"},{"metadata":{"collapsed":true,"_uuid":"76c98276076f54b83a560efea44a06127716099e","_cell_guid":"a056658e-d139-4f5d-ae70-a99192049525","trusted":false},"cell_type":"markdown","source":"#### Classifier performances over scrubbed_text and text"},{"metadata":{"_uuid":"26cc2fe60efbbe32c41f8088339f6530643dd6ca","_cell_guid":"d0a906d1-6501-4f1d-847c-2766038590c6","trusted":false},"cell_type":"markdown","source":"Now, lets see the performance of the classifiers when texts are cleaned with punctuations, lowercased and properly stemmed.\n\nThis is what I have done in first place.\n\n         Algorithm            Mean accuracy    Training time            O/P\n    Multinomial Naive Bayes      0.823            0.093        .82 accuracy with the least amount of training time\n     Logistic Regression         0.805            2.654        .80 accuracy with some amount of training time\n            SGDC                 0.794            0.322        .79 accuracy with the least of training time\n          XGBoost                0.612            52.510       .61 accuracy with unacceptable training time\n            SVM                  0.403            299.708      least accuracy with highest amount of training time"},{"metadata":{"collapsed":true,"_uuid":"30fb13499987450173d36e691545a969175b6a13","_cell_guid":"e895083f-130a-43e9-b42d-6e300c6abf94","trusted":false},"cell_type":"markdown","source":"##### Model performance conclusion"},{"metadata":{"_uuid":"9e994e22e8782799ff26beff6595215c6f1291d2","_cell_guid":"f7dd3e0d-7455-4e2f-87e6-8a51c483e8a1","trusted":false},"cell_type":"markdown","source":"Comparing both of the statistics, we see improvement in the accuracy of Multinomial Naive Bayes when raw features are iterated i.e. without cleaning up the texts.\n\nWe shall proceed with the Multinomial Naive Bayes classifier."},{"metadata":{"collapsed":true,"_uuid":"57ed7f17c4489684fca62069fea802c87061118c","_cell_guid":"76bbe5c6-b932-456f-8f67-39fba9612b8e","trusted":false},"cell_type":"markdown","source":"### Parameter tuning"},{"metadata":{"_uuid":"682414e997d050ef57b42e51dca895d293b35afb","_cell_guid":"55aba5f0-8688-44d3-816c-bdf545d7c033","trusted":false},"cell_type":"markdown","source":"Fine tune the chosen model. Use grid search (GridSearchCV) with at least one important parameter tuned with at least 3 different values. We will need to use the entire training set for this.\n\nIn the code cell below, we shall implement the following:\n\n- Import sklearn.grid_search.GridSearchCV and sklearn.metrics.make_scorer.\n- Initialize the classifier you've chosen and store it in `model`.\n- Create a dictionary of parameters you wish to tune for the chosen model.\n  Example: parameters = {'parameter' : [list of values]}.\n- Use make_scorer to create an fbeta_score scoring object (with $\\beta = 0.5$).\n- Perform grid search on the classifier clf using the 'scorer', and store it in grid_obj.\n- Fit the grid search object to the training data (`training_vectorizer, Y2_train`), and store it in grid_fit."},{"metadata":{"_uuid":"6e84a4fd9f1457f935074c16417393e88ffb0695","_cell_guid":"06f4b530-ebec-4fcc-ade9-f9768799f6df","trusted":true},"cell_type":"code","source":"from sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"2342943556cb0383e5363974b2f02d2e759fafe8","_cell_guid":"e6b2d892-6d5b-4d37-a43f-3601f7308198","trusted":true},"cell_type":"code","source":"model = MultinomialNB()\n\nparameters = { 'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0] }\n\nscorer = make_scorer(fbeta_score, beta=0.5)\n\ngrid_obj = GridSearchCV(model, parameters)\n\ngrid_fit = grid_obj.fit(training_vectorizer, Y2_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\ngrid_fit.best_params_","execution_count":43,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"dff09c89de6ca376a3275bbfa541173c23f4e34e","_cell_guid":"bf814328-460a-4ddd-afbe-4590b99bb561","trusted":false},"cell_type":"markdown","source":"### Model training"},{"metadata":{"collapsed":true,"_uuid":"2d5912336baed36c3917e1fe3ec2cb56a72ad059","_cell_guid":"4e386b42-03c5-49ae-845e-7e9e127e12f8","trusted":false},"cell_type":"markdown","source":"##### Fit and predict the data"},{"metadata":{"collapsed":true,"_uuid":"9d05a47ef763eb6f03f9840182ccb774e06d6df4","_cell_guid":"a403e078-610f-4bfd-97b2-2215a27d208b","trusted":true},"cell_type":"code","source":"mnb = MultinomialNB(alpha = 0.5)","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"41cbb4fb24d8d0eda6fc87a14eca0fddbc416ab3","_cell_guid":"03bb721d-3a6d-4cfa-9ea7-e32a5bc4d6e6","trusted":true},"cell_type":"code","source":"mnb.fit(training_vectorizer, Y2_train)","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"d98d4e498f845813c170dc6423483956fd5fa0c6","_cell_guid":"80e6c57f-ded1-4629-8b05-18c20418c4c7","trusted":true},"cell_type":"code","source":"mnb.predict_proba(training_vectorizer)","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"b6789318e520827a21d44333f9a12f49233396ab","_cell_guid":"1af4b6d9-125b-411f-90b4-79a480c3f848","trusted":true},"cell_type":"code","source":"mnb.feature_log_prob_","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"8eac85e803101d2b975b2ce77931a1f333a741db","_cell_guid":"3fb3edb2-6adf-4a78-88a7-27597c39bd3b","trusted":true,"collapsed":true},"cell_type":"code","source":"Y_train_prediction = mnb.predict(training_vectorizer)","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"668d6eac6e8ad48cb3656fc4924810cf6168daca","_cell_guid":"b5575888-cb38-46c6-9063-7260d0d20c5a","trusted":true,"collapsed":true},"cell_type":"code","source":"Y_test_prediction = mnb.predict(testing_vectorizer)","execution_count":49,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5deb3a9f446344b0bf357cc39f998a54233dd4a2","_cell_guid":"03bb55c9-2eaf-4027-b84a-ff69a804e9b8","trusted":false},"cell_type":"markdown","source":"##### Accuracy score"},{"metadata":{"_uuid":"2003c25c59e738e38aaa9740c247ef81918ffe67","_cell_guid":"a1d0ce84-2351-4259-8423-e2bf5d48e93f","trusted":true},"cell_type":"code","source":"# calculate score for predicted data against testing data\nfrom sklearn import metrics\n\n# compare predicted resultset with the test set\nmetrics.accuracy_score(Y2_test, Y_test_prediction)","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"d9cd3b5fccf8cd80bd0a87fa968af0cdf763d6ad","_cell_guid":"d6f6cd50-e09d-4008-bb9b-ae22e86fdf9b","trusted":true},"cell_type":"code","source":"# calculate score for predicted data against training data\n\nmetrics.accuracy_score(Y2_train, Y_train_prediction)","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"e5f0653eae4c88db0fd411f586763e8860e84fa1","_cell_guid":"20e660ab-55ba-4b98-be43-a862cd3a38b8","trusted":true},"cell_type":"code","source":"# Calculate confusion matrix\n\nmetrics.confusion_matrix(Y2_test, Y_test_prediction)","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"1df82cdf512b16dc671dff10360d4d42ac12d17f","_cell_guid":"b3ae4ac6-3993-4b53-ba96-00f1408bfe4c","trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for X_test_dtm\ny_pred_prob = mnb.predict_proba(testing_vectorizer)\ny_pred_prob[:10]","execution_count":53,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a747ea943880f239eba46cd60d8e24fbbadc8ebb","_cell_guid":"e602d6d5-31b8-4c8f-9f2f-147ac67c425e","trusted":false},"cell_type":"markdown","source":"##### Classification report"},{"metadata":{"_uuid":"eeed0ce6db4a742e76f8f1e3f7ca41993c187df7","_cell_guid":"22264f14-cf63-4483-aed4-17011c758b3b","trusted":true},"cell_type":"code","source":"print(classification_report(Y2_train, Y_train_prediction, target_names=['EAP', 'HPL', 'MWS']))","execution_count":54,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e435abe18aba8f8757cd268aa814727062bb2486","_cell_guid":"bc0a41dc-b327-4577-b274-fe2c5124c31c","trusted":false},"cell_type":"markdown","source":"##### ROC and AUC curves"},{"metadata":{"_uuid":"915deb21cb5bbd1e307924c8d48dbfbe21b2882d","_cell_guid":"70d78a79-206c-4897-b5ce-1f7ef0530b93","trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(Y2_train, Y_train_prediction, pos_label = 1)\n\nprint(\"Multinomial naive bayes AUC: \" + str(metrics.auc(fpr, tpr)))","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"268b9d319e219fb287aff04dc5bce33d348c7d43","_cell_guid":"c75ba5a8-58bf-4d34-a8fc-fc616f344033","trusted":false},"cell_type":"markdown","source":"Precision, Recall and F1 score is reported.\n\nAUC metric reported to be 0.535"},{"metadata":{"collapsed":true,"_uuid":"f959271d5eee83b44a05a96d775fa1dcfdb6c65b","_cell_guid":"c164d9b5-55c8-47aa-bc00-54fc3a613b15","trusted":false},"cell_type":"markdown","source":"### Predict unexplored data"},{"metadata":{"collapsed":true,"_uuid":"a798935c173d1464857034f4d65057140aea2ac4","_cell_guid":"c6fe6287-f81a-42f1-892f-c547e3ebfa09","trusted":false},"cell_type":"markdown","source":"##### Predict"},{"metadata":{"_uuid":"20bfb4dd36e1fcf5c1cd5e48d84ec0ba425f8713","_cell_guid":"cd990dfa-0a0f-42c3-8f1d-5eef4a9f0eea","trusted":true,"collapsed":true},"cell_type":"code","source":"# vectorise the unauthored_contents\n\nunpredicted_texts = unauthored_contents['text']\n\nunpredicted_texts_vectorizer = vectorizer.transform(unpredicted_texts)","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"7a2f17ddabdf07a35cd80844715021588915756a","_cell_guid":"81daf94c-a78d-432f-b450-e69eefd276b6","trusted":true},"cell_type":"code","source":"unpredicted_texts_vectorizer","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"85a5d72e9676ede04fc5bcf5a158b5be40a3d345","_cell_guid":"4f20476e-234f-4f5f-bf45-31af204f4187","trusted":true,"collapsed":true},"cell_type":"code","source":"unpredicted_texts_prediction = mnb.predict(unpredicted_texts_vectorizer)","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"24c3d0eb0408b79e653bdf01f1d1b6da21608664","_cell_guid":"581f5e02-0b0e-411f-ab85-3cb867fcfff8","trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for X_test_dtm\npredicted_prob = mnb.predict_proba(unpredicted_texts_vectorizer)\npredicted_prob[:10]","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"e9e082946dcbdba7c035b5e5cacda72d543b3c00","_cell_guid":"22ec9901-3574-4c45-b4e9-6e03cff8f177","trusted":true},"cell_type":"code","source":"unpredicted_texts_prediction","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"e179b6869cd50007c263e42505a03d25693243bd","_cell_guid":"94d3774a-ed6d-4870-9bf9-98da012717d9","trusted":true},"cell_type":"code","source":"len(unauthored_contents)","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"2d4df671d6da27ff3e66339fce1bd8be53ef9c88","_cell_guid":"75709cec-93e8-4aea-b7fc-80f367197e9b","trusted":true,"collapsed":true},"cell_type":"code","source":"numerical_authors = pandas.DataFrame(unpredicted_texts_prediction, columns=['num_author'])\n\npredicted_unauthored_contents = pandas.concat([unauthored_contents, numerical_authors], axis=1)","execution_count":64,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c1865029ee93e552238afa94bf37263989327af7","_cell_guid":"95b3c015-544e-4858-b70f-b7e0758355f4","trusted":false},"cell_type":"markdown","source":"##### Reflect author names"},{"metadata":{"_uuid":"c9460209a191cff41821727db8fde300ef928a68","_cell_guid":"37905439-d284-427c-8fce-fd3cf2b75808","trusted":true,"collapsed":true},"cell_type":"code","source":"predicted_unauthored_contents['author'] = predicted_unauthored_contents.num_author.map({ 0: 'EAP', 1: 'HPL', 2: 'MWS' })","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"3b7febf99c83715cf92d7baabffdfd7abb989266","_cell_guid":"5b8bd478-0123-4c82-aad9-346612f0aad6","trusted":true},"cell_type":"code","source":"predicted_unauthored_contents.groupby('author').size()","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"2bd66a7ec64ab172cf68647a272e0de76151ac19","_cell_guid":"4e92e6a1-6906-46e7-a124-e9c2b171d0e3","trusted":true},"cell_type":"code","source":"predicted_unauthored_contents.head()","execution_count":67,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"423afc64ca2062bd8c8167a2422d2d71e00db3d6","_cell_guid":"2bbea473-dda6-4100-b738-e568375417cc","trusted":false},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{"collapsed":true,"_uuid":"a47bfe65121eb5d43c34bebe3b218d4fd53a65b6","_cell_guid":"c4e829eb-3367-4be1-a030-207a5b663fb1","trusted":false},"cell_type":"markdown","source":"##### Compare heatmaps from the prediction"},{"metadata":{"_uuid":"324814918c2cb98ed085b2df66ebff6d329d9c37","_cell_guid":"f4f64354-33e9-4bc1-8b3c-eb5f9c0b7924","trusted":true},"cell_type":"code","source":"# Plotting over the test.csv that we ran the prediction above to study the analysis.\n\nplot_heatmap(predicted_unauthored_contents)","execution_count":69,"outputs":[]},{"metadata":{"_uuid":"8cc6a35496db87c8a68f0e7b309e581310b012a6","_cell_guid":"2ee13574-74cd-4d32-8bf2-fa540a2b605b","trusted":true},"cell_type":"code","source":"# Comparing the heatmap that we plotted above using `authored_contents`\n\nplot_heatmap(authored_contents)","execution_count":71,"outputs":[]},{"metadata":{"_uuid":"8e9d74af2a98f1b0ce0e1dac57023c3daa4819d1","_cell_guid":"90745be6-cb0a-4ceb-81c6-cc8566ea40ee","trusted":false},"cell_type":"markdown","source":"Considering parameters:\n\n1. Punctuations: Both of them represent identical plots for both of the datasets.\n   \n2. Unique words used: Both of them utilised and represented identically."},{"metadata":{"collapsed":true,"_uuid":"8141d74ea8e984e132dc89585e7a8fb465dfee78","_cell_guid":"c558dc22-2144-4db3-93b7-6ab20417a0ae","trusted":false},"cell_type":"markdown","source":"##### Compare Wordclouds from the prediction"},{"metadata":{"_uuid":"f90c880489ffa34bbd1fef21cce356a83f7aeadf","_cell_guid":"07024b2b-2a5a-41d7-be52-d9f02a1f780a","trusted":true,"collapsed":true},"cell_type":"code","source":"predicted_eap_documents = predicted_unauthored_contents[predicted_unauthored_contents.author == 'EAP']['text'].values\npredicted_hpl_documents = predicted_unauthored_contents[predicted_unauthored_contents.author == 'HPL']['text'].values\npredicted_mws_documents = predicted_unauthored_contents[predicted_unauthored_contents.author == 'MWS']['text'].values","execution_count":72,"outputs":[]},{"metadata":{"_uuid":"6438fed93106fb40b022753bcde8fdd04341849d","_cell_guid":"0a32d72a-7e95-41b6-98ab-30a8ab21f8bb","trusted":true,"collapsed":true},"cell_type":"code","source":"predicted_eap_words = \" \".join(predicted_eap_documents)\npredicted_hpl_words = \" \".join(predicted_hpl_documents)\npredicted_mws_words = \" \".join(predicted_mws_documents)","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"8370ed3c411564bbd9d91d1ebef582dfa4a909c8","_cell_guid":"2a7ac140-a6ec-4fac-9c9f-195105e67140","trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(16,13))\n\nwordcloud = WordCloud(relative_scaling = 1.0, stopwords = STOPWORDS, max_font_size= 35)\nwordcloud.generate(predicted_eap_words)\npyplot.imshow(wordcloud.recolor(colormap= 'Pastel2' , random_state=17), alpha=0.98)\npyplot.axis('off')\npyplot.show()","execution_count":74,"outputs":[]},{"metadata":{"_uuid":"cd20ccb3a9beefb83be40cbe8bd07e5f4fba2509","_cell_guid":"b75f7c0d-3516-46e2-bc8f-3a61c2a7844c","trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(16,13))\n\nwordcloud = WordCloud(relative_scaling = 1.0, stopwords = STOPWORDS, max_font_size= 35)\nwordcloud.generate(predicted_hpl_words)\npyplot.imshow(wordcloud.recolor(colormap= 'Pastel2' , random_state=17), alpha=0.98)\npyplot.axis('off')\npyplot.show()","execution_count":75,"outputs":[]},{"metadata":{"_uuid":"8dc294c3bc71bac08cbf0912dc28a1af5b64b6d8","scrolled":true,"_cell_guid":"d5671fd4-c5ce-4426-b816-47dcd8c177b6","trusted":true},"cell_type":"code","source":"pyplot.figure(figsize=(16,13))\n\nwordcloud = WordCloud(relative_scaling = 1.0, stopwords = STOPWORDS, max_font_size= 35)\nwordcloud.generate(predicted_mws_words)\npyplot.imshow(wordcloud.recolor(colormap= 'Pastel2' , random_state=17), alpha=0.98)\npyplot.axis('off')\npyplot.show()","execution_count":76,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"94073fffaa34375acecb6c20efcb8ab51d8bc48b","_cell_guid":"b1754a74-5e71-4016-a880-61aef5996fdf","trusted":true},"cell_type":"code","source":"## Closing notes to come...","execution_count":77,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0ada45a3bf2976f25c091d8da0050330be122071"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}