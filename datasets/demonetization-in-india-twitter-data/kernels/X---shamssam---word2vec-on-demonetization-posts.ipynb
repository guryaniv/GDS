{"cells":[{"metadata":{"_uuid":"4ec461c530c985eb5bd53cfdf4ecbff4a9f3f32e"},"cell_type":"markdown","source":"# Training Word2Vec on Demonetization Posts\n\n- Corpus is the raw dataset with minimal preprocessing code for which is below.\n- The word similarity is learnt from the tweets and nothing else.\n- The dimensionality of vector representations is 2 for the ease of visualization.\n\n\n**PS: Implemented the kernel to understand how well can one train a word2vec model individually.  In case of any issues in code or errors in logic please comment. It will help me understand things better. Thank you.**"},{"metadata":{"_uuid":"4b8e0c0dfdd9d5bfbb7b07f5129d90aa7d3c52de"},"cell_type":"markdown","source":"# Preprocessing Script"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import re\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n\n\ndef preprocessing(\n    text,\n    remove_stopwords=False,\n    stem_words=False,\n    stopwords_addition=[],\n    stopwords_exclude=[],\n    HYPHEN_HANDLE=1\n):\n    \"\"\"\n    convert string text to lower and split into words.\n    most punctuations are handled by replacing them with empty string.\n    some punctuations are handled differently based on their occurences in the data.\n    -  replaced with ' '\n    few peculiar cases for better uniformity.\n    'non*' replaced with 'non *'\n    few acronyms identified\n    SCID  Severe Combined ImmunoDeficiency\n    ADA   Adenosine DeAminase\n    PNP   Purine Nucleoside Phosphorylase\n    LFA-1 Lymphocyte Function Antigen-1\n    \"\"\"\n    text = text.lower().split()\n\n    if remove_stopwords:\n        stops = list(set(stopwords.words('english')) -\n                     set(stopwords_exclude)) + stopwords_addition\n        text = [w for w in text if w not in stops]\n\n    text = \" \".join(text)\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=_]\", \" \", text)\n    text = re.sub(r\"(that|what)(\\'s)\", r\"\\g<1> is\", text)\n    text = re.sub(r\"i\\.e\\.\", \"that is\", text)\n    text = re.sub(r\"(^non| non)\", r\"\\g<1> \", text)\n    text = re.sub(r\"(^anti| anti)\", r\"\\g<1> \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    if HYPHEN_HANDLE == 1:\n        text = re.sub(r\"\\-\", \"-\", text)\n    elif HYPHEN_HANDLE == 2:\n        text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    text = text.lower().split()\n\n    if remove_stopwords:\n        stops = list(set(stopwords.words('english')) -\n                     set(stopwords_exclude)) + stopwords_addition\n        text = [w for w in text if w not in stops]\n\n    text = \" \".join(text)\n\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n\n    # Return a list of words\n    return(text)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"327226d751560c0c38d3d90e4a659231a4ab63aa"},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from functools import partial\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\npreprocessing = partial(preprocessing, HYPHEN_HANDLE=2)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"1730217b8c8f0b02587bbb575c3bd0450bc8e4c4"},"cell_type":"markdown","source":"# Generate Data from Corpus"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3ec913cb4dc54f16464be58ff6f56600d2b9bd0b"},"cell_type":"code","source":"def generate_data(corpus, _slice=3):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(corpus)\n    corpus = tokenizer.texts_to_sequences(corpus)\n    \n    data = []\n    targets = []\n    for sentence in corpus:\n        slices = [sentence[i: i+_slice] for i in range(0, len(sentence) - (_slice-1))]\n        center = int(np.floor(_slice/2))\n        for s in slices:\n            data.append([s[center]])\n            targets.append([_ for idx, _ in enumerate(s) if idx != center])\n    \n    X = np.zeros((len(data), len(tokenizer.word_index)+1))\n    y = np.zeros((len(data), len(tokenizer.word_index)+1))\n    for idx, (i, j) in enumerate(zip(data, targets)):\n        X[idx][i] = 1\n        y[idx][j] = 1\n\n    print(\"X_shape:\", X.shape)\n    print(\"y_shape:\", y.shape)\n    print(\"# Words:\", len(tokenizer.word_index))\n\n    return X, y, tokenizer","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"84ebfaa2eec3c3374b5370d59100e6212e2c0a16"},"cell_type":"markdown","source":"# Read CSV and Generate Data"},{"metadata":{"trusted":true,"_uuid":"26692db9efac368b027d68fc878be92058af5b79"},"cell_type":"code","source":"df_data = pd.read_csv('../input/demonetization-tweets.csv', encoding='latin-1', usecols=['text'])\ndf_data.drop_duplicates(inplace=True)\ndf_data.dropna(inplace=True)\ndf_data.text = df_data.text.apply(preprocessing)\ncorpus = [_ for sent in df_data.text.tolist() for _ in sent.split(\".\")]\nX, y, tokenizer = generate_data(corpus, 5)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"79a7522e9952a184d0fcb3d26a9545ee13d95944"},"cell_type":"markdown","source":"# Define Model"},{"metadata":{"trusted":true,"_uuid":"68011ae429c856b9ddae341223aa662154f4429e"},"cell_type":"code","source":"model = Sequential([\n    Dense(2, input_shape=(X.shape[1],)),\n    Dense(X.shape[1]),\n    Activation('softmax')\n])\nmodel.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\nmodel.summary()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"6cb26844af7c6688726494b0d070e8ee1ef2ad37"},"cell_type":"markdown","source":"# Training Model"},{"metadata":{"trusted":true,"_uuid":"4ca84c6cf268d93d1e3155b897a13fc639bb86fd"},"cell_type":"code","source":"try:\n    model.fit(X, y, epochs=10, verbose=1)\nexcept KeyboardInterrupt:\n    print('\\n\\nExited by User')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"93b5c7c54ff1faaee946c55c68fa4a86e7122547"},"cell_type":"code","source":"points = model.layers[0].get_weights()[0]\nword_embedding = {word: embedding for word, embedding in zip(tokenizer.word_index.keys(), points[1:])}\ninverse_idx = {v: k for k, v in tokenizer.word_index.items()}","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"65a0bb86b6c99e2176d6b65bb34adf2a41c90296"},"cell_type":"markdown","source":"# Utility Functions"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3f1ae3192006a8fa20c9af5b31beff8fb2c65da3"},"cell_type":"code","source":"def closest(word, _top=5):\n    word = word_embedding[word]\n    cos_sim = cosine_similarity(word.reshape(1, -1), points)\n    top_n = cos_sim.argsort()[0][-_top:][::-1]\n    return [inverse_idx[_] for _ in top_n if _ in inverse_idx]\n\ndef similarity(word_1, word_2):\n    return cosine_similarity(\n        word_embedding[word_1].reshape(1, -1), \n        word_embedding[word_2].reshape(1, -1)\n    ).flatten()[0]","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"056c3a284b8c3a2e2bb3eb4b041f8ea429fa1c5f"},"cell_type":"markdown","source":"# Some Results\n- skipping the similarity with derogatory terms like \"liar\" etc. for obvious reasons. \n- users can train and check interesting similarities on their own :D"},{"metadata":{"trusted":true,"_uuid":"496479db1f40cff66c1ccaf78ad3ccaeb5e0a3c8"},"cell_type":"code","source":"print('Similarity between \"money\" and \"cash\" %.4f' % similarity('money', 'cash'))\nprint('Similarity between \"atm\" and \"bank\" %.4f' % similarity('atm', 'bank'))\nprint('Similarity between \"congress\" and \"rahul\" %.4f' % similarity('liar', 'rahul'))\nprint('Similarity between \"bjp\" and \"modi\" %.4f' % similarity('liar', 'modi'))","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"df19c02474c1f420ce5bfb3bb718a75027f2e306"},"cell_type":"markdown","source":"# Plot of Word Representations\n- The vector dimension of the words was kept at 2 for this very purpose.\n- Alternatively train a higher dimensional word embedding and then reduce dimensions using PCA for visualization"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9cf1fcd1327ece9876fb508dca14cbfeb7b98390"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt_x = points.transpose()[0, 1:]\nplt_y = points.transpose()[1, 1:]\nfig = plt.figure(figsize=(15, 250))\nax = fig.subplots()\nax.scatter(plt_x, plt_y)\n\nfor i, txt in enumerate([_ for _ in tokenizer.word_index]):\n    if i%5 == 0:\n        ax.annotate(txt, (plt_x[i], plt_y[i]))\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}