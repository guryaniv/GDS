{"nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["This is a basic LogisticRegression model trained using the data from https://www.kaggle.com/eoveson/convai-datasets-baseline-models\n", "\n", "The baseline model in that kernal is tuned a little to get the data for this kernal\n", "This kernal scored 0.045 in the LB"], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["# loading libraries\n", "import pandas as pd, numpy as np"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "4bec1f3e60c75b2f5694d85851008926c770a1ee", "_cell_guid": "12b0e062-9372-43eb-b26c-2c3bb069d63f"}}, {"cell_type": "code", "outputs": [], "source": ["# fixing seed.!!\n", "seed = 7\n", "np.random.seed(seed)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "377f028f429bba39a20bfd674d80f0a0a3fcaeab", "_cell_guid": "7381b67f-ad06-4e01-b2ed-19fa15b00b23"}}, {"cell_type": "code", "outputs": [], "source": ["# output of the kernal https://www.kaggle.com/eoveson/convai-datasets-baseline-models with some tunings\n", "test_new = pd.read_csv('../input/convai-datasets-baseline-models/test_with_convai.csv')\n", "train_new = pd.read_csv('../input/convai-datasets-baseline-models/train_with_convai.csv')"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "a0c99b65b855c65427c768fac481da88e377d815", "_cell_guid": "bcf50cb6-0514-4f3b-a13c-88bdf5d57b67"}}, {"cell_type": "code", "outputs": [], "source": ["# features we are interesed on\n", "feats_to_concat = ['comment_text', 'toxic_level', 'attack', 'aggression']"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "a781c415eef3c4178ed9e0ec83eb2532fd70b851", "_cell_guid": "c2addf90-27fe-4ebe-9b4d-d97bb5af7517"}}, {"cell_type": "code", "outputs": [], "source": ["# combining test and train\n", "alldata = pd.concat([train_new[feats_to_concat], test_new[feats_to_concat]], axis=0)\n", "alldata.comment_text.fillna('unknown', inplace=True)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "81056be77a391b390e23a562d606a68ac8e71187", "_cell_guid": "dc0ca431-70fe-40fa-b94e-85776cb55b65"}}, {"cell_type": "code", "outputs": [], "source": ["# loading libraries\n", "import nltk\n", "nltk.download('wordnet')\n", "from nltk.corpus import stopwords\n", "from nltk.stem import PorterStemmer\n", "from nltk.stem import WordNetLemmatizer\n", "import re"], "execution_count": null, "metadata": {"_uuid": "ae77a08bdf798e19edf38c8874741b9d60dd0c6d", "_cell_guid": "5e562d1a-20f3-44a7-b23e-7cb58b844fda"}}, {"cell_type": "code", "outputs": [], "source": ["# define function for cleaning..!!\n", "\n", "def cleanData(text, stemming = False, lemmatize=False):\n", "    \n", "    text = text.lower().split()\n", "    text = \" \".join(text)\n", "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n", "    text = re.sub(r\"what's\", \"what is \", text)\n", "    text = re.sub(r\"\\'s\", \" \", text)\n", "    text = re.sub(r\"\\'ve\", \" have \", text)\n", "    text = re.sub(r\"can't\", \"cannot \", text)\n", "    text = re.sub(r\"n't\", \" not \", text)\n", "    text = re.sub(r\"i'm\", \"i am \", text)\n", "    text = re.sub(r\"\\'re\", \" are \", text)\n", "    text = re.sub(r\"\\'d\", \" would \", text)\n", "    text = re.sub(r\"\\'ll\", \" will \", text)\n", "    text = re.sub(r\",\", \" \", text)\n", "    text = re.sub(r\"\\.\", \" \", text)\n", "    text = re.sub(r\"!\", \" ! \", text)\n", "    text = re.sub(r\"\\/\", \" \", text)\n", "    text = re.sub(r\"\\^\", \" ^ \", text)\n", "    text = re.sub(r\"\\+\", \" + \", text)\n", "    text = re.sub(r\"\\-\", \" - \", text)\n", "    text = re.sub(r\"\\=\", \" = \", text)\n", "    text = re.sub(r\"'\", \" \", text)\n", "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n", "    text = re.sub(r\":\", \" : \", text)\n", "    text = re.sub(r\" e g \", \" eg \", text)\n", "    text = re.sub(r\" b g \", \" bg \", text)\n", "    text = re.sub(r\" u s \", \" american \", text)\n", "    text = re.sub(r\"\\0s\", \"0\", text)\n", "    text = re.sub(r\" 9 11 \", \"911\", text)\n", "    text = re.sub(r\"e - mail\", \"email\", text)\n", "    text = re.sub(r\"j k\", \"jk\", text)\n", "    text = re.sub(r\"\\s{2,}\", \" \", text)\n", "    \n", "   \n", "    if stemming:\n", "        st = PorterStemmer()\n", "        txt = \" \".join([st.stem(w) for w in text.split()])\n", "        \n", "    if lemmatize:\n", "        wordnet_lemmatizer = WordNetLemmatizer()\n", "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])\n", "\n", "\n", "    return text"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "78d45e24dfd66206a5383fdae10012a7e1da6b15", "_cell_guid": "0dd15f0d-025c-4c42-9292-b8b9739daaad"}}, {"cell_type": "code", "outputs": [], "source": ["# cleaning data - stemm and lemm are done later\n", "alldata['comment_text'] = alldata['comment_text'].map(lambda x: cleanData(x,  stemming = False, lemmatize=False))"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "3826014d622ac8621a9778ad5a993fda33628091", "_cell_guid": "054f6e37-7eab-4dc6-8923-ee2a26845f0c"}}, {"cell_type": "code", "outputs": [], "source": ["# again libraries.!!\n", "from matplotlib import pyplot as plt\n", "from nltk.tokenize import wordpunct_tokenize\n", "from nltk.stem.snowball import EnglishStemmer\n", "from nltk.stem import WordNetLemmatizer\n", "from functools import lru_cache\n", "from tqdm import tqdm as tqdm\n", "from sklearn.metrics import log_loss\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.pipeline import Pipeline\n", "from scipy import sparse"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "e4d2ea1d8ca7311f8659fe3d1f8eaa89786bc65d", "_cell_guid": "c26fe15d-025f-4669-a8e3-4f3a0a7d2884"}}, {"cell_type": "code", "outputs": [], "source": ["# set stopwords\n", "from nltk.corpus import stopwords\n", "\n", "eng_stopwords = set(stopwords.words(\"english\"))"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "370ed58fb2e93e545b58b482914c8a00e0b5e0b0", "_cell_guid": "818af801-81f1-4a28-89e3-cdefcab0c33e"}}, {"cell_type": "code", "outputs": [], "source": ["# stemming and lemmatizing\n", "# adapted from the kernal \n", "stemmer = EnglishStemmer()\n", "\n", "@lru_cache(30000)\n", "def stem_word(text):\n", "    return stemmer.stem(text)\n", "\n", "\n", "lemmatizer = WordNetLemmatizer()\n", "\n", "@lru_cache(30000)\n", "def lemmatize_word(text):\n", "    return lemmatizer.lemmatize(text)\n", "\n", "\n", "def reduce_text(conversion, text):\n", "    return \" \".join(map(conversion, wordpunct_tokenize(text.lower())))\n", "\n", "\n", "def reduce_texts(conversion, texts):\n", "    return [reduce_text(conversion, str(text))\n", "            for text in tqdm(texts)]"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "91930ebb920a4584ceb23e97a4a08b873b43fe54", "_cell_guid": "75d8a64e-59af-4528-aef7-f050afbec4ee"}}, {"cell_type": "code", "outputs": [], "source": ["# lemmatizing and stemming\n", "alldata['comment_text'] = reduce_texts(stem_word, alldata['comment_text'])\n", "alldata['comment_text'] = reduce_texts(lemmatize_word, alldata['comment_text'])"], "execution_count": null, "metadata": {"_uuid": "8638487f82962c96ee7fe6cbe5f64e90e19b05f7", "_cell_guid": "b26a1073-e0da-461f-9a68-00a0f7728e98"}}, {"cell_type": "code", "outputs": [], "source": ["# making placeholder for prediction\n", "col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "\n", "only_col = ['toxic']\n", "\n", "preds = np.zeros((test_new.shape[0], len(col)))"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "479a47cfb10222633d154a4702b671a5e282c4b4", "_cell_guid": "934ffb62-8c22-4da2-90e5-61aae99c81f5"}}, {"cell_type": "code", "outputs": [], "source": ["# TfidfVectorizer for words and chars\n", "vect_words = TfidfVectorizer(max_features=40000, analyzer='word', ngram_range=(1, 1))\n", "vect_chars = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(1, 3))"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "63f59a9e8880cfa429b3dc6bbc4d80b5d2752a22", "_cell_guid": "dc07560b-d84f-4460-a712-cbc2a92ebc82"}}, {"cell_type": "code", "outputs": [], "source": ["# Creating features\n", "all_words = vect_words.fit_transform(alldata.comment_text)\n", "all_chars = vect_chars.fit_transform(alldata.comment_text)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "13ef5bc73e40832e03df02804cc2140553046822", "_cell_guid": "65253ee8-b182-42e6-9567-1d4b9a1ee3db"}}, {"cell_type": "code", "outputs": [], "source": ["# splitting to train and test\n", "train_words = all_words[:len(train_new)]\n", "test_words = all_words[len(train_new):]\n", "\n", "train_chars = all_chars[:len(train_new)]\n", "test_chars = all_chars[len(train_new):]"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "09f2f806538b375bc32fbb0a43001444c36996ca", "_cell_guid": "50d9a084-21cc-4ca6-8fe3-a2a139f59c2d"}}, {"cell_type": "markdown", "source": ["It can be seen from the dataset that the features attack and aggression is very much same. So we will only take one.\n", "Here I take attack leaving aggression"], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["# needed feats.!!\n", "feats = ['toxic_level', 'attack']"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "a3aadec5446294b7562bbf1ac199ef8a0267edc6", "_cell_guid": "f30e63b0-3c19-4feb-b9bb-7ed817be5a29"}}, {"cell_type": "code", "outputs": [], "source": ["# make sparse matrix with needed data for train and test\n", "train_feats = sparse.hstack([train_words, train_chars, alldata[feats][:len(train_new)]])\n", "test_feats = sparse.hstack([test_words, test_chars, alldata[feats][len(train_new):]])"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "4725cf76651697d80da9d3bef90d41a833df3edc", "_cell_guid": "606265d2-a25e-4181-b2bd-acc4bc2fa1f4"}}, {"cell_type": "code", "outputs": [], "source": ["# libraries.!\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import log_loss\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "0f4cae7efd9bb89b940d6c7ec316a9a596980164", "_cell_guid": "07088448-6278-4cea-9f70-d46316c823c0"}}, {"cell_type": "code", "outputs": [], "source": ["# fit a LogisticRegression model on full train data and make prediction\n", "for i, j in enumerate(col):\n", "    print('===Fit '+j)\n", "    \n", "    model = LogisticRegression(C=4.0, solver='sag')\n", "    print('Fitting model')\n", "    model.fit(train_feats, train_new[j])\n", "      \n", "    print('Predicting on test')\n", "    preds[:,i] = model.predict_proba(test_feats)[:,1]"], "execution_count": null, "metadata": {"_uuid": "1f451754ac9680abdb7cc2441fed85db02fc8e58", "_cell_guid": "3b256937-6512-48b9-b07c-e959e5602377"}}, {"cell_type": "code", "outputs": [], "source": ["# make submission..!!\n", "subm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n", "\n", "submid = pd.DataFrame({'id': subm[\"id\"]})\n", "submission = pd.concat([submid, pd.DataFrame(preds, columns = col)], axis=1)\n", "submission.to_csv('feat_lr_2cols.csv', index=False) # 0.045 in the LB"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "8f7fbf044a6cd80279ad839def5e67976d882e4e", "_cell_guid": "14f3a10f-ab93-42f3-82e8-239e44a79516"}}, {"cell_type": "code", "outputs": [], "source": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "66577e4b1500b0f6fb1411aa54966267bcfa7875", "_cell_guid": "4444de1c-5160-4e5d-a8dd-6bf6fbcb45e9"}}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "pygments_lexer": "ipython3"}}}