{"cells": [{"cell_type": "markdown", "source": ["# Introduction\n", "This is my first time working with textual data. Thanks to Edgar Allan Poe for the motivation!"], "metadata": {"_uuid": "bb2da9a141343f8dee9fbad7a4dd199a4ac2fbec", "_cell_guid": "d4400f82-681b-4e3c-82cb-46841c9638a1"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# load necessary modules\n", "import pandas as pd\n", "from wordcloud import WordCloud\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import re\n", "from nltk.tokenize import sent_tokenize\n", "from nltk.tokenize import word_tokenize\n", "from collections import Counter\n", "from nltk.corpus import stopwords\n", "from nltk.stem import WordNetLemmatizer\n", "from gensim.corpora.dictionary import Dictionary\n", "from collections import defaultdict\n", "import itertools\n", "from gensim.models.tfidfmodel import TfidfModel\n", "from nltk import pos_tag\n", "from nltk import ne_chunk_sents\n", "import spacy\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn import metrics\n", "from sklearn.naive_bayes import MultinomialNB\n", "import numpy as np\n", "\n", "# read data sets\n", "df_train = pd.read_csv(\"../input/train.csv\")\n", "df_tests = pd.read_csv(\"../input/test.csv\")\n", "df_sampl = pd.read_csv(\"../input/sample_submission.csv\")"], "metadata": {"_uuid": "2e6b7e9c659f531b73a88bbdc75e6e8a5677b37d", "collapsed": true, "_cell_guid": "a89df7d0-7053-461e-8ee4-73da127a2cf8"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# inspect training set\n", "print(df_train.shape)\n", "df_train.head()"], "metadata": {"_uuid": "89b21faf1133ce92d75ac03d0020f4a5f7a5f6b5", "_cell_guid": "57216857-267d-4905-917b-0db0ef1e5e62"}}, {"cell_type": "markdown", "source": ["The training set contains 19,579 rows and 3 columns: id, text, and author. We're obviously going to have to do some feature engineering. The target variable, author, has three possible values: EAP for Edgar Allan Poe, HPL for HP Lovecraft, and MWS for Mary Wollstonecraft Shelley."], "metadata": {"_uuid": "9f1fe7e776402966213b36b3addf524cbbb6193d", "_cell_guid": "330ac6ba-6538-467b-83ea-bb0a3e5ac36f"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# analyze distribution of target variable, author\n", "df_train[\"author\"].value_counts().plot.bar()"], "metadata": {"_uuid": "1c5a43911438bb93f12c237d3872431c85365efc", "_cell_guid": "3aa065f0-0d3a-4f1e-a5d7-99c107de0189"}}, {"cell_type": "markdown", "source": ["The distribution of authors is pretty balanced so I don't think we will need to use any type of subsampling."], "metadata": {"_uuid": "fd4e3e6b2295a31ea79fcb7db75b4c0f9f04e4f3", "_cell_guid": "c3ad93a5-9f6b-460b-a1ef-5675a7e259c3"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# inspect the test set\n", "print(df_tests.shape)\n", "df_tests.head()"], "metadata": {"_uuid": "21cd4ed1da9b50735c062b93c33a21e0f48b4de6", "_cell_guid": "7f58e597-46ff-4958-9940-50c3272bf837"}}, {"cell_type": "markdown", "source": ["The test set contains 8,392 rows and 2 columns: id and text."], "metadata": {"_uuid": "3b694a63fe630421a8de7e3ea5d70ec74db500c6", "_cell_guid": "27acf95c-7d87-4501-bed7-0269f793acab"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# inspect the sample submission\n", "print(df_sampl.shape)\n", "df_sampl.head()"], "metadata": {"_uuid": "713e01387067983416a42be78f38c13fe47dd48e", "_cell_guid": "de43a16e-bc2b-4f1a-ae10-ef89f833ac35"}}, {"cell_type": "markdown", "source": ["The sample submission contains 8,392 rows (consistent with the test set) and 4 columns: id and the probability that the text was written by EAP, HPL, and MWS.\n", "\n", "# Visualizations\n", "I want to make one of those plots where the size of the word is related to the number of times is appears in the text. Here's how I see this working. I concatenate the text for EAP, HPL, and MWS, and then make a plot for each author."], "metadata": {"_uuid": "f5456f2d7b272f66fa3a0690817152495b706539", "_cell_guid": "52d6c931-b4c8-488d-ba9e-04373dcd068a"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# concatenate text written by EAP\n", "eap_texts = df_train[df_train[\"author\"] == \"EAP\"][\"text\"].str.cat(sep = \" \")"], "metadata": {"_uuid": "04285d68916ffb1471624febef2449824deae439", "collapsed": true, "_cell_guid": "43ea7f15-141a-4f81-989c-69c08a9c0a01"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# generate word cloud\n", "eap_wordcloud = WordCloud().generate(eap_texts)\n", "\n", "# Display the generated image:\n", "# the matplotlib way:\n", "plt.imshow(eap_wordcloud, interpolation = \"bilinear\")\n", "plt.axis(\"off\")"], "metadata": {"_uuid": "c04633e974961cce3b5d181ce8ce96e11355d6f2", "_cell_guid": "c65fd13f-f490-4d94-acf0-bd655837b940"}}, {"cell_type": "markdown", "source": ["Very exciting! EAP really likes to use the word \"upon\"."], "metadata": {"_uuid": "49ca3503d0552003883b64490c84c0dcc0efba05", "_cell_guid": "4bbd128e-91cd-489b-9785-7f16eea101c9"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# count number of times \"upon\" appears in the text written by EAP\n", "eap_texts.count(\"upon\")"], "metadata": {"_uuid": "ef15ecfbfce0e3876cf35e1fd988fcbb449925ed", "_cell_guid": "5e1d076d-5c5a-4ad6-afb8-7bdc3051f50d"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# concatenate text written by HPL and MWS\n", "hpl_texts = df_train[df_train[\"author\"] == \"HPL\"][\"text\"].str.cat(sep = \" \")\n", "mws_texts = df_train[df_train[\"author\"] == \"MWS\"][\"text\"].str.cat(sep = \" \")\n", "\n", "# generate word cloud\n", "hpl_wordcloud = WordCloud().generate(hpl_texts)\n", "mws_wordcloud = WordCloud().generate(mws_texts)\n", "\n", "# display the generated image\n", "plt.imshow(hpl_wordcloud, interpolation = \"bilinear\")\n", "plt.axis(\"off\")"], "metadata": {"_uuid": "eb8d00bd3fd65419cae23f81f8a449c79b33adfb", "_cell_guid": "71f01911-1cdc-4e9d-8fb3-24d66deb4469"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# display the generated image\n", "plt.imshow(mws_wordcloud, interpolation = \"bilinear\")\n", "plt.axis(\"off\")"], "metadata": {"_uuid": "4fc7565840f2616814a1bc0528cff610f6744fac", "_cell_guid": "5803312e-af72-4acb-9a6d-c937361530b9"}}, {"cell_type": "markdown", "source": ["This word cloud for MWS has an interesting feature, the name \"Raymond\". This is likely the name of a character in one of her works (https://en.wikipedia.org/wiki/The_Last_Man). Therefore, the presence of the word \"Raymond\" most likely corresponds to text written by MWS. Let's see if this is so."], "metadata": {"_uuid": "75db1f9c51301d81c987ab05aa66ea7e37d83b9b", "_cell_guid": "2ffd1b43-93f9-4ef5-ba97-1f223d26d341"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# count the number of occurences of \"Raymond\" in each authors' texts\n", "print(eap_texts.count(\"Raymond\"))\n", "print(hpl_texts.count(\"Raymond\"))\n", "print(mws_texts.count(\"Raymond\"))"], "metadata": {"_uuid": "c8c373a025977c56d757e100c1b8b29b81c1214c", "_cell_guid": "9e812821-98f1-47ce-90aa-e21e3fdc73b1"}}, {"cell_type": "markdown", "source": ["Indeed, the word \"Raymond\" appears in text written by MWS more than 99% of the time."], "metadata": {"_uuid": "910bb825fbe172f2ff2d519c75b6cb3364e5f4d2", "_cell_guid": "3397257c-1586-4e6a-8910-d169760e21a7"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# add the number of characters in text as a feature\n", "df_train[\"n_char\"] = df_train[\"text\"].map(lambda x: len(x))"], "metadata": {"_uuid": "40f8b864775d0eb1b5bb040c4cac7218b853d404", "collapsed": true, "_cell_guid": "7fd65b4d-9a75-4633-9424-e93f0b4d76ad"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["df_train[\"n_char\"].plot.hist(bins = 500)"], "metadata": {"_uuid": "31dd28a4a70337ad972ce298e2e6d8a9bdf24642", "_cell_guid": "be474e3e-6a91-4827-9772-628fe113be3c"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["df_train[\"n_char\"].describe()"], "metadata": {"_uuid": "f2041bad41e14c4461fefdeaf3b21039a088ec39", "_cell_guid": "d909a6ec-6a0a-4343-aa5e-793d72e220ed"}}, {"cell_type": "markdown", "source": ["The median number of characters is 128. Since typical word length is 5 characters, this corresponds to about 25 words per text. Interestingly, the maximum number of characters is 4,663."], "metadata": {"_uuid": "37363d3d49f5b6de573ee4f4d7de212a188367ac", "_cell_guid": "a9712709-855b-4cab-b8ea-6a755c697dea"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["df_train[df_train[\"n_char\"] == 4663][\"text\"].values"], "metadata": {"_uuid": "71f2f8e386ac44311e2c9ddbb105eb2da9e177ac", "_kg_hide-output": true, "_cell_guid": "59d6db33-b2a6-4858-b908-325695cb076d"}}, {"cell_type": "markdown", "source": ["This excerpt is from Mathilda by MWS (https://en.wikipedia.org/wiki/Mathilda_(novella)). Upon inspection of the source, it appears that there is no punctuation in this passage. This is why the number of characters in this text is so large."], "metadata": {"_uuid": "e7a7b7a5c726a5b512f3d354894989a751fa131e", "_cell_guid": "7d465c1e-f085-42b7-a88c-1c71e29b5fb4"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["sns.FacetGrid(df_train, hue = \"author\", size = 6) \\\n", "   .map(sns.kdeplot, \"n_char\") \\\n", "   .add_legend()\n", "plt.xlim(0, 500)"], "metadata": {"_uuid": "827296ffd9863b2544f40b768d15e4fee5b32e13", "_cell_guid": "11559923-a7fe-4d4a-b82d-d4467fdaf64b"}}, {"cell_type": "markdown", "source": ["This plot shows that EAP tends to write shorter sentences than HPL and MWS. There is not enough separation in the target variable, however, for the number of characters to be an important feature in author classification.\n", "\n", "# Natural language processing\n", "It has been established that natural language processing is an integral part of building textual classifiers. Since I don't have any experience with these techniques, my analyses will follow the DataCamp course \"Natural Language Processing Fundamentals in Python\" (https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python). Please comment if you have any suggestions for processing techniques that I have not covered.\n", "\n", "## Regular expressions & word tokenization"], "metadata": {"_uuid": "7754836ca2ffdd250e8ca875691582d6f1a9f656", "_cell_guid": "58a700e6-4754-4eef-861f-b919a362194a"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# split EAP texts into sentences\n", "sentences = sent_tokenize(eap_texts)\n", "sentences[3]"], "metadata": {"_uuid": "a0773e4c7658152c59cd84d616852306b4bb6e70", "_cell_guid": "97e09e0f-13bc-4296-8eb0-89920ccbd0d0"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# tokenize the 4th sentence\n", "tokenized_sent = word_tokenize(sentences[3])\n", "tokenized_sent"], "metadata": {"_uuid": "f4c0c3ecfbc3f85de12d86767004d9531b4bb367", "_cell_guid": "a9012742-976a-4f14-910d-5d6da42ab6c8"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# make a set of unique tokens in EAP texts\n", "unique_tokens = set(word_tokenize(eap_texts))\n", "unique_tokens"], "metadata": {"_uuid": "f8db137ecadec136e1a3d729f8b4763865323b34", "_kg_hide-output": true, "_cell_guid": "af8e421e-9cd4-4fb8-b982-6e9c800b65f4"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# search for the first occurrence of \"Raymond\" in MWS texts\n", "match = re.search(r\"Raymond\", mws_texts)\n", "print(match)\n", "print(match.start(), match.end())"], "metadata": {"_uuid": "e5d619c6a42f2e4fa9a6795ce1c9ed011dd760b3", "_cell_guid": "3d64fdac-96f9-4a08-9561-890c91abeaac"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# search for anything in quotes in EAP texts\n", "# https://stackoverflow.com/questions/171480/regex-grabbing-values-between-quotation-marks\n", "pattern1 = r'\"(.*?)\"'\n", "\n", "# find the first text in quotes\n", "print(re.search(pattern1, eap_texts))"], "metadata": {"_uuid": "834c07ae25f934d6dcf7205e9ab69f6e2fce6581", "_cell_guid": "ab9eaa82-59e9-4f6c-9788-028d75d711be"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# plot a histogram of the word lengths in texts written by all authors\n", "eap_words = word_tokenize(eap_texts)\n", "hpl_words = word_tokenize(hpl_texts)\n", "mws_words = word_tokenize(mws_texts)\n", "eap_word_lenghts = [len(w) for w in eap_words]\n", "hpl_word_lenghts = [len(w) for w in hpl_words]\n", "mws_word_lenghts = [len(w) for w in mws_words]\n", "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey = True)\n", "ax1.hist(eap_word_lenghts, bins = 20)\n", "ax2.hist(hpl_word_lenghts, bins = 20)\n", "ax3.hist(mws_word_lenghts, bins = 20)\n", "plt.show()"], "metadata": {"_uuid": "207cf3857c271ca6a986cb681bf6d0c065a519b4", "_cell_guid": "ac089144-083b-4c97-86f1-a9c45504b059"}}, {"cell_type": "markdown", "source": ["## Simple topic identification"], "metadata": {"_uuid": "90b0d5efef384eef54fae9c083842037fb1bfe0c", "_cell_guid": "6d9373ae-0aa2-484a-947e-0d0d99c2f994"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# building a counter with bag-of-words for EAP texts\n", "# tokenize\n", "tokens = word_tokenize(eap_texts)\n", "\n", "# convert the tokens into lowercase\n", "lower_tokens = [t.lower() for t in tokens]\n", "\n", "# create a counter with the lowercase tokens\n", "bow_simple = Counter(lower_tokens)\n", "\n", "# print the 10 most common tokens\n", "print(bow_simple.most_common(10))"], "metadata": {"_uuid": "a4c494a829cf06e0a838b0c866461e55457685dc", "_cell_guid": "0d1523eb-5655-4e3a-834f-d365ce60dec4"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# text preprocessing for EAP texts\n", "# retain alphabetic words\n", "alpha_only = [t for t in lower_tokens if t.isalpha()]\n", "\n", "# remove all stop words (and, the, etc.)\n", "no_stops = [t for t in alpha_only if t not in stopwords.words(\"english\")]\n", "\n", "# instantiate the WordNetLemmatizer\n", "wordnet_lemmatizer = WordNetLemmatizer()\n", "\n", "# lemmatize (reduce words to their word stem) all tokens into a new list\n", "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n", "\n", "# create the bag-of-words\n", "bow = Counter(lemmatized)\n", "\n", "# print the 10 most common tokens\n", "print(bow.most_common(10))"], "metadata": {"_uuid": "ec3760f8fe3ebea851c5de9278d979e799fad4c4", "_kg_hide-output": false, "_cell_guid": "46995bad-6de1-4d6f-bc91-ffa9c798a988"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# generate word cloud\n", "bow_wordcloud = WordCloud().generate_from_frequencies(bow)\n", "\n", "# Display the generated image:\n", "# the matplotlib way:\n", "plt.imshow(bow_wordcloud, interpolation = \"bilinear\")\n", "plt.axis(\"off\")"], "metadata": {"_uuid": "2f2cdd5d0d265d3fd694b460b6e9f393b043bdc8", "_cell_guid": "8545a724-470e-486d-bc84-af6e36333559"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# creating a corpus with gensim for EAP texts\n", "my_documents = df_train[df_train[\"author\"] == \"EAP\"][\"text\"].values\n", "\n", "# tokenize and convert the tokens into lowercase\n", "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n", "\n", "# retain alphabetic words\n", "# https://stackoverflow.com/questions/18072759/python-nested-list-comprehension\n", "alpha_only_docs = [[t for t in doc if t.isalpha()] for doc in tokenized_docs]\n", "\n", "# remove all stop words (and, the, etc.)\n", "# for some reason, this is much faster than list comprehension\n", "no_stops_docs = []\n", "for i, doc in enumerate(alpha_only_docs) :\n", "    print(i + 1, \" of \", len(alpha_only_docs), \" texts\")\n", "    no_stops = []\n", "    for t in doc :\n", "        if t not in stopwords.words(\"english\") :\n", "            no_stops.append(t)\n", "    no_stops_docs.append(no_stops)\n", "\n", "# lemmatize (reduce words to their word stem) all tokens into a new list\n", "lemmatized_docs = [[wordnet_lemmatizer.lemmatize(t) for t in doc] for doc in no_stops_docs]\n", "\n", "# create a dictionary from the articles\n", "dictionary = Dictionary(lemmatized_docs)\n", "\n", "# create a MmCorpus\n", "corpus = [dictionary.doc2bow(doc) for doc in lemmatized_docs]"], "metadata": {"_uuid": "c3ce2c793c6d7d784a65d81f50f60dbad80cb263", "_kg_hide-output": true, "_cell_guid": "c920857d-164e-4c5a-8699-437c569c0669"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# save the first document\n", "doc = corpus[0]\n", "\n", "# sort the doc for frequency\n", "bow_doc = sorted(doc, key = lambda w : w[1], reverse = True)\n", "\n", "# print the top 5 words of the document alongside the count\n", "print(\"First Document:\")\n", "for word_id, word_count in bow_doc[:5] :\n", "    print(dictionary.get(word_id), word_count)\n", "\n", "# create the defaultdict\n", "total_word_count = defaultdict(int)\n", "for word_id, word_count in itertools.chain.from_iterable(corpus):\n", "    total_word_count[word_id] += word_count\n", "\n", "# create a sorted list from the defaultdict\n", "sorted_word_count = sorted(total_word_count.items(), key = lambda w : w[1], reverse = True)\n", "\n", "# print the top 5 words across all documents alongside the count\n", "print(\" \")\n", "print(\"All Documents:\")\n", "for word_id, word_count in sorted_word_count[:5]:\n", "    print(dictionary.get(word_id), word_count)"], "metadata": {"_uuid": "6c968ae583c37c6dd7d84acf0b9050a38c6d86a6", "scrolled": true, "_cell_guid": "e29e0dc2-a156-4439-b4f0-157714311dc6"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# tf-idf\n", "\n", "# creating a corpus with gensim for EAP texts\n", "all_documents = [eap_texts, hpl_texts, mws_texts]\n", "\n", "# tokenize and convert the tokens into lowercase\n", "tokenized_docs = [word_tokenize(doc.lower()) for doc in all_documents]\n", "\n", "# retain alphabetic words\n", "alpha_only_docs = [[t for t in doc if t.isalpha()] for doc in tokenized_docs]\n", "\n", "# remove all stop words\n", "no_stops_docs = []\n", "for i, doc in enumerate(alpha_only_docs) :\n", "    print(i + 1, \" of \", len(alpha_only_docs), \" authors\")\n", "    no_stops = []\n", "    for t in doc :\n", "        if t not in stopwords.words(\"english\") :\n", "            no_stops.append(t)\n", "    no_stops_docs.append(no_stops)\n", "\n", "# lemmatize (reduce words to their word stem) all tokens into a new list\n", "lemmatized_docs = [[wordnet_lemmatizer.lemmatize(t) for t in doc] for doc in no_stops_docs]\n", "\n", "# create a dictionary from the articles\n", "dictionary = Dictionary(lemmatized_docs)\n", "\n", "# create a MmCorpus\n", "corpus = [dictionary.doc2bow(doc) for doc in lemmatized_docs]"], "metadata": {"_uuid": "d048a9ac21ba4f4a5fb28dfc9ad86b1f2b75566a", "_kg_hide-output": true, "_cell_guid": "cda0c319-a667-444e-991e-6a720bf0f932"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# create a new TfidfModel using the corpus\n", "tfidf = TfidfModel(corpus)\n", "\n", "# calculate the tfidf weights of EAP texts\n", "tfidf_weights = tfidf[corpus[0]]\n", "\n", "# sort the weights from highest to lowest\n", "sorted_tfidf_weights = sorted(tfidf_weights, key = lambda w : w[1], reverse = True)\n", "\n", "# print the top 5 weighted words\n", "for term_id, weight in sorted_tfidf_weights[:5] :\n", "    print(dictionary.get(term_id), weight)"], "metadata": {"_uuid": "8873747260f5fee42279d01544d6cd7f844ee796", "_cell_guid": "cbfdbec1-2dc3-459c-af50-0a0fb59664ac"}}, {"cell_type": "markdown", "source": ["Ok, so this is very cool. These are the top 5 words sorted by their tf-idf weights. The higher the weight, the more  that particular word uniquely classifies the author. So where do these words come from?\n", "* Dupin: https://en.wikipedia.org/wiki/C._Auguste_Dupin\n", "* Marie: https://en.wikipedia.org/wiki/The_Mystery_of_Marie_Rog%C3%AAt\n", "* Jupiter: https://en.wikipedia.org/wiki/The_Gold-Bug\n", "* Ellison: http://xroads.virginia.edu/~hyper/poe/l_garden.html\n", "\n", "## Named-entity recognition\n", "### NLTK"], "metadata": {"_uuid": "ff69deda993ddaa853155921bf242909c1bd891b", "_kg_hide-output": false, "_cell_guid": "917e8919-d13c-49fe-8cf6-5406bcf1f179"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# tokenize EAP texts into sentences\n", "sentences = sent_tokenize(eap_texts)\n", "\n", "# tokenize each sentence into words\n", "token_sentences = [word_tokenize(sent) for sent in sentences]\n", "\n", "# tag each tokenized sentence into parts of speech\n", "pos_sentences = [pos_tag(sent) for sent in token_sentences] \n", "\n", "# create the named entity chunks\n", "chunked_sentences = ne_chunk_sents(pos_sentences, binary = True)\n", "\n", "# test for stems of the tree with \"NE\" tags\n", "for sent in chunked_sentences:\n", "    for chunk in sent:\n", "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n", "            print(chunk)"], "metadata": {"_uuid": "176c89b34470b89d571ce97f81ac9b0879d2ab6f", "_kg_hide-output": true, "_cell_guid": "e4b658e8-7e7c-44f3-bf55-5026ca553377"}}, {"cell_type": "markdown", "source": ["### spaCy"], "metadata": {"_uuid": "bd8c5ee67a05f016c2a08e403e42f708bccedd8c", "_cell_guid": "41c969fc-0ab0-4d95-b684-b45a4ce7c3ea"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# instantiate the english model\n", "nlp = spacy.load('en', tagger = False, parser = False, matcher = False)\n", "\n", "# create a new document\n", "doc = nlp(eap_texts)\n", "\n", "# Print all of the found entities and their labels\n", "for ent in doc.ents:\n", "    print(ent.label_, ent.text)"], "metadata": {"_uuid": "2b8b669cae96acce0a07822705303377a6a392a9", "_kg_hide-output": true, "_cell_guid": "8413575f-1367-4828-a2c8-7625fd8733c6"}}, {"cell_type": "markdown", "source": ["## Supervised learning\n", "Awesome! So far we've learned a lot about how to process textual data, identify topics, and recognize named entities. Now, we will learn how to train a supervised learning model that can predict the author from the text."], "metadata": {"_uuid": "42370b4c5c966d7d0bb9144d631bcd490e7d8cc8", "_cell_guid": "6e8688e3-edc0-4d41-9851-1c9f467e6f91"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# create a series to store the labels\n", "y = df_train[\"author\"]\n", "\n", "# create training and test sets\n", "X_train, X_test, y_train, y_test = train_test_split(df_train[\"text\"], y, test_size = 0.33, random_state = 53)\n", "\n", "# initialize a CountVectorizer object\n", "count_vectorizer = CountVectorizer(stop_words=\"english\")\n", "\n", "# transform the training data using only the 'text' column values\n", "count_train = count_vectorizer.fit_transform(X_train.values)\n", "\n", "# transform the test data using only the 'text' column values\n", "count_test = count_vectorizer.transform(X_test.values)\n", "\n", "# print the first 10 features of the count_vectorizer\n", "print(count_vectorizer.get_feature_names()[:10])"], "metadata": {"_uuid": "edd8aa0337b5d277f24569ff1879cdc6c54417ae", "_cell_guid": "1cf4ab3e-5bd2-49f1-b858-b1b149b611d9"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# initialize a TfidfVectorizer object\n", "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n", "\n", "# transform the training data\n", "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n", "\n", "# transform the test data\n", "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n", "\n", "# print the first 10 features\n", "print(tfidf_vectorizer.get_feature_names()[:10])\n", "\n", "# print the first 5 vectors of the tfidf training data\n", "print(tfidf_train[:5])"], "metadata": {"_uuid": "46566772fc4c94c32624ecf0a2ef4b4c223a360a", "_kg_hide-output": false, "_cell_guid": "fef7c0c6-54dc-4008-ae71-740ae24d9c4d"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# create the CountVectorizer DataFrame\n", "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n", "\n", "# create the TfidfVectorizer DataFrame\n", "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n", "\n", "# print the head of count_df\n", "print(count_df.head())\n", "\n", "# print the head of tfidf_df\n", "print(tfidf_df.head())\n", "\n", "# calculate the difference in columns\n", "difference = set(count_df.columns) - set(tfidf_df.columns)\n", "print(difference)\n", "\n", "# check whether the DataFrames are equal\n", "print(count_df.equals(tfidf_df))"], "metadata": {"_uuid": "4286822a04dff61b6abc57d7c816c1e1f5614638", "_cell_guid": "586da1db-22d7-4e4f-a4cd-1782ca67c36e"}}, {"cell_type": "markdown", "source": ["### Training and testing with CountVectorizer"], "metadata": {"_uuid": "45c71ca864ec2cdf956606866ab70386a47b343d", "_cell_guid": "a79adae5-47f0-45ea-9b51-de3c90f8093c"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# instantiate a Multinomial Naive Bayes classifier\n", "nb_classifier = MultinomialNB()\n", "\n", "# fit the classifier to the training data\n", "nb_classifier.fit(count_train, y_train)\n", "\n", "# create the predicted tags\n", "pred = nb_classifier.predict(count_test)\n", "\n", "# calculate the accuracy score\n", "score = metrics.accuracy_score(y_test, pred)\n", "print(score)\n", "\n", "# calculate the confusion matrix\n", "cm = metrics.confusion_matrix(y_test, pred, labels=['EAP', 'MWS', 'HPL'])\n", "print(cm)"], "metadata": {"_uuid": "3b4fe67e722ff0121076e5ddc33bb08c2ae42820", "_cell_guid": "479bc2ee-8ba5-446e-9d67-51cac02fb51b"}}, {"cell_type": "markdown", "source": ["### Training and testing with TfidfVectorizer"], "metadata": {"_uuid": "f871eb812cdd7358a02a79413e798f2f92030834", "_cell_guid": "4b41c7f2-05f9-44ac-a5c6-15ebbcbfb135"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# create a Multinomial Naive Bayes classifier\n", "nb_classifier = MultinomialNB()\n", "\n", "# fit the classifier to the training data\n", "nb_classifier.fit(tfidf_train, y_train)\n", "\n", "# create the predicted tags\n", "pred = nb_classifier.predict(tfidf_test)\n", "\n", "# calculate the accuracy score\n", "score = metrics.accuracy_score(y_test, pred)\n", "print(score)\n", "\n", "# calculate the confusion matrix\n", "cm = metrics.confusion_matrix(y_test, pred, labels=['EAP', 'MWS', 'HPL'])\n", "print(cm)"], "metadata": {"_uuid": "a7b35c36e4a87b2fcad76d264c5f097f8ed5b885", "_cell_guid": "d1fa55f8-c3fb-4f58-8a01-393839a5dd09"}}, {"cell_type": "markdown", "source": ["### Improving the model"], "metadata": {"_uuid": "a950567d7979e5e2b72d6b09e71e9a3db721fd54", "_cell_guid": "8c35c707-99d3-4a84-b8ca-e100763827a5"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# create the list of alphas\n", "alphas = np.arange(0, 1, 0.1)\n", "\n", "# define train_and_predict()\n", "def train_and_predict(alpha):\n", "    # instantiate the classifier\n", "    nb_classifier = MultinomialNB(alpha=alpha)\n", "    # fit to the training data\n", "    nb_classifier.fit(tfidf_train, y_train)\n", "    # predict the labels\n", "    pred = nb_classifier.predict(tfidf_test)\n", "    # compute accuracy\n", "    score = metrics.accuracy_score(y_test, pred)\n", "    return score\n", "\n", "# Iterate over the alphas and print the corresponding score\n", "for alpha in alphas:\n", "    print('Alpha: ', alpha)\n", "    print('Score: ', train_and_predict(alpha))\n", "    print()"], "metadata": {"_uuid": "d27f916b0c38a28b163e1a90ef06272bd386e59f", "_cell_guid": "20992dbc-9100-46f4-869b-8e8cd21cee19"}}, {"cell_type": "markdown", "source": ["#### Multi-class logarithmic loss"], "metadata": {"_uuid": "fef29ef79d558252fc966f9074ac7c573bd575a8", "_cell_guid": "dfbb5974-3fd3-4075-95c0-8b055632bcd4"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# create a Multinomial Naive Bayes classifier\n", "nb_classifier = MultinomialNB(alpha = 0.1)\n", "\n", "# fit the classifier to the training data\n", "nb_classifier.fit(tfidf_train, y_train)\n", "\n", "# create the predicted tags\n", "pred = nb_classifier.predict(tfidf_test)\n", "pred_prob = nb_classifier.predict_proba(tfidf_test)\n", "\n", "# calculate the accuracy score\n", "acc = metrics.accuracy_score(y_test, pred)\n", "logloss = metrics.log_loss(y_test, pred_prob)\n", "print(acc)\n", "print(logloss)\n", "\n", "# calculate the confusion matrix\n", "cm = metrics.confusion_matrix(y_test, pred, labels=['EAP', 'MWS', 'HPL'])\n", "print(cm)"], "metadata": {"_uuid": "da86e86a4ab163828e5593a5e057ff1386669467", "_cell_guid": "bab1dff0-3abd-415c-9c31-bb14f5960104"}}, {"cell_type": "markdown", "source": ["# Submission\n", "Wow! We have come a long way. Now we are ready to make predictions on the test data and generate a submission file."], "metadata": {"_uuid": "0f131dae889e62ff8313cef578d47928e77b7376", "_cell_guid": "19a51ee3-1b06-43b1-b117-3826b8ee34df"}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["# transform the test data\n", "tfidf_test = tfidf_vectorizer.transform(df_tests[\"text\"].values)\n", "\n", "pred_prob = nb_classifier.predict_proba(tfidf_test)\n", "\n", "df_sampl[\"EAP\"] = pred_prob[:, 0]\n", "df_sampl[\"HPL\"] = pred_prob[:, 1]\n", "df_sampl[\"MWS\"] = pred_prob[:, 2]\n", "\n", "df_sampl.to_csv(\"submission.csv\", index=False)"], "metadata": {"_uuid": "c1c29ff5d343fe1ea79838ab732df1b40e84e64b", "collapsed": true, "_cell_guid": "cc9a5a38-5e18-4e10-a88e-f3557285d11d"}}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}}}