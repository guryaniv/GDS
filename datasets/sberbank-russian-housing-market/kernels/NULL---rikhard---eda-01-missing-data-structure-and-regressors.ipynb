{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0fe73dcd-9946-6eb9-cff2-a5fee0cac69f"
      },
      "source": [
        "# Sberbank Russian Housing Market - Exploratory Data Analysis\n",
        "\n",
        "In this kernel, we will take a look at the train data to see what are we dealing with in this competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1251522e-1c60-4178-7bf2-3878af44f22a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bd1878d7-c793-41a0-40b6-65979cf9919d"
      },
      "source": [
        "### Amount of data\n",
        "\n",
        "After loading a few useful libraries, let's load the data and see how many data do we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4f0353e-31a7-4739-15f5-8ed8853c269c"
      },
      "outputs": [],
      "source": [
        "# Load train data\n",
        "df = pd.read_csv('../input/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3a8161b5-9e40-2aaa-16fa-580085892335"
      },
      "outputs": [],
      "source": [
        "print(df.columns.tolist())\n",
        "print('\\nNumber of columns on train data:',len(df.columns))\n",
        "print('\\nNumber of data points:',len(df))\n",
        "print('\\nNumber of unique timestamp data points:',len(df['timestamp'].unique()))\n",
        "print('\\nNumber of unique id data points:',len(df['id'].unique()))\n",
        "print('\\nData types:',df.dtypes.unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d531e103-7c44-f4fb-4a26-d1087ecefe73"
      },
      "source": [
        "If we discard 'id' and 'timestamp' as features, and price_doc because that is the target variable, we have 289 features and only 30471 data points, so overfitting could be a problem here.\n",
        "\n",
        "It seems we have both categorical and numerical data. Let's take a look to that categorical data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b55d69c6-a7dd-4472-4739-90eb937ec244"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=['O']).columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7cd4e5cc-7050-c95d-543a-09da05d30006"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=['O']).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "82510c4a-79b8-ade3-6a2a-ff152475b191"
      },
      "source": [
        "### Missing data\n",
        "\n",
        "Let's check the quality of the data: next we are going to look at the structure of NaN's in the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b5561ee6-55ff-80c7-5d19-b5b3eebb86be"
      },
      "outputs": [],
      "source": [
        "print('\\nNumber of columns which have any NaN:',df.isnull().any().sum(),'/',len(df.columns))\n",
        "print('\\nNumber of rows which have any NaN:',df.isnull().any(axis=1).sum(),'/',len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8d26591d-0277-1448-f086-a84284d1cdb8"
      },
      "source": [
        "Now we are going to plot a bar chart representing the % of missing data for each of the 51 features which have lacking observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "870f2fa1-3d42-cd77-3d55-5b9968852614"
      },
      "outputs": [],
      "source": [
        "# Get the number of NaN's for each column, discarding those with zero NaN's\n",
        "ranking = df.loc[:,df.isnull().any()].isnull().sum().sort_values()\n",
        "# Turn into %\n",
        "x = ranking.values/len(df)\n",
        "\n",
        "# Plot bar chart\n",
        "index = np.arange(len(ranking))\n",
        "plt.bar(index, x)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('% NaN observations')\n",
        "plt.title('% of null data points for each feature')\n",
        "plt.show()\n",
        "\n",
        "print('Features:',ranking.index.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c9359429-296a-ab3c-1ab4-bcba1035b358"
      },
      "source": [
        "As we can see in the graph, for some features there are almost 45% of missing data. \n",
        "\n",
        "Since the intersection of the missing data between features may not be empty, it is important to see how many data we actually lose when we use more than one feature. It is important to note that there are 51! (1.55 e+66) different ways to sequentially accumulate the features wich have missing observations in order to see how many data is missing as a function of the features selected. Here we will use a very simple heuristic, wich may be suboptimal but I guess that is near-optimal: add features to the list sorted in ascending order by number of missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "abaf84b5-546f-3f39-1604-2c01e1829ceb"
      },
      "outputs": [],
      "source": [
        "rank_features = ranking.index\n",
        "accum_nulls = list()\n",
        "accum_features = list()\n",
        "for i, feature in enumerate(rank_features):\n",
        "    # On each step, we add a new feature to the list\n",
        "    accum_features.append(feature)\n",
        "    # We calculate the number of rows with NaN's for that list of features\n",
        "    accum_nulls.append(len(df)-len(df[accum_features].dropna()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1e19261e-7c59-33ca-a6c1-358e2051f4dc"
      },
      "outputs": [],
      "source": [
        "# Calculate the % of missing data\n",
        "x = np.array(accum_nulls)/len(df)\n",
        "\n",
        "# Plot\n",
        "index = np.arange(len(x))\n",
        "plt.bar(index, x)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('% NaN accumulated observations')\n",
        "plt.title('% of null data points accumulated until each feature')\n",
        "plt.show()\n",
        "\n",
        "print('Features:',accum_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b421ac58-6297-2210-1d10-5921a83f2fc8"
      },
      "source": [
        "In this graph we see that, as we add features which contain missing data, we start to gradually lose more and more data points, until we lose 80% of data if we use all the features. There seems to be groups of features, wich have full intersection. This may be due to a changing data collection procedure, in which new features were added after starting the database. This is a very typical problem in this type of data. Let's check if I'm right:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d1893bf5-f43a-8851-13ca-0fc070c27650"
      },
      "outputs": [],
      "source": [
        "y = df.groupby('timestamp').apply(lambda f: f.isnull().sum().sum()).values\n",
        "\n",
        "plt.plot(y)\n",
        "plt.xlabel('timestamp')\n",
        "plt.ylabel('Number of NaNs')\n",
        "plt.title('Missing data structure over timestamp')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "32ad6ea5-c635-9f13-cea2-dacf47ad1dab"
      },
      "source": [
        "Unexpected result. If my initial guess were right (that new features were added to the data over time), we should see that the number of missing data decreases. Maybe there are differences in the number of data points for each timestamp, so let's normalize the series dividing by the number of observations on each timestamp:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4aef458a-64df-3ccf-7ab5-66a361ea5ffe"
      },
      "outputs": [],
      "source": [
        "y = df.groupby('timestamp').apply(lambda x: x.isnull().sum().sum()/len(x)).values\n",
        "\n",
        "plt.plot(y)\n",
        "plt.xlabel('timestamp')\n",
        "plt.ylabel('Number of NaNs')\n",
        "plt.title('Missing data structure over timestamp normalized')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b550c05e-9af7-4f18-38f7-615d5417e945"
      },
      "source": [
        "This plot looks better, but still doesn't shows the pattern I was expecting. There is not a clear time-dependent structure on the missing data.\n",
        "\n",
        "\n",
        "### Correlations between features and target\n",
        "\n",
        "Next we are going to calculate the correlations between all the features and the target variable in order to see how difficult is the problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "06742db6-2448-58e7-edab-6213b7c6d7ba"
      },
      "outputs": [],
      "source": [
        "# Get the list of features\n",
        "features = df.iloc[:,2:-1].select_dtypes(exclude=['O']).columns.tolist()\n",
        "# Get the target name\n",
        "target = df.iloc[:,-1].name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3bf04327-a170-4d17-2dd4-3404e6f5faa3"
      },
      "outputs": [],
      "source": [
        "correlations = dict()\n",
        "for feat in features:\n",
        "    df_temp = df[[feat,target]]\n",
        "    df_temp = df_temp.dropna()\n",
        "    x1 = df_temp[feat].values\n",
        "    x2 = df_temp[target].values\n",
        "    key = feat + ' vs ' + target\n",
        "    correlations[key] = pearsonr(x1,x2)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7384896d-bf84-3b20-ee17-7d5d1660b30c"
      },
      "outputs": [],
      "source": [
        "df_corrs = pd.DataFrame(correlations, index=['R']).T\n",
        "df_corrs.loc[df_corrs['R'].abs().sort_values(ascending=False).index].iloc[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f5427318-90ec-1f1b-a399-4777e33ff8a4"
      },
      "source": [
        "It seems that number of rooms and square area are the best regressors (for the moment). Let's plot them jointly with the target variable to see how good is that correlation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e0f69701-7724-de9b-8ad3-f085e120dbe1"
      },
      "outputs": [],
      "source": [
        "y = df.loc[:,['num_room','full_sq',target]].dropna().sort_values(target,ascending=True).values\n",
        "x = np.arange(y.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "46bbce9c-0a97-13bc-591d-958169e96580"
      },
      "outputs": [],
      "source": [
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(x,y[:,0])\n",
        "plt.title('num_room & full_sq vs price')\n",
        "plt.ylabel('num_room')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(x,y[:,1])\n",
        "plt.ylabel('full_sq')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(x,y[:,2],'r')\n",
        "plt.ylabel('price')\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "565a3709-1f26-68e2-6ff4-2bb4c09f97a8"
      },
      "source": [
        "The dataset has some price outliers. It might be interesting to remove them and see if the correlations improve. Let's plot the distribution of the price:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cea116b2-6e13-b08a-cc6d-e6108f758fab"
      },
      "outputs": [],
      "source": [
        "x = df[target].values.astype('int')\n",
        "sns.distplot(x)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "628df7f2-aaae-f94a-682c-8b83d14a7b2a"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "Very interesting problem. None of the house features can explain the price fully, so the features engineering will play an important role in this competition.\n",
        "\n",
        "I will post a new analysis on macro data soon.\n",
        "\n",
        "Thanks for reading, this is my first kernel, I hope you enjoyed it."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}