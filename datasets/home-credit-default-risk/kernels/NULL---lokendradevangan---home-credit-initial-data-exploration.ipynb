{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as plt\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9c1be8386373e5f22b5dcb41bb185c09627dd08"},"cell_type":"code","source":"##Initial data understanding \ndataset=pd.read_csv(\"../input/application_train.csv\")\ntest=pd.read_csv(\"../input/application_test.csv\")\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"##summary statistics\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90fc7fb1fcf15a571a53fd1ccfef07bb58378917"},"cell_type":"code","source":"# Number of each type of column\ndataset.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"999dfdd04815ee0efb3132d8f64523a0e0ae51d1"},"cell_type":"code","source":"#Function\n#Getting the percentage of missing values by column in the dataframe dataset \n\ndef export(Path,Object1,Object2,Sheetname1,Sheetname2):\n    \n    # Create a Pandas Excel writer using XlsxWriter\n    writer = pd.ExcelWriter(Path)\n    # Convert the dataframe to an XlsxWriter Excel object\n    Object1.to_excel(writer,Sheetname1)                        #DataFrame.to_excel(excel_writer, sheet_name='Sheet1') : Write DataFrame to an excel sheet\n    Object2.to_excel(writer,Sheetname2)\n    # Close the Pandas Excel writer and output the Excel file\n    writer.save()\ndata_f=dataset.select_dtypes(include=['float64'])              #Dataframe containing only float variables\ndata_o=dataset.select_dtypes(include=['object','category'])    #Dataframe containing object and categorical variables\nsummary_f=data_f.describe() \nsummary_o=data_o.describe() \nsummary_f=summary_f.transpose()\nsummary_o=summary_o.transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30c1b513c5e3d3a18fd6a4bdec57b7809834ffdf","collapsed":true},"cell_type":"code","source":"# of records\ntotal_rows = dataset[\"SK_ID_CURR\"].count()\n#Getting the missing percentage \nsummary_o[\"MissingCount\"] = total_rows-summary_o[\"count\"] \nsummary_o[\"MissingPerc\"] = summary_o[\"MissingCount\"]/total_rows\nsummary_f[\"MissingCount\"] = total_rows-summary_f[\"count\"] \nsummary_f[\"MissingPerc\"] = summary_f[\"MissingCount\"]/total_rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78eaec70fe29cbe8eb68419b4275c722b4420045"},"cell_type":"code","source":"#Missig in some field may be actually zero. That is a judgement we have to make\nsummary_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8411a4662355858aa917af8a0d67aa7cf0fc7464"},"cell_type":"code","source":"#Missing percentage in categrical field\nsummary_o","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc46bce365cb89987987d0b820ae0c72955bb7cc"},"cell_type":"code","source":"#listing the count of values in each category\ndataset.select_dtypes(include=['object']).apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d9286ce0ab07b06e3ad02d864568c5fac653364","collapsed":true},"cell_type":"markdown","source":"Exploring unique values in categorical fields having more than 5 values. Values can be combiined to reduce the unique values. 8 variables have more than 5 distinct values. We will be  exploring these variables now\n"},{"metadata":{"trusted":true,"_uuid":"0df7dd7ee8919a900831eaf865e76ba4e63af72b"},"cell_type":"code","source":"# can we combine in less number of groups\ndataset.groupby(['NAME_TYPE_SUITE']).SK_ID_CURR.count()\ndataset.groupby(['NAME_TYPE_SUITE']).TARGET.mean() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ef421c208c2fc799af2bf3b32bd8e57ba2e4174","collapsed":true},"cell_type":"code","source":"#reducing the unique values in occupation by grouping by skill level. This grouping can differ based on more information about each occupation\ndataset['NAME_TYPE_SUITE'].replace({'Children':'Family',\n                                    'Group of people':'Other',\n                                    'Other_A':'Other',\n                                    'Other_B':'Other',\n                                    'Spouse, partner':'Family'},inplace=True)\n\ntest['NAME_TYPE_SUITE'].replace({'Children':'Family',\n                                    'Group of people':'Other',\n                                    'Other_A':'Other',\n                                    'Other_B':'Other',\n                                    'Spouse, partner':'Family'},inplace=True)\ndataset.groupby(['OCCUPATION_TYPE']).SK_ID_CURR.count()\n\ndataset.groupby(['NAME_EDUCATION_TYPE']).SK_ID_CURR.count()\ndataset.groupby(['NAME_EDUCATION_TYPE']).TARGET.mean() \ndataset['NAME_EDUCATION_TYPE'].replace({'Academic degree':'Higher education '},inplace=True)\ntest['NAME_EDUCATION_TYPE'].replace({'Academic degree':'Higher education '},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02063048aad4733b9357ca6a2e624b14992ec831"},"cell_type":"code","source":"dataset.groupby(['WEEKDAY_APPR_PROCESS_START']).SK_ID_CURR.count() \n#does not have any interesting information. Lets check mean of target value\ndataset.groupby(['WEEKDAY_APPR_PROCESS_START']).TARGET.mean() \n#even target does not show any pattern","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac04ae4e637de97088edbb7bf3693638c5e6a8ca"},"cell_type":"code","source":"dataset.groupby(['OCCUPATION_TYPE']).SK_ID_CURR.count()\n#group low skilled, medium skilled and high skillled profession to reduce the unique values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a1f090c4f73d6bd55b7c5a0616aa2bdd973acb9"},"cell_type":"code","source":"#reducing the unique values in occupation by grouping by skill level. This grouping can differ based on more information about each occupation\ndataset['OCCUPATION_TYPE'].replace({'High skill tech staff':'High_Skill',\n                                    'Managers':'High_Skill',\n                                    'Accountants':'High_Med_Skill',\n                                    'HR staff':'High_Med_Skill',\n                                    'Core staff':'Med_Skill',\n                                   'Cooking staff':'Med_Skill',\n                                    'Realty agents':'Med_Skill',\n                                    'Sales staff':'Med_Skill',\n                                    'IT staff':'High_Med_Skill',\n                                    'Medicine staff':'High_Med_Skill',\n                                    'Secretaries':'Med_Skill',\n                                    'Security staff':'Med_Skill',\n                                    'Cleaning staff':'Low_Skill',\n                                      'Laborers':'Low_Skill',\n                                      'Low-skill Laborers':'Low_Skill',\n                                      'Cleaning staff':'Low_Skill',\n                                    'Waiters/barmen staff':'Low_Skill',\n                                    'Private service staff':'Low_Skill',\n                                    'Drivers':'Med_Skill'\n                                   },inplace=True)\ntest['OCCUPATION_TYPE'].replace({'High skill tech staff':'High_Skill',\n                                    'Managers':'High_Skill',\n                                    'Accountants':'High_Med_Skill',\n                                    'HR staff':'High_Med_Skill',\n                                    'Core staff':'Med_Skill',\n                                   'Cooking staff':'Med_Skill',\n                                    'Realty agents':'Med_Skill',\n                                    'Sales staff':'Med_Skill',\n                                    'IT staff':'High_Med_Skill',\n                                    'Medicine staff':'High_Med_Skill',\n                                    'Secretaries':'Med_Skill',\n                                    'Security staff':'Med_Skill',\n                                    'Cleaning staff':'Low_Skill',\n                                      'Laborers':'Low_Skill',\n                                      'Low-skill Laborers':'Low_Skill',\n                                      'Cleaning staff':'Low_Skill',\n                                    'Waiters/barmen staff':'Low_Skill',\n                                    'Private service staff':'Low_Skill',\n                                    'Drivers':'Med_Skill'\n                                   },inplace=True)\ndataset.groupby(['OCCUPATION_TYPE']).SK_ID_CURR.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b97585ee7cf9b8d5b5fb6602e0ce8126f0ec16c"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#fig, ax = plt.subplots(3, 1, figsize = (15, 10))\nsns.countplot(dataset.CODE_GENDER)\nplt.show()\nsns.countplot(dataset.NAME_INCOME_TYPE)\nplt.show()\nsns.countplot(dataset.NAME_FAMILY_STATUS)\nplt.show()\nsns.countplot(dataset.NAME_HOUSING_TYPE)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a71610945fccc537c0c244a68965f60e0ee3aaaa"},"cell_type":"markdown","source":"Interesting to note that Female customers are much more than Male.\nMost customers do not have higher education.\nCan we regroup organization_type ? Lets check."},{"metadata":{"trusted":true,"_uuid":"30bbea2a2246a7e0d5166cfbf59bc4080e1a5005"},"cell_type":"code","source":"dataset.groupby(['NAME_INCOME_TYPE']).TARGET.mean()\ndataset.groupby(['NAME_INCOME_TYPE']).SK_ID_CURR.count() \n\n# Group the data frame by NAME_INCOME_TYPE  extract a number of stats from each group\ndataset.groupby(['NAME_INCOME_TYPE']).agg({'TARGET': [\"mean\", \"count\"], 'AMT_GOODS_PRICE': [\"mean\", \"median\"]   \n                                    })  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54316388402ab6024a5bee9fc881f742381e3e86"},"cell_type":"markdown","source":"Businessman, Maternity leave, students and unemployed are very less. Grouping these into one except Unemployed as it has very high target rate though it can be misleading"},{"metadata":{"trusted":true,"_uuid":"3b25c93275672e3e4283a6d50372b9e520ac4368"},"cell_type":"code","source":"#grouping\ndataset['NAME_INCOME_TYPE'].replace({'Businessman':'Other','Student':'Other','Maternity leave':'Other'},inplace=True)\ntest['NAME_INCOME_TYPE'].replace({'Businessman':'Other','Student':'Other','Maternity leave':'Other'},inplace=True)\n#exploring Housing type\ndataset.groupby(['NAME_HOUSING_TYPE']).agg({'TARGET': [\"mean\", \"count\"], 'AMT_GOODS_PRICE': [\"mean\", \"median\"]   \n                                    }) \n\ndataset.groupby(['NAME_FAMILY_STATUS']).agg({'TARGET': [\"mean\", \"count\"], 'AMT_CREDIT': [\"mean\", \"median\"] ,'AMT_INCOME_TOTAL': [\"mean\", \"median\"]   \n                                    }) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd3a5ba78513eb680167d735adc4169c4f3d77f8"},"cell_type":"markdown","source":"Significant difference is observed for Amount credit but not in income\nLet us see unique values in organization_type\n"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"5c084c9ac7a58e70505cc18ae31ad46ac98db665"},"cell_type":"code","source":"dataset.groupby(['ORGANIZATION_TYPE']).agg({'TARGET': [\"mean\", \"count\"], 'AMT_CREDIT': [\"mean\", \"median\"] ,'AMT_INCOME_TOTAL': [\"mean\", \"median\"]   \n                                    }) \n#lets not do anything for now. Later, we will be encoding all the categorical variables","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87d2ea03306f91eb4eac8bb846a0e4988ffbd1e3"},"cell_type":"markdown","source":"**Label Encoding and One-Hot Encoding**\nLet's implement the policy described above: for any categorical variable (dtype == object) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding.\n\nFor label encoding, we use the Scikit-Learn LabelEncoder and for one-hot encoding, the pandas get_dummies(df) function.\nreference https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0e3be4f0e05d7518784a61afc23c4182218c8b1c"},"cell_type":"code","source":"# Create a label encoder object\napp_train=dataset\napp_test=test\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            #app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"be45331105c512d23824fc088e12a378d71f8938"},"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Training Features shape: ', app_test.shape)\n#print('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecdd3bd7a119af353079963fec8d646a4372fb8c"},"cell_type":"code","source":"#distribution of the AMT_GOODS_PRICE\n#histogram of price by condition and brand\n# Histogram\n# bins = number of bar in figure\ndataset.AMT_GOODS_PRICE.plot(kind = 'hist',bins = 25,figsize = (15,15))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"061de3c62bb203498bc8e1ff7537b38280f241e1"},"cell_type":"markdown","source":"It is observed that there are outliers in the amt_goods_price. There are less than 1000 records with values more than 2000000. We will later use log transform to treat the outlier values"},{"metadata":{"trusted":true,"_uuid":"15d493ab90ac6ae7727060f4a055cf8bd013b220"},"cell_type":"code","source":"#distribution of other variables \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfig, ax = plt.subplots(1, 2, figsize = (15, 10))\nsns.boxplot(dataset.AMT_INCOME_TOTAL, showfliers = False, ax = ax[0])\ndataset.AMT_CREDIT.plot(kind = 'hist',bins = 25,figsize = (15,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d6773e962c336cc291cd53d265204a83b0336db"},"cell_type":"code","source":"#distribution of other variables \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfig, ax = plt.subplots(1, 2, figsize = (15, 10))\nsns.boxplot(dataset.AMT_ANNUITY, showfliers = False, ax = ax[0])\ndataset.DAYS_EMPLOYED.plot(kind = 'hist',bins = 15,figsize = (15,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a795e3136e8322a63d75b8a317c732a6dd707075"},"cell_type":"code","source":"#distribution of the DAYS_EMPLOYED has extreme values. Does noot look correct\napp_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c832079bcf4c80fb132199510754517f56ce3d71"},"cell_type":"markdown","source":"DAYS_EMPLOYED has anomally in the data. It must be capped."},{"metadata":{"trusted":true,"_uuid":"278fa9965595b60a8713132395f969cad8635330"},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_train.fillna(dataset.median(),inplace = True)\n\napp_test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\napp_test.fillna(dataset.median(),inplace = True)\napp_train.DAYS_EMPLOYED.plot(kind = 'hist',bins = 25,figsize = (15,15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf298de6c2b82aca547458c65dd1acc9292d620f"},"cell_type":"code","source":"#how is age variable\n(dataset['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57936d3da92defa7490d628bcd1c74f13d1afd12"},"cell_type":"markdown","source":"These variables have missing values less than 15% of the times and these values may be actually zero hence, we can replace with zero values \n\nOBS_30_CNT_SOCIAL_CIRCLE,\nDEF_30_CNT_SOCIAL_CIRCLE,\nOBS_60_CNT_SOCIAL_CIRCLE,\nDEF_60_CNT_SOCIAL_CIRCLE,\nDAYS_LAST_PHONE_CHANGE,\nAMT_REQ_CREDIT_BUREAU_HOUR,\nAMT_REQ_CREDIT_BUREAU_DAY,\nAMT_REQ_CREDIT_BUREAU_WEEK,\nAMT_REQ_CREDIT_BUREAU_MON,\nAMT_REQ_CREDIT_BUREAU_QRT,\nAMT_REQ_CREDIT_BUREAU_YEAR"},{"metadata":{"trusted":true,"_uuid":"d4f4f9575fe5fb0f9de158a0db8486838c9832d0"},"cell_type":"code","source":"#replace all  NaN in the var_list with zero\nVar_List=('OBS_30_CNT_SOCIAL_CIRCLE','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE',\n        'DAYS_LAST_PHONE_CHANGE','AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK',\n         'AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR')\ndef missing_val_replace(data,Var_List):\n    for col in data:\n        for i in Var_List:\n            if col==i:\n                data[col].fillna(0)\n                print (col)\n    return data\ndataset=missing_val_replace(dataset,Var_List) \n#replace all other NaN with median values\ndataset=dataset.fillna(dataset.median)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0da72c08b9f7be4b7dee94369a17cf0db945ae9"},"cell_type":"code","source":"#check if missing values got replaced \n#describe \ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69ee01c5d33e27a0c366474492cfd585aa9fc063"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}