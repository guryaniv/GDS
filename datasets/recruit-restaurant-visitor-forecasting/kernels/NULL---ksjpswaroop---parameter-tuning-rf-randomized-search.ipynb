{"cells":[{"metadata":{"_uuid":"5469cf87dc7fba61d827912f421e9c17447467d4"},"cell_type":"markdown","source":"# Abstract\nGrid search and manual search are the most widely used strategies for hyper-parameter optimiza tion. This kernelshows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Compared with a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con- figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven.  this work shows that random search is a natural base- line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms."},{"metadata":{"_cell_guid":"032c2608-54da-4e8b-a19b-93e51600d03c","_uuid":"a58c3f19d00c154bea0556b29dc38c8254db24c2","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"13c6d4a8-37b4-4f0d-9c09-04805ec58690","_uuid":"619372f7448401c01c225011d905098217edf6ca","trusted":false,"collapsed":true},"cell_type":"code","source":"import glob, re\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nfrom datetime import datetime\nfrom xgboost import XGBRegressor\n\nfrom keras.layers import Embedding, Input, Dense\nfrom keras.models import Model\nimport keras\nimport keras.backend as K\n\nimport matplotlib.pyplot as plt\nfrom sklearn.externals import joblib","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1231e6d7-ce8e-4a35-9f7e-cd984c67c945","_uuid":"ffaa28a4de7ad2f24e2c29dcf08d2bd18fbc7f46","trusted":false,"collapsed":true},"cell_type":"code","source":"data = {\n    'tra': pd.read_csv('../input/air_visit_data.csv'),\n    'as': pd.read_csv('../input/air_store_info.csv'),\n    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n    'ar': pd.read_csv('../input/air_reserve.csv'),\n    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n    'id': pd.read_csv('../input/store_id_relation.csv'),\n    'tes': pd.read_csv('../input/sample_submission.csv'),\n    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n    }\n\ndata['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\nfor df in ['ar','hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n    data[df]['visit_dow'] = data[df]['visit_datetime'].dt.dayofweek\n    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n    # Exclude same-week reservations - from aharless kernel\n    data[df] = data[df][data[df]['reserve_datetime_diff'] > data[df]['visit_dow']]\n    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n\ndata['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\ndata['tra']['year'] = data['tra']['visit_date'].dt.year\ndata['tra']['month'] = data['tra']['visit_date'].dt.month\ndata['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n\ndata['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n\nunique_stores = data['tes']['air_store_id'].unique()\nstores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n\n#sure it can be compressed...\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n\nstores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n# NEW FEATURES FROM Georgii Vyshnia\nstores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\nstores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\nlbl = preprocessing.LabelEncoder()\nfor i in range(10):\n    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\nstores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\nstores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ntrain = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n\ntrain = pd.merge(train, stores, how='inner', on=['air_store_id','dow']) \ntest = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n\nfor df in ['ar','hr']:\n    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n\ntrain['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n\ntrain['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\ntrain['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\ntrain['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n\ntest['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\ntest['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\ntest['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n\n# NEW FEATURES FROM JMBULL\ntrain['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntrain['var_max_lat'] = train['latitude'].max() - train['latitude']\ntrain['var_max_long'] = train['longitude'].max() - train['longitude']\ntest['var_max_lat'] = test['latitude'].max() - test['latitude']\ntest['var_max_long'] = test['longitude'].max() - test['longitude']\n\n# NEW FEATURES FROM Georgii Vyshnia\ntrain['lon_plus_lat'] = train['longitude'] + train['latitude'] \ntest['lon_plus_lat'] = test['longitude'] + test['latitude']\n\nlbl = preprocessing.LabelEncoder()\ntrain['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\ntest['air_store_id2'] = lbl.transform(test['air_store_id'])\n\ncol = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bec06905-e88c-4f0e-9b79-c2aa30480c6f","collapsed":true,"_uuid":"0267aad049dd1e56b2721c1a0251ed5affd04089","trusted":false},"cell_type":"code","source":"def RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e01ddd7d-a4f5-4158-8b86-a708ec43697a","_uuid":"b4de4420aca3558a8be00e08dc58e22d8bb93e6d","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Start of Data Load\")\nvalue_col = ['holiday_flg','min_visitors','mean_visitors','median_visitors','max_visitors','count_observations',\n'rs1_x','rv1_x','rs2_x','rv2_x','rs1_y','rv1_y','rs2_y','rv2_y','total_reserv_sum','total_reserv_mean',\n'total_reserv_dt_diff_mean','date_int','var_max_lat','var_max_long','lon_plus_lat']\n\nnn_col = value_col + ['dow', 'year', 'month', 'air_store_id2', 'air_area_name', 'air_genre_name',\n'air_area_name0', 'air_area_name1', 'air_area_name2', 'air_area_name3', 'air_area_name4',\n'air_area_name5', 'air_area_name6', 'air_genre_name0', 'air_genre_name1',\n'air_genre_name2', 'air_genre_name3', 'air_genre_name4']\n\n\nX = train.copy()\nX_test = test[nn_col].copy()\n\nvalue_scaler = preprocessing.MinMaxScaler()\nfor vcol in value_col:\n    X[vcol] = value_scaler.fit_transform(X[vcol].values.astype(np.float64).reshape(-1, 1))\n    X_test[vcol] = value_scaler.transform(X_test[vcol].values.astype(np.float64).reshape(-1, 1))\n\nX_train = list(X[nn_col].T.as_matrix())\nY_train = np.log1p(X['visitors']).values\nnn_train = [X_train, Y_train]\nnn_test = [list(X_test[nn_col].T.as_matrix())]\nprint(\"Train and test data prepared\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12cf0a93-3cd9-4971-9765-f43c67a59e98","collapsed":true,"_uuid":"cb05a1bb76fd181beb26a1b91ad189e14491bbda","trusted":false},"cell_type":"code","source":"#***************************************Random Forest\nmodel2 = ensemble.RandomForestRegressor(n_estimators=13, random_state=3, max_depth=18,\n                                        min_weight_fraction_leaf=0.0002)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb1a77cd-7280-4196-a866-7f51b1332109","_uuid":"9967713bf6bddc6202a422d6852838fa523c75ef","collapsed":true,"trusted":false},"cell_type":"code","source":"# *********************** Training and Validation Data *************************#\ntrain_X = train[col]\ntrain_X = train_X[train_X['year'] == 2016]\ntrain_y = train[train['year'] == 2016]\ntest_X =  train[col]\ntest_X = test_X[test_X['year'] == 2017]\ntest_y = train[train['year'] == 2017]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae660114-c5cd-48f1-a439-02f3a8b6164a","collapsed":true,"_uuid":"1fe7f0f89dfbaba950a79bc37f768060a663d71e","trusted":false},"cell_type":"code","source":"# *********************** Training and Validation Data *************************#\n#**********************************************************\nmodel2.fit(train_X, np.log1p(train_y['visitors'].values))\nprint(\"Model2 trained\")\npreds2 = model2.predict(test_X)\nprint('RMSE RandomForestRegressor: ', RMSLE(np.log1p(test_y['visitors'].values), preds2))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e79f47a-a853-43c6-9106-89ea22cac7b9","_uuid":"0a6c3951aa9b99b5827a2dd1254529bc160c0ae0"},"cell_type":"markdown","source":"**Points to remember**\n- We have used 2016 data for training and validating on 2017 data\n- We got a RMSE of 0.548500079658"},{"metadata":{"_cell_guid":"23b0afe5-6adc-4881-9198-0d9bc0f92e42","_uuid":"71187c3a716f0b859ca28cdab054385ad518ea6c","collapsed":true,"trusted":false},"cell_type":"code","source":"# Commented after running on a local computer\n# takes a longer time\n'''\nfrom sklearn.model_selection import GridSearchCV\n\n# parameters for GridSearchCV\nparam_grid2 = {\"n_estimators\": [10, 18, 22],\n              \"max_depth\": [3, 5],\n              \"min_samples_split\": [15, 20],\n              \"min_samples_leaf\": [5, 10, 20],\n              \"max_leaf_nodes\": [20, 40],\n              \"min_weight_fraction_leaf\": [0.1]}\ngrid_search = GridSearchCV(model2, param_grid=param_grid2)\ngrid_search.fit(train_X, np.log1p(train_y['visitors'].values))\n'''\nfrom operator import itemgetter","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6c238016-75b5-4278-a022-8d16629b0a6e","collapsed":true,"_uuid":"58d05d463c2f3aeab4d066deb9dc73b89c17ad77","trusted":false},"cell_type":"code","source":"# Utility function to report best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3709a71f-3a73-4c93-934c-9a525e8fc407","_uuid":"66f3b5e852698876d6baa4516cb9dc349fb3a4cf"},"cell_type":"markdown","source":"**Output from the local computer for the report**\nModel with rank: 1  \nMean validation score: 0.5483)  \nParameters: {'n_estimators': 18, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.1, 'max_depth': 3, 'max_leaf_nodes': 20, 'min_samples_split': 15}  \n\nModel with rank: 2  \nMean validation score: 0.5483)  \nParameters: {'n_estimators': 18, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.1, 'max_depth': 3, 'max_leaf_nodes': 20, 'min_samples_split': 20}  \n\nModel with rank: 3  \nMean validation score: 0.5483)  \nParameters: {'n_estimators': 18, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'max_depth': 3, 'max_leaf_nodes': 20, 'min_samples_split': 15}  \n\nModel with rank: 4  \nMean validation score: 0.5483)  \nParameters: {'n_estimators': 18, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'max_depth': 3, 'max_leaf_nodes': 20, 'min_samples_split': 20}  "},{"metadata":{"_cell_guid":"4db82ef8-b5de-4a66-a493-b77db37f7c7f","collapsed":true,"_uuid":"0299620956620697d6d0b943e4be1cd4069624c2","trusted":false},"cell_type":"code","source":"#Change parameters and test if it performs better than the prior model\n#***************************************Random Forest\nmodel2 = ensemble.RandomForestRegressor(n_estimators=18, random_state=3, max_depth=3,\n                                        min_weight_fraction_leaf=0.1,max_leaf_nodes = 20,\n                                       min_samples_split = 20)\n#**********************************************************\nmodel2.fit(train_X, np.log1p(train_y['visitors'].values))\nprint(\"Model2 trained\")\npreds2 = model2.predict(test_X)\nprint('RMSE RandomForestRegressor: ', RMSLE(np.log1p(test_y['visitors'].values), preds2))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f126001a-5a51-4ec1-b4fa-e72785d51b5e","_uuid":"bb601b149dde533a590d9b8d7450a371936de58e"},"cell_type":"markdown","source":"**RMSE RandomForestRegressor:  0.529405212091**\n- clearly it performs better than the prior model but needs a lot of tuning\n**Happy Tuning the models  -  you can use the gridCV for tuning parameters for XGB, GBM etc...**"},{"metadata":{"_cell_guid":"d0a4fd9e-37bf-409f-b30f-3ca714e3f81e","collapsed":true,"_uuid":"37becb7c9d63c9865f570af8da43728a771b7fe7"},"cell_type":"markdown","source":"**Randomized Search**\n- Search for the Parameters consists of following:\n    1. an estimator - in our case it is RandomForestRegressor\n    2. a parameter space -  this the grid we have passed onto the GridSearchCV\n    3. a method for searching or sampling candidates  - we have seen GridsearchCV and other one is RandomizedSearchCV\n    4. a cross-validation scheme\n    5. a scoring function -  evaluates the parameters\n- GridSearchCV is an exhaustive search or say it is a brute force technique.\n- RandomizedSearchCV as the name mentions does a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. Which is far more efficient in searching the parameter values for fine tuning\n# Randomized Search"},{"metadata":{"_cell_guid":"2252111a-91f7-4ca9-ad98-2ba7d512d369","_uuid":"6cf863eef00da7bed4a41f6d828099686c3c5306","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n# parameters for GridSearchCV\n# specify parameters and distributions to sample from\nparam_dist = {\"max_depth\": [3, 5],\n              \"max_features\": sp_randint(1, 11),\n              \"min_samples_split\": sp_randint(2, 11),\n              \"min_samples_leaf\": sp_randint(1, 11),\n              \"bootstrap\": [True, False]\n             }\n# run randomized search\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(model2, param_distributions=param_dist,\n                                   n_iter=n_iter_search)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ecf515e2-90ae-4c81-85e2-17ce8ea51752","_uuid":"2a89286872cc38d139d8f66694b664c3b42dc198","collapsed":true,"trusted":false},"cell_type":"code","source":"random_search.fit(train_X, np.log1p(train_y['visitors'].values))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"199a9b91-bced-4568-8f8d-b4fb2795da38","_uuid":"a85fe88f9946130e98256355127129fdd3514468","trusted":false,"collapsed":true},"cell_type":"code","source":"report(random_search.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ced894a8-b1e5-4ea9-9b5e-a8fd326c879a","_uuid":"397483bf2a90247f4e887b72289da395a81076a5"},"cell_type":"markdown","source":"It is evident that Randomized search has searched quite efficiently and in less time!!"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}