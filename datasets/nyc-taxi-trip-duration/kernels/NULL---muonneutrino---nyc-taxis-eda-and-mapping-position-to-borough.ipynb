{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}}, "cells": [{"metadata": {"_uuid": "e4fc37e9dc6dc5e32eca4984ed7047f3ab0d4a25", "_cell_guid": "70f75dc7-5fa3-4616-ba88-02f52ae43678"}, "outputs": [], "cell_type": "markdown", "source": "# Introduction\n\nIn this notebook, I will use some census tract information to get a mapping between the pickup \nand dropoff locations and associated borough/county. The geography around the New York City area\nmeans that there are generally large jumps in travel times when going between boroughs and between New York and New Jersey. Features such as the George Washington Bridge, the Triboro Bridge, the Queens Midtown Tunnel, Holland and Lincoln Tunnels provide chokepoints where traffic jams are very common. Including this information may help improve models without needing to train something like a neural net to find where these common traffic problem areas are.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "667cea764176d6494dadfa3d068cdb4f5f4d7056", "collapsed": true, "_cell_guid": "38b48cd0-1cc0-4c03-b214-fb875dd89bb0"}, "execution_count": null, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntrain = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv',index_col=0)\ntest = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv',index_col=0)", "outputs": []}, {"metadata": {"_uuid": "ee1a38ce4a7e09bb789ebe1210142cef492a6f49", "_cell_guid": "9c49ffe0-bc44-43aa-b9f6-041001e91e55"}, "outputs": [], "cell_type": "markdown", "source": "## Pickup Locations\n\nTo display the locations as a histogram, we actually want a logarithmic color scale because the data is heavily weighted toward taxi trips in Manhattan below about 96th St. The densest spot for pickups is in Midtown.\n\nThere is also a decent density of pickups along the waterfronts across from Manhattan.\n\nThe largest clusters outside Manhattan seem to be LaGuardia and JFK Airports and the highways leading up to the airports.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "3712146742ce6db094230d17398210dd8c1c33c6", "_cell_guid": "a724881f-adb8-4e28-beab-da3847cb1198"}, "execution_count": null, "cell_type": "code", "source": "latmin = 40.48\nlonmin = -74.28\nlatmax = 40.93\nlonmax = -73.65\nratio = np.cos(40.7 * np.pi/180) * (lonmax-lonmin) /(latmax-latmin)\nfrom matplotlib.colors import LogNorm\nfig = plt.figure(1, figsize=(8,ratio*8) )\nhist = plt.hist2d(train.pickup_longitude,train.pickup_latitude,bins=199,range=[[lonmin,lonmax],[latmin,latmax]],norm=LogNorm())\nplt.xlabel('Longitude [degrees]')\nplt.ylabel('Latitude [degrees]')\nplt.title('Pickup Locations')\nplt.colorbar(label='Number')\nplt.show()", "outputs": []}, {"metadata": {"_uuid": "c02a7d68051c73d4b6a50ae0fb59db2ec380b78a", "_cell_guid": "d4b4ff5b-f715-4586-82f6-87b6a4f1cdab"}, "outputs": [], "cell_type": "markdown", "source": "## Dropoff Locations\n\nThe dropoff locations appear to do a better job covering the whole city but are still very Manhattan-centric.\nWe also see a number of dropoffs at Newark Airport (close to (-74.2,40.7)) and in Jersey City and Hoboken.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "670c79f52442b28830ca639c98108d55c278a197", "_cell_guid": "20b2fe87-ee66-4a8b-908d-169e3ea7268c"}, "execution_count": null, "cell_type": "code", "source": "fig = plt.figure(1, figsize=(8,ratio*8) )\nhist = plt.hist2d(train.dropoff_longitude,train.dropoff_latitude,bins=199,range=[[lonmin,lonmax],[latmin,latmax]],norm=LogNorm())\nplt.xlabel('Longitude [degrees]')\nplt.ylabel('Latitude [degrees]')\nplt.title('Dropoff Locations')\nplt.colorbar(label='Number')\nplt.show()", "outputs": []}, {"metadata": {"_uuid": "06f0e1099e9eec4091d505d900bdffd28889b5d8", "_cell_guid": "311b6342-3f47-41d2-af24-41eee523fdee"}, "outputs": [], "cell_type": "markdown", "source": "## Getting Census Tract Information\n\nI recently uploaded a dataset with some census demographic and economic information\nand also some data to map from coordinates to census tract.\n\nFor now, I'll just add in the census tract information. Tracts are a good way to package the data\nsince they often follow political and physical boundaries. In New York, this will allow us\nto figure out things such as which trips require crossing a river.\nThese types physical boundaries will potentially cause significant increases in trip time.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "e1400c27f879396f61356519398e7155aa622f3f", "collapsed": true, "_cell_guid": "56c6a647-afa6-4041-8b46-f92b02bc9185"}, "execution_count": null, "cell_type": "code", "source": "def get_census_data():\n    blocks = pd.read_csv('../input/new-york-city-census-data/census_block_loc.csv')\n    #blocks = blocks[blocks.County.isin(['Bronx','Kings','New York','Queens','Richmond'])]\n    census = pd.read_csv('../input/new-york-city-census-data/nyc_census_tracts.csv',index_col=0)\n    blocks['Tract'] = blocks.BlockCode // 10000\n    blocks = blocks.merge(census,how='left',left_on='Tract',right_index=True)\n    #blocks = blocks.dropna(subset=['Borough'],axis=0)\n    return blocks,census\n\ndef convert_to_2d(lats,lons,values):\n    latmin = 40.48\n    lonmin = -74.28\n    latmax = 40.93\n    lonmax = -73.65\n    lon_vals = np.mgrid[lonmin:lonmax:200j]\n    lat_vals = np.mgrid[latmin:latmax:200j]\n    map_values = np.zeros([200,200],'l')\n    dlat = lat_vals[1] - lat_vals[0]\n    dlon = lon_vals[1] - lon_vals[0]\n    for lat,lon,value in zip(lats,lons,values):\n        lat_idx = int(np.rint((lat - latmin) / dlat))\n        lon_idx = int(np.rint((lon-lonmin) / dlon ))        \n        if not np.isnan(value):\n            map_values[lon_idx,lat_idx] = value\n    return lat_vals,lon_vals,map_values\n\nblocks,census = get_census_data()\nblocks_tmp = blocks[blocks.County_x.isin(['Bronx','Kings','New York','Queens','Richmond'])]\nmap_lats, map_lons,map_tracts_nyc = convert_to_2d(blocks_tmp.Latitude,blocks_tmp.Longitude,blocks_tmp.Tract)\nmap_lats, map_lons,map_tracts = convert_to_2d(blocks.Latitude,blocks.Longitude,blocks.Tract)", "outputs": []}, {"metadata": {"_uuid": "66a417b0db1c6e2bb3a9b34ca03c871d0305703f", "_cell_guid": "5a9cf31a-cfd4-49d7-bf95-294f45beaba0"}, "outputs": [], "cell_type": "markdown", "source": "## Coordinates to Census Tract Mapping\n\nNow I'll define a function to map from latitude and longitude to census tract.\nAnything defined outside the area right near New York will be given tract number 0 so that \nwe can easily find those trips.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "cf75e94e858521dde3cb2cdc9e84d44778496937", "collapsed": true, "_cell_guid": "9b13bcec-e4f5-4470-83a4-fe5acc0d5a50"}, "execution_count": null, "cell_type": "code", "source": "def get_tract(lat,lon):\n    latmin = 40.48\n    lonmin = -74.28\n    latmax = 40.93\n    lonmax = -73.65\n    dlat = (latmax-latmin) / 199\n    dlon = (lonmax-lonmin) / 199\n    if (latmin<lat<latmax) and (lonmin<lon<lonmax):\n        lat_idx = int(np.rint((lat - latmin) / dlat))\n        lon_idx = int(np.rint((lon-lonmin) / dlon )) \n        return map_tracts[lon_idx,lat_idx]\n    return 0", "outputs": []}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "0cb9003816692379b963830a52d016556eef1273", "_cell_guid": "d00b41e5-4d7c-4f45-9d27-e88bb9e48450"}, "execution_count": null, "cell_type": "code", "source": "train.info()", "outputs": []}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "028147489df852dea8afbc41ff0e6d27926c3447", "collapsed": true, "_cell_guid": "964842e4-c03d-4273-906e-647c1db49f46"}, "execution_count": null, "cell_type": "code", "source": "train['pu_tracts'] = np.array([get_tract(lat,lon) for lat,lon in zip(train.pickup_latitude,train.pickup_longitude)])\ntrain['do_tracts'] = np.array([get_tract(lat,lon) for lat,lon in zip(train.dropoff_latitude,train.dropoff_longitude)])\n\ntest['pu_tracts'] = np.array([get_tract(lat,lon) for lat,lon in zip(test.pickup_latitude,test.pickup_longitude)])\ntest['do_tracts'] = np.array([get_tract(lat,lon) for lat,lon in zip(test.dropoff_latitude,test.dropoff_longitude)])", "outputs": []}, {"metadata": {"_uuid": "2693de2db55f033c6dbfe7d04d1c9f17713535dc", "_cell_guid": "5587bf5b-a650-466b-902a-b3e17a6c8da3"}, "outputs": [], "cell_type": "markdown", "source": "## What tracts have the most pickups and dropoffs?\n\nNow that we have added in the tract information, we can see pickup and dropoff stats for individual tracts.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "6222412577cc0af1cab58ecda7393d52fe257f01", "collapsed": true, "_cell_guid": "0b62e14f-cdfa-4816-b9e1-48914a75b7e3"}, "execution_count": null, "cell_type": "code", "source": "pickups = train['pu_tracts'].value_counts()\ndropoffs = train['do_tracts'].value_counts()", "outputs": []}, {"metadata": {"_uuid": "937d80c4983a012ffa533b2095e84ce9ee96fa82", "_cell_guid": "f63e289f-8e23-44c3-9b40-84003d0a6c0b"}, "outputs": [], "cell_type": "markdown", "source": "Here, I plot out the tracts with the 5 largest numbers of pickups. The color scale just has everything in order but the absolute scale is meaningless.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "f1f087b29947a9d1d9b60a665c3b41715cbe3f8d", "_cell_guid": "399097a9-adc6-463e-ab92-eaf2d90487ec"}, "execution_count": null, "cell_type": "code", "source": "top_tracts = [x for x in pickups.index.values[0:5]]\ntop_tracts.reverse()\nvalues = 0.250*(1-np.isin(map_tracts_nyc,top_tracts+[0]))\n\nfor i in range(len(top_tracts)):\n    values += (i+7)*(map_tracts_nyc==top_tracts[i])/11\n\nfig = plt.figure(1,figsize=[7,7])\nim = plt.imshow(values.T,origin='lower',cmap='jet')\nplt.xlabel('Longitude [degrees]')\nplt.ylabel('Latitude [degrees]')\nplt.title('Most Common Pickup Points in NYC Limits')\nplt.colorbar(im,fraction=0.045, pad=0.04)\nplt.show()", "outputs": []}, {"metadata": {"_uuid": "7e3e469f094fceb3ad3b78137668c376a22e0700", "_cell_guid": "2492432f-5f41-4a89-8451-107eb4958337"}, "outputs": [], "cell_type": "markdown", "source": "It turns out that we could have probably guessed the biggest pickup points. In descending order, it looks like they are:\n\n  1. LaGuardia Airport\n  2. Penn Station\n  3. JFK Airport\n  4. Central Park\n  5. Columbus Circle\n  \nThe airports and Penn Station are transportation hubs, so it would make sense for them to have many pickups. They all have large numbers of people with luggage that might be inconvenient on public transportation.\n\nCentral Park also has many pickups but it likely only has so many because it takes up a large area in the middle of Manhattan. Other tracts will just be much smaller.\n\nNext, we can look at dropoffs.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "030e239563253adf619c7826479090479478d29f", "_cell_guid": "81bdc454-d0d2-406e-bbb6-2dab470e78d9"}, "execution_count": null, "cell_type": "code", "source": "top_tracts = [x for x in dropoffs.index.values[0:5]]\ntop_tracts.reverse()\nvalues = 0.250*(1-np.isin(map_tracts_nyc,top_tracts+[0]))\n\nfor i in range(len(top_tracts)):\n    values += (i+7)*(map_tracts_nyc==top_tracts[i])/11\n\nfig = plt.figure(1,figsize=[7,7])\nim = plt.imshow(values.T,origin='lower',cmap='jet')\nplt.xlabel('Longitude [degrees]')\nplt.ylabel('Latitude [degrees]')\nplt.title('Most Common Dropoff Points in NYC Limits')\nplt.colorbar(im,fraction=0.045, pad=0.04)\nplt.show()", "outputs": []}, {"metadata": {"_uuid": "f6a2654df40bd7ce29debc9c20fc35bfe5472e4d", "_cell_guid": "afe3bd72-e856-4332-8a5a-178e61ae5501"}, "outputs": [], "cell_type": "markdown", "source": "The top dropoff points are all in Manhattan. Penn Station and Central Park are the top two. The other three appear to be a piece of the West Side including part of Chelsea, a section of Midtown east of Penn Station (it might be close to Grand Central), and finally a small bit of Midtown between Times Square and Columbus Circle.\n\n## Densest Pickup/Dropoff Points\n\nWith our tract mapping, we also can calculate an approximate area for each tract to then find the densest places where taxi pickups and dropoffs are done. This will get more accurate as a finer grid of points is used.\n\nThis is not perfect, however, since many tracts also include a large amount of water.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "da20671983929cddede89ff3bc1a141f1a82c475", "collapsed": true, "_cell_guid": "0f4ccd46-8403-464f-b4bd-466405bf3e94"}, "execution_count": null, "cell_type": "code", "source": "areas = blocks.Tract.value_counts()", "outputs": []}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "f23a51cdda65360d0c2b804a1476c9de05b72f2a", "collapsed": true, "_cell_guid": "67ee7c6e-c714-4117-bf3c-09d9d21523d1"}, "execution_count": null, "cell_type": "code", "source": "pu_area_norm = pickups\ndo_area_norm = dropoffs\n\npu_area_norm = pd.concat([pu_area_norm,areas],join='inner',axis=1)\ndo_area_norm = pd.concat([do_area_norm,areas],join='inner',axis=1)\n\npu_area_norm['areas'] = pu_area_norm.pu_tracts/pu_area_norm.Tract\ndo_area_norm['areas'] = do_area_norm.do_tracts/do_area_norm.Tract\n\npu_areas = pu_area_norm.areas.sort_values(ascending=False)\ndo_areas = do_area_norm.areas.sort_values(ascending=False)", "outputs": []}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "b6d52cec68bfeaf0e2c0a73479a4a4440dc1c890", "_cell_guid": "dabfec1d-5871-4c49-9b84-087accc29459"}, "execution_count": null, "cell_type": "code", "source": "top_tracts = [x for x in pu_areas.index.values[0:10]]\ntop_tracts.reverse()\nvalues = 0.250*(1-np.isin(map_tracts_nyc,top_tracts+[0]))\n\nfor i in range(len(top_tracts)):\n    values += (i+7)*(map_tracts_nyc==top_tracts[i])/16\n    \n\nfig = plt.figure(1,figsize=[7,7])\nim = plt.imshow(values.T,origin='lower',cmap='jet')\nplt.xlabel('Longitude [degrees]')\nplt.ylabel('Latitude [degrees]')\nplt.title('Densest Pickup Points in NYC Limits')\nplt.colorbar(im,fraction=0.045, pad=0.04)\nplt.show()", "outputs": []}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "58a7d9a39b1b9d683efa2e836f5d778aec041c79", "_cell_guid": "ccae8321-cf3f-4261-8356-543247ec7107"}, "execution_count": null, "cell_type": "code", "source": "top_tracts = [x for x in do_areas.index.values[0:10]]\ntop_tracts.reverse()\nvalues = 0.250*(1-np.isin(map_tracts_nyc,top_tracts+[0]))\n\nfor i in range(len(top_tracts)):\n    values += (i+7)*(map_tracts_nyc==top_tracts[i])/16\n\nfig = plt.figure(1,figsize=[7,7])\nim = plt.imshow(values.T,origin='lower',cmap='jet')\nplt.xlabel('Longitude [degrees]')\nplt.ylabel('Latitude [degrees]')\nplt.title('Densest Dropoff Points in NYC Limits')\nplt.colorbar(im,fraction=0.045, pad=0.04)\nplt.show()", "outputs": []}, {"metadata": {"_uuid": "dfa915c250ce56516a27769d49afecf21dcaa3a2", "_cell_guid": "e7cf0167-9f35-41c9-a6cd-6cf9daa3b464"}, "outputs": [], "cell_type": "markdown", "source": "For both pickups and dropoffs, the densest spots for taxi activity are around Midtown.\n\nMidtown is also the biggest business district and home to many tourist attractions, so this is probably what we might guess if we didn't have any data.", "execution_count": null}, {"metadata": {"_uuid": "2076d0bbf2ccef64e5be9f8a1ff9b9790de40fe4", "_cell_guid": "04a9dbe9-c0dc-459d-ba62-4d461a7568dd"}, "outputs": [], "cell_type": "markdown", "source": "## Borough/County Mapping\n\nNear New York City, the boundaries between many of the areas are rivers. There are not many crossings across the Hudson and East Rivers, so bridges and tunnels create chokepoints that might have large effects on travel times.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "b60344f0def65382c710ad3e3d96b0a12f56e988", "_cell_guid": "d506b8ec-205f-4234-be03-d02f64b3b14b"}, "execution_count": null, "cell_type": "code", "source": "train['PU_Location'] =  train.pu_tracts//1000000\nfips_codes = {36061:'Manhattan',36081:'Queens',36047:'Brooklyn',\n              36005:'Bronx',36085:'Staten Island',36059:'Nassau',36119:'Westchester',\n              34017:'NJ',34013:'NJ',34003:'NJ',34039:'NJ',\n              34031:'NJ',34023:'NJ',34025:'NJ',0:'Unknown'\n             }\ntrain.PU_Location = train.PU_Location.map(fips_codes)\n\ntest['PU_Location'] = test.pu_tracts//1000000\ntest.PU_Location = test.PU_Location.map(fips_codes)\n\ntrain.PU_Location.value_counts()\n", "outputs": []}, {"metadata": {"_uuid": "84d82d2395c3026c700563830957f8c17eb79536", "_cell_guid": "daebf89d-965e-4b95-9bab-f073152600a9"}, "outputs": [], "cell_type": "markdown", "source": "We see that in this dataset, taxi pickups are mainly in Manhattan, with Queens second. The Queens pickups are mostly at the airports, which explains why there is such a large difference between Queens and Brooklyn.\n\nBelow, we see similar stats for dropoffs, but with Brooklyn and Queens almost even. There are far fewer dropoffs in the Bronx compared to Queens and Brooklyn. Some of this may be because much of the Bronx is closer to Manhattan, so public transportation options are more attractive. \n\nThere are also several thousand dropoffs in New Jersey. Our histograms showed that these are concentrated at Newark Airport, Jersey City, and Hoboken.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "2c155a04c6add9b551ca383cd1ba112c4aff90ab", "_cell_guid": "d85d651d-a0b4-411d-905e-dfe5a5e22baf"}, "execution_count": null, "cell_type": "code", "source": "train['DO_Location'] =  train.do_tracts//1000000\ntrain.DO_Location = train.DO_Location.map(fips_codes)\ntest['DO_Location'] =  test.do_tracts//1000000\ntest.DO_Location = test.DO_Location.map(fips_codes)\ntrain.DO_Location.value_counts()", "outputs": []}, {"metadata": {"_uuid": "b0ae80e1c02007a8e11c8ef4c0bfe5d69100d4fb", "_cell_guid": "a5559d6f-69da-481d-9bfa-c1084c5a1db4"}, "outputs": [], "cell_type": "markdown", "source": "## Travel Times for Travel Between Different Counties", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "b7cbadf4b4eb563564066ef5adb6ec0f4d80f8e6", "_cell_guid": "4ea9b5aa-3e4f-4da6-b251-ccb04ef24703"}, "execution_count": null, "cell_type": "code", "source": "pd.set_option('display.max_rows', 200)\nprint(train.groupby(['PU_Location','DO_Location'])['trip_duration'].describe()[['count','25%','50%','75%']])\npd.reset_option('display.max_rows')\n", "outputs": []}, {"metadata": {"_uuid": "8805259edc5e13124550cf2447bd0b3963156b1c", "_cell_guid": "455c6fb9-9287-45cd-9709-a7b0f338a92b"}, "outputs": [], "cell_type": "markdown", "source": "# A Very Simple Model\n\nFrom the leaderboard, we see that just taking the mean duration gives us a score of 0.892.\n\nCan we beat that using just borough/county mappings?\n\nWe can easily implement this by converting the mapping into integers and then training a decision tree.\nI'll do this using sklearn with a DecisionTreeRegressor with the default parameters.\n\nThis would be a very simple model, but it should give us somewhat better results. Instead of taking the\noverall mean, we would be getting the mean for each combination (except for maybe combining combinations that have\na very small number of samples).\n\nTo check the performance, I will use a 10-fold cross validation.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "77c0280c589fd77569992979d8ef13dac3662e3b", "collapsed": true, "_cell_guid": "f6f12dd9-f114-4447-abb8-8a23444bee1c"}, "execution_count": null, "cell_type": "code", "source": "loc_map = {\"Manhattan\":0,\"Queens\":1,\"Brooklyn\":2,\"Bronx\":3,\n           \"NJ\":4,\"Unknown\":5,\"Staten Island\":6,\"Nassau\":7,\n           \"Westchester\":8}\n\ntrain.PU_Location = train.PU_Location.map(loc_map)\ntrain.DO_Location = train.DO_Location.map(loc_map)\n\ntest.PU_Location = test.PU_Location.map(loc_map)\ntest.DO_Location = test.DO_Location.map(loc_map)", "outputs": []}, {"metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "2df4fb97d2934a9a7b37fb60874c840dcd87a220", "_cell_guid": "a5587b57-9f0b-4ad5-a3a5-eb8d33c83fb1"}, "execution_count": null, "cell_type": "code", "source": "targets = np.log(train.trip_duration+1)\nXall = train[['PU_Location',\"DO_Location\"]]\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nnsplits = 10\ntree_model = DecisionTreeRegressor()\nkf = KFold(n_splits = nsplits,random_state=999)\n\nerr_train_ave = 0 \nerr_test_ave = 0\nerr_train_std = 0\nerr_test_std = 0\ncounter = 0\nfor train_index, test_index in kf.split(Xall):\n    X_train, X_test = Xall.iloc[train_index], Xall.iloc[test_index]\n    y_train, y_test = targets.iloc[train_index], targets.iloc[test_index]\n    tree_model.fit(X_train,y_train)\n    pred_train = tree_model.predict(X_train)\n    pred_test = tree_model.predict(X_test)\n    err_train = np.sqrt(mean_squared_error(y_train,pred_train))\n    err_test = np.sqrt(mean_squared_error(y_test,pred_test))\n    err_train_ave += err_train\n    err_test_ave += err_test\n    err_train_std += err_train*err_train\n    err_test_std += err_test * err_test\n    print('%i Train Err: %f, Validation Err: %f'%(counter,err_train,err_test))\n    counter+=1\n\nerr_train_ave /= nsplits\nerr_test_ave /= nsplits\nerr_train_std /= nsplits\nerr_test_std /= nsplits\nerr_train_std = np.sqrt((err_train_std - err_train_ave*err_train_ave)*nsplits / (nsplits-1))\nerr_test_std = np.sqrt((err_test_std - err_test_ave*err_test_ave)*nsplits / (nsplits-1))\n\nprint(\"\\nTrain Ave: %f Std. Dev: %f\" %(err_train_ave,err_train_std))\nprint(\"Validation Ave: %f Std. Dev: %f\"%(err_test_ave,err_test_std))", "outputs": []}, {"metadata": {"_uuid": "b469b383e853ba4738395ccb5be0630ee49e3ffd", "collapsed": true, "_cell_guid": "d0b638d2-f623-431a-b237-8410246c7d73"}, "outputs": [], "cell_type": "markdown", "source": "## Results\n\nWe see here that we get an average score of 0.707 for both the train and validation sets.\nThis tells us that our model is not overtraining. This is what we expect since we are ignoring most of the \ninformation in the data. The samples are heavily Manhattan-centric, so while this improves upon the \nmean-value model, it is still not very good. To keep improving this, we will want to improve \nour model and also do some feature engineering.\n\n## Additional features\n\nObviously, we will want to do something directly with the latitude and longitude points. For simplicity, rather\nthan include both the start and end points, we may want to instead include the start position,\nthe distance, and the direction.\n\nThe time should also matter a lot, so we will want to engineer some features out of the start time. The time \nof day and day of weeek (or combined into a time of week) should be powerful features. There will be large\ndifferences between commute times and non-commute times as well as weekday/weekend differences.", "execution_count": null}, {"metadata": {"_execution_state": "idle", "_uuid": "3f01485115ac2bc1152c71914483492ca8efb2c8", "collapsed": true, "_cell_guid": "88b2c586-35c8-4634-b4fe-867deb939a18"}, "outputs": [], "cell_type": "markdown", "source": "# A Simple Linear Model\n\nWe can start by building a model for linear regression. I'll do this by getting the distance travelled in each trip.", "execution_count": null}, {"metadata": {"trusted": true, "_uuid": "f69fea146ac92414751f5d4d73951efdf412000c", "_cell_guid": "c2aa691e-83d2-4cde-9bb1-7d24fbf436eb"}, "execution_count": null, "cell_type": "code", "source": "train.head()", "outputs": []}, {"metadata": {"trusted": true, "_uuid": "ba44104b902d26af4197d4898fe3820c3c30cdd8", "_cell_guid": "e7fc79a5-1f2a-4da3-9376-542418c2c52d"}, "execution_count": null, "cell_type": "code", "source": "train['dlon'] = (train.dropoff_longitude-train.pickup_longitude) * np.pi/180 *\\\n                np.cos((train.dropoff_latitude+train.pickup_latitude) * 0.5 * np.pi/180)\ntrain['dlat'] = (train.dropoff_latitude-train.pickup_latitude) * np.pi/180\nRe = 6371 # Earth radius in km\n\ntrain['dist'] = Re*np.hypot(train.dlon,train.dlat)\ntrain['pu_do_code'] = train.PU_Location + 10 * train.DO_Location\n\ntest['dlon'] = (test.dropoff_longitude-test.pickup_longitude) * np.pi/180 *\\\n                np.cos((test.dropoff_latitude+test.pickup_latitude) * 0.5 * np.pi/180)\ntest['dlat'] = (test.dropoff_latitude-test.pickup_latitude) * np.pi/180\ntest['dist'] = Re*np.hypot(test.dlon,test.dlat)\n# Encode Pickup/Dropoff location into a single number\ntest['pu_do_code'] = test.PU_Location + 10 * test.DO_Location\n\n\ntrain.head()", "outputs": []}, {"metadata": {"_uuid": "0c5d968af495da547a142cbb3e7128dbdaf41199", "_cell_guid": "f08ac56c-6efe-448e-97f7-07114a0cd512"}, "outputs": [], "cell_type": "markdown", "source": "## Histogram of Distances\n\nWe actually see that there are several peaks at larger distances seen here, probably corresponding to the airports.", "execution_count": null}, {"metadata": {"trusted": true, "_uuid": "e794b315de7aebeac3607a09055673796f29c4a9", "_cell_guid": "e57905c9-ea22-4d86-b3e9-930bbc4fd94b"}, "execution_count": null, "cell_type": "code", "source": "plt.hist(train.dist,bins=100,range=[0,50])\nplt.yscale('log')\nplt.show()", "outputs": []}, {"metadata": {"_uuid": "d1b820fa7db44ceac8ad94142d162baeff3f7008", "_cell_guid": "a9da1081-61a3-43b2-955f-2fd4b6767c38"}, "outputs": [], "cell_type": "markdown", "source": "The metric that we're using to score this competition is the squared difference between the logarithm of the duration and the logarithm of the prediction. Since we generally expect time to be proportional to distance, here we'll expect that log(time) is proportional to log(distance). Thus, we'll want the logarithm in our fit.\n\nI'll also add several other terms to allow for the fact that the scaling may not be perfect at very short or very long distances due to things like traffic lights, highway access times, etc. Here, I'm using log(d), sqrt(d), d, d^1.5, and d^2.\n\nIt actually turns out that for the linear model, the location encoding doesn't really add anything, so I've removed it, but the code is still there. I've done it this way to avoid pitfalls of blindly running a one hot encoding function, where we might get different columns in the test and training sets.", "execution_count": null}, {"metadata": {"trusted": true, "_uuid": "85b78e40ecf95733334757f57c7fa53f3bf27857", "collapsed": true, "_cell_guid": "a6f42655-e725-43b9-a2df-a96cbe97265d"}, "execution_count": null, "cell_type": "code", "source": "train['ldist'] = np.log(train.dist + 0.01)\ntrain['d2'] = train.dist*train.dist\ntrain['d1_2'] = np.sqrt(train.dist)\ntrain['d3_2'] = train.dist * train.d1_2\n\ntest['ldist'] = np.log(test.dist + 0.01)\ntest['d2'] = test.dist*test.dist\ntest['d1_2'] = np.sqrt(test.dist)\ntest['d3_2'] = test.dist * test.d1_2\n\nX_all = train[['dist','ldist','d2','d1_2','d3_2']]\nX_test = test[['dist','ldist','d2','d1_2','d3_2']]\n\n#for i in range(9):\n#    for j in range(9):\n#        code = 1.0*(train['pu_do_code'] == (10*i+j))\n#        if code.sum() > 10:\n#            X_train['PU_DO_%i%i'%(i,j)] = code\n\ny_all = np.log(train['trip_duration']+1)", "outputs": []}, {"metadata": {"trusted": true, "_uuid": "89a584ddb59098a0a39e93d1a59f9ca49ebed749", "_cell_guid": "375b8e34-08d7-4c5a-b431-290400718897"}, "execution_count": null, "cell_type": "code", "source": "X_all.head()", "outputs": []}, {"metadata": {"_uuid": "4c2d66fbeaf8525644361f988f944c7bdf7bbb1b", "_cell_guid": "05a629d0-458f-472d-9777-c67d20312393"}, "outputs": [], "cell_type": "markdown", "source": "## Model Validation\n\nI have a huge number of samples and only 5 features, so I'm just going to use a default linear regression model with no regularization. I haven't checked this but it is highly likely that regularization will basically never do anything useful here.\n\nI'm also not normalizing the feature set, which might hurt the fit process a bit but it seems like things converge pretty readily.\n\nFor validation, I'm using a 10-fold cross validation.", "execution_count": null}, {"metadata": {"trusted": true, "_uuid": "53f291444564dd206b9522145cc3ae2f433c7071", "_cell_guid": "91736b31-bfed-4db1-b651-bc88dee3079c"}, "execution_count": null, "cell_type": "code", "source": "from sklearn.linear_model import LinearRegression\n\nnsplits = 10\nlin_model = LinearRegression()\nkf = KFold(n_splits = nsplits,random_state=999)\nerr_train_ave = 0 \nerr_test_ave = 0\nerr_train_std = 0\nerr_test_std = 0\ncounter = 0\nfor train_index, test_index in kf.split(X_all):\n    X_train, X_val = X_all.iloc[train_index], X_all.iloc[test_index]\n    y_train, y_val = y_all.iloc[train_index], y_all.iloc[test_index]\n    lin_model.fit(X_train,y_train)\n    pred_train = lin_model.predict(X_train)\n    pred_test = lin_model.predict(X_val)\n    err_train = np.sqrt(mean_squared_error(y_train,pred_train))\n    err_test = np.sqrt(mean_squared_error(y_val,pred_test))\n    err_train_ave += err_train\n    err_test_ave += err_test\n    err_train_std += err_train*err_train\n    err_test_std += err_test * err_test\n    print('%i Train Err: %f, Validation Err: %f'%(counter,err_train,err_test))\n    counter+=1\nerr_train_ave /= nsplits\nerr_test_ave /= nsplits\nerr_train_std /= nsplits\nerr_test_std /= nsplits\nerr_train_std = np.sqrt((err_train_std - err_train_ave*err_train_ave)*nsplits / (nsplits-1))\nerr_test_std = np.sqrt((err_test_std - err_test_ave*err_test_ave)*nsplits / (nsplits-1))\n\nprint(\"\\nTrain Ave: %f Std. Dev: %f\" %(err_train_ave,err_train_std))\nprint(\"Validation Ave: %f Std. Dev: %f\"%(err_test_ave,err_test_std))", "outputs": []}, {"metadata": {"_uuid": "40958cfa92b18603da56e5a2c78372a4483c591c", "_cell_guid": "fa63f439-d01c-42bb-a18d-7151829d8462"}, "outputs": [], "cell_type": "markdown", "source": "## Running on the Test Set\n\nNow I'll fit the model to the full training set and then calculate the predicted values for the test set.\n\nI'll expect a score of around 0.515, which isn't bad for such a simple model given a complicated data set. I haven't even looked at timing information yet, so there are going to be many ways to improve.", "execution_count": null}, {"metadata": {"trusted": true, "_uuid": "dd325cdf8b9f46b11f434b29fc43290c6a4c5603", "_cell_guid": "272009e4-6a1a-474e-93cf-41a433541df5"}, "execution_count": null, "cell_type": "code", "source": "lin_model.fit(X_all,y_all)\n#print(lin_model.coef_)\n#print(lin_model.intercept_)\n#print(X_test[X_test.isnull().any(axis=1)])\nlog_test_pred = lin_model.predict(X_test)\n\ntest_pred = np.exp(log_test_pred) - 1\n#print(test_pred)\ntest['pred'] = [ np.max(x,0) for x in test_pred ]\nX_out = test[['pred']]\nX_out.columns.values[0] = 'trip_duration'\n#print(X_out)\n#X_out['trip_duration'] = test_pred\nX_out.to_csv('LinearModel.csv')", "outputs": []}], "nbformat_minor": 2, "nbformat": 4}