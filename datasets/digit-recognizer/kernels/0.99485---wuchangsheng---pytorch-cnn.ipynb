{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#learn from https://www.kaggle.com/juiyangchang/cnn-with-pytorch-0-995-accuracyport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install torch torchvision","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"92c74905-e68d-495b-aec4-fc7b191f6cca","_uuid":"b469dfbd773bc1dba716b5d035fb3c5f379d01dc","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\n\nimport math\nimport random\n\nfrom PIL import Image, ImageOps, ImageEnhance\nimport numbers\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":72,"outputs":[]},{"metadata":{"_cell_guid":"5290018a-d49e-44ca-8b42-55278ffd374f","_uuid":"5223c9438c122f5fc9f58ad7606fe17410e83451","collapsed":true,"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n","execution_count":73,"outputs":[]},{"metadata":{"_cell_guid":"6370c608-b8ef-416b-97c1-c7cac8b0cc89","_uuid":"f6555d50e3525108b25a5a54394dfb60e6bb0488","collapsed":true,"trusted":true},"cell_type":"code","source":"class mydata(Dataset):\n     def __init__(self, file_path, \n                 transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), \n                     transforms.Normalize(mean=(0.5,), std=(0.5,))])\n                ):\n        \n        df = pd.read_csv(file_path)\n        \n        if len(df.columns) == n_pixels:\n            # test data\n            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = None\n        else:\n            # training data\n            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = torch.from_numpy(df.iloc[:,0].values)\n            \n        self.transform = transform\n    \n     def __len__(self):\n        return len(self.X)\n\n     def __getitem__(self, idx):\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        else:\n            return self.transform(self.X[idx])","execution_count":74,"outputs":[]},{"metadata":{"_cell_guid":"8fe88853-5feb-4f72-8f83-d3233a235957","_uuid":"3c37c1d31f77f6c847a5634caee60d4e29339555","collapsed":true,"trusted":true},"cell_type":"code","source":"class RandomRotation(object):\n    \"\"\"\n    https://github.com/pytorch/vision/tree/master/torchvision/transforms\n    Rotate the image by angle.\n    Args:\n        degrees (sequence or float or int): Range of degrees to select from.\n            If degrees is a number instead of sequence like (min, max), the range of degrees\n            will be (-degrees, +degrees).\n        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n            An optional resampling filter.\n            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n        expand (bool, optional): Optional expansion flag.\n            If true, expands the output to make it large enough to hold the entire rotated image.\n            If false or omitted, make the output image the same size as the input image.\n            Note that the expand flag assumes rotation around the center and no translation.\n        center (2-tuple, optional): Optional center of rotation.\n            Origin is the upper left corner.\n            Default is the center of the image.\n    \"\"\"\n\n    def __init__(self, degrees, resample=False, expand=False, center=None):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(\"If degrees is a single number, it must be positive.\")\n            self.degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n            self.degrees = degrees\n\n        self.resample = resample\n        self.expand = expand\n        self.center = center\n\n    @staticmethod\n    def get_params(degrees):\n        \"\"\"Get parameters for ``rotate`` for a random rotation.\n        Returns:\n            sequence: params to be passed to ``rotate`` for random rotation.\n        \"\"\"\n        angle = np.random.uniform(degrees[0], degrees[1])\n\n        return angle\n\n    def __call__(self, img):\n        \"\"\"\n            img (PIL Image): Image to be rotated.\n        Returns:\n            PIL Image: Rotated image.\n        \"\"\"\n        \n        def rotate(img, angle, resample=False, expand=False, center=None):\n            \"\"\"Rotate the image by angle and then (optionally) translate it by (n_columns, n_rows)\n            Args:\n            img (PIL Image): PIL Image to be rotated.\n            angle ({float, int}): In degrees degrees counter clockwise order.\n            resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n            An optional resampling filter.\n            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n            expand (bool, optional): Optional expansion flag.\n            If true, expands the output image to make it large enough to hold the entire rotated image.\n            If false or omitted, make the output image the same size as the input image.\n            Note that the expand flag assumes rotation around the center and no translation.\n            center (2-tuple, optional): Optional center of rotation.\n            Origin is the upper left corner.\n            Default is the center of the image.\n            \"\"\"\n                \n            return img.rotate(angle, resample, expand, center)\n\n        angle = self.get_params(self.degrees)\n\n        return rotate(img, angle, self.resample, self.expand, self.center)","execution_count":75,"outputs":[]},{"metadata":{"_cell_guid":"72328c50-4cf3-496e-99fb-c3e337f8e4ba","_uuid":"4c45a79132c757c2c6ff7b210f9f1316b3e99857","collapsed":true,"trusted":true},"cell_type":"code","source":"class RandomShift(object):\n    def __init__(self, shift):\n        self.shift = shift\n        \n    @staticmethod\n    def get_params(shift):\n        \"\"\"Get parameters for ``rotate`` for a random rotation.\n        Returns:\n            sequence: params to be passed to ``rotate`` for random rotation.\n        \"\"\"\n        hshift, vshift = np.random.uniform(-shift, shift, size=2)\n\n        return hshift, vshift \n    def __call__(self, img):\n        hshift, vshift = self.get_params(self.shift)\n        \n        return img.transform(img.size, Image.AFFINE, (1,0,hshift,0,1,vshift), resample=Image.BICUBIC, fill=1)\n    ","execution_count":76,"outputs":[]},{"metadata":{"_cell_guid":"a094fdab-5edf-4094-8b73-d9533449a4c9","_uuid":"2d9c20c936a8c900e69fb3038d1ba96da30174e6","collapsed":true,"trusted":true},"cell_type":"code","source":"batch_size = 64\nn_test = len(test_df)\nn_pixels = len(test_df.columns)\ntrain_dataset = mydata('../input/train.csv', transform= transforms.Compose(\n                            [transforms.ToPILImage(), RandomRotation(degrees=20), RandomShift(3),\n                             transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))]))\ntest_dataset = mydata('../input/test.csv', )\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                           batch_size=batch_size, shuffle=False)","execution_count":77,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"aaa450695b0970d8d70a4fb8331ca8da68003d9c"},"cell_type":"code","source":"class Net(nn.Module):    \n    def __init__(self):\n        super(Net, self).__init__()\n          \n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p = 0.25),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1,padding=1),\n             nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n             nn.BatchNorm2d(64),\n            \n            nn.Conv2d(64, 128, kernel_size=3, stride=1,padding=1),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1,padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),         \n            nn.Dropout(p = 0.25),\n        )\n          \n        self.classifier = nn.Sequential(\n            nn.Dropout(p = 0.25),\n            nn.Linear(128 * 7 * 7, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.25),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1) \n        x = self.classifier(x)\n        \n        return x     ","execution_count":109,"outputs":[]},{"metadata":{"_cell_guid":"e4d63e8e-3ae8-47ad-9101-edc73f7ee3db","_uuid":"9f2da3daf0154546e61f6888a50308419892c909","collapsed":true,"trusted":true},"cell_type":"code","source":"model = Net()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\ncriterion = nn.CrossEntropyLoss()\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()","execution_count":110,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ce82d87f65b4d4856cd91341ba47e754b685989"},"cell_type":"code","source":"print(model)","execution_count":113,"outputs":[]},{"metadata":{"_cell_guid":"3f49ec25-977e-49e1-a712-d630a4871533","_uuid":"2e044f0ab05eb7dd331d701cca497b70858d6045","collapsed":true,"trusted":true},"cell_type":"code","source":"def train(epoch):\n    model.train()\n    exp_lr_scheduler.step()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n        \n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        \n        loss.backward()\n        optimizer.step()\n        \n        if (batch_idx + 1)% 100 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) / len(train_loader), loss.data[0]))","execution_count":114,"outputs":[]},{"metadata":{"_cell_guid":"fa24669e-01e8-4e65-957b-b108b442c5de","_uuid":"3bd36696a25e0c0a55326124eebe572f569108da","collapsed":true,"trusted":true},"cell_type":"code","source":"def evaluate(data_loader):\n    model.eval()\n    loss = 0\n    correct = 0\n    \n    for data, target in data_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        output = model(data)\n        \n        loss += F.cross_entropy(output, target, size_average=False).data[0]\n\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n        \n    loss /= len(data_loader.dataset)\n        \n    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n        loss, correct, len(data_loader.dataset),\n        100. * correct / len(data_loader.dataset)))","execution_count":115,"outputs":[]},{"metadata":{"_cell_guid":"6699ff22-84a3-499d-8179-71bb7dd19a62","_uuid":"449cf81040defb0da0cf78af21b1b56fb871906e","trusted":true},"cell_type":"code","source":"n_epochs = 20\n\nfor epoch in range(n_epochs):\n    train(epoch)\n    evaluate(train_loader)","execution_count":116,"outputs":[]},{"metadata":{"_cell_guid":"0f33df49-92f1-4356-9451-4a0c8c4b8d15","_uuid":"0bd76e85a92cd4953c9692e242e2312c7a14ddc7","collapsed":true,"trusted":true},"cell_type":"code","source":"def prediciton(data_loader):\n    model.eval()\n    test_pred = torch.LongTensor()\n    \n    for i, data in enumerate(data_loader):\n        data = Variable(data, volatile=True)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            \n        output = model(data)\n        \n        pred = output.cpu().data.max(1, keepdim=True)[1]\n        test_pred = torch.cat((test_pred, pred), dim=0)\n        \n    return test_pred\ntest_pred = prediciton(test_loader)\nout_df = pd.DataFrame(np.c_[np.arange(1, len(test_dataset)+1)[:,None], test_pred.numpy()], \n                      columns=['ImageId', 'Label'])","execution_count":117,"outputs":[]},{"metadata":{"_cell_guid":"ff865829-6546-4282-90d4-8b6d496fba89","_uuid":"d727aa7f49cd15c99f0e0b1dc4a6d8819dc411e4","collapsed":true,"trusted":true},"cell_type":"code","source":"out_df.to_csv('submission111.csv', index=False)","execution_count":120,"outputs":[]},{"metadata":{"_cell_guid":"9d419ec6-1205-40a6-a5d0-9506110e00d1","_uuid":"87298364d6915ce3402c06adde69738f8bdee60f","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}