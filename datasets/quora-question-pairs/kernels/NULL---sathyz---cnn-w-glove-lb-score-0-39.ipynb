{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"956b7176a0eeedf7ff00f6a62172bfd0fbc513a5","collapsed":true,"trusted":false},"cell_type":"code","source":"from collections import Counter\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"5eb70f850b1855bb43ab7f6d41dceade10dbbb0b","trusted":false,"collapsed":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nENG_STOP_WORDS = set(stopwords.words(\"english\"))\nGLOVE_DIR = \"../input/glove-global-vectors-for-word-representation/\"\n\ndef build_corpus(df, size):\n    \"\"\"\n    build unique set of questions given question pairs in dataframe\n    \"\"\"\n    print(\"building unique question list\")\n    questions = np.empty(size, dtype='O')\n    \n    for i, row in df.fillna(\"\").iterrows():\n        #if not questions[row.qid1 - 1]:\n        questions[row.qid1 - 1] = row.question1\n        #if not questions[row.qid2 - 1]:\n        questions[row.qid2 - 1] = row.question2\n\n    return questions\n\ndef sentence2words(sentences):\n    return [ [ word for word in word_tokenize(sentence.lower())\n                  if word not in ENG_STOP_WORDS \n             ]\n                    for sentence in sentences]\n\ndef word_indexer(index_to_word):\n    return { word: i  for i, word in enumerate(index_to_word)}\n\ndef build_vocab(tokenized_sentences, size=None):\n    \"\"\"\n    Given the words, build word <=> index mapping.\n    \"\"\"\n    word_counter = Counter()\n    for i, line in enumerate(tokenized_sentences):\n        word_counter.update(map(str.lower, line))\n\n    if size is None:\n        size = len(word_counter)\n    \n    print(\"vocab size: {}\".format(size))\n\n    # index => word\n    index_to_word = [\"<UNK>\",] + list( map( lambda x: x[0], word_counter.most_common(size)))\n    # word => index\n    word_to_index = word_indexer(index_to_word)\n    return word_to_index, index_to_word\n\ndef indexer(tokenized_sentences, word_to_index):\n    \"\"\"Convert list of words to list of word indices.\"\"\"\n    return [[word_to_index.get(word, 0) \n                 for word in sentence] \n                    for sentence in tokenized_sentences]\n\ndef build_xy(questions, df, indices):\n    mdf = df.loc[indices]\n    # X => question pair\n    # Y => is_dup?\n    return ([ questions[mdf.qid1 - 1],\n              questions[mdf.qid2 - 1] ],\n            mdf.is_duplicate )\n\ndef load_train_dataset(filepath, vocab_size=None):\n    \"\"\"\n    Load the training data. \n    input: train csv containing questions and labels\n    output: each question (sequence of words) is converted to \n        sequence of indices, word to index mapping, index to word mapping\n    \"\"\"\n    df = pd.read_csv(filepath, index_col=\"id\")\n    n_questions = df[[\"qid1\", \"qid2\"]].max().values.max()\n    print(\"Total Questions: {}\".format(n_questions))\n    \n    questions = build_corpus(df, n_questions)\n    # question: \"Where is India?\"\n    # tokenized: [\"where\", \"india\", \"?\"]\n    # indexed: [450, 220, 1]\n    tokenized_questions = sentence2words(questions)\n    del questions\n    \n    word_to_index, index_to_word = build_vocab(tokenized_questions, vocab_size)    \n        \n    indexed_questions = indexer(tokenized_questions, word_to_index)\n    del tokenized_questions\n    \n    max_len = max([len(seq) for seq in indexed_questions])\n    print(\"Max. sequence length = {}\".format(max_len))\n    \n    padded_questions = pad_sequences(indexed_questions, max_len)\n    del indexed_questions\n    \n    train_indices, test_indices = train_test_split(range(df.shape[0]),\n                                                   test_size=40000,\n                                                   random_state=1421)\n    \n    X_train, y_train = build_xy(padded_questions, df, train_indices)\n    X_val, y_val = build_xy(padded_questions, df, test_indices)\n    return (X_train, y_train),  (X_val, y_val), index_to_word, word_to_index\n","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"90e40b9e7be8678364c9fdf57a58e1e67432a5d7","trusted":false,"collapsed":true},"cell_type":"code","source":"(X_train, y_train), (X_val, y_val), words, word_to_index = load_train_dataset(\"../input/quora-question-pairs/train.csv\")","execution_count":31,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ebae05482f296a9fa0ed15f518641f257b6fc502","collapsed":true},"cell_type":"code","source":"X_train[1].shape","execution_count":32,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d274a9d94637af5d7d0f1291abac1660c872ec94"},"cell_type":"code","source":"def load_glove(embedding_dim):\n    embeddings_index = {}    \n    f = open(os.path.join(GLOVE_DIR, 'glove.6B.{}d.txt'.format(embedding_dim)))\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs        \n    f.close()\n    return embeddings_index\n\ndef emb2matrix(words, embeddings, embedding_dim):\n    \"\"\"create a matrix from the word embedding\"\"\"\n    matrix = np.zeros((len(words), embedding_dim))\n    missing = 0 # word is in vocab but we don't have embedding.\n    \n    # first word is UNK token. leave the embedding to 0 for UNKNOWN words\n    for i, word in enumerate(words[1:], 1):\n        vec = embeddings.get(word)\n        if vec is not None:\n            missing += 1\n            matrix[i, :] = vec\n        # FIXME if the word in vocab is not in word embedding, we are setting it to 0 (same as UNK), try random init.\n    print(\"Embedding Matrix: {}\".format(matrix.shape))\n    print(\"Words missings in embedding: {}\".format(missing))\n    return matrix\n","execution_count":33,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"122fc7d35a61ae220d44c32d0d470a8b320b415c","collapsed":true},"cell_type":"code","source":"EMBEDDING_DIM = 100\n\nword_embeddings = load_glove(EMBEDDING_DIM)\nembedding_matrix = emb2matrix(words, word_embeddings, EMBEDDING_DIM)","execution_count":34,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"de35eca261748594b1912642691c76c28a74834b","collapsed":true},"cell_type":"code","source":"print(\"Duplicate : {:.2%}\".format(y_train.sum()/y_train.shape[0]))","execution_count":35,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1c3421767cdd0cc92a152775a4830603c82ef0e3","collapsed":true},"cell_type":"code","source":"MAX_SEQ_LEN = X_train[0].shape[1]\nVOCAB_SIZE = len(words)","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"88932f63ef908a53f422b9f1f2da7e2333e518a5","collapsed":true,"trusted":false},"cell_type":"code","source":"from keras.layers import Input, Dense, Embedding, Dropout, Activation\nfrom keras.layers import Concatenate, Subtract, Multiply, Dot\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.models import Sequential, Model\n\ndef encoding_layer():\n    filters = 250\n    kernel_size = 3\n    \n    model = Sequential(name=\"encoding\")\n    model.add( Embedding(VOCAB_SIZE, # already includes UNK\n                         EMBEDDING_DIM,\n                         weights=[embedding_matrix],\n                         input_length=MAX_SEQ_LEN,\n                         trainable=False) )\n    model.add(Conv1D(filters, kernel_size, padding=\"valid\", activation=\"relu\", strides=1))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(128))\n    model.add(Dropout(0.2))\n    model.add(Activation(\"relu\"))\n    return model\n\ndef merge(towers):\n    t_diff = Subtract()(towers)\n    t_mul = Multiply()(towers)\n    return Concatenate()([*towers, t_diff, t_mul])\n    #return Dot(1, normalize=True)(towers)\n\ndef build_model():\n    encoder = encoding_layer()\n    encoder.summary()\n    \n    q1 = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n    q2 = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n    \n    t1 = encoder(q1)\n    t2 = encoder(q2)\n    \n    x = merge([t1, t2])\n    x = Dropout(0.5)(x)\n    x = Activation(\"relu\")(x)\n    x = Dense(64)(x)\n    x = Dropout(0.2)(x)\n    x = Activation(\"relu\")(x)    \n    x = Dense(1)(x)\n    out = Activation(\"sigmoid\")(x)\n    \n    model = Model([q1, q2], out)\n    model.summary()\n    return model","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"b953bc13b6e8adb147b951be3afa4bd254fdd78d","scrolled":false,"trusted":false,"collapsed":true},"cell_type":"code","source":"model = build_model()","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"a2ae33836b91d6057345a8bfdd83dcb245bea9e8","scrolled":false,"trusted":false,"collapsed":true},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=1024, epochs=5)","execution_count":50,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f9570dd61237a53982f1c7390f21efc84049d5e3"},"cell_type":"code","source":"model.save(\"01-cnn-model.h5\")","execution_count":64,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"da92019d9967f4e5b1e41dff644d3d87ef671076"},"cell_type":"code","source":"from keras.utils import Sequence\nfrom io import StringIO\nimport linecache\n\n# using a data generSequential for \nclass DataGenerator(Sequence):\n    def __init__(self, filepath, size, word_to_index, max_seq_len, batch_size=32):\n        self.filepath = filepath\n        self.size = size\n        self.word_to_index = word_to_index\n        self.max_seq_len = max_seq_len\n        \n        self.batch_size = batch_size\n        self.indexes = np.arange(size)\n\n    def __len__(self):\n        return (self.size // self.batch_size) + int((self.size % self.batch_size) > 0)\n    \n    def __getitem__(self, index):\n        \"\"\"Generate one batch.\"\"\"\n        buffer = StringIO()\n        for test_id in range(index * self.batch_size, (index + 1) * batch_size):\n            line = linecache.getline(self.filepath, test_id + 1 + 1 ) # since first line is header.\n            buffer.write(line)\n        \n        buffer.seek(0)\n        df = pd.read_csv(buffer, names=[\"test_id\", \"question1\", \"question2\"])\n        #print(df.shape)\n        X1 = preprocess(df.question1.fillna(\"\"), self.word_to_index, self.max_seq_len)\n        X2 = preprocess(df.question2.fillna(\"\"), self.word_to_index, self.max_seq_len)\n        return [X1, X2]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1d5e5a1e7e614d00b6fa323c8d83ef50891eb83a"},"cell_type":"code","source":"def preprocess(questions, word_to_index, max_len):\n    return pad_sequences( indexer(sentence2words(questions), word_to_index), max_len)\n\n# def load_test_dataset(filepath, word_to_index, max_seq_len):\n#     df = pd.read_csv(filepath, index_col=\"test_id\", nrows=10000)\n#     X1 = np.vstack( df.question1.fillna(\"\").apply(lambda x: preprocess([x,], word_to_index, max_seq_len)) )\n#     X2 = np.vstack( df.question2.fillna(\"\").apply(lambda x: preprocess([x,], word_to_index, max_seq_len)) )\n#     return [X1, X2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5bb29c44f9d34d03fa7195536f56b5aebbb65c49"},"cell_type":"code","source":"batch_size = 32\n# word_to_index = word_indexer(words)\ntest_data_generator = DataGenerator(\"../input/quora-question-pairs/test.csv\", \n                                    2345805, \n                                    word_to_index, \n                                    MAX_SEQ_LEN, \n                                    batch_size)\n\nnrows = 10000\nsteps = (nrows // batch_size) + int( (nrows % batch_size) > 0 )\n#steps = None\n\npredictions = np.squeeze( model.predict_generator(test_data_generator, steps, workers=6, use_multiprocessing=True, verbose=1) )\n#is_duplicate = (predictions > 0.5).astype(\"int32\")\n#print(is_duplicate.shape)\n#print(is_duplicate.sum() / is_duplicate.shape[0])\nprint(predictions.sum()/ len(predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9c7c5b06258b2d41fb0d22c8c51d6b55170a1772"},"cell_type":"code","source":"import csv\nimport gzip\n\nwith gzip.open(\"submission-v3.csv.gz\", \"wt\") as fp:\n    writer = csv.writer(fp)\n    writer.writerow([\"test_id\", \"is_duplicate\"])\n    writer.writerows(enumerate(predictions))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}