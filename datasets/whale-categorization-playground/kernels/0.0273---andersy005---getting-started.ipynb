{"nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["# Who can identify whale species in images?\n", "\n", "In this competition, we are given a [**multilabel classification**](https://jmread.github.io/talks/Tutorial-MLC-Porto.pdf) problem, which is basically a problem where we have to decide, given an image, which labels does it belong to?\n", "\n", "Our task is as follows:\n", ">\"For each Image in the test set, you may predict up to 5 labels for the whale Id\"\n", "\n", "In this notebook, we will:\n", "1. Look at the actual images to get a first impression of the data\n", "2. Cluster the images according to their pixel intensities to find potentially formed groups\n", "3. Generate a baseline bernoulli sample submission\n", "\n", "A standard approach to multilabel classification is to learn as many OVA (one vs all) models as there are distinct labels and then assign labels by the classifier output of each of the models, we'll get to that later.\n", "\n", "**If this notebook earns your upvote, please upvote this :)**\n"], "metadata": {}}, {"cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "import os\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "execution_count": null, "metadata": {"_uuid": "d2fd7390691cca938af31db97af2a2c271e8334e", "_cell_guid": "44ed5561-4741-4c87-8dd2-bc7e7098c934"}}, {"cell_type": "markdown", "source": ["# Load the dataset"], "metadata": {"collapsed": true, "_uuid": "5d941f2545e46553e3ef014ee443d508c7c8afe7", "_cell_guid": "e5ffc9a3-2479-4bd2-b771-e948365d9668"}}, {"cell_type": "code", "source": ["# Get the list of training files \n", "train = os.listdir('../input/train')\n", "# Get the list of test files\n", "test = os.listdir('../input/test')\n", "\n", "print(\"Total number of training images: \",len(train))\n", "print(\"Toal number of test images: \",len(test))"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "50485b1a30130295c60802d13ac221085bd1abf7", "_cell_guid": "f9c4a04a-2ba7-42df-8ef7-f33f97772239"}}, {"cell_type": "code", "source": ["sample = pd.read_csv('../input/sample_submission.csv')\n", "print(sample.shape)\n", "sample.head()"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["# load training labels into a pandas dataframe\n", "train_labels = pd.read_csv('../input/train.csv')\n", "train_labels.head()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "892977b7b5bd0eaa74b39b9ca0e45dcb73528070", "_cell_guid": "02570680-b42e-4bf1-80da-20ba3f714fe7"}}, {"cell_type": "code", "source": ["train_labels.info()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "9639b2123a87f7cdd0c9203fa8cf90a39e028328", "_cell_guid": "82e20011-0392-440e-ab89-c2fe4245a402"}}, {"cell_type": "markdown", "source": ["# Id counts\n", "Let's count all of the ids."], "metadata": {}}, {"cell_type": "code", "source": ["all_labels = train_labels['Id']\n", "unique_labels = all_labels.unique()"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["print(\"There are {} unique IDs\".format(unique_labels.shape[0]))"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["print(\"There are {} non unique IDs\".format(all_labels.shape[0]))"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["print(\"Average number of labels per image {}\".format(1.0*all_labels.shape[0]/train_labels.shape[0]))"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["all_ids = [item for sublist in list(train_labels['Id'].apply(lambda row: row.split(\" \")).values) for item in sublist]\n", "print('total of {} non-unique tags in all training images'.format(len(all_ids)))\n", "print('average number of labels per image {}'.format(1.0*len(all_ids)/train_labels.shape[0]))\n"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["Now, let's do the actual counting. We are going to use pandas dataframe groupby method for that.  "], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["ids_counted_and_sorted = pd.DataFrame({'Id': all_labels}).groupby('Id')\\\n", "                            .size().reset_index().sort_values(0, ascending=False)\n", "ids_counted_and_sorted.head(20)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["There are only a few ids that occur very often in the data:\n", "1.  new_whale\n", "2.  w_1287bfc\n", "3. w_98baff9\n"], "metadata": {}}, {"cell_type": "markdown", "source": ["\n", "# Submission from training tag counts\n", "\n", "It is time for the fun part. Let's take the training id distribution and sample from it as a prior for our test data. For that we will configure a bernoulli distribution for each sample with the observed training frequency and sample from that for each test image. With that we'll generate a submission without ever looking at the actual images.\n"], "metadata": {}}, {"cell_type": "code", "source": ["from scipy.stats import bernoulli"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["id_probas = ids_counted_and_sorted[0].values / (ids_counted_and_sorted[0].values.sum())\n", "indicators = np.hstack([bernoulli.rvs(p, 0, sample.shape[0]).reshape(sample.shape[0], 1) for p in id_probas])"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["indicators = np.array(indicators)\n", "indicators.shape"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["indicators[:10,:]"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["sorted_ids = ids_counted_and_sorted['Id'].values\n", "all_test_ids = []"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["for index in range(indicators.shape[0]):\n", "    all_test_ids.append(' '.join(list(sorted_ids[np.where(indicators[index, :] == 1)[0]])))"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["len(all_test_ids)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["sample['Id'] = all_test_ids\n", "sample.head()\n", "sample.to_csv('bernoulli_submission.csv', index=False)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["!ls"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": [], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}], "metadata": {"language_info": {"name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.4", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}}