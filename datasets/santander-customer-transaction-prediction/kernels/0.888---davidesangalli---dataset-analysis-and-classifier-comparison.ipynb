{"cells":[{"metadata":{},"cell_type":"markdown","source":"**PREPROCESS AND TRAINING**"},{"metadata":{},"cell_type":"markdown","source":"In this kernel I'm gonna make a classifier comparison, in order to understand which is the most suitable classifier for this calssification problem."},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"Here I'm using these classification model: RandomForest, Gaussian Naive Bayes and MLP. Basically, my idea is to use different classification algorithms that belong to different classification categories.\nRandomForest belongs to tree-based algorithms, GNB to the probabilistic algorithms and MLP is a kind of Neural Network (more or less)."},{"metadata":{"trusted":true,"_uuid":"791e328d719546e7e068c86513d29e7925898e4e","_kg_hide-input":false},"cell_type":"code","source":"#importing all the necessary libraries\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_curve,auc,precision_recall_fscore_support\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\nimport csv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/train.csv\")\n#dropping the ID code, which is useless for our classification task\ndf=df.drop(['ID_code'],axis=1)\n\n#Y is a list in which all the target values are stored\nY=[]\n\nfor i in range(len(df)):\n    Y.append(df.iloc[i]['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'].value_counts().plot.bar()\n#dropping the target, because it is not an explainatory feature\ndf=df.drop(['target'],axis=1)\n\n#printing the correlation matrix among the features\nprint(df.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is no strong correlation among the explainatory variable, so we can assume that they are indipendent from each other. All the features have almost the same importance, so they all could be used to train our classification models.\nMaybe, I will upload a new kernel where feauture selection will be implemented, and just the features that are most correlated with the target variable will be used during the training phase. For now, let's use all the features (although it may be a little bit time consuming and computational demanding).\nBesides, from the plot above, we can figure out that this dataset is highly unbalanced. Truth to be told, it doesn't really matter, because we are interested in ROC Curve.\n\n**ROC Curve depicts the performance of a classifier, regardless to the target class distribution**\n\nThere is no need, in my opinion, to perform oversampling (with SMOTE technique or whatever). Besides, with oversampling, we could end up with a (little) biased dataset, because some samples would be \"artificially\" added.\nOn the other hand, downsampling could be performed, but it is not necessary either.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#getting all the column headers of the dataset\nindex_list=df.columns.values\n\nX=[]\naux_list=[]\n\nfor i in range(0,len(df)):\n    for j in range(len(index_list)):\n        aux_list.append(df.iloc[i][index_list[j]])\n    X.append(aux_list)\n    aux_list=[]\n    \n#X is a list of lists where all the values of explainatory variables are stored. It contains all the values\n#for each row of our training set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting into train and validation set\nX_train, X_val, y_train, y_val = train_test_split(X,Y,test_size=0.33,shuffle=False)\n\n#scaling the features using the standard scaler (so that variables have mean=0 and std=1)\nscaler=StandardScaler()\nX_train_std=scaler.fit_transform(X_train)\nX_val_std=scaler.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We splitted the dataset into train and validation sets and then we used a StandardScaler to standardize our data (with mean=0 and std=1), so that the training phase can be a little faster.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_nb=GaussianNB()\nclf_nb.fit(X_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_mlp=MLPClassifier()\nclf_mlp.fit(X_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf_svm=SVC(kernel='rbf',probability=True)\n#clf_svm.fit(X_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf=RandomForestClassifier()\nclf_rf.fit(X_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We trained our different classifiers, so we are ready to test how the  perform on our validation set.\nRemember that we are interested in ROC Curve, so we need to predict **the probabilities**, so in the next code cell we are using the *predict_proba* function."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores_nb=clf_nb.predict_proba(X_val_std)\ny_scores_mlp=clf_mlp.predict_proba(X_val_std)\ny_scores_rf=clf_rf.predict_proba(X_val_std)\n\n#y_scores_svm=clf_svm.predict_proba(X_val_std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we calculated the probabilities for each classifier, we can use them to generate the ROC Curves.\nWe are interested in people who made a transaction, so we need to specify that the positive label is 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"false_positive_rate_nb, true_positive_rate_nb, thresholds_nb = roc_curve(y_val,y_scores_nb[:,1],pos_label=1)\nfalse_positive_rate_mlp, true_positive_rate_mlp, thresholds_mlp = roc_curve(y_val,y_scores_mlp[:,1],pos_label=1) \nfalse_positive_rate_rf, true_positive_rate_rf, thresholds_rf = roc_curve(y_val,y_scores_rf[:,1],pos_label=1)\n\n#false_positive_rate_svm, true_positive_rate_svm, thresholds_svm = roc_curve(y_val,y_scores_svm[:,1],pos_label=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we calculate the values of AUCs for each classifier and then we print them out"},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_nb=auc(false_positive_rate_nb,true_positive_rate_nb)\nroc_auc_mlp=auc(false_positive_rate_mlp,true_positive_rate_mlp)\nroc_auc_rf=auc(false_positive_rate_rf,true_positive_rate_rf)\n\n#roc_auc_svm=auc(false_positive_rate_svm,true_positive_rate_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Area under curve of Naive Bayes:\",roc_auc_nb)\nprint(\" \")\nprint(\"Area under curve of MultiLayer Perceptron:\",roc_auc_mlp)\nprint(\" \")\nprint(\"Area under curve of Random Forest:\",roc_auc_rf)\n\n#print(\"Area under curve of Support Vector Machine:\",roc_auc_svm)\n#print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the code cell below, we just plot the ROC Curves and the AUCs related to the different calssifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Receiver Operating Characteristic Comparison')\nplt.plot(false_positive_rate_nb,true_positive_rate_nb, 'b', label = 'AUC GNB = %0.2f' % roc_auc_nb)\nplt.plot(false_positive_rate_mlp,true_positive_rate_mlp, 'r', label = 'AUC MLP = %0.2f' % roc_auc_mlp)\nplt.plot(false_positive_rate_rf,true_positive_rate_rf, 'g', label = 'AUC RF = %0.2f' % roc_auc_rf)\n\n#plt.plot(false_positive_rate_svm,true_positive_rate_svm, 'y', label = 'AUC SVM = %0.2f' % roc_auc_svm)\n\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the ROC Curve plot above, it is clear that the ROC Curve related to the GNB is the best one, without any doubt, in fact its AUC is 0.89.\nIt seems that Gaussian Naive Bayes performs well on this dataset. This is probably because the variables are indipendent from each other, as shown above.\n\nNB: The AUC value of GNB is almost the same as the one reached by LGBM (the most optimized LGBM algorithms reached an AUC value on the validation set which is more or less around 0.90 or maybe 0.91), but training a GNB is less time consuming and far less computational demanding.\nSo, at the end of the day, I think that Gaussian Naive Bayes, for the reasons mentioned above, is the most suitable classifier for this problem. "},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"**TESTING**"},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to use the GNB classifier to predict the samples of our test set.\nAfter reading the test file, we save the id codes in a list and then we drop them out, because we don't need them for ur prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test=pd.read_csv(\"../input/test.csv\")\n\nid_codes=df_test['ID_code'].tolist()\n\ndf_test=df_test.drop(['ID_code'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the code below, we do the same thing as before, storing the test values in a list called X_test, and after that, we standardize our test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"index_list_new=df_test.columns.values\n\nX_test=[]\naux_list_new=[]\n\nfor i in range(len(df_test)):\n    for j in range(len(index_list_new)):\n        aux_list_new.append(df_test.iloc[i][index_list_new[j]])\n    X_test.append(aux_list_new)\n    aux_list_new=[]\n\nX_test_std=scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After doing the preprocessing, we are finally ready to predict the probability of each sample in our test set to be a person who made a transaction. Besides, here we predict also the class which each sample belongs to. Then, we store the id code, the probability and the class of each test sample in a file called *sample_submission.csv*. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores_test_nb=clf_nb.predict_proba(X_test_std)[:,1]\ny_test_pred=clf_nb.predict(X_test_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"ID_code\": id_codes})\nsubmission[\"target\"] = y_scores_test_nb\nsubmission.to_csv(\"Sample_Submission.csv\",index=False)","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6a917396cbf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ID_code\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mid_codes\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_scores_test_nb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample_Submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"NB: I commented the part related to SVC, because it is too computationally and time expensive. Besides, the GNB is still better than SVC."},{"metadata":{},"cell_type":"markdown","source":"This is my first Kaggle competition, I hope you like it :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}