{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class FeatureAggregator(object):\n\n    \"\"\"Feature aggregator - automated feature aggregation method.\n    Two ways of usage, either selected aggregations can be applied onto\n    numerical and categorical columns or specific combinations of aggregates\n    can be set for each column.\n\n    # Arguments:\n        df: (pandas DataFrame), DataFrame to create features from.\n        aggregates_cat: (list), list containing aggregates for\n            categorical features\n        aggregates_num: (list), list containing aggregates for\n            numerical features.\n\n    \"\"\"\n\n    def __init__(self,\n                 df,\n                 aggregates_cat=['mean', 'std'],\n                 aggregates_num=['mean', 'std', 'sem', 'min', 'max']):\n\n        self.df = df.copy()\n        self.aggregates_cat = aggregates_cat\n        self.aggregates_num = aggregates_num\n\n    def process_features_batch(self,\n                               categorical_columns=None,\n                               categorical_int_columns=None,\n                               numerical_columns=None,\n                               to_group=['SK_ID_CURR'], prefix='BUREAU'):\n        \"\"\"Process, group features in batch.\n\n        # Arguments:\n            categorical_columns: (list), list of categorical columns, which need\n            to be label-encoded (factorized).\n            categorical_int_columns: (list), list of categorical columns, which\n            are already of integer type.\n            numerical_columns: (list), list of numerical columns.\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for columns names.\n\n        # Returns:\n            df_cat/df_num: (pandas DataFrame), DataFrame with aggregated columns.\n\n        \"\"\"\n\n        assert isinstance(\n            to_group, list), 'Variable to group by must be of type list.'\n\n        if categorical_columns is not None:\n            assert len(categorical_columns) > 0, 'No columns to encode.'\n            self.categorical_features_factorize(categorical_columns)\n            df_cat = self.create_aggregates_set(\n                columns=categorical_columns,\n                aggregates=self.aggregates_cat,\n                to_group=to_group, prefix=prefix)\n            print('\\nAggregated df_cat shape: {}'.format(df_cat.shape))\n            return df_cat\n\n        if categorical_int_columns is not None:\n            assert len(categorical_int_columns) > 0, 'No columns to encode.'\n            df_cat = self.create_aggregates_set(\n                columns=categorical_int_columns,\n                aggregates=self.aggregates_cat,\n                to_group=to_group, prefix=prefix)\n            print('\\nAggregated df_cat int shape: {}'.format(df_cat.shape))\n            return df_cat\n\n        if numerical_columns is not None:\n            assert len(numerical_columns) > 0, 'No columns to encode.'\n            df_num = self.create_aggregates_set(\n                columns=numerical_columns,\n                aggregates=self.aggregates_num,\n                to_group=to_group, prefix=prefix)\n            print('\\nAggregated df_num shape: {}'.format(df_num.shape))\n            return df_num\n\n        return\n\n    def process_features_selected(self,\n                                  aggregations,\n                                  categorical_columns,\n                                  to_group=['SK_ID_CURR'], prefix='BUREAU'):\n        \"\"\"Process, group features for selected combinations of aggregates\n        and columns.\n\n        # Arguments:\n            categorical_columns: (list), list of categorical columns, which need\n            to be label-encoded (factorized).\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for columns names.\n\n        # Returns:\n            df_agg: (pandas DataFrame), DataFrame with aggregated columns.\n\n        \"\"\"\n\n        assert isinstance(\n            to_group, list), 'Variable to group by must be of type list.'\n\n        if categorical_columns:\n            # Provide categorical_columns argument if some features need to be factorized.\n            self.categorical_features_factorize(categorical_columns)\n\n        df_agg = self.create_aggregates_set(\n            aggregations=aggregations,\n            to_group=to_group, prefix=prefix)\n\n        print('\\nAggregated df_agg shape: {}'.format(df_agg.shape))\n\n        return df_agg\n\n    def create_aggregates_set(self,\n                              aggregations=None,\n                              columns=None,\n                              aggregates=None,\n                              to_group=['SK_ID_CURR'],\n                              prefix='BUREAU'):\n        \"\"\"Create selected aggregates.\n\n        # Arguments:\n            aggregations: (dict), dictionary specifying aggregates for selected columns.\n            columns: (list), list of columns to group for batch aggregation.\n            aggregates: (list), list of aggregates to apply on columns argument\n            for batch aggregation.\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for columns names.\n\n        # Returns:\n            df_agg: (pandas DataFrame), DataFrame with aggregated columns.\n\n        \"\"\"\n\n        assert isinstance(\n            to_group, list), 'Variable to group by must be of type list.'\n\n        if aggregations is not None:\n            print('Selected aggregations:\\n{}\\n.'.format(aggregations))\n            df_agg = self.df.groupby(\n                to_group).agg(aggregations)\n\n        if columns is not None and aggregates is not None:\n            print('Batch aggregations on columns:\\n{}\\n.'.format(columns))\n            df_agg = self.df.groupby(\n                to_group)[columns].agg(aggregates)\n\n        df_agg.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in df_agg.columns.tolist()])\n        df_agg = df_agg.reset_index()\n\n        return df_agg\n\n    def get_column_types(self):\n        \"\"\"Select categorical (to be factorized), categorical integer and numerical\n        columns based on their dtypes. This facilitates proper grouping and aggregates selection for\n        different types of variables.\n        Categorical columns needs to be factorized, if they are not of\n        integer type.\n\n        # Arguments:\n            self.df: (pandas DataFrame), DataFrame to select variables from.\n\n        # Returns:\n            categorical_columns: (list), list of categorical columns which need factorization.\n            categorical_columns_int: (list), list of categorical columns of integer dtype.\n            numerical_columns: (list), list of numerical columns.\n        \"\"\"\n\n        categorical_columns = [\n            col for col in self.df.columns if self.df[col].dtype == 'object']\n        categorical_columns_int = [\n            col for col in self.df.columns if self.df[col].dtype == 'int']\n        numerical_columns = [\n            col for col in self.df.columns if self.df[col].dtype == 'float']\n\n        categorical_columns = [\n            x for x in categorical_columns if 'SK_ID' not in x]\n        categorical_columns_int = [\n            x for x in categorical_columns_int if 'SK_ID' not in x]\n\n        print('DF contains:\\n{} categorical object columns\\n{} categorical int columns\\n{} numerical columns.\\n'.format(\n            len(categorical_columns), len(categorical_columns_int), len(numerical_columns)))\n\n        return categorical_columns, categorical_columns_int, numerical_columns\n\n    def categorical_features_factorize(self, categorical_columns):\n        \"\"\"Factorize categorical columns, which are of non-number dtype.\n\n        # Arguments:\n            self.df: (pandas DataFrame), DataFrame to select variables from.\n            Transformation is applied inplace.\n\n        \"\"\"\n\n        print('\\nCategorical features encoding: {}'.format(categorical_columns))\n\n        for col in categorical_columns:\n            self.df[col] = pd.factorize(self.df[col])[0]\n\n        print('Categorical features encoded.\\n')\n\n        return\n\n    def check_and_save_file(self, df, filename, dst='../input/'):\n        \"\"\"Utility function to check if there isn't a file with the same name already.\n\n        # Arguments:\n            df: (pandas DataFrame), DataFrame to save.\n            filename: (string), filename to save DataFrame with.\n\n        \"\"\"\n\n        filename = '{}{}.pkl'.format(dst, filename)\n        if not os.path.isfile(filename):\n            print('Saving: {}'.format(filename))\n            df.to_pickle('{}'.format(filename))\n        return\n\n\ndef feature_aggregator_on_df(df,\n                             aggregates_cat,\n                             aggregates_num,\n                             to_group,\n                             prefix,\n                             suffix='basic',\n                             save=False,\n                             categorical_columns_override=None,\n                             categorical_int_columns_override=None,\n                             numerical_columns_override=None):\n    \"\"\"Wrapper for FeatureAggregator to process dataframe end-to-end using batch aggregation.\n    It takes lists of aggregates for categorical and numerical features, which are created for\n    selected column (to_group), by which data is grouped. In addition to that, prefix and suffix can\n    be provided to facilitate column naming.\n    _override arguments can be used if only selected subset of each type of columns should\n    be aggregated. If those are not provided, FeatureAggregator processes all columns for each type.\n\n        # Arguments:\n            aggregates_cat: (list), list of aggregates to apply to categorical features.\n            aggregates_num: (list), list of aggregates to apply to numerical features.\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for column names.\n            suffix: (string), suffix for filename.\n            save: (boolean), whether to save processed DF.\n            categorical_columns_override: (list), list of categorical columns\n            to override default, inferred list.\n            categorical_int_columns_override: (list), list of categorical integer\n            columns to override default, inferred list.\n            numerical_columns_override: (list), list of numerical columns\n            to override default, inferred list.\n\n        # Returns:\n            to_return: (list of pandas DataFrames), DataFrames with aggregated columns,\n            one for each type of column types. This is due to the fact that not every\n            raw dataframe may contain all types of columns.\n\n        \"\"\"\n\n    assert isinstance(aggregates_cat, list), 'Aggregates must be of type list.'\n    assert isinstance(aggregates_num, list), 'Aggregates must be of type list.'\n\n    t = time.time()\n    to_return = []\n\n    column_base = ''\n    for i in to_group:\n        column_base += '{}_'.format(i)\n\n    feature_aggregator_df = FeatureAggregator(\n        df=df,\n        aggregates_cat=aggregates_cat,\n        aggregates_num=aggregates_num)\n\n    print('DF prefix: {}, suffix: {}'.format(prefix, suffix))\n    print('Categorical aggregates - {}'.format(aggregates_cat))\n    print('Numerical aggregates - {}'.format(aggregates_num))\n\n    df_cat_cols, df_cat_int_cols, df_num_cols = feature_aggregator_df.get_column_types()\n\n    if categorical_columns_override is not None:\n        print('Overriding categorical_columns.')\n        df_cat_cols = categorical_columns_override\n    if categorical_columns_override is not None:\n        print('Overriding categorical_int_columns.')\n        df_cat_int_cols = categorical_int_columns_override\n    if categorical_columns_override is not None:\n        print('Overriding numerical_columns.')\n        df_num_cols = numerical_columns_override\n\n    if len(df_cat_cols) > 0:\n        df_curr_cat = feature_aggregator_df.process_features_batch(\n            categorical_columns=df_cat_cols,\n            to_group=to_group, prefix=prefix)\n        if save:\n            feature_aggregator_df.check_and_save_file(\n                df_curr_cat, '{}_cat_{}_{}'.format(prefix, column_base, suffix))\n        to_return.append(df_curr_cat)\n        del df_curr_cat\n        gc.collect()\n\n    if len(df_cat_int_cols) > 0:\n        df_curr_cat_int = feature_aggregator_df.process_features_batch(\n            categorical_int_columns=df_cat_int_cols,\n            to_group=to_group, prefix=prefix)\n        if save:\n            feature_aggregator_df.check_and_save_file(\n                df_curr_cat_int, '{}_cat_int_{}_{}'.format(prefix, column_base, suffix))\n        to_return.append(df_curr_cat_int)\n        del df_curr_cat_int\n        gc.collect()\n\n    if len(df_num_cols) > 0:\n        df_curr_num = feature_aggregator_df.process_features_batch(\n            numerical_columns=df_num_cols,\n            to_group=to_group, prefix=prefix)\n        if save:\n            feature_aggregator_df.check_and_save_file(\n                df_curr_num, '{}_num_{}_{}'.format(prefix, column_base, suffix))\n        to_return.append(df_curr_num)\n        del df_curr_num\n        gc.collect()\n\n    print('\\nTime it took to create features on df: {:.3f}s'.format(\n        time.time() - t))\n\n    return to_return\n\n\ndef feature_aggregator_on_df_selected(df,\n                                      aggregations,\n                                      to_group,\n                                      prefix,\n                                      suffix='basic',\n                                      save=False):\n    \"\"\"Wrapper for FeatureAggregator to process dataframe end-to-end using selected\n    aggregates/columns combinations.\n    It takes dictionary of aggregates/columns combination for selected features,\n    which are created for selected column (to_group), by which data is grouped.\n    In addition to that, prefix and suffix can be provided to facilitate column naming.\n\n        # Arguments:\n            aggregations: (dict), dictionary containing combination of columns/aggregates.\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for column names.\n            suffix: (string), suffix for filename.\n            save: (boolean), whether to save processed DF.\n\n        # Returns:\n            to_return: (list of pandas DataFrames), DataFrames with aggregated columns,\n            one for each type of column types. This is due to the fact that not every\n            raw dataframe may contain all types of columns.\n\n        \"\"\"\n\n    assert isinstance(\n        to_group, list), 'Variable to group by must be of type list.'\n\n    t = time.time()\n    to_return = []\n\n    column_base = ''\n    for i in to_group:\n        column_base += '{}_'.format(i)\n\n    feature_aggregator_df = FeatureAggregator(df=df)\n\n    print('DF prefix: {}, suffix: {}'.format(prefix, suffix))\n\n    df_cat_cols, df_cat_int_cols, df_num_cols = feature_aggregator_df.get_column_types()\n\n    if len(df_cat_cols) > 0:\n        df_aggs = feature_aggregator_df.process_features_selected(\n            aggregations=aggregations,\n            categorical_columns=df_cat_cols,\n            to_group=to_group,\n            prefix=prefix)\n    else:\n        df_aggs = feature_aggregator_df.process_features_selected(\n            aggregations=aggregations,\n            to_group=to_group,\n            prefix=prefix)\n\n    if save:\n        feature_aggregator_df.check_and_save_file(\n            df_aggs, '{}_selected_{}_{}'.format(prefix, column_base, suffix))\n\n    to_return.append(df_aggs)\n    del df_aggs\n    gc.collect()\n\n    print('\\nTime it took to create features on df: {:.3f}s'.format(\n        time.time() - t))\n\n    return to_return\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c82c909a5c7c225c1aa275fd79d343f345605de0"},"cell_type":"markdown","source":"## I. Idea\n\nGroup features, aggregates, are one of the most powerful way to capture relationships between variables in the dataset. Sometimes it is possible to group by target variable and thus provide model with direct information about it (although one should be careful when doing that in order not to introduce a leak, only training data subset can be grouped this way).\nFor grouping of other variables, whole dataset can be used, as you are given both train and test data.\n**Important!** - this concerns Kaggle competitions, one should not do this in real-life ML, as you never know what exactly will the distribution of variables in test data be.\nIn this kernel I try to create an end-to-end feature engineering solution based on groupby features.\n\nWhole process can be divided into a few steps:\n  1. Selection of columns for each type:\n    1. `categorical` - categorical features which must be encoded (standard label encoding is used).\n    2. `categorical_int` - categorical features which are alredy in integer dtype, encoding is not needed.\n    3. `numerical` - numerical features\n  2. Factorization of `categorical` columns, if needed.\n  3. Creation of aggregates with columns renaming for each type of columns.\n  4. Resulting DataFrame is saved (if possible, will not work in Kaggle Kernels).\n\nThere are two ways of aggregated features creation:\n  1. Batch aggregation: all columns from DataFrame are processed, each column is appended to one of three types, `categorical` columns are factorized is there's a need and selected aggregates are applied to each column type. There is a distinction between aggregates for categorical columns and those for numerical, as each type requires a different approach.\n  2. Selected aggregation: combination of aggregates/columns should be provided in form of a dictionary, where for each column aggregates are specified in a list.\n\n## II. Setup\n\nFirst, we need to choose aggregates, which will be used for grouping categorical and numerical variables.\nFor numerical variables `aggs_num_basic` will be used, for categoricals - `aggs_cat_basic`.\nThose types of aggregations can be extended further, as is shown in `aggs1` list.\n\n```python\naggs_num_basic = ['mean', 'min', 'max', 'std', 'sem', 'sum']\naggs_cat_basic = ['mean', 'std', 'sum']\naggs1 = ['mean', 'median', 'min', 'max', 'count', 'std', 'sem', 'sum', 'mad']\n```"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d3d6d82b7825e3469b134435712ce242f2564414"},"cell_type":"code","source":"aggs1 = ['mean', 'median', 'min', 'max', 'count', 'std', 'sem', 'sum', 'mad']\naggs2 = ['mean', 'median', 'min', 'max', 'count', 'std', 'sem', 'sum']\n\naggs_num_basic = ['mean', 'min', 'max', 'std', 'sem', 'sum']\naggs_cat_basic = ['mean', 'std', 'sum']\n\naggs_num = aggs_num_basic\naggs_cat = aggs_cat_basic","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1e01b6573554063ded73c931f1257ee1b0d3edf"},"cell_type":"markdown","source":"## Loading the data:"},{"metadata":{"trusted":true,"_uuid":"39831853422866ab46c3a9957dc813aca80ca745","collapsed":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/application_train.csv\") \ntest = pd.read_csv(\"../input/application_test.csv\")\n\nbureau = pd.read_csv(\"../input/bureau.csv\")\nbureau_bal = pd.read_csv('../input/bureau_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"54b914cb12d7a308a40f267a8fc7d44d497939bc"},"cell_type":"markdown","source":"## IIIa. Bureau balance & Bureau, batch aggregation:\n\n\nAggregates for Bureau Balance will be created at first.\nHere, `bureau_bal` DF is grouped by `SK_ID_BUREAU`, which will serve as a basis for further merge of this set to `bureau` data. `aggs_cat` and `aggs_num` are used for categorical and numerical features aggregations.\n`bureau_balance` is the prefix for column names and `basic` is the suffix for filename, if resulting DFs should be saved.\n\nSecond step is aggregating the `bureau` data. We begin with merging `bureau_bal` data into the Bureau set, as this will enable engineering of the features from Bureal Balance dataset in a way enabling their merge to main training DataFrame. `bureau` is grouped by `SK_ID_CURR`, as this is the ID column in `train`."},{"metadata":{"trusted":true,"_uuid":"6b75837ab26cc2fa2efd5481512b3b5e9974b906"},"cell_type":"code","source":"%%time\n\nbureau_bal_dfs = feature_aggregator_on_df(\n    bureau_bal, aggs_cat, aggs_num, ['SK_ID_BUREAU'], 'bureau_balance', 'basic', save=False)\n\nbureau_bal_dfs[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df8642a8f94451ebf91bc37a5ddc1a6f82df77cf"},"cell_type":"code","source":"%%time\n\nbureau_ = bureau.merge(bureau_bal_dfs[0], how='left', on='SK_ID_BUREAU', copy=False)\nbureau_ = bureau_.merge(bureau_bal_dfs[1], how='left', on='SK_ID_BUREAU', copy=False)\n\nbureau_dfs = feature_aggregator_on_df(\n    bureau_, aggs_cat, aggs_num, ['SK_ID_CURR'], 'bureau', 'basic', save=False)\n\nbureau_dfs[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c9feab9c0812cb95875e2acb3124d86cb427c9c"},"cell_type":"markdown","source":"## IIIb. Previous applications, selected aggregation:\n\nThe other way of aggregating features enabled by FeatureAggregator is __selected_aggregation__. Here, a dictionary is of selected combinations between columns and aggregates for them is specified.\nDF processed as example is `previous_application`. We start with replacing some of the values, which do not make sense with `np.nan` and create one new column, `APP_CREDIT_PERC` - those are steps made in one of the other kernels :).\nAfterwards, a dictionary specifying aggregations is created in `num_aggregations`, as this is one for numerical columns.\nDF is grouped according to this dictionary and is then grouped by `SK_ID_CURR` to facilitate merging onto the train table."},{"metadata":{"trusted":true,"_uuid":"83a99d5e0507f7835d7ce610b4efba6b32ebb535","scrolled":true},"cell_type":"code","source":"%%time\n\nprev = pd.read_csv(\"../input/previous_application.csv\")\nprev = prev.drop(['SK_ID_PREV'], axis=1)\n\nprev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\nprev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\nprev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\nprev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\nprev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\nprev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n\n\nnum_aggregations = {\n        'AMT_ANNUITY': ['min', 'max', 'mean'],\n        'AMT_APPLICATION': ['min', 'max', 'mean'],\n        'AMT_CREDIT': ['min', 'max', 'mean'],\n        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'DAYS_DECISION': ['min', 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n    }\n\nprev_selected_aggs = feature_aggregator_on_df_selected(\n    prev, num_aggregations, to_group=['SK_ID_CURR'], prefix='prev', suffix='basic_selected', save=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdd87ec11e178b8d98faaddc1e9d58295c9e159e"},"cell_type":"markdown","source":"### and batch aggregations examples for the rest of the tables..."},{"metadata":{"trusted":true,"_uuid":"919286b5d6bbc9860aaa345190a2fb9ec7354f66"},"cell_type":"code","source":"cred_card_bal = pd.read_csv(\"../input/credit_card_balance.csv\")\ncred_card_bal = cred_card_bal.drop(['SK_ID_PREV'], axis=1)\n\ncred_card_bal_dfs = feature_aggregator_on_df(\n    cred_card_bal, aggs_cat, aggs_num, ['SK_ID_CURR'], 'cred_card_balance', 'basic', save=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"914d124891fb397aef95a497771875a652eaf579"},"cell_type":"code","source":"pos_cash_bal = pd.read_csv(\"../input/POS_CASH_balance.csv\")\npos_cash_bal = pos_cash_bal.drop(['SK_ID_PREV'], axis=1)\n\npos_cash_bal_dfs = feature_aggregator_on_df(\n    pos_cash_bal, aggs_cat, aggs_num, ['SK_ID_CURR'], 'pos_cash_balance', 'basic', save=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddf425564f4e478cca4f242d193602d350c43f65"},"cell_type":"code","source":"ins = pd.read_csv('../input/installments_payments.csv')\nins = ins.drop(['SK_ID_PREV'], axis=1)\n\nins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\nins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\nins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\nins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\nins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\nins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n\nins_dfs = feature_aggregator_on_df(\n    ins, aggs_cat, aggs_num, ['SK_ID_CURR'], 'installments', 'basic', save=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5a343342b2088690813c7f531226f04e098df564"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}