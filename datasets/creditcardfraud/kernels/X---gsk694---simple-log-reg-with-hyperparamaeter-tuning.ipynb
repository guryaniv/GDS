{"nbformat": 4, "cells": [{"cell_type": "code", "metadata": {"_uuid": "b46cdaf8a772f964cbe00f5c21208b26442ccf1a", "_cell_guid": "0dbcf9ab-a99f-442c-8cc4-40db57dab44f"}, "execution_count": null, "source": ["\"\"\"\n", "USED SOME IDEAS FROM LEADERBOARD\n", "WILL IMPLEMENT THESE LATER : \n", "    RANDOM FORESTS\n", "    SVM WITH MODIFIED KERNELS\n", "    KNN\n", "    I have uploaded some implementations on github \n", "    here : https://github.com/sanjaykrishnagouda/Anomaly-Detection\n", "    \n", "    This dataset is a good place to practice and understand more about SVM\n", "    because the data is highly skewed, searching for best hyper parameters gives\n", "    some good intuition about SVM working.\n", "    Feel free to add suggestions/ comments\n", "\"\"\"\n", "\n", "import numpy as np\n", "import time,itertools\n", "import pandas as pd\n", "import operator\n", "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n", "from sklearn import linear_model\n", "from sklearn.svm import SVC, LinearSVC\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV,train_test_split,KFold, cross_val_score\n", "import sklearn,matplotlib\n", "import matplotlib.pyplot as plt\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report\n", "from sklearn.utils.testing import ignore_warnings, assert_raises\n", "%matplotlib inline\n", "def load(str):\n", "\tdata = pd.read_csv(str)\n", "\t# dropping two columns : \n", "\tdata['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\n", "\tdata = data.drop(['Time','Amount'], axis = 1)\n", "\tx = data.loc[:,data.columns != 'Class']\n", "\ty = data.loc[:,data.columns == 'Class']\n", "\treturn data,x,y\n", "\n", "def sampling_data(matrix, input, output):\n", "\t\n", "\ta = matrix[matrix.Class ==1]\n", "\tnumber_records_fraud = len(a)\n", "\tfraud_indices = np.array(a.index)\n", "\n", "\t# picking normal classes\n", "\n", "\tnonfraud_indices = matrix[matrix.Class == 0].index\n", "\n", "\t# selecting fraud number of normal samples\n", "\trandom_normal_samples = np.random.choice(nonfraud_indices,\n", "\t\tnumber_records_fraud,replace = False)\n", "\trandom_normal_samples = np.array(random_normal_samples)\n", "\n", "\tunder_sample_indices = np.concatenate([fraud_indices,random_normal_samples])\n", "\n", "\t# collecting corresponding data\n", "\n", "\tunder_sample_data = matrix.iloc[under_sample_indices,:]\n", "\tX_undersample = under_sample_data.iloc[:, under_sample_data.columns != 'Class']\n", "\ty_undersample = under_sample_data.iloc[:, under_sample_data.columns == 'Class']\n", "\n", "\n", "\tX_train, X_test, y_train, y_test = train_test_split(input,output,test_size = 0.3, random_state = 0)\n", "\n", "\tX_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample,y_undersample,\n", "\t\ttest_size = 0.3, random_state = 0)\n", "\n", "\treturn(X_train,y_train,X_train_undersample,y_train_undersample)\n", "\n", "def printing_Kfold_scores(x_train_data,y_train_data): # LOGISTIC REGRESSION WITH 7 FOLD CV\n", "\tstart = time.time()\n", "\tkf = KFold(n_splits = 5)\n", "\tc_param_range,t = [],0.0000001\n", "\t#t=0.0001\n", "\twhile t<=pow(10,2):\n", "\t\tc_param_range.append(t)\n", "\t\tt*=10\n", "\n", "\n", "\tresults_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n", "\tresults_table['C_parameter'] = c_param_range\n", "\tresults_table_svm = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n", "\tresults_table_svm['C_parameter'] = c_param_range\n", "\t# the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n", "\tj = 0\n", "\trecall_dict={}\n", "\trecall_dict_svm={}\n", "\tfor c_param in c_param_range:\n", "\n", "\t\trecall_accs = []\n", "\t\trecall_accs_svm = []\n", "\t\tfor iteration, (train,test) in enumerate(kf.split(x_train_data,y_train_data)):\n", "\t\t\tlr = LogisticRegression(C = c_param, penalty = 'l2')\n", "\t\t\tlr.fit(x_train_data.iloc[train],y_train_data.iloc[train].values.ravel())\n", "\t\t\ty_pred_undersample = lr.predict(x_train_data.iloc[test].values)\n", "\t\t\t# Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n", "\t\t\trecall_acc = recall_score(y_train_data.iloc[test].values,y_pred_undersample)\n", "\t\t\trecall_accs.append(recall_acc)\n", "\t        \n", "\n", "\t\trecall_dict[c_param]=np.mean(recall_accs)\n", "\t\tresults_table.loc[j,'Mean recall score'] = np.mean(recall_accs)\n", "\n", "\t\tj += 1\n", "\n", "\n", "\tbest_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n", "\t#print(\"USING Logistic Regression::\\nBest Mean: %f with inverse regularization strength %f\"%(max(recall_dict.items(),key = operator.itemgetter(1))[1],\n", "\t\t#max(recall_dict.items(),key = operator.itemgetter(1))[0]))\n", "\n", "\n", "\tlists2 = sorted(recall_dict.items())\n", "\tx2,y2 = zip(*lists2)\n", "\tplt.plot(x2,y2)\n", "\tplt.legend(['Logistic Regression'],loc='lower right')\n", "\tplt.draw()\n", "\t#return best_c,best_c_svm\n", "\tend = time.time()\n", "\tprint(\"Logistic regressions took %.2f\"%(end-start),\"seconds\\nBest C = \",best_c)\n", "\treturn best_c\n", "\n", "def using_SVM(x_train_data,y_train_data,k):\n", "    # This function is for using SVM with different kernels\n", "\tkernel = str(k)\n", "\tkf = KFold(n_splits = 5)\n", "\tc_param_range = []\n", "\tt=0.00001\n", "\twhile t<=204800:\n", "\t\tc_param_range.append(t)\n", "\t\tt*=2\n", "\n", "\n", "\tresults_table_svm = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n", "\tresults_table_svm['C_parameter'] = c_param_range\n", "\t# the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n", "\tj = 0\n", "\t\n", "\trecall_dict_svm={}\n", "\tfor c_param in c_param_range:\n", "\t\t\n", "\t\t#print('C parameter: ', c_param,'\\t kernel',kernel)\n", "\t\t\n", "\t\trecall_accs_svm = []\n", "\t\tfor iteration, (train,test) in enumerate(kf.split(x_train_data,y_train_data)):\n", "\t\t\tif kernel!='linear':\n", "\t\t\t\tclf = BaggingClassifier(SVC(C = c_param, kernel = kernel),n_jobs=-1)\n", "\t\t\tif kernel == 'linear':\n", "\t\t\t\tclf = BaggingClassifier(LinearSVC(C = c_param), n_jobs = 2)\n", "\t\t\tclf.fit(x_train_data.iloc[train],y_train_data.iloc[train].values.ravel())\n", "\n", "\t\t\ty_pred_undersample_svm = clf.predict(x_train_data.iloc[test].values)\n", "\n", "\t\t\trecall_acc_svm = recall_score(y_train_data.iloc[test].values,y_pred_undersample_svm)\n", "\t\t\trecall_accs_svm.append(recall_acc_svm)\n", "\n", "\n", "\t\trecall_dict_svm[c_param]=np.mean(recall_accs_svm)\n", "\t\tresults_table_svm.ix[j,'Mean recall score'] = np.mean(recall_accs_svm)\n", "\t\tj += 1\n", "\n", "\n", "\tbest_c_svm = results_table_svm.loc[results_table_svm['Mean recall score'].idxmax()]['C_parameter']\n", "\tprint(\"Best Mean: %f with C param =  %.4f using %s kernel\"%(max(recall_dict_svm.items(),key = operator.itemgetter(1))[1],\n", "\t\tmax(recall_dict_svm.items(),key = operator.itemgetter(1))[0],kernel))\n", "\treturn recall_dict_svm,best_c_svm\n", "\n", "def diff_kerns(x,y):\n", "\t\n", "\tkernels = ['rbf','linear','sigmoid','poly']\n", "\t\n", "\tarr =[]\n", "\tfor i in kernels:\n", "\t\tstart = time.time()\n", "\t\t#print(\"currently running \",i,\"kernel\")\n", "\t\ta=using_SVM(x,y,i)\n", "\t\ttemp_dict,temp_best = a[0],a[1]\n", "\t\tarr.append(temp_dict[temp_best])\n", "\t\tend = time.time()\n", "\t\tprint(i,\"took %.2f seconds for completion.\"%(end-start))\n", "\tplt.scatter([1,2,3,4],arr)\n", "\tplt.xticks([1,2,3,4],kernels)\n", "\tplt.ylabel(\"Mean Recall of 7 iterations\")\n", "\t\n", "\tplt.show()\n", "\n", "def RandomForest(x,y):\n", "\t#first lets try GMM\n", "\tpass\n", "def plot_confusion_matrix(cm, classes,\n", "\t\t\t\t\t\t\tnormalize=False,\n", "\t\t\t\t\t\t\ttitle='Confusion matrix',\n", "\t\t\t\t\t\t\tcmap=plt.cm.Blues):\n", "\t\"\"\"\n", "\tThis function prints and plots the confusion matrix.\n", "\tNormalization can be applied by setting `normalize=True`.\n", "\t\"\"\"\n", "\tplt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "\tplt.title(title)\n", "\tplt.colorbar()\n", "\ttick_marks = np.arange(len(classes))\n", "\tplt.xticks(tick_marks, classes, rotation=0)\n", "\tplt.yticks(tick_marks, classes)\n", "\n", "\tif normalize:\n", "\t\tcm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "\t\t#print(\"Normalized confusion matrix\")\n", "\telse:\n", "\t\t1#print('Confusion matrix, without normalization')\n", "\n", "    #print(cm)\n", "\n", "\tthresh = cm.max() / 2.\n", "\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "\t\tplt.text(j, i, cm[i, j],\n", "\t\t\t\t\thorizontalalignment=\"center\",\n", "\t\t\t\t\tcolor=\"white\" if cm[i, j] > thresh else \"black\")\n", "\n", "\tplt.tight_layout()\n", "\tplt.ylabel('True label')\n", "\tplt.xlabel('Predicted label')\n", "def main(_):\t\n", "\ta,b,c = load('../input/creditcard.csv')\n", "\ttemp = sampling_data(a,b,c)\n", "\t#printing_Kfold_scores(temp[0],temp[1]) # USING COMPLETE DATASET Logistic Regression\n", "\t#diff_kerns(temp[0],temp[1]) # SVM with various kernels on complete dataset.\n", "\tbest_c = printing_Kfold_scores(temp[2],temp[3]) # USING UNDERSAMPLED DATASET Logistic Regression\n", "\t#diff_kerns(temp[2],temp[3]) # SVM with various kernels, run for 5 iterations\n", "\tlr = LogisticRegression(C = best_c, penalty = 'l2')\n", "\tlr.fit(temp[2],temp[3].values.ravel())\n", "\ty_pred_undersample = lr.predict(temp[2].values)\n", "\n", "\t# Compute confusion matrix\n", "\tcnf_matrix = confusion_matrix(temp[3],y_pred_undersample)\n", "\tnp.set_printoptions(precision=2)\n", "\n", "\tprint(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n", "\n", "\t# Plot non-normalized confusion matrix\n", "\tclass_names = [0,1]\n", "\tplt.figure()\n", "\tplot_confusion_matrix(cnf_matrix\n", "\t\t\t\t\t\t\t, classes=class_names\n", "\t\t\t\t\t\t\t, title='Confusion matrix')\n", "\tplt.show()\n", "if __name__ == \"__main__\":\n", "\tmain(2)"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "4bce422373abcbe928e7e0233399f158a0b28377", "collapsed": true, "_cell_guid": "537ea028-6afb-4305-81eb-533128f9ff93"}, "execution_count": null, "source": [], "outputs": []}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python"}}, "nbformat_minor": 1}