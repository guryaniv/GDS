{"cells":[{"metadata":{"_uuid":"4d3fab5d21ae3b3992984b238a67f5b0d95b393a"},"cell_type":"markdown","source":"# Clean Manual Feature Engineering\n\nThe purpose of this notebook is to clean up the manual feature engineering I had scattered over several other kernels. We will implement the complete manual feature engineering and then test the results.\n\nUpdate August 7: __After some modifications, this can now run in a kernel!__ The features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clean_manual.csv`. \n\n### Roadmap\n\nOur plan of action is as follows.We have to be very careful about memory usage in the kernels, which affects the order of operations:\n\n1. Define functions:\n    * `agg_numeric`\n    * `agg_categorical`\n    * `agg_child` \n    * `agg_grandchild`\n 2. Add in domain knowledge features to `app`\n 3. Work through the `bureau` and `bureau_balance` data\n     * Add in hand built features\n     * Aggregate both using the appropriate functions\n     * Merge with `app` and delete the dataframes\n4. Work through `previous`, `installments`, `cash`, and `credit`\n    * Add in hand built features\n    * Aggregate using the appropriate functions\n    * Merge with `app` and delete the dataframes\n5. Modeling using a Gradient Boosting Machine\n    * Train model on training data using best hyperparameters from random search notebook\n    * Make predictions and submit\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"72ea12d8f9d4ac1d58219d0d99b0c9a391240d5c"},"cell_type":"code","source":"import sys\n\ndef return_size(df):\n    \"\"\"Return size of dataframe in gigabytes\"\"\"\n    return round(sys.getsizeof(df) / 1e9, 2)\n\ndef convert_types(df):\n    print(f'Original size of data: {return_size(df)} gb.')\n    for c in df:\n        if df[c].dtype == 'object':\n            df[c] = df[c].astype('category')\n    print(f'New size of data: {return_size(df)} gb.')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Read in the datasets and replace the anomalous values\napp_train = pd.read_csv('../input/application_train.csv').replace({365243: np.nan})\napp_test = pd.read_csv('../input/application_test.csv').replace({365243: np.nan})\nbureau = pd.read_csv('../input/bureau.csv').replace({365243: np.nan})\nbureau_balance = pd.read_csv('../input/bureau_balance.csv').replace({365243: np.nan})\n\napp_test['TARGET'] = np.nan\napp = app_train.append(app_test, ignore_index = True, sort = True)\n\napp = convert_types(app)\nbureau = convert_types(bureau)\nbureau_balance = convert_types(bureau_balance)\n\nimport gc\ngc.enable()\ndel app_train, app_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce614aaf0bf38ad124fc998807953150442eb0b6"},"cell_type":"markdown","source":"# Numeric Aggregation Function\n\nThe following function aggregates all the numeric variables in a child dataframe at the parent level. That is, for each parent, gather together (group) all of their children, and calculate the aggregations statistics across the children. The function also removes any columns that share the exact same values (which might happen using `count`). "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f474ba5bc0ea8aabf811ba31a59caa07625264bc"},"cell_type":"code","source":"def agg_numeric(df, parent_var, df_name):\n    \"\"\"\n    Groups and aggregates the numeric values in a child dataframe\n    by the parent variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the child dataframe to calculate the statistics on\n        parent_var (string): \n            the parent variable used for grouping and aggregating\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated by the `parent_var` for \n            all numeric columns. Each observation of the parent variable will have \n            one row in the dataframe with the parent variable as the index. \n            The columns are also renamed using the `df_name`. Columns with all duplicate\n            values are removed. \n    \n    \"\"\"\n    \n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != parent_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    # Only want the numeric variables\n    parent_ids = df[parent_var].copy()\n    numeric_df = df.select_dtypes('number').copy()\n    numeric_df[parent_var] = parent_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n\n    # Need to create new column names\n    columns = []\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        if var != parent_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n    \n    agg.columns = columns\n    \n    # Remove the columns with all redundant values\n    _, idx = np.unique(agg, axis = 1, return_index=True)\n    agg = agg.iloc[:, idx]\n    \n    return agg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc017cbe13c09e1651ef18a73066ac4bec2e1c59"},"cell_type":"markdown","source":"# Categorical Aggregation Function\n\nMuch like the numerical aggregation function, the `agg_categorical` function works on a child dataframe to aggregate statistics at the parent level. This can work with any child of `app` and might even be extensible to other problems with only minor changes in syntax."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d848e33f0e04383c378798f02025344be5e7349b"},"cell_type":"code","source":"def agg_categorical(df, parent_var, df_name):\n    \"\"\"\n    Aggregates the categorical features in a child dataframe\n    for each observation of the parent variable.\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    parent_var : string\n        The variable by which to group and aggregate the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with aggregated statistics for each observation of the parent_var\n        The columns are also renamed and columns with duplicate values are removed.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('category'))\n\n    # Make sure to put the identifying id on the column\n    categorical[parent_var] = df[parent_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['sum', 'count', 'mean']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    # Remove duplicate columns by values\n    _, idx = np.unique(categorical, axis = 1, return_index = True)\n    categorical = categorical.iloc[:, idx]\n    \n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec2d17e34ca09c57038be703e48de7fbe1a7272b"},"cell_type":"markdown","source":"# Combined Aggregation Function\n\nWe can put these steps together into a function that will handle a child dataframe. The function will take care of both the numeric and categorical variables and will return the result of merging the two dataframes. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"54aee5b5b6c615f629c1fbd6b535e69178df67ae"},"cell_type":"code","source":"import gc\n\ndef agg_child(df, parent_var, df_name):\n    \"\"\"Aggregate a child dataframe for each observation of the parent.\"\"\"\n    \n    # Numeric and then categorical\n    df_agg = agg_numeric(df, parent_var, df_name)\n    df_agg_cat = agg_categorical(df, parent_var, df_name)\n    \n    # Merge on the parent variable\n    df_info = df_agg.merge(df_agg_cat, on = parent_var, how = 'outer')\n    \n    # Remove any columns with duplicate values\n    _, idx = np.unique(df_info, axis = 1, return_index = True)\n    df_info = df_info.iloc[:, idx]\n    \n    # memory management\n    gc.enable()\n    del df_agg, df_agg_cat\n    gc.collect()\n    \n    return df_info","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8d4cfa0b4c67163a58a25cf155b3d22f0dc193f"},"cell_type":"markdown","source":"This function can be applied to both `bureau` and `previous` because these are direct children of `app`. For the children of the children, we will need to take an additional aggregation step. "},{"metadata":{"_uuid":"734cf323c1e22f5baa4f073a720245e28a03a0ef"},"cell_type":"markdown","source":"# Aggregate Grandchild Data Tables\n\nSeveral of the tables (`bureau_balance, cash, credit_card`, and `installments`) are children of the child dataframes. In other words, these are grandchildren of the main `app` data table. To aggregate these tables, they must first be aggregated at the parent level (which is on a per loan basis) and then at the grandparent level (which is on the client basis). For example, in the `bureau_balance` dataframe, there is monthly information on the loans in `bureau`. To get this data into the `app` dataframe will first require grouping the monthly information for each loan and then grouping the loans for each client. \n\nHopefully, the nomenclature does not get too confusing, but here's a rounddown:\n\n* __grandchild__: the child of a child data table, for instance, `bureau_balance`. For every row in the child table, there can be multiple rows in the grandchild. \n* __parent__: the parent table of the grandchild that links the grandchild to the grandparent. For example, the `bureau` dataframe is the parent of the `bureau_balance` dataframe in this situation. `bureau` is in turn the child of the `app` dataframe. `bureau_balance` can be connected to `app` through `bureau`.\n* __grandparent__: the parent of the parent of the grandchild, in this problem the `app` dataframe. The end goal is to aggregate the information in the grandchild into the grandparent. This will be done in two stages: first at the parent (loan) level and then at the grandparent (client) level\n* __parent variable__: the variable linking the grandchild to the parent. For the `bureau` and `bureau_balance` data this is `SK_ID_BUREAU` which uniquely identifies each previous loan\n* __grandparent variable__: the variable linking the parent to the grandparent. This is `SK_ID_CURR` which uniquely identifies each client in `app`.\n\n### Aggregating Grandchildren Function\n\nWe can take the individual steps required for aggregating a grandchild dataframe at the grandparent level in a function. These are:\n\n1. Aggregate the numeric variables at the parent (the loan, `SK_ID_BUREAU` or `SK_ID_PREV`) level.\n2. Merge with the parent of the grandchild to get the grandparent variable in the data (for example `SK_ID_CURR`)\n3. Aggregate the numeric variables at the grandparent (the client, `SK_ID_CURR`) level. \n4. Aggregate the categorical variables at the parent level.\n5. Merge the aggregated data with the parent to get the grandparent variable\n6. Aggregate the categorical variables at the grandparent level\n7. Merge the numeric and categorical dataframes on the grandparent varible\n8. Remove the columns with all duplicated values.\n9. The resulting dataframe should now have one row for every grandparent (client) observation\n10. Merge with the main dataframe (`app`) on the grandparent variable (`SK_ID_CURR`). \n\nThis function can be applied to __all 4 grandchildren__ without the need for hard-coding in specific variables. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1cebb65093e6937825eb5e99ffe4b1b395203d02"},"cell_type":"code","source":"def agg_grandchild(df, parent_df, parent_var, grandparent_var, df_name):\n    \"\"\"\n    Aggregate a grandchild dataframe at the grandparent level.\n    \n    Parameters\n    --------\n        df : dataframe\n            Data with each row representing one observation\n            \n        parent_df : dataframe\n            Parent table of df that must have the parent_var and \n            the grandparent_var. Used only to get the grandparent_var into\n            the dataframe after aggregations\n            \n        parent_var : string\n            Variable representing each unique observation in the parent.\n            For example, `SK_ID_BUREAU` or `SK_ID_PREV`\n            \n        grandparent_var : string\n            Variable representing each unique observation in the grandparent.\n            For example, `SK_ID_CURR`. \n            \n        df_name : string\n            String for renaming the resulting columns.\n            The columns are name with the `df_name` and with the \n            statistic calculated in the column\n    \n    Return\n    --------\n        df_info : dataframe\n            A dataframe with one row for each observation of the grandparent variable.\n            The grandparent variable forms the index, and the resulting dataframe\n            can be merged with the grandparent to be used for training/testing. \n            Columns with all duplicate values are removed from the dataframe before returning.\n    \n    \"\"\"\n    \n    # set the parent_var as the index of the parent_df for faster merges\n    parent_df = parent_df[[parent_var, grandparent_var]].copy().set_index(parent_var)\n    \n    # Aggregate the numeric variables at the parent level\n    df_agg = agg_numeric(df, parent_var, '%s_LOAN' % df_name)\n    \n    # Merge to get the grandparent variable in the data\n    df_agg = df_agg.merge(parent_df, \n                          on = parent_var, how = 'left')\n    \n    # Aggregate the numeric variables at the grandparent level\n    df_agg_client = agg_numeric(df_agg, grandparent_var, '%s_CLIENT' % df_name)\n    \n    # Can only apply one-hot encoding to categorical variables\n    if any(df.dtypes == 'category'):\n    \n        # Aggregate the categorical variables at the parent level\n        df_agg_cat = agg_categorical(df, parent_var, '%s_LOAN' % df_name)\n        df_agg_cat = df_agg_cat.merge(parent_df,\n                                      on = parent_var, how = 'left')\n\n        # Aggregate the categorical variables at the grandparent level\n        df_agg_cat_client = agg_numeric(df_agg_cat, grandparent_var, '%s_CLIENT' % df_name)\n        df_info = df_agg_client.merge(df_agg_cat_client, on = grandparent_var, how = 'outer')\n        \n        gc.enable()\n        del df_agg, df_agg_client, df_agg_cat, df_agg_cat_client\n        gc.collect()\n    \n    # If there are no categorical variables, then we only need the numeric aggregations\n    else:\n        df_info = df_agg_client.copy()\n    \n        gc.enable()\n        del df_agg, df_agg_client\n        gc.collect()\n    \n    # Drop the columns with all duplicated values\n    _, idx = np.unique(df_info, axis = 1, return_index=True)\n    df_info = df_info.iloc[:, idx]\n    \n    return df_info","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1139fff129bb081dda6eab4a4b987d8cd9b278d0"},"cell_type":"markdown","source":"# Putting it Together\n\nNow that we have the individual pieces of semi-automated feature engineering, we need to put them together. There are two functions that can handle the children and the grandchildren data tables:\n\n1. `agg_child(df, parent_var, df_name)`: aggregate the numeric and categorical variables of a child dataframe at the parent level. For example, the `previous` dataframe is a child of the `app` dataframe that must be aggregated for each client. \n2. `agg_grandchild(df, parent_df, parent_var, grandparent_var, df_name)`: aggregate the numeric and categorical variables of a grandchild dataframe at the grandparent level. For example, the `bureau_balance` dataframe is the grandchild of the `app` dataframe with `bureau` as the parent. \n\nFor each of the children dataframes of `app`, (`previous` and `bureau`), we will use the first function and merge the result into the `app` on the parent variable, `SK_ID_CURR`. For the four grandchild dataframes, we will use the second function, which returns a single dataframe that can then be merged into app on `SK_ID_CURR`. "},{"metadata":{"_uuid":"4bf4d830a18b8f7407e1574f81390c81aba5c8be"},"cell_type":"markdown","source":"## Hand-Built Features\n\nAlong the way, we will add in hand-built features to the datasets. These have come from my own ideas (probably not very optimal) and from the community.\n\nFirst we will add in \"domain knowledge\" features to the `app` dataframe. These were developed based on work done in other kernels (both from the community and my own work)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"61bd024933e9318fd59e77df40874bfaee83e2fa"},"cell_type":"code","source":"# Add domain features to base dataframe\napp['LOAN_RATE'] = app['AMT_ANNUITY'] / app['AMT_CREDIT'] \napp['CREDIT_INCOME_RATIO'] = app['AMT_CREDIT'] / app['AMT_INCOME_TOTAL']\napp['EMPLOYED_BIRTH_RATIO'] = app['DAYS_EMPLOYED'] / app['DAYS_BIRTH']\napp['EXT_SOURCE_SUM'] = app[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].sum(axis = 1)\napp['EXT_SOURCE_MEAN'] = app[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis = 1)\napp['AMT_REQ_SUM'] = app[[x for x in app.columns if 'AMT_REQ_' in x]].sum(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0b45a58bba944d0e81ba5b12cd18a0e3f87fd84"},"cell_type":"markdown","source":"### Hand-Built Features for other Dataframes\n\nWe can also add in hand built features for the other dataframes. Since these are not the main dataframe, these features will end up being aggregated in different ways. These will be added as we go through the tables."},{"metadata":{"_uuid":"7144bf4ba6ff73fc88cf0e598212db49d8950a56"},"cell_type":"markdown","source":"#### Aggregate the bureau data\n\nFirst add the loan rate for previous loans at other institutions."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7dc86240f1043ad2e0aa6c1525a2161f373de8d4"},"cell_type":"code","source":"bureau['LOAN_RATE'] = bureau['AMT_ANNUITY'] / bureau['AMT_CREDIT_SUM']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3791f2c7b78f7b92e545ef5e752a92819efac8c5","collapsed":true},"cell_type":"code","source":"bureau_info = agg_child(bureau, 'SK_ID_CURR', 'BUREAU')\nbureau_info.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23f89c917b161b5ed8e4c742e6cd1f087b7f540f","collapsed":true},"cell_type":"code","source":"bureau_info.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6595cbe7b551e8473faf934f3d87d2e38dc1afea"},"cell_type":"markdown","source":"#### Aggregate the bureau balance\n\nNow we turn to the `bureau_balance` dataframe. We will make a column indicating whether a loan was past due for the month or whether the payment was on time."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d6fbc3d307e40a1c7449c44964170009d303895e"},"cell_type":"code","source":"bureau_balance['PAST_DUE'] = bureau_balance['STATUS'].isin(['1', '2', '3', '4', '5'])\nbureau_balance['ON_TIME'] = bureau_balance['STATUS'] == '0'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c1237cece65f7db963b2b5d5ab33afe38ec04cb","collapsed":true},"cell_type":"code","source":"bureau_balance_info = agg_grandchild(bureau_balance, bureau, 'SK_ID_BUREAU', 'SK_ID_CURR', 'BB')\ndel bureau_balance, bureau\nbureau_balance_info.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"093efb9f7bab68e65a480d405fbd3a5d7c259b37","collapsed":true},"cell_type":"code","source":"bureau_balance_info.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d913f7aec61e6e620c0a7a0659800f485a0cb39e"},"cell_type":"markdown","source":"## Merge with the main dataframe\n\nThe individual dataframes can all be merged into the main `app` dataframe. Merging is much quicker if done on any index, so it's good practice to first set the index to the variable on which we will merge. In each case, we use a `left` join so that all the observations in `app` are kept even if they are not present in the other dataframes (which occurs because not every client has previous records at Home Bureau or other credit institutions). After each step of mergning, we remove the dataframe from memory in order to hopefully let the kernel continue to run.\n\nThe final result is one dataframe with a single row for each client that can be used for training a machine learning model. "},{"metadata":{"trusted":true,"_uuid":"ad82e75a9d61b5a98d28185ff240c3fbbdcd188c","collapsed":true},"cell_type":"code","source":"app = app.set_index('SK_ID_CURR')\napp = app.merge(bureau_info, on = 'SK_ID_CURR', how = 'left')\ndel bureau_info\napp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a9e9c7427bd6b80a941ca5c56bb4ef48d17c6f1","collapsed":true},"cell_type":"code","source":"app = app.merge(bureau_balance_info, on = 'SK_ID_CURR', how = 'left')\ndel bureau_balance_info\napp.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca7bea55407fe95859939c54eb857eb69f50949d"},"cell_type":"markdown","source":"#### Aggregate previous loans at Home Credit\n\nWe will add in two domain features, first the loan rate and then the difference between the amount applied for and the amount awarded."},{"metadata":{"trusted":true,"_uuid":"13fb6c70040535bfe10f3f56d3a4feee3054bef3","collapsed":true},"cell_type":"code","source":"previous = pd.read_csv('../input/previous_application.csv').replace({365243: np.nan})\nprevious = convert_types(previous)\nprevious['LOAN_RATE'] = previous['AMT_ANNUITY'] / previous['AMT_CREDIT']\nprevious[\"AMT_DIFFERENCE\"] = previous['AMT_CREDIT'] - previous['AMT_APPLICATION']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d107be6e3cf635d0846886bfa97da497915a45a8"},"cell_type":"markdown","source":"`AMT_DIFFERENCE` is the difference between what was given to the client and what the client requested on previous loans at Home Credit."},{"metadata":{"trusted":true,"_uuid":"9f967df7bba80f8fd0f8fce318cd30a6dc746a0b","collapsed":true},"cell_type":"code","source":"previous_info = agg_child(previous, 'SK_ID_CURR', 'PREVIOUS')\nprevious_info.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46eede109fab05a7665392c2d57c25c45866ed48","collapsed":true},"cell_type":"code","source":"app = app.merge(previous_info, on = 'SK_ID_CURR', how = 'left')\ndel previous_info\napp.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca236129c93ce2cea3d859bee1fc3f985bd0f8d3"},"cell_type":"markdown","source":"#### Aggregate Installments Data\n\nThe installments table has each installment (payment) for previous loans at Home Credit. We can create a column indicating whether or not a loan was late."},{"metadata":{"trusted":true,"_uuid":"a49ddecbc20ccc1deaede33a6921ebe92f29c21e","collapsed":true},"cell_type":"code","source":"installments = pd.read_csv('../input/installments_payments.csv').replace({365243: np.nan})\ninstallments = convert_types(installments)\ninstallments['LATE'] = installments['DAYS_ENTRY_PAYMENT'] > installments['DAYS_INSTALMENT']\ninstallments['LOW_PAYMENT'] = installments['AMT_PAYMENT'] < installments['AMT_INSTALMENT']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c5a277729fdd8800f9a76ab6c1498f40abde821"},"cell_type":"markdown","source":"`LOW_PAYMENT` represents a payment that was less than the prescribed amount. "},{"metadata":{"trusted":true,"_uuid":"ec76d1aadb13750f75352eb65f852fc0adb32b55","collapsed":true},"cell_type":"code","source":"installments_info = agg_grandchild(installments, previous, 'SK_ID_PREV', 'SK_ID_CURR', 'IN')\ndel installments\ninstallments_info.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"129b59883521f0dd2ea4789612540b041422a6bc","collapsed":true},"cell_type":"code","source":"app = app.merge(installments_info, on = 'SK_ID_CURR', how = 'left')\ndel installments_info\napp.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90d1f7e93b4d93cc014ee01ac600b8ff383e695b"},"cell_type":"markdown","source":"#### Aggregate Cash previous loans\n\nThe next dataframe is the `cash` which has monthly information on previous cash loans at Home Credit. We can create a column indicating if the loan was overdue for the month. "},{"metadata":{"trusted":true,"_uuid":"3f777ed7ad8cdeb34b80f1f99242a2947ba3ac67","collapsed":true},"cell_type":"code","source":"cash = pd.read_csv('../input/POS_CASH_balance.csv').replace({365243: np.nan})\ncash = convert_types(cash)\ncash['LATE_PAYMENT'] = cash['SK_DPD'] > 0.0\ncash['INSTALLMENTS_PAID'] = cash['CNT_INSTALMENT'] - cash['CNT_INSTALMENT_FUTURE']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd946d6dd97d170ab0e22904dda4e6eafde4e944"},"cell_type":"markdown","source":"`INSTALLMENTS_PAID` is meant to represent the number of already paid (or I guess missed) installments by subtracting the future installments from the total installments."},{"metadata":{"trusted":true,"_uuid":"11ef3be67752e60cbeaad78b3da521889253d408","collapsed":true},"cell_type":"code","source":"cash_info = agg_grandchild(cash, previous, 'SK_ID_PREV', 'SK_ID_CURR', 'CASH')\ndel cash\ncash_info.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"690bbf472ea0b4934e0589c45caeb6590f073724","collapsed":true},"cell_type":"code","source":"app = app.merge(cash_info, on = 'SK_ID_CURR', how = 'left')\ndel cash_info\napp.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dd5fbe21c4bdcc6f04d3b2d094f38e1ad8ceb8f"},"cell_type":"markdown","source":"#### Aggregate Credit previous loans\n\nThe last dataframe is `credit` which has previous credit card loans at Home Credit. We can make a column indicating whether the balance is greater than the credit limit, a column showing whether or not the balance was cleared (equal to 0), whether or not the payment was below the prescribed amount, and whether or not the payment was behind. Then we aggregate as with the other grandchildren."},{"metadata":{"trusted":true,"_uuid":"0dc16ee25b0acbb3042179cc80e17fda23e0ecf4","collapsed":true},"cell_type":"code","source":"credit = pd.read_csv('../input/credit_card_balance.csv').replace({365243: np.nan})\ncredit = convert_types(credit)\ncredit['OVER_LIMIT'] = credit['AMT_BALANCE'] > credit['AMT_CREDIT_LIMIT_ACTUAL']\ncredit['BALANCE_CLEARED'] = credit['AMT_BALANCE'] == 0.0\ncredit['LOW_PAYMENT'] = credit['AMT_PAYMENT_CURRENT'] < credit['AMT_INST_MIN_REGULARITY']\ncredit['LATE'] = credit['SK_DPD'] > 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"854c362609f29368606d6a4459374cdb02ece9bc","collapsed":true},"cell_type":"code","source":"credit_info = agg_grandchild(credit, previous, 'SK_ID_PREV', 'SK_ID_CURR', 'CC')\ndel credit, previous\ncredit_info.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4bcf80e7ff66a8e995cb770bbf6e021b45133757"},"cell_type":"code","source":"gc.collect()\ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c89e4da8a4719b2e1c0dbbb7f8be507939ed1db5"},"cell_type":"markdown","source":"__This is usually the point at which the kernel fails.__ To try and alleviate the problem, I have added a pause of 10 minutes."},{"metadata":{"trusted":true,"_uuid":"eee572c759717fa9629cb4a91eea9468d00cf3e8","collapsed":true},"cell_type":"code","source":"import time\ntime.sleep(600)\napp = app.merge(credit_info, on = 'SK_ID_CURR', how = 'left')\ndel credit_info\napp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bcd815bb39284de46390de086da3aed84f1b598","collapsed":true},"cell_type":"code","source":"print('After manual feature engineering, there are {} features.'.format(app.shape[1] - 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a0b0a1a8f48e9019fa849f6131f5a5b97ed8333","collapsed":true},"cell_type":"code","source":"gc.enable()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d12b5c47937cb2aea247645020da8846a9e7cb57","collapsed":true},"cell_type":"code","source":"print(f'Final size of data {return_size(app)}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c5bc4b21856f62faf015dd9ba93b752800efb8e"},"cell_type":"markdown","source":"__Update August 7__: The kernel can now run!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ef97710d56b1c58ac1d5332269836136b8b36f07"},"cell_type":"code","source":"# Check for columns with duplicated values\n# _, idx = np.unique(app, axis = 1, return_index = True)\n# print('There are {} columns with all duplicated values.'.format(app.shape[1] - len(idx)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"74da0ff7aa54d65b885638471bdebeccda54ffe7"},"cell_type":"code","source":"app.to_csv('clean_manual_features.csv', chunksize = 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8af0cb98f65449572d40c7d678a534c7df1c8e63"},"cell_type":"markdown","source":"# Modeling\n\nAfter all the hard work, now we get to test our features! We will use a model with the hyperparameters from random search that are documented in another notebook. \n\nThe final model scores __0.792__ when uploaded to the competition."},{"metadata":{"trusted":true,"_uuid":"66f7f7b37549747b30030eaa8b2e7a70ee161b4a","collapsed":true},"cell_type":"code","source":"app.reset_index(inplace = True)\ntrain, test = app[app['TARGET'].notnull()].copy(), app[app['TARGET'].isnull()].copy()\ngc.enable()\ndel app\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7feb3e3029f28812bc166ba80c2128a1af1e6ab5"},"cell_type":"code","source":"import lightgbm as lgb\n\nparams = {'is_unbalance': True, \n              'n_estimators': 2673, \n              'num_leaves': 77, \n              'learning_rate': 0.00764, \n              'min_child_samples': 460, \n              'boosting_type': 'gbdt', \n              'subsample_for_bin': 240000, \n              'reg_lambda': 0.20, \n              'reg_alpha': 0.88, \n              'subsample': 0.95, \n              'colsample_bytree': 0.7}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bc6a2b44da845ddab8b7f88395112d290844f17","collapsed":true},"cell_type":"code","source":"train_labels = np.array(train.pop('TARGET')).reshape((-1, ))\n\ntest_ids = list(test.pop('SK_ID_CURR'))\ntest = test.drop(columns = ['TARGET'])\ntrain_ids = train.pop('SK_ID_CURR')\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"61ebfbe2de9bdf188ca270b50dd0c28316e855b1"},"cell_type":"code","source":"model = lgb.LGBMClassifier(**params)\nmodel.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f6add38b6fa4652158c47f63e8fdf207037ab3d9"},"cell_type":"code","source":"preds = model.predict_proba(test)[:, 1]\nsubmission = pd.DataFrame({'SK_ID_CURR': test_ids,\n                           'TARGET': preds})\n\nsubmission['SK_ID_CURR'] = submission['SK_ID_CURR'].astype(int)\nsubmission['TARGET'] = submission['TARGET'].astype(float)\nsubmission.to_csv('submission_manual.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eddaafba40d5cc85a6c549200738050cd3f7c699"},"cell_type":"markdown","source":"## Feature Importances\n\nNow we can see if all that time was worth it! In the code below, we find the most important features and show them in a plot and dataframe."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3fb3f9a6f83a74cf410a93e51fcd382c84b6e680"},"cell_type":"code","source":"features = list(train.columns)\nfi = pd.DataFrame({'feature': features,\n                   'importance': model.feature_importances_})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f7addb0e674080e3559a253b4dc3cbc574ff8b35"},"cell_type":"code","source":"def plot_feature_importances(df, n = 15, threshold = None):\n    \"\"\"\n    Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be \"feature\" and \"importance\"\n    \n    n : int, default = 15\n        Number of most important features to plot\n    \n    threshold : float, default = None\n        Threshold for cumulative importance plot. If not provided, no plot is made\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    Note\n    --------\n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n    \n    \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'blue', edgecolor = 'k', figsize = (12, 8),\n                            legend = False)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'Top {n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.2, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, 100 * threshold))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f753460123fbac52ff0b7a17af3ecb1bda2a2a61","collapsed":true},"cell_type":"code","source":"norm_fi = plot_feature_importances(fi, 25)\nnorm_fi.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e5d09cc14ebba7cb5c6ccca5ba5db0e103d9873b"},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    params = {'is_unbalance': True, \n              'n_estimators': 2673, \n              'num_leaves': 77, \n              'learning_rate': 0.00764, \n              'min_child_samples': 460, \n              'boosting_type': 'gbdt', \n              'subsample_for_bin': 240000, \n              'reg_lambda': 0.20, \n              'reg_alpha': 0.88, \n              'subsample': 0.95, \n              'colsample_bytree': 0.7}\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(**params, \n                                   n_estimators=10000, objective = 'binary', \n                                   n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"22fab9ac9763c00cfe7ad2d190f2059727824d5a"},"cell_type":"code","source":"train['TARGET'] = list(train_labels)\ntrain['SK_ID_CURR'] = list(train_ids)\ntest['SK_ID_CURR'] = list(test_ids)\n\nsubmission, feature_importances, metrics = model(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8cdb3c17150989f850392993c74ed22114312457"},"cell_type":"code","source":"submission.to_csv('submission_clean_manual2.csv')\nnorm_fi = plot_feature_importances(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8a7f98ac270a6d8e1edf2f7ee32ab1122a8692e"},"cell_type":"markdown","source":"# Conclusions\n\nThis code is a little too much to run in the Kaggle kernels. However, the features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clean_manual.csv`. \n\nThis notebook is meant to serve as a clean version of the manual feature engineering I had scattered across several other notebooks. We were able to build a complete set of __ features that scored 0.792 on the public leaderboard__. Further hyperparameter tuning might improve the performance. For additional feature engineering, we will probably want to turn to more technical operations such as treating this as a time-series problem. Since we have relative time information (relative to the current loan at Home Credit), it's possible to find the most recent information and also trends over time. These can be useful because changes in behavior might inform us as to whether or not a client will be able to repay a loan! \n\nThanks for reading and as always, I welcome feedback and constructive criticism. I'll see you in the next notebook.\n\nBest,\n\nWill"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ab8c60ef7fddf53afdca48e0c1ed15a665b82c8b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}