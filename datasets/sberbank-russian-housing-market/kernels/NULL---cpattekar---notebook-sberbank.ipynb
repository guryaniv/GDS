{"nbformat": 4, "nbformat_minor": 0, "metadata": {"_is_fork": false, "_change_revision": 0, "language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.0"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "cells": [{"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "f2b5ddd3596a708d235fda7632356cb9e3d23bea", "_cell_guid": "daf836e3-98fe-f2be-f6c6-ec61d50f3150"}, "source": ["In this exploration notebook, we shall try to uncover the basic information about the dataset which will help us build our models / features.\n", "\n", "Let us start with importing the necessary modules."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "2b353d1b1c441356513779eed920c0c07fad4e13", "_cell_guid": "010c0e14-3312-826e-ecd6-b7db41d41f94"}, "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection, preprocessing\nimport xgboost as xgb\n\nfrom sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n                              GradientBoostingClassifier)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n\ncolor = sns.color_palette()\n\n#%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.set_option('display.max_columns', 500)\nrun1 = False\nrun2 = False\nrun3 = False"}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "0d8381813ce37dc0638ac352540cfd5820a30cf5", "_cell_guid": "78b69b00-2ca9-ec50-0e64-df78c0698238"}, "source": ["First let us import the train file and get some idea about the data."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "08e4bb552d2316d2b2d8ac197e84d565e68b03ca", "_cell_guid": "cf59f829-b0bd-fe3c-cd3b-c9c176c678ea"}, "source": ["train_df = pd.read_csv(\"../input/train.csv\")\n", "train_df.shape"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "ccfca48549fa39dcdda73eabc43b8f96113ee205", "_cell_guid": "094c89d2-2c4a-49a4-5536-7d26173f2909"}, "source": ["train_df.head()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "72a5caf02d894eb03311e51883b6343835c2f02f", "_cell_guid": "4d035267-d1d5-e27c-1f36-887d9bc0359e"}, "source": ["There are quite a few variables in this dataset. \n", "\n", "Let us start with target variable exploration - 'price_doc'. First let us do a scatter plot to see if there are any outliers in the data."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "982d05eaa1a6a0c4d583cd2c53e14e1ffeb66a78", "_cell_guid": "06c00812-d557-7797-7b4b-0d28632c39d9"}, "source": ["print(train_df.shape[0])"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "a187fdb5fa8c73b8ae61016150287b93ca53617d", "_cell_guid": "3b2b42e1-a5e8-79ff-af6b-91b40664e13e"}, "source": ["plt.figure(figsize=(8,6))\n", "plt.scatter(range(train_df.shape[0]), np.sort(train_df.price_doc.values))\n", "plt.xlabel('index', fontsize=12)\n", "plt.ylabel('price', fontsize=12)\n", "plt.show()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "f1c0920ced8587df0d1a86fd4de47a204c6a4fc8", "_cell_guid": "d975201d-d9c3-f456-4dac-4964e964d96b"}, "source": ["Looks okay to me. Also since the metric is RMSLE, I think it is okay to have it as such. However if needed, one can truncate the high values. \n", "\n", "We can now bin the 'price_doc' and plot it."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "077550ad15fdb54c6cb9a724a2067662db733de8", "_cell_guid": "03e5a2f2-c68b-9965-33dd-b698775c11cf"}, "source": ["plt.figure(figsize=(12,8))\n", "dfsnsplt = pd.DataFrame(train_df.price_doc.values.astype(int))\n", "print(type(dfsnsplt))\n", "sns.distplot(dfsnsplt, bins=60, kde=True)\n", "#sns.distplot(train_df.price_doc.values)\n", "plt.xlabel('price', fontsize=12)\n", "plt.show()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "0eaf1b1c86ed72a170453dfb042950f4a61bae58", "_cell_guid": "205d91d5-47ce-f88b-154e-b441c043898b"}, "source": ["Certainly a very long right tail. Since our metric is Root Mean Square **Logarithmic** error, let us plot the log of price_doc variable."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "989e016746ad78c49e3dad40d57960e1c86d9138", "_cell_guid": "27b08122-7009-d756-e695-c463a5a27d60"}, "source": ["plt.figure(figsize=(12,8))\n", "sns.distplot(np.log(train_df.price_doc.values), bins=50, kde=True)\n", "plt.xlabel('price', fontsize=12)\n", "plt.show()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "07e2feebdd983f233c47ce748ace20b7cc7ef8b6", "_cell_guid": "d80989f8-eae8-bede-f088-4d7c6c8cd7fb"}, "source": ["This looks much better than the previous one. \n", "\n", "Now let us see how the median housing price change with time. "]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "80abf7a55695230fcf84cb2886d8b9f1a33f7070", "_cell_guid": "c2cc67da-dd73-7fb7-473f-3987f1a5ecb8"}, "source": ["train_df['yearmonth'] = train_df['timestamp'].apply(lambda x: x[:4]+x[5:7])\n", "grouped_df = train_df.groupby('yearmonth')['price_doc'].aggregate(np.median).reset_index()"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "5a97dddccf48cf1ac11675d50c97ac80b78a7448", "_cell_guid": "18daf0fc-7fc4-f117-adb6-b4d3c2483412"}, "source": ["plt.figure(figsize=(12,8))\n", "sns.barplot(grouped_df.yearmonth.values, grouped_df.price_doc.values, alpha=0.8, color=color[2])\n", "plt.ylabel('Median Price', fontsize=12)\n", "plt.xlabel('Year Month', fontsize=12)\n", "plt.xticks(rotation='vertical')\n", "plt.show()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "87b98c1a2f57693fcba6482051446c53fcf6319e", "_cell_guid": "4a23d010-ccde-e93a-56f3-4b1d1774091b"}, "source": ["There are some variations in the median price with respect to time. Towards the end, there seems to be some linear increase in the price values.\n", "\n", "Now let us dive into other variables and see. Let us first start with getting the count of different data types. "]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "3357ad49e806179f84e75c6aa7e573f671f50c1a", "_cell_guid": "4a510255-4568-c574-0a56-1baeb94e38d1"}, "source": ["train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n", "dtype_df = train_df.dtypes.reset_index()\n", "dtype_df.columns = [\"Count\", \"Column Type\"]\n", "dtype_df.groupby(\"Column Type\").aggregate('count').reset_index()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "819841cc31c20dbf0fe44105b64ee461eb5e2661", "_cell_guid": "636d0880-b0df-cb3c-6a26-633643b03046"}, "source": ["So majority of them are numerical variables with 15 factor variables and 1 date variable.\n", "\n", "Let us explore the number of missing values in each column."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "a4a5b6772045134dff7936381fc0c747bd6c2355", "_cell_guid": "a4693174-8f48-e979-9c2c-b018e50b6394"}, "source": ["missing_df_temp = train_df.isnull()"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "5cce275d427ddedb2a4ef05ac281b01f98a24d66", "_cell_guid": "1bd91585-c40d-96ee-8031-5f6cff98f451"}, "source": ["missing_df_temp.head()"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "ba001bd82c1ea27c8bb55b5b0f5d0d30ffb2553d", "_cell_guid": "c2c98c5e-b912-094f-2b8a-630fe1b363b2"}, "source": ["missing_df_temp.sum(axis=0).reset_index().head()"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "b4132b37ce21e71f7eb863d7dfd97a68f4abbb42", "_cell_guid": "2e68a733-1357-4adb-af07-53b7440913c9"}, "source": ["missing_df = train_df.isnull().sum(axis=0).reset_index()\n", "missing_df.columns = ['column_name', 'missing_count']\n", "missing_df = missing_df.ix[missing_df['missing_count']>0]\n", "ind = np.arange(missing_df.shape[0])\n", "width = 0.9\n", "fig, ax = plt.subplots(figsize=(12,18))\n", "rects = ax.barh(ind, missing_df.missing_count.values, color='y')\n", "ax.set_yticks(ind)\n", "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n", "ax.set_xlabel(\"Count of missing values\")\n", "ax.set_title(\"Number of missing values in each column\")\n", "plt.show()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "cea5edfa0eea1e4feabb682a79d252c296fc85c5", "_cell_guid": "0f893fb6-3526-3fd1-9780-db189ca5addf"}, "source": ["Seems variables are found to missing as groups.\n", "\n", "Since there are 292 variables, let us build a basic xgboost model and then explore only the important variables."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "729ef085100173bf09c4de3dab62185936ffd6b1", "collapsed": false}, "source": "df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\nid_test = list(df_test['id'])"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "08bae5da97122ec8da93e146439a8e8c9bf480f1", "collapsed": false}, "source": "for f in df_test.columns:\n    if df_test[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_test[f].values)) \n        df_test[f] = lbl.transform(list(df_test[f].values))"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "0109ac3ded3e3de975797e5bec2f3acce065815c", "collapsed": false}, "source": "#clean data\nbad_index = train_df[train_df.life_sq > train_df.full_sq].index\ntrain_df.ix[bad_index, \"life_sq\"] = np.NaN\nequal_index = [601,1896,2791]\ndf_test.ix[equal_index, \"life_sq\"] = df_test.ix[equal_index, \"full_sq\"]\nbad_index = df_test[df_test.life_sq > df_test.full_sq].index\ndf_test.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train_df[train_df.life_sq < 5].index\ntrain_df.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = df_test[df_test.life_sq < 5].index\ndf_test.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train_df[train_df.full_sq < 5].index\ntrain_df.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = df_test[df_test.full_sq < 5].index\ndf_test.ix[bad_index, \"full_sq\"] = np.NaN\nkitch_is_build_year = [13117]\ntrain_df.ix[kitch_is_build_year, \"build_year\"] = train_df.ix[kitch_is_build_year, \"kitch_sq\"]\nbad_index = train_df[train_df.kitch_sq >= train_df.life_sq].index\ntrain_df.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = df_test[df_test.kitch_sq >= df_test.life_sq].index\ndf_test.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train_df[(train_df.kitch_sq == 0).values + (train_df.kitch_sq == 1).values].index\ntrain_df.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = df_test[(df_test.kitch_sq == 0).values + (df_test.kitch_sq == 1).values].index\ndf_test.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train_df[(train_df.full_sq > 210) & (train_df.life_sq / train_df.full_sq < 0.3)].index\ntrain_df.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = df_test[(df_test.full_sq > 150) & (df_test.life_sq / df_test.full_sq < 0.3)].index\ndf_test.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = train_df[train_df.life_sq > 300].index\ntrain_df.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\nbad_index = df_test[df_test.life_sq > 200].index\ndf_test.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\ntrain_df.product_type.value_counts(normalize= True)\ndf_test.product_type.value_counts(normalize= True)\nbad_index = train_df[train_df.build_year < 1500].index\ntrain_df.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = df_test[df_test.build_year < 1500].index\ndf_test.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = train_df[train_df.num_room == 0].index \ntrain_df.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = df_test[df_test.num_room == 0].index \ndf_test.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\ntrain_df.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [3174, 7313]\ndf_test.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = train_df[(train_df.floor == 0).values * (train_df.max_floor == 0).values].index\ntrain_df.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\nbad_index = train_df[train_df.floor == 0].index\ntrain_df.ix[bad_index, \"floor\"] = np.NaN\nbad_index = train_df[train_df.max_floor == 0].index\ntrain_df.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = df_test[df_test.max_floor == 0].index\ndf_test.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = train_df[train_df.floor > train_df.max_floor].index\ntrain_df.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = df_test[df_test.floor > df_test.max_floor].index\ndf_test.ix[bad_index, \"max_floor\"] = np.NaN\ntrain_df.floor.describe(percentiles= [0.9999])\nbad_index = [23584]\ntrain_df.ix[bad_index, \"floor\"] = np.NaN\ntrain_df.material.value_counts()\ndf_test.material.value_counts()\ntrain_df.state.value_counts()\nbad_index = train_df[train_df.state == 33].index\ntrain_df.ix[bad_index, \"state\"] = np.NaN\ndf_test.state.value_counts()"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "45bdb4414db2f56c04a8eb4dc2373b05e85f8520", "collapsed": false}, "source": "ulimit = np.percentile(train_df.price_doc.values, 99.5)\nllimit = np.percentile(train_df.price_doc.values, 0.5)\ntrain_df['price_doc'].ix[train_df['price_doc']>ulimit] = ulimit\ntrain_df['price_doc'].ix[train_df['price_doc']<llimit] = llimit\n\ncol = \"full_sq\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "50120a37b2025d7de66006cc00fe0c9a6e533a02", "collapsed": false}, "source": "\n# Add month-year\n\"\"\"\nmonth_year = (train_df.timestamp.dt.month.astype(int) + (train_df.timestamp.dt.year * 100).astype(int))\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ntrain_df['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\nmonth_year = (df_test.timestamp.dt.month.astype(int) + (df_test.timestamp.dt.year * 100).astype(int))\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf_test['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n\n# Add week-year count\nweek_year = (train_df.timestamp.dt.weekofyear.astype(int) + (train_df.timestamp.dt.year * 100).astype(int))\nweek_year_cnt_map = week_year.value_counts().to_dict()\ntrain_df['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\nweek_year = (df_test.timestamp.dt.weekofyear.astype(int) + (df_test.timestamp.dt.year * 100).astype(int))\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf_test['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n\n# Add month and day-of-week\ntrain_df['month'] = train_df.timestamp.dt.month\ntrain_df['dow'] = train_df.timestamp.dt.dayofweek\n\ndf_test['month'] = df_test.timestamp.dt.month\ndf_test['dow'] = df_test.timestamp.dt.dayofweek\n\n\n# Other feature engineering\ntrain_df['rel_floor'] = train_df['floor'] / train_df['max_floor'].astype(float)\ntrain_df['rel_kitch_sq'] = train_df['kitch_sq'] / train_df['full_sq'].astype(float)\n\ndf_test['rel_floor'] = df_test['floor'] / df_test['max_floor'].astype(float)\ndf_test['rel_kitch_sq'] = df_test['kitch_sq'] / df_test['full_sq'].astype(float)\n\ntrain_df.apartment_name=train_df.sub_area + train_df['metro_km_avto'].astype(str)\ndf_test.apartment_name=df_test.sub_area + train_df['metro_km_avto'].astype(str)\n\ntrain_df['room_size'] = train_df['life_sq'] / train_df['num_room'].astype(float)\ndf_test['room_size'] = df_test['life_sq'] / df_test['num_room'].astype(float)\n\"\"\""}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "e622b7b39b94d02cfc3c8b6a52b27962c7a21f97", "collapsed": false}, "source": "for f in train_df.columns:\n    if train_df[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[f].values)) \n        train_df[f] = lbl.transform(list(train_df[f].values))\n        \ntrain_y = train_df.price_doc.values\ntrain_X = train_df.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "e7ffc622f2bd590c128dc8525a0b347ae249a7d9", "collapsed": false}, "source": "train_df.columns.values"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "59ae08a1a3443d7847ed6827fa8cc1071615bcf0", "collapsed": false}, "source": "print(train_X.shape)\nprint(train_y.shape)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "ba9fe6328a9b0fe11892c85441551c550464a834", "collapsed": false}, "source": "#model gradient boost\n\n#clfdata_X = pd.DataFrame(np.nan_to_num(train_df.drop(['id','timestamp','price_doc'],axis=1)))\n#clfdata_y = pd.DataFrame(np.nan_to_num(train_df['price_doc']))\n\n#train_X, X_val, train_y, y_val = train_test_split(clfdata_X, clfdata_y, test_size=0.30,random_state=21)\n\ntrain_X1 = np.nan_to_num(train_X[:25000]).astype(int)\nval_X = np.nan_to_num(train_X[25000:]).astype(int)\ntrain_y1 = np.nan_to_num(train_y[:25000]).astype(int)\nval_y = np.nan_to_num(train_y[25000:]).astype(int)\n"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "9f25d8fb777965116788ec52c36b36893157a848", "collapsed": false}, "source": "GBclf= GradientBoostingClassifier(max_depth=4,min_samples_leaf=2)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "4da316d50e9144508fd7f538dab51ad27b03acd3", "collapsed": false}, "source": "clfX_train.head(1)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "f7cfa4dec023f2e52d639585dc93e74d060065ad", "collapsed": false}, "source": "train_y1[0]"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "3037d6ad53f9c1763f5c6a1f120cf8d7901f3101", "collapsed": false}, "source": "#train_X, X_val, train_y, y_val\n\nGBclf.fit(train_X1,train_y1)\nGBclf.score(val_X,val_y)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "5854d758ef6a45b742ee0d226543a251a1907baf", "collapsed": false}, "source": "#predict = GBmodel.predict(test_df.drop([\"id\", \"timestamp\"],axis=1))\npredict = GBclf.predict(test_df.drop(['id','timestamp'],axis=1))\noutput = pd.DataFrame({'id': id_test, 'price_doc': np.expm1(predict)})\n#output['price_doc'] = lab\n"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "0905c853a6fca6e8c3d59ff5165e2c4af783e0fc", "collapsed": false}, "source": "output.to_csv('Sberbank_GBclf.csv', index=False)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "3e7eb8af761a89d9213aea009cefc0d9bb4d64be", "_cell_guid": "31677ea2-0321-f709-dac8-120a2d02278f"}, "source": "if run1 == True:\n\n    xgb_params = {\n        'eta': 0.05,\n        'max_depth': 6,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'silent': 1\n    }\n    dtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\n    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100)\n\n    # plot the important features #\n    fig, ax = plt.subplots(figsize=(12,18))\n    xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n    plt.show()"}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "d0e40ffae4774d0972a338387936e0d26577a1b1", "_cell_guid": "7f10f4aa-05e8-0818-505e-9a5166e6a52b"}, "source": ["So the top 5 variables and their description from the data dictionary are:\n", "\n", " 1. full_sq - total area in square meters, including loggias, balconies and other non-residential areas\n", " 2. life_sq - living area in square meters, excluding loggias, balconies and other non-residential areas\n", " 3. floor - for apartments, floor of the building\n", " 4. max_floor - number of floors in the building\n", " 5. build_year - year built\n", "\n", "Now let us see how these important variables are distributed with respect to target variable.\n", "\n", "**Total area in square meters:**"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "d6e189c8c8b03f60e6515c28447480634d962282", "collapsed": false}, "source": "if run1 == True:\n    print(id_test[:10])"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "c0709046ec0bf8dcdb08ae46850293480e6f01df", "collapsed": false}, "source": "if run1 == True:\n    df_test.drop([\"id\", \"timestamp\"], axis=1, inplace=True)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "e5ed31f2fee5e46cfbfe6566a214fab826ce1970", "collapsed": false}, "source": "if run1 == True:\n    dtest = xgb.DMatrix(df_test, feature_names=train_X.columns)\n\n    y_pred = model.predict(dtest)\n\n    y_pred = np.round(y_pred * 1.008)\n    df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n\n    df_sub.to_csv('Sberbank_0.csv', index=False)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "8e14f8985273da8b45decf7d8f2e09e257d6bd55", "collapsed": false}, "source": "df_sub.head()"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "8d9c3311d8a53b8867d73c0296d92b6973b1131b", "_cell_guid": "407b8469-fcc2-5d41-44de-2b3833523006"}, "source": "if run2 == True:\n    ulimit = np.percentile(train_df.price_doc.values, 99.5)\n    llimit = np.percentile(train_df.price_doc.values, 0.5)\n    train_df['price_doc'].ix[train_df['price_doc']>ulimit] = ulimit\n    train_df['price_doc'].ix[train_df['price_doc']<llimit] = llimit\n\n    col = \"full_sq\"\n    ulimit = np.percentile(train_df[col].values, 99.5)\n    llimit = np.percentile(train_df[col].values, 0.5)\n    train_df[col].ix[train_df[col]>ulimit] = ulimit\n    train_df[col].ix[train_df[col]<llimit] = llimit\n\n    plt.figure(figsize=(12,12))\n    sns.jointplot(x=np.log1p(train_df.full_sq.values), y=np.log1p(train_df.price_doc.values), size=10)\n    plt.ylabel('Log of Price', fontsize=12)\n    plt.xlabel('Log of Total area in square metre', fontsize=12)\n    plt.show()"}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "8b85d4f4689420f66ac2ceaeaf1f2eb9eb597fa6", "_cell_guid": "c6ec261b-ba31-57a5-999a-5efcead8dbff"}, "source": ["**Living area in square meters:**"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "be45ff1aa95a76a03ef92d8840db6674d39c9236", "_cell_guid": "e4e720e7-075c-0bed-2982-61c3a97775b0"}, "source": "if run2 == True:\n    col = \"life_sq\"\n    train_df[col].fillna(0, inplace=True)\n    ulimit = np.percentile(train_df[col].values, 95)\n    llimit = np.percentile(train_df[col].values, 5)\n    train_df[col].ix[train_df[col]>ulimit] = ulimit\n    train_df[col].ix[train_df[col]<llimit] = llimit\n\n    plt.figure(figsize=(12,12))\n    sns.jointplot(x=np.log1p(train_df.life_sq.values), y=np.log1p(train_df.price_doc.values), \n              kind='kde', size=10)\n    plt.ylabel('Log of Price', fontsize=12)\n    plt.xlabel('Log of living area in square metre', fontsize=12)\n    plt.show()"}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "18dd56b77665d5d91a178940e3dd9fd90788b2ae", "_cell_guid": "038f8a9b-0b95-633d-6a8d-0bbfa10116f3"}, "source": ["**Floor:**\n", "\n", "We will see the count plot of floor variable."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "392f35c55ac6551ed3eed3ec2207b51efa09df69", "_cell_guid": "0c89affd-44b0-6c6c-f616-8bb2c518957a"}, "source": "if run2 == True:\n    plt.figure(figsize=(12,8))\n    sns.countplot(x=\"floor\", data=train_df)\n    plt.ylabel('Count', fontsize=12)\n    plt.xlabel('floor number', fontsize=12)\n    plt.xticks(rotation='vertical')\n    plt.show()"}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "48a1c4fd7c60e7d8d95a4846c495e189bef0e876", "_cell_guid": "0e11ee5f-88bb-b092-e522-64c91cee25b0"}, "source": ["The distribution is right skewed. There are some good drops in between (5 to 6, 9 to 10, 12 to 13, 17 to 18). Now let us see how the price changes with respect to floors."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "48302704fd2146918f0ac6cef842062ad6b69666", "_cell_guid": "a0cae509-6568-0b07-ae4c-f5adf825f709"}, "source": "if run2 == True:\n    grouped_df = train_df.groupby('floor')['price_doc'].aggregate(np.median).reset_index()\n    plt.figure(figsize=(12,8))\n    sns.pointplot(grouped_df.floor.values, grouped_df.price_doc.values, alpha=0.8, color=color[2])\n    plt.ylabel('Median Price', fontsize=12)\n    plt.xlabel('Floor number', fontsize=12)\n    plt.xticks(rotation='vertical')\n    plt.show()"}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "902e2078d0784b7d48331d4c7d65fa5072b5a300", "_cell_guid": "af624cc2-1563-02e1-c87b-47b0bfbaacfd"}, "source": ["This shows an overall increasing trend (individual houses seems to be costlier as well - check price of 0 floor houses). \n", "A sudden increase in the house price is also observed at floor 18.\n", "\n", "**Max floor:**\n", "\n", "Total number of floors in the building is one another important variable. So let us plot that one and see."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "dffc078d6d4406260dc34047f8d8b86645670431", "_cell_guid": "7bd45c6e-b9e2-c95c-fe31-52e307e2b1ca"}, "source": "if run2 == True:\n    plt.figure(figsize=(12,8))\n    sns.countplot(x=\"max_floor\", data=train_df)\n    plt.ylabel('Count', fontsize=12)\n    plt.xlabel('Max floor number', fontsize=12)\n    plt.xticks(rotation='vertical')\n    plt.show()"}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "207709020a12338d3f8c137d6a97610b79be04c5", "_cell_guid": "39981f77-9e03-79d5-33c9-91e4325d8ab6"}, "source": ["We could see that there are few tall bars in between (at 5,9,12,17 - similar to drop in floors in the previous graph). May be there are some norms / restrictions on the number of maximum floors present(?). \n", "\n", "Now let us see how the median prices vary with the max floors. "]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "c5a437ea8ba8c7dc93bcfc8ba4a83cef0c0b9ab9", "_cell_guid": "5087aae6-a3e9-fef6-9d05-e8240716a9c1"}, "source": "if run2 == True:\n    plt.figure(figsize=(12,8))\n    sns.boxplot(x=\"max_floor\", y=\"price_doc\", data=train_df)\n    plt.ylabel('Median Price', fontsize=12)\n    plt.xlabel('Max Floor number', fontsize=12)\n    plt.xticks(rotation='vertical')\n    plt.show()"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "9800607de67725e9cb9e54105a59e420c5a4e931", "_cell_guid": "bed7cbff-146d-91cc-d7d0-45321542bf99"}, "source": "if run3 == True:\n    import numpy as np\n    import pandas as pd\n    import xgboost as xgb\n    import matplotlib.pyplot as plt"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "9063f65e02219f46da84cbd716f7161d1b16c870", "_cell_guid": "28bf4b7f-742c-450f-cd3c-67d29e975b78"}, "source": "if run3 == True:\n    df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n    df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\n    df_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "598b57899d177a82eb77d899590831086ba6a670", "_cell_guid": "d227967e-1880-4c24-2bbc-87572577fbad"}, "source": "if run3 == True:\n    df_train.head()"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "ca26b4913c1e07db0a113cdd943865e226e7bffb", "_cell_guid": "caeb0606-e028-6bd8-c2c5-a72cce548282"}, "source": "if run3 == True:\n    # cleanup\n    print(df_train.shape)\n    df_train.loc[df_train.full_sq == 0, 'full_sq'] = 30\n    df_train = df_train[df_train.price_doc/df_train.full_sq <= 600000]\n    df_train = df_train[df_train.price_doc/df_train.full_sq >= 10000]\n    print(df_train.shape)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "de44e3b90f51ed5318a1adbdc423cf4f3959eb2c", "_cell_guid": "045214cb-205f-6f1d-54b8-e936b9899877"}, "source": "if run3 == True:\n    #print(df_train.loc[df_train.full_sq == 30, 'full_sq'])"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "8a21c01ed2c058927d29c1f1103dd63fa68bb29b", "_cell_guid": "57f20de4-5715-c6b9-5666-ed35bb1ad469"}, "source": "if run3 == True:\n    print(df_test.shape+df_train.shape)\n    #print(df_train.shape)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "98a21b699fb16f84618bd3cbec545bce69531e5e", "_cell_guid": "601fa6a6-c1f3-e94f-1b4e-f7abbef2f5ed"}, "source": "if run3 == True:\n    #print(df_test['id'][:10])\n    #df_test.head()\n    #df_train.head()"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "1622a921f41db4ac121115afbd0dd766e9170821", "_cell_guid": "19c60f35-d786-b9cf-d52e-71922644c262"}, "source": "if run3 == True:\n    #print(df_train.loc[df_train.full_sq == 30, 'price_doc'],  df_train.loc[df_train.full_sq == 30, 'full_sq'])"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "47c497960142e14ba8c77f7a8ae9e14c02ddb717", "_cell_guid": "96d3b42e-adc8-e062-d765-64977b487398"}, "source": "if run3 == True:\n    y_train = df_train['price_doc'].values\n    id_test = df_test['id']\n\n    df_train.drop(['id', 'price_doc'], axis=1, inplace=True)\n    df_test.drop(['id'], axis=1, inplace=True)\n\n    # Build df_all = (df_train+df_test).join(df_macro)\n    num_train = len(df_train)\n    df_all = pd.concat([df_train, df_test])\n    df_all = df_all.join(df_macro, on='timestamp', rsuffix='_macro')\n    print(df_all.shape)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "8c1e6c58a869bb189d1d5d60ebe4c89faf4d0f8b", "_cell_guid": "df0a2031-4c31-3d44-b9dc-82d1c5f308af"}, "source": "if run3 == True:\n    #df_all.describe"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "c8e7290be6767e7f51d56b1d393b9a2ab81dc85c", "_cell_guid": "b67e1515-a5a8-0429-3550-61e22f0098e8"}, "source": "if run3 == True:\n    # Add month-year\n    #year= df_all.timestamp.dt.year\n    #year_cnt_map = year.value_counts().to_dict()\n    #df_all['year_cnt'] = year.map(year_cnt_map)\n    #df_all['Age_building']=2018-df_all['build_year']\n\n    # Other feature engineering\n    df_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\n    df_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n\n    # Remove timestamp column (may overfit the model in train)\n    df_all.drop(['timestamp', 'timestamp_macro'], axis=1, inplace=True)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "f50a1362205ea90d4b304e4fbdaf18088e671d6c", "_cell_guid": "2ce28202-fa7a-aa37-1d61-109bb8014b68"}, "source": "if run3 == True:\n    factorize = lambda t: pd.factorize(t[1])[0]"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "fd4364c1b859625b5ccc33ac795ac26c09670736", "_cell_guid": "6ce1902d-076d-edbe-728b-2c5de0243289"}, "source": "if run3 == True:\n    df_obj = df_all.select_dtypes(include=['object'])"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "019560f2a795ea7a247f6ed246edebedcfdeb52e", "_cell_guid": "d481411f-926e-67fc-f802-49cec928e479"}, "source": "if run3 == True:\n    df_obj.shape"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "ab02ef737ce65698ecb5cb365a05d1c2f17d884b", "_cell_guid": "0427f6a2-b523-66de-2342-826cc15cde9c"}, "source": "if run3 == True:\n    z = np.array(list(map(factorize, df_obj.iteritems()))).T"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "36661b908e634053f1da5a5836c3a7e377928e9e", "_cell_guid": "3b6b89ca-9ce4-ccfc-2d81-5288f1eabea0"}, "source": "if run3 == True:\n    q = np.array([x for x in df_obj.iteritems()]).T"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "d83b16a1fee40c1b13ce939f9c4e759d333b24bf", "_cell_guid": "eba96579-82eb-88bb-9640-84eed42b5f21"}, "source": "if run3 == True:\n    print(q[:10])"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "2aaee46b7bf869defe864279f48e84e673a09b47", "_cell_guid": "e52b372b-5a10-7f14-5727-d895a13da240"}, "source": "if run3 == True:\n    #df_obj = df_all.select_dtypes(include=['object'])\n\n    #X_all = np.c_[\n    #    df_all.select_dtypes(exclude=['object']).values,\n    #    np.array(list(map(factorize, df_obj.iteritems()))).T\n    #]\n    #print(X_all.shape)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "ee407a94038e471a348b19849d738c6430f99c34", "_cell_guid": "e530b589-83b9-7159-84f2-8a07ce19f4d1"}, "source": "if run3 == True:\n    #X_train = X_all[:num_train]\n    #X_test = X_all[num_train:]"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "f5f993671057a942fdd449c859b7c12df8ac21d6", "_cell_guid": "1b74da38-3e8e-d106-f802-2cd425cfc6e2"}, "source": "if run3 == True:\n    # Deal with categorical values\n    df_numeric = df_all.select_dtypes(exclude=['object'])\n    df_obj = df_all.select_dtypes(include=['object']).copy()\n\n    for c in df_obj:\n        df_obj[c] = pd.factorize(df_obj[c])[0]\n\n    df_values = pd.concat([df_numeric, df_obj], axis=1)\n\n\n    # Convert to numpy values\n    X_all = df_values.values\n    print(X_all.shape)\n\n    X_train = X_all[:num_train]\n    X_test = X_all[num_train:]\n\n    df_columns = df_values.columns\n    X_train=np.nan_to_num(X_train)\n    X_test=np.nan_to_num(X_test)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "e43cf725ed11938530154bb7916cdfcafcfc06d4", "_cell_guid": "2b7fc4fd-2c94-c3cd-f002-e181dce7a816"}, "source": "if run3 == True:\n    xgb_params = {\n        'eta': 0.05,\n        'max_depth': 5,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'silent': 1\n    }\n\n    dtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\n    dtest = xgb.DMatrix(X_test, feature_names=df_columns)\n"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "3326ea690dc05736ff65d3ef2ee679312712420e", "_cell_guid": "61ff35f7-6137-de13-80e6-680b3166f552"}, "source": "if run3 == True:\n    # Uncomment to tune XGB `num_boost_rounds`\n\n    #cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n    #    verbose_eval=True, show_stdv=False)\n    #cv_result[['train-rmse-mean', 'test-rmse-mean']].plot()\n    #num_boost_rounds = len(cv_result)\n\n    num_boost_round = 489\n\n    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "491f484ae52cfb9e7af020109c1a369b59fcdec6", "_cell_guid": "07c0ea31-b20f-cdc3-e073-35fd04931f4f"}, "source": "if run3 == True:\n    fig, ax = plt.subplots(figsize=(12,18))\n    xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n    plt.show()"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "048f7f95e78527b19199775cdd7bb09123a4e8b8", "_cell_guid": "6ab9b9db-060c-2f2a-fc24-3cdd5a932434"}, "source": "if run3 == True:\n    #fig, ax = plt.subplots(1, 1, figsize=(8, 16))\n    #xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)\n\n    y_pred = model.predict(dtest)\n    y_pred = np.round(y_pred * 1.008)\n    df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n\n    df_sub.to_csv('Sberbank_1.csv', index=False)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "2b83067bc97e2eb0640882240cb5de1bb8862005", "_cell_guid": "770e179f-15e9-f8a4-e7c9-01ab642c777e"}, "source": "if run3 == True:\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn import model_selection, preprocessing\n    import xgboost as xgb\n    import datetime"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "54bf825fa723a44e0b2d65ab0bf168b4d0b9f1bc", "_cell_guid": "240371ae-87a1-8182-00de-7b8f1f385d2a"}, "source": "if run3 == True:\n    #load files\n    train = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\n    test = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\n    macro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])\n    id_test = test.id\n\n    #multiplier = 0.969"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "fbb96fc660ab6c634befefeeea1a242be87e5821", "_cell_guid": "40b21bff-8026-90aa-c250-dd47ea913ef5"}, "source": "if run3 == True:\n    #clean data\n    bad_index = train[train.life_sq > train.full_sq].index\n    train.ix[bad_index, \"life_sq\"] = np.NaN\n    equal_index = [601,1896,2791]\n    test.ix[equal_index, \"life_sq\"] = test.ix[equal_index, \"full_sq\"]\n    bad_index = test[test.life_sq > test.full_sq].index\n    test.ix[bad_index, \"life_sq\"] = np.NaN\n    bad_index = train[train.life_sq < 5].index\n    train.ix[bad_index, \"life_sq\"] = np.NaN\n    bad_index = test[test.life_sq < 5].index\n    test.ix[bad_index, \"life_sq\"] = np.NaN\n    bad_index = train[train.full_sq < 5].index\n    train.ix[bad_index, \"full_sq\"] = np.NaN\n    bad_index = test[test.full_sq < 5].index\n    test.ix[bad_index, \"full_sq\"] = np.NaN\n    kitch_is_build_year = [13117]\n    train.ix[kitch_is_build_year, \"build_year\"] = train.ix[kitch_is_build_year, \"kitch_sq\"]\n    bad_index = train[train.kitch_sq >= train.life_sq].index\n    train.ix[bad_index, \"kitch_sq\"] = np.NaN\n    bad_index = test[test.kitch_sq >= test.life_sq].index\n    test.ix[bad_index, \"kitch_sq\"] = np.NaN\n    bad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index\n    train.ix[bad_index, \"kitch_sq\"] = np.NaN\n    bad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index\n    test.ix[bad_index, \"kitch_sq\"] = np.NaN\n    bad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index\n    train.ix[bad_index, \"full_sq\"] = np.NaN\n    bad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index\n    test.ix[bad_index, \"full_sq\"] = np.NaN\n    bad_index = train[train.life_sq > 300].index\n    train.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n    bad_index = test[test.life_sq > 200].index\n    test.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n    train.product_type.value_counts(normalize= True)\n    test.product_type.value_counts(normalize= True)\n    bad_index = train[train.build_year < 1500].index\n    train.ix[bad_index, \"build_year\"] = np.NaN\n    bad_index = test[test.build_year < 1500].index\n    test.ix[bad_index, \"build_year\"] = np.NaN\n    bad_index = train[train.num_room == 0].index \n    train.ix[bad_index, \"num_room\"] = np.NaN\n    bad_index = test[test.num_room == 0].index \n    test.ix[bad_index, \"num_room\"] = np.NaN\n    bad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\n    train.ix[bad_index, \"num_room\"] = np.NaN\n    bad_index = [3174, 7313]\n    test.ix[bad_index, \"num_room\"] = np.NaN\n    bad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index\n    train.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\n    bad_index = train[train.floor == 0].index\n    train.ix[bad_index, \"floor\"] = np.NaN\n    bad_index = train[train.max_floor == 0].index\n    train.ix[bad_index, \"max_floor\"] = np.NaN\n    bad_index = test[test.max_floor == 0].index\n    test.ix[bad_index, \"max_floor\"] = np.NaN\n    bad_index = train[train.floor > train.max_floor].index\n    train.ix[bad_index, \"max_floor\"] = np.NaN\n    bad_index = test[test.floor > test.max_floor].index\n    test.ix[bad_index, \"max_floor\"] = np.NaN\n    train.floor.describe(percentiles= [0.9999])\n    bad_index = [23584]\n    train.ix[bad_index, \"floor\"] = np.NaN\n    train.material.value_counts()\n    test.material.value_counts()\n    train.state.value_counts()\n    bad_index = train[train.state == 33].index\n    train.ix[bad_index, \"state\"] = np.NaN\n    test.state.value_counts()"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "544679fc283f94fb89cce0741f5fc08f6e24ee1e", "_cell_guid": "eb830d0a-b8ac-7e95-7a74-ac341de9d099"}, "source": "if run3 == True:\n    # brings error down a lot by removing extreme price per sqm\n    train.loc[train.full_sq == 0, 'full_sq'] = 50\n    train = train[train.price_doc/train.full_sq <= 600000]\n    train = train[train.price_doc/train.full_sq >= 10000]"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "2f705dda371dea4c6c2c595f5e33acb9d1d348f1", "_cell_guid": "77b4a188-6e3d-4a23-b4c0-4c6069551135"}, "source": "if run3 == True:\n    # Add month-year\n    month_year = (train.timestamp.dt.month + train.timestamp.dt.year * 100)\n    month_year_cnt_map = month_year.value_counts().to_dict()\n    train['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n    month_year = (test.timestamp.dt.month + test.timestamp.dt.year * 100)\n    month_year_cnt_map = month_year.value_counts().to_dict()\n    test['month_year_cnt'] = month_year.map(month_year_cnt_map)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "4c9b29e6d628b941f8d7c301b0eb5e7836c61ac3", "_cell_guid": "d5a3ec13-ca69-ba1b-7c43-c71960f1810b"}, "source": "if run3 == True:\n    # Add week-year count\n    week_year = (train.timestamp.dt.weekofyear + train.timestamp.dt.year * 100)\n    week_year_cnt_map = week_year.value_counts().to_dict()\n    train['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n    week_year = (test.timestamp.dt.weekofyear + test.timestamp.dt.year * 100)\n    week_year_cnt_map = week_year.value_counts().to_dict()\n    test['week_year_cnt'] = week_year.map(week_year_cnt_map)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "300165f3052b377460070558aa5fc057edd86ca8", "_cell_guid": "9f0ec436-0517-1142-ff31-2232c1a7e627"}, "source": "if run3 == True:\n    # Add month and day-of-week\n    train['month'] = train.timestamp.dt.month\n    train['dow'] = train.timestamp.dt.dayofweek\n\n    test['month'] = test.timestamp.dt.month\n    test['dow'] = test.timestamp.dt.dayofweek"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "e160d3f381ea12704ca06a3d02cb1fcf72360401", "_cell_guid": "aa245a0d-ae75-8a01-fc9e-a9b374ebb5e4"}, "source": "if run3 == True:\n    # Other feature engineering\n    train['rel_floor'] = train['floor'] / train['max_floor'].astype(float)\n    train['rel_kitch_sq'] = train['kitch_sq'] / train['full_sq'].astype(float)\n\n    test['rel_floor'] = test['floor'] / test['max_floor'].astype(float)\n    test['rel_kitch_sq'] = test['kitch_sq'] / test['full_sq'].astype(float)\n\n    train.apartment_name=train.sub_area + train['metro_km_avto'].astype(str)\n    test.apartment_name=test.sub_area + train['metro_km_avto'].astype(str)\n\n    train['room_size'] = train['life_sq'] / train['num_room'].astype(float)\n    test['room_size'] = test['life_sq'] / test['num_room'].astype(float)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "7a67bc962e4eee7cc6134e6921721ed06b714168", "_cell_guid": "107c8731-16cc-730e-4c46-3864a8f368d7"}, "source": "if run3 == True:\n    y_train = train[\"price_doc\"]\n    x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n    x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n\n    for c in x_train.columns:\n        if x_train[c].dtype == 'object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(x_train[c].values)) \n            x_train[c] = lbl.transform(list(x_train[c].values))\n        #x_train.drop(c,axis=1,inplace=True)\n        \n    for c in x_test.columns:\n        if x_test[c].dtype == 'object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(x_test[c].values)) \n            x_test[c] = lbl.transform(list(x_test[c].values))\n            #x_test.drop(c,axis=1,inplace=True)  "}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "7dc030604ea401709caaca86175cc8f7273302a1", "_cell_guid": "32b41275-2ad5-07ae-9db9-8b26d0ac217c"}, "source": "if run3 == True:\n    xgb_params = {\n        'eta': 0.05,\n        'max_depth': 5,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'silent': 1\n    }\n\n    dtrain = xgb.DMatrix(x_train, y_train)\n    dtest = xgb.DMatrix(x_test)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "0afa70f27c09555faee6b1fb7e1ed138760d9598", "_cell_guid": "38814d59-0557-00ec-313e-39d9282518dc"}, "source": "if run3 == True:\n    #cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n    #    verbose_eval=50, show_stdv=False)\n    #cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\n\n    #num_boost_rounds = len(cv_output)\n    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= 350)\n\n    #fig, ax = plt.subplots(1, 1, figsize=(8, 13))\n    #xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)\n\n    y_predict = model.predict(dtest)\n    y_predict = np.round(y_predict * 0.99)\n    output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n    output.head()\n"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "9576c64f3a88932617de084986037a737dc05a55", "_cell_guid": "224db05c-65fc-4b5d-3782-1d339fede7bd"}, "source": "if run3 == True:\n    output.to_csv('Sberbank_2.csv', index=False)"}]}