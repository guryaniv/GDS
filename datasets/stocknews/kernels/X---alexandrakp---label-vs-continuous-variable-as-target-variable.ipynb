{"metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.5.4", "name": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "name": "tensorflow", "display_name": "Tensorflow Env"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<h1> Daily News for Stock Market Prediction </h1>\n", "\n", "Thanks to aaron7sun for the idea and data!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<h2> Data Preparation </h2>\n", "<h3> Import data</h3>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Import two tables: the pre-combined and the raw djia data and combine them to one DataFrame."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "source": ["import numpy as np\n", "import pandas as pd\n", "from IPython.display import display # Allows the use of display() for DataFrames\n", "import matplotlib.pyplot as pl\n", "import matplotlib.patches as mpatches\n", "\n", "\n", "# Pretty display for notebooks\n", "%matplotlib inline\n", "\n", "#the table combined_news= pd.read_csv('Combined_News_DJIA.csv') has the Top25 news headlines as single columns, the date \n", "# and the label\n", "# label 1: the stock price increased\n", "# label 2: the stock prce decreased\n", "\n", "\n", "combined_news= pd.read_csv('../input/Combined_News_DJIA.csv')\n", "\n", "#combine the 25 headlines to one single long string in one column\n", "combined_news['CombinedTop'] = combined_news.iloc[:,2:].astype(str).apply(' '.join, axis=1)\n", "\n", "#add continous data from the DJIA_table.csv table: difference from open and close value\n", "djia_data = pd.read_csv('../input/DJIA_table.csv')\n", "djia_data['diff']=djia_data['Close']-djia_data['Open']\n", "\n", "#merge both tables to one\n", "combined_data = pd.merge(left=djia_data[['Date', 'diff']],right=combined_news[['Date',  'Label', 'CombinedTop']], \n", "                         left_on='Date', right_on='Date')\n", "print (combined_data.shape)\n", "display(combined_data.head(5))\n"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["#missing data?\n", "\n", "missing = combined_data.loc[combined_data['CombinedTop'] == '']\n", "print (missing)\n", "\n", "missing = combined_data.loc[combined_data['CombinedTop'] == 'NaN']\n", "print (missing)\n", "\n", "missing_diff = combined_data.loc[combined_data['diff'] == None]\n", "print (missing_diff)\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["There are no missing values."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["print(\"Describing statistics about the diff feature.\")\n", "print (combined_data['diff'].describe())\n", "pl.xlabel('Diff')\n", "pl.ylabel('Amount')\n", "pl.hist(combined_data['diff'])\n", "pl.show()"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The most used 50 words excluding english stop words:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "from collections import Counter\n", "\n", "ngram_vectorizer = CountVectorizer(analyzer='word', stop_words = 'english', ngram_range=(1, 1), min_df=1)\n", "\n", "# X matrix where the row represents sentences and column is our one-hot vector for each token in our vocabulary\n", "X = ngram_vectorizer.fit_transform(combined_data['CombinedTop'])\n", "\n", "# Vocabulary\n", "vocab = list(ngram_vectorizer.get_feature_names())\n", "\n", "# Column-wise sum of the X matrix.\n", "counts = X.sum(axis=0).A1\n", "\n", "freq_distribution = Counter(dict(zip(vocab, counts)))\n", "most_frequent = freq_distribution.most_common(25)\n", "\n", "print (most_frequent)\n", "\n", "word_list, amount_list = zip(*most_frequent)\n", "\n", "df = pd.DataFrame(list(zip(amount_list, word_list))).set_index(1)\n", "\n", "df.plot.barh()\n"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["#textlength\n", "\n", "pl.xlabel('Length of words in headlines')\n", "length_of_words = combined_data['CombinedTop'].str.split(\" \").str.len()\n", "pl.hist(length_of_words)\n", "pl.show()\n", "display(length_of_words.describe())"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<h3>Split data into training and testing set</h3>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Split the data into training and testing sets\n", "# as instructed in the dataset the train / test split is at the 01 Jan 2015\n", "\n", "train = combined_data[combined_data['Date'] < '2015-01-01']\n", "test = combined_data[combined_data['Date'] > '2014-12-31']\n", "\n", "train_features = train['CombinedTop']\n", "test_features = test['CombinedTop']\n", "\n", "labels_train = train['Label']\n", "labels_test = test['Label']\n", "\n", "diff_y_train = train['diff']\n", "diff_y_test = test['diff']"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["#Can accuracy be used as a valid measure? Meaning: how is the proportion of label 0 and 1\n", "\n", "print (combined_data['Label'].value_counts())"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The frequency of label 0 and 1 is very similar. Therefore, accuracy can be used for evaluation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<h3>Vectorize words to features</h3>\n", "\n", "First, in order for the text to be usable the words need to turned into single features. Two different alternatives are tried out: every single word as one feature and two to three words together as one feature. Later both alternatives will be tested in the model to see, if one or the other performs better."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["#takes an vectorizer and returnes the transformed train and test set as well as the names of each feature\n", "def vectorize_words (vectorizer):\n", "   \n", "    features_train_transformed = vectorizer.fit_transform(train_features)\n", "    features_test_transformed  = vectorizer.transform(test_features).toarray()\n", "    feature_names = vectorizer.get_feature_names()\n", "\n", "    print(\"Shape of vectorized words: \" + str(features_train_transformed.shape))\n", "    \n", "    return features_train_transformed, features_test_transformed, feature_names"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["#use tfidf vectorizer\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "\n", "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=.5,\n", "                             stop_words='english')\n", "\n", "standard_train, standard_test, standard_feature_names = vectorize_words(vectorizer)\n", "\n", "#use tfidf vectorizer with multiple words\n", "vectorizer_mult = TfidfVectorizer(sublinear_tf=True, min_df = 0.04, max_df=.8, ngram_range=(2, 3),\n", "                             stop_words='english')\n", "\n", "mult_train, mult_test, mult_feature_names = vectorize_words(vectorizer_mult)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["<h2>Feature selection</h2>\n", "\n", "<h3>Univariate feature selection</h3>\n", "Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["#takes a percentile, train and test data and the feature names as input\n", "#returns the x percent best features for prediction of the given label\n", "\n", "def select_percentile(percentile, train_data, test_data, target_variable, feature_names):\n", "        selector = SelectPercentile(f_classif, percentile=percentile)\n", "        selector.fit(train_data, target_variable)\n", "        final_train = selector.transform(train_data)\n", "        final_test  = selector.transform(test_data)\n", "        index_converter =  np.asarray(feature_names)[selector.get_support()]\n", "        print(\"Matrix of selected words: \")\n", "        display(final_train)\n", "        \n", "        return final_train, final_test, index_converter"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["from sklearn.feature_selection import SelectPercentile, f_classif\n", "\n", "### feature selection, because text is super high dimensional and \n", "### can be really computationally chewy as a result\n", "standard_final_train, standard_final_test, standard_index_converter = select_percentile(10, \n", "                                                                standard_train, standard_test, labels_train, standard_feature_names)\n", "\n", "#2-3 word snippets already reduced to 52 words. No further selection needed\n", "\n", "\n", "# continous data as target variable\n", "cont_final_train, cont_final_test, cont_index_converter = select_percentile(10, \n", "                                                                standard_train, standard_test, diff_y_train, standard_feature_names)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["<h2>Models</h2>\n", "\n", "Different machine learning models will be tried out to analyse the predicting power of news. First the label categorizing the data into \"price going up\" and \"price going down\" will be used as the target label. Since there are only two possible outcomes, the baseline comparison is chance with 50%. The model should perform better, otherwise it has no value for the prediction."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "<h3>Decision Tree</h3>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["\n", "\n", "from sklearn.metrics import accuracy_score\n", "\n", "#input variables: the decision tree classifier, train and test data and the wordlist of the features\n", "#output: the accuracy of the model and the sorted list of best predicting features\n", "\n", "def use_decision_tree(dtClassifier, train_data, test_data, word_list):\n", "    \n", "    dtClassifier.fit(train_data, labels_train)\n", "   \n", "    index = []\n", "    scores = []\n", "    words = []\n", "    \n", "    #put the feature importance of each feature together with the wordd\n", "    for x  in range (0, len(dtClassifier.feature_importances_)): \n", "        if dtClassifier.feature_importances_[x] > 0.015:\n", "            index.append('Index: ' + str(x))\n", "            scores.append(dtClassifier.feature_importances_[x])\n", "            words.append(word_list[x])\n", "\n", "    decision_tree_selection = pd.DataFrame({'Index' : index, \n", "                            'Scores' : scores, 'Words': words})\n", "\n", "    decision_tree_selection = decision_tree_selection.sort_values(['Scores'], ascending=False)\n", "    return (dtClassifier, decision_tree_selection)\n", "   "], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "source": ["from sklearn.tree import DecisionTreeClassifier\n", "\n", "dtClassifier = DecisionTreeClassifier(random_state= 0,  min_samples_split= 10)\n", "dtClassifier, standard_dt_words = use_decision_tree(dtClassifier, \n", "                                        standard_final_train, standard_final_test, standard_index_converter)\n", "\n", "# to check for overfitting: how high is the accuarcy of the train data compared to the test data?\n", "dtPredTrain = dtClassifier.predict(standard_final_train)\n", "dtAccTrain = accuracy_score(dtPredTrain, labels_train)    \n", "print (\"Accuracy score for the decision tree train data: \" + str(dtAccTrain))\n", "\n", "# display the accuracy of the test data\n", "dtPred = dtClassifier.predict(standard_final_test)\n", "dtAcc = accuracy_score(dtPred, labels_test)    \n", "print (\"Accuracy score for the decision tree test data: \" + str(dtAcc))\n", "print\n", "print (\"most important words detected by decision tree:\")\n", "\n", "\n", "#plot the most important words\n", "display (standard_dt_words)\n", "index = np.arange(len(standard_dt_words))\n", "bar_width = 0.35\n", "pl.bar(index, standard_dt_words['Scores'], bar_width)\n", "pl.xticks(index + bar_width / 2, (standard_dt_words['Words']))\n", "pl.xlabel('Words')\n", "pl.ylabel('Scores')\n", "pl.title('Feature importance scoring')\n", "pl.tight_layout()\n", "pl.show"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# calculate the average accuracy for a certain min_sample_split over 100 different random states\n", "min_samples_split = 10\n", "\n", "dtAcc_for_loop = 0\n", "for random_state in range (0, 99):\n", "    dtClassifier_for_loop = DecisionTreeClassifier(random_state= random_state,  min_samples_split= min_samples_split)\n", "    dtClassifier_for_loop, standard_dt_words = use_decision_tree(dtClassifier_for_loop, \n", "                                        standard_final_train, standard_final_test, standard_index_converter)\n", "    dtPred_for_loop = dtClassifier_for_loop.predict(standard_final_test)\n", "    dtAcc_for_loop += accuracy_score(dtPred_for_loop, labels_test)  \n", "print (\"Over 100 random states average accuracy for min sample size of \", min_samples_split , \": \", dtAcc_for_loop/100)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# test the decision tree with the multiple words as features\n", "dtClassifier_mult = DecisionTreeClassifier(random_state= 0, min_samples_split= 10)\n", "\n", "dtClassifier_mult, mult_dt_words = use_decision_tree(dtClassifier_mult, mult_train, mult_test, mult_feature_names)\n", "dtPredMult = dtClassifier_mult.predict(mult_test)\n", "dtAccMult = accuracy_score(dtPredMult, labels_test)    \n", "print (\"Accuracy score for the decision tree: \" + str(dtAccMult))\n", "print\n", "print (\"most important words detected by decision tree:\")\n", "display(mult_dt_words)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The accuracy for for both decision trees - the one with only single words as features as well as the one with 2-3 words as features only perform as good as chance (about 50%)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<h3>Logistic regression</h3>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["#use logistic regression\n", "from sklearn.linear_model import LogisticRegression\n", "\n", "def logRegression (train_data, test_data, words):\n", "    logRegression = LogisticRegression()\n", "    logRegTest = logRegression.fit(train_data, labels_train)    \n", "    basiccoeffs = logRegTest.coef_.tolist()[0]\n", "    coeffdf = pd.DataFrame({'Words' : words, \n", "                            'Coefficients' : basiccoeffs})\n", "    coeffdf = coeffdf.sort_values(['Coefficients', 'Words'], ascending=[0, 1])\n", "\n", "    return logRegTest, coeffdf"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["logReg, standard_coeffdf = logRegression(standard_final_train, standard_final_test, standard_index_converter)\n", "\n", "logRegPred = logReg.predict(standard_final_train)\n", "standard_log_accuracy_train = accuracy_score(labels_train, logRegPred)    \n", "print('Standard - Logistic Regression accuracy Train Data: ', standard_log_accuracy_train)\n", "\n", "logRegPred = logReg.predict(standard_final_test)\n", "standard_log_accuracy = accuracy_score(labels_test, logRegPred)    \n", "\n", "print('Standard - Logistic Regression accuracy Test Data: ',standard_log_accuracy)\n", "standard_coeffdf.head(10)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["logRegMult, mult_coeffdf = logRegression(mult_train, mult_test, mult_feature_names)\n", "logRegPredMult = logRegMult.predict(mult_test)\n", "mult_log_accuracy = accuracy_score(labels_test, logRegPredMult) \n", "\n", "print('Mult - Logistic Regression accuracy: ',mult_log_accuracy)\n", "mult_coeffdf.head(10)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Also logistic regression is not doing better than chance. Also different words are pointed out as most important. Only \"force\", \"irish\" and \"iran\" make it into the top 10 of both - the decision tree and the logistic regression."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<h3>Neural Network</h3>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["from keras.models import Sequential\n", "import tensorflow as tf\n", "from keras.layers.core import Dense, Dropout, Activation, Lambda\n", "from keras.utils import np_utils\n", "from sklearn.model_selection import KFold\n", "\n", "batch_size = 32\n", "nb_classes = 2\n", "input_dim = standard_final_train.shape[1]\n", "\n", "\n", "#one-hot encode labels\n", "labels_train_encoded = Y_test = np_utils.to_categorical(labels_train, nb_classes)\n", "labels_test_encoded = Y_test = np_utils.to_categorical(labels_test, nb_classes)\n", "\n", "\n", "#build the sequential model with multiple dense, activation and dropout layers\n", "model = Sequential()\n", "model.add(Dense(256, input_dim=input_dim))\n", "model.add(Activation('relu'))\n", "model.add(Dropout(0.4))\n", "model.add(Dense(128))\n", "model.add(Activation('relu'))\n", "model.add(Dropout(0.4))\n", "model.add(Dense(nb_classes))\n", "model.add(Activation('softmax'))\n", "model.summary()\n", "\n", "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["epochs = 20\n", "\n", "model.fit(standard_final_train, labels_train_encoded, epochs= epochs, batch_size=16, validation_split=0.15)\n"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["dlPredTrain = model.predict_classes(standard_final_train, verbose=0)\n", "dlAccTrain = accuracy_score(labels_train, dlPredTrain)\n", "print (dlAccTrain)\n", "\n", "dlPred = model.predict_classes(standard_final_test, verbose=0)\n", "dlAcc = accuracy_score(labels_test, dlPred)\n", "print (dlAcc)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["So far none of the model seemed to do better than chance. \n", "Maybe if not is discrete label is used but a continous, the results turn out better."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<h3>Lasso Regression</h3>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["from sklearn import linear_model\n", "  \n", "def lasso_reg(train_data, test_data):\n", "    reg = linear_model.Lasso(alpha=0.1)\n", "    reg.fit(train_data, diff_y_train)\n", "    return reg\n", "\n", "reg = lasso_reg(cont_final_train, cont_final_test)\n", "lassoAccTrain = reg.score(cont_final_train, diff_y_train)\n", "print ('standard - Lasso Regression R2 Train Data: ', lassoAccTrain)\n", "lassoAcc = reg.score(cont_final_test, diff_y_test)\n", "print ('standard - Lasso Regression R2 Test Data: ', lassoAcc)\n", "\n", "regMult = lasso_reg(mult_train, mult_test)\n", "lassoAccMult = regMult.score(mult_test, diff_y_test)\n", "print ('mult - Lasso Regression R2: ', lassoAccMult) "], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["<h3>Recurrent Neural Network</h3>\n", "\n", "Since the data is a time sequence the data could be assumed to not only rely on the current input but also on the input of previous data points. Recurrent neural networks (RNN) include such \"memory\" and do not treat all inputs as being independent of each other."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# create and fit the LSTM network\n", "from keras.layers import LSTM\n", "\n", "\n", "standard_final_train_new= standard_final_train.toarray()\n", "\n", "# reshape input to be [samples, time steps, features]\n", "trainX = np.reshape(standard_final_train_new, (standard_final_train_new.shape[0], 1, standard_final_train_new.shape[1]))\n", "testX = np.reshape(standard_final_test, (standard_final_test.shape[0], 1, standard_final_test.shape[1]))\n", "\n", "model_lstm = Sequential()\n", "model_lstm.add(LSTM(4, batch_input_shape= (1, trainX.shape[1], trainX.shape[2]), stateful=True))\n", "model_lstm.add(Dense(1))\n", "model_lstm.summary()\n", "\n", "model_lstm.compile(loss='mean_squared_error', optimizer='RMSProp')\n", "\n", "epochs_lstm = 5\n", "\n", "#for i in range(epochs_lstm):\n", "model_lstm.fit(trainX, diff_y_train, epochs=epochs_lstm, batch_size=1, verbose=2, shuffle=False)\n", "#    model_lstm.reset_states()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["from math import sqrt\n", "from sklearn.metrics import mean_squared_error\n", "from sklearn.metrics import r2_score\n", "\n", "# make predictions\n", "trainPredict = model_lstm.predict(trainX, 1)\n", "testPredict = model_lstm.predict(testX, 1)\n", "\n", "# report performance\n", "rmse = sqrt(mean_squared_error(diff_y_train, trainPredict))\n", "print('Train RMSE: %.3f' % rmse)\n", "\n", "r2_train = r2_score(diff_y_train, trainPredict)\n", "print('Train r2: %.3f' % r2_train)\n", "\n", "rmse = sqrt(mean_squared_error(diff_y_test, testPredict))\n", "print('Test RMSE: %.3f' % rmse)\n", "r2_test = r2_score(diff_y_test, testPredict)\n", "print('Test r2: %.3f' % r2_test)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["Also the lasso regression and the RNN have R^2 values around 0 and therefore no prediction power. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": [], "outputs": []}], "nbformat": 4, "nbformat_minor": 1}