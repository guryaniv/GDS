{"cells": [{"metadata": {"_cell_guid": "907772f8-e7b0-4b30-b144-cc58259c9f10", "_uuid": "ca9caf3a492d2078ef878a7306614f8d9b84f4dd"}, "source": ["Also found on my Github profile Mizioand,\n", "https://github.com/MizioAnd/PortoSeguroInsur/blob/master/porto_seguro_insur.py"], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "8bf94415-ceb5-4709-9315-2e699625d640", "_uuid": "ce928bbdb584c99cf8b985f8f4fa4e91c9eeee47"}, "source": ["# porto_seguro_insur.py\n", "#  Assumes python vers. 3.6\n", "# __author__ = 'mizio'\n", "\n", "import csv as csv\n", "import numpy as np\n", "import pandas as pd\n", "import pylab as plt\n", "from fancyimpute import MICE\n", "import random\n", "from sklearn.model_selection import cross_val_score\n", "import datetime\n", "import tensorflow as tf\n", "\n", "\n", "class PortoSeguroInsur:\n", "    def __init__(self):\n", "        self.df = PortoSeguroInsur.df\n", "        self.df_test = PortoSeguroInsur.df_test\n", "        self.df_submission = PortoSeguroInsur.df_submission\n", "        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%Hh%Mm%Ss')\n", "\n", "\n", "    # Load data into Pandas DataFrame\n", "    # For .read_csv, always use header=0 when you know row 0 is the header row\n", "    df = pd.read_csv('../input/train.csv', header=0)\n", "    df_test = pd.read_csv('../input/test.csv', header=0)\n", "    df_submission = pd.read_csv('../input/sample_submission.csv', header=0)\n", "\n", "    @staticmethod\n", "    def features_with_null_logical(df, axis=1):\n", "        row_length = len(df._get_axis(0))\n", "        # Axis to count non null values in. aggregate_axis=0 implies counting for every feature\n", "        aggregate_axis = 1 - axis\n", "        features_non_null_series = df.count(axis=aggregate_axis)\n", "        # Whenever count() differs from row_length it implies a null value exists in feature column and a False in mask\n", "        mask = row_length == features_non_null_series\n", "        return mask\n", "\n", "    def missing_values_in_dataframe(self, df):\n", "        mask = self.features_with_null_logical(df)\n", "        print(df[mask[mask == 0].index.values].isnull().sum())\n", "        print('\\n')\n", "\n", "    @staticmethod\n", "    def extract_numerical_features(df):\n", "        df = df.copy()\n", "        df = df.copy()\n", "        non_numerical_feature_names = df.columns[np.where(PortoSeguroInsur.numerical_feature_logical_incl_hidden_num(\n", "            df) == 0)]\n", "        return non_numerical_feature_names\n", "\n", "    @staticmethod\n", "    def extract_non_numerical_features(df):\n", "        df = df.copy()\n", "        non_numerical_feature_names = df.columns[np.where(PortoSeguroInsur.numerical_feature_logical_incl_hidden_num(\n", "            df))]\n", "        return non_numerical_feature_names\n", "\n", "    @staticmethod\n", "    def numerical_feature_logical_incl_hidden_num(df):\n", "        logical_of_non_numeric_features = np.zeros(df.columns.shape[0], dtype=int)\n", "        for ite in np.arange(0, df.columns.shape[0]):\n", "            try:\n", "                str(df[df.columns[ite]][0]) + df[df.columns[ite]][0]\n", "                logical_of_non_numeric_features[ite] = True\n", "            except TypeError:\n", "                print('Oops')\n", "        return logical_of_non_numeric_features\n", "\n", "    def clean_data(self, df, is_train_data=1):\n", "        df = df.copy()\n", "        if df.isnull().sum().sum() > 0:\n", "            if is_train_data:\n", "                df = df.dropna()\n", "            else:\n", "                df = df.dropna(1)\n", "        return df\n", "\n", "    def reformat_data(self, labels, num_labels):\n", "        # Map labels/target value to one-hot-encoded frame. None is same as implying newaxis() just replicating array\n", "        # if num_labels > 2:\n", "        labels = (np.arange(num_labels) == labels[:, None]).astype(np.float64)\n", "        return labels\n", "\n", "    def accuracy(self, predictions, labels):\n", "        # Sum the number of cases where the predictions are correct and divide by the number of predictions\n", "        number_of_correct_predictions = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n", "        return 100*number_of_correct_predictions/predictions.shape[0]\n", "\n", "    def linear_model(self, input_vector, weight_matrix, bias_vector):\n", "        # f(x) = Wx + b\n", "        # W is the weight matrix with elements w_ij\n", "        # x is the input vector\n", "        # b is the bias vector\n", "        # In the machine learning literature f(x) is called an activation\n", "        return tf.matmul(input_vector, weight_matrix) + bias_vector\n", "\n", "    def activation_out(self, logit):\n", "        return self.activation(logit, switch_var=0)\n", "\n", "    def activation_hidden(self, logit):\n", "        return self.activation(logit, switch_var=0)\n", "\n", "    def activation(self, logit, switch_var=0):\n", "        # Also called the activation function\n", "        if switch_var == 0:\n", "            # Logistic sigmoid function.\n", "            # sigma(a) = 1/(1+exp(-a))\n", "            return tf.nn.sigmoid(logit)\n", "        elif switch_var == 1:\n", "            # Using Rectifier as activation function. Rectified linear unit (ReLU). Compared to sigmoid or other\n", "            # activation functions it allows for faster and effective training of neural architectures.\n", "            # f(x) = max(x,0)\n", "            return tf.nn.relu(logit)\n", "        else:\n", "            # Softmax function.\n", "            # S(y_i) = e^y_i/(Sum_j e^y_j)\n", "            return tf.nn.softmax(logit)\n", "\n", "\n", "def main():\n", "    porto_seguro_insur = PortoSeguroInsur()\n", "    df = porto_seguro_insur.df.copy()\n", "    df_test = porto_seguro_insur.df_test.copy()\n", "    df_submission = porto_seguro_insur.df_submission.copy()\n", "\n", "    df = df.replace(-1, np.NaN)\n", "    df_test = df_test.replace(-1, np.NaN)\n", "\n", "    print(df.shape)\n", "    print(df_test.shape)\n", "    # Clean data for NaN\n", "    df = porto_seguro_insur.clean_data(df)\n", "    df_test = porto_seguro_insur.clean_data(df_test, is_train_data=0)\n", "    print('df_test.shape: %s' % str(df_test.shape))  # (892816, 46)\n", "    # df_test = porto_seguro_insur.clean_data(df_test, is_train_data=0)\n", "    id_df_test = df_test['id']  # Submission column\n", "    print(\"After dropping NaN\")\n", "    print(df.shape)\n", "    print(df_test.shape)\n", "\n", "    is_explore_data = 1\n", "    if is_explore_data:\n", "        # Overview of train data\n", "        print('\\n TRAINING DATA:----------------------------------------------- \\n')\n", "        print(df.head(3))\n", "        print('\\n')\n", "        print(df.info())\n", "        print('\\n')\n", "        print(df.describe())\n", "        print('\\n')\n", "        print(df.dtypes)\n", "        print(df.get_dtype_counts())\n", "\n", "        # missing_values\n", "        print('All df set missing values')\n", "        porto_seguro_insur.missing_values_in_dataframe(df)\n", "\n", "        print('Uniques')\n", "        uniques_in_id = np.unique(df.id.values).shape[0]\n", "        print(uniques_in_id)\n", "        print('uniques_in_id == df.shape[0]')\n", "        print(uniques_in_id == df.shape[0])\n", "\n", "        # Overview of sample_submission format\n", "        print('\\n sample_submission \\n')\n", "        print(df_submission.head(3))\n", "        print('\\n')\n", "        print(df_submission.info())\n", "        print('\\n')\n", "\n", "    is_prepare_data = 1\n", "    if is_prepare_data:\n", "        df_test_num_features = porto_seguro_insur.extract_numerical_features(df_test)\n", "        df_y = df.loc[:, ['target']]\n", "        df = df.loc[:, df_test_num_features]\n", "\n", "    is_prediction = 1\n", "    if is_prediction:\n", "        # Subset the data to make it run faster\n", "        # (595212, 59)\n", "        # (892816, 58)\n", "        # After dropping NaN\n", "        # (124931, 59)\n", "        # (186567, 58)\n", "        subset_size = 10000\n", "\n", "        num_labels = np.unique(df_y.loc[:subset_size, 'target'].values).shape[0]\n", "        num_columns = df[(df.columns[(df.columns != 'target') & (df.columns != 'id')])].shape[1]\n", "        # Reformat datasets\n", "        x_train = df.loc[:subset_size, (df.columns[(df.columns != 'target') & (df.columns != 'id')])].values\n", "        y_train = df_y.loc[:subset_size, 'target'].values\n", "        # We only need to one-hot-encode our labels since otherwise they will not match the dimension of the\n", "        # logits in our later computation.\n", "        y_train = porto_seguro_insur.reformat_data(y_train, num_labels=num_labels)\n", "\n", "        # Todo: we need testdata with labels to benchmark test results.\n", "        # Hack. Use subset of training data (not used in training model) as test data, since it has a label/target value\n", "        # In case there are duplicates in training data it may imply that test results are too good, when using\n", "        # a subset of training data for test.\n", "        # Todo: do cross-validation instead of only one subset testing as below in x_val\n", "        # Validation data is a subset of training data.\n", "        x_val = df.loc[subset_size:2*subset_size, (df.columns[(df.columns != 'target') & (df.columns != 'id')])].values\n", "        y_val = df_y.loc[subset_size:2*subset_size, 'target'].values\n", "        y_val = porto_seguro_insur.reformat_data(y_val, num_labels=num_labels)\n", "        # Test data.\n", "        x_test = df_test.loc[:, (df_test.columns[(df_test.columns != 'id')])].values\n", "\n", "        # Todo: we need validation data with labels to perform crossvalidation while training and get a better result.\n", "\n", "        # Tensorflow uses a dataflow graph to represent your computations in terms of dependencies.\n", "        graph = tf.Graph()\n", "        with graph.as_default():\n", "            # Load training and test data into constants that are attached to the graph\n", "            tf_train = tf.constant(x_train)\n", "            tf_train_labels = tf.constant(y_train)\n", "            tf_val = tf.constant(x_val)\n", "            tf_test = tf.constant(x_test)\n", "\n", "            # As in a neural network the goal is to compute the cross-entropy D(S(w,x), L)\n", "            # x, input training data\n", "            # w_ij, are elements of the weight matrix\n", "            # L, labels or target values of the training data (classification problem)\n", "            # S(), is softmax function\n", "            # Do the Multinomial Logistic Classification\n", "            # step 1.\n", "            # Compute y from the linear model y = WX + b, where b is the bias and W is randomly chosen on\n", "            # Gaussian distribution and bias is set to zero. The result is called the logit.\n", "            # step 2.\n", "            # Compute the softmax function S(Y) which gives distribution\n", "            # step 3.\n", "            # Compute cross-entropy D(S, L) = - Sum_i L_i*log(S_i)\n", "            # step 4.\n", "            # Compute loss L = 1/N * D(S, L)\n", "            # step 5.\n", "            # Use gradient-descent to find minimum of loss wrt. w and b by minimizing L(w,b).\n", "            # Update your weight and bias until minimum of loss function is reached\n", "            # w_i -> w_i - alpha*delta_w L\n", "            # b -> b - alpha*delta_b L\n", "            # OBS. step 5 is faster optimized if you have transformed the data to have zero mean and equal variance\n", "            # mu(x_i) = 0\n", "            # sigma(x_i) = sigma(x_j)\n", "            # This transformation makes it a well conditioned problem.\n", "\n", "            # Make a 2-layer Neural network (count number of layers of adaptive weights) with num_columns nodes\n", "            # in hidden layer.\n", "            # Initialize weights on truncated normal distribution. Initialize biases to zero.\n", "            # For every input vector corresponding to one sample we have D features s.t.\n", "            # a_j = Sum_i^D w^(1)_ji x_i + w^(1)_j0 , where index j is the number of nodes in the first hidden layer\n", "            # and it runs j=1,...,M\n", "            # Vectorizing makes the notation more compact\n", "            #     | --- x_1 --- |\n", "            #     | --- x_2 --- |\n", "            # X = | --- ..  --- |\n", "            #     | --- x_N --- |\n", "            # where each x is now a sample vector of dimension (1 x D) and where N is the number of samples.\n", "            # Similarly, define a tiling of N weight matrices w,\n", "            #     | --- w --- |\n", "            #     | --- w --- |\n", "            # W = | --- ..--- |\n", "            #     | --- w --- |\n", "            # where each w is now a matrix of dimension (M x D)\n", "            # We now form the tensor product between W and X but we need to transpose X as x.T to get (M x D).(D x 1)\n", "            # multiplication,\n", "            #       |  w.(x_1.T) |\n", "            #       |  w.(x_2.T) |\n", "            # W.X = |  ..        |\n", "            #       |  w.(x_N.T) |\n", "            # with W.X having dimensions (M*N x 1).\n", "            # Additionally, define a tiling of N bias vectors b that each are of dimension (M x 1),\n", "            #     |  b  |\n", "            #     |  b  |\n", "            # B = |  .. |\n", "            #     |  b  |\n", "            # with B having dimensions (M*N x 1).\n", "            # Finally, the activation is a (M*N x 1) vector given as A = W.X + B.\n", "            # Next, this is passed to an activation function like a simoid and then inserted in second layer of the NN.\n", "            # Let Z = sigmoid(A)\n", "            # Let C be the activation of the second layer,\n", "            # C = W^(2).Z + B^(2)\n", "            # where W^(2) is the tiling N second layer weight matrices w^(2) each with dimension (K x M). K is the\n", "            # number of outputs in the classification. The dimension of C is (K x N).\n", "            # Lastly, apply the sigmoid function to get the predictions\n", "            # P = sigmoid( C )\n", "            # which has dimensions (K x N) and is as expected an output vector (K x 1) for every N samples in our\n", "            # dataset. The output (K x 1)-vector is in a one-hot-encoded form.\n", "\t\t\t\n", "\t\t\t# Choose number of nodes > than number of features.\n", "            M_nodes = x_train.shape[1]\n", "            weights_1_layer = tf.Variable(tf.truncated_normal([num_columns, M_nodes], dtype=np.float64))\n", "            biases_1_layer = tf.Variable(tf.zeros([M_nodes], dtype=np.float64))\n", "            weights_2_layer = tf.Variable(tf.truncated_normal([M_nodes, num_labels], dtype=np.float64))\n", "            biases_2_layer = tf.Variable(tf.zeros([num_labels], dtype=np.float64))\n", "\n", "            # Logits and loss function.\n", "            logits_hidden_1_layer = porto_seguro_insur.linear_model(tf_train, weights_1_layer, biases_1_layer)\n", "            # Output unit activations of first layer\n", "            a_1_layer = porto_seguro_insur.activation_hidden(logits_hidden_1_layer)\n", "            logits_2_layer = porto_seguro_insur.linear_model(a_1_layer, weights_2_layer, biases_2_layer)\n", "            switch_var = 0\n", "            if switch_var == 1:\n", "                loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,\n", "                                                                                       logits=logits_2_layer))\n", "            else:\n", "                loss_function = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_train_labels,\n", "                                                                                       logits=logits_2_layer))\n", "\n", "            # Find minimum of loss function using gradient-descent.\n", "            optimized_weights_and_bias = tf.train.GradientDescentOptimizer(0.5).minimize(loss=loss_function)\n", "\n", "            # Accuracy variables using the initial values for weights and bias of our linear model.\n", "            train_prediction = porto_seguro_insur.activation_out(logits_2_layer)\n", "            # Applying optimized weights and bias to validation data\n", "            logits_hidden_1_layer_val = porto_seguro_insur.linear_model(tf_val, weights_1_layer, biases_1_layer)\n", "            a_1_layer_val = porto_seguro_insur.activation_hidden(logits_hidden_1_layer_val)\n", "            logits_2_layer_val = porto_seguro_insur.linear_model(a_1_layer_val, weights_2_layer, biases_2_layer)\n", "            val_prediction = porto_seguro_insur.activation_out(logits_2_layer_val)\n", "\n", "            # Applying optimized weights and bias to test data\n", "            logits_hidden_1_layer_test = porto_seguro_insur.linear_model(tf_test, weights_1_layer, biases_1_layer)\n", "            a_1_layer_test = porto_seguro_insur.activation_hidden(logits_hidden_1_layer_test)\n", "            logits_2_layer_test = porto_seguro_insur.linear_model(a_1_layer_test, weights_2_layer, biases_2_layer)\n", "            test_prediction = porto_seguro_insur.activation_out(logits_2_layer_test)\n", "\n", "        number_of_iterations = 900\n", "        # Creating a tensorflow session to effeciently run same computation multiple times using definitions in defined\n", "        # dataflow graph.\n", "        with tf.Session(graph=graph) as session:\n", "            # Ensure that variables are initialized as done in our graph defining the dataflow.\n", "            tf.global_variables_initializer().run()\n", "            for ite in range(number_of_iterations):\n", "                # Compute loss and predictions\n", "                loss, predictions = session.run([optimized_weights_and_bias, loss_function, train_prediction])[1:3]\n", "                if ite % 100 == 0:\n", "                    print('Loss at iteration %d: %f' % (ite, loss))\n", "                    print('Training accuracy: %.1f%%' % porto_seguro_insur.accuracy(predictions, y_train))\n", "            print('Test accuracy: %.1f%%' % porto_seguro_insur.accuracy(val_prediction.eval(), y_val))\n", "            output = test_prediction.eval()\n", "\n", "    is_make_prediction = 1\n", "    if is_make_prediction:\n", "        ''' Submission '''\n", "        # Submission requires a csv file with id and target columns.\n", "        submission = pd.DataFrame({'id': id_df_test, 'target': np.argmax(output, 1)})\n", "        submission.to_csv(''.join(['submission_porto_seguro_insur_', porto_seguro_insur.timestamp, '.csv']), index=False)\n", "        print(porto_seguro_insur.timestamp)\n", "\n", "if __name__ == '__main__':\n", "    main()"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "e84dfb99-a8b5-4af1-b5bf-e885fca30d5a", "_uuid": "6b62e146e8e1af3e7eb2b4a44d8dec706f8b8335", "collapsed": true}, "source": [], "cell_type": "code"}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.3", "mimetype": "text/x-python"}}}