{"nbformat": 4, "cells": [{"cell_type": "markdown", "source": ["In this kernel I present an end-to-end solution using the custom Keras Directory Iterator and a small training/validation subset that I compiled. You can find my training data as well as the whole test dataset here https://www.kaggle.com/voglinio/speechtest. Once loaded, this dataset will give you access to the following folders\n", "\n", "* train_path='../input/speechtest/part1/part1/train/'\n", "* val_path ='../input/speechtest/part1/part1/val/'\n", "* test_path ='../input/speechtest/test/'\n", "\n", "This kernel ends with a submission file, that scores 0.72 on the LB. I have used limited number of epochs because of the 1 hour running time limit imposed by the kernel."], "metadata": {"_cell_guid": "56376727-f3ff-4618-bdaa-b9602ed267ac", "_uuid": "a5f3d1677a66f695d7979a2f61c80e3d44ae14f3"}}, {"cell_type": "markdown", "source": ["## Some statistics about the training/validation files"], "metadata": {"_cell_guid": "3f520269-8af7-4ff1-90d6-deebc2bed8fd", "_uuid": "5d9676904a734b9d7702fcfa392d78a7d4e0f766"}}, {"cell_type": "code", "outputs": [], "source": ["import numpy as np\n", "from keras import backend as K\n", "from keras.preprocessing.image import Iterator\n", "from keras.preprocessing.image import img_to_array\n", "\n", "import librosa\n", "import os\n", "import multiprocessing.pool\n", "from functools import partial\n", "from random import getrandbits\n", "\n", "train_path='../input/speechtest/part1/part1/train/'\n", "val_path ='../input/speechtest/part1/part1/val/'\n", "test_path ='../input/speechtest/test/test/'\n", "\n", "classnames=os.listdir(train_path)\n", "train_count_dict = {}\n", "for d in classnames:\n", "    train_count_dict[d] = len(os.listdir(os.path.join(train_path, d)))\n", "print('train freq')\n", "for k, v in train_count_dict.items():\n", "    print ( '%7s  %i' % (k, v))\n", "val_count_dict = {}\n", "for d in classnames:\n", "    val_count_dict[d] = len(os.listdir(os.path.join(val_path, d)))\n", "print('\\nval freq')\n", "for k, v in val_count_dict.items():\n", "    print ( '%7s  %i' % (k, v))\n", "print ('')\n", "print ('test files', len(os.listdir(test_path+'/audio')))"], "metadata": {"_cell_guid": "d84ac3bc-6fe4-4430-bdc5-1f4cbb70c88b", "_uuid": "05e4765e5b14b61511b2ca91b54721c0848defab"}, "execution_count": 15}, {"cell_type": "markdown", "source": ["## Directory Iterator definition"], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["def spect_loader(path, window_size, window_stride, window, normalize, max_len=101, \n", "                 augment=False, allow_speedandpitch=False, allow_pitch=False,\n", "                 allow_speed=False, allow_dyn=False, allow_noise=False,\n", "                allow_timeshift=False ):\n", "    y, sr = librosa.load(path, sr=None)\n", "    # n_fft = 4096\n", "    n_fft = int(sr * window_size)\n", "    win_length = n_fft\n", "    hop_length = int(sr * window_stride)\n", "\n", "    # STFT\n", "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n", "                     win_length=win_length, window=window)\n", "    spect, phase = librosa.magphase(D)\n", "\n", "    # S = log(S+1)\n", "    spect = np.log1p(spect)\n", "\n", "    # make all spects with the same dims\n", "    # TODO: change that in the future\n", "    if spect.shape[1] < max_len:\n", "        pad = np.zeros((spect.shape[0], max_len - spect.shape[1]))\n", "        spect = np.hstack((spect, pad))\n", "    elif spect.shape[1] > max_len:\n", "        spect = spect[:max_len, ]\n", "    spect = np.resize(spect, (1, spect.shape[0], spect.shape[1]))\n", "    #spect = torch.FloatTensor(spect)\n", "\n", "    # z-score normalization\n", "    if normalize:\n", "        mean = np.mean(np.ravel(spect))\n", "        std = np.std(np.ravel(spect))\n", "        if std != 0:\n", "            spect = spect -mean\n", "            spect = spect / std\n", "\n", "    return spect\n", "\n", "def _count_valid_files_in_directory(directory, white_list_formats, follow_links):\n", "    \"\"\"Count files with extension in `white_list_formats` contained in a directory.\n", "    # Arguments\n", "        directory: absolute path to the directory containing files to be counted\n", "        white_list_formats: set of strings containing allowed extensions for\n", "            the files to be counted.\n", "    # Returns\n", "        the count of files with extension in `white_list_formats` contained in\n", "        the directory.\n", "    \"\"\"\n", "    def _recursive_list(subpath):\n", "        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n", "\n", "    samples = 0\n", "    for root, _, files in _recursive_list(directory):\n", "        for fname in files:\n", "            is_valid = False\n", "            for extension in white_list_formats:\n", "                if fname.lower().endswith('.' + extension):\n", "                    is_valid = True\n", "                    break\n", "            if is_valid:\n", "                samples += 1\n", "    return samples\n", "\n", "def _list_valid_filenames_in_directory(directory, white_list_formats,\n", "                                       class_indices, follow_links):\n", "    \"\"\"List paths of files in `subdir` relative from `directory` whose extensions are in `white_list_formats`.\n", "    # Arguments\n", "        directory: absolute path to a directory containing the files to list.\n", "            The directory name is used as class label and must be a key of `class_indices`.\n", "        white_list_formats: set of strings containing allowed extensions for\n", "            the files to be counted.\n", "        class_indices: dictionary mapping a class name to its index.\n", "    # Returns\n", "        classes: a list of class indices\n", "        filenames: the path of valid files in `directory`, relative from\n", "            `directory`'s parent (e.g., if `directory` is \"dataset/class1\",\n", "            the filenames will be [\"class1/file1.jpg\", \"class1/file2.jpg\", ...]).\n", "    \"\"\"\n", "    def _recursive_list(subpath):\n", "        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n", "\n", "    classes = []\n", "    filenames = []\n", "    subdir = os.path.basename(directory)\n", "    basedir = os.path.dirname(directory)\n", "    for root, _, files in _recursive_list(directory):\n", "        for fname in sorted(files):\n", "            is_valid = False\n", "            for extension in white_list_formats:\n", "                if fname.lower().endswith('.' + extension):\n", "                    is_valid = True\n", "                    break\n", "            if is_valid:\n", "                classes.append(class_indices[subdir])\n", "                # add filename relative to directory\n", "                absolute_path = os.path.join(root, fname)\n", "                filenames.append(os.path.relpath(absolute_path, basedir))\n", "    return classes, filenames\n", "\n", "class SpeechDirectoryIterator(Iterator):\n", "    \"\"\"Iterator capable of reading images from a directory on disk.\n", "    # Arguments\n", "       \n", "    \"\"\"\n", "\n", "    def __init__(self, directory, window_size, window_stride, \n", "                 window_type, normalize, max_len=101,\n", "                 target_size=(256, 256), color_mode='grayscale',\n", "                 classes=None, class_mode='categorical',\n", "                 batch_size=32, shuffle=True, seed=None,\n", "                 data_format=None, save_to_dir=None,\n", "                 save_prefix='', save_format='png',\n", "                 follow_links=False, interpolation='nearest', augment=False,\n", "                allow_speedandpitch = False, allow_pitch = False,\n", "                allow_speed = False, allow_dyn = False, allow_noise = False, allow_timeshift=False ):\n", "        if data_format is None:\n", "            data_format = K.image_data_format()\n", "        self.window_size = window_size\n", "        self.window_stride = window_stride\n", "        self.window_type = window_type\n", "        self.normalize = normalize\n", "        self.max_len = max_len\n", "        self.directory = directory\n", "        self.allow_speedandpitch = allow_speedandpitch\n", "        self.allow_pitch = allow_pitch\n", "        self.allow_speed = allow_speed \n", "        self.allow_dyn = allow_dyn\n", "        self.allow_noise = allow_noise\n", "        self.allow_timeshift = allow_timeshift \n", "        self.augment = augment\n", "#        self.image_data_generator = image_data_generator\n", "        self.target_size = tuple(target_size)\n", "        if color_mode not in {'rgb', 'grayscale'}:\n", "            raise ValueError('Invalid color mode:', color_mode,\n", "                             '; expected \"rgb\" or \"grayscale\".')\n", "        self.color_mode = color_mode\n", "        self.data_format = data_format\n", "        if self.color_mode == 'rgb':\n", "            if self.data_format == 'channels_last':\n", "                self.image_shape = self.target_size + (3,)\n", "            else:\n", "                self.image_shape = (3,) + self.target_size\n", "        else:\n", "            if self.data_format == 'channels_last':\n", "                self.image_shape = self.target_size + (1,)\n", "            else:\n", "                self.image_shape = (1,) + self.target_size\n", "        self.classes = classes\n", "        if class_mode not in {'categorical', 'binary', 'sparse',\n", "                              'input', None}:\n", "            raise ValueError('Invalid class_mode:', class_mode,\n", "                             '; expected one of \"categorical\", '\n", "                             '\"binary\", \"sparse\", \"input\"'\n", "                             ' or None.')\n", "        self.class_mode = class_mode\n", "        self.save_to_dir = save_to_dir\n", "        self.save_prefix = save_prefix\n", "        self.save_format = save_format\n", "        self.interpolation = interpolation\n", "\n", "        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm', 'wav'}\n", "\n", "        # first, count the number of samples and classes\n", "        self.samples = 0\n", "\n", "        if not classes:\n", "            classes = []\n", "            for subdir in sorted(os.listdir(directory)):\n", "                if os.path.isdir(os.path.join(directory, subdir)):\n", "                    classes.append(subdir)\n", "        self.num_classes = len(classes)\n", "        self.class_indices = dict(zip(classes, range(len(classes))))\n", "\n", "        pool = multiprocessing.pool.ThreadPool()\n", "        function_partial = partial(_count_valid_files_in_directory,\n", "                                   white_list_formats=white_list_formats,\n", "                                   follow_links=follow_links)\n", "        self.samples = sum(pool.map(function_partial,\n", "                                    (os.path.join(directory, subdir)\n", "                                     for subdir in classes)))\n", "\n", "        print('Found %d images belonging to %d classes.' % (self.samples, self.num_classes))\n", "\n", "        # second, build an index of the images in the different class subfolders\n", "        results = []\n", "\n", "        self.filenames = []\n", "        self.classes = np.zeros((self.samples,), dtype='int32')\n", "        i = 0\n", "        for dirpath in (os.path.join(directory, subdir) for subdir in classes):\n", "            results.append(pool.apply_async(_list_valid_filenames_in_directory,\n", "                                            (dirpath, white_list_formats,\n", "                                             self.class_indices, follow_links)))\n", "            \n", "        \n", "        for res in results:\n", "            classes, filenames = res.get()\n", "            self.classes[i:i + len(classes)] = classes\n", "            self.filenames += filenames\n", "            if i==0:\n", "                img = spect_loader(os.path.join(self.directory, filenames[0]), \n", "                               self.window_size, \n", "                               self.window_stride, \n", "                               self.window_type, \n", "                               self.normalize, \n", "                               self.max_len, \n", "                               self.augment,\n", "                               self.allow_speedandpitch,\n", "                               self.allow_pitch,\n", "                               self.allow_speed, \n", "                               self.allow_dyn,\n", "                               self.allow_noise,\n", "                               self.allow_timeshift ) \n", "                img=np.swapaxes(img, 0, 2)\n", "                self.target_size = tuple((img.shape[0], img.shape[1]))\n", "                print(self.target_size)\n", "                if self.color_mode == 'rgb':\n", "                    if self.data_format == 'channels_last':\n", "                        self.image_shape = self.target_size + (3,)\n", "                    else:\n", "                        self.image_shape = (3,) + self.target_size\n", "                else:\n", "                    if self.data_format == 'channels_last':\n", "                        self.image_shape = self.target_size + (1,)\n", "                    else:\n", "                        self.image_shape = (1,) + self.target_size\n", "                        \n", "            i += len(classes)\n", "        pool.close()\n", "        pool.join()\n", "        super(SpeechDirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n", "    \n", "\n", "    \n", "    \n", "    def _get_batches_of_transformed_samples(self, index_array):\n", "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n", "        batch_f = []\n", "        grayscale = self.color_mode == 'grayscale'\n", "        # build batch of image data\n", "        #print(index_array)\n", "        for i, j in enumerate(index_array):\n", "            #print(i, j, self.filenames[j])\n", "            fname = self.filenames[j]\n", "            #img = load_img(os.path.join(self.directory, fname),\n", "            #               grayscale=grayscale,\n", "            #               target_size=self.target_size,\n", "            #               interpolation=self.interpolation)\n", "            img = spect_loader(os.path.join(self.directory, fname), \n", "                               self.window_size, \n", "                               self.window_stride, \n", "                               self.window_type, \n", "                               self.normalize, \n", "                               self.max_len, \n", "                                )\n", "            img=np.swapaxes(img, 0, 2)\n", "            \n", "            x = img_to_array(img, data_format=self.data_format)\n", "            #x = self.image_data_generator.random_transform(x)\n", "            #x = self.image_data_generator.standardize(x)\n", "            batch_x[i] = x\n", "            batch_f.append(fname)\n", "        # optionally save augmented images to disk for debugging purposes\n", "        if self.save_to_dir:\n", "            for i, j in enumerate(index_array):\n", "                img = array_to_img(batch_x[i], self.data_format, scale=True)\n", "                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n", "                                                                  index=j,\n", "                                                                  hash=np.random.randint(1e7),\n", "                                                                  format=self.save_format)\n", "                img.save(os.path.join(self.save_to_dir, fname))\n", "        # build batch of labels\n", "        if self.class_mode == 'input':\n", "            batch_y = batch_x.copy()\n", "        elif self.class_mode == 'sparse':\n", "            batch_y = self.classes[index_array]\n", "        elif self.class_mode == 'binary':\n", "            batch_y = self.classes[index_array].astype(K.floatx())\n", "        elif self.class_mode == 'categorical':\n", "            batch_y = np.zeros((len(batch_x), self.num_classes), dtype=K.floatx())\n", "            for i, label in enumerate(self.classes[index_array]):\n", "                batch_y[i, label] = 1.\n", "        else:\n", "            return batch_x\n", "        return batch_x, batch_y\n", "\n", "    def next(self):\n", "        \"\"\"For python 2.x.\n", "        # Returns\n", "            The next batch.\n", "        \"\"\"\n", "        with self.lock:\n", "            index_array = next(self.index_generator)[0]\n", "        # The transformation of images is not under thread lock\n", "        # so it can be done in parallel\n", "        return self._get_batches_of_transformed_samples(index_array)"], "metadata": {"collapsed": true}, "execution_count": 16}, {"cell_type": "code", "outputs": [], "source": ["window_size=.02\n", "window_stride=.01\n", "window_type='hamming'\n", "normalize=True\n", "max_len=101\n", "batch_size = 64\n", "train_iterator = SpeechDirectoryIterator(directory=train_path, \n", "                                   batch_size=batch_size, \n", "                                   window_size=window_size, \n", "                                   window_stride=window_stride, \n", "                                   window_type=window_type,\n", "                                   normalize=normalize, \n", "                                   max_len=max_len)"], "metadata": {}, "execution_count": 17}, {"cell_type": "code", "outputs": [], "source": ["val_iterator = SpeechDirectoryIterator(directory=val_path, \n", "                                   batch_size=batch_size, \n", "                                   window_size=window_size, \n", "                                   window_stride=window_stride, \n", "                                   window_type=window_type,\n", "                                   normalize=normalize, \n", "                                   max_len=max_len)"], "metadata": {}, "execution_count": 18}, {"cell_type": "markdown", "source": ["## Model definition"], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["from keras.models import Sequential\n", "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n", "model = Sequential()\n", "model.add(Conv2D(12, (5, 5), activation = 'relu', input_shape=train_iterator.image_shape))\n", "\n", "model.add(MaxPooling2D(pool_size=(2, 2)))\n", "\n", "model.add(Conv2D(25, (5, 5), activation = 'relu'))\n", "\n", "model.add(MaxPooling2D(pool_size=(2, 2)))\n", "\n", "model.add(Flatten())\n", "model.add(Dense(180, activation = 'relu'))\n", "model.add(Dropout(0.5))\n", "model.add(Dense(100, activation = 'relu'))\n", "model.add(Dropout(0.5))\n", "model.add(Dense(len(classnames), activation = 'softmax')) #Last layer with one output per class\n", "\n", "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n", "model.summary()"], "metadata": {}, "execution_count": 19}, {"cell_type": "markdown", "source": ["## Training and callbacs"], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n", "\n", "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n", "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='auto', min_lr=0.00001)\n", "model.fit_generator(train_iterator,\n", "        steps_per_epoch=int(np.ceil(train_iterator.n / batch_size)),\n", "        epochs=6,\n", "        validation_data=val_iterator,\n", "        validation_steps=int(np.ceil(val_iterator.n / batch_size)),\n", "        verbose=1, callbacks=[early, reduce])"], "metadata": {}, "execution_count": 20}, {"cell_type": "markdown", "source": ["## Prediction on test set\n", "\n", "I use the keras.util.Sequence class to iterate through test images. It is a thread safe way to predict on a series of test files."], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["test_path_audio=os.path.join(test_path, 'audio')\n", "test_filenames = os.listdir(test_path_audio) \n", "test_filenames=np.sort(test_filenames)\n", "list(test_filenames)[:10]"], "metadata": {}, "execution_count": 37}, {"cell_type": "code", "outputs": [], "source": ["import math\n", "from keras.utils import Sequence\n", "from keras.preprocessing.image import img_to_array\n", "\n", "def loadAndSpect(fname,  window_size, window_stride, window_type, normalize, max_len):\n", "    img = spect_loader(os.path.join(test_path_audio, fname), \n", "                       window_size, \n", "                       window_stride, \n", "                       window_type, \n", "                       normalize, \n", "                       max_len)\n", "    img=np.swapaxes(img, 0, 2)\n", "\n", "    x = img_to_array(img, data_format='channels_last')\n", "    return x\n", "            \n", "class WavSequence(Sequence):\n", "\n", "    def __init__(self, x_set, batch_size=64, window_size=0.02, window_stride=0.01, window_type='hamming', normalize=True, max_len=101):\n", "        self.x = x_set\n", "        self.batch_size = batch_size\n", "        self.window_size = window_size\n", "        self.window_stride = window_stride\n", "        self.window_type = window_type\n", "        self.normalize = normalize\n", "        self.max_len = max_len\n", "\n", "    def __len__(self):\n", "        return math.ceil(len(self.x) / self.batch_size)\n", "\n", "    def __getitem__(self, idx):\n", "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n", "\n", "        return np.array([\n", "            loadAndSpect(file_name, window_size, window_stride, window_type, normalize, max_len)\n", "               for file_name in batch_x])"], "metadata": {"collapsed": true}, "execution_count": 22}, {"cell_type": "code", "outputs": [], "source": ["seq = WavSequence(test_filenames, batch_size=batch_size)\n", "preds = model.predict_generator(generator=seq, \n", "                        steps=len(seq), \n", "                        workers=1, \n", "                        use_multiprocessing=False, \n", "                        verbose=1)"], "metadata": {}, "execution_count": 23}, {"cell_type": "markdown", "source": ["We can now create a submission file."], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["inv_map = {v: k for k, v in train_iterator.class_indices.items()}\n", "print(inv_map)\n", "classes = np.argmax(preds, axis=1)\n", "probes = np.max(preds, axis=1)\n", "print (classes[:10])\n", "print (probes[:10])\n", "\n", "unique_elements, counts_elements = np.unique(classes, return_counts=True)\n", "print(np.asarray((unique_elements, counts_elements)))\n", "\n", "res = []\n", "for cl in classes:\n", "    res.append(inv_map[cl])\n", "\n", "\n", "import pandas as pd\n", "df = pd.DataFrame(np.transpose(np.vstack((np.array(test_filenames), res))), columns=['fname', 'label'])\n", "df.to_csv('submission.csv', header=True, quoting=0, index=False)"], "metadata": {}, "execution_count": 24}, {"cell_type": "markdown", "source": ["Let us view the first 20 predictions:"], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["df.head(n=20)"], "metadata": {}, "execution_count": 25}, {"cell_type": "markdown", "source": ["and listen to a couple of them."], "metadata": {}}, {"cell_type": "code", "outputs": [], "source": ["import IPython.display as ipd\n", "ipd.Audio(test_path_audio+'/'+'clip_d54618666.wav')"], "metadata": {}, "execution_count": 32}, {"cell_type": "code", "outputs": [], "source": ["ipd.Audio(test_path_audio+'/'+'clip_ca68ee4c5.wav')"], "metadata": {}, "execution_count": 34}, {"cell_type": "code", "outputs": [], "source": [], "metadata": {"collapsed": true}, "execution_count": null}], "nbformat_minor": 1, "metadata": {"language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "version": "3.6.3", "name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}}