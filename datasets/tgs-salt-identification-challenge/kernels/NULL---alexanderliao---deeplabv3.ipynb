{"cells":[{"metadata":{"_uuid":"fa525d982fdb7e518de47cd8ec52f6ee868d312b"},"cell_type":"markdown","source":"## Update\n-Add Batch Normalization layer after each conv\n-Add shuffle=True in model.fit() method for a better BN effect (so that we have different batch to normalize in each epoch during the training)\n-You can use crf method (https://www.kaggle.com/meaninglesslives/apply-crf) to improve the result \n## Changelog\n- Changed uncov to uconv, but removed the dropout in the last layer\n- Corrected sanity check of predicted validation data (changed from ids_train to ids_valid)\n- Used correct mask (from original train_df) for threshold tuning (inserted y_valid_ori)\n- Added DICE loss functions\n- Added DeepLabv3+ Model\n\n### Attention: this script is only a proof-of-concept work; you need more RAM or other tricks in order to train on the full training dataset."},{"metadata":{"_uuid":"77e05df3218e009eafa91b1afffbb5cdd6535a41","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n\"\"\" Deeplabv3+ model for Keras.\nThis model is based on TF repo:\nhttps://github.com/tensorflow/models/tree/master/research/deeplab\nOn Pascal VOC, original model gets to 84.56% mIOU\n\nNow this model is only available for the TensorFlow backend,\ndue to its reliance on `SeparableConvolution` layers, but Theano will add\nthis layer soon.\n\nMobileNetv2 backbone is based on this repo:\nhttps://github.com/JonathanCMitchell/mobilenet_v2_keras\n\n# Reference\n- [Encoder-Decoder with Atrous Separable Convolution\n    for Semantic Image Segmentation](https://arxiv.org/pdf/1802.02611.pdf)\n- [Xception: Deep Learning with Depthwise Separable Convolutions]\n    (https://arxiv.org/abs/1610.02357)\n- [Inverted Residuals and Linear Bottlenecks: Mobile Networks for\n    Classification, Detection and Segmentation](https://arxiv.org/abs/1801.04381)\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom keras.models import Model\nfrom keras import layers\nfrom keras.layers import Input\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\nfrom keras.layers import Add\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Conv2D\nfrom keras.layers import DepthwiseConv2D\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import AveragePooling2D\nfrom keras.engine import Layer\nfrom keras.engine import InputSpec\nfrom keras.engine.topology import get_source_inputs\nfrom keras import backend as K\nfrom keras.applications import imagenet_utils\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\n\nWEIGHTS_PATH_X = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5\"\nWEIGHTS_PATH_MOBILE = \"https://github.com/bonlime/keras-deeplab-v3-plus/releases/download/1.1/deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels.h5\"\n\n\nclass BilinearUpsampling(Layer):\n    \"\"\"Just a simple bilinear upsampling layer. Works only with TF.\n       Args:\n           upsampling: tuple of 2 numbers > 0. The upsampling ratio for h and w\n           output_size: used instead of upsampling arg if passed!\n    \"\"\"\n\n    def __init__(self, upsampling=(2, 2), output_size=None, data_format=None, **kwargs):\n\n        super(BilinearUpsampling, self).__init__(**kwargs)\n\n        #self.data_format = K.normalize_data_format(data_format)\n        self.data_format = None\n        self.input_spec = InputSpec(ndim=4)\n        if output_size:\n            self.output_size = conv_utils.normalize_tuple(\n                output_size, 2, 'output_size')\n            self.upsampling = None\n        else:\n            self.output_size = None\n            self.upsampling = conv_utils.normalize_tuple(\n                upsampling, 2, 'upsampling')\n\n    def compute_output_shape(self, input_shape):\n        if self.upsampling:\n            height = self.upsampling[0] * \\\n                input_shape[1] if input_shape[1] is not None else None\n            width = self.upsampling[1] * \\\n                input_shape[2] if input_shape[2] is not None else None\n        else:\n            height = self.output_size[0]\n            width = self.output_size[1]\n        return (input_shape[0],\n                height,\n                width,\n                input_shape[3])\n\n    def call(self, inputs):\n        if self.upsampling:\n            return K.tf.image.resize_bilinear(inputs, (inputs.shape[1] * self.upsampling[0],\n                                                       inputs.shape[2] * self.upsampling[1]),\n                                              align_corners=True)\n        else:\n            return K.tf.image.resize_bilinear(inputs, (self.output_size[0],\n                                                       self.output_size[1]),\n                                              align_corners=True)\n\n    def get_config(self):\n        config = {'upsampling': self.upsampling,\n                  'output_size': self.output_size,\n                  'data_format': self.data_format}\n        base_config = super(BilinearUpsampling, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\ndef SepConv_BN(x, filters, prefix, stride=1, kernel_size=3, rate=1, depth_activation=False, epsilon=1e-3):\n    \"\"\" SepConv with BN between depthwise & pointwise. Optionally add activation after BN\n        Implements right \"same\" padding for even kernel sizes\n        Args:\n            x: input tensor\n            filters: num of filters in pointwise convolution\n            prefix: prefix before name\n            stride: stride at depthwise conv\n            kernel_size: kernel size for depthwise convolution\n            rate: atrous rate for depthwise convolution\n            depth_activation: flag to use activation between depthwise & poinwise convs\n            epsilon: epsilon to use in BN layer\n    \"\"\"\n\n    if stride == 1:\n        depth_padding = 'same'\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        x = ZeroPadding2D((pad_beg, pad_end))(x)\n        depth_padding = 'valid'\n\n    if not depth_activation:\n        x = Activation('relu')(x)\n    x = DepthwiseConv2D((kernel_size, kernel_size), strides=(stride, stride), dilation_rate=(rate, rate),\n                        padding=depth_padding, use_bias=False, name=prefix + '_depthwise')(x)\n    x = BatchNormalization(name=prefix + '_depthwise_BN', epsilon=epsilon)(x)\n    if depth_activation:\n        x = Activation('relu')(x)\n    x = Conv2D(filters, (1, 1), padding='same',\n               use_bias=False, name=prefix + '_pointwise')(x)\n    x = BatchNormalization(name=prefix + '_pointwise_BN', epsilon=epsilon)(x)\n    if depth_activation:\n        x = Activation('relu')(x)\n\n    return x\n\n\ndef _conv2d_same(x, filters, prefix, stride=1, kernel_size=3, rate=1):\n    \"\"\"Implements right 'same' padding for even kernel sizes\n        Without this there is a 1 pixel drift when stride = 2\n        Args:\n            x: input tensor\n            filters: num of filters in pointwise convolution\n            prefix: prefix before name\n            stride: stride at depthwise conv\n            kernel_size: kernel size for depthwise convolution\n            rate: atrous rate for depthwise convolution\n    \"\"\"\n    if stride == 1:\n        return Conv2D(filters,\n                      (kernel_size, kernel_size),\n                      strides=(stride, stride),\n                      padding='same', use_bias=False,\n                      dilation_rate=(rate, rate),\n                      name=prefix)(x)\n    else:\n        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n        pad_total = kernel_size_effective - 1\n        pad_beg = pad_total // 2\n        pad_end = pad_total - pad_beg\n        x = ZeroPadding2D((pad_beg, pad_end))(x)\n        return Conv2D(filters,\n                      (kernel_size, kernel_size),\n                      strides=(stride, stride),\n                      padding='valid', use_bias=False,\n                      dilation_rate=(rate, rate),\n                      name=prefix)(x)\n\n\ndef _xception_block(inputs, depth_list, prefix, skip_connection_type, stride,\n                    rate=1, depth_activation=False, return_skip=False):\n    \"\"\" Basic building block of modified Xception network\n        Args:\n            inputs: input tensor\n            depth_list: number of filters in each SepConv layer. len(depth_list) == 3\n            prefix: prefix before name\n            skip_connection_type: one of {'conv','sum','none'}\n            stride: stride at last depthwise conv\n            rate: atrous rate for depthwise convolution\n            depth_activation: flag to use activation between depthwise & pointwise convs\n            return_skip: flag to return additional tensor after 2 SepConvs for decoder\n            \"\"\"\n    residual = inputs\n    for i in range(3):\n        residual = SepConv_BN(residual,\n                              depth_list[i],\n                              prefix + '_separable_conv{}'.format(i + 1),\n                              stride=stride if i == 2 else 1,\n                              rate=rate,\n                              depth_activation=depth_activation)\n        if i == 1:\n            skip = residual\n    if skip_connection_type == 'conv':\n        shortcut = _conv2d_same(inputs, depth_list[-1], prefix + '_shortcut',\n                                kernel_size=1,\n                                stride=stride)\n        shortcut = BatchNormalization(name=prefix + '_shortcut_BN')(shortcut)\n        outputs = layers.add([residual, shortcut])\n    elif skip_connection_type == 'sum':\n        outputs = layers.add([residual, inputs])\n    elif skip_connection_type == 'none':\n        outputs = residual\n    if return_skip:\n        return outputs, skip\n    else:\n        return outputs\n\n\ndef relu6(x):\n    return K.relu(x, max_value=6)\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id, skip_connection, rate=1):\n    in_channels = inputs._keras_shape[-1]\n    pointwise_conv_filters = int(filters * alpha)\n    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n    x = inputs\n    prefix = 'expanded_conv_{}_'.format(block_id)\n    if block_id:\n        # Expand\n\n        x = Conv2D(expansion * in_channels, kernel_size=1, padding='same',\n                   use_bias=False, activation=None,\n                   name=prefix + 'expand')(x)\n        x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n                               name=prefix + 'expand_BN')(x)\n        x = Activation(relu6, name=prefix + 'expand_relu')(x)\n    else:\n        prefix = 'expanded_conv_'\n    # Depthwise\n    x = DepthwiseConv2D(kernel_size=3, strides=stride, activation=None,\n                        use_bias=False, padding='same', dilation_rate=(rate, rate),\n                        name=prefix + 'depthwise')(x)\n    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n                           name=prefix + 'depthwise_BN')(x)\n\n    x = Activation(relu6, name=prefix + 'depthwise_relu')(x)\n\n    # Project\n    x = Conv2D(pointwise_filters,\n               kernel_size=1, padding='same', use_bias=False, activation=None,\n               name=prefix + 'project')(x)\n    x = BatchNormalization(epsilon=1e-3, momentum=0.999,\n                           name=prefix + 'project_BN')(x)\n\n    if skip_connection:\n        return Add(name=prefix + 'add')([inputs, x])\n\n    # if in_channels == pointwise_filters and stride == 1:\n    #    return Add(name='res_connect_' + str(block_id))([inputs, x])\n\n    return x\n\n\ndef Deeplabv3(weights='pascal_voc', input_tensor=None, input_shape=(512, 512, 3), classes=21, backbone='mobilenetv2', OS=16, alpha=1.):\n    \"\"\" Instantiates the Deeplabv3+ architecture\n\n    Optionally loads weights pre-trained\n    on PASCAL VOC. This model is available for TensorFlow only,\n    and can only be used with inputs following the TensorFlow\n    data format `(width, height, channels)`.\n    # Arguments\n        weights: one of 'pascal_voc' (pre-trained on pascal voc)\n            or None (random initialization)\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: shape of input image. format HxWxC\n            PASCAL VOC model was trained on (512,512,3) images\n        classes: number of desired classes. If classes != 21,\n            last layer is initialized randomly\n        backbone: backbone to use. one of {'xception','mobilenetv2'}\n        OS: determines input_shape/feature_extractor_output ratio. One of {8,16}.\n            Used only for xception backbone.\n        alpha: controls the width of the MobileNetV2 network. This is known as the\n            width multiplier in the MobileNetV2 paper.\n                - If `alpha` < 1.0, proportionally decreases the number\n                    of filters in each layer.\n                - If `alpha` > 1.0, proportionally increases the number\n                    of filters in each layer.\n                - If `alpha` = 1, default number of filters from the paper\n                    are used at each layer.\n            Used only for mobilenetv2 backbone\n\n    # Returns\n        A Keras model instance.\n\n    # Raises\n        RuntimeError: If attempting to run this model with a\n            backend that does not support separable convolutions.\n        ValueError: in case of invalid argument for `weights` or `backbone`\n\n    \"\"\"\n\n    if not (weights in {'pascal_voc', None}):\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `pascal_voc` '\n                         '(pre-trained on PASCAL VOC)')\n\n    if K.backend() != 'tensorflow':\n        raise RuntimeError('The Deeplabv3+ model is only available with '\n                           'the TensorFlow backend.')\n\n    if not (backbone in {'xception', 'mobilenetv2'}):\n        raise ValueError('The `backbone` argument should be either '\n                         '`xception`  or `mobilenetv2` ')\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    if backbone == 'xception':\n        if OS == 8:\n            entry_block3_stride = 1\n            middle_block_rate = 2  # ! Not mentioned in paper, but required\n            exit_block_rates = (2, 4)\n            atrous_rates = (12, 24, 36)\n        else:\n            entry_block3_stride = 2\n            middle_block_rate = 1\n            exit_block_rates = (1, 2)\n            atrous_rates = (6, 12, 18)\n\n        x = Conv2D(32, (3, 3), strides=(2, 2),\n                   name='entry_flow_conv1_1', use_bias=False, padding='same')(img_input)\n        x = BatchNormalization(name='entry_flow_conv1_1_BN')(x)\n        x = Activation('relu')(x)\n\n        x = _conv2d_same(x, 64, 'entry_flow_conv1_2', kernel_size=3, stride=1)\n        x = BatchNormalization(name='entry_flow_conv1_2_BN')(x)\n        x = Activation('relu')(x)\n\n        x = _xception_block(x, [128, 128, 128], 'entry_flow_block1',\n                            skip_connection_type='conv', stride=2,\n                            depth_activation=False)\n        x, skip1 = _xception_block(x, [256, 256, 256], 'entry_flow_block2',\n                                   skip_connection_type='conv', stride=2,\n                                   depth_activation=False, return_skip=True)\n\n        x = _xception_block(x, [728, 728, 728], 'entry_flow_block3',\n                            skip_connection_type='conv', stride=entry_block3_stride,\n                            depth_activation=False)\n        for i in range(16):\n            x = _xception_block(x, [728, 728, 728], 'middle_flow_unit_{}'.format(i + 1),\n                                skip_connection_type='sum', stride=1, rate=middle_block_rate,\n                                depth_activation=False)\n\n        x = _xception_block(x, [728, 1024, 1024], 'exit_flow_block1',\n                            skip_connection_type='conv', stride=1, rate=exit_block_rates[0],\n                            depth_activation=False)\n        x = _xception_block(x, [1536, 1536, 2048], 'exit_flow_block2',\n                            skip_connection_type='none', stride=1, rate=exit_block_rates[1],\n                            depth_activation=True)\n\n    else:\n        OS = 8\n        first_block_filters = _make_divisible(32 * alpha, 8)\n        x = Conv2D(first_block_filters,\n                   kernel_size=3,\n                   strides=(2, 2), padding='same',\n                   use_bias=False, name='Conv')(img_input)\n        x = BatchNormalization(\n            epsilon=1e-3, momentum=0.999, name='Conv_BN')(x)\n        x = Activation(relu6, name='Conv_Relu6')(x)\n\n        x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n                                expansion=1, block_id=0, skip_connection=False)\n\n        x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n                                expansion=6, block_id=1, skip_connection=False)\n        x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n                                expansion=6, block_id=2, skip_connection=True)\n\n        x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n                                expansion=6, block_id=3, skip_connection=False)\n        x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n                                expansion=6, block_id=4, skip_connection=True)\n        x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n                                expansion=6, block_id=5, skip_connection=True)\n\n        # stride in block 6 changed from 2 -> 1, so we need to use rate = 2\n        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,  # 1!\n                                expansion=6, block_id=6, skip_connection=False)\n        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=7, skip_connection=True)\n        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=8, skip_connection=True)\n        x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=9, skip_connection=True)\n\n        x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=10, skip_connection=False)\n        x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=11, skip_connection=True)\n        x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, rate=2,\n                                expansion=6, block_id=12, skip_connection=True)\n\n        x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, rate=2,  # 1!\n                                expansion=6, block_id=13, skip_connection=False)\n        x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, rate=4,\n                                expansion=6, block_id=14, skip_connection=True)\n        x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, rate=4,\n                                expansion=6, block_id=15, skip_connection=True)\n\n        x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1, rate=4,\n                                expansion=6, block_id=16, skip_connection=False)\n\n    # end of feature extractor\n\n    # branching for Atrous Spatial Pyramid Pooling\n\n    # Image Feature branch\n    #out_shape = int(np.ceil(input_shape[0] / OS))\n    b4 = AveragePooling2D(pool_size=(int(np.ceil(input_shape[0] / OS)), int(np.ceil(input_shape[1] / OS))))(x)\n    b4 = Conv2D(256, (1, 1), padding='same',\n                use_bias=False, name='image_pooling')(b4)\n    b4 = BatchNormalization(name='image_pooling_BN', epsilon=1e-5)(b4)\n    b4 = Activation('relu')(b4)\n    b4 = BilinearUpsampling((int(np.ceil(input_shape[0] / OS)), int(np.ceil(input_shape[1] / OS))))(b4)\n\n    # simple 1x1\n    b0 = Conv2D(256, (1, 1), padding='same', use_bias=False, name='aspp0')(x)\n    b0 = BatchNormalization(name='aspp0_BN', epsilon=1e-5)(b0)\n    b0 = Activation('relu', name='aspp0_activation')(b0)\n\n    # there are only 2 branches in mobilenetV2. not sure why\n    if backbone == 'xception':\n        # rate = 6 (12)\n        b1 = SepConv_BN(x, 256, 'aspp1',\n                        rate=atrous_rates[0], depth_activation=True, epsilon=1e-5)\n        # rate = 12 (24)\n        b2 = SepConv_BN(x, 256, 'aspp2',\n                        rate=atrous_rates[1], depth_activation=True, epsilon=1e-5)\n        # rate = 18 (36)\n        b3 = SepConv_BN(x, 256, 'aspp3',\n                        rate=atrous_rates[2], depth_activation=True, epsilon=1e-5)\n\n        # concatenate ASPP branches & project\n        x = Concatenate()([b4, b0, b1, b2, b3])\n    else:\n        x = Concatenate()([b4, b0])\n\n    x = Conv2D(256, (1, 1), padding='same',\n               use_bias=False, name='concat_projection')(x)\n    x = BatchNormalization(name='concat_projection_BN', epsilon=1e-5)(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.1)(x)\n\n    # DeepLab v.3+ decoder\n\n    if backbone == 'xception':\n        # Feature projection\n        # x4 (x2) block\n        x = BilinearUpsampling(output_size=(int(np.ceil(input_shape[0] / 4)),\n                                            int(np.ceil(input_shape[1] / 4))))(x)\n        dec_skip1 = Conv2D(48, (1, 1), padding='same',\n                           use_bias=False, name='feature_projection0')(skip1)\n        dec_skip1 = BatchNormalization(\n            name='feature_projection0_BN', epsilon=1e-5)(dec_skip1)\n        dec_skip1 = Activation('relu')(dec_skip1)\n        x = Concatenate()([x, dec_skip1])\n        x = SepConv_BN(x, 256, 'decoder_conv0',\n                       depth_activation=True, epsilon=1e-5)\n        x = SepConv_BN(x, 256, 'decoder_conv1',\n                       depth_activation=True, epsilon=1e-5)\n\n    # you can use it with arbitary number of classes\n    if classes == 21:\n        last_layer_name = 'logits_semantic'\n    else:\n        last_layer_name = 'custom_logits_semantic'\n\n    x = Conv2D(classes, (1, 1), padding='same', name=last_layer_name)(x)\n    x = BilinearUpsampling(output_size=(input_shape[0], input_shape[1]))(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n\n    model = Model(inputs, x, name='deeplabv3+')\n\n    # load weights\n\n    if weights == 'pascal_voc':\n        if backbone == 'xception':\n            weights_path = get_file('deeplabv3_xception_tf_dim_ordering_tf_kernels.h5',\n                                    WEIGHTS_PATH_X,\n                                    cache_subdir='models')\n        else:\n            weights_path = get_file('deeplabv3_mobilenetv2_tf_dim_ordering_tf_kernels.h5',\n                                    WEIGHTS_PATH_MOBILE,\n                                    cache_subdir='models')\n        model.load_weights(weights_path, by_name=True)\n    return model\n\n\ndef preprocess_input(x):\n    \"\"\"Preprocesses a numpy array encoding a batch of images.\n    # Arguments\n        x: a 4D numpy array consists of RGB values within [0, 255].\n    # Returns\n        Input array scaled to [-1.,1.]\n    \"\"\"\n    return imagenet_utils.preprocess_input(x, mode='tf')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ada861a85e9549dca27667692da408c5fdccbaa5"},"cell_type":"markdown","source":"# About\nSince I am new to learning from image segmentation and kaggle in general I want to share my noteook.\nI saw it is similar to others as it uses the U-net approach. I want to share it anyway because:\n\n- As said, the field is new to me so I am open to suggestions.\n- It visualizes some of the steps, e.g. scaling, to learn if the methods do what I expect which might be useful to others (I call them sanity checks).\n- Added stratification by the amount of salt contained in the image.\n- Added augmentation by flipping the images along the y axes (thanks to the forum for clarification).\n- Added dropout to the model which seems to improve performance."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nimport tensorflow as tf\nK.set_session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"},"cell_type":"markdown","source":"# Params and helpers"},{"metadata":{"_uuid":"e54e151245d665e42bb95d9cf2e1a33cb9440e48","trusted":true,"collapsed":true},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 512\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0aa7bb3e1c4bf4cf5a08e62ca6f3ee0d73e575e","trusted":true,"collapsed":true},"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred / (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) / K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 / w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss\n\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec), axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530c358f2868a444e8233936996463a66c2cc4f3"},"cell_type":"markdown","source":"# Loading of training/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"#ROOT = \"/home/alexanderliao/data/Kaggle/competitions/tgs-salt-identification-challenge\"\ntrain_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])[0:600]\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")[0:600]\n#train_df = pd.read_csv(ROOT+\"/train.csv\", index_col=\"id\", usecols=[0])\n#depths_df = pd.read_csv(ROOT+\"/depths.csv\", index_col=\"id\")\n\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24d7f3d982bfa582b222f012129acdda55282b6d"},"cell_type":"markdown","source":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255."},{"metadata":{"_uuid":"b18c1f50cefd7504eae7e7b9605be3814c7cad6d","trusted":true},"cell_type":"code","source":"#train_df[\"images\"] = [np.array(load_img(ROOT+\"/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\ntrain_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86620c6a070571895f4f36ec050a25803915ed74","trusted":true},"cell_type":"code","source":"#train_df[\"masks\"] = [np.array(load_img(ROOT+\"/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\ntrain_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1137f0a009f10b5f69e4dade5f689e744e9ce1d6"},"cell_type":"markdown","source":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage."},{"metadata":{"_uuid":"18d2aa182a44c65a87c75f41047c653a79bc1c3f","trusted":true,"collapsed":true},"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b13d1ecc7004832e8e042d034922796263054b7","trusted":true,"collapsed":true},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5e66ff4809ea2f9a679b7ddbda5028dc324137a","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dd39993eb2c7e77e5ce2d3388ea8ff1d581a670","trusted":true,"collapsed":true},"cell_type":"code","source":"plt.scatter(train_df.coverage, train_df.coverage_class)\nplt.xlabel(\"Coverage\")\nplt.ylabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2391c568019151b098a002937516bb77a506f403"},"cell_type":"markdown","source":"# Plotting the depth distributions\nSeparatelty plotting the depth distributions for the training and the testing data."},{"metadata":{"_uuid":"6ae7b7011b7de3caed58f9ca3939df15ffa319ad","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"},"cell_type":"markdown","source":"# Show some example images"},{"metadata":{"_uuid":"1a6bc85ee458f72c0917edf77895d5abc5eaf3ee","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00655e32f93f96ebd90dbe94e35ee052f52217cd"},"cell_type":"markdown","source":"# Create train/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."},{"metadata":{"_uuid":"2d3c3157512d11e71ac74ce51a937b85bedfe1d1","trusted":true},"cell_type":"code","source":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2f1ab00f03e71e6d7f9b2214408b5a9779fc235","trusted":true},"cell_type":"code","source":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63ac58ab47921b4e4f54102e2c8b85fa318225f1"},"cell_type":"markdown","source":"# Build model"},{"metadata":{"_uuid":"1aa78bd7c607e1f0e0235e4b2f82056c0361dac5","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"def conv_block(m, dim, acti, bn, res, do=0):\n\tn = Conv2D(dim, 3, activation=acti, padding='same')(m)\n\tn = BatchNormalization()(n) if bn else n\n\tn = Dropout(do)(n) if do else n\n\tn = Conv2D(dim, 3, activation=acti, padding='same')(n)\n\tn = BatchNormalization()(n) if bn else n\n\treturn Concatenate()([m, n]) if res else n\n\ndef level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n\tif depth > 0:\n\t\tn = conv_block(m, dim, acti, bn, res)\n\t\tm = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n\t\tm = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n\t\tif up:\n\t\t\tm = UpSampling2D()(m)\n\t\t\tm = Conv2D(dim, 2, activation=acti, padding='same')(m)\n\t\telse:\n\t\t\tm = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n\t\tn = Concatenate()([n, m])\n\t\tm = conv_block(n, dim, acti, bn, res)\n\telse:\n\t\tm = conv_block(m, dim, acti, bn, res, do)\n\treturn m\n\ndef UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n\t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n\ti = Input(shape=img_shape)\n\to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n\to = Conv2D(out_ch, 1, activation='sigmoid')(o)\n\treturn Model(inputs=i, outputs=o)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2382b088c8a6be16490354ebd386120a9ced414d","trusted":true},"cell_type":"code","source":"#model = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)\nmodel =  Deeplabv3(input_shape=(512,512,3),backbone=\"mobilenetv2\", classes=1)  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3399029adb039b049e3d6ca01fef30ed8653482b","trusted":true,"collapsed":true},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",mean_iou,\"binary_crossentropy\",dice_coef])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7ded4adc1757c88a1bea59ea36b1a9f7941bd28","scrolled":true,"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c007157c2fd3d7dadcaeee2a6376351852d1e565"},"cell_type":"markdown","source":"# Data augmentation"},{"metadata":{"_uuid":"88b3f57eac3ec3719b401730dc6d8d2d89d09ccc","trusted":true,"collapsed":true},"cell_type":"code","source":"#x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n#y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7040f72549212dd4f71c13dfbd8bf013481ea369","trusted":true,"scrolled":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a6b1abaa4681cba3b608bc5f33cf260370d82a"},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"547543a70d4e3f75f1e1521dc7f8ff3b5ff71ebc","collapsed":true},"cell_type":"code","source":"print(x_train.shape)\nprint(np.repeat(x_train,3,3).shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":true,"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=50, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"./deeplabv3.model\", save_best_only=True, verbose=1)\n#reduce_lr = ReduceLROnPlateau(factor=0.5, patience=2, min_lr=0.00001, verbose=1)\n\nepochs = 200\nbatch_size = 4\n\nwith tf.device (\"/gpu:0\"):\n    history = model.fit(np.repeat(x_train,3,3), y_train,\n                        validation_data=[np.repeat(x_valid,3,3), y_valid], \n                        epochs=epochs,\n                        batch_size=batch_size,\n                        callbacks=[early_stopping, model_checkpoint],shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c824f6bca47f051500966c433ce7fb5a9528f6d7","trusted":false,"collapsed":true},"cell_type":"code","source":"model.load_weights(\"./deeplabv3.model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f168318eadb324daa8c020f0e3e0a24d82a464f"},"cell_type":"markdown","source":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions."},{"metadata":{"_uuid":"16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46","trusted":false,"collapsed":true},"cell_type":"code","source":"preds_valid = model.predict(np.repeat(x_valid,3,3)).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b1198b6fb7369c3cfb70e68cd1b78d36aa188bc","trusted":false,"collapsed":true},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd973023204ebf921fe1f23748856e6a6f692aa4","collapsed":true},"cell_type":"markdown","source":"# Scoring\nScore the model and do a threshold optimization by the best IoU."},{"metadata":{"_uuid":"d261beec66b6867ac0d5c94684f12aa08b70d638","trusted":false,"collapsed":true},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85f6d9567cec0ef8976730a6834b6569b6e108a0","trusted":false,"collapsed":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"183d37ad32bc2f1f0d17a9538702c45a826ccefc","trusted":false,"collapsed":true},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ced29761f2d1760245112a30a7abd4783b373dd","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"423b3268c580dc1eae84f54deeeb0f691eff6028"},"cell_type":"markdown","source":"# Another sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold."},{"metadata":{"_uuid":"40c263765ac6d53a8c0c1361ff1e6f061eecf825","trusted":false,"collapsed":true},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"332a614c0ae837c115ec6563f355753ffbb8cd83"},"cell_type":"markdown","source":"# Submission\nLoad, predict and submit the test image predictions."},{"metadata":{"_uuid":"72128add82c6853441671fde67e7e66601a01787","trusted":false,"collapsed":true},"cell_type":"code","source":"# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ecb152b492c7126d12c5ef2c701eec8ea3d86f1","trusted":true,"collapsed":true},"cell_type":"code","source":"#x_test = np.array([upsample(np.array(load_img(ROOT+\"/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)\nx_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f278d0b87320c117b4ed7c116a991782b82ba5a7","trusted":false,"collapsed":true},"cell_type":"code","source":"preds_test = model.predict(np.repeat(x_test,3,3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"113f816f9db8b87ca7f6845fe6e61328ab606f41","trusted":false,"collapsed":true},"cell_type":"code","source":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4243166f91c4bcb4da00208f4f53dd912dbb429f","trusted":false,"collapsed":true},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cf680f2d0494e08ed40eee89243df92bf1482655","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}