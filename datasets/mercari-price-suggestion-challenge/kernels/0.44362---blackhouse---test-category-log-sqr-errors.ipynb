{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "version": "3.6.4", "nbconvert_exporter": "python", "mimetype": "text/x-python", "name": "python", "file_extension": ".py"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "cells": [{"cell_type": "code", "metadata": {"_cell_guid": "3b4fb300-08e6-4d1a-b690-6c4b66c9d70c", "collapsed": true, "_uuid": "90c12427206e7f47e4259440e91f351641077bde"}, "source": ["# Credit: \n", "# https://www.kaggle.com/apapiu/ridge-script/code\n", "# https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "d367423b-1b4a-47e2-a9d9-b5b19a82e9a9", "collapsed": true, "_uuid": "fde26cbc50f7472fd5478ba4331a01658de3be9b"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import scipy\n", "\n", "from sklearn.feature_extraction.text import CountVectorizer #convert text to word count vectors\n", "from sklearn.feature_extraction.text import TfidfVectorizer #convert text to word frequency vectors\n", "from sklearn.feature_extraction.text import HashingVectorizer #convert text to unique integers \n", "from sklearn.preprocessing import LabelBinarizer\n", "\n", "from sklearn.linear_model import Ridge, LogisticRegression"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "87a23634-51dc-439d-938a-5c643bcc5990", "collapsed": true, "_uuid": "260181f31baea413dea79fd1b8b1a86088720e24"}, "source": ["#NUM_BRANDS = 2500  # set a maximum number of brand names recorded in the dataset - pop_brand\n", "NAME_MIN_DF = 5\n", "MAX_FEAT_DESCP = 60000\n", "pd.set_option('display.max_colwidth', -1)\n", "pd.set_option('display.max_rows', 300)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "af9e1c07-cd96-4ec0-844c-6b0735cf8836", "_uuid": "e7f89f3666df438be64596bc9c6ba3572ba9df82"}, "source": ["from nltk.corpus import stopwords\n", "stopWords = set(stopwords.words('english'))\n", "[stopWords.discard(w) for w in ['with','not','doesn\\'t','don\\'t','didn\\'t','under','above','all',\n", "                               'aren','aren\\'t','below','both','but','couldn','couldn\\'t','didn',\n", "                               'doesn','don','doesn','don','down','few','hadn','hadn\\'t','hasn',\n", "                               'hasn\\'t','haven','haven\\'t','isn','isn\\'t','mightn','mightn\\'t',\n", "                               'more','most','mustn','mustn\\'t','needn','needn\\'t','no','nor',\n", "                               'off','once','only','other',\"shouldn't\",'some','up','very',\"wasn't\",\n", "                               \"weren't\",\"won't\",\"wouldn't\"]]\n", "#stopWords.update(['like','\"','go','don\\'t','get','know','wikipedia','people'])\n", "len(stopWords)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "8ed55ca0-55d7-422d-a8a0-30dd48a3db00", "collapsed": true, "_uuid": "ce9bbe8500a6f59a1364412e3d889e7b74dd40c5"}, "source": ["train = pd.read_table('../input/train.tsv')\n", "test = pd.read_table('../input/test.tsv')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "75791f39-a5fd-406a-802d-1f6796b65976", "collapsed": true, "_uuid": "42863091eeb226176296ef3674492008a079fa68"}, "source": ["#features = test.columns.values # test.tsv test_id vs train_id in train.tsv\n", "# Remove items with price $0 in the training set\n", "train = train[train.price>0.0]"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["#train.category_name[train.category_name.str.contains('Women', na=False)].value_counts()\n", "m = (train.category_name.value_counts()>1000).sum()\n", "top_categories = train.category_name.value_counts().index[:m]\n", "#top_categories"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "8caf85bd-e93e-4c9b-809f-01e3ec775c64", "_uuid": "152f7ab30e6bcd7c6101c475d236f4c07f17372a"}, "source": ["'''\n", "# Split the datasets for segmented linear regression\n", "# 'Electronics', 'Women/Jewelry', 'Women/Women's Handbags', 'Women/Tops & Blouses', 'Men', other\n", "train2 = train[train.category_name.str.contains('Women\\'s Handbags',na=False)]\n", "train1 = train[train.category_name.str.contains('Smartphones',na=False)]\n", "train0 = train[-train.category_name.str.contains('Smartphones|Women\\'s Handbags',na=False)]\n", "#n2 = len(train2)\n", "#n1 = len(train1)\n", "#n0 = len(train0)\n", "#y_train1 = np.log1p(train.price[train.category_name.str.contains('Electronics',na=False)]) # target: price\n", "#y_train0 = np.log1p(train.price[-train.category_name.str.contains('Electronics',na=False)]) # target: price\n", "test2 = test[test.category_name.str.contains('Women\\'s Handbags',na=False)]\n", "test1 = test[test.category_name.str.contains('Smartphones',na=False)]\n", "test0 = test[-test.category_name.str.contains('Smartphones|Women\\'s Handbags',na=False)]\n", "'''\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "041195c8-9d3b-4780-9a32-10298b1e02af", "_uuid": "11b85c52df71c45927b7655478327a9c7d6e58f1"}, "source": ["'''\n", "df2 = pd.concat([train2, test2], 0)\n", "df1 = pd.concat([train1, test1], 0)\n", "df0 = pd.concat([train0, test0], 0)\n", "'''"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "a9385003-4076-4a04-bbe5-ca1d331f41a6", "collapsed": true, "_uuid": "7e557b3b962176a870693e1b64d27b6fb67b8c99"}, "source": ["\n", "def segmented_linear_regression(df):\n", "    df[\"category_name\"] = df[\"category_name\"].fillna(\"unknown\").astype('category')\n", "    df[\"brand_name\"] = df[\"brand_name\"].fillna(\"unknown\")\n", "    pop_brands = df[\"brand_name\"].value_counts().index[:]\n", "    df.loc[~df[\"brand_name\"].isin(pop_brands), \"brand_name\"] = \"Other\"\n", "    df[\"brand_name\"] = df[\"brand_name\"].astype(\"category\")\n", "    df[\"item_description\"] = df[\"item_description\"].fillna(\"None\")\n", "    df[\"item_condition_id\"] = df[\"item_condition_id\"].astype(\"category\")\n", "        \n", "    # 'name' - count vectorizer\n", "    count_name = CountVectorizer(min_df=NAME_MIN_DF,ngram_range=(1,3),stop_words = stopWords)\n", "    X_name = count_name.fit_transform(df[\"name\"])\n", "\n", "    # 'category_name' - count vecrtorizer\n", "    count_cat = CountVectorizer(ngram_range = (1,3), stop_words = stopWords)\n", "    X_category = count_cat.fit_transform(df[\"category_name\"])\n", "    \n", "    # 'item_description' - Tfidf\n", "    tfidf_descp = TfidfVectorizer(max_features = MAX_FEAT_DESCP, \n", "                                  ngram_range = (1,3),\n", "                                  stop_words = stopWords)\n", "    X_descp = tfidf_descp.fit_transform(df[\"item_description\"])\n", "    \n", "    # 'brand_name' - Label Binarizer\n", "    bnrz_brand = LabelBinarizer(sparse_output=True)\n", "    X_brand = bnrz_brand.fit_transform(df[\"brand_name\"])\n", "    \n", "    # 'item_condition_id': One Hot Key Encoding\n", "    X_condition = scipy.sparse.csr_matrix(pd.get_dummies(df[[\n", "        \"item_condition_id\", \"shipping\"]], sparse = True).values)\n", "    \n", "    X = scipy.sparse.hstack((X_condition, \n", "                         X_descp,\n", "                         X_brand,\n", "                         X_category,\n", "                         X_name)).tocsr()\n", "    \n", "    n = df.train_id.count()\n", "    X_train = X[:n]\n", "    y_train = np.log1p(df['price'][:n])\n", "    \n", "    model = Ridge(solver = \"lsqr\", fit_intercept=False)\n", "    print(\"Fitting Model\")\n", "    model.fit(X_train, y_train)\n", "    \n", "    train_sum_sqr_error = (model.predict(X_train) - y_train).pow(2).sum()\n", "    \n", "    X_test = X[n:]\n", "    p = model.predict(X_test)\n", "\n", "    df_test = df[n:]\n", "    df_test['price'] = np.expm1(p)\n", "    return (df_test[['test_id','price']], train_sum_sqr_error, n)\n", "    "], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["test_new = pd.DataFrame()\n", "train_sum_sqr_error = 0\n", "train_others = train.copy()\n", "test_others = test.copy()\n", "for cat in top_categories:\n", "    train_by_cat = train[train.category_name==cat]\n", "    test_by_cat = test[test.category_name==cat]\n", "    df = pd.concat([train_by_cat, test_by_cat], 0)\n", "    df_seg, error, n = segmented_linear_regression(df)\n", "    test_new = pd.concat([test_new,df_seg], 0)\n", "    print(cat, n, (error/n)**0.5)\n", "    train_sum_sqr_error += error\n", "    train_others = train_others[train_others.category_name!=cat]\n", "    test_others = test_others[test_others.category_name!=cat]\n", "df = pd.concat([train_others, test_others], 0)\n", "df_seg, error, n = segmented_linear_regression(df)\n", "test_new = pd.concat([test_new,df_seg], 0)\n", "print(\"Others\", n, (error/n)**0.5)\n", "train_sum_sqr_error += error\n", "\n", "train_error = (train_sum_sqr_error/len(train))**0.5\n", "print(\"train error:\", train_error)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "source": ["'''\n", "test_new = pd.DataFrame()\n", "train_sum_sqr_error = 0\n", "for df in [df2, df1, df0]:\n", "    df_seg, error, n = segmented_linear_regression(df)\n", "    test_new = pd.concat([test_new,df_seg], 0)\n", "    print(error, n, (error/n)**0.5)\n", "    train_sum_sqr_error += error \n", "train_error = (train_sum_sqr_error/len(train))**0.5\n", "print(\"train error:\", train_error)\n", "'''"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "0b36365d-7d26-4c5b-be84-97f3abd8eeab", "collapsed": true, "_uuid": "0b8bf5c75ea051d7828a28736fc97e786a27b138"}, "source": ["test_new = test_new.sort_values('test_id')\n", "test_new['test_id'] = test_new.test_id.astype('int64')\n", "test_new[[\"test_id\", \"price\"]].to_csv(\"submission_ridge.csv\", index = False)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "827359eb-0312-4b93-9c54-299a86b0a9a9", "_uuid": "3e838dd8d100ba08b29dabb333a69d4e9c48071d"}, "source": ["test_new.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "6b70c6af-df05-49de-89e0-e44b450c688b", "collapsed": true, "_uuid": "321da77cd384161e6253f2b2b67b9c9ebde42b28"}, "source": [], "execution_count": null, "outputs": []}], "nbformat_minor": 1}