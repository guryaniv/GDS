{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "755de377-4598-7348-4ce9-5374323356b1"
      },
      "source": [
        "# An example of parsing text with Spacy. And using POS tags to make some fuzzy feature metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e5553f1-117d-53f6-97f5-2ca97f82ab82"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.en import English\n",
        "from spacy.symbols import *\n",
        "nlp = English()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from fuzzywuzzy import fuzz\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "root_path = '../input/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a1e20f3d-db1b-bca2-6f01-8f269cb874ff"
      },
      "outputs": [],
      "source": [
        "def get_distinct_questions(train, test):\n",
        "    df1 = train[['question1']].copy()\n",
        "    df2 = train[['question2']].copy()\n",
        "    df1_test = test[['question1']].copy()\n",
        "    df2_test = test[['question2']].copy()\n",
        "\n",
        "    df2.rename(columns = {'question2':'question1'},inplace=True)\n",
        "    df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
        "\n",
        "    questions = df1.append(df2)\n",
        "    questions = questions.append(df1_test)\n",
        "    questions = questions.append(df2_test)\n",
        "    \n",
        "    questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
        "\n",
        "    questions.reset_index(inplace=True,drop=True)\n",
        "    del df1,df1_test, df2, df2_test\n",
        "    return questions\n",
        "\n",
        "def parse_question(doc):\n",
        "    #doc = nlp(question)\n",
        "    \n",
        "    pobj = []\n",
        "    dobj = []\n",
        "    num = []\n",
        "    qword = []\n",
        "    noun = []\n",
        "    verb = []\n",
        "    adj = []\n",
        "    ents = []\n",
        "    ent_types = []\n",
        "    \n",
        "    \n",
        "    ent = []\n",
        "    ent_type = \"\"\n",
        "    sent_count = 0\n",
        "    for s in doc.sents:\n",
        "        sent_count+=1\n",
        "        for word in s:\n",
        "            #print(\"(\" + word.ent_type_  + \",\" + str(word.pos)  + \",\" + word.pos_  + \",\" + str(word.pos)  + \",\" + word.tag_  \\\n",
        "            #      + \",\" + str(word.tag)  + \",\" + word.dep_    + \",\" + str(word.dep)  + \",\" + word.lemma_ + \") \")\n",
        "            #ENTITIES\n",
        "            if word.ent_type == 0:\n",
        "                if len(ent) > 0:\n",
        "                    ents.append('_'.join(ent))                    \n",
        "                    ent_types.append(ent_type)\n",
        "                    ent = []\n",
        "                    ent_type = \"\"\n",
        "            elif word.ent_type > 0 and word.ent_iob == 3:\n",
        "                if len(ent) > 0:\n",
        "                    ents.append('_'.join(ent))                    \n",
        "                    ent_types.append(ent_type)\n",
        "                    ent = []\n",
        "                ent.append(word.lemma_)\n",
        "                ent_type = word.ent_type_ \n",
        "            elif word.ent_type > 0 and word.ent_iob == 1:                \n",
        "                ent.append(word.lemma_)\n",
        "                \n",
        "            #QUESTIONS\n",
        "            if word.tag_.find('W') == 0:\n",
        "                qword.append(word.lemma_)\n",
        "            #NOUNS\n",
        "            elif word.pos in [90,94]:\n",
        "                noun.append(word.lemma_)\n",
        "                \n",
        "                #pobj\n",
        "                if word.dep == 435:\n",
        "                    pobj.append(word.lemma_)\n",
        "                #dobj\n",
        "                elif word.dep == 412:\n",
        "                    dobj.append(word.lemma_)\n",
        "\n",
        "            #NUMBER\n",
        "            elif word.pos in [91]:\n",
        "                num.append(word.lemma_)\n",
        "            #ADJ\n",
        "            elif word.pos in [82]:\n",
        "                adj.append(word.lemma_)\n",
        "            #VERB\n",
        "            elif word.pos in [98]:\n",
        "                verb.append(word.lemma_)     \n",
        "            \n",
        "    if len(ent) > 0:\n",
        "        ents.append('_'.join(ent))                    \n",
        "        ent_types.append(ent_type)\n",
        "        ent = []   \n",
        "    #print(sent_count, pobj, dobj, num, qword, noun, verb, adj, ents, ent_types)\n",
        "    return sent_count, pobj, dobj, num, qword, noun, verb, adj, ents, ent_types\n",
        "\n",
        "\n",
        "def match_count(list1, list2):\n",
        "    return len(set(list1).intersection(set(list2)))\n",
        "\n",
        "def diff_count(list1, list2):\n",
        "    return len([obj for obj in list1 if obj not in list2] + [obj for obj in list2 if obj not in list1])\n",
        "\n",
        "def get_nlp_features(nlp_parts1, nlp_parts2):\n",
        "\n",
        "    sent_count1, pobj1, dobj1, num1, qword1, noun1, verb1, adj1, ents1, ent_types1 = nlp_parts1\n",
        "    sent_count2, pobj2, dobj2, num2, qword2, noun2, verb2, adj2, ents2, ent_types2 = nlp_parts2\n",
        "    \n",
        "    ret = []\n",
        "    \n",
        "    f = diff_count\n",
        "    ret1 = [abs(sent_count1 - sent_count2), f(pobj1,pobj2), f(dobj1,dobj2),\\\n",
        "            f(num1,num2), f(qword1,qword2), f(noun1,noun2),\\\n",
        "            f(verb1,verb2), f(adj1,adj2), f(ents1,ents2), f(ent_types1,ent_types1)]\n",
        "    \n",
        "    f = match_count\n",
        "    ret2 = [f(pobj1,pobj2), f(dobj1,dobj2),\\\n",
        "            f(num1,num2), f(qword1,qword2), f(noun1,noun2),\\\n",
        "            f(verb1,verb2), f(adj1,adj2), f(ents1,ents2), f(ent_types1,ent_types1)]\n",
        "    ret3 = [ret2[0] * ret2[0],ret2[1] * ret2[1],ret2[2] * ret2[2],ret2[3] * ret2[3],ret2[4] * ret2[4],ret2[5] * ret2[5]\\\n",
        "            ,ret2[6] * ret2[6],ret2[7] * ret2[7],ret2[8] * ret2[8]]\n",
        "    f = fuzz.QRatio\n",
        "    ret4 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n",
        "    \n",
        "    f = fuzz.WRatio\n",
        "    ret5 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n",
        "          \n",
        "    f = fuzz.token_set_ratio\n",
        "    ret6 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n",
        "    \n",
        "    f = fuzz.token_sort_ratio\n",
        "    ret7 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n",
        "    \n",
        "    f = fuzz.partial_ratio\n",
        "    ret8 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n",
        "    \n",
        "    f = fuzz.partial_token_sort_ratio\n",
        "    ret9 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n",
        "    \n",
        "    f = fuzz.partial_token_set_ratio\n",
        "    ret10 = [f(pobj1,pobj2), f(dobj1,dobj2), f(num1,num2), f(qword1,qword2), f(noun1,noun2), f(verb1,verb2), f(adj1,adj2), f(ents1,ents2)]\n",
        "    \n",
        "    ret11 = [sent_count1, sent_count2, len(pobj1), len(pobj2), len(dobj1), len(dobj2), len(num1),len(num2),\\\n",
        "            len(qword1),len(qword2), len(noun1),len(noun2), len(verb1),len(verb2), len(adj1),len(adj2), len(ents1),len(ents2)]\n",
        "    \n",
        "    \n",
        "    ret.extend(ret1)\n",
        "    ret.extend(ret2)\n",
        "    ret.extend(ret3)\n",
        "    ret.extend(ret4)\n",
        "    ret.extend(ret5)\n",
        "    ret.extend(ret6)\n",
        "    ret.extend(ret7)\n",
        "    ret.extend(ret8)\n",
        "    ret.extend(ret9)\n",
        "    ret.extend(ret10)\n",
        "    ret.extend(ret11)\n",
        "    return tuple(ret)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3d8828de-4e94-1386-3366-1758c7c88e78"
      },
      "source": [
        "# Cache a parse of all the distinct questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "65aff1be-5b27-03cc-ff33-0ba18ac0f461",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_data =  pd.read_csv(root_path + 'train.csv', header=0)\n",
        "test_data =  pd.read_csv(root_path + 'test.csv', header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2a2b383-4e50-5527-5e61-33821e124ae4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_questions = get_distinct_questions(train_data, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7f6e983-44b9-474a-7f7a-0bf68322239d"
      },
      "outputs": [],
      "source": [
        "nlp_parse_lookup = {}\n",
        "index = 0\n",
        "for doc in tqdm_notebook(nlp.pipe([str(q) for q in train_questions['question1']], n_threads=16, batch_size=10000), total = len(train_questions)):\n",
        "    nlp_parse_lookup[str(train_questions.iloc[index]['question1'])] = parse_question(doc)\n",
        "    index += 1\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "32545be8-18e2-ceac-a678-0d678c399111"
      },
      "source": [
        "# Make some fuzzy metrics feature from the parsed content\n",
        "These features get 0.35 LB score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "480323c7-0aca-26ba-fd14-7b125fb2249d"
      },
      "outputs": [],
      "source": [
        "type_list =['pobj', 'dobj', 'num', 'qword', 'noun', 'verb', 'adj', 'ents']\n",
        "\n",
        "columns=[]\n",
        "columns.append('sent_diff_count') \n",
        "columns.extend([t + \"_diff_count\" for t in type_list])  \n",
        "columns.append('ent_types_diff_count') \n",
        "\n",
        "columns.extend([t + \"_match_count\" for t in type_list])  \n",
        "columns.append('ent_types_match_count') \n",
        "\n",
        "columns.extend([t + \"_match_square\" for t in type_list])  \n",
        "columns.append('ent_types_match_square')                \n",
        "                                                                                                                \n",
        "columns.extend([t + \"_QRatio\" for t in type_list])                                                             \n",
        "columns.extend([t + \"_WRatio\" for t in type_list])                                                             \n",
        "columns.extend([t + \"_token_set_ratio\" for t in type_list])                                                             \n",
        "columns.extend([t + \"_token_sort_ratio\" for t in type_list])                                                             \n",
        "columns.extend([t + \"_partial_ratio\" for t in type_list])                                                             \n",
        "columns.extend([t + \"_partial_token_sort_ratio\" for t in type_list])                                                             \n",
        "columns.extend([t + \"_partial_token_set_ratio\" for t in type_list])\n",
        "                                                                                                              \n",
        "columns.extend(['sent_count1', 'sent_count2','len_pobj1', 'len_pobj2', 'len_dobj1', 'len_dobj2',\\\n",
        "                'len_num1', 'len_num2', 'len_qword1', 'len_qword2', 'len_noun1', 'len_noun2', \\\n",
        "                'len_verb1', 'len_verb2', 'len_adj1', 'len_adj2', 'len_ents1', 'len_ents2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0b1896c-0445-5b50-0167-619b241f2708"
      },
      "outputs": [],
      "source": [
        "feature_list = [get_nlp_features(nlp_parse_lookup[str(q[0])], nlp_parse_lookup[str(q[1])]) \\\n",
        "                for q in tqdm_notebook(train_data[['question1','question2']].values, total = len(train_data))]\n",
        "nlp_feat = pd.DataFrame(feature_list, columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b37d363-209b-74eb-d05f-0396461d3c06"
      },
      "outputs": [],
      "source": [
        "feature_list = [get_nlp_features(nlp_parse_lookup[str(q[0])], nlp_parse_lookup[str(q[1])]) \\\n",
        "                for q in tqdm_notebook(test_data[['question1','question2']].values, total = len(test_data))]\n",
        "nlp_test_feat = pd.DataFrame(feature_list, columns=columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "beb3f829-cb9c-a681-9590-5d1d6044bffd"
      },
      "source": [
        "# Save Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "61802df9-18c0-f392-3b7c-d99a7d5f69a7"
      },
      "outputs": [],
      "source": [
        "nlp_feat.to_csv(root_path + 'quora_train_features_nlp.tsv', index=False, sep='\\t')\n",
        "nlp_test_feat.to_csv(root_path + 'quora_test_features_nlp.tsv', index=False, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a2e5a728-15e3-756e-7bbf-b756235ee83b",
        "collapsed": true
      },
      "source": [
        "# Some analysis of the features gathered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca814ee6-cd9a-fdcd-2394-4b19abf92c17"
      },
      "outputs": [],
      "source": [
        "nlp_feat['is_duplicate'] = train_data['is_duplicate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d9361c68-3644-1609-6a7e-29617e4bc248",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "mcorr = nlp_feat.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0576f4fb-ce86-0af5-3186-dcc9cd51849e"
      },
      "outputs": [],
      "source": [
        "#Check\n",
        "mcorr.sort_values(['is_duplicate'])['is_duplicate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b34abfd6-e8c4-6ea1-1b1d-b4fd6e87ac53"
      },
      "outputs": [],
      "source": [
        "for column_name in mcorr.columns:\n",
        "    index = 0\n",
        "    matches = mcorr.query('abs(' + str(column_name) + ') >= 0.995').sort_values(column_name)[column_name]    \n",
        "    if len(matches) > 1:\n",
        "        print()\n",
        "        print(column_name  + \"\\n----------------\")\n",
        "        for match in matches:            \n",
        "            if matches.index[index] != column_name:\n",
        "                print(matches.index[index] + '\\t' + str(match))\n",
        "       \n",
        "            index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "155e1bdb-a205-2562-43b4-b05482ec437e",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def plot_real_feature(fname, train_feat):\n",
        "    fig = plt.figure()\n",
        "    ax1 = plt.subplot2grid((3, 2), (0, 0), colspan=2)\n",
        "    ax2 = plt.subplot2grid((3, 2), (1, 0), colspan=2)\n",
        "    ax3 = plt.subplot2grid((3, 2), (2, 0))\n",
        "    ax4 = plt.subplot2grid((3, 2), (2, 1))\n",
        "    ax1.set_title('Distribution of %s' % fname, fontsize=20)\n",
        "    sns.distplot(train_feat[fname], \n",
        "                 bins=50, \n",
        "                 ax=ax1)    \n",
        "    sns.distplot(train_feat[train_feat.is_duplicate == 1][fname], \n",
        "                 bins=50, \n",
        "                 ax=ax2,\n",
        "                 label='is dup')    \n",
        "    sns.distplot(train_feat[train_feat.is_duplicate == 0][fname], \n",
        "                 bins=50, \n",
        "                 ax=ax2,\n",
        "                 label='not dup')\n",
        "    ax2.legend(loc='upper right', prop={'size': 18})\n",
        "    sns.boxplot(y=fname, \n",
        "                x='is_duplicate', \n",
        "                data=train_feat, \n",
        "                ax=ax3)\n",
        "    sns.violinplot(y=fname, \n",
        "                   x='is_duplicate', \n",
        "                   data=train_feat, \n",
        "                   ax=ax4)\n",
        "    plt.show()\n",
        "\n",
        "def plot_corr(mcorr):    \n",
        "    \n",
        "    mask = np.zeros_like(mcorr, dtype=np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "    g = sns.heatmap(mcorr, mask=mask, cmap=cmap, square=True, annot=True, fmt='0.2f')\n",
        "    g.set_xticklabels(mcorr.columns, rotation=90)\n",
        "    g.set_yticklabels(reversed(mcorr.columns))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1df29d83-f4b1-a660-6998-7a41fee67f77"
      },
      "outputs": [],
      "source": [
        "plot_real_feature('pobj_match_count', nlp_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9daac28b-de93-ca89-c79f-a3266d5f6842",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}