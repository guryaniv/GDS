{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import libraries\n\nimport datetime\nimport json\nfrom functools import reduce\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.linear_model import LogisticRegression,LassoCV\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.pipeline import Pipeline,FeatureUnion\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.preprocessing import StandardScaler\nimport sys\nimport os\nprint('libraries done')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aca4901d7c6234948fefb057e244e178bcc1323"},"cell_type":"code","source":"''' 1.get data - train/test '''\n''' 2.glimpse of Data '''\n''' 3.Data Exploration '''\n''' concrete variables '''\n''' concrete not dummy vars '''\n''' general variables '''\n''' general not dummy variables '''\n''' 4.many missing rows '''\n''' 5.manage missings '''\n''' 6.Flattern id by idhogar within (house mates form rows to columns) '''\n''' 7.aggregate by idhogar '''\n''' 8.watch outliers '''\n''' 9.get training and testing sets '''\n''' 10.get models and thier parameters '''\n''' 11.train models '''\n''' 12.make submission '''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71b2e27e72ca1d792373e52cf7b4e43db8ee7493"},"cell_type":"code","source":"''' 1.get data - train/test '''\n\npath='../input/'\n\n# get train data\niid='Id'\ntrain=pd.read_csv(path+'train.csv',index_col=0)\ntrain['stage']='train'\ntarget='Target'\n\n#get test data\ntest=pd.read_csv(path+'test.csv',index_col=0)\ntest['stage']='test'\ntest[target]=-99 # set target to -99 so that it is in train/test when combined\n\n# make one daatset in order to prepare both test and train variables\ncol=list(set(train.columns)&set(test.columns))\ntrain=pd.concat([train[col],test[col]],axis=0)\nprint(train.head(2)) # watch data\nprint(train.describe()) # watch data without nas\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd7a74b24e4f63e589f3b10737575b952013c582"},"cell_type":"code","source":"''' 2.glimpse of Data '''\n\n\n# prove we should watch household as whole\nprint(train.groupby('idhogar')[target].nunique().value_counts())\n\n# plot numeric variables with words in it\nfor i in ['edjefe','edjefa','rez_esc','dependency']:\n    print('########'+i)\n    print(train[i].value_counts())\n    \n# fix text in some vars\ntrain.loc[train['dependency']=='yes','dependency']='8'\ntrain.loc[train['dependency']=='no','dependency']='.125'\ntrain['dependency']=train['dependency'].astype(float)\n\n# check education vars separately\ntrain.loc[train['edjefa']=='yes','edjefa']='1'\ntrain.loc[train['edjefa']=='no','edjefa']='0'\ntrain['edjefa']=train['edjefa'].astype(float)\ntrain.loc[train['edjefe']=='yes','edjefe']='1'\ntrain.loc[train['edjefe']=='no','edjefe']='0'\ntrain['edjefe']=train['edjefe'].astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed4781ac54cb1f4c0f6dcf75773ee36d9470ffdb"},"cell_type":"code","source":"\n# check 'edjefe','edjefa' variables meaning\nh=train.loc[train['parentesco1']==1]\nhm=h.loc[h.male==1,['edjefe','escolari']]\nhf=h.loc[h.female==1,['edjefa','escolari']]\nprint(hf.corr())\nprint(hm.corr())\n# we finally see that correlation is great - so i would rather omit wordy variables\n\n''' 'rez_esc' is bad - so omit by other reason - see further '''","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"''' 3.Data Exploration '''\n\ndtrain=train.loc[train['stage']=='train']\n\n# watch variables unique quantity\nu=train.groupby('idhogar').nunique()\nu=u.mean(axis=0)\ngen=list(u[u<=1].index) # get variables idhogar unique\nconc=list(u[u>1].index) # get variables id unique\n\n''' concrete variables '''\n\n# watch dummy variables - we group 'em\n\na=pd.DataFrame([i[:3] for i in conc])\na=a[0].value_counts()\nl=[]\nfor i in a.index:\n  g=[j for j in conc if i in j[:3]]\n  l.append(g)\nl0=[i for i in l if len(i)>1]\nl1=[i for i in l if len(i)<=1]\nl1.remove(['male'])\nl1.remove(['female'])\nl0.append(['male','female'])\n\nconcrete_dum_vars=l0.copy()\n\nprint(concrete_dum_vars)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2f71066c89a63a5e8eca5e25b4f0ded06a0410a"},"cell_type":"code","source":"# plot id unique variables on train set\nfor v in l0[1:3]:\n    dtrain1=dtrain.copy()\n    dtrain1['var']=dtrain1[v].idxmax(axis=1)\n\n    fr=np.unique(dtrain1['var'])\n    fh=''.join([i for i in fr[0] if not i.isdigit()])\n    ff=np.sort([int(i.replace(fh,'')) for i in fr ])\n    ff=[fh+str(i) for i in ff]\n\n    plt.figure()\n    sns.countplot(x='var',  data=dtrain1,order=ff )\n    plt.xticks(rotation=50)\n    plt.show()\n\n    plt.figure()\n    sns.factorplot(x='var',y=target, data=dtrain1, kind=\"bar\",order=ff)\n    plt.xticks(rotation=50)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aee1100804be78597e30c49a1f86f33a9cd1540a","scrolled":true},"cell_type":"code","source":"''' concrete not dummy vars '''\n\nconcrete_contin_vars=[v[0] for v in l1]\nconcrete_contin_vars.remove('Target')\n\n\nprint(concrete_contin_vars)\n\n# intersting plot\nsns.jointplot(dtrain['escolari'], dtrain[target], kind=\"kde\")\n# with education level target variance increases","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93da4b081f5e599ed06d8a4a4e43639f14b5177b"},"cell_type":"code","source":"'edjefa'  in train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9527cb76f8084cef877485b1b30d9c9d8f36b71"},"cell_type":"code","source":"''' general variables '''\n\nimport matplotlib.pyplot as plt\n# aggregate target by household\ngra=train.loc[train['stage']=='train'].groupby('idhogar')[gen+[target]].mean()\ngra[target]=np.round(gra[target])\n\n# deneral dummy vars\na=pd.DataFrame([i[:3] for i in gen])\na=a[0].value_counts()\nl=[]\nfor i in a.index:\n  g=[j for j in gen if i in j[:3]]\n  l.append(g)\nl0=[i for i in l if len(i)>1]\nl1=[i[0] for i in l if len(i)==1]\nb0=['public','planpri','noelec','coopele'] # not intersting\nl1=[i for i in l1 if i not in b0]\nl0.append(b0)\nl0=[i for i in l0 if set(i) != set(['v18q1','v18q'])] # these we will watch separately\n\n\ngeneral_categ_vars=[l0[i] for i in [1,5,10]]\n\n\nprint(general_categ_vars)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5766706348693569349dbf15ac5c3c9594951e4d"},"cell_type":"code","source":"\n# drop edu vars (see top :) )\ntrain=train.drop(['edjefa','edjefe'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f6db47266dc7f41eeda9cc682138ef8b477d74b"},"cell_type":"code","source":"# general dummy variables\nfor v in l0:\n\n    gra1=gra.copy()\n    gra1['var']=gra1[v].idxmax(axis=1)\n\n    plt.figure()\n    sns.countplot(x='var',  data=gra1)\n    plt.xticks(rotation=50)\n    plt.show()\n\n    plt.figure()\n    sns.factorplot(x='var',y=target, data=gra1, kind=\"bar\")\n    plt.xticks(rotation=50)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f37c7f9d42b7655c39eec717574e6a2fe9aa13a"},"cell_type":"code","source":"''' general not dummy variables '''\nfrom scipy import stats\n\nsns.jointplot(gra['v2a1'], gra['meaneduc'], kind=\"kde\")\nsns.jointplot(gra['overcrowding'], gra['meaneduc'], kind=\"kde\")\nsns.jointplot(gra['v2a1'], gra['qmobilephone'], kind=\"kde\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1176aa91f44e3700f03a69fd8c1ef8cbd9ecefea"},"cell_type":"code","source":"# watch dependency to target relation\nfig, ax =plt.subplots(1,2)\nsns.violinplot(x=target,y='dependency', data=gra, ax=ax[0])\nsns.factorplot(x=target,y='dependency', data=gra, kind=\"bar\", palette=\"muted\", ax=ax[1])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34c02d70e6bb396a6889f072f59ea8f97eb25957"},"cell_type":"code","source":"# smoother normal distribution\nplt.figure()\nstats.probplot(gra['bedrooms'], dist='norm', fit=True, plot=plt.subplot(111))\n\nbed_t=gra['bedrooms']/gra['rooms']\nplt.figure()\nsns.distplot(bed_t)\nplt.figure()\nstats.probplot(bed_t, dist='norm', fit=True, plot=plt.subplot(111))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"412ff93260bdf227eb87cec31f3b250b83bb45c3"},"cell_type":"code","source":"# pair plot of continious variables\npaircol=['v2a1', 'rooms', 'v18q', 'v18q1', 'tamhog',\n 'tamviv', 'rez_esc', 'hhsize', 'hogar_adul',\n 'hogar_mayor', 'hogar_total', 'dependency', 'bedrooms', 'overcrowding', 'qmobilephone', 'area2', ]\nsns.pairplot(gra[paircol].dropna(how='any'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df47fe828daacd2132848503a1a4478059d86a4c"},"cell_type":"code","source":"# find some more correlated variables\n\nprint(gra[['tamhog','hhsize']].corr() )\nprint('We will choose only on of them- hhsize seems more representable')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"910ef703261406485bbee6eafd6d4ed9d91a4e32"},"cell_type":"code","source":"# plot frequency of cross variables *(age - hogar_total)\nsns.heatmap(pd.crosstab(train['age'], train['hogar_total']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"743d87422760a24581bbc9e2450f11226bdbd232"},"cell_type":"code","source":"# plot frequency of cross variables *(v18q - area2)\nsns.heatmap(pd.crosstab(train['v18q'], train['area2']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4f911d6a09d279f11e982860236e74cf29d0114"},"cell_type":"code","source":"''' 4.many missing rows '''\n\ncon=train.loc[train['stage']=='train',:]\n\nrowna=con.T.apply(lambda x: sum(x.isnull().values), axis = 0)\nrowna=rowna/train.shape[1]\n\nsns.distplot(rowna)\na=0.4 # cutoff to drop a row\nprint(sum(rowna<a)/train.shape[0])\n\nprp=set(list(rowna[rowna>a].index))\nprp=list(set(con.index)-prp)\n\ntrain=pd.concat([con.loc[prp ,:],train.loc[train['stage']=='test',:]],axis=0)\nprint('done')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40abdec4aa2bb3e075115d6f976f7455098b3fae"},"cell_type":"code","source":"''' 5.manage missings '''\n\nmv={'bin_int': \n            ['sanitario1',  'tipovivi1',  'instlevel3',  'etecho2', 'parentesco1',  'lugar6',  'parentesco10',  'paredblolad',  'parentesco7',  'parentesco9',  'area2',  'parentesco2',  'pisoother',  'parentesco12', 'eviv2',\n          'epared2',  'television',  'energcocinar4',  'epared3',  'lugar4',  'sanitario2',  'energcocinar3',  'paredother',  'tipovivi4',  'pisomoscer',  'instlevel2',  'instlevel1',  'abastaguadentro',  'paredmad',  'pisomadera',  'techocane',  'energcocinar2',  'v14a',  'dis',  'elimbasu1',  'instlevel9',\n          'lugar1',  'pisocemento',  'parentesco3',  'paredfibras',  'instlevel8',  'paredzinc',  'sanitario3',  'epared1',  'estadocivil7',  'tipovivi5',  'techozinc',  'area1',  'planpri',  'paredzocalo',  'parentesco11',  'lugar2',  'sanitario6',  'etecho3',  'estadocivil1',  'pisonotiene',  'estadocivil4',  'techoentrepiso',  'mobilephone',\n          'instlevel6',  'hacdor',  'hacapo',  'lugar3',  'tipovivi3',  'coopele',  'paredpreb',  'estadocivil2',  'parentesco4',  'eviv3',  'cielorazo',  'techootro',  'eviv1',  'etecho1',  'v18q',  'estadocivil5',  'instlevel5',  'elimbasu4',  'male',  'abastaguafuera',  'abastaguano',  'tipovivi2',  'computer',  'estadocivil6',  'pisonatur',  'parentesco5',  'instlevel4',\n          'instlevel7',  'parentesco6',  'noelec',  'estadocivil3',  'female',  'energcocinar1',  'elimbasu6',  'refrig',  'lugar5',  'pareddes',  'parentesco8',  'elimbasu5',  'sanitario5',  'public',  'elimbasu3',  'elimbasu2'],\n    'categ': \n            ['idhogar'],\n    'cont': \n            ['r4h3',\n          'age',\n          'SQBescolari',\n          'overcrowding',\n          'r4m1',  'SQBedjefe',  'v2a1',  'r4t1', 'SQBhogar_total',  'r4h2',  'meaneduc',  'SQBmeaned',  'rez_esc', 'agesq',  'hogar_total',  'v18q1',  'hhsize',  'qmobilephone',  'rooms',  'hogar_adul',\n          'dependency',  'hogar_nin',  'r4t2', 'escolari',  'SQBage',  'r4t3', 'r4m3',  'r4m2',  'tamviv',  'SQBhogar_nin',  'tamhog', 'hogar_mayor',  'SQBovercrowding',  'SQBdependency',  'bedrooms', 'r4h1']\n }\n\n\n# watch continious variables\nfrom sklearn.impute import SimpleImputer\n\ncontin_vars=train[mv['cont']]\n\n\n\ndef miss_map(dx,k=0.5):\n    missingValueColumns = dx.columns[dx.isnull().sum(axis=0)/dx.shape[0]>k].tolist()\n    return missingValueColumns\nmanymiss=miss_map(con)\nprint(manymiss) \nprint(' ')\n# 'v2a1' we consider an important varible- so we would preserve it\n\nmanymiss=['rez_esc', 'v18q1']\n# give less then 30% observations of data\nmv['cont']=list(set(mv['cont'])-set(manymiss))\nmy_imputer = SimpleImputer( strategy='mean') #most_frequent\ncontin_vars = my_imputer.fit_transform(contin_vars[mv['cont']])\ncontin_vars=pd.DataFrame(contin_vars,columns=mv['cont'])\ncontin_vars.index=train.index\n\n\n\n# categorical variables are good\ncateg_vars=train[mv['categ']+mv['bin_int']]\n\n\nd=pd.merge(contin_vars,categ_vars,left_index=True, right_index=True)\nprint('done')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcdc8416a0f9c6c1b853abbbf037ff8ea89f691c"},"cell_type":"code","source":"''' 6.Flattern id by idhogar within (house mates form rows to columns) '''\n\n# As we watch clusters as a whole we want to determine most useful household mates( choose them )\n# these are 'parentesco_' columns\n\n\nfrom sklearn import cluster\n\n\nparen_group=d.groupby('idhogar')[general_categ_vars[0]].sum()\nfor i in paren_group.columns:\n    paren_group.loc[paren_group[i]>1,i]=1\n\nn=10\nagg=cluster.AgglomerativeClustering( n_clusters=n, linkage='average')\nagg.fit(paren_group)\nparen_group['l']=agg.labels_\n# draw heat map by count of mates in same house\nparen_heat1=paren_group.groupby('l')[general_categ_vars[0]].sum()\nsns.heatmap(paren_heat1)\nprint('We see that mates 1,2,3 are most met - 8000 cases')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e1d060aff119d0becdcdae48f4fa7c7533ce919"},"cell_type":"code","source":"\n# drop parentesco1,2,3 to see most met cases of other mates\nvv=['parentesco7',\n 'parentesco10',\n 'parentesco6',\n 'parentesco11',\n 'parentesco9',\n 'parentesco4',\n 'parentesco5',\n 'parentesco12',\n 'parentesco8']\nparen_group1=d.groupby('idhogar')[vv].sum()\nfor i in paren_group1.columns:\n    paren_group1.loc[paren_group1[i]>1,i]=1\n# build clustering\nn=60\nagg=cluster.AgglomerativeClustering( n_clusters=n, linkage='average')\nagg.fit(paren_group1)\nparen_group1['l']=agg.labels_\n# draw heat map by count of mates in same house\nparen_heat2=paren_group1.groupby('l')[vv].sum()\nsns.heatmap(paren_heat2)\nprint('We see that mates not 1,2,3 are only 300 cases - not much')\n\nd['par']=d[concrete_dum_vars[0]].idxmax(axis=1)\nd['par']=d['par'].map(lambda x: int(x.replace('parentesco','') ))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0c63bf619ae1cf18939704b441617b201631d96"},"cell_type":"code","source":"v=d.copy()\n\nv[target]=train[target]\nv['stage']=train['stage']\nv=v.loc[v['stage']=='train',:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d123daa2a986be9f8d1db47d937f2cba3ebb2358"},"cell_type":"code","source":"# check  how mates1,2,3 are though representative\n\ng=v.groupby('idhogar')[target].agg({target:'mean'})\nv1=v.loc[v.par.isin([1,2,3])]\ng1=v1.groupby('idhogar')[target].agg({'target2':'mean'})\ng=pd.merge(g,g1,right_index=True,left_index=True)\ng['d']=g.Target-g.target2\ng=g.loc[g.d!=0]\nprint(g.head(3))\nprint('There are only {0} outlier households which dont follow my assumptions'.format(g.shape[0]) )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"894c51cc57deb8b1dd27dbd6370a72b78e3616c1"},"cell_type":"code","source":"# one more check of mates to concrete variables dependencies\n\n# plot graphs of common variables to target\n\n# get columns to label encoding\nv['instlevel']=v[concrete_dum_vars[1]].idxmax(axis=1)\nv['instlevel']=v['instlevel'].map(lambda x: int(x.replace('instlevel','') ))\nv['estadocivil']=v[concrete_dum_vars[2]].idxmax(axis=1)\nv['estadocivil']=v['estadocivil'].map(lambda x: int(x.replace('estadocivil','') ))\nv['sex']=v[concrete_dum_vars[5]].idxmax(axis=1)\nv['sex']=v['sex'].map({'male':0, 'female':1})\n\n\nfor h in ['instlevel','sex','estadocivil','instlevel']:\n        \n    plt.figure(figsize=(20,10))\n    sns.violinplot(x='par',y=target,hue=h, data=v)\n    plt.show()\n    plt.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a55b3e6a010f2cb17d9fef573996410309ce7a9"},"cell_type":"code","source":"''' 7.aggregate by idhogar '''\n\n# Finally we aggregate variables by idhogar\n# general have same values for all idhogar members\n# conrete have varying values for each idhogar member\n\nimport itertools\n\nbvc = list(itertools.chain.from_iterable(concrete_dum_vars[1:]))\nprint(bvc)\n# get only important members of household\ntt=d.loc[d.par.isin([1,2,3]),['par','idhogar']+bvc+concrete_contin_vars+['dependency'] ]\n# get all members in one row\nt=pd.pivot_table(tt,values=bvc,index='idhogar', columns='par')\nt.columns =['par'+str(v)+'_'+str(k) for k,v in zip(t.columns.get_level_values(0),\nt.columns.get_level_values(1) )]\nt=t.reset_index()\nt=t.fillna(0)\n\n\nbvg = list(itertools.chain.from_iterable(general_categ_vars))\nvt1=['meaneduc','overcrowding','v2a1','qmobilephone','hhsize','bedrooms' ,'rooms','hogar_total','v18q','area2']    \nt1=d[['idhogar']+bvg+vt1 ].reset_index().drop('Id',axis=1).drop_duplicates()\n\nt2=pd.merge(t,t1,on='idhogar')    \nt2=t2.set_index('idhogar')\n\n\n# Need to mention we are missing idhogar 'ce6154327' as it has only one mate par7 - we will manage it further\n \nvv=train.loc[(train.parentesco1==1)|(train.parentesco2==1)|(train.parentesco3==1)]\nvv=vv.groupby(['idhogar','stage']).agg({target:'mean'}).reset_index('stage')\nvv.Target=np.round(vv.Target)\n\ndd=pd.merge(t2,vv,right_index=True,left_index=True)\n\nprint(dd.head()) \nprint(dd.index.nunique())\n    \n\nprint(dd.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"11868d7a954cfcdd89a5ddba5714ba5352869388"},"cell_type":"code","source":"''' 8.watch outliers '''\n\n\nfrom collections import Counter\n\n\ndef outliers_iqr(df,n):\n    features=df.columns\n    outlier_indices = []\n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col],75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers\n\ndtrain=dd.loc[dd['stage']=='train',:].drop('stage',axis=1)\n\n# watch outliers via method #1\ncon=dtrain.drop(target,axis=1)\nell=EllipticEnvelope().fit(con)\nprr = ell.predict(con) # rows to drop\npp0=sum(prr==1)/len(prr) \nw={}  \n\n# watch outliers via method #2\nfor i in range(2,con.shape[1]//2): #choose number of neighbours to get outliers\n  pr1=outliers_iqr(con,i)\n  pp1=1-len(pr1)/con.shape[0]\n  w[pp1]=pr1\nk=np.array(list(w.keys()))\npp1=min(k[k>0.7])\npr1=w[pp1]\n\n# obtain combined set of rows left after both methods applied\nprp=set(pr1)&set(con.index[prr!=1])\nprp=list(set(dtrain.index)-prp)\npr0=(pp0+pp1)/2\n\n# omit outlier rows\nif 0.7<=pr0:\n  dtrain=dtrain.loc[prp,:]\n  print(dtrain.shape[0]/train.shape[0])\n\ndtrain=dtrain.astype(float)\n\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39d3afe195e0779375b08cd6138e6f8f6c5b4375"},"cell_type":"code","source":"''' 9.get training and testing sets ''' \n\ntrain_m=dtrain\ntest_m=dd.loc[dd['stage']=='test'].drop(target,axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb611d71aa559b81f97ac20dbc33a0d3001ce331"},"cell_type":"code","source":"''' 10.get models and thier parameters '''\n\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import ( AdaBoostClassifier,\n                              GradientBoostingClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\nparam_grid=dict(\n\nnbc={'alpha':[0.1,0.3,0.6,1],\n    'binarize':[True,False],\n    'fit_prior':[True,False] # learn class prior probabilities or not.\n    },\n\nadc = {\n    'n_estimators': list(np.arange(50,251,50)),\n    'learning_rate' : [0.1, 0.01, 0.001],\n    \"algorithm\" : [\"SAMME\",\"SAMME.R\"]\n    },\n    \ngbc = {\n    'n_estimators': [40,120],\n    'max_depth': [3,10],\n    'min_samples_leaf': [0.1, 0.5],\n    'min_samples_split':[0.3, 0.7],\n    'loss' : [\"deviance\"],#,'exponential'\n    'learning_rate': [0.1, 0.01],\n    'max_features': [0.3, 0.7]\n\n    }\n\n)\n\n\ndef param_mods(parameters):\n\n    return  dict(\n            nbc=BernoulliNB(**parameters['nbc']),\n            adc=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),**parameters['adc']),\n            gbc=GradientBoostingClassifier(**parameters['gbc'] ),\n\n            )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2ee3cdd3e2081efbb17db0ebf7d76812cbe8463"},"cell_type":"code","source":"''' 11.train models '''\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import cross_validation\nfrom sklearn.model_selection import StratifiedKFold,KFold,cross_val_score\nfrom sklearn.grid_search import GridSearchCV\n\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,f1_score, make_scorer\n\nfrom sklearn.model_selection import learning_curve\n#from funnn import plot_learning_curve\nimport os\n\n\n\n# prepare data and parameteres\ndfs=train_m\nb=dfs.columns[:-1]\nscaler=StandardScaler().fit(dfs.loc[:,b])\ndfs.loc[:,b]=scaler.transform(dfs.loc[:,b])\n\n\nseed=1\nX=dfs.iloc[:,:-1]\nY=dfs.iloc[:,-1]\nX_val, X_train, Y_val, Y_train = cross_validation.train_test_split(X, Y, test_size=0.7,random_state=seed)\ncol=list(param_grid.keys())\ncol_c=[x for x in col if list(x)[-1]=='c']\ncol_r=[x for x in col if list(x)[-1]=='r']\nmethods = col_r if len(Y.unique()) > 10 else col_c\n\nkfold = StratifiedKFold(n_splits=10)\ncl_param={'scoring' :'f1_macro'}\n\nmodd=['nbc', 'adc', 'gbc']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"398f2cfcabb55fd6dd7ddc75f6aa356d236b4c42"},"cell_type":"code","source":"print(X_train.head(3) )\nprint(X_train.shape)\nprint(Y_train.head(3) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"9e406b509e52dd0a0d5b73a132962daaa065063a"},"cell_type":"code","source":"# grid search\n\nmodels_dict = param_mods(param_grid)\nbest_mods={}\nscor={}\n\nfor i in modd:\n  mod = GridSearchCV(models_dict[i],param_grid = param_grid[i], **cl_param) # n_jobs=1,,cv= 10\n  mod.fit(X_train,Y_train )\n  best_mods[i] = mod.best_estimator_\n  scor[i] = mod.best_score_\n\n\n# cv results search\nmodels_dict =  best_mods if len(scor)>0 else param_mods(param)\ncv_results = {}\nfor i in modd:\n    cv_results[i]=cross_val_score(models_dict[i], X= X_train, y = Y_train,**cl_param,cv= 5) #,  n_jobs=1\ncv_results = {k:np.round(np.mean(v)-3*np.std(v),2) for k,v in cv_results.items()}\n\nprint('done')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c0b911ecd2b335c75d998f2befb302904465060"},"cell_type":"code","source":"print(cv_results)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ca02352888f4313566b9269b6f681aae998fd9e"},"cell_type":"code","source":"# plot training results\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n\n    cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\n\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nmodf=best_mods['nbc']\n\nplt.figure()\nf=plot_learning_curve(modf, 'Learningcurve', X_train, Y_train, ylim=None,  n_jobs=-1)\n\ny_true=Y_val\ny_pred=modf.predict(X_val)\n\nprint(f1_score(y_true, y_pred,average='macro'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"945b84cdbfdb634dab637080ff2e3fbc976f8872"},"cell_type":"code","source":"''' 12.make submission '''\n\n#  predict by idhogar\nX_test=test_m.drop('stage',axis=1)\ndft=StandardScaler().fit_transform(X_test)\nY_test=modf.predict(dft)\nX_test['Target']=Y_test\nX_test=X_test.reset_index()\nX_test=X_test[['idhogar','Target']]\n\nprint(X_test.head())\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83b9202eb15392b10e4e2b0e49bba480b6c014c9"},"cell_type":"code","source":"#  predict by id\ntest=pd.read_csv(path+'test.csv',index_col=0).reset_index()\ntest=test[['idhogar','Id']]\nfin_test=pd.merge(test,X_test,on='idhogar',how='left')\nY_test=fin_test.drop('idhogar',axis=1)\nY_test=Y_test.fillna(1)\nY_test.Target=Y_test.Target.astype(int)\n\n   \n  \n    \n#Y_test=pd.DataFrame({'target':Y_test},index=X_test.index)\nY_test.to_csv('submission.csv', index=False)\n\nprint(Y_test.head(2)) \nprint(Y_test.shape) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e48244e56fee0c81a382a4299e3a5edbede93c67"},"cell_type":"code","source":"\n'''\n# get train data\n\nsub=pd.read_csv(path+'sample_submission.csv')\nprint(sub.head(2))\nsu=pd.merge(sub,Y_test, on = 'Id')\nprint(su.shape)\nprint(sub.shape)\n''''''\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}