{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, BatchNormalization, Conv2D, AveragePooling2D, MaxPooling2D, UpSampling2D\nfrom keras.models import Model, load_model\nfrom keras.initializers import glorot_uniform\nfrom keras.utils import plot_model\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e441578ef946b9ec8bdd8deb060a4ff9b8b684e1"},"cell_type":"code","source":"from skimage.util import montage2d\nfrom skimage.morphology import binary_opening, disk\nfrom skimage.io import imread, imshow\nfrom skimage.segmentation import mark_boundaries\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffd1c5f248242cc0b1c3bae2d4e6740bcd9de6d6"},"cell_type":"code","source":"#create grid of images from rgb images\nmontage_rgb = lambda x: np.stack([montage2d(x[:,:,:,i]) for i in range(x.shape[3])],-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8998896d456010321b55cd4649add751dc7081b6"},"cell_type":"code","source":"SAMPLE_PER_SHIP_COUNT=2000\nTEST_PROP=0.2\nBATCH_SIZE=16\nMAX_TRAIN_EPOCHS=99\n# downsampling in preprocessing\nIMG_SCALING = (3, 3)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d51004d34efe795fbc1925c492d274df40cd6f7"},"cell_type":"markdown","source":"**reading segmentation file having train data bounding boxes**"},{"metadata":{"trusted":true,"_uuid":"30da496aa54f65b07d18562b5a44617e68dc0a9d"},"cell_type":"code","source":"segmentations = pd.read_csv('../input/train_ship_segmentations.csv')\nsegmentations.head()\nprint ('total images in train data: {}'.format(segmentations['ImageId'].drop_duplicates().count()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a58be1b35fd406d225281541277bdff14be46ed"},"cell_type":"markdown","source":"#### adding #ships in each image\n#### sampling from each ship count df (having representation of images having different ship count)"},{"metadata":{"trusted":true,"_uuid":"489bef5a98dce91258e6d212e8e819b8e87a2d48"},"cell_type":"code","source":"segmentations['has_ship']=segmentations['EncodedPixels'].apply(lambda x: 0 if pd.isnull(x) else 1)\nsegmentations['path']=segmentations['ImageId'].apply(lambda x: '../input/train/'+x)\nships_per_image=segmentations.groupby('ImageId').agg({'has_ship': 'sum'}).reset_index().rename(columns={'has_ship': 'ship_count'})\nships_per_image=ships_per_image.groupby('ship_count').apply(lambda x: x.sample(SAMPLE_PER_SHIP_COUNT) if len(x)> SAMPLE_PER_SHIP_COUNT else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f933ded5e1550a38e1b0a5133e346bfa1a6e2db"},"cell_type":"code","source":"ships_per_image.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"583dfd4c76388a8f24c794f62163e9fd75d8cb4d"},"cell_type":"code","source":"sampled_set=pd.merge(segmentations, ships_per_image)\nsample_segmentations = sampled_set[['ImageId','ship_count']].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a281460c73f70081f81f52e125357f1d6b01661c"},"cell_type":"code","source":"sampled_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"650fdc429c53c18906ea12b36e7f68d49f4c876d"},"cell_type":"code","source":"sampled_set_train_img, sampled_set_valid_img = train_test_split(sample_segmentations,  test_size = TEST_PROP, stratify = sample_segmentations['ship_count'],random_state=42)\nsampled_set_train, sampled_set_valid = pd.merge(sampled_set, sampled_set_train_img), pd.merge(sampled_set, sampled_set_valid_img)\nprint ('training data shape: {}'.format(sampled_set_train.shape))\nprint ('test data shape: {}'.format(sampled_set_valid.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ce2d0153e48c0c8c5c57614930eca5f4488133a"},"cell_type":"markdown","source":"#### encoding image from bounding box and vice versa\n#### decoding mask image from all bounding boxes of a rgb image (incase of multiple ships)"},{"metadata":{"trusted":true,"_uuid":"e71dffc41364c2d67e6766fd2d5ecbb2e91654b4"},"cell_type":"code","source":"def rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef rle_decode(mask_rle, shape=(768, 768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction #array of \n\ndef multi_rle_decode(mask_rle_list):\n    all_masks = np.zeros((768, 768), dtype = np.uint8)\n    for mask in mask_rle_list:\n        if isinstance(mask,str):\n            all_masks |= rle_decode(mask)\n    return all_masks","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bedc6dfb59bf293823b5cc565bcc8c5e492498b2"},"cell_type":"markdown","source":"#### preparing batches of data from training set"},{"metadata":{"trusted":true,"_uuid":"ce0bd72b5b908639891f81c62f2a241ef57e6f62"},"cell_type":"code","source":"def make_image_gen(df, batch_size=BATCH_SIZE):\n    all_list = list(df.groupby('ImageId'))\n    c_img = []\n    c_mask= []\n    while True:\n        np.random.shuffle(all_list)\n        for img_id, img_df in all_list:\n            img_rgb_array = imread(img_df['path'].values[0])\n            img_mask_array = multi_rle_decode(img_df['EncodedPixels'].values).reshape((768, 768, 1))\n            if IMG_SCALING is not None:\n                    img_rgb_array = img_rgb_array[::IMG_SCALING[0], ::IMG_SCALING[1]]\n                    img_mask_array = img_mask_array[::IMG_SCALING[0], ::IMG_SCALING[1]]\n            c_img += [img_rgb_array]\n            c_mask += [img_mask_array]\n            if len(c_img)>= batch_size:\n                yield np.stack(c_img,0) , np.stack(c_mask,0)\n                c_img = []\n                c_mask= []\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd90a5f6be66874dc097fc388b8a119a0dbcaff1"},"cell_type":"code","source":"train_set_batches = make_image_gen(sampled_set_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4042a9cb184c00601e334f3b65466bbd6943944"},"cell_type":"code","source":"train_x, train_y = next(train_set_batches)\nprint ('train x shape:{}'.format(train_x.shape))\nprint ('train y shape:{}'.format(train_y.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cce435a00cb655388ea8abf44653bdc80560a0e7"},"cell_type":"code","source":"train_y.max(), train_y.min(), train_x.max(), train_x.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3afa5060e8226a6a13ae5bd8fd81863ee40e8282"},"cell_type":"code","source":"np.unique(train_y) #all elements in response image are 0/1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf4d837364b7061e78f97978cd70810a62430d9e"},"cell_type":"code","source":"valid_set_batches = make_image_gen(sampled_set_valid, 900)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90048c5e587f2d14ab72dd3c320cfee109f3ffd6"},"cell_type":"code","source":"valid_x, valid_y = next(valid_set_batches)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70b195b80a8e12bc3fb9cf9d73551cf62e43222f"},"cell_type":"code","source":"valid_x, valid_y = valid_x.astype('f')/255.0, valid_y.astype('f')/1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cf732b971ad34bf11d016ec4a6c4e5da9f621d1"},"cell_type":"code","source":"print ('valid x shape:{}'.format(valid_x.shape))\nprint ('valid y shape:{}'.format(valid_y.shape))\nprint('valid_x', valid_x.dtype, valid_x.min(), valid_x.max())\nprint('valid_y', valid_y.dtype, valid_y.min(), valid_y.max())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf682f2d177e20c154c100de4aa6a5282fc5a290"},"cell_type":"markdown","source":"#### preparing augmented data for each of the batches generated above"},{"metadata":{"trusted":true,"_uuid":"a290ff4e9e409705b096e99b8858009080f2ad33"},"cell_type":"code","source":"data_gen_args = dict(rotation_range = 45,\n                      width_shift_range=0.1,\n                      height_shift_range=0.1,\n                      shear_range=0.02,\n                      zoom_range=0.2,\n                      horizontal_flip=True,\n                      vertical_flip=True)\n\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen = ImageDataGenerator(**data_gen_args)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b19d15ae49644c9bf3d72f0d5cc353331870c851"},"cell_type":"code","source":"def create_aug_data(train_set_batches, seed=1):\n    for img, mask in train_set_batches:\n        t_x = image_datagen.flow(img, batch_size=img.shape[0], shuffle=True, seed=seed)\n        t_y = mask_datagen.flow(mask, batch_size=img.shape[0], shuffle=True, seed=seed)\n        yield next(t_x)/255.0, next(t_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b5bd20c41f7358bfa1aab2dd232ce0d40f11ef3"},"cell_type":"code","source":"cur_gen = create_aug_data(train_set_batches)\nt_x, t_y = next(cur_gen)\nprint('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\nprint('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"641f844267ef64aa5d455b54c982326fd3a3af18"},"cell_type":"markdown","source":"### Visualizing one batch of images"},{"metadata":{"trusted":true,"_uuid":"d21f9254474db1e3391775a327cd398c5752dffc"},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(40,20))\nbatch_rgb = montage_rgb(t_x)\nbatch_seg = montage2d(t_y[:, :, :, 0])\nax1.imshow(batch_rgb)\nax1.set_title('Images')\nax2.imshow(batch_seg)\nax2.set_title('Segmentations')\nax3.imshow(mark_boundaries(batch_rgb, batch_seg.astype(int),outline_color=(1,1,1)))\nax3.set_title('Outlined Ships')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65419a0082e8509cef441244b206bf6dfec60f66"},"cell_type":"markdown","source":"### Building model"},{"metadata":{"trusted":true,"_uuid":"97b4bf688f2554b17c6ad8d8c1c61fc9648acfc6"},"cell_type":"code","source":"def autoencoder(input_shape):\n    #encoder\n    input_img = Input(input_shape)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) \n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) \n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) \n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) \n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) \n        \n    #decoder\n    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) \n    up1 = UpSampling2D((2,2))(conv4) \n    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) \n    up2 = UpSampling2D((2,2))(conv5) \n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) \n    \n    model =  Model(inputs = input_img, outputs = decoded, name = 'Ship_model')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dfa2b5b1e14b85d714da446f04c0f1b94ac6743"},"cell_type":"code","source":"model = autoencoder(input_shape = t_x.shape[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81229bd2582d2e9991289d9ed7741ee5191baf2c"},"cell_type":"code","source":"def IoU(y_true, y_pred, eps=1e-6):\n    if np.max(y_true) == 0.0:\n        return IoU(1-y_true, 1-y_pred) ## empty image; calc IoU of zeros\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n    return -K.mean( (intersection + eps) / (union + eps), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8975ed819b9a055e9356c9ad94146d76c71bd32d"},"cell_type":"code","source":"model.compile(optimizer='adam', loss=IoU, metrics=['binary_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4408c30803310855dc9ebbbac0004e39bfaa096b"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0932c447e2c9160d9da051c53da750e15d79e97a"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"model_weights.best.hdf5\"\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True, period=1)\n\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2,\n                      patience=10) # patience is number of epochs with no improvement after which training will be stopped\n\ncallbacks_list = [checkpoint, early]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afacb3258442c92d70a02fbddebeeeca78f46502","scrolled":false},"cell_type":"code","source":"aug_gen = create_aug_data(make_image_gen(sampled_set_train))\nloss_history = [model.fit_generator(aug_gen,\n                                 steps_per_epoch=7,#Total number of steps (batches of samples) to yield from generator before declaring one epoch finished \n                                 epochs=MAX_TRAIN_EPOCHS,\n                                 validation_data=(valid_x, valid_y),\n                                 callbacks=callbacks_list,\n                                workers=1 # the generator is not very thread safe\n                                           )]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b0f11df5775e98c6bf3149ea04cc53c53e7c7be"},"cell_type":"code","source":"IMG_SCALING","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15a5e7632dcee9eb1b26148703b0aaafd01b9ac2"},"cell_type":"code","source":"model.load_weights(weight_path)\nmodel.save('model.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62cc7436d32da9b9a8014e26e701cc468ff39ca4"},"cell_type":"code","source":"epochs = np.concatenate([mh.epoch for mh in loss_history])\nloss = np.concatenate([mh.history['loss'] for mh in loss_history])\nval_loss  = np.concatenate([mh.history['val_loss'] for mh in loss_history])\ntrain_accuracy = np.concatenate([mh.history['binary_accuracy'] for mh in loss_history])\ntest_accuracy = np.concatenate([mh.history['val_binary_accuracy'] for mh in loss_history])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64fecd2971581a21f264dc6704aa7c809bae318c"},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2, figsize = (30,10))\n\nax1.plot(epochs,train_accuracy, epochs,test_accuracy)\nax1.legend(['Training', 'Validation'])\nax1.set_xlabel('epoch')\nax1.set_ylabel('accuracy')\nax1.set_title('accuracy train vs validation')\n\nax2.plot(epochs,loss, epochs,val_loss)\nax2.legend(['Training', 'Validation'])\nax2.set_xlabel('epoch')\nax2.set_ylabel('loss')\nax2.set_title('loss train vs validation')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fd458a9f608a0bdc3f45fea4dacc54c89222f7b"},"cell_type":"code","source":"if IMG_SCALING is not None:\n    X_input = Input(shape=(None, None, 3))\n    X = AveragePooling2D(IMG_SCALING)(X_input)\n    X = model(X)\n    X = UpSampling2D(IMG_SCALING)(X)\n    fullres_model = Model(inputs = X_input, outputs = X)\nelse:\n    fullres_model = model\nfullres_model.save('fullres_model.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42c80c95e0baa1ae8b629adfbf7ea57e1262d661"},"cell_type":"code","source":"fullres_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18ed6fbd70a5acfdd24cff8a02951752955d857e"},"cell_type":"markdown","source":"### preparing validation set"},{"metadata":{"trusted":true,"_uuid":"5ebbb56b2fcd0ab36bf3c74ae029b677d309c1ac"},"cell_type":"code","source":"#selecting batch size for display \nn=16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d837c538bc946b940805bf30be3c2c2dfc55c3d"},"cell_type":"code","source":"valid_img_set = pd.DataFrame(sampled_set_valid['ImageId'].sample(n))\nvalid_set = pd.merge(sampled_set,valid_img_set)\nIMG_SCALING = None\nvalid_set_batches = make_image_gen(valid_set, n)\nv_x, v_y = next(valid_set_batches)\nprint ('valid x shape:{}'.format(v_x.shape))\nprint ('valid y shape:{}'.format(v_y.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da9d8adeedccc20ffdb71bc704567b984c764f81"},"cell_type":"code","source":"v_img = []\nv_mask= []\nfor i in range(n):\n    rgb_img = v_x[i]/255.\n    mask_img = v_y[i]/1.\n    v_img += [rgb_img]\n    v_mask += [mask_img]\nv_img, v_mask = np.stack(v_img,0) , np.stack(v_mask,0)\nprint ('valid x shape:{}'.format(v_img.shape))\nprint ('valid y shape:{}'.format(v_mask.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38d37929f9e874cd97e73d77c74ec37856a518e8"},"cell_type":"code","source":"v_mask_pred = fullres_model.predict(v_img)\nprint ('valid y pred shape:{}'.format(v_mask_pred.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"832efb4eaa475384b1c281e10377c54f047a2c82"},"cell_type":"code","source":"def smooth(cur_seg):\n    return binary_opening(cur_seg>0.999, np.expand_dims(disk(2), -1))\nsmooth_mask = []\nfor i in range(n):\n    smooth_mask += [smooth(v_mask_pred[i])]\nsmooth_mask = np.stack(smooth_mask,0)\nprint ('smooth_mask shape:{}'.format(smooth_mask.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d47a98b048680a4bb6b0e5f4038641b2c0b8543"},"cell_type":"markdown","source":"### plotting validation images, true and pred masking"},{"metadata":{"trusted":true,"_uuid":"d400a57ffed45a4e288e41db089f1922dc309152"},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(40,20))\nbatch_rgb = montage_rgb(v_img)\nbatch_seg = montage2d(v_mask[:, :, :, 0])\nbatch_seg_pred = montage2d(smooth_mask[:, :, :, 0])\nax1.imshow(batch_rgb)\nax1.set_title('Images')\nax2.imshow(batch_seg)\nax2.set_title('actual')\nax3.imshow(batch_seg_pred)\nax3.set_title('pred')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90922d3b33b3403d16311a41025d6d8184083375"},"cell_type":"code","source":"t_y_pred = model.predict(t_x)\nsmooth_mask2 = []\nfor i in range(t_y_pred.shape[0]):\n    smooth_mask2 += [smooth(t_y_pred[i])]\nsmooth_mask2 = np.stack(smooth_mask2,0)\nprint ('smooth_mask2 shape:{}'.format(smooth_mask2.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96bd11cb70eef9160863cd04d90d34eeabd354c4"},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(40,20))\nbatch_rgb = montage_rgb(t_x)\nbatch_seg = montage2d(t_y[:, :, :, 0])\nbatch_seg_pred = montage2d(smooth_mask2[:, :, :, 0])\nax1.imshow(batch_rgb)\nax1.set_title('Images')\nax2.imshow(batch_seg)\nax2.set_title('actual')\nax3.imshow(batch_seg_pred)\nax3.set_title('pred')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7c04b0ad95e0b3a230af40d09375035a0b8088e"},"cell_type":"markdown","source":"### predicting test data"},{"metadata":{"trusted":true,"_uuid":"fc40b28fce99cd62a3c50de34e73fd82fe2a7004"},"cell_type":"code","source":"segmentations_test = pd.read_csv('../input/test_ship_segmentations.csv')\nsegmentations_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8b64e5556a1fd48557efb08605ac1efc3a96807"},"cell_type":"code","source":"#distinct images in test data\nprint ('Images in submission file: {}'.format(segmentations_test['ImageId'].drop_duplicates().count()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1996946280bd762607f42b0b13532f3c1a65da82"},"cell_type":"code","source":"test_df = segmentations_test.groupby('ImageId').apply(lambda x: list(x['EncodedPixels'].values)).reset_index().rename(columns={0: 'encode_list'})\ntest_df['EncodedPixels'] = test_df['encode_list'].apply(lambda x: rle_encode(multi_rle_decode(x)) if rle_encode(multi_rle_decode(x)) != '' else np.nan)\ntest_df['path'] = test_df['ImageId'].apply(lambda x: '../input/test/'+x)\ntest_df = test_df[['ImageId','EncodedPixels','path']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c453aa29bb6dfd85a14f7d8b3d1488015e5f43c"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c0cdbd30d21e4da2e3a648bfb8435d1cde6073f"},"cell_type":"code","source":"#test_df_sample = test_df.sample(frac=0.2).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6873982040eba6f0f5077dfc9de0d64e54e9a1d"},"cell_type":"code","source":"def test_pred_gen(path):\n    img_rgb_array = imread(path)\n    img_rgb_array = img_rgb_array/255.\n    img_rgb_array = np.expand_dims(img_rgb_array,0)\n    c_pred = fullres_model.predict(img_rgb_array)\n    c_ep_pred = rle_encode(smooth(c_pred[0])) if rle_encode(smooth(c_pred[0]))!='' else np.nan\n    return c_ep_pred\n\n#function check\ntest_pred_gen('../input/test/000532683.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67e8cd55ffcbe3f451cb97556663504789077579"},"cell_type":"code","source":"#test_df_sample['encoded_pixel_pred'] = test_df_sample['path'].apply(lambda x: test_pred_gen(x))\ntest_df['encoded_pixel_pred'] = test_df['path'].apply(lambda x: test_pred_gen(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a818087bc8dbcea46b87a772de7693bcdf7b028d"},"cell_type":"code","source":"#test_df_sample.head()\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df40146909cf1fa7d8cecf1e0815c1378973dd4"},"cell_type":"code","source":"submit = test_df[['ImageId','encoded_pixel_pred']].rename(columns={'encoded_pixel_pred': 'EncodedPixels'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d80bd4fb8bfdd8647cd8109b89bedf887dc15b6a"},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"194836afc3ffb9c5d8d37dbcd0a7fc486988a9ae"},"cell_type":"code","source":"submit.to_csv('submission_file.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f80c46043138cf9c5a04023bec99940039635ae"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}