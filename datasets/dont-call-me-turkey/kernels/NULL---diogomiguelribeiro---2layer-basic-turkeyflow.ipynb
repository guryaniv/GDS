{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom sklearn.model_selection import train_test_split\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Load the data in\ndf = pd.read_json('../input/train.json')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16cc6890cc4ec246e94596c33e3169281b1220c5"},"cell_type":"code","source":"#Examine the audio embedding data\ndist_encodings = {}\ndist_samples = {}\nfor enc_array in df['audio_embedding']:\n    if len(enc_array) in dist_samples.keys():\n        dist_samples[len(enc_array)]+=1\n    else:\n        dist_samples[len(enc_array)]=1\n    for enc in enc_array:\n        for v in enc:\n            if v in dist_encodings.keys():\n                dist_encodings[v]+=1\n            else:\n                dist_encodings[v]=1\n\nprint(\"Encodings range: {} to {}\".format(min(dist_encodings.keys()), max(dist_encodings.keys())))\nprint(\"Encodings: {}\".format(dist_encodings))\n\nprint(\"Encoding samples range: {} to {}\".format(min(dist_samples.keys()), max(dist_samples.keys())))\nprint(\"Encoding samples: {}\".format(dist_samples))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bba649de23c9085a2fab57832854aeabd605640"},"cell_type":"code","source":"#Split the data into training and validation\nx_train_data, x_val_data = train_test_split(df,test_size=0.1,train_size=None,random_state=34,shuffle=True)\n\ndef normalise_and_pad(sequence, max_val=255.0, max_seq_len=10):\n    ret = np.pad(np.array(sequence) / max_val, ((0, max_seq_len-len(sequence)),(0,0)), 'wrap')\n    return ret\n\n\ndef create_binary_classifier(binary_array):\n    yvals = np.zeros(shape=(len(binary_array), 2), dtype='float32')\n    for idx, val in enumerate(binary_array):\n        if val == 1:\n            yvals[idx][1] = 1\n        else:\n            yvals[idx][0] = 1\n    return yvals\n    \n    \n\nxtrain = np.asarray([normalise_and_pad(x) for x in x_train_data['audio_embedding']], dtype='float32')\nytrain = create_binary_classifier(x_train_data['is_turkey'].values)\n\n\nxval = np.asarray([normalise_and_pad(x) for x in x_val_data['audio_embedding']], dtype='float32')\nyval = create_binary_classifier(x_val_data['is_turkey'].values)\n\n#Examine types and compare outputs\nprint(\"xtrain: {}; ytrain:{}, xval: {}; yval: {}\".format(xtrain.shape, ytrain.shape, xval.shape, yval.shape))\n\nprint(x_train_data['is_turkey'].values[:10])\nprint(ytrain[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55ad03d9448291420162c310766ef48e1cefc2ed"},"cell_type":"code","source":"def get_batches(x_train, y_train, batch_size):\n    current_index=0\n    while current_index+batch_size < len(x_train):\n        batch_x = x_train[current_index:current_index+batch_size]\n        batch_y = y_train[current_index:current_index+batch_size]\n        yield (batch_x, batch_y)\n        current_index += batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e28ed64c33aab78ebda92bb1f60c3d16450d536"},"cell_type":"code","source":"import tensorflow as tf\nimport time\n\n#Set logging and reset the graph\ntf.reset_default_graph()\n\nsave_file = './model.ckpt'\n\n# Parameters\nlearning_rate = 0.000005\ntraining_epochs = 800\nbatch_size = 128  # Decrease batch size if you don't have enough memory\ndisplay_step = 5\nkeep_prob_val = 0.5\n\nn_input = 10*128  #10*128 audio embeddings\nn_classes = 2  # is not vs is turkey \n\n#Size of the network:\nn_hidden_layer_1 = 512 # layer number of features\nn_hidden_layer_2 = 256 # layer number of features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04d85c879d8f758fddb364612c9e856481c6482e"},"cell_type":"code","source":"with tf.name_scope(\"variables_scope\"):\n    \n    with tf.name_scope(\"input_variables\"):\n        # tf Graph input\n        x = tf.placeholder(\"float32\", [None, 10, 128], name=\"input_x\")\n        y = tf.placeholder(\"float32\", [None, n_classes], name=\"targets\")\n        keep_prob = tf.placeholder(tf.float32) # probability to keep units\n\n        x_flat = tf.reshape(x, [-1, n_input], name=\"input_x_flat\")\n    \n    \n    with tf.name_scope(\"weights_scope\"):\n        # Store layers weight & bias\n        weights = {\n            'hidden_layer_1': tf.Variable(tf.random_normal([n_input, n_hidden_layer_1]), name=\"w_hidden_1\"),\n            'hidden_layer_2': tf.Variable(tf.random_normal([n_hidden_layer_1, n_hidden_layer_2]), name=\"w_hidden_2\"),\n            'out': tf.Variable(tf.random_normal([n_hidden_layer_2, n_classes]), name=\"w_out\")\n        }\n        \n        biases = {\n            'hidden_layer_1': tf.Variable(tf.random_normal([n_hidden_layer_1])),\n            'hidden_layer_2': tf.Variable(tf.random_normal([n_hidden_layer_2])),\n            'out': tf.Variable(tf.random_normal([n_classes]))\n        }\n    \n    \n    with tf.name_scope(\"network_scope\"):\n        # Hidden layer with RELU activation\n        layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer_1']),biases['hidden_layer_1'])\n        layer_1 = tf.nn.relu(layer_1)\n        layer_1 = tf.nn.dropout(layer_1, keep_prob)\n\n        layer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer_2']),biases['hidden_layer_2'])\n        layer_2 = tf.nn.relu(layer_2)\n        layer_2 = tf.nn.dropout(layer_2, keep_prob)\n\n        # Output layer with linear activation\n        logits = tf.add(tf.matmul(layer_2, weights['out']), biases['out'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fccb30611e6fc526169124948154f3461a79a44f"},"cell_type":"code","source":"with tf.name_scope(\"training_scope\"):\n    # Define loss and optimizer\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y), name='cost')\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost, name='gradDescent')\n\n\n# Calculate accuracy\nwith tf.name_scope(\"accuracy_scope\"):\n    argmax_logits = tf.argmax(logits, 1)\n    argmax_y = tf.argmax(y, 1)\n    correct_prediction = tf.equal(argmax_logits, argmax_y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5fab041defa3ea671ebea18d2a21dad0c0c10da"},"cell_type":"code","source":"# Initializing the variables\ninit = tf.global_variables_initializer()\n\nsaver = tf.train.Saver()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    # Training cycle\n    for epoch in range(training_epochs):\n        # Loop over all batches\n        for batch_x, batch_y in get_batches(xtrain, ytrain, batch_size):\n            # Run optimization op (backprop) and cost op (to get loss value)\n            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: keep_prob_val})\n        \n        # Print status for every 10 epochs        \n        if epoch % display_step == 0:\n            valid_accuracy = sess.run(\n                accuracy,\n                feed_dict={\n                    x: xval,\n                    y: yval,\n                    keep_prob: 1.0})\n            print('Epoch {:<3} - Validation Accuracy: {}'.format(\n                epoch,\n                valid_accuracy))\n            \n    saver.save(sess, save_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"199501ecc7639a1b3dbb9f2ca01c0c66c9c7f672"},"cell_type":"code","source":"#Load the test data in\ndf_test = pd.read_json('../input/test.json')\nprint(len(df_test['vid_id']))\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"914c724c0249ef0d2d607d2b8cae890535be2247"},"cell_type":"code","source":"#Process the data to be ready to feed the model\nxsubmission = np.asarray([normalise_and_pad(x) for x in df_test['audio_embedding']], dtype='float32')\n\n#Examine types and compare outputs\nprint(\"xsubmission: {}\".format(xsubmission.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81113f30dd6ce151eb5732ea3ef11e188d2552a8"},"cell_type":"code","source":"with tf.Session() as sess:\n    saver.restore(sess, save_file)\n    argmax_output = sess.run(\n                argmax_logits,\n                feed_dict={\n                    x: xsubmission,\n                    y: yval,\n                    keep_prob: 1.0})\n\n\nsubmit_df = pd.DataFrame(columns=['vid_id', 'is_turkey'])\nsubmit_df['vid_id'] = df_test['vid_id']\nsubmit_df['is_turkey'] = list(argmax_output)\nprint(\"Dataframe size: {}\".format(len(submit_df['vid_id'])))\nsubmit_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7fd434868c9e741eeedd465ffb589365233f345"},"cell_type":"code","source":"submit_df.to_csv('../submission_turkey.csv',index=None,columns=['vid_id','is_turkey'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}