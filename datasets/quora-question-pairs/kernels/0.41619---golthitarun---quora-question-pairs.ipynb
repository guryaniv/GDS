{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b66a89eb-5ec4-096e-70ea-d0565f5c5179"
      },
      "outputs": [],
      "source": [
        "#authors:\n",
        "#Shiva Ganga\n",
        "#Tharunn Golthi\n",
        "#Abhinaya\n",
        "#Susmitha\n",
        "#importing libraries numpy,pandas,mathplotlib for extracting, modifying and visualizing the data\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7f6b7179-60b6-be20-abdc-ee8d72d877a9"
      },
      "outputs": [],
      "source": [
        "#loading input train file to train\n",
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "#loading iput test file to test\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "#printing the top\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "72f8d60e-82ec-b011-f49d-c09b2949a651"
      },
      "source": [
        "This is how the training data is given. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8be97387-1777-4016-1f8e-02687c72a3b5"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "344a6e0d-f644-f3df-d41d-bf520e4e5177"
      },
      "source": [
        "The test data only contains questions but not their id's as in train data, as you can see above. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23c67ba6-4073-f62f-fa16-20f8bcda8e16"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9852875c-3834-3555-6a90-9c065071032f"
      },
      "source": [
        "The training data has 404290 instances. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "806648cb-0bb3-48a4-9a24-1781dbfad8f0"
      },
      "outputs": [],
      "source": [
        "test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1de3fb26-b036-7be9-f1b6-1320599ac315"
      },
      "source": [
        "The test data has 2345796 instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f7e95e37-e913-fa85-9b43-e1b1a748793f"
      },
      "outputs": [],
      "source": [
        "train_duplicate_mean = train['is_duplicate'].mean()\n",
        "print (\"mean of train data is_duplicate column\",train_duplicate_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b5c20797-ea81-113c-8f35-7b257f8913d5"
      },
      "source": [
        "By finding the mean on the is_duplicate field of train data, we see that about 37% of the train data have pair of questions, which are labeled is_duplicate as 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0eac5c39-f63f-1a5a-32a1-393f3ac8c41d"
      },
      "outputs": [],
      "source": [
        "pt = train.groupby('is_duplicate')['id'].count()\n",
        "pt.plot.bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "da2b8734-2f4e-34aa-105c-c0666c945ea6"
      },
      "source": [
        "The plot shows the is_duplicate distribution in the train data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0388ecf7-b1fb-a2ba-579d-446156ca0a56"
      },
      "outputs": [],
      "source": [
        "# plotting data for number of questions vs number of occurences of the question \n",
        "question_id_1 = train['qid1'].tolist()\n",
        "question_id_2 = train['qid2'].tolist()\n",
        "\n",
        "question_id = pd.Series(question_id_1+question_id_2)\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.hist(question_id.value_counts(), bins= 30)\n",
        "plt.yscale('log', nonposy='clip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f3e7f0f4-ddd7-c668-db98-56667cd121ce"
      },
      "source": [
        "By plotting the no. of questions vs no. of occurences of the question, we observe that most of the questions only appear a few times, except very few. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "74224b66-0638-4beb-fa91-ceaedabf6e53"
      },
      "outputs": [],
      "source": [
        "#using nltk corpus for stopwords\n",
        "from nltk.corpus import stopwords as st\n",
        "#stopwords\n",
        "stopwords_set = set(st.words(\"english\"))\n",
        "\n",
        "#returns total words in a sentence\n",
        "def word_dict(sentence):\n",
        "    question_words_dict = {}\n",
        "    for word in sentence.lower().split():\n",
        "        if word not in stopwords_set:\n",
        "            question_words_dict[word] = 1\n",
        "    return question_words_dict\n",
        "#calculating feature common_word_percentage for each row\n",
        "def common_words_percentage(entry):\n",
        "    question_1_words = word_dict(str(entry['question1']))\n",
        "    question_2_words = word_dict(str(entry['question2']))\n",
        "     \n",
        "    if len(question_1_words) == 0 or len(question_2_words) == 0:\n",
        "        return 0\n",
        "    shared_in_q1 = [word for word in question_1_words.keys() if word in question_2_words]\n",
        "    feature_Ratio = ( 2*len(shared_in_q1) )/(len(question_1_words)+len(question_2_words))\n",
        "    return feature_Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7822518-3d6d-61a5-c4d1-48a0568774a2"
      },
      "outputs": [],
      "source": [
        "#calculating tfidf weights \n",
        "def tfidf_weights(entry):\n",
        "    question_1_words = word_dict(str(entry['question1']))\n",
        "    question_2_words = word_dict(str(entry['question2']))\n",
        "    if len(question_1_words) == 0 or len(question_2_words) == 0:\n",
        "        return 0\n",
        "    \n",
        "    common_wts_1 = [weights.get(w, 0) for w in question_1_words.keys() if w in question_2_words]  \n",
        "    common_wts_2 = [weights.get(w, 0) for w in question_2_words.keys() if w in question_2_words]\n",
        "    common_wts = common_wts_1 + common_wts_2\n",
        "    whole_wts = [weights.get(w, 0) for w in question_1_words] + [weights.get(w, 0) for w in question_2_words]\n",
        "    \n",
        "    feature_tfidf = np.sum(common_wts) / np.sum(whole_wts)\n",
        "    return feature_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf91ac4b-7057-d853-a528-cab8288e58a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "list_of_questions = (train['question1'].str.lower().astype('U').tolist() + train['question2'].str.lower().astype('U').tolist())\n",
        "#calcutaing Tfifs feature using inbuilt libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df = 50,max_features = 3000000,ngram_range = (1,10))\n",
        "X = vectorizer.fit_transform(list_of_questions)\n",
        "idf = vectorizer.idf_\n",
        "weights = (dict(zip(vectorizer.get_feature_names(), idf)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "47131045-01a0-c6d4-fbd6-0d7bb043cbc4"
      },
      "outputs": [],
      "source": [
        "#feature train data frame\n",
        "X_TrainData = pd.DataFrame()\n",
        "#feature test data frame\n",
        "X_TestData = pd.DataFrame()\n",
        "# adding common_word_percent feature to train data\n",
        "X_TrainData['common_word_percent'] = train.apply(common_words_percentage, axis=1, raw=True)\n",
        "# adding feature_ifidf feature to train data\n",
        "X_TrainData['feature_ifidf'] = train.apply(tfidf_weights, axis = 1, raw = True)\n",
        "Y_TrainData = train['is_duplicate'].values\n",
        "# adding common_word_percent feature to test data\n",
        "X_TestData['common_word_percent'] = test.apply(common_words_percentage, axis = 1, raw = True)\n",
        "# adding feature_ifidf feature to test data\n",
        "X_TestData['feature_ifidf'] = test.apply(tfidf_weights, axis = 1, raw = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e583c5e8-fc23-aa11-aeb2-7298a16ea8cf"
      },
      "outputs": [],
      "source": [
        "# calculating jacardian similarity\n",
        "import nltk\n",
        "def jaccard_similarity_coefficient(row):\n",
        "    if (type(row['question1']) is str) and (type(row['question2']) is str):\n",
        "        words_1 = row['question1'].lower().split()\n",
        "        words_2 = row['question2'].lower().split()\n",
        "    else:\n",
        "        #tokeninzing using nltk\n",
        "        words_1 = nltk.word_tokenize(str(row['question1']))\n",
        "        words_2 = nltk.word_tokenize(str(row['question2']))\n",
        "   \n",
        "    joint_words = set(words_1).union(set(words_2))\n",
        "    intersection_words = set(words_1).intersection(set(words_2))\n",
        "    return len(intersection_words)/len(joint_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fd1c25f0-7c2f-3915-e556-329d7afcbc2b"
      },
      "outputs": [],
      "source": [
        "# removing NA values in tarainig data\n",
        "train = train.fillna(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "029631d4-f834-78b9-330d-d226315a2425"
      },
      "outputs": [],
      "source": [
        "# adding jaccard distance feature to train and test data \n",
        "X_TrainData['Jacard_Distance'] = train.apply(jaccard_similarity_coefficient, axis = 1, raw = True)\n",
        "X_TestData['Jacard_Distance'] = test.apply(jaccard_similarity_coefficient, axis = 1, raw = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "24a5bdcd-4819-8e07-0de3-633e18bef49e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
        "import re, math\n",
        "from collections import Counter\n",
        "\n",
        "WORD = re.compile(r'\\w+')\n",
        "# calculating the cosine similarity between two vectors\n",
        "def _cosine_similarity(vector_1, vector_2):\n",
        "    \n",
        "    common_keys = set(vector_1.keys()) & set(vector_2.keys())\n",
        "    array1 = [vector_1[x]**2 for x in vector_1.keys()]\n",
        "    array2 = [vector_2[x]**2 for x in vector_2.keys()]\n",
        "    \n",
        "    if not (math.sqrt(sum(array1)) * math.sqrt(sum(array2))):\n",
        "        return 0.0\n",
        "    else:\n",
        "        return (float(sum([vector_1[x] * vector_2[x] for x in common_keys]))) / (math.sqrt(sum(array1)) * math.sqrt(sum(array2)))\n",
        "# making sentence to vector format\n",
        "def sentence_transform(sentence):\n",
        "     words = WORD.findall(sentence)\n",
        "     return Counter(words)\n",
        "#method used to find cosine similarity for each row of data frame\n",
        "def cosine_sim(row):\n",
        "    vector1 = sentence_transform(str(row['question1']))\n",
        "    vector2 = sentence_transform(str(row['question2']))\n",
        "    sim = _cosine_similarity(vector1,vector2)\n",
        "    return sim\n",
        "\n",
        "X_TrainData['cosine_sim'] = train.apply(cosine_sim,axis = 1,raw = True )\n",
        "X_TestData['cosine_sim'] = test.apply(cosine_sim,axis = 1,raw = True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15a36c7a-8037-ddaa-2f47-7174da80c64f"
      },
      "outputs": [],
      "source": [
        "import csv, math, random , sys, random\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import brown\n",
        "import math\n",
        "import nltk\n",
        "import sys\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import re\n",
        "from pandas import read_csv\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import brown\n",
        "import math\n",
        "import nltk\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "##################\n",
        "ALPHA = 0.2\n",
        "BETA = 0.45\n",
        "ETA = 0.4\n",
        "PHI = 0.2\n",
        "DELTA = 0.85\n",
        "\n",
        "brown_freqs = dict()\n",
        "N = 0\n",
        "\n",
        "\n",
        "######################### word similarity ##########################\n",
        "def get_best_synset_pair(word_1, word_2):\n",
        "    \"\"\" \n",
        "    Choose the pair with highest path similarity among all pairs. \n",
        "    Mimics pattern-seeking behavior of humans.\n",
        "    \"\"\"\n",
        "    max_sim = -1.0\n",
        "    synsets_1 = wn.synsets(word_1)\n",
        "    synsets_2 = wn.synsets(word_2)\n",
        "    if len(synsets_1) == 0 or len(synsets_2) == 0:\n",
        "        return None, None\n",
        "    else:\n",
        "        max_sim = -1.0\n",
        "        best_pair = None, None\n",
        "        for synset_1 in synsets_1:\n",
        "            for synset_2 in synsets_2:\n",
        "                sim = wn.path_similarity(synset_1, synset_2)\n",
        "                if sim != None and sim > max_sim:\n",
        "                    max_sim = sim\n",
        "                    best_pair = synset_1, synset_2\n",
        "        return best_pair\n",
        "\n",
        "\n",
        "def length_dist(synset_1, synset_2):\n",
        "    \"\"\"\n",
        "    Return a measure of the length of the shortest path in the semantic \n",
        "    ontology (Wordnet in our case as well as the paper's) between two \n",
        "    synsets.\n",
        "    \"\"\"\n",
        "    l_dist = sys.maxsize\n",
        "    if synset_1 is None or synset_2 is None:\n",
        "        return 0.0\n",
        "    if synset_1 == synset_2:\n",
        "        # if synset_1 and synset_2 are the same synset return 0\n",
        "        l_dist = 0.0\n",
        "    else:\n",
        "        wset_1 = set([str(x.name()) for x in synset_1.lemmas()])\n",
        "        wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n",
        "        if len(wset_1.intersection(wset_2)) > 0:\n",
        "            # if synset_1 != synset_2 but there is word overlap, return 1.0\n",
        "            l_dist = 1.0\n",
        "        else:\n",
        "            # just compute the shortest path between the two\n",
        "            l_dist = synset_1.shortest_path_distance(synset_2)\n",
        "            if l_dist is None:\n",
        "                l_dist = 0.0\n",
        "    # normalize path length to the range [0,1]\n",
        "    return math.exp(-ALPHA * l_dist)\n",
        "\n",
        "\n",
        "def hierarchy_dist(synset_1, synset_2):\n",
        "    \"\"\"\n",
        "    Return a measure of depth in the ontology to model the fact that \n",
        "    nodes closer to the root are broader and have less semantic similarity\n",
        "    than nodes further away from the root.\n",
        "    \"\"\"\n",
        "    h_dist = sys.maxsize\n",
        "    if synset_1 is None or synset_2 is None:\n",
        "        return h_dist\n",
        "    if synset_1 == synset_2:\n",
        "        # return the depth of one of synset_1 or synset_2\n",
        "        h_dist = max([x[1] for x in synset_1.hypernym_distances()])\n",
        "    else:\n",
        "        # find the max depth of least common subsumer\n",
        "        hypernyms_1 = {x[0]: x[1] for x in synset_1.hypernym_distances()}\n",
        "        hypernyms_2 = {x[0]: x[1] for x in synset_2.hypernym_distances()}\n",
        "        lcs_candidates = set(hypernyms_1.keys()).intersection(\n",
        "            set(hypernyms_2.keys()))\n",
        "        if len(lcs_candidates) > 0:\n",
        "            lcs_dists = []\n",
        "            for lcs_candidate in lcs_candidates:\n",
        "                lcs_d1 = 0\n",
        "                if lcs_candidate in hypernyms_1:\n",
        "                    lcs_d1 = hypernyms_1[lcs_candidate]\n",
        "                lcs_d2 = 0\n",
        "                if lcs_candidate in hypernyms_2:\n",
        "                    lcs_d2 = hypernyms_2[lcs_candidate]\n",
        "                lcs_dists.append(max([lcs_d1, lcs_d2]))\n",
        "            h_dist = max(lcs_dists)\n",
        "        else:\n",
        "            h_dist = 0\n",
        "    return ((math.exp(BETA * h_dist) - math.exp(-BETA * h_dist)) /\n",
        "            (math.exp(BETA * h_dist) + math.exp(-BETA * h_dist)))\n",
        "\n",
        "\n",
        "def word_similarity(word_1, word_2):\n",
        "    synset_pair = get_best_synset_pair(word_1, word_2)\n",
        "    return (length_dist(synset_pair[0], synset_pair[1]) *\n",
        "            hierarchy_dist(synset_pair[0], synset_pair[1]))\n",
        "\n",
        "\n",
        "######################### sentence similarity ##########################\n",
        "\n",
        "def most_similar_word(word, word_set):\n",
        "    \"\"\"\n",
        "    Find the word in the joint word set that is most similar to the word\n",
        "    passed in. We use the algorithm above to compute word similarity between\n",
        "    the word and each word in the joint word set, and return the most similar\n",
        "    word and the actual similarity value.\n",
        "    \"\"\"\n",
        "    max_sim = -1.0\n",
        "    sim_word = \"\"\n",
        "    for ref_word in word_set:\n",
        "        sim = word_similarity(word, ref_word)\n",
        "        if sim > max_sim:\n",
        "            max_sim = sim\n",
        "            sim_word = ref_word\n",
        "    return sim_word, max_sim\n",
        "\n",
        "\n",
        "def info_content(lookup_word):\n",
        "    \"\"\"\n",
        "    Uses the Brown corpus available in NLTK to calculate a Laplace\n",
        "    smoothed frequency distribution of words, then uses this information\n",
        "    to compute the information content of the lookup_word.\n",
        "    \"\"\"\n",
        "    global N\n",
        "    if N == 0:\n",
        "        # poor man's lazy evaluation\n",
        "        for sent in brown.sents():\n",
        "            for word in sent:\n",
        "                word = word.lower()\n",
        "                if word not in brown_freqs:\n",
        "                    brown_freqs[word] = 0\n",
        "                brown_freqs[word] = brown_freqs[word] + 1\n",
        "                N = N + 1\n",
        "    lookup_word = lookup_word.lower()\n",
        "    n = 0 if lookup_word not in brown_freqs else brown_freqs[lookup_word]\n",
        "    return 1.0 - (math.log(n + 1) / math.log(N + 1))\n",
        "\n",
        "\n",
        "def semantic_vector(words, joint_words, info_content_norm):\n",
        "    \"\"\"\n",
        "    Computes the semantic vector of a sentence. The sentence is passed in as\n",
        "    a collection of words. The size of the semantic vector is the same as the\n",
        "    size of the joint word set. The elements are 1 if a word in the sentence\n",
        "    already exists in the joint word set, or the similarity of the word to the\n",
        "    most similar word in the joint word set if it doesn't. Both values are \n",
        "    further normalized by the word's (and similar word's) information content\n",
        "    if info_content_norm is True.\n",
        "    \"\"\"\n",
        "    sent_set = set(words)\n",
        "    semvec = np.zeros(len(joint_words))\n",
        "    i = 0\n",
        "    for joint_word in joint_words:\n",
        "        if joint_word in sent_set:\n",
        "            # if word in union exists in the sentence, s(i) = 1 (unnormalized)\n",
        "            semvec[i] = 1.0\n",
        "            if info_content_norm:\n",
        "                semvec[i] = semvec[i] * math.pow(info_content(joint_word), 2)\n",
        "        else:\n",
        "            # find the most similar word in the joint set and set the sim value\n",
        "            sim_word, max_sim = most_similar_word(joint_word, sent_set)\n",
        "            semvec[i] = PHI if max_sim > PHI else 0.0\n",
        "            if info_content_norm:\n",
        "                semvec[i] = semvec[i] * info_content(joint_word) * info_content(sim_word)\n",
        "        i = i + 1\n",
        "    return semvec\n",
        "\n",
        "\n",
        "def semantic_similarity(row):\n",
        "    \"\"\"\n",
        "    Computes the semantic similarity between two sentences as the cosine\n",
        "    similarity between the semantic vectors computed for each sentence.\n",
        "    \"\"\"\n",
        "    info_content_norm = True\n",
        "    sentence_1 = row['question1']\n",
        "    sentence_2 = row['question2']\n",
        "    \n",
        "    words_1 = nltk.word_tokenize(sentence_1)\n",
        "    words_2 = nltk.word_tokenize(sentence_2)\n",
        "    joint_words = set(words_1).union(set(words_2))\n",
        "    vec_1 = semantic_vector(words_1, joint_words, info_content_norm)\n",
        "    vec_2 = semantic_vector(words_2, joint_words, info_content_norm)\n",
        "    return np.dot(vec_1, vec_2.T) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "33874e71-8b28-ad41-c468-470a1869bf58"
      },
      "outputs": [],
      "source": [
        "#if we implement semantic similarity it will take hours and hours of processing time \n",
        "#so we are not including the follwing feature \n",
        "# we have included the whole results and description of this feature in the report\n",
        "\"\"\"X_TrainData['semantic_sim'] = train.apply(semantic_similarity,axis = 1,raw = True )\n",
        "X_TestData['semantic_sim'] = test.apply(semantic_similarity,axis = 1,raw = True )\"\"\"\n",
        "X_TrainData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a1315190-bce4-926e-550f-98e77eb89d05"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "# train test split validation data 20% and test data 80%\n",
        "X_TrainData, X_ValidData, Y_TrainData, Y_ValidData = train_test_split(X_TrainData, Y_TrainData, test_size=0.20, random_state=4242)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "55a97d52-6a96-3c01-8eda-dc2f102767a3"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xg_TrainData = xgb.DMatrix(X_TrainData, label=Y_TrainData)\n",
        "xg_ValidData = xgb.DMatrix(X_ValidData, label=Y_ValidData)\n",
        "\n",
        "watchlist = [(xg_TrainData, 'train'), (xg_ValidData, 'valid')]\n",
        "#training using XGBoost using evalustion metric as logloss\n",
        "bst = xgb.train({'objective':'binary:logistic','eval_metric':'logloss','eta':0.02,'max_depth' :5}, xg_TrainData, 500, watchlist, early_stopping_rounds=50, verbose_eval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4717ecee-0732-9a85-bd4c-d1dc849e6d5d"
      },
      "outputs": [],
      "source": [
        "X_TestData.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9377d71b-f652-f139-5079-7f90af56d3c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "xg_TestData = xgb.DMatrix(X_TestData)\n",
        "xg_ValidData = xgb.DMatrix(X_ValidData)\n",
        "#predited values using XG boost\n",
        "Predict_TestData = bst.predict(xg_TestData)\n",
        "Predict_ValidData = bst.predict(xg_ValidData)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fe07d703-74fc-1bb6-3428-9eba589a3679"
      },
      "outputs": [],
      "source": [
        "#Roc metric\n",
        "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
        "fpr, tpr, _ = roc_curve(Y_ValidData, Predict_ValidData)\n",
        "roc_area = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr)\n",
        "np.round(roc_area, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "48df184c-b0cf-ad24-47ef-518bbd84f205"
      },
      "outputs": [],
      "source": [
        "# precision Recall curve\n",
        "precison, recall, _ = precision_recall_curve(Y_ValidData, Predict_ValidData)\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.plot(recall, precison)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "auc(recall, precison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b83b33e6-efbc-61e9-4478-2cc49ce1be6e"
      },
      "outputs": [],
      "source": [
        "#final classes to result.csv\n",
        "result = pd.DataFrame()\n",
        "result['test_id'] = test['test_id']\n",
        "result['is_duplicate'] = Predict_TestData\n",
        "result.to_csv('result.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "08649e93-0809-aea9-eb07-9a20a52e76b5"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}