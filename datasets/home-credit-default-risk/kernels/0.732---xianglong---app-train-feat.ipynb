{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nos.chdir('/kaggle/input/home-credit-default-risk')\n#os.chdir('/Users/xianglongtan/Desktop/kaggle')\n#print(os.getcwd())\n#print(os.listdir())\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_activity = 'all'\n# Any results you write to the current directory are saved as output.\napp_train = pd.read_csv('application_train.csv')\n#app_train = pd.read_csv('application_train.csv')\n#app_train.head()\napp_test = pd.read_csv('application_test.csv')\n#app_test = pd.read_csv('application_test.csv')\n#app_test.head()","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"os.chdir('../imputed')\n#print(os.listdir())\ntrain_and_test_imputed = pd.read_csv('train_and_test_imputed.csv')","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"914c21d769a54332123845d4314e6db932fdf394","trusted":true,"collapsed":true},"cell_type":"code","source":"#os.chdir('/Users/xianglongtan/Desktop/kaggle/submission')\nos.chdir('/kaggle/working')\nos.getcwd()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"34f14eda9a83c793318b6c0227f7a3b70ec1d16c","trusted":true,"collapsed":true},"cell_type":"code","source":"# Check missing value\npd.set_option('display.max_rows',1000)\npd.set_option('display.max_columns',1000)\n#print(train_and_test_imputed.isnull().any().sum())\n#train_and_test_imputed = train_and_test_imputed.drop('Unnamed: 0',axis=1)\n#train_and_test_imputed.head(10)\n\n# Process features \n## Encoding categorical data\n### Transform Education\nmap_educ = {'Secondary / secondary special':int(2),'Higher education':int(4),'Incomplete higher':int(3),'Lower secondary':int(1),'Academic degree':int(5)}\ntrain_and_test_imputed['NAME_EDUCATION_TYPE'] = train_and_test_imputed['NAME_EDUCATION_TYPE'].map(map_educ)\n#train_and_test_imputed['NAME_EDUCATION_TYPE'].head()\ntrain_and_test_imputed['NAME_EDUCATION_TYPE'] = train_and_test_imputed['NAME_EDUCATION_TYPE'].fillna(0)\n#train_and_test_imputed['NAME_EDUCATION_TYPE'].head()\n\n### One-hot encoding categorical features\none_hot = 1\nif one_hot == 1:\n    tnt_imp_dum = pd.get_dummies(train_and_test_imputed)\n    hour = pd.get_dummies(tnt_imp_dum['HOUR_APPR_PROCESS_START'],prefix='HOUR')\n    tnt_imp_dum = pd.concat([tnt_imp_dum,hour],axis=1).drop('HOUR_APPR_PROCESS_START',axis=1)\n    tnt_imp_dum.head()\n    ## Normalize numeric features\n    # Combine these (average)\n    #train_and_test['MEAN_CNT_SOCIAL_CIRCLE'] = (train_and_test['OBS_30_CNT_SOCIAL_CIRCLE']+train_and_test['DEF_30_CNT_SOCIAL_CIRCLE']+train_and_test['OBS_60_CNT_SOCIAL_CIRCLE']+train_and_test['DEF_60_CNT_SOCIAL_CIRCLE'])/4\n    # Transform to years\n    #train_and_test['TIME_LAST_PHONE_CHANGE'] = train_and_test['DAYS_LAST_PHONE_CHANGE ']/365\n    # round counting features\n    col = [x for x in tnt_imp_dum.columns if x.startswith('AMT_REQ_CREDIT_BUREAU')]\n    tnt_imp_dum[col] = tnt_imp_dum[col].apply(lambda x: round(x))\n    # extract columns that have more than 2 unique values and the maximum large than 1 and normalized\n    cols = [x for x in tnt_imp_dum.columns if len(tnt_imp_dum[x].value_counts())>2 and np.nanmax(np.abs(tnt_imp_dum[x].values)) > 1]\n    cols.remove('SK_ID_CURR')\n    cols.remove('Unnamed: 0')\n    from sklearn import preprocessing\n    tnt_imp_dum[cols] = preprocessing.normalize(tnt_imp_dum[cols],axis=0)\n    #standardscaler = preprocessing.StandardScaler()\n    #tnt_imp_dum[cols] = standardscaler.fit_transform(tnt_imp_dum[cols])\n    #for col in cols:\n        #tnt_imp_dum[col] = preprocessing.normalize(tnt_imp_dum[col])\nelse:\n    col = [x for x in tnt_imp_dum.columns if x.startswith('AMT_REQ_CREDIT_BUREAU')]\n    tnt_imp_dum[col] = tnt_imp_dum[col].apply(lambda x: round(x))\n    col = [x for x in tnt_imp_dum.columns if tnt_imp_dum[x].dtype == 'float64']\n    #print(col)\n    #from sklearn import preprocessing\n    #normalizer = preprocessing.Normalizer(norm='l2')\n    #tnt_imp_dum[col] = normalizer.fit_transform(tnt_imp_dum[col])","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"0f5da7296fd077e69229d88efc4103a753ae6fe8","collapsed":true},"cell_type":"markdown","source":"# Split training set and test set"},{"metadata":{"_uuid":"29c8ceaa63fe36f52b70afbae0ccb05b4d22cbfd","trusted":true,"collapsed":true},"cell_type":"code","source":"train_X = tnt_imp_dum.loc[0:len(app_train)-1,:]\ntest_X = tnt_imp_dum.loc[len(app_train):(len(app_train)+len(app_test)),:]\ntrain_Y = app_train['TARGET']\ntrain_X = train_X.drop(['Unnamed: 0','SK_ID_CURR'],axis=1)\ntrain_X.head()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"70abe268fa3b0c379b8e660929bad3c8717f7a5b","trusted":true,"collapsed":true},"cell_type":"code","source":"test_ID = pd.DataFrame(test_X['SK_ID_CURR'])\ntest_ID = test_ID.reset_index().drop('index',axis=1)\ntest_X = test_X.drop(['Unnamed: 0','SK_ID_CURR'],axis=1)\n#test_X.head()\n#test_ID.head()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"f3e7da06635b500fa44c7e8c78ce538b121ba1ad","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost.sklearn import XGBClassifier\nimport time\nfrom plotnine import *\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nseed = 0\nX_train,X_val,y_train,y_val = train_test_split(train_X,train_Y,random_state=seed,stratify=train_Y)","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"c33ccb8dd8d0d9b7aa868b80e9b5c85c4b1ef9d4","trusted":true,"collapsed":true},"cell_type":"code","source":"flag = None\nif flag == 'KNN':\n    start = time.time()\n    model = KNeighborsClassifier()\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(train_X,train_Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('app_train_knn1.csv')\nelif flag == 'SVM':\n    start = time.time()\n    model = SVC()\n    model.fit(X_train, y_train)\n    pred_train = model.predict(X_train)\n    pred_val = model.predict(X_val)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('only_nonan_svm1.csv')\nelif flag == 'logit':\n    start = time.time()\n    model = LogisticRegression(C=2,max_iter=200,random_state=seed)\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(train_X,train_Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('app_train_logit1.csv')\nelif flag == 'RF':\n    start = time.time()\n    model = RandomForestClassifier(n_estimators=120,random_state=seed)\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(train_X,train_Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('app_train_rf1.csv')\nelif flag == 'XGB':\n    start = time.time()\n    model = XGBClassifier(max_depth = 6, n_estimators = 200)\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(train_X,train_Y,eval_metric='auc')\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('app_train_xgb1.csv')\nelif flag == 'ET':\n    start = time.time()\n    model = ExtraTreesClassifier()\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(train_X,train_Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('app_train_ext1.csv')\nelif flag == 'AB':\n    start = time.time()\n    model = AdaBoostClassifier(base_estimator = LogisticRegression(C=2,max_iter=200,random_state=seed),random_state=seed, learning_rate=0.8)\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(train_X,train_Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('app_train_ab1.csv')\nelif flag == 'BC':\n    start = time.time()\n    model = BaggingClassifier(base_estimator=LogisticRegression(C=2,max_iter=200,random_state=seed),n_estimators = 20, random_state=seed)\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(train_X,train_Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('app_train_bc1.csv')\nelif flag == 'GNB':\n    start = time.time()\n    model = GaussianNB(prior=[24825/(282686+24825),282686/(282686+24825)])\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(train_X,train_Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('app_train_gnb1.csv')\nelif flag == 'DT':\n    start = time.time()\n    model = DecisionTreeClassifier()\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(train_X,train_Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('app_train_dt1.csv')\nelse:\n    pass","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"2421539238de20a9533caf660bd28aa1bb32dded","trusted":true,"collapsed":true},"cell_type":"code","source":"result.head(20)","execution_count":9,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a07ae72d3b50bed0c2e926f3fdc8e30a485a73b8","collapsed":true},"cell_type":"code","source":"","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"aca887f9c3f1b204bc0f084ac0d90e142b697f26","collapsed":true,"trusted":false},"cell_type":"code","source":"# CV for logistic regression\ncv_logit = 0\nif cv_logit == 1\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.model_selection import StratifiedKFold\n    model = LogisticRegression(class_weight='balanced')\n    param_grid = dict(C=[0.1,0.5,1,1.5,2],\n                     penalty=['l1','l2'])\n\n    kfold = StratifiedKFold(n_splits = 5, shuffle=True,random_state = seed)\n    grid_search = GridSearchCV(model, param_grid, scoring = 'f1', cv=kfold)\n    start = time.time()\n    grid_result = grid_search.fit(train_X,train_Y)\n    end = time.time()\n    print(end-start)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d93169fdce71cb5d60da07bec3898b581693e82a","collapsed":true,"trusted":false},"cell_type":"code","source":"# CV for XGB\ncv_xgb = 0\nif xv_xgb == 1:\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.model_selection import StratifiedKFold\n    model = XGBClassifier(class_weight='balanced')\n    learning_rate = [0.0001, 0.001,0.01]\n    max_depth = [2,4,6,8]\n    subsample = [0.2, 0.4, 0.6, 0.8, 1.0]\n    colsample_bytree = [0.2, 0.4, 0.6, 0.8, 1.0]\n    n_estimator = range(100, 500, 50)\n\n    param_grid = dict(learning_rate = learning_rate,\n                     #max_depth = max_depth,\n                     #subsample = subsample,\n                     #colsample_bytree = colsample_bytree,\n                     n_estimator = n_estimator)\n    kfold = StratifiedKFold(n_splits = 10, shuffle=True,random_state = seed)\n    grid_search = GridSearchCV(model, param_grid, scoring = 'f1', cv=kfold)\n    start = time.time()\n    grid_result = grid_search.fit(train_X,train_Y)\n    end = time.time()\n    print(end-start)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebc1ef59b2d419ac2799902a032eb476dc4aca67","trusted":true,"collapsed":true},"cell_type":"code","source":"NN = 1\nif NN == 1:\n    import tensorflow as tf\n\n    # Hyperparam\n    LR = 0.0001 # learning rate\n    ITERATION = 5000\n    BATCH_SIZE = 20000\n    KEEP_PROB = 0.7\n    NUM_FEAT = 264\n    NUM_CLASS = 2\n\n    class DataIter2():\n        def __init__(self, X,Y):\n            self.X = X\n            self.Y = Y\n            self.size = len(self.X)\n            self.epochs = 0\n            self.df = pd.concat([X,Y],axis=1)\n            self.shuffle()\n        def shuffle(self):\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n            self.cursor = 0\n        def next_batch(self, n):\n            if self.cursor + n > self.size:\n                #res = self.df.iloc[self.cursor:self.size]\n                self.epochs += 1\n                self.shuffle()\n            res = self.df.iloc[self.cursor:(self.cursor+n)]\n            self.cursor += n\n            return x, res['y'], res['length'],res.index\n\n    class DataIter():\n        def __init__(self, X,Y):\n            self.X = X\n            self.Y = Y\n            self.size = len(self.X)\n            self.epochs = 0\n            self.df = pd.concat([X,Y],axis=1)\n            self.pos = self.df.loc[self.Y == 1]\n            self.neg = self.df.loc[self.Y == 0]\n        def next_batch(self,n):\n            #X_train,X_val,y_train,y_val = train_test_split(X,Y,test_size = n/self.size,random_state=seed,stratify=Y)\n            #res = pd.concat([X_val,y_val],axis=1)\n            pos_sample = self.pos.sample(n, replace=True)\n            neg_sample = self.neg.sample(n, replace=True)\n            res = pd.concat([neg_sample, pos_sample],axis=0)\n            return res\n\n    class DataIter2():\n        def __init__(self, X,Y):\n            self.X = X\n            self.Y = Y\n            self.size = len(self.X)\n            self.epochs = 0\n            self.df = pd.concat([X,Y],axis=1)\n            self.shuffle()\n        def shuffle(self):\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n            self.cursor = 0\n        def next_batch(self, n):\n            if self.cursor + 2*n > self.size:\n                #res = self.df.iloc[self.cursor:self.size]\n                self.epochs += 1\n                self.shuffle()\n            res = self.df.iloc[self.cursor:(self.cursor+2*n)]\n            self.cursor += n\n            return res\n\n\n    # build graph\n    tf.reset_default_graph()\n    x = tf.placeholder(tf.float32,[BATCH_SIZE*2, NUM_FEAT])\n    y = tf.placeholder(tf.int32,[BATCH_SIZE*2])\n    keep_prob = tf.constant(KEEP_PROB)\n    nn_inputs = tf.layers.dense(x, units = round(1.5*NUM_FEAT), kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid)# hidden layer 1\n    nn_inputs = tf.layers.dropout(nn_inputs, KEEP_PROB,training=True)\n    #print(nn_inputs.get_shape)\n    nn_inputs = tf.layers.dense(x, units = round(2*NUM_FEAT), kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid) # hidden layer 2\n    nn_inputs = tf.layers.dropout(nn_inputs, KEEP_PROB,training=True)\n    #print(nn_inputs.get_shape)\n    nn_inputs = tf.layers.dense(x, units = round(1*NUM_FEAT), kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid) # hidden layer 3\n    nn_inputs = tf.layers.dropout(nn_inputs, KEEP_PROB,training=True)\n    #print(nn_inputs.get_shape)\n    nn_inputs = tf.layers.dense(x, units = 0.5*NUM_FEAT, kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid) # hidden layer 4\n    nn_inputs = tf.layers.dropout(nn_inputs, KEEP_PROB,training=True)\n    with tf.variable_scope('softmax'):\n        W = tf.get_variable('W', [0.5*NUM_FEAT, NUM_CLASS],initializer = tf.truncated_normal_initializer())\n        b = tf.get_variable('b', [NUM_CLASS],initializer = tf.constant_initializer(0.0))\n    logits = tf.matmul(nn_inputs, W)+b\n    #print(logits.get_shape)\n    preds = tf.nn.softmax(logits)\n    prediction = tf.cast(tf.argmax(preds,1), tf.int32)\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = preds))\n    precision, precision_op = tf.metrics.precision(y,prediction)\n    #print(precision.get_shape)\n    recall, recall_op = tf.metrics.recall(y,prediction)\n    #print(recall.get_shape)\n    f1score = 2*precision*recall/(precision+recall)\n    train_step = tf.train.AdamOptimizer(LR).minimize(loss)\n\n    # session\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n        tr = DataIter2(train_X,train_Y)\n        for i in range(ITERATION):\n            batch = tr.next_batch(BATCH_SIZE)\n            sess.run(train_step, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n            if i%200 == 0:\n                _,prec = sess.run([precision,precision_op], feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n                _,rec = sess.run([recall,recall_op], feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n                f1s = sess.run(f1score, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n                los = sess.run(loss, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n                print('losss after',i,'round',los)\n                print('precision after',i,'round',prec)\n                print('recall after',i,'round',rec)\n                print('F1 score after',i,'round:',f1s)\n                #print('\\n----------------------------------\\n')\n                print('logits:\\n',sess.run(logits,feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})[0:10])\n                print('preds:\\n',sess.run(preds, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})[0:10])\n                print('prediction:\\n',sess.run(prediction, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})[0:10])\n                print('y:\\n',sess.run(y,feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})[0:10])\n                print('\\n----------------------------------\\n')\n        cursor = 0\n        while cursor <= len(test_X):\n            if cursor+2*BATCH_SIZE <= len(test_X):\n                te = test_X.iloc[cursor:cursor+2*BATCH_SIZE]\n            else:\n                te = pd.concat([test_X.iloc[cursor:len(test_X)],test_X.iloc[0:2*BATCH_SIZE-len(test_X.iloc[cursor:len(test_X)])]])\n            results = sess.run(preds, feed_dict={x:te})\n            if cursor == 0:\n                prediction_test = pd.DataFrame(data=results, columns=['0','TARGET'])\n            else:\n                prediction_test = pd.concat([prediction_test, pd.DataFrame(data=results,columns=['0','TARGET'])])\n            cursor += 2*BATCH_SIZE\n    result = prediction_test.iloc[0:len(test_X)]\n    pred_test = pd.DataFrame(result['TARGET']).reset_index()\n    result_final = pd.concat([test_ID, pred_test],axis=1).drop('index',axis=1)\n    result_final.columns = ['SK_ID_CURR','TARGET']\n    result_final = result_final.set_index('SK_ID_CURR')\n    result_final.to_csv('app_feat_NN1.csv')","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"7f8b98eaedb23fc02cf8d807fd6b0a5672e818b1","trusted":true,"collapsed":true},"cell_type":"code","source":"result_final.head()","execution_count":11,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f79102b76b5014431849f265b351c1b707201307","collapsed":true},"cell_type":"code","source":"","execution_count":12,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b83cdb5d998143bcf877d76ff98e027e541c77b7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}