{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport imageio\n\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Maximum\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras import regularizers\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom skimage.transform import resize as imresize\nfrom tqdm import tqdm\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 10\nEPOCHS = 1000\nRANDOM_STATE = 123\n\nCLASS = {\n    'Black-grass': 0,\n    'Charlock': 1,\n    'Cleavers': 2,\n    'Common Chickweed': 3,\n    'Common wheat': 4,\n    'Fat Hen': 5,\n    'Loose Silky-bent': 6,\n    'Maize': 7,\n    'Scentless Mayweed': 8,\n    'Shepherds Purse': 9,\n    'Small-flowered Cranesbill': 10,\n    'Sugar beet': 11\n}\n\nINV_CLASS = {\n    0: 'Black-grass',\n    1: 'Charlock',\n    2: 'Cleavers',\n    3: 'Common Chickweed',\n    4: 'Common wheat',\n    5: 'Fat Hen',\n    6: 'Loose Silky-bent',\n    7: 'Maize',\n    8: 'Scentless Mayweed',\n    9: 'Shepherds Purse',\n    10: 'Small-flowered Cranesbill',\n    11: 'Sugar beet'\n}","execution_count":2,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6a6f7b7d7a9ded42db3946325c9b1e1ae396735e","_cell_guid":"9b55fb60-40f0-47cb-b5a1-5a47ccdcc950","trusted":true},"cell_type":"code","source":"# Dense layers set\ndef dense_set(inp_layer, n, activation, drop_rate=0.):\n    dp = Dropout(drop_rate)(inp_layer)\n    dns = Dense(n)(dp)\n    bn = BatchNormalization(axis=-1)(dns)\n    act = Activation(activation=activation)(bn)\n    return act","execution_count":3,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"8919f8f6feeae5addf15a9633b2b93fbfc497c19","_cell_guid":"04bc3f0c-fbf8-4430-98de-9340e719706e","trusted":true},"cell_type":"code","source":"# Conv. layers set\ndef conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False):\n    if zp_flag:\n        zp = ZeroPadding2D((1,1))(feature_batch)\n    else:\n        zp = feature_batch\n    conv = Conv2D(filters=feature_map, kernel_size=kernel_size, strides=strides)(zp)\n    bn = BatchNormalization(axis=3)(conv)\n    act = LeakyReLU(1/10)(bn)\n    return act","execution_count":4,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"759dd33108c8bfd39d1e0b553da8726f0a6a5ecf","_cell_guid":"74aef5ea-ef5e-4d39-bb6e-0df503d97968","trusted":true},"cell_type":"code","source":"# simple model \ndef get_model():\n    inp_img = Input(shape=(51, 51, 3))\n\n    # 51\n    conv1 = conv_layer(inp_img, 64, zp_flag=False)\n    conv2 = conv_layer(conv1, 64, zp_flag=False)\n    mp1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv2)\n    # 23\n    conv3 = conv_layer(mp1, 128, zp_flag=False)\n    conv4 = conv_layer(conv3, 128, zp_flag=False)\n    mp2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv4)\n    # 9\n    conv7 = conv_layer(mp2, 256, zp_flag=False)\n    conv8 = conv_layer(conv7, 256, zp_flag=False)\n    conv9 = conv_layer(conv8, 256, zp_flag=False)\n    mp3 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv9)\n    # 1\n    # dense layers\n    flt = Flatten()(mp3)\n    ds1 = dense_set(flt, 128, activation='tanh')\n    out = dense_set(ds1, 12, activation='softmax')\n\n    model = Model(inputs=inp_img, outputs=out)\n    \n    # The first 50 epochs are used by Adam opt.\n    # Then 30 epochs are used by SGD opt.\n    \n    #mypotim = Adam(lr=2 * 1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n    mypotim = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy',\n                   optimizer=mypotim,\n                   metrics=['accuracy'])\n    model.summary()\n    return model","execution_count":5,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e862845fa5c7e620c664ec85b1fc82b129264a40","_cell_guid":"d968679d-a5dd-4de5-937c-b6f03c767557","trusted":true},"cell_type":"code","source":"def get_callbacks(filepath, patience=5):\n    lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=1e-5, patience=patience, verbose=1)\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [lr_reduce, msave]","execution_count":6,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"fb70f53ed1e9d83177567f64ba5653b923aa32a1","_cell_guid":"0a3fee6c-d79c-4a77-a2da-f3c24a6fc698","trusted":true},"cell_type":"code","source":"# I trained model about 12h on GTX 950.\ndef train_model(img, target):\n    callbacks = get_callbacks(filepath='../input/model-weight/model_weight_SGD.hdf5', patience=6)\n    gmodel = get_model()\n    gmodel.load_weights(filepath='../input/model-weight/model_weight_Adam.hdf5')\n    x_train, x_valid, y_train, y_valid = train_test_split(\n                                                        img,\n                                                        target,\n                                                        shuffle=True,\n                                                        train_size=0.8,\n                                                        random_state=RANDOM_STATE\n                                                        )\n    gen = ImageDataGenerator(\n            rotation_range=360.,\n            width_shift_range=0.3,\n            height_shift_range=0.3,\n            zoom_range=0.6,\n            horizontal_flip=True,\n            vertical_flip=True\n    )\n    gmodel.fit_generator(gen.flow(x_train, y_train,batch_size=BATCH_SIZE),\n               steps_per_epoch=10*len(x_train)/BATCH_SIZE,\n               epochs=EPOCHS,\n               verbose=1,\n               shuffle=True,\n               validation_data=(x_valid, y_valid),\n               callbacks=callbacks)","execution_count":7,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0c088bd3bf0b535c98d221ab9fbfada755841ed3","_cell_guid":"2dbc5aed-d129-43ed-a17f-f321809f5bf3","trusted":true},"cell_type":"code","source":"def test_model(img, label):\n    gmodel = get_model()\n    gmodel.load_weights(filepath='../input/model-weight/model_weight_SGD.hdf5')\n    prob = gmodel.predict(img, verbose=1)\n    pred = prob.argmax(axis=-1)\n    sub = pd.DataFrame({\"file\": label,\n                         \"species\": [INV_CLASS[p] for p in pred]})\n    sub.to_csv(\"sub.csv\", index=False, header=True)","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"03b8f1eb-095e-4633-bcfe-f08310c584bd","_uuid":"35727fcdc1629e3cde559f10ebbde4ea2ce453fa","trusted":true,"collapsed":true},"cell_type":"code","source":"# Resize all image to 51x51 \ndef img_reshape(img):\n    img = imresize(img, (51, 51, 3))\n    return img","execution_count":9,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"46acfb642974c61f1fcb722375f863860173da76","_cell_guid":"fbc234ce-ced1-42d9-b5c6-c1e3d0abda55","trusted":true},"cell_type":"code","source":"# get image tag\ndef img_label(path):\n    return str(str(path.split('/')[-1]))\n\n# get plant class on image\ndef img_class(path):\n    return str(path.split('/')[-2])","execution_count":10,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"361565d8f3847fa8861828112a374ce05736cf9a","_cell_guid":"87a22608-b7b4-4aca-917e-69bf25cc0431","trusted":true},"cell_type":"code","source":"# fill train and test dict\ndef fill_dict(paths, some_dict):\n    text = ''\n    if 'train' in paths[0]:\n        text = 'Start fill train_dict'\n    elif 'test' in paths[0]:\n        text = 'Start fill test_dict'\n\n    for p in tqdm(paths, ascii=True, ncols=85, desc=text):\n        img = imageio.imread(p)\n        img = img_reshape(img)\n        some_dict['image'].append(img)\n        some_dict['label'].append(img_label(p))\n        if 'train' in paths[0]:\n            some_dict['class'].append(img_class(p))\n\n    return some_dict","execution_count":11,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b09736923f4d4376be9fd99e60ffaa00867fa6a7","_cell_guid":"394a9daf-9dfd-467b-a803-d32c97856b5f","trusted":true},"cell_type":"code","source":"# read image from dir. and fill train and test dict\ndef reader():\n    file_ext = []\n    train_path = []\n    test_path = []\n\n    for root, dirs, files in os.walk('../input'):\n        if dirs != []:\n            print('Root:\\n'+str(root))\n            print('Dirs:\\n'+str(dirs))\n        else:\n            for f in files:\n                ext = os.path.splitext(str(f))[1][1:]\n\n                if ext not in file_ext:\n                    file_ext.append(ext)\n\n                if 'train' in root:\n                    path = os.path.join(root, f)\n                    train_path.append(path)\n                elif 'test' in root:\n                    path = os.path.join(root, f)\n                    test_path.append(path)\n    train_dict = {\n        'image': [],\n        'label': [],\n        'class': []\n    }\n    test_dict = {\n        'image': [],\n        'label': []\n    }\n\n    #train_dict = fill_dict(train_path, train_dict)\n    test_dict = fill_dict(test_path, test_dict)\n    return train_dict, test_dict","execution_count":14,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"dda25f3e74420f1de09c6152e5b547ca241ff657","_cell_guid":"8f779f6b-a7c8-4766-925c-5475d545c68c","trusted":true},"cell_type":"code","source":"# I commented out some of the code for learning the model.\ndef main():\n    train_dict, test_dict = reader()\n    #X_train = np.array(train_dict['image'])\n    #y_train = to_categorical(np.array([CLASS[l] for l in train_dict['class']]))\n\n    X_test = np.array(test_dict['image'])\n    label = test_dict['label']\n    \n    # I do not recommend trying to train the model on a kaggle.\n    #train_model(X_train, y_train)\n    test_model(X_test, label)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"f5f4b15fec746b0f8f7409545df61c89c1873e00","_cell_guid":"bbc40a16-dab0-4514-917c-1a8043d92340","trusted":true},"cell_type":"code","source":"main()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d12dc74bdbf5e4f93e4b53facf19605d0d04b7a9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}