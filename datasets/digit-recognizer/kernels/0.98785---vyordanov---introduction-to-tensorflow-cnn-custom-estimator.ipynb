{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Introduction to Tensorflow (CNN Custom Estimator) and Tensorboard\n\nHi Everyone, \n\nI have seen lots of kernels which are using Tensorflow really inefficiently. Thus I have decided to share with you how you can build a custom Convolutional Neural Network estimator, train it, evaluate its performance and submit predictions.\n\n## Estimator API\n\nLet's start by exploring the [documentation](https://www.tensorflow.org/guide/estimators) (optional). So an Estimator is simply a python object which already has the following methods defined:\n* train\n* evaluate\n* predict\n\nand the only thing which you need to do is to serve your data into those methods via an **input function**.\n\nNow here is the place to mention that it is this simple only for premade (canned) estimators, where the structure of the neural network had been defined. Examples for such estimators are:\n* [DNN Classifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier)\n* [DNN Regressor](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor)\n* [Boosted Trees Classifier](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier)\n* [Boosted Trees Regressor](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesRegressor)\n\nHowever when you are preparing your custom estimator you also need to provide a **model function** which describes the model of the neural network (convolutions, dropout layers, logits layer and so on). Now when you have a basic idea, let's build the project step by step.\n\n### Includes and loading the datasets in Python\n\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n# this is important line if you intend to monitor your training/evaluation metrics in tensorboard\n# stuff like accuracy, loss and etc.\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# helper method for loading and preprocessing the data\ndef load_datasets():\n    '''\n    Method for loading and preprocessing the data\n    \n    OUTPUT:\n    np.array of training pixel data\n    np.array of training labels\n    np.array of inference pixel data\n    '''\n    # reading the Kaggle datasets\n    train = pd.read_csv('../input/train.csv')\n    infer = pd.read_csv('../input/test.csv')\n    \n    # print some summary for the datasets\n    print('Reading Successful.')\n    print('Size of the training dataset {}'.format(train.shape))\n    print('Size of the inference dataset {}'.format(infer.shape))\n    \n    # helper function for normalization of the images and converting them to np.arrays\n    def image_preprocessing(df):\n        df = df / 255.0 # normalization\n        df = df.values.tolist() # convert to list\n        array = np.asarray(df, dtype=np.float32) # convert to numpy arrays\n        \n        return array\n    \n    # here we pass only the pixel data to our preprocessing method\n    train_images = image_preprocessing(train.drop('label', axis=1))\n    infer_images = image_preprocessing(infer)\n    train_labels = np.asarray(train['label'],dtype=np.int32)\n    \n    print('Preprocessing Successful.')\n    print('Dimensions of train_images array {}'.format(train_images.shape))\n    print('Dimensions of infer_images array {}'.format(infer_images.shape))\n    print('Dimensions of train_labels array {}'.format(train_labels.shape))\n    \n    return train_images, train_labels, infer_images\n\ntrain_images, train_labels, infer_images = load_datasets()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05e09591cfaa4c9a45c8190e03a30475a5cb08cc"},"cell_type":"markdown","source":"Now once we have the data already in our hands we can start with the actual Tensorflow part.\n\n### Input functions\n\nHere I will define the methods which will serve our data to the Tensorflow neural network during training, evaluating and predicting. Fortenately the Estimator API already provides a method which can read numpy arrays and convert them to tensors, the [numpy_input_fn](https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn). \n\nAll which I will do now is to define small wrapper functions for training, evaluating and predicting using the numpy_input_fn. \n\nNOTE: these functions are not actually performing the training, evaluation and prediction of the neural network. They just serve information to those methods, which are already implemented in the Estimator API!\n\nLet's start with the **training input function**:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3147cb73ddc1ff7904f12bd73f381ef5a9664c42"},"cell_type":"code","source":"'''\nx - according to the documentation I am passing a dictionary of my training features\ny - passing the training labels\nbatch_size - how many images I want to pass to my NN in a single training step. \n             The gradients and the loss for the training step will be calculated only on these images. \nnum_epochs - when I want to complete the execution of this method. With the current setting (1) \n             when the input function goes through all images once it will complete.\nshuffle    - do I want to read the images in order or no. It is a better strategy to \n             shuffle within the training images during training.\n'''\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n      x={'x':train_images},\n      y=train_labels,\n      batch_size=80,\n      num_epochs=1,\n      shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"638bc6693d28291d7d16ffa32d12e2d7973d8f1a"},"cell_type":"markdown","source":"Here is the **evaluation input function**:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b934668be6c157862085833e426bdec82615878f"},"cell_type":"code","source":"'''\nWhat the eval_input_fn will do with its current settings is to read the entire training dataset \n(in order) using batch_size of 128 (default setting). \n\nKeep in mind that it will also terminate when completes 1 epoch.\n'''\neval_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={'x':train_images},\n    y=train_labels,\n    shuffle=False,\n    num_epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"339624dbac2d23ee138a5eaa509b80dd1ca2b207"},"cell_type":"markdown","source":"And finally the **prediction input function**:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"08377beac13b2d1fd70a40aaf40b72131433f19d"},"cell_type":"code","source":"'''\nWhat the predict_input_fn will do with its current settings is to read the entire inference dataset \n(in order) using batch_size of 128 (default setting). \n\nKeep in mind that it will also terminate when completes 1 epoch.\n'''\npredict_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={'x':infer_images},\n    shuffle=False,\n    num_epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbd23732652e31db5002fd44871ecc105857b915"},"cell_type":"markdown","source":"### Model Function\n\nSo far we have defined only the input functions, now let's define the model of the neural network which we will apply them to:\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b78b9b62c6409cc73dc615fca2d924f79497298e"},"cell_type":"code","source":"def cnn_model_function(features, labels, mode):\n    '''\n    This is the function which describes the structure of the neural network\n    '''\n    \n    # input layer\n    # reshaping x to 4-D tensor: [batch_size, width, height, channels]\n    # features['x'] - the dictionary we passed for x in the input functions\n    layer_1 = tf.reshape(features['x'], [-1, 28, 28, 1]) \n    \n    # convolution layer 1\n    # computes 32 features using 10x10 filter with ReLU activation.\n    # input tensor: [batch_size, 28, 28, 1]\n    # output tensor: [batch_size, 28, 28, 32]\n    layer_2 = tf.layers.conv2d(                          \n                inputs=layer_1,\n                filters=32,\n                kernel_size=[10, 10],\n                padding=\"same\",\n                activation=tf.nn.relu)\n    \n    # convolution layer 2\n    # computes 32 features using 5x5 filter with ReLU activation.\n    # input tensor: [batch_size, 28, 28, 32]\n    # output tensor: [batch_size, 28, 28, 64]\n    layer_3 = tf.layers.conv2d(\n                inputs=layer_2,\n                filters=64,\n                kernel_size=[5, 5],\n                padding=\"same\",\n                activation=tf.nn.relu)\n    \n    # Flatten tensor into a batch of vectors\n    # Input Tensor Shape: [batch_size, 28, 28, 64]\n    # Output Tensor Shape: [batch_size, 28 * 28 * 64]\n    layer_4 = tf.reshape(layer_3, [-1, 28 * 28 * 64])\n    \n    # Dense Layer\n    # Densely connected layer with 1024 neurons\n    # Input Tensor Shape: [batch_size, 28 * 28 * 64]\n    # Output Tensor Shape: [batch_size, 1024]\n    layer_5 = tf.layers.dense(inputs=layer_4, units=1024, activation=tf.nn.relu)\n    \n    # Dropout operation; 0.6 probability that element will be kept\n    # notice that this layer will perform droupout only during training!\n    layer_6 = tf.layers.dropout(inputs=layer_5, rate=0.4, training=(mode == tf.estimator.ModeKeys.TRAIN))\n    \n    # Logits layer\n    # Input Tensor Shape: [batch_size, 1024]\n    # Output Tensor Shape: [batch_size, 10]\n    logits = tf.layers.dense(inputs=layer_6, units=10)\n    \n    # define the values which our neural network will output\n    # classes - which number the NN 'thinks' is on the image\n    # probabilities - how certain our NN is about its prediction\n    predictions = {\n                \"classes\": tf.argmax(input=logits, axis=1),\n                \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n    }\n    \n    # here we define what happens if we call the predict method of our estimator\n    # with the current settings it will return the dictionary defined above\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n    \n    # here we define the loss for our training (the thing we minimize)\n    # I do not need to perform one-hot-encoding to my training labels because the method\n    # sparse_softmax_cross_entropy will do that for me and I don't need to think about that\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n    \n    # here we define how we calculate our accuracy\n    # if you want to monitor your training accuracy you need these two lines\n    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions['classes'], name='acc_op')\n    tf.summary.scalar('accuracy', accuracy[1])\n    \n    # here we define what happens if we call the train method of our estimator\n    # with its current settings it will adjust the weights and biases of our neurons\n    # using the Adam Optimization Algorithm based on the loss function we defined earlier\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.AdamOptimizer()\n        train_op = optimizer.minimize(\n            loss=loss,\n            global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n    \n    # what evaluation metric we want to show\n    eval_metric_ops = {\"accuracy\": accuracy}\n    \n    # here we define what happens if we call the evaluate method of our estimator\n    # with its current settings it will display the loss and the accuracy which we defined earlier\n    return tf.estimator.EstimatorSpec(\n                mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc395bc1e4041c36c88b7ea81f2434bda02c2ce6"},"cell_type":"markdown","source":"### Training our Neural Network\n\nWell at this stage we can already define our neural network and train it."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2e7e844185e01ccb0c1298ca3d1b87ec8b536ae2","collapsed":true},"cell_type":"code","source":"# first we define a folder where tensorflow will keep its progress\n# this includes periodical saves of our weight, biases, accuracy, loss and etc.\n# so if we have more training images we can simply continue training on them\nOUTDIR = './CNN_CLASSIFIER'\n\n# we create an estimator object which:\n# - is using the Neural Net structure from the cnn_model_function\n# - reads/writes the files written in the directory which we defined earlier\ncnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_function, model_dir=OUTDIR)\n\n# here we start the FileWriter method which will actually save the progress in the folder defined above\nfile_writer = tf.summary.FileWriter(OUTDIR)\n\n# a small helper function which trains/evaluates our network for a given number of epochs\n# remember that our input functions go through the datasets only once\ndef train_and_evaluate(estimator, epochs=30):\n    for i in range(epochs):\n        estimator.train(input_fn=train_input_fn)\n        estimator.evaluate(input_fn=eval_input_fn)\n\n# and here we finally start training/evaluating the NN for 30 epochs\ntrain_and_evaluate(cnn_classifier)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b16c6ba5f06f4d13406f9f951c45e341f2dfe409"},"cell_type":"markdown","source":"The training finished with 99.69% accuracy for 10 epochs. I think this is a good enough model to use for prediction. Let's start:"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2f1bde042f3f24eabc609b0da016c0dd14cf9f2c","collapsed":true},"cell_type":"code","source":"# we call the predict method on our estimator\n# this method will not return the entire prediction dataset at once but it returns a python generator \n# which we can use to iterate through the predictions one by one\n# first we initialize our generator\ngenerator = cnn_classifier.predict(input_fn=predict_input_fn)\n\n# then we store all predictions into a list of dictionaries\n# (dictionary from classes and probabilities which we defined in the model function)\npredictions = [next(generator) for i in range(len(infer_images))]\n\n# Kaggle are interested only in the classes predictions without the probabilities\n# thus we get only the classes in a new list\nclasses = [predictions[i]['classes'] for i in range(len(predictions))]\n\n# finally we write our predictions into a csv file\ndef generate_submission_file(predictions, fileName):\n    submission = pd.DataFrame()\n    \n    submission['ImageId'] = range(1,28001,1)\n    submission['Label'] = predictions\n    submission.set_index('ImageId', inplace=True)\n    submission.to_csv(fileName)\n    \n    print('Submission Ready!')\n    \ngenerate_submission_file(classes, 'submission-01.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}