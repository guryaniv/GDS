{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom sklearn import preprocessing\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f869d80c65c655027f246dc63b6cdf635480998f","trusted":true},"cell_type":"code","source":"TRAIN_PATH = '../input/train.csv'\n# trainDF =  pd.read_csv('../input/train.csv', chunksize = 100000)\n# trainDF = trainDF.get_chunk(100000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b8e47b0e79378b20615f752f1a386f875be8bb5","scrolled":true,"trusted":true},"cell_type":"code","source":"%%time\n# Assume we only know that the csv file is somehow large, but not the exact size\n# we want to know the exact number of rows\n\n# Method 1, using file.readlines. Takes about 20 seconds.\nwith open(TRAIN_PATH) as file:\n    n_rows = len(file.readlines())\n\nprint ('Exact number of rows: '+str(n_rows))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d02ec20fa710ec44f9de154dc158d4906b0241b","scrolled":true,"trusted":true},"cell_type":"code","source":"chunksize = 5000000 # 5 million rows at one go. Or try 10 million\ntotal_chunk = n_rows // chunksize + 1\n\nprint('Total chunks required:'+str(total_chunk))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a66241c30756e219ab169e282357af2937dcfe62","trusted":true},"cell_type":"code","source":"traintypes = {'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\ncols = list(traintypes.keys())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bcfd6baf33f13b73a4fa4526cec912527ec86f3","scrolled":false,"trusted":true},"cell_type":"code","source":"%%time\ndf_list = [] # list to hold the batch dataframe\ni=0\n\nfor df_chunk in pd.read_csv(TRAIN_PATH, usecols=cols, dtype=traintypes, chunksize=chunksize):\n    \n    i = i+1\n    # Each chunk is a corresponding dataframe\n     \n    \n    # Neat trick from https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost\n    # Using parse_dates would be much slower!\n    df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n    df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n    \n    # Can process each chunk of dataframe here\n    # clean_data(), feature_engineer(),fit()\n    \n    # Alternatively, append the chunk to list and merge all\n    df_list.append(df_chunk) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aad1488798527942d4f873e498b7e160c5afa31","trusted":true},"cell_type":"code","source":"trainDF = pd.concat(df_list)\ntrainDF=trainDF.dropna()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"966e73700aa3b1b583a500ee98e81fd44b34af97","trusted":true},"cell_type":"code","source":"testDF =  pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6d556945cd07b4f8f4a5eb5c49288192afa8235","scrolled":true,"trusted":true},"cell_type":"code","source":"trainDF.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"719cd232818246ea4affa7a845e6248d5eeca188","trusted":true},"cell_type":"code","source":"testDF.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"690cea9b7dc72677fe88b3e1be8558eca196a896","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04bc9996789887e6ffb0f846c50bf045e231734f","trusted":true},"cell_type":"code","source":"y = trainDF[\"fare_amount\"]\n# y_test = testDF[\"fare_amount\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e62e98d143ff75afd2685ae3eed30eb7e292ca55","scrolled":true,"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5869d9cd0f07e6f40107015f2d755bf962e8889","scrolled":true,"trusted":true},"cell_type":"code","source":"trainDF.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8319315909d666464077bfbbe055afb9f60a501","trusted":true},"cell_type":"code","source":"X = trainDF[[  'pickup_longitude',\n       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n       'passenger_count']]\nX_pred = testDF[[  'pickup_longitude',\n       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n       'passenger_count']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a44d06ee2128ae0b6866eb5362ebec407c7f6f31","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88518d1b0018fa65c678196d77793d352dc4f309","trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d8b829aead75929504031c5b7b58937aa4b0d59"},"cell_type":"code","source":"lab_enc = preprocessing.LabelEncoder()\n\ny_train = lab_enc.fit_transform(y_train)\ny_test = lab_enc.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ddd772a4e09888dbd9cf17cc5f371ea575fd58a","scrolled":true,"trusted":true},"cell_type":"code","source":"\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bb8e94fb38c1a4a96290c296c2563e076aa86c1","scrolled":true,"trusted":true},"cell_type":"code","source":"# from xgboost import XGBRegressor\n\n# my_model = XGBRegressor()\n# Add silent=True to avoid printing out updates with each cycle\n# my_model.fit(X_train, y_train, verbose=False)\n# my_model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"893d9d112b6ea42c2631ae39ff439335a57caf1a","trusted":true},"cell_type":"code","source":"# testDF1 = testDF[[  'pickup_longitude',\n#        'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n#        'passenger_count']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13a882568cfad0186ffd833fa6f319cddbd124d2","trusted":true},"cell_type":"code","source":"y_pred = my_model.predict(X_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89010a7797a6b6ae74e51eca5275d45a67b2c8ef","trusted":true},"cell_type":"code","source":"y_pred.size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a870f1ebb89d216b3c15ae1c6d96fc8dffcb9d92","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"key\": testDF['key'],\n        \"fare_amount\": y_pred.round(3)\n})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a63563ad5af8ce2bbfc4240a3790ce7c1358bdca","trusted":true},"cell_type":"code","source":"submission.to_csv('submit.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fed29c45eb9d42aaf8d8818693919c526eb89e7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9235c21791e51263bc7648eab76e6c702559ffe0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}