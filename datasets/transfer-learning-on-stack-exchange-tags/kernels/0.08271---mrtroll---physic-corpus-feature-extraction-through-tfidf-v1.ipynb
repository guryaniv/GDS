{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e4b8ad13-7998-9476-11d4-69db6de1d2b8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from subprocess import check_output\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f5c98bad-801d-0d6c-937f-479400f19680"
      },
      "outputs": [],
      "source": [
        "physic = pd.read_csv(\"../input/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7c4159dc-aeaa-d1dc-3afb-f553b6457474"
      },
      "outputs": [],
      "source": [
        "physic.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bcff54e4-07ee-a085-ed1c-414f713617ae"
      },
      "outputs": [],
      "source": [
        "punctuations = string.punctuation\n",
        "\n",
        "def data_clean(data):\n",
        "    print('Cleaning data')\n",
        "    data = data.apply(lambda x: x.lower())\n",
        "    data = data.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
        "    data = data.apply(lambda x: re.sub(r'^\\W+|\\W+$',' ',x))\n",
        "    data = data.apply(lambda i: ''.join(i.strip(punctuations))  )\n",
        "    #print('tokenize')\n",
        "    data = data.apply(lambda x: word_tokenize(x))\n",
        "\n",
        "    #Select only the nouns\n",
        "    is_noun = lambda pos: pos[:2] == 'NN' \n",
        "    for i in range(len(data)):\n",
        "        data[i] = [word for (word, pos) in nltk.pos_tag(data[i]) if is_noun(pos)]\n",
        "    \n",
        "    #print('Lemmatizing')\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    data = data.apply(lambda x: [wordnet_lemmatizer.lemmatize(i) for i in x])\n",
        "    data = data.apply(lambda x: [i for i in x if len(i)>2])\n",
        "    return(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3040199a-6cce-106c-23e7-c3764ff82a2d"
      },
      "outputs": [],
      "source": [
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "30b27017-9530-31ea-ae24-90556f1efbd4"
      },
      "outputs": [],
      "source": [
        "def get_frequency(title):\n",
        "    \n",
        "    frequency = []\n",
        "    inverse_frequency = {}\n",
        "    for i in range(len(title)):\n",
        "        word_count = {}\n",
        "\n",
        "        for word in title[i]:\n",
        "            if word in word_count:    \n",
        "                word_count[word] = word_count[word] + 1\n",
        "            else:\n",
        "                word_count[word] = 1\n",
        "                \n",
        "        for word in word_count:\n",
        "            if word in inverse_frequency:\n",
        "                inverse_frequency[word] = inverse_frequency[word] + 1\n",
        "            else:\n",
        "                inverse_frequency[word] = 1            \n",
        "        frequency.append(word_count)\n",
        "        \n",
        "    return (frequency, inverse_frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7a8d6f6-af03-1da4-8c23-9453904ffa98"
      },
      "outputs": [],
      "source": [
        "title = data_clean(physic.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d53c0adb-3e29-42ba-502a-d9543659ba65"
      },
      "outputs": [],
      "source": [
        "frequency, inverse_frequency = get_frequency(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d4d92b7a-7bb0-7891-641e-f67218a7432a"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "frequency_words = {}\n",
        "for document in frequency:\n",
        "    for word in document:\n",
        "        if word in frequency_words:\n",
        "            frequency_words[word] = frequency_words[word] + document[word]\n",
        "        else:\n",
        "            frequency_words[word] = document[word]            \n",
        "frequency_words = sorted(frequency_words.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e4f63b4b-574d-ef17-fc28-19506995e5e2"
      },
      "outputs": [],
      "source": [
        "print('number of words:',len(frequency_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "25e03bc5-6f4c-6307-4467-eddedd0da90f"
      },
      "outputs": [],
      "source": [
        "plt.plot(frequency_words)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "449075bb-9cd8-d813-2fa0-2be8ac453ae7"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.log(frequency_words))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "26fa7009-73a5-6453-840a-960442ed3cff"
      },
      "outputs": [],
      "source": [
        "tfidf = frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "098c7900-8a71-47ea-3edb-cb6d6f464d18"
      },
      "outputs": [],
      "source": [
        "tfidf_distribution = []\n",
        "for document in tfidf:\n",
        "    if document == {}:\n",
        "        continue\n",
        "    max_frequency = sorted(document.items(), key=operator.itemgetter(1), reverse=True)[0][1]\n",
        "    for word in document:\n",
        "        document[word] = document[word]/(max_frequency + 0.0)*np.log(len(tfidf)/(inverse_frequency[word]+0.))\n",
        "        tfidf_distribution.append(document[word])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea725b10-595c-9c26-2a08-47dd590034b7"
      },
      "outputs": [],
      "source": [
        "index = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7f04a68-53ac-c21a-1bcf-5fc04caf5dad"
      },
      "outputs": [],
      "source": [
        "sorted(tfidf[index].items(), key=operator.itemgetter(1), reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f4fed8b-fd4a-782a-7602-8561ce9db78e"
      },
      "outputs": [],
      "source": [
        "print(physic.title[index])\n",
        "print(physic.content[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "92fb7ca9-a343-8844-9443-732c26f6ef65"
      },
      "outputs": [],
      "source": [
        "tfidf_distribution = sorted(tfidf_distribution)\n",
        "print(len(tfidf_distribution))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "99a25076-4f6b-e856-c218-985797ec05ff"
      },
      "outputs": [],
      "source": [
        "plt.plot(tfidf_distribution)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "61686d8c-c0cc-cc1b-5b65-37d2ea198521"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.log(tfidf_distribution))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b185bf6e-9d2e-4251-4b79-074e5a5c0e40"
      },
      "outputs": [],
      "source": [
        "top = 8\n",
        "output = []\n",
        "for i in range(0,len(physic)):\n",
        "    prediction = sorted(tfidf[i], key=tfidf[i].get, reverse=True)[0:top]\n",
        "    output.append([physic.id[i], ' '.join(prediction)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7628e338-8c3a-763f-c2e0-fcd52ff23f0f"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(data=output,columns = ['id','tags']).to_csv('Submission.csv', index=False)       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4460b461-ded3-edac-110c-b8ed7e250ed5"
      },
      "source": [
        "This is my first try, i'm going to try another techniques in order to increase the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c45e6872-fb4c-f5ca-3144-2ca82c4d6254",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}