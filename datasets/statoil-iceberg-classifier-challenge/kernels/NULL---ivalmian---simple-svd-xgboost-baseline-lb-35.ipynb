{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "mimetype": "text/x-python", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {}, "source": ["We aim to use svd to dimensionally reduce the images to just a few features (20 per image in this particular case), which works nicely as it turns out most of variance in the image is just noise. LB should be aroun ~0.35, but I've played a bit with xgb parameters and the crossvalidation improved, this suggest that perhaps you can get lower LB if you submit this notebook. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "e6b43e7b7d786bc565deb7c52acd78d2885d9837", "_cell_guid": "08e02f57-9c97-487a-ab0d-b2afad161d53", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["#load with pandas, manipulate with numpy, plot with matplotlib\n", "import numpy as np \n", "import pandas as pd \n", "import matplotlib.pyplot as plt\n", "\n", "#ML - we will classify using a naive xgb with stratified cross validation\n", "import xgboost as xgb\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.metrics import log_loss\n", "\n", "\n", "\n"], "cell_type": "code"}, {"metadata": {"_uuid": "027ce3e753ae41fbe6600369f6dd38047e1fdd59", "_cell_guid": "232aecf4-a711-415c-8b23-d277824788c3", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["#filenames\n", "inputFolder = \"../input/\"\n", "trainSet = 'train.json'\n", "testSet = 'test.json'\n", "subName = 'iceberg-svd-xgb-3fold.csv'\n"], "cell_type": "code"}, {"metadata": {"_uuid": "f52fbe147c7723767b3f03200c9746ba5abdaae5", "_cell_guid": "2d055ac8-6c5d-4bd3-8f93-c981379fe57a", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["#load data\n", "trainDF = pd.read_json(inputFolder+trainSet)\n", "testDF = pd.read_json(inputFolder+testSet)"], "cell_type": "code"}, {"metadata": {"_uuid": "7f08caa08f99b025488c086b4a610333e306d087", "_cell_guid": "a2ff70e4-e630-419b-b6b3-9a8be46a7014", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["#get numpy arrays for train/test data, prob there is a more pythonic approach\n", "band1 = trainDF['band_1'].values\n", "im1 = np.zeros((len(band1),len(band1[0])))\n", "for j in range(len(band1)):\n", "    im1[j,:]=np.asarray(band1[j])\n", "    \n", "band2 = trainDF['band_2'].values\n", "im2 = np.zeros((len(band2),len(band2[0])))\n", "for j in range(len(band2)):\n", "    im2[j,:]=np.asarray(band2[j])\n", "    \n", "#get numpy array for test data\n", "band1test = testDF['band_1'].values\n", "im1test = np.zeros((len(band1test),len(band1test[0])))\n", "for j in range(len(band1test)):\n", "    im1test[j,:]=np.asarray(band1test[j])\n", "    \n", "band2test = testDF['band_2'].values\n", "im2test = np.zeros((len(band2test),len(band2test[0])))\n", "for j in range(len(band2test)):\n", "    im2test[j,:]=np.asarray(band2test[j])"], "cell_type": "code"}, {"metadata": {"_uuid": "78e22d6cc95e32ddbcbbf4d07dc7138b7b81e978", "_cell_guid": "eec13fca-3462-46e8-8f1e-c462f1c90d99", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["#svd of the two bands\n", "U1,s1,V1 = np.linalg.svd(im1,full_matrices = 0)\n", "U2,s2,V2 = np.linalg.svd(im2,full_matrices = 0)"], "cell_type": "code"}, {"metadata": {"_uuid": "785d0669746961b99c6eb84a50d57b0965dbd328", "_cell_guid": "36b32c28-2701-4840-96e7-0f3b0ed1f2e0"}, "execution_count": null, "outputs": [], "source": ["#fraction of variance explained in the first 100 modes of train data.\n", "#note band 2 is somehow much more dependent on the first svd mode than band 1\n", "\n", "plt.figure()\n", "\n", "frac1 = np.cumsum(s1)/np.sum(s1)\n", "frac2 = np.cumsum(s2)/np.sum(s2)\n", "\n", "plt.plot(frac1[:100])\n", "plt.plot(frac2[:100],'r')\n", "\n"], "cell_type": "code"}, {"metadata": {"_uuid": "e8e87c37e83b9be919a0f98a8ccda59495466ee8", "_cell_guid": "28f0bea0-ad1e-4dac-be86-f76096aeab2f"}, "execution_count": null, "outputs": [], "source": ["#original \n", "\n", "fig, ax = plt.subplots(2,3)\n", "plt.suptitle('original')\n", "ax[0,0].imshow(np.reshape(im2[0,:],(75,75)))\n", "\n", "ax[0,1].imshow(np.reshape(im2[1,:],(75,75)))\n", "ax[0,2].imshow(np.reshape(im2[2,:],(75,75)))\n", "ax[1,0].imshow(np.reshape(im2[3,:],(75,75)))\n", "ax[1,1].imshow(np.reshape(im2[4,:],(75,75)))\n", "ax[1,2].imshow(np.reshape(im2[5,:],(75,75)))\n", "\n", "#first 100 modes (only 1/16th total modes, ~35% of variance)\n", "\n", "nmodes = 100\n", "\n", "im1p=np.dot(np.dot(U1[:,:nmodes],np.diag(s1[:nmodes])),V1[:nmodes,])\n", "im2p=np.dot(np.dot(U2[:,:nmodes],np.diag(s2[:nmodes])),V2[:nmodes,])\n", "\n", "fig, ax = plt.subplots(2,3)\n", "plt.suptitle('first 100 modes')\n", "ax[0,0].imshow(np.reshape(im2p[0,:],(75,75)))\n", "\n", "ax[0,1].imshow(np.reshape(im2p[1,:],(75,75)))\n", "ax[0,2].imshow(np.reshape(im2p[2,:],(75,75)))\n", "ax[1,0].imshow(np.reshape(im2p[3,:],(75,75)))\n", "ax[1,1].imshow(np.reshape(im2p[4,:],(75,75)))\n", "ax[1,2].imshow(np.reshape(im2p[5,:],(75,75)))\n", "\n", "#first 20 modes (~27% of variance explained)\n", "\n", "nmodes = 20\n", "\n", "im1p=np.dot(np.dot(U1[:,:nmodes],np.diag(s1[:nmodes])),V1[:nmodes,])\n", "im2p=np.dot(np.dot(U2[:,:nmodes],np.diag(s2[:nmodes])),V2[:nmodes,])\n", "\n", "fig, ax = plt.subplots(2,3)\n", "\n", "plt.suptitle('first 20 modes')\n", "ax[0,0].imshow(np.reshape(im2p[0,:],(75,75)))\n", "\n", "ax[0,1].imshow(np.reshape(im2p[1,:],(75,75)))\n", "ax[0,2].imshow(np.reshape(im2p[2,:],(75,75)))\n", "ax[1,0].imshow(np.reshape(im2p[3,:],(75,75)))\n", "ax[1,1].imshow(np.reshape(im2p[4,:],(75,75)))\n", "ax[1,2].imshow(np.reshape(im2p[5,:],(75,75)))"], "cell_type": "code"}, {"metadata": {"_uuid": "8f769429220e7b3b4cb252da26dc75d308bf1404", "_cell_guid": "8907b35b-6d4a-4b52-9c64-7ef47f604809", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["# OK, so first 20 modes (20 numbers per image) have most of useful information, \n", "# as most of variance is just noise. Let's run a simple xgboost classifier\n", "\n", "#transofrm test data\n", "U1test=np.dot(np.dot(im1test,V1.T),np.diag(1/s1))\n", "U2test=np.dot(np.dot(im2test,V2.T),np.diag(1/s2))\n", "\n", "nmodes = 20\n", "\n", "X = np.hstack((U1[:,:nmodes],U2[:,:nmodes]))\n", "X_test = np.hstack((U1test[:,:nmodes],U2test[:,:nmodes]))\n", "y = trainDF['is_iceberg'].values"], "cell_type": "code"}, {"metadata": {"_uuid": "581325179817744f640cbad47b040c4dc282552a", "_cell_guid": "a1569666-7286-43af-b31e-842c9dbb8795", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["#is there a native xgb way of doing it?\n", "def logloss_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    score = log_loss(labels, preds)\n", "    return 'logloss', score"], "cell_type": "code"}, {"metadata": {"_uuid": "b1f4522cbe45692088e1fe1d817f1db387cddaee", "_cell_guid": "db22c09b-3f05-4cab-b65c-98b96ef2a2c5"}, "execution_count": null, "outputs": [], "source": ["nfolds = 3;\n", "xgb_mdl=[None]*nfolds\n", "\n", "\n", "xgb_params = {\n", "        'objective': 'binary:logistic',\n", "        'n_estimators':1000,\n", "        'max_depth': 8,\n", "        'subsample': 0.9,\n", "        'colsample_bytree': 0.9 ,\n", "     #   'max_delta_step': 1,\n", "     #   'min_child_weight': 10,\n", "        'eta': 0.01,\n", "      #  'gamma': 0.5\n", "        }\n", "\n", "\n", "folds = list(StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=2016).split(X, y))\n", "\n", "d_test = xgb.DMatrix(X_test)\n", "\n", "preds = np.zeros((X_test.shape[0],nfolds))\n", "\n", "for j, (train_idx, valid_idx) in enumerate(folds):\n", "    X_train = X[train_idx]\n", "    y_train = y[train_idx]\n", "    \n", "    X_valid = X[valid_idx]\n", "    y_valid = y[valid_idx]\n", "    \n", "    d_train =  xgb.DMatrix(X_train,label=y_train)\n", "    d_valid =  xgb.DMatrix(X_valid,label=y_valid)\n", "    \n", "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "    \n", "    xgb_mdl[j]=xgb.train(\n", "            xgb_params, \n", "            d_train, \n", "            1600, watchlist, \n", "            early_stopping_rounds=70, \n", "            feval=logloss_xgb, \n", "            maximize=False, \n", "            verbose_eval=100)\n", "    preds[:,j] = xgb_mdl[j].predict(d_test)"], "cell_type": "code"}, {"metadata": {"_uuid": "ee960d6bb03549e9b7956715ebd3a670af600a8e", "_cell_guid": "1fe79978-e5fa-4e5d-9192-9b309ae12e45", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["y_pred = np.mean(preds,axis=1)\n", "sub = pd.DataFrame()\n", "sub['id'] = testDF['id']\n", "sub['is_iceberg'] = y_pred\n", "sub.to_csv(subName, index=False)\n"], "cell_type": "code"}]}