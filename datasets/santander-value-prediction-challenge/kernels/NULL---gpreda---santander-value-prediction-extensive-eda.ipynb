{"cells":[{"metadata":{"_uuid":"7aabe34fcf9c5698461b8743fb88d8f35d9964e1"},"cell_type":"markdown","source":"\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b8/Banco_Santander_Logotipo.svg\" width=\"800\"></img>\n\n<h1><center><font size=\"6\">Santander Value Prediction Extensive EDA</font></center></h1>\n\n\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Load packages</a>  \n- <a href='#3'>Read the data</a>  \n- <a href='#4'>Check the data</a>\n    - <a href='#41'>Glimpse the data</a>  \n    - <a href='#42'>Check missing data</a>  \n    - <a href='#43'>Check data sparsity</a>\n- <a href='#5'>Data exploration</a>\n    - <a href='#51'>Features type</a>\n    - <a href='#52'>Data sparsity per column type</a>\n    - <a href='#521'>Constant columns</a>  \n    - <a href='#53'>Target variable</a>  \n    - <a href='#54'>Distribution of non-zeros per row</a>  \n    - <a href='#55'>Distribution of non-zeros per column</a>  \n    - <a href='#56'>Float features</a>  \n    - <a href='#57'>Integer features</a>      \n    - <a href='#58'>Highly correlated features</a>  \n- <a href='#6'>Model</a>    \n- <a href='#7'>Submission</a>\n- <a href='#8'>References</a>"},{"metadata":{"_uuid":"5acc8159029b94c25b941ebdb4b0b9411de2fd69"},"cell_type":"markdown","source":"# <a id=\"1\">Introduction</a>  \n\nSantander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to **determine the amount or value of the customer's transaction**. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.\n\nIn this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.\n\n**Note**: This Kernel was made before Giba published his findings about the leak, which changed dramatically this competition.\n\n**Late note**: Congratulation to Giba for winning 1st place in this Competition!\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>"},{"metadata":{"_uuid":"d5c67a45ff7927350c847c4b49b231e6cf5f86c7"},"cell_type":"markdown","source":"# <a id=\"2\">Load packages</a>"},{"metadata":{"_uuid":"383770d81eea75c4d71d3978497c4edd618afc2e","trusted":false,"collapsed":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom math import pi\nimport seaborn as sns\n%matplotlib inline \n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Print all rows and columns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nIS_LOCAL = False\n\nimport os\n\nif(IS_LOCAL):\n    PATH=\"../input/santander-value-prediction-challenge\"\nelse:\n    PATH=\"../input\"\nprint(os.listdir(PATH))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fe4c4e8b3c2f488f24ef89f06a396149cf6641f"},"cell_type":"markdown","source":"# <a id=\"3\">Read the data</a>"},{"metadata":{"_uuid":"86b330255ac7a7aec64df554035872be63ce6a94","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df = pd.read_csv(PATH+\"/train.csv\")\ntest_df = pd.read_csv(PATH+\"/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8423d18690442a3340af903d304e49f078eb62da"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>\n\n# <a id=\"4\">Check the data</a>"},{"metadata":{"_uuid":"74c1551e512810f2116b1f7e9bb5196e5dee3a3c","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Santander Value Prediction Challenge train -  rows:\",train_df.shape[0],\" columns:\", train_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49c33c56ee1e87627a8d8393f6dbfa7ca919b72a"},"cell_type":"markdown","source":"There are **4459** data rows and **4993** columns."},{"metadata":{"_uuid":"d733d05eaeb18f6e394afc4da2d94529dbd98cfd","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Santander Value Prediction Challenge test -  rows:\",test_df.shape[0],\" columns:\", test_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bac18eb540e6189110a07350571d590dc5a8a5a"},"cell_type":"markdown","source":"The schema dataset contains **49342** rows - and **4992** columns (target column missing).\n\n\nThere are few observations that we can already make:\n* The column number exceeds the rows number for the train data.  \n* The test data is containing almost 10 times more data than the train data.  \n\n\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>"},{"metadata":{"_uuid":"e24517b12a692ece5990e1154debece0c8031b00"},"cell_type":"markdown","source":"## <a id=\"41\">Glimpse the data</a>\n\nWe start by looking to the data features (first 5 rows)."},{"metadata":{"_uuid":"864dbba32dd49c86ad16a00e2b9f68e407b60f93","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6355c11d3bfa881bb4579fc2e33a13666b267c89"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86cbf50880c376fa396da650c983c9bb5d53f249"},"cell_type":"markdown","source":"The columns in the train data are as following: \n\n* **ID**: we will have to check if the ID is in anyway connected with the column names; these are hexa numbers with 9 digits.\n* **target**: this is the *target* variable and has numeric (real) values;  \n* 4991 columns with names anonymized - there are hexa large numbers with 9 digits.  Most of the columns have 0 values, the dataset is sparse. The columns types seems to be integers and reals.\n\n\nTest data has the same columns, without **target**.\n"},{"metadata":{"_uuid":"f30dc4013282ca676f9421efb05b2a778738d7ce"},"cell_type":"markdown","source":"## <a id=\"42\">Check missing data</a>  \n\nLet's check the missing data for train set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e75311ea5976249d5f70a891b334426a543d4150"},"cell_type":"code","source":"def check_nulls(df):\n    nulls = df.isnull().sum(axis=0).reset_index()\n    nulls.columns = ['column', 'missing']\n    nulls = nulls[nulls['missing']>0]\n    nulls = nulls.sort_values(by='missing')\n    return nulls    \n\n\ncheck_nulls(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b95477e6b22099d757413916288932a6de3304f9"},"cell_type":"markdown","source":"There are no missing data in the train set.  \n\nLet's check the missing data for test set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ca38a5160a57e997b65a453a94506933c76724d2"},"cell_type":"code","source":"check_nulls(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7bd779e257b612bc92f186bfea6c94f68dd091e"},"cell_type":"markdown","source":"There are no missing data in the test set either.\n\n\n\n## <a id=\"43\">Check data sparsity</a>  \n\nLet's check the data sparsity for train set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"eb65d5c16f6aea35c2ba1e96a1891aee7311a4b6"},"cell_type":"code","source":"def check_sparsity(df):\n    non_zeros = (df.ne(0).sum(axis=1)).sum()\n    total = df.shape[1]*df.shape[0]\n    zeros = total - non_zeros\n    sparsity = round(zeros / total * 100,2)\n    density = round(non_zeros / total * 100,2)\n\n    print(\" Total:\",total,\"\\n Zeros:\", zeros, \"\\n Sparsity [%]: \", sparsity, \"\\n Density [%]: \", density)\n    return density\n\nd1 = check_sparsity(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c4b6350020314de09af69132c18afa7cf3abe47"},"cell_type":"markdown","source":"Let's check the data sparsity for test set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fe6fd3e226f392be2f3b62fce51564751fef4a8a"},"cell_type":"code","source":"d2 = check_sparsity(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77f4920f5dc4a14b72ceb1e1be4062b390c2583e"},"cell_type":"markdown","source":"One important observation is that the data sparsity is slightly larger for the test set than for the train set (density is more than double for the train set). We will look into more details about the data distribution in the following section.\n\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>"},{"metadata":{"_uuid":"79faf877f1b914088462da9758877e00f58737fc"},"cell_type":"markdown","source":"# <a id=\"5\">Data exploration</a>\n\n##  <a id=\"51\">Features type</a>\n\n\nLet's check the features type in the data."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"60b605ca2183f4b81d24db22011e5e4b6ea7410b"},"cell_type":"code","source":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a6cf3ced3a3970f1a44e0f7153f0ce23e57ceff"},"cell_type":"markdown","source":"There are 3147 integer features, 1845 float values and one non-numeric value (the ID field).  \n\nLet's save the **metadata** for the columns. For each feature we set the metadata for **role**, if we will use the feature - **keep** and the **dtype**. "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"959521a5e6ca2b5f58cd0abaed1752bfa91202e2"},"cell_type":"code","source":"data = []\nfor feature in train_df.columns:\n    # Defining the role\n    if feature == 'target':\n        use = 'target'\n    elif feature == 'ID':\n        use = 'id'\n    else:\n        use = 'input'\n         \n        \n    # Initialize keep to True for all variables except for `ID`\n    keep = True\n    if feature == 'ID':\n        keep = False\n    \n    # Defining the data type \n    dtype = train_df[feature].dtype\n    \n    \n    \n    # Creating a Dict that contains all the metadata for the variable\n    feature_dictionary = {\n        'varname': feature,\n        'use': use,\n        'keep': keep,\n        'dtype': dtype,\n    }\n    data.append(feature_dictionary)\n    \n# Create the metadata\nmetadata = pd.DataFrame(data, columns=['varname', 'use', 'keep', 'dtype'])\nmetadata.set_index('varname', inplace=True)\n\n# Sample the metadata\nmetadata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a369768929cc72f29e2aedf0afae0871f6dc3edc"},"cell_type":"markdown","source":"Let's check that we have the expected distribution of the dtype in the metadata (as identified before)."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b4d160d47d53af27c97737475f9edca6ef9c4b78"},"cell_type":"code","source":"pd.DataFrame({'count' : metadata.groupby(['dtype'])['dtype'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ec058bb7823a4d52d126865fc92f5914ebacb8e"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id=\"52\">Data sparsity per column type</a>\n\n\n### Integer type\n\nFor train data:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"137a32d60c29b069497431ea3d7d1e532f765ca3"},"cell_type":"code","source":"int_data = []\nvar = metadata[(metadata.dtype == 'int64') & (metadata.use == 'input')].index\nd3 = check_sparsity(train_df[var])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd1204f110301765824605639e1f0434bc5be632"},"cell_type":"markdown","source":"For test data:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"467b4f0aa7d31140247e9d6f4bfac6fd1f4c764d"},"cell_type":"code","source":"d4 = check_sparsity(test_df[var])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"789b433cfd7b31567df5d90442260b55f6cb9c6d"},"cell_type":"markdown","source":"### Float type\n\nFor train data:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cd4e99b43c515798e68c7b3ff33f3a720bdb5676"},"cell_type":"code","source":"var = metadata[(metadata.dtype == 'float64') & (metadata.use == 'input')].index\nd5 = check_sparsity(train_df[var])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d5ec7a3a0140313ed0b1e92fdb675c55c42e215"},"cell_type":"markdown","source":"For test data:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7cc68e6e288c382b6314e98532054c67f04f5ce2"},"cell_type":"code","source":"d6 = check_sparsity(test_df[var])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c483c35d45fc13d89e6b81b700d57790e6ab017"},"cell_type":"markdown","source":"Let's put together all these data and compare them. For convenience, we will compare the densities."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"77ba53213a1b47e0f99edcbfb292694306aa02f5"},"cell_type":"code","source":"data = {'Dataset': ['Train', 'Test'], 'All': [d1, d2], 'Integer': [d3,d4], 'Float': [d5,d6]}\n    \ndensity_data = pd.DataFrame(data)\ndensity_data.set_index('Dataset', inplace=True)\ndensity_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00f1146f3b5fecf2109152861af2311fbc71c72a"},"cell_type":"markdown","source":"The data density for train set is 2.23 times larger than for the test set.   \n\nAs well, the train float data density is 5.7 times larger than the train integer data density.  \nThe test float data density is 2.87 time larger than test integer data density.  \n\nIn general density is larger in train set than in test set and in float data than in integer data."},{"metadata":{"_uuid":"5028a761d87b6559d1e7d03b6ba1694a07459ccb"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"521\">Constant columns</a>\n\nLet's check if there are constant columns."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6845f7be35620ab178d061bd81ace13dbe338827"},"cell_type":"code","source":"# check constant columns\ncolsConstant = []\ncolumnsList = [x for x in train_df.columns if not x in ['ID','target']]\n\nfor col in columnsList:\n    if train_df[col].std() == 0: \n        colsConstant.append(col)\nprint(\"There are\", len(colsConstant), \"constant columns in the train set.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea324236df6d07b74eb3bcb10999bcc0e22a0ade"},"cell_type":"markdown","source":"Let's mark all these columns to drop."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2aebd4959335cc2c93eca1daf79f8b3f5f63406e"},"cell_type":"code","source":"metadata['keep'].loc[colsConstant] = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4fa956844d97a6e70f2310d7a36028dba7ae0ff"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"53\">Target variable</a>\n\n\nLet's check the target variable distribution."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e62ffe990d4f99ba6ef48fd98885f53fe0eca227"},"cell_type":"code","source":"# Plot distribution of one feature\ndef plot_distribution(df,feature,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % feature)\n    sns.distplot(df[feature].dropna(),color=color, kde=True,bins=100)\n    plt.show()   \n    \nplot_distribution(train_df, \"target\", \"blue\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09c1e6ed3ff481f703385db83bfaaacce1b23330"},"cell_type":"markdown","source":"Let's check the distribution of log(target)."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4fbf7434f4ab98d2ae67de21b446a8ac8053e0bd"},"cell_type":"code","source":"def plot_log_distribution(df,feature,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % feature)\n    sns.distplot(np.log1p(df[feature]).dropna(),color=color, kde=True,bins=100)\n    plt.title(\"Distribution of log(target)\")\n    plt.show()   \n\nplot_log_distribution(train_df, \"target\", \"green\")  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c09ac871aaa778f96561bb73a85c17153b12a5d7"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"54\">Distribution of non-zero features values per row</a>\n\nLet's check what is the distribution of non-zero features values per row in the train set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6b0e854c53015a4cfe335c9017e5fdfb1d4f5465"},"cell_type":"code","source":"non_zeros = (train_df.ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - train set\")\nsns.distplot(np.log1p(non_zeros),color=\"red\", kde=True,bins=100)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd69cbea876b8fa570e039d3fcf2e87b789302fd"},"cell_type":"markdown","source":"Let's check distribution of non-zero features values per row in the test set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"804cc5d6d4343d6a5169c5b5288741a74713bf7d"},"cell_type":"code","source":"non_zeros = (test_df.ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - test set\")\nsns.distplot(np.log1p(non_zeros),color=\"magenta\", kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb657cc9905e364ed8471a00d8e5fb507f29b4cf"},"cell_type":"markdown","source":"Let's separate only the **real** values, excepting the **target**. And let's represent the distribution of non-zero features values only for these.\n\n### Distribution of non-zeros for float type features"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ef9bc2c9f81a25847bfaee6caf80744c65e33c7d"},"cell_type":"code","source":"var = metadata[(metadata.dtype == 'float64') & (metadata.use == 'input')].index\nnon_zeros = (train_df[var].ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - floats only - train set\")\nsns.distplot(np.log1p(non_zeros),color=\"green\", kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2e33e4d8db3332adad880c283c6e8576c2716943"},"cell_type":"code","source":"non_zeros = (test_df[var].ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - floats only - test set\")\nsns.distplot(np.log1p(non_zeros),color=\"blue\", kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f298d71a798f5062df95c2874e410ea5f60cdef4"},"cell_type":"markdown","source":"### Distribution of non-zeros for integer type features"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ce935a4da9f73b2040abde68625aba63450a40e9"},"cell_type":"code","source":"var = metadata[(metadata.dtype == 'int64') & (metadata.use == 'input')].index\nnon_zeros = (train_df[var].ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - integers only -  train set\")\nsns.distplot(np.log1p(non_zeros),color=\"yellow\", kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b3910ba9566aba51537c83076c59e93997760aa1"},"cell_type":"code","source":"non_zeros = (test_df[var].ne(0).sum(axis=1))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per row) - integers only - train set\")\nsns.distplot(np.log1p(non_zeros),color=\"cyan\", kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38641242258e49873b6d4d10dbd9dc865cc3f090"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>\n\n\n##  <a id=\"55\">Distribution of non-zero features values per column</a>\n\nLet's check what is the distribition of non-zero features values per column in the train set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"76229a691db92960b7dfc38d8a3ddb6448e0d80b"},"cell_type":"code","source":"non_zeros = (train_df.ne(0).sum(axis=0))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per column) - train set\")\nsns.distplot(np.log1p(non_zeros),color=\"darkblue\", kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"816168e05dfcfa4ba58125bd54e9ab73e5221bb7"},"cell_type":"markdown","source":"Let's check distribution of non-zero features values per row in the test set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cd186eca7dece3b466cece9786651666b907d4f0"},"cell_type":"code","source":"non_zeros = (test_df.ne(0).sum(axis=0))\n\nplt.figure(figsize=(10,6))\nplt.title(\"Distribution of log(number of non-zeros per column) - test set\")\nsns.distplot(np.log1p(non_zeros),color=\"darkgreen\", kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b2a1149ba7c6be1e8d4d120e90281337212fb27"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"56\">Float features</a>\n\nLet's see now  the distribution of the sum of float features values per column."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"78fdb79fe4800f9d11c619008ad3b10bc88062ba"},"cell_type":"code","source":"var = metadata[(metadata.dtype == 'float64') & (metadata.use == 'input')].index\nval = train_df[var].sum()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\nsns.boxplot(val, palette=\"Blues\",  showfliers=False,ax=ax1)\nsns.boxplot(val, palette=\"Greens\",  showfliers=True,ax=ax2)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10777393c4bbf68ced46c7e51c53e89c372ded24"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"57\">Integer features</a>\n\nLet's see now  the distribution of the sum of integer features values per column."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"00ace647d65dd5f3adc804cdc7021bab72e9dac2"},"cell_type":"code","source":"var = metadata[(metadata.dtype == 'int64') & (metadata.use == 'input')].index\nval = train_df[var].sum()\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\nsns.boxplot(val, palette=\"Reds\",  showfliers=False,ax=ax1)\nsns.boxplot(val, palette=\"Blues\",  showfliers=True,ax=ax2)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d6a27275fc66a8a1aa54b1a348767e193791c2e"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n##  <a id=\"58\">Highly correlated features</a>\n\n\nWe use a code snapshot from <a href=\"#7\">[1]</a> to extract the features that are highly correlated with **target** feature. We select only the features correlated or inverse correlated with **target** and having a corrlelation coefficient \n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"40594e1aa8f1dbadf894c6ab177c3edc04171015"},"cell_type":"code","source":"labels = []\nvalues = []\nfor col in train_df.columns:\n    if col not in [\"ID\", \"target\"]:\n        labels.append(col)\n        values.append(np.corrcoef(train_df[col].values, train_df[\"target\"].values)[0,1])\ncorr_df = pd.DataFrame({'columns_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n \ncorr_df = corr_df[(corr_df['corr_values']>0.25) | (corr_df['corr_values']<-0.25)]\nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,6))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='gold')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.columns_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d6a322fa97e13ec0feebdc9df5dafc08af50938"},"cell_type":"markdown","source":"Let's represent the correlation map between these selected features."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4c40e6ac0db4f14a1cba36ad6ccadfb0538daa63"},"cell_type":"code","source":"temp_df = train_df[corr_df.columns_labels.tolist()]\ncorrmat = temp_df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 12))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True, cmap=\"YlOrRd\")\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52debfb8a4f2a1acd69bcf57e7340c84bb63a1e6"},"cell_type":"markdown","source":"Let's represent, for the highly correlated features, the distribution in the train and test set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d39d0e1d2974a2bc16a51764bb87273b60179843"},"cell_type":"code","source":"corrmat","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2f3740f4739ed9c6f23e557b0ef0b0d561ae5b65"},"cell_type":"code","source":"var = temp_df.columns.values\n\ni = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(5,4,figsize=(12,15))\n\nfor feature in var:\n    i += 1\n    plt.subplot(5,4,i)\n    sns.kdeplot(train_df[feature], bw=0.5,label=\"train\")\n    sns.kdeplot(test_df[feature], bw=0.5,label=\"test\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25193cf24a474aee49e63d0cef899e32da47dbf4"},"cell_type":"markdown","source":"Let's represent the relationship between two of the highest correlated features ('429687d5a';'e4159c59e') and ('6b119d8ce';'e8d9394a0')."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b79f58573502794dd26a4a4a9187716bdd6a1e9a"},"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure()\ns = sns.lmplot(x='429687d5a', y='e4159c59e',data=train_df, fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='6b119d8ce', y='e8d9394a0',data=train_df, fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='cbbc9c431', y='f296082ec',data=train_df, fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='cbbc9c431', y='51707c671',data=train_df, fit_reg=True,scatter_kws={'s':2})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8603e7bd28f78ab88f4e9ab73b14dc53a41a143b"},"cell_type":"markdown","source":"\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>"},{"metadata":{"_uuid":"b493fb1fd1bb0e407b1ae335f6c545a81ade5e83"},"cell_type":"markdown","source":"# <a id=\"6\">Model</a>  \n\nLet's create now a model. We start by droping the duplicate columns.\n\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"534e693a1bed609be24a76809e0780e6a6a2ce0d"},"cell_type":"code","source":"var = metadata[(metadata.keep == False) & (metadata.use == 'input')].index\ntrain_df.drop(var, axis=1, inplace=True)  \ntest_df.drop(var, axis=1, inplace=True)  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"373f64bfc991d86e96914af315bb565d43775527"},"cell_type":"markdown","source":"Let's check the shape of train and test set after droping the columns."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8ef097f1e036843abd64140eb4177e8115c9a7cc"},"cell_type":"code","source":"print(\"Santander Value Prediction Challenge train -  rows:\",train_df.shape[0],\" columns:\", train_df.shape[1])\nprint(\"Santander Value Prediction Challenge test -  rows:\",test_df.shape[0],\" columns:\", test_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"454dd71995ded8c5527975de9cd46b34660efd7b"},"cell_type":"markdown","source":"Let's add few statistical features. But before let's replace all 0s with NAs."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3b806eecb051b3726d781055439f70c8a8b56053"},"cell_type":"code","source":"# Replace 0 with NAs\ntrain_df.replace(0, np.nan, inplace=True)\ntest_df.replace(0, np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4b96a6e105f6d906a9ed4e505fbf0052f82d6b2c"},"cell_type":"code","source":"all_features = [f for f in train_df.columns if f not in ['target', 'ID']]\nfor df in [train_df, test_df]:\n    df['nans'] = df[all_features].isnull().sum(axis=1)\n    # All of the stats will be computed without the 0s \n    df['median'] = df[all_features].median(axis=1)\n    df['mean'] = df[all_features].mean(axis=1)\n    df['sum'] = df[all_features].sum(axis=1)\n    df['std'] = df[all_features].std(axis=1)\n    df['kurtosis'] = df[all_features].kurtosis(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2947a751bda413718c631c46cb31c332275750a9"},"cell_type":"markdown","source":"We include only the selected input features to keep and the new statistical features calculated. "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"845f3eec0f9c68c696a812ff296c6ce79e0bc4fd"},"cell_type":"code","source":"features = all_features + ['nans', 'median', 'mean', 'sum', 'std', 'kurtosis']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1676d206d1f0000f4e42e8e81c931f85f5566f2b"},"cell_type":"markdown","source":"Let's check again the shape."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"45bcd7d9862e6c38ec9b642924c21c2ae6981e45"},"cell_type":"code","source":"print(\"Santander Value Prediction Challenge train -  rows:\",train_df.shape[0],\" columns:\", train_df.shape[1])\nprint(\"Santander Value Prediction Challenge test -  rows:\",test_df.shape[0],\" columns:\", test_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e8f22ff4ffb7518d98afbb5af542880f4b04c65"},"cell_type":"markdown","source":"We create the split with 5 folds. We build the model for training and we init the predictions."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"6fa88a20830d6f6ce73b399acd6bb78b4626ef47"},"cell_type":"code","source":"# Create folds\nfolds = KFold(n_splits=5, shuffle=True, random_state=1)\n\n\n# Convert to lightgbm Dataset\ndtrain = lgb.Dataset(data=train_df[features], label=np.log1p(train_df['target']), free_raw_data=False)\n# Construct dataset so that we can use slice()\ndtrain.construct()\n# Init predictions\nsub_preds = np.zeros(test_df.shape[0])\noof_preds = np.zeros(train_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca9285b615b2585b31bb9e00aa936aa6f7f4fe08"},"cell_type":"markdown","source":"Let's add the lgb parameters."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"fcdb92305a9793740ceb33be3ee1d42fc87c7fe1"},"cell_type":"code","source":"lgb_params = {\n    'objective': 'regression',\n    'num_leaves': 100,\n    'subsample': 0.8,\n    'colsample_bytree': 0.75,\n    'verbose': -1,\n    'seed': 2018,\n    'boosting_type': 'gbdt',\n    'max_depth': 10,\n    'learning_rate': 0.04,\n    'metric': 'l2',\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"586c13746c103b1d525156a791977bbbf05c9bb2"},"cell_type":"markdown","source":"Train and fit the lgb model with 5 folds. Then calculate the Full Out-Of-Fold score according to <a href=\"#8\">[3]</font></a>."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c25504321d16730aa13751927970db23f513889c"},"cell_type":"code","source":"# Run KFold\nfor trn_idx, val_idx in folds.split(train_df):\n    # Train lightgbm\n    clf = lgb.train(\n        params=lgb_params,\n        train_set=dtrain.subset(trn_idx),\n        valid_sets=dtrain.subset(val_idx),\n        num_boost_round=10000, \n        early_stopping_rounds=100,\n        verbose_eval=50\n    )\n    # Predict Out Of Fold and Test targets\n    # Using lgb.train, predict will automatically select the best round for prediction\n    oof_preds[val_idx] = clf.predict(dtrain.data.iloc[val_idx])\n    sub_preds += clf.predict(test_df[features]) / folds.n_splits\n    # Display current fold score\n    print('Current fold score : %9.6f' % mean_squared_error(np.log1p(train_df['target'].iloc[val_idx]), \n                             oof_preds[val_idx]) ** .5)\n    \n# Display Full OOF score (square root of a sum is not the sum of square roots)\nprint('Full Out-Of-Fold score : %9.6f' \n      % (mean_squared_error(np.log1p(train_df['target']), oof_preds) ** .5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f5bd65f87530532947aa8c080740bfd28699040"},"cell_type":"markdown","source":"Let's plot feature importance. We select the first 50 features."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ba77c0a01ad8d8a9a6cbabc87edf288f72ce030b"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,10))\nlgb.plot_importance(clf, max_num_features=50, height=0.8,color=\"tomato\",ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a60fb144982a25d022b75e883e71df24ed9f2c6"},"cell_type":"markdown","source":"# <a id=\"7\">Submission</a>\n\nLet's prepare a submission."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a1b22cc8aed47567802cf8dc344ebb1ce031bb67"},"cell_type":"code","source":"sub = test_df[['ID']].copy()\nsub['target'] = np.expm1(sub_preds)\nsub[['ID', 'target']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1356af003b919cf2ac30e7c20b18dbbcc5152010"},"cell_type":"markdown","source":"# <a id=\"8\">References</a>  \n\n[1] <a href=\"https://www.kaggle.com/sudalairajkumar\">SRK</a>, <a href=\"https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-santander-value\">Simple Exploration Notebook - Santander Value</a>  \n[2] <a href=\"https://www.kaggle.com/samratp\">Samrat Pandiri</a>, <a href=\"https://www.kaggle.com/samratp/aggregates-sumvalues-sumzeros-k-means-pca\">Aggregates + SumValues + SumZeros + K-Means + PCA</a>  \n[3] <a href=\"https://www.kaggle.com/ogrellier\">olivier</a>, <a href=\"https://www.kaggle.com/ogrellier/santander-46-features\">Santander_46_features</a>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}