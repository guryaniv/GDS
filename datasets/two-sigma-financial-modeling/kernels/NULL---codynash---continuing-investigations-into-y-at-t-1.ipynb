{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f6af43e3-bd0a-2d16-4aec-b195229e6e7b"
      },
      "source": [
        "Hello Kaggle,\n",
        "\n",
        "Here's a few of my observations building upon the previous work on reverse engineering y values for the previous timestep. I do wonder if 2sigma wants us to look at this. If they wanted us to use y(t-1), they could have just provided it. Unless all of this is just a recruitment puzzle. But I digress.\n",
        "\n",
        "Chenjx1005 really laid the groundwork, lets take a look at that model first.\n",
        "\n",
        "https://www.kaggle.com/chenjx1005/two-sigma-financial-modeling/physical-meanings-of-technical-20-30/discussion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aee40be9-7278-75c0-8f04-95445457ba1f"
      },
      "outputs": [],
      "source": [
        "# setup\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Loading file data.\") ; \n",
        "with pd.HDFStore('../input/train.h5', \"r\") as train: train = train.get(\"train\") \n",
        "min_y, max_y = min(train.y), max(train.y)\n",
        "train.sort_values(by=['id', 'timestamp'], inplace=True)\n",
        "train['y1'] = train.groupby('id')['y'].shift(1).fillna(0) # setup y(t-1) values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff47a476-23dd-86ac-c52a-a2442745b7cd"
      },
      "outputs": [],
      "source": [
        "# my interpretation of the model from:\n",
        "# https://www.kaggle.com/chenjx1005/two-sigma-financial-modeling/physical-meanings-of-technical-20-30/discussion\n",
        "# using formula from\n",
        "# https://www.kaggle.com/c/two-sigma-financial-modeling/discussion/29142\n",
        "\n",
        "alpha1 = 0.92\n",
        "alpha0 = 0.07\n",
        "train['f0'] = train['technical_20'] - train['technical_30'] + train['technical_13'] \n",
        "train['f1'] = train.groupby('id')['f0'].shift(1).values\n",
        "train['f2'] = ( train['f0'] - train['f1'] * alpha1 ) / alpha0\n",
        "train['f2'] = train['f2'].clip(min_y,max_y).fillna(0)\n",
        "\n",
        "train['f2d'] = train['y1'] - train['f2'] # check difference between true y1\n",
        "\n",
        "print('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\n",
        "plt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\n",
        "\n",
        "n0s = 10000\n",
        "plt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) # use transparency to see dense regions\n",
        "plt.grid() ; plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "27f52d23-601e-87c4-1871-ac265ce06b4c"
      },
      "outputs": [],
      "source": [
        "# Rbauld's notebook proposed using ewma's of a few other features, let's evaluate.\n",
        "# It seems like t13 should be added as well.\n",
        "# https://www.kaggle.com/rbauld/two-sigma-financial-modeling/rebuilding-y-t-1/discussion\n",
        "\n",
        "train['y_shifted'] = train['y1']\n",
        "def ewm_mean(x,span_in):\n",
        "    return(x.ewm(span=span_in).mean())\n",
        "train['EWM_26_mean_s']  = train.groupby('id')['y_shifted'].apply(lambda x: ewm_mean(x,span_in=26))\n",
        "\n",
        "# ewm_features = ['technical_30','technical_20','technical_21','technical_19','technical_17','technical_11','technical_2']\n",
        "# t13 added to ewm features\n",
        "ewm_features = ['technical_30','technical_20','technical_21','technical_19','technical_17','technical_11','technical_2','technical_13']\n",
        "mean_values = train[ewm_features].mean(axis=0)\n",
        "train[ewm_features] = train[ewm_features].fillna(mean_values)\n",
        "\n",
        "# n0 = int(len(train)/4) # really slow\n",
        "n0 = 100000\n",
        "import sklearn as sk\n",
        "from sklearn import ensemble\n",
        "my_model = sk.ensemble.GradientBoostingRegressor(loss='ls', max_depth=5, learning_rate=0.05)\n",
        "my_model.fit(X=train.loc[:n0,ewm_features],y=train.loc[:n0,'EWM_26_mean_s'])\n",
        "train['EWM_26s_pred'] = my_model.predict(X=train[ewm_features]) \n",
        "\n",
        "# Inverse transform\n",
        "def ewm_reverse(data,span=26):\n",
        "    alpha = 2/(span+1)\n",
        "    return (data-(1-alpha)*data.shift(1).fillna(0))/alpha\n",
        "train['yEWM_26'] = train.groupby('id')['EWM_26s_pred'].apply(lambda x: ewm_reverse(x, span=26))\n",
        "\n",
        "train['f2'] = train['yEWM_26'].clip(min_y,max_y).fillna(0)\n",
        "train['f2d'] = train['y1'] - train['f2']\n",
        "\n",
        "print('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\n",
        "plt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\n",
        "n0s = 10000\n",
        "plt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) # use transparency to see dense regions\n",
        "plt.grid() ; plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "32da1ab6-9148-4327-0ee2-ace50be3450e"
      },
      "outputs": [],
      "source": [
        "# It seems more elegant to only have a single alpha value, and that they would be related\n",
        "# (As noted by Ricardus in the thread)\n",
        "# this also allows us to scale the data easily\n",
        "# lets look at the cases without t13, with t13 added and with t13 subtracted.\n",
        "\n",
        "alpha0 = 0.9327 # identified by manual newtonian maximization\n",
        "\n",
        "title0 = 't20-t30'\n",
        "train['f0'] = train['technical_20'] - train['technical_30'] # + train['technical_13'] \n",
        "train['f1'] = train.groupby('id')['f0'].shift(1).values\n",
        "train['f2'] = train['f0'] - train['f1'] * alpha0\n",
        "train['f2'] = train['f2'] / (1-alpha0) # scale\n",
        "train['f2'] = train['f2'].clip(min_y,max_y).fillna(0)\n",
        "train['f2d'] = train['y1'] - train['f2'] # check difference between true y1\n",
        "print(title0) ; print('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\n",
        "plt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\n",
        "n0s = 10000 ; plt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) ; plt.grid() ; plt.show()\n",
        "\n",
        "title0 = 't20-t30-t13'\n",
        "train['f0'] = train['technical_20'] - train['technical_30'] - train['technical_13'] \n",
        "train['f1'] = train.groupby('id')['f0'].shift(1).values\n",
        "train['f2'] = ( ( train['f0'] - train['f1'] * alpha0 ) / (1-alpha0) ).clip(min_y,max_y).fillna(0)\n",
        "train['f2d'] = train['y1'] - train['f2'] # check difference between true y1\n",
        "print(title0) ; print('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\n",
        "plt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\n",
        "n0s = 10000 ; plt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) ; plt.grid() ; plt.show()\n",
        "\n",
        "title0 = 't20-t30+t13'\n",
        "train['f0'] = train['technical_20'] - train['technical_30'] + train['technical_13'] \n",
        "train['f1'] = train.groupby('id')['f0'].shift(1).values\n",
        "train['f2'] = ( ( train['f0'] - train['f1'] * alpha0 ) / (1-alpha0) ).clip(min_y,max_y).fillna(0)\n",
        "train['f2d'] = train['y1'] - train['f2'] # check difference between true y1\n",
        "print(title0) ; print('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\n",
        "plt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\n",
        "n0s = 10000 ; plt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) ; plt.grid() ; plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b2f696a3-5f7f-ce88-0a06-30a20fe7e15e"
      },
      "source": [
        "So we can see the single alpha model works better.\n",
        "\n",
        "We see an improvement in accuracy both when we add t13 and when we subtract t13. I couldn't figure out when to add or subtract.\n",
        "\n",
        "In the t20-t30 plot you can see a vertical line of samples at the axis that goes away in the +t13 and -t13 plots. You can also see all of the samples in the -t13 plot shooting off on the opposite slope. Those seem to be the samples where t13 should be added. It's also present on the +t13 plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e8e5e084-5df9-2e3d-789a-8246da31fcb2"
      },
      "outputs": [],
      "source": [
        "# Errors from t20t30t13 are frequently coincident, but there are other error components\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "n0s, n1s = 0, 200\n",
        "\n",
        "ids0 = train.id.unique()\n",
        "timestamps0 = train.timestamp.unique()\n",
        "for id in ids0[:11]:\n",
        "    rows0 = train.id.isin([id]) & train.timestamp.isin(timestamps0[n0s:n1s])\n",
        "    plt.plot( train.loc[rows0,'timestamp'], train.loc[rows0,'f2d'] )\n",
        "plt.grid()"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}