{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f11df408-2522-154b-1f3c-d6c462738a86"
      },
      "source": [
        "#Ensemble kernel\n",
        "##Consist of xgboost, lstm network, ensemble algorithms\n",
        "###Using weights voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12f333e5-4fe4-cf3a-47af-2be1f5cdcd85"
      },
      "source": [
        "###Import all libs, set seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "21fe097e-77c7-ae87-5387-c3aadad8d148"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "from sklearn.random_projection import SparseRandomProjection\n",
        "from sklearn.decomposition import PCA, FastICA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0c38456c-d218-e596-babf-ef1f71943eef"
      },
      "source": [
        "###Add features function (PCA, ICA...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5d9655bf-d710-4633-af2e-5d2aa231a8f2"
      },
      "outputs": [],
      "source": [
        "def add_features(train, test):    \n",
        "    n_compute = 20\n",
        "\n",
        "    tsvd = TruncatedSVD(n_components=n_compute, random_state=seed)\n",
        "    tsvd_results_train = tsvd.fit_transform(train)\n",
        "    tsvd_results_test = tsvd.transform(test)\n",
        "\n",
        "    pca = PCA(n_components=n_compute, random_state=seed)\n",
        "    pca2_results_train = pca.fit_transform(train)\n",
        "    pca2_results_test = pca.transform(test)\n",
        "\n",
        "    ica = FastICA(n_components=n_compute, random_state=seed)\n",
        "    ica2_results_train = ica.fit_transform(train)\n",
        "    ica2_results_test = ica.transform(test)\n",
        "    \n",
        "    grp = GaussianRandomProjection(n_components=n_compute, eps=0.1, random_state=seed)\n",
        "    grp_results_train = grp.fit_transform(train)\n",
        "    grp_results_test = grp.transform(test)\n",
        "\n",
        "    srp = SparseRandomProjection(n_components=n_compute, dense_output=True, random_state=seed)\n",
        "    srp_results_train = srp.fit_transform(train)\n",
        "    srp_results_test = srp.transform(test)\n",
        "\n",
        "    for i in range(1, n_compute + 1):\n",
        "        train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
        "        test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
        "        \n",
        "        train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
        "        test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
        "\n",
        "        train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
        "        test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
        "\n",
        "        train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
        "        test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
        "\n",
        "        train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
        "        test['srp_' + str(i)] = srp_results_test[:, i - 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cb62336c-35db-10e3-6c27-175552264030"
      },
      "source": [
        "###Inverse scale function (Scale function include next cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec59be56-66dc-6094-7b75-e8b49f1e91fc"
      },
      "outputs": [],
      "source": [
        "scale_const = 1\n",
        "\n",
        "def inverse_scale(predict_value):  \n",
        "    return scale_const * predict_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "44ee2115-9326-fd9a-60b4-ab460cad0706"
      },
      "source": [
        "##Main load data function. Incude some useful features from regressor + dummy coding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "64c2b400-204c-5ae2-079a-98ed8cdb1a8d"
      },
      "outputs": [],
      "source": [
        "def load_data(path='../input/'):\n",
        "    df_train = pd.read_csv(path.__add__('train.csv'))\n",
        "    df_test = pd.read_csv(path.__add__('test.csv'))\n",
        "    \n",
        "    num_train = len(df_train)\n",
        "    \n",
        "    id_test = df_test['ID'].values\n",
        "    \n",
        "    y_train = df_train['y'].values.astype(np.float32)\n",
        "    \n",
        "    df_train_dummies = pd.get_dummies(df_train, drop_first=True)\n",
        "    df_test_dummies = pd.get_dummies(df_test, drop_first=True)\n",
        "\n",
        "    df_train_dummies = df_train_dummies.drop(['ID','y'], axis=1)\n",
        "    df_test_dummies = df_test_dummies.drop('ID', axis=1)\n",
        "    \n",
        "    df_temp = pd.concat([df_train_dummies, df_test_dummies], join='inner')\n",
        "    \n",
        "    df_train = df_temp[:num_train]\n",
        "    df_test = df_temp[num_train:]\n",
        "    \n",
        "    add_features(df_train, df_test)\n",
        "\n",
        "    clf = ExtraTreesRegressor(n_estimators=250, max_depth=4, random_state=seed)\n",
        "\n",
        "    clf.fit(df_train, y_train)\n",
        "\n",
        "    features = pd.DataFrame()\n",
        "    features['feature'] = df_train.columns\n",
        "    features['importance'] = clf.feature_importances_\n",
        "    features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
        "    features.set_index('feature', inplace=True)\n",
        "\n",
        "    model = SelectFromModel(clf, prefit=True)\n",
        "    train_reduced = model.transform(df_train)   \n",
        "\n",
        "    test_reduced = model.transform(df_test.copy())\n",
        "    \n",
        "    df_train = pd.concat([df_train, pd.DataFrame(train_reduced)], axis=1)\n",
        "    df_test = pd.concat([df_test, pd.DataFrame(test_reduced)], axis=1)\n",
        "        \n",
        "    df_all = pd.concat([df_train, df_test])\n",
        "    \n",
        "    x_train, x_test = df_all.values[:num_train], df_all.values[num_train:]\n",
        "                                   \n",
        "    y_train /= scale_const\n",
        "    \n",
        "    return id_test, x_train, y_train, x_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c33e5dc5-475c-5bcd-c216-5d5473cbcec2"
      },
      "source": [
        "#Useful ensemble class with mixture of experts mode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1d0f822c-2741-318d-6aa9-f87001e12ecc"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "class Ensemble(object):\n",
        "    def __init__(self, stack, weights='mean', bias=None):\n",
        "        self.stack = stack\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.x = None\n",
        "        \n",
        "    def fit(self, x_train, y_train, lr_moe=0.001, n_epochs_moe=10):\n",
        "        len_s = len(self.stack)\n",
        "        \n",
        "        for b in self.stack:\n",
        "            b.fit(x_train, y_train)\n",
        "            print('\\n')\n",
        "        \n",
        "        if self.weights == 'mean':\n",
        "            self.weights = np.full(len_s, 1./len_s)\n",
        "        elif self.weights == 'moe':\n",
        "            print('Train moe algorithm on {}\\n'.format(x_train.shape[0]))\n",
        "            \n",
        "            self.x = np.random.uniform(low=0, high=1, size=len_s)\n",
        "            self.bias = np.random.uniform(low=0, high=5, size=len_s)\n",
        "            \n",
        "            for t in range(1, n_epochs_moe + 1):                \n",
        "                y_predict = np.vstack(self.stack_predict(x_train))\n",
        "                prob = softmax(self.x)        \n",
        "                \n",
        "                for k in range(x_train.shape[0]):\n",
        "                    dG_dx = []\n",
        "                    dG_db = []\n",
        "                    \n",
        "                    for i in range(len_s):                     \n",
        "                        dG_dx.append(0.5 * prob[i] * (1 - prob[i]) * (y_train - y_predict[i][k] + self.bias[i])**2)\n",
        "                        dG_db.append(prob[i] * (y_train - y_predict[i][k] + self.bias[i]))\n",
        "                             \n",
        "                        self.x[i] -= lr_moe * dG_dx[i][k]\n",
        "                        self.bias[i] -= lr_moe * dG_db[i][k]\n",
        "                    \n",
        "                print('Epoch {}; weights {}; alpha {}'.format(t, softmax(self.x), self.bias))\n",
        "                \n",
        "            self.weights = softmax(self.x)\n",
        "        \n",
        "    def stack_predict(self, x_valid):\n",
        "        b = [] \n",
        "        for b_ in self.stack:\n",
        "            b.append(b_.predict(x_valid))\n",
        "            \n",
        "        return b\n",
        "    \n",
        "    def predict(self, x_valid):              \n",
        "        predict = np.average(self.stack_predict(x_valid), axis=0, weights=self.weights)\n",
        "        \n",
        "        if self.bias != None:\n",
        "            predict += np.average(self.bias, weights=self.weights, axis=0)\n",
        "        return predict\n",
        "  \n",
        "\n",
        "print('Ensemble class succsessful build')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12bcec70-52b2-dd75-6fd8-c710bbccab23"
      },
      "source": [
        "##R2 keras metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18299de1-a1d1-464a-5ff3-534ddb957d27"
      },
      "outputs": [],
      "source": [
        "def r2_score_keras(y_true, y_pred):\n",
        "    SS_res = K.sum(K.square(y_true - y_pred)) \n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
        "    \n",
        "    return (1 - SS_res / (SS_tot + K.epsilon()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5c8eca9f-5052-39fb-406e-d8e2ff48e6a8"
      },
      "source": [
        "#Built models\n",
        "##Import all libs and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "baa12750-90ec-d938-3a40-7401984d5a14"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Dropout, Reshape, Activation, BatchNormalization\n",
        "from keras.optimizers import sgd\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.linear_model import LassoLarsCV\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "id_test, x_train, y_train, x_test = load_data()\n",
        "\n",
        "input_shape = (x_test.shape[1], )\n",
        "re_input_shape = (1, x_test.shape[1]) \n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(\n",
        "    x_train, \n",
        "    y_train, \n",
        "    test_size=0.2, \n",
        "    random_state=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3681fc4d-501e-07ac-dd8b-bbb5e7641fbf"
      },
      "source": [
        "###Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b40dc221-6d31-ccce-d1f5-0508937d71fa"
      },
      "outputs": [],
      "source": [
        "def nn_model():       \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Reshape(re_input_shape, input_shape=input_shape))\n",
        "    model.add(LSTM(100, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Dense(250))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    \n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='mse', \n",
        "                  metrics=[r2_score_keras])\n",
        "   \n",
        "    return model\n",
        "\n",
        "nn_regressor = KerasRegressor(build_fn=nn_model, \n",
        "                           epochs=150,  \n",
        "                           batch_size=32,\n",
        "                           validation_data=(x_valid, y_valid), \n",
        "                           shuffle=True,\n",
        "                           verbose=2)\n",
        "\n",
        "gbr = GradientBoostingRegressor(learning_rate=0.001, \n",
        "                                loss=\"huber\", \n",
        "                                max_depth=3, \n",
        "                                max_features=0.55, \n",
        "                                min_samples_leaf=18, \n",
        "                                min_samples_split=14, \n",
        "                                subsample=0.7)\n",
        "\n",
        "lasso = LassoLarsCV(normalize=True)\n",
        "\n",
        "rfr = RandomForestRegressor(n_estimators=250, \n",
        "                           min_samples_leaf=25,\n",
        "                           min_samples_split=25,\n",
        "                           n_jobs=4,\n",
        "                           max_depth=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7f1ae4aa-e923-8a0a-0d5d-2f14ac03dcc9"
      },
      "outputs": [],
      "source": [
        "# Xgboost\n",
        "import xgboost as xgb\n",
        "\n",
        "xgb_params = {\n",
        "    'n_trees': 1500, \n",
        "    'eta': 0.006,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.93,\n",
        "    'objective': 'reg:linear',\n",
        "    'eval_metric': 'rmse',\n",
        "    'base_score': np.mean(y_train),\n",
        "    'silent': 1\n",
        "}\n",
        "\n",
        "dtrain = xgb.DMatrix(x_train, y_train)\n",
        "dtest = xgb.DMatrix(x_test)\n",
        "dvalid = xgb.DMatrix(x_valid)\n",
        "\n",
        "model_xgb = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=750)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2457b771-5bbd-cb3e-837b-63e67fe997e0"
      },
      "outputs": [],
      "source": [
        "model = Ensemble(stack=[nn_regressor, gbr, rfr, lasso], weights='mean')\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d2992067-6f12-784e-05db-fc9018de6829"
      },
      "outputs": [],
      "source": [
        "y_predict = 0.25 * inverse_scale(model.predict(x_test)) + 0.75 * inverse_scale(model_xgb.predict(dtest)) \n",
        "y_predict_test = 0.25 * inverse_scale(model.predict(x_train)) + 0.75 * inverse_scale(model_xgb.predict(dtrain))\n",
        "y_predict_valid = 0.25 * inverse_scale(model.predict(x_valid)) + 0.75 * inverse_scale(model_xgb.predict(dvalid))\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'ID':id_test, 'y':y_predict.ravel()})\n",
        "df.to_csv('stack_test.csv', index=False)\n",
        "\n",
        "print('R2 score test', r2_score(inverse_scale(y_train), y_predict_test))\n",
        "print('R2 score test', r2_score(inverse_scale(y_valid), y_predict_valid))"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}