{"nbformat_minor": 1, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "version": "3.6.3", "file_extension": ".py", "nbconvert_exporter": "python"}}, "cells": [{"cell_type": "markdown", "source": ["## The Directory Structure\n", "```\n", "Project\n", "|-- datasets\n", "|   |-- dev_set\n", "|   |-- test\n", "|   |-- test_set\n", "|   |-- train\n", "|   `-- train_set\n", "|-- model\n", "|-- pretrained-model\n", "|-- submissions\n", "|-- datalab.py\n", "|-- dataset_clusterer.py\n", "|-- make_file.py\n", "|-- model.py\n", "|-- vgg16.py\n", "|-- predict.py\n", "|-- test.py\n", "`-- train.py\n", "```\n", "\n", "**Code with proper differentiation in scripts can be found [here](https://github.com/piyush2896/Transfer-Learning-Vgg-16)**"], "metadata": {"_cell_guid": "54360db6-6718-48f3-8180-65adaea6188a", "_uuid": "8a88a278356fa0340fae17d2bd71e39a704ee2e4"}}, {"cell_type": "markdown", "source": ["## Preprocessing and Batches Creation\n", "Unfortunately we cannot directly feed data into the model as it is huge for machine and is of variable size.\n", "\n", "Making each image of size (224, 224, 3) and creating batches of the images. Run [dataset_clusterer.py](https://github.com/piyush2896/Transfer-Learning-Vgg-16/blob/master/dataset_clusterer.py) to get through it"], "metadata": {"_cell_guid": "4372024c-cb32-4ecb-ab0a-d1da5700de93", "_uuid": "b2b86bbf618e5d7e3ab7a4afb845dba2bd83af76"}}, {"cell_type": "markdown", "source": ["## VGG-16 Pretrained\n", "I used [vgg-16](http://arxiv.org/abs/1409.1556.pdf) pretrained model and fine-tuned the last layer. The checkpoint file used in code of [vgg16.py](https://github.com/piyush2896/Transfer-Learning-Vgg-16/blob/master/vgg16.py) can be downloaded from [here](http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz)"], "metadata": {"_cell_guid": "542e48ca-db92-4851-aa1f-283f24c8b036", "_uuid": "ddb39b248761f74a876223cfcd0a9fa2e7052f37"}}, {"cell_type": "markdown", "source": ["## Generators as Pipeline\n", "I created a pipeline to feed data into model. This ensured that I donot load the complete dataset in the memory. There are two generators \n", "1. DataLabTrain - used for train and dev set\n", "2. DataLabTest - used for test set\n", "\n", "They simply load the batches made by dataset_clusterer.py and make them available using a generator method. Code can be found in [datalab.py](https://github.com/piyush2896/Transfer-Learning-Vgg-16/blob/master/datalab.py)."], "metadata": {"_cell_guid": "8063a7e8-27df-43b8-b850-0d527d9adbcf", "_uuid": "069fb23f04a55be2d1afd2dc98cadac84a895d59"}}, {"cell_type": "markdown", "source": ["## Training time\n", "I just trained for 1 epoc and got a dev set loss of around ~0.04. And a test set loss at kaggle of 0.08426. The code of train.py is displaye below"], "metadata": {"_cell_guid": "f2877969-6cb7-4679-b6d7-ef8ba28ee3d1", "_uuid": "ed5ad9fa8885f0646784d212c63c074c71e9ada9"}}, {"cell_type": "code", "source": ["import tensorflow as tf\n", "from vgg16 import vgg16\n", "import numpy as np\n", "import os\n", "from datalab import DataLabTrain"], "execution_count": null, "metadata": {"_cell_guid": "d804a30c-cf6f-487c-8f3d-43f48ea48129", "_uuid": "8c4bb1f955311ec1aa7fa88d3004989bb99ce7d0", "collapsed": true, "_kg_hide-output": true}, "outputs": []}, {"cell_type": "code", "source": ["def train(n_iters):\n", "    model, params = vgg16(fine_tune_last=True, n_classes=2)\n", "    X = model['input']\n", "    Z = model['out']\n", "    Y = tf.placeholder(dtype=tf.float32, shape=[None, 2])\n", "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z[:, 0, 0, :], labels=Y))\n", "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n", "    saver = tf.train.Saver()\n", "\n", "    with tf.Session() as sess:\n", "        try:\n", "            sess.run(tf.global_variables_initializer())\n", "            for i in range(n_iters):\n", "                dl = DataLabTrain('./datasets/train_set/')\n", "                train_gen = dl.generator()\n", "                dev_gen = DataLabTrain('./datasets/dev_set/').generator()\n", "                for X_train, Y_train in train_gen:\n", "                    print('Samples seen: '.format(dl.cur_index), end='\\r')\n", "                    sess.run(train_step, feed_dict={X: X_train, Y: Y_train})\n", "                print()\n", "                l = 0\n", "                count = 0\n", "                for X_test, Y_test in dev_gen:\n", "                    count += 1\n", "                    l += sess.run(loss, feed_dict={X: X_test, Y: Y_test})\n", "\n", "                print('Epoch: {}\\tLoss: {}'.format(i, l/count))\n", "                saver.save(sess, './model/vgg16-dog-vs-cat.ckpt')\n", "                print(\"Model Saved\")\n", "\n", "        finally:\n", "            sess.close()"], "execution_count": null, "metadata": {"_cell_guid": "d0545e6a-a35a-44fe-9c0b-c6be0524f064", "_uuid": "500b98974d921e997d0dfa93cd5bc57719eb17c5", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["train(n_iters=1)"], "execution_count": null, "metadata": {"_cell_guid": "1d49d537-abe6-4921-9017-27006c9d4a95", "_uuid": "069bc0128a31f966c271f7a67882f9d962ed3fd5", "collapsed": true, "_kg_hide-output": true}, "outputs": []}, {"cell_type": "markdown", "source": ["## Prediction Time\n", "The training script saves the model in \"model\" folder which can be restored and used for prediction. Code of predict.py is displayed below"], "metadata": {"_cell_guid": "e5df5926-0143-498b-8d9e-4803df04eb23", "_uuid": "c3e80561debae54352baf06d7576aa5cbf443476"}}, {"cell_type": "code", "source": ["from make_file import make_sub\n", "\n", "\n", "def predict(model_path, batch_size):\n", "    model, params = vgg16(fine_tune_last=True, n_classes=2)\n", "    X = model['input']\n", "    Y_hat = tf.nn.softmax(model['out'])\n", "\n", "    saver = tf.train.Saver()\n", "\n", "    dl_test = DataLabTest('./datasets/test_set/')\n", "    test_gen = dl_test.generator()\n", "\n", "    Y = []\n", "    with tf.Session() as sess:\n", "        saver.restore(sess, model_path)\n", "        for i in range(12500//batch_size+1):\n", "            y = sess.run(Y_hat, feed_dict={X: next(test_gen)})\n", "            #print(y.shape, end='   ')\n", "            Y.append(y[:,0, 0, 1])\n", "            print('Complete: {}%'.format(round(len(Y) / dl_test.max_len * 100, 2)), end='\\r')\n", "    Y = np.concatenate(Y)\n", "\n", "    print()\n", "    print('Total Predictions: '.format(Y.shape))\n", "    return Y\n", "\n", "Y = predict('./model/vgg16-dog-vs-cat.ckpt', 16)\n", "np.save('out.npy', Y)\n", "make_sub('sub_1.csv')"], "execution_count": null, "metadata": {"_cell_guid": "bf95d493-771f-4f7f-a994-3e978a38bfb5", "_uuid": "9dc833ba6365d31c33fa50baec86b0205f5ea361", "collapsed": true, "_kg_hide-output": true}, "outputs": []}, {"cell_type": "markdown", "source": ["[make_file.py](https://github.com/piyush2896/Transfer-Learning-Vgg-16/blob/master/make_file.py) is a helper script used to create submission file."], "metadata": {"_cell_guid": "959e67f9-dea8-4a02-af47-2d8c506e5d31", "_uuid": "2b7c3d135a200db76f17ea5743587564f823cc92"}}], "nbformat": 4}