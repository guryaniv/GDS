{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "afd14466-af82-5073-39a6-ceab333ef3ff"
      },
      "source": [
        "umm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6726476-c087-7d59-f505-38266161482c"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords, brown\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize, ngrams\n",
        "import nltk\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "import xgboost as xgb\n",
        "\n",
        "import difflib\n",
        "\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "stops = eng_stopwords = set(stopwords.words('english'))\n",
        "color = sns.color_palette()\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "795e6ea0-da8e-e72e-9d03-37d0b159e3f1"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"../input/train.csv\")\n",
        "test_df = pd.read_csv(\"../input/test.csv\")\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23b853e7-1216-fc4d-4eba-34217ae95bd5"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e7212d4-aa22-e83b-9697-a6e94fc1dcf9"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e601825-63b6-240c-b0d2-16c662d3de16"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "train_qs = pd.Series(train_df['question1'].tolist() + train_df['question2'].tolist()).astype(str)\n",
        "\n",
        "def word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).lower().split():\n",
        "        if word not in stops:\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).lower().split():\n",
        "        if word not in stops:\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
        "    return R\n",
        "\n",
        "# If a word appears only once, we ignore it completely (likely a typo)\n",
        "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
        "def get_weight(count, eps=10000, min_count=2):\n",
        "    if count < min_count:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1 / (count + eps)\n",
        "\n",
        "eps = 5000 \n",
        "words = (\" \".join(train_qs)).lower().split()\n",
        "counts = Counter(words)\n",
        "weights = {word: get_weight(count) for word, count in counts.items()}\n",
        "\n",
        "\n",
        "def tfidf_word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).lower().split():\n",
        "        if word not in stops:\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).lower().split():\n",
        "        if word not in stops:\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
        "        return 0\n",
        "    \n",
        "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
        "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
        "    \n",
        "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
        "    return R\n",
        "\n",
        "def diff_ratios(st1, st2):\n",
        "    seq = difflib.SequenceMatcher()\n",
        "    seq.set_seqs(str(st1).lower(), str(st2).lower())\n",
        "    return seq.ratio()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "936b7aff-f8e3-a439-b064-9cfb7173b7fd"
      },
      "outputs": [],
      "source": [
        "def feature_extraction(row):\n",
        "    #if int(row['id']) % 10000 == 0:\n",
        "    #    print(row['id'])\n",
        "    que1 = str(row['question1'])\n",
        "    que2 = str(row['question2'])\n",
        "    out_list = []\n",
        "    # get unigram features #\n",
        "    unigrams_que1 = [word for word in que1.lower().split() if word not in eng_stopwords]\n",
        "    unigrams_que2 = [word for word in que2.lower().split() if word not in eng_stopwords]\n",
        "    common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n",
        "    common_unigrams_ratio = float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1)\n",
        "    out_list.extend([common_unigrams_len, common_unigrams_ratio])\n",
        "\n",
        "    # get bigram features #\n",
        "    bigrams_que1 = [i for i in ngrams(unigrams_que1, 2)]\n",
        "    bigrams_que2 = [i for i in ngrams(unigrams_que2, 2)]\n",
        "    common_bigrams_len = len(set(bigrams_que1).intersection(set(bigrams_que2)))\n",
        "    common_bigrams_ratio = float(common_bigrams_len) / max(len(set(bigrams_que1).union(set(bigrams_que2))),1)\n",
        "    out_list.extend([common_bigrams_len, common_bigrams_ratio])\n",
        "\n",
        "    # get trigram features #\n",
        "    trigrams_que1 = [i for i in ngrams(unigrams_que1, 3)]\n",
        "    trigrams_que2 = [i for i in ngrams(unigrams_que2, 3)]\n",
        "    common_trigrams_len = len(set(trigrams_que1).intersection(set(trigrams_que2)))\n",
        "    common_trigrams_ratio = float(common_trigrams_len) / max(len(set(trigrams_que1).union(set(trigrams_que2))),1)\n",
        "    out_list.extend([common_trigrams_len, common_trigrams_ratio])\n",
        "    \n",
        "    #f = similarity(que1, que2, False)\n",
        "    #t = similarity(que1, que2, True)\n",
        "    #out_list.extend([f, t])\n",
        "    \n",
        "    return out_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a037e402-5a46-e819-5ed3-1f73d6194aca"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame()\n",
        "data['match_ratio'] = train_df.apply(lambda r: diff_ratios(r.question1, r.question2), axis=1)\n",
        "print('6')\n",
        "data['word_match_share'] = train_df.apply(lambda row: word_match_share(row), axis=1)\n",
        "print('5')\n",
        "data['tfidf_word_match_share'] = train_df.apply(lambda row: tfidf_word_match_share(row), axis=1)\n",
        "\n",
        "#data['u1'], data['u2'], data['b1'], data['b2'], data['t1'], data['t2'] = train_df.apply(lambda row: feature_extraction(row), axis=1)\n",
        "\n",
        "z = '''\n",
        "print('1')\n",
        "data['len_q1'] = train_df.question1.apply(lambda x: len(str(x)))\n",
        "data['len_q2'] = train_df.question2.apply(lambda x: len(str(x)))\n",
        "data['diff_len'] = data.len_q1 - data.len_q2\n",
        "print('2')\n",
        "data['len_char_q1'] = train_df.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "data['len_char_q2'] = train_df.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "data['len_word_q1'] = train_df.question1.apply(lambda x: len(str(x).split()))\n",
        "data['len_word_q2'] = train_df.question2.apply(lambda x: len(str(x).split()))\n",
        "print('3')\n",
        "data['common_words'] = train_df.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
        "print('4')\n",
        "\n",
        "data['fuzz_qratio'] = train_df.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_WRatio'] = train_df.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_partial_ratio'] = train_df.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "print('5')\n",
        "data['fuzz_partial_token_set_ratio'] = train_df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_partial_token_sort_ratio'] = train_df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "print('6')\n",
        "data['fuzz_token_set_ratio'] = train_df.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_token_sort_ratio'] = train_df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "#data[] = train_df.apply(lambda row: feature_extraction(row), axis=1)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "05fc5449-0ea9-fd68-f422-511ca309501f"
      },
      "outputs": [],
      "source": [
        "#data.drop(['fuzz_qratio','fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', 'fuzz_token_sort_ratio'], inplace=True, axis=1)\n",
        "#data.drop(['u1','u2', 'b1', 'b2', 't1', 't2'], inplace=True, axis=1)\n",
        "data['fuzz_partial_ratio'] = train_df.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_token_set_ratio'] = train_df.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_token_sort_ratio'] = train_df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "data = data.fillna(0)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8fc8e216-9e09-bb17-1580-1cd8e20a300c"
      },
      "outputs": [],
      "source": [
        "s1 = data.as_matrix()\n",
        "s2 = np.vstack( np.array(train_df.apply(lambda row: feature_extraction(row), axis=1)) )\n",
        "#final = np.hstack((s1, s2))\n",
        "argh = pd.DataFrame(np.hstack((s1, s2)))\n",
        "argh = argh.fillna(0)\n",
        "final = argh.as_matrix()\n",
        "pd.isnull(argh).any(1).nonzero()[0]\n",
        "print(final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8225e1d9-f5cb-b39c-85c4-ffd1ff3dec4e"
      },
      "outputs": [],
      "source": [
        "#https://github.com/sujitpal/nltk-examples/blob/master/src/semantic/short_sentence_similarity.py#L133\n",
        "\n",
        "# Parameters to the algorithm. Currently set to values that was reported\n",
        "# in the paper to produce \"best\" results.\n",
        "ALPHA = 0.2\n",
        "BETA = 0.45\n",
        "ETA = 0.4\n",
        "PHI = 0.2\n",
        "DELTA = 0.85\n",
        "\n",
        "brown_freqs = dict()\n",
        "N = 0\n",
        "\n",
        "######################### word similarity ##########################\n",
        "\n",
        "def get_best_synset_pair(word_1, word_2):\n",
        "    \"\"\" \n",
        "    Choose the pair with highest path similarity among all pairs. \n",
        "    Mimics pattern-seeking behavior of humans.\n",
        "    \"\"\"\n",
        "    max_sim = -1.0\n",
        "    synsets_1 = wn.synsets(word_1)\n",
        "    synsets_2 = wn.synsets(word_2)\n",
        "    #print(word_1, word_2)\n",
        "    #print(synsets_1, synsets_2)\n",
        "    if len(synsets_1) == 0 or len(synsets_2) == 0:\n",
        "        return None, None\n",
        "    else:\n",
        "        max_sim = -1.0\n",
        "        best_pair = None, None\n",
        "        for synset_1 in synsets_1:\n",
        "            for synset_2 in synsets_2:\n",
        "                #print(synset_1, synset_2)\n",
        "                sim = wn.wup_similarity(synset_1, synset_2)\n",
        "                if not sim:\n",
        "                    continue\n",
        "                if sim > max_sim:\n",
        "                    max_sim = sim\n",
        "                    best_pair = synset_1, synset_2\n",
        "        return best_pair\n",
        "\n",
        "def length_dist(synset_1, synset_2):\n",
        "    \"\"\"\n",
        "    Return a measure of the length of the shortest path in the semantic \n",
        "    ontology (Wordnet in our case as well as the paper's) between two \n",
        "    synsets.\n",
        "    \"\"\"\n",
        "    l_dist = sys.maxsize\n",
        "    if synset_1 is None or synset_2 is None: \n",
        "        return 0.0\n",
        "    if synset_1 == synset_2:\n",
        "        # if synset_1 and synset_2 are the same synset return 0\n",
        "        l_dist = 0.0\n",
        "    else:\n",
        "        wset_1 = set([str(x.name()) for x in synset_1.lemmas()])        \n",
        "        wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n",
        "        if len(wset_1.intersection(wset_2)) > 0:\n",
        "            # if synset_1 != synset_2 but there is word overlap, return 1.0\n",
        "            l_dist = 1.0\n",
        "        else:\n",
        "            # just compute the shortest path between the two\n",
        "            l_dist = synset_1.shortest_path_distance(synset_2)\n",
        "            if l_dist is None:\n",
        "                l_dist = 0.0\n",
        "    # normalize path length to the range [0,1]\n",
        "    return math.exp(-ALPHA * l_dist)\n",
        "\n",
        "def hierarchy_dist(synset_1, synset_2):\n",
        "    \"\"\"\n",
        "    Return a measure of depth in the ontology to model the fact that \n",
        "    nodes closer to the root are broader and have less semantic similarity\n",
        "    than nodes further away from the root.\n",
        "    \"\"\"\n",
        "    h_dist = sys.maxsize\n",
        "    if synset_1 is None or synset_2 is None: \n",
        "        return h_dist\n",
        "    if synset_1 == synset_2:\n",
        "        # return the depth of one of synset_1 or synset_2\n",
        "        h_dist = max([x[1] for x in synset_1.hypernym_distances()])\n",
        "    else:\n",
        "        # find the max depth of least common subsumer\n",
        "        hypernyms_1 = {x[0]:x[1] for x in synset_1.hypernym_distances()}\n",
        "        hypernyms_2 = {x[0]:x[1] for x in synset_2.hypernym_distances()}\n",
        "        lcs_candidates = set(hypernyms_1.keys()).intersection(\n",
        "            set(hypernyms_2.keys()))\n",
        "        if len(lcs_candidates) > 0:\n",
        "            lcs_dists = []\n",
        "            for lcs_candidate in lcs_candidates:\n",
        "                lcs_d1 = 0\n",
        "                if lcs_candidate in hypernyms_1:\n",
        "                    lcs_d1 = hypernyms_1[lcs_candidate]\n",
        "                lcs_d2 = 0\n",
        "                if lcs_candidate in hypernyms_2:\n",
        "                    lcs_d2 = hypernyms_2[lcs_candidate]\n",
        "                lcs_dists.append(max([lcs_d1, lcs_d2]))\n",
        "            h_dist = max(lcs_dists)\n",
        "        else:\n",
        "            h_dist = 0\n",
        "    return ((math.exp(BETA * h_dist) - math.exp(-BETA * h_dist)) / \n",
        "        (math.exp(BETA * h_dist) + math.exp(-BETA * h_dist)))\n",
        "    \n",
        "def word_similarity(word_1, word_2):\n",
        "    synset_pair = get_best_synset_pair(word_1, word_2)\n",
        "    return (length_dist(synset_pair[0], synset_pair[1]) * \n",
        "        hierarchy_dist(synset_pair[0], synset_pair[1]))\n",
        "\n",
        "######################### sentence similarity ##########################\n",
        "\n",
        "def most_similar_word(word, word_set):\n",
        "    \"\"\"\n",
        "    Find the word in the joint word set that is most similar to the word\n",
        "    passed in. We use the algorithm above to compute word similarity between\n",
        "    the word and each word in the joint word set, and return the most similar\n",
        "    word and the actual similarity value.\n",
        "    \"\"\"\n",
        "    max_sim = -1.0\n",
        "    sim_word = \"\"\n",
        "    for ref_word in word_set:\n",
        "      sim = word_similarity(word, ref_word)\n",
        "      if sim > max_sim:\n",
        "          max_sim = sim\n",
        "          sim_word = ref_word\n",
        "    return sim_word, max_sim\n",
        "    \n",
        "def info_content(lookup_word):\n",
        "    \"\"\"\n",
        "    Uses the Brown corpus available in NLTK to calculate a Laplace\n",
        "    smoothed frequency distribution of words, then uses this information\n",
        "    to compute the information content of the lookup_word.\n",
        "    \"\"\"\n",
        "    global N\n",
        "    if N == 0:\n",
        "        # poor man's lazy evaluation\n",
        "        for sent in brown.sents():\n",
        "            for word in sent:\n",
        "                word = word.lower()\n",
        "                if word not in brown_freqs:\n",
        "                    #if not brown_freqs.has_key(word):\n",
        "                    brown_freqs[word] = 0\n",
        "                brown_freqs[word] = brown_freqs[word] + 1\n",
        "                N = N + 1\n",
        "    lookup_word = lookup_word.lower()\n",
        "    #n = 0 if not brown_freqs.has_key(lookup_word) else brown_freqs[lookup_word]\n",
        "    n = 0 if lookup_word not in brown_freqs else brown_freqs[lookup_word]\n",
        "    return 1.0 - (math.log(n + 1) / math.log(N + 1))\n",
        "    \n",
        "def semantic_vector(words, joint_words, info_content_norm):\n",
        "    \"\"\"\n",
        "    Computes the semantic vector of a sentence. The sentence is passed in as\n",
        "    a collection of words. The size of the semantic vector is the same as the\n",
        "    size of the joint word set. The elements are 1 if a word in the sentence\n",
        "    already exists in the joint word set, or the similarity of the word to the\n",
        "    most similar word in the joint word set if it doesn't. Both values are \n",
        "    further normalized by the word's (and similar word's) information content\n",
        "    if info_content_norm is True.\n",
        "    \"\"\"\n",
        "    sent_set = set(words)\n",
        "    semvec = np.zeros(len(joint_words))\n",
        "    i = 0\n",
        "    for joint_word in joint_words:\n",
        "        if joint_word in sent_set:\n",
        "            # if word in union exists in the sentence, s(i) = 1 (unnormalized)\n",
        "            semvec[i] = 1.0\n",
        "            if info_content_norm:\n",
        "                semvec[i] = semvec[i] * math.pow(info_content(joint_word), 2)\n",
        "        else:\n",
        "            # find the most similar word in the joint set and set the sim value\n",
        "            sim_word, max_sim = most_similar_word(joint_word, sent_set)\n",
        "            semvec[i] = PHI if max_sim > PHI else 0.0\n",
        "            if info_content_norm:\n",
        "                semvec[i] = semvec[i] * info_content(joint_word) * info_content(sim_word)\n",
        "        i = i + 1\n",
        "    return semvec                \n",
        "            \n",
        "def semantic_similarity(sentence_1, sentence_2, info_content_norm):\n",
        "    \"\"\"\n",
        "    Computes the semantic similarity between two sentences as the cosine\n",
        "    similarity between the semantic vectors computed for each sentence.\n",
        "    \"\"\"\n",
        "    words_1 = nltk.word_tokenize(sentence_1)\n",
        "    words_2 = nltk.word_tokenize(sentence_2)\n",
        "    joint_words = set(words_1).union(set(words_2))\n",
        "    vec_1 = semantic_vector(words_1, joint_words, info_content_norm)\n",
        "    vec_2 = semantic_vector(words_2, joint_words, info_content_norm)\n",
        "    return np.dot(vec_1, vec_2.T) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n",
        "\n",
        "######################### word order similarity ##########################\n",
        "\n",
        "def word_order_vector(words, joint_words, windex):\n",
        "    \"\"\"\n",
        "    Computes the word order vector for a sentence. The sentence is passed\n",
        "    in as a collection of words. The size of the word order vector is the\n",
        "    same as the size of the joint word set. The elements of the word order\n",
        "    vector are the position mapping (from the windex dictionary) of the \n",
        "    word in the joint set if the word exists in the sentence. If the word\n",
        "    does not exist in the sentence, then the value of the element is the \n",
        "    position of the most similar word in the sentence as long as the similarity\n",
        "    is above the threshold ETA.\n",
        "    \"\"\"\n",
        "    wovec = np.zeros(len(joint_words))\n",
        "    i = 0\n",
        "    wordset = set(words)\n",
        "    for joint_word in joint_words:\n",
        "        if joint_word in wordset:\n",
        "            # word in joint_words found in sentence, just populate the index\n",
        "            wovec[i] = windex[joint_word]\n",
        "        else:\n",
        "            # word not in joint_words, find most similar word and populate\n",
        "            # word_vector with the thresholded similarity\n",
        "            sim_word, max_sim = most_similar_word(joint_word, wordset)\n",
        "            if max_sim > ETA:\n",
        "                wovec[i] = windex[sim_word]\n",
        "            else:\n",
        "                wovec[i] = 0\n",
        "        i = i + 1\n",
        "    return wovec\n",
        "\n",
        "def word_order_similarity(sentence_1, sentence_2):\n",
        "    \"\"\"\n",
        "    Computes the word-order similarity between two sentences as the normalized\n",
        "    difference of word order between the two sentences.\n",
        "    \"\"\"\n",
        "    words_1 = nltk.word_tokenize(sentence_1)\n",
        "    words_2 = nltk.word_tokenize(sentence_2)\n",
        "    joint_words = list(set(words_1).union(set(words_2)))\n",
        "    windex = {x[1]: x[0] for x in enumerate(joint_words)}\n",
        "    r1 = word_order_vector(words_1, joint_words, windex)\n",
        "    r2 = word_order_vector(words_2, joint_words, windex)\n",
        "    return 1.0 - (np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2))\n",
        "\n",
        "######################### overall similarity ##########################\n",
        "\n",
        "def similarity(sentence_1, sentence_2, info_content_norm):\n",
        "    \"\"\"\n",
        "    Calculate the semantic similarity between two sentences. The last \n",
        "    parameter is True or False depending on whether information content\n",
        "    normalization is desired or not.\n",
        "    \"\"\"\n",
        "    return DELTA * semantic_similarity(sentence_1, sentence_2, info_content_norm) + \\\n",
        "        (1.0 - DELTA) * word_order_similarity(sentence_1, sentence_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6fb4064-aa17-fef7-b258-dbec789806fa"
      },
      "outputs": [],
      "source": [
        "#print(train_df.loc[train_df['is_duplicate'] == 1][['question1','question2']])\n",
        "for i in [5,7,11,12,13,15,16,18]:\n",
        "    print(train_df['question1'][i])\n",
        "    print(train_df['question2'][i])\n",
        "    print(train_df['is_duplicate'][i])\n",
        "    print(similarity(train_df['question1'][i], train_df['question2'][i], False))\n",
        "    print(similarity(train_df['question1'][i], train_df['question2'][i], True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc18e744-c5db-ebe5-646d-48b363e1247c"
      },
      "outputs": [],
      "source": [
        "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0):\n",
        "        params = {}\n",
        "        params[\"objective\"] = \"binary:logistic\"\n",
        "        params['eval_metric'] = 'logloss'\n",
        "        params[\"eta\"] = 0.02\n",
        "        params[\"subsample\"] = 0.7\n",
        "        params[\"min_child_weight\"] = 1\n",
        "        params[\"colsample_bytree\"] = 0.7\n",
        "        params[\"max_depth\"] = 4\n",
        "        params[\"silent\"] = 1\n",
        "        params[\"seed\"] = seed_val\n",
        "        num_rounds = 250 \n",
        "        plst = list(params.items())\n",
        "        xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
        "\n",
        "        if test_y is not None:\n",
        "                xgtest = xgb.DMatrix(test_X, label=test_y)\n",
        "                watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
        "                model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=10)\n",
        "        else:\n",
        "                xgtest = xgb.DMatrix(test_X)\n",
        "                model = xgb.train(plst, xgtrain, num_rounds)\n",
        "                \n",
        "        pred_test_y = model.predict(xgtest)\n",
        "\n",
        "        loss = 1\n",
        "        if test_y is not None:\n",
        "                loss = log_loss(test_y, pred_test_y)\n",
        "                return pred_test_y, loss, model\n",
        "        else:\n",
        "            return pred_test_y, loss, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "22d7ae60-c4f3-561d-4aec-6ff6e222da67"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
        "                              GradientBoostingClassifier)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "\n",
        "def fit(X_train, y_train, X_test, y_test):\n",
        "    n_estimator = 100\n",
        "\n",
        "    rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator)\n",
        "    rt_lm = LogisticRegression()\n",
        "    pipeline = make_pipeline(rt, rt_lm)\n",
        "\n",
        "    rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\n",
        "    rf_enc = OneHotEncoder()\n",
        "    rf_lm = LogisticRegression()\n",
        "\n",
        "    grd = GradientBoostingClassifier(n_estimators=n_estimator)\n",
        "    grd_enc = OneHotEncoder()\n",
        "    grd_lm = LogisticRegression() \n",
        "    \n",
        "    X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train,\n",
        "                                                            y_train,\n",
        "                                                            test_size=0.5)\n",
        "    print('random trees + lt')\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred_rt = pipeline.predict_proba(X_test)[:, 1]\n",
        "    fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)\n",
        "    #print(pd.DataFrame([pipeline.predict(X_test), y_test]).head())\n",
        "    print(log_loss(y_test, pipeline.predict_proba(X_test)))\n",
        "    \n",
        "    print('random forest + lt')\n",
        "    rf.fit(X_train, y_train)\n",
        "    rf_enc.fit(rf.apply(X_train))\n",
        "    rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n",
        "    \n",
        "    y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]\n",
        "    fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)\n",
        "    print(log_loss(y_test, rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))))\n",
        "    \n",
        "    print('gradient boosting + lt')\n",
        "    grd.fit(X_train, y_train)\n",
        "    grd_enc.fit(grd.apply(X_train)[:, :, 0])\n",
        "    grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n",
        "\n",
        "    y_pred_grd_lm = grd_lm.predict_proba(\n",
        "        grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\n",
        "    fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n",
        "    print(log_loss(y_test, grd_lm.predict_proba(\n",
        "        grd_enc.transform(grd.apply(X_test)[:, :, 0]))))\n",
        "\n",
        "    print('gradient boosting')\n",
        "    # The gradient boosted model by itself\n",
        "    y_pred_grd = grd.predict_proba(X_test)[:, 1]\n",
        "    fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
        "    print(log_loss(y_test, grd.predict_proba(X_test)))\n",
        "\n",
        "    print('random forest')\n",
        "    # The random forest model by itself\n",
        "    y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
        "    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
        "    print(log_loss(y_test, rf.predict_proba(X_test)))\n",
        "    \n",
        "    plt.figure(1)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\n",
        "    plt.plot(fpr_rf, tpr_rf, label='RF')\n",
        "    plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\n",
        "    plt.plot(fpr_grd, tpr_grd, label='GBT')\n",
        "    plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\n",
        "    plt.xlabel('False positive rate')\n",
        "    plt.ylabel('True positive rate')\n",
        "    plt.title('ROC curve')\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f5a66c3d-6426-0b23-d24f-0bb4e1aea25c"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
        "                              GradientBoostingClassifier)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def fit(X_train, y_train, X_test, y_test):\n",
        "    ens = pd.DataFrame()\n",
        "    n_estimator = 100\n",
        "\n",
        "    rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator)\n",
        "    rt_lm = LogisticRegression()\n",
        "    pipeline = make_pipeline(rt, rt_lm)\n",
        "\n",
        "    rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\n",
        "    rf_enc = OneHotEncoder()\n",
        "    rf_lm = LogisticRegression()\n",
        "\n",
        "    grd = GradientBoostingClassifier(n_estimators=n_estimator)\n",
        "    grd_enc = OneHotEncoder()\n",
        "    grd_lm = LogisticRegression() \n",
        "    \n",
        "    X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train,\n",
        "                                                            y_train,\n",
        "                                                            test_size=0.5)\n",
        "    print('random trees + lt')\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred_rt = pipeline.predict_proba(X_test)[:, 1]\n",
        "    fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)\n",
        "    ens['rt_lt'] = pipeline.predict(X_test)\n",
        "    print(log_loss(y_test, pipeline.predict_proba(X_test)))\n",
        "    \n",
        "    print('random forest + lt')\n",
        "    rf.fit(X_train, y_train)\n",
        "    rf_enc.fit(rf.apply(X_train))\n",
        "    rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n",
        "    \n",
        "    y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]\n",
        "    fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)\n",
        "    ens['rf_lm'] = rf_lm.predict(rf_enc.transform(rf.apply(X_test)))\n",
        "    print(log_loss(y_test, rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))))\n",
        "    \n",
        "    print('gradient boosting + lt')\n",
        "    grd.fit(X_train, y_train)\n",
        "    grd_enc.fit(grd.apply(X_train)[:, :, 0])\n",
        "    grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n",
        "\n",
        "    y_pred_grd_lm = grd_lm.predict_proba(\n",
        "        grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\n",
        "    fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n",
        "    ens['grd_lm'] = grd_lm.predict(grd_enc.transform(grd.apply(X_test)[:, :, 0]))\n",
        "    print(log_loss(y_test, grd_lm.predict_proba(\n",
        "        grd_enc.transform(grd.apply(X_test)[:, :, 0]))))\n",
        "\n",
        "    print('gradient boosting')\n",
        "    # The gradient boosted model by itself\n",
        "    y_pred_grd = grd.predict_proba(X_test)[:, 1]\n",
        "    fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
        "    ens['grd'] = grd.predict(X_test)\n",
        "    print(log_loss(y_test, grd.predict_proba(X_test)))\n",
        "\n",
        "    print('random forest')\n",
        "    # The random forest model by itself\n",
        "    y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
        "    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
        "    ens['rf'] = rf.predict(X_test)\n",
        "    print(log_loss(y_test, rf.predict_proba(X_test)))\n",
        "    \n",
        "    return ens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ebc6298c-ea81-90bf-3f15-3179ec264e22"
      },
      "outputs": [],
      "source": [
        "#train_X = np.vstack( np.array(train_df.apply(lambda row: feature_extraction(row), axis=1)) ) \n",
        "#test_X = np.vstack( np.array(test_df.apply(lambda row: feature_extraction(row), axis=1)) )\n",
        "train_X = data.as_matrix()\n",
        "train_y = np.array(train_df[\"is_duplicate\"])\n",
        "test_id = np.array(test_df[\"test_id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ba42a588-f909-09b7-d636-29e2e8c0be48"
      },
      "outputs": [],
      "source": [
        "train_X_dup = train_X[train_y==1]\n",
        "train_X_non_dup = train_X[train_y==0]\n",
        "\n",
        "train_X = np.vstack([train_X_non_dup, train_X_dup, train_X_non_dup, train_X_non_dup])\n",
        "train_y = np.array([0]*train_X_non_dup.shape[0] + [1]*train_X_dup.shape[0] + [0]*train_X_non_dup.shape[0] + [0]*train_X_non_dup.shape[0])\n",
        "del train_X_dup\n",
        "del train_X_non_dup\n",
        "print(\"Mean target rate : \",train_y.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d8101094-885c-012b-9672-52860ae9cae7"
      },
      "outputs": [],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
        "    dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    #preds, lloss, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
        "    ens = fit(dev_X, dev_y, val_X, val_y)\n",
        "    ens['avg'] = ens.mean(axis=1)\n",
        "    ens['median'] = ens.median(axis=1)\n",
        "    #print(accuracy_score(ens['avg'], val_y))\n",
        "    #print(accuracy_score(ens['median'], val_y))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "527ee433-09bb-3e16-07d1-a758f4842dd0"
      },
      "outputs": [],
      "source": [
        "print(ens.mean(axis=0))\n",
        "\n",
        "print(accuracy_score(ens['median'].as_matrix().astype(int),val_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "209e9a1f-e7df-5557-7156-1f3727da4cbe"
      },
      "outputs": [],
      "source": [
        "xgb.plot_importance(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "585f114c-4f41-a128-54a2-d0e21a07aaad"
      },
      "outputs": [],
      "source": [
        "testz = pd.DataFrame()\n",
        "testz['word_match_share'] = test_df.apply(lambda row: word_match_share(row), axis=1)\n",
        "print('5')\n",
        "testz['tfidf_word_match_share'] = test_df.apply(lambda row: tfidf_word_match_share(row), axis=1)\n",
        "print('0')\n",
        "testz['len_q1'] = test_df.question1.apply(lambda x: len(str(x)))\n",
        "testz['len_q2'] = test_df.question2.apply(lambda x: len(str(x)))\n",
        "testz['diff_len'] = data.len_q1 - data.len_q2\n",
        "print('1')\n",
        "testz['len_char_q1'] = test_df.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "testz['len_char_q2'] = test_df.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "testz['len_word_q1'] = test_df.question1.apply(lambda x: len(str(x).split()))\n",
        "testz['len_word_q2'] = test_df.question2.apply(lambda x: len(str(x).split()))\n",
        "print('2')\n",
        "testz['common_words'] = test_df.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
        "print('3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6d6a1eec-a20e-4778-6d14-219bc3c0d966"
      },
      "outputs": [],
      "source": [
        "zzz='''testz['fuzz_qratio'] = test_df.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "print('1')\n",
        "testz['fuzz_WRatio'] = test_df.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "testz['fuzz_partial_ratio'] = test_df.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "print('2')\n",
        "testz['fuzz_partial_token_set_ratio'] = test_df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "testz['fuzz_partial_token_sort_ratio'] = test_df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "print('3')\n",
        "testz['fuzz_token_set_ratio'] = test_df.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "testz['fuzz_token_sort_ratio'] = test_df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15917b34-8739-bf1b-2896-0be17319657e"
      },
      "outputs": [],
      "source": [
        "s3 = testz.as_matrix()\n",
        "s4 = np.vstack( np.array(test_df.apply(lambda row: feature_extraction(row), axis=1)) )\n",
        "print(s3.shape, s4.shape)\n",
        "final_test = np.hstack((s3, s4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "56176a8f-ca0b-8498-170e-bd01899e8ddf"
      },
      "outputs": [],
      "source": [
        "print(final.shape, final_test.shape)\n",
        "testz.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8795006f-8cad-485a-31d0-9b15bff41268"
      },
      "outputs": [],
      "source": [
        "xgtest = xgb.DMatrix(final_test)\n",
        "preds = model.predict(xgtest)\n",
        "\n",
        "out_df = pd.DataFrame({\"test_id\":test_id, \"is_duplicate\":preds})\n",
        "out_df.to_csv(\"idkhowtosubmit.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}