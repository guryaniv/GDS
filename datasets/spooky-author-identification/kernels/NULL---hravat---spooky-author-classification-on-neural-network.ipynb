{"cells": [{"source": ["#####while this model scores low on accuracy it is still a working version\n", "###############of nueral net using tensorfl\n", "##import necessary packages\n", "#####\n", "import tensorflow as tf \n", "import numpy as np # linear algebra\n", "import scipy as sp \n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib\n", "import seaborn as sns \n", "import nltk\n", "from nltk.corpus import stopwords\n", "import wordcloud\n", "from wordcloud import WordCloud, STOPWORDS\n", "from matplotlib import pyplot as plt\n", "##read in the data file an display \n", "df_spooky_author=pd.read_csv('../input/train.csv')\n", "df_spooky_author\n", "\n", "sentence_list=df_spooky_author['text'].values.tolist()\n", "author_list=df_spooky_author['author'].values.tolist()\n", "id_list=df_spooky_author['id'].values.tolist()\n", "sentence_list\n", "author_list\n", "\n", "\n", "combined_list=[list(author_list) for author_list in zip(author_list, sentence_list)]\n", "\n", "tokenized_list=[]\n", "\n", "for authors,sentence in combined_list:\n", "    tokenized_list.append([authors,nltk.word_tokenize(sentence)])\n", "\n", "tokenized_list\n", "\n", "##make individual authorwise list of words after removing stop words.\n", "##This is to prepare the data for wordcloud\n", "stop = set(stopwords.words('english'))    \n", "\n", "tokenized_stop_words_list=[]\n", "\n", "\n", "for author,sentence in tokenized_list:\n", "    tokenized_stop_words_list_temp=[]   \n", "    for word in sentence:\n", "        if not word in stop:\n", "            tokenized_stop_words_list_temp.append(word)\n", "    tokenized_stop_words_list.append([author,tokenized_stop_words_list_temp])        \n", "            \n", "tokenized_stop_words_list\n", "\n", "tokenized_words_EAP=[]\n", "tokenized_words_MWS=[]\n", "tokenized_words_HPL=[]\n", "\n", "for author,sentence in tokenized_stop_words_list:\n", "    if author=='EAP':\n", "        for words in sentence:\n", "            tokenized_words_EAP.append(words)\n", "    if author=='MWS':\n", "        for words in sentence:\n", "            tokenized_words_MWS.append(words)\n", "    if author=='HPL':\n", "        for words in sentence:\n", "            tokenized_words_HPL.append(words)\n", "            \n", "##word cloud for HPLovenCraft            \n", "plt.figure(figsize=(30,30))\n", "wc = WordCloud(background_color=\"black\", max_words=10000, \n", "               stopwords=stop, max_font_size= 40)\n", "wc.generate(\" \".join(tokenized_words_HPL))\n", "##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n", "##Uncomment below line and run to see wordcloud for HP Lovencraft\n", "##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n", "plt.axis('off')\n", "\n", "\n", "\n", "##word cloud for Edgar Allen Poe         \n", "plt.figure(figsize=(30,30))\n", "wc = WordCloud(background_color=\"black\", max_words=10000, \n", "               stopwords=stop, max_font_size= 40)\n", "wc.generate(\" \".join(tokenized_words_EAP))\n", "##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n", "##Uncomment below line and run to see wordcloud for Edgar Allen Poe\n", "##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n", "plt.axis('off')\n", "\n", "\n", "\n", "##word cloud for Mary Shelley            \n", "plt.figure(figsize=(30,30))\n", "wc = WordCloud(background_color=\"black\", max_words=10000, \n", "               stopwords=stop, max_font_size= 40)\n", "wc.generate(\" \".join(tokenized_words_HPL))\n", "##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n", "##Uncomment below line and run to see wordcloud for Mary Shelley \n", "##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n", "plt.axis('off')\n", "\n", "df_test=pd.DataFrame(tokenized_words_HPL)\n", "df_test[0].value_counts()\n", "\n", "\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import preprocessing  \n", "from sklearn import linear_model\n", "from sklearn import metrics\n", "from sklearn.metrics import accuracy_score\n", "\n", "## TF-IDF implementation\n", "count_vect = CountVectorizer()\n", "\n", "sentence_train_counts = count_vect.fit_transform(sentence_list)\n", "sentence_train_counts.shape\n", "\n", "\n", "tfidf_transformer = TfidfTransformer()\n", "sentence_train_tfidf = tfidf_transformer.fit_transform(sentence_train_counts)\n", "sentence_train_tfidf\n", "\n", "sentence_train_tfidf.shape\n", "len(sentence_list)\n", "\n", "##Label Encoder to encode values\n", "le = preprocessing.LabelEncoder()\n", "le.fit(author_list)\n", "\n", "list(le.classes_)\n", "\n", "author_list_encoded=le.transform(author_list)\n", "print(author_list_encoded)\n", "\n", "\n", "####One Hot encoding\n", "#onehot_encoder = preprocessing.OneHotEncoder()\n", "#author_list_oh_encoded = onehot_encoder.fit_transform(author_list_encoded)\n", "\n", "#author_list\n", "#author_list_encoded\n", "#print(author_list_oh_encoded.size)\n", "\n", "########Setting up tensor flow\n", "learning_rate = 0.05\n", "epochs = 3\n", "batch_size = 2000\n", "\n", "# declare the training data placeholders\n", "# There are a total of 19579 examples \n", "# There are a total of 25068 inputs\n", "x = tf.placeholder(tf.float32, [None, 25068])\n", "\n", "# 3 digits for the 3 authors\n", "y = tf.placeholder(tf.float32, [None, 3])\n", "\n", "###declare number of nodes and layers\n", "inputs=25068\n", "hidden_layer_1_nodes=100\n", "hidden_layer_2_nodes=100\n", "output_layer=3\n", "\n", "####declare the weights and biases for the nueral network\n", "\n", "# Weights and biases between input layer and hidden layer 1 \n", "W1 = tf.Variable(tf.random_normal([inputs,hidden_layer_1_nodes], stddev=0.03), name='W1')\n", "b1 = tf.Variable(tf.random_normal([hidden_layer_1_nodes]), name='b1')\n", "\n", "# Weights and biases between hidden layer 1 and hidden layer2\n", "W2 = tf.Variable(tf.random_normal([hidden_layer_1_nodes,hidden_layer_2_nodes], stddev=0.03), name='W2')\n", "b2 = tf.Variable(tf.random_normal([hidden_layer_2_nodes]), name='b2')\n", "\n", "# Weights and biases between hidden layer 2 and ourput layer\n", "W3 = tf.Variable(tf.random_normal([hidden_layer_2_nodes,output_layer], stddev=0.03), name='W3')\n", "b3 = tf.Variable(tf.random_normal([output_layer]), name='b3')\n", "\n", "\n", "####Calculate output of hidden layer 1\n", "hidden_out_1 = tf.add(tf.matmul(x, W1), b1)\n", "hidden_out_1 = tf.nn.sigmoid(hidden_out_1)\n", "\n", "####Calculate output of hidden layer 2\n", "hidden_out_2 = tf.add(tf.matmul(hidden_out_1, W2), b2)\n", "hidden_out_2 = tf.nn.sigmoid(hidden_out_2)\n", "\n", "####Calculate output of output layer\n", "final_output = tf.nn.softmax(tf.add(tf.matmul(hidden_out_2, W3), b3))\n", "final_output=tf.nn.sigmoid(final_output)\n", "\n", "\n", "#limit the output to 1e-10, 0.9999999 to prevent o from being retutned\n", "final_output_clipped = tf.clip_by_value(final_output, 1e-10, 0.9999999)\n", "\n", "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(final_output_clipped)\n", "                         + (1 - y) * tf.log(1 - final_output_clipped), axis=1))\n", "\n", "\n", "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n", "\n", "# varaibles initialzer\n", "init_op = tf.global_variables_initializer()\n", "\n", "# define an accuracy assessment operation\n", "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(final_output, 1))\n", "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n", "\n", "X_scale_train,X_scale_test,Y_scale_train,Y_scale_test = train_test_split(sentence_train_tfidf,author_list_encoded,test_size=0.3,random_state=78)\n", "\n", "y_onehot_labels_train = tf.one_hot(Y_scale_train, 3)\n", "y_onehot_labels_test = tf.one_hot(Y_scale_test, 3)\n", "\n", "X_scale_train_toarray=X_scale_train.toarray()\n", "X_scale_test_toarray=X_scale_test.toarray()\n", "\n", "\n", "#######prepare the preidction data\n", "df_spooky_author_test=pd.read_csv('../input/test.csv')\n", "df_spooky_author_test\n", "\n", "sentence_list_test=df_spooky_author_test['text'].values.tolist()\n", "id_list_test=df_spooky_author_test['id'].values.tolist()\n", "\n", "\n", "#####Very important use transorm here instread of fit transofrm\n", "#####Training data has been fitted above hence here we need to use only transform\n", "##### If fit transform is used we get dimensional mismatch while making predictions from the trained model\n", "sentence_test_count=count_vect.transform(sentence_list_test)\n", "sentence_test_count.shape\n", "\n", "sentence_test_tfidf = tfidf_transformer.transform(sentence_test_count)\n", "sentence_test_tfidf_to_array=sentence_test_tfidf.toarray()\n", "\n", "prediction=tf.argmax(y,1)\n", "\n", "\n", "\n", "# start the session\n", "with tf.Session() as sess:\n", "    # initialise the variables\n", "    sess.run(init_op)\n", "    total_batch = 1 ##int(len(Y_scale_train) / batch_size)\n", "    \n", "    \n", "    for epoch in range(epochs):\n", "        print(\"Epoch:\", (epoch + 1))\n", "        avg_cost = 0\n", "        for i in range(total_batch):\n", "            print('Batch:-',i)\n", "            _, c = sess.run([optimiser, cross_entropy], \n", "                         feed_dict={x: X_scale_train_toarray, y: y_onehot_labels_train.eval()}) \n", "        avg_cost += c / total_batch\n", "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n", "    print(sess.run(accuracy, feed_dict={x: X_scale_test_toarray , y: y_onehot_labels_test.eval()}))        \n", "    ##print(sess.run(y,prediction.eval(feed_dict={x: sentence_test_tfidf_to_array,y:})))\n", "    final_pred=sess.run(final_output, feed_dict={x: sentence_test_tfidf_to_array})   \n", "    \n", "    \n", "####Prepareoutput forsubmission\n", "df_X_prob_predict=pd.DataFrame(final_pred)\n", "df_X_prob_predict.columns=['EAP', 'HPL', 'MWS']\n", "\n", "df_X_prob_predict\n", "df_X_ID=pd.DataFrame(id_list_test)\n", "df_X_ID.columns=['id']\n", "df_predict_final=df_X_ID.join(df_X_prob_predict)\n", "df_predict_final\n", "\n", "###Export Data to csv for final submission\n", "df_predict_final.to_csv('hravat_spooky_autor_predict.csv',sep=',')\n", "\n", "\n"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "22b5ecdf-73f3-45dd-b400-780b58b35d1b", "_kg_hide-output": true, "_kg_hide-input": false, "scrolled": false, "_uuid": "056ff353273e0845a64b92eb2d2322134aa1852a"}, "execution_count": 3}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "mimetype": "text/x-python", "version": "3.6.3", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1}