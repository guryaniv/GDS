{"metadata": {"language_info": {"name": "python", "version": "3.6.3", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "ff735d409bf28a7a5b3632d1a58f7d1708e53759", "_cell_guid": "2cac2a00-aec7-409b-9dbd-8ba33d94db76"}, "source": ["Vladimir Demidov has [an excellent kernel](https://www.kaggle.com/yekenot/simple-stacker-lb-0-284) that shows how to do stacking elegantly and also shows (via its public leaderboard score) why to do stacking.  But there were a couple of things I wanted to do differently, so I decided to release my own version.  Aside from being a notebook, this is almost identical to the original, except for two substantive changes (and a few minor cosmetic ones).\n", "\n", "First, I apply a log-odds transformation to the base models' predictions.  Since the top-level model is a logistic regression, it takes a linear combination of its inputs and then applies a logistic transformation (the inverse of log-odds) to the result.  If the inputs are themselves expressed as probabilities, then it's kind of like doing the logistic transformation twice (and if one of the base models were a logistic regression, it would be exactly like doing the logistic transformation twice), which would be hard to justify.  To put the base model predictions in \"units that a linear model understands,\" I express them as log odds ratios rather than probabilities.\n", "\n", "Second, I fit the logistic regression without an intercept.  Although the intercept might be necessary without the log-odds transformation, the regression with log odds gives reasonable results without it, and it's not clear why it should be there.  In my view, since the gini coefficient depends on order, an added constant has no substantive meaning, and the opportunity to add one is simply an opportunity to overfit.  (If we cared about the actual probabilities, you could make a case for using a constant term as being sort of like adding another base model that always predicts the same number, but here that justification doesn't apply.)\n", "\n", "My model (as you should be able to see by comparing this notebook to his log) produces a very slightly higher CV score (\"Stacker score\") than Vladimir's.  I haven't submitted the output, but I expect it would get the same public leaderboard score.  But this is one of those cases where you have to make a judgment based on what you think is a better modeling practice rather than what results it gets in limited tests."]}, {"cell_type": "code", "metadata": {"_uuid": "db6d6a7bd282140d3f80d931e32011c3d4f031fd", "collapsed": true, "_cell_guid": "c57e3bd4-a98a-449e-bc08-d8af8a6fe236"}, "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.model_selection import cross_val_score\n", "\n", "from xgboost import XGBClassifier\n", "from lightgbm import LGBMClassifier\n", "from catboost import CatBoostClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n", "from sklearn.metrics import roc_auc_score\n", "\n", "# Regularized Greedy Forest\n", "from rgf.sklearn import RGFClassifier     # https://github.com/fukatani/rgf_python"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "30f70e2d06c42765668ec92b7e7f8c1dcb89818e", "collapsed": true, "_cell_guid": "fa13ecd1-ee03-4722-9ed2-b9c09deac4b9"}, "source": ["train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "a44ce0e2d9e05a8b5eb037cf5233bc709a60f7b0", "collapsed": true, "_cell_guid": "1bb44551-07b4-4815-8e84-4137b08324bd"}, "source": ["# Preprocessing \n", "id_test = test['id'].values\n", "target_train = train['target'].values\n", "\n", "train = train.drop(['target','id'], axis = 1)\n", "test = test.drop(['id'], axis = 1)\n", "\n", "\n", "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n", "train = train.drop(col_to_drop, axis=1)  \n", "test = test.drop(col_to_drop, axis=1)  \n", "\n", "\n", "train = train.replace(-1, np.nan)\n", "test = test.replace(-1, np.nan)\n", "\n", "\n", "cat_features = [a for a in train.columns if a.endswith('cat')]\n", "\n", "# Make sure both train & test have a full set of dummies\n", "train['intrain'] = True  \n", "test['intrain'] = False\n", "both = pd.concat([train,test],axis=0)\n", "for column in cat_features:\n", "\ttemp = pd.get_dummies(pd.Series(both[column]))\n", "\tboth = pd.concat([both,temp],axis=1)\n", "\tboth = both.drop([column],axis=1)\n", "train = both[both.intrain==True].drop(['intrain'],axis=1)\n", "test = both[both.intrain==False].drop(['intrain'],axis=1)\n", "\n", "print(train.values.shape, test.values.shape)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "1cb52ec099b2690ea35e1f97eacd5c5d916a6b3f", "collapsed": true, "_cell_guid": "6477d361-6f04-484f-8f04-e65b72e8768b"}, "source": ["class Ensemble(object):\n", "    def __init__(self, n_splits, stacker, base_models):\n", "        self.n_splits = n_splits\n", "        self.stacker = stacker\n", "        self.base_models = base_models\n", "\n", "    def fit_predict(self, X, y, T):\n", "        X = np.array(X)\n", "        y = np.array(y)\n", "        T = np.array(T)\n", "\n", "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n", "\n", "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n", "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n", "        for i, clf in enumerate(self.base_models):\n", "\n", "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n", "\n", "            for j, (train_idx, test_idx) in enumerate(folds):\n", "                X_train = X[train_idx]\n", "                y_train = y[train_idx]\n", "                X_holdout = X[test_idx]\n", "#                y_holdout = y[test_idx]\n", "\n", "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n", "                clf.fit(X_train, y_train)\n", "#                cross_score = cross_val_score(clf, X_train, y_train, cv=self.n_splits, scoring='roc_auc')\n", "#                print(\"    cross_score: %.5f\" % (cross_score.mean()))\n", "                y_pred = clf.predict_proba(X_holdout)[:,1]                \n", "\n", "                S_train[test_idx, i] = y_pred\n", "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n", "            S_test[:, i] = S_test_i.mean(axis=1)\n", "\n", "            print(\"     Model score: %.5f\\n\" % roc_auc_score(y, S_train[:,i]))\n", "\n", "        self.base_preds = S_test\n", "        \n", "        # Log odds transformation\n", "        almost_zero = 1e-12\n", "        almost_one = 1 - almost_zero  # To avoid division by zero\n", "        S_train[S_train>almost_one] = almost_one\n", "        S_train[S_train<almost_zero] = almost_zero\n", "        S_train = np.log(S_train/(1-S_train))\n", "        S_test[S_test>almost_one] = almost_one\n", "        S_test[S_test<almost_zero] = almost_zero\n", "        S_test = np.log(S_test/(1-S_test))\n", "        \n", "        results = cross_val_score(self.stacker, S_train, y, cv=self.n_splits, scoring='roc_auc')\n", "        print(\"Stacker score: %.5f\" % (results.mean()))\n", "\n", "        self.stacker.fit(S_train, y)\n", "        print( 'Coefficients:', self.stacker.coef_ )\n", "\n", "        res = self.stacker.predict_proba(S_test)[:,1]\n", "        return res"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "2fd027339ab65cd98d58fa0e73c51556a923dfd9", "collapsed": true, "_cell_guid": "6cc335a0-fa2b-4197-8bc8-2d705837bf84"}, "source": ["# LightGBM params\n", "lgb_params = {}\n", "lgb_params['learning_rate'] = 0.02\n", "lgb_params['n_estimators'] = 650\n", "lgb_params['max_bin'] = 10\n", "lgb_params['subsample'] = 0.8\n", "lgb_params['subsample_freq'] = 10\n", "lgb_params['colsample_bytree'] = 0.8   \n", "lgb_params['min_child_samples'] = 500\n", "lgb_params['random_state'] = 99\n", "\n", "\n", "lgb_params2 = {}\n", "lgb_params2['n_estimators'] = 1090\n", "lgb_params2['learning_rate'] = 0.02\n", "lgb_params2['colsample_bytree'] = 0.3   \n", "lgb_params2['subsample'] = 0.7\n", "lgb_params2['subsample_freq'] = 2\n", "lgb_params2['num_leaves'] = 16\n", "lgb_params2['random_state'] = 99\n", "\n", "\n", "lgb_params3 = {}\n", "lgb_params3['n_estimators'] = 1100\n", "lgb_params3['max_depth'] = 4\n", "lgb_params3['learning_rate'] = 0.02\n", "lgb_params3['random_state'] = 99\n", "\n", "\n", "# RandomForest params\n", "#rf_params = {}\n", "#rf_params['n_estimators'] = 650\n", "#rf_params['max_depth'] = 14\n", "#rf_params['min_samples_split'] = 40\n", "#rf_params['min_samples_leaf'] = 35\n", "\n", "\n", "# ExtraTrees params\n", "#et_params = {}\n", "#et_params['n_estimators'] = 300\n", "#et_params['max_features'] = .2\n", "#et_params['max_depth'] = 10\n", "#et_params['min_samples_split'] = 7\n", "#et_params['min_samples_leaf'] = 40\n", "\n", "\n", "# XGBoost params\n", "#xgb_params = {}\n", "#xgb_params['objective'] = 'binary:logistic'\n", "#xgb_params['learning_rate'] = 0.04\n", "#xgb_params['n_estimators'] = 490\n", "#xgb_params['max_depth'] = 4\n", "#xgb_params['subsample'] = 0.9\n", "#xgb_params['colsample_bytree'] = 0.9  \n", "#xgb_params['min_child_weight'] = 10\n", "\n", "\n", "# CatBoost params\n", "#cat_params = {}\n", "#cat_params['iterations'] = 900\n", "#cat_params['depth'] = 8\n", "#cat_params['rsm'] = 0.95\n", "#cat_params['learning_rate'] = 0.03\n", "#cat_params['l2_leaf_reg'] = 3.5  \n", "#cat_params['border_count'] = 8\n", "#cat_params['gradient_iterations'] = 4\n", "\n", "\n", "# Regularized Greedy Forest params\n", "#rgf_params = {}\n", "#rgf_params['max_leaf'] = 2000\n", "#rgf_params['learning_rate'] = 0.5\n", "#rgf_params['algorithm'] = \"RGF_Sib\"\n", "#rgf_params['test_interval'] = 100\n", "#rgf_params['min_samples_leaf'] = 3 \n", "#rgf_params['reg_depth'] = 1.0\n", "#rgf_params['l2'] = 0.5  \n", "#rgf_params['sl2'] = 0.005\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "66dce17a99afdb65474779095db22257994a4202", "collapsed": true, "_cell_guid": "8b1f5da0-35bb-4c52-bd78-f5c6276dd2f4"}, "source": ["lgb_model = LGBMClassifier(**lgb_params)\n", "\n", "lgb_model2 = LGBMClassifier(**lgb_params2)\n", "\n", "lgb_model3 = LGBMClassifier(**lgb_params3)\n", "\n", "#rf_model = RandomForestClassifier(**rf_params)\n", "\n", "#et_model = ExtraTreesClassifier(**et_params)\n", "        \n", "#xgb_model = XGBClassifier(**xgb_params)\n", "\n", "#cat_model = CatBoostClassifier(**cat_params)\n", "\n", "#rgf_model = RGFClassifier(**rgf_params) \n", "\n", "#gb_model = GradientBoostingClassifier(max_depth=5)\n", "\n", "#ada_model = AdaBoostClassifier()\n", "\n", "log_model = LogisticRegression(fit_intercept=False)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "14ea06c5f845f1181f259eaccc1bfe18bee8a75b", "scrolled": true, "collapsed": true, "_cell_guid": "9602585c-a3ad-4f3f-9aba-b4b020e01c01"}, "source": ["stack = Ensemble(n_splits=3,\n", "        stacker = log_model,\n", "        base_models = (lgb_model, lgb_model2, lgb_model3))        \n", "        \n", "y_pred = stack.fit_predict(train, target_train, test)        "], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "5fb310698ca102c915a424db98b0ea376e3f2ca6", "collapsed": true, "_cell_guid": "06863541-8913-42f8-978d-657d89930f0d"}, "source": ["sub = pd.DataFrame()\n", "sub['id'] = id_test\n", "sub['target'] = y_pred\n", "sub.to_csv('stacked_1.csv', index=False)"], "execution_count": null, "outputs": []}], "nbformat_minor": 1, "nbformat": 4}