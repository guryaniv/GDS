{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Read in files with pandas, grouping by the series id\n\ndef read_file(filename):\n    df_csv = pd.read_csv(filename)\n    X_out = np.zeros((len(df_csv)//128,10,128))\n    for group,x in df_csv.groupby('series_id'):\n        X_out[group] = x.values[:,3:].T\n    return X_out\n\nX_data = read_file('../input/X_train.csv')\n\n# Define class - id mapping\n\nclasses = ['fine_concrete', 'concrete', 'soft_tiles', 'tiled', 'soft_pvc',\n           'hard_tiles_large_space', 'carpet', 'hard_tiles', 'wood']\nclass_dict = {x:i for i,x in enumerate(classes)}\n\n# Import labels\n\ny_data_csv = pd.read_csv('../input/y_train.csv')\ny_data = np.array([class_dict[x] for x in y_data_csv.values[:,2]])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import torch\nimport torch.utils.data\nfrom sklearn.model_selection import train_test_split\n\n# Create pytorch dataloaders after doing train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_data, y_data, test_size=0.33, random_state=42\n)\ndef get_dataset(x,y):\n    return torch.utils.data.TensorDataset(torch.FloatTensor(x),torch.LongTensor(y))\n\ntrain_set = get_dataset(X_train, y_train)\nval_set = get_dataset(X_val,y_val)\n\nbatch_size = 32\ntrain_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_set,batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64d8ea29ccebc400e083a7982572d1c849ab7e53"},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# A really simple 1d convnet:\n\nclass ConvNet(nn.Module):\n    def __init__(self, hidden_size=32, in_channels=10, num_classes=9):\n        super(ConvNet, self).__init__()\n        \n        # Map in channels to number of hidden layers, kernel size is 9, stride is 2\n        self.conv = nn.Conv1d(in_channels, hidden_size, 9, stride=2)\n        self.relu = nn.ReLU(inplace=True)\n        \n        # AdaptiveAvgPool1d will let us just specify the size of output\n        # In our case we just average over the whole timeline\n        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n        \n        # We then map the output to the number of classes to finish!\n        self.linear = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        # Normalize data by subtracting the mean\n        x = x - torch.mean(x, -1, keepdim=True)\n        \n        x = self.relu(self.conv(x))\n        x = self.avg_pool(x)\n        return self.linear(x.view(x.size(0),-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b073a2a4f98a5d954219112bb7fd5992107fb30"},"cell_type":"code","source":"# Get started! Using CEL and SGD\n\ntorch.manual_seed(1)\nmodel = ConvNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9df9440b0625e538f4bbf8bd66821b9e6d10f45"},"cell_type":"code","source":"# Training loop -- only check validation every print_freq epochs since it is really fast\n\nepochs = 100\nprint_freq = 10\nfor e in range(epochs):\n    train_loss = 0.0\n    train_correct = 0.0\n    for x,y in train_loader:\n        p = model(x)\n        loss = criterion(p,y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * len(y)\n        _,pred = p.max(1)\n        train_correct += sum(pred == y).float().item()\n    \n    train_loss = train_loss / len(train_set)\n    train_correct = train_correct / len(train_set)\n    \n    if e > 0 and e % print_freq == 0:\n        val_loss = 0.0\n        val_correct = 0.0\n        for x,y in val_loader:\n            p = model(x)\n            loss = criterion(p,y)\n\n            val_loss += loss.item() * len(y)\n            _,pred = p.max(1)\n            val_correct += sum(pred == y).float().item()\n\n        val_loss = val_loss / len(val_set)\n        val_correct = val_correct / len(val_set)\n        print(\"Train loss: {:.4f}, acc: {:.4f};\\t Val loss: {:.4f}, acc: {:.4f}\".format(\n            train_loss,train_correct,val_loss,val_correct\n        ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b97a0153188101773c2a118c82141201b826e426"},"cell_type":"code","source":"# Run on test data\n\nX_test = read_file('../input/X_test.csv')\ny_test = ['']*len(X_test)\nfor i,x in enumerate(X_test):\n    p = model(torch.FloatTensor(x).unsqueeze(0))\n    _,pred = p.max(1)\n    y_test[i] = classes[pred.item()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09037ad827af5ed7a2933b569ab6d7d47a806052"},"cell_type":"code","source":"# Export to CSV\ndf = pd.DataFrame(y_test,columns=['surface'])\ndf.to_csv('submission.csv',index_label='series_id')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"802a872831cdd80e0280a0517d2a214e7ff1d5cf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}