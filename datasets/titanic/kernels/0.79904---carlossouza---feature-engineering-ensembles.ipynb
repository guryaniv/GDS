{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import random\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8df355a13ccc6aab42461fde5dd67977b64091d5"},"cell_type":"markdown","source":"This notebook is based on [A Data Science Framework: To Achieve 99% Accuracy](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy) and [Titanic Basic Approach](https://www.kaggle.com/carlossouza/titanic-basic-approach)."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ndatasets = [train, test]\n\n# Sample data\nprint(train.info())\ntrain.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d39b687c11b0c2ea88046b399e0710e405d44259"},"cell_type":"markdown","source":"# 1. Check what is missing and fill missing values"},{"metadata":{"trusted":true,"_uuid":"994e50a8fba29c4552afd48fd02b209a1b911aa1"},"cell_type":"code","source":"print('Train columns with null values:\\n', train.isnull().sum())\nprint(\"-\"*20)\n\nprint('Test/Validation columns with null values:\\n', test.isnull().sum())\nprint(\"-\"*20)\n\ntrain.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5870892a636b55451cfc39d3ab78efb3d4cd115"},"cell_type":"code","source":"# Completing missing values\nfor dataset in datasets:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n\n# Dropping useless columns\ntrain.drop(columns=['PassengerId', 'Ticket', 'Cabin'], inplace=True)\n\n# Check again what is missing\nprint(train.isnull().sum())\nprint(\"-\"*20)\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d4635e2df9b904c6c37fe7881877e2c2e6d7309"},"cell_type":"markdown","source":"# 2. Feature Engineering\nVery important to read [Data Description](https://www.kaggle.com/c/titanic/data)."},{"metadata":{"trusted":true,"_uuid":"7b385ef4ac45d06fb9e0dd10208ea747921c0c0c"},"cell_type":"code","source":"# First, create a feature with Family Size\nfor dataset in datasets: \n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n# Now, create a binary feature to indicate whether person is alone or not\nfor dataset in datasets:\n    dataset['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d19952ecdb78d17c65326cd2a106991e5051e46d"},"cell_type":"code","source":"sample_name = train.sample(1)['Name'].values[0]\nprint('Name: ' + sample_name)\ntitle = sample_name.split(', ')[1].split('.')[0]\nprint('Title: ' + title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1484a3cefaa3f891c159d1b9a0f12c78c0bfb640"},"cell_type":"code","source":"# Now, create a feature with passenger's Title\nfor dataset in datasets:\n    dataset['Title'] = dataset['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a79ef46d7d2fd95a77b9d65b0104b8c1f39fbedf"},"cell_type":"code","source":"# Now, create bins with Age and Fare\nfor dataset in datasets:\n    #Continuous variable bins; qcut vs cut: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3b471f9af22abcb3d0449d784c0e64e71aa30be"},"cell_type":"code","source":"print(train['Title'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c514397682fc46dd7c3f99d6eacb748226490be"},"cell_type":"code","source":"# Fixing the problem of rare titles\n\nstat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\ntitle_names_train = (train['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\ntitle_names_test = (test['Title'].value_counts() < stat_min)\ntitle_names_all = title_names_train.append(title_names_test)\ntitle_names_union = pd.Series([])\n\nfor index, value in title_names_all.iteritems():\n    title_names_union = title_names_union.set_value(index, value)\n\nfor dataset in datasets:\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names_union.loc[x] == True else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45aa0187c02a7c6814628d78c97969784840616a"},"cell_type":"code","source":"# Preview data again\ntrain.info()\ntest.info()\ntrain.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd6899a85ab0d8f73cb738a6b8ea1b4625448556"},"cell_type":"markdown","source":"# 3. Convert categorical data to dummy variables"},{"metadata":{"trusted":true,"_uuid":"32605b1c6886eb88ddc9dc2218006b01b1664333"},"cell_type":"code","source":"# Code categorical data\nlabel = LabelEncoder()\nfor dataset in datasets:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc95928507ecd4d3e1f068ed225b5f84b9c7758d"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da87935cd2dd0944b02f1910137c78f61a002545"},"cell_type":"markdown","source":"# 4. Final data check before training"},{"metadata":{"trusted":true,"_uuid":"e5af701f2c89ff472a77e305a572fb61ab233878"},"cell_type":"code","source":"print('Train columns with null values: \\n', train.isnull().sum())\nprint(\"-\"*20)\nprint (train.info())\nprint(\"-\"*20)\n\nprint('Test/Validation columns with null values: \\n', test.isnull().sum())\nprint(\"-\"*20)\nprint (test.info())\nprint(\"-\"*20)\n\ntrain.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddcaacc39d460ef90f5f9aceca454dfa67be8a41"},"cell_type":"markdown","source":"# 5. Split training and testing data"},{"metadata":{"trusted":true,"_uuid":"be0cdfbe613bfb2c6cde75f563f5ae05336cab50"},"cell_type":"code","source":"features = ['Sex_Code', 'Pclass', 'Embarked_Code', 'FamilySize', 'Title_Code', 'AgeBin_Code', 'FareBin_Code', 'IsAlone']\ntarget = ['Survived']\n\n#train_X, val_X, train_y, val_y = train_test_split(train[features], train[target], random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ff63a1b72d98864c0bb125055623f9a6a83b96e"},"cell_type":"code","source":"# Doing a first prediction\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\ncv_results = model_selection.cross_validate(XGBClassifier(), train[features], train[target], cv  = cv_split)\nprint(\"XGBoost Train Accuracy Mean: {:,.4f}\".format(cv_results['train_score'].mean()))\nprint(\"XGBoost Test Accuracy Mean: {:,.4f}\".format(cv_results['test_score'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0a5eb5e8549690b9c1e3d2086ac9586cf8c6b4d"},"cell_type":"markdown","source":"# 6. Testing all possible algorithms"},{"metadata":{"trusted":true,"_uuid":"56c0ab4fd0f621bd1e807a377ab64a5fb4f0e8a1"},"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = train[target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, train[features], train[target], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(train[features], train[target])\n    MLA_predict[MLA_name] = alg.predict(train[features])\n    \n    row_index += 1\n\n#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f24b46252f1ca86f649cc514c05c656c52c1bfcc"},"cell_type":"code","source":"# Submitting a first prediction\n#model = XGBClassifier()\n#model.fit(train[features], train[target])\n#predicted_survival = model.predict(test[features])\n#print(predicted_survival)\n#print(len(predicted_survival))\n\n#my_submission = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predicted_survival})\n#my_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b4cf0e2c00908723704aef55af94f6905f4ce13"},"cell_type":"code","source":"#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\n#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c2f8c02047f9a9ab64edfe5686f2954ea720f6b"},"cell_type":"markdown","source":"# 7. Ensembling all possible algorithms"},{"metadata":{"trusted":true,"_uuid":"24b557ebf26f0a111727070c1c2ed13e12e49396"},"cell_type":"code","source":"#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    #SVM: http://scikit-learn.org/stable/modules/svm.html\n    ('svc', svm.SVC(probability=True)),\n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    ('xgb', XGBClassifier())\n]\n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, train[features], train[target], cv  = cv_split)\nvote_hard.fit(train[features], train[target])\n\nprint(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, train[features], train[target], cv  = cv_split)\nvote_soft.fit(train[features], train[target])\n\nprint(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e53f399e4f6eb69158f153a1b998f15974fefecc"},"cell_type":"markdown","source":"# 8. Finetuning hyperparameters"},{"metadata":{"trusted":true,"_uuid":"a5e847322eac32b4b3bda4f8edd2810b24933ec2"},"cell_type":"code","source":"#WARNING: Running is very computational intensive and time expensive.\n#Code is written for experimental/developmental purposes and not production ready!\n\n#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\ngrid_param = [\n            [{\n            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n            'random_state': grid_seed\n            }],\n       \n            [{\n            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n             }],\n\n            [{\n            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=”gini”\n            'max_depth': grid_max_depth, #default=None\n            'random_state': grid_seed\n             }],\n\n            [{\n            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=’deviance’\n            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }],\n\n            [{\n            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=”gini”\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': grid_seed\n             }],\n    \n            [{    \n            #GaussianProcessClassifier\n            'max_iter_predict': grid_n_estimator, #default: 100\n            'random_state': grid_seed\n            }],\n        \n            [{\n            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            #'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n            'random_state': grid_seed\n             }],\n            \n            [{\n            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': grid_ratio, #default: 1.0\n             }],\n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = ‘uniform’\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }],\n    \n            [{\n            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': grid_ratio, #edfault: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': grid_seed\n             }],\n\n            [{\n            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }]   \n        ]\n\nstart_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\nfor clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n    #print(param)\n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(train[features], train[target])\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n\nprint('-'*20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"166a4ad7f37f361a828298b90c6695524dba978a"},"cell_type":"code","source":"# Hard Vote or majority rules w/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, train[features], train[target], cv  = cv_split)\ngrid_hard.fit(train[features], train[target])\n\nprint(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*20)\n\n#Soft Vote or weighted probabilities w/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, train[features], train[target], cv  = cv_split)\ngrid_soft.fit(train[features], train[target])\n\nprint(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eab061cbc29b52bf6ee1ef48e43191979c61ac9c"},"cell_type":"markdown","source":"# Prepare submission"},{"metadata":{"trusted":true,"_uuid":"45ab436be9df68baf7602279ea54507c14522eca"},"cell_type":"code","source":"test['Survived'] = grid_hard.predict(test[features])\nmy_submission = test[['PassengerId','Survived']]\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dce1e3c55777379d997c6c23ab4fbfdee367a52"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}