{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# NN Embedding using Keras Functional API\n\nRecent researhes are exploring the idea of trying to predict outcome of tabular data using DNN and embeddings. Although it's very common for this approach to be used for image classification or NLP, predicting tabular data has not seen it's fair share of NN approach.\n\nMy inspiration to explore this approach came from the following-\n\n- https://arxiv.org/pdf/1703.02596.pdf\n- https://arxiv.org/ftp/arxiv/papers/1701/1701.06624.pdf\n- https://arxiv.org/pdf/1807.04098.pdf\n- https://medium.com/tensorflow/predicting-the-price-of-wine-with-the-keras-functional-api-and-tensorflow-a95d1c2c1b03"},{"metadata":{"trusted":true,"_uuid":"0e7b4d3e0110e34c6b745a947cb56d89e2b4e4a7"},"cell_type":"code","source":"#This is my first competition and first kernel in Kaggle.\n#Any constructive criticism/ suggestion/ feedback will be appreciated :)\n#This was done as part of a Capstone project","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import sys\nprint(sys.version)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da4e967861e01f49f94c5e9b63b8717f7f1d3cf6"},"cell_type":"code","source":"#Had additional packages imported as I's experimenting with various methods\n\n%time\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sbn\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport os\nimport json\nfrom pandas.io.json import json_normalize\nimport datetime\nfrom datetime import datetime\nfrom ast import literal_eval\nimport copy\nimport pydot\nimport warnings\nimport gc\n\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import OneHotEncoder\nimport xgboost as xgb\nfrom sklearn import neighbors\nfrom sklearn.exceptions import DataConversionWarning\n\nfrom keras import models\nfrom keras import optimizers\nfrom keras.models import *\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Input, Dense, Activation, Reshape, Concatenate, Flatten, concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.utils import plot_model\n\nfrom bokeh.core.properties import value\nfrom bokeh.io import show, output_file, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearInterpolator, CategoricalColorMapper\nfrom bokeh.transform import dodge\nfrom bokeh.resources import INLINE\nTOOLS = 'crosshair,save,pan,box_zoom,reset,wheel_zoom'\noutput_notebook()\n\nfrom plotly import __version__\nimport plotly.offline\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode()\n\nprint('Package import complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d65e995685e107b8ffe32f4f7f26df7bc20c8fe4"},"cell_type":"code","source":"#File directory to upload or save data\nDIR = '../input/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c74fa91321ca357862ffa4510b4897cd910617e7"},"cell_type":"code","source":"gc.enable()\n\nfeatures = ['channelGrouping', 'date', 'fullVisitorId', 'visitId',\\\n       'visitNumber', 'visitStartTime', 'device.browser',\\\n       'device.deviceCategory', 'device.isMobile', 'device.operatingSystem',\\\n       'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country',\\\n       'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region',\\\n       'geoNetwork.subContinent', 'totals.bounces', 'totals.hits',\\\n       'totals.newVisits', 'totals.pageviews', 'totals.transactionRevenue',\\\n       'trafficSource.adContent', 'trafficSource.campaign',\\\n       'trafficSource.isTrueDirect', 'trafficSource.keyword',\\\n       'trafficSource.medium', 'trafficSource.referralPath',\\\n       'trafficSource.source', 'customDimensions']\n\ndef load_df(csv_path):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    ans = pd.DataFrame()\n    dfs = pd.read_csv(csv_path, sep=',',\n            converters={column: json.loads for column in JSON_COLUMNS}, \n            dtype={'fullVisitorId': 'str'}, # Important!!\n            chunksize=100000)\n    for df in dfs:\n        df.reset_index(drop=True, inplace=True)\n        for column in JSON_COLUMNS:\n            column_as_df = json_normalize(df[column])\n            column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n            df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n\n        #print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n        use_df = df[features]\n        del df\n        gc.collect()\n        ans = pd.concat([ans, use_df], axis=0).reset_index(drop=True)\n        #print(ans.shape)\n    return ans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d3d4c40d44beb0746788b2424cb6cae01685378"},"cell_type":"code","source":"%%time\n#Import training data\ncsvfile=DIR +'train_v2.csv'\ndf_train=load_df(csvfile)\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"282d849e7389c25d13658f435e0322cb1b6d246e"},"cell_type":"code","source":"%%time\n#Import test data\ncsvfile=DIR +'test_v2.csv'\ndf_test=load_df(csvfile)\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f77d29a4cabcafa658855bb53b9ddf8b8c73298f"},"cell_type":"code","source":"#Create backup and print dimension of file imported after flattening\ntrain_backup = df_train\ntest_backup = df_test\n\nprint('Training file dimension: ',df_train.shape)\nprint('Test file dimension: ',df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"427dc102ef6cc0c7dd633a019447f88a92cb44e7"},"cell_type":"code","source":"#Create a function to format datasets\ndef format_data(df):\n     \n    #Replace redundant words with NaN\n    word_replace=['(not set)', '(none)', '(direct)', '(not provided)','not available in demo dataset', 'unknown.unknown']\n    df = df.replace(word_replace, np.nan)\n    \n    #Replace null values in transactionrevenue column with '0'\n    df['totals_transactionRevenue']=df['totals.transactionRevenue'].fillna(0)\n    \n    #Replace null value in numerical data with '0' and in categorical data with 'Unknown'\n\n    fillna_num = ['totals.bounces',\n              'totals.newVisits',\n              'totals.pageviews',\n              'trafficSource.isTrueDirect']\n\n    fillna_cat = ['trafficSource.keyword',\n            'trafficSource.referralPath',\n            'trafficSource.adContent',\n            'trafficSource.source',\n            'trafficSource.medium',\n            'device.operatingSystem',\n            'geoNetwork.networkDomain',\n            'geoNetwork.subContinent',\n            'geoNetwork.country',\n            'geoNetwork.continent',\n            'device.browser']\n\n    for col in fillna_num:\n        df[col] = df[col].fillna(0)\n\n    for col in fillna_cat:\n        df[col] = df[col].fillna('Unknown')\n    \n    #Print number and percentage of NaN in the dataset\n    count = df.isnull().sum().sort_values(ascending = False)\n    percentage = ((df.isnull().sum()/df.isnull().count())*100).sort_values(ascending = False)\n    unique = df.nunique()\n    missing_data = pd.concat([count, percentage, unique], axis=1, keys=['Count', 'Percentage', 'Unique'], sort=False)\n    print('Table showing percentage of missing data: \\n',missing_data, '\\n')\n    \n    #Delete columns that have more than 50% data missing\n    df = df.drop((missing_data[missing_data['Percentage'] > 50]).index,1)\n    \n    #Extract feature from \"visitStartTime\" column, and then add back columns to the dataframe \n\n    df['Date_time'] = pd.to_datetime(df['visitStartTime'].astype(int), unit='s')\n    df['Hour'] = df['Date_time'].dt.hour\n    df['Day'] = df['Date_time'].dt.day\n    df['Day_of_week'] = df['Date_time'].dt.dayofweek\n    df['Month'] = df['Date_time'].dt.month\n    df['Week_number'] = df['Date_time'].dt.strftime('%V')\n    df['Year'] = df['Date_time'].dt.year\n    \n    df['Date_time'] = pd.to_datetime(df['Date_time'],format='%Y%m%d %H:%M:%S')\n\n    columns_to_drop = ['date','visitStartTime']\n\n    df = df.drop(columns_to_drop, axis=1)\n    \n    print('Columns with more than 50% of missing data have been removed \\n')\n    print('Shape of formatted dataframe: ',df.shape, '\\n')\n    print('Highest count of null values in formatted dataframe: ', max(df.isnull().sum()))\n           \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"403c803df2ef278cc015de333206c6f7dfab0745"},"cell_type":"code","source":"%%time\n#Generate clean training data\ndf_train = format_data(train_backup)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85afdb60df0793e7199576d66e157f4d2ac9c1e9"},"cell_type":"code","source":"%%time\n#Generate clean test data\ndf_test = format_data(test_backup)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"595ac69200db245f1e04052993daa537e01ad2f1"},"cell_type":"code","source":"df_train.to_csv('df_train_clean.csv',header = True, index=False)\nprint('df_train export complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c82250162b47dc0e2c87905a6493808c359df37d"},"cell_type":"code","source":"df_test.to_csv('df_test_clean.csv',header = True, index=False)\nprint('df_test export complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8d7fda7a2987b0b4838bbe7bb6f9b3f67c45905"},"cell_type":"code","source":"#Check if the same columns occur in both dataframes\n(df_train.columns.intersection(df_test.columns)).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d36c184a84bf07ab86a7e348463fb067a158e96"},"cell_type":"code","source":"#See what the training data set looks like after formatting\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5a3b089af6cd43b3d28248064a74c459373f0d2"},"cell_type":"code","source":"#See what the test data set looks like after formatting\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ecfd410ce6da6ebd3b23cd25af8ae9c70e30f62"},"cell_type":"code","source":"#Convert from boolean/object to int\n\ncol_to_int = ['device.isMobile', 'Week_number', 'totals.bounces','totals.hits', 'totals.newVisits', 'totals.pageviews']\n\ndf_train[col_to_int] = df_train[col_to_int].astype(int)\ndf_test[col_to_int] = df_test[col_to_int].astype(int)\n\n#Convert totals_transactionRevenue to float for log transform\ndf_train['totals_transactionRevenue'] = df_train['totals_transactionRevenue'].astype ('float64')\ndf_test['totals_transactionRevenue'] = df_test['totals_transactionRevenue'].astype ('float64')\n\n#Log transfrom revenue values\ndf_train['totals_transactionRevenue'] = np.log1p(df_train['totals_transactionRevenue'])\ndf_test['totals_transactionRevenue'] = np.log1p(df_test['totals_transactionRevenue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"486a91441ab88f841c9eecbaaf420abd097b625c"},"cell_type":"code","source":"print(df_train.info())\nprint(df_test.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db527ff3b44714cadaa85d24d4fb092f72b5fa62"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true,"_uuid":"3291ae7734805904b1fda3e9b09727b5d320a8c4"},"cell_type":"code","source":"#Combine train and test data for exploratory analysis\ndf_combined = pd.concat([df_train, df_test], axis=0)\n\n#Add a column that would say \"1\" for revenue contributor, \"0\" otherwise\ndf_combined['revenue_contributor'] = np.where(df_combined['totals_transactionRevenue']>0,1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efa15603d0d7816483e37e47c69148dc7f5f18a6"},"cell_type":"code","source":"#Set style context for Seaborn\nsbn.set_context(font_scale = 2, rc = {\"font.size\":15, \"axes.labelsize\":20, \"grid.linewidth\": 0.1})\nsbn.set_style(\"whitegrid\")\nmypalette = [\"#57606f\",\"#3742fa\",\"#5352ed\",\"#1e90ff\",\"#70a1ff\",\n             \"#2ed573\",\"#ffa502\",\"#ff6348\",\"#ff4757\",\"#ced6e0\" ]\ncolor = sbn.set_palette(mypalette)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a117dd3709a8db45b186eb73330c398891a1140"},"cell_type":"code","source":"#Analyze geospatial distribution by visit volume\n\n#Referece: https://www.kaggle.com/arthurtok/generation-unemployed-interactive-plotly-visuals\n\ntmp = df_combined[\"geoNetwork.country\"].value_counts()\n\n\ncolorscale = [[0, 'rgb(102,194,165)'], [0.005, 'rgb(102,194,165)'], \n              [0.01, 'rgb(171,221,164)'], [0.02, 'rgb(230,245,152)'], \n              [0.04, 'rgb(255,255,191)'], [0.05, 'rgb(254,224,139)'], \n              [0.10, 'rgb(253,174,97)'], [0.25, 'rgb(213,62,79)'], [1.0, 'rgb(158,1,66)']]\n\ndata = [ dict(\n        type = 'choropleth',\n        autocolorscale = False,\n        colorscale = colorscale,\n        showscale = True,\n        locations = tmp.index,\n        z = tmp.values,\n        locationmode = 'country names',\n        text = tmp.values,\n        marker = dict(\n            line = dict(color = '#fff', width = 0.25)) \n            )\n       ]\n\nlayout = dict(\n    height=500,\n    title = 'Number of visits by Country',\n    geo = dict(\n        showframe = True,\n        showocean = True,\n        oceancolor = 'rgb(28,107,160)',\n        projection = dict(\n        type = 'orthographic',\n            rotation = dict(\n                    lon = 60,\n                    lat = 10),\n        ),\n        lonaxis =  dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n            ),\n        lataxis = dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n                )\n            ),\n        )\nfig = dict(data=data, layout=layout)\niplot(fig)\n\n#Analyze geospatial distribution by revenue\n\ntmp = df_combined.groupby(\"geoNetwork.country\").agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\n\n\ncolorscale = [[0, 'rgb(102,194,165)'], [0.005, 'rgb(102,194,165)'], \n              [0.01, 'rgb(171,221,164)'], [0.02, 'rgb(230,245,152)'], \n              [0.04, 'rgb(255,255,191)'], [0.05, 'rgb(254,224,139)'], \n              [0.10, 'rgb(253,174,97)'], [0.25, 'rgb(213,62,79)'], [1.0, 'rgb(158,1,66)']]\n\ndata = [ dict(\n        type = 'choropleth',\n        autocolorscale = False,\n        colorscale = colorscale,\n        showscale = True,\n        locations = tmp['geoNetwork.country'],\n        z = tmp['totals_transactionRevenue'],\n        locationmode = 'country names',\n        text = tmp['totals_transactionRevenue'],\n        marker = dict(\n            line = dict(color = '#fff', width = 0.25)) \n            )\n       ]\n\nlayout = dict(\n    height=500,\n    title = 'Average revenue distribution by Country',\n    geo = dict(\n        showframe = True,\n        showocean = True,\n        oceancolor = 'rgb(28,107,160)',\n        projection = dict(\n        type = 'orthographic',\n            rotation = dict(\n                    lon = 60,\n                    lat = 10),\n        ),\n        lonaxis =  dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n            ),\n        lataxis = dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n                )\n            ),\n        )\nfig = dict(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3c3c87422438ee760aea9f5a55bcaf28793b641"},"cell_type":"code","source":"#Explore revenue distribution accross months over the years\ncolumns = ['Year', 'Month', 'totals_transactionRevenue']\ngroup = df_combined[columns].groupby(['Year', 'Month']).sum().reset_index()\n\nprint(group)\n\nplot = sbn.catplot(x = 'Month', y = 'totals_transactionRevenue', hue = 'Year', \n                       palette = [\"#3498db\",\"#2ecc71\",\"#f1c40f\"], data = group, kind = \"bar\",\n                      height = 6, aspect = 2)\n\nplot.set(xlabel='Month', ylabel='Sum of Log Revenue', title = 'Revenue distribution accross months over the years')\n\nplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01fb71027f2882d72b63b5e22018a2daa839e1de"},"cell_type":"code","source":"columns3 = ['device.browser','device.isMobile']\n\ngroup3 = df_combined[columns3].groupby(['device.browser','device.isMobile'])\\\n                              .aggregate({'device.browser':['count']}).reset_index()\n\ngroup3.columns = ['device.browser', 'device.isMobile', 'browser.count']\ngroup3 = group3.sort_values(by = 'browser.count', ascending = False).head(10)\n\nprint('Table showing the top 10 browsers preferred by users:\\n')\nprint(group3)\n\n#Plot top 10 browsers by usage\nplot3 = sbn.catplot(x = 'device.browser', y = 'browser.count', hue = 'device.isMobile',\n                       palette = [\"#3498db\",\"#f1c40f\"], data = group3, kind = \"bar\",\n                      height = 6, aspect = 2)\n\nplot3.set(xlabel='Device Browser', ylabel='Count of usage', title = 'Top 10 browsers preferred by users')\n\nprint(plot3)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"974aa3792a8874058a4d5a60e5195ad0cf6e02ce"},"cell_type":"markdown","source":"## Neural Network Model with Embeddings"},{"metadata":{"trusted":true,"_uuid":"b379014dd0db62a2c45e506aedd9f4860572531b"},"cell_type":"code","source":"cat_cols = ['channelGrouping', 'fullVisitorId',\n       'device.browser', 'device.deviceCategory',\n       'device.operatingSystem', 'geoNetwork.continent', 'geoNetwork.country',\n       'geoNetwork.networkDomain', 'geoNetwork.subContinent',\n       'trafficSource.adContent', 'trafficSource.isTrueDirect',\n       'trafficSource.keyword', 'trafficSource.medium',\n       'trafficSource.referralPath', 'trafficSource.source',\n       'customDimensions']\n\ncont_cols = ['visitId', 'visitNumber', 'device.isMobile',\n       'totals.bounces', 'totals.hits', 'totals.newVisits', 'totals.pageviews',\n       'Hour', 'Day','Day_of_week', 'Month', 'Week_number', 'Year']\n\ntarget = ['totals_transactionRevenue']\n\nprint('Number of categorical variable columns = '+str(len(cat_cols)))\nprint('Number of continuous variable columns = '+str(len(cont_cols)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b11a6b54a3780c5d28790cb11845d2ee33e91fb2"},"cell_type":"code","source":"#Normalize continuous variables\n\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\ndf_train_cont = df_train[cont_cols].values\nmin_max_scaler = preprocessing.MinMaxScaler()\ndf_train_cont_scaled = min_max_scaler.fit_transform(df_train_cont)\ndf_train_cont = pd.DataFrame(df_train_cont_scaled, columns = cont_cols)\n\ndf_test_cont = df_test[cont_cols].values\nmin_max_scaler = preprocessing.MinMaxScaler()\ndf_test_cont_scaled = min_max_scaler.fit_transform(df_test_cont)\ndf_test_cont = pd.DataFrame(df_test_cont_scaled, columns = cont_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3747a9ff73aac57526d7e0b99ea1d14dedf72f7"},"cell_type":"code","source":"#Define function for label encoding\n\ndef data_le(df):\n    \n    les = []\n    les_num_classes = []\n\n    print('Number of classes in the dataframe: \\n')\n    \n    for i in range(len(cat_cols)):\n\n        encoder = LabelEncoder()\n        encoder.fit(df[cat_cols[i]])\n        encoded_column = encoder.transform(df[cat_cols[i]])\n        les.append(encoded_column)\n        num_classes = np.max(les[i])+1\n        les_num_classes.append(num_classes)\n    \n        print('Column '+str(cat_cols[i])+' number of classes -->'+str(num_classes))\n    \n    return les, les_num_classes #returns array of label encoded data and number classes for each column of data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43782e8905d026be735d9f7acfa3fdcf6173a1f8"},"cell_type":"code","source":"df_train_cat, les_num_classes_train = data_le(df_train)\n\ndf_train_cat = pd.DataFrame(df_train_cat).astype('int32').transpose() #restore shape of array after label encoding\ndf_train_cat.columns = cat_cols\n\nles_num_classes_train = pd.DataFrame(les_num_classes_train, columns = ['Num_classes']).astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ca50291c7eb3b144bbafeff474b2fb8b67ed9df"},"cell_type":"code","source":"df_test_cat, les_num_classes_test = data_le(df_test)\n\ndf_test_cat = pd.DataFrame(df_test_cat).astype('int32').transpose()\ndf_test_cat.columns = cat_cols\nles_num_classes_test = pd.DataFrame(les_num_classes_test, columns = ['Num_classes']).astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eac58b42ca14a3c4f5efc0dec03146066e248be8"},"cell_type":"code","source":"#Prepare input dimension to be used in the embedding layer\nnum_classes = pd.concat([les_num_classes_train, les_num_classes_test], axis = 1)\ninput_dimension = np.array(num_classes.max(axis = 1)+1)\ninput_dimension","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51692c9dd915f88b579656019123a7f75cbc876d"},"cell_type":"code","source":"#Train test split\n\ntrain_size = int(len(df_train) * .8)\nprint (\"Train size: %d\" % train_size)\nprint (\"Test size: %d\" % (len(df_test)))\nprint (\"Validation size: %d\" % (len(df_train) - train_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3765e3fc4447a9992580f7817b85bb9fae05cb91"},"cell_type":"code","source":"X_train_cont = df_train_cont.iloc[:train_size]\nX_train_cat = df_train_cat.iloc[:train_size]\ny_train = df_train[target][:train_size]\n\nX_val_cont = df_train_cont.iloc[train_size:]\nX_val_cat = df_train_cat.iloc[train_size:]\ny_val = df_train[target][train_size:]\n\nX_test_cont = df_test_cont\nX_test_cat = df_test_cat\ny_test = df_test[target]\n\nprint('Shape of X_train_cont: '+str(X_train_cont.shape))\nprint('Shape of X_train_cat: '+str(X_train_cat.shape))\nprint('Shape of y_train: '+str(y_train.shape), '\\n')\n\nprint('Shape of X_val_cont: '+str(X_val_cont.shape))\nprint('Shape of X_val_cat: '+str(X_val_cat.shape))\nprint('Shape of y_val: '+str(y_val.shape), '\\n')\n\nprint('Shape of X_test_cont: '+str(X_test_cont.shape))\nprint('Shape of X_test_cat: '+str(X_test_cat.shape))\nprint('Shape of y_test: '+str(y_test.shape), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0e4d36f3487c084d977dfedd801c26df8484214"},"cell_type":"code","source":"#Create the model\n\n#Define input layer\n\n#Input for the non-embedded layer\ninput_cont = Input(shape=(13,)) #since input is n X 16 matrix after min-max scalar transformation\n\n#Input for embedding layer\ninput_cat1 = Input(shape=(1,))\ninput_cat2 = Input(shape=(1,))\ninput_cat3 = Input(shape=(1,))\ninput_cat4 = Input(shape=(1,))\ninput_cat5 = Input(shape=(1,))\ninput_cat6 = Input(shape=(1,))\ninput_cat7 = Input(shape=(1,))\ninput_cat8 = Input(shape=(1,))\ninput_cat9 = Input(shape=(1,))\ninput_cat10 = Input(shape=(1,))\ninput_cat11 = Input(shape=(1,))\ninput_cat12 = Input(shape=(1,))\ninput_cat13 = Input(shape=(1,))\ninput_cat14 = Input(shape=(1,))\ninput_cat15 = Input(shape=(1,))\ninput_cat16 = Input(shape=(1,))\n\ndeep_inputs = [input_cont,\n              input_cat1,\n              input_cat2,\n              input_cat3,\n              input_cat4,\n              input_cat5,\n              input_cat6,\n              input_cat7,\n              input_cat8,\n              input_cat9,\n              input_cat10,\n              input_cat11,\n              input_cat12,\n              input_cat13,\n              input_cat14,\n              input_cat15,\n              input_cat16,]\n\n#Define embedding layer\nembed1 = Embedding(output_dim = 1, input_dim = input_dimension[0], input_length = 1)(input_cat1)\nembed2 = Embedding(output_dim = 1, input_dim = input_dimension[1], input_length = 1)(input_cat2)\nembed3 = Embedding(output_dim = 1, input_dim = input_dimension[2], input_length = 1)(input_cat3)\nembed4 = Embedding(output_dim = 1, input_dim = input_dimension[3], input_length = 1)(input_cat4)\nembed5 = Embedding(output_dim = 1, input_dim = input_dimension[4], input_length = 1)(input_cat5)\nembed6 = Embedding(output_dim = 1, input_dim = input_dimension[5], input_length = 1)(input_cat6)\nembed7 = Embedding(output_dim = 1, input_dim = input_dimension[6], input_length = 1)(input_cat7)\nembed8 = Embedding(output_dim = 1, input_dim = input_dimension[7], input_length = 1)(input_cat8)\nembed9 = Embedding(output_dim = 1, input_dim = input_dimension[8], input_length = 1)(input_cat9)\nembed10 = Embedding(output_dim = 1, input_dim = input_dimension[9], input_length = 1)(input_cat10)\nembed11 = Embedding(output_dim = 1, input_dim = input_dimension[10], input_length = 1)(input_cat11)\nembed12 = Embedding(output_dim = 1, input_dim = input_dimension[11], input_length = 1)(input_cat12)\nembed13 = Embedding(output_dim = 1, input_dim = input_dimension[12], input_length = 1)(input_cat13)\nembed14 = Embedding(output_dim = 1, input_dim = input_dimension[13], input_length = 1)(input_cat14)\nembed15 = Embedding(output_dim = 1, input_dim = input_dimension[14], input_length = 1)(input_cat15)\nembed16 = Embedding(output_dim = 1, input_dim = input_dimension[15], input_length = 1)(input_cat16)\n\n#Flatten embedding layer\nembed1 = Flatten()(embed1)\nembed2 = Flatten()(embed2)\nembed3 = Flatten()(embed3)\nembed4 = Flatten()(embed4)\nembed5 = Flatten()(embed5)\nembed6 = Flatten()(embed6)\nembed7 = Flatten()(embed7)\nembed8 = Flatten()(embed8)\nembed9 = Flatten()(embed9)\nembed10 = Flatten()(embed10)\nembed11 = Flatten()(embed11)\nembed12 = Flatten()(embed12)\nembed13 = Flatten()(embed13)\nembed14 = Flatten()(embed14)\nembed15 = Flatten()(embed15)\nembed16 = Flatten()(embed16)\n\ninput_cont = Dense(500, activation = 'relu')(input_cont)\ninput_cont = Dense(1, activation = 'linear')(input_cont)\n\n#Merge embedded layer and input_cont\nmerged_layer = concatenate([input_cont,\n                           embed1,\n                           embed2,\n                           embed3,\n                           embed4,\n                           embed5,\n                           embed6,\n                           embed7,\n                           embed8,\n                           embed9,\n                           embed10,\n                           embed11,\n                           embed12,\n                           embed13,\n                           embed14,\n                           embed15,\n                           embed16])\n\n#Define hidden layer\nhidden1 = Dense(1000, activation = 'relu')(merged_layer)\nhidden1 = Dropout(0.2)(hidden1)\nhidden2 = Dense(500, activation = 'relu')(hidden1)\n\n#Define output layer\noutput = Dense(1, activation = 'linear')(hidden2)\n\n#Define model\nnnembedding_model = Model(inputs = deep_inputs, outputs = output)\n\n#Compile model\nnnembedding_model.compile(loss='mse',\n                       optimizer='adam',\n                       metrics=['accuracy'])\n\n# Checkpoint\nfilepath=\"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"325c6268c6e0a50bcf0e30ae8f68f57019a9dfb7"},"cell_type":"code","source":"nnembedding_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30ec19bd0de4bd2d5a448afe4bacf8001b859a4c"},"cell_type":"code","source":"#Train the model\n\nnnembedding_model.fit([X_train_cont,\n                                X_train_cat['channelGrouping'],\n                                X_train_cat['fullVisitorId'],\n                                X_train_cat['device.browser'],\n                                X_train_cat['device.deviceCategory'],\n                                X_train_cat['device.operatingSystem'],\n                                X_train_cat['geoNetwork.continent'],\n                                X_train_cat['geoNetwork.country'],\n                                X_train_cat['geoNetwork.networkDomain'],\n                                X_train_cat['geoNetwork.subContinent'],\n                                X_train_cat['trafficSource.adContent'],\n                                X_train_cat['trafficSource.isTrueDirect'],\n                                X_train_cat['trafficSource.keyword'],\n                                X_train_cat['trafficSource.medium'],\n                                X_train_cat['trafficSource.referralPath'],\n                                X_train_cat['trafficSource.source'],\n                                X_train_cat['customDimensions']],\n                                y_train,\n                                epochs=10, batch_size=128, verbose = 1, callbacks=callbacks_list,\n                                validation_data = ([X_val_cont,\n                                X_val_cat['channelGrouping'],\n                                X_val_cat['fullVisitorId'],\n                                X_val_cat['device.browser'],\n                                X_val_cat['device.deviceCategory'],\n                                X_val_cat['device.operatingSystem'],\n                                X_val_cat['geoNetwork.continent'],\n                                X_val_cat['geoNetwork.country'],\n                                X_val_cat['geoNetwork.networkDomain'],\n                                X_val_cat['geoNetwork.subContinent'],\n                                X_val_cat['trafficSource.adContent'],\n                                X_val_cat['trafficSource.isTrueDirect'],\n                                X_val_cat['trafficSource.keyword'],\n                                X_val_cat['trafficSource.medium'],\n                                X_val_cat['trafficSource.referralPath'],\n                                X_val_cat['trafficSource.source'],\n                                X_val_cat['customDimensions']],\n                                                   y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b18873327ac3b0dcfd0a413358006fab80c002ce"},"cell_type":"code","source":"nnembedding_model.load_weights(\"weights.best.hdf5\")\n\ndf_val = y_val.copy()\n\ndf_val['pred'] = nnembedding_model.predict([X_val_cont,\n                                X_val_cat['channelGrouping'],\n                                X_val_cat['fullVisitorId'],\n                                X_val_cat['device.browser'],\n                                X_val_cat['device.deviceCategory'],\n                                X_val_cat['device.operatingSystem'],\n                                X_val_cat['geoNetwork.continent'],\n                                X_val_cat['geoNetwork.country'],\n                                X_val_cat['geoNetwork.networkDomain'],\n                                X_val_cat['geoNetwork.subContinent'],\n                                X_val_cat['trafficSource.adContent'],\n                                X_val_cat['trafficSource.isTrueDirect'],\n                                X_val_cat['trafficSource.keyword'],\n                                X_val_cat['trafficSource.medium'],\n                                X_val_cat['trafficSource.referralPath'],\n                                X_val_cat['trafficSource.source'],\n                                X_val_cat['customDimensions']])\n\ndf_val['diff'] = df_val['pred'] - df_val['totals_transactionRevenue']\ndf_val['abs_diff'] = np.abs(df_val['diff'])\ndf_val['abs_perc_diff'] = (df_val['abs_diff']/df_val['totals_transactionRevenue'])*100\nRMSE = ((df_val['diff'] ** 2).mean()) ** .5\n\nprint('RMSE for validation set: '+str(RMSE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81e7033fe7ab57f7fecd470e9eb12f024671b568"},"cell_type":"code","source":"nnembedding_model.load_weights(\"weights.best.hdf5\")\n\ndf_test_copy = y_test.copy()\n\ndf_test_copy['pred'] = nnembedding_model.predict([X_test_cont,\n                                X_test_cat['channelGrouping'],\n                                X_test_cat['fullVisitorId'],\n                                X_test_cat['device.browser'],\n                                X_test_cat['device.deviceCategory'],\n                                X_test_cat['device.operatingSystem'],\n                                X_test_cat['geoNetwork.continent'],\n                                X_test_cat['geoNetwork.country'],\n                                X_test_cat['geoNetwork.networkDomain'],\n                                X_test_cat['geoNetwork.subContinent'],\n                                X_test_cat['trafficSource.adContent'],\n                                X_test_cat['trafficSource.isTrueDirect'],\n                                X_test_cat['trafficSource.keyword'],\n                                X_test_cat['trafficSource.medium'],\n                                X_test_cat['trafficSource.referralPath'],\n                                X_test_cat['trafficSource.source'],\n                                X_test_cat['customDimensions']])\n\ndf_test_copy['diff'] = df_test_copy['pred'] - df_test_copy['totals_transactionRevenue']\ndf_test_copy['abs_diff'] = np.abs(df_test_copy['diff'])\ndf_test_copy['abs_perc_diff'] = (df_test_copy['abs_diff']/df_test_copy['totals_transactionRevenue'])*100\nRMSE = ((df_test_copy['diff'] ** 2).mean()) ** .5\n\nprint('RMSE for test set: '+str(RMSE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c10508aa1dc81f3f569ed5d250d7fe229bdd1126"},"cell_type":"code","source":"nnembedding_model.load_weights(\"weights.best.hdf5\")\n\ncvscores = []\n\nscores = nnembedding_model.evaluate([X_test_cont,\n                                X_test_cat['channelGrouping'],\n                                X_test_cat['fullVisitorId'],\n                                X_test_cat['device.browser'],\n                                X_test_cat['device.deviceCategory'],\n                                X_test_cat['device.operatingSystem'],\n                                X_test_cat['geoNetwork.continent'],\n                                X_test_cat['geoNetwork.country'],\n                                X_test_cat['geoNetwork.networkDomain'],\n                                X_test_cat['geoNetwork.subContinent'],\n                                X_test_cat['trafficSource.adContent'],\n                                X_test_cat['trafficSource.isTrueDirect'],\n                                X_test_cat['trafficSource.keyword'],\n                                X_test_cat['trafficSource.medium'],\n                                X_test_cat['trafficSource.referralPath'],\n                                X_test_cat['trafficSource.source'],\n                                X_test_cat['customDimensions']],\n                        y_test, verbose=1)\n\nprint(\"%s: %.2f%%\" % (nnembedding_model.metrics_names[1], scores[1]*100))\ncvscores.append(scores[1] * 100)\nprint(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7add7f1e3a37f0735f3582a49fb9d6e45d7e6e68"},"cell_type":"code","source":"#Prepare file for submission\nsubmission = df_test[['fullVisitorId']].copy()\nsubmission['PredictedLogRevenue'] = df_test_copy['pred']\nsubmission = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\nsubmission.to_csv('Shujon_submission.csv',header = True, index=False)\nsubmission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32eb83d9a40d9e1ed513022de0a4e07e9a63f607"},"cell_type":"markdown","source":"## Random Forest Regressor Model"},{"metadata":{"trusted":true,"_uuid":"55a6102e0f01df2fdac6ec6c6d20fd883e8ea7c4"},"cell_type":"code","source":"df_train_rf = pd.concat([df_train_cat, df_train_cont], axis = 1, sort = False)\ndf_test_rf = pd.concat([df_test_cat, df_test_cont], axis = 1, sort = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"965431966bb1414c39a269b69c341c7b871a4572"},"cell_type":"code","source":"X_train_rf = df_train_rf.iloc[:train_size]\ny_train_rf = df_train[target][:train_size]\n\nX_val_rf = df_train_rf.iloc[train_size:]\ny_val_rf = df_train[target][train_size:]\n\nX_test_rf = df_test_rf\ny_test_rf = df_test[target]\n\n\nprint('Shape of X_train_rf: '+str(X_train_rf.shape))\nprint('Shape of y_train_rf: '+str(y_train_rf.shape), '\\n')\n\nprint('Shape of X_val_rf: '+str(X_val_rf.shape))\nprint('Shape of y_val_rf: '+str(y_val_rf.shape), '\\n')\n\nprint('Shape of X_test_rf: '+str(X_test_rf.shape))\nprint('Shape of y_test_rf: '+str(y_test_rf.shape), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c53665e469024672c8c7101f209ed696ba75520"},"cell_type":"code","source":"%%time\n\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42, verbose = 2)\n\n# Train the model on training data\nrf.fit(X_train_rf, y_train_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f06ce08574c007607abdf4c6183a7283f2885c0f"},"cell_type":"code","source":"# Use the forest's predict method on the test data\npredictions_rf = rf.predict(X_test_rf)\npredictions_rf = pd.DataFrame(predictions_rf, columns = ['Prediction_rf'])\n\n# Calculate the absolute errors\nerrors_rf = abs(predictions_rf - y_test_rf)\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors_rf), 2), 'degrees.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cf2645ca5cb8c87f0baf25ccd189b5b7c77ddc3"},"cell_type":"code","source":"df_val_rf = y_val_rf.copy()\n\ndf_val_rf['pred'] = rf.predict(X_val_rf)\n\ndf_val_rf['diff'] = df_val_rf['pred'] - df_val_rf['totals_transactionRevenue']\ndf_val_rf['abs_diff'] = np.abs(df_val_rf['diff'])\ndf_val_rf['abs_perc_diff'] = (df_val_rf['abs_diff']/df_val_rf['totals_transactionRevenue'])*100\nRMSE = ((df_val_rf['diff'] ** 2).mean()) ** .5\n\nprint('RMSE for validation set (Random Forest Regressor): '+str(RMSE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddda53505dd98e063bd41a6a782f8997126ff606"},"cell_type":"code","source":"df_test_rf = y_test_rf.copy()\n\ndf_test_rf['pred'] = rf.predict(X_test_rf)\n\ndf_test_rf['diff'] = df_test_rf['pred'] - df_test_rf['totals_transactionRevenue']\ndf_test_rf['abs_diff'] = np.abs(df_test_rf['diff'])\ndf_test_rf['abs_perc_diff'] = (df_test_rf['abs_diff']/df_test_rf['totals_transactionRevenue'])*100\nRMSE = ((df_test_rf['diff'] ** 2).mean()) ** .5\n\nprint('RMSE for validation set (Random Forest Regressor): '+str(RMSE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d2df995d2085baad759c415bae478a2fee386af"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}