{"cells": [{"source": ["**DataWitness team - Zillow Price competition - notebook 01:**\n", "\n", "This notebook contains preliminary data exploration for Zillow prize competition, from ACN DataWitness team.\n", "\n", "**Objective** build in Python a model that predicts house prices in Los Angeles Orange county and improves the Zestimates residual error for Kaggle Zillow Price competition: https://www.kaggle.com/c/zillow-prize-1#Competition%20Overview \n", "**Details** This notebook uses Python3 environment based on the kaggle/python docker image: https://github.com/kaggle/docker-python. You can run it locally using JupyterHub/Anaconda, Rodeo, pyCharm IDE or your favourite Python IDE\n", "\n", "We perform the following steps:\n", "1. Data loading\n", "2. Exploratory Data Analysis: cleaning, visualising, pre-processing and feature engineering\n", "3. Modelling (several models)\n", "4. Results and interpretation\n", "5. Prediction"], "metadata": {"_uuid": "c023f967e8766e85f9287022fb7200c1d3eb0934", "_cell_guid": "6cd23899-6746-41d3-8374-8b9834127c89"}, "cell_type": "markdown"}, {"source": ["#Import libraries\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import xgboost as xgb # ML\n", "import matplotlib.pyplot as plt # Data visualization\n", "import random\t#random number generator\n", "import datetime as dt\n", "import gc\n", "import seaborn as sns #python visualization library\n", "import sklearn as sk # ML\n", "from matplotlib.pyplot import show\n", "from matplotlib.colors import ListedColormap\n", "from ggplot import *\n", "from sklearn import ensemble\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.model_selection import train_test_split\n", "\n", "from datetime import datetime\n", "import numpy as numpy\n", "import pylab\n", "import calendar\n", "from scipy import stats\n", "from sklearn import model_selection, preprocessing\n", "from scipy.stats import kendalltau\n", "import warnings\n", "import matplotlib.pyplot as plt\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.layers import Dropout, BatchNormalization\n", "from keras.wrappers.scikit_learn import KerasRegressor\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import KFold\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.preprocessing import Imputer\n", "\n", "from sklearn.metrics import mean_absolute_error\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.datasets import make_moons, make_circles, make_classification\n", "from sklearn.neural_network import MLPClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.gaussian_process import GaussianProcessClassifier\n", "from sklearn.gaussian_process.kernels import RBF\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n", "\n", "import gc\n", "\n", "# Inputs\n", "from xgboost import XGBRegressor\n", "from sklearn.linear_model import Lasso, ElasticNet\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "# Data viz\n", "from mlens.visualization import corr_X_y, corrmat\n", "\n", "# Model evaluation\n", "from mlens.metrics import make_scorer\n", "from mlens.model_selection import Evaluator\n", "from mlens.preprocessing import EnsembleTransformer\n", "\n", "# Ensemble\n", "from mlens.ensemble import SuperLearner\n", "\n", "from scipy.stats import uniform, randint\n", "\n", "from subprocess import check_output\n", "\n", "np.random.seed(1)\n", "\n", "\n", "color = sns.color_palette()\n", "np.random.seed(1)\n", "\n", "pd.options.mode.chained_assignment = None\n", "pd.options.display.max_columns = 999\n", "\n", "color = sns.color_palette()\n", "%matplotlib inline\n", "\n", "pd.options.mode.chained_assignment = None\n", "pd.options.display.max_columns = 999\n", "\n", "#Statistics on input files\n", "print(\"\\n1. Input files:\\n\",check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "#### DATA LOADING\n", "\n", "#read training input files and returns data structure\n", "def load_data():\n", "    train_2016 = pd.read_csv('../input/train_2016_v2.csv')\n", "    train_2017 = pd.read_csv('../input/train_2017.csv')\n", "    \n", "    train = pd.concat([train_2016, train_2017], ignore_index=True)\n", "    properties = pd.read_csv('../input/properties_2017.csv')\n", "    sample = pd.read_csv('../input/sample_submission.csv')\n", "    \n", "    print(\"Preprocessing...\")\n", "    for c, dtype in zip(properties.columns, properties.dtypes):\n", "        if dtype == np.float64:\n", "            properties[c] = properties[c].astype(np.float32)\n", "            \n", "    print(\"Set train/test data...\")\n", "    \n", "    # Add Features\n", "    # life of property\n", "    properties['N-life'] = 2018 - properties['yearbuilt']\n", "\n", "    properties['A-calculatedfinishedsquarefeet'] = properties['finishedsquarefeet12'] + properties['finishedsquarefeet15']\n", "\n", "    # error in calculation of the finished living area of home\n", "    properties['N-LivingAreaError'] = properties['calculatedfinishedsquarefeet'] / properties['finishedsquarefeet12']\n", "\n", "    # proportion of living area\n", "    properties['N-LivingAreaProp'] = properties['calculatedfinishedsquarefeet'] / properties['lotsizesquarefeet']\n", "    properties['N-LivingAreaProp2'] = properties['finishedsquarefeet12'] / properties['finishedsquarefeet15']\n", "\n", "    # Amout of extra space\n", "    properties['N-ExtraSpace'] = properties['lotsizesquarefeet'] - properties['calculatedfinishedsquarefeet']\n", "    properties['N-ExtraSpace-2'] = properties['finishedsquarefeet15'] - properties['finishedsquarefeet12']\n", "\n", "    # Total number of rooms\n", "    properties['N-TotalRooms'] = properties['bathroomcnt'] + properties['bedroomcnt']\n", "\n", "    # Average room size\n", "    #properties['N-AvRoomSize'] = properties['calculatedfinishedsquarefeet'] / properties['roomcnt']\n", "\n", "    # Number of Extra rooms\n", "    properties['N-ExtraRooms'] = properties['roomcnt'] - properties['N-TotalRooms']\n", "\n", "    # Ratio of the built structure value to land area\n", "    properties['N-ValueProp'] = properties['structuretaxvaluedollarcnt'] / properties['landtaxvaluedollarcnt']\n", "\n", "    # Does property have a garage, pool or hot tub and AC?\n", "    #properties['N-GarPoolAC'] = ((properties['garagecarcnt'] > 0) & (properties['pooltypeid10'] > 0) & (properties['airconditioningtypeid'] != 5)) * 1\n", "\n", "    properties[\"N-location\"] = properties[\"latitude\"] + properties[\"longitude\"]\n", "    properties[\"N-location-2\"] = properties[\"latitude\"] * properties[\"longitude\"]\n", "    #properties[\"N-location-2round\"] = properties[\"N-location-2\"].round(-4)\n", "\n", "    # Ratio of tax of property over parcel\n", "    properties['N-ValueRatio'] = properties['taxvaluedollarcnt'] / properties['taxamount']\n", "\n", "    # TotalTaxScore\n", "    properties['N-TaxScore'] = properties['taxvaluedollarcnt'] * properties['taxamount']\n", "\n", "    # polnomials of tax delinquency year\n", "    properties[\"N-taxdelinquencyyear-2\"] = properties[\"taxdelinquencyyear\"] ** 2\n", "    properties[\"N-taxdelinquencyyear-3\"] = properties[\"taxdelinquencyyear\"] ** 3\n", "\n", "    # Length of time since unpaid taxes\n", "    properties['N-live'] = 2018 - properties['taxdelinquencyyear']\n", "\n", "    # Number of properties in the zip\n", "    zip_count = properties['regionidzip'].value_counts().to_dict()\n", "    properties['N-zip_count'] = properties['regionidzip'].map(zip_count)\n", "\n", "    # Number of properties in the city\n", "    city_count = properties['regionidcity'].value_counts().to_dict()\n", "    properties['N-city_count'] = properties['regionidcity'].map(city_count)\n", "\n", "    # Number of properties in the city\n", "    region_count = properties['regionidcounty'].value_counts().to_dict()\n", "    properties['N-county_count'] = properties['regionidcounty'].map(region_count)\n", "\n", "\n", "    id_feature = ['heatingorsystemtypeid','propertylandusetypeid', 'storytypeid', 'airconditioningtypeid',\n", "        'architecturalstyletypeid', 'buildingclasstypeid', 'buildingqualitytypeid', 'typeconstructiontypeid']\n", "    for c in properties.columns:\n", "        properties[c]=properties[c].fillna(-1)\n", "        if properties[c].dtype == 'object':\n", "            lbl = LabelEncoder()\n", "            lbl.fit(list(properties[c].values))\n", "            properties[c] = lbl.transform(list(properties[c].values))\n", "        if c in id_feature:\n", "            lbl = LabelEncoder()\n", "            lbl.fit(list(properties[c].values))\n", "            properties[c] = lbl.transform(list(properties[c].values))\n", "            dum_df = pd.get_dummies(properties[c])\n", "            dum_df = dum_df.rename(columns=lambda x:c+str(x))\n", "            properties = pd.concat([properties,dum_df],axis=1)\n", "            properties = properties.drop([c], axis=1)\n", "            #print np.get_dummies(properties[c])\n", "    \n", "    #\n", "    # Make train and test dataframe\n", "    #\n", "    train = train.merge(properties, on='parcelid', how='left')\n", "    sample['parcelid'] = sample['ParcelId']\n", "    test = sample.merge(properties, on='parcelid', how='left')\n", "\n", "    # drop out ouliers\n", "    train = train[train.logerror > -0.4]\n", "    train = train[train.logerror < 0.418]\n", "\n", "    train[\"transactiondate\"] = pd.to_datetime(train[\"transactiondate\"])\n", "    train[\"Month\"] = train[\"transactiondate\"].dt.month\n", "    train[\"quarter\"] = train[\"transactiondate\"].dt.quarter\n", "    \n", "    test[\"Month\"] = 10\n", "    test['quarter'] = 4\n", "\n", "    x_train = train.drop(['parcelid', 'logerror','transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n", "    y_train = train[\"logerror\"].values\n", "    \n", "    x_test = test[x_train.columns]\n", "    del test, train    \n", "    print(x_train.shape, y_train.shape, x_test.shape)\n", "    \n", "    return x_train, y_train, x_test\n", "\n", "x_train, y_train, x_test = load_data()\n", "\n"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "f6c823cd89790059af34c1cfa2b9fac49d98bb9b", "_cell_guid": "a8ff378b-cd62-4d6a-972c-8cc09a96658f"}, "cell_type": "code"}, {"source": ["**Data Quality**"], "metadata": {"collapsed": true, "_uuid": "da6f8f0fb3806b6c259eadef4b6cd8707e3255a5", "_cell_guid": "c3c0a6f9-a01e-4908-be3c-cbe7cc0f92da"}, "cell_type": "markdown"}, {"source": ["**Exploratory data analysis.**\n", "Plot distribution of transactions per month"], "metadata": {"_uuid": "ae932d1b6ec8b0a2723572c4d922c4775f275a02", "_cell_guid": "ee9deffb-86cc-45f8-b17b-9e520a224d66"}, "cell_type": "markdown"}, {"source": ["train_df = x_train\n", "train_y = y_train\n", "\n", "##Feature importance\n", "cat_cols = [\"hashottuborspa\", \"propertycountylandusecode\", \"propertyzoningdesc\", \"fireplaceflag\", \"taxdelinquencyflag\"]\n", "#train_df = train_df.drop(['parcelid', 'logerror', 'transactiondate', 'transaction_month']+cat_cols, axis=1)\n", "feat_names = train_df.columns.values\n", "\n", "from sklearn import ensemble\n", "model = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30, max_features=0.3, n_jobs=-1, random_state=0)\n", "model.fit(train_df, train_y)\n", "\n", "## plot the feature importance ##\n", "importances = model.feature_importances_\n", "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n", "indices = np.argsort(importances)[::-1][:20]\n", "\n", "plt.figure(figsize=(12,12))\n", "plt.title(\"Feature importances\")\n", "plt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n", "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n", "plt.xlim([-1, len(indices)])\n", "plt.show()\n"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "a79a7366a10d5358702024edc03d375fdcb2a122", "_cell_guid": "8aa71868-3470-42ff-b867-6d9a2e82d64c"}, "cell_type": "code"}, {"source": ["**Data prep**"], "metadata": {"_uuid": "d854eb2c1d170cb18206fdd254643412f5f50e24", "_cell_guid": "ff15bef8-8660-4026-9f65-5a90b37da183"}, "cell_type": "markdown"}, {"source": ["#Drop properties that are identifiers and will not have any relevance in prediction\n", "x_train = train_df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n", "y_train = train_df['logerror'].values\n", "print(\"\\n---\\n\",x_train.shape, y_train.shape)\n", "\n", "train_columns = x_train.columns"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "14739009535b59d8a024fb99f81f050305190183", "_cell_guid": "da43cc2e-7078-4a25-b804-dfdd98ea5718"}, "cell_type": "code"}, {"source": ["**Properties 2016:** Exploration of the actual properties from 2016 file."], "metadata": {"_uuid": "d212c90b2bd81764830f67ff4a648cfe772fa245", "_cell_guid": "1817da44-2226-4814-9b83-4cf9f7c30dcc"}, "cell_type": "markdown"}, {"source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "properties = pd.read_csv(\"../input/properties_2016.csv\")\n", "print(\"\\n---\\nnumber of rows x cols in train data: \")\n", "properties.shape\n", "print (\"\\n---\\n\")\n", "#show first 10 rows \n", "properties.head(5)\n", "\n", "missing_df = properties.isnull().sum(axis=0).reset_index()\n", "missing_df.columns = ['column_name', 'missing_count']\n", "missing_df = missing_df.sort_values(by='missing_count')\n", "\n", "ind = np.arange(missing_df.shape[0])\n", "width = 0.9\n", "(fig, ax) = plt.subplots(figsize=(12,18))\n", "rects = ax.barh(ind, missing_df.missing_count.values, color='blue')\n", "ax.set_yticks(ind)\n", "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n", "ax.set_xlabel(\"Count of missing values\")\n", "ax.set_title(\"Number of missing values in each column\")\n", "plt.show()\n"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "a8605ca53c9ae1faac16d3d62c1bd53797b15b25", "_cell_guid": "44bb8f6e-2c55-4559-b0d9-4b42493c719e"}, "cell_type": "code"}, {"source": ["**02: MODEL BUILDING**"], "metadata": {"collapsed": true, "_uuid": "3e36de2ced778329825ed69a4d554a642274b4fa", "_cell_guid": "8c48c6b7-6247-49b9-b4a2-3d1113a39325"}, "cell_type": "markdown"}, {"source": ["### Importing Libraries or Packages that are needed throughout the Program ###\n", "import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "import random\n", "import datetime as dt\n", "import gc\n", "\n", "import seaborn as sns #python visualization library \n", "color = sns.color_palette()\n", "\n", "#%matplotlib inline\n", "np.random.seed(1)\n", "###Load the Datasets ###\n", "\n", "# We need to load the datasets that will be needed to train our machine learning algorithms, handle our data and make predictions. Note that these datasets are the ones that are already provided once you enter the competition by accepting terms and conditions #\n", "\n", "train = pd.read_csv('../input/train_2016_v2.csv' , parse_dates=[\"transactiondate\"]) \n", "properties = pd.read_csv('../input/properties_2016.csv')   \n", "test = pd.read_csv('../input/sample_submission.csv') \n", "test= test.rename(columns={'ParcelId': 'parcelid'}) #To make it easier for merging datasets on same column_id later\n", "\n", "\n", "### Analyse the Dimensions of our Datasets.\n", "\n", "print(\"Training Size:\" + str(train.shape))\n", "print(\"Property Size:\" + str(properties.shape))\n", "print(\"Sample Size:\" + str(test.shape))\n", "\n", "### Type Converting the DataSet ###\n", "# The processing of some of the algorithms can be made quick if data representation is made in int/float32 instead of int/float64. Therefore, in order to make sure that all of our columns types are in float32, we are implementing the following lines of code #\n", "for c, dtype in zip(properties.columns, properties.dtypes):\n", "    if dtype == np.float64:        \n", "        properties[c] = properties[c].astype(np.float32)\n", "    if dtype == np.int64:\n", "        properties[c] = properties[c].astype(np.int32)\n", "\n", "\n", "for column in test.columns:\n", "    if test[column].dtype == int:\n", "        test[column] = test[column].astype(np.int32)\n", "    if test[column].dtype == float:\n", "        test[column] = test[column].astype(np.float32)\n", "\n", "\n", "### feature engineering\n", "#living area proportions \n", "properties['living_area_prop'] = properties['calculatedfinishedsquarefeet'] / properties['lotsizesquarefeet']\n", "#tax value ratio\n", "properties['value_ratio'] = properties['taxvaluedollarcnt'] / properties['taxamount']\n", "#tax value proportions\n", "properties['value_prop'] = properties['structuretaxvaluedollarcnt'] / properties['landtaxvaluedollarcnt']\n", "\n", "\n", "###Merging the Datasets ###\n", "\n", "# We are merging the properties dataset with training and testing dataset for model building and testing prediction #\n", "\n", "df_train = train.merge(properties, how='left', on='parcelid') \n", "df_test = test.merge(properties, how='left', on='parcelid')\n", "\n", "\n", "### Remove previos variables to keep some memory\n", "del properties, train\n", "gc.collect();\n", "\n", "\n", "print('Memory usage reduction...')\n", "df_train[['latitude', 'longitude']] /= 1e6\n", "df_test[['latitude', 'longitude']] /= 1e6\n", "\n", "df_train['censustractandblock'] /= 1e12\n", "df_test['censustractandblock'] /= 1e12\n", "\n", "### Let's do some pre-exploratory analysis to identify how much missing values do we have in our datasets. \n", "\n", "# Let's do some engineering with fireplaceflag variable.\n", "\n", "print(df_train.fireplaceflag.isnull().sum())\n", "print(df_train.fireplacecnt.isnull().sum())\n", "# By using fireplacecnt variable we can recover some fields of fireplaceflag\n", "\n", "df_train['fireplaceflag']= \"No\"\n", "df_train.loc[df_train['fireplacecnt']>0,'fireplaceflag']= \"Yes\"\n", "\n", "# Remaining Missing fireplacecnt will be replaced with 0.\n", "index = df_train.fireplacecnt.isnull()\n", "df_train.loc[index,'fireplacecnt'] = 0\n", "\n", "#Tax deliquency flag - assume if it is null then doesn't exist\n", "index = df_train.taxdelinquencyflag.isnull()\n", "df_train.loc[index,'taxdelinquencyflag'] = \"None\"\n", "\n", "\n", "# Similar step performed for Pool/Spa/hot tub\n", "print(df_train.hashottuborspa.value_counts())\n", "print(df_train.pooltypeid10.value_counts())\n", "\n", "#lets remove 'pooltypeid10' as has more missing values\n", "print(df_train.hashottuborspa.value_counts())\n", "print(df_train.pooltypeid10.value_counts())\n", "\n", "#Assume if the pooltype id is null then pool/hottub doesnt exist \n", "index = df_train.pooltypeid2.isnull()\n", "df_train.loc[index,'pooltypeid2'] = 0\n", "\n", "index = df_train.pooltypeid7.isnull()\n", "df_train.loc[index,'pooltypeid7'] = 0\n", "\n", "index = df_train.poolcnt.isnull()\n", "df_train.loc[index,'poolcnt'] = 0\n", "\n", "### Label Encoding For Machine Learning & Filling Missing Values ###\n", "# We are now label encoding our datasets. All of the machine learning algorithms employed in scikit learn assume that the data being fed to them is in numerical form. LabelEncoding ensures that all of our categorical variables are in numerical representation. Also note that we are filling the missing values in our dataset with a zero before label encoding them. This is to ensure that label encoder function does not experience any problems while carrying out its operation #\n", "\n", "from sklearn.preprocessing import LabelEncoder  \n", "\n", "lbl = LabelEncoder()\n", "for c in df_train.columns:\n", "    df_train[c]=df_train[c].fillna(0)\n", "    if df_train[c].dtype == 'object':\n", "        lbl.fit(list(df_train[c].values))\n", "        df_train[c] = lbl.transform(list(df_train[c].values))\n", "\n", "for c in df_test.columns:\n", "    df_test[c]=df_test[c].fillna(0)\n", "    if df_test[c].dtype == 'object':\n", "        lbl.fit(list(df_test[c].values))\n", "        df_test[c] = lbl.transform(list(df_test[c].values))     \n", "\n", "\n", "# Drop unuseful features and align/include same features in test as in the training set #\n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n", "                         'propertycountylandusecode' ], axis=1)\n", "\n", "x_test = df_test.drop(['parcelid', 'propertyzoningdesc',\n", "                       'propertycountylandusecode', '201610', '201611', \n", "                       '201612', '201710', '201711', '201712'], axis = 1) \n", "\n", "x_train = x_train.values\n", "y_train = df_train['logerror'].values\n", "\n", "### Cross Validation ###\n", "# We are dividing our datasets into the training and validation sets so that we could monitor and the test the progress of our machine learning algorithm. This would let us know when our model might be over or under fitting on the dataset that we have employed. #\n", "\n", "from sklearn.model_selection import train_test_split\n", "\n", "X = x_train\n", "y = y_train \n", "\n", "Xtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.1, random_state=47)\n", "\n", "###Implement the Xgboost### \n", "\n", "# We can now select the parameters for Xgboost and monitor the progress of results on our validation set. The explanation of the xgboost parameters and what they do can be found on the following link http://xgboost.readthedocs.io/en/latest/parameter.html #\n", "\n", "dtrain = xgb.DMatrix(Xtrain, label=ytrain)\n", "dvalid = xgb.DMatrix(Xvalid, label=yvalid)\n", "dtest = xgb.DMatrix(x_test.values)\n", "\n", "# Try out different parameters\n", "xgb_params = {'min_child_weight': 10, 'eta': 0.035, 'colsample_bytree': 0.5, 'max_depth': 4,\n", "            'subsample': 0.85, 'lambda': 0.9, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n", "            'eval_metric': 'mae', 'objective': 'reg:linear' }           \n", "\n", "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n", "\n", "model_xgb = xgb.train(xgb_params, dtrain, 1000, watchlist, early_stopping_rounds=100,\n", "                  maximize=False, verbose_eval=10)\n", "\n", "###Predicting the results###\n", "\n", "# Let us now predict the target variable for our test dataset. All we have to do now is just fit the already trained model on the test set that we had made merging the sample file with properties dataset #\n", "\n", "Predicted_test_xgb = model_xgb.predict(dtest)\n", "\n", "### Submitting the Results ###\n", "\n", "# Once again load the file and start submitting the results in each column #\n", "sample_file = pd.read_csv('../input/sample_submission.csv') \n", "for c in sample_file.columns[sample_file.columns != 'ParcelId']:\n", "    sample_file[c] = Predicted_test_xgb\n", "\n", "print('Preparing the csv file ...')\n", "sample_file.to_csv('xgb_predicted_results.csv', index=False, float_format='%.4f')\n", "print(\"Finished writing the file\")\n", "\n", "# plot the important features #\n", "fig, ax = plt.subplots(figsize=(12,18))\n", "xgb.plot_importance(model_xgb, max_num_features=50, height=0.8, ax=ax)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "88086c2ef0c343f1ff6fdacf2c7473e6bd2c997b", "_cell_guid": "bc8e8cc2-a43a-4719-8f4e-7f1c1673e0a7"}, "cell_type": "code"}, {"source": ["#Neural Networks\n", "from datetime import datetime\n", "import numpy as np\n", "import numpy as numpy\n", "import pandas as pd\n", "import pylab\n", "import calendar\n", "from scipy import stats\n", "import seaborn as sns\n", "from sklearn import model_selection, preprocessing\n", "from scipy.stats import kendalltau\n", "import warnings\n", "import matplotlib.pyplot as plt\n", "import pandas\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.layers import Dropout, BatchNormalization\n", "from keras.wrappers.scikit_learn import KerasRegressor\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import KFold\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import LabelEncoder\n", "\n", "##  READ DATA  ##\n", "# Load train, Prop and sample\n", "#print('Loading train, prop and sample data')\n", "#train = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n", "#prop = pd.read_csv('../input/properties_2016.csv')\n", "#sample = pd.read_csv('../input/sample_submission.csv')\n", "\n", "print('Fitting Label Encoder on properties')\n", "for c in prop.columns:\n", "    prop[c]=prop[c].fillna(-1)\n", "    if prop[c].dtype == 'object':\n", "        lbl = LabelEncoder()\n", "        lbl.fit(list(prop[c].values))\n", "        prop[c] = lbl.transform(list(prop[c].values))\n", "        \n", "#Create df_train and x_train y_train from that\n", "print('Creating training set:')\n", "df_train = train.merge(prop, how='left', on='parcelid')\n", "\n", "###########################################################\n", "df_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n", "df_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\n", "df_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\n", "df_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\n", "df_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day\n", "\n", "\n", "###########################################\n", "\n", "print('Fill  NA/NaN values using suitable method' )\n", "df_train.fillna(-1.0)\n", "\n", "print('Create x_train and y_train from df_train' )\n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\n", "y_train = df_train[\"logerror\"]\n", "\n", "y_mean = np.mean(y_train)\n", "print(x_train.shape, y_train.shape)\n", "train_columns = x_train.columns\n", "\n", "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n", "    x_train[c] = (x_train[c] == True)\n", "# Create df_test and test set\n", "print('Creating df_test  :')\n", "sample['parcelid'] = sample['ParcelId']\n", "\n", "print(\"Merge Sample with property data :\")\n", "df_test = sample.merge(prop, on='parcelid', how='left')\n", "\n", "\n", "########################\n", "df_test[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n", "df_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\n", "df_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\n", "df_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\n", "df_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \n", "\n", "#################################\n", "x_test = df_test[train_columns]\n", "\n", "print('Shape of x_test:', x_test.shape)\n", "print(\"Preparing x_test:\")\n", "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n", "    x_test[c] = (x_test[c] == True)\n", "  \n", "from sklearn.preprocessing import Imputer\n", "imputer= Imputer()\n", "imputer.fit(x_train.iloc[:, :])\n", "x_train = imputer.transform(x_train.iloc[:, :])\n", "imputer.fit(x_test.iloc[:, :])\n", "x_test = imputer.transform(x_test.iloc[:, :])\n", "\n", "sc = StandardScaler()\n", "x_train = sc.fit_transform(x_train)\n", "x_test = sc.transform(x_test)\n", "\n", "\n", "##  RUN NETWORK  ##\n", "len_x=int(x_train.shape[1])\n", "print(\"len_x is:\",len_x)\n", "\n", "####################ANN Starts here#\n", "\n", "nn = Sequential()\n", "nn.add(Dense(units = 360 , kernel_initializer = 'normal', activation = 'tanh', input_dim = len_x))\n", "nn.add(Dropout(.17))\n", "nn.add(Dense(units = 150 , kernel_initializer = 'normal', activation = 'relu'))\n", "nn.add(BatchNormalization())\n", "nn.add(Dropout(.4))\n", "nn.add(Dense(units = 60 , kernel_initializer = 'normal', activation = 'relu'))\n", "nn.add(BatchNormalization())\n", "nn.add(Dropout(.32))\n", "nn.add(Dense(units = 25, kernel_initializer = 'normal', activation = 'relu'))\n", "nn.add(BatchNormalization())\n", "nn.add(Dropout(.22))\n", "nn.add(Dense(1, kernel_initializer='normal'))\n", "nn.compile(loss='mae', optimizer='adam')\n", "\n", "nn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 100, verbose=2)\n", "\n", "print(\"x_test.shape:\",x_test.shape)\n", "y_pred_ann = nn.predict(x_test)\n", "\n", "#######################################################################################\n", "print( \"\\nWriting predictions...\" )\n", "##  WRITE RESULTS  ##\n", "y_pred = y_pred_ann.flatten()\n", "\n", "#output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n", "output = pd.DataFrame({'ParcelId': prop['parcelid'].astype(np.int32),\n", "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n", "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n", "# set col 'ParceID' to first col\n", "cols = output.columns.tolist()\n", "cols = cols[-1:] + cols[:-1]\n", "output = output[cols]\n", "\n", "print( \"\\nWriting results to disk:\" )\n", "output.to_csv('NN_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n", "\n", "print( \"\\nFinished!\" )"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "d7a516673ccf6c1743e11f61eebf7765d58370a4", "_cell_guid": "8155fc97-f664-4701-ba14-35e1eafc3a71"}, "cell_type": "code"}, {"source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from matplotlib.colors import ListedColormap\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.datasets import make_moons, make_circles, make_classification\n", "from sklearn.neural_network import MLPClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.gaussian_process import GaussianProcessClassifier\n", "from sklearn.gaussian_process.kernels import RBF\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n", "\n", "h = .02  # step size in the mesh\n", "\n", "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n", "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n", "         \"Naive Bayes\", \"QDA\"]\n", "\n", "classifiers = [\n", "    KNeighborsClassifier(3),\n", "    SVC(kernel=\"linear\", C=0.025),\n", "    SVC(gamma=2, C=1),\n", "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n", "    DecisionTreeClassifier(max_depth=5),\n", "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n", "    MLPClassifier(alpha=1),\n", "    AdaBoostClassifier(),\n", "    GaussianNB(),\n", "    QuadraticDiscriminantAnalysis()]\n", "\n", "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n", "                           random_state=1, n_clusters_per_class=1)\n", "rng = np.random.RandomState(2)\n", "X += 2 * rng.uniform(size=X.shape)\n", "linearly_separable = (X, y)\n", "\n", "datasets = [train, properties, test]\n", "\n", "figure = plt.figure(figsize=(27, 9))\n", "i = 1\n", "# iterate over datasets\n", "for ds_cnt, ds in enumerate(datasets):\n", "    # preprocess dataset, split into training and test part\n", "    X, y = ds\n", "    X = StandardScaler().fit_transform(X)\n", "    X_train, X_test, y_train, y_test = \\\n", "        train_test_split(X, y, test_size=.4, random_state=42)\n", "\n", "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n", "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n", "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n", "                         np.arange(y_min, y_max, h))\n", "\n", "    # just plot the dataset first\n", "    cm = plt.cm.RdBu\n", "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n", "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n", "    if ds_cnt == 0:\n", "        ax.set_title(\"Input data\")\n", "    # Plot the training points\n", "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n", "               edgecolors='k')\n", "    # and testing points\n", "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n", "               edgecolors='k')\n", "    ax.set_xlim(xx.min(), xx.max())\n", "    ax.set_ylim(yy.min(), yy.max())\n", "    ax.set_xticks(())\n", "    ax.set_yticks(())\n", "    i += 1\n", "\n", "    # iterate over classifiers\n", "    for name, clf in zip(names, classifiers):\n", "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n", "        clf.fit(X_train, y_train)\n", "        score = clf.score(X_test, y_test)\n", "\n", "        # Plot the decision boundary. For that, we will assign a color to each\n", "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n", "        if hasattr(clf, \"decision_function\"):\n", "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n", "        else:\n", "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", "\n", "        # Put the result into a color plot\n", "        Z = Z.reshape(xx.shape)\n", "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n", "\n", "        # Plot also the training points\n", "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n", "                   edgecolors='k')\n", "        # and testing points\n", "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n", "                   edgecolors='k', alpha=0.6)\n", "\n", "        ax.set_xlim(xx.min(), xx.max())\n", "        ax.set_ylim(yy.min(), yy.max())\n", "        ax.set_xticks(())\n", "        ax.set_yticks(())\n", "        if ds_cnt == 0:\n", "            ax.set_title(name)\n", "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n", "                size=15, horizontalalignment='right')\n", "        i += 1\n", "\n", "plt.tight_layout()\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "c07ac53e817f92466a646a846b9863682ccd6574", "_cell_guid": "ea7a3967-f8bb-4ee6-a737-e74dd821c3b8"}, "cell_type": "code"}, {"source": ["**SUBMISSION**"], "metadata": {"collapsed": true, "_uuid": "3fe20d1e594d62cc6e95cffa04213c2f11bc4c9f", "_cell_guid": "68dfd686-1c63-4199-9fa8-81e3ecdb333f"}, "cell_type": "markdown"}], "nbformat_minor": 1, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "version": "3.6.3"}}, "nbformat": 4}