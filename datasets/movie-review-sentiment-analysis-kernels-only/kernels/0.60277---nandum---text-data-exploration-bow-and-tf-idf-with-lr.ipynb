{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.tsv' , delimiter= '\\t')\ntest = pd.read_csv('../input/test.tsv', delimiter= '\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f28a1fe2a5e754b60b9bf7a5bc7f83990eea8ee6","collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"376bf67d87419713cd2cd7048e447b60dd6df59a"},"cell_type":"markdown","source":"The sentiment labels are:\n0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"38788ec2ad7859fb091a5d3d995a380e8bf11285"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.sparse import hstack, vstack\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8ab2dbaf004bbe58dcef6c83b60e25a164a421c","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(8,10))\nsentiment_counts = train.Sentiment.value_counts()\nsns.barplot(sentiment_counts.index , sentiment_counts.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"995cb64d140247c2a5d41550b26c267608686897"},"cell_type":"markdown","source":"Looks like neutral sentiments are more. This should be because of the phrases present in the dataset"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b246141829fa3d10763abe8f4d984f2d738c71df"},"cell_type":"code","source":"train_y = train['Sentiment'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cf5ca52a11cd6845905f0474f113d6e8e739096"},"cell_type":"markdown","source":"My initial approach is to do a word and character tfidf and Countvectorizer on the text data.  The result of tfidf and BOW will be a sparse matri and hence I will go ahead with TruncatedSVD for dimensionality reduction. Since the term frequency is considered in TfIdf, I am ensembling the predictions of Bag of Words approach to the SVD as my final feature set"},{"metadata":{"trusted":true,"_uuid":"8970a28cf6fd0808191a9e10c00820cd5555ce0e","collapsed":true},"cell_type":"code","source":"tfv = TfidfVectorizer(ngram_range=(1,3), use_idf= True , analyzer= 'word')\ntfv.fit(train['Phrase'].values.tolist() + test['Phrase'].values.tolist())\ntrain_tfidf = tfv.transform(train['Phrase'].values)\ntest_tfidf = tfv.transform(test['Phrase'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c5ed0dd3b3614ec8f44cbc3a79ee306ca84f1d2","collapsed":true},"cell_type":"code","source":"cv_object = CountVectorizer(ngram_range=(1,3), analyzer= 'word')\ncv_object.fit(train['Phrase'].values.tolist() + test['Phrase'].values.tolist())\ntrain_cv = cv_object.transform(train['Phrase'].values)\ntest_cv = cv_object.transform(test['Phrase'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3ee05ad0531bd99b279cbffccf1b7f13e2454a36"},"cell_type":"code","source":"tf_char = TfidfVectorizer(ngram_range=(1,6), analyzer= 'char' , max_features=20000)\ntf_char.fit(train['Phrase'].values.tolist() + test['Phrase'].values.tolist())\ntrain_tfidf_char = tf_char.transform(train['Phrase'].values)\ntest_tfidf_char = tf_char.transform(test['Phrase'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8286650f22da69e32a458a500f6e47b9a9c7c524"},"cell_type":"code","source":"bow_char = CountVectorizer(ngram_range=(1,6), analyzer= 'char' , max_features=20000)\nbow_char.fit(train['Phrase'].values.tolist() + test['Phrase'].values.tolist())\ntrain_cv_char = bow_char.transform(train['Phrase'].values)\ntest_cv_char = bow_char.transform(test['Phrase'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14e1b3f8776dacc51afadf5febff65e161050aa7","collapsed":true},"cell_type":"code","source":"def SVD(train , test, keyword, n_components = 25):\n    svd_obj = TruncatedSVD(n_components=n_components, algorithm='arpack')\n    svd_obj.fit(vstack([train, test]))\n    columns = ['svd_'+ keyword + '_' + str(i) for i in range(n_components)]\n    train_df = pd.DataFrame(data = svd_obj.transform(train) , columns = columns)\n    test_df = pd.DataFrame(data = svd_obj.transform(test) , columns = columns)\n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc249faf1df409edc2e1b5ca452d3040bc64f68e","collapsed":true},"cell_type":"code","source":"print(train_tfidf.shape)\nprint(test_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"073e65adb8e90980b1b52c099caa7eb9611aef07","collapsed":true},"cell_type":"code","source":"final_train , final_test = SVD(train_tfidf, test_tfidf , 'word_tfidf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"854f901007145e3e6a1e5ca631ec605a7af5dfef","collapsed":true},"cell_type":"code","source":"tf_char_train,  tf_char_test = SVD(train_tfidf_char, test_tfidf_char, 'char_tfidf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a599ed49fd08c756bcf45c9dc604229782ef84db"},"cell_type":"code","source":"final_train = pd.concat([final_train, tf_char_train], axis=1)\nfinal_test = pd.concat([final_test, tf_char_test], axis= 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c77348f022037ed592d80c5d9eb06f8dc6e72f7e","collapsed":true},"cell_type":"code","source":"del tf_char_train, tf_char_test, train_tfidf, test_tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0edf2d749a00fed7b66459a0bef0503622377344"},"cell_type":"code","source":"def model_multinomial(train_X, train_y, test_X, test_y, test_X2):\n    model = MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09e0d87c6251defb266408475f316accbdb09189","collapsed":true},"cell_type":"code","source":"def calculate_cv_score(model, train_x , train_y, test_x, num_splits = 3, loss = log_loss, is_dataframe = False):\n    ''' model needs to return validation prediction , test prediction and model itself after fitting'''\n    cv_scores = []\n    pred_train = np.zeros([train_x.shape[0] , 5])\n    pred_test_final = 0\n    kfold = KFold(n_splits= num_splits, random_state= 2018 , shuffle= True)\n    for dev_index , val_index in kfold.split(train_x):\n        if is_dataframe:\n            dev_X, val_X = train_x.loc[dev_index], train_x.loc[val_index]\n        else:            \n            dev_X, val_X = train_x[dev_index], train_x[val_index]\n        dev_y, val_y = train_y[dev_index], train_y[val_index]\n        pred_val , pred_test , fit_model = model(dev_X, dev_y, val_X , val_y , test_x)\n        pred_test_final = pred_test_final + pred_test\n        loss_score = loss(val_y , pred_val)\n        pred_train[val_index,:] = pred_val\n        cv_scores.append(loss_score)\n    avg_cv_score = np.mean(cv_scores)\n    pred_test_final = pred_test_final / num_splits\n    print(pred_train.shape)\n    return avg_cv_score, pred_test_final, pred_train \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20a5bb6ab5e04c7947908a5af654047f1191ad51","collapsed":true},"cell_type":"code","source":"cvscore_bow_words, bow_predictions_words, pred_train = calculate_cv_score(model_multinomial, train_cv, train_y, test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"929148a42e2728d1bfd1714258ffd5ce309145d1","collapsed":true},"cell_type":"code","source":"cvscore_bow_char, bow_predictions_char, pred_train_char = calculate_cv_score(model_multinomial, train_cv_char, train_y, test_cv_char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85fa3536fdcaffca102a03790de669af734f778a","collapsed":true},"cell_type":"code","source":"final_train[\"mnb_bow_word_0\"] = pred_train[:,0]\nfinal_train[\"mnb_bow_word_1\"] = pred_train[:,1]\nfinal_train[\"mnb_bow_word_2\"] = pred_train[:,2]\nfinal_train[\"mnb_bow_word_3\"] = pred_train[:,3]\nfinal_train[\"mnb_bow_word_4\"] = pred_train[:,4]\nfinal_test[\"mnb_bow_word_0\"] = bow_predictions_words[:,0]\nfinal_test[\"mnb_bow_word_1\"] = bow_predictions_words[:,1]\nfinal_test[\"mnb_bow_word_2\"] = bow_predictions_words[:,2]\nfinal_test[\"mnb_bow_word_3\"] = bow_predictions_words[:,3]\nfinal_test[\"mnb_bow_word_4\"] = bow_predictions_words[:,4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3c23ff6ed68e90078313f3647880e5f496e84dc8"},"cell_type":"code","source":"final_train[\"mnb_bow_char_0\"] = pred_train_char[:,0]\nfinal_train[\"mnb_bow_char_1\"] = pred_train_char[:,1]\nfinal_train[\"mnb_bow_char_2\"] = pred_train_char[:,2]\nfinal_train[\"mnb_bow_char_3\"] = pred_train_char[:,3]\nfinal_train[\"mnb_bow_char_4\"] = pred_train_char[:,4]\n\nfinal_test[\"mnb_bow_char_0\"] = bow_predictions_char[:,0]\nfinal_test[\"mnb_bow_char_1\"] = bow_predictions_char[:,1]\nfinal_test[\"mnb_bow_char_2\"] = bow_predictions_char[:,2]\nfinal_test[\"mnb_bow_char_3\"] = bow_predictions_char[:,3]\nfinal_test[\"mnb_bow_char_4\"] = bow_predictions_char[:,4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3f597387297fa33b1ed467a73c7a8ab8beae3fc","collapsed":true},"cell_type":"code","source":"print(final_test.shape)\nprint(final_train.shape)\nprint(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce61cc994e23dfb818583598fb7b003a74dfc37c","collapsed":true},"cell_type":"code","source":"def run_lr(train_X, train_y, val_X, val_y, test_X):\n    lr = LogisticRegression(C=0.01)\n    lr.fit(train_X, train_y)\n    pred_val = lr.predict_proba(val_X)\n    pred_test = lr.predict_proba(test_X)\n    return pred_val, pred_test, lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d63477e2024cd866cd2004de57857ff74caa9d1","collapsed":true},"cell_type":"code","source":"final_cv, final_pred, _ = calculate_cv_score(run_lr, final_train, train_y, final_test, is_dataframe=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fac97e27f8e151c6477eb2a050ae2d811af0961","collapsed":true},"cell_type":"code","source":"sub1 = np.argmax(final_pred, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b055c709562d5646977d20af7222a75d734cfe5f","collapsed":true},"cell_type":"code","source":"np.bincount(sub1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"21830c1f17fc17fedd441eb8441758bbfa1b11c2"},"cell_type":"code","source":"test['Sentiment'] = sub1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05aafb2807ef23a7dbf0a51f2bb5415bf7f07450","collapsed":true},"cell_type":"code","source":"test[['PhraseId' , 'Sentiment']].to_csv('sub_1.csv' , index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86091b6641d3d66a43bb4a56b590056c20b3e65b","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1ee2106683feebd52e1333ca7094afe9f0dc5011"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}