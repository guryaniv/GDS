{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b90b78282d47ed1d618d80f90b8c6a110909c598"},"cell_type":"markdown","source":"This is my first notebook. I am closely following SRK to build this notebook. In this notebook I am trying to analyze different features that will help us in identifying the spooky authors."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\ncolor = sns.color_palette()\n\n%matplotlib inline\n\neng_stopwords = set(stopwords.words(\"english\"))\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e55a0a7fa6a5d70469d13bf905d6609c9e223329","collapsed":true},"cell_type":"code","source":"## Read the train and test dataset and check the top few lines ##\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Number of rows & columns in train dataset : \",train_df.shape)\nprint(\"Number of rows $ columns in test dataset : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3144fdbd6a2e9b5cdd950ae57680f4786eb8c760","collapsed":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9139dbd5ab3598ccccbd34447401ae3bfc966800","collapsed":true},"cell_type":"code","source":"## check class imbalance\ncounts = train_df['author'].value_counts()\nprint(counts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dbc9c4e9120e60d6abe0501f3058c84d7dc1b2d"},"cell_type":"markdown","source":"This looks good as there is not much class imbalance."},{"metadata":{"trusted":true,"_uuid":"3c7b533cf2c1a92c48b1ae36e732f3685134709a","collapsed":true},"cell_type":"code","source":"grouped_df = train_df.groupby('author')\nprint(grouped_df.head())\nfor name, group in grouped_df:\n    print(\"\\nAuthor name : \", name)\n    cnt = 0\n    for ind, row in group.iterrows():\n        print(row[\"text\"])\n        cnt += 1\n        if cnt == 5:\n            break\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d18358cd69311259c9d386a7b785403264294cb1"},"cell_type":"markdown","source":"**Feature Engineering:**\n\nLets see how the meta features help to indentify the authors.\nMeta features - features that are extracted from the text like number of words, number of stop words, number of punctuations etc\nText based features - features directly based on the text / words like frequency, svd, word2vec etc.\n\n**Meta Features:**\n* Number of words in the text\n* Number of unique words in the text\n* Number of characters in the text\n* Number of stopwords\n* Number of punctuations\n* Number of upper case words\n* Number of title case words\n* Average length of the words"},{"metadata":{"trusted":true,"_uuid":"7fd240c5a0e9bd431d0e67d11182956b4420bfd5","collapsed":true},"cell_type":"code","source":"## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df[\"unique_words\"] = train_df[\"text\"].apply(lambda x: len(pd.unique(str(x).split())))\ntest_df[\"unique_words\"] = test_df[\"text\"].apply(lambda x: len(pd.unique(str(x).split())))\n\n## Number of stop words in the text ##\ntrain_df[\"stop_words\"] = train_df[\"text\"].apply(lambda x: len([i for i in str(x).lower().split() if i in eng_stopwords]))\ntest_df[\"stop_words\"] = test_df[\"text\"].apply(lambda x: len([i for i in str(x).lower().split() if i in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: np.mean([len(i) for i in str(x).split()]))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: np.mean([len(i) for i in str(x).split()]))\n\n# summary statistics of the meta features\nprint(\"\\nSummary of Training Set Numeric Variables\\n\")\nprint(train_df.groupby(\"author\").mean())\nprint(\"\\nSummary of Testing Set Numeric Variables\\n\",train_df.groupby(\"author\").mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"741adf2e25ed7b6447bd3b6f76e63bfe96b305ba","collapsed":true},"cell_type":"code","source":"train_df['num_words'].loc[train_df['num_words']>80] = 80 #truncation for better visuals\nplt.figure(figsize=(12,8))\nsns.violinplot(x='author', y='num_words', data=train_df)\nplt.xlabel('Author Name', fontsize=12)\nplt.ylabel('Number of words in text', fontsize=12)\nplt.title(\"Number of words by author\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e396f1a0c399f5776a2cd13882b5dc02aab475d"},"cell_type":"markdown","source":"**Text Based Features :**"},{"metadata":{"_uuid":"ca301ec7d443bee9d8e7fdb8f934987a5a8f028f"},"cell_type":"markdown","source":"Lets try Classification of Auther using sparse features. I am going to try using countVectorizer and TFIDF transformer.\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7cc54134a933bd4131cba904acf4408a250b7d2e"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import cross_validation, metrics\n\nencoder = LabelEncoder()\nencoder.fit(train_df['author'])\ntrain_y = encoder.transform(train_df['author'])\n\ntrain_tfidf = train_df['text'].values.tolist()\ntest_tfidf = test_df['text'].values.tolist()\n\nNBclassifier = Pipeline([('vect', CountVectorizer()),\n                      ('tfidf', TfidfTransformer()),\n                      ('clf', MultinomialNB()),\n])\nNBclassifier.fit(train_tfidf, train_y)\n\n#Predict training set:\ndtrain_predictions = NBclassifier.predict_proba(train_tfidf)\n\n#Predict testing set:\ny_pred_proba = NBclassifier.predict_proba(test_tfidf)\n\n#Perform cross-validation:\ncv_score = cross_validation.cross_val_score(NBclassifier, train_tfidf, train_y, cv=5, scoring='roc_auc')  \nprint(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n    \n#id,EAP,HPL,MWS\n#id07943,0.33,0.33,0.33\nx = y_pred_proba[:,0]\n\nmy_submission = pd.DataFrame({'id': test_df['id'], 'EAP': y_pred_proba[:,0], \n                              'HPL': y_pred_proba[:,1], 'MWS': y_pred_proba[:,2] })\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f6c5bc107be95b2e5d439929689023da5d24be7f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"35cd076bbaf05ed2741b336b350a340f8e6a6dc5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0bff129a6a80f940902fbee02a04337532f76a79"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ad20b8b197c5008f5b2ac2282f7f870e38f70e49"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d2add6c6e327519932a7097fb0abd56c0c2b5e81"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"50b9b76b0ae4c047f34ff7e4af40a73362d76799"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}