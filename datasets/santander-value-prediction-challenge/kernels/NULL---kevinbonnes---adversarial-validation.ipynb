{"cells":[{"metadata":{"_uuid":"b0f2b5e3a26327f3f74e3acb394fe55b4856489e"},"cell_type":"markdown","source":"Since the distributions between train and test seem to be significantly different (e.g. see https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data, https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/59172 and https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/59139), it is probably a good idea to adjust your validation strategy in order to get a model that generalizes well to the leaderboard.\n\nAdversarial validation is a technique that can be used to select training samples that are most similar to test samples. We can then use these samples to validate our models on, so that they generalize to the leaderboard. More info about adversarial validation here: http://fastml.com/adversarial-validation-part-one/\n\nIn this kernel we test if the distributions between train and test are actually different, and if we can find rows in our training set that are very similar to our test set."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load libraries\nimport numpy as np\nimport pandas as pd\nimport gc\nimport datetime\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\n# Params\nNFOLD = 5\nDATA_PATH = '../input/'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load data\ntrain = pd.read_csv(DATA_PATH + \"train.csv\")\ntest = pd.read_csv(DATA_PATH + \"test.csv\")\n\n# Mark train as 1, test as 0\ntrain['target'] = 1\ntest['target'] = 0\n\n# Concat dataframes\nn_train = train.shape[0]\ndf = pd.concat([train, test], axis = 0)\ndel train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c1db7ede9d1a51d76cc5e3d142f41cc2df01f394"},"cell_type":"code","source":"# Remove columns with only one value in our training set\npredictors = list(df.columns.difference(['ID', 'target']))\ndf_train = df.iloc[:n_train].copy()\ncols_to_remove = [c for c in predictors if df_train[c].nunique() == 1]\ndf.drop(cols_to_remove, axis=1, inplace=True)\n\n# Update column names\npredictors = list(df.columns.difference(['ID', 'target']))\n\n# Get some basic meta features\ndf['cols_mean'] = df[predictors].replace(0, np.NaN).mean(axis=1)\ndf['cols_count'] = df[predictors].replace(0, np.NaN).count(axis=1)\ndf['cols_sum'] = df[predictors].replace(0, np.NaN).sum(axis=1)\ndf['cols_std'] = df[predictors].replace(0, np.NaN).std(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8c17503acbf649cc5b5df9e09b300b4557f677d6"},"cell_type":"code","source":"# Prepare for training\n\n# Shuffle dataset\ndf = df.iloc[np.random.permutation(len(df))]\ndf.reset_index(drop = True, inplace = True)\n\n# Get target column name\ntarget = 'target'\n\n# lgb params\nlgb_params = {\n        'boosting': 'gbdt',\n        'application': 'binary',\n        'metric': 'auc', \n        'learning_rate': 0.1,\n        'num_leaves': 32,\n        'max_depth': 8,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 5,\n        'feature_fraction': 0.7,\n}\n\n# Get folds for k-fold CV\nfolds = KFold(n_splits = NFOLD, shuffle = True, random_state = 0)\nfold = folds.split(df)\n    \neval_score = 0\nn_estimators = 0\neval_preds = np.zeros(df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6099aa185a9d5313f9f4541438d8b4387508a552"},"cell_type":"code","source":"# Run LightGBM for each fold\nfor i, (train_index, test_index) in enumerate(fold):\n    print( \"\\n[{}] Fold {} of {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), i+1, NFOLD))\n    train_X, valid_X = df[predictors].values[train_index], df[predictors].values[test_index]\n    train_y, valid_y = df[target].values[train_index], df[target].values[test_index]\n\n    dtrain = lgb.Dataset(train_X, label = train_y,\n                          feature_name = list(predictors)\n                          )\n    dvalid = lgb.Dataset(valid_X, label = valid_y,\n                          feature_name = list(predictors)\n                          )\n        \n    eval_results = {}\n    \n    bst = lgb.train(lgb_params, \n                         dtrain, \n                         valid_sets = [dtrain, dvalid], \n                         valid_names = ['train', 'valid'], \n                         evals_result = eval_results, \n                         num_boost_round = 5000,\n                         early_stopping_rounds = 100,\n                         verbose_eval = 100)\n    \n    print(\"\\nRounds:\", bst.best_iteration)\n    print(\"AUC: \", eval_results['valid']['auc'][bst.best_iteration-1])\n\n    n_estimators += bst.best_iteration\n    eval_score += eval_results['valid']['auc'][bst.best_iteration-1]\n   \n    eval_preds[test_index] += bst.predict(valid_X, num_iteration = bst.best_iteration)\n    \nn_estimators = int(round(n_estimators/NFOLD,0))\neval_score = round(eval_score/NFOLD,6)\n\nprint(\"\\nModel Report\")\nprint(\"Rounds: \", n_estimators)\nprint(\"AUC: \", eval_score)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9be534b35a88366312600f4d74808452b983cd5d"},"cell_type":"code","source":"# Feature importance\nlgb.plot_importance(bst, max_num_features = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb01bae182275fbbf7cea81310fc99b0db4024e4"},"cell_type":"code","source":"# Get training rows that are most similar to test\ndf_av = df[['ID', 'target']].copy()\ndf_av['preds'] = eval_preds\ndf_av_train = df_av[df_av.target == 1]\ndf_av_train = df_av_train.sort_values(by=['preds']).reset_index(drop=True)\n\n# Check distribution\ndf_av_train.preds.plot()\n\n# Store to feather\ndf_av_train[['ID', 'preds']].reset_index(drop=True).to_feather('adversarial_validation.ft')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acfe40f24c52915e3e094db51f67cbe8a08f4bbf"},"cell_type":"code","source":"# Check first 20 rows\ndf_av_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d5823908008e583509eee00842644bc8dd21900"},"cell_type":"markdown","source":"It seems our model's AUC is around 0.94, which means train and test are highly separable. It also seems that there are round 1000 rows in our training set (20% of the total) that have a prediction smaller than 0.1, i.e. the distribution of these rows are similar to those in the test set. We can use these rows for validating our model."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}