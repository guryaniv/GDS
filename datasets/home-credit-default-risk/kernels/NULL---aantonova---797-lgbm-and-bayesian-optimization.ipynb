{"cells":[{"metadata":{"_uuid":"ca25a2f95f0fc9498b9b2e3a9d96607fbb682015"},"cell_type":"markdown","source":"# Home Credit Default Risk 2018"},{"metadata":{"_uuid":"2776978fa169449d5bf3c8f036668afe10a47502"},"cell_type":"markdown","source":"__Warning!__ This kernel cannot run on Kaggle: not enough memory. But the code works fine and quickly on the local computer with the same amount of memory."},{"metadata":{"_uuid":"e45783c7652aae4607fc8c1ac79c979d7b387043"},"cell_type":"markdown","source":"Based on kernels: \n\n- https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features\n\n- https://www.kaggle.com/poohtls/fork-of-fork-lightgbm-with-simple-features"},{"metadata":{"_uuid":"cc4088625ae2209899d05c70dfd7bcb108cb4c3a","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport time\nimport warnings\nwarnings.simplefilter(action = 'ignore', category = FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be294207f4e12ccf54d922814789b27998577dca","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, precision_score, recall_score\nfrom sklearn.model_selection import KFold, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"962799c45d0c88ee9237cb0e202b11758abac536"},"cell_type":"code","source":"from lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42960cc0257a40fee0876294c6956e0aeba23023","trusted":true,"collapsed":true},"cell_type":"code","source":"from scipy.stats import ranksums","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b5e7cfa7c94294ad8b1eab00438592d24450e11","trusted":true,"collapsed":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32f2b23ed9dabb4b8ec59cc86f1e9cecdc9abad3"},"cell_type":"markdown","source":"## Aggregating datasets"},{"metadata":{"_uuid":"3687d746e5fe89d3477cff12521e1402431c7ab1"},"cell_type":"markdown","source":"### Service functions"},{"metadata":{"_uuid":"4b1d9d1291ce3da6dc089ec6c3918f485528caf0","trusted":true,"collapsed":true},"cell_type":"code","source":"def reduce_mem_usage(data, verbose = True):\n    start_mem = data.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n    \n    for col in data.columns:\n        col_type = data[col].dtype\n        \n        if col_type != object:\n            c_min = data[col].min()\n            c_max = data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    data[col] = data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    data[col] = data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    data[col] = data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    data[col] = data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    data[col] = data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    data[col] = data[col].astype(np.float32)\n                else:\n                    data[col] = data[col].astype(np.float64)\n\n    end_mem = data.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a418aa46cd824c9d60454df6e2625f3cd5437931","trusted":true,"collapsed":true},"cell_type":"code","source":"def one_hot_encoder(data, nan_as_category = True):\n    original_columns = list(data.columns)\n    categorical_columns = [col for col in data.columns \\\n                           if not pd.api.types.is_numeric_dtype(data[col].dtype)]\n    for c in categorical_columns:\n        if nan_as_category:\n            data[c].fillna('NaN', inplace = True)\n        values = list(data[c].unique())\n        for v in values:\n            data[str(c) + '_' + str(v)] = (data[c] == v).astype(np.uint8)\n    data.drop(categorical_columns, axis = 1, inplace = True)\n    return data, [c for c in data.columns if c not in original_columns]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2b6b0a9f460b76c27a9b9f3bf20731aaac77f19"},"cell_type":"markdown","source":"### Aggregating functions"},{"metadata":{"_uuid":"c7b1f6a694e8c78d4edff34f27514ceb06f0eb8d","trusted":true,"collapsed":true},"cell_type":"code","source":"file_path = '../input/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b047b0a2701c2586e817a0a4acd4ce88c4b29d65","trusted":true,"collapsed":true},"cell_type":"code","source":"def application_train_test(file_path = file_path, nan_as_category = True):\n    # Read data and merge\n    df_train = pd.read_csv(file_path + 'application_train.csv')\n    df_test = pd.read_csv(file_path + 'application_test.csv')\n    df = pd.concat([df_train, df_test], axis = 0, ignore_index = True)\n    del df_train, df_test\n    gc.collect()\n    \n    # Remove some rows with values not present in test set\n    df.drop(df[df['CODE_GENDER'] == 'XNA'].index, inplace = True)\n    df.drop(df[df['NAME_INCOME_TYPE'] == 'Maternity leave'].index, inplace = True)\n    df.drop(df[df['NAME_FAMILY_STATUS'] == 'Unknown'].index, inplace = True)\n    \n    # Remove some empty features\n    df.drop(['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', \n            'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n            'FLAG_DOCUMENT_21'], axis = 1, inplace = True)\n    \n    # Replace some outliers\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n    df.loc[df['OWN_CAR_AGE'] > 80, 'OWN_CAR_AGE'] = np.nan\n    df.loc[df['REGION_RATING_CLIENT_W_CITY'] < 0, 'REGION_RATING_CLIENT_W_CITY'] = np.nan\n    df.loc[df['AMT_INCOME_TOTAL'] > 1e8, 'AMT_INCOME_TOTAL'] = np.nan\n    df.loc[df['AMT_REQ_CREDIT_BUREAU_QRT'] > 10, 'AMT_REQ_CREDIT_BUREAU_QRT'] = np.nan\n    df.loc[df['OBS_30_CNT_SOCIAL_CIRCLE'] > 40, 'OBS_30_CNT_SOCIAL_CIRCLE'] = np.nan\n    \n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], _ = pd.factorize(df[bin_feature])\n        \n    # Categorical features with One-Hot encode\n    df, _ = one_hot_encoder(df, nan_as_category)\n    \n    # Some new features\n    df['app missing'] = df.isnull().sum(axis = 1).values\n    \n    df['app EXT_SOURCE mean'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis = 1)\n    df['app EXT_SOURCE std'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis = 1)\n    df['app EXT_SOURCE prod'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['app EXT_SOURCE_1 * EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']\n    df['app EXT_SOURCE_1 * EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']\n    df['app EXT_SOURCE_2 * EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['app EXT_SOURCE_1 * DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']\n    df['app EXT_SOURCE_2 * DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']\n    df['app EXT_SOURCE_3 * DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']\n    df['app EXT_SOURCE_1 / DAYS_BIRTH'] = df['EXT_SOURCE_1'] / df['DAYS_BIRTH']\n    df['app EXT_SOURCE_2 / DAYS_BIRTH'] = df['EXT_SOURCE_2'] / df['DAYS_BIRTH']\n    df['app EXT_SOURCE_3 / DAYS_BIRTH'] = df['EXT_SOURCE_3'] / df['DAYS_BIRTH']\n    \n    df['app AMT_CREDIT - AMT_GOODS_PRICE'] = df['AMT_CREDIT'] - df['AMT_GOODS_PRICE']\n    df['app AMT_CREDIT / AMT_GOODS_PRICE'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n    df['app AMT_CREDIT / AMT_ANNUITY'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n    df['app AMT_CREDIT / AMT_INCOME_TOTAL'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n    \n    df['app AMT_INCOME_TOTAL / 12 - AMT_ANNUITY'] = df['AMT_INCOME_TOTAL'] / 12. - df['AMT_ANNUITY']\n    df['app AMT_INCOME_TOTAL / AMT_ANNUITY'] = df['AMT_INCOME_TOTAL'] / df['AMT_ANNUITY']\n    df['app AMT_INCOME_TOTAL - AMT_GOODS_PRICE'] = df['AMT_INCOME_TOTAL'] - df['AMT_GOODS_PRICE']\n    df['app AMT_INCOME_TOTAL / CNT_FAM_MEMBERS'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n    df['app AMT_INCOME_TOTAL / CNT_CHILDREN'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n    \n    df['app most popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \\\n                        .isin([225000, 450000, 675000, 900000]).map({True: 1, False: 0})\n    df['app popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \\\n                        .isin([1125000, 1350000, 1575000, 1800000, 2250000]).map({True: 1, False: 0})\n    \n    df['app OWN_CAR_AGE / DAYS_BIRTH'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n    df['app OWN_CAR_AGE / DAYS_EMPLOYED'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n    \n    df['app DAYS_LAST_PHONE_CHANGE / DAYS_BIRTH'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n    df['app DAYS_LAST_PHONE_CHANGE / DAYS_EMPLOYED'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n    df['app DAYS_EMPLOYED - DAYS_BIRTH'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n    df['app DAYS_EMPLOYED / DAYS_BIRTH'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n    \n    df['app CNT_CHILDREN / CNT_FAM_MEMBERS'] = df['CNT_CHILDREN'] / df['CNT_FAM_MEMBERS']\n    \n    return reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e1514251cc659151f83eaed84deb9559536e04","trusted":true,"collapsed":true},"cell_type":"code","source":"def bureau_and_balance(file_path = file_path, nan_as_category = True):\n    df_bureau_b = reduce_mem_usage(pd.read_csv(file_path + 'bureau_balance.csv'), verbose = False)\n    \n    # Some new features in bureau_balance set\n    tmp = df_bureau_b[['SK_ID_BUREAU', 'STATUS']].groupby('SK_ID_BUREAU')\n    tmp_last = tmp.last()\n    tmp_last.columns = ['First_status']\n    df_bureau_b = df_bureau_b.join(tmp_last, how = 'left', on = 'SK_ID_BUREAU')\n    tmp_first = tmp.first()\n    tmp_first.columns = ['Last_status']\n    df_bureau_b = df_bureau_b.join(tmp_first, how = 'left', on = 'SK_ID_BUREAU')\n    del tmp, tmp_first, tmp_last\n    gc.collect()\n    \n    tmp = df_bureau_b[['SK_ID_BUREAU', 'MONTHS_BALANCE']].groupby('SK_ID_BUREAU').last()\n    tmp = tmp.apply(abs)\n    tmp.columns = ['Month']\n    df_bureau_b = df_bureau_b.join(tmp, how = 'left', on = 'SK_ID_BUREAU')\n    del tmp\n    gc.collect()\n    \n    tmp = df_bureau_b.loc[df_bureau_b['STATUS'] == 'C', ['SK_ID_BUREAU', 'MONTHS_BALANCE']] \\\n                .groupby('SK_ID_BUREAU').last()\n    tmp = tmp.apply(abs)\n    tmp.columns = ['When_closed']\n    df_bureau_b = df_bureau_b.join(tmp, how = 'left', on = 'SK_ID_BUREAU')\n    del tmp\n    gc.collect()\n    \n    df_bureau_b['Month_closed_to_end'] = df_bureau_b['Month'] - df_bureau_b['When_closed']\n\n    for c in range(6):\n        tmp = df_bureau_b.loc[df_bureau_b['STATUS'] == str(c), ['SK_ID_BUREAU', 'MONTHS_BALANCE']] \\\n                         .groupby('SK_ID_BUREAU').count()\n        tmp.columns = ['DPD_' + str(c) + '_cnt']\n        df_bureau_b = df_bureau_b.join(tmp, how = 'left', on = 'SK_ID_BUREAU')\n        df_bureau_b['DPD_' + str(c) + ' / Month'] = df_bureau_b['DPD_' + str(c) + '_cnt'] / df_bureau_b['Month']\n        del tmp\n        gc.collect()\n    df_bureau_b['Non_zero_DPD_cnt'] = df_bureau_b[['DPD_1_cnt', 'DPD_2_cnt', 'DPD_3_cnt', 'DPD_4_cnt', 'DPD_5_cnt']].sum(axis = 1)\n    \n    df_bureau_b, bureau_b_cat = one_hot_encoder(df_bureau_b, nan_as_category)\n\n    # Bureau balance: Perform aggregations \n    aggregations = {}\n    for col in df_bureau_b.columns:\n        aggregations[col] = ['mean'] if col in bureau_b_cat else ['min', 'max', 'size']\n    df_bureau_b_agg = df_bureau_b.groupby('SK_ID_BUREAU').agg(aggregations)\n    df_bureau_b_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in df_bureau_b_agg.columns.tolist()])\n    del df_bureau_b\n    gc.collect()\n\n    df_bureau = reduce_mem_usage(pd.read_csv(file_path + 'bureau.csv'), verbose = False)\n                  \n    # Replace\\remove some outliers in bureau set\n    df_bureau.loc[df_bureau['AMT_ANNUITY'] > .8e8, 'AMT_ANNUITY'] = np.nan\n    df_bureau.loc[df_bureau['AMT_CREDIT_SUM'] > 3e8, 'AMT_CREDIT_SUM'] = np.nan\n    df_bureau.loc[df_bureau['AMT_CREDIT_SUM_DEBT'] > 1e8, 'AMT_CREDIT_SUM_DEBT'] = np.nan\n    df_bureau.loc[df_bureau['AMT_CREDIT_MAX_OVERDUE'] > .8e8, 'AMT_CREDIT_MAX_OVERDUE'] = np.nan\n    df_bureau.loc[df_bureau['DAYS_ENDDATE_FACT'] < -10000, 'DAYS_ENDDATE_FACT'] = np.nan\n    df_bureau.loc[(df_bureau['DAYS_CREDIT_UPDATE'] > 0) | (df_bureau['DAYS_CREDIT_UPDATE'] < -40000), 'DAYS_CREDIT_UPDATE'] = np.nan\n    df_bureau.loc[df_bureau['DAYS_CREDIT_ENDDATE'] < -10000, 'DAYS_CREDIT_ENDDATE'] = np.nan\n    \n    df_bureau.drop(df_bureau[df_bureau['DAYS_ENDDATE_FACT'] < df_bureau['DAYS_CREDIT']].index, inplace = True)\n    \n    # Some new features in bureau set\n    df_bureau['bureau AMT_CREDIT_SUM - AMT_CREDIT_SUM_DEBT'] = df_bureau['AMT_CREDIT_SUM'] - df_bureau['AMT_CREDIT_SUM_DEBT']\n    df_bureau['bureau AMT_CREDIT_SUM - AMT_CREDIT_SUM_LIMIT'] = df_bureau['AMT_CREDIT_SUM'] - df_bureau['AMT_CREDIT_SUM_LIMIT']\n    df_bureau['bureau AMT_CREDIT_SUM - AMT_CREDIT_SUM_OVERDUE'] = df_bureau['AMT_CREDIT_SUM'] - df_bureau['AMT_CREDIT_SUM_OVERDUE']\n\n    df_bureau['bureau DAYS_CREDIT - CREDIT_DAY_OVERDUE'] = df_bureau['DAYS_CREDIT'] - df_bureau['CREDIT_DAY_OVERDUE']\n    df_bureau['bureau DAYS_CREDIT - DAYS_CREDIT_ENDDATE'] = df_bureau['DAYS_CREDIT'] - df_bureau['DAYS_CREDIT_ENDDATE']\n    df_bureau['bureau DAYS_CREDIT - DAYS_ENDDATE_FACT'] = df_bureau['DAYS_CREDIT'] - df_bureau['DAYS_ENDDATE_FACT']\n    df_bureau['bureau DAYS_CREDIT_ENDDATE - DAYS_ENDDATE_FACT'] = df_bureau['DAYS_CREDIT_ENDDATE'] - df_bureau['DAYS_ENDDATE_FACT']\n    df_bureau['bureau DAYS_CREDIT_UPDATE - DAYS_CREDIT_ENDDATE'] = df_bureau['DAYS_CREDIT_UPDATE'] - df_bureau['DAYS_CREDIT_ENDDATE']\n    \n    # Categorical features with One-Hot encode\n    df_bureau, bureau_cat = one_hot_encoder(df_bureau, nan_as_category)\n    \n    # Bureau balance: merge with bureau.csv\n    df_bureau = df_bureau.join(df_bureau_b_agg, how = 'left', on = 'SK_ID_BUREAU')\n    df_bureau.drop('SK_ID_BUREAU', axis = 1, inplace = True)\n    del df_bureau_b_agg\n    gc.collect()\n    \n    # Bureau and bureau_balance aggregations for application set\n    categorical = bureau_cat + bureau_b_cat\n    aggregations = {}\n    for col in df_bureau.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_bureau_agg = df_bureau.groupby('SK_ID_CURR').agg(aggregations)\n    df_bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in df_bureau_agg.columns.tolist()])\n    \n    # Bureau: Active credits\n    active_agg = df_bureau[df_bureau['CREDIT_ACTIVE_Active'] == 1].groupby('SK_ID_CURR').agg(aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    df_bureau_agg = df_bureau_agg.join(active_agg, how = 'left')\n    del active_agg\n    gc.collect()\n    \n    # Bureau: Closed credits\n    closed_agg = df_bureau[df_bureau['CREDIT_ACTIVE_Closed'] == 1].groupby('SK_ID_CURR').agg(aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    df_bureau_agg = df_bureau_agg.join(closed_agg, how = 'left')\n    del closed_agg, df_bureau\n    gc.collect()\n    \n    return reduce_mem_usage(df_bureau_agg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"611655246d527af034c24dae72e5cdfee3b404cb","trusted":true,"collapsed":true},"cell_type":"code","source":"def previous_application(file_path = file_path, nan_as_category = True):\n    df_prev = pd.read_csv(file_path + 'previous_application.csv')\n    \n    # Replace some outliers\n    df_prev.loc[df_prev['AMT_CREDIT'] > 6000000, 'AMT_CREDIT'] = np.nan\n    df_prev.loc[df_prev['SELLERPLACE_AREA'] > 3500000, 'SELLERPLACE_AREA'] = np.nan\n    df_prev[['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', \n             'DAYS_LAST_DUE', 'DAYS_TERMINATION']].replace(365243, np.nan, inplace = True)\n    \n    # Some new features\n    df_prev['prev missing'] = df_prev.isnull().sum(axis = 1).values\n    df_prev['prev AMT_APPLICATION / AMT_CREDIT'] = df_prev['AMT_APPLICATION'] / df_prev['AMT_CREDIT']\n    df_prev['prev AMT_APPLICATION - AMT_CREDIT'] = df_prev['AMT_APPLICATION'] - df_prev['AMT_CREDIT']\n    df_prev['prev AMT_APPLICATION - AMT_GOODS_PRICE'] = df_prev['AMT_APPLICATION'] - df_prev['AMT_GOODS_PRICE']\n    df_prev['prev AMT_GOODS_PRICE - AMT_CREDIT'] = df_prev['AMT_GOODS_PRICE'] - df_prev['AMT_CREDIT']\n    df_prev['prev DAYS_FIRST_DRAWING - DAYS_FIRST_DUE'] = df_prev['DAYS_FIRST_DRAWING'] - df_prev['DAYS_FIRST_DUE']\n    df_prev['prev DAYS_TERMINATION less -500'] = (df_prev['DAYS_TERMINATION'] < -500).astype(int)\n    \n    # Categorical features with One-Hot encode\n    df_prev, categorical = one_hot_encoder(df_prev, nan_as_category)\n\n    # Aggregations for application set\n    aggregations = {}\n    for col in df_prev.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_prev_agg = df_prev.groupby('SK_ID_CURR').agg(aggregations)\n    df_prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in df_prev_agg.columns.tolist()])\n    \n    # Previous Applications: Approved Applications\n    approved_agg = df_prev[df_prev['NAME_CONTRACT_STATUS_Approved'] == 1].groupby('SK_ID_CURR').agg(aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    df_prev_agg = df_prev_agg.join(approved_agg, how = 'left')\n    del approved_agg\n    gc.collect()\n    \n    # Previous Applications: Refused Applications\n    refused_agg = df_prev[df_prev['NAME_CONTRACT_STATUS_Refused'] == 1].groupby('SK_ID_CURR').agg(aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    df_prev_agg = df_prev_agg.join(refused_agg, how = 'left')\n    del refused_agg, df_prev\n    gc.collect()\n    \n    return reduce_mem_usage(df_prev_agg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb88e53d8bc09a2d4be77ad31dfb60bb39d3cef6","trusted":true,"collapsed":true},"cell_type":"code","source":"def pos_cash(file_path = file_path, nan_as_category = True):\n    df_pos = pd.read_csv(file_path + 'POS_CASH_balance.csv')\n    \n    # Replace some outliers\n    df_pos.loc[df_pos['CNT_INSTALMENT_FUTURE'] > 60, 'CNT_INSTALMENT_FUTURE'] = np.nan\n    \n    # Some new features\n    df_pos['pos CNT_INSTALMENT more CNT_INSTALMENT_FUTURE'] = \\\n                    (df_pos['CNT_INSTALMENT'] > df_pos['CNT_INSTALMENT_FUTURE']).astype(int)\n    \n    # Categorical features with One-Hot encode\n    df_pos, categorical = one_hot_encoder(df_pos, nan_as_category)\n    \n    # Aggregations for application set\n    aggregations = {}\n    for col in df_pos.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_pos_agg = df_pos.groupby('SK_ID_CURR').agg(aggregations)\n    df_pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in df_pos_agg.columns.tolist()])\n\n    # Count POS lines\n    df_pos_agg['POS_COUNT'] = df_pos.groupby('SK_ID_CURR').size()\n    del df_pos\n    gc.collect()\n    \n    return reduce_mem_usage(df_pos_agg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1270941b2152488627f025154b918d4b8e285576","trusted":true,"collapsed":true},"cell_type":"code","source":"def installments_payments(file_path = file_path, nan_as_category = True):\n    df_ins = pd.read_csv(file_path + 'installments_payments.csv')\n    \n    # Replace some outliers\n    df_ins.loc[df_ins['NUM_INSTALMENT_VERSION'] > 70, 'NUM_INSTALMENT_VERSION'] = np.nan\n    df_ins.loc[df_ins['DAYS_ENTRY_PAYMENT'] < -4000, 'DAYS_ENTRY_PAYMENT'] = np.nan\n    \n    # Some new features\n    df_ins['ins DAYS_ENTRY_PAYMENT - DAYS_INSTALMENT'] = df_ins['DAYS_ENTRY_PAYMENT'] - df_ins['DAYS_INSTALMENT']\n    df_ins['ins NUM_INSTALMENT_NUMBER_100'] = (df_ins['NUM_INSTALMENT_NUMBER'] == 100).astype(int)\n    df_ins['ins DAYS_INSTALMENT more NUM_INSTALMENT_NUMBER'] = (df_ins['DAYS_INSTALMENT'] > df_ins['NUM_INSTALMENT_NUMBER'] * 50 / 3 - 11500 / 3).astype(int)\n    df_ins['ins AMT_INSTALMENT - AMT_PAYMENT'] = df_ins['AMT_INSTALMENT'] - df_ins['AMT_PAYMENT']\n    df_ins['ins AMT_PAYMENT / AMT_INSTALMENT'] = df_ins['AMT_PAYMENT'] / df_ins['AMT_INSTALMENT']\n    \n    # Categorical features with One-Hot encode\n    df_ins, categorical = one_hot_encoder(df_ins, nan_as_category)\n\n    # Aggregations for application set\n    aggregations = {}\n    for col in df_ins.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_ins_agg = df_ins.groupby('SK_ID_CURR').agg(aggregations)\n    df_ins_agg.columns = pd.Index(['INS_' + e[0] + \"_\" + e[1].upper() for e in df_ins_agg.columns.tolist()])\n    \n    # Count installments lines\n    df_ins_agg['INSTAL_COUNT'] = df_ins.groupby('SK_ID_CURR').size()\n    del df_ins\n    gc.collect()\n    \n    return reduce_mem_usage(df_ins_agg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abda66f62993adecb7b8e1a1286de38504312930","trusted":true,"collapsed":true},"cell_type":"code","source":"def credit_card_balance(file_path = file_path, nan_as_category = True):\n    df_card = pd.read_csv(file_path + 'credit_card_balance.csv')\n    \n    # Replace some outliers\n    df_card.loc[df_card['AMT_PAYMENT_CURRENT'] > 4000000, 'AMT_PAYMENT_CURRENT'] = np.nan\n    df_card.loc[df_card['AMT_CREDIT_LIMIT_ACTUAL'] > 1000000, 'AMT_CREDIT_LIMIT_ACTUAL'] = np.nan\n\n    # Some new features\n    df_card['card missing'] = df_card.isnull().sum(axis = 1).values\n    df_card['card SK_DPD - MONTHS_BALANCE'] = df_card['SK_DPD'] - df_card['MONTHS_BALANCE']\n    df_card['card SK_DPD_DEF - MONTHS_BALANCE'] = df_card['SK_DPD_DEF'] - df_card['MONTHS_BALANCE']\n    df_card['card SK_DPD - SK_DPD_DEF'] = df_card['SK_DPD'] - df_card['SK_DPD_DEF']\n    \n    df_card['card AMT_TOTAL_RECEIVABLE - AMT_RECIVABLE'] = df_card['AMT_TOTAL_RECEIVABLE'] - df_card['AMT_RECIVABLE']\n    df_card['card AMT_TOTAL_RECEIVABLE - AMT_RECEIVABLE_PRINCIPAL'] = df_card['AMT_TOTAL_RECEIVABLE'] - df_card['AMT_RECEIVABLE_PRINCIPAL']\n    df_card['card AMT_RECIVABLE - AMT_RECEIVABLE_PRINCIPAL'] = df_card['AMT_RECIVABLE'] - df_card['AMT_RECEIVABLE_PRINCIPAL']\n\n    df_card['card AMT_BALANCE - AMT_RECIVABLE'] = df_card['AMT_BALANCE'] - df_card['AMT_RECIVABLE']\n    df_card['card AMT_BALANCE - AMT_RECEIVABLE_PRINCIPAL'] = df_card['AMT_BALANCE'] - df_card['AMT_RECEIVABLE_PRINCIPAL']\n    df_card['card AMT_BALANCE - AMT_TOTAL_RECEIVABLE'] = df_card['AMT_BALANCE'] - df_card['AMT_TOTAL_RECEIVABLE']\n\n    df_card['card AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_ATM_CURRENT'] = df_card['AMT_DRAWINGS_CURRENT'] - df_card['AMT_DRAWINGS_ATM_CURRENT']\n    df_card['card AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_OTHER_CURRENT'] = df_card['AMT_DRAWINGS_CURRENT'] - df_card['AMT_DRAWINGS_OTHER_CURRENT']\n    df_card['card AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_POS_CURRENT'] = df_card['AMT_DRAWINGS_CURRENT'] - df_card['AMT_DRAWINGS_POS_CURRENT']\n    \n    # Categorical features with One-Hot encode\n    df_card, categorical = one_hot_encoder(df_card, nan_as_category)\n    \n    # Aggregations for application set\n    aggregations = {}\n    for col in df_card.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_card_agg = df_card.groupby('SK_ID_CURR').agg(aggregations)\n    df_card_agg.columns = pd.Index(['CARD_' + e[0] + \"_\" + e[1].upper() for e in df_card_agg.columns.tolist()])\n\n    # Count credit card lines\n    df_card_agg['CARD_COUNT'] = df_card.groupby('SK_ID_CURR').size()\n    del df_card\n    gc.collect()\n    \n    return reduce_mem_usage(df_card_agg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"390cbb9f2068e8feef6aac66a797c54697924d63","trusted":true,"collapsed":true},"cell_type":"code","source":"def aggregate(file_path = file_path):\n    warnings.simplefilter(action = 'ignore')\n    \n    print('-' * 20)\n    print('1: application train & test (', time.ctime(), ')')\n    print('-' * 20)\n    df = application_train_test(file_path)\n    print('     DF shape:', df.shape)\n    \n    print('-' * 20)\n    print('2: bureau & balance (', time.ctime(), ')')\n    print('-' * 20)\n    bureau = bureau_and_balance(file_path)\n    df = df.join(bureau, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del bureau\n    gc.collect()\n    \n    print('-' * 20)\n    print('3: previous_application (', time.ctime(), ')')\n    print('-' * 20)\n    prev = previous_application(file_path)\n    df = df.join(prev, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del prev\n    gc.collect()\n    \n    print('-' * 20)\n    print('4: POS_CASH_balance (', time.ctime(), ')')\n    print('-' * 20)\n    pos = pos_cash(file_path)\n    df = df.join(pos, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del pos\n    gc.collect()\n    \n    print('-' * 20)\n    print('5: installments_payments (', time.ctime(), ')')\n    print('-' * 20)\n    ins = installments_payments(file_path)\n    df = df.join(ins, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del ins\n    gc.collect()\n    \n    print('-' * 20)\n    print('6: credit_card_balance (', time.ctime(), ')')\n    print('-' * 20)\n    cc = credit_card_balance(file_path)\n    df = df.join(cc, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del cc\n    gc.collect()\n    \n    print('-' * 20)\n    print('7: final dataset (', time.ctime(), ')')\n    print('-' * 20)\n    return reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbc835b5355c087c1fdf34b8655848400b2666bf","trusted":true,"collapsed":true},"cell_type":"code","source":"# Kaggle has not ehough memory to clean this dataset\n# Aggregated dataset has 3411 features\n\n#df = aggregate()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cdf22861b989e1dfe6331ecef61b13ef3fa2d05"},"cell_type":"markdown","source":"## Cleaning dataset"},{"metadata":{"_uuid":"c8536f211c9768b6f7622a43413e8bdd2a46092d","trusted":true,"collapsed":true},"cell_type":"code","source":"def corr_feature_with_target(feature, target):\n    c0 = feature[target == 0].dropna()\n    c1 = feature[target == 1].dropna()\n        \n    if set(feature.unique()) == set([0, 1]):\n        diff = abs(c0.mean(axis = 0) - c1.mean(axis = 0))\n    else:\n        diff = abs(c0.median(axis = 0) - c1.median(axis = 0))\n        \n    p = ranksums(c0, c1)[1] if ((len(c0) >= 20) & (len(c1) >= 20)) else 2\n        \n    return [diff, p]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c04f8067c2a3971de8cc0bd521732e18b78ee4c5","trusted":true,"collapsed":true},"cell_type":"code","source":"def clean_data(data):\n    warnings.simplefilter(action = 'ignore')\n    \n    # Removing empty features\n    nun = data.nunique()\n    empty = list(nun[nun <= 1].index)\n    \n    data.drop(empty, axis = 1, inplace = True)\n    print('After removing empty features there are {0:d} features'.format(data.shape[1]))\n    \n    # Removing features with the same distribution on 0 and 1 classes\n    corr = pd.DataFrame(index = ['diff', 'p'])\n    ind = data[data['TARGET'].notnull()].index\n    \n    for c in data.columns.drop('TARGET'):\n        corr[c] = corr_feature_with_target(data.loc[ind, c], data.loc[ind, 'TARGET'])\n\n    corr = corr.T\n    corr['diff_norm'] = abs(corr['diff'] / data.mean(axis = 0))\n    \n    to_del_1 = corr[((corr['diff'] == 0) & (corr['p'] > .05))].index\n    to_del_2 = corr[((corr['diff_norm'] < .5) & (corr['p'] > .05))].drop(to_del_1).index\n    to_del = list(to_del_1) + list(to_del_2)\n    if 'SK_ID_CURR' in to_del:\n        to_del.remove('SK_ID_CURR')\n        \n    data.drop(to_del, axis = 1, inplace = True)\n    print('After removing features with the same distribution on 0 and 1 classes there are {0:d} features'.format(data.shape[1]))\n    \n    # Removing features with not the same distribution on train and test datasets\n    corr_test = pd.DataFrame(index = ['diff', 'p'])\n    target = data['TARGET'].notnull().astype(int)\n    \n    for c in data.columns.drop('TARGET'):\n        corr_test[c] = corr_feature_with_target(data[c], target)\n\n    corr_test = corr_test.T\n    corr_test['diff_norm'] = abs(corr_test['diff'] / data.mean(axis = 0))\n    \n    bad_features = corr_test[((corr_test['p'] < .05) & (corr_test['diff_norm'] > 1))].index\n    bad_features = corr.loc[bad_features][corr['diff_norm'] == 0].index\n    \n    data.drop(bad_features, axis = 1, inplace = True)\n    print('After removing features with not the same distribution on train and test datasets there are {0:d} features'.format(data.shape[1]))\n    \n    del corr, corr_test\n    gc.collect()\n    \n    # Removing features not interesting for classifier\n    clf = LGBMClassifier(random_state = 0)\n    train_index = data[data['TARGET'].notnull()].index\n    train_columns = data.drop('TARGET', axis = 1).columns\n\n    score = 1\n    new_columns = []\n    while score > .7:\n        train_columns = train_columns.drop(new_columns)\n        clf.fit(data.loc[train_index, train_columns], data.loc[train_index, 'TARGET'])\n        f_imp = pd.Series(clf.feature_importances_, index = train_columns)\n        score = roc_auc_score(data.loc[train_index, 'TARGET'], \n                              clf.predict_proba(data.loc[train_index, train_columns])[:, 1])\n        new_columns = f_imp[f_imp > 0].index\n\n    data.drop(train_columns, axis = 1, inplace = True)\n    print('After removing features not interesting for classifier there are {0:d} features'.format(data.shape[1]))\n\n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"031a2b4708575b98912023fe7c7cddabbaef6898","trusted":true,"collapsed":true},"cell_type":"code","source":"# Kaggle has not ehough memory to run this code - more than 14 Gb RAM\n\n# Dataset for cleaning has 3411 features\n# After removing empty features there are 3289 features\n# After removing features with the same distribution on 0 and 1 classes there are 2171 features\n# After removing features with not the same distribution on train and test datasets there are 2115 features\n# After removing features not interesting for classifier there are 1505 features\n\n#df = clean_data(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f7495063700c9e611ea1e27f90f40acd5085cae"},"cell_type":"markdown","source":"## Optimization LGBM parameters"},{"metadata":{"_uuid":"9148f2a806be7e22a5cfb90442d82a72e1da4084"},"cell_type":"markdown","source":"### Optimization and visualisation functions"},{"metadata":{"_uuid":"b440750e95bc79ad0603245387b08aefe048b0af","trusted":true,"collapsed":true},"cell_type":"code","source":"def cv_scores(df, num_folds, params, stratified = False, verbose = -1, \n              save_train_prediction = False, train_prediction_file_name = 'train_prediction.csv',\n              save_test_prediction = True, test_prediction_file_name = 'test_prediction.csv'):\n    warnings.simplefilter('ignore')\n    \n    clf = LGBMClassifier(**params)\n\n    # Divide in training/validation and test data\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 1001)\n    else:\n        folds = KFold(n_splits = num_folds, shuffle = True, random_state = 1001)\n        \n    # Create arrays and dataframes to store results\n    train_pred = np.zeros(train_df.shape[0])\n    train_pred_proba = np.zeros(train_df.shape[0])\n\n    test_pred = np.zeros(train_df.shape[0])\n    test_pred_proba = np.zeros(train_df.shape[0])\n    \n    prediction = np.zeros(test_df.shape[0])\n    \n    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    \n    df_feature_importance = pd.DataFrame(index = feats)\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        print('Fold', n_fold, 'started at', time.ctime())\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        clf.fit(train_x, train_y, \n                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n                verbose = verbose, early_stopping_rounds = 200)\n\n        train_pred[train_idx] = clf.predict(train_x, num_iteration = clf.best_iteration_)\n        train_pred_proba[train_idx] = clf.predict_proba(train_x, num_iteration = clf.best_iteration_)[:, 1]\n        test_pred[valid_idx] = clf.predict(valid_x, num_iteration = clf.best_iteration_)\n        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n        \n        prediction += \\\n                clf.predict_proba(test_df[feats], num_iteration = clf.best_iteration_)[:, 1] / folds.n_splits\n\n        df_feature_importance[n_fold] = pd.Series(clf.feature_importances_, index = feats)\n        \n        print('Fold %2d AUC : %.6f' % (n_fold, roc_auc_score(valid_y, test_pred_proba[valid_idx])))\n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    roc_auc_train = roc_auc_score(train_df['TARGET'], train_pred_proba)\n    precision_train = precision_score(train_df['TARGET'], train_pred, average = None)\n    recall_train = recall_score(train_df['TARGET'], train_pred, average = None)\n    \n    roc_auc_test = roc_auc_score(train_df['TARGET'], test_pred_proba)\n    precision_test = precision_score(train_df['TARGET'], test_pred, average = None)\n    recall_test = recall_score(train_df['TARGET'], test_pred, average = None)\n\n    print('Full AUC score %.6f' % roc_auc_test)\n    \n    df_feature_importance.fillna(0, inplace = True)\n    df_feature_importance['mean'] = df_feature_importance.mean(axis = 1)\n    \n    # Write prediction files\n    if save_train_prediction:\n        df_prediction = train_df[['SK_ID_CURR', 'TARGET']]\n        df_prediction['Prediction'] = test_pred_proba\n        df_prediction.to_csv(train_prediction_file_name, index = False)\n        del df_prediction\n        gc.collect()\n\n    if save_test_prediction:\n        df_prediction = test_df[['SK_ID_CURR']]\n        df_prediction['TARGET'] = prediction\n        df_prediction.to_csv(test_prediction_file_name, index = False)\n        del df_prediction\n        gc.collect()\n    \n    return df_feature_importance, \\\n           [roc_auc_train, roc_auc_test,\n            precision_train[0], precision_test[0], precision_train[1], precision_test[1],\n            recall_train[0], recall_test[0], recall_train[1], recall_test[1], 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c8cd095e8a6472475c239973fc25af25e10534e","trusted":true,"collapsed":true},"cell_type":"code","source":"def display_folds_importances(feature_importance_df_, n_folds = 5):\n    n_columns = 3\n    n_rows = (n_folds + 1) // n_columns\n    _, axes = plt.subplots(n_rows, n_columns, figsize=(8 * n_columns, 8 * n_rows))\n    for i in range(n_folds):\n        sns.barplot(x = i, y = 'index', data = feature_importance_df_.reset_index().sort_values(i, ascending = False).head(20), \n                    ax = axes[i // n_columns, i % n_columns])\n    sns.barplot(x = 'mean', y = 'index', data = feature_importance_df_.reset_index().sort_values('mean', ascending = False).head(20), \n                    ax = axes[n_rows - 1, n_columns - 1])\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4549a6ebeb42683066f5a2d1da493cc98135a61d"},"cell_type":"markdown","source":"### Table for scores"},{"metadata":{"_uuid":"f0714d9b17343b47fb218d6559906018e5673735","trusted":true,"collapsed":true},"cell_type":"code","source":"scores_index = [\n    'roc_auc_train', 'roc_auc_test', \n    'precision_train_0', 'precision_test_0', \n    'precision_train_1', 'precision_test_1', \n    'recall_train_0', 'recall_test_0', \n    'recall_train_1', 'recall_test_1', \n    'LB'\n]\n\nscores = pd.DataFrame(index = scores_index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe4fb091c39f4edd0bbf997bcc5630e35b47228b"},"cell_type":"markdown","source":"### First scores with parameters from Tilii kernel"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bba1ec6f8d9f45f0c0b0d5c927df6f8b893c4560"},"cell_type":"code","source":"# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\nlgbm_params = {\n            'nthread': 4,\n            'n_estimators': 10000,\n            'learning_rate': .02,\n            'num_leaves': 34,\n            'colsample_bytree': .9497036,\n            'subsample': .8715623,\n            'max_depth': 8,\n            'reg_alpha': .041545473,\n            'reg_lambda': .0735294,\n            'min_split_gain': .0222415,\n            'min_child_weight': 39.3259775,\n            'silent': -1,\n            'verbose': -1\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0c18d584ffb1abbc944ae5f2f78e9b30b67c3f7","trusted":true,"collapsed":true},"cell_type":"code","source":"#feature_importance, scor = cv_scores(df, 5, lgbm_params, test_prediction_file_name = 'prediction_0.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da6f3a9750b217acb21bc76a34239aa438567228","trusted":true,"collapsed":true},"cell_type":"code","source":"#step = 'Tilii`s Bayesian optimization'\n#scores[step] = scor\n#scores.loc['LB', step] = .797\n#scores.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b3d39f6ddc4ee9faec197ff043c1955dbafd90a","trusted":true,"collapsed":true},"cell_type":"code","source":"#display_folds_importances(feature_importance)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4b2b24d30d0283b4eae36dba1cfc166fc70dcdc","trusted":true,"collapsed":true},"cell_type":"code","source":"#feature_importance[feature_importance['mean'] == 0].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ecd58af84525839510d1669efb1cc0bb251e310","trusted":true,"collapsed":true},"cell_type":"code","source":"#feature_importance.sort_values('mean', ascending = False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cd32bd6d9279b1575d87877790dc235283903a8"},"cell_type":"markdown","source":"### New Bayesian Optimization"},{"metadata":{"_uuid":"64a682f66238fa787178ee8302ea5273c8646555","trusted":true,"collapsed":true},"cell_type":"code","source":"def lgbm_evaluate(**params):\n    warnings.simplefilter('ignore')\n    \n    params['num_leaves'] = int(params['num_leaves'])\n    params['max_depth'] = int(params['max_depth'])\n        \n    clf = LGBMClassifier(**params, n_estimators = 10000, nthread = 4)\n\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n\n    folds = KFold(n_splits = 2, shuffle = True, random_state = 1001)\n        \n    test_pred_proba = np.zeros(train_df.shape[0])\n    \n    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        clf.fit(train_x, train_y, \n                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n                verbose = False, early_stopping_rounds = 100)\n\n        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n        \n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    return roc_auc_score(train_df['TARGET'], test_pred_proba)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"556c867c985588834ef080975c6be88a5c54cd90","trusted":true,"collapsed":true},"cell_type":"code","source":"params = {'colsample_bytree': (0.8, 1),\n          'learning_rate': (.01, .02), \n          'num_leaves': (33, 35), \n          'subsample': (0.8, 1), \n          'max_depth': (7, 9), \n          'reg_alpha': (.03, .05), \n          'reg_lambda': (.06, .08), \n          'min_split_gain': (.01, .03),\n          'min_child_weight': (38, 40)}\n#bo = BayesianOptimization(lgbm_evaluate, params)\n#bo.maximize(init_points = 5, n_iter = 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e79d57053f66ddc8c166e705514030870bc67d0e","trusted":true,"collapsed":true},"cell_type":"code","source":"#best_params = bo.res['max']['max_params']\n#best_params['num_leaves'] = int(best_params['num_leaves'])\n#best_params['max_depth'] = int(best_params['max_depth'])\n\n#best_params","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e07d4e2ce1ea005682fe5cb68a7e8c89f2e938da","trusted":true,"collapsed":true},"cell_type":"code","source":"#bo.res['max']['max_val']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c7bb52b49aa2647c5535031dd95ef516670ec175"},"cell_type":"code","source":"#feature_importance, scor = cv_scores(df, 5, best_params, test_prediction_file_name = 'prediction_1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a2f3ff539febdda719dc338caa89d6cd02a21c7"},"cell_type":"code","source":"#step = 'Bayesian optimization for new set'\n#scores[step] = scor\n#scores.loc['LB', step] = .797\n#scores.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"108d69bb84fc2899d246c748b5173719622f4aa2"},"cell_type":"code","source":"#display_folds_importances(feature_importance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8cf68f0e2afb1ac125af0dbf14cce64455965e5a"},"cell_type":"code","source":"#feature_importance[feature_importance['mean'] == 0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"65e0e56886249c8b1c3acae9e0411a34baf75fe5"},"cell_type":"code","source":"#feature_importance.sort_values('mean', ascending = False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7394bcdf79fc237f81aa13636f39783e42d84a41"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}