{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6748f8d1-eb06-039a-9b7a-93b2de5ca8e9"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3cf573cb-3835-a3a1-b7b1-14783d890bbf"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from scipy import sparse\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import Imputer, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression, BayesianRidge, LinearRegression, SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, log_loss, classification_report\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c4a40a-d7c5-2eb6-b24e-bfa810bd4d5e"
      },
      "outputs": [],
      "source": [
        "data_path = \"../input/\"\n",
        "train_file = data_path + \"train.json\"\n",
        "test_file = data_path + \"test.json\"\n",
        "train_df = pd.read_json(train_file)\n",
        "test_df = pd.read_json(test_file)\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c7aa1154-5828-a0a5-800c-17589cf824f5"
      },
      "outputs": [],
      "source": [
        "test_df[[\"listing_id\", \"display_address\",\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2d2aa6d2-131a-9e0b-280d-41ec95db2086"
      },
      "outputs": [],
      "source": [
        "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n",
        "    param = {}\n",
        "    param['objective'] = 'multi:softprob'\n",
        "    param['eta'] = 0.1\n",
        "    param['max_depth'] = 6\n",
        "    param['silent'] = 1\n",
        "    param['num_class'] = 3\n",
        "    param['eval_metric'] = \"mlogloss\"\n",
        "    param['min_child_weight'] = 1\n",
        "    param['subsample'] = 0.7\n",
        "    param['colsample_bytree'] = 0.7\n",
        "    param['seed'] = seed_val\n",
        "    num_rounds = num_rounds\n",
        "\n",
        "    plst = list(param.items())\n",
        "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
        "\n",
        "    if test_y is not None:\n",
        "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
        "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
        "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
        "    else:\n",
        "        xgtest = xgb.DMatrix(test_X)\n",
        "        model = xgb.train(plst, xgtrain, num_rounds)\n",
        "\n",
        "    pred_test_y = model.predict(xgtest)\n",
        "    return pred_test_y, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6704b36-8c1a-46ec-743c-a75164e7a3ab"
      },
      "outputs": [],
      "source": [
        "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]\n",
        "features_for_xgb  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "12e75972-ddcf-2e49-8d22-6ee875b1fe8a"
      },
      "outputs": [],
      "source": [
        "# count of photos #\n",
        "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
        "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
        "\n",
        "# count of \"features\" #\n",
        "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
        "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
        "\n",
        "# count of words present in description column #\n",
        "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "\n",
        "# convert the created column to datetime object so as to extract more features \n",
        "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
        "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
        "\n",
        "# Let us extract some features like year, month, day, hour from date columns #\n",
        "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
        "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
        "\n",
        "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
        "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
        "\n",
        "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
        "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
        "\n",
        "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
        "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
        "\n",
        "train_df[\"weekday\"] = train_df[\"created\"].dt.weekday\n",
        "test_df[\"weekday\"] = test_df[\"created\"].dt.weekday\n",
        "\n",
        "# adding all these new features to use list #\n",
        "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\", \"created_month\", \"created_day\", \"created_hour\", \"weekday\"])\n",
        "features_for_xgb.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ee949888-3758-31b8-18b7-877df4ba7727"
      },
      "outputs": [],
      "source": [
        "# Building Level\n",
        "train_df.ix[train_df.building_id == '0', 'new_building_id'] = train_df['building_id'] + train_df['manager_id']\n",
        "train_df.ix[train_df.building_id != '0', 'new_building_id'] = train_df['building_id']\n",
        "\n",
        "a=[np.nan]*len(train_df)\n",
        "building_level={}\n",
        "\n",
        "for bid in train_df['new_building_id'].values:\n",
        "    building_level[bid]=[0,0,0]\n",
        "    \n",
        "for j in range(train_df.shape[0]):\n",
        "    rec=train_df.iloc[j]\n",
        "    if rec['interest_level']=='low':\n",
        "        building_level[rec['new_building_id']][0]+=1\n",
        "    if rec['interest_level']=='medium':\n",
        "        building_level[rec['new_building_id']][1]+=1\n",
        "    if rec['interest_level']=='high':\n",
        "        building_level[rec['new_building_id']][2]+=1\n",
        "        \n",
        "for j in range(train_df.shape[0]):    \n",
        "        rec=train_df.iloc[j]\n",
        "        occurance = sum(building_level[rec['new_building_id']])\n",
        "        if occurance!=0:\n",
        "            a[j]= (building_level[rec['new_building_id']][0]*0.0 + building_level[rec['new_building_id']][1]*1.0 \\\n",
        "                   + building_level[rec['new_building_id']][2]*2.0) / occurance\n",
        "\n",
        "train_df['building_level']=a\n",
        "\n",
        "test_df.ix[test_df.building_id == '0', 'new_building_id'] = test_df['building_id'] + test_df['manager_id']\n",
        "test_df.ix[test_df.building_id != '0', 'new_building_id'] = test_df['building_id']\n",
        "\n",
        "b=[]\n",
        "for i in test_df['new_building_id'].values:\n",
        "    if i not in building_level.keys():\n",
        "        b.append(np.nan)\n",
        "    else:\n",
        "        occurance = sum(building_level[i])\n",
        "        b.append((building_level[i][0]*0.0 + building_level[i][1]*1.0 \\\n",
        "                   + building_level[i][2]*2.0) / occurance)\n",
        "\n",
        "test_df['building_level']=b\n",
        "\n",
        "train_df = train_df.drop(['new_building_id'], axis=1)\n",
        "test_df = test_df.drop(['new_building_id'], axis=1)\n",
        "\n",
        "features_to_use.append('building_level')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f1949e53-fbf9-90d2-86f7-180ae0b2f197"
      },
      "outputs": [],
      "source": [
        "# Manager Level\n",
        "index=list(range(train_df.shape[0]))\n",
        "random.shuffle(index)\n",
        "a=[np.nan]*len(train_df)\n",
        "b=[np.nan]*len(train_df)\n",
        "c=[np.nan]*len(train_df)\n",
        "\n",
        "for i in range(5):\n",
        "    building_level={}\n",
        "    for j in train_df['manager_id'].values:\n",
        "        building_level[j]=[0,0,0]\n",
        "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
        "    train_index=list(set(index).difference(test_index))\n",
        "    for j in train_index:\n",
        "        temp=train_df.iloc[j]\n",
        "        if temp['interest_level']=='low':\n",
        "            building_level[temp['manager_id']][0]+=1\n",
        "        if temp['interest_level']=='medium':\n",
        "            building_level[temp['manager_id']][1]+=1\n",
        "        if temp['interest_level']=='high':\n",
        "            building_level[temp['manager_id']][2]+=1\n",
        "    for j in test_index:\n",
        "        temp=train_df.iloc[j]\n",
        "        if sum(building_level[temp['manager_id']])!=0:\n",
        "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
        "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
        "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
        "train_df['manager_level_low']=a\n",
        "train_df['manager_level_medium']=b\n",
        "train_df['manager_level_high']=c\n",
        "\n",
        "#### Prepare test data\n",
        "a=[]\n",
        "b=[]\n",
        "c=[]\n",
        "building_level={}\n",
        "for j in train_df['manager_id'].values:\n",
        "    building_level[j]=[0,0,0]\n",
        "for j in range(train_df.shape[0]):\n",
        "    temp=train_df.iloc[j]\n",
        "    if temp['interest_level']=='low':\n",
        "        building_level[temp['manager_id']][0]+=1\n",
        "    if temp['interest_level']=='medium':\n",
        "        building_level[temp['manager_id']][1]+=1\n",
        "    if temp['interest_level']=='high':\n",
        "        building_level[temp['manager_id']][2]+=1\n",
        "\n",
        "for i in test_df['manager_id'].values:\n",
        "    if i not in building_level.keys():\n",
        "        a.append(np.nan)\n",
        "        b.append(np.nan)\n",
        "        c.append(np.nan)\n",
        "    else:\n",
        "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
        "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
        "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
        "test_df['manager_level_low']=a\n",
        "test_df['manager_level_medium']=b\n",
        "test_df['manager_level_high']=c\n",
        "\n",
        "features_to_use.append('manager_level_low') \n",
        "features_to_use.append('manager_level_medium') \n",
        "features_to_use.append('manager_level_high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c8c5ee4-be7c-c308-92a6-ce63a1a39735"
      },
      "outputs": [],
      "source": [
        "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
        "for f in categorical:\n",
        "        if train_df[f].dtype=='object':\n",
        "            #print(f)\n",
        "            lbl = preprocessing.LabelEncoder()\n",
        "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
        "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
        "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
        "            features_to_use.append(f)\n",
        "            features_for_xgb.append(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f5940ba-8b2d-a54f-a7fc-c63e0955740a"
      },
      "outputs": [],
      "source": [
        "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
        "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
        "\n",
        "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
        "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
        "te_sparse = tfidf.transform(test_df[\"features\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23ed7ac5-2d7c-3236-2ca0-f5129efadfb9"
      },
      "outputs": [],
      "source": [
        "#Train and test set for XGBoost\n",
        "train_X = sparse.hstack([train_df[features_for_xgb], tr_sparse]).tocsr()\n",
        "test_X = sparse.hstack([test_df[features_for_xgb], te_sparse]).tocsr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9bccb8dd-3aed-de29-fb77-22fddf19d3f2"
      },
      "outputs": [],
      "source": [
        "target_num_map = {'high':2, 'medium':1, 'low':0}\n",
        "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c7a6c164-6e92-e0ab-2d0b-6d1dcbc8b65c"
      },
      "outputs": [],
      "source": [
        "#Prepare Train and test sets for Trees\n",
        "fill_NaN = Imputer(missing_values=np.nan, strategy='mean', axis=1)\n",
        "train_imputed = pd.DataFrame(fill_NaN.fit_transform(train_df[features_to_use]))\n",
        "train_imputed.columns = train_df[features_to_use].columns\n",
        "train_imputed.index = train_df.index\n",
        "\n",
        "test_imputed = pd.DataFrame(fill_NaN.fit_transform(test_df[features_to_use]))\n",
        "test_imputed.columns = test_df[features_to_use].columns\n",
        "test_imputed.index = test_df.index\n",
        "\n",
        "\n",
        "train_Xtree = train_imputed\n",
        "test_Xtree = test_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1824537b-2f10-b315-d480-ca20d9ff891d"
      },
      "outputs": [],
      "source": [
        "#Train and test set for XGBoost\n",
        "print(train_X.shape, test_X.shape)\n",
        "\n",
        "#Train and test set for trees\n",
        "print(train_Xtree.shape, test_Xtree.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "45793f79-035d-b352-882b-33b0b86ab06d"
      },
      "source": [
        "## Begin Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "891419d1-3474-c433-20d8-150ccf2fbbde"
      },
      "outputs": [],
      "source": [
        "NFOLDS = 5\n",
        "SEED = 0\n",
        "y_train = train_y\n",
        "\n",
        "ntrain = train_Xtree.shape[0]\n",
        "ntest = test_Xtree.shape[0]\n",
        "print(\"{},{}\".format(ntrain, ntest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9140a62c-4699-b5bd-5d97-0bb1fb617fd5"
      },
      "outputs": [],
      "source": [
        "class SklearnWrapper(object):\n",
        "    def __init__(self, clf, seed=0, params=None):\n",
        "        params['random_state'] = seed\n",
        "        self.clf = clf(**params)\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        self.clf.fit(x_train, y_train)\n",
        "\n",
        "    def predict_prb(self, x):\n",
        "        return self.clf.predict(x)\n",
        "    \n",
        "    def predict_proba(self, x):\n",
        "        return self.clf.predict_proba(x)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2584c69-9c4f-c741-5c9c-548dfe57acc0"
      },
      "outputs": [],
      "source": [
        "def get_oof(clf):\n",
        "    oof_train = np.zeros((ntrain,3))\n",
        "    oof_test = np.zeros((ntest,3))\n",
        "    oof_test_skf = np.empty((NFOLDS, ntest, 3))\n",
        "\n",
        "    i = 0\n",
        "    for train_index, test_index in skf.split(x_train, y_train):\n",
        "        x_tr = x_train[train_index]\n",
        "        y_tr = y_train[train_index]\n",
        "        x_te = x_train[test_index]\n",
        "\n",
        "        rf1.train(x_tr, y_tr)\n",
        "\n",
        "        oof_train[test_index]= rf1.predict_proba(x_te)\n",
        "        oof_test_skf[i, :, :] = rf1.predict_proba(x_test)\n",
        "        i += 1\n",
        "\n",
        "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
        "    return oof_train, oof_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd439a36-2956-5a9d-41f4-a7211109d9b4"
      },
      "outputs": [],
      "source": [
        "train_test = pd.concat((train_Xtree, test_Xtree)).reset_index(drop=True)\n",
        "x_train = np.array(train_test.iloc[:ntrain,:])\n",
        "x_test = np.array(train_test.iloc[ntrain:,:])\n",
        "\n",
        "print(\"{},{},{}\".format(x_train.shape, y_train.shape, x_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "42e4b347-42a5-55d6-e287-d95fa4c6cfdf"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits=NFOLDS, random_state=SEED, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8ce4f703-9f5d-6e40-678f-fad8c71f4eff"
      },
      "outputs": [],
      "source": [
        "rf1_params = {\n",
        "    'n_jobs': 16,\n",
        "    'n_estimators': 10,\n",
        "    'criterion' : \"entropy\",\n",
        "    'max_features': 0.5,\n",
        "    'max_depth': 6,\n",
        "    'min_samples_leaf': 2,\n",
        "}\n",
        "\n",
        "rf2_params = {\n",
        "    'n_jobs': 16,\n",
        "    'criterion' : \"gini\",\n",
        "    'n_estimators': 1000,\n",
        "    'max_features': None,\n",
        "    'max_depth': 8,\n",
        "    'min_samples_leaf': 1,\n",
        "}\n",
        "\n",
        "et1_params = {\n",
        "    'n_jobs': 16,\n",
        "    'n_estimators': 10,\n",
        "    'max_features': \"auto\",\n",
        "    'criterion' : \"gini\",\n",
        "    'max_depth': 4,\n",
        "    'min_samples_leaf': 2,\n",
        "}\n",
        "\n",
        "et2_params = {\n",
        "    'n_jobs': 16,\n",
        "    'n_estimators': 1000,\n",
        "    'criterion' : \"entropy\",\n",
        "    'max_depth': 12,\n",
        "    'min_samples_leaf': 2,\n",
        "    'max_features': 0.8,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ef651ee2-380c-539a-d15d-17aee17429ac"
      },
      "outputs": [],
      "source": [
        "et1 = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et1_params)\n",
        "et2 = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et2_params)\n",
        "\n",
        "rf1 = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf1_params)\n",
        "rf2 = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf2_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d389fcbd-a632-7470-49b6-1eb0035d2576"
      },
      "outputs": [],
      "source": [
        "gbc1 = GradientBoostingClassifier(n_estimators = 10, max_depth = 4, subsample = 0.5,\n",
        "          learning_rate = 0.1, min_samples_leaf = 2, random_state = 0)\n",
        "\n",
        "gbc2 = GradientBoostingClassifier(n_estimators = 1000, max_depth = 8, \n",
        "          learning_rate = 0.5, min_samples_leaf = 1, random_state = 0)\n",
        "\n",
        "gbc1_oof_train, gbc1_oof_test = get_oof(gbc1)\n",
        "gbc2_oof_train, gbc2_oof_test = get_oof(gbc2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bb0aa4ea-9779-8bbd-e049-768c4656be17"
      },
      "outputs": [],
      "source": [
        "et1_oof_train, et1_oof_test = get_oof(et1)\n",
        "et2_oof_train, et2_oof_test = get_oof(et2)\n",
        "\n",
        "rf1_oof_train, rf1_oof_test = get_oof(rf1)\n",
        "rf2_oof_train, rf2_oof_test = get_oof(rf2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f1ea91ce-21cb-1319-2c13-23ede025537b"
      },
      "outputs": [],
      "source": [
        "print(\"ET1-CV: {}\".format(log_loss(y_train, et1_oof_train)))\n",
        "print(\"ET2-CV: {}\".format(log_loss(y_train, et2_oof_train)))\n",
        "\n",
        "print(\"RF1-CV: {}\".format(log_loss(y_train, rf1_oof_train)))\n",
        "print(\"RF2-CV: {}\".format(log_loss(y_train, rf2_oof_train)))\n",
        "\n",
        "print(\"GBC1-CV: {}\".format(log_loss(y_train, gbc1_oof_train)))\n",
        "print(\"GBC2-CV: {}\".format(log_loss(y_train, gbc2_oof_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a9a98e01-5b31-57df-cbc1-307e59a6d1aa"
      },
      "source": [
        "## Prepare XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "64040798-4c8a-a48d-bb08-9b25e677f24f"
      },
      "outputs": [],
      "source": [
        "class XgbWrapper(object):\n",
        "    def __init__(self, seed=0, params=None):\n",
        "        self.param = params\n",
        "        self.param['seed'] = seed\n",
        "        self.nrounds = params.pop('nrounds', 250)\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
        "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.gbdt.predict(xgb.DMatrix(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "281a74c2-2a76-8ba3-ab40-a6c5c434cede"
      },
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'seed': 0,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'silent': 1,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7,\n",
        "    'learning_rate': 0.075,\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': 3,\n",
        "    'max_depth': 7,\n",
        "    'num_parallel_tree': 1,\n",
        "    'min_child_weight': 1,\n",
        "    'eval_metric': \"mlogloss\",\n",
        "    'nrounds': 400\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "99d759af-45d5-60be-706b-a24ad4e001af"
      },
      "outputs": [],
      "source": [
        "train_test = vstack([train_X, test_X]).toarray()\n",
        "x_train = np.array(train_test[:ntrain,:])\n",
        "x_test = np.array(train_test[ntrain:,:])\n",
        "\n",
        "print(\"{},{},{}\".format(x_train.shape, y_train.shape, x_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6810c2e-8bc3-24ba-ac5e-2cc76819fed6"
      },
      "outputs": [],
      "source": [
        "xg = XgbWrapper(seed=SEED, params=xgb_params)\n",
        "xg_oof_train, xg_oof_test = get_oof(xg)\n",
        "print(\"XG-CV: {}\".format(log_loss(y_train, xg_oof_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b2b722ed-5204-72b2-0d87-b3cfb9b04736"
      },
      "outputs": [],
      "source": [
        "x_train = np.concatenate((xg_oof_train, et1_oof_train, rf2_oof_train, gbc2_oof_train), axis=1)\n",
        "x_test = np.concatenate((xg_oof_test, et1_oof_test, rf2_oof_test, gbc2_oof_test), axis=1)\n",
        "\n",
        "print(\"{},{}\".format(x_train.shape, x_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69279589-63c0-9341-1d91-fcd353cda6d0"
      },
      "outputs": [],
      "source": [
        "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
        "dtest = xgb.DMatrix(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e44c3133-1d3f-831c-6b57-3295502c14b9"
      },
      "source": [
        "## Level 2 stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03ef166b-1220-4bd3-ed6b-4cb564344c69"
      },
      "outputs": [],
      "source": [
        "xgb_params_2 = {\n",
        "    'seed': 0,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'eta': 0.1,\n",
        "    'silent': 1,\n",
        "    'subsample': 0.6,\n",
        "    'learning_rate': 0.01,\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': 3,\n",
        "    'max_depth': 7,\n",
        "    'num_parallel_tree': 1,\n",
        "    'min_child_weight': 1,\n",
        "    'eval_metric': 'mlogloss',   \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5188f1ef-cf4f-6141-bf93-7b8e467c0133"
      },
      "outputs": [],
      "source": [
        "res = xgb.cv(xgb_params, dtrain, num_boost_round=500, nfold=4, seed=SEED, stratified=False,\n",
        "             early_stopping_rounds=25, verbose_eval=10, show_stdv=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6570d3e5-2f09-58cd-0ffd-eb5ae3c0cff7"
      },
      "outputs": [],
      "source": [
        "best_nrounds = res.shape[0] - 1\n",
        "cv_mean = res.iloc[-1, 0]\n",
        "cv_std = res.iloc[-1, 1]\n",
        "\n",
        "print('Ensemble-CV: {0}+{1}'.format(cv_mean, cv_std))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b48dd185-ee69-17f7-4b92-1f4605cd313b"
      },
      "outputs": [],
      "source": [
        "gbdt = xgb.train(xgb_params_2, dtrain, best_nrounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4577ea9e-703f-3132-ba64-09bab4f0e78d"
      },
      "outputs": [],
      "source": [
        "preds = gbdt.predict(dtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "84503177-10dd-1daf-141f-ad3b8cafbddd"
      },
      "outputs": [],
      "source": [
        "out_df = pd.DataFrame(preds)\n",
        "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
        "out_df[\"listing_id\"] = test_df.listing_id.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "95d9fa5a-4232-b397-1a19-ce6b9ce4e5c0"
      },
      "outputs": [],
      "source": [
        "out_df.to_csv(\"./stacker_starter_1.csv\", index=False) "
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}