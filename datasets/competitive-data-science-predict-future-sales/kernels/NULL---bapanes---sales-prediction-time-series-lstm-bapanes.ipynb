{"cells":[{"metadata":{"_cell_guid":"795bbe4b-51b2-42ec-810a-4f4c18c84f53","_uuid":"e4eb15fdb1237ea12fda77b898eb315b00a205ce","trusted":true},"cell_type":"code","source":"# always start with checking out the files!\n!ls ../input/*","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport math\nimport time\n\n# Basic packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random as rd # generating random numbers\nimport datetime # manipulating date formats\n# Viz\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # for prettier plots\n\n\n# TIME SERIES\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nimport datetime as dt\n\nfrom numpy import newaxis\nfrom keras.layers import Dense, Activation, Dropout, LSTM\nfrom keras.models import Sequential, load_model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nimport matplotlib.pyplot as plt\n\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6541e1a6-a353-4709-a1fa-730e0f2a308d","_uuid":"debe15ae99f3596923efc37ce2f609920213be54","trusted":true},"cell_type":"code","source":"# Import all of data inmediately  \nsales=pd.read_csv(\"../input/sales_train.csv\")\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nitem_cat=pd.read_csv(\"../input/item_categories.csv\")\nitem=pd.read_csv(\"../input/items.csv\")\nsub=pd.read_csv(\"../input/sample_submission.csv\")\nshops=pd.read_csv(\"../input/shops.csv\")\ntest=pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dc6fc0f9-45a9-4146-b88d-d4bddcb224b2","_uuid":"8e1875bb64b6efc577e8b121217e2ded20ea9ce9","trusted":true},"cell_type":"code","source":"#formatting the date column correctly\nsales.date=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\n# check\nprint(sales.info())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c8e0a7f3-9a16-46e0-aae3-273fe0f21d0e","_uuid":"a051b790a453f6e28632435a6c30efae02538113","trusted":true},"cell_type":"code","source":"# number of items per cat \n#notice that the analisis tupla is simply x\n\nx=item.groupby(['item_category_id']).count()\n\nx=x.sort_values(by='item_id',ascending=False)\nx=x.iloc[0:10].reset_index()\nx\n# #plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.item_category_id, x.item_id, alpha=0.8)\nplt.title(\"Items per Category\")\nplt.ylabel('# of items', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"68d378e2-2302-4381-8423-ede818fce32e","_uuid":"8dadea026ac25a550cb6725894e1117c67e88757","collapsed":true},"cell_type":"markdown","source":"Of course, there is a lot more that we can explore in this dataset, but let's dive into the time-series part.\n\n# Single series:\n\nThe objective requires us to predict sales for the next month at a store-item combination.\n\nSales over time of each store-item is a time-series in itself. Before we dive into all the combinations, first let's understand how to forecast for a single series.\n\nI've chosen to predict for the total sales per month for the entire company.\n\nFirst let's compute the total sales per month and plot that data.\n"},{"metadata":{"trusted":true,"_uuid":"a5a365bf32c7818ebecdbf056b7833a0d5dd3fa3"},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0081eb2ca831427c34d2d0b18f12c3546b9c06a5"},"cell_type":"markdown","source":"# Definition of ts = date_block,  item_cnt_day.sum dataframe\nNow, we declare a new variable to define the data to analyze in terms of time series predictions\nbasically, the item_cnt_day added in differents directions, such as date_block_num or date_block_num x shop_id x item_id, etc."},{"metadata":{"_cell_guid":"a783e367-da29-47fd-97be-f3ff756f32fe","_uuid":"95eaf40635366294662b228680cb6e425940c7db","trusted":true},"cell_type":"code","source":"#declaration of time-series kind of variable ts with groupby, but the structure is simple\n\nts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n\n\nts.astype('float')\nplt.figure(figsize=(16,8))\nplt.title('Total Sales of the company')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b98fb1f6-f3a2-434f-94c6-af01f3ffdfd4","_uuid":"bee64faeaacd2f60ff85ac8d2b61eea4e80afda8","trusted":true},"cell_type":"code","source":"#with ts.rolling we are computing the mean and std of sales grouped in windows of size 12 in units of time (block times, indeed)\n\nplt.figure(figsize=(16,6))\nplt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');\nplt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"648e56360ea5a8bc8c212ca5a413fe1e0de94054"},"cell_type":"markdown","source":"# Starting the LSTM model \n\nWe consider the predictio  of each node independently by using as basis source of information to run the LSTM algorithm the monthly evolution of the count of sales for each node "},{"metadata":{"trusted":true,"_uuid":"1a36023ba8c03083539b0a20b464c26bc55efd75"},"cell_type":"code","source":"#checking the requirements of the test file\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c30c6e875b3d821cdd7d707f8ae5fee98a309189"},"cell_type":"code","source":"#Function that generate a dataframe with featured columns (total_cnt for instance)\n    \ndef sales_summary_dataframe(shop_id_history, item_id_history):\n   \n    sales_shop_item = sales[(sales.shop_id==shop_id_history) & (sales.item_id == item_id_history)]\n    monthly_sales_shop_item = sales_shop_item.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n    \n    dates = pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n    \n    nullhistory = pd.DataFrame({\"node_tot_cnt\":[0]*dates.shape[0]},columns=[\"node_tot_cnt\"])\n    \n    if (sales_shop_item.shape[0] > 0):\n        for con in range(monthly_sales_shop_item.shape[0]): \n            index_date_block = monthly_sales_shop_item.index[con]\n            count_date_block = monthly_sales_shop_item.iloc[con]\n   \n            # here we fill the histories, always leaving the zero position to the featiure \n            # to be predicted [0]!!!!\n            nullhistory.iloc[index_date_block,0] = count_date_block\n    \n    return nullhistory    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf1e377efe1b029e6ef7b9f5f37c252f159274f4"},"cell_type":"code","source":"def plot_results(predicted_data, true_data):\n    fig = plt.figure(facecolor='white')\n    ax = fig.add_subplot(111)\n    ax.plot(true_data, label='True Data')\n    plt.plot(predicted_data, label='Prediction')\n    plt.legend()\n    plt.show()\n\n\ndef plot_results_multiple(predicted_data, true_data, prediction_len):\n    fig = plt.figure(facecolor='white')\n    ax = fig.add_subplot(111)\n    ax.plot(true_data, label='True Data')\n    # Pad the list of predictions to shift it in the graph to it's correct start\n    for i, data in enumerate(predicted_data):\n        padding = [None for p in range(i * prediction_len)]\n        plt.plot(padding + data, label='Prediction')\n        plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"437171b15495cf4e39dec3937ac85c6cf20ebe65"},"cell_type":"code","source":"class Timer():\n\n    def __init__(self):\n        self.start_dt = None\n\n    def start(self):\n        self.start_dt = dt.datetime.now()\n\n    def stop(self):\n        end_dt = dt.datetime.now()\n        print('Time taken: %s' % (end_dt - self.start_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"331209478dd948caab9fac3d6e408670cb3e1ad1"},"cell_type":"code","source":"class Model():\n    \"\"\"A class for an building and inferencing an lstm model\"\"\"\n\n    def __init__(self):\n        self.model = Sequential()\n\n    def load_model(self, filepath):\n        print('[Model] Loading model from file %s' % filepath)\n        self.model = load_model(filepath)\n\n    def build_model(self, configs):\n        timer = Timer()\n        timer.start()\n\n        for layer in configs['model']['layers']:\n            neurons = layer['neurons'] if 'neurons' in layer else None\n            dropout_rate = layer['rate'] if 'rate' in layer else None\n            activation = layer['activation'] if 'activation' in layer else None\n            return_seq = layer['return_seq'] if 'return_seq' in layer else None\n            input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n            input_dim = layer['input_dim'] if 'input_dim' in layer else None\n\n            if layer['type'] == 'dense':\n                self.model.add(Dense(neurons, activation=activation))\n            if layer['type'] == 'lstm':\n                self.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n            if layer['type'] == 'dropout':\n                self.model.add(Dropout(dropout_rate))\n\n        self.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'])\n\n        print('[Model] Model Compiled')\n        timer.stop()\n\n    def train(self, x, y, epochs, batch_size, save_dir):\n        timer = Timer()\n        timer.start()\n        print('[Model] Training Started')\n        print('[Model] %s epochs, %s batch size' % (epochs, batch_size))\n\n        save_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n        callbacks = [\n            EarlyStopping(monitor='val_loss', patience=2),\n            ModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True)\n        ]\n        \n        self.model.fit(x, y, epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n        self.model.save(save_fname)\n\n        print('[Model] Training Completed. Model saved as %s' % save_fname)\n        timer.stop()\n\n    def train_generator(self, data_gen, epochs, batch_size, steps_per_epoch, save_dir):\n        timer = Timer()\n        timer.start()\n        print('[Model] Training Started')\n        print('[Model] %s epochs, %s batch size, %s batches per epoch' % (epochs, batch_size, steps_per_epoch))\n        \n        save_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n        callbacks = [\n            ModelCheckpoint(filepath=save_fname, monitor='loss', save_best_only=True)\n        ]\n        self.model.fit_generator(\n            data_gen,\n            steps_per_epoch=steps_per_epoch,\n            epochs=epochs,\n            callbacks=callbacks,\n            workers=1\n        )\n\n        print('[Model] Training Completed. Model saved as %s' % save_fname)\n        timer.stop()\n\n    def predict_point_by_point(self, data):\n        #Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time\n        print('[Model] Predicting Point-by-Point...')\n        predicted = self.model.predict(data)\n        predicted = np.reshape(predicted, (predicted.size,))\n        return predicted\n\n    def predict_sequences_multiple(self, data, window_size, prediction_len):\n        #Predict sequence of 50 steps before shifting prediction run forward by 50 steps\n        print('[Model] Predicting Sequences Multiple...')\n        prediction_seqs = []\n        for i in range(int(len(data)/prediction_len)):\n            curr_frame = data[i*prediction_len]\n            predicted = []\n            for j in range(prediction_len):\n                predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n                curr_frame = curr_frame[1:]\n                curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n            prediction_seqs.append(predicted)\n        return prediction_seqs\n\n    def predict_sequence_full(self, data, window_size):\n        #Shift the window by 1 new prediction each time, re-run predictions on new window\n        print('[Model] Predicting Sequences Full...')\n        curr_frame = data[0]\n        predicted = []\n        for i in range(len(data)):\n            predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n            curr_frame = curr_frame[1:]\n            curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n        return predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb8f0745c3915aa34be0d9018764e4a69c4bd0fa"},"cell_type":"code","source":"class DataLoader():\n    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n\n    def __init__(self, dataframe, split, cols):\n        i_split = int(len(dataframe) * split)\n        self.data_train = dataframe.get(cols).values[:i_split]\n        self.data_test  = dataframe.get(cols).values[i_split:]\n        self.len_train  = len(self.data_train)\n        self.len_test   = len(self.data_test)\n        self.len_train_windows = None\n\n    def get_test_data(self, seq_len, normalise):\n        '''\n        Create x, y test data windows\n        Warning: batch method, not generative, make sure you have enough memory to\n        load data, otherwise reduce size of the training split.\n        '''\n        data_windows = []\n        for i in range(self.len_test - seq_len):\n            data_windows.append(self.data_test[i:i+seq_len])\n\n        data_windows = np.array(data_windows).astype(float)\n        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n\n        x = data_windows[:, :-1]\n        y = data_windows[:, -1, [0]]\n        return x,y\n\n    def get_train_data(self, seq_len, normalise):\n        '''\n        Create x, y train data windows\n        Warning: batch method, not generative, make sure you have enough memory to\n        load data, otherwise use generate_training_window() method.\n        '''\n        data_x = []\n        data_y = []\n        for i in range(self.len_train - seq_len):\n            x, y = self._next_window(i, seq_len, normalise)\n            data_x.append(x)\n            data_y.append(y)\n        return np.array(data_x), np.array(data_y)\n\n    def generate_train_batch(self, seq_len, batch_size, normalise):\n        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n        i = 0\n        while i < (self.len_train - seq_len):\n            x_batch = []\n            y_batch = []\n            for b in range(batch_size):\n                if i >= (self.len_train - seq_len):\n                    # stop-condition for a smaller final batch if data doesn't divide evenly\n                    yield np.array(x_batch), np.array(y_batch)\n                    i = 0\n                x, y = self._next_window(i, seq_len, normalise)\n                x_batch.append(x)\n                y_batch.append(y)\n                i += 1\n            yield np.array(x_batch), np.array(y_batch)\n\n    def _next_window(self, i, seq_len, normalise):\n        '''Generates the next data window from the given index location i'''\n        window = self.data_train[i:i+seq_len]\n        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n        x = window[:-1]\n        y = window[-1, [0]]\n        return x, y\n\n    def normalise_windows(self, window_data, single_window=False):\n        '''Normalise window with a base value of zero'''\n        normalised_data = []\n        window_data = [window_data] if single_window else window_data\n        for window in window_data:\n            normalised_window = []\n            for col_i in range(window.shape[1]):\n                \n                \n                #normalised_col = [((float(p) / window[0,col_i]) - 1) for p in window[:, col_i]]\n                \n                #new normalization necessary since there are several times when window[0,col_i]=0\n                #but still we like the idea to normalize each window wrt to the initial value\n                \n                normalised_col = [(float(p) - window[0,col_i]) for p in window[:, col_i]]\n                \n                normalised_window.append(normalised_col)\n                \n            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n            normalised_data.append(normalised_window)\n        return np.array(normalised_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cf8f5c9457c8af69962e4a19d69ffc5f04b4e79"},"cell_type":"code","source":"#we are going to use all the input file as train train_test_split\": 1.0\n#since we just need to predict the next month sales\n\n#input_timesteps MUST BE EQUAL to sequence_length - 1\n#in order to avoid array size compatibility problems\n\nsequence_length = 6\ninput_timesteps = sequence_length - 1\n\nconfigs = {\"data\": {\"filename\": \"sales_train.csv\", \"columns\": [\"node_tot_cnt\"],\n                    \"sequence_length\": sequence_length, \"train_test_split\": 1.0, \"normalise\": True},\n           \"training\": {\"epochs\": 2, \"batch_size\": 8},\n           \"model\": {\"loss\": \"mse\", \"optimizer\": \"adam\", \"save_dir\": \"saved_models\",\n                     \"layers\": [{\"type\": \"lstm\", \"neurons\": 100, \"input_timesteps\": input_timesteps, \n                                 \"input_dim\": 1, \"return_seq\": True},\n                                {\"type\": \"dropout\", \"rate\": 0.2},\n                                {\"type\": \"lstm\", \"neurons\": 100, \"return_seq\": True},\n                                {\"type\": \"lstm\", \"neurons\": 100, \"return_seq\": False},\n                                {\"type\": \"dropout\", \"rate\": 0.2},\n                                {\"type\": \"dense\", \"neurons\": 1, \"activation\": \"linear\"}]}}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce7607c821b38ed5f615246440e45cd66ef7dea7"},"cell_type":"code","source":"start_time=time.time()\n\nif not os.path.exists(configs['model']['save_dir']): os.makedirs(configs['model']['save_dir'])\n\nmodel = Model()\nmodel.build_model(configs)\n \nlist_of_nodes =[]\nlist_of_predictions = []\n\n#for node in range(len(test)):\nfor node in range(40):    \n    \n    # out-of memory generative training\n    # here we should start the loop\n\n    test_shop_id_index = test.iloc[node,1]\n    test_item_id_index = test.iloc[node,2]\n    \n    sales_summary_df = sales_summary_dataframe(test_shop_id_index, test_item_id_index)\n    data = DataLoader(sales_summary_df,\n                  configs['data']['train_test_split'],\n                  configs['data']['columns'])\n\n    steps_per_epoch = math.ceil((data.len_train - configs['data']['sequence_length']) / configs['training']['batch_size'])\n    \n    model.train_generator(\n                data_gen=data.generate_train_batch(\n                seq_len=configs['data']['sequence_length'],\n                batch_size=configs['training']['batch_size'],\n                normalise=configs['data']['normalise']\n                ),\n                epochs=configs['training']['epochs'],\n                batch_size=configs['training']['batch_size'],\n                steps_per_epoch=steps_per_epoch,\n                save_dir=configs['model']['save_dir']\n        )\n\n     #making prediction based in the last block of months (for now we are using 5 months)\n\n    ini_element = sales_summary_df.values.shape[0]-configs['data']['sequence_length'] + 1\n    end_element = sales_summary_df.values.shape[0]\n    last_train_block = np.array([sales_summary_df.values[ini_element:end_element]])\n\n    #prediction for next month based on precious sequence_length months (last train block!!)\n    memory_based_prediction_for_next_month = model.predict_point_by_point(last_train_block)\n    \n    list_of_nodes.append(node)\n    list_of_predictions.append(memory_based_prediction_for_next_month[0])\n    \n    if (node % 10 == 0):\n        end_time=time.time()\n        print(\"forecasting for \",node,\"th node and took\",end_time-start_time,\"s\")\n        start_time=end_time\n\n#total submission file        \n        \ndfsubmission = pd.DataFrame({\"ID\":list_of_nodes,\"item_cnt_month\":list_of_predictions},\n                            columns=[\"ID\",\"item_cnt_month\"])\n\ndfsubmission.to_csv('submission_file.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b681c526e9fe86c11c85386b557f1b0814fd5848"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}