{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "nbconvert_exporter": "python", "mimetype": "text/x-python", "file_extension": ".py", "name": "python", "pygments_lexer": "ipython3"}}, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "b8dca1448ff85da52dd499b0a9cae4293d53e368", "_kg_hide-output": false, "_cell_guid": "c7255b6d-fead-43ea-8469-b4ceba1cf05b", "_kg_hide-input": false, "collapsed": true}, "source": ["######  ABOUT THIS KERNEL ####\n", "Hello Everyone!!!\n", "I understand it is very late for publishing a Kernel for this competition as most of the possible options have already been tried and worked upon.\n", "\n", "But still I will try to give a chance and do some of the brain-storming on the 'Text based' feature engineering as per my limited wisdom on the topic.\n", "\n", "My perspective of feature creation, will be - to find writing patterns which we see in day - to - day life.\n", "\n", "I am pretty sure that some of these are not tried yet and may help you in this - or any other similar tasks.\n", "\n", "I will also try to write this Kernel in a tutorial form so as to help anyone who might try to benefit from this.\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "62d4fe7a5affb94abcb274a44e2c60bb4876b0ed", "_cell_guid": "efa4e605-c1e7-45a5-9b53-4d6e2ab3e2b9", "collapsed": true}, "source": ["######  ABOUT THE COMPETION ####\n", "In this competition, text strings written by authors 1- Edgar Allan Poe (EAP),2- HP Lovecraft (HPL) & 3-Mary Wollstonecraft Shelley (MWS) are given as inputs with the respective author of the 'string'.\n", "\n", "The test data contains a group of string and we have to provide the \"probability for each of the three classes(EAP,HPL,MWS)\". In simple terms we are supposed to identify the author of the string."]}, {"cell_type": "code", "metadata": {"_uuid": "093149c7c1c8f3e18a0afe5288d26b398a3c24a9", "_cell_guid": "883c28fd-7afc-4226-abd4-bb2a3cfdd73a", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#As usual, in the first step we will import the libraries which we are gonna use.\n", "import pandas as pd #pandas will help us reading the csv data to dataframes(df) and then working on the df.\n", "import matplotlib.pyplot as plt #matplotlib will be used to visualize the data in the form of graphs.\n", "import numpy as np # linear algebra\n", "import string #for text pre-processing\n", "from nltk.corpus import stopwords #for removing stopwords\n", "import re #Regular expression operations\n", "import xgboost as xgb #For predicting the values\n", "from sklearn.model_selection import KFold #for cross validations(CV)\n", "from sklearn import metrics #for getting CV score\n", "from collections import Counter #counting of words in the texts\n", "import operator\n", "from nltk import ngrams\n", "import nltk\n", "from nltk import word_tokenize\n", "\n"]}, {"cell_type": "code", "metadata": {"_uuid": "370ae0a4f1b4def7117a2b648587252707fc6ef9", "_cell_guid": "bddcb0e9-75ff-4b40-a9f4-8b29086e79a7"}, "outputs": [], "execution_count": null, "source": ["#Now we will read the input data and store them in dataframes for further processing.\n", "training_df = pd.read_csv(\"../input/spooky-author-identification/train.csv\")\n", "testing_df = pd.read_csv(\"../input/spooky-author-identification/test.csv\")\n", "\n", "#Once the data is loaded, we need to check the data and if it is loaded correctly. Executing the below\n", "#command will display the top 5 rows of the training_df dataframe.\n", "training_df.head(5)"]}, {"cell_type": "code", "metadata": {"_uuid": "bb94139335df110283fe1ce6bf2d13d9c2be2eb1", "_cell_guid": "ee631d96-7dc4-46de-9acb-719f7a1b779a"}, "outputs": [], "execution_count": null, "source": ["#similarly we will check the testing dataframe.\n", "testing_df.head(5)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "70023d8e2deda344cec16ef09c0119182172edc8", "_cell_guid": "3e31e515-7dc1-465e-8055-aa0228ea62e0"}, "source": ["As expected the testing data does not have the author name. :-)\n", "As we can see in the training_df, the three authors(classes) have been mentioned against the strings(text).\n", "\n", "We need to check if the classes are balanced - i.e. the ratio of inputs provided for each author. \n", "\n", "In the below line we have created new dataframe, in which grouping is done on the basis of author."]}, {"cell_type": "code", "metadata": {"_uuid": "b801c59ef6b29cec90e3921a2ae33a59452b997a", "_cell_guid": "cc4f4d2e-d6be-444c-afbf-6b3099e80067"}, "outputs": [], "execution_count": null, "source": ["training_author_df = training_df.groupby('author',as_index=False).count()\n", "training_author_df.head()"]}, {"cell_type": "code", "metadata": {"_uuid": "1f969952257bf82fc4621e6bb5ef17aa4b68203c", "_cell_guid": "21f2a4a6-0e7e-4932-8f78-73e18447dab7"}, "outputs": [], "execution_count": null, "source": ["#Though it is clear that there is no much difference between the inputs for each classes(authors),\n", "#we can still have a look at bar-chart for better visualisation.\n", "\n", "objects = training_author_df['author']  #storing values of authors in objects\n", "y_pos = np.arange(len(objects)) #creating numpy array for the count of authors\n", "ids = training_author_df['id'] #assignig values of id for each author\n", " \n", "plt.bar(y_pos, ids, align='center', alpha=0.4,color = 'bgk') #basic configuration for bar chart\n", "plt.xticks(y_pos, objects) #assigning Labels to be displayed on X-axis\n", "plt.ylabel('Input Count') #Labels for Y-axis\n", "plt.title('Inputs per author') #Label for Chart\n", " \n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "d30e70e4ab5e43375de79824a2207c39028323ad", "_cell_guid": "faebd0bf-1099-431a-98b9-02f09fbd6135"}, "source": ["As it is clear for above chart and training_author_df, there is no much difference between inputs-we shall proceed with feature creation for the inputs.\n", "\n", "Before feature creation it is important to do some pre-processing for the data. As a part of pre-processing,we will be removing punctuations and stopwords from text using string library.\n", "\n", "Lets pick a string from data, for easy re-presentation.\n"]}, {"cell_type": "code", "metadata": {"_uuid": "9fda5ef80bd516954f44756186d412c0e63f2ba6", "_cell_guid": "ad0e709c-d488-4a0f-8697-7875cee62831"}, "outputs": [], "execution_count": null, "source": ["test_string = training_df.iloc[0]['text']\n", "\n", "test_string"]}, {"cell_type": "markdown", "metadata": {"_uuid": "0235d12e6fa1fe886436c8deca9792e869e2d74f", "_cell_guid": "16a1545f-a38a-4e06-a3c2-cf57bf38553e"}, "source": ["For effective processing of the text, it is better to remove punctuations and stopwords from the text.\n", "To remove punctuations we will write a function: remove_punctuations_from_string\n", "\n", "Lets see what all punctuations are present in string.punctuation\n"]}, {"cell_type": "code", "metadata": {"_uuid": "00cc88c00d7a929df5135f9429d4f47526af551a", "_cell_guid": "14ddb4bc-3b4c-46df-a6e1-2dea8d095003"}, "outputs": [], "execution_count": null, "source": ["string.punctuation"]}, {"cell_type": "code", "metadata": {"_uuid": "b3b5b854ef1f5895c9775a3a0ae045fa1c7a49c3", "_cell_guid": "467f95c4-2762-4ed1-b3c6-70933bcde9d6"}, "outputs": [], "execution_count": null, "source": ["#Function for removing punctuations from string\n", "def remove_punctuations_from_string(string1):\n", "    string1 = string1.lower() #changing to lower case\n", "    translation_table = dict.fromkeys(map(ord, string.punctuation), ' ') #creating dictionary of punc & None\n", "    string2 = string1.translate(translation_table) #translating string1\n", "    return string2\n", "#lets check the function on our test string.\n", "\n", "print('After processing')\n", "test_string = remove_punctuations_from_string(test_string)\n", "test_string\n"]}, {"cell_type": "code", "metadata": {"_uuid": "8f98ba8a0c9e891af3d6e844128e44b9c4898076", "_cell_guid": "66b80949-3b15-4b34-a678-f8f2837f25e0"}, "outputs": [], "execution_count": null, "source": ["#The punctuations have been removed from the string. Lets write a similar function for removing\n", "#stopwords.\n", "\n", "def remove_stopwords_from_string(string1):\n", "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*') #compiling all stopwords.\n", "    string2 = pattern.sub('', string1) #replacing the occurrences of stopwords in string1\n", "    return string2\n", "\n", "print('After processing')\n", "test_string = remove_stopwords_from_string(test_string)\n", "test_string\n"]}, {"cell_type": "code", "metadata": {"_uuid": "4d35c606d98e385bff267e7beffef937c00fe38d", "_cell_guid": "93b65b74-21ce-48f3-9b1c-f7d3bef9c8d2"}, "outputs": [], "execution_count": null, "source": ["#This seems to be fine. Now we can apply above functions on our dataframe. Lets check our dataframe\n", "#again.\n", "\n", "training_df.head(5)"]}, {"cell_type": "code", "metadata": {"_uuid": "72ab1bf9d1d9b328e07d4f8b4d82d49cd4ed7ac6", "_cell_guid": "f5d8237d-5fd5-4549-85a3-8e022b4c531a"}, "outputs": [], "execution_count": null, "source": ["#Lets take backup of un-processed text, we might need it for future functions.\n", "#We will perform all actions on testing_df aswell to avoid any errors in future.\n", "training_df[\"text_backup\"] = training_df[\"text\"] #Creating new column text_backup same as text.\n", "testing_df[\"text_backup\"] = testing_df[\"text\"] #Creating new column text_backup same as text.\n", "\n", "\n", "#Applying above made functions on text.\n", "training_df[\"text\"] = training_df[\"text\"].apply(lambda x:remove_punctuations_from_string(x))\n", "training_df[\"text\"] = training_df[\"text\"].apply(lambda x:remove_stopwords_from_string(x))\n", "testing_df[\"text\"] = testing_df[\"text\"].apply(lambda x:remove_punctuations_from_string(x))\n", "testing_df[\"text\"] = testing_df[\"text\"].apply(lambda x:remove_stopwords_from_string(x))\n", "\n", "training_df.head(5)"]}, {"cell_type": "code", "metadata": {"_uuid": "de8840fdcf07ec6c56a1fcbbd5cad63b47f2d4c3", "_cell_guid": "cedf0a4c-7870-4fce-b495-730ba4f921fc"}, "outputs": [], "execution_count": null, "source": ["#Now we have processed and pre-processed text in our dataframe. Lets start making features from\n", "#the above data.\n", "\n", "#Initially we will create the basic features: 1 - Count of words in a statement(Vocab size), \n", "#2 - Count of characters in a statement & 3 - Diversity_score.\n", "\n", "#In most of the cases above 3 features display the variations between writing styles of the authors.\n", "\n", "#Feature 1 - Length of the input OR count of the words in the statement(Vocab size).\n", "training_df['Feature_1']= training_df[\"text_backup\"].apply(lambda x: len(str(x).split()))\n", "testing_df['Feature_1']= testing_df[\"text_backup\"].apply(lambda x: len(str(x).split()))\n", "\n", "#Feature 2 - Count of characters in a statement\n", "training_df['Feature_2'] = training_df[\"text_backup\"].apply(lambda x: len(str(x)))\n", "testing_df['Feature_2'] = testing_df[\"text_backup\"].apply(lambda x: len(str(x)))\n", "\n", "#Feature 3-Diversity_score i.e. Average length of words used in statement\n", "training_df['Feature_3'] = training_df['Feature_2'] / training_df['Feature_1']\n", "testing_df['Feature_3'] = testing_df['Feature_2'] / testing_df['Feature_1']\n", "\n", "training_df.head(5)"]}, {"cell_type": "code", "metadata": {"_uuid": "e73a9d672b47549ebafd99270cdcab5d7cf5619c", "_cell_guid": "181a088c-4f37-47d3-9d6b-080c987c546c", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#The usage of stop words can be another writing pattern. So the fourth feature is count of stopwords.\n", "#Feature_4 = Count of stopwords in the sentence.\n", "stop_words = set(stopwords.words('english'))\n", "training_df['Feature_4'] = training_df[\"text_backup\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]) )\n", "testing_df['Feature_4'] = testing_df[\"text_backup\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]) )\n", "\n", "#Let us identify the highest used words(other than stopwords) in our input data for further feature generation.\n", "\n", "#getting all text in single list: Though there are several other quicker options to do this, but\n", "#this is the most accurate and convinient of them.\n", "all_text_without_sw = ''\n", "for i in training_df.itertuples():\n", "    all_text_without_sw = all_text_without_sw +  str(i.text)\n", "#getting counts of each words:\n", "counts = Counter(re.findall(r\"[\\w']+\", all_text_without_sw))\n", "#deleting ' from counts\n", "del counts[\"'\"]\n", "#getting top 50 used words:\n", "sorted_x = dict(sorted(counts.items(), key=operator.itemgetter(1),reverse=True)[:50])\n", "\n", "#Feature-5: The count of top used words.\n", "training_df['Feature_5'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in sorted_x]) )\n", "testing_df['Feature_5'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in sorted_x]) )\n"]}, {"cell_type": "code", "metadata": {"_uuid": "e6edb52a8ff1e1bda44c090b582fa853b40c2ead", "_cell_guid": "2272cd7f-abc0-4897-a608-79fafa088edf", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#Similarly lets identify the least used words:\n", "reverted_x = dict(sorted(counts.items(), key=operator.itemgetter(1))[:10000])\n", "#Feature-6: The count of least used words.\n", "training_df['Feature_6'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in reverted_x]) )\n", "testing_df['Feature_6'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in reverted_x]) )\n", "\n", "#Feature-7: Count of punctuations in the input.\n", "training_df['Feature_7'] = training_df['text_backup'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]) )\n", "testing_df['Feature_7'] = testing_df['text_backup'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]) )\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": ["#Let's plot these features on a chart. To view these feature we will write a function:\n", "def plot_bar_chart_from_dataframe(dataframe1,key_column,columns_to_be_plotted):\n", "    import pandas as pd\n", "    test_df1 = dataframe1.groupby(key_column).sum()\n", "    test_df2 = pd.DataFrame()\n", "    for column in columns_to_be_plotted:\n", "        test_df2[column] = round(test_df1[column]/ test_df1[column].sum()*100,2)\n", "    test_df2 = test_df2.T \n", "    ax = test_df2.plot(kind='bar', stacked=True, figsize =(10,5),legend = 'reverse',title = '% usage for each author')\n", "    for p in ax.patches:\n", "        a = p.get_x()+0.4\n", "        ax.annotate(str(p.get_height()), (a, p.get_y()), xytext=(5, 10), textcoords='offset points')\n", "\n", "key_column = 'author'\n", "columns_to_be_plotted = ['Feature_4','Feature_5','Feature_6','Feature_7']\n", "plot_bar_chart_from_dataframe(training_df,key_column,columns_to_be_plotted)"]}, {"cell_type": "code", "metadata": {"_uuid": "6f313023706d639d6b48a84f6aaf87750182704c", "_cell_guid": "4339e00d-540a-4ae9-ab1f-ffbd592a2a87", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#Feature-8: Count of UPPER case words.\n", "training_df['Feature_8'] = training_df['text_backup'].apply(lambda x: len([w for w in str(x).replace('I','i').replace('A','a').split() if w.isupper() == True]) )\n", "testing_df['Feature_8'] = testing_df['text_backup'].apply(lambda x: len([w for w in str(x).replace('I','i').replace('A','a').split() if w.isupper() == True]) )\n", "\n", "#Feature-9: Count of Title case words.\n", "training_df['Feature_9'] = training_df['text_backup'].apply(lambda x: len([w for w in str(x).replace('I','i').replace('A','a').split() if w.istitle() == True]) )\n", "testing_df['Feature_9'] = testing_df['text_backup'].apply(lambda x: len([w for w in str(x).replace('I','i').replace('A','a').split() if w.istitle() == True]) )\n", "\n", "#The above features are common features which can indicate a writing pattern. There might be a possibility\n", "#that a writer is using words which START WITH or END WITH particular characters. Lets try to identify them.\n", "\n", "starting_words = sorted(list(map(lambda word : word[:2],filter(lambda word : len(word) > 3,all_text_without_sw.split()))))\n", "sw_counts = Counter(starting_words)\n", "top_30_sw = dict(sorted(sw_counts.items(), key=operator.itemgetter(1),reverse=True)[:30])\n", "\n", "#Feature-10: Count of (Most words start with)\n", "training_df['Feature_10'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_sw and w not in stop_words]) )\n", "testing_df['Feature_10'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_sw and w not in stop_words]) )\n", "\n", "#Feature-11: Count of (Most words end with)\n", "ending_words = sorted(list(map(lambda word : word[-2:],filter(lambda word : len(word) > 3,all_text_without_sw.split()))))\n", "ew_counts = Counter(ending_words)\n", "top_30_ew = dict(sorted(sw_counts.items(), key=operator.itemgetter(1),reverse=True)[:30])\n", "training_df['Feature_11'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_ew and w not in stop_words]) )\n", "testing_df['Feature_11'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w[:2] in top_30_ew and w not in stop_words]) )\n", "\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "9f907a20d838fce0d1027dcd2d1d4cf74d566930", "_cell_guid": "066c6c5c-e7c4-4a7b-b6a5-400328be5aa5", "collapsed": true}, "source": ["It is a possibility that some author uses reference of particular locations in their text, it can be a city or country. To check this, we will create features for City and Country names used. For this we need City and Country corpus, but I did not find any readily available corpus.\n", "\n", "So I created a list in excel file and have used as input for these features."]}, {"cell_type": "code", "metadata": {"_uuid": "9e3eff67cf6003f5b3b5e1b8f24e8e7638184253", "_cell_guid": "5ee4d930-1509-48ba-a9b9-e5314eb742da"}, "outputs": [], "execution_count": null, "source": ["list_of_cities_excel_file ='../input/city-country/list_of_cities.xlsx'\n", "city_df = pd.read_excel(open(list_of_cities_excel_file,'rb'), sheet_name='Sheet1')\n", "\n", "city_df.head(5)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "060d21dd267657ab19997e8692201d8a7a8e74a9", "_cell_guid": "7fa93173-8bd9-43b1-8425-f87506c40d9e"}, "source": ["As the list of City & Country is available in dataframe, now we will create Features dependent on them."]}, {"cell_type": "code", "metadata": {"_uuid": "6a2664aeb10e9c679aa9b147d5296fd9122e9da6", "_cell_guid": "6661d061-4164-46c7-ad74-adda54ff99a0", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["city_list = city_df['City'].tolist()\n", "city_list = [x.lower() for x in city_list]\n", "\n", "#We have list of cities used in the input text. We will create our Feature 12 on the basis of this list.\n", "\n", "#Feature-12: Count of City names used.\n", "training_df['Feature_12'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in city_list]) )\n", "testing_df['Feature_12'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in city_list]) )\n", "\n"]}, {"cell_type": "code", "metadata": {"_uuid": "0b44aa7a59e474369de4a16fe7160b5dfab5d8e9", "_cell_guid": "c467fd0d-87d7-48a3-9d87-2e96036f3bda", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["country_list = city_df['Country'].tolist()\n", "country_list = [x.lower() for x in country_list]\n", "\n", "#Feature-13: Count of Country names used.\n", "training_df['Feature_13'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in country_list]) )\n", "testing_df['Feature_13'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in country_list]) )\n", "\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "9d5a8231c9318ff73d129c34b5f1696b9eec10be", "_cell_guid": "18c89b32-c722-4558-a69f-773655308ea2"}, "source": ["Another possible option to identify writing patterns can be Trigrams. So we will identify top 10 trigrams from the text and check for its usage and make our 14th Feature."]}, {"cell_type": "code", "metadata": {"_uuid": "066afec6bf3db6524b715bae34746f12d32c83e1", "_cell_guid": "3c63bbd7-5baa-49cb-bc32-b92dbe75445d", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#Function for getting Trigram from text:\n", "def ngram_list_from_string(string1,count_of_words_in_ngram):\n", "    string1 = string1.lower()\n", "    string1 = string1.replace('.','. ')\n", "    all_grams = ngrams(string1.split(), count_of_words_in_ngram)\n", "    grams_list = []\n", "    for grams in all_grams:\n", "        grams_list.append(grams)\n", "    return(grams_list)"]}, {"cell_type": "code", "metadata": {"_uuid": "e97fd544087b0280de68c62f2502c3f281e85e66", "_cell_guid": "cf79045a-e409-43b8-8307-d99693bc1341", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#Getting Trigram for text:\n", "ngram_list = ngram_list_from_string(all_text_without_sw,3)\n", "\n", "#Getting count for every ngram:\n", "ngram_counts = Counter(ngram_list)\n", "\n", "#Getting top 10 ngram as per highest count:\n", "sorted_ngram = dict(sorted(ngram_counts.items(), key=operator.itemgetter(1),reverse=True)[:10])\n", "\n", "#Feature-14: Top 10 trigram occurence:\n", "training_df['Feature_14'] = training_df['text_backup'].apply(lambda x: len([w for w in ngram_list_from_string(x,3)if w in sorted_ngram]) )\n", "testing_df['Feature_14'] = testing_df['text_backup'].apply(lambda x: len([w for w in ngram_list_from_string(x,3)if w in sorted_ngram]) )\n", "\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "615b2bf6be7edc8eb243f81529af70b12c28b6c7", "_cell_guid": "932d33be-ae38-4df2-b8e9-83ea5195cb07"}, "source": ["Now we will identify the Part Of Speech from the written text. We will create 5 features for Nouns, Pronouns, Verbs, Adverbs, Adjectives.\n"]}, {"cell_type": "code", "metadata": {"_uuid": "3ce286ee97cb4249c490d0f678f624f8305bfa36", "_cell_guid": "57e87c48-7dc9-4f35-8613-5a307d272366", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["tokenized_all_text = word_tokenize(all_text_without_sw) #tokenize the text\n", "list_of_tagged_words = nltk.pos_tag(tokenized_all_text) #adding POS Tags to tokenized words\n", "\n", "set_pos  = (set(list_of_tagged_words)) # set of POS tags & words\n", "\n", "nouns = ['NN','NNS','NNP','NNPS'] #POS tags of nouns\n", "list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  nouns, set_pos)))\n", "training_df['Feature_15'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n", "testing_df['Feature_15'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n", "\n", "pronouns = ['PRP','PRP$','WP','WP$'] # POS tags of pronouns\n", "list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  pronouns, set_pos)))\n", "training_df['Feature_16'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n", "testing_df['Feature_16'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n", "\n", "verbs = ['VB','VBD','VBG','VBN','VBP','VBZ'] #POS tags of verbs\n", "list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  verbs, set_pos)))\n", "training_df['Feature_17'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n", "testing_df['Feature_17'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n", "\n", "adverbs = ['RB','RBR','RBS','WRB'] #POS tags of adverbs\n", "list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adverbs, set_pos)))\n", "training_df['Feature_18'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n", "testing_df['Feature_18'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n", "\n", "adjectives = ['JJ','JJR','JJS'] #POS tags of adjectives\n", "list_of_words = set(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  adjectives, set_pos)))\n", "training_df['Feature_19'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n", "testing_df['Feature_19'] = testing_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]) )\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "7ecf4a1f875fcad14b7853c84124792c3a9f0368", "_cell_guid": "19ba5044-3ff9-450c-a417-bb5f8e49e18d"}, "source": ["Currently, I am publishing the above code with 19 Text based features, though have some more in mind but will do in recent future.\n", "\n", "Also this is my FIRST Kernel,request you to please comment with your advise, suggestions and honest feedback on the same.\n"]}], "nbformat_minor": 1, "nbformat": 4}