{"cells":[{"metadata":{"_uuid":"a4aa673abaa35d22c0796017ad91ad89191c454f"},"cell_type":"markdown","source":"# New York City Taxi Fare Prediction Playground Competition\n\n## Comparing models\n\nThis notebook is a framework for testing multiple models, selecting the best one and analysing the best model. You can add/remove your own models.\n\nIt will generate a Kaggle submission file for the best model. \n\nThis kernel can take hours to compute all models. By default I use 50k datapoints. Select your own models and number of datapoints for your research purpose."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e1ad4aca4cd1cb9a10b4f0baa3707d8034e7eeb4"},"cell_type":"code","source":"# load some default Python modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n% matplotlib inline\nplt.style.use('seaborn-whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba54d043edeea854d6f5366b712ea5e4b947326e"},"cell_type":"markdown","source":"## Import and preprocess data\n\nSee my previous notebook \"NYC Taxi Fare Data Exploration\" (  https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration) for an indepth analysis of the data and reasoning for selecting & preprocessing the data."},{"metadata":{"trusted":true,"_uuid":"b1673e9136f3d66d675cf0c6af3ced344572c00c","collapsed":true},"cell_type":"code","source":"# read data in pandas dataframe\ndf_train =  pd.read_csv('../input/train.csv', nrows = 50_000, parse_dates=[\"pickup_datetime\"])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"6f5f0f0424ed2bfbf99fc462558035e712ce7d60"},"cell_type":"code","source":"# define bounding box\nBB = (-75, -72.9, 40, 41.8)\n\n# this function will be used with the test set below\ndef select_within_boundingbox(df, BB):\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n            \n# This function is based on https://stackoverflow.com/questions/27928/\n# calculate-distance-between-two-latitude-longitude-points-haversine-formula \n# Returns distance in miles\ndef distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295 # Pi/180\n    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a)) # 2*R*asin...","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"02e51147aeed287c14798a3a48d926ef83762add"},"cell_type":"code","source":"# add distance in miles\ndf_train['distance_miles'] = distance(df_train.pickup_latitude, df_train.pickup_longitude, \\\n                                      df_train.dropoff_latitude, df_train.dropoff_longitude)\n# add distance to NYC center\nnyc = (-74.0063889, 40.7141667)\ndf_train['distance_to_center'] = distance(nyc[1], nyc[0], df_train.pickup_latitude, df_train.pickup_longitude)\n# add year\ndf_train['year'] = df_train.pickup_datetime.apply(lambda t: t.year)\n# add hour\ndf_train['hour'] = df_train.pickup_datetime.apply(lambda t: t.hour)\n# add weekday 0:monday, 6:sunday\ndf_train['weekday'] = df_train.pickup_datetime.apply(lambda t: t.weekday())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9849da9cdf017932fd9fe61306a980acf9042b70"},"cell_type":"code","source":"print('Old size: %d' % len(df_train))\n# remove non-zero fare\ndf_train = df_train[df_train.fare_amount>=0]\n# remove missing data\ndf_train = df_train.dropna(how = 'any', axis = 'rows')\n# remove datapoints outside boundingbox near NYC\ndf_train = df_train[select_within_boundingbox(df_train, BB)]\n# remove datapoints with zero distance traveled\ndf_train = df_train[df_train.distance_miles >= 0]\n# remove datapoints with zero passengers\ndf_train = df_train[df_train.passenger_count > 0]\nprint('New size: %d' % len(df_train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2de7f00fe1d8e49cb49fffb5633e9596b9f2fe2"},"cell_type":"markdown","source":"## Preparing dataset for model training"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"d645d7ab4238c0de5ce55eba4dbcec4e89468615"},"cell_type":"code","source":"features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n            'passenger_count', 'distance_miles', 'distance_to_center', 'year', 'weekday', 'hour']\nX = df_train[features].values\ny = df_train['fare_amount'].values","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"507d6e466b5c4c8bfcb1ded01c49f29ff25cbcee"},"cell_type":"code","source":"# create training and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"100066196772118d699dc746a761668d55ed7f2b"},"cell_type":"markdown","source":"## Train models\n\nFirst some functions are defined for analysing the models. Next, a python dictionary is created with models. Each model will be evaluated. The best model will be analysed further."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"36c0ba5420fbb4af08bb3f5ceea33593f1060ea0"},"cell_type":"code","source":"# define some handy analysis support function\nfrom sklearn.metrics import mean_squared_error, explained_variance_score\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split\n\ndef calculate_kfold_rmse(model, X, y, nfolds):\n    kf = KFold(n_splits=nfolds, shuffle=False, random_state=None)\n    return np.sqrt(-cross_val_score(model, X, y, cv=kf, scoring=\"neg_mean_squared_error\")).mean()\n\n\ndef plot_prediction_analysis(y, y_pred, figsize=(10,4), title=''):\n    fig, axs = plt.subplots(1, 2, figsize=figsize)\n    axs[0].scatter(y, y_pred)\n    mn = min(np.min(y), np.min(y_pred))\n    mx = max(np.max(y), np.max(y_pred))\n    axs[0].plot([mn, mx], [mn, mx], c='red')\n    axs[0].set_xlabel('$y$')\n    axs[0].set_ylabel('$\\hat{y}$')\n    rmse = np.sqrt(mean_squared_error(y, y_pred))\n    evs = explained_variance_score(y, y_pred)\n    axs[0].set_title('rmse = {:.2f}, evs = {:.2f}'.format(rmse, evs))\n    \n    axs[1].hist(y-y_pred, bins=50)\n    avg = np.mean(y-y_pred)\n    std = np.std(y-y_pred)\n    axs[1].set_xlabel('$y - \\hat{y}$')\n    axs[1].set_title('Histrogram prediction error, $\\mu$ = {:.2f}, $\\sigma$ = {:.2f}'.format(avg, std))\n    \n    if title!='':\n        fig.suptitle(title)\n        \n        \n# some handy function to see how sensitive the model is to the selection\n# of the training and test set\ndef plot_rmse_analysis(model, X, y, N=400, test_size=0.25, figsize=(10,4), title=''):\n    rmse_train, rmse_test = [], []\n    for i in range(N):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n\n        model.fit(X_train, y_train)\n        y_train_pred = model.predict(X_train)\n        y_test_pred = model.predict(X_test)\n\n        rmse_train.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n        rmse_test.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))\n\n    g = sns.jointplot(np.array(rmse_train), np.array(rmse_test), kind='scatter', stat_func=None, size=5)\n    g.set_axis_labels(\"RMSE training ($\\mu$={:.2f})\".format(np.mean(rmse_train)), \n                      \"RMSE test ($\\mu$={:.2f})\".format(np.mean(rmse_test)))\n    plt.subplots_adjust(top=0.9)\n    g.fig.suptitle('{} (N={}, test_size={:0.2f})'.format(title, N, test_size))\n    \ndef plot_learning_curve(model, X_train, X_test, y_train, y_test, nsteps=1, figsize=(6, 5), title=''):\n    train_error, test_error = [], []\n    number_of_samples = []\n    m_samples = X_train.shape[0]\n    for m in range(int(m_samples/nsteps), m_samples+1, int(m_samples/nsteps)):\n        number_of_samples.append(m)\n        model.fit(X_train[:m,:], y_train[:m])\n        y_train_pred = model.predict(X_train[:m,:])\n        train_error.append(np.sqrt(mean_squared_error(y_train[:m], y_train_pred)))\n        y_test_pred = model.predict(X_test)\n        test_error.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))\n    plt.figure(figsize=figsize)\n    plt.plot(number_of_samples, train_error, label='Training data')\n    plt.plot(number_of_samples, test_error, label='Test data')\n    plt.xlabel('Training set size')\n    plt.ylabel('RMSE')\n    plt.legend()\n    if title!='':\n        plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"d18ac3ffd3ea3392426174517bd0261d08fb0f62"},"cell_type":"code","source":"# prepare python dictionary with models to test\nmodels = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"be591b907bd2a89e16d0b1a150a9040db4c73b9e"},"cell_type":"code","source":"# Add linear regression model\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nmodels['linear_model'] = Pipeline((\n        (\"standard_scaler\", StandardScaler()),\n        (\"lin_reg\", LinearRegression()),\n    ))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"4bba20674c8c1f87d94159e58a49e9fdd764b769"},"cell_type":"code","source":"# Add linear model with polynomial features. Use Ridge for L2 regularization\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\n\nmodels['polynomial'] = Pipeline((\n        (\"standard_scaler\", StandardScaler()),    \n        (\"poly_features\", PolynomialFeatures(degree=2)),\n        (\"ridge\", Ridge()),\n    ))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3fcc51993c9c9c9ab74898b0ae73beb3c417d518"},"cell_type":"code","source":"# Add KNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodels['kneighbors'] = Pipeline((\n        (\"standard_scaler\", StandardScaler()),\n        (\"kneighborsregressor\", KNeighborsRegressor()),\n    ))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"352f0d17f3466fb764d936312ad35ed37cef1677"},"cell_type":"code","source":"# Add RandomForestRegressor with several different parameters\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodels['random_forest_regressor_n10'] = RandomForestRegressor(n_estimators=10, max_depth=10, min_samples_leaf=10)\nmodels['random_forest_regressor_n100'] = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=10)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"bbbf35bfc4a1675f612c50d477b80ba28ffa0c34"},"cell_type":"code","source":"# Add RandomForestRegressor with several different parameters\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nmodels['gradient_boosting_n10'] = GradientBoostingRegressor(max_depth=2, n_estimators=10, learning_rate=1.0)\nmodels['gradient_boosting_n100'] = GradientBoostingRegressor(max_depth=2, n_estimators=100, learning_rate=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a8d2c0b76562de499c673caf92a4110640f7b01e"},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmodels['xgboost10'] = XGBRegressor(n_estimators=10, max_depth=3)\nmodels['xgboost100'] = XGBRegressor(n_estimators=100, max_depth=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d8022817add7efc2a453fde1cb5f6a248fad8d5"},"cell_type":"markdown","source":"## Start evaluating all models and selecting the best one"},{"metadata":{"trusted":true,"_uuid":"2d61cdeea9d22bf8c76f206cd33a77fc87b940f9"},"cell_type":"code","source":"nfolds = 10\nscores = []\nprint(\"Starting evaluating all models: datapoints = {}, nfolds = {}\".format(X.shape[0], nfolds))\nfor name, model in models.items():\n    print('\\n... calculating {} ...'.format(name))\n    %time score = calculate_kfold_rmse(model, X, y, nfolds)\n    scores.append((name, score))\n    \nprint(\"\\n\")\nsorted_scores = sorted(scores, key=lambda x: x[1], reverse=False)\nprint(\"rmsr - model (nfolds={})\".format(nfolds))\nprint(\"============================================\")\nfor r in sorted_scores:\n    print(\"{:0.4f} - {}\".format(r[1], r[0]))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"d09e9714f6dc4bba5ed3f694b11768f360740c63"},"cell_type":"code","source":"name_best_model, best_model = sorted_scores[0][0], models[sorted_scores[0][0]]\n\nbest_model.fit(X_train, y_train)\n\ny_train_pred = best_model.predict(X_train)\nplot_prediction_analysis(y_train, y_train_pred, title='{} - Trainingset'.format(name_best_model))\n\ny_test_pred = best_model.predict(X_test)\nplot_prediction_analysis(y_test, y_test_pred, title='{} - Testset'.format(name_best_model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9220f968993d433d1b6dc76a1b187e6a7192f04e"},"cell_type":"code","source":"plot_rmse_analysis(best_model, X, y, N=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b6606b580d4e898ef16906d1b3f6ed6ae456422"},"cell_type":"code","source":"plot_learning_curve(best_model, X_train, X_test, y_train, y_test, nsteps=20, title=name_best_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd50052c1cf7740732de34a5abf8e02e10f3833b"},"cell_type":"markdown","source":"## Generate Kaggle submission"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f8f295099f286a90e1d05f77280a3934e7bcfa7e"},"cell_type":"code","source":"# read test data\ndf_test =  pd.read_csv('../input/test.csv', parse_dates=[\"pickup_datetime\"])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"a72696318fa602e723b4357e41e78769dbb88d5d"},"cell_type":"code","source":"# add distance in km\ndf_test['distance_miles'] = distance(df_test.pickup_latitude, df_test.pickup_longitude, \\\n                                     df_test.dropoff_latitude, df_test.dropoff_longitude)\n# add distance to NYC center\ndf_test['distance_to_center'] = distance(nyc[1], nyc[0], df_test.pickup_latitude, df_test.pickup_longitude)\n# add year\ndf_test['year'] = df_test.pickup_datetime.apply(lambda t: t.year)\n# add hour\ndf_test['hour'] = df_test.pickup_datetime.apply(lambda t: t.hour)\n# add weekday 0:monday, 6:sunday\ndf_test['weekday'] = df_test.pickup_datetime.apply(lambda t: t.weekday())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29a6dca662d44f00e4132951c57d4b5c52f244e9","collapsed":true},"cell_type":"code","source":"# define dataset\nXTEST = df_test[features].values\n\nfilename = 'submission_best_model_{}.csv'.format(name_best_model)\n\ny_pred_final = best_model.predict(XTEST)\n\nsubmission = pd.DataFrame(\n    {'key': df_test.key, 'fare_amount': y_pred_final},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv(filename, index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"234445a86d0b8584532fa975737acdcab11ac994","collapsed":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"01a3c544d203d2384a2c6067249242de6cfdce5a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}