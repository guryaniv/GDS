{"cells":[{"metadata":{"_uuid":"ae4a6a91f1ceb6c33cb3b3712cdd13d71bc1da32","_cell_guid":"257c25d0-0c9b-4ae6-81d4-8ea473347fea","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Handle table-like data and matrices\nimport numpy as np\nimport pandas as pd\n\n# Modelling Algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nimport xgboost as xgb\nfrom sklearn.grid_search import GridSearchCV\n# Modelling Helpers\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.cross_validation import train_test_split , StratifiedKFold\nfrom sklearn.feature_selection import RFECV\n\n# Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n# Configure visualisations\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nsns.set_style( 'white' )\npylab.rcParams[ 'figure.figsize' ] = 8 , 6\n\ndef plot_histograms( df , variables , n_rows , n_cols ):\n    fig = plt.figure( figsize = ( 16 , 12 ) )\n    for i, var_name in enumerate( variables ):\n        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n        df[ var_name ].hist( bins=10 , ax=ax )\n        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n        ax.set_xticklabels( [] , visible=False )\n        ax.set_yticklabels( [] , visible=False )\n    fig.tight_layout()  # Improves appearance a bit.\n    plt.show()\n\ndef plot_distribution( df , var , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n    facet.map( sns.kdeplot , var , shade= True )\n    facet.set( xlim=( 0 , df[ var ].max() ) )\n    facet.add_legend()\n\ndef plot_categories( df , cat , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , row = row , col = col )\n    facet.map( sns.barplot , cat , target )\n    facet.add_legend()\n\ndef plot_correlation_map( df ):\n    corr = titanic.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n\ndef describe_more( df ):\n    var = [] ; l = [] ; t = []\n    for x in df:\n        var.append( x )\n        l.append( len( pd.value_counts( df[ x ] ) ) )\n        t.append( df[ x ].dtypes )\n    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n    levels.sort_values( by = 'Levels' , inplace = True )\n    return levels\n\ndef plot_variable_importance( X , y ):\n    tree = DecisionTreeClassifier( random_state = 99 )\n    tree.fit( X , y )\n    plot_model_var_imp( tree , X , y )\n    \ndef plot_model_var_imp( model , X , y ):\n    imp = pd.DataFrame( \n        model.feature_importances_  , \n        columns = [ 'Importance' ] , \n        index = X.columns \n    )\n    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n    imp[ : 10 ].plot( kind = 'barh' )\n    print (model.score( X , y ))\n    # get titanic & test csv files as a DataFrame\ntrain = pd.read_csv(\"../input/train.csv\")\ntest    = pd.read_csv(\"../input/test.csv\")\n\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\n#print(full.PassengerId)\n#del train , test\n\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)\n\ntitanic.head()\n\ntitanic.describe()\n\nplot_correlation_map( titanic )\n\n# Plot distributions of Age of passangers who survived or did not survive\nplot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )\n\n# Plot survival rate by Embarked\nplot_categories( titanic , cat = 'Embarked' , target = 'Survived' )\n\nplot_categories( titanic , cat = 'Sex' , target = 'Survived' )\n\nplot_categories( titanic , cat = 'Pclass' , target = 'Survived' )\nplot_categories( titanic , cat = 'SibSp' , target = 'Survived' )\n\nplot_categories( titanic , cat = 'Parch' , target = 'Survived' )\n#PassengerId=\nsex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )\nsex_T=pd.Series( np.where( test.Sex == 'male' , 1 , 0 ) , name = 'Sex' )\n\nembarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\nembarked_T=pd.get_dummies( test.Embarked , prefix='Embarked' )\nembarked.head()\n\npclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\npclass_T = pd.get_dummies( test.Pclass , prefix='Pclass' )\npclass.head()\n\nimputed = pd.DataFrame()\nimputed_T = pd.DataFrame()\n# Fill missing values of Age with the average of Age (mean)\nimputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\nimputed_T[ 'Age' ] = test.Age.fillna( full.Age.mean() )\n# Fill missing values of Fare with the average of Fare (mean)\nimputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\nimputed_T[ 'Fare' ] = test.Fare.fillna( full.Fare.mean() )\nimputed.head()\ntitle = pd.DataFrame()\n# we extract the title from each name\ntitle[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n#title_t[ 'Title' ] = test[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n# a map of more aggregated titles\nTitle_Dictionary = {\n                    \"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Royalty\",\n                    \"Don\":        \"Royalty\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"the Countess\":\"Royalty\",\n                    \"Dona\":       \"Royalty\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Royalty\"\n\n                    }\n\n# we map each title\ntitle[ 'Title' ] = title.Title.map( Title_Dictionary )\n#title_T[ 'Title' ] = title_T.Title.map( Title_Dictionary )\n#title_T = pd.get_dummies( title_T.Title )\ntitle = pd.get_dummies( title.Title )\n#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n\ntitle.head()\n\ncabin = pd.DataFrame()\n\n# replacing missing cabins with U (for Uknown)\ncabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n#cabin_T[ 'Cabin' ] = test.Cabin.fillna( 'U' )\n# mapping each Cabin value with the cabin letter\ncabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n#cabin_T[ 'Cabin' ] = cabin_T[ 'Cabin' ].map( lambda c : c[0] )\n\n# dummy encoding ...\ncabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n#cabin_T = pd.get_dummies( cabin_T['Cabin'] , prefix = 'Cabin' )\n\ncabin.head()\n\ndef cleanTicket( ticket ):\n    ticket = ticket.replace( '.' , '' )\n    ticket = ticket.replace( '/' , '' )\n    ticket = ticket.split()\n    ticket = map( lambda t : t.strip() , ticket )\n    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n    if len( ticket ) > 0:\n        return ticket[0]\n    else: \n        return 'XXX'\n\nticket = pd.DataFrame()\nticket_T = pd.DataFrame()\n# Extracting dummy variables from tickets:\nticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\nticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\nticket_T[ 'Ticket' ] = test[ 'Ticket' ].map( cleanTicket )\nticket_T = pd.get_dummies( ticket_T[ 'Ticket' ] , prefix = 'Ticket' )\n\nticket.shape\nticket.head()\nfamily = pd.DataFrame()\n\n# introducing a new feature : the size of families (including the passenger)\nfamily[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n\n# introducing other features based on the family size\nfamily[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\nfamily[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\nfamily[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n\nfamily.head()\nfull_X = pd.concat( [full.PassengerId, imputed , embarked , cabin , sex,family,ticket,title ] , axis=1 )\nfull_X.head()\n\ntrain_valid_X = full_X[ 0:891 ]\ntrain_valid_y = titanic.Survived\ntest_X = full_X[ 891: ]\ntrain_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7)\n\nprint (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\nplot_variable_importance(train_X, train_y)\nxgb_model = xgb.XGBClassifier()\n\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [ 0.001,0.005 ,0.01, 0.1], #so called `eta` value\n              'max_depth': [4,5,6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [150,200,250], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n              'seed': [1337]}\n\n\nmodel = GridSearchCV(xgb_model, parameters, n_jobs=5, \n                   cv=StratifiedKFold(train_y, n_folds=5, shuffle=True), \n                   scoring='roc_auc',\n                   verbose=2, refit=True)\n#model = RandomForestClassifier(n_estimators=100)\n#model = SVC()\n#model = GradientBoostingClassifier()\n#model = GaussianNB()\n#model = LogisticRegression() \n#model = KNeighborsClassifier(n_neighbors = 3)\nmodel.fit( train_valid_X ,train_valid_y )\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))\n#print(test_X)\n#print(full_X[891:])\n\nmy_submission = pd.DataFrame({'PassengerId': test_X.PassengerId, 'Survived': model.predict(test_X).astype(int)})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}