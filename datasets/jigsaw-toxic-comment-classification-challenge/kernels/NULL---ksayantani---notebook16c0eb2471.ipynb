{"nbformat": 4, "metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.4", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat_minor": 1, "cells": [{"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import nltk\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "import re\n", "from nltk import word_tokenize\n", "from nltk.stem import WordNetLemmatizer \n", "from nltk.corpus import stopwords\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "\n", "import sklearn.ensemble\n", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score, log_loss\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_uuid": "f0bb6c539af67469c38bed52f203db8442ac5808", "_cell_guid": "9268b28a-2dce-4556-88e6-b8d5dff83578"}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["ENGLISH_STOP_WORDS = set(stopwords.words(\"english\"))\n", "targets = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']"], "metadata": {}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["space_join = \" \".join\n", "\n", "def custom_preprocessor(comment):\n", "    comment = re.sub(r'(\\w+!)', r'EMOTION', comment)\n", "    comment = re.sub(r'(\\n\\n*)', '', comment)\n", "    comment = re.sub(r'\\d+[.]\\d+[.]\\d+[.]\\d+', '', comment)\n", "    comment = re.sub(r'([.?!;])(\\s+)(\\w+)', r'\\1\\3', comment)\n", "    return comment\n", "\n", "def custom_tokenizer(comment):\n", "    if comment == \"\":\n", "        return []\n", "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n", "    tokens = token_pattern.findall(comment)\n", "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n", "    return list(set(tokens))\n", "\n", "def get_nonalphanumeric_tokens(comment):\n", "    if comment == \"\":\n", "        return []\n", "    return list(set(re.findall(r'[^\\w\\s\\.]', comment)))\n", "\n", "def get_datetime(comment):\n", "    searches = re.findall(r'[012][0-9][:][0-9][0-9]\\W?\\s+[0-9][0-9]\\s[\\w]+', comment)\n", "    if len(searches) > 0:\n", "        return 1\n", "    return 0\n", "\n", "def get_random_indx(df, type_of_question):\n", "    return np.random.choice(df.loc[df[type_of_question] == 1].index)\n", "\n", "def get_sum(row):\n", "    sum_target = 0.0\n", "    for t in targets:\n", "        sum_target += row[t]\n", "    return sum_target"], "metadata": {"_uuid": "610fbbafe70821627c6b8f4818667540f88736c4", "_cell_guid": "0f4d4ef9-b8d0-421f-b36c-a5f7e0a48212", "collapsed": true}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["train = pd.read_csv('../input/train.csv')\n", "train['unlabeled'] = train.apply(lambda x: get_sum(x), axis=1)"], "metadata": {}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["train['preprocessed_text'] = train['comment_text'].apply(lambda text: custom_preprocessor(text))\n", "train['tokens'] = train['preprocessed_text'].apply(lambda text: custom_tokenizer(text))\n", "\n", "train['n_vocab']  = train['preprocessed_text'].apply(lambda row: len(sorted(set(row))))\n", "train['n_tokens'] = train['tokens'].apply(lambda row: len(row))\n", "\n", "train['non_alphas'] = train['preprocessed_text'].apply(lambda text: get_nonalphanumeric_tokens(text))\n", "train['n_non_alphas'] = train['non_alphas'].map(len)"], "metadata": {"_uuid": "b55e0f18e0bbb15f273d07a7ea76fcc4ad3ec4a9", "_cell_guid": "2bd11961-9ca0-4a49-a113-68c3deca98c6", "collapsed": true}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["count_vect = CountVectorizer(min_df=1, max_df=3, max_features = 256)\n", "X_train_counts = count_vect.fit_transform(pd.Series(train['preprocessed_text'].tolist()))\n", "\n", "vocabulary = count_vect.get_feature_names()\n", "bow = pd.DataFrame(X_train_counts.toarray(), columns=vocabulary)\n", "bow['id'] = train['id']\n", "train_bow = train.merge(bow, how='inner', on='id')"], "metadata": {"_uuid": "b50bbcad6531bd2506f433cfab964d6bdc8effb2", "_cell_guid": "da6c9aa0-ee7b-4976-93e6-80a6b8cc2dee"}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["estimators = {}\n", "features = vocabulary + ['n_non_alphas']\n", "\n", "for target in targets:\n", "    X = train_bow[features]\n", "    y = train_bow[target]\n", "    \n", "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n", "    \n", "    clf = RandomForestClassifier(random_state=0)\n", "    clf = clf.fit(X_train, y_train)\n", "    estimators[target] = clf\n", "    print(\"{0:<20} {1}\".format(target, log_loss(y_test, clf.predict_proba(X_test))))"], "metadata": {}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["test = pd.read_csv('../input/test.csv')\n", "test.fillna(\"\", inplace=True)\n", "\n", "test['preprocessed_text'] = test['comment_text'].apply(lambda text: custom_preprocessor(text))\n", "test['tokens'] = test['preprocessed_text'].apply(lambda text: custom_tokenizer(text))\n", "\n", "test['n_vocab']  = test['preprocessed_text'].apply(lambda row: len(sorted(set(row))))\n", "test['n_tokens'] = test['tokens'].apply(lambda row: len(row))\n", "\n", "test['non_alphas'] = test['preprocessed_text'].apply(lambda text: get_nonalphanumeric_tokens(text))\n", "test['n_non_alphas'] = test['non_alphas'].map(len)"], "metadata": {"_uuid": "74f6a4ad42735fe7283a993b7e277a7cd89bcfa4", "_cell_guid": "74f17572-e208-448f-9020-d1b56543cd30", "collapsed": true}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["X_test_counts = count_vect.transform(pd.Series(test['preprocessed_text'].tolist()))\n", "\n", "X_test = pd.DataFrame(X_test_counts.toarray(), columns=vocabulary)\n", "X_test['n_non_alphas'] = test['n_non_alphas']"], "metadata": {"collapsed": true}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["submission = pd.read_csv('../input/sample_submission.csv')\n", "for key, clf in estimators.items():\n", "    pred = clf.predict_proba(X_test[features])[:, 1]\n", "    submission[key] = pd.Series(pred)"], "metadata": {"_uuid": "40909292c3308ab0bf25898a8747e1b7e1364403", "_cell_guid": "223a581a-cc72-4b2d-a374-09296ed0b41b"}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["pd.isnull(submission).sum()"], "metadata": {"_uuid": "ff1c905c354eefb147ba5ae6920020376aabd638", "_cell_guid": "3520f6e5-0a32-457e-9180-90cdf870b3fa"}, "execution_count": null, "outputs": [], "cell_type": "code"}, {"source": ["submission.to_csv('submission.csv', index=False)"], "metadata": {"_uuid": "61f2ca7359462831d0dc4b680f69d18f5fca6d93", "_cell_guid": "520884ea-d6ab-4e76-b3f7-4b8fba573248"}, "execution_count": null, "outputs": [], "cell_type": "code"}]}