{"nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1, "cells": [{"outputs": [], "execution_count": null, "cell_type": "code", "source": ["# based on: https://www.kaggle.com/the1owl/surprise-me/code\n", "\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn import *\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_cell_guid": "a4c6baee-2808-4dee-baa8-6f5979890f52", "_uuid": "a4bda34ebeeb78f4342979aaf05067e9890a4fcc"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data = {\n", "    'tra': pd.read_csv('../input/air_visit_data.csv'),\n", "    'as': pd.read_csv('../input/air_store_info.csv'),\n", "    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n", "    'ar': pd.read_csv('../input/air_reserve.csv'), #need to create reservation features\n", "    'hr': pd.read_csv('../input/hpg_reserve.csv'), #need to create reservation features\n", "    'id': pd.read_csv('../input/store_id_relation.csv'),\n", "    'tes': pd.read_csv('../input/sample_submission.csv'),\n", "    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n", "    }"], "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data['tra'].head()"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data['id'].head()"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n", "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n", "\n", "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n", "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n", "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n", "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n"], "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["unique_stores = data['tes']['air_store_id'].unique()\n", "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n"], "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n"], "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) "], "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["stores.head()"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": [" stores.air_genre_name.value_counts()"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["# stores[\"air_genre_name_0\"] = stores.air_genre_name.str.split(\"/\",expand=True)[0]"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["lbl = preprocessing.LabelEncoder()\n", "# stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n", "# stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])"], "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n", "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n", "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n", "\n", "train = pd.merge(data['tra'], stores, how='left', on=['air_store_id','dow']) \n", "test = pd.merge(data['tes'], stores, how='left', on=['air_store_id','dow']) \n"], "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["train.head()"], "metadata": {}}, {"cell_type": "markdown", "source": ["## remaining data:"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data['hs'].head()"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data['hr'].head()"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data['ar'].head()"], "metadata": {"scrolled": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data['id'].head()"], "metadata": {"scrolled": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["data['id'].nunique()"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["train.shape"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["# map IDs\n", "## https://stackoverflow.com/questions/36971661/python-pandas-map-using-2-columns-as-reference?noredirect=1&lq=1\n", "# train.set_index(\"air_store_id\").join(data[\"id\"]).tail()\n", "# train.set_index(\"air_store_id\").join(data[\"id\"].set_index([\"air_store_id\",\"hpg_store_id\"])).isnull().sum()\n", "\n", "train = train.set_index(\"air_store_id\").join(data[\"id\"].set_index(\"air_store_id\"))\n", "# train.set_index(\"air_store_id\").join(data[\"id\"].set_index(\"air_store_id\")).isnull().sum()\n", "# train.set_index(\"air_store_id\").join(data[\"id\"].set_index(\"air_store_id\")).nunique()\n", "test = test.set_index(\"air_store_id\").join(data[\"id\"].set_index(\"air_store_id\"))"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["train.head(3)"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["train.info()"], "metadata": {}}, {"cell_type": "markdown", "source": ["### add a aniave regression model (doesn\\'t even have OHE for the ID or recent average. mainly just Day of week)\n", "* would be best to do temporal split but that's trickeier wit hcross val predict. for something this simple, who cares"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["train.reset_index().select_dtypes(['number']).iloc[:,1:7]"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["lr = linear_model.LinearRegression(normalize=True, n_jobs=-1)\n", "# lr.fit(train[col], np.log1p(train['visitors'].values))\n", "# lr.fit(train.reset_index().select_dtypes(['number']).iloc[:,1:7].fillna(-1), np.log1p(train['visitors'].values))"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["train[\"pred_lr_naive\"] = model_selection.cross_val_predict(lr,train.reset_index().select_dtypes(['number']).iloc[:,1:7].fillna(-1),np.log1p(train['visitors'].values  ))"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["lr.fit(train.reset_index().select_dtypes(['number']).iloc[:,1:7].fillna(-1), np.log1p(train['visitors'].values))\n", "\n", "test[\"pred_lr_naive\"] = lr.predict(test.reset_index().select_dtypes(['number']).iloc[:,1:7].fillna(-1))"], "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["train.head()"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["test.head()"], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["train.to_csv(\"train_partmerged_v1.csv.gz\",compression=\"gzip\")\n", "test.to_csv(\"test_partmerged_v1.csv.gz\",compression=\"gzip\")"], "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": [], "metadata": {"collapsed": true}}]}