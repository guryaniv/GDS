{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "234e2bea-59fc-45b9-ae9f-51adc3b15c5e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from scipy.optimize import minimize\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "import xgboost as xgb\n",
        "from sklearn.cross_validation import train_test_split\n",
        "import multiprocessing\n",
        "import difflib\n",
        "\n",
        "train_all = pd.read_csv('../input/train.csv')\n",
        "test_all = pd.read_csv('../input/test.csv')\n",
        "\n",
        "train = pd.read_csv('../input/train.csv')[:55555]\n",
        "#test = pd.read_csv('../input/test.csv')[:10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f376c90-0b7d-46e1-7574-cbbc95f0b635"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "#cvect = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "\n",
        "tfidf_txt = pd.Series(train_all['question1'].tolist() + train_all['question2'].tolist() + test_all['question1'].tolist() + test_all['question2'].tolist()).astype(str)\n",
        "tfidf.fit_transform(tfidf_txt)\n",
        "#cvect.fit_transform(tfidf_txt)\n",
        "\n",
        "def diff_ratios(st1, st2):\n",
        "    seq = difflib.SequenceMatcher()\n",
        "    seq.set_seqs(str(st1).lower(), str(st2).lower())\n",
        "    return seq.ratio()\n",
        "\n",
        "def word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).lower().split():\n",
        "        if word not in stops:\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).lower().split():\n",
        "        if word not in stops:\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
        "    return R\n",
        "\n",
        "def begin_same(w1, w2):\n",
        "    return str(w1).split()[0].lower() == str(w2).lower().split()[0].lower()\n",
        "\n",
        "def get_features(df_features):\n",
        "    #print('nouns...')\n",
        "    #df_features['question1_nouns'] = df_features.question1.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(x).lower())) if t[:1] in ['N']])\n",
        "    #df_features['question2_nouns'] = df_features.question2.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(x).lower())) if t[:1] in ['N']])\n",
        "    #df_features['z_noun_match'] = df_features.apply(lambda r: sum([1 for w in r.question1_nouns if w in r.question2_nouns]), axis=1)  #takes long\n",
        "    print('lengths...')\n",
        "    df_features['z_len1'] = df_features.question1.map(lambda x: len(str(x)))\n",
        "    df_features['z_len2'] = df_features.question2.map(lambda x: len(str(x)))\n",
        "    df_features['z_word_len1'] = df_features.question1.map(lambda x: len(str(x).split()))\n",
        "    df_features['z_word_len2'] = df_features.question2.map(lambda x: len(str(x).split()))\n",
        "    df_features['z_match_ratio'] = df_features.apply(lambda r: diff_ratios(r.question1, r.question2), axis=1)  #takes long\n",
        "    print('word match...')\n",
        "    df_features['z_word_match'] = df_features.apply(word_match_share, axis=1, raw=True)\n",
        "    #print('tfidf...')\n",
        "    #df_features['z_tfidf_sum1'] = df_features.question1.map(lambda x: np.sum(tfidf.transform([str(x)]).data))\n",
        "    #df_features['z_tfidf_sum2'] = df_features.question2.map(lambda x: np.sum(tfidf.transform([str(x)]).data))\n",
        "    #df_features['z_tfidf_mean1'] = df_features.question1.map(lambda x: np.mean(tfidf.transform([str(x)]).data))\n",
        "    #df_features['z_tfidf_mean2'] = df_features.question2.map(lambda x: np.mean(tfidf.transform([str(x)]).data))\n",
        "    #df_features['z_tfidf_len1'] = df_features.question1.map(lambda x: len(tfidf.transform([str(x)]).data))\n",
        "    #df_features['z_tfidf_len2'] = df_features.question2.map(lambda x: len(tfidf.transform([str(x)]).data))\n",
        "    #df_features['z_tfidf_sum_same'] = df_features['z_tfidf_sum1'] == df_features['z_tfidf_sum2']\n",
        "    #df_features['z_tfidf_len_same'] = df_features['z_tfidf_len1'] == df_features['z_tfidf_len2']\n",
        "    print('words...')\n",
        "    df_features['z_begin_same'] = df_features.apply(lambda r: begin_same(r.question1, r.question2), axis=1)\n",
        "    return df_features.fillna(0.0)\n",
        "\n",
        "train = get_features(train)\n",
        "#train.to_csv('train.csv', index=False)\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7c8d2a0b-9745-0037-88e4-ed2d71f27389"
      },
      "outputs": [],
      "source": [
        "tfidf.transform(['increase speed internet connection using VPN']).data\n",
        "tfidf.transform(['Internet speed increased hacking DNS']).data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "38df5c3e-cd84-ca4a-d127-08f24f9d1839"
      },
      "outputs": [],
      "source": [
        "print(train['is_duplicate'][train['tfidf_sum1']==train['tfidf_sum2']].count())\n",
        "print(train['is_duplicate'][train['tfidf_sum1']==train['tfidf_sum2']].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "348b452c-76d9-abdf-5295-63ce1ecf3512"
      },
      "outputs": [],
      "source": [
        "col = [c for c in train.columns if c[:1]=='z']\n",
        "\n",
        "pos_train = train[train['is_duplicate'] == 1]\n",
        "neg_train = train[train['is_duplicate'] == 0]\n",
        "p = 0.165\n",
        "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
        "while scale > 1:\n",
        "    neg_train = pd.concat([neg_train, neg_train])\n",
        "    scale -=1\n",
        "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
        "train = pd.concat([pos_train, neg_train])\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train[col], train['is_duplicate'], test_size=0.2, random_state=0)\n",
        "\n",
        "params = {}\n",
        "params[\"objective\"] = \"binary:logistic\"\n",
        "params['eval_metric'] = 'logloss'\n",
        "params[\"eta\"] = 0.02\n",
        "params[\"subsample\"] = 0.7\n",
        "params[\"min_child_weight\"] = 1\n",
        "params[\"colsample_bytree\"] = 0.7\n",
        "params[\"max_depth\"] = 7\n",
        "params[\"silent\"] = 1\n",
        "params[\"seed\"] = 1639\n",
        "\n",
        "d_train = xgb.DMatrix(x_train, label=y_train)\n",
        "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
        "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bcccca3b-1e9e-0edb-98e3-defd888ee49d"
      },
      "outputs": [],
      "source": [
        "bst = xgb.train(params, d_train, 600, watchlist, early_stopping_rounds=50, verbose_eval=100) #change to higher #s\n",
        "print('train done')\n",
        "print(log_loss(train.is_duplicate, bst.predict(xgb.DMatrix(train[col]))))\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "799374e5-e52f-010d-7b6e-710b210c677d"
      },
      "outputs": [],
      "source": [
        "valid = pd.read_csv('../input/train.csv')[259876:271627]\n",
        "valid = get_features(valid)\n",
        "\n",
        "pos_valid = valid[valid['is_duplicate'] == 1]\n",
        "neg_valid = valid[valid['is_duplicate'] == 0]\n",
        "p = 0.165\n",
        "scale = ((len(pos_valid) / (len(pos_valid) + len(neg_valid))) / p) - 1\n",
        "while scale > 1:\n",
        "    neg_valid = pd.concat([neg_valid, neg_valid])\n",
        "    scale -=1\n",
        "neg_valid = pd.concat([neg_valid, neg_valid[:int(scale * len(neg_valid))]])\n",
        "valid = pd.concat([pos_valid, neg_valid])\n",
        "\n",
        "#sub = pd.DataFrame()\n",
        "#sub['test_id'] = test['test_id']\n",
        "print('begin predict')\n",
        "#sub['is_duplicate'] = bst.predict(xgb.DMatrix(test[col]))\n",
        "print(log_loss(valid.is_duplicate, bst.predict(xgb.DMatrix(valid[col]))))\n",
        "print('end predict')\n",
        "\n",
        "#sub.to_csv('team_02.csv', index=False)\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b7615d02-5742-b0d0-86a2-61fac88a2f7d"
      },
      "outputs": [],
      "source": [
        "sub = pd.DataFrame()\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "test = get_features(test)\n",
        "sub['test_id'] = test['test_id']\n",
        "sub['is_duplicate'] = bst.predict(xgb.DMatrix(test[col]))\n",
        "\n",
        "sub.to_csv('to_submit.csv', index=False)\n",
        "print('Done')"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}