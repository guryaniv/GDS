{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# import Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\n% matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"677f0fbd96de7303883e05d7eca761d24597b4c0"},"cell_type":"code","source":"# Utilities from kaggle kernels\n# Instead of data = pd.read_csv(\"../input/train_V2.csv\")\n# We use : data = read_fast(\"../input/train_V2.csv\")\nimport random\nimport time\n\ndef reduce_mem_usage_func(df):\n    \"\"\" Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n        iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef get_sampled_data(filename, sample_size):\n    n = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)\n    skip = sorted(random.sample(range(1,n+1),n-sample_size)) #the 0-indexed header will not be included in the skip list\n    df = pd.read_csv(filename, skiprows=skip)\n    return df\n\n\ndef read_fast(filename, sample=True, sample_size=3000000, reduce_mem_usage=True):\n    start_time = time.time()\n    df = get_sampled_data(filename, sample_size) if sample else pd.read_csv(filename)\n    new_df = reduce_mem_usage_func(df) if reduce_mem_usage else df\n    elapsed_time = int(time.time() - start_time)\n    print('Time to get data frame: {:02d}:{:02d}:{:02d}'.format(\n               elapsed_time // 3600,\n               (elapsed_time % 3600 // 60),\n               elapsed_time % 60))\n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d07839847f3cbcffe59217d0abc86d477be4ba2b"},"cell_type":"code","source":"train = read_fast(\"../input/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"881f444844c4713a75a2632e57c3438ec0d4ae55"},"cell_type":"code","source":"test = read_fast(\"../input/test.csv\", sample = False)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a2ab589ca3833b0deb5a8aea0feea65bc1b719e"},"cell_type":"code","source":"# check index of dataframe\ntrain.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f88519b8fba73bca5ea771dfbe02a3997b20ce6"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,15)) \nsns.heatmap(train.corr(), cmap ='RdBu')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a64aa20364bfb596388e15b16c7adff648ba0406"},"cell_type":"code","source":"train = train.dropna(thresh=0.70*len(train), axis=1)\n\ntest = test.dropna(thresh=0.70*len(test), axis=1)\ntrain = train.drop(['SMode'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"175c10b372056ebbdbed8de1ecbbd6251810c3fa"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = ('EngineVersion', 'AppVersion', 'AvSigVersion', \"OsPlatformSubRelease\",\"OsBuildLab\",\"SkuEdition\", \"Census_MDC2FormFactor\",\n               \"Census_PrimaryDiskTypeName\", \"Census_ChassisTypeName\",\"Census_PowerPlatformRoleName\", \"Census_OSBranch\",\"Census_OSEdition\",\n               \"Census_OSSkuName\",\"Census_OSInstallTypeName\",\"Census_GenuineStateName\",\"Census_ActivationChannel\")\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train[c].values)) \n    train[c] = lbl.transform(list(train[c].values))\n    lbl.fit(list(test[c].values)) \n    test[c] = lbl.transform(list(test[c].values))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6cd580ce53eac504fb857bf21fc7b6f1880ef68"},"cell_type":"code","source":"train = train.select_dtypes(include=[np.number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f9d74254576a7336fdcd927f5834b501dd6d5a9"},"cell_type":"code","source":"train = train.fillna(train.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"483e825e5ecc1f1d6b967501165ff4c15c9bfbb8"},"cell_type":"code","source":"y = train[\"HasDetections\"]\n\nX = train.drop(labels = [\"HasDetections\"],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4155dd5a54001a6c4b67cac2fb555bd06d319d6"},"cell_type":"code","source":"#from lightgbm import LGBMClassifier\n#from sklearn.model_selection import train_test_split\n#\n#def identify_zero_importance_features(X, y, iterations = 1):\n#    \"\"\"\n#    Identify zero importance features in a training dataset based on the \n#    feature importances from a gradient boosting model. \n#    \n#    Parameters\n#    --------\n#    train : dataframe\n#        Training features\n#        \n#    train_labels : np.array\n#        Labels for training data\n#        \n#    iterations : integer, default = 2\n#        Number of cross validation splits to use for determining feature importances\n#    \"\"\"\n#    \n#    # Initialize an empty array to hold feature importances\n#    feature_importances = np.zeros(X.shape[1])\n#\n#    # Create the model with several hyperparameters\n#    model = LGBMClassifier(objective = 'binary',num_leaves=60,\n#                        learning_rate=0.01,\n#                        n_estimators=700,\n#                     max_bin=55, boosting = 'gbdt',\n#                              bagging_fraction=0.8,\n#                              bagging_freq=1, \n#                              feature_fraction=0.8,\n#                              feature_fraction_seed=9, \n#                              bagging_seed=11,metric = 'auc',\n#                              min_data_in_leaf=60, \n#                              min_sum_hessian_in_leaf=2)\n#    \n#    # Fit the model multiple times to avoid overfitting\n#    for i in range(iterations):\n#\n#        # Split into training and validation set\n#        train_features, valid_features, train_y, valid_y = train_test_split(X, y, \n#                                                                            test_size = 0.25, \n#                                                                            random_state = i)\n#\n#        # Train using early stopping\n#        model.fit(train_features, train_y, early_stopping_rounds=100, \n#                  eval_set = [(valid_features, valid_y)])\n#\n#        # Record the feature importances\n#        feature_importances += model.feature_importances_ / iterations\n#    \n#    feature_importances = pd.DataFrame({'feature': list(X.columns), \n#                            'importance': feature_importances}).sort_values('importance', \n#                                                                            ascending = False)\n#    \n#    # Find the features with zero importance\n#   zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n#    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n#    \n#    return zero_features, feature_importances\n\n#zero_features, feature_importances = identify_zero_importance_features(X, y, iterations = 1)\n#print('zero_features:',zero_features)\n#print('feature_importances : ', feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98874a09342794bfa0edd5d2d461552732af4bd4"},"cell_type":"code","source":"#feature_importances.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99fd9784e312f7b89dae9f05da0d609a3b6f5708"},"cell_type":"code","source":"#pp =np.percentile(feature_importances['importance'], 20) \n#print(pp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da34449c13b2a441c40e59a4ae32679e67c06494"},"cell_type":"code","source":"#to_drop = feature_importances[feature_importances['importance'] <= pp]['feature']\n#X = X.drop(columns = to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2999857b754273777210e379cd75d1390e7fa8db"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)             ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"935fd815caa57ec990b942ab0ea075f96ea69f5c"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\n    # Cross validate model with Kfold stratified cross val\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\nkfold = StratifiedKFold(n_splits=2, shuffle=False, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19b5c5d52ab4ad8d5fe7a74b998187110c146ce4"},"cell_type":"code","source":"# RFC Parameters tunning \n#from sklearn.ensemble import RandomForestClassifier\n\n#RFC = RandomForestClassifier()\n\n\n\n## Search grid for optimal parameters\n#rf_param_grid = {\"max_depth\":  [n for n in range(9, 12)],  \n #             \"max_features\": [1, 3, 10],\n  #            \"min_samples_split\": [n for n in range(4, 9)],\n   #           \"min_samples_leaf\": [n for n in range(2, 5)],\n    #          \"bootstrap\": [False],\n     #         \"n_estimators\" :[n for n in range(10, 20, 10)],\n      #        \"criterion\": [\"gini\"]}\n\n\n#gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = -1)\n\n#gsRFC.fit(X_train,y_train)\n\n#RFC_best = gsRFC.best_estimator_\n\n# Best score\n#gsRFC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3c84ca5451e04dd4b3c32e1f33ec03277666bf2"},"cell_type":"code","source":"#lgbm \nimport lightgbm as lgb\nlbm = lgb.LGBMClassifier()\n\n\n## Search grid for optimal parameters\nlbm_param_grid = model_params = {\n        \n        \"objective\": [\"binary\"],\n        \"boosting_type\": [\"gbdt\"], \n        \"learning_rate\":[ 0.05],\n        \"max_depth\": [8],\n        \"num_leaves\": [120],\n        \"n_estimators\": [1000],\n        \"bagging_fraction\": [0.7],\n        \"feature_fraction\": [0.7],\n        \"bagging_freq\": [5],\n        \"bagging_seed\": [2018],\n        'min_child_samples':[ 80], \n        'min_child_weight': [100.0], \n        'min_split_gain': [0.1], \n        'reg_alpha': [0.005], \n        'reg_lambda': [0.1], \n        'subsample_for_bin': [25000], \n        'min_data_per_group': [100], \n        'max_cat_to_onehot': [4], \n        'cat_l2':[ 25.0], \n        'cat_smooth':[ 2.0], \n        'max_cat_threshold':[ 32], \n        \"random_state\": [1],\n        \"silent\": [True],\n        \"metric\": [\"auc\"],\n    }\n\n\ngsExtC = GridSearchCV(lbm,param_grid = lbm_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c30091e546f1cec9127d6eddb6ad9ce16f79800d"},"cell_type":"code","source":"# RFC Parameters tunning \n#from sklearn.ensemble import RandomForestClassifier\n#\n#RFC = RandomForestClassifier()\n#\n#\n#\n### Search grid for optimal parameters\n#rf_param_grid = {\"max_depth\": [None],\n#              \"max_features\": [1, 3, 10],\n#              \"min_samples_split\": [2, 3, 10],\n#              \"min_samples_leaf\": [1, 3, 10],\n#              \"bootstrap\": [False],\n#              \"n_estimators\" :[100,300],\n#              \"criterion\": [\"gini\"]}\n#\n#\n#gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n#\n#gsRFC.fit(X_train,y_train)\n#\n#RFC_best = gsRFC.best_estimator_\n#\n## Best score\n#gsRFC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ddef1fd57ca332c7611cc41f6ee55fa4c91199c"},"cell_type":"code","source":"# Adaboost\n#from sklearn.ensemble import AdaBoostClassifier\n#from sklearn.tree import DecisionTreeClassifier\n#\n#DTC = DecisionTreeClassifier()\n#\n#adaDTC = AdaBoostClassifier(DTC, random_state=7)\n#\n#ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n#              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n#              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n#              \"n_estimators\" :[30],\n#              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n#\n#gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n#\n#gsadaDTC.fit(X_train,y_train)\n#\n#ada_best = gsadaDTC.best_estimator_\n#\n#gsadaDTC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a813f72f9eb24994da6c9cd46e42dbe6325544df"},"cell_type":"code","source":"#Gradient boosting tunning\n#from sklearn.ensemble import GradientBoostingClassifier\n#\n#GBC = GradientBoostingClassifier()\n#gb_param_grid = {'loss' : [\"deviance\"],\n#              'n_estimators' : [n for n in range(10, 60, 10)],\n#              'learning_rate': [0.1, 0.05, 0.01],\n#              'max_depth':  [n for n in range(9, 14)],  \n#              'min_samples_leaf': [n for n in range(2, 5)],\n#              'max_features': [0.3, 0.1] \n#              }\n#\n#gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n#\n#gsGBC.fit(X_train,y_train)\n#\n#GBC_best = gsGBC.best_estimator_\n#\n## Best score\n#gsGBC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96cbb13af7be5bfd5bb645e9d9ee40f6c733c43a"},"cell_type":"code","source":"#from sklearn.ensemble import VotingClassifier\n#\n#votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n#('gbc',GBC_best)], voting='soft', n_jobs=4)\n#\n#votingC = votingC.fit(X_train, y_train)\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7243516ce3e99c2664d8fde5121d7dbf2e319a9e"},"cell_type":"code","source":"test_id = test['MachineIdentifier']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05642dbe4aaeb893020a588e671dd520c0ce5d04"},"cell_type":"code","source":"feats = test.drop(['MachineIdentifier'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04dad98cd6f3c163ac4222384d2a1495e60d1b17"},"cell_type":"code","source":"feats = feats.select_dtypes(include=[np.number])\n\nfeats = feats[X_train.columns]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63d0dd78a77683a7338ecb16089d991a97e778c0"},"cell_type":"code","source":"feats = feats.fillna(feats.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0585159eeb7e0da5f9a9336da523986c3693cfa3"},"cell_type":"code","source":"predictions = gsExtC.predict(feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1966dda1e96c4a25bc876d5da8cd347e65d8ece"},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['MachineIdentifier'] = test_id\nsubmission['HasDetections'] = predictions \nsubmission.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19b7af4883a6c69906e3549e93138e0b8084a290"},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8160426367c71d8ed97c21acb68534683398793"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}