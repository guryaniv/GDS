{"cells":[{"metadata":{"_uuid":"03163e16176ae511bf06e7263d16ad4eb0b405b4","collapsed":true},"cell_type":"markdown","source":"This kernal is an attempt to use Stcking method for combining different models at multiple levels. The intent is to demostrate \nthe stacking technique. The data preprocessing and feature engineering part is taken from \"Exploratory data analysis + LightGBM\"\nkernal by \"Gaxx\". \n\nI used LGB as base models and extra tree, Logistic and MLP models on 2nd and 3rd level stack. Feel free to add your own models. Comments and suggestions are welcome."},{"metadata":{"trusted":true,"_uuid":"cd0ca466de422419c2beb14504ba2afa55fd729d"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 300)\n%matplotlib inline\n\nsns.set(style='white', context='notebook', palette='deep')\n\nmycols = [\"#66c2ff\", \"#5cd6d6\", \"#00cc99\", \"#85e085\", \"#ffd966\", \"#ffb366\", \"#ffb3b3\", \"#dab3ff\", \"#c2c2d6\"]\nsns.set_palette(palette = mycols, n_colors = 4)\n\n\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_set = pd.read_csv('../input/train.csv')\ntest_set = pd.read_csv('../input/test.csv')\n\n#print(f'train set has {train_set.shape[0]} rows, and {train_set.shape[1]} features')\n#print(f'test set has {test_set.shape[0]} rows, and {test_set.shape[1]} features')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20b51144c6ff9d8da7a3f0eb4df12587cbbc553b","trusted":true},"cell_type":"code","source":"#Let's take a look at target\ntarget = train_set['Target']\ntarget.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cac672fe0a74db69bed50e3f00cbaf0f0c2d6cd"},"cell_type":"markdown","source":"## Outlier\n\nI had a look into train and test set, it turned out there is only one outlier to **rez_esc** in test set, and acorrding to the answer from competition host(https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403), we can safely change the value to 5"},{"metadata":{"_uuid":"e63c96ffa1a579326c997669f805f7437aa2b910","trusted":true},"cell_type":"code","source":"#outlier in test set which rez_esc is 99.0\ntest_set.loc[test_set['rez_esc'] == 99.0 , 'rez_esc'] = 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91474a271cb33326deae7dd9c9ab51986b0d3762"},"cell_type":"markdown","source":"## Missing Value"},{"metadata":{"_uuid":"cdf205203f38708c0c2895a5d3035b5b3d79557d","trusted":true},"cell_type":"code","source":"data_na = train_set.isnull().sum().values / train_set.shape[0] *100\ndf_na = pd.DataFrame(data_na, index=train_set.columns, columns=['Count'])\ndf_na = df_na.sort_values(by=['Count'], ascending=False)\n\nmissing_value_count = df_na[df_na['Count']>0].shape[0]\n\n#print(f'We got {missing_value_count} rows which have missing value in train set ')\ndf_na.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0faa5992610eb926fe8846828f4dde041e6ee0ae"},"cell_type":"markdown","source":"*  **rez_esc** represents \"years behind in school\", missing value could be filled as 0\n*  **meaneduc** represents \"average years of education for adults (18+)\", missing value could be filled as 0\n*  **v18q1** really depends on v18q\n*  **v2a1** depends on tipovivi3\n\nWe do not really need SQBxxxx features for polynomial in our case, and i will use fillna as 0 after at the last step of feature engineering"},{"metadata":{"_uuid":"cade5d48350aa95149c4d719b06bed2ac42cb9bd","trusted":true},"cell_type":"code","source":"data_na = train_set.isnull().sum().values / train_set.shape[0] *100\ndf_na = pd.DataFrame(data_na, index=train_set.columns, columns=['Count'])\ndf_na = df_na.sort_values(by=['Count'], ascending=False)\n\nmissing_value_count = df_na[df_na['Count']>0].shape[0]\n\n#print(f'We got {missing_value_count} rows which have missing value in test set ')\ndf_na.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"954c123321909d1c399f04c92d07dba193d6985a","trusted":true},"cell_type":"code","source":"#Fill na\ndef repalce_v18q1(x):\n    if x['v18q'] == 0:\n        return x['v18q']\n    else:\n        return x['v18q1']\n\ntrain_set['v18q1'] = train_set.apply(lambda x : repalce_v18q1(x),axis=1)\ntest_set['v18q1'] = test_set.apply(lambda x : repalce_v18q1(x),axis=1)\n\ntrain_set['v2a1'] = train_set['v2a1'].fillna(value=train_set['tipovivi3'])\ntest_set['v2a1'] = test_set['v2a1'].fillna(value=test_set['tipovivi3'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07e0b4f570715a4549790e7448f883303efcb0fd"},"cell_type":"markdown","source":"## Feature Engineering\n\nReplace object value, because some labels were generated whenever continuous variables have 1 or 0. The rule is to have being 1 yes=1 and no=0"},{"metadata":{"_uuid":"baf15867d29fb88d76ae7a6d42dbe9ce81fcdcef","trusted":true},"cell_type":"code","source":"cols = ['edjefe', 'edjefa']\ntrain_set[cols] = train_set[cols].replace({'no': 0, 'yes':1}).astype(float)\ntest_set[cols] = test_set[cols].replace({'no': 0, 'yes':1}).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89e6aa2f9f8c5d04f65a0fc7adc40ec5aa0b13e9"},"cell_type":"markdown","source":"It turns out orignial data lost one feature both for **roof** and **electricity**, so we manually add new feature"},{"metadata":{"_uuid":"96c687f372df2a40687ec0bfecb2c0bcfe252f9f","trusted":true},"cell_type":"code","source":"train_set['roof_waste_material'] = np.nan\ntest_set['roof_waste_material'] = np.nan\ntrain_set['electricity_other'] = np.nan\ntest_set['electricity_other'] = np.nan\n\ndef fill_roof_exception(x):\n    if (x['techozinc'] == 0) and (x['techoentrepiso'] == 0) and (x['techocane'] == 0) and (x['techootro'] == 0):\n        return 1\n    else:\n        return 0\n    \ndef fill_no_electricity(x):\n    if (x['public'] == 0) and (x['planpri'] == 0) and (x['noelec'] == 0) and (x['coopele'] == 0):\n        return 1\n    else:\n        return 0\n\ntrain_set['roof_waste_material'] = train_set.apply(lambda x : fill_roof_exception(x),axis=1)\ntest_set['roof_waste_material'] = test_set.apply(lambda x : fill_roof_exception(x),axis=1)\ntrain_set['electricity_other'] = train_set.apply(lambda x : fill_no_electricity(x),axis=1)\ntest_set['electricity_other'] = test_set.apply(lambda x : fill_no_electricity(x),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35c334bf4514fbe2c0ba6ce5c5fb64c922cff7df","collapsed":true},"cell_type":"markdown","source":"More feature engineering "},{"metadata":{"_uuid":"394ec0f30fcf45e7f35fbb871f6c05868ef12e17","trusted":true},"cell_type":"code","source":"train_set['adult'] = train_set['hogar_adul'] - train_set['hogar_mayor']\ntrain_set['dependency_count'] = train_set['hogar_nin'] + train_set['hogar_mayor']\ntrain_set['dependency'] = train_set['dependency_count'] / train_set['adult']\ntrain_set['child_percent'] = train_set['hogar_nin']/train_set['hogar_total']\ntrain_set['elder_percent'] = train_set['hogar_mayor']/train_set['hogar_total']\ntrain_set['adult_percent'] = train_set['hogar_adul']/train_set['hogar_total']\ntest_set['adult'] = test_set['hogar_adul'] - test_set['hogar_mayor']\ntest_set['dependency_count'] = test_set['hogar_nin'] + test_set['hogar_mayor']\ntest_set['dependency'] = test_set['dependency_count'] / test_set['adult']\ntest_set['child_percent'] = test_set['hogar_nin']/test_set['hogar_total']\ntest_set['elder_percent'] = test_set['hogar_mayor']/test_set['hogar_total']\ntest_set['adult_percent'] = test_set['hogar_adul']/test_set['hogar_total']\n\ntrain_set['rent_per_adult'] = train_set['v2a1']/train_set['hogar_adul']\ntrain_set['rent_per_person'] = train_set['v2a1']/train_set['hhsize']\ntest_set['rent_per_adult'] = test_set['v2a1']/test_set['hogar_adul']\ntest_set['rent_per_person'] = test_set['v2a1']/test_set['hhsize']\n\ntrain_set['overcrowding_room_and_bedroom'] = (train_set['hacdor'] + train_set['hacapo'])/2\ntest_set['overcrowding_room_and_bedroom'] = (test_set['hacdor'] + test_set['hacapo'])/2\n\ntrain_set['no_appliances'] = train_set['refrig'] + train_set['computer'] + train_set['television']\ntest_set['no_appliances'] = test_set['refrig'] + test_set['computer'] + test_set['television']\n\ntrain_set['r4h1_percent_in_male'] = train_set['r4h1'] / train_set['r4h3']\ntrain_set['r4m1_percent_in_female'] = train_set['r4m1'] / train_set['r4m3']\ntrain_set['r4h1_percent_in_total'] = train_set['r4h1'] / train_set['hhsize']\ntrain_set['r4m1_percent_in_total'] = train_set['r4m1'] / train_set['hhsize']\ntrain_set['r4t1_percent_in_total'] = train_set['r4t1'] / train_set['hhsize']\ntest_set['r4h1_percent_in_male'] = test_set['r4h1'] / test_set['r4h3']\ntest_set['r4m1_percent_in_female'] = test_set['r4m1'] / test_set['r4m3']\ntest_set['r4h1_percent_in_total'] = test_set['r4h1'] / test_set['hhsize']\ntest_set['r4m1_percent_in_total'] = test_set['r4m1'] / test_set['hhsize']\ntest_set['r4t1_percent_in_total'] = test_set['r4t1'] / test_set['hhsize']\n\ntrain_set['rent_per_room'] = train_set['v2a1']/train_set['rooms']\ntrain_set['bedroom_per_room'] = train_set['bedrooms']/train_set['rooms']\ntrain_set['elder_per_room'] = train_set['hogar_mayor']/train_set['rooms']\ntrain_set['adults_per_room'] = train_set['adult']/train_set['rooms']\ntrain_set['child_per_room'] = train_set['hogar_nin']/train_set['rooms']\ntrain_set['male_per_room'] = train_set['r4h3']/train_set['rooms']\ntrain_set['female_per_room'] = train_set['r4m3']/train_set['rooms']\ntrain_set['room_per_person_household'] = train_set['hhsize']/train_set['rooms']\n\ntest_set['rent_per_room'] = test_set['v2a1']/test_set['rooms']\ntest_set['bedroom_per_room'] = test_set['bedrooms']/test_set['rooms']\ntest_set['elder_per_room'] = test_set['hogar_mayor']/test_set['rooms']\ntest_set['adults_per_room'] = test_set['adult']/test_set['rooms']\ntest_set['child_per_room'] = test_set['hogar_nin']/test_set['rooms']\ntest_set['male_per_room'] = test_set['r4h3']/test_set['rooms']\ntest_set['female_per_room'] = test_set['r4m3']/test_set['rooms']\ntest_set['room_per_person_household'] = test_set['hhsize']/test_set['rooms']\n\ntrain_set['rent_per_bedroom'] = train_set['v2a1']/train_set['bedrooms']\ntrain_set['edler_per_bedroom'] = train_set['hogar_mayor']/train_set['bedrooms']\ntrain_set['adults_per_bedroom'] = train_set['adult']/train_set['bedrooms']\ntrain_set['child_per_bedroom'] = train_set['hogar_nin']/train_set['bedrooms']\ntrain_set['male_per_bedroom'] = train_set['r4h3']/train_set['bedrooms']\ntrain_set['female_per_bedroom'] = train_set['r4m3']/train_set['bedrooms']\ntrain_set['bedrooms_per_person_household'] = train_set['hhsize']/train_set['bedrooms']\n\ntest_set['rent_per_bedroom'] = test_set['v2a1']/test_set['bedrooms']\ntest_set['edler_per_bedroom'] = test_set['hogar_mayor']/test_set['bedrooms']\ntest_set['adults_per_bedroom'] = test_set['adult']/test_set['bedrooms']\ntest_set['child_per_bedroom'] = test_set['hogar_nin']/test_set['bedrooms']\ntest_set['male_per_bedroom'] = test_set['r4h3']/test_set['bedrooms']\ntest_set['female_per_bedroom'] = test_set['r4m3']/test_set['bedrooms']\ntest_set['bedrooms_per_person_household'] = test_set['hhsize']/test_set['bedrooms']\n\ntrain_set['tablet_per_person_household'] = train_set['v18q1']/train_set['hhsize']\ntrain_set['phone_per_person_household'] = train_set['qmobilephone']/train_set['hhsize']\ntest_set['tablet_per_person_household'] = test_set['v18q1']/test_set['hhsize']\ntest_set['phone_per_person_household'] = test_set['qmobilephone']/test_set['hhsize']\n\ntrain_set['age_12_19'] = train_set['hogar_nin'] - train_set['r4t1']\ntest_set['age_12_19'] = test_set['hogar_nin'] - test_set['r4t1']    \n\ntrain_set['escolari_age'] = train_set['escolari']/train_set['age']\ntest_set['escolari_age'] = test_set['escolari']/test_set['age']\n\ntrain_set['rez_esc_escolari'] = train_set['rez_esc']/train_set['escolari']\ntrain_set['rez_esc_r4t1'] = train_set['rez_esc']/train_set['r4t1']\ntrain_set['rez_esc_r4t2'] = train_set['rez_esc']/train_set['r4t2']\ntrain_set['rez_esc_r4t3'] = train_set['rez_esc']/train_set['r4t3']\ntrain_set['rez_esc_age'] = train_set['rez_esc']/train_set['age']\ntest_set['rez_esc_escolari'] = test_set['rez_esc']/test_set['escolari']\ntest_set['rez_esc_r4t1'] = test_set['rez_esc']/test_set['r4t1']\ntest_set['rez_esc_r4t2'] = test_set['rez_esc']/test_set['r4t2']\ntest_set['rez_esc_r4t3'] = test_set['rez_esc']/test_set['r4t3']\ntest_set['rez_esc_age'] = test_set['rez_esc']/test_set['age']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b32875cc4bb262c0abf8dbf3572a53b5de54a116","trusted":true},"cell_type":"code","source":"train_set['dependency'] = train_set['dependency'].replace({np.inf: 0})\ntest_set['dependency'] = test_set['dependency'].replace({np.inf: 0})\n\n#print(f'train set has {train_set.shape[0]} rows, and {train_set.shape[1]} features')\n#print(f'test set has {test_set.shape[0]} rows, and {test_set.shape[1]} features')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0de981d7bb9dab13b1986f51dd785126424de1c1","scrolled":true,"trusted":true},"cell_type":"code","source":"df_train = pd.DataFrame()\ndf_test = pd.DataFrame()\n\naggr_mean_list = ['rez_esc', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', 'parentesco2',\n             'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11', 'parentesco12',\n             'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',]\n\nother_list = ['escolari', 'age', 'escolari_age']\n\nfor item in aggr_mean_list:\n    group_train_mean = train_set[item].groupby(train_set['idhogar']).mean()\n    group_test_mean = test_set[item].groupby(test_set['idhogar']).mean()\n    new_col = item + '_aggr_mean'\n    df_train[new_col] = group_train_mean\n    df_test[new_col] = group_test_mean\n\nfor item in other_list:\n    for function in ['mean','std','min','max','sum']:\n        group_train = train_set[item].groupby(train_set['idhogar']).agg(function)\n        group_test = test_set[item].groupby(test_set['idhogar']).agg(function)\n        new_col = item + '_' + function\n        df_train[new_col] = group_train\n        df_test[new_col] = group_test\n\n#print(f'new aggregate train set has {df_train.shape[0]} rows, and {df_train.shape[1]} features')\n#print(f'new aggregate test set has {df_test.shape[0]} rows, and {df_test.shape[1]} features')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89a3bcb5a500b9a453df40f0d436cc1fb9b32513","trusted":true},"cell_type":"code","source":"df_test = df_test.reset_index()\ndf_train = df_train.reset_index()\n\ntrain_agg = pd.merge(train_set, df_train, on='idhogar')\ntest = pd.merge(test_set, df_test, on='idhogar')\n\n#fill all na as 0\ntrain_agg.fillna(value=0, inplace=True)\ntest.fillna(value=0, inplace=True)\n#print(f'new train set has {train_agg.shape[0]} rows, and {train_agg.shape[1]} features')\n#print(f'new test set has {test.shape[0]} rows, and {test.shape[1]} features')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f59d61995955c7cf2a82f277c2c9cbee82511d6","trusted":true},"cell_type":"code","source":"#According to data descriptions,ONLY the heads of household are used in scoring. /\n#All household members are included in test + the sample submission, but only heads of households are scored.\ntrain = train_agg.query('parentesco1==1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84543b31c9523d0f262155d72cc727a54291a45a","scrolled":true,"trusted":true},"cell_type":"code","source":"submission = test[['Id']]\n\n#Remove useless feature to reduce dimension\ntrain.drop(columns=['idhogar','Id', 'tamhog', 'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\ntest.drop(columns=['idhogar','Id', 'tamhog', 'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\n\ncorrelation = train.corr()\ncorrelation = correlation['Target'].sort_values(ascending=False)\n#print(f'The most 20 positive feature: \\n{correlation.head(20)}')\n#print('*'*50)\n\n#print(f'The most 20 negative feature: \\n{correlation.tail(20)}')\ny = train['Target']\n\ntrain.drop(columns=['Target'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d10faf13019c099dbe615a025a43e039f7430c60"},"cell_type":"code","source":"class Ensemble(object):    \n    def __init__(self, mode, n_splits, stacker_2, stacker_1, base_models):\n        self.mode = mode\n        self.n_splits = n_splits\n        self.stacker_2 = stacker_2\n        self.stacker_1 = stacker_1\n        self.base_models = base_models\n\n    def fit_predict(self, X, y, T):\n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n\n\n        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \n                                                             random_state=2016).split(X, y))\n        \n        OOF_columns = []\n\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n        \n        for i, clf in enumerate(self.base_models):\n\n            S_test_i = np.zeros((T.shape[0], self.n_splits))\n\n            for j, (train_idx, test_idx) in enumerate(folds):                \n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                X_holdout = X[test_idx]\n\n                print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n                clf.fit(X_train, y_train)\n\n                S_train[test_idx, i] = clf.predict(X_holdout)  \n                S_test_i[:, j] = clf.predict(T)\n            S_test[:, i] = S_test_i.mean(axis=1)\n            \n            #print(\"  Base model_%d score: %.5f\\n\" % (i+1, roc_auc_score(y, S_train[:,i])))\n        \n            OOF_columns.append('Base model_'+str(i+1))\n        OOF_S_train = pd.DataFrame(S_train, columns = OOF_columns)\n        print('\\n')\n        print('Correlation between out-of-fold predictions from Base models:')\n        print('\\n')\n        print(OOF_S_train.corr())\n        print('\\n')\n            \n        \n        if self.mode==1:\n            \n            folds_2 = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True,\n                                                                   random_state=2016).split(S_train, y))\n            \n            OOF_columns = []\n\n            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n            \n            for i, clf in enumerate(self.stacker_1):\n            \n                S_test_i_2 = np.zeros((S_test.shape[0], self.n_splits))\n\n                for j, (train_idx, test_idx) in enumerate(folds_2):\n                    X_train_2 = S_train[train_idx]\n                    y_train_2 = y[train_idx]\n                    X_holdout_2 = S_train[test_idx]\n\n                    print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n                    clf.fit(X_train_2, y_train_2)\n                                 \n                    S_train_2[test_idx, i] = clf.predict(X_holdout_2)\n                    S_test_i_2[:, j] = clf.predict(S_test)\n                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n                \n                #print(\"  1st level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, S_train_2.mean(axis=1))))\n                \n                OOF_columns.append('1st level model_'+str(i+1))\n            OOF_S_train = pd.DataFrame(S_train_2, columns = OOF_columns)\n            print('\\n')\n            print('Correlation between out-of-fold predictions from 1st level models:')\n            print('\\n')\n            print(OOF_S_train.corr())\n            print('\\n')\n\n\n        if self.mode==2:\n            \n            WOC_columns = []\n        \n            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n               \n            for i, clf in enumerate(self.stacker_1):\n            \n                S_train_i_2= np.zeros((S_train.shape[0], S_train.shape[1]))\n                S_test_i_2 = np.zeros((S_test.shape[0], S_train.shape[1]))\n                                       \n                for j in range(S_train.shape[1]):\n                                \n                    S_tr = S_train[:,np.arange(S_train.shape[1])!=j]\n                    S_te = S_test[:,np.arange(S_test.shape[1])!=j]\n                                               \n                    print (\"Fit %s_%d subset %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n                    clf.fit(S_tr, y)\n\n                    S_train_i_2[:, j] = clf.predict(S_tr)\n                    S_test_i_2[:, j] = clf.predict(S_te)\n                S_train_2[:, i] = S_train_i_2.mean(axis=1)    \n                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n            \n                #print(\"  1st level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, S_train_2.mean(axis=1))))\n                \n                WOC_columns.append('1st level model_'+str(i+1))\n            WOC_S_train = pd.DataFrame(S_train_2, columns = WOC_columns)\n            print('\\n')\n            print('Correlation between without-one-column predictions from 1st level models:')\n            print('\\n')\n            print(WOC_S_train.corr())\n            print('\\n')\n            \n            \n        try:\n            num_models = len(self.stacker_2)\n            if self.stacker_2==(et_model):\n                num_models=1\n        except TypeError:\n            num_models = len([self.stacker_2])\n            \n        if num_models==1:\n                \n            print (\"Fit %s for final\\n\" % (str(self.stacker_2).split(\"(\")[0]))\n            self.stacker_2.fit(S_train_2, y)\n            \n            stack_res = self.stacker_2.predict(S_test_2)\n        \n            stack_score = self.stacker_2.predict(S_train_2)\n            #print(\"2nd level model final score: %.5f\" % (roc_auc_score(y, stack_score)))\n                \n        else:\n            \n            F_columns = []\n            \n            stack_score = np.zeros((S_train_2.shape[0], len(self.stacker_2)))\n            res = np.zeros((S_test_2.shape[0], len(self.stacker_2)))\n            \n            for i, clf in enumerate(self.stacker_2):\n                \n                print (\"Fit %s_%d\" % (str(clf).split(\"(\")[0], i+1))\n                clf.fit(S_train_2, y)\n                \n                stack_score[:, i] = clf.predict(S_train_2)\n                #print(\"  2nd level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, stack_score[:, i])))\n                \n                res[:, i] = clf.predict(S_test_2)\n                \n                F_columns.append('2nd level model_'+str(i+1))\n            F_S_train = pd.DataFrame(stack_score, columns = F_columns)\n            print('\\n')\n            print('Correlation between final predictions from 2nd level models:')\n            print('\\n')\n            print(F_S_train.corr())\n            print('\\n')\n        \n            stack_res = res.mean(axis=1)            \n            #print(\"2nd level models final score: %.5f\" % (roc_auc_score(y, stack_score.mean(axis=1))))\n\n            \n        return stack_res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61d0ba39fdbcab86d0c104426e300044aa2da50e"},"cell_type":"code","source":"# LightGBM params\n# LightGBM params\nlgb_params_1 = {\n    'learning_rate': 0.01,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'subsample_freq': 5,\n    'colsample_bytree': 0.8,\n    'max_bin': 10,\n    'min_child_samples': 44,\n    'seed': 99,\n    'metric': 'multi_logloss',\n    'boosting_type': 'gbdt'\n}\n\nlgb_params_2 = {\n    'learning_rate': 0.02,\n    'n_estimators': 300,\n    'subsample': 0.7,\n    'subsample_freq': 2,\n    'colsample_bytree': 0.3,  \n    'num_leaves': 16,\n    'seed': 99\n}\n\nlgb_params_3 = {\n    'learning_rate': 0.01,\n    'n_estimators': 100,\n    'subsample': 0.7,\n    'subsample_freq': 3,\n    'colsample_bytree': 0.85,  \n    'num_leaves': 28,\n    'max_bin': 10,\n    'min_child_samples': 70,\n    'seed': 99\n}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8de8b760c8885ab8621e5276bd84239d717b667a"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"421223c547ca485760151e2a5b1345f99b510e3b"},"cell_type":"code","source":"\n# Base models\nlgb_model_1 = LGBMClassifier(**lgb_params_1)\n\nlgb_model_2 = LGBMClassifier(**lgb_params_2)\n\nlgb_model_3 = LGBMClassifier(**lgb_params_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c23ecde34a7c80bf43ab764cfbe158ee534c451f"},"cell_type":"code","source":"# Stacker models\nlog_model = LogisticRegression()\n\net_model = ExtraTreesClassifier(n_estimators=100, max_depth=6, min_samples_split=10, random_state=10)\nxgb = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmlp_model = MLPClassifier(max_iter=7, random_state=42)\nRFC = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16b0d211e6a1e64a9eabc947bb77d20f94543e23"},"cell_type":"code","source":"# Mode 2 run\nstack = Ensemble(mode=2,\n        n_splits=3,\n        stacker_2 = (log_model),         \n        stacker_1 = (xgb,et_model),\n        base_models = (lgb_model_1,lgb_model_2,lgb_model_3))       \n        \ny_pred = stack.fit_predict(train, y, test)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07145abad0caa3846bd8e295193d754045429783"},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\nsample_submission['Target'] = y_pred\nsample_submission.to_csv('Stacking.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4d6f4eea25f030466dc17930f42f48d1eb8d655"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"581af9ac7acc6d50027ce695ccdb8ac2b45edc0f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}