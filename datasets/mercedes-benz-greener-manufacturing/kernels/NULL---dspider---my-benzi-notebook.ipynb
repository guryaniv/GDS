{"cells": [{"source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, minmax_scale\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\nfrom sklearn.linear_model import LassoLarsCV, ElasticNet, SGDRegressor\n\nfrom sklearn.tree import ExtraTreeRegressor\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\n\nfrom sklearn.neural_network import MLPRegressor\n\nimport xgboost as xgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.", "execution_count": null, "metadata": {"_uuid": "3423aad5b893acee5848933f8706806400ebd387", "_cell_guid": "0a956332-7062-4b17-a520-f8a061e92d6a", "trusted": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\n\ntrain_y = train_df['y']\ntrain_id = train_df['ID']\ntrain_df = train_df.drop(\"y\", 1)\ntrain_df = train_df.drop(\"ID\", 1)\n\ntest_id = test_df['ID']\ntest_df = test_df.drop(\"ID\", 1)\n\nnum_train = len(train_df)\n\ndf_all = pd.concat([train_df, test_df])\ndf_all = pd.get_dummies(df_all, drop_first=True)\n\ntrain_df = df_all[:num_train]\ntest_df = df_all[num_train:]\n\n#############################\n\nn_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train_df)\ntsvd_results_test = tsvd.transform(test_df)\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train_df)\npca2_results_test = pca.transform(test_df)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train_df)\nica2_results_test = ica.transform(test_df)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train_df)\ngrp_results_test = grp.transform(test_df)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train_df)\nsrp_results_test = srp.transform(test_df)\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp+1):\n    train_df['pca_' + str(i)] = pca2_results_train[:,i-1]\n    test_df['pca_' + str(i)] = pca2_results_test[:, i-1]\n    \n    train_df['ica_' + str(i)] = ica2_results_train[:,i-1]\n    test_df['ica_' + str(i)] = ica2_results_test[:, i-1]\n\n    train_df['tsvd_' + str(i)] = tsvd_results_train[:,i-1]\n    test_df['tsvd_' + str(i)] = tsvd_results_test[:, i-1]\n    \n    train_df['grp_' + str(i)] = grp_results_train[:,i-1]\n    test_df['grp_' + str(i)] = grp_results_test[:, i-1]\n    \n    train_df['srp_' + str(i)] = srp_results_train[:,i-1]\n    test_df['srp_' + str(i)] = srp_results_test[:, i-1]\n\nX_dtrain, X_test, y_dtrain, y_test = train_test_split(train_df, train_y, random_state=7, test_size=0.3)", "execution_count": null, "metadata": {"_uuid": "7c64d2bab278872b9fa6685af5f8f3e16038da97", "_cell_guid": "5a174037-ef99-4bdb-9c3a-5134b18cc81d", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "We will try to train some models, and stack them with a super-model to obtain better combined predictions.\nAll the algorithms parameters above have been pre-optimized.", "execution_count": null, "metadata": {"_uuid": "24a1be8c11593e2b4e470e552829abd4e30038ec", "_cell_guid": "c62b8803-9a31-40c0-920f-d070ec226689", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "RandomForest\n------------", "execution_count": null, "metadata": {"_uuid": "965aca7c4758358bb22cdcd6f356bd017be0f293", "_cell_guid": "10c6d645-6051-4984-b81b-465a07568d62", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "model_rfr = RandomForestRegressor(n_estimators=600, max_depth=3, min_samples_split=4, min_samples_leaf=60)\n\n# Let's see the feature importance for this model\nimportances = model_rfr.fit(train_df, train_y).feature_importances_\nfeatures = pd.DataFrame()\nfeatures['feature'] = train_df.columns\nfeatures['importance'] = importances\n\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\n#features[features.size-100:].plot(kind='barh', figsize=(12,24))\n\n\n#results = cross_val_score(model_rfr, train_df, train_y, cv=10)\n#print(\"RandomForest score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))", "execution_count": null, "metadata": {"_uuid": "3a1e6231ae9293f9570c7efc51ce7984274ab0a7", "_cell_guid": "42f915e5-9da9-4137-99e4-06ef94d1b3c5", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "best_feature='X314'\ntodrop = features.loc[features['importance'] == 0].index\nnew_train_df = train_df.drop(todrop, 1)\nnew_train_df.head()\nnew_train_df.shape\n\nnew_test_df = test_df.drop(todrop, 1)", "execution_count": null, "metadata": {"_uuid": "9555b33869848a51edd2cb5a6efa78e58a1523b5", "_cell_guid": "8e69cb0e-b295-48e1-8415-76fd35a66c46", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "model_rfr = RandomForestRegressor(n_estimators=600, max_depth=3, min_samples_split=4, min_samples_leaf=60)\n#results = cross_val_score(model_rfr, new_train_df, train_y, cv=10)\n#print(\"RandomForest score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\n#print(results)", "execution_count": null, "metadata": {"_uuid": "b2cf3124aca385fdffd83aba3f259ac5f5a857fc", "_cell_guid": "abfc41a2-4d22-4bd3-9902-ef436b53a37e", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "SVR\n---", "execution_count": null, "metadata": {"_uuid": "9bcc8efef3e899e9ca47732a9cfa7381a049111b", "_cell_guid": "92c3f7f1-79dd-48a9-86e5-5e4edcf8850b", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "model_svr = SVR(kernel='rbf',gamma=0.005, C=10, epsilon=5.0)\n\n'''\nresults = cross_val_score(model_svr, train_df, train_y, cv=10)\nprint(\"SVR score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n\nresults = cross_val_score(model_svr, new_train_df, train_y, cv=10)\nprint(\"SVR score (only on most important features): %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n'''", "execution_count": null, "metadata": {"_uuid": "d7002b84e608cc338bae17018044649778060b55", "_cell_guid": "a01a5c6c-df20-4a9d-b48f-6737a08870e4", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "GBR\n---", "execution_count": null, "metadata": {"_uuid": "b4b0ba3c00d28e46660c778cbd0b0ee575b8b84a", "_cell_guid": "85198610-ac6b-431e-8791-3b86c9e67c23", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "model_gbr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.007, max_depth=3, min_samples_split=6, \n                                      min_samples_leaf=60)\n#results = cross_val_score(model_gbr, train_df, train_y, cv=10)\n#print(\"GBR score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\n#print(results)\n\n#results = cross_val_score(model_gbr, new_train_df, train_y, cv=10)\n#print(\"GBR score (imp features): %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\n#print(results)", "execution_count": null, "metadata": {"_uuid": "cc5ccb0a4902a9ece894e3feeec9d8c00ca5088f", "_cell_guid": "fe9390b3-b75c-4141-8ffb-74549f084742", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "'''\nimportances = model_gbr.fit(new_train_df, train_y).feature_importances_\nfeatures = pd.DataFrame()\nfeatures['feature'] = new_train_df.columns\nfeatures['importance'] = importances\n\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(12,24))\n'''", "execution_count": null, "metadata": {"_uuid": "8186f6842c39143d7d37cd2989fe2c19c48d4ac3", "_cell_guid": "1563c525-9313-42c0-a62e-18d44bc52626", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "'''\ntodrop = features.loc[features['importance'] == 0].index\nnew_train_df2 = new_train_df.drop(todrop, 1)\nnew_train_df2.head()\nnew_train_df2.shape\n'''", "execution_count": null, "metadata": {"_uuid": "5278cd31417a9ddf95e16189010dd322d932a141", "_cell_guid": "e41d34ee-4b79-4d7e-9b3e-de9014f65e1f", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "model_gbr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.007, max_depth=3, min_samples_split=6, \n                                      min_samples_leaf=60)\n'''\nresults = cross_val_score(model_gbr, new_train_df, train_y, cv=10)\nprint(\"GBR score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n\nresults = cross_val_score(model_gbr, new_train_df2, train_y, cv=10)\nprint(\"GBR score (imp features): %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n'''", "execution_count": null, "metadata": {"_uuid": "b94a9eddafce2c8809348dc05f0ee4853536431b", "_cell_guid": "0ec516ef-02c5-4a19-b7b5-64cc915b67c8", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "MLPRegressor\n------------", "execution_count": null, "metadata": {"_uuid": "472911b6bf79ee0939cf73558e9c4e8b94ec2508", "_cell_guid": "f0b1a7aa-d4a3-4ced-b656-74fdac64598f", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "model_mlp = MLPRegressor(max_iter=200, solver='adam', learning_rate=\"constant\")\n\n'''\nresults = cross_val_score(model_mlp, train_df, train_y, cv=10)\nprint(\"MLP score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n\nresults = cross_val_score(model_mlp, new_train_df, train_y, cv=10)\nprint(\"MLP score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n'''", "execution_count": null, "metadata": {"_uuid": "5d007afbc51d9cab5a53f94f990f488c8f2d3b90", "_cell_guid": "7489dd7a-61b0-493b-814b-32882fcddea4", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "ExtraTreeRegressor\n------------------", "execution_count": null, "metadata": {"_uuid": "fedf78e9878382c6a876c5525e6571feb6817165", "_cell_guid": "34eec49a-375c-49f5-bf5b-3d84e36e6ead", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "model_ex = ExtraTreesRegressor(n_estimators=700, max_depth=3, min_samples_split=24, min_samples_leaf=5, bootstrap=True, oob_score=True)\n\n'''\nresults = cross_val_score(model_ex, train_df, train_y, cv=10)\nprint(\"EXR score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n\nresults = cross_val_score(model_ex, new_train_df, train_y, cv=10)\nprint(\"EXR score (only on most important features): %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n'''", "execution_count": null, "metadata": {"_uuid": "0bc31d03624a7ac71d2388f95b56378aad19ffa6", "_cell_guid": "f8da48b6-5786-4c66-89d8-2f598444d469", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "AdaBoost\n--------", "execution_count": null, "metadata": {"_uuid": "0723427879f9ae0c0e61d4458e11213d97dee25f", "_cell_guid": "dfae0540-012a-498f-b59b-1db8d17a0ece", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "model_ada = AdaBoostRegressor(n_estimators=50, learning_rate=0.01)\n\n'''\nresults = cross_val_score(model_ada, train_df, train_y, cv=10)\nprint(\"ADA score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n\nresults = cross_val_score(model_ada, new_train_df, train_y, cv=10)\nprint(\"ADA score (only on most important features): %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n'''", "execution_count": null, "metadata": {"_uuid": "78b59260d1e4607421bdbf7ac9878bae515e3451", "_cell_guid": "01b6f404-256c-46aa-b5ee-cc010e565b3c", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "XGBoost\n-------", "execution_count": null, "metadata": {"_uuid": "c0b66a319c481c9c8a8595061097f8851f63d8b0", "_cell_guid": "5a4013a1-ad52-4d6f-aff3-550344101786", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "model_xgb = xgb.XGBRegressor(objective='reg:linear', n_estimators=50, max_depth=3, learning_rate=0.1, min_child_weight=30, subsample=0.9, colsample_bytree=0.7, reg_alpha=0.01)\n\n'''\nresults = cross_val_score(model_xgb, train_df, train_y, cv=10)\nprint(\"XGB score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n\nresults = cross_val_score(model_xgb, new_train_df, train_y, cv=10)\nprint(\"XGB score (only on most important features): %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n'''", "execution_count": null, "metadata": {"_uuid": "80fa6a56c28a5ff74d0fea0ed857437c0bc82ae1", "_cell_guid": "82e4f50a-3bf9-4263-b275-8ffe1aa00d99", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "SGDRegressor\n------------", "execution_count": null, "metadata": {"_uuid": "f396533fb3981ccf5e15e38496017a8c4a5ac648", "_cell_guid": "cfecf334-5896-4651-9c7f-4ab8b990f05c", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "model_sgd = SGDRegressor(alpha=0.02, penalty='l1', n_iter=10, power_t=0.2, average=False)\n\n'''\nresults = cross_val_score(model_sgd, train_df, train_y, cv=10)\nprint(\"SGD score: %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n\nresults = cross_val_score(model_sgd, new_train_df, train_y, cv=10)\nprint(\"SGD score (only on most important features): %.4f (%.4f)\" % (results.mean()*100, results.std()*100))\nprint(results)\n'''", "execution_count": null, "metadata": {"_uuid": "f196457b4c436ff42f64d4e6fbbffc6c58c97432", "_cell_guid": "2e69503c-8801-407a-bdf6-627bf9b1d066", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "Ensembling\n----------", "execution_count": null, "metadata": {"_uuid": "b4397c17b27b9cf2e3fc856a09df62e9bbf00bfd", "_cell_guid": "9c67cd63-7872-40b8-94f9-f0d60f18c075", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "markdown"}, {"source": "'''\n    This code was borrowed and adapted\n'''\nclass Stacking(object):\n    def __init__(self, n_folds, stacker, base_models):\n        self.n_folds = n_folds\n        self.stacker = stacker\n        self.base_models = base_models\n    def fit_predict(self, X, X2, y, T, T2):\n        X = np.array(X)\n        X2 = np.array(X2)\n        y = np.array(y)\n        T = np.array(T)\n        T2 = np.array(T2)\n        folds = KFold(n_splits=self.n_folds, shuffle=True, random_state=2016)\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n        \n        for i, clf in enumerate(self.base_models):\n            print(\"Predicting with: \", clf[0])\n            S_test_i = np.zeros((T.shape[0], self.n_folds))\n            for j, (train_idx, test_idx) in enumerate(folds.split(X)):\n                y_train = y[train_idx]  \n                if clf[1] == 1:\n                    X_train=X2[train_idx]\n                    X_holdout = X2[test_idx]\n                else:\n                    X_train=X[train_idx]\n                    X_holdout = X[test_idx]\n                        \n                clf[0].fit(X_train, y_train)\n                y_pred = clf[0].predict(X_holdout)[:]\n                S_train[test_idx, i] = y_pred\n                if clf[1] == 1:\n                    S_test_i[:, j] = clf[0].predict(T2)[:]\n                else:\n                    S_test_i[:, j] = clf[0].predict(T)[:]        \n            S_test[:, i] = S_test_i.mean(1)\n            \n        #self.stacker.fit(S_train, y)\n        #y_pred = self.stacker.predict(S_test)[:]\n        \n        return S_train, S_test", "execution_count": null, "metadata": {"_uuid": "209c5118c34dc372639b523e0eb21dfeb8fc8d2d", "_cell_guid": "b03b9fc6-55b3-451b-b6c0-2d2e3bda1c29", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "We'll first get the combined predictions of the different models we have. And then try to optimize another model using these new data. ", "metadata": {"_uuid": "8571bfda4a89e16be5f385caa6609b7823878456", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"source": "base_models=[(model_rfr, 0), (model_gbr, 0), (model_xgb, 1), (model_ada, 0), (model_ex, 0)]\n\nens = Stacking(n_folds=10, stacker=model_gbr, base_models=base_models)\ns_train, s_test = ens.fit_predict(train_df, new_train_df, train_y, test_df, new_test_df)", "metadata": {"_uuid": "a0953467564a8fd595e8564c474ba398eeef36ae", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": "new_train = pd.DataFrame({\n        \"rfr\": s_train[:, 0],\n        \"gbr\": s_train[:, 1],\n        \"xgb\": s_train[:, 2],\n        \"ada\": s_train[:, 3],\n        \"ex\": s_train[:, 4],\n        \"y\": train_y\n    })\nnew_train.to_csv('new_train.csv', index=False)", "metadata": {"_uuid": "ffa6cf0edcf955aa95f2a80496a9b00ed6abed04", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": "new_test = pd.DataFrame({\n        \"rfr\": s_test[:, 0],\n        \"gbr\": s_test[:, 1],\n        \"xgb\": s_test[:, 2],\n        \"ada\": s_test[:, 3],\n        \"ex\": s_test[:, 4]\n    })\nnew_test.to_csv('new_test.csv', index=False)", "metadata": {"_uuid": "21d2dc8131863256f7f5d03cb3574dfde9be5cd6", "collapsed": false, "_execution_state": "idle"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": "\nstacker = ElasticNet(normalize=True)\nbase_models=[(model_rfr, 0), (model_gbr, 0), (model_xgb, 1), (model_ada, 0), (model_ex, 0)]\n\nens = Stacking(n_folds=10, stacker=model_gbr, base_models=base_models)\n\ny_pred=ens.fit_predict(train_df, new_train_df, train_y, test_df, new_test_df)\n", "execution_count": null, "metadata": {"_uuid": "fb0c7f4fa9dfc73b200585ea6224f94ddf8f89eb", "_cell_guid": "3c88e79e-e251-4775-b3a4-b70051d86587", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}, {"source": "\nsubmission = pd.DataFrame({\n        \"ID\": test_id,\n        \"y\": y_pred\n    })\nsubmission.to_csv('mercedes_ens_opt.csv', index=False)\n", "execution_count": null, "metadata": {"_uuid": "592ce4d229e98d3cfb92b16211eb86e6d94dc6b7", "_cell_guid": "73124cd1-2bda-41fb-8a9c-124cc3dcdbc4", "trusted": false, "collapsed": false, "_execution_state": "idle"}, "outputs": [], "cell_type": "code"}], "nbformat_minor": 0, "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.1", "file_extension": ".py", "name": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat": 4}