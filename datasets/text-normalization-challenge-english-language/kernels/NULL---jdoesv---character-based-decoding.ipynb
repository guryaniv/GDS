{"cells": [{"source": ["Data Prep"], "metadata": {"_cell_guid": "5f2bf1c7-1919-4226-9007-759bfad51529", "_uuid": "dad823507a06d59c40f15be60b2537d3e0708241"}, "cell_type": "markdown"}, {"source": ["from tensorflow.python.ops.rnn_cell_impl import *"], "metadata": {"collapsed": true, "_cell_guid": "6cebe4d3-8636-42d2-a0e4-1f1bb84f1aca", "_uuid": "3d42b091039af4c5b79d130281d9c54ec889b7be"}, "cell_type": "code", "outputs": [], "execution_count": 2}, {"source": ["#https://arxiv.org/pdf/1603.09727.pdf\n", "import pandas as pd\n", "import numpy as np, os, shutil\n", "import string\n", "train = pd.read_csv('../input/en_train.csv')\n", "len(train)\n", "x = train.groupby('sentence_id')['before'].apply(lambda x : x.str.cat(sep = ' '))\n", "len(x)\n", "y = train.groupby('sentence_id')['after'].apply(lambda x : x.str.cat(sep = ' '))\n", "split = int(0.9*len(x))\n", "split\n", "x, y\n", "train_x, valid_x = x[:split], x[split:]\n", "train_y, valid_y = y[:split], y[split:]\n", "\n", "train_x\n", "#shutil.rmtree('./char')\n", "os.mkdir('./char')\n", "train_x.to_csv('./char/train.x.txt', index = False, header = False)\n", "valid_x.to_csv('./char/valid.x.txt', index = False, header = False)\n", "train_y.to_csv('./char/train.y.txt', index = False, header = False)\n", "valid_y.to_csv('./char/valid.y.txt', index = False, header = False)\n", "os.listdir('./char')"], "metadata": {"_cell_guid": "9ec68bce-a39f-4aa2-85ce-128e08b09e21", "_uuid": "3c4f5bcfde979e3c6e84afa5f5844b77f31057b1"}, "cell_type": "code", "outputs": [], "execution_count": 3}, {"source": ["NLC Data"], "metadata": {"_cell_guid": "035d89b5-0f80-4687-a63a-6b799d8417fb", "_uuid": "0e7b44ca23f79e69bb081531bb353f8e123e5d49"}, "cell_type": "markdown"}, {"source": ["from __future__ import absolute_import\n", "from __future__ import division\n", "from __future__ import print_function\n", "import re\n", "import gzip\n", "import os\n", "import re\n", "import tarfile\n", "from six.moves import urllib\n", "from tensorflow.python.platform import gfile\n", "\n", "# Special vocabulary symbols - we always put them at the start.\n", "_PAD = b\"<pad>\"\n", "_SOS = b\"<sos>\"\n", "_EOS = b\"<eos>\"\n", "_UNK = b\"<unk>\"\n", "_START_VOCAB = [_PAD, _SOS, _EOS, _UNK]\n", "\n", "PAD_ID = 0\n", "SOS_ID = 1\n", "EOS_ID = 2\n", "UNK_ID = 3\n", "\n", "# Regular expressions used to tokenize.\n", "_WORD_SPLIT = re.compile(b\"([.,!?\\\"':;)(])\")\n", "_DIGIT_RE = re.compile(br\"\\d\")\n", "\n", "#./train.x.txt ==> train['before'] groupby sentences train.y.txt group by after\n", "def get_nlc_train_set(directory):\n", "  train_path = os.path.join(directory, \"train\")\n", "  print (train_path + \".x.txt\")\n", "  print (train_path + \".y.txt\")\n", "  return train_path\n", "\n", "\n", "def get_nlc_dev_set(directory):\n", "  dev_name = \"valid\"\n", "  dev_path = os.path.join(directory, dev_name)\n", "  return dev_path\n", "\n", "\n", "def basic_tokenizer(sentence):\n", "  words = []\n", "  for space_separated_fragment in sentence.strip().split():\n", "    words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n", "  return [w for w in words if w]\n", "\n", "def char_tokenizer(sentence):\n", "  return list(sentence.strip())\n", "\n", "def bpe_tokenizer(sentence):\n", "  tokens = sentence.strip().split()\n", "  tokens = [w + \"</w>\" if not w.endswith(\"@@\") else w for w in tokens]\n", "  tokens = [w.replace(\"@@\", \"\") for w in tokens]\n", "  return tokens\n", "\n", "def remove_nonascii(text):\n", "  return re.sub(r'[^\\x00-\\x7F]', '', text.decode(\"utf-8\") ).encode('utf-8')\n", "\n", "def create_vocabulary(vocabulary_path, data_paths, max_vocabulary_size,\n", "                      tokenizer=None, normalize_digits=False):\n", "  if not gfile.Exists(vocabulary_path):\n", "    print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, str(data_paths)))\n", "    vocab = {}\n", "    for path in data_paths:\n", "      with gfile.GFile(path, mode=\"rb\") as f:\n", "        counter = 0\n", "        for line in f:\n", "          counter += 1\n", "          if counter % 100000 == 0:\n", "            print(\"  processing line %d\" % counter)\n", "          # Remove non-ASCII characters\n", "          line = remove_nonascii(line)\n", "          tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n", "          for w in tokens:\n", "            word = re.sub(_DIGIT_RE, b\"0\", w) if normalize_digits else w\n", "            if word in vocab:\n", "              vocab[word] += 1\n", "            else:\n", "              vocab[word] = 1\n", "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n", "    print(\"Vocabulary size: %d\" % len(vocab_list))\n", "    if len(vocab_list) > max_vocabulary_size:\n", "      vocab_list = vocab_list[:max_vocabulary_size]\n", "    with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n", "      for w in vocab_list:\n", "        vocab_file.write(str(w).encode('utf-8')+ b\"\\n\")\n", "\n", "\n", "def initialize_vocabulary(vocabulary_path, bpe=False):\n", "  if gfile.Exists(vocabulary_path):\n", "    rev_vocab = []\n", "    with gfile.GFile(vocabulary_path, mode=\"rb\") as f:\n", "      rev_vocab.extend(f.readlines())\n", "    rev_vocab = [line.strip(b'\\n') for line in rev_vocab]\n", "    # Call ''.join below since BPE outputs split pairs with spaces\n", "    if bpe:\n", "      vocab = dict([(''.join(x.split(' ')), y) for (y, x) in enumerate(rev_vocab)])\n", "    else:\n", "      vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n", "    return vocab, rev_vocab\n", "  else:\n", "    raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n", "\n", "\n", "def sentence_to_token_ids(sentence, vocabulary,\n", "                          tokenizer=None, normalize_digits=False):\n", "  if tokenizer:\n", "    words = tokenizer(sentence)\n", "  else:\n", "    words = basic_tokenizer(sentence)\n", "  if not normalize_digits:\n", "    return [vocabulary.get(w, UNK_ID) for w in words]\n", "  # Normalize digits by 0 before looking words up in the vocabulary.\n", "  return [vocabulary.get(re.sub(_DIGIT_RE, b\"0\", w), UNK_ID) for w in words]\n", "\n", "\n", "def data_to_token_ids(data_path, target_path, vocabulary_path,\n", "                      tokenizer=None, normalize_digits=False):\n", "  if not gfile.Exists(target_path):\n", "    print(\"Tokenizing data in %s\" % data_path)\n", "    vocab, _ = initialize_vocabulary(vocabulary_path, bpe=(tokenizer==bpe_tokenizer))\n", "    with gfile.GFile(data_path, mode=\"rb\") as data_file:\n", "      with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n", "        counter = 0\n", "        for line in data_file:\n", "          counter += 1\n", "          if counter % 100000 == 0:\n", "            print(\"  tokenizing line %d\" % counter)\n", "          line = remove_nonascii(line)\n", "          token_ids = sentence_to_token_ids(line, vocab, tokenizer,\n", "                                            normalize_digits)\n", "          tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n", "\n", "\n", "def prepare_nlc_data(data_dir, max_vocabulary_size, tokenizer=char_tokenizer, other_dev_path=None):\n", "  # Get nlc data to the specified directory.\n", "  train_path = get_nlc_train_set(data_dir)\n", "  dev_path = get_nlc_dev_set(data_dir)\n", "  vocab_path = os.path.join(data_dir, \"vocab.dat\")\n", "  if tokenizer != bpe_tokenizer:\n", "    create_vocabulary(vocab_path, [train_path + \".y.txt\", train_path + \".x.txt\"],\n", "                      max_vocabulary_size, tokenizer)\n", "\n", "  # Create token ids for the training data.\n", "  y_train_ids_path = train_path + \".ids.y\"\n", "  x_train_ids_path = train_path + \".ids.x\"\n", "  data_to_token_ids(train_path + \".y.txt\", y_train_ids_path, vocab_path, tokenizer)\n", "  data_to_token_ids(train_path + \".x.txt\", x_train_ids_path, vocab_path, tokenizer)\n", "\n", "  # Create token ids for the development data.\n", "  y_dev_ids_path = dev_path + \".ids.y\"\n", "  x_dev_ids_path = dev_path + \".ids.x\"\n", "  data_to_token_ids(dev_path + \".y.txt\", y_dev_ids_path, vocab_path, tokenizer)\n", "  data_to_token_ids(dev_path + \".x.txt\", x_dev_ids_path, vocab_path, tokenizer)\n", "\n", "  return (x_train_ids_path, y_train_ids_path,\n", "          x_dev_ids_path, y_dev_ids_path, vocab_path)"], "metadata": {"collapsed": true, "_cell_guid": "5a755bed-9bf9-4f5a-b42e-1015a9860ddb", "_uuid": "9125f7afae6d631771b1757fca0c7d8d43f0998c"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["NLC Model"], "metadata": {"_cell_guid": "339681f5-6fce-41be-8b27-5b6d77c90554", "_uuid": "d9b961758de01204c3c97ee8a8c07967e58ae617"}, "cell_type": "markdown"}, {"source": ["from __future__ import absolute_import\n", "from __future__ import division\n", "from __future__ import print_function\n", "\n", "import random\n", "\n", "import numpy as np\n", "from six.moves import xrange  # pylint: disable=redefined-builtin\n", "import tensorflow as tf\n", "from tensorflow.python.framework import ops\n", "from tensorflow.python.framework import dtypes\n", "from tensorflow.python.ops import array_ops\n", "from tensorflow.python.ops import control_flow_ops\n", "from tensorflow.python.ops import embedding_ops\n", "from tensorflow.python.ops import rnn\n", "from tensorflow.python.ops import rnn_cell\n", "from tensorflow.python.ops import variable_scope as vs\n", "from tensorflow.python.ops.math_ops import sigmoid\n", "from tensorflow.python.ops.math_ops import tanh\n", "from tensorflow.python.ops.rnn_cell_impl import *\n", "\n", "def get_optimizer(opt):\n", "  if opt == \"adam\":\n", "    optfn = tf.train.AdamOptimizer\n", "  elif opt == \"sgd\":\n", "    optfn = tf.train.GradientDescentOptimizer\n", "  else:\n", "    assert(False)\n", "  return optfn\n", "\n", "class GRUCellAttn(rnn_cell.GRUCell):\n", "  def __init__(self, num_units, encoder_output, scope=None):\n", "    self.hs = encoder_output\n", "    with vs.variable_scope(scope or type(self).__name__):\n", "      with vs.variable_scope(\"Attn1\") as scope:\n", "        hs2d = tf.reshape(self.hs, [-1, num_units])\n", "        phi_hs2d = tanh(_linear(hs2d, num_units, True, 1.0))\n", "        self.phi_hs = tf.reshape(phi_hs2d, tf.shape(self.hs))\n", "    super(GRUCellAttn, self).__init__(num_units)\n", "\n", "  def __call__(self, inputs, state, scope=None):\n", "    gru_out, gru_state = super(GRUCellAttn, self).__call__(inputs, state, scope)\n", "    with vs.variable_scope(scope or type(self).__name__):\n", "      with vs.variable_scope(\"Attn2\"):\n", "        gamma_h = tanh(_linear(gru_out, self._num_units, True, 1.0))\n", "      weights = tf.reduce_sum(self.phi_hs * gamma_h, reduction_indices=2, keep_dims=True)\n", "      weights = tf.exp(weights - tf.reduce_max(weights, reduction_indices=0, keep_dims=True))\n", "      weights = weights / (1e-6 + tf.reduce_sum(weights, reduction_indices=0, keep_dims=True))\n", "      context = tf.reduce_sum(self.hs * weights, reduction_indices=0)\n", "      with vs.variable_scope(\"AttnConcat\"):\n", "        out = tf.nn.relu(_linear([context, gru_out], self._num_units, True, 1.0))\n", "      self.attn_map = tf.squeeze(tf.slice(weights, [0, 0, 0], [-1, -1, 1]))\n", "      return (out, out)\n", "\n", "\n", "class NLCModel(object):\n", "  def __init__(self, vocab_size, size, num_layers, max_gradient_norm, batch_size, learning_rate,\n", "               learning_rate_decay_factor, dropout, forward_only=False, optimizer=\"adam\"):\n", "\n", "    self.size = size\n", "    self.vocab_size = vocab_size\n", "    self.batch_size = batch_size\n", "    self.num_layers = num_layers\n", "    self.keep_prob_config = 1.0 - dropout\n", "    self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n", "    self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * learning_rate_decay_factor)\n", "    self.global_step = tf.Variable(0, trainable=False)\n", "\n", "    self.keep_prob = tf.placeholder(tf.float32)\n", "    self.source_tokens = tf.placeholder(tf.int32, shape=[None, None])\n", "    self.target_tokens = tf.placeholder(tf.int32, shape=[None, None])\n", "    self.source_mask = tf.placeholder(tf.int32, shape=[None, None])\n", "    self.target_mask = tf.placeholder(tf.int32, shape=[None, None])\n", "    self.beam_size = tf.placeholder(tf.int32)\n", "    self.target_length = tf.reduce_sum(self.target_mask, reduction_indices=0)\n", "\n", "    self.decoder_state_input, self.decoder_state_output = [], []\n", "    for i in xrange(num_layers):\n", "      self.decoder_state_input.append(tf.placeholder(tf.float32, shape=[None, size]))\n", "\n", "    with tf.variable_scope(\"NLC\", initializer=tf.uniform_unit_scaling_initializer(1.0)):\n", "      self.setup_embeddings() \n", "      self.setup_encoder()\n", "      self.setup_decoder()\n", "      self.setup_loss()\n", "\n", "      self.setup_beam()\n", "\n", "    params = tf.trainable_variables()\n", "    if not forward_only:\n", "      opt = get_optimizer(optimizer)(self.learning_rate)\n", "\n", "      gradients = tf.gradients(self.losses, params)\n", "      clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n", "#      self.gradient_norm = tf.global_norm(clipped_gradients)\n", "      self.gradient_norm = tf.global_norm(gradients)\n", "      self.param_norm = tf.global_norm(params)\n", "      self.updates = opt.apply_gradients(\n", "        zip(clipped_gradients, params), global_step=self.global_step)\n", "\n", "    self.saver = tf.train.Saver(tf.all_variables(), max_to_keep=0)\n", "\n", "  def setup_embeddings(self):\n", "    with vs.variable_scope(\"embeddings\") as scope:\n", "      self.L_enc = tf.get_variable(\"L_enc\", [self.vocab_size, self.size])\n", "      self.L_dec = tf.get_variable(\"L_dec\", [self.vocab_size, self.size])\n", "      self.encoder_inputs = embedding_ops.embedding_lookup(self.L_enc, self.source_tokens)\n", "      self.decoder_inputs = embedding_ops.embedding_lookup(self.L_dec, self.target_tokens)\n", "\n", "  def setup_encoder(self):\n", "    self.encoder_cell = rnn_cell.GRUCell(self.size)\n", "    with vs.variable_scope(\"PryamidEncoder\"):\n", "      inp = self.encoder_inputs\n", "      mask = self.source_mask\n", "      out = None\n", "      for i in xrange(self.num_layers):\n", "        with vs.variable_scope(\"EncoderCell%d\" % i) as scope:\n", "          srclen = tf.reduce_sum(mask, reduction_indices=0)\n", "          out, _ = self.bidirectional_rnn(self.encoder_cell, inp, srclen, scope=scope)\n", "          dropin, mask = self.downscale(out, mask)\n", "          inp = self.dropout(dropin)\n", "      self.encoder_output = out\n", "\n", "  def setup_decoder(self):\n", "    if self.num_layers > 1:\n", "      self.decoder_cell = rnn_cell.GRUCell(self.size)\n", "    self.attn_cell = GRUCellAttn(self.size, self.encoder_output, scope=\"DecoderAttnCell\")\n", "\n", "    with vs.variable_scope(\"Decoder\"):\n", "      inp = self.decoder_inputs\n", "      for i in xrange(self.num_layers - 1):\n", "        with vs.variable_scope(\"DecoderCell%d\" % i) as scope:\n", "          out, state_output = rnn.dynamic_rnn(self.decoder_cell, inp, time_major=True,\n", "                                              dtype=dtypes.float32, sequence_length=self.target_length,\n", "                                              scope=scope, initial_state=self.decoder_state_input[i])\n", "          inp = self.dropout(out)\n", "          self.decoder_state_output.append(state_output)\n", "\n", "      with vs.variable_scope(\"DecoderAttnCell\") as scope:\n", "        out, state_output = rnn.dynamic_rnn(self.attn_cell, inp, time_major=True,\n", "                                            dtype=dtypes.float32, sequence_length=self.target_length,\n", "                                            scope=scope, initial_state=self.decoder_state_input[i+1])\n", "        self.decoder_output = self.dropout(out)\n", "        self.decoder_state_output.append(state_output)\n", "\n", "  def decoder_graph(self, decoder_inputs, decoder_state_input):\n", "    decoder_output, decoder_state_output = None, []\n", "    inp = decoder_inputs\n", "\n", "    with vs.variable_scope(\"Decoder\", reuse=True):\n", "      for i in xrange(self.num_layers - 1):\n", "        with vs.variable_scope(\"DecoderCell%d\" % i) as scope:\n", "          inp, state_output = self.decoder_cell(inp, decoder_state_input[i])\n", "          decoder_state_output.append(state_output)\n", "\n", "      with vs.variable_scope(\"DecoderAttnCell\") as scope:\n", "        decoder_output, state_output = self.attn_cell(inp, decoder_state_input[i+1])\n", "        decoder_state_output.append(state_output)\n", "\n", "    return decoder_output, decoder_state_output\n", "\n", "  def setup_beam(self):\n", "    time_0 = tf.constant(0)\n", "    beam_seqs_0 = tf.constant([[SOS_ID]])\n", "    beam_probs_0 = tf.constant([0.])\n", "\n", "    cand_seqs_0 = tf.constant([[EOS_ID]])\n", "    cand_probs_0 = tf.constant([-3e38])\n", "\n", "    state_0 = tf.zeros([1, self.size])\n", "    states_0 = [state_0] * self.num_layers\n", "\n", "    def beam_cond(time, beam_probs, beam_seqs, cand_probs, cand_seqs, *states):\n", "      return tf.reduce_max(beam_probs) >= tf.reduce_min(cand_probs)\n", "\n", "    def beam_step(time, beam_probs, beam_seqs, cand_probs, cand_seqs, *states):\n", "      batch_size = tf.shape(beam_probs)[0]\n", "      inputs = tf.reshape(tf.slice(beam_seqs, [0, time], [batch_size, 1]), [batch_size])\n", "      decoder_input = embedding_ops.embedding_lookup(self.L_dec, inputs)\n", "      decoder_output, state_output = self.decoder_graph(decoder_input, states)\n", "\n", "      with vs.variable_scope(\"Logistic\", reuse=True):\n", "        do2d = tf.reshape(decoder_output, [-1, self.size])\n", "        logits2d = _linear(do2d, self.vocab_size, True, 1.0)\n", "        logprobs2d = tf.nn.log_softmax(logits2d)\n", "\n", "      total_probs = logprobs2d + tf.reshape(beam_probs, [-1, 1])\n", "      total_probs_noEOS = tf.concat(1, [tf.slice(total_probs, [0, 0], [batch_size, EOS_ID]),\n", "                                        tf.tile([[-3e38]], [batch_size, 1]),\n", "                                        tf.slice(total_probs, [0, EOS_ID + 1],\n", "                                                 [batch_size, self.vocab_size - EOS_ID - 1])])\n", "\n", "      flat_total_probs = tf.reshape(total_probs_noEOS, [-1])\n", "      beam_k = tf.minimum(tf.size(flat_total_probs), self.beam_size)\n", "      next_beam_probs, top_indices = tf.nn.top_k(flat_total_probs, k=beam_k)\n", "\n", "      next_bases = tf.floordiv(top_indices, self.vocab_size)\n", "      next_mods = tf.mod(top_indices, self.vocab_size)\n", "\n", "      next_states = [tf.gather(state, next_bases) for state in state_output]\n", "      next_beam_seqs = tf.concat(1, [tf.gather(beam_seqs, next_bases),\n", "                                     tf.reshape(next_mods, [-1, 1])])\n", "\n", "      cand_seqs_pad = tf.pad(cand_seqs, [[0, 0], [0, 1]])\n", "      beam_seqs_EOS = tf.pad(beam_seqs, [[0, 0], [0, 1]])\n", "      new_cand_seqs = tf.concat(0, [cand_seqs_pad, beam_seqs_EOS])\n", "      EOS_probs = tf.slice(total_probs, [0, EOS_ID], [batch_size, 1])\n", "      new_cand_probs = tf.concat(0, [cand_probs, tf.reshape(EOS_probs, [-1])])\n", "\n", "      cand_k = tf.minimum(tf.size(new_cand_probs), self.beam_size)\n", "      next_cand_probs, next_cand_indices = tf.nn.top_k(new_cand_probs, k=cand_k)\n", "      next_cand_seqs = tf.gather(new_cand_seqs, next_cand_indices)\n", "\n", "      return [time + 1, next_beam_probs, next_beam_seqs, next_cand_probs, next_cand_seqs] + next_states\n", "\n", "    var_shape = []\n", "    var_shape.append((time_0, time_0.get_shape()))\n", "    var_shape.append((beam_probs_0, tf.TensorShape([None,])))\n", "    var_shape.append((beam_seqs_0, tf.TensorShape([None, None])))\n", "    var_shape.append((cand_probs_0, tf.TensorShape([None,])))\n", "    var_shape.append((cand_seqs_0, tf.TensorShape([None, None])))\n", "    var_shape.extend([(state_0, tf.TensorShape([None, self.size])) for state_0 in states_0])\n", "    loop_vars, loop_var_shapes = zip(* var_shape)\n", "    ret_vars = tf.while_loop(cond=beam_cond, body=beam_step, loop_vars=loop_vars, shape_invariants=loop_var_shapes, back_prop=False)\n", "#    time, beam_probs, beam_seqs, cand_probs, cand_seqs, _ = ret_vars\n", "    cand_seqs = ret_vars[4]\n", "    cand_probs = ret_vars[3]\n", "    self.beam_output = cand_seqs\n", "    self.beam_scores = cand_probs\n", "\n", "  def setup_loss(self):\n", "    with vs.variable_scope(\"Logistic\"):\n", "      doshape = tf.shape(self.decoder_output)\n", "      T, batch_size = doshape[0], doshape[1]\n", "      do2d = tf.reshape(self.decoder_output, [-1, self.size])\n", "      logits2d = _linear(do2d, self.vocab_size, True, 1.0)\n", "      outputs2d = tf.nn.log_softmax(logits2d)\n", "      self.outputs = tf.reshape(outputs2d, tf.pack([T, batch_size, self.vocab_size]))\n", "\n", "      targets_no_GO = tf.slice(self.target_tokens, [1, 0], [-1, -1])\n", "      masks_no_GO = tf.slice(self.target_mask, [1, 0], [-1, -1])\n", "      # easier to pad target/mask than to split decoder input since tensorflow does not support negative indexing\n", "      labels1d = tf.reshape(tf.pad(targets_no_GO, [[0, 1], [0, 0]]), [-1])\n", "      mask1d = tf.reshape(tf.pad(masks_no_GO, [[0, 1], [0, 0]]), [-1])\n", "      losses1d = tf.nn.sparse_softmax_cross_entropy_with_logits(logits2d, labels1d) * tf.to_float(mask1d)\n", "      losses2d = tf.reshape(losses1d, tf.pack([T, batch_size]))\n", "      self.losses = tf.reduce_sum(losses2d) / tf.to_float(batch_size)\n", "\n", "  def dropout(self, inp):\n", "    return tf.nn.dropout(inp, self.keep_prob)\n", "\n", "  def downscale(self, inp, mask):\n", "    return inp, mask\n", "\n", "    with vs.variable_scope(\"Downscale\"):\n", "      inshape = tf.shape(inp)\n", "      T, batch_size, dim = inshape[0], inshape[1], inshape[2]\n", "      inp2d = tf.reshape(tf.transpose(inp, perm=[1, 0, 2]), [-1, 2 * self.size])\n", "      out2d = _linear(inp2d, self.size, True, 1.0)\n", "      out3d = tf.reshape(out2d, tf.pack((batch_size, tf.to_int32(T/2), dim)))\n", "      out3d = tf.transpose(out3d, perm=[1, 0, 2])\n", "      out3d.set_shape([None, None, self.size])\n", "      out = tanh(out3d)\n", "\n", "      mask = tf.transpose(mask)\n", "      mask = tf.reshape(mask, [-1, 2])\n", "      mask = tf.cast(mask, tf.bool)\n", "      mask = tf.reduce_any(mask, reduction_indices=1)\n", "      mask = tf.to_int32(mask)\n", "      mask = tf.reshape(mask, tf.pack([batch_size, -1]))\n", "      mask = tf.transpose(mask)\n", "    return out, mask\n", "\n", "  def bidirectional_rnn(self, cell, inputs, lengths, scope=None):\n", "    name = scope.name or \"BiRNN\"\n", "    # Forward direction\n", "    with vs.variable_scope(name + \"_FW\") as fw_scope:\n", "      output_fw, output_state_fw = rnn.dynamic_rnn(cell, inputs, time_major=True, dtype=dtypes.float32,\n", "                                                   sequence_length=lengths, scope=fw_scope)\n", "    # Backward direction\n", "    inputs_bw = tf.reverse_sequence(inputs, tf.to_int64(lengths), seq_dim=0, batch_dim=1)\n", "    with vs.variable_scope(name + \"_BW\") as bw_scope:\n", "      output_bw, output_state_bw = rnn.dynamic_rnn(cell, inputs_bw, time_major=True, dtype=dtypes.float32,\n", "                                                   sequence_length=lengths, scope=bw_scope)\n", "\n", "    output_bw = tf.reverse_sequence(output_bw, tf.to_int64(lengths), seq_dim=0, batch_dim=1)\n", "\n", "    outputs = output_fw + output_bw\n", "    output_state = output_state_fw + output_state_bw\n", "\n", "    return (outputs, output_state)\n", "\n", "  def set_default_decoder_state_input(self, input_feed, batch_size):\n", "    default_value = np.zeros([batch_size, self.size])\n", "    for i in xrange(self.num_layers):\n", "      input_feed[self.decoder_state_input[i]] = default_value\n", "\n", "  def train(self, session, source_tokens, source_mask, target_tokens, target_mask):\n", "    input_feed = {}\n", "    input_feed[self.source_tokens] = source_tokens\n", "    input_feed[self.target_tokens] = target_tokens\n", "    input_feed[self.source_mask] = source_mask\n", "    input_feed[self.target_mask] = target_mask\n", "    input_feed[self.keep_prob] = self.keep_prob_config\n", "    self.set_default_decoder_state_input(input_feed, target_tokens.shape[1])\n", "\n", "    output_feed = [self.updates, self.gradient_norm, self.losses, self.param_norm]\n", "\n", "    outputs = session.run(output_feed, input_feed)\n", "\n", "    return outputs[1], outputs[2], outputs[3]\n", "\n", "  def test(self, session, source_tokens, source_mask, target_tokens, target_mask):\n", "    input_feed = {}\n", "    input_feed[self.source_tokens] = source_tokens\n", "    input_feed[self.target_tokens] = target_tokens\n", "    input_feed[self.source_mask] = source_mask\n", "    input_feed[self.target_mask] = target_mask\n", "    input_feed[self.keep_prob] = 1.\n", "    self.set_default_decoder_state_input(input_feed, target_tokens.shape[1])\n", "\n", "    output_feed = [self.losses]\n", "\n", "    outputs = session.run(output_feed, input_feed)\n", "\n", "    return outputs[0]\n", "\n", "  def encode(self, session, source_tokens, source_mask):\n", "    input_feed = {}\n", "    input_feed[self.source_tokens] = source_tokens\n", "    input_feed[self.source_mask] = source_mask\n", "    input_feed[self.keep_prob] = 1.\n", "\n", "    output_feed = [self.encoder_output]\n", "\n", "    outputs = session.run(output_feed, input_feed)\n", "\n", "    return outputs[0]\n", "\n", "  def decode(self, session, encoder_output, target_tokens, target_mask=None, decoder_states=None):\n", "    input_feed = {}\n", "    input_feed[self.encoder_output] = encoder_output\n", "    input_feed[self.target_tokens] = target_tokens\n", "    input_feed[self.target_mask] = target_mask if target_mask else np.ones_like(target_tokens)\n", "    input_feed[self.keep_prob] = 1.\n", "\n", "    if not decoder_states:\n", "      self.set_default_decoder_state_input(input_feed, target_tokens.shape[1])\n", "    else:\n", "      for i in xrange(self.num_layers):\n", "        input_feed[self.decoder_state_input[i]] = decoder_states[i]\n", "\n", "    output_feed = [self.outputs] + self.decoder_state_output\n", "\n", "    outputs = session.run(output_feed, input_feed)\n", "\n", "    return outputs[0], None, outputs[1:]\n", "\n", "  def decode_beam(self, session, encoder_output, beam_size=8):\n", "    input_feed = {}\n", "    input_feed[self.encoder_output] = encoder_output\n", "    input_feed[self.keep_prob] = 1.\n", "    input_feed[self.beam_size] = beam_size\n", "\n", "    output_feed = [self.beam_output, self.beam_scores]\n", "\n", "    outputs = session.run(output_feed, input_feed)\n", "\n", "    return outputs[0], outputs[1]"], "metadata": {"collapsed": true, "_cell_guid": "099d053d-da41-40d7-bb2b-0f4080f1c536", "_uuid": "4b68878451a36a5906dbe89b9d210156e7076dac"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Train data"], "metadata": {"_cell_guid": "ac98af7a-0996-4f36-8ea5-2da86ffdc1c0", "_uuid": "0c34769dc06238d5ad898bfb92713eaa50886cdd"}, "cell_type": "markdown"}, {"source": ["from __future__ import absolute_import\n", "from __future__ import division\n", "from __future__ import print_function\n", "\n", "import math\n", "import os\n", "import random\n", "import sys\n", "import time\n", "import random\n", "import json\n", "\n", "import numpy as np\n", "from six.moves import xrange\n", "import tensorflow as tf\n", "\n", "import logging\n", "logging.basicConfig(level=logging.INFO)\n", "\n", "FLAGS = tf.app.flags.FLAGS\n", "\n", "def tokenize(string):\n", "  return [int(s) for s in string.split()]\n", "\n", "def pair_iter(fnamex, fnamey, batch_size, num_layers, sort_and_shuffle=True):\n", "  fdx, fdy = open(fnamex), open(fnamey)\n", "  batches = []\n", "\n", "  while True:\n", "    if len(batches) == 0:\n", "      refill(batches, fdx, fdy, batch_size, sort_and_shuffle=sort_and_shuffle)\n", "    if len(batches) == 0:\n", "      break\n", "\n", "    x_tokens, y_tokens = batches.pop(0)\n", "    y_tokens = add_sos_eos(y_tokens)\n", "    x_padded, y_padded = padded(x_tokens, num_layers), padded(y_tokens, 1)\n", "\n", "    source_tokens = np.array(x_padded).T\n", "    source_mask = (source_tokens != PAD_ID).astype(np.int32)\n", "    target_tokens = np.array(y_padded).T\n", "    target_mask = (target_tokens != PAD_ID).astype(np.int32)\n", "\n", "    yield (source_tokens, source_mask, target_tokens, target_mask)\n", "\n", "  return\n", "\n", "def refill(batches, fdx, fdy, batch_size, sort_and_shuffle=True):\n", "  line_pairs = []\n", "  linex, liney = fdx.readline(), fdy.readline()\n", "\n", "  while linex and liney:\n", "    x_tokens, y_tokens = tokenize(linex), tokenize(liney)\n", "\n", "    if len(x_tokens) < FLAGS.max_seq_len and len(y_tokens) < FLAGS.max_seq_len:\n", "      line_pairs.append((x_tokens, y_tokens))\n", "    if len(line_pairs) == batch_size * 16:\n", "      break\n", "    linex, liney = fdx.readline(), fdy.readline()\n", "\n", "  if sort_and_shuffle:\n", "    line_pairs = sorted(line_pairs, key=lambda e: len(e[0]))\n", "\n", "  for batch_start in xrange(0, len(line_pairs), batch_size):\n", "    x_batch, y_batch = zip(*line_pairs[batch_start:batch_start+batch_size])\n", "#    if len(x_batch) < batch_size:\n", "#      break\n", "    batches.append((x_batch, y_batch))\n", "\n", "  if sort_and_shuffle:\n", "    random.shuffle(batches)\n", "  return\n", "\n", "def add_sos_eos(tokens):\n", "  return map(lambda token_list: [SOS_ID] + token_list + [EOS_ID], tokens)\n", "\n", "def padded(tokens, depth):\n", "  maxlen = max(map(lambda x: len(x), tokens))\n", "  align = pow(2, depth - 1)\n", "  padlen = maxlen + (align - maxlen) % align\n", "  return map(lambda token_list: token_list + [PAD_ID] * (padlen - len(token_list)), tokens)\n", "\n", "\n", "def get_tokenizer(flags):\n", "  if flags.tokenizer.lower() == 'bpe':\n", "    return bpe_tokenizer\n", "  elif flags.tokenizer.lower() == 'char':\n", "    return char_tokenizer\n", "  elif flags.tokenizer.lower() == 'word':\n", "    return basic_tokenizer\n", "  else:\n", "    raise\n", "  return tokenizer\n", "\n", "\n", "tf.app.flags.DEFINE_float(\"learning_rate\", 0.0003, \"Learning rate.\")\n", "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.95, \"Learning rate decays by this much.\")\n", "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 10.0, \"Clip gradients to this norm.\")\n", "tf.app.flags.DEFINE_float(\"dropout\", 0.15, \"Fraction of units randomly dropped on non-recurrent connections.\")\n", "tf.app.flags.DEFINE_integer(\"batch_size\", 128, \"Batch size to use during training.\")\n", "tf.app.flags.DEFINE_integer(\"epochs\", 40, \"Number of epochs to train.\")\n", "tf.app.flags.DEFINE_integer(\"size\", 400, \"Size of each model layer.\")\n", "tf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\n", "tf.app.flags.DEFINE_integer(\"max_vocab_size\", 40000, \"Vocabulary size limit.\")\n", "#tf.app.flags.DEFINE_integer(\"max_seq_len\", 200, \"Maximum sequence length.\")\n", "tf.app.flags.DEFINE_integer(\"max_seq_len\", 100, \"Maximum sequence length.\")\n", "tf.app.flags.DEFINE_string(\"data_dir\", \"./\", \"Data directory\")\n", "tf.app.flags.DEFINE_string(\"train_dir\", \"./\", \"Training directory.\")\n", "tf.app.flags.DEFINE_string(\"tokenizer\", \"CHAR\", \"BPE / CHAR / WORD.\")\n", "tf.app.flags.DEFINE_string(\"optimizer\", \"adam\", \"adam / sgd\")\n", "tf.app.flags.DEFINE_integer(\"print_every\", 1, \"How many iterations to do per print.\")\n", "FLAGS = tf.app.flags.FLAGS\n", "\n", "def create_model(session, vocab_size, forward_only):\n", "  model = NLCModel(\n", "      vocab_size, FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\n", "      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor, FLAGS.dropout,\n", "      forward_only=forward_only, optimizer=FLAGS.optimizer)\n", "  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n", "  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n", "    logging.info(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n", "    model.saver.restore(session, ckpt.model_checkpoint_path)\n", "  else:\n", "    logging.info(\"Created model with fresh parameters.\")\n", "    session.run(tf.initialize_all_variables())\n", "    logging.info('Num params: %d' % sum(v.get_shape().num_elements() for v in tf.trainable_variables()))\n", "  return model\n", "\n", "\n", "def validate(model, sess, x_dev, y_dev):\n", "  valid_costs, valid_lengths = [], []\n", "  for source_tokens, source_mask, target_tokens, target_mask in pair_iter(x_dev, y_dev, FLAGS.batch_size, FLAGS.num_layers):\n", "    cost = model.test(sess, source_tokens, source_mask, target_tokens, target_mask)\n", "    valid_costs.append(cost * target_mask.shape[1])\n", "    valid_lengths.append(np.sum(target_mask[1:, :]))\n", "  valid_cost = sum(valid_costs) / float(sum(valid_lengths))\n", "  return valid_cost\n", "\n", "\n", "def train():\n", "  \"\"\"Train a translation model using NLC data.\"\"\"\n", "  # Prepare NLC data.\n", "  logging.info(\"Preparing NLC data in %s\" % FLAGS.data_dir)\n", "\n", "  x_train, y_train, x_dev, y_dev, vocab_path = prepare_nlc_data(\n", "    FLAGS.data_dir + '/' + FLAGS.tokenizer.lower(), FLAGS.max_vocab_size,\n", "    tokenizer=get_tokenizer(FLAGS))\n", "  vocab, _ = initialize_vocabulary(vocab_path)\n", "  vocab_size = len(vocab)\n", "  logging.info(\"Vocabulary size: %d\" % vocab_size)\n", "\n", "  file_handler = logging.FileHandler(\"{0}/log.txt\".format(FLAGS.train_dir))\n", "  logging.getLogger().addHandler(file_handler)\n", "\n", "  print(vars(FLAGS))\n", "  with open(os.path.join(FLAGS.train_dir, \"flags.json\"), 'w') as fout:\n", "    json.dump(FLAGS.__flags, fout)\n", "\n", "  with tf.Session() as sess:\n", "    logging.info(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n", "    model = create_model(sess, vocab_size, False)\n", "\n", "    logging.info('Initial validation cost: %f' % validate(model, sess, x_dev, y_dev))\n", "\n", "    if False:\n", "      tic = time.time()\n", "      params = tf.trainable_variables()\n", "      num_params = sum(map(lambda t: np.prod(tf.shape(t.value()).eval()), params))\n", "      toc = time.time()\n", "      print (\"Number of params: %d (retreival took %f secs)\" % (num_params, toc - tic))\n", "\n", "    epoch = 0\n", "    best_epoch = 0\n", "    previous_losses = []\n", "    exp_cost = None\n", "    exp_length = None\n", "    exp_norm = None\n", "    total_iters = 0\n", "    start_time = time.time()\n", "    while (FLAGS.epochs == 0 or epoch < FLAGS.epochs):\n", "      epoch += 1\n", "      current_step = 0\n", "\n", "      ## Train\n", "      epoch_tic = time.time()\n", "      for source_tokens, source_mask, target_tokens, target_mask in pair_iter(x_train, y_train, FLAGS.batch_size, FLAGS.num_layers):\n", "        # Get a batch and make a step.\n", "        tic = time.time()\n", "\n", "        grad_norm, cost, param_norm = model.train(sess, source_tokens, source_mask, target_tokens, target_mask)\n", "\n", "        toc = time.time()\n", "        iter_time = toc - tic\n", "        total_iters += np.sum(target_mask)\n", "        tps = total_iters / (time.time() - start_time)\n", "        current_step += 1\n", "\n", "        lengths = np.sum(target_mask, axis=0)\n", "        mean_length = np.mean(lengths)\n", "        std_length = np.std(lengths)\n", "\n", "        if not exp_cost:\n", "          exp_cost = cost\n", "          exp_length = mean_length\n", "          exp_norm = grad_norm\n", "        else:\n", "          exp_cost = 0.99*exp_cost + 0.01*cost\n", "          exp_length = 0.99*exp_length + 0.01*mean_length\n", "          exp_norm = 0.99*exp_norm + 0.01*grad_norm\n", "\n", "        cost = cost / mean_length\n", "\n", "        if current_step % FLAGS.print_every == 0:\n", "          logging.info('epoch %d, iter %d, cost %f, exp_cost %f, grad norm %f, param norm %f, tps %f, length mean/std %f/%f' %\n", "                (epoch, current_step, cost, exp_cost / exp_length, grad_norm, param_norm, tps, mean_length, std_length))\n", "      epoch_toc = time.time()\n", "\n", "      ## Checkpoint\n", "      checkpoint_path = os.path.join(FLAGS.train_dir, \"best.ckpt\")\n", "\n", "      ## Validate\n", "      valid_cost = validate(model, sess, x_dev, y_dev)\n", "\n", "      logging.info(\"Epoch %d Validation cost: %f time: %f\" % (epoch, valid_cost, epoch_toc - epoch_tic))\n", "\n", "      if len(previous_losses) > 2 and valid_cost > previous_losses[-1]:\n", "        logging.info(\"Annealing learning rate by %f\" % FLAGS.learning_rate_decay_factor)\n", "        sess.run(model.learning_rate_decay_op)\n", "        model.saver.restore(sess, checkpoint_path + (\"-%d\" % best_epoch))\n", "      else:\n", "        previous_losses.append(valid_cost)\n", "        best_epoch = epoch\n", "        model.saver.save(sess, checkpoint_path, global_step=epoch)\n", "      sys.stdout.flush()\n", "\n", "train()"], "metadata": {"_cell_guid": "65d79171-dce0-4672-8301-a4d21447820a", "_uuid": "a4d3a726d0ec24ff2dbf681cb1e694e7ec07541d"}, "cell_type": "code", "outputs": [], "execution_count": null}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "file_extension": ".py", "mimetype": "text/x-python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1}