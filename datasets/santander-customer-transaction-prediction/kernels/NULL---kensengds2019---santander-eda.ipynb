{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sb \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc   # area under precision-recall-curve\n\n\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f09d14e9d693e0d86079a195a4d54f573a0cc6ac"},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09bf80b3a2e9557435c1e3748c2f0c6a9d36a2f3"},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fbad91059e1f8889e2d36e0f4d44c17f2fc0d3e"},"cell_type":"markdown","source":"Excluding the ID column, there are 200 features in the training dataset."},{"metadata":{"trusted":true,"_uuid":"4a37bdb89cbd8bb2c82e393b5afa120da4060a05"},"cell_type":"code","source":"#check datatypes\ntrain_data.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"895faa3d679deb1ff81089727359ffaefdbc90f9"},"cell_type":"markdown","source":"there is one object column, which is the ID columns. there is one int64 column, which is the target column. Other than that, every column is float64."},{"metadata":{"trusted":true,"_uuid":"9e04d4cc3cdb071247d682ca6bdbd5ac1b79abf3"},"cell_type":"code","source":"#split the dataset into features and target\nfeatures = train_data.iloc[:,2:]\ntarget = train_data.iloc[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6671fedaa2ebcff0d49dd8a4660e47bc6ddc609"},"cell_type":"code","source":"#distribution of labels\nsb.countplot(target)\n\ntarget.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a7b35ab1c7a05f3eb7c14e525703b018643b11c"},"cell_type":"markdown","source":"The labels are heavily skewed; majority of datapoints have the target of \"0\". around 10% have the target of \"1\". "},{"metadata":{"trusted":true,"_uuid":"37e179f43b726d6f0e10dd262672ebe5d34bfbdb"},"cell_type":"code","source":"# check for missing values\ntrain_data.isnull().sum().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cd185cd1efeb99aa7dea29cf7b71a99f987e41b"},"cell_type":"markdown","source":"There are no missing values in training set."},{"metadata":{"trusted":true,"_uuid":"fee25d5699d11068f0b0f690333043c61abf4c11"},"cell_type":"code","source":"train_data.hist(figsize = (20,20))\n\nplt.tight_layout","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9359ede16fb6c1f4b817baccd12a10adfd8591a9"},"cell_type":"markdown","source":"As shown in plot, the target variables is skewed but other than that other columns seem to follow the bell curve roughly. "},{"metadata":{"trusted":true,"_uuid":"216c9ac95774fa96acff0020f870f29b57c89061"},"cell_type":"code","source":"#Standard scaling\n\nscaler = StandardScaler()\nfeatures_scaled = pd.DataFrame(scaler.fit_transform(features), columns= features.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"480b83baa9df7e6372e248fd59ba95163eeba94a"},"cell_type":"code","source":"#PCA \npca = PCA()\npca.fit(features_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea5822306cd951c89c0b26c9863f9f83c4172384"},"cell_type":"code","source":"plt.plot(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea684abf56ac9e50ec2d1e7ec8f9aed421694469"},"cell_type":"code","source":"def show_pca_variance(pca_obj, desired_ratio = None):\n    \n    var = pca_obj.explained_variance_ratio_\n    cumu = var.cumsum()\n    \n    f, ax1 = plt.subplots()\n    ax1.set_xlabel('n_components')\n    ax1.set_ylabel('ratio of explained variance')\n    ax1.plot(var)\n    \n    ax2 = ax1.twinx()\n    \n    \n    ax2.set_ylabel('cumulative variance')\n    ax2.plot(cumu, color = 'tab:red')\n    f.tight_layout()\n    \n    #get x such that x number of components retain the desired ratio of variance.\n    if desired_ratio != None:\n        x = np.where(cumu > desired_ratio)[0][0] + 1\n        plt.axvline(x = x, color = 'tab:green')\n        print(f' {desired_ratio*100}% of variability can be achieved by minimum of {x} components, or {x * 100/pca_obj.n_components_:.2f}% of total features')\n    \n    plt.show()\n     \n    return cumu\n\n#sort weights\ndef sorted_weights_pca(pca_obj, i, features, k ):\n    \n    # first create a dictionary that contain map components weights to feature names.\n    dict_feature_weight = {}\n    \n    for j in range(len(pca_obj.components_[i])):\n        dict_feature_weight[features[j]] = [pca_obj.components_[i][j]]\n        \n    \n    # dataframe\n    df = pd.DataFrame( data = dict_feature_weight)\n    df = df.T\n    df.columns = ['weights']\n    \n    df = df.sort_values(by = 'weights' , ascending=False)\n    \n    #plot the top 10 features\n    fig = plt.figure(1)\n    plt.figure(figsize = (10,5))\n    \n        \n    plt.subplot(121)\n    plt.barh(np.arange(k), df['weights'][-k:])\n    plt.yticks(np.arange(k), tuple(df.index)[-k::])\n    \n    plt.subplot(122)\n    plt.barh(np.arange(k), df['weights'][:k])\n    plt.yticks(np.arange(k), tuple(df.index)[:k:])\n    \n    plt.tight_layout()\n    \n    #print(tuple(df.index))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"4cde54d25892d9e67adc531e433c3e28aa5bcc03"},"cell_type":"code","source":"show_pca_variance(pca, desired_ratio= 0.99);\nshow_pca_variance(pca, desired_ratio= 0.95);\nshow_pca_variance(pca, desired_ratio= 0.90);\nshow_pca_variance(pca, desired_ratio= 0.85);\nshow_pca_variance(pca, desired_ratio= 0.80);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"accf3a966445919ba80f5f8d367adbb53013b085"},"cell_type":"markdown","source":"after looking at the explained variance ratios, I decide to refit the PCA with 179 components. This reduces 10.5% number of features and still retain 90% variability in the data. "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"74f2a8335c0b9031716f5b13f7312bfcc9daf56a"},"cell_type":"code","source":"#refit pca\npca = PCA(n_components= 179)\npca.fit(features_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ae1bec2dc883a7686c15773daad16147aff131b"},"cell_type":"code","source":"features_scaled_pca = pca.transform(features_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26d8891332c9e64ea7aa40eb38a5d9736801f44a"},"cell_type":"code","source":"features_scaled_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c65f7cc1def394aeec7f1ee7d6616be6b7ce4c7a"},"cell_type":"code","source":"#fit a logistic model. this serves as our baseline model.\n\nmodel = LogisticRegression(random_state=42, solver= \"liblinear\")\n\nmodel.fit(features_scaled_pca,target)\n\n#model.score(features_scaled_pca, target)\n\n#alternaive, we can first use predict(), then accuracy_score().\ntrain_predict = model.predict(features_scaled_pca)\naccuracy_score(target, train_predict)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c23305f095eafaad8ad84153c71858057efbe59"},"cell_type":"markdown","source":"0.91451 seems high percentage of accuarcy. However, since the target is heavily skewed (more than 90% of the labels are \"0\") accuarcy is not a good measure. Let's look at precisioin, recalls, and f score."},{"metadata":{"trusted":true,"_uuid":"c4cd560ea1a0bcbc6fbac1288e02f5667c48527b"},"cell_type":"code","source":"#precision\n# tp/(tp + fp)\n# out of all the datapoints we identified as positive, how many of them are right?\n# measures ability to \n# 0.69 means every 100 positives, 69 of them are correct. \n\nprecision_score(target, train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2598d2f58ed61eb6a6967a1a2ce77cf1a5335993"},"cell_type":"code","source":"#recalls\n#tp/(tp + fn)\n#out of all the datapoints that are actually positive, how many of them get identified as positive?\n# measures the ability to \n# 0.27 means every 100 datapoints that are actually positive, 27 of them would get identified correctly.\n\nrecall_score(target, train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0f1e7d0b342641b7c809f0220e049215e0d2404"},"cell_type":"code","source":"#given equal weights on precision and recall\n\nfbeta_score(target, train_predict, beta = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb5f4f13ffe928a2941767a1d6c869e52cbb9195"},"cell_type":"code","source":"#or alternative, this function output precision, recall, f score and support altogether. \nprecision_recall_fscore_support(target, train_predict, beta=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da5a2172ac51f331a6b0a2431856bdf2b4514350"},"cell_type":"code","source":"#confusion matrix\n#C_ij: number of observations known to be in group i but predicted to be in group j.\n\nsb.heatmap(confusion_matrix(target, train_predict), cmap=\"YlGnBu\")\nconfusion_matrix(target, train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed3c2aba05522c74b35f439e4ad8192de4a830a5"},"cell_type":"code","source":"train_predict_prob = model.predict_proba(features_scaled_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d207be7ff005474899a1f3a2d41c805cec6d7a6a"},"cell_type":"code","source":"#ROC curve\n\nfpr, tpr, thresholds = roc_curve(target, train_predict_prob[:,1])\n\n\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\n\nauc = roc_auc_score(target, train_predict_prob[:,1])\n\nprint(f'auc: {auc}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cad5daf035363f0aa6d7acf2e5e52fdb1bd4927b"},"cell_type":"code","source":"#If positive class is rare or if we care more about false positive than false negative, use Precision/Recall curve instead.\n\npre, rec, thres = precision_recall_curve(target, train_predict_prob[:,1])\n\nauc_pr = auc(rec, pre)\n\nprint(f'auc precision recall curve: {auc_pr}')\n\nplt.plot(rec, pre)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5246ddd252d085c55fc025a8a1964fbfa0305c9"},"cell_type":"markdown","source":"this shows that while ROC shows good result, the precision recall curve is showing the model is just slightly better than random guess. "},{"metadata":{"_uuid":"c688f2ab0729a53db55a5dfeb13474b069c23213"},"cell_type":"markdown","source":"Now the initial investigation is complete. Let's work on model selections. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}