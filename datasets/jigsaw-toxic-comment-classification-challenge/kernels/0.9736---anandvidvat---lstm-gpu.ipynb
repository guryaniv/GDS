{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n'''\nkeras for deep learning models\n\nPreprocessing Imports\n'''\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n'''\nDifferent Neural Network Layers\n'''\nfrom keras.layers import Dense,  Input, GlobalMaxPooling1D\nfrom keras.layers import CuDNNGRU, MaxPool1D, Embedding, Bidirectional\nfrom keras.layers import Dropout, SpatialDropout1D\n'''\nBuild Model\n'''\nfrom keras.models import Model\n# ROC Curve\nfrom sklearn.metrics import roc_auc_score\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#set configurations and dimensions \n\nMAX_SEQUENCE_LENGTH = 200\nMAX_VOCAB_SIZE = 20000\n\nVALIDATION_SPLIT = 0.2\nEMBEDDING_DIM = 300\nBATCH_SIZE = 1000\nEPOCHS = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a79314e76d902e8691acdadf2a6dd1bf0b63e09b"},"cell_type":"code","source":"# Path to data \ntrain_data_path = '../input/jigsaw-toxic-comment-classification-challenge/train.csv'\ntest_data_path = '../input/jigsaw-toxic-comment-classification-challenge/test.csv'\n\n# path to GloVe\nglove_path = '../input/glove6b/glove.6B.{0}d.txt'.format(EMBEDDING_DIM)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b28e67043f5b890fc80919efe20554c56970f6fe"},"cell_type":"code","source":"'''\nloading word2vectors from GloVe\n'''\nprint ('loading word2vec...')\n\nword2vec = {}\n\nwith open(os.path.join(glove_path), encoding='utf8') as fs:\n    for line in fs:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint ('number of vectors : {0}'.format(len(word2vec)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bbee7c4518899e2cc7ab3ce6beefb091ab2b6bc"},"cell_type":"code","source":"'''\nloading training data\n'''\ntrain_data = pd.read_csv(train_data_path)\ntest_data = pd.read_csv(test_data_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8395c442fddeb67797a292f8cad89c4f4c34dcd5"},"cell_type":"code","source":"#loading all row wise comment_text data into sentences\nsentences = train_data['comment_text'].fillna('DUMMY_VALUES').values\n# storing labels intp possible_labels\npossible_labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n# loading all row wise possible_labels into target\ntargets = train_data[possible_labels].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"970953da810f207dc4dbe21b2316eebb9f4f1b99"},"cell_type":"code","source":"'''\n    converting sentences into interger sequences\n\n'''\n# initialize tokenizer\ntokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE)\n# downsizing or fitting the sentences into respective tokens\ntokenizer.fit_on_texts(sentences)\n# transforming text to integer sequences \nsequences = tokenizer.texts_to_sequences(sentences)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddb8ce73103a88ad6b3990c1f2649aab683fe888"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e18d871b3a740779df72ee76fc632b9897045725"},"cell_type":"code","source":"# map word to integer [indexing]\nword_index = tokenizer.word_index\n# number of unique words\nprint(len(word_index))\n# type \nprint(type(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"394dd2615eeb20fd279d8e94ec6386f33f625730"},"cell_type":"code","source":"# convert all different input sizes into constant size of max_sequence_length\ndata = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)\n# checking shape of data\nprint('shape of data {0}'.format(data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5d3bfd5c999bc853c53418a5a331e7fcd0e48df"},"cell_type":"code","source":"# preparing embedding matrix \nprint('Filling pre-trained embeddings...')\n\nnum_words = min(MAX_VOCAB_SIZE,len(word_index)+1)\n\n# initially populate embedding matrix to be all zeros\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    if i < MAX_VOCAB_SIZE:\n        embedding_vector = word2vec.get(word)\n        \n        if embedding_vector is not None:\n            # words which are found will be updated\n            embedding_matrix[i] = embedding_vector\n\n#shape of embedding_matrix\nprint('shape of embedding matrix is {0}'.format(embedding_matrix.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"333213b68221ef76e450f2ad9030c972b4c6ad05"},"cell_type":"code","source":"# creating a embeddings object for neural net using pretrained weights\nembedding_layer = Embedding(\nnum_words,\nEMBEDDING_DIM,\nweights =[embedding_matrix],\ninput_length = MAX_SEQUENCE_LENGTH,\ntrainable = False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e5efc40f9ca9e39c041511b8cccb6d674e16c38"},"cell_type":"code","source":"print('Building the Model...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdf634f675225142b7d6ee49d7caaecaf71d8471"},"cell_type":"code","source":"input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\n\nx = embedding_layer(input_)\n\nx = Bidirectional(CuDNNGRU(50, return_sequences= True))(x)\n\nx = SpatialDropout1D(0.1)(x)\n\nx = GlobalMaxPooling1D()(x)\n\nx = Dense(128, activation ='relu')(x)\n\nx = Dropout(0.2)(x)\n\noutput = Dense(len(possible_labels), activation = 'sigmoid')(x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93c7cfd4dd103acd688c8f5c03c431f8941aa8ba"},"cell_type":"code","source":"model = Model(input_, output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c4f5408dc0c7c94dd32dc7950b72fbf5b674879"},"cell_type":"code","source":"model.compile( \nloss = 'binary_crossentropy',\noptimizer = 'adam',\nmetrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3212adb5c9875c08d0dd825444ffb185475d93d"},"cell_type":"code","source":"print('Training Model...')\nr = model.fit(\n    data,\n    targets,\n    batch_size = BATCH_SIZE,\n    epochs = EPOCHS,\n    validation_split = VALIDATION_SPLIT\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6ef27764cbc8dc8a558c32e91fe015c0a29a715"},"cell_type":"code","source":"test_sentences = test_data['comment_text'].fillna('DUMMY_VALUES').values\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_feed = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\npredict = model.predict(test_feed)\nsubmission_path = '../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv'\n\nsubmission  = pd.read_csv(submission_path)\n\nsubmission[possible_labels] = predict\n\nsubmission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}