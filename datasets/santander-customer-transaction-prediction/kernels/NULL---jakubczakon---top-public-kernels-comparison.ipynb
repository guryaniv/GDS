{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"High all,\n\nI figured it would be interesting to compare the top scoring public kernels.\n\nThe analysis is also available(and may be updated) [here](https://ui.neptune.ml/jakub-czakon/santander/wiki/1-public_kernel_comparison).\n\n# Comparison Setup\n\n## Kernels considered:\n- https://www.kaggle.com/mhviraf/santander-compact-solution-14-lines-will-do\n- https://www.kaggle.com/kamalchhirang/simple-lightgbm-with-good-parameters\n- https://www.kaggle.com/sandeepkumar121995/magic-parameters\n- https://www.kaggle.com/jesucristo/30-lines-starter-solution-fast\n- https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works\n- https://www.kaggle.com/lavanyadml/santander-ls\n- https://www.kaggle.com/jesucristo/santander-magic-lgb\n- https://www.kaggle.com/gpreda/santander-fast-compact-solution\n- https://www.kaggle.com/jesucristo/40-lines-starter-solution-fast DELETED\n\nI hope I didn't forget to upvote/fork any of those kernels. Good job people!\n\n## Validation:\nI adjusted all of them so that they would have the same validation schema:\n\n    N_SPLITS = 15\n    SEED = 2319   \n    folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=False, random_state=SEED)\n    \n## Tracking\nI logged all the experiment information to [Neptune](http://bit.ly/2FndEZO) to be able to easily compare it later:\n  - hyperparameters\n  - lightgbm training curves\n  - roc_auc metric on out of fold predictions\n  - confusion matrix\n  - ROC AUC curve\n  - prediction distribution plot\n \n**Note** \nYou don't have to know how to track stuff with Neptune to follow this post. \n\nIf you are interested, [here is an example neptune kernel](https://www.kaggle.com/jakubczakon/example-lightgbm-neptune-tracking).  \n[Neptune](http://bit.ly/2FndEZO) is free for non-organizations and you can easily use it from inside kaggle kernels (or any other place for that matter) to track your experiments. \n  \n# Results\nOk, let's explore the results.\n\n## Scores\nFirst, lets have a look at the validation results of those models:\n\n[Results dashboard](https://ui.neptune.ml/jakub-czakon/santander/experiments?filterId=288495a5-c965-4896-815c-7474fd95234f)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison8.png)\n\nAll the top kernels, which is no surprise, perform quite well and get us `Local CV &gt; 0.9`.\nThe results vary from `0.900464` to `0.901075`, which may not be a lot in a large picture of things, but for this particular competition could mean a difference of 1000+ places.\nIt seems that the `Random Shuffled Data Also Works` not only \"also works\" but works the best, which was surprising to me. \n\n## Hyperparameters\n\nLooking at the parameters in the table above it seems that every kernel apart from the best one used `num_leaves=13`. Interesting.\nTo get more insights I decided to use the `plot_evaluations` function from the `skopt.plots` package: \n\n[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison2.png)\n\nIt's a pretty useful tool, that lets you see what were the hyperparameter values that were explored, and which values were explored the most. The red dot shows the best run. \n\nSometimes, you can see clear feature interactions (in the hyperparameter space) or the over/underexplored areas in the hyperparameter space. \nIn this particular case my conclusions are:\n- explore other `num_leaves` values between `3` and `13`\n- higher bagging fraction perform better\n- there seems to be a sweet spot when it comes to `early_stopping_rounds=3000`\n\n## Learning curves\nSince I logged all the metrics during training with the `neptune_monitor` callback we can try and get some insights here. If you want to see how to create a custom callback for your tracking tool/setup [go to this example kernel](https://www.kaggle.com/jakubczakon/example-lightgbm-neptune-tracking).\nLet's start with the comparison of the learning curves for all the experiments:\n\n[Learning curves comparison](https://ui.neptune.ml/jakub-czakon/santander/compare?shortId=%5B%22SAN1-84%22%2C%22SAN1-59%22%2C%22SAN1-124%22%2C%22SAN1-142%22%2C%22SAN1-56%22%2C%22SAN1-140%22%2C%22SAN1-141%22%2C%22SAN1-58%22%2C%22SAN1-57%22%5D)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison9.png)\n\nApart from the `Randomly Shuffled Data Also Works` all those curves are quite similar. We can see that the \"shuffled\" gets to 0.9 slower but is overfitting considerably less. That is the result of the `num_leaves=3` but maybe shuffling features somehow plays a role here as well. It is probably worth exploring.\n\nLet's now take a closer look at the learning curves of the top two kernels `Randomly Shuffled Data Also Works` and `Magic Parameters`.\n\n**`Randomly Shuffled Data Also Works`**\n\n[Experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-84/charts)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison12_random_shuffle.png)\n\nWe can see that there is some overfitting but not a lot. Training curves are almost identical for each fold, but when we look at the validation curves there is quite a lot of difference:\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison13_random_shuffle.png)\n\nThe worst result was just 0.889 for fold 4 while the best one got 0.9111 on fold 13. \n\n**`Magic Parameters`**\nLooking at this kernel:\n\n[Experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-59/charts)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison10_magic_lgb.png)\n\nOverfitting is significantly more visible here.\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison11_magic_lgb.png)\n\nThe difference between best and worst folds is pretty much the same `fold 4 0.889` vs `fold 13 0.910`.\n\nMy conclusions are:\n - `num_leaves=3` gives better results and controls overfitting, maybe other regularization params like l1/l2 could push the score up a bit more.\n - the difference between best and worst folds is large.  Maybe the investigation of this problem can reveal insights about the underlying structure of the problem.\n\n## Prediction correlations\nLet's look at the prediction correlations for out of fold train predictions and averaged test predictions respectively:\n\n[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison3.png)\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison4.png)\n\nBoth for the train oof and test predictions, the correlations between public kernels are extremely high.\nIt seems that it will be very difficult to squeeze extra juice from blending/stacking those models.\n\nMy conclusions are:\n- we may need to add diversity to gain anything by blending\n\n## Predictions exploration\n\nLet's start by looking at the standard model diagnostics for the best model:\n\n[Best experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-84/channels)\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/4d52325ae3ac0f2aeabf406a6c62a650b7f1b0c3/comparsion15.png)\n\nOk, it looks like it has trouble with giving a very low score to positive examples. \n\nLet's take a look at the predictions that public kernels produce. \n\nI will begin by looking at the error distribution plot. I calculated it simply by substructing `prediction` from `target`.\n\n[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison1.png)\n\nOk, it seems that our model is pretty good when it comes to predicting negative cases but has a lot of problems when it comes to predicting positive cases. Not only that, but it is quite confidently wrong, giving very low scores for some positive cases. \n\nNow, I would like to take a look at the predictions themselves.\n[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparsion7.png)\n\nAnd if we split it into positive and negative examples respectively:\n[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1It is not super important to know how-170/channels)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison6.png)\n\n[Exploration experiment](https://ui.neptune.ml/jakub-czakon/santander/e/SAN1-170/channels)\n\n![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/2507ca2f4ff706b4a358decf0ba714ead7010a0b/comparison5.png)\n\nEven though the prediction distributions at the first sight look very similar there could still be benefits to blending.\nEspecially those positive predictions differ from model to model quite a bit.\n\nMy conclusions are:\n- maybe there is some LB sauce that can be squeezed from blending after all\n- we should investigate how to make our model less sure when it is wrong about the positive cases (potentially add some models that do)\n\n# Final thoughts:\n- There is probably room to explore on the hyperparameter front\n- Difference between worst and best fold results is huge. Exploring it could bring insights\n- There is probably to blending/stacking public kernels but adding diversity may be important.\n\nWhat do you think?\n\n\n# Edits:\n\nThank you @tilii7 for cleaning my thought.\nAdded: \"Not only that, it is quite confidently wrong (giving very low scores) for some positive cases.\"  \nDropped: \"In simple words, some positive cases are given very low predictions by our models. \""},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}