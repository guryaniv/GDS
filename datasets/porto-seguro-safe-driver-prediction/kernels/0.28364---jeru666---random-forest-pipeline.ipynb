{"cells": [{"metadata": {"_uuid": "8a58dba16746f81e1a955f55aa2f3ae777423d3f", "_cell_guid": "65e40b62-9f77-4f12-a1d4-3987cfc503c0"}, "cell_type": "markdown", "source": ["In this edited version of my kernel, I have included some new features and some others are under progress. Some have been influenced from [THIS KERNEL](https://www.kaggle.com/sudosudoohio/stratified-kfold-xgboost-eda-tutorial-0-281).\n", "\n", "# What lies ahead of you?\n", "\n", "*  **Data Exploration**\n", "    * Analyzing Datatypes\n", "    * Analyzing Missing Values\n", "    * Visualizing missing values\n", "    * Memory Usage Analysis\n", "    \n", "*  **Data Analysis** (visualizing each and every type of feature in the data set)\n", "    * Splitting columns based on types\n", "    * Binary Features\n", "    * Categorical Features\n", "    * Continuous/Ordinal Features\n", "    * Correlation (**ps_calc** have an outrageous attitude!!!)\n", "* **Feature Engineering** \n", "    * New Binary features\n", "    * New Continuous/Ordinal features (*in progress*)\n", "*  **Modeling**\n", "    * Gradient Boosting\n", "    * XGBoost"]}, {"metadata": {"_uuid": "a56251bda185c7171eba71d63e25283616809d4a", "_cell_guid": "bfa0a3f1-2ef3-4cc1-8765-90dcebe10e09"}, "cell_type": "markdown", "source": ["# Importing Libraries and Loading Data"]}, {"metadata": {"_uuid": "23743af38f0ec256b8c02169a62340606707c134", "collapsed": true, "_cell_guid": "d61160ed-4ab6-4fee-9769-95ed0079280f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["import numpy as np # linear algebra\n", "import seaborn as sns\n", "import missingno as msno\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from matplotlib import pyplot as plt\n", "from subprocess import check_output\n", "from sklearn import *\n", "import xgboost as xgb\n", "from multiprocessing import *\n", "from ggplot import *\n", "\n", "df_train = pd.read_csv('../input/train.csv')\n", "df_test = pd.read_csv('../input/test.csv')\n", "df_sample = pd.read_csv('../input/sample_submission.csv')\n", "# Any results you write to the current directory are saved as output."]}, {"metadata": {"_uuid": "6ae2e0b6c56cfb4ee5ec84c602444cd6c85e2fae", "collapsed": true, "_cell_guid": "79213fd9-8be7-458d-91e3-61d10673a28a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_train.shape"]}, {"metadata": {"_uuid": "0ebe47c489a6672491194ec0db2b31af53559217", "collapsed": true, "_cell_guid": "853fab44-f65e-40f9-b2ce-8900a6414cc5"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(len(df_train.columns))\n", "#new_cont_ord_cols = [c for c in df_train.columns if not c.startswith('ps_calc_')]\n", "new_cont_ord_cols = [c for c in df_train.columns if not c.endswith('bin')]\n", "no_bin_cat_cols = [c for c in new_cont_ord_cols if not c.endswith('cat')][2:]"]}, {"metadata": {"_uuid": "9ff8a646222ce9f2fcad53a1189fb14c70925172", "collapsed": true, "_cell_guid": "6aaa924e-5c50-45db-96bd-c691b56cf8ae"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "col = [c for c in train.columns if c not in ['id','target']]\n", "print(len(col))\n", "col = [c for c in col if not c.startswith('ps_calc_')]\n", "print(len(col))\n", "\n", "train = train.replace(-1, np.NaN)\n", "d_median = train.median(axis=0)\n", "d_mean = train.mean(axis=0)\n", "train = train.fillna(-1)\n", "one_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}\n", "\n", "'''"]}, {"metadata": {"_uuid": "f658449fe7fba95295adef06a04191ed1a8e4a18", "collapsed": true, "_cell_guid": "be37c1ac-0a1a-4e9c-8fa3-5cd6ed69dd6a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''\n", "def transform_df(df):\n", "    df = pd.DataFrame(df)\n", "    dcol = [c for c in df.columns if c not in ['id','target']]\n", "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n", "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n", "    for c in dcol:\n", "        if '_bin' not in c: #standard arithmetic\n", "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n", "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n", "            #df[c+str('_sq')] = np.power(df[c].values,2).astype(np.float32)\n", "            #df[c+str('_sqr')] = np.square(df[c].values).astype(np.float32)\n", "            #df[c+str('_log')] = np.log(np.abs(df[c].values) + 1)\n", "            #df[c+str('_exp')] = np.exp(df[c].values) - 1\n", "    for c in one_hot:\n", "        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\n", "            for val in one_hot[c]:\n", "                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n", "    return df\n", "\n", "def multi_transform(df):\n", "    print('Init Shape: ', df.shape)\n", "    p = Pool(cpu_count())\n", "    df = p.map(transform_df, np.array_split(df, cpu_count()))\n", "    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n", "    p.close(); p.join()\n", "    print('After Shape: ', df.shape)\n", "    return df\n", "\n", "def gini(y, pred):\n", "    fpr, tpr, thr = metrics.roc_curve(y, pred, pos_label=1)\n", "    g = 2 * metrics.auc(fpr, tpr) -1\n", "    return g\n", "\n", "def gini_xgb(pred, y):\n", "    y = y.get_label()\n", "    return 'gini', gini(y, pred)\n", "\n", "params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'seed': 99, 'silent': True}\n", "x1, x2, y1, y2 = model_selection.train_test_split(train, train['target'], test_size=0.25, random_state=99)\n", "\n", "x1 = multi_transform(x1)\n", "x2 = multi_transform(x2)\n", "test = multi_transform(test)\n", "\n", "col = [c for c in x1.columns if c not in ['id','target']]\n", "col = [c for c in col if not c.startswith('ps_calc_')]\n", "print(x1.values.shape, x2.values.shape)\n", "\n", "#remove duplicates just in case\n", "tdups = multi_transform(train)\n", "dups = tdups[tdups.duplicated(subset=col, keep=False)]\n", "\n", "x1 = x1[~(x1['id'].isin(dups['id'].values))]\n", "x2 = x2[~(x2['id'].isin(dups['id'].values))]\n", "print(x1.values.shape, x2.values.shape)\n", "\n", "y1 = x1['target']\n", "y2 = x2['target']\n", "x1 = x1[col]\n", "x2 = x2[col]\n", "\n", "watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n", "model = xgb.train(params, xgb.DMatrix(x1, y1), 5000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=50, early_stopping_rounds=200)\n", "test['target'] = model.predict(xgb.DMatrix(test[col]), ntree_limit=model.best_ntree_limit+45)\n", "test['target'] = (np.exp(test['target'].values) - 1.0).clip(0,1)\n", "\n", "sub = pd.DataFrame()\n", "sub['id'] = test['id']\n", "sub['target'] = test['target']\n", "sub.to_csv('xgb1.csv', index=False)\n", "\n", "#test[['id','target']].to_csv('xgb_submission.csv', index=False, float_format='%.5f')\n", "'''"]}, {"metadata": {"_uuid": "99b89f8ed83433947005e3c7cc7e36545599328d", "_cell_guid": "6854b2c5-4f52-4bbe-8bb5-470b709f370c"}, "cell_type": "markdown", "source": ["# Data Exploration\n", "\n", "First things first, let us explore what we have!"]}, {"metadata": {"_uuid": "c1518fd7b4ef43ee34be4d3faa8bdc9b854e3a0d", "collapsed": true, "_cell_guid": "4d6e5017-015f-4d74-b457-db69d7d19ea1"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_train.head()"]}, {"metadata": {"_uuid": "2dcebc1e8edbfeda80c4158ab75a116e8331a911", "_cell_guid": "9a620fd4-441d-49c3-8724-17ce7e18ca3f"}, "cell_type": "markdown", "source": ["Saving the **target** variable separately and dropping it from the training set."]}, {"metadata": {"_uuid": "671195790116b5317030db01b9616c2d9a356d1c", "collapsed": true, "_cell_guid": "05091036-b20e-470a-99c6-e75c4096f7bc"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["target = df_train['target']\n", "#df_train = df_train.drop('target', 1)"]}, {"metadata": {"_uuid": "6dd9111c3366668b3d40104d78e4a248fad8a488", "_cell_guid": "72a9741b-7170-4db5-a6cd-4c89ad290d0c"}, "cell_type": "markdown", "source": ["## Analyzing Datatypes\n", "\n", "We only have two datatypes in our dataset: **int** and **float**."]}, {"metadata": {"_uuid": "8f5cc2d12dedc01105c271530bf764885833f23f", "collapsed": true, "_cell_guid": "20cd566e-ae52-4c2a-bdbf-71039fe406ed"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(df_train.dtypes.unique())\n", "print(df_train.dtypes.nunique())\n", "\n", "print(df_test.dtypes.unique())\n", "print(df_test.dtypes.nunique())"]}, {"metadata": {"_uuid": "b179ec1687d97fefe4147ab87ac300fff1a80197", "collapsed": true, "_cell_guid": "1f1f17c5-962b-42b3-9436-9f425c5714a7"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["pp = pd.value_counts(df_train.dtypes)\n", "pp.plot.bar()\n", "plt.show()"]}, {"metadata": {"_uuid": "847774dfe1aa5f6b915653de8059e361541690b6", "_cell_guid": "2adf413c-f9d5-49f6-be8a-ffbe72486cc3"}, "cell_type": "markdown", "source": ["## Analyzing Missing Values"]}, {"metadata": {"_uuid": "7de41aabb54441fb75f2a0171a04b98c7f3ebad0", "collapsed": true, "_cell_guid": "1549d650-816b-443a-aa07-529b359eca0a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print (df_train.isnull().values.any())\n", "print (df_test.isnull().values.any())"]}, {"metadata": {"_uuid": "2da9a0a3f03aa5aa5b7a6f894e286e07a59c499f", "_cell_guid": "a36c890d-859e-4c37-a0e9-5b9bf2338eb5"}, "cell_type": "markdown", "source": ["However, as mentioned by someone in the comments, \"This isn't true!\" The missing values have been replaced by -1.\n", "\n", "We will replace them using np.nan and see how it is distributed.\n", "\n"]}, {"metadata": {"_uuid": "e8569604229ef89357bdca8f7a59d4faf80e8c62", "collapsed": true, "_cell_guid": "764bfa14-b9bf-4312-a826-02312334076f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#df_train.replace(-1, np.nan)\n", "#df_test.replace(-1, np.nan)\n", "df_train[(df_train == -1)] = np.nan\n", "df_test[(df_test == -1)] = np.nan\n", "\n", "print('done') "]}, {"metadata": {"_uuid": "8095b98db72ea138ad565a6be52f30bcc0b81131", "_cell_guid": "bac4c14e-c91d-4f77-8913-d4e8926434d6"}, "cell_type": "markdown", "source": ["Checking for missing values again"]}, {"metadata": {"_uuid": "4f58cf31aa682a46df44e3b5339e4461e112ed05", "collapsed": true, "_cell_guid": "fe6ca5a8-1f48-4d62-a10a-371a3fe08cb5"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print (df_train.isnull().values.any())\n", "print (df_test.isnull().values.any())   "]}, {"metadata": {"_uuid": "3a0a6025937e1489ec18c994e2309df177c0f5c3", "_cell_guid": "5b8c0b43-5207-4d91-a0fd-3d6f90677797"}, "cell_type": "markdown", "source": ["Printing list of columns with missing values in both the train and test dataframe:"]}, {"metadata": {"_uuid": "084a8dbf2d29ac5ed8be5761d5dcb5428cd66acb", "collapsed": true, "_cell_guid": "69ea56de-7e9d-4b31-90ab-db7308813bc6"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["cols_missing_val_train = df_train.columns[df_train.isnull().any()].tolist()\n", "print(cols_missing_val_train)\n", "print('\\n')\n", "\n", "cols_missing_val_test = df_test.columns[df_test.isnull().any()].tolist()\n", "print(cols_missing_val_test)"]}, {"metadata": {"_uuid": "e34029659734dd2eae8581733921b2c74398cfb3", "_cell_guid": "fceddfef-6cfc-4ad6-aa4e-107430b314ca"}, "cell_type": "markdown", "source": ["We see that the train dataframe has an extra column with missing values (**ps_car_12**).\n", "\n", "## Visualizing missing values"]}, {"metadata": {"_uuid": "65a0532b38ac77f4ee1fcfd39633b66e3d0f8450", "collapsed": true, "_cell_guid": "2e8d84ce-8f28-4e9b-a4ae-bdead8f174a0"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#--- Train dataframe ---\n", "msno.bar(df_train[cols_missing_val_train],figsize=(20,8),color=\"#19455e\",fontsize=18,labels=True,)"]}, {"metadata": {"_uuid": "f86d616eb207258e327ab5a5af92de060d905439", "collapsed": true, "_cell_guid": "bda5e206-0667-43db-9ea8-bc2188786e9e"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#--- Test dataframe ---\n", "msno.bar(df_test[cols_missing_val_test],figsize=(20,8),color=\"#50085e\",fontsize=18,labels=True,)"]}, {"metadata": {"_uuid": "34ee946f25678c096eab7af5652faa0ea3b7f471", "_cell_guid": "957d13b0-0b4e-4845-a5da-5478bc2de640"}, "cell_type": "markdown", "source": ["We can see that the missing values a proportional in both the test and train dataframes."]}, {"metadata": {"_uuid": "706f1f49217a75220d1a5ba3cde828293b195757", "collapsed": true, "_cell_guid": "30454b9e-9073-4cc4-a75d-57d47469e457"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#--- Train dataframe ---\n", "msno.matrix(df_train[cols_missing_val_train],width_ratios=(10,1),\\\n", "            figsize=(20,8),color=(0.2,0.2,0.2),fontsize=18,sparkline=True,labels=True)"]}, {"metadata": {"_uuid": "246f4861e54c5ad3e2f1b7fe53c3b5ce1d7b492e", "collapsed": true, "_cell_guid": "d6f21c58-9353-4f24-9d87-1f2ae178e62f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#--- Test dataframe ---\n", "msno.matrix(df_test[cols_missing_val_test],width_ratios=(10,1),\\\n", "            figsize=(20,8),color=(0.2,0.2,0.2),fontsize=18,sparkline=True,labels=True)"]}, {"metadata": {"_uuid": "a9dcf5864909adcf50bb576d23320990de9c4f08", "_cell_guid": "ed6f6b70-e6fe-4f83-bf48-39d523f438f9"}, "cell_type": "markdown", "source": ["We see a similar resemblance of proportional missing values in the train and test dataframes!"]}, {"metadata": {"_uuid": "4b0cf425a9bcf1920c69a1fb30c365077be7be1c", "_cell_guid": "cc0a0691-f8bf-40b5-8eb8-dd9139bda988"}, "cell_type": "markdown", "source": ["Replacing the missing values to -1."]}, {"metadata": {"_uuid": "9bd6e3b38aa061fc3ac25ba94baa25da3d17e064", "collapsed": true, "_cell_guid": "ac0e94f3-19ac-4a4d-8958-62b83851d9ad"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_train.replace(np.nan, -1, inplace=True)\n", "df_test.replace(np.nan, -1, inplace=True)"]}, {"metadata": {"_uuid": "ce7d79a652ffe0b377a3007151882f02bd10f61c", "_cell_guid": "048e11ec-6269-46a6-96f4-1bd52a852b55"}, "cell_type": "markdown", "source": ["## Memory Usage"]}, {"metadata": {"_uuid": "134235fdd1ee002ca52553e2ff00cd5c3bd025e7", "collapsed": true, "_cell_guid": "c8370db6-90d8-43a5-9ff3-71c15514112a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#--- memory consumed by train dataframe ---\n", "mem = df_train.memory_usage(index=True).sum()\n", "print(\"Memory consumed by training set  :   {} MB\" .format(mem/ 1024**2))\n", "print('\\n')\n", "#--- memory consumed by test dataframe ---\n", "mem = df_test.memory_usage(index=True).sum()\n", "print(\"Memory consumed by test set      :   {} MB\" .format(mem/ 1024**2))"]}, {"metadata": {"_uuid": "cd3a51aac56c31c45656e57cf6b7410521b4552e", "_cell_guid": "4967ee31-9786-482e-8a5d-7cb971b77079"}, "cell_type": "markdown", "source": ["By altering the datatypes we can reduce memory usage:"]}, {"metadata": {"_uuid": "d70b7771266e71347dd329398025bb2e0a579aa5", "collapsed": true, "_cell_guid": "b0a4959f-971c-4a60-9254-f9439435a578"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["def change_datatype(df):\n", "    float_cols = list(df.select_dtypes(include=['int']).columns)\n", "    for col in float_cols:\n", "        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n", "            df[col] = df[col].astype(np.int8)\n", "        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n", "            df[col] = df[col].astype(np.int16)\n", "        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n", "            df[col] = df[col].astype(np.int32)\n", "        else:\n", "            df[col] = df[col].astype(np.int64)\n", "\n", "change_datatype(df_train)\n", "change_datatype(df_test) "]}, {"metadata": {"_uuid": "76c8387f532ac92e00127203de2c7087b3e32bd8", "collapsed": true, "_cell_guid": "27cd988f-fece-40bf-991a-8ee80fa573dd"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#--- Converting columns from 'float64' to 'float32' ---\n", "def change_datatype_float(df):\n", "    float_cols = list(df.select_dtypes(include=['float']).columns)\n", "    for col in float_cols:\n", "        df[col] = df[col].astype(np.float32)\n", "        \n", "change_datatype_float(df_train)\n", "change_datatype_float(df_test)"]}, {"metadata": {"_uuid": "b3fa512371b42483a315abcf7258c0a2f3bd5cec", "_cell_guid": "47827cf2-3562-41d4-8483-9da10534a5be"}, "cell_type": "markdown", "source": ["Let us check the memory consumed again:"]}, {"metadata": {"_uuid": "e582f29d035799ddc9d4013e544eef5db287180b", "collapsed": true, "_cell_guid": "7d1b2ef8-459b-458e-b30c-a0ad6188bf91"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#--- memory consumed by train dataframe ---\n", "mem = df_train.memory_usage(index=True).sum()\n", "print(\"Memory consumed by training set  :   {} MB\" .format(mem/ 1024**2))\n", "print('\\n') \n", "#--- memory consumed by test dataframe ---\n", "mem = df_test.memory_usage(index=True).sum()\n", "print(\"Memory consumed by test set      :   {} MB\" .format(mem/ 1024**2))"]}, {"metadata": {"_uuid": "f12e8f6f7985dac071cb11c64b3b05f09852e020", "_cell_guid": "8b41a6f2-dbc8-4d50-b8a3-4005aee7c669"}, "cell_type": "markdown", "source": ["That is memory consumption reduced by **greater than 50%** !!!"]}, {"metadata": {"_uuid": "e5047aadbad6e54d4e91a8384b5e045e9ccb18a6", "collapsed": true, "_cell_guid": "2d7a9ee5-9716-4b01-afe0-f5e363bbc243"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(len(df_test.columns))\n", "print(len(df_train.columns))\n", "#print(len(target.columns))"]}, {"metadata": {"_uuid": "b78863353b1eb3c35fe409f47fd9ffc8109ac717", "_cell_guid": "290da136-b20b-485f-8b87-baf3322d3b94"}, "cell_type": "markdown", "source": ["# Quick Modeling (without any analysis)"]}, {"metadata": {"_uuid": "6ce0a0acd790d5157b21941ab9c97eb6a9600dac", "_cell_guid": "43071b6e-0efd-40c3-ba55-ae03eccdd0f3"}, "cell_type": "markdown", "source": ["Quick check to make sure the columns are the same in both `train` and `test` data."]}, {"metadata": {"_uuid": "f967aa56829f962ffe4f4b1cfab03f32cdf0ca59", "collapsed": true, "_cell_guid": "a64cb294-6a9b-4f47-9c0e-424fc993876f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["len(set(df_test.columns) and set(df_train.columns))"]}, {"metadata": {"_uuid": "3c098d2560e89e932e97fd28d333f975fef8576c", "_cell_guid": "1323f62f-40f8-41c0-8971-2f786e7a9ba3"}, "cell_type": "markdown", "source": ["## Random Forest"]}, {"metadata": {"_uuid": "fbc9436bf1521c4befe855c0114a2c9aa27e0b92", "collapsed": true, "_cell_guid": "92e55304-c4ee-4bc5-9a96-9589f5073b20"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_train = df_train.replace(-1, np.NaN)\n", "d_median = df_train.median(axis=0)\n", "d_mean = df_train.mean(axis=0)\n", "df_train = df_train.fillna(-1)\n", "\n", "dcol = [c for c in df_train.columns if c not in ['id','target']]\n", "df_train['ps_car_13_x_ps_reg_03'] = df_train['ps_car_13'] * df_train['ps_reg_03']\n", "#df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n", "for c in dcol:\n", "        if '_bin' not in c: #standard arithmetic\n", "            df_train[c+str('_median_range')] = (df_train[c].values > d_median[c]).astype(np.int)\n", "            df_train[c+str('_mean_range')] = (df_train[c].values > d_mean[c]).astype(np.int)\n", "            df_train[c+str('_sq')] = np.power(df_train[c].values,2).astype(np.float32)\n", "            #df[c+str('_sqr')] = np.square(df[c].values).astype(np.float32)\n", "            df_train[c+str('_log')] = np.log(np.abs(df_train[c].values) + 1)\n", "            df_train[c+str('_exp')] = np.exp(df_train[c].values) - 1"]}, {"metadata": {"_uuid": "5f79b4464fd0c0511e2c404aefdc0c3d4de637e3", "collapsed": true, "_cell_guid": "673b8eb0-f458-452c-b10e-f6c6908d79b7"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["change_datatype(df_train)"]}, {"metadata": {"_uuid": "dc9b8500679d498300a1d8a02f9b7a738cc61da2", "collapsed": true, "_cell_guid": "9688c31d-5736-4385-967a-4f41c593f9c9"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_train.head()"]}, {"metadata": {"_uuid": "e1b79a01d9fe6b286bc49eaa18701c4602ed4243", "collapsed": true, "_cell_guid": "1a8c54f6-f39a-4d05-b9b1-16869c537520"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "features= [c for c in df_train.columns.values if c  not in ['id', 'target']]\n", "#numeric_features= [c for c in df.columns.values if c  not in ['id','text','author','processed']]\n", "#target = 'author'\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(df_train[features], df_train['target'], test_size=0.33, random_state=42)\n", "X_train.head()"]}, {"metadata": {"_uuid": "046f73de0614a5d7a89a37da35d3793632f60c69", "collapsed": true, "_cell_guid": "dccec6f7-f368-48b4-ab98-cc391144ae7d"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "\n", "pipeline = Pipeline([\n", "    #('features',feats),\n", "    ('classifier', RandomForestClassifier(random_state = 42))\n", "    #('classifier', GradientBoostingClassifier(random_state = 42))\n", "])\n", "\n", "pipeline.fit(X_train, y_train)\n", "\n", "preds = pipeline.predict(X_test)\n", "np.mean(preds == y_test)"]}, {"metadata": {"_uuid": "068ff9652fb90394185ebd74721ccde18bb75dc5", "collapsed": true, "_cell_guid": "73d744a1-dac6-4699-9ba8-549eff4423b5"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["pipeline.get_params().keys()"]}, {"metadata": {"_uuid": "92c4599b77b8b55afa27acd820508e84e2385c95", "collapsed": true, "_cell_guid": "505b63f0-a5b3-490f-8829-b852efbe713b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.model_selection import GridSearchCV\n", "\n", "hyperparameters = { #'features__text__tfidf__max_df': [0.9, 0.95],\n", "                    #'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n", "                    #'classifier__learning_rate': [0.1, 0.2],\n", "                    'classifier__n_estimators': [20, 30, 50],\n", "                    'classifier__max_depth': [2, 4],\n", "                    'classifier__min_samples_leaf': [2, 4]\n", "                  }\n", "clf = GridSearchCV(pipeline, hyperparameters, cv = 3)\n", " \n", "# Fit and tune model\n", "clf.fit(X_train, y_train)"]}, {"metadata": {"_uuid": "3cc439fdeb4e50983d7b619412907919d9684802", "collapsed": true, "_cell_guid": "11da6112-db42-4620-a509-d5bfad61e5ef"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["clf.best_params_"]}, {"metadata": {"_uuid": "cea970f6c1d485d3ac3013f1dedf421d0c664ff5", "collapsed": true, "_cell_guid": "40f0b8f2-67ee-464d-bf34-95ac8ec9a1a4"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#refitting on entire training data using best settings\n", "clf.refit\n", "\n", "preds = clf.predict(X_test)\n", "probs = clf.predict_proba(X_test)\n", "\n", "np.mean(preds == y_test)"]}, {"metadata": {"_uuid": "9f20c96a6c76903201fefa42b20354a8de7e5cce", "collapsed": true, "_cell_guid": "b8694bcf-1e55-44c0-a791-294cd80bb3c9"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_test = df_test.replace(-1, np.NaN)\n", "dt_median = df_test.median(axis=0)\n", "dt_mean = df_test.mean(axis=0)\n", "df_test = df_test.fillna(-1)\n", "\n", "dtcol = [c for c in df_test.columns if c not in ['id']]\n", "df_test['ps_car_13_x_ps_reg_03'] = df_test['ps_car_13'] * df_test['ps_reg_03']\n", "#df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n", "for c in dtcol:\n", "        if '_bin' not in c: #standard arithmetic\n", "            df_test[c+str('_median_range')] = (df_test[c].values > dt_median[c]).astype(np.int)\n", "            df_test[c+str('_mean_range')] = (df_test[c].values > dt_mean[c]).astype(np.int)\n", "            df_test[c+str('_sq')] = np.power(df_test[c].values,2).astype(np.float32)\n", "            #df[c+str('_sqr')] = np.square(df[c].values).astype(np.float32)\n", "            df_test[c+str('_log')] = np.log(np.abs(df_test[c].values) + 1)\n", "            df_test[c+str('_exp')] = np.exp(df_test[c].values) - 1"]}, {"metadata": {"_uuid": "a202783552b2b3e6b500e495218c1ff796146ca1", "collapsed": true, "_cell_guid": "e576741c-93bc-4642-a309-319291a1e35f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["change_datatype(df_test)"]}, {"metadata": {"_uuid": "81f129440860e66ad5cd500750e9dd99bdd4af1f", "collapsed": true, "_cell_guid": "c33b2448-dd5f-4251-87ee-11481e98fc3e"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["submission = pd.read_csv('../input/test.csv')\n", "\n", "#preprocessing\n", "#test_features= [c for c in submission.columns.values if c  not in ['id']]\n", "test_features= [c for c in df_test.columns.values if c  not in ['id']]\n", "#submission = processing(submission)\n", "predictions = clf.predict_proba(df_test[test_features])\n", "\n", "preds = pd.DataFrame(data = predictions, columns = clf.best_estimator_.named_steps['classifier'].classes_)\n", "\n", "#generating a submission file\n", "result = pd.concat([submission[['id']], preds], axis=1)\n", "result = result.drop(0, axis=1)\n", "result.columns = ['id', 'target']\n", "result.head()\n", "\n", "result.to_csv('random_forest.csv', index=False)"]}, {"metadata": {"_uuid": "d4ccd84d1a26887fc9617c3f8ccb41d0b0db54ee", "collapsed": true, "_cell_guid": "a4c01040-fd8f-44cc-9593-d4e4be004978"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": []}, {"metadata": {"_uuid": "a9f13317aa950bbef8ab9d214f8aae2a8eb26eca", "collapsed": true, "_cell_guid": "5fa9e661-7932-42b6-bd79-0d02b80047ab"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''  \n", "\n", "from sklearn.cross_validation import train_test_split\n", "import xgboost as xgb\n", "\n", "X_train = df_train.drop(['id'],axis = 1)\n", "X_id_train = df_train['id'].values\n", "Y_train = target.values\n", "\n", "X_test = df_test.drop(['id'], axis=1)\n", "X_id_test = df_test['id'].values\n", "\n", "x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size = 0.4, random_state = 1000)\n", "print('Train samples: {} Validation samples: {}'.format(len(x_train), len(x_valid)))\n", "\n", "d_train = xgb.DMatrix(x_train, y_train)\n", "d_valid = xgb.DMatrix(x_valid, y_valid)\n", "d_test = xgb.DMatrix(X_test)\n", "\n", "params = {}\n", "params['min_child_weight'] = 10.0\n", "params['objective'] = 'binary:logistic'\n", "params['eta'] = 0.02\n", "params['silent'] = True\n", "params['max_depth'] = 9\n", "params['subsample'] = 0.9\n", "params['colsample_bytree'] = 0.9\n", "\n", "# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n", "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n", "    assert( len(actual) == len(pred) )\n", "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n", "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n", "    totalLosses = all[:,0].sum()\n", "    giniSum = all[:,0].cumsum().sum() / totalLosses\n", "    \n", "    giniSum -= (len(actual) + 1) / 2.\n", "    return giniSum / len(actual)\n", " \n", "def gini_normalized(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "\n", "# Create an XGBoost-compatible metric from Gini\n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = gini_normalized(labels, preds)\n", "    return [('gini', gini_score)]\n", "\n", "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "\n", "model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=100, feval=gini_xgb, maximize=True, verbose_eval=10)\n", "\n", "xgb.plot_importance(model)\n", "fig, ax = plt.subplots(figsize=(12,18))\n", "plt.show()\n", "\n", "p_test = model.predict(d_test)\n", "\n", "#--- Submission file ---\n", "\n", "sub = pd.DataFrame()\n", "sub['id'] = X_id_test\n", "sub['target'] = p_test\n", "sub.to_csv('xgb.csv', index=False)\n", "\n", "\n", "importance = model.get_fscore(fmap='xgb.fmap')\n", "importance = sorted(importance.items(), key=operator.itemgetter(1))\n", "\n", "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n", "\n", "plt.figure()\n", "df.plot()\n", "df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\n", "plt.gcf().savefig('features_importance.png')\n", "\n", "''' "]}, {"metadata": {"_uuid": "e2455b1cf9c1d913d528a60439b382d5dd8c8911", "_cell_guid": "a024635e-676d-4839-a158-2825806cf2ed"}, "cell_type": "markdown", "source": ["# Data Analysis\n", "## Splitting columns based on types\n", "According to the data given to us:\n", "* features that belong to similar groupings are tagged as such in the feature names (e.g., **ind**, **reg**, **car**, **calc**). \n", "* feature names include the postfix **bin** to indicate binary features and **cat** to indicate categorical features.\n", "* feature names without **boon** or **cat** are grouped as** continuous/ordinal** features."]}, {"metadata": {"_uuid": "5471b6d52e3f61b5d3993ef2e7cd29f52feffac4", "collapsed": true, "_cell_guid": "8b1478e4-fb5f-4b3f-9dbc-5d74b4d53b83"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#-- List of all columns --\n", "train_cols = df_train.columns.tolist()\n", "\n", "#--- binary and categorical features list ---\n", "bin_cols = []\n", "cat_cols = []\n", "\n", "#--- continous/ordinal features list ---\n", "cont_ord_cols = []\n", "\n", "#--- different feature groupings ---\n", "ind_cols = []\n", "reg_cols = []\n", "car_cols = []\n", "calc_cols = []\n", "\n", "for col in train_cols:\n", "    if (('ps' in str(col)) & ('bin' not in str(col)) & ('cat' not in str(col))):\n", "        cont_ord_cols.append(col)\n", "    \n", "for col in train_cols:\n", "    if ('bin' in str(col)):\n", "        bin_cols.append(col)\n", "    if ('cat' in str(col)):\n", "        cat_cols.append(col)\n", "        \n", "    if ('ind' in str(col)):\n", "        ind_cols.append(col)\n", "    if ('reg' in str(col)):\n", "        reg_cols.append(col)\n", "    if ('car' in str(col)):\n", "        car_cols.append(col)\n", "    if ('calc' in str(col)):\n", "        calc_cols.append(col)\n", "        "]}, {"metadata": {"_uuid": "44b50c2bc028d949e32ab10ff8487b788a1f2112", "_cell_guid": "a9d338e0-1bce-4b2f-9f45-daf90040a750"}, "cell_type": "markdown", "source": ["Columns present in `cont_ord_cols` list have a collection of different types of columns.\n", "\n", "So we can divide them into **continuous** and **ordinal** variables based on their data types."]}, {"metadata": {"_uuid": "1b9189777a8926e08c5a4b1e2363a8bf419f85b5", "collapsed": true, "_cell_guid": "7df83856-8d3b-4751-88db-75e12977e4e1"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["float_cols = []\n", "int_cols = []\n", "for col in cont_ord_cols:\n", "    if (df_train[col].dtype == np.float32):\n", "          float_cols.append(col)        #--- continuous variables ---\n", "    elif ((df_train[col].dtype == np.int8) or (df_train[col].dtype == np.int16)):\n", "          int_cols.append(col)          #--- ordinal variables ---"]}, {"metadata": {"_uuid": "85ca0dca1cb03f41508d7c56ab88d51ca0a1841a", "_cell_guid": "2b532554-01cc-4268-9309-e35111ca1547"}, "cell_type": "markdown", "source": ["The following snippet is confirmation that all the variables are **ordinal** beacuse they have more than 2 unique values."]}, {"metadata": {"_uuid": "8e0bc790307b1d9a6bc46e802989860accd8238e", "collapsed": true, "_cell_guid": "126c4688-828a-4b65-bd2f-60549f644496"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["for col in int_cols:\n", "    print (df_train[col].nunique())"]}, {"metadata": {"_uuid": "c04f78ed6c918b916a6948ffb626a14ec55c79af", "_cell_guid": "c92587ce-4620-4ab9-9100-934c85e53995"}, "cell_type": "markdown", "source": ["Exploring each of the above extracted grouped features individually:\n", "\n", "## Binary features:\n", "\n", "Binary features whose single attribute is less than 10% will be collected in a separate list"]}, {"metadata": {"_uuid": "5d4e1db46403c770c0a1284943de8222f5590675", "collapsed": true, "_cell_guid": "bb1849ff-286e-4845-b5c8-253c43c475b0"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["cols_to_delete = []\n", "th = 0.1\n", "for col in range(0, len(bin_cols)):\n", "    print (bin_cols[col])\n", "    print (df_train[bin_cols[col]].unique())\n", "    pp = pd.value_counts(df_train[bin_cols[col]])\n", "    \n", "    for i in range(0, len(pp)):\n", "        if((pp[i]/float(len(df_train))) <= th):\n", "            cols_to_delete.append(bin_cols[col])\n", "            \n", "    pp.plot.bar()\n", "    plt.show()"]}, {"metadata": {"_uuid": "f282ef3d4610f1588bf79ee23e824912e9db5c03", "collapsed": true, "_cell_guid": "2d63b04d-1bd7-479d-8916-cd5144714cd1"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(cols_to_delete)"]}, {"metadata": {"_uuid": "4291edbc13125f8190834df48d677b453cc1b676", "_cell_guid": "f88be7c8-5d9d-4c30-9f3e-db5a10847eac"}, "cell_type": "markdown", "source": ["The above mentioned columns have highly skewed values hence can be dropped from both the training and test set."]}, {"metadata": {"_uuid": "0b11ed516b4eaffc30a4dc5f7cc7dd43deab97b9", "collapsed": true, "_cell_guid": "0d932784-4b72-47e9-9251-8fc7794a6b9c"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": [" \n", "for col in cols_to_delete:\n", "    df_train.drop([col], axis=1, inplace=True)\n", "    df_test.drop([col], axis=1, inplace=True)\n", "    "]}, {"metadata": {"_uuid": "cc92dee9e0e1fbbb08102bf9a057f00b8b8be524", "_cell_guid": "d0d2bad2-73c4-40ea-b034-b7b6e22a0a58"}, "cell_type": "markdown", "source": ["## Categorical Features\n", "\n", "Exploring the categorical variables:"]}, {"metadata": {"_uuid": "27de8d32ef546283cd6617b593b2efc55f25848d", "collapsed": true, "_cell_guid": "d8af278e-e134-4fc1-ae26-4984136b1726"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["for col in range(0, len(cat_cols)):\n", "    print (cat_cols[col])\n", "    print (df_train[cat_cols[col]].unique())\n", "    pp = pd.value_counts(df_train[cat_cols[col]])      \n", "    pp.plot.bar()\n", "    plt.show()"]}, {"metadata": {"_uuid": "d8032acbd75ac68a1c466d40da66236349916d73", "_cell_guid": "9888ab86-5b9d-444f-a542-2451f5186514"}, "cell_type": "markdown", "source": ["From the graphs, only **ps_car_10_cat** is highly skewed hence can be removed from training and test set."]}, {"metadata": {"_uuid": "09cf720d0c5c72fd5294e45a947867d0990389f5", "collapsed": true, "_cell_guid": "feca851c-421f-46ce-8f85-ae744ec0e973"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''\n", "\n", "cat_cols_to_delete = [ 'ps_car_10_cat']\n", "\n", "for col in cat_cols_to_delete:\n", "    df_train.drop([col], axis=1, inplace=True)\n", "    df_test.drop([col], axis=1, inplace=True) \n", "\n", "''' "]}, {"metadata": {"_uuid": "326d49d900f6c5060506560b0ee21ef9499aab6d", "_cell_guid": "ed41cfc2-21a3-4129-9e48-af692ebc923c"}, "cell_type": "markdown", "source": ["## Continuous/Ordinal Features\n", "\n", "Features having different prefixes such as **ind**, **reg**, **car** and **calc**; excluding binary and categorical features."]}, {"metadata": {"_uuid": "8966a72e5c1f20546e4a192730dd4bf76e48e04d", "collapsed": true, "_cell_guid": "e6875b9a-9655-4150-b266-a15f8aa8ded7"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ind_cols_no_bin_cat = []\n", "reg_cols_no_bin_cat = []\n", "car_cols_no_bin_cat = []\n", "calc_cols_no_bin_cat = []\n", "\n", "for col in train_cols:\n", "    if (('ind' in str(col)) and ('bin' not in str(col)) and ('cat' not in str(col))):\n", "        ind_cols_no_bin_cat.append(col)\n", "    if (('reg' in str(col)) and ('bin' not in str(col)) and ('cat' not in str(col))):\n", "        reg_cols_no_bin_cat.append(col)\n", "    if (('car' in str(col)) and ('bin' not in str(col)) and ('cat' not in str(col))):\n", "        car_cols_no_bin_cat.append(col)\n", "    if (('calc' in str(col)) and ('bin' not in str(col)) and ('cat' not in str(col))):\n", "        calc_cols_no_bin_cat.append(col)"]}, {"metadata": {"_uuid": "4c56748d03e33aed945dada1eddec2a9c3de5cec", "_cell_guid": "e8619816-c4f0-4b95-8bf5-0d69d3867ae8"}, "cell_type": "markdown", "source": ["### Visualizing **ind** features\n", "\n", "(Uncomment the following snippets of code to visualzie the various grouped features. They take a long time to load hence I have commented them out)"]}, {"metadata": {"_uuid": "bee57f45702121a55678c96050b4e6f314595c72", "collapsed": true, "_cell_guid": "cf945bf3-5067-4d3d-9f5f-247b635fed20"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''\n", "what_col = ind_cols_no_bin_cat\n", "for col in range(0, len(what_col)):\n", "    print (what_col[col])\n", "    print (df_train[what_col[col]].unique())\n", "    pp = pd.value_counts(df_train[what_col[col]])      \n", "    pp.plot.bar()\n", "    plt.show()\n", "'''   "]}, {"metadata": {"_uuid": "dc33ccbd3642e1e7787e5a4395c213c5d3adc184", "_cell_guid": "7b6e9d2a-03e8-4ed7-8e10-42371ac7b6d3"}, "cell_type": "markdown", "source": ["Column **ps_ind_14** is heavily skewed hence can be removed."]}, {"metadata": {"_uuid": "fd0babb91e2f2ee0d6f499f8b72c907e821491d5", "_cell_guid": "5cebbef0-19c1-4ee6-90cc-e00a912772cb"}, "cell_type": "markdown", "source": ["### Visualizing **reg** features"]}, {"metadata": {"_uuid": "c9beb6a44fda8863d90ae9c043f1e496ccded8b9", "collapsed": true, "_cell_guid": "e984ce2d-8f23-4f42-95fa-bebfb463bad1"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''\n", "what_col = reg_cols_no_bin_cat\n", "for col in range(0, len(what_col)):\n", "    print (what_col[col])\n", "    print (df_train[what_col[col]].unique())\n", "    pp = pd.value_counts(df_train[what_col[col]])      \n", "    pp.plot.bar()\n", "    plt.show()\n", " '''  "]}, {"metadata": {"_uuid": "82f32b22717ef34c74c2b397c75bc15b37a5c6c1", "_cell_guid": "990841c0-bb9d-405c-930a-bd8665399536"}, "cell_type": "markdown", "source": ["Column **ps_reg_03** does not seem to show anything at all, hence can be removed.\n", "\n", "### Visualizing **car** features"]}, {"metadata": {"_uuid": "cd0057cc7442aea8a7f4d2a936f92210b0b7a2c2", "collapsed": true, "_cell_guid": "5066a831-8953-480d-96e5-59f4c1585e5e"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "what_col = car_cols_no_bin_cat\n", "for col in range(0, len(what_col)):\n", "    print (what_col[col])\n", "    print (df_train[what_col[col]].unique())\n", "    pp = pd.value_counts(df_train[what_col[col]])      \n", "    pp.plot.bar()\n", "    plt.show()\n", "'''"]}, {"metadata": {"_uuid": "386ee75ec13f28a8c37173311c2b88b1ce86ab86", "_cell_guid": "b29d6c0d-0cb6-4000-bc90-e221155d586b"}, "cell_type": "markdown", "source": ["### Visualizing **calc** features"]}, {"metadata": {"_uuid": "d7ec732d37b939391fc50c17a1c17ea7c015ae48", "collapsed": true, "_cell_guid": "4a1b4306-f51c-48b3-af9d-608ca954d13b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "what_col = calc_cols_no_bin_cat\n", "for col in range(0, len(what_col)):\n", "    print (what_col[col])\n", "    print (df_train[what_col[col]].unique())\n", "    pp = pd.value_counts(df_train[what_col[col]])      \n", "    pp.plot.bar()\n", "    plt.show()\n", "'''"]}, {"metadata": {"_uuid": "f0c604138d2b59cc5ed77fe655001a0fee9e7d3a", "_cell_guid": "b11cbbee-d910-4a36-9729-518f9e267188"}, "cell_type": "markdown", "source": ["Colukmns belonging to type ***calc***: \n", "* **ps_calc_01**,\n", "* **ps_calc_02**,\n", "* **ps_calc_03** \n", "\n", "have a uniform distribution, which do not offer anything significant. Hence these can also be removed."]}, {"metadata": {"_uuid": "31052acea6cb8b059135ce79a3a47f10ee3d281d", "collapsed": true, "_cell_guid": "50bca12a-ab93-49ae-b8ee-65c366301889"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' other_cols_to_delete = ['ps_ind_14', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_reg_03']\n", "\n", "for col in other_cols_to_delete:\n", "    df_train.drop([col], axis=1, inplace=True)\n", "    df_test.drop([col], axis=1, inplace=True)''' "]}, {"metadata": {"_uuid": "4ee560528f553939434ab93d40b2ea6ae44da9b2", "_cell_guid": "4c5c3c6f-ea43-4f10-a7db-664d7ac81de7"}, "cell_type": "markdown", "source": ["# Feature Engineering\n", "\n", "### NOTE: ALWAYS REMEMBER TO INCLUDE SAME SET OF FEATURES FOR THE TEST DATA ALSO!!"]}, {"metadata": {"_uuid": "ee6e40e819cc441af705c12afc9d84b7d99e8e36", "_cell_guid": "0d998f90-860d-4770-b0dc-b4ecaf0cd7bc"}, "cell_type": "markdown", "source": []}, {"metadata": {"_uuid": "b64b317fbb20fb7f815d6a535594aeeb8356abf5", "collapsed": true, "_cell_guid": "89d139b0-fa35-4a6f-872f-3b63f631bcfc"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''        \n", "for col1 in int_cols:\n", "    for col2 in float_cols:\n", "        l_mean = \n", "        df_train[col1 + '_' + col2] = \n", "''' "]}, {"metadata": {"_uuid": "13d0eb1346eec09166c4b3c5ca34c35df14a3427", "_cell_guid": "c3041fa6-d91d-43be-93a8-f0e4ec070cf5"}, "cell_type": "markdown", "source": ["## New Binary Features\n", "\n", "Here I have included logical AND, OR and XOR operation between every binary feature."]}, {"metadata": {"_uuid": "d0f0057b65680c1e005952ef19f9e48b6be79d50", "collapsed": true, "_cell_guid": "322abe86-6610-4232-bd98-5b71a66c712a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["train_cols = df_train.columns\n", "bin_cols = df_train.columns[df_train.columns.str.endswith('bin')]\n", "''' \n", "for i in [\"X1\",\"X2\"]:\n", "    for j in [\"X2\",\"X3\"]:\n", "        if i != j:\n", "            col_name = i + j\n", "            k[col_name + '_OR'] = k[i]|k[j] \n", "            k[col_name + '_AND'] = k[i]&k[j] \n", "            k[col_name + '_XOR'] = k[i]^k[j] \n", "           \n", "def second_order(df, c_names):\n", "    names_col=[]\n", "    pp=0\n", "    for i in c_names[:c_names.size-1]:\n", "        for j in c_names[pp:c_names.size]:\n", "            if i != j:\n", "                col_name = i + str('_') + j\n", "                df[col_name + '_OR'] = df[i]|df[j] \n", "                df[col_name + '_AND'] = df[i]&df[j] \n", "                df[col_name + '_XOR'] = df[i]^df[j]\n", "            \n", "                #col_name = ii + str('_and_') + jj\n", "                #names_col.append(col_name)\n", "                #df[col_name] = df[ii]&df[jj]\n", "        pp+=1\n", "    return df, names_col   \n", "\n", "df_train, train_new_cols = second_order(df_train, bin_cols)\n", "df_test, test_new_cols = second_order(df_test, bin_cols)\n", "\n", "print(len(df_train.columns))\n", "print(len(df_test.columns))\n", "'''"]}, {"metadata": {"_uuid": "e1b1bd0e3a6e830eb260963fa5daaeb59aa223d7", "_cell_guid": "29751663-4e79-45fa-83f5-8743f4d54769"}, "cell_type": "markdown", "source": ["## New Continuous/Ordinal Features (*in progress*)"]}, {"metadata": {"_uuid": "01896e3d531b4d83ee684c25cb9fe421e863f61b", "collapsed": true, "_cell_guid": "c1f044c7-0f66-4b08-bae0-3109fca0ec5f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "print(len(df_train.columns))\n", "#new_cont_ord_cols = [c for c in df_train.columns if not c.startswith('ps_calc_')]\n", "#new_cont_ord_cols = [c for c in df_train.columns if not c.endswith('bin') ]\n", "for col in no_bin_cat_cols:\n", "    #df_train[col + str('_greater_median')] = (df_train[col].values > df_train[col].median()).astype(np.int)\n", "    #df_train[col + str('_greater_mean')] = (df_train[col].values > df_train[col].mean()).astype(np.int)\n", "    df_train[col + str('_sq')] = np.power(df_train[col].values,2).astype(np.float32)\n", "    df_train[col + str('_sqr')] = np.square(df_train[col].values).astype(np.float32)\n", "    df_train[col + str('_log')] = np.log(np.abs(df_train[col].values) + 1)\n", "    #df_train[col + str('_exp')] = np.exp(df_train[col].values) - 1\n", "    \n", "#new_cont_ord_test_cols = [c for c in df_test.columns if not c.startswith('ps_calc_')]\n", "for col in no_bin_cat_cols:\n", "    #df_test[col + str('_greater_median')] = (df_test[col].values > df_test[col].median()).astype(np.int)\n", "    #df_test[col + str('_greater_mean')] = (df_test[col].values > df_test[col].mean()).astype(np.int)\n", "    df_test[col + str('_sq')] = np.power(df_test[col].values,2).astype(np.float32)\n", "    df_test[col + str('_sqr')] = np.square(df_test[col].values).astype(np.float32)\n", "    df_test[col + str('_log')] = np.log(np.abs(df_test[col].values) + 1)\n", "    #df_test[col + str('_exp')] = np.exp(df_test[col].values) - 1    \n", "'''    "]}, {"metadata": {"_uuid": "d4dddb7c90ae67700b519967849aa775cb1c45c7", "_cell_guid": "be43ab4a-82d2-4964-9850-839e000a3835"}, "cell_type": "markdown", "source": ["## New Second Order Continuous/Ordinal Features (*based on Gradient Boosting feature importance*)"]}, {"metadata": {"_uuid": "91e7adb833bcdc4e4d7d4783b24d41f23694e273", "collapsed": true, "_cell_guid": "61519a29-46de-40e7-bc7c-a75a7cd0ef5b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''\n", "new_col =['ps_car_12', 'ps_car_14', 'ps_car_15', 'ps_car_13', 'ps_reg_03', 'ps_ind_03', 'ps_ind_15', 'ps_reg_02', 'ps_reg_01', 'ps_calc_02', 'ps_calc_11', 'ps_calc_10']\n", "\n", "\n", "def new_second_order(df, c_names):\n", "    names_col=[]\n", "    pp=0\n", "    for i in c_names[:len(c_names)-1]:\n", "        for j in c_names[pp:len(c_names)]:\n", "            if i != j:\n", "                col_name = i + str('_*_') + j\n", "                df[col_name] = df[i] * df[j] \n", "                \n", "            \n", "                #col_name = ii + str('_and_') + jj\n", "                #names_col.append(col_name)\n", "                #df[col_name] = df[ii]&df[jj]\n", "        pp+=1\n", "    return df, names_col   \n", "\n", "df_train, train_new_cols = new_second_order(df_train, new_col)\n", "df_test, test_new_cols = new_second_order(df_test, new_col)\n", "'''"]}, {"metadata": {"_uuid": "62a780379f95b546b53107d62ba5dc8e3a8c4ce6", "collapsed": true, "_cell_guid": "64b6d144-7b7e-4747-ac5d-f60361cb7c8e"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(len(df_train.columns))\n", "print(len(df_test.columns))"]}, {"metadata": {"_uuid": "6b80e22d2e3439f0cb7f936974e0856b0a3fb8ae", "_cell_guid": "cef918e0-e684-4c0b-8773-e539e6388bb8"}, "cell_type": "markdown", "source": ["## Correlation"]}, {"metadata": {"_uuid": "d0e8de9e8644d237108d575c4116f451faaef9e9", "collapsed": true, "_cell_guid": "f5acedb2-b89f-459e-b6e5-d42622a11ea3"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "sns.set(style=\"white\")\n", "corr = df_train.corr()\n", "f, ax = plt.subplots(figsize=(18, 15))\n", "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n", "sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n", "plt.show()\n", "'''"]}, {"metadata": {"_uuid": "1df399ebfd1d03ebd73840e43205602eae81e671", "_cell_guid": "628abcae-4489-4b78-82fc-8db61e391988"}, "cell_type": "markdown", "source": ["Outrageous! Not even a single **calc** feature seems to have any interest in indulging themselves with anything!! It is better to remove them all!!"]}, {"metadata": {"_uuid": "cc73aea7b54c691c2d1515b3ec073d2c907dbe7d", "collapsed": true, "_cell_guid": "6504b8ea-ae88-4280-b82b-fd5ae7c8526b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''\n", "removed_calc_cols = []\n", "for col in df_train.columns:\n", "    if ('calc' in str(col)):\n", "        removed_calc_cols.append(col)\n", "    \n", "#unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\n", "\n", "df_train = df_train.drop(removed_calc_cols, axis=1)  \n", "df_test = df_test.drop(removed_calc_cols, axis=1)  \n", "''' "]}, {"metadata": {"_uuid": "fe00c4dd1f497fd168420852e51619fcbbbd4de0", "collapsed": true, "_cell_guid": "6eff7b53-c5cd-4fb4-8ef8-e2b6a77df691"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_train.replace(np.nan, -1, inplace=True)\n", "df_test.replace(np.nan, -1, inplace=True)\n", "print('Done')"]}, {"metadata": {"_uuid": "73984fe2974a0cd9d2c06b869f47c430815d3dcc", "collapsed": true, "_cell_guid": "012682c6-487b-42d3-9856-e4f30a187f2d"}, "cell_type": "markdown", "source": ["# Modeling\n", "## Gradient Boosting"]}, {"metadata": {"_uuid": "20036b57ad7317470fccadf3d25e6014cdacd7d5", "collapsed": true, "_cell_guid": "d9a8ece6-5ebf-4850-a55f-1f1310d21d39"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "X_train = df_train.drop(['id'],axis = 1)\n", "X_id_train = df_train['id'].values\n", "Y_train = target.values\n", "\n", "X_test = df_test.drop(['id'], axis=1)\n", "X_id_test = df_test['id'].values\n", "'''"]}, {"metadata": {"_uuid": "ca8bca20047bdc809301fae8ba4d0b79833e72bb", "collapsed": true, "_cell_guid": "43fd4d84-f0fb-4101-81d1-a276a1ecfbf0"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "\n", "GBR = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.02, max_depth=7, random_state = 0, loss='ls')\n", "#GBR = GradientBoostingClassifier(learning_rate = 0.02, n_estimators = 500, max_depth = 9, min_samples_split = 2, min_samples_leaf = 2, max_features = 10, random_state=123)\n", "    \n", "GBR.fit(X_train, Y_train)\n", "\n", "print (GBR)\n", "'''"]}, {"metadata": {"_uuid": "43a5a94d7d93155ac95b686e3f01c0036db28da3", "collapsed": true, "_cell_guid": "34181575-bbc1-4580-8557-1e70dba998eb"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#--- List of important features for Gradient Boosting Regressor ---\n", "''' \n", "features_list = X_train.columns.values\n", "feature_importance = GBR.feature_importances_\n", "sorted_idx = np.argsort(feature_importance)\n", "\n", "print(sorted_idx)\n", "''' "]}, {"metadata": {"_uuid": "35af0b205e76fe833376e23c89a51817e1218208", "collapsed": true, "_cell_guid": "2e725180-9b13-4538-b301-038d9703302a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "plt.figure(figsize=(15, 15))\n", "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n", "plt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\n", "plt.xlabel('Importance')\n", "plt.title('Feature importances')\n", "plt.draw()\n", "plt.show()\n", "''' "]}, {"metadata": {"_uuid": "84757621c0bcca610e842b06e78d495d1c8dcb06", "collapsed": true, "_cell_guid": "de77d964-9b10-4acc-a7dc-399644cdfd22"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#--- Predicting Gradient boost result for test data ---\n", "# y_GBR = GBR.predict(X_test)"]}, {"metadata": {"_uuid": "02d6f0ccc104c0e87ca3c06d35c6b1abe9c9d1eb", "collapsed": true, "_cell_guid": "708f6a6f-fd53-42c1-bf2d-763c0701ebc4"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "final = pd.DataFrame()\n", "final['id'] = X_id_test\n", "final['target'] = y_GBR\n", "final.to_csv('Gradient_Boost_1.csv', index=False)\n", "print('DONE!!')\n", "'''"]}, {"metadata": {"_uuid": "263f3bc5c3edd453f54907e68df2fef624b7d737", "_cell_guid": "d5c41bc7-9563-4c62-b52f-bbe5f468d02e"}, "cell_type": "markdown", "source": ["## XGBoost"]}, {"metadata": {"_uuid": "64e860b8d7937ce309b5ad31a00f69a888bb9fc6", "collapsed": true, "_cell_guid": "e3a23733-74c0-4e13-b7c0-43d38afc2383"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["import xgboost as xgb"]}, {"metadata": {"_uuid": "b6893a21f2ff117a4bbfb9bdb9597161d8e8bf18", "collapsed": true, "_cell_guid": "08582fdb-1c70-42a4-993f-ab2593b3eea7"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["def gini(actual, pred, cmpcol = 0, sortcol = 1):\n", "    assert( len(actual) == len(pred) )\n", "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n", "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n", "    totalLosses = all[:,0].sum()\n", "    giniSum = all[:,0].cumsum().sum() / totalLosses\n", "    \n", "    giniSum -= (len(actual) + 1) / 2.\n", "    return giniSum / len(actual)\n", " \n", "def gini_normalized(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = gini_normalized(labels, preds)\n", "    return 'gini', gini_score"]}, {"metadata": {"_uuid": "e5d0b7d00a3c6d3c2983ad50c29a695331b857d0", "collapsed": true, "_cell_guid": "ec199e9d-9ed7-4a92-ac4b-3aed87c56263"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''\n", "from sklearn.model_selection import StratifiedKFold\n", "\n", "kfold = 5\n", "skf = StratifiedKFold(n_splits=kfold, random_state=42)\n", "'''"]}, {"metadata": {"_uuid": "ac3e195d0bd032268ff8640a3b5d7768137a5669", "collapsed": true, "_cell_guid": "f28897d3-e8f4-4528-934f-36a516fceb08"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["params = {\n", "    'min_child_weight': 10.0,\n", "    'objective': 'binary:logistic',\n", "    'max_depth': 7,\n", "    'max_delta_step': 1.8,\n", "    'colsample_bytree': 0.4,\n", "    'subsample': 0.8,\n", "    'eta': 0.025,\n", "    'gamma': 0.65,\n", "    'num_boost_round' : 700\n", "    }"]}, {"metadata": {"_uuid": "95a1e30e3226746c093ad0913dee2e4ed6c4d233", "collapsed": true, "_cell_guid": "93073909-596a-4832-a8e3-5fd6d2b38423"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''\n", "for i, (train_index, test_index) in enumerate(skf.split(X_train, Y_train)):\n", "    print('[Fold %d/%d]' % (i + 1, kfold))\n", "    X_train, X_valid = X_train[train_index], X_train[test_index]\n", "    y_train, y_valid = Y_train[train_index], Y_train[test_index]\n", "    # Convert our data into XGBoost format\n", "    d_train = xgb.DMatrix(X_train, y_train)\n", "    d_valid = xgb.DMatrix(X_valid, y_valid)\n", "    d_test = xgb.DMatrix(X_test.values)\n", "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "\n", "    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n", "    # and the custom metric (maximize=True tells xgb that higher metric is better)\n", "    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, feval=gini_xgb, maximize=True, verbose_eval=100)\n", "\n", "    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n", "    # Predict on our test data\n", "    p_test = mdl.predict(d_test)\n", "    sub['target'] += p_test/kfold\n", "'''    "]}, {"metadata": {"_uuid": "edc4580ae21c42cf117b4d10849dd201ec00eacc", "_cell_guid": "011b7ea5-4093-48f1-8354-05e29c3e139c"}, "cell_type": "markdown", "source": ["## Random Forest"]}, {"metadata": {"_uuid": "d806c29880e0c4a925b52d5b793b2486cdf75b70", "collapsed": true, "_cell_guid": "1764612d-ac0c-4193-b656-d3f10cd02c03"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["''' \n", "from sklearn.ensemble import RandomForestClassifier  \n", "\n", "RF = RandomForestClassifier(n_estimators=100, max_depth=8, criterion='entropy', min_samples_split=10, max_features=120, n_jobs=-1, random_state=123, verbose=1, class_weight = \"balanced\")\n", "RF.fit(X_train, Y_train)\n", "\n", "print(RF)\n", "\n", "#--- List of important features ---\n", "\n", "features_list = X_train.columns.values\n", "feature_importance = RF.feature_importances_\n", "sorted_idx = np.argsort(feature_importance)\n", "\n", "print(sorted_idx)\n", "\n", "plt.figure(figsize=(15, 15))\n", "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n", "plt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\n", "plt.xlabel('Importance')\n", "plt.title('Feature importances')\n", "plt.draw()\n", "plt.show()\n", "\n", " \n", "Y_pred = RF.predict(X_test)\n", "\n", "final = pd.DataFrame()\n", "final['id'] = X_id_test\n", "final['target'] = Y_pred\n", "final.to_csv('RF.csv', index=False)\n", "print('DONE!!')\n", "\n", "'''"]}, {"metadata": {"_uuid": "6fa07c28d97f5e5318f741aaf62264ef86d19b53", "collapsed": true, "_cell_guid": "f5c7f77d-a103-46bd-a882-eba967a19f8c"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#-- Adaboost ---\n", "'''\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.ensemble import AdaBoostRegressor\n", "\n", "Ada_R = AdaBoostRegressor(DecisionTreeRegressor(max_depth=7), n_estimators = 400, random_state = 99)\n", "\n", "Ada_R.fit(X_train, Y_train)\n", "\n", "print (Ada_R)\n", "\n", "features_list = X_train.columns.values\n", "feature_importance = Ada_R.feature_importances_\n", "sorted_idx = np.argsort(feature_importance)\n", "\n", "print(sorted_idx)\n", "\n", "plt.figure(figsize=(15, 15))\n", "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n", "plt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\n", "plt.xlabel('Importance')\n", "plt.title('Feature importances')\n", "plt.draw()\n", "plt.show()\n", "\n", "#--- Predicting Ada boost result for test data ---\n", "y_Ada = Ada_R.predict(X_test)\n", "\n", "\n", "final = pd.DataFrame()\n", "final['id'] = X_id_test\n", "final['target'] = y_Ada\n", "final.to_csv('Ada_Boost_1.csv', index=False)\n", "print('DONE!!')\n", "''' "]}, {"metadata": {"_uuid": "d17ebbfa606c7db59e9562373b78b04cbd0ee35a", "collapsed": true, "_cell_guid": "7ea31b75-c6c1-4854-be44-6a497e020341"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["'''  \n", "\n", "from sklearn.cross_validation import train_test_split\n", "import xgboost as xgb\n", "\n", "X_train = df_train.drop(['id'],axis = 1)\n", "X_id_train = df_train['id'].values\n", "Y_train = target.values\n", "\n", "X_test = df_test.drop(['id'], axis=1)\n", "X_id_test = df_test['id'].values\n", "\n", "x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=4242)\n", "print('Train samples: {} Validation samples: {}'.format(len(x_train), len(x_valid)))\n", "\n", "d_train = xgb.DMatrix(x_train, y_train)\n", "d_valid = xgb.DMatrix(x_valid, y_valid)\n", "d_test = xgb.DMatrix(X_test)\n", "\n", "params = {}\n", "params['min_child_weight'] = 10.0\n", "params['objective'] = 'binary:logistic'\n", "params['eta'] = 0.02\n", "params['silent'] = True\n", "params['max_depth'] = 9\n", "params['subsample'] = 0.9\n", "params['colsample_bytree'] = 0.9\n", "\n", "# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n", "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n", "    assert( len(actual) == len(pred) )\n", "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n", "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n", "    totalLosses = all[:,0].sum()\n", "    giniSum = all[:,0].cumsum().sum() / totalLosses\n", "    \n", "    giniSum -= (len(actual) + 1) / 2.\n", "    return giniSum / len(actual)\n", " \n", "def gini_normalized(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "\n", "# Create an XGBoost-compatible metric from Gini\n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = gini_normalized(labels, preds)\n", "    return [('gini', gini_score)]\n", "\n", "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "\n", "model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=100, feval=gini_xgb, maximize=True, verbose_eval=10)\n", "\n", "xgb.plot_importance(model)\n", "fig, ax = plt.subplots(figsize=(12,18))\n", "plt.show()\n", "\n", "p_test = model.predict(d_test)\n", "\n", "#--- Submission file ---\n", "\n", "sub = pd.DataFrame()\n", "sub['id'] = X_id_test\n", "sub['target'] = p_test\n", "sub.to_csv('xgb2.csv', index=False)\n", "\n", "\n", "importance = model.get_fscore(fmap='xgb.fmap')\n", "importance = sorted(importance.items(), key=operator.itemgetter(1))\n", "\n", "df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n", "\n", "plt.figure()\n", "df.plot()\n", "df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\n", "plt.gcf().savefig('features_importance.png')\n", "''' "]}, {"metadata": {"_uuid": "2600a5e8c0cf4304f33d4df8a351eb5686448983", "collapsed": true, "_cell_guid": "42b05673-ff63-4245-9fde-a44b99568bf7"}, "cell_type": "markdown", "source": ["### Can you think of more features? Let me know in the comments!\n", "\n", "# STAY TUNED FOR MORE UPDATES !!!"]}], "metadata": {"language_info": {"version": "3.6.3", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "file_extension": ".py", "name": "python", "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1, "nbformat": 4}