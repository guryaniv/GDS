{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"ver = 'linear_v20'\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nimport time\nfrom datetime import datetime\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.style.use(['seaborn-darkgrid'])\nplt.rcParams['font.family'] = 'DejaVu Sans'\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression, Lasso, Ridge, SGDClassifier\n\nfrom sklearn.feature_selection import RFE\n\nimport eli5\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom hyperopt import fmin, tpe, hp, anneal, Trials, STATUS_OK\n\nRANDOM_STATE = 78\n\n","execution_count":27,"outputs":[{"output_type":"stream","text":"['test.csv', 'train.csv', 'sample_submission.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Load datasets**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"((250, 302), (19750, 301))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"               id      target     ...             298         299\ncount  250.000000  250.000000     ...      250.000000  250.000000\nmean   124.500000    0.640000     ...        0.009372   -0.128952\nstd     72.312977    0.480963     ...        1.008099    0.971219\nmin      0.000000    0.000000     ...       -3.211000   -3.500000\n25%     62.250000    0.000000     ...       -0.550000   -0.754250\n50%    124.500000    1.000000     ...       -0.009000   -0.132500\n75%    186.750000    1.000000     ...        0.654250    0.503250\nmax    249.000000    1.000000     ...        3.530000    2.771000\n\n[8 rows x 302 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>...</th>\n      <th>260</th>\n      <th>261</th>\n      <th>262</th>\n      <th>263</th>\n      <th>264</th>\n      <th>265</th>\n      <th>266</th>\n      <th>267</th>\n      <th>268</th>\n      <th>269</th>\n      <th>270</th>\n      <th>271</th>\n      <th>272</th>\n      <th>273</th>\n      <th>274</th>\n      <th>275</th>\n      <th>276</th>\n      <th>277</th>\n      <th>278</th>\n      <th>279</th>\n      <th>280</th>\n      <th>281</th>\n      <th>282</th>\n      <th>283</th>\n      <th>284</th>\n      <th>285</th>\n      <th>286</th>\n      <th>287</th>\n      <th>288</th>\n      <th>289</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>...</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n      <td>250.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>124.500000</td>\n      <td>0.640000</td>\n      <td>0.023292</td>\n      <td>-0.026872</td>\n      <td>0.167404</td>\n      <td>0.001904</td>\n      <td>0.001588</td>\n      <td>-0.007304</td>\n      <td>0.032052</td>\n      <td>0.078412</td>\n      <td>-0.036920</td>\n      <td>0.035448</td>\n      <td>-0.005032</td>\n      <td>0.110248</td>\n      <td>0.019808</td>\n      <td>-0.001108</td>\n      <td>-0.016280</td>\n      <td>-0.039644</td>\n      <td>0.017260</td>\n      <td>-0.106856</td>\n      <td>0.036184</td>\n      <td>-0.043296</td>\n      <td>-0.110832</td>\n      <td>0.072680</td>\n      <td>0.017296</td>\n      <td>-0.030728</td>\n      <td>-0.128252</td>\n      <td>0.154736</td>\n      <td>0.083408</td>\n      <td>0.039552</td>\n      <td>-0.091784</td>\n      <td>0.054636</td>\n      <td>-0.048288</td>\n      <td>-0.017296</td>\n      <td>0.007708</td>\n      <td>-0.134460</td>\n      <td>0.093852</td>\n      <td>-0.020588</td>\n      <td>-0.002492</td>\n      <td>-0.141400</td>\n      <td>...</td>\n      <td>0.005780</td>\n      <td>-0.102304</td>\n      <td>-0.013796</td>\n      <td>0.089384</td>\n      <td>0.036368</td>\n      <td>0.016276</td>\n      <td>-0.069448</td>\n      <td>-0.113236</td>\n      <td>0.035696</td>\n      <td>0.034484</td>\n      <td>-0.066236</td>\n      <td>-0.057988</td>\n      <td>0.091556</td>\n      <td>-0.029896</td>\n      <td>0.115648</td>\n      <td>0.007372</td>\n      <td>0.033552</td>\n      <td>0.090524</td>\n      <td>0.001576</td>\n      <td>-0.007784</td>\n      <td>0.043184</td>\n      <td>0.082696</td>\n      <td>0.098476</td>\n      <td>0.055356</td>\n      <td>0.111708</td>\n      <td>-0.015688</td>\n      <td>0.035992</td>\n      <td>0.026452</td>\n      <td>-0.059152</td>\n      <td>0.077272</td>\n      <td>0.044652</td>\n      <td>0.126344</td>\n      <td>0.018436</td>\n      <td>-0.012092</td>\n      <td>-0.065720</td>\n      <td>-0.106112</td>\n      <td>0.046472</td>\n      <td>0.006452</td>\n      <td>0.009372</td>\n      <td>-0.128952</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>72.312977</td>\n      <td>0.480963</td>\n      <td>0.998354</td>\n      <td>1.009314</td>\n      <td>1.021709</td>\n      <td>1.011751</td>\n      <td>1.035411</td>\n      <td>0.955700</td>\n      <td>1.006657</td>\n      <td>0.939731</td>\n      <td>0.963688</td>\n      <td>1.019689</td>\n      <td>1.085089</td>\n      <td>1.036265</td>\n      <td>1.050041</td>\n      <td>1.024305</td>\n      <td>0.926789</td>\n      <td>0.955915</td>\n      <td>1.025655</td>\n      <td>1.012777</td>\n      <td>0.945099</td>\n      <td>1.055935</td>\n      <td>1.003178</td>\n      <td>1.039556</td>\n      <td>0.988482</td>\n      <td>0.945902</td>\n      <td>0.997026</td>\n      <td>0.997894</td>\n      <td>1.040371</td>\n      <td>0.922270</td>\n      <td>1.047282</td>\n      <td>1.041432</td>\n      <td>1.010971</td>\n      <td>0.992464</td>\n      <td>0.986350</td>\n      <td>1.015563</td>\n      <td>1.117898</td>\n      <td>0.958191</td>\n      <td>0.948855</td>\n      <td>1.042429</td>\n      <td>...</td>\n      <td>0.994761</td>\n      <td>1.094494</td>\n      <td>1.026025</td>\n      <td>0.963489</td>\n      <td>1.026373</td>\n      <td>1.008207</td>\n      <td>0.989451</td>\n      <td>1.002857</td>\n      <td>0.944743</td>\n      <td>1.023709</td>\n      <td>0.985451</td>\n      <td>0.951879</td>\n      <td>1.027877</td>\n      <td>0.966882</td>\n      <td>1.037173</td>\n      <td>1.004543</td>\n      <td>1.006219</td>\n      <td>1.037119</td>\n      <td>1.024067</td>\n      <td>1.056086</td>\n      <td>1.012516</td>\n      <td>1.068741</td>\n      <td>0.934163</td>\n      <td>0.988100</td>\n      <td>1.043230</td>\n      <td>1.010720</td>\n      <td>1.058982</td>\n      <td>0.896318</td>\n      <td>1.113760</td>\n      <td>0.972530</td>\n      <td>1.011416</td>\n      <td>0.972567</td>\n      <td>0.954229</td>\n      <td>0.960630</td>\n      <td>1.057414</td>\n      <td>1.038389</td>\n      <td>0.967661</td>\n      <td>0.998984</td>\n      <td>1.008099</td>\n      <td>0.971219</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-2.319000</td>\n      <td>-2.931000</td>\n      <td>-2.477000</td>\n      <td>-2.359000</td>\n      <td>-2.566000</td>\n      <td>-2.845000</td>\n      <td>-2.976000</td>\n      <td>-3.444000</td>\n      <td>-2.768000</td>\n      <td>-2.361000</td>\n      <td>-3.302000</td>\n      <td>-2.851000</td>\n      <td>-2.681000</td>\n      <td>-2.596000</td>\n      <td>-3.275000</td>\n      <td>-3.512000</td>\n      <td>-2.476000</td>\n      <td>-3.619000</td>\n      <td>-2.428000</td>\n      <td>-3.229000</td>\n      <td>-3.024000</td>\n      <td>-2.775000</td>\n      <td>-2.962000</td>\n      <td>-2.490000</td>\n      <td>-3.107000</td>\n      <td>-2.943000</td>\n      <td>-2.933000</td>\n      <td>-2.942000</td>\n      <td>-2.957000</td>\n      <td>-2.911000</td>\n      <td>-2.568000</td>\n      <td>-2.649000</td>\n      <td>-3.031000</td>\n      <td>-2.913000</td>\n      <td>-3.265000</td>\n      <td>-2.372000</td>\n      <td>-3.037000</td>\n      <td>-3.340000</td>\n      <td>...</td>\n      <td>-2.512000</td>\n      <td>-2.873000</td>\n      <td>-2.549000</td>\n      <td>-2.721000</td>\n      <td>-2.578000</td>\n      <td>-2.239000</td>\n      <td>-3.046000</td>\n      <td>-2.755000</td>\n      <td>-2.507000</td>\n      <td>-3.369000</td>\n      <td>-2.448000</td>\n      <td>-2.771000</td>\n      <td>-2.903000</td>\n      <td>-2.522000</td>\n      <td>-2.759000</td>\n      <td>-2.915000</td>\n      <td>-2.618000</td>\n      <td>-3.623000</td>\n      <td>-2.673000</td>\n      <td>-3.229000</td>\n      <td>-2.537000</td>\n      <td>-2.748000</td>\n      <td>-2.850000</td>\n      <td>-2.577000</td>\n      <td>-2.973000</td>\n      <td>-2.709000</td>\n      <td>-3.605000</td>\n      <td>-2.357000</td>\n      <td>-2.904000</td>\n      <td>-2.734000</td>\n      <td>-2.804000</td>\n      <td>-2.443000</td>\n      <td>-2.757000</td>\n      <td>-2.466000</td>\n      <td>-3.287000</td>\n      <td>-3.072000</td>\n      <td>-2.634000</td>\n      <td>-2.776000</td>\n      <td>-3.211000</td>\n      <td>-3.500000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>62.250000</td>\n      <td>0.000000</td>\n      <td>-0.644750</td>\n      <td>-0.739750</td>\n      <td>-0.425250</td>\n      <td>-0.686500</td>\n      <td>-0.659000</td>\n      <td>-0.643750</td>\n      <td>-0.675000</td>\n      <td>-0.550750</td>\n      <td>-0.689500</td>\n      <td>-0.643500</td>\n      <td>-0.693500</td>\n      <td>-0.524000</td>\n      <td>-0.708500</td>\n      <td>-0.692000</td>\n      <td>-0.677000</td>\n      <td>-0.634500</td>\n      <td>-0.683500</td>\n      <td>-0.801500</td>\n      <td>-0.574250</td>\n      <td>-0.758000</td>\n      <td>-0.870500</td>\n      <td>-0.596000</td>\n      <td>-0.725750</td>\n      <td>-0.652000</td>\n      <td>-0.779500</td>\n      <td>-0.424250</td>\n      <td>-0.585750</td>\n      <td>-0.625000</td>\n      <td>-0.751250</td>\n      <td>-0.582500</td>\n      <td>-0.713500</td>\n      <td>-0.750000</td>\n      <td>-0.588000</td>\n      <td>-0.829000</td>\n      <td>-0.648500</td>\n      <td>-0.659750</td>\n      <td>-0.614000</td>\n      <td>-0.816750</td>\n      <td>...</td>\n      <td>-0.622750</td>\n      <td>-1.009250</td>\n      <td>-0.693250</td>\n      <td>-0.567750</td>\n      <td>-0.696500</td>\n      <td>-0.684000</td>\n      <td>-0.703750</td>\n      <td>-0.771250</td>\n      <td>-0.624500</td>\n      <td>-0.653000</td>\n      <td>-0.786750</td>\n      <td>-0.701000</td>\n      <td>-0.543250</td>\n      <td>-0.672750</td>\n      <td>-0.626750</td>\n      <td>-0.730250</td>\n      <td>-0.649750</td>\n      <td>-0.589500</td>\n      <td>-0.725750</td>\n      <td>-0.667750</td>\n      <td>-0.605000</td>\n      <td>-0.637750</td>\n      <td>-0.458250</td>\n      <td>-0.553500</td>\n      <td>-0.566750</td>\n      <td>-0.778250</td>\n      <td>-0.693250</td>\n      <td>-0.596750</td>\n      <td>-0.789000</td>\n      <td>-0.671250</td>\n      <td>-0.617000</td>\n      <td>-0.510500</td>\n      <td>-0.535750</td>\n      <td>-0.657000</td>\n      <td>-0.818500</td>\n      <td>-0.821000</td>\n      <td>-0.605500</td>\n      <td>-0.751250</td>\n      <td>-0.550000</td>\n      <td>-0.754250</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>124.500000</td>\n      <td>1.000000</td>\n      <td>-0.015500</td>\n      <td>0.057000</td>\n      <td>0.184000</td>\n      <td>-0.016500</td>\n      <td>-0.023000</td>\n      <td>0.037500</td>\n      <td>0.060500</td>\n      <td>0.183500</td>\n      <td>-0.012500</td>\n      <td>0.052000</td>\n      <td>0.066000</td>\n      <td>0.115500</td>\n      <td>0.090000</td>\n      <td>0.016000</td>\n      <td>0.009500</td>\n      <td>0.010000</td>\n      <td>-0.119000</td>\n      <td>-0.164500</td>\n      <td>-0.009500</td>\n      <td>-0.018000</td>\n      <td>-0.161500</td>\n      <td>0.048000</td>\n      <td>0.135000</td>\n      <td>-0.016000</td>\n      <td>-0.165500</td>\n      <td>0.125500</td>\n      <td>0.036500</td>\n      <td>0.045000</td>\n      <td>-0.026000</td>\n      <td>0.045000</td>\n      <td>-0.130000</td>\n      <td>0.016000</td>\n      <td>0.023500</td>\n      <td>-0.216500</td>\n      <td>0.231500</td>\n      <td>0.014000</td>\n      <td>-0.012500</td>\n      <td>-0.234000</td>\n      <td>...</td>\n      <td>-0.048500</td>\n      <td>-0.134000</td>\n      <td>-0.065500</td>\n      <td>0.097000</td>\n      <td>-0.088000</td>\n      <td>0.019500</td>\n      <td>0.001500</td>\n      <td>-0.107500</td>\n      <td>0.045500</td>\n      <td>-0.023500</td>\n      <td>-0.101000</td>\n      <td>-0.109000</td>\n      <td>0.050500</td>\n      <td>-0.081500</td>\n      <td>0.087500</td>\n      <td>0.035000</td>\n      <td>0.086500</td>\n      <td>0.126500</td>\n      <td>0.053500</td>\n      <td>0.063500</td>\n      <td>0.052500</td>\n      <td>0.103000</td>\n      <td>0.135500</td>\n      <td>0.039000</td>\n      <td>0.093500</td>\n      <td>0.014500</td>\n      <td>-0.007500</td>\n      <td>0.000500</td>\n      <td>-0.122500</td>\n      <td>0.057500</td>\n      <td>0.067500</td>\n      <td>0.091000</td>\n      <td>0.057500</td>\n      <td>-0.021000</td>\n      <td>-0.009000</td>\n      <td>-0.079500</td>\n      <td>0.009500</td>\n      <td>0.005500</td>\n      <td>-0.009000</td>\n      <td>-0.132500</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>186.750000</td>\n      <td>1.000000</td>\n      <td>0.677000</td>\n      <td>0.620750</td>\n      <td>0.805000</td>\n      <td>0.720000</td>\n      <td>0.735000</td>\n      <td>0.660500</td>\n      <td>0.783250</td>\n      <td>0.766250</td>\n      <td>0.635000</td>\n      <td>0.733000</td>\n      <td>0.694250</td>\n      <td>0.786250</td>\n      <td>0.805250</td>\n      <td>0.654000</td>\n      <td>0.611000</td>\n      <td>0.578000</td>\n      <td>0.699000</td>\n      <td>0.496500</td>\n      <td>0.686000</td>\n      <td>0.698000</td>\n      <td>0.560500</td>\n      <td>0.797000</td>\n      <td>0.631500</td>\n      <td>0.619250</td>\n      <td>0.579250</td>\n      <td>0.719500</td>\n      <td>0.798250</td>\n      <td>0.721750</td>\n      <td>0.678500</td>\n      <td>0.728750</td>\n      <td>0.670500</td>\n      <td>0.698750</td>\n      <td>0.602750</td>\n      <td>0.557750</td>\n      <td>0.736250</td>\n      <td>0.599250</td>\n      <td>0.612000</td>\n      <td>0.557000</td>\n      <td>...</td>\n      <td>0.675750</td>\n      <td>0.708750</td>\n      <td>0.726000</td>\n      <td>0.739250</td>\n      <td>0.856750</td>\n      <td>0.709250</td>\n      <td>0.616000</td>\n      <td>0.569500</td>\n      <td>0.655750</td>\n      <td>0.634750</td>\n      <td>0.525750</td>\n      <td>0.612000</td>\n      <td>0.864000</td>\n      <td>0.630250</td>\n      <td>0.836750</td>\n      <td>0.718000</td>\n      <td>0.791500</td>\n      <td>0.726000</td>\n      <td>0.688000</td>\n      <td>0.665750</td>\n      <td>0.603750</td>\n      <td>0.705500</td>\n      <td>0.688250</td>\n      <td>0.759000</td>\n      <td>0.704750</td>\n      <td>0.673750</td>\n      <td>0.748750</td>\n      <td>0.604250</td>\n      <td>0.650500</td>\n      <td>0.772500</td>\n      <td>0.797250</td>\n      <td>0.804250</td>\n      <td>0.631500</td>\n      <td>0.650250</td>\n      <td>0.739500</td>\n      <td>0.493000</td>\n      <td>0.683000</td>\n      <td>0.794250</td>\n      <td>0.654250</td>\n      <td>0.503250</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>249.000000</td>\n      <td>1.000000</td>\n      <td>2.567000</td>\n      <td>2.419000</td>\n      <td>3.392000</td>\n      <td>2.771000</td>\n      <td>2.901000</td>\n      <td>2.793000</td>\n      <td>2.546000</td>\n      <td>2.846000</td>\n      <td>2.512000</td>\n      <td>2.959000</td>\n      <td>3.271000</td>\n      <td>2.998000</td>\n      <td>2.729000</td>\n      <td>2.651000</td>\n      <td>2.913000</td>\n      <td>2.508000</td>\n      <td>3.286000</td>\n      <td>2.430000</td>\n      <td>2.557000</td>\n      <td>2.868000</td>\n      <td>2.703000</td>\n      <td>2.691000</td>\n      <td>2.604000</td>\n      <td>2.362000</td>\n      <td>2.927000</td>\n      <td>2.976000</td>\n      <td>2.581000</td>\n      <td>2.305000</td>\n      <td>2.489000</td>\n      <td>2.895000</td>\n      <td>2.457000</td>\n      <td>2.407000</td>\n      <td>2.882000</td>\n      <td>2.649000</td>\n      <td>2.914000</td>\n      <td>2.995000</td>\n      <td>2.382000</td>\n      <td>2.481000</td>\n      <td>...</td>\n      <td>2.612000</td>\n      <td>2.680000</td>\n      <td>2.964000</td>\n      <td>2.663000</td>\n      <td>2.406000</td>\n      <td>3.457000</td>\n      <td>2.496000</td>\n      <td>2.501000</td>\n      <td>2.832000</td>\n      <td>2.897000</td>\n      <td>3.753000</td>\n      <td>2.498000</td>\n      <td>2.725000</td>\n      <td>2.680000</td>\n      <td>3.445000</td>\n      <td>2.846000</td>\n      <td>2.315000</td>\n      <td>2.780000</td>\n      <td>2.364000</td>\n      <td>2.908000</td>\n      <td>2.926000</td>\n      <td>3.441000</td>\n      <td>2.319000</td>\n      <td>2.842000</td>\n      <td>3.343000</td>\n      <td>3.266000</td>\n      <td>3.061000</td>\n      <td>2.146000</td>\n      <td>2.853000</td>\n      <td>3.026000</td>\n      <td>2.865000</td>\n      <td>2.801000</td>\n      <td>2.736000</td>\n      <td>2.596000</td>\n      <td>2.226000</td>\n      <td>3.131000</td>\n      <td>3.236000</td>\n      <td>2.626000</td>\n      <td>3.530000</td>\n      <td>2.771000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts().sort_index(ascending=False).plot(kind='barh', \n                                                                          figsize=(15,6))\nplt.title('Target', fontsize=18)","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"Text(0.5, 1.0, 'Target')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Data preparation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_tst = test.drop(['id'], axis=1)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc0 = StandardScaler()\nsc0.fit(X_train)\nX_train = sc0.transform(X_train)\nX_test = sc0.transform(X_tst)\n","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nrepfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=20, random_state=RANDOM_STATE)\nlogreg0 = LogisticRegression(C=0.5, random_state=RANDOM_STATE, solver='liblinear', penalty='l1')\nlass0 = Lasso(alpha=0.031, tol=0.01, selection='random', random_state=RANDOM_STATE)\nridg0 = Ridge(alpha=20, fit_intercept=True, solver='auto', tol=0.0025, random_state=RANDOM_STATE)\nsgd0 = SGDClassifier(eta0=1, max_iter=1000, tol=0.0001, random_state=RANDOM_STATE, loss='log')","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit simple logreg**"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg0.fit(X_train, y_train)\nsc = cross_val_score(logreg0, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":35,"outputs":[{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    4.2s\n","name":"stderr"},{"output_type":"stream","text":"0.7757291666666666\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.3s finished\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(logreg0, top=10)","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <style>\n    table.eli5-weights tr:hover {\n        filter: brightness(85%);\n    }\n</style>\n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n        \n            \n                \n                \n    \n        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n            <b>\n    \n        y=1.0\n    \n</b>\n\ntop features\n        </p>\n    \n    <table class=\"eli5-weights\"\n           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n        <thead>\n        <tr style=\"border: none;\">\n            \n                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n                    Weight<sup>?</sup>\n                </th>\n            \n            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n            \n        </tr>\n        </thead>\n        <tbody>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +1.349\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        x33\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 82.26%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +1.137\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        &lt;BIAS&gt;\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 82.97%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +1.073\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        x65\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 89.67%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.525\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        x199\n    </td>\n    \n</tr>\n        \n        \n            <tr style=\"background-color: hsl(120, 100.00%, 89.67%); border: none;\">\n                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n                    <i>&hellip; 43 more positive &hellip;</i>\n                </td>\n            </tr>\n        \n\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 91.12%); border: none;\">\n                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n                    <i>&hellip; 62 more negative &hellip;</i>\n                </td>\n            </tr>\n        \n        \n            <tr style=\"background-color: hsl(0, 100.00%, 91.12%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.423\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        x117\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.84%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.442\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        x295\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.73%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.450\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        x82\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 88.31%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.627\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        x73\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 87.19%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.714\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        x91\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 87.13%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.719\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        x217\n    </td>\n    \n</tr>\n        \n\n        </tbody>\n    </table>\n\n            \n        \n\n        \n\n\n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"el_df =pd.Series(logreg0.coef_[0],index=train.drop(['id', 'target'], axis=1).columns)\nel_df = el_df[(logreg0.coef_[0]<=-0.5) | (logreg0.coef_[0]>=0.5)].sort_values(ascending=False)\nplt.figure(figsize=(8,6))\nel_df.plot(kind='barh')\nplt.xlabel(\"Importance\",fontsize=12)\nplt.ylabel(\"Features\",fontsize=12)\nplt.title(\"Top Features\",fontsize=16)\nplt.show()","execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 576x432 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAfQAAAGFCAYAAAALqAHuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt8THfi//H3ZMYQogkRSVxaEhIWYbtSdQ3aLCsJbVO9repaHupbW0pbdalShK9qa1VtUdmW7UVdtlqX3jZpWkoX7beLVr9FXduQIFERuTq/P/w6XylqkORMPl7Px2Mfj8w5Z2benc/KO59zzpzjsCzLEgAAqNL87A4AAACuHoUOAIABKHQAAAxAoQMAYAAKHQAAA1DoAAAYwGV3AMB00dHRl9ymYcOGSk9Pr4Q00ptvvqnJkydfcN3rr7+u9u3bl/t7vv/++8rKytLAgQPL/bUBnEWhAxXsrbfeKvP4L3/5i6Kjo/Xwww97lrnd7sqOpZdeekl169Yts6xZs2YV8l4ffPCBduzYQaEDFYhCBypYu3btyjx2u92qU6fOecsr229+8xuFhYXZmuFqlJaWyrIsuVz8GgMkjqEDPmflypVKSkpS69atdfPNN2vcuHE6duxYmW06d+6sCRMm6PXXX9ctt9yiNm3aKDk5WVu3bi23HEePHtWTTz6pLl26qHXr1urTp49WrlxZZpusrCw9+eSTio+PV0xMjLp3764xY8YoOzvbs82oUaO0bt06HThwQNHR0YqOjlbv3r0lnd39Hx0dXWZ7SXr22WfVpk0bz+PCwkJFR0frxRdf1Lx589SjRw+1bt1a+/fv9zrrkSNH9Oijj3q26dKli4YNG6YTJ06U22cG2Ik/bQEfsmTJEqWkpKhv37567LHHlJmZqdmzZ2vbtm1auXKlatSo4dl2w4YN2rZtmx577DE5nU4tWLBAgwcP1po1a9S4ceNLvldpaalKSko8j/38/OTnd/Zv/BMnTujuu++WJI0cOVINGjRQRkaGJkyYoNLSUt11112SpJycHNWqVUuPP/646tSpo8OHDys1NVUDBgzQmjVrVK1aNT3yyCPKzc3V999/rzlz5khSmf+Oy/HWW2+padOmGj9+vNxut+rWret11lGjRik3N1djx45VaGiosrOztXHjRhUWFl5RFsDXUOiAjygqKtK8efPUpUsXzZo1y7P8+uuv16BBg/TOO+94iks6W6YrVqxQSEiIJKlDhw7q0aOHFixYoGnTpl3y/Xr27FnmcceOHfXqq69Kkv7+97/r2LFjWrNmjRo1aiTp7F6B3NxcvfDCC7rzzjvl5+en6OhojRs3zvMaJSUlatOmjXr16qWNGzcqLi5ON9xwg4KCguR2u6/6MEO1atW0aNGiMucczJ49+5JZHQ6Htm3bpokTJyoxMdHz3D59+lxVHsCXUOiAj9i1a5dyc3PVt2/fMss7deqkevXqafPmzWUKvX379p4yl6TAwEB16dJFX331lVfvt3DhwjInxQUEBHh+Xr9+vX73u98pLCyszCy+S5cuevfdd7V//341bdpUlmXpH//4h5YvX65Dhw4pPz/fs+3evXsVFxfn/QfghW7dup13AqG3WVu1aqUFCxaouLhYHTp0UPPmzcs1G2A3Ch3wET8fy61fv/556+rVq3fesd7g4OALbrdp0yav3i86OvqiJ8UdO3ZMX3/9tVq1anXB9bm5uZKk1NRUPfvssxoyZIg6duyo2rVrq7CwUAMGDFBRUZFXOS7HuX/AXG7WF198UXPnztX8+fM1depUhYaG6o9//KOGDh0qh8NR7lmBykahAz4iMDBQks47QUw6e9LXL79S9ssT5X7eLjQ09KqzBAUF6frrr9eYMWMuuD4iIkKStG7dOsXFxemxxx7zrNuzZ4/X71O9enVJUnFxcZnlP5fwL12oeL3NGhISoilTpmjKlCnavXu3Vq5cqeeff1716tVTcnKy15kBX0WhAz6iefPmCgoK0rp168rsdt+0aZOOHj2qDh06lNl+69atys7O9sxaT5w4oQ0bNpTLceGuXbvq7bffVuPGjRUUFHTR7U6fPn3e18Z+eXa5dParegUFBectb9CggaSzhxt+/rmoqMjrvQyXk/VczZo10xNPPKE333xTu3bt8vq9AF9GoQM+wu12a/jw4UpJSdG4ceP0hz/8wXOWe7Nmzc47tl6nTh39+c9/1kMPPeQ5y720tFTDhg276ixDhgzRBx98oPvuu08PPPCAmjRpolOnTmnPnj3atm2b5s6dK+lsmb722mt6+eWX1apVK61fv15paWnnvV5kZKRWrVql5cuXKzo6Wv7+/mrevLluvPFGhYeHa/r06SoqKpLD4dBrr70my7LKNevRo0f10EMPKTExUU2bNpXT6dT777+vgoICderU6ao/L8AXUOiADxk4cKBq1qypxYsXa/Xq1QoICFD37t31+OOPn/dVry5duug3v/mNZs2apaysLEVFRSk1NdVzpvfVCAoK0rJlyzRv3jzNnz9f2dnZuu666xQREaE//OEPnu1Gjhyp/Px8paamqqioSDfffLMWLFjg+Z75z+677z7t2LFDM2fO1MmTJ9W0aVO9//77crvdmj9/vqZMmaIxY8Z4/kg5fPiwFi9eXG5Za9asqaioKC1dulSZmZny8/NTZGSkZs+erW7dul315wX4Aod1OX8KA/AJnTt3Vvfu3ZWSkmJ3FAA+givFAQBgAAodAAADsMsdAAADMEMHAMAAFDoAAAao0l9by84+aXcE2wQEVFdeHneJshvj4BsYB9/AOFS8kJDaF13HDL2KcrmcdkeAGAdfwTj4BsbBXhQ6AAAGoNABADAAhQ4AgAEodAAADEChAwBgAAodAAADUOgAABiAQgcAwAAUOgAABqDQAQAwAIUOAIABKHQAAAxQpe+2Vt6W/OVeuyOgihnx2jt2RwAASczQAQAwAoUOAIABKHQAAAxAoQMAYAAKHQAAA1DoAAAYgEIHAMAAFDoAAAaolELPzMzU/fffrz59+ighIUGLFy+WJL333ntKSEhQixYttH37ds/27777rvr16+f5X4sWLbRz587KiAoAQJVUKVeKczqdGjt2rFq1aqW8vDwlJyerc+fOioqK0ty5czVp0qQy2/ft21d9+/aVJP3v//6vhg8frpYtW1ZGVAAAqqRKKfT69eurfv36kqSAgABFREToyJEj6ty58yWfu3btWiUkJFR0RAAAqrRKP4Z+6NAh7dy5U23btvVq+3Xr1lHoAABcQqXenOXUqVMaMWKExo8fr4CAgEtu/5///Ef+/v6Kioq64PqAgOpyuZzlHRPwmtPpp6CgmnbHuOYxDr6BcbBXpRV6cXGxRowYoaSkJP3+97/36jmX2t2el1dYXvGAK1Jaeka5ufl2x7jmBQXVZBx8AONQ8UJCal90XaXscrcsSxMmTFBERIQGDRrk1XPOnDnjOQseAAD8ukqZoX/xxRd65513FBUVpX79+kmSRo8eraKiIk2dOlXHjx/Xgw8+qJYtWyo1NVWStGXLFoWHh6tx48aVEREAgCrNYVmWZXeIK5WdfbJcX2/JX+4t19eD+Ua89g67GH0Au3p9A+NQ8Wzf5Q4AACoWhQ4AgAEodAAADEChAwBgAAodAAADUOgAABiAQgcAwAAUOgAABqDQAQAwQKXebc3XDXzxTbsjeI0rMgEAzsUMHQAAA1DoAAAYgEIHAMAAFDoAAAag0AEAMACFDgCAASh0AAAMQKEDAGAACh0AAANQ6AAAGIBCBwDAABQ6AAAGoNABADAAhQ4AgAEodAAADEChAwBgAAodAAADUOgAABiAQgcAwAAUOgAABqDQAQAwAIUOAIABKHQAAAxAoQMAYAAKHQAAA1DoAAAYwGV3AF+zbOIWuyOgChk6N87uCAAgiRk6AABGoNABADAAhQ4AgAEodAAADEChAwBgAAodAAADUOgAABiAQgcAwAAUOgAABrC90BcvXqzExEQlJCTo1VdflSS99957SkhIUIsWLbR9+3Z7AwIAUAXYWujfffedli9fruXLl+udd95RRkaG9u/fr6ioKM2dO1exsbF2xgMAoMqwtdD37NmjmJgY+fv7y+VyKTY2Vh9++KEiIyMVERFhZzQAAKoUWws9KipKX3zxhXJycnT69Gl9+umnOnz4sJ2RAACokmy921pkZKSGDBmiwYMHy9/fXy1atJCfn/d/YwQEVJfL5azAhMCvczr9FBRU0+4Y1zzGwTcwDvay/fap/fv3V//+/SVJzz//vEJDQ71+bl5eYUXFArxSWnpGubn5dse45gUF1WQcfADjUPFCQmpfdJ3tZ7kfO3ZMkvTjjz/qww8/VFJSks2JAACoemyfoT/88MPKzc2Vy+XSpEmTdN111+mjjz7S1KlTdfz4cT344INq2bKlUlNT7Y4KAIDPsr3Q33jjjfOWxcfHKz4+3oY0AABUTbbvcgcAAFePQgcAwAAUOgAABqDQAQAwAIUOAIABKHQAAAxAoQMAYAAKHQAAA9h+YRlfc9fUqnEPdq6ZDAA4FzN0AAAMQKEDAGAACh0AAANQ6AAAGIBCBwDAABQ6AAAGoNABADAAhQ4AgAEodAAADEChAwBgAAodAAADUOgAABiAQgcAwAAUOgAABqDQAQAwAIUOAIABKHQAAAxAoQMAYAAKHQAAA1DoAAAYgEIHAMAAFDoAAAag0AEAMACFDgCAASh0AAAMQKEDAGAAl90BTHS0600V/x4V/g7wRtD2HXZHAABJzNABADAChQ4AgAEodAAADEChAwBgAAodAAADUOgAABiAQgcAwAAUOgAABqDQAQAwgO1Xivv+++81atQoz+ODBw9qxIgRys3NVVpamvz8/BQcHKwZM2YoNDTUxqQAAPguh2VZlt0hflZaWqpu3bpp2bJlCgwMVEBAgCRpyZIl2r17t6ZMmVJm++zsk3bEvKTKuPQrfEOz7TuUm5tvd4xrXlBQTcbBBzAOFS8kpPZF19k+Qz/Xpk2b1LhxYzVs2LDM8tOnT8vhcNiUCgAA3+dThb527VolJiZ6Hs+ePVurVq1S7dq1tWTJEhuTAQDg23xml3tRUZG6du2qtWvXql69emXWLViwQIWFhRoxYkSZ5adPF8nlclZmTK/sbtPa7gioJNHffKPS0jN2x7jmOZ1+jIMPYBwqXrVqF+88n5mhf/rpp2rVqtV5ZS5JSUlJGjp06HmFnpdXWFnxgAsqLT3DMUMfwLFb38A4VLxfO4buM19bW7t2rRISEjyP9+3b5/k5LS1NERERNqQCAKBq8IkZen5+vjZu3FjmLPbnnntOe/fulcPhUMOGDfX000/bmBAAAN92xYVeUFAgPz8/ud3uqw5Rs2ZN/fvf/y6zbO7cuVf9ugAAXCu83uU+c+ZMbdu2TZKUkZGhm266SbGxsUpPT6+wcAAAwDteF/rq1avVvHlzSdK8efM0a9YsvfTSS5o9e3aFhQMAAN7xepf76dOn5e/vr5ycHB08eFC9evWSJP3www8VFg4AAHjH60Jv0qSJ3n33XR04cECdO3eWJB0/flw1atSosHAAAMA7Xhf6pEmTNH36dLlcLk2fPl2StGHDBk+5AwAA+/jMleKuBDdngd24OYtv4IImvoFxqHjldnOWzz77TGvXrtXx48c1f/58bd++XXl5eerYseNVhwQAAFfO67Pc//GPf2jy5Mlq0qSJtmzZIkmqUaOG5syZU2HhAACAd7wu9MWLF+uVV17R0KFD5ed39mkRERHau3dvhYUDAADe8XqX+6lTpxQeHi5JnnuTl5SUqFq1ahWTrAqrt35zhb8Hx6oAAOfyeoYeGxurhQsXllm2ZMkSdejQodxDAQCAy+P1We5ZWVkaNmyYcnNzdeTIETVq1Ei1atXSggULFBISUtE5L8hXz3KvDMzQfQPj4BsYB9/AOFS8cjnLvV69elq5cqW2b9+uH374QeHh4YqJifEcTwcAAPbxqtBLS0v129/+Vlu3blVMTIxiYmIqOhcAALgMXk2vnU6nmjRpopycnIrOAwAAroDXu9yTkpI0bNgwDRw4UGFhYWXWcWEZAADs5XWhv/nmm5KkuXPnllnucDiUlpZWvqkAAMBl8brQ09PTKzIHAAC4CpyiDgCAAbyeocfFxXmuEPdLGRkZ5ZUHAABcAa8LfdasWWUeZ2dna8mSJerTp0+5hwIAAJfH60K/6abz7/F90003aciQIXrggQfKNRQAALg8V3UM3e1269ChQ+WVBQAAXCGvZ+i/vO95QUGBPvnkE3Xr1q3cQwEAgMvjdaEfPny4zGN/f38NGjRI/fr1K/dQAADg8nhd6KNHj77gXdWys7Ntu9saAAA4y+tj6L169brg8oSEhHILAwAArozXhX6h26bn5eVd9LvpAACg8lxyl/vPF5QpLCxU9+7dy6zLzc1lhg4AgA+4ZKHPmjVLlmVp6NCheuaZZzzLHQ6HgoODFRERUaEBAQDApV2y0H++oMznn38uf3//Cg8EAAAun9dnufv7+2vnzp3aunWrcnJyyhxTHzlyZIWEAwAA3vH6pLi33npL9957rz7//HO9/PLL+u677/TKK6/owIEDFZkPAAB4wetCX7RokRYtWqR58+apRo0amjdvnubMmSOXy+tJPgAAqCBeF/qxY8fUvn37s0/y89OZM2cUFxenjz/+uMLCAQAA73g9vQ4LC9OhQ4fUqFEjNWnSRGlpaapTp46qVatWkfkAAIAXvC70IUOGaM+ePWrUqJEeeughjRw5UsXFxZowYUJF5gN82o1vtLM7Ai4ivc9GuyMAlcrrQr/jjjs8P8fFxWnz5s0qLi5WrVq1KiQYAADw3mXdDz0nJ0erVq3Syy+/LLfbrby8vPPuwgYAACqf14W+efNm9e7dW6tXr9bf/vY3SdL+/fs1efLkisoGAAC85HWhT58+XX/961+Vmprq+apa27ZttW3btgoLBwAAvON1of/www/q2LGjJHnusFatWjWVlpZWTDIAAOA1rws9MjJS69evL7Ns48aNioqKKvdQAADg8nh9lvvYsWP14IMPqnv37iooKNBTTz2l9PR0z/F0AABgn0vO0LOzsyVJ7dq107vvvqtmzZopOTlZjRo10ooVKxQTE1PhIQEAwK+75Ay9V69e+vLLLyVJoaGh+s9//qMXX3zxst5k3LhxysjIUHBwsNasWSNJ+vbbbzVp0iTl5+erYcOGevbZZxUQEKCioiJNmjRJO3bskMPh0IQJE9ShQ4cr+E8DAODacckZ+rm3SZXOfn3tct1xxx1atGhRmWUTJkzQo48+qtWrV+vWW2/1rF++fLkkafXq1XrllVc0c+ZMnTlz5rLfEwCAa8klC/3nM9qvRmxsrAIDA8ss27dvn2JjYyVJnTt31ocffihJ2r17t2dGHhwcrNq1a2vHjh1XnQEAAJNdcpd7aWmpPv/8c89MvaSkpMxjSZ6vs12O5s2bKy0tTbfeeqvef/99ZWZmSpJatGih9PR0JSYmKjMzU19//bUyMzM5Vg8AwK+4ZKEHBwdr/PjxnsdBQUFlHjscDqWlpV32G6ekpCglJUV/+9vf1LNnT7ndbklScnKy9uzZo+TkZDVo0EC//e1v5XQ6L/gaAQHV5XJdeJ3pnE4/BQXVtDsG4LP491H5+L1kr0sWenp6eoW8cWRkpP7+979Lkvbu3auMjIyzgVyuMn8w3HPPPWrSpMkFXyMvr7BCslUFQUE1lZubb3cMwGfx76Py8Xup4oWE1L7ousu6OUt5OnbsmCTpzJkzeumll3TPPfdIkk6fPq38/LP/h/jss8/kdDrVrFkzu2ICAFAleH1hmasxevRobd68WTk5OerWrZsefvhh5efn64033pAkxcfHKzk5WdLZoh88eLD8/PwUGhqqZ555pjIiAgBQpTmsX34vrQrJzj5pdwTbsGvLN/Rc18nuCLiI9D4b7Y5wzeH3UsXzyV3uAACg/FDoAAAYgEIHAMAAFDoAAAag0AEAMACFDgCAASh0AAAMQKEDAGAACh0AAANUyqVfAVN9ed9XXBnLB3CFMoAZOgAARqDQAQAwAIUOAIABKHQAAAxAoQMAYAAKHQAAA1DoAAAYgEIHAMAAFDoAAAag0AEAMACFDgCAASh0AAAMQKEDAGAACh0AAANQ6AAAGIBCBwDAABQ6AAAGoNABADAAhQ4AgAEodAAADEChAwBgAAodAAADUOgAABiAQgcAwAAUOgAABqDQAQAwgMvuAEBVVi2lrkLsDgFJYhx8xJWOQ/bwQ+Wa41rEDB0AAANQ6AAAGIBCBwDAABQ6AAAGoNABADAAhQ4AgAEodAAADEChAwBgAAodAAAD2H6luJ9++klPPvmkvvvuOzkcDk2fPl0bNmzQsmXLVLduXUnS6NGjFRcXZ3NSAAB8l+2FnpKSoq5du+qFF15QUVGRCgoKtGHDBv3pT3/S4MGD7Y4HAECVYOsu95MnT2rLli268847JUlut1vXXXednZEAAKiSbC30Q4cOqW7duho3bpxuu+02TZgwQfn5+ZKk119/XUlJSRo3bpxOnDhhZ0wAAHyew7Isy6433759u+6++269+eabatu2raZNm6aAgAANGDBAderUkcPh0Jw5c5SVlaUZM2ac9/zTp4vkcjltSG4/p9NPpaVn7I5xzauWUtfuCIARiicctztClVCt2sU7z9Zj6GFhYQoLC1Pbtm0lSb1799bChQtVr149zzb9+/fXsGHDLvj8vLzCSsnpi4KCaio3N9/uGNc8btkJlA9+n3knJKT2RdfZuss9JCREYWFh+v777yVJmzZtUmRkpLKysjzb/Otf/1Lz5s3tiggAQJVg+1nuEydO1GOPPabi4mI1btxYM2bM0LRp0/Ttt99Kkho2bKgpU6bYnBIAAN9m6zH0q5WdfdLuCLZhl7tvCJnXyO4IgBGyhx+yO0KV4LO73AEAQPmg0AEAMACFDgCAASh0AAAMQKEDAGAACh0AAANQ6AAAGIBCBwDAALZfKQ6oyoonHOcCPz6ACy35BsbBXszQAQAwAIUOAIABKHQAAAxAoQMAYAAKHQAAA1DoAAAYgEIHAMAAFDoAAAag0AEAMACFDgCAASh0AAAMQKEDAGAACh0AAANQ6AAAGIBCBwDAABQ6AAAGoNABADAAhQ4AgAEodAAADEChAwBgAAodAAADUOgAABiAQgcAwAAUOgAABqDQAQAwAIUOAIABKHQAAAzgsjsAUJU1n/i+3REA+LAtj3artPdihg4AgAEodAAADEChAwBgAAodAAADUOgAABiAQgcAwAAUOgAABqDQAQAwgO0XliksLNQf//hHFRUVqbS0VL169dKIESM0fvx47dixQ5ZlqWnTppoxY4Zq1apld1wAAHyS7YXudru1ePFi1apVS8XFxbrvvvvUrVs3jR8/XgEBAZKkGTNm6PXXX9fQoUNtTgsAgG+yfZe7w+HwzLxLSkpUUlIih8PhKXPLslRQUGBnRAAAfJ7thS5JpaWl6tevnzp16qROnTqpbdu2kqRx48apc+fO+v7773X//ffbnBIAAN/lsCzLsjvEz3766ScNHz5cEydOVFRUlKSzZT916lS1adNGycnJZbY/fbpILpfTjqi2czr9VFp6xu4Y1zxuzgLg1+ya2rtcX69atYt3nu3H0M913XXXqUOHDlq/fr2n0J1OpxISErRo0aLzCj0vr9COmD4hKKimcnPz7Y4BAPgV5f17OiSk9kXX2b7L/fjx4/rpp58kSQUFBdq4caOaNm2q/fv3Szp7DD09PV0RERF2xgQAwKfZPkPPysrS2LFjVVpaKsuy1Lt3b3Xv3l333XefTp06JcuyFB0draefftruqAAA+CzbC71FixZatWrVecuXLl1qQxoAAKom23e5AwCAq0ehAwBgAAodAAADUOgAABiAQgcAwAAUOgAABqDQAQAwAIUOAIABKHQAAAxg+5XigKps19Te3CTHB3CzIt/AONiLGToAAAag0AEAMACFDgCAASh0AAAMQKEDAGAACh0AAANQ6AAAGIBCBwDAABQ6AAAGoNABADAAhQ4AgAEodAAADOCwLMuyOwQAALg6zNABADAAhQ4AgAEodAAADEChVxG5ubkaNGiQfv/732vQoEE6ceLEBbdr2bKl+vXrp379+mnYsGGVnNJcn376qXr16qX4+HgtXLjwvPVFRUV65JFHFB8fr/79++vQoUM2pDTfpcbhn//8p26++WbPv4Hly5fbkNJ848aNU8eOHZWYmHjB9ZZladq0aYqPj1dSUpK+/vrrSk54jbJQJcycOdNasGCBZVmWtWDBAuuZZ5654Hbt2rWrzFjXhJKSEuuWW26xDhw4YBUWFlpJSUnWrl27ymzz2muvWRMnTrQsy7LWrFljjRw50o6oRvNmHFauXGk9/fTTNiW8dmzevNnasWOHlZCQcMH1GRkZ1uDBg60zZ85Y//M//2PdeeedlZzw2sQMvYpIS0vTbbfdJkm67bbb9K9//cvmRNeObdu26YYbblDjxo3ldruVkJCgtLS0Mtukp6fr9ttvlyT16tVLmzZtksUXSMqVN+OAyhEbG6vAwMCLrv/595XD4VC7du30008/KSsrqxITXpso9Cri2LFjql+/viQpJCREx44du+B2hYWFuuOOO3TXXXdR+uXkyJEjCgsL8zwODQ3VkSNHztsmPDxckuRyuVS7dm3l5ORUak7TeTMOkvThhx8qKSlJI0aMUGZmZmVGxP/3y7EKCwu74FihfLnsDoD/86c//UlHjx49b/kjjzxS5rHD4ZDD4bjga3z88ccKDQ3VwYMH9cADDygqKkrXX399heQFfE2PHj2UmJgot9utpUuX6oknntCSJUvsjgVUCgrdh7z66qsXXRccHKysrCzVr19fWVlZqlu37gW3Cw0NlSQ1btxYN910k7755hsK/SqFhobq8OHDnsdHjhzxfM7nbpOZmamwsDCVlJTo5MmTqlOnTmVHNZo343DuZ96/f3/NmjWr0vLh//xyrA4fPnzeWKH8scu9iujZs6dWrVolSVq1apVuueWW87Y5ceKEioqKJEnHjx/Xl19+qWbNmlVqThO1adNG+/bt08GDB1VUVKS1a9eqZ8+eZbbp2bOn3n7A0bxOAAAGRklEQVT7bUnSBx98oJtvvvmie1FwZbwZh3OP06anpysyMrKyY0L/9/vKsix99dVXql27tueQISoOl36tInJycvTII48oMzNTDRo00F//+lcFBQVp+/btWrp0qVJSUvTll19q0qRJcjgcsixLAwcOVP/+/e2OboRPPvlE06dPV2lpqZKTk/Vf//VfmjNnjlq3bq1bbrlFhYWFevzxx7Vz504FBgZq9uzZaty4sd2xjXOpcXjuueeUnp4up9OpwMBATZ48mVKvAKNHj9bmzZuVk5Oj4OBgPfzwwyopKZEk3XvvvbIsS1OmTNH69evl7++v6dOnq02bNjanNh+FDgCAAdjlDgCAASh0AAAMQKEDAGAACh0AAANQ6AAAGIBCBwDAABQ6UMX07NlTGzdutDuG7r//fm5PCvgQLv0K4LJYlsWd5AAfxIVlgCqmZ8+emjZtmg4fPqxly5YpJiZG//znPxUYGKhZs2Zp3759mjNnjoqKijRmzBjPbV3Hjh0rt9utgwcP6quvvlKrVq00c+ZMNWzYUJL05ZdfKiUlRfv27VOTJk00YcIE3XjjjZLOzsZvvPFG/fvf/9Y333yj+Ph4rVu3Ti6XSy6XS7fffrueeuopTZs2TR999JFOnjypJk2aaPz48Wrfvr0kae7cudq9e7eqV6+ujz76SA0aNNB///d/e64glpmZqZSUFG3dulWWZSkhIUFPPfWUJGnFihVKTU3V0aNHFRMToylTpnhyA/j/bLkLO4Ar1qNHD+uzzz6zVq5cabVs2dJasWKFVVJSYj3//PNWXFycNXnyZKuwsNBav3691a5dOysvL8+yLMt64oknrHbt2lmbN2+2CgsLralTp1r33HOPZVmWlZOTY7Vv3956++23reLiYmv16tVW+/btrePHj1uWZVkDBgyw4uLirO+++84qLi62ioqKrAEDBljLli0rk23VqlXW8ePHreLiYis1NdXq1KmTVVBQYFmWZb3wwgtW69atrYyMDKukpMR69tlnrf79+1uWZVklJSVWUlKSlZKSYp06dcoqKCiwtmzZYlmWZX300UfWrbfeau3evdsqLi625s2bZ919992V8lkDVQnH0IEqrFGjRkpOTpbT6VSfPn2UmZmp4cOHy+12q0uXLnK73Tpw4IBn++7duys2NlZut1ujRo3SV199pczMTGVkZOiGG27QbbfdJpfLpcTEREVEROjjjz/2PPf2229X8+bN5XK5VK1atQvm6devn+rUqSOXy6U///nPKioq0t69ez3rf/e73ykuLk5Op1P9+vXTt99+K0natm2bsrKyNGbMGNWsWVPVq1f3zOyXLl2qoUOHKjIyUi6XS8OGDdPOnTv1ww8/VMRHClRZHEMHqrDg4GDPzzVq1JAk1atXz7OsevXqOnXqlOdxWFiY5+datWopMDBQWVlZysrKUoMGDcq8doMGDXTkyBHP4/Dw8EvmSU1N1YoVK5SVlSWHw6G8vDzl5OR41p+brUaNGiosLFRJSYnnpkMu1/m/kn788UdNnz5dM2fO9CyzLEtHjhxhtztwDgoduIace4/qU6dO6cSJE6pfv77q16+vH3/8scy2mZmZ6tq1q+fxpW4Hu3XrVi1atEivvvqqmjdvLj8/P8XGxnp1Al14eLgyMzNVUlJyXqmHh4dr2LBh6tu3rzf/icA1i13uwDXkk08+0datW1VUVKQ5c+aobdu2Cg8PV1xcnPbt26fVq1erpKRE69at0+7du9W9e/eLvla9evV08OBBz+NTp07J6XSqbt26Kikp0Ysvvqi8vDyvcsXExCgkJETPPfec8vPzVVhYqC+++EKSdM8992jhwoXatWuXJOnkyZN67733rvxDAAxFoQPXkMTERM2bN08dOnTQ119/rVmzZkmS6tSpo/nz5+uVV15Rhw4dtGjRIs2fP19169a96GsNHDhQH3zwgWJjYzVt2jR16dJFXbt2Va9evdSzZ09Vr17dq930kuR0OjV//nzt379fPXr0ULdu3TylHR8fryFDhmj06NG68cYblZiYqE8//fTqPwzAMHxtDbhGjB07VqGhoRo1apTdUQBUAGboAAAYgEIHAMAA7HIHAMAAzNABADAAhQ4AgAEodAAADEChAwBgAAodAAADUOgAABjg/wGEu6K48Vu8IgAAAABJRU5ErkJggg==\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Hyperparameters search for logreg with *GridSearchCV***"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_lr = {'class_weight' : ['balanced', None], \n                'penalty' : ['l2','l1'],  \n                'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n           }","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_lr = GridSearchCV(estimator = logreg0, param_grid = param_lr , scoring = 'roc_auc', verbose = 1, n_jobs = -1, cv=repfold)\n\ngrid_lr.fit(X_train,y_train)\n\nprint(\"Best Score:\" + str(grid_lr.best_score_))\nprint(\"Best Parameters: \" + str(grid_lr.best_params_))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters_lr = grid_lr.best_params_\nlogreg = LogisticRegression(**best_parameters_lr)\nlogreg.fit(X_train,y_train)\nsc = cross_val_score(logreg, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())\n\nselector_lr = RFE(logreg, 25, step=1)\nselector_lr.fit(X_train,y_train)\nsc = cross_val_score(selector_lr, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyperparameters search for logreg with *hyperopt***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def acc_model(params):\n    clf = LogisticRegression(**params)\n    return cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=repfold).mean()","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space4lr = {'C': hp.uniform('C', .0001, 100.0), \n            'solver' : hp.choice('solver', ['liblinear']),\n            'penalty' : hp.choice('penalty', ['l1', 'l2']),\n            #'dual' : hp.choice('dual', [True, False]),\n            #'fit_intercept': hp.choice('fit_intercept', ['True', 'False']),\n            'class_weight': hp.choice('class_weight', ['balanced', None]),\n            'max_iter': hp.choice('max_iter', [50000]),\n            'random_state': 78, #hp.uniformint('random_state', 1, 100),\n            #'n_jobs': -1\n           }\n\nbest = 0\npr = []\ndef f(params):\n    global best\n    acc = acc_model(params)\n    if acc > best:\n        best = acc\n        print ('new best:', best, params)\n        pr.append(params)\n    return {'loss': -acc, 'status': STATUS_OK}\n\ntrials = Trials()\nbest = fmin(f, space4lr, algo=tpe.suggest, max_evals=3000, trials=trials)\nprint ('best for logreg: ')\nprint (pr[-1])","execution_count":40,"outputs":[{"output_type":"stream","text":"new best:                                             \n0.7386111111111111                                    \n{'C': 88.24353361414632, 'class_weight': 'balanced', 'max_iter': 50000, 'penalty': 'l2', 'random_state': 78, 'solver': 'liblinear'}\nnew best:                                                                         \n0.7613368055555554                                                                \n{'C': 72.47812008319777, 'class_weight': 'balanced', 'max_iter': 50000, 'penalty': 'l1', 'random_state': 78, 'solver': 'liblinear'}\nnew best:                                                                         \n0.7617881944444445                                                                \n{'C': 74.827631786164, 'class_weight': None, 'max_iter': 50000, 'penalty': 'l1', 'random_state': 78, 'solver': 'liblinear'}\nnew best:                                                                         \n0.7631597222222223                                                                \n{'C': 34.28366314606136, 'class_weight': 'balanced', 'max_iter': 50000, 'penalty': 'l1', 'random_state': 78, 'solver': 'liblinear'}\n  0%|          | 7/3000 [00:12<1:33:00,  1.86s/it, best loss: -0.7631597222222223]\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-36c56615dddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace4lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'best for logreg: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-36c56615dddf>\u001b[0m in \u001b[0;36mf\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-39-39f9fe1da712>\u001b[0m in \u001b[0;36macc_model\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0macc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    752\u001b[0m             tasks = BatchedCalls(itertools.islice(iterator, batch_size),\n\u001b[1;32m    753\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nested_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                                  self._pickle_cache)\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                 \u001b[0;31m# No more tasks available in the iterator: tell caller to stop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterator_slice, backend_and_jobs, pickle_cache)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_and_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend_and_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdelayed\u001b[0;34m(function, check_pickle)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mdelayed_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;34m\" functools.wraps fails on some callable objects \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/functools.py\u001b[0m in \u001b[0;36mupdate_wrapper\u001b[0;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pr[-1])\nbest = pr[-1]\nlogreg1 = LogisticRegression(**best)\nlogreg1.fit(X_train,y_train)\nsc = cross_val_score(logreg1, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())\n\nselector_lr1 = RFE(logreg1, 25, step=1)\nselector_lr1.fit(X_train,y_train)\nsc = cross_val_score(selector_lr1, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit simple Lasso**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lass0.fit(X_train, y_train)\nsc = cross_val_score(lass0, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(lass0, top=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"el_df =pd.Series(lass0.coef_,index=train.drop(['id', 'target'], axis=1).columns)\nel_df = el_df[(lass0.coef_<=-0.05) | (lass0.coef_>=0.05)].sort_values(ascending=False)\nplt.figure(figsize=(8,6))\nel_df.plot(kind='barh')\nplt.xlabel(\"Importance\",fontsize=12)\nplt.ylabel(\"Features\",fontsize=12)\nplt.title(\"Top Features\",fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyperparameters search for Lasso with *GridSearchCV***"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_lass = {\n            'alpha' : [0.022, 0.021, 0.02, 0.019, 0.023, 0.024, 0.025, 0.026, 0.027, 0.029, 0.031],\n            'tol'   : [0.0013, 0.0014, 0.001, 0.0015, 0.0011, 0.0012, 0.0016, 0.0017]\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_lass = GridSearchCV(estimator = lass0, param_grid = param_lass , scoring = 'roc_auc', verbose = 1, n_jobs = -1, cv=repfold)\n\ngrid_lass.fit(X_train,y_train)\n\nprint(\"Best Score:\" + str(grid_lass.best_score_))\nprint(\"Best Parameters: \" + str(grid_lass.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters_lass = grid_lass.best_params_\nlass = Lasso(**best_parameters_lass)\nlass.fit(X_train,y_train)\nsc = cross_val_score(lass, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())\n\nselector_lass = RFE(lass, 5, step=1)\nselector_lass.fit(X_train,y_train)\nsc = cross_val_score(selector_lass, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyperparameters search for Lasso with *hyperopt***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def acc_model(params):\n    clf = Lasso(**params)\n    return cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=repfold).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space4lass = {'alpha' : hp.uniform('alpha', .0001, 1),\n            'tol'   : hp.uniform('tol', .0001, 1),\n            'random_state': 78, #hp.uniformint('random_state', 1, 100),\n            'max_iter': hp.choice('max_iter', [50000]),\n             }\n\nbest = 0\npr = []\ndef f(params):\n    global best\n    acc = acc_model(params)\n    if acc > best:\n        best = acc\n        print ('new best:', best, params)\n        pr.append(params)\n    return {'loss': -acc, 'status': STATUS_OK}\n\ntrials = Trials()\nbest = fmin(f, space4lass, algo=tpe.suggest, max_evals=3000, trials=trials)\nprint ('best for lasso: ')\nprint (pr[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pr[-1])\nbest = pr[-1]\nlass1 = Lasso(**best)\nlass1.fit(X_train,y_train)\nsc = cross_val_score(lass1, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())\n\nselector_lass1 = RFE(lass1, 5, step=1)\nselector_lass1.fit(X_train,y_train)\nsc = cross_val_score(selector_lass1, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit simple Ridge**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridg0.fit(X_train, y_train)\nsc = cross_val_score(ridg0, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyperparameters search for Ridge with *hyperopt***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def acc_model(params):\n    clf = Ridge(**params)\n    return cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=repfold).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space4ri = {'alpha' : hp.uniform('alpha', 0.01, 1000),\n            'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n            'tol' : hp.uniform('tol', .0001, 1),\n            'random_state' : 78, #hp.uniformint('random_state', 1, 100),\n            'max_iter' : hp.choice('max_iter', [30000]),\n            'solver' : hp.choice('solver', ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'])\n             }\n\nbest = 0\npr = []\ndef f(params):\n    global best\n    acc = acc_model(params)\n    if acc > best:\n        best = acc\n        print ('new best:', best, params)\n        pr.append(params)\n    return {'loss': -acc, 'status': STATUS_OK}\n\ntrials = Trials()\nbest = fmin(f, space4ri, algo=tpe.suggest, max_evals=3000, trials=trials)\nprint ('best for Ridge: ')\nprint (pr[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pr[-1])\nbest = pr[-1]\nridg1 = Ridge(**best)\nridg1.fit(X_train,y_train)\nsc = cross_val_score(ridg1, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())\n\nselector_ridg1 = RFE(ridg1, 5, step=1)\nselector_ridg1.fit(X_train,y_train)\nsc = cross_val_score(selector_ridg1, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit simple SGDClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd0.fit(X_train, y_train)\nsc = cross_val_score(sgd0, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def acc_model(params):\n    clf = SGDClassifier(**params)\n    return cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=repfold).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space4sgd = {'loss': hp.choice('loss', ['log', 'modified_huber']),\n             'penalty': hp.choice('penalty', ['l1', 'l2', 'elasticnet']),\n             'alpha' : hp.uniform('alpha', 0.01, 1000),\n             'fit_intercept' : False, #hp.choice('fit_intercept', [True, False]),\n             'tol' : hp.uniform('tol', .0001, 1),\n             'random_state' : 78, #hp.uniformint('random_state', 1, 100),\n             'max_iter' : hp.choice('max_iter', [30000]),\n             'l1_ratio': hp.choice('l1_ratio', [0, 0.15, 0.5, 1.0]),\n             'learning_rate': hp.choice('learning_rate', ['optimal', 'invscaling', 'adaptive']),\n             'shuffle' : True,\n             #'n_jobs': -1,\n             'eta0': 1\n             }\n\nbest = 0\npr = []\ndef f(params):\n    global best\n    acc = acc_model(params)\n    if acc > best:\n        best = acc\n        print ('new best:', best, params)\n        pr.append(params)\n    return {'loss': -acc, 'status': STATUS_OK}\n\ntrials = Trials()\nbest = fmin(f, space4sgd, algo=tpe.suggest, max_evals=2000, trials=trials)\nprint ('best for SGDClassifier: ')\nprint (pr[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pr[-1])\nbest = pr[-1]\nsgd1 = SGDClassifier(**best)\nsgd1.fit(X_train,y_train)\nsc = cross_val_score(sgd1, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())\n\nselector_sgd1 = RFE(sgd1, 5, step=1)\nselector_sgd1.fit(X_train,y_train)\nsc = cross_val_score(selector_sgd1, X_train, y_train, scoring='roc_auc', cv=repfold, n_jobs=-1, verbose=1)\nprint(sc.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sub_pr(clf, filename):\n    prediction_ = clf.predict_proba(X_test)[:,1]\n    submission_ = pd.read_csv('../input/sample_submission.csv')\n    submission_['target'] = prediction_\n    submission_.to_csv(filename, index=False)\n    print(submission_.head())\n    \ndef sub_prp(clf, filename):\n    prediction_ = clf.predict(X_test)\n    submission_ = pd.read_csv('../input/sample_submission.csv')\n    submission_['target'] = prediction_\n    submission_.to_csv(filename, index=False)\n    print(submission_.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfilename = 'subm_{}_{}_'.format(ver, datetime.now().strftime('%Y-%m-%d'))\nfilename","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_pr(logreg1, filename+'lr1.csv') # 0.848\nsub_pr(selector_lr1, filename+'sel_lr1.csv') # 0.845","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_prp(lass1, filename+'lass1.csv')\nsub_prp(selector_lass1, filename+'sel_lass1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_pr(ridg1, filename+'ridg1.csv')\nsub_pr(selector_ridg1, filename+'sel_ridg1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_prp(sgd1, filename+'sgd1.csv')\nsub_prp(selector_sgd1, filename+'sel_sgd1.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}