{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport keras\nimport tensorflow as tf\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\nimport gc\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nimport matplotlib.pyplot as plt\nfrom time import time, sleep\nimport seaborn as sns\nfrom IPython.display import display, clear_output\nsns.set()\ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f85fe718540f2ed14648f7f3965e0d1b42fc5c0","_kg_hide-input":true},"cell_type":"code","source":"def trainingPlot(hh):\n    ks = list(hh.keys())\n    fig = plt.figure(figsize=(20,9))\n    fig.add_subplot(1,2,1)\n    plt.plot(hh[ks[2]], label=\"train\")\n    plt.plot(hh[ks[0]], label=\"valid\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(ks[2])\n    plt.legend()\n    plt.grid(True)\n    fig.add_subplot(1,2,2)\n    plt.plot(hh[ks[3]], label=\"train\")\n    plt.plot(hh[ks[1]], label=\"valid\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(ks[3])\n    plt.grid(True)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9ed10a7ffed048bfaf287f6e8e5708dbc404aee"},"cell_type":"markdown","source":"## Load the preencoded data based on https://www.kaggle.com/bogorodvo/lightgbm-baseline-model-using-sparse-matrix\nBecause of the memory and training and predicting time we will use just a 2.5M rows of the data."},{"metadata":{"_uuid":"ac5abaf7a4a3947a6f97abe783fca81981eabf42","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\n#%% Load the preencoded data based on https://www.kaggle.com/bogorodvo/lightgbm-baseline-model-using-sparse-matrix\nX = pd.read_pickle(\"../input/forked-from-lightgbm-to-get-all-as-category/train.pkl\")\n# test = pd.read_pickle(\"../input/forked-from-lightgbm-to-get-all-as-category/test.pkl\")\n\n#%% Feature selection based on https://www.kaggle.com/jiegeng94/everyone-do-this-at-the-beginning and others.\nused_cols = [i for i in X.columns if i not in [\"MachineIdentifier\", \"HasDetections\",\n                                                \"PuaMode\", \"Census_ProcessorClass\",  # mostly missing\n                                                \"Census_IsWIMBootEnabled\",\"IsBeta\",\n                                                \"Census_IsFlightsDisabled\",\"Census_IsFlightingInternal\",\n                                                \"AutoSampleOptIn\",\"Census_ThresholdOptIn\",\n                                                \"SMode\",\"Census_IsPortableOperatingSystem\",\n                                                \"Census_DeviceFamily\",\"UacLuaenable\", \"Census_IsVirtualDevice\",  # too skewed columns\n                                                \"Census_OSSkuName\",    # hightly-correlated features\n                                                \"Processor\", \"Census_OSInstallLanguageIdentifier\", \"train\"]]\n\ny = X[\"HasDetections\"]\nX = X[used_cols]\n\nencoding_map = {}\nembedded_layer_parameters = {}\nfor col in used_cols:\n    key_map = {i:n for n,i in enumerate(X[col].cat.categories)}\n    encoding_map[col] = key_map\n    X[col] = X[col].map(key_map).astype(\"category\")\n    embedded_layer_parameters[col] = len(X[col].unique())\n    \nchunk_size = int(2.5e6)\nX = X.loc[:chunk_size-1]\ny = y[:chunk_size]\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"227a79eb3da75cfc9c7ed939dc3a4d5e627053e9"},"cell_type":"markdown","source":"### Unfortunately keras doesn't have a AUC/ROC as metrics. Therefore I use the the following two functions as a workaround. "},{"metadata":{"trusted":true,"_uuid":"38f9949356fd51f9530595ef2d54cbfcc5b3eb35"},"cell_type":"code","source":"#%% Some helper function\ndef my_metric_func(y_true, y_pred):\n    try:\n        score = roc_auc_score(y_true, y_pred)\n    except:\n        score = 0.5\n    return score\n\ndef auroc(y_true, y_pred):\n    return tf.py_func(my_metric_func, (y_true, y_pred), tf.double)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2595404a97f26f413fe0fecaf44cafc713834940"},"cell_type":"markdown","source":"### The following function creates a NN model with ```keras.layers.Embedding``` for all categorical features followed by two ```Dense``` ```Dropout``` ```BatchNormalization``` layers. The ```use_in_prediction``` parameter controls the dropout-rate while predicting. ```use_in_prediction = False``` --> Dropoute-rate == 0.0 | ```use_in_prediction = True``` --> Dropoute-rate != 0.0"},{"metadata":{"trusted":true,"_uuid":"0d22ddb24ab5b167f87c21ad3c47873dffc8dd1b"},"cell_type":"code","source":"#%% Model creating based on https://www.kaggle.com/learn/embeddings\ndef create_model(embedded_layer_parameters, use_in_prediction=True):\n    \n    hidden_units = (500,500)\n    dpo_values = (0.8, 0.8)\n    embedding_size = 4\n    embedded = []\n    inputs = []\n    \n    for col in embedded_layer_parameters.keys():\n        input_layer = keras.Input(shape=(1,), name=col)\n        embedded_layer = keras.layers.Embedding(input_dim=embedded_layer_parameters[col], \n                                                output_dim=embedding_size, \n                                                input_length=1, name=f\"{col}_emb\")(input_layer)\n        \n        inputs.append(input_layer)\n        embedded.append(embedded_layer)\n    \n    concatenated = keras.layers.Concatenate()(embedded)\n    out = keras.layers.Flatten()(concatenated)\n    \n    for n_hidden, dpo_val in zip(hidden_units, dpo_values):\n        out = keras.layers.Dense(n_hidden, activation='relu')(out)\n        out = keras.layers.Dropout(dpo_val)(out, training=use_in_prediction)\n        out = keras.layers.BatchNormalization()(out)\n    \n    out = keras.layers.Dense(1, activation='sigmoid')(out)\n    \n    model = keras.Model(inputs = inputs, outputs = out,)\n    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[auroc]) #'accuracy'\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e588f2e9f3438f456c2ccde415a1de242ceb859"},"cell_type":"markdown","source":"### Now we have to know the indices of the dropout layers."},{"metadata":{"trusted":true,"_uuid":"ff5224691709073ed6a61842e4d7499331a5e515"},"cell_type":"code","source":"model = create_model(embedded_layer_parameters, use_in_prediction=True)\n\nfor n,i in enumerate(model.layers):\n    if \"Dropout\" in str(i):\n        print(n,i)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c7b6c9a89ca0a1b6a0d53e9a12e5179d97f94e3"},"cell_type":"markdown","source":"## Control the dropout-range while training\n### The idea behind the ```dropout_control``` callback class is to add more variance to the model. Before every batch the dropout-rate changes in a specific range."},{"metadata":{"trusted":true,"_uuid":"3c7ef30a0814db8c7aeb98fa0616425fe2d08e50"},"cell_type":"code","source":"class dropout_control(keras.callbacks.Callback):\n    def __init__(self, rate_min=0.3, rate_max=1):\n#         super(printAUC, self).__init__()\n        self.rate_min = rate_min\n        self.rate_max = rate_max\n        \n    def on_batch_begin(self, batch, logs={}):\n        self.model.layers[133].rate = np.random.uniform(self.rate_min, self.rate_max, 1)[0]\n        self.model.layers[136].rate = np.random.uniform(self.rate_min, self.rate_max, 1)[0]\n        return","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f8a1569058d952ed838f4a496dbe1ff5ba6e2b0"},"cell_type":"markdown","source":"## Training\n### The large size of the NN shoud lead to more bias and the stochastically dropout-range while predicting should add more variance to the model."},{"metadata":{"trusted":true,"_uuid":"e91c042d86c483fbe2c6ca4c17727fb0ad5d9a66"},"cell_type":"code","source":"X, X_test, y, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b29ff448d73ea52a97e43f1c8f90446cd1e3edd6","_kg_hide-output":false},"cell_type":"code","source":"dpo = dropout_control(0.7, 0.95)\nmodel = create_model(embedded_layer_parameters, use_in_prediction=True)\nhistory = model.fit([X[col].values for col in X.columns], y, batch_size=100000, epochs=100, verbose=0, callbacks=[dpo], validation_split=0.01)\nmodel.save(f'my_model.h5')\ntrainingPlot(history.history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37daa68a99bb3a8335b742eef3fad0f25e309dbf"},"cell_type":"markdown","source":"## Testing\n### Now lets create some different models out of the trained model. \n>  ```model_normal``` with ```dropout_rate = 0.0```  <br>\n> ```model_dpo``` with ```dropout_rate = 0.8``` <br>\n> ```dpo_with_range``` with ```dropout_rate_range = 0.0 - 0.8```"},{"metadata":{"trusted":true,"_uuid":"54a566aca628b0b1a277cea0878cbe8a972a36e4"},"cell_type":"code","source":"model_bagg = {}\n\nmodel_normal = create_model(embedded_layer_parameters, use_in_prediction=False)\nmodel_normal.load_weights(\"my_model.h5\")\nmodel_bagg[\"normal\"] = model_normal\n\nmodel_dpo_on = create_model(embedded_layer_parameters, use_in_prediction=True)\nmodel_dpo_on.load_weights(\"my_model.h5\")\nmodel_bagg[\"dpo_on\"] = model_dpo_on\n\nmodel_bagg[\"dpo_with_range\"] = model_dpo_on","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efed83761e1c8e0a8e19a808b126ca7509ab4dda"},"cell_type":"code","source":"X_test = [X_test[col].values for col in X_test.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03e917d01044c80d9cde2c04a333172604050ede","scrolled":true},"cell_type":"code","source":"%%time\ndpo = dropout_control(0, 0.9)\nfig = plt.figure(figsize=(20,9))\nfor name,model in model_bagg.items():\n\n    n_estimators = 200\n    scores = []\n    predictions = np.zeros((X_test[0].shape[0], n_estimators))\n\n    if name == \"normal\":\n        n_estimators = 2\n    for n in range(n_estimators):\n        \n        predictions[:,n] = model.predict(X_test, verbose=0, batch_size=100000)[:,0]\n        if name == \"dpo_with_range\":\n            model.layers[133].rate = np.random.uniform(dpo.rate_min, dpo.rate_max, 1)[0]\n            model.layers[136].rate = np.random.uniform(dpo.rate_min, dpo.rate_max, 1)[0]\n        \n        y_pred = np.mean(predictions[:,:n+1], axis=1)\n        res = roc_auc_score(y_test, y_pred)\n        print(f\"\\r{name} {n+1} | auroc: {res*100:.4f}%\", flush=True, end=\"\")\n        scores.append(res*100)\n    print(\"\")\n    x_ax = np.linspace(1,predictions.shape[1]+1, len(scores))\n    plt.plot(x_ax, scores, \"-\", label=name)\n\nplt.xlabel(\"estimators\")\nplt.ylabel(\"roc score\")\nplt.grid(True)\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e07c13c9f0b3f4962c57a66fba90c37063b2461"},"cell_type":"markdown","source":"## Conclusion\n### The results show that it's possible to increase the accuracy of a NN by using the dropout-layer while training as well as while predicting. The increase of the accuracy in this simpe example isn't very high but with some tuning it's possible to get more out of the NN, I believe."},{"metadata":{"_uuid":"34a6b4c6110ea9f0617d63092a7fe347102a4393"},"cell_type":"markdown","source":"## Feedback\n### It's basicly my first kernel with the intend to share information. To improve my kernel skills I would be happy to get some feedback on the content as well as on the every thing you think it's important like style or the English language."},{"metadata":{"trusted":true,"_uuid":"b0e114d91de7dcfd2da389007e48f81237fc27fa"},"cell_type":"code","source":"del X, X_test, y, y_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62995c96e68fd39b8aeb0e8c0957ca81771468dd"},"cell_type":"markdown","source":"# Let's make a prediction for LB with 100 estimators"},{"metadata":{"trusted":true,"_uuid":"7015e7ee11d541a9601a651a0e7a8e0ebab767a0"},"cell_type":"code","source":"def chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da5de4332a5e15198aec09ca88daed72e1796d00","scrolled":false},"cell_type":"code","source":"%%time\nchunk = int(2e6) #7853253\n###################### Load, encode and transform test data ############################\ntest = pd.read_pickle(\"../input/forked-from-lightgbm-to-get-all-as-category/test.pkl\")\ntest = test[used_cols]\n\nfor col, key_map in encoding_map.items():\n    print(f\"\\r{col}\", flush=True, end=\"\")\n    test[col] = test[col].map(key_map).astype(\"category\")\n    \npredictions = np.zeros(test.shape[0])\nmodel = model_bagg[\"dpo_with_range\"]\nprint(\"Predicting.\")\nfor m,x in enumerate(chunker(test, chunk)):\n    print(f\"\\rchunk: {m}\", flush=True, end=\"\")\n    start = x.index[0]\n    end = x.index[-1]+1\n    \n    x = [x[col].values for col in test.columns]\n    gc.collect()    \n\n    n_estimators = 100\n    blend = np.zeros((x[0].shape[0], n_estimators))\n    for j in range(n_estimators):\n        blend[:,j] = model.predict(x, verbose=0, batch_size=100000)[:,0]\n\n    predictions[start:end] = np.mean(blend, axis=1)\n\nprint(\"\\nDone\")\nb = plt.hist(predictions, bins=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3222b6939a784c439bcb51a8e484147747a9f75"},"cell_type":"code","source":"%%time\nmy_submission = pd.read_csv(f'../input/microsoft-malware-prediction/sample_submission.csv')\nmy_submission['HasDetections'] = predictions\nmy_submission.to_csv(f'submission.csv', index=False)\nmy_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63eb8a03fafd88f8d7fa8f26b21e6c9494116b99"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8881e978f3cdaef5b9a02fd01bafd4d0498a4d31"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56b7fe54eab8cdad75ec163565c42bc650c5cda6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d431f947e2fb3beb86a60b03046c3ff774f1c952"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93354cc86e6eb7f60ce3f8405c422ef37419dfed"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96d3111a6ce385d17afd857624838f213e011594"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b66ceac566fa8ddad621a67f66349197a8a5bfc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}