{"cells":[{"metadata":{"_uuid":"638a679df112bc4baf47ad2251a92cbceeca99f9"},"cell_type":"markdown","source":"# ***INTRODUCTION***\n\nHello Everyone!\n\nI am very excited to share my capstone project for the UDACITY Machine Learning Nanodegree. I thought about sharing it to help other ND students who, like me, must go over the process of picking a project, and make it happen! This can be a daunting decision given the immense possibilities out there, so I hope this helps others on cohorts after me. In my case I decided to go with a project that took me away from my comfort zone as a mean to not only practice the ML concepts, but also to challenge me to gain Domain Knowledge in an industry that I look forward to getting involved with.\n\nThe project information is located on my [GitHub](https://github.com/lambertopisani/Udacity_ML_Projects/tree/master/6_Capstone). You'll see my other projects I created along the Nanodegree so feel free to review all of them and provide any input or guidance to improve them.\n\nThis notebook/kernel was modified from its original version, so it can work on Python 3 (the project was based on Python 2.7). \nThe objective of the project was(is):\n*This project’s objective is to define directionality of the closing price of a ETF instrument “M” days in the future relative to its daily closing price i.e. whether its price is predicted to increase (up) or decrease (down).*\n\nMany of the studies I evaluated during my initial research had similar characteristics. Below what I found on the studies and, to share with you some background in the decision-making process I followed, I also include what I ended up doing:\n- Most studies were dedicated to company stocks, Because I was trying to look for a \"safer\" investment vehicle, I selected Technology ETFs as opposed to specific individual company stocks. Also, by predicting on each ETF, we could pick the \"best\" performing one.\n- Most studies defined rather \"fixed\" values to generate some of the features of the analysis (i.e. momentum of the ETF is dependent of the number of days picked, which was fixed in most studies).  This made me wonder why or how this decision was made, how would the author know whether 10 days is better than 25 for calculating momentum to maximize the performance of their ML algorithm. Because of this I decided to follow a Sensitivity Analysis-like process that would allow me to confidently pick the best set of \"momentum durations\" that would maximize model's performance\n- Another item I did notice during my research is that Accuracy was used as evaluation metric for most, if not all of the studies. Accuracy suffers if the data is skewed, hence, I decided to use a evaluation metric that performs better under skewed data conditions: the AUC_ROC curve.\n\nAs part of my ML \"pipeline\" on my local PC, I evaluated Neural Networks as well, however, for this kernel, I skipped the NN algorithm because it enlengthens the running time by quite a bit.\n\nWell, enough introduction, let’s dig in!\n"},{"metadata":{"_uuid":"a87f6b6da0487327376abc4da73761fddb046fec"},"cell_type":"markdown","source":"# *LOAD LIBRARIES AND INITIALIZE VARIABLES*"},{"metadata":{"trusted":true,"_uuid":"d2a27af55fbb0270c72b052de2262b920ef8c5b5","scrolled":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport random\nimport datetime as dt\nfrom pandas.core import datetools\n\npd.set_option('precision', 2)\nimport os\n#print(os.listdir(\"../input/price-volume-data-for-all-us-stocks-etfs/Data/ETFs/\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c1b11cdccec080599a453ee17d72bac87ec50e7","collapsed":true},"cell_type":"code","source":"import time\nfrom matplotlib import pyplot as pyplot\n\nfrom sklearn import linear_model\nfrom sklearn import tree\nimport csv\nimport os\n\n## Preprocessign libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import FunctionTransformer\n\n# Cross-Validation and Hyper-Parameter Tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\n\n## Evaluation Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc,roc_auc_score\n\n## Classifiers\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n## GGPLOT2-like library\nfrom plotnine import *\nimport seaborn as sns\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"93a6296c3b3dca2c751f5a4bf5239254cb9ef46e"},"cell_type":"code","source":"\n## INITIALIZE VARIABLES\nnumeric_columns = ['open','high','low','close','openint'] \ndate_columns = [0,8,13,18]\nclosing_columns = ['date','ticker','close','vix_close','vxn_close','ndxt_close','delta_close_etf','delta_close_vix','delta_close_vxn','delta_close_ndxt']\nX_Cols = ['ticker',  ### Only need delta and daily change because I will only be looking into Momentum (daily delta Diff) and Volatility (daily % delta change)\n          'delta_close_etf','dailychange_close_etf',\n          'delta_close_vix','dailychange_close_vix',\n          'delta_close_vxn','dailychange_close_vxn',\n          'delta_close_ndxt','dailychange_close_ndxt']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ed85e7ef2eeec20ce23840f9143d8e288f9e825"},"cell_type":"markdown","source":"# *READ AND COMBINE INDIVIDUAL FILES FROM ETFs*"},{"metadata":{"trusted":true,"_uuid":"1868affe6c3a820e1e74a8833ed4d7d1835c9577","scrolled":false},"cell_type":"code","source":"## Define Path where files are.\nETFPath = \"../input/price-volume-data-for-all-us-stocks-etfs/Data/ETFs/\"\nfilesETF = pd.Series(os.listdir(ETFPath))\n#filesETF\n\n## Define ETFs to be picked\n## Selected 49 ETFs that are performing really well accoring to: (as of January 2018)\n# http://etfdb.com/etfdb-category/technology-equities/%23etfs&sort_name=ytd_percent_return&sort_order=desc&page=1\nChosen_ETF = ['PNQI','ARKK','ROBO','FDN','IPAY','FINX','QQQC','ARKW','IGV','IGM','CQQQ','XTH','FXL','PRNT','RYT','SOCL','XITK','XT','XSW','FTEC','VGT','IXN','SMH','PSJ','QTEC','SKYY','JHMT','GAMR','IYW','PTF','SOXX','XWEB','TCHF','XLK','PSI','FTXL','ITEQ','HACK','TDIV','PXQ','CIBR','XSD','SNSR','PSCT','IGN','FCOM','DTEC','BLOK','BLCN']\nChosen_ETF_files = [x.lower()+'.us.txt' for x in Chosen_ETF]\nChosen_ETF_files = filesETF[filesETF.isin(Chosen_ETF_files)]\n\n## Read Files and Combine them into one large file\nstart = time.time()\ndata_ETF = pd.DataFrame([])\nfilecount = 0\nfor f in Chosen_ETF_files:\n    filecount += 1\n    mid = pd.read_csv(ETFPath+f, index_col=False, skiprows = 0,header='infer', parse_dates=[0], infer_datetime_format  = True)\n    mid['Ticker'] = f\n    data_ETF = data_ETF.append(mid)\nprint(filecount,\"- ETF files have been read and took \",time.time()-start,\" seconds\")\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed61e476767e0d1e469f3718290fadafab69a796","collapsed":true},"cell_type":"markdown","source":"\n***PREPARATION OF ETF DATASET***"},{"metadata":{"trusted":true,"_uuid":"f9fa4fec7dc9aebc1e1f85374198413cde407e92"},"cell_type":"code","source":"## RENAME TICKER FIELD TO A MORE READABLE FORM\ndata_ETF['Ticker'] = data_ETF['Ticker'].str.replace('.us.txt','')\ndata_ETF['Ticker'] = data_ETF['Ticker'].str.upper()\ndata_ETF.columns = data_ETF.columns.str.strip().str.lower().str.replace(' ', '_')\n\n#Backup ETF dataset\ndata_ETF_Backup = data_ETF\n\n# First Look at the ETF dataset\nprint('ETF Columns')\nprint(data_ETF.columns.values)\nprint('ETF Shape')\nprint(data_ETF.shape)\n\ndata_ETF.head() \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69f5416f9ab0833baad7c47fdb0c81ab51d9c81a"},"cell_type":"markdown","source":"# *READ MARKET AND VOLATILITY INDEX FILES* #"},{"metadata":{"trusted":true,"_uuid":"dd6d46098649668d97b1996f5e7377715ca2fd36"},"cell_type":"code","source":"####READ MARKET INDEX FILES\nfilenameVIX = '../input/vix-index-until-jan-202018/Jan20_vixcurrent_Jan20.csv'\ndfVIX = pd.read_csv(filenameVIX,skiprows=1, parse_dates=[0])\n\nfilenameVXN = '../input/vxn-index-until-jan-202018/Jan20_vxncurrent_Jan20.csv'\ndfVXN = pd.read_csv(filenameVXN,skiprows=2, parse_dates=[0])\n\nfilenameNDXT = '../input/ndxt-index-until-jan-202018/Jan20_NDXT.csv'\ndfNDXT = pd.read_csv(filenameNDXT, parse_dates=[0])\n\n# First Look at the Index datasets\nprint(\"VIX\")\nprint(dfVIX.shape)\nprint(\"VXN\")\nprint(dfVXN.shape)\nprint(\"NDXT\")\nprint(dfNDXT.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21d739fa3c260426cae2c7a9a885f8f7e82d7355"},"cell_type":"markdown","source":"### MERGE ETF AND INDEX FILES INTO ONE DATASET ###"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"57a08f0ef940e66aeeb99da769c5a7f201ef621f"},"cell_type":"code","source":"### Merge all index files with ETF dataframe\nprint(\"ETF Data set size\", data_ETF.shape)\ndata_ETF = data_ETF.merge(dfVIX, left_on='date', right_on='Date', how='left')\n\ndata_ETF = data_ETF.merge(dfVXN, left_on='date', right_on='Date', how='left')\ndata_ETF = data_ETF.merge(dfNDXT, left_on='date', right_on='Date', how='left')\ndata_ETF.rename(columns={'Date':'Date_z'}, inplace=True)\ndata_ETF.columns = data_ETF.columns.str.strip().str.lower().str.replace(' ', '_')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22d717b530cdab6c179758b26a385b9965678333"},"cell_type":"markdown","source":"# Preparation for Data Exploration\n### *DATA MANIPULATION TO END UP WITH ONE DATA SET WITH COMMON DATES AND THE ETFs OF INTEREST*"},{"metadata":{"trusted":true,"_uuid":"112c43fae2ab9a4c306f6f9d5ece068a67df7994"},"cell_type":"code","source":"#########################################################################\n## Explore Original Dataset by focusing on Market Indexes available dates\n#########################################################################\n\nprint(\"Total Unique Tickers\")\nprint(data_ETF['ticker'].nunique())\n\nprint(\"Total Unique Tickers for Chosen ETFs\") #FiltereddfETF['ticker'].value_counts()\nTotalTickers = data_ETF['ticker'].loc[data_ETF['ticker'].isin(Chosen_ETF)].nunique()   \nprint(TotalTickers)\n# If you wonder, in my original code I used to read all ETFs from the dataset and then filtered out the Chosen_ETFs. \n# To speed up this Kernel, I have started by reading only those ETFs I am interested in\n\n## Filter full list of ETFs with Chosen ETFs. For this KErnel, they are the same.\nFiltereddfETF = data_ETF.loc[data_ETF['ticker'].isin(Chosen_ETF)]\n\n\n#Filter Dataset only for the ETFs of interest\nTotalRowsPerTickers = FiltereddfETF.groupby('ticker')['ticker'].count()\n\n# Obtain minimun date per ticker\nTotalChosenRows = TotalRowsPerTickers/TotalRowsPerTickers.sum()*100\n\nMinMarketIndex= np.empty((4), dtype='datetime64[D]')\ni=0\nfor col in FiltereddfETF.iloc[:,[0,8,13,18] ]: ## LOOKS INTO DATE COLUMNS\n    MinMarketIndex[i] = FiltereddfETF.loc[:,col].min()\n    print(\"The Columns \", col, \" has a minimum date of \",MinMarketIndex[i])\n    i +=1\n\nMinMarketIndex = MinMarketIndex.max()\nprint(\"The earliest Market Index to start on will be on \",MinMarketIndex)         \n\n####### NDXT'S FIRST DATE IS ON 2006. IT IS THE THE WORST CASE FOR THE DATASET BECAUSE IT IS MISSING MOST DATES\n### NEED TO FILTER TEH DATA SET TO INCLUDE ONLY DATES AVAILABLE FOR ALL INDEXES\nFiltereddfETF = FiltereddfETF[FiltereddfETF.date>=MinMarketIndex]\nprint(FiltereddfETF.shape)\n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56974a5461704e38fe9db4594f6083b788f1d012"},"cell_type":"code","source":"#########################################################################\n## Explore  Dataset by focusing on TICKER  Available DATES\n#########################################################################\n\nMinTickerIndex = {}\n\nfor group, matrix in FiltereddfETF.groupby('ticker'):\n    for col in matrix.iloc[:,date_columns]: ## LOOKS INTO DATA COLUMNS OF THE SUBMATRIX\n        MinTickerIndex[group] = matrix.loc[:,col].min()\nMinTickerIndex =  pd.DataFrame(data = MinTickerIndex, index = ['MinDates',])\n# TRanspose Dataframe\nMinTickerIndex = MinTickerIndex.T\nMinTickerIndex.sort_index(inplace=True)\nMinTickerIndex.sort_values('MinDates',inplace=True)\n\n## From here it can be seen the first 13 Tickers have data from 2/22/2006 so these will be selected so training includes the 2008 crisis\nFinal_Chosen_ETFs =  MinTickerIndex.loc[MinTickerIndex.MinDates == '2006-02-22'].index\nprint(\"Chosen ETF due to full dates available:\",Final_Chosen_ETFs.values)\n\n## Filter Out other ETFs not on final list\nprint(\"Before Filtering:\",FiltereddfETF.shape)\ndf_Final_Chosen_ETFs = FiltereddfETF.loc[FiltereddfETF['ticker'].isin(Final_Chosen_ETFs)]\nprint(\"After Filtering:\",df_Final_Chosen_ETFs.shape)\n\n## Confirm there are not missing dates or any fields in the dataset\nprint(\"Total of NAs in final dataset\",df_Final_Chosen_ETFs.isnull().values.ravel().sum())\n## Summary of dates for Final Dataset\ndf_Final_Chosen_ETFs.iloc[:,date_columns].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d88a030e1f7efd2574ba08f25f4165158de07606","scrolled":true},"cell_type":"code","source":"#########################################################################\n## CHECK/IDENTIFY WHICH ARE MISSING OR NON NUMERIC VALUES IN FINAL DATASET ON FOCUSED COLUMNS (CLOSE,vix_close,vXN_close,NDXT_close)\n#########################################################################\n\ntmp = ['ticker','date','close','vix_close','vxn_close','ndxt_close']\ntmp = df_Final_Chosen_ETFs.loc[:,tmp]\ntmp['close'] = pd.to_numeric(tmp['close'], errors='coerce')\ntmp['vix_close'] = pd.to_numeric(tmp['vix_close'], errors='coerce')\ntmp['vxn_close'] = pd.to_numeric(tmp['vxn_close'], errors='coerce')\ntmp['ndxt_close'] = pd.to_numeric(tmp['ndxt_close'], errors='coerce')\nprint(\"Total of NAs in final dataset\",tmp.isnull().values.ravel().sum())\n\ntmp = tmp[tmp.isnull().any(axis=1)].index.values\nAllNAs = tmp\n#print(type(AllNAs))\nprint(AllNAs.shape)\n\n# Add pre and post index/dates of the NULL value to the ALLNAs list\nfor ind in tmp:\n    AllNAs = np.append(AllNAs,ind+1)\n    AllNAs = np.append(AllNAs,ind-1)\n    AllNAs = np.sort(AllNAs)\n\n\nprint( 'Sample of rows with missign NAs below')\n#print(AllNAs)\n#df_closed_prices = tmp\n#print(type(df_closed_prices))\ndf_Final_Chosen_ETFs.loc[AllNAs].head(18)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e98e8badf6daec015eccfae16e7d93141fc2b8a8"},"cell_type":"markdown","source":"## Fill NAs in the data set ##"},{"metadata":{"trusted":true,"_uuid":"2b3819dd8907b85bd02344f53f47bb6ec1893128"},"cell_type":"code","source":"#########################################################################\n## FILL NAs VIA FORWARD IMPUTATION - Any recommendations on how to best impute on this type of exercise? (stock investing)\n#########################################################################\nprint(\"Total of NAs before filling NAs \",df_Final_Chosen_ETFs[['ticker','date','close','vix_close','vxn_close','ndxt_close']].isnull().values.sum())\n\n# Imputing Group by Group to ensure my imputation does not take values from other tickers by mistake\nfor group, matrix in df_Final_Chosen_ETFs.groupby('ticker'):\n    tmp = matrix.index.values\n    pd.to_numeric(df_Final_Chosen_ETFs.loc[tmp,'volume'], errors='coerce').fillna(method='ffill', inplace=True)\n\n    ### Convert CLOSE columns to numeric\n    pd.to_numeric(df_Final_Chosen_ETFs['close'], errors='coerce').fillna(method='ffill', inplace=True)\n    pd.to_numeric(df_Final_Chosen_ETFs['vix_close'], errors='coerce').fillna(method='ffill', inplace=True)\n    pd.to_numeric(df_Final_Chosen_ETFs['vxn_close'], errors='coerce').fillna(method='ffill', inplace=True)\n    pd.to_numeric(df_Final_Chosen_ETFs['ndxt_close'], errors='coerce').fillna(method='ffill', inplace=True)\n\n    ### Convert OPEN columns to numeric\n    pd.to_numeric(df_Final_Chosen_ETFs['open'], errors='coerce').fillna(method='ffill', inplace=True)\n    pd.to_numeric(df_Final_Chosen_ETFs['vix_open'], errors='coerce').fillna(method='ffill', inplace=True)\n    pd.to_numeric(df_Final_Chosen_ETFs['vxn_open'], errors='coerce').fillna(method='ffill', inplace=True)\n    pd.to_numeric(df_Final_Chosen_ETFs['ndxt_open'], errors='coerce').fillna(method='ffill', inplace=True)\n\nprint( df_Final_Chosen_ETFs.loc[AllNAs,['ticker','date','close','vix_close','vxn_close','ndxt_close']].head(10))\n\nprint(\"\\nReconfirm the total of NAs in final dataset\",df_Final_Chosen_ETFs[['ticker','date','close','vix_close','vxn_close','ndxt_close']].isnull().values.ravel().sum())\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eeeda8518f860c1506bffdfe0d13335d5edbe5ff"},"cell_type":"markdown","source":"## Create daily returns for ETFs and Indexes for Data Explorations\n### Feature Engineering for Exploration"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"bcb2f92928add071b0ad88abd045f2b8551748dd"},"cell_type":"code","source":"#########################################################################\n## CREATE SHIFT OF COLUMNS TO CALCULATE DELTAS AMONG DAYS\n#########################################################################\n\ndf_Final_Chosen_ETFs = df_Final_Chosen_ETFs.reindex(columns = df_Final_Chosen_ETFs.columns.tolist() + ['delta_close_etf','delta_open_etf','delta_volume_etf','delta_close_vix','delta_open_vix','delta_close_vxn','delta_open_vxn','delta_close_ndxt','delta_open_ndxt']) \n\nfor group, matrix in df_Final_Chosen_ETFs.groupby('ticker'):\n    tmp = matrix.index.values\n    df2 = matrix.loc[tmp,:].shift(+1)\n     #_etf\n    df_Final_Chosen_ETFs.loc[tmp,'delta_close_etf'] = df_Final_Chosen_ETFs.loc[tmp,'close'] - df2['close']\n    df_Final_Chosen_ETFs.loc[tmp,'delta_open_etf'] = df_Final_Chosen_ETFs.loc[tmp,'open'] - df2['open']\n    df_Final_Chosen_ETFs.loc[tmp,'delta_volume_etf'] = df_Final_Chosen_ETFs.loc[tmp,'volume'] - df2['volume']\n    df_Final_Chosen_ETFs.loc[tmp,'dailychange_close_etf'] = df_Final_Chosen_ETFs.loc[tmp,'close'] / df2['close'] - 1\n\n    #_vix\n    df_Final_Chosen_ETFs.loc[tmp,'delta_close_vix'] = df_Final_Chosen_ETFs.loc[tmp,'vix_close'] - df2['vix_close']\n    df_Final_Chosen_ETFs.loc[tmp,'delta_open_vix'] = df_Final_Chosen_ETFs.loc[tmp,'vix_open'] - df2['vix_open']\n    df_Final_Chosen_ETFs.loc[tmp,'dailychange_close_vix'] = df_Final_Chosen_ETFs.loc[tmp,'vix_close'] / df2['vix_close'] - 1\n\n    #_vxn\n    df_Final_Chosen_ETFs.loc[tmp,'delta_close_vxn'] = df_Final_Chosen_ETFs.loc[tmp,'vxn_close'] - df2['vxn_close']\n    df_Final_Chosen_ETFs.loc[tmp,'delta_open_vxn'] = df_Final_Chosen_ETFs.loc[tmp,'vxn_open'] - df2['vxn_open']\n    df_Final_Chosen_ETFs.loc[tmp,'dailychange_close_vxn'] = df_Final_Chosen_ETFs.loc[tmp,'vxn_close'] / df2['vxn_close'] - 1\n\n    #_ndxt\n    df_Final_Chosen_ETFs.loc[tmp,'delta_close_ndxt'] = df_Final_Chosen_ETFs.loc[tmp,'ndxt_close'] - df2['ndxt_close']\n    df_Final_Chosen_ETFs.loc[tmp,'delta_open_ndxt'] = df_Final_Chosen_ETFs.loc[tmp,'ndxt_open'] - df2['ndxt_open']\n    df_Final_Chosen_ETFs.loc[tmp,'dailychange_close_ndxt'] = df_Final_Chosen_ETFs.loc[tmp,'ndxt_close'] / df2['ndxt_close'] - 1\n\nprint(df_Final_Chosen_ETFs.shape)\n#print(df_Final_Chosen_ETFs.columns.values)\ndf_Final_Chosen_ETFs.loc[:,closing_columns].describe(include = 'all')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17e3a5adc18a3bd3cbb2071031cfe12202c63dcc"},"cell_type":"markdown","source":"# Stardardize Target Columns "},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"7365fbcc5e6acc944cc685776fb6ffdc82a2151d"},"cell_type":"code","source":"#########################################################################\n## STANDARIZE DELTA COLUMN RESULTS - \n#########################################################################\n# Standardize data (0 mean, 1 stdev)\n\n\nclosing_columns = ['ticker','close','vix_close','vxn_close','ndxt_close','delta_close_etf','delta_close_vix','delta_close_vxn','delta_close_ndxt']\nclosing_columns2 = ['Std_'+ s for s in closing_columns[1:]]\n\ndataframe = df_Final_Chosen_ETFs.reindex(columns = df_Final_Chosen_ETFs.columns.tolist() + closing_columns2) \n\n## SUBSET ONLY FOR THOSE DATES FOR WHICH ALL ETFs, AND INDEXES HAVE DATA AVAILABLE\ndataframe = dataframe.loc[df_Final_Chosen_ETFs.date > MinMarketIndex]\n\nprint(\"before \",dataframe.shape)\nfor group, matrix in dataframe.groupby('ticker'):\n    ind = matrix.index.values\n    tmp=ind\n    scaler = StandardScaler()\n    dataframe.loc[ind,closing_columns2] = scaler.fit_transform(dataframe.loc[ind,closing_columns[1:]])\n\ndf_Final_Chosen_ETFs_Std = dataframe\ndel dataframe\ndf_Final_Chosen_ETFs_Std[X_Cols].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c755ddce01ceb3b9250523bb9af96c79cf1a15e9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3cf14958d4c2670da75285950831f564fc43563"},"cell_type":"markdown","source":"# GENERATE UP/DOWN LABELS GIVEN DAILY CHANGES"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"cde0f23cd9c25100b652c18f4c4835733ee31c0a"},"cell_type":"code","source":"#########################################################################\n## GENERATE UP/DOWN/NO CHANGE IN DELTA CLOSE COLUMNS\n#########################################################################\n\ndef setlabels(x,thresholdMin = 0,thresholdMax = 0):\n    if x >= thresholdMax :\n        return \"Up\"\n    elif x < thresholdMin :\n        return \"Down\"\n    elif (x > thresholdMin) & (x < thresholdMax):\n        return \"No Change\"\n        \n\nfrq = pd.DataFrame(index = ['Down','No Change','Up','All'])        \n        \ntmp = df_Final_Chosen_ETFs_Std.columns[25:46]\nfor col in tmp: ## LOOKS INTO DATA COLUMNS OF THE SUBMATRIX\n    df_Final_Chosen_ETFs_Std['Labeled_' + col] = df_Final_Chosen_ETFs_Std[col].apply(lambda x: setlabels(x)) #,thresholdMin = -.125,thresholdMax = .125\n    frqtab = pd.crosstab(index=df_Final_Chosen_ETFs_Std['Labeled_' + col],columns=col,margins=True)#\n    frq = pd.concat([frq,frqtab.loc[:,col]],axis=1)\n\n\nfrq = frq.T\nfrq['Up_Perc'] = frq['Up']/frq['All']*100\nfrq['Dw_Perc'] = frq['Down']/frq['All']*100\n\nprint(\"CLOSE ETF and CLOSE NDXT are correlated\")\n#print(X_Cols)\nfrq.loc[X_Cols[1:8:2],:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0904f0f4f574737526ccc4e6b6915a8d982c919c"},"cell_type":"markdown","source":"# *At this point the dataset is complete for further exploration and visualization *\n-----"},{"metadata":{"trusted":true,"_uuid":"3bc8be6add509038d51b4635cfe5bedf660e5e36","collapsed":true},"cell_type":"code","source":"## Save the File Final File so it can be used later in the Training/Testing Phase (when in a local PC)\nfilenameOut = './PreProc_AllETFs_Chosen.csv'\ndf_Final_Chosen_ETFs_Std.to_csv(path_or_buf = filenameOut,index_label = \"index\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"785ec8e950e5cef3c2d17b35ab70be9359af213c"},"cell_type":"markdown","source":"# * Exploratory Visualization *\n***\n"},{"metadata":{"_uuid":"4f7d5fd51cedb3dcb1052135dd6aa51fedc6a9d0"},"cell_type":"markdown","source":"##  Simple Time Series Plot "},{"metadata":{"trusted":true,"_uuid":"bde2b887838ed6e6296fe0890f317fe58caed391"},"cell_type":"code","source":"df_Final_Chosen_ETFs_Std.loc[:,['date','close']+ X_Cols[1:8:2]].set_index('date').plot(subplots=True, legend=True, figsize=(20,10)) #ax=axes[0,0],\n\ndf_Final_Chosen_ETFs_Std.loc[:,['date','close']+ X_Cols[2:9:2]].set_index('date').plot(subplots=True, legend=True, figsize=(20,10))\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"9dc62837fdd32b003e58954afc20696bfb1390d5"},"cell_type":"code","source":"###### Identify variables/feature with higher chance of UP \n\nprint(frq.columns.values)\npyplot.bar(np.arange(frq.shape[0]),frq['Up_Perc']) #,figsize = (8,2.5\npyplot.xticks(np.arange(frq.shape[0]), frq.index.values, fontsize=6,rotation=45)\npyplot.axhline(y=50,linewidth=4, color='r')\npyplot.rcParams[\"figure.figsize\"] = [20,10]\npyplot.xlabel('Feature', fontsize=18)\npyplot.ylabel('Up (%)', fontsize=16)\npyplot.tick_params(labelsize = 14, rotation  = 90)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59a848510c9b9881326251106dbb7eedfa841aa0"},"cell_type":"markdown","source":"# * Distribution of Input and Target Variables *\n***\n"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"2e2fb97967a77598d4422d7414ba4a3bb12e7492"},"cell_type":"code","source":"### EXPLORATORY VISUALIZATION\n## Function to Round up values\ndef myround(x, prec=1, base=.5):\n    return round(base * round(float(x)/base),prec)\n\nNumCols = df_Final_Chosen_ETFs_Std.columns\nprint(\"HISTOGRAMS OF INDEXES\")\nprint()\ntmp = [25,34,38,35,39,36,40,37,41]\nfor col in df_Final_Chosen_ETFs_Std.columns[tmp]: ## LOOKS INTO DATA COLUMNS OF THE SUBMATRIX\n    print(\"Variable to be Plotted\",col.upper())\n    fig, ax = pyplot.subplots()    \n    maxi = myround(df_Final_Chosen_ETFs_Std[col].max())\n    mini = myround(df_Final_Chosen_ETFs_Std[col].min())\n    if maxi- mini >= 7:\n        bin = np.arange(mini-.5,maxi+.5,.5)\n    else:\n        bin = np.arange(mini-.5,maxi+.5,.25)\n    \n    if col.upper().find('DAILYCHANGE')>=0:\n        ax.set_xlabel('Daily Return Change (unit)')\n        pyplot.xticks(np.arange(-.25,.25,.05), fontsize=9,rotation=45)\n        df_Final_Chosen_ETFs_Std[col].hist(bins = np.arange(-.25,.25,.05), figsize = (15,3))\n    elif col.upper().find('STD')>=0:\n        ax.set_xlabel('Standarized Value')\n        df_Final_Chosen_ETFs_Std[col].hist(bins = bin, figsize = (15,3))\n    else:\n        ax.set_xlabel(col.upper())\n        df_Final_Chosen_ETFs_Std[col].hist(bins = bin, figsize = (15,3))\n        \n\n    ax.set_ylabel('Number of Samples')\n    pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43541285394a1edbcaa98ce6d78bfb1e4631bcac"},"cell_type":"markdown","source":"# *MODEL TRAINING AND TESTING PHASE*\n\n***"},{"metadata":{"_uuid":"cbc57cc60e968c5ac6c24607f6e4f242fb713d90"},"cell_type":"markdown","source":"***DEFINE VARIABLES AND FUNCTIONS***"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d94c63995239228c1853fd14ca4748bf56222ebe"},"cell_type":"code","source":"################## DEFINE HELPER FUNCTIONS FOR EVALUATING MODELS AND PERFORMING CLASSIFICATIONS\n\ndef EvaluateROC(y_test,y_pred, plot = False):\n        tprs = []\n        std_auc=[]\n        mean_fpr = np.linspace(0, 1, 100)\n        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n        tprs.append(np.interp(mean_fpr, fpr, tpr))\n        tprs[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        \n\n        mean_tpr = np.mean(tprs, axis=0)\n        mean_tpr[-1] = 1.0\n        mean_auc = auc(mean_fpr, mean_tpr)\n        #std_auc = np.std(aucs)\n\n        std_tpr = np.std(tprs, axis=0)\n        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n        \n        if plot:\n            pyplot.rcParams[\"figure.figsize\"] = [7,7]\n            pyplot.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC Score (AUC = %0.2f)' % (roc_auc))\n            pyplot.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Luck', alpha=.8)\n            pyplot.plot(mean_fpr, mean_tpr, color='b',label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=2, alpha=.8)\n            pyplot.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n            pyplot.xlim([-0.05, 1.05])\n            pyplot.ylim([-0.05, 1.05])\n            pyplot.xlabel('False Positive Rate')\n            pyplot.ylabel('True Positive Rate')\n            pyplot.title('Receiver operating characteristic Direction Prediction')\n            pyplot.legend(loc=\"lower right\")\n            pyplot.show()\n            \n        return mean_auc,roc_auc_score(y_test,y_pred)\n    \ndef RunClassificationPredictions(X_train, y_train, X_test, y_test, classifier, Plot=False):\n    #print(\"Do FIT no GridSearch\")\n    classifier.fit(X_train, y_train)\n    #print(\"Do SCORE no GridSearch\")\n    score = clf.score(X_test, y_test)\n    #print(\"Do predict no GridSearch\")\n    y_pred = clf.predict(X_test).ravel()  \n    # ROC Evaluation\n    #print(\"Evaluate ROC no GridSearch\")\n    auc_mean , roc_auc_score_mean = EvaluateROC(y_test,y_pred,plot = Plot)\n    return y_pred,score,auc_mean,roc_auc_score_mean\n\ndef RunGridSearchClassification(X_train, y_train, X_test, y_test, classifier,TSCV,param, Plot=False):\n    #print(\"Do GridSearch\")\n    gsearch = GridSearchCV(estimator=classifier, cv=TSCV,param_grid=param,n_jobs=-1)\n    #print(\"Do FIT and GridSearch\")\n    gsearch.fit(X_train, y_train)\n    #print(\"Do SCORE and GridSearch\")\n    score = gsearch.score(X_test, y_test)\n    #print(\"Do predict and GridSearch\")\n    y_pred = gsearch.predict(X_test).ravel()  \n   #print(\"Evaluate ROC for GridSearch\")\n    auc_mean , roc_auc_score_mean = EvaluateROC(y_test,y_pred,plot = Plot)\n    return y_pred,score,auc_mean,roc_auc_score_mean,gsearch.best_params_\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a6d3908ce03a39019530c99956d25b66886b49f6"},"cell_type":"code","source":"### INITIALIZE VARIABLES FOR TRAINING\nnumeric_columns = ['open','high','low','close','openint']\n#date_columns = [1,8,13,18]\nclosing_columns = ['date','ticker','close','Labeled_delta_close_etf',\n                   'vix_close','Labeled_delta_close_vix',\n                   'vxn_close','Labeled_delta_close_vxn',\n                   'ndxt_close','Labeled_delta_close_ndxt',\n                   'dailychange_close_etf','dailychange_close_vix',\n                   'dailychange_close_vxn','dailychange_close_ndxt', u'Std_close']\n\n####### TARGET FEATURES FOR TRAINING\nX_Cols = ['ticker',  \n          'delta_close_etf','dailychange_close_etf',\n          'delta_close_vix','dailychange_close_vix',\n          'delta_close_vxn','dailychange_close_vxn',\n          'delta_close_ndxt','dailychange_close_ndxt']\n\n####### LABEL FEATURE FOR TRAINING\nY_Cols = ['Labeled_delta_close_etf']\n\n\n####### INITIATE VARIABLES FOR CLASSIFIERS AND GRIDSEARCHCV\nrandomstate = 1975\nnames = [\n         \"SVM\",\n         \"Decision Tree\",\n         \"Random Forest\",\n         \"Gaussian Naive Bayes\",\n         \"Neural Net\"\n]\n\nclassifiers = [\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),     \n    GaussianNB(),\n    MLPClassifier()\n]\n\nparametersets = [\n    {'kernel':('linear','rbf', 'sigmoid'),'random_state':[randomstate],'C':[0.0001,.025,1],'gamma':[0.0001,.025,1,10]},\n    {'max_depth':[100,10, 5], 'max_features':('log2','sqrt'),'random_state':[randomstate]},\n    {'n_estimators':[10],'random_state':[randomstate]},\n    {None},\n    {'activation':['relu','logistic'],'solver':( 'sgd', 'adam'), 'alpha':[0.0001, 1,10] }, \n]\n\n\n#############################################################\n## Variable Targets -\n## m -> Number of Days in the Future for forecasting\n## n1 - > Moving Average for Market Index (days)\n## n2 - > Moving Average gor Volatility (days)\n#############################################################\n\nm = [5,10,20,90,270]    #,5,10,20,90,270\nn = [5,10,20,90,270]     # ,270,5,10,20\n\n#############################################################\n\n### In case of using the data set from previous code\nPreProcessed_data_ETF = df_Final_Chosen_ETFs_Std\n\n###READ THE FILE(S) THIS MAY USED DURING DEVELOPMENT TIMES TO SAVE TIME RE-PROCESSING OF THE DATA EACH TIME\n### In case of reading the file from previous stages, Open Pre-processed Files\n#filename = './Dataset/1Day/ETFs/PreProc_AllETFs_Chosen_WO_NoChange.csv'\n### PreProcessed_data_ETF = pd.read_csv(filename, parse_dates=['date','date_x','date_y'],index_col='index')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ab2f38cdf741ed447b6fcaf9518f26e1cb8e123"},"cell_type":"markdown","source":"*RUN TRAINING INCLUDING GRIDSEARCH*"},{"metadata":{"trusted":true,"_uuid":"70bef80a1448a34c803e684d3a383e849aca729f","scrolled":false},"cell_type":"code","source":"#########################################################################################################\n#### RUN PREDICTIONS AND HYPERPARAMETER OPTIMIZATION USING GRIDSEARCHCV AND TIME SERIES CROSS-VALIDATION\n#########################################################################################################\n########## CALCULATE VOLATILITY, MOMENTUM FOR ETF AND SECTOR INDEXES\n########## EACH FEATURE CALCULATED AVERAGING OVER THE PAST N DAYS FOR 5,10,20,90,270\n\n##  0. For each combination of ML Algorithm/Classifier, m, n1 and n2 execute the following:\n##    1. Subset the original full dataset by Ticker/ETF \n##    2. Shift by \"m\" days to the past the target label\n##    3. Because the last \"m\" rows now show as NA, these are removed from the dataset\n##    4. Encode Label feature (targe predicted variable)\n##    5. Calculate two features(volatility and momentum using Moving Average) to each variable given n1, n2 (N2 is parameter for ETF, and N1 for all other three indexes)\n##       5.1 n1 and n2 may be 5,10,20,90,270. This will make NAs mane of the first rows of the data set\n##       5.2 First \"d\" rows are removed from the dataset where d = (max(n1,n2)+1) th date\n##    6. Subset dataset columns to only include explanatory features and target label feature\n##    7. Transform the data: Apply Log10 (to daily returns) and Standardize all explanatory features \n##    8. Break dataset into TRAIN and TESTING sets with a 70% ratio for Training\n##    9. Create Time Series Cross-Validator Object\n##    10. Train Model using Cross-Validation and Hyper-Parameter optimization using GridSearchCV \n##       3.1 Random forest and Gausian Naive Bayes don't pass by GridSearchCV. The former because it needs to be out-of-the-cox and the latter because it does not have parametes to be passed to GridSearchCV\n##    11. Predict on Testign given current m, n1 and n2 values\n##    12. Evaluate accuracy using \"accuracy score\" and \"AUC_ROC\" score\n##    13. Record on dataframe results from training/testing exercise: ['Ticker','INDEX_N','ETF_N','Forecast','Classifier','Accuracy Score','AUC Trap. Score','AUC_ROC Score', 'TrainingTesting Time', 'Optimized Hyper Parameters']\n\n\n## Run simulation for one set of m,n1,n2\ntimestart  = time.time()\nprint(\"Local current time :\", timestart)\nTrain_Cols = ['mom_ETF','vol_ETF','mom_vix','vol_vix','mom_vxn','vol_vxn','mom_ndxt','vol_ndxt']\nAll_Scores = pd.DataFrame(columns = ['Ticker','INDEX_N','ETF_N','Forecast','Classifier','Accuracy Score','AUC Trapezoidal Score','AUC_ROC_Score','Simulation Duration in Secs','Final Estimator'])\n# Defines # of folds for TS Cross-Validation\nTS_splits = 3\n# Defines if ROC charts will be plot or not\nShowPlots = False\n\n# In a local machine this file allows to record results for each iteration\n#filenameOut = './Dataset/1Day/ETFs/20180309 TrainingTestingResults.csv'\nprint('Expected Number of Iterations (13 Tickers each) = ', len(m)*len(n)*len(n)*4,'\\n')\nind = 0\nCycletime2 = 0\n\nfor i in np.arange(4):#np.arange(5):# THIS IS A WORK AROUND FOR THE ML ALGORITHM TRAINNING CYCLE BELOW. I HAVE REMOVED THE NEURAL NET ALGORITHM BECAUSE EACH ITERATION TAKES TOO LONG ON THIS PLATFORM\n    print(\"\\n################\\n################ ML Algorithm:\",names[i],\"\\n################\")\n    for n1 in n: #FOR INDEX\n        #All_Scores.to_csv(path_or_buf = filenameOut,index_label = \"Iteration\")      # Used for saving in local PC\n        for n2 in n: #FOR ETF\n            for m1 in m:\n                Cycletime = time.time()\n                # Below PRINT statement is useful ofr one-to-one analysis. I've removed to minimize size of the Kernel\n                #print('Start Estimator for 13 Tickers = %s n1 = %d, n2 = %d, m = %d' % (names[i],n1,n2,m1),'Last Training took ',myround(Cycletime2, prec=2, base=.1),'seconds')\n                ## Setting up the simulation for one only Ticker so runing time don't over run Kaglles\n                for group, matrix in PreProcessed_data_ETF[X_Cols+Y_Cols].loc[PreProcessed_data_ETF['ticker']!='IyW'].groupby('ticker'): #.loc[PreProcessed_data_ETF['ticker']!='IoooM',\n                    ind +=1\n                    matrix = matrix.copy()\n\n                    ## Shift Target feature for Forecasting\n                    matrix[Y_Cols] = matrix[Y_Cols].shift(-m1)  \n                    matrix  = matrix[:-m1]\n\n                    # Encode Target feature\n                    le = LabelEncoder()\n                    matrix[Y_Cols] = le.fit_transform(matrix[Y_Cols].values.ravel())\n\n                    ### Create Features for Momentum and Volatility for each variable\n                    matrix['mom_ETF'] = matrix['delta_close_etf'].rolling(window = n2).mean()\n                    matrix['vol_ETF'] = matrix['dailychange_close_etf'].rolling(window = n2).mean()\n                    matrix['mom_vix'] = matrix['delta_close_vix'].rolling(window = n1).mean()\n                    matrix['vol_vix'] = matrix['dailychange_close_vix'].rolling(window = n1).mean()\n                    matrix['mom_vxn'] = matrix['delta_close_vxn'].rolling(window = n1).mean()\n                    matrix['vol_vxn'] = matrix['dailychange_close_vxn'].rolling(window = n1).mean()\n                    matrix['mom_ndxt'] = matrix['delta_close_ndxt'].rolling(window = n1).mean()\n                    matrix['vol_ndxt'] = matrix['dailychange_close_ndxt'].rolling(window = n1).mean()\n\n                    ### Extract Explanatory Features and Target Features from raw dataset, remove NA rows after moving average features creation\n                    FirstGoodSample = max(n1,n2)+1\n                    matrix = matrix[Train_Cols+Y_Cols]\n                    matrix = matrix[FirstGoodSample:]\n                    X_All = matrix.loc[FirstGoodSample:,Train_Cols].values\n                    Y_All = matrix.loc[FirstGoodSample:,Y_Cols].values\n\n                    ### Transform Explanatory Features: Apply Log10 to Daily Returns and Standardize all explanatory features\n                    X_All[:,[1,3,5,7]] = np.log10(X_All[:,[1,3,5,7]]+1)\n                    scaler = StandardScaler()\n                    X_All = scaler.fit_transform(X_All)\n\n                    ### SEPARATE TRAININIG AND TESTING SETS 70% training, 30% Testing\n                    train = int(len(X_All)*.7)\n                    X_train = X_All[:train]\n                    y_train = Y_All[:train].ravel()\n                    X_test = X_All[train:]\n                    y_test = Y_All[train:].ravel()\n\n                    #Create TS Cross-Validator\n                    my_cv = TimeSeriesSplit(n_splits = TS_splits).split(X_train)\n\n                    #print('Start Training and Testing for ETF: ',group)\n\n                    # Training Process including Cross-Validation and Hyper-Parameter optimization using GridSearchCV\n                    for name,clf,GS_Param in list(zip(names,classifiers,parametersets))[i:i+1]:\n                        ## This FOR Loop is failing so a work around was created as the first FOR loop of the cycle above. It's been kept for others to see and with the hope to fix it on a later day\n                        ## I have opened a forum question for it: https://discussions.udacity.com/t/iterating-multiple-estimators-with-gridsearchcv-valueerror-need-more-than-0-values-to-unpack/617716\n                        ##http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n\n                        # Localtime1 will be used to time length of time for each training/testing iteration\n                        localtime1 = time.time()\n\n                        # Run Classification and Testing procedure; it returns predicted Y values for testing set, and \n                        #print(matrix['ticker'].unique())\n                        if name in ['Random Forest','Gaussian Naive Bayes']:\n                            y_pred, score, auc_mean, roc_auc_score_mean = RunClassificationPredictions(X_train, y_train, X_test, y_test, clf, Plot = ShowPlots)\n                            estim = clf.get_params()\n                        else:\n                            y_pred, score, auc_mean, roc_auc_score_mean,estim = RunGridSearchClassification(X_train, y_train, X_test, y_test, clf,my_cv,GS_Param, Plot = ShowPlots)\n\n                        SimTime = time.time() - localtime1\n\n                        # Record Results in Output Sumamry dataframe\n                        All_Scores.loc[len(All_Scores)+1] = [group,n1,n2,m1,name,score,auc_mean,roc_auc_score_mean,SimTime,estim]   #['Ticker','INDEX_N','ETF_N','Forecast','Classifier','Accuracy Score','AUC Trap. Score','AUC_ROC Score', 'TrainingTesting Time', 'Optimized Hyper Parameters']]\n                Cycletime2 = time.time() - Cycletime\n######\ntimeend  = time.time()\nprint(\"Local current time :\", timeend)\nprint(\"Total duration in Secs :\", timeend  - timestart)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc9b47d7d971fc511a3850d20b142dc0105e4cd5","collapsed":true},"cell_type":"markdown","source":"# *One iteration of SVM runs in about 127 sec for 13 ETFs*\n# *One iteration for any of the other models takes less than 3 sec for 13 ETFs*\n\n#### Total duration in Secs : 635.9629626274109 for 13 Tickers. Almost 11 minutes with m  = [10,90]     n =[90,270]\n#### Total duration in Secs : 48.86628794670105 for 1 Tickers. Almost 1 minute with m  = [10,90]  n =[90,270]\n\n"},{"metadata":{"_uuid":"5464454cf6c07a77a2cf8372beba8820efad6799"},"cell_type":"markdown","source":"# *FREE VISUALIZATION MODEL COMPARISON*"},{"metadata":{"trusted":true,"_uuid":"547686ff848f8600edf32200dcfeb43eecee87a0","collapsed":true},"cell_type":"code","source":"## Select the best Forecast timeframe coming from Traning/Testing. Details on my GitHub repository (https://github.com/lambertopisani/Udacity_ML_Projects/tree/master/6_Capstone).\nForecast_df = All_Scores[All_Scores.Forecast == 90]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b1bd189415b333e9076430f30a427dd76fd1657"},"cell_type":"markdown","source":"# *COMPARE ROC ACCURACY AND VARIABILITY OF ML ALGORITHMS*"},{"metadata":{"trusted":true,"_uuid":"5c77f647900f40907a3d30c8caf29b0af2ff8e1d"},"cell_type":"code","source":"ggplot(Forecast_df,aes(x = 'Classifier', y='AUC_ROC_Score', fill = 'Classifier')) + geom_boxplot() + facet_grid('INDEX_N~ETF_N') + theme_bw()+\\\ntheme(strip_background=element_rect(color='blue', fill='blue', size=2),strip_text = element_text(size=9,color=\"white\", face=\"bold\")) +\\\ntheme(legend_position = \"right\")  +\\\ntheme(axis_text_x = element_text(size=8,face=\"bold\",angle = 90, hjust = 0.5, vjust=1)) +\\\nggtitle('AUC_ROC for Index Momentum(days) vs\\nETF Momentum(days)\\n @ Forecast of 90 days\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ead8480c9691f09f02ae446b67c86312051f3b3f"},"cell_type":"markdown","source":"**COMPARE ACCURACY VS AUC_ROC IN THE ENTIRE POPULATION OF TRAINIGNT/TESTING**"},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"0fcbbab82d536d87a0c80e2e67b505b47a2995fc"},"cell_type":"markdown","source":"**COMPARE SIMULATION TIMES**\n\n"},{"metadata":{"trusted":true,"_uuid":"6cdec2c25693882f702c1c93d0b17c38b5b77e51","scrolled":false},"cell_type":"code","source":"#ggplot(aes(x = 'Classifier', y='AUC_ROC_Score', fill = 'Classifier' ),data = Forecast_df) + geom_bar(position = position_dodge(width = 0.9)) \nAll_Scores_Melted = pd.melt(All_Scores, id_vars=['Ticker', 'INDEX_N', 'ETF_N', 'Forecast', 'Classifier', 'Final Estimator'], value_vars=['Accuracy Score', 'AUC Trapezoidal Score', 'AUC_ROC_Score','Simulation Duration in Secs'], var_name='Metric', value_name='value', col_level=None)\nAll_Scores_Melted\n\nggplot(All_Scores_Melted[All_Scores_Melted.Metric != 'Simulation Duration in Secs'],aes(x = 'Metric', y='value', fill ='Metric')) + geom_boxplot() + facet_grid('Forecast ~ Classifier') + theme_bw()+\\\ntheme(strip_background=element_rect(color='blue', fill='blue', size=2),strip_text = element_text(size=9,color=\"white\", face=\"bold\")) +\\\ntheme(legend_position = \"right\")  +\\\ntheme(axis_text_x = element_text(size=8,face=\"bold\",angle = 90, hjust = 0.5, vjust=1)) +\\\nggtitle('ML ALgorithm vs Forecast Horizon')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0804d53c2a22af6408160f79e6b85913e1aa928f"},"cell_type":"markdown","source":"### Accuracy shows \"better\" predictive results"},{"metadata":{"trusted":true,"_uuid":"0e79abc0736fb0618a4472657174ae0a636c3205"},"cell_type":"code","source":"ggplot(All_Scores_Melted[All_Scores_Melted.Metric != 'Simulation Duration in Secs'],aes(x = 'Metric', y='value', fill ='Metric')) + geom_boxplot() + theme_bw()+\\\ntheme(strip_background=element_rect(color='blue', fill='blue', size=2),strip_text = element_text(size=9,color=\"white\", face=\"bold\")) +\\\ntheme(legend_position = \"right\")  +\\\ntheme(axis_text_x = element_text(size=8,face=\"bold\",angle = 90, hjust = 0.5, vjust=1)) +\\\nggtitle('Evaluation Metric Comparison by ML Algorithm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e932e988910ff3c2421787dd3598fcd48baad6c6"},"cell_type":"markdown","source":"### Accuracy shows \"better\" predictive results in most ML Algorithms"},{"metadata":{"trusted":true,"_uuid":"0c3a1944601f00afd5031d5f2d7f7d3fc7058db1"},"cell_type":"code","source":"ggplot(All_Scores_Melted[All_Scores_Melted.Metric != 'Simulation Duration in Secs'],aes(x = 'Metric', y='value', fill ='Metric')) + geom_boxplot() + facet_grid('. ~ Classifier') + theme_bw()+\\\ntheme(strip_background=element_rect(color='blue', fill='blue', size=2),strip_text = element_text(size=9,color=\"white\", face=\"bold\")) +\\\ntheme(legend_position = \"right\")  +\\\ntheme(axis_text_x = element_text(size=8,face=\"bold\",angle = 90, hjust = 0.5, vjust=1)) +\\\nggtitle('Evaluation Metric Comparison by ML Algorithm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bd5b9b6a9f102b683bfa060f5e4dbee1cf23adf","collapsed":true},"cell_type":"markdown","source":"# ** *As a reminder, all details about the decisions made will be found on my [GitHub](https://github.com/lambertopisani/Udacity_ML_Projects/tree/master/6_Capstone) repository on the Capstone Project Report. In this space (Kaggle), I tried to make available the code for others to play and comment since this is an area which I'd really like to learn more about* **"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1b1e1b0eb66edff349f63360edbbef11cbd5b5c2"},"cell_type":"markdown","source":"# ***I hope you enjoyed this Kernel as much as I did to create it. Feel free to comment and offer improvement ideas either to the code or about the domain and the process/assumptions I followed***"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6d65999a53af3a43951eb3f60f35b732b38e5e7a"},"cell_type":"code","source":"## Save the File Final File so it can be used later in the Training/Testing Phase (when in a local PC)\nfilenameOut = './All_Scores.csv'\nAll_Scores.to_csv(path_or_buf = filenameOut,index_label = \"index\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}