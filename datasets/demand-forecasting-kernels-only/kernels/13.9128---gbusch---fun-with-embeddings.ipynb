{"cells":[{"metadata":{"_uuid":"64640604282fc6951f48cbf21a4f29c4b8ce2a8a"},"cell_type":"markdown","source":"# Fun with Embeddings\nIn this kernel, I show a solution with Embeddings of all categorical features. This approach was popularized by the 3rd place solution of the Rossmann Competition (see python scripts on [github](https://github.com/entron/entity-embedding-rossmann)) and this kernel is inspired by their scripts. Further information about this approach including guidelines about the number of embeddings are given in the [fast.ai](http://fast.ai) course. A nice summary of the approach is given in this [fast.ai blogpost](http://www.fast.ai/2018/04/29/categorical-embeddings/).\nThe beauty of the embedding approach is that not much feature engineering is needed. In theory, the fitted embeddings could be analyzed and interpreted. However, this is not done in this version.\nI use the Keras API in this example. Would be interesting to try pytorch with fast.ai-wrapper."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"074f1cf9f717792d684a47acd1ffb56005378cec"},"cell_type":"markdown","source":"## Read in the training data set\nThe training data set has the following columns: date, store, item, sales. Aim is to predict the sales for each date, store and item. It is important to learn how the data set is ordered (do this by plyaing around with the nrows parameter): It goes through all dates for store=1 and item=1 (1826 entries), then it will go through all dates for store=2 and item=1 etc.\nThis becomes relevant when defining the validation set: If we want the last N days as validation set (because we see it as a time series problem), then we cannot simply take the last N entries of the table."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"nrows = 10\n\ndat = pd.read_csv(\"../input/train.csv\")\ndat.head(nrows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfa77165bb9e4b474597e8b25d235498914ccd78"},"cell_type":"markdown","source":"## Expand date features\nThe following function takes the input data frame and produces the feature matrix. The data is converted to datetime format and year, month, day, weekday are extracted. Also (in the case of training data) the target column (\"sales\") and in the case of training data the \"id\" column are split from the feature data. Also, I subtract the first year of the training set (2013) from the year column so that all values in the feature matrix are in the same order of magnitude. Playing around with the model showed that this actually makes a difference..."},{"metadata":{"trusted":true,"_uuid":"3d1b25ce08935f810af2815dfe0665699004e6b1"},"cell_type":"code","source":"def expand_table(dat):\n    dn = dat['date'].map(lambda x: pd.to_datetime(x, format='%Y-%m-%d', errors='ignore'))\n    X = pd.DataFrame({'year': dn.dt.year-2013, 'month': dn.dt.month, 'day': dn.dt.day, \n                       'weekday': dn.dt.weekday,\n                       'store': dat.store, 'item': dat.item,\n                      }, columns = ['year', 'month', 'day', 'weekday', 'store', 'item'],\n                    )\n    \n    X = np.array(X)\n    if 'sales' in list(dat):\n        y = np.array(dat['sales'])\n    if 'id' in list(dat):\n        y = np.array(dat['id'])\n\n    print(X.shape, y.shape)\n    \n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6589c6b37a994c5f20f85feda79c2c6184eb69f2"},"cell_type":"code","source":"X, y = expand_table(dat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d3e51e899fa7887e0a8d3dd1d402de71946c252"},"cell_type":"markdown","source":"## Read in test set\nI already read in the test set here, since we need the whole data set for one-hot encoding in one of the next steps. Since the data set is quite small, there is no memory issue. Actually, we need the one-hot encoding only for the RandomForest Benchmark, so for a pure Embedding approach, we could also skip the one-hot encoding and read in the test set later."},{"metadata":{"trusted":true,"_uuid":"46ad0e1a4f8ef93d200f2497fd20d8b0915fb35c"},"cell_type":"code","source":"dat_test = pd.read_csv(\"../input/test.csv\")\ndat_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21f40e8b2cc7ae49e0f6d0588fb1eb857083e10b"},"cell_type":"code","source":"X_test, id_test = expand_table(dat_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77bc7cc82a9c53effce45711d01aa3820330b91b"},"cell_type":"markdown","source":"## Determine number of unique entries for embedding\nHere, I determine the number of unique entries in each column (the number of categories). This is needed to set the embedding sizes. (Also it can be used to double-check the one-hot encoding result)\nFor the embedding sizes, fast.ai suggest to take about half of the original number of the categories (+1) but sometimes it is worth playing around with this. (I am also still testing if there is room for improvement here.)"},{"metadata":{"trusted":true,"_uuid":"63e89da600f75f5c7960b58b7119c3128213889b"},"cell_type":"code","source":"print(\"determine number of unique entries for embedding...\")\nX_tot = np.vstack((X, X_test))\nprint(\"# of unique years: \", len(np.unique(X_tot[:,0])))\nprint(\"# of unique months: \", len(np.unique(X_tot[:,1])))\nprint(\"# of unique days: \", len(np.unique(X_tot[:,2])))\nprint(\"# of unique weekdays: \", len(np.unique(X_tot[:,3])))\nprint(\"# of unique stores: \", len(np.unique(X_tot[:,4])))\nprint(\"# of unique items: \", len(np.unique(X_tot[:,5])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7972701a0d0f27bdc6b140c9c7c5481efcdca76d"},"cell_type":"markdown","source":"## One-hot encoding\nThe following cell allows one-hot encoding of the feature matrices. This is recommended for the Mean and Random Forest Benchmark. For the Embedding approach, however, we don't need it!"},{"metadata":{"trusted":true,"_uuid":"f7b4b02f08f1a8c428266b02375fd556f9dbacb5"},"cell_type":"code","source":"import sklearn.preprocessing as prepro\n\ninput_one_hot = False\n\nif input_one_hot:\n    print(\"Using one-hot encoding...\")\n    enc = prepro.OneHotEncoder(sparse=False, categories='auto', handle_unknown='ignore')\n    enc.fit(np.vstack((X[:,1:], X_test[:,1:])))\n    X = np.hstack((X[...,[0]],enc.transform(X[:,1:])))\n    X_test = np.hstack((X_test[...,[0]],enc.transform(X_test[:,1:])))\n\nprint(X.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e1f1b12668cffdea1390f7f4df325846badb96a"},"cell_type":"markdown","source":"## Validation set\nChoose whether you want a random split or take the last xx% of the data as validation set. Beware: If we treat it as a time series and we want the last 10% of the data as validation set, we mean the last 10% in time. However, this is not how the data is sorted... So what I do here is take the data of the last 6 month (July - December 2017) as validation set."},{"metadata":{"trusted":true,"_uuid":"854e0a854668fb0f93ac43625b477631e8e48a01"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nrandom_split = True\ntrain_ratio = 0.9\n\nif random_split:\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=(1-train_ratio), random_state=0, shuffle = True)\nelse:\n    train_size = int(train_ratio*len(X))\n    X_train = X[(X[:,0]==4)&(X[:,1]>6)]\n    y_train = y[(X[:,0]==4)&(X[:,1]>6)]\n    X_val = X[(X[:,0]!=4)|(X[:,1]<=6)]\n    y_val = y[(X[:,0]!=4)|(X[:,1]<=6)]\n    \nprint(\"training: \", X_train.shape, y_train.shape)\nprint(\"validation: \", X_val.shape, y_val.shape)\nprint(\"test: \", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"738184afc555f36efbf8c834449d46650faf23ed"},"cell_type":"markdown","source":"## Speeding it up for quick test...\nChoose whether you want to run the test with a sub sample only to check the workflow. For the final test, you want to use all the data of course."},{"metadata":{"trusted":true,"_uuid":"6a689e29dda79b6864da384e762d547a57ffa6c8"},"cell_type":"code","source":"sub_sample = False\nsub_sample_size = 50000\n\ndef subsample(X, y, n=sub_sample_size):\n    ind = np.random.randint(X.shape[0], size=n)\n    return X[ind,:], y[ind]\n\nif sub_sample:\n    X_train, y_train = subsample(X_train, y_train, 50000)\n    print(\"subsample of training set: \", X_train.shape, y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3457e0c625d5cab4bbcefb88f58187b39eed571f"},"cell_type":"markdown","source":"## Modelling\nHere I define a class Model that has the method \"evaluate\". This method makes predictions and returns the SMAPE, which is the evalutation metric used in this competition."},{"metadata":{"trusted":true,"_uuid":"33d3afcf3abdb464a6e476f512326bcc9b1f079c"},"cell_type":"code","source":"class Model(object):\n    \n    def evaluate(self, X, y):\n        preds = self.prediction(X)\n        ind = np.where((y!=0)|(preds!=0))\n        return 100*np.sum(np.abs(preds[ind] - y[ind])/((np.abs(preds[ind]) + np.abs(y[ind]))/2))/len(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"176f2e02d586f8ff67f0170f3d3d9aa2fd989596"},"cell_type":"markdown","source":"### Benchmark Model: Mean\nThis is a simple benchmark. The model returns the mean of the training set for every input row."},{"metadata":{"trusted":true,"_uuid":"f12726830f667ac546a575aabef8a765995e9235"},"cell_type":"code","source":"class BenchmarkMean(Model):\n    \n    def __init__(self, X_train, y_train, X_val, y_val):\n        super().__init__()\n        self.pred = np.mean(y_train)\n        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n        \n    def prediction(self, X):\n        return np.full(X.shape[0], self.pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfdafce8c2fe76ac114fc76d79a3b7b1c1ab99d1"},"cell_type":"code","source":"benchmark_mean = False\n\nif benchmark_mean:\n    res = BenchmarkMean(X_train, y_train, X_val, y_val)\n    pred = res.prediction(X_test)\n    pd.Series(pred).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81456ab1227690948b5f4987c25f12e975850a54"},"cell_type":"markdown","source":"### Benchmark: Random Forest\nAnother benchmark: A simple random forest. For the random forest, a one-hot-encoding is recommended. In this kernel, the RandomForest only serves as a benchmark. To give better predictions with the Random Forest, more time series features should be employed."},{"metadata":{"trusted":true,"_uuid":"bca1f5f60bfebeb00fd65ea46141e6b1964b2b71"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor        \n        \nclass RandomForest(Model):\n    \n    def __init__(self, X_train, y_train, X_val, y_val):\n        super().__init__()\n        self.clf = RandomForestRegressor(n_estimators=200, verbose=True, max_depth=35, min_samples_split=2)\n        self.clf.fit(X_train, y_train)\n        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n    \n    def prediction(self, X):\n        return self.clf.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"39b7b3b42a9e179e26782dcb8fd6b0ce8b5da823"},"cell_type":"code","source":"random_forest = False\n\nif random_forest:\n    res = RandomForest(X_train, y_train, X_val, y_val)\n    pred = res.prediction(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bc17e1baae2d7aa4107edd9f0d5ab6ecb814bc8"},"cell_type":"markdown","source":"### The main model: Neural Network with Embeddings\nThe following model is the main model of this kernel: A Neural Network with Embeddings. I embed all features except for year . This is for the following reason: 1. the sales increase from year to year, in that sense there is a simple linear trend. 2. the test set is from a different year that the training set. If we were to embed the year, the year category of the test set has not been seen while training, so basically all information in the year would be lost...\nI am still experimenting with Dropout and Early Stopping, till now I don't see any clear advantage.\nI run the model fit 5 times and then take the average. It might be worth testing a bit more with different epoch numbers, learning rates, etc."},{"metadata":{"trusted":true,"_uuid":"31b4f1f920f1c64afaccf1457581c3033456ff4f"},"cell_type":"code","source":"from keras.models import Model as KerasModel\nfrom keras.layers import Input, Dense, Activation, Reshape, Dropout\nfrom keras.layers import Concatenate\nfrom keras.layers.embeddings import Embedding\nfrom keras import optimizers, regularizers\nfrom keras.callbacks import EarlyStopping\nimport keras.backend as K\n\n# helper function for NN input\ndef split_features(X):\n    X_list = []\n    for i in range(6):\n        X_list.append(X[:,i])\n    \n    return X_list\n\n# custom loss function\ndef smape(x, y):\n    return 100.*K.mean(2*K.abs(x-y)/(K.abs(x)+K.abs(y)))\n\n\nclass NNwithEmbeddings(Model):\n    \n    def __init__(self, X_train, y_train, X_val, y_val):\n        super().__init__()\n        self._build_model()\n        self.fit(X_train, y_train, X_val, y_val)\n        \n    def preprocess(self, X):\n        X_list = split_features(X)\n        return X_list\n        \n    def _build_model(self):\n        ## year is a continuous feature\n        inp_year = Input(shape=(1,), name=\"year\")\n        \n        ## all other features are categorical and need embedding\n        inp_month = Input(shape=(1,))\n        out_month = Embedding(12+1, 7, name='month_embedding')(inp_month)\n        out_month = Reshape(target_shape=(7,))(out_month)\n        \n        inp_day = Input(shape=(1,))\n        out_day = Embedding(31+1, 16, name='day_embedding')(inp_day)\n        out_day = Reshape(target_shape=(16,))(out_day)\n        \n        inp_weekday = Input(shape=(1,))\n        out_weekday = Embedding(7+1, 4, name='weekday_embedding')(inp_weekday)\n        out_weekday = Reshape(target_shape=(4,))(out_weekday)\n        \n        inp_stores = Input(shape=(1,))\n        out_stores = Embedding(10+1, 6, name='stores_embedding')(inp_stores)\n        out_stores = Reshape(target_shape=(6,))(out_stores)\n        \n        inp_items = Input(shape=(1,))\n        out_items = Embedding(50+1, 26, name='items_embedding')(inp_items)\n        out_items = Reshape(target_shape=(26,))(out_items)\n        \n        \n        inp_model = [inp_year, inp_month, inp_day, inp_weekday, inp_stores, inp_items]\n        out_embeddings = [inp_year, out_month, out_day, out_weekday, out_stores, out_items]\n        \n        out_model = Concatenate()(out_embeddings)\n        out_model = Dense(100)(out_model)\n        out_model = Activation('relu')(out_model)\n        #out_model = Dropout(0.3)(out_model)\n        out_model = Dense(10)(out_model)\n        out_model = Activation('relu')(out_model)\n        #out_model = Dropout(0.3)(out_model)\n        out_model = Dense(1)(out_model)\n        \n        self.model = KerasModel(inputs=inp_model, outputs=out_model)\n        \n        self.model.compile(optimizer='Adam', loss=smape)\n        \n    \n    def fit(self, X_train, y_train, X_val, y_val):\n        self.model.fit(self.preprocess(X_train), y_train,\n                       validation_data=(self.preprocess(X_val), y_val),\n                       epochs=10, batch_size=128,\n                       #callbacks=[EarlyStopping(monitor='val_loss', patience=2)],\n                      )\n        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n        \n    def prediction(self, X):\n        return self.model.predict(self.preprocess(X)).flatten()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c081e90b6774fbbacd4d158140246286e92bb7e4"},"cell_type":"code","source":"embedding_nn = True\n\nif embedding_nn:\n    res = []\n    for i in range(5):\n        res.append(NNwithEmbeddings(X_train, y_train, X_val, y_val))\n    ps = np.array([r.prediction(X_test) for r in res])\n    pred = ps.mean(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e70f7d28b387a7582b13d50212f8a4168c6f54f3"},"cell_type":"markdown","source":"## Submission\nThe following lines produce an output file for submission."},{"metadata":{"trusted":true,"_uuid":"9f883d749fe7ebc3d1c42620590560fbbbb2b1b2"},"cell_type":"code","source":"subm = pd.DataFrame({'id': id_test, 'sales': pred})\nsubm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2306278d289ac95deb95fbd69c0bfada3df85d03"},"cell_type":"code","source":"subm.to_csv('subm_embedding.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}