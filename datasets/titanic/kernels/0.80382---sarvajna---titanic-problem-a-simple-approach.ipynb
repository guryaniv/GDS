{"cells":[{"metadata":{"_uuid":"f1026e37c02a0ec397296d11a6a35d3965081dff","_cell_guid":"7205e380-5ffb-42f2-bdb7-1abedd0828f8"},"cell_type":"markdown","source":"**Machine Learning Classification Problem**\n\nFirst of all, Credits : https://www.kaggle.com/apapiu/regularized-linear-models?scriptVersionId=1097183\n\nThe key point is to to log_transform the numeric variables since most of them are skewed.\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"7dd97c45cc599fb8914372165923516cf0b0c1be","_cell_guid":"f23fb000-85d9-494e-99c1-bbaa150c2efe","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\n\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline\n\n\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\ntrain = train.drop(['Name'], axis=1)\ntest = test.drop(['Name'], axis=1)\n\ntrain.head()\n#print('_'*80)\n#test.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afe5ee5be95dc25fdc3d329eed96c997315ee1b9","_cell_guid":"c03e1e55-ac1f-46d0-b059-212b5ee6472a","collapsed":true,"trusted":true},"cell_type":"code","source":"all_data = pd.concat((train.loc[:,'Pclass':'Embarked'],\n                      test.loc[:,'Pclass':'Embarked']))\n\n#all_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d02ac46755ae0f2acdf99cc3f7855d269207369","_cell_guid":"1ace2c18-d8de-4052-b91c-7498dd29423d"},"cell_type":"markdown","source":"**Data preprocessing:**\n\nNot doing anything fancy\n\n1. First, transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal\n2. Create Dummy variables for the categorical features\n3. Replace the numeric missing values (NaN's) with the mean of their respective columns","outputs":[],"execution_count":null},{"metadata":{"_uuid":"05bb2eb896249d9d7fe83d20f5cd0942a318ba8c","_cell_guid":"20b927c2-f17c-4564-8740-71758eeeb932","trusted":true},"cell_type":"code","source":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"Fare\":train[\"Fare\"], \"log(Fare + 1)\":np.log1p(train[\"Fare\"])})\nprices.hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c78531903b0a6db370c4fde178f5c5d4691c687f","_cell_guid":"f9fe66dc-9073-4119-a18a-cdedb77d81f2","collapsed":true,"trusted":true},"cell_type":"code","source":"#log transform the target:\n#train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96afe3090bb45b07c3ad20f9d3ed5e34a3b20ea1","_cell_guid":"7174fe5f-2546-425f-98b8-64020b4c3681","collapsed":true,"trusted":true},"cell_type":"code","source":"all_data = pd.get_dummies(all_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5f628fc9c7557c4d376107f4586df3524cacbb8","_cell_guid":"90a62f79-e781-48fa-a58e-d4558f109181","collapsed":true,"trusted":true},"cell_type":"code","source":"#filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15f75f5ff4178c39a7478c9d50d8d7d61c4c18ec","_cell_guid":"ae33d18d-2829-4c5c-92ff-a3875ea68cc3","collapsed":true,"trusted":true},"cell_type":"code","source":"#print(all_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9680d25f9eca3455c955a07be10a5693347d154","_cell_guid":"8e594d0c-5c9e-4d9c-bcb9-af70f02dfeed","trusted":true},"cell_type":"code","source":"#creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.Survived\n#print(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf29d7a8e46d00ae9af6f23f4160ca26d96953eb","_cell_guid":"037f1ce0-82d4-41d2-b3dd-cdc35681ad43"},"cell_type":"markdown","source":"**Models**\n\nNow we are going to use  logistic regression and Random Forest(the two that I know of) from the scikit learn module.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"7ec0f12219ae546cab157a8cc10a82bfebb57876","_cell_guid":"99bc7d09-f324-4ddb-9845-2ae68e954139","collapsed":true,"trusted":true},"cell_type":"code","source":"# machine learning\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cff0f68648eec7b7413994813b965a38e512411f","_cell_guid":"b96f8bf8-c4e4-4978-b39f-1f8235f5d8b7","trusted":true},"cell_type":"code","source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y)\naccuracy = round(logreg.score(X_train, y) * 100, 2)\nprint(accuracy)\n\nlogreg_preds = logreg.predict(X_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02b68d02a9ba5e34ade5fe73423a6c621f61d5b1","_cell_guid":"8e1f2171-bfab-4731-b725-97c38c2aabaf","collapsed":true,"trusted":true},"cell_type":"code","source":"#Logistic regression submission\n#solution = pd.DataFrame({\"PassengerId\":test.PassengerId, \"Survived\":logreg_preds})\n#solution.to_csv(\"logreg_solution.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"037913926766ed41e717ffac4f54c46a20b7f079","_cell_guid":"0adb9a91-b139-4505-ac76-620bf98760eb","trusted":true},"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y)\nrandom_forest_preds = random_forest.predict(X_test)\nrandom_forest.score(X_train, y)\naccuracy = round(random_forest.score(X_train, y) * 100, 2)\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbc8200897ccacdbbbcf59f87521988d622d2a11","_cell_guid":"fa3085c0-0bac-4ede-bb16-6d9205e2b458"},"cell_type":"markdown","source":"Random forest seems to provide more accuracy on the training set. Hence, using it for submisison.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3d58de82813ea29f0a6a442eb3e53ed599b2f923","_cell_guid":"05c28be2-ea7e-4de2-adce-dc2f2b5f999d","collapsed":true,"trusted":true},"cell_type":"code","source":"#Random forest submission\nsolution = pd.DataFrame({\"PassengerId\":test.PassengerId, \"Survived\":random_forest_preds})\nsolution.to_csv(\"random_forest_solution.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58db66f3b39f2597f2c12b340b3ef1b2022bbeeb","_cell_guid":"ef12a255-76dc-444b-a624-bce3d05a997a"},"cell_type":"markdown","source":"**Kindly upvote if you find this kernel useful**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b09bd9dd554554bab9ab0801060283e39d4fdb33"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}