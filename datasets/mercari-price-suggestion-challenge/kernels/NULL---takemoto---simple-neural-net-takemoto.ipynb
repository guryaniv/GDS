{"cells": [{"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a804979531697f5934ea328ad582c4ae7e412841", "_cell_guid": "a5bdf498-50b6-48a3-89d3-fe38e3743024", "collapsed": true}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import types\n", "from sklearn.linear_model import LinearRegression\n", "from scipy.sparse import coo_matrix, hstack\n", "from scipy import io\n", "import tensorflow as tf\n", "import math\n", "import sys, csv, h5py\n", "\n", "from scipy.sparse import coo_matrix, hstack\n", "from scipy import io\n", "from sklearn.model_selection import train_test_split\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "eefffcb1715c66efb6db40c315f3b0bb7d1655fc", "_cell_guid": "3726f4d8-5fc0-43b4-9992-6dfb1bfc7898", "collapsed": true}, "outputs": [], "source": ["# pandas\u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u8fd4\u3059\n", "# train_or_test\u306b\u306f'train'\u304b'test'\u3092\u5165\u308c\u308b\n", "def load_data(path,train_or_test,brand_threshold = 100,category_threshold = 50,frequent_brands=None,frequent_categories=None):\n", "    data_pd = pd.read_csv(path, error_bad_lines=False, encoding='utf-8', header=0, delimiter='\\t')\n", "    #\u30d6\u30e9\u30f3\u30c9\u540d\u304c\u306a\u3044\u3082\u306e\u3092'NO_BRAND'\u3068\u3059\u308b\n", "    data_pd['brand_name'] = data_pd['brand_name'].fillna('NO_BRAND')\n", "    data_pd=data_pd.fillna(\"\")\n", "\n", "    if train_or_test == 'train':\n", "        frequent_brands = data_pd['brand_name'].value_counts()[data_pd['brand_name'].value_counts()>brand_threshold].index\n", "        frequent_categories = data_pd['category_name'].value_counts()[data_pd['category_name'].value_counts()>category_threshold].index\n", "    elif train_or_test != 'test':\n", "        print('Error : Please input \"train\" or \"test\" in train_or_test')\n", "        return\n", "    \n", "    if type(frequent_brands)==type(None) or type(frequent_categories)==type(None):\n", "        print('Error : Please load train data first')\n", "        return\n", "    else:\n", "        data_pd.loc[~data_pd['brand_name'].isin(frequent_brands),'brand_name']= 'SOME_BRAND'\n", "        data_pd.loc[~data_pd['category_name'].isin(frequent_categories),'category_name'] = 'SOME_CATEGORY'\n", "        \n", "    return data_pd,frequent_brands,frequent_categories"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "5f331b3122af5714a3f16d3beaa81ede3f7cb02d", "_cell_guid": "cf34e9b2-7eeb-4d50-af73-e08288a0db6b", "collapsed": true}, "outputs": [], "source": ["csv_train_path = u'../input/train.tsv'\n", "csv_test_path = u'../input/test.tsv'\n", "train_data_pd, frequent_brands, frequent_categories = load_data(csv_train_path,'train',brand_threshold=100,category_threshold=50)\n", "test_data_pd, _, _ = load_data(csv_test_path,'test',frequent_brands=frequent_brands,frequent_categories=frequent_categories)\n", "print('loading data completed')"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9eb2bad1aa71892ddeeb365cb49ca850da598f78", "_cell_guid": "58889cdd-516c-4934-9e28-d3fdaf9c263b", "collapsed": true}, "outputs": [], "source": ["use_cols = ['item_condition_id','brand_name','shipping','category_name']\n", "train_num = len(train_data_pd)\n", "test_num = len(test_data_pd)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "80b02996d0fe5c960604dee7a4b13a75575a4df2", "_cell_guid": "1cc38025-abee-4d85-9d1f-00633178f0e4", "collapsed": true}, "outputs": [], "source": ["prices = np.array(train_data_pd['price'])\n", "prices_log = np.log(prices+1)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "7921ba31c3011204682959675519d54346ae75d2", "_cell_guid": "07f0ce87-e0b2-4144-b6a9-0bdf82a689f7", "collapsed": true}, "outputs": [], "source": ["# scipy\u306esparse matrix(coo_matrix)X_transform \u3068 \u5909\u6570\u306e\u30ea\u30b9\u30c8variables \u3092\u8fd4\u3059\n", "# save_path\u306b\u4f55\u3082\u6307\u5b9a\u3057\u306a\u3044\u5834\u5408\u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\u3057\u306a\u3044 \u6307\u5b9a\u3057\u305f\u5834\u5408\u6307\u5b9a\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u5185\u306b\u4fdd\u5b58\u3059\u308b\n", "def make_onehot(use_cols,data_pd,train_or_test,save_path=None):\n", "    variables = []\n", "    flag = 0\n", "    for use_col in use_cols:\n", "        dummy_pd = pd.get_dummies(data_pd[use_col]).astype(np.uint8)\n", "        if flag==0:\n", "            X_transform = coo_matrix(dummy_pd.values)\n", "            flag=1\n", "        else:\n", "            X_transform = hstack([X_transform,coo_matrix(dummy_pd.values)])\n", "        \n", "        variables.extend( list( dummy_pd.columns ) )\n", "        \n", "        if save_path is not None:\n", "            if train_or_test != 'test' and train_or_test != 'train':\n", "                print('Error : Please input \"train\" or \"test\" in train_or_test')\n", "                return\n", "            save_path_ = '{}/{}_{}.csv'.format(save_path,use_col,train_or_test)\n", "            dummy_pd.to_csv(save_path_,index=False,encoding=\"utf8\")\n", "            \n", "    if save_path is not None:\n", "        # sparse matrix\u306e\u4fdd\u5b58\n", "        io.savemat(\"{}/X_transform_{}\".format(save_path,train_or_test), {\"X_transform\":X_transform})\n", "        print('sparse matrix\u3092\u4fdd\u5b58\u3057\u307e\u3057\u305f\u3002\u6b21\u56de\u304b\u3089\u306fsparse matrix\u3092\u8aad\u307f\u8fbc\u3093\u3067\u5b66\u7fd2\u306b\u5229\u7528\u3057\u3066\u304f\u3060\u3055\u3044')\n", "\n", "    return X_transform,np.array(variables)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "2310e8663f5ba6645888afc145c17c4ea6e15967", "_cell_guid": "e56bb840-bd5d-4ad7-b114-b5916c49ce24", "collapsed": true}, "outputs": [], "source": ["X_transform_train,variables = make_onehot(use_cols,train_data_pd,'train',save_path=None)\n", "X_transform_test,variables_ = make_onehot(use_cols,test_data_pd,'test',save_path=None)\n", "print('converting data completed')"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8992129b8a5bd1ba3bcef1cb4b214da375c15379", "_cell_guid": "033a3603-19c5-46e7-a0c8-6a80cbc2b9b6", "collapsed": true}, "outputs": [], "source": ["# NN\u306e\u5b66\u7fd2\n", "MAX_EPOCH = 5\n", "BATCH_SIZE = 1000\n", "UNIT_NOS = [500,50]\n", "features = X_transform_test.shape[1]\n", "data_path = \"working_dir\"\n", "patience = 2\n", "\n", "max_size = 100000000\n", "hidden_no = len(UNIT_NOS)\n", "x = tf.placeholder(tf.float32, [None, features])\n", "W_list = []\n", "b_list = []\n", "\n", "old_unit_no = features\n", "z = x\n", "for i, unit_no in enumerate(UNIT_NOS):\n", "    W = tf.Variable(tf.random_normal([old_unit_no, unit_no], mean=0.0, stddev=0.05))\n", "    b = tf.Variable(tf.constant(0.1, shape=[unit_no]))\n", "    W_list.append(W)\n", "    b_list.append(b)\n", "    z = tf.nn.relu(tf.matmul(z, W) + b)\n", "    old_unit_no = unit_no\n", "W_last = tf.Variable(tf.random_normal([UNIT_NOS[-1], 1], mean=0.0, stddev=0.05))\n", "b_last = tf.Variable(tf.random_normal([1], mean=0.0, stddev=0.05))\n", "y = tf.matmul(z, W_last) + b_last\n", "\n", "y_ = tf.placeholder(tf.float32, [None, 1])\n", "mse = tf.reduce_mean((y - y_) * (y - y_))\n", "\n", "train_step = tf.train.AdamOptimizer(1e-2).minimize(mse)\n", "init = tf.initialize_all_variables()\n"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b54c4ade970e91b5880853055c48eb113c2c0226", "_cell_guid": "9fd8b150-a296-465c-beec-7fd484ff60a2", "collapsed": true}, "outputs": [], "source": ["sess = tf.Session()\n", "sess.run(init)\n", "\n", "#\u5b66\u7fd2\u306e\u4fdd\u5b58\n", "saver = tf.train.Saver(max_to_keep=1)\n", "ckpt = tf.train.get_checkpoint_state(data_path)\n", "if ckpt and ckpt.model_checkpoint_path:\n", "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n", "    saver.restore(sess, ckpt.model_checkpoint_path)\n"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "600a0e753fbf8584825fd77df83b017945e83487", "_cell_guid": "f36936ec-debe-4f56-a818-5d65a07fa31c", "collapsed": true}, "outputs": [], "source": ["# \u30c7\u30fc\u30bf\u53d6\u5f97\n", "# # sparse matrix\u306e\u8aad\u307f\u8fbc\u307f\n", "# X_transform = io.loadmat(\"../../../onehots/X_transform_train\")[\"X_transform\"]\n", "# y = np.load('../../../onehots/y_log.npy')\n", "# X_transform_test = io.loadmat(\"../../../onehots/X_transform_test\")[\"X_transform\"]\n", "\n", "train_X, test_X, train_y, test_y = train_test_split(X_transform_train, prices_log, test_size=0.1, random_state=42)\n", "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.1, random_state=42)\n", "\n", "print('data_length:'+str(train_X.shape[0]))\n", "batch_no = int( (train_X.shape[0] - 1) / BATCH_SIZE + 1)\n", "print('batch_no:'+str(batch_no))\n"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "abd573a3935e9748d756bc1292589d9d49b95b53", "_cell_guid": "4b1d1354-2af4-42c8-93cd-ccccbcaf5695", "collapsed": true}, "outputs": [], "source": ["count = 0"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "13e5093b9cb5b45094bcd161e75fb55ed506adfe", "_cell_guid": "0f89d0a2-baa9-45d2-8036-0361ae6dc055", "collapsed": true}, "outputs": [], "source": ["min = np.inf\n", "for i in range(MAX_EPOCH):\n", "    print(\"epoch:\"+str(i+1))\n", "\n", "    # SGD\u3092\u5b9f\u88c5\u3057\u3066\u3044\u308b\n", "    for j in range(batch_no):\n", "        batch_xs = (train_X.toarray())[j * BATCH_SIZE:(j + 1) * BATCH_SIZE]\n", "        batch_ys = train_y[j * BATCH_SIZE:(j + 1) * BATCH_SIZE].reshape(-1, 1)\n", "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n", "#         if j%100==1:\n", "#             print('   batch:'+str(j))\n", "    train_cost = sess.run(mse, feed_dict={x: (train_X.toarray())[:BATCH_SIZE], y_: train_y[:BATCH_SIZE].reshape(-1, 1)})\n", "    valid_cost = sess.run(mse, feed_dict={x: valid_X.toarray(), y_: valid_y.reshape(-1, 1)})\n", "    print (str(i + 1) + \"epoch:train cost(rmse)=\" + str(math.sqrt(train_cost)) + \", rmse=\" + str(math.sqrt(valid_cost)) )\n", "\n", "    if valid_cost < min:\n", "        count = 0\n", "        # for i, W, b in zip(range(hidden_no), W_list, b_list):\n", "        #     np.savetxt(path + \"W\" + str(i + 1) + \".csv\", sess.run(W), delimiter=\",\")\n", "        #     np.savetxt(path + \"b\" + str(i + 1) + \".csv\", sess.run(b), delimiter=\",\")\n", "        # np.savetxt(path + \"W.csv\", sess.run(W_last), delimiter=\",\")\n", "        # np.savetxt(path + \"b.csv\", sess.run(b_last), delimiter=\",\")\n", "        min = valid_cost\n", "    else:\n", "        count += 1\n", "\n", "\n", "    # \u6539\u5584\u3055\u308c\u306a\u304b\u3063\u305f\u56de\u6570\u304cpatience\u56de\u4ee5\u4e0a\u3067\u5b66\u7fd2\u7d42\u4e86\n", "    if count >= patience:\n", "        break\n", "print (\"test rmse=\" + str(math.sqrt(sess.run(mse, feed_dict={x: test_X.toarray(), y_: test_y[:].reshape(-1, 1)}))) )\n"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8cbbda1f11e7d9ae982501fb64543e7ba72a43a7", "_cell_guid": "c76c029e-4a52-4151-ad2e-0d6b83157368", "collapsed": true}, "outputs": [], "source": ["prediction_log = sess.run(y,feed_dict={x:X_transform_test.toarray()}).reshape(-1)\n", "prediction = np.exp(prediction_log)-1\n", "test_id = np.arange(prediction.shape[0])"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "99ee6a7d4afbb4a145f6bfb194cdf45aff0bcf41", "_cell_guid": "cd30300c-7185-4cd0-a2d8-12aafce969c6", "collapsed": true}, "outputs": [], "source": ["submission = pd.DataFrame([])\n", "submission['test_id'] = test_id\n", "submission['price'] = pd.DataFrame(prediction)\n", "submission.to_csv('submission.csv',index=None)\n", "print(submission.iloc[:10])"]}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "file_extension": ".py", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1}