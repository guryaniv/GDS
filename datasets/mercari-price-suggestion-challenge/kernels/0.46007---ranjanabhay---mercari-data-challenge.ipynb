{"nbformat_minor": 1, "metadata": {"language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.4", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat": 4, "cells": [{"execution_count": null, "cell_type": "code", "source": ["# Based on Bojan: https://www.kaggle.com/tunguz/wordbatch-ftrl-fm-lgb-lbl-0-42506\n", "\n", "import re\n", "import gc\n", "import time\n", "import numpy as np\n", "import pandas as pd\n", "from scipy.sparse import csr_matrix,hstack\n", "from time import gmtime,strftime\n", "import wordbatch\n", "from wordbatch.extractors import WordBag,WordHash\n", "from wordbatch.models import FTRL,FM_FTRL\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.preprocessing import LabelBinarizer\n", "from sklearn.model_selection import train_test_split\n", "from nltk.corpus import stopwords\n", "from scipy.sparse import csr_matrix,hstack\n", "import lightgbm as lgb\n", "\n", "\n", "# Function to handle the missing data.\n", "def handle_missing_data(comb):\n", "    comb['first_category'].fillna(value='missing_category',inplace=True)\n", "    comb['second_category'].fillna(value='missing_category',inplace=True)\n", "    comb['third_category'].fillna(value='missing_category',inplace=True)\n", "    comb['item_description'].fillna(value='missing_description',inplace=True)\n", "    comb['brand_name'].fillna(value='missing_brand',inplace=True)\n", "    comb['name'].fillna(value='missing_name',inplace=True)\n", "    #return comb\n", "\n", "# Function to split the category into sub-categories\n", "def custom_split(description):\n", "    try:\n", "        return description.split(\"/\")\n", "    except:\n", "        return [\"No Label\",\"No Label\",\"No Label\"]\n", "\n", "# Function to change the data type of the categorical variable.\n", "def convert_to_categorical(comb):\n", "    comb['first_category'] = comb['first_category'].astype('category')\n", "    comb['second_category'] = comb['second_category'].astype('category')\n", "    comb['third_category'] = comb['third_category'].astype('category')\n", "    comb['item_condition_id'] = comb['item_condition_id'].astype('category')\n", "\n", "    \n", "# Function to filter the dataset to feed to the model.\n", "def filtering_dataset(comb):\n", "    popular_brand = comb['brand_name'].value_counts().loc[lambda x: x.index !='missing_brand'].index[:4500]\n", "    comb.loc[~comb['brand_name'].isin(popular_brand),'brand_name'] = 'missing'\n", "    popular_first_category = comb['first_category'].value_counts().loc[lambda x: x.index!='missing_category'].index[0:1250]\n", "    comb.loc[~comb['first_category'].isin(popular_first_category),'first_category'] ='missing'\n", "    popular_second_category = comb['second_category'].value_counts().loc[lambda x: x.index!='missing_category'].index[0:1250]\n", "    comb.loc[~comb['second_category'].isin(popular_second_category),'third_category'] = 'missing'\n", "    popular_third_category = comb['third_category'].value_counts().loc[lambda x: x.index!='missing_category'].index[0:1250]\n", "    comb.loc[~comb['third_category'].isin(popular_third_category),'third_category'] = 'missing'\n", "\n", "stopwords = {x:1 for x in stopwords.words('english')}\n", "non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n", "# def normalize_text(text):\n", "#     return u\" \".join([x for x in [y for y in non_aplhanums.sub(' ',text).lower().strip().split(\" \")]\\\n", "#                      if len(x) > 1 and x not in stopwords])\n", "\n", "# Function to normalize the text\n", "def normalize_text(text):\n", "    return u\" \".join(\n", "        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n", "         if len(x) > 1 and x not in stopwords])\n", "\n", "# Function to compute the Root Mean Squared Logarithmic Error.\n", "def rmsle(y_act,y_pred):\n", "    assert len(y_act) == len(y_pred)\n", "    return np.sqrt(np.mean(np.power(np.log1p(y_act) - np.log1p(y_pred),2)))\n", "\n", "\n", "# Function for implementing the business logic.\n", "def main_function():\n", "    start_time = time.time()\n", "    print(start_time)\n", "    print(strftime(\"%Y-%m-%d %H:%M:%S\",gmtime()))\n", "    mercari_data_train = pd.read_table('../input/train.tsv',engine='c')\n", "    mercari_data_test = pd.read_table('../input/test.tsv',engine='c')\n", "    print('[{}] Finished to load the data'.format(time.time() - start_time))\n", "    print('Shape of training Data',mercari_data_train.shape)\n", "    print('Shape of testing Data',mercari_data_test.shape)\n", "    nrow_test = mercari_data_train.shape[0]\n", "    dftt = mercari_data_train[(mercari_data_train.price < 1.0)]\n", "    mercari_data_train = mercari_data_train.drop(mercari_data_train[(mercari_data_train.price < 1.0)].index)\n", "    del dftt['price']\n", "    nrow_train = mercari_data_train.shape[0]\n", "    y = np.log1p(mercari_data_train[\"price\"])\n", "    print('Fields of training dataset',mercari_data_train.columns)\n", "    print('Fields of testing dataset',mercari_data_test.columns) \n", "    comb: pd.DataFrame = pd.concat([mercari_data_train,dftt,mercari_data_test])\n", "    print('Shape of training data:',comb.shape)\n", "    submission:pd.Dataframe = mercari_data_test[['test_id']]\n", "    #comb = comb[comb['category_name'].notnull()]\n", "    comb['first_category'],comb['second_category'],comb['third_category'] = zip(*comb['category_name'].apply(lambda x: custom_split(x)))\n", "    handle_missing_data(comb)\n", "    comb.drop(['category_name'],axis=1,inplace=True)\n", "    print('[{}] Split categories complete and original dropped'.format(time.time() - start_time))\n", "    filtering_dataset(comb)\n", "    convert_to_categorical(comb)\n", "    print(comb.dtypes)\n", "    comb = comb[comb['name'].notnull()]\n", "    print(comb['name'].head(n=500))\n", "    word_batch = wordbatch.WordBatch(normalize_text,extractor=(WordBag,{\"hash_ngrams\":2,\"hash_ngrams_weights\":[1.5,1.0],\"hash_size\":2**29,\"norm\":None,\"tf\":\"binary\",\"idf\":None}),procs=4)\n", "    word_batch.dictionary_freeze=True\n", "    #comb.head(n=5)\n", "    #comb.columns.values\n", "    X_name = word_batch.fit_transform(comb['name'])\n", "    X_name = X_name[:,np.where(X_name.getnnz(axis=0) > 1)[0]]\n", "    del(word_batch)\n", "    word_batch = CountVectorizer()\n", "    X_first_category = word_batch.fit_transform(comb['first_category'])\n", "    X_second_category = word_batch.fit_transform(comb['second_category'])\n", "    X_third_category = word_batch.fit_transform(comb['third_category'])\n", "    print('[{}] Count Vectorize categories completed.'.format(time.time() - start_time))\n", "    # Word Batch for item description\n", "    word_batch = wordbatch.WordBatch(normalize_text,extractor=(WordBag,{\"hash_ngrams\":2,\"hash_ngrams_weights\":[1.5,1.0],\"hash_size\":2**29,\"norm\":\"l2\",\"tf\":1.0,\"idf\":None,}),procs=8)\n", "    word_batch.dictionary_freeze=True\n", "    X_description = word_batch.fit_transform(comb['item_description'])\n", "    del(word_batch)\n", "    X_description = X_description[:,np.where(X_description.getnnz(axis=0)>1)[0]]\n", "    print('[{}] Vectorize item_description completed.'.format(time.time() - start_time))\n", "    lb = LabelBinarizer(sparse_output=True)\n", "    X_brand = lb.fit_transform(comb['brand_name'])\n", "    print('[{}] Label Binarize brand name completed.'.format(time.time() - start_time))\n", "    X_dummies = csr_matrix(pd.get_dummies(comb[['item_condition_id','shipping']],sparse=True).values)\n", "    print('[{}] Get dummies on item condition and shipping done.'.format(time.time() - start_time))\n", "    print(X_dummies.shape,X_description.shape,X_brand.shape,X_first_category.shape,X_second_category.shape,X_third_category.shape,X_name)\n", "    sparse_merge = hstack((X_dummies,X_description,X_brand,X_first_category,X_second_category,X_third_category,X_name)).tocsr()\n", "    print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n", "    print(sparse_merge.shape)\n", "    sparse_merge = sparse_merge[:,np.where(sparse_merge.getnnz(axis=0) > 100)[0]]\n", "    X = sparse_merge[:nrow_train]\n", "    X_test = sparse_merge[nrow_test:]\n", "    #print(sparse_merge.head(n=5))\n", "    \n", "    gc.collect()\n", "    train_X,train_y = X,y\n", "    train_X,valid_X,train_y,valid_y = train_test_split(X,y,test_size=0.05,random_state=100)\n", "    model  = FTRL(alpha=0.01,beta=0.1,L1=0.00001,L2=1.0,D=sparse_merge.shape[1],iters=50,inv_link=\"identity\",threads=1)\n", "    model.fit(train_X,train_y)\n", "    print('[{}] Train FTRL completed'.format(time.time() - start_time))\n", "    preds = model.predict(X = valid_X)\n", "    print(\"FTRL Dev MSLE:\",rmsle(np.expm1(valid_y),np.expm1(preds)))\n", "    \n", "    predsF = model.predict(X_test)\n", "    print('[{}] Predict FTRL completed'.format(time.time() - start_time))\n", "    \n", "    model = FM_FTRL(alpha=0.01,beta=0.01,L1=0.00001,L2=0.1,D=sparse_merge.shape[1],alpha_fm=0.01,L2_fm=0.0,init_fm=0.01,D_fm=200,e_noise=0.0001,iters=20,inv_link='identity',threads=4)\n", "    model.fit(train_X,train_y)\n", "    print('[{}] Train ridge v2 completed'.format(time.time() - start_time))\n", "    preds = model.predict(X=valid_X)\n", "    print(\"FM_FTRL dev RMSLE:\",rmsle(np.expm1(valid_y),np.expm1(preds)))\n", "    \n", "    predsFM = model.predict(X_test)\n", "    print('[{}] Predict FM_FTRL completed'.format(time.time() - start_time))\n", "    \n", "    params = {'learning_rate':0.2,'application':'regression','max_depth':4,'num_leaves':15,'verbosity':-1,'metric':'RMSE','data_random_seed':1,'bagging_fraction':0.6,'bagging_freq':5,'feature_fraction':0.65,'nthread':4,'min_data_in_leaf':100,'max_bin':16}\n", "    \n", "    # Remove features with document frequency <=100\n", "    print(sparse_merge.shape)\n", "    sparse_merge = sparse_merge[:,np.where(sparse_merge.getnnz(axis=0) > 100)[0]]\n", "    X = sparse_merge[:nrow_train]\n", "    X_test = sparse_merge[nrow_test:]\n", "    print(sparse_merge.shape)\n", "    train_X,train_y = X,y\n", "    train_X,valid_X,train_y,valid_y = train_test_split(X,y,test_size=0.05,random_state=100)\n", "    d_train = lgb.Dataset(train_X,label=train_y)\n", "    watch_list = [d_train]\n", "    d_valid = lgb.Dataset(valid_X,label=valid_y)\n", "    watch_list = [d_train,d_valid]\n", "    model = lgb.train(params,train_set=d_train,num_boost_round=200,valid_sets=watch_list,early_stopping_rounds=50,verbose_eval=20)\n", "    preds = model.predict(valid_X)\n", "    print(\"LGB dev RMSLE:\",rmsle(np.expm1(valid_y),np.expm1(preds)))\n", "    predsL = model.predict(X_test)\n", "    print('[{}] Predict LGB completed.'.format(time.time() - start_time))\n", "    preds = (predsF*0.18 + predsL*0.27 + predsFM*0.55)\n", "    submission['price'] = np.expm1(preds)\n", "    submission.to_csv(\"wordbatch_ftrl_fm_lgb.csv\",index=False)\n", "    \n", "main_function()\n", "\n", "#from subprocess import check_output\n", "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "metadata": {"_uuid": "1213de58c84354310e861196cc061b9388620e11", "_cell_guid": "aef56894-4d7b-449d-87c7-3d550cd70614"}}]}