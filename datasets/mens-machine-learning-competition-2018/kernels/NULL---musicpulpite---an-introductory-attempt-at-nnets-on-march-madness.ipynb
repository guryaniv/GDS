{"cells":[{"metadata":{"_uuid":"2555897865a205d208f5fcf6f717157fb2bd4b5f"},"cell_type":"markdown","source":"**An Introductory Attempt at NNETS on March Madness**\nThe first section of this notebook transforms the raw dataset (team statistics for each regular season game for all the years that the NCAA Basketball Tournament was run) into per-game season average statistics for every team that played in the tournament. In this way, we hoped to use regular season stats as a rudimentary predictor for post-season success and hoped to artificially explore patterns within these statistics using the machine learning technique of nerual networks (both implemented in the Python libraries Tensorflow and Keras).   \nTo establish a base-line (which hopefully our neural networks should have surpassed) we ran a logistics regression on the data set and found a test-set accuracy of 66%. Unfortunately, neither of our nnet implementations were able to surpass that number despite lots of tweaking. The best conclusion that we can come up with is that there is something fundamentally spurious about the use of vectorized team statistics to determine win probabilities and that somehow the input data should be restructured or that a composite score should be calculated or that multivariable regression could help us pair down the input parameters size. "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.linear_model import LogisticRegression\nimport math\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nseed = 5\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"#Import necessary dataframes\nTeams = pd.read_csv('../input/Teams.csv')\nSeasons = pd.read_csv('../input/Seasons.csv')\nRegSeasResults = pd.read_csv('../input/RegularSeasonDetailedResults.csv')\nTournResults = pd.read_csv('../input/NCAATourneyDetailedResults.csv')\nTournSeeds = pd.read_csv('../input/NCAATourneySeeds.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c13f6435-ae2f-4337-8cee-ba05ee72eb02","_uuid":"a6b97b65bd33947f2a3fda2afa685551def16be0","trusted":false,"collapsed":true},"cell_type":"code","source":"#List the variables of each\nprint(Teams.columns)\nprint(Seasons.columns)\nprint(RegSeasResults.columns)\nprint(TournResults.columns)\nprint(TournSeeds.columns)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"13e74e79-18a1-406d-88ff-66696069b8c4","collapsed":true,"_uuid":"c221928cfc19f0756c591f016fda265c473389a3","trusted":false},"cell_type":"code","source":"#For each season with regular season statistics, aggregate all of the game statistics for the winning and losing team for each game played.\n#These Statistics will be combined in later steps.\nRegSeasStats1 = RegSeasResults.groupby(['WTeamID', 'Season']).agg({'WScore': [\"mean\", \"count\"], 'WFGM': [\"mean\", \"count\"], 'WFGA': [\"mean\", \"count\"], 'WFGM3': [\"mean\", \"count\"], 'WFGA3': [\"mean\", \"count\"], \n                                                       'WFTM': [\"mean\", \"count\"], 'WFTA': [\"mean\", \"count\"], 'WOR': [\"mean\", \"count\"], 'WDR': [\"mean\", \"count\"], 'WAst': [\"mean\", \"count\"], 'WTO': [\"mean\", \"count\"],\n                                                      'WTO': [\"mean\", \"count\"], 'WStl': [\"mean\", \"count\"], 'WBlk': [\"mean\", \"count\"], 'WPF': [\"mean\", \"count\"]})\nRegSeasStats2 = RegSeasResults.groupby(['LTeamID', 'Season']).agg({'LScore': [\"mean\", \"count\"], 'LFGM': [\"mean\", \"count\"], 'LFGA': [\"mean\", \"count\"], 'LFGM3': [\"mean\", \"count\"], 'LFGA3': [\"mean\", \"count\"], \n                                                       'LFTM': [\"mean\", \"count\"], 'LFTA': [\"mean\", \"count\"], 'LOR': [\"mean\", \"count\"], 'LDR': [\"mean\", \"count\"], 'LAst': [\"mean\", \"count\"], 'LTO': [\"mean\", \"count\"],\n                                                      'LTO': [\"mean\", \"count\"], 'LStl': [\"mean\", \"count\"], 'LBlk': [\"mean\", \"count\"], 'LPF': [\"mean\", \"count\"]})\n#Rename the index names so that they can be merged\nRegSeasStats1.index.names = ['TeamID', 'Season']\nRegSeasStats2.index.names = ['TeamID', 'Season']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"257194b9-7f29-47c6-8eaf-1d04474b2ef8","collapsed":true,"_uuid":"ed596acf195041fd84b7a74a851daf8a018e2910","trusted":false},"cell_type":"code","source":"#Join the aggregated dataframes by 'TeamID' and 'Season' so that each row is the winning/losing game statistics for each team for each season.\nRegSeasStats = RegSeasStats1.join(RegSeasStats2, how = 'outer')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d9feade-c819-42dc-990e-e76dba7c097f","collapsed":true,"_uuid":"22131441e2bde930905eb3a3ea6df8d3736b2256","trusted":false},"cell_type":"code","source":"#Include calculated win percentage. 'GamesPlayed' will be used to compute the weighted averages of the winning/losing game statistics for each team for each season.\nRegSeasStats['WinPercent'] = RegSeasStats['WScore']['count']/(RegSeasStats['WScore']['count'] + RegSeasStats['LScore']['count'])\nRegSeasStats['GamesPlayed'] = RegSeasStats['WScore']['count'] + RegSeasStats['LScore']['count']\n\n#Game statistics names for the combined list.\nStats = ['Score', 'FGM', 'FGA', 'FGM3', 'FGA3', 'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n#Combine winning/losing game stats into season stats for each team.\nfor stat in Stats:\n    RegSeasStats[stat] = (RegSeasStats['W' + stat]['mean']*RegSeasStats['W' + stat]['count'] + RegSeasStats['L' + stat]['mean']*RegSeasStats['L' + stat]['count'])/RegSeasStats['GamesPlayed']\n    del RegSeasStats['W' + stat]\n    del RegSeasStats['L' + stat]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c4af246c-6c37-4605-a931-30e59c508e2b","_uuid":"a3326c4e0df1c1686a2ff3a872a9130579cd69c2"},"cell_type":"markdown","source":"We now have a dataframe of all of the per game statistics for each team for each regular season. These statistics will be used as initial 'naive' inputs into a neural network predicting win probability for one team over another in the NCAA Tournament given the regular season statistics for both."},{"metadata":{"_cell_guid":"e2bf98a2-a0dc-4be5-bcf8-1215fb038925","_uuid":"488d2b4d49eaca7626d4e91d9d1dd2a9dfb6b2d2","trusted":false,"collapsed":true},"cell_type":"code","source":"RegSeasStats[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0306940e-0693-40f5-a2ca-70a7545e8188","_uuid":"b261489a8db485caf6cfa345746b871410f6c873","trusted":false,"collapsed":true},"cell_type":"code","source":"TournResults[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a2a4cd02-013b-460f-b33e-97d388049d02","_uuid":"6c721aaead0b2ce9f154a01b2e99be5f8d4ad56f"},"cell_type":"markdown","source":"This function produces a dictionary with each key being a season and the corresponding value being a 3-d array with dimensions (regular season statistics x 2 teams x number of games played in the tournament for that year). "},{"metadata":{"_cell_guid":"8bb373dd-cfc2-4aa6-a8a6-93bee1467129","collapsed":true,"_uuid":"68b1cc8dab80c96c260ac32afdd979e47ccfe33c","trusted":false},"cell_type":"code","source":"#The find function returns the index for each element of 'elem' in 'searchList'. This way we can extract the regular season statistics for every winning and losing team in the tournament and stack them.\nfind = lambda searchList, elem: [[i for i, x in enumerate(searchList) if x == e] for e in elem]\nInputStats = {}\nfor season in TournResults.Season.unique(): \n    #The regular season statistics for all teams that won at least one game in the tournament for the current season. The same for teams that appeared in a tournament game and lost.\n    WTeamStats = RegSeasStats.loc[(TournResults[TournResults.Season == season].WTeamID, [season]),]\n    LTeamStats = RegSeasStats.loc[(TournResults[TournResults.Season == season].LTeamID, [season]),]\n    #Use the 'find' function to find the regular season statistics index for each winning/losing team in the tournament (possibly indexing the same winning team multiple times that is not eliminated immediately).  \n    idx1 = find(WTeamStats.index.get_level_values(level = 0), list(TournResults[TournResults.Season == season].WTeamID))\n    idx2 = find(LTeamStats.index.get_level_values(level = 0), list(TournResults[TournResults.Season == season].LTeamID))   \n    #Convert dataframes to matrices for concatenation into one 3-d array\n    WTeamStats = WTeamStats.reset_index().as_matrix()\n    LTeamStats = LTeamStats.reset_index().as_matrix()\n    #Concatenate the winning and losing game statistics matrices so that every row corresponds to a particular matchup in the tournament.\n    #Transpose the resulting matrix so that the first two dimensions are the team and statistics dimensions and the third dimension is for each game of the tournament.\n    InputStats[season] = np.transpose(np.concatenate((WTeamStats[idx1,:], LTeamStats[idx2,:]), axis = 1))\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9744bc9d-9349-43e5-86e3-4e534e736dfe","_uuid":"3ee0d5bc70e591a06053b3cb3b5a5ea69dd6ed92","trusted":false,"collapsed":true},"cell_type":"code","source":"#Example for the first game of the 2003 tournament between UNC Asheville (TeamID = 1421) and Texas Southern (TeamID = 1411)\nInputStats[2003][:,:,0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2f2a32fb-50f6-41b0-95bd-6c748c45db5e","_uuid":"89e905e00c92cbce8b6167e39b37c6cc348507cb"},"cell_type":"markdown","source":"**Implementing The First Neural Network With Tensorflow**  \nWe will begin with the implementation of a logistic regression model on the datasets to establish a baseline accuracy to be improved upon with the inclusion of one or more hidden layers comprising a true neural network.  \nOne thing should be made clear about how we are constructing our input dataset to be learned by our models. Each individual input example that we have extracted from historical tournament data will be a 32-element column vector with the first 16 elements being the regular season per game statistics for the team that won that particular tournament game and 16 elements being the statistics for the losing team. These column vectors will be bound together into a single input matrix.  \nIn other words, when we label our input examples every example will be labelled '1' indicating that Team 1 (represented by the first 16 data points) beat Team 2 (represented by the last 16 data points). It will be very easy for any of our models to overfit this lopsided dataset by finding meaningless patterns within the per game statistics that produce high output values (interpreted by the logistic function as near-1 win probability). To prevent overfitting and to influence the model to find patterns between the first 16 team statistics and the last 16 team statistics we will duplicate every example with the positions for Team 1 and Team 2 flipped and label all of those example as '0' indicating that the first team lost that particular matchup. The origianl and generated examples will be shuffled together into the train and test sets.  "},{"metadata":{"_cell_guid":"ba94b0a0-e507-4761-b6f4-6de3981199ef","collapsed":true,"_uuid":"eabf734cbd87a8ea1c8297cbd32e8a8a49569041","trusted":false},"cell_type":"code","source":"#Dictionary for the generated tournament matchup examples, vectors for the reshaped original and generated tournament matchup examples.\nInputStats_alt = {} \nX = np.empty([1, 32])\nX_alt = np.empty([1, 32])\n\nfor season in TournResults.Season.unique():\n    #Delete the first two rows of every example which contain 'TeamID' and 'Season' values. These will not be fed into our machine learning models. \n    InputStats[season] = np.delete(InputStats[season], (0, 1), axis = 0)\n    #Create the alternative dictionary with the position of Team 1 and Team 2 in each array reversed. These will be our examples labeled '0'.\n    InputStats_alt[season] = InputStats[season]\n    InputStats_alt[season] = np.flip(InputStats_alt[season], 1)\n    #Stack the two team regular season statistics one on top of the other for both the original and alternative array\n    InputStats[season] = np.transpose(np.reshape(InputStats[season], (32, InputStats[season].shape[2]), order = 'F'))\n    InputStats_alt[season] = np.transpose(np.reshape(InputStats_alt[season], (32, InputStats_alt[season].shape[2]), order = 'F'))\n    #Add these examples to the new input matrices. \n    X = np.concatenate((X, InputStats[season]), axis = 0)\n    X_alt = np.concatenate((X_alt, InputStats_alt[season]), axis = 0)\nX = np.delete(X, (0), axis = 0)\nX_alt = np.delete(X_alt, (0), axis = 0)\n#Create label vectors for the original and alternative examples.\ny = np.ones((X.shape[0], 1))\ny_alt = np.zeros((X_alt.shape[0], 1))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8fbd6ffc-18b9-4e82-87aa-ba33820789a3","collapsed":true,"_uuid":"f553c756e2ec05e7c668bcbd59e52c3b23397f58","trusted":false},"cell_type":"code","source":"#Concatenate the original and alternative examples.\nX = np.concatenate((X, X_alt), axis = 0)\ny = np.concatenate((y, y_alt), axis = 0)\n\n#Eliminate any examples with any 'nan' values.\nindex_complete = np.sum(np.isfinite(X), axis = 1) == 32\nX = X[index_complete,:]\ny = y[index_complete,:]\n\n#Shuffle the examples and partition them into train and test sets on a 80:20 split.\ntrain_index = np.random.choice(X.shape[0], round(X.shape[0] * 0.8), replace=False)\ntest_index = [i for i in range(X.shape[0]) if i not in train_index]\ntrain_X = X[train_index,:]\ntrain_y = y[train_index,:]\ntest_X = X[test_index,:]\ntest_y = y[test_index,:]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db8aa1b4-1e60-4c65-bbcf-6fee8bc96f61","_uuid":"97abac6b5ba9d7c013152964d7d1b463f0ffa395","trusted":false,"collapsed":true},"cell_type":"code","source":"logisticRegr = LogisticRegression()\nlogisticRegr.fit(train_X, np.ravel(train_y))\npredictions = logisticRegr.predict(test_X)\n#Training accuracy\ntrainscore = logisticRegr.score(train_X, np.ravel(train_y))\nprint(trainscore)\n# Testing accuracy\ntestscore = logisticRegr.score(test_X, np.ravel(test_y))\nprint(testscore)\n\nfrom sklearn import metrics\ncm = metrics.confusion_matrix(np.ravel(test_y), predictions)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0663045d-53b5-4f01-8997-2a71928659a9","collapsed":true,"_uuid":"3c530c784dcb464b6a3bc65c868e7dc344c2e724","trusted":false},"cell_type":"code","source":"learning_rate = 0.01\nnum_iter = 50000","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bd74fa42-ba26-4971-b132-68675cad65c4","collapsed":true,"_uuid":"4a11ba16aad6d2044cf8c3e965f85703fcc9a414","trusted":false},"cell_type":"code","source":"Xtens = tf.placeholder(tf.float64, shape = (None, 32))\nytens = tf.placeholder(tf.float64, shape = (None, 1))\nW = tf.Variable(tf.random_normal([32, 1], dtype = tf.float64)*0.01)\nb = tf.zeros(shape = (1,1), dtype = tf.float64)\n\nlogits = tf.matmul(Xtens, W) + b\n\ncost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = ytens, logits = logits))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n\nprediction = tf.round(tf.sigmoid(logits))\ncorrect = tf.cast(tf.equal(prediction, ytens), dtype=tf.float32)\naccuracy = tf.reduce_mean(correct)\n\ninit = tf.global_variables_initializer()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fadf5f38-3029-4726-9e66-7ce7983eac8e","_uuid":"ff48437d2f552b9ea4d8dd0eaec220e3d58256b2","trusted":false,"collapsed":true},"cell_type":"code","source":"cost_trace = []\ntrain_acc = []\ntest_acc = []\nwith tf.Session() as sess:\n    sess.run(init)\n    \n    for it in range(num_iter):\n        _, temp_cost = sess.run([optimizer, cost], feed_dict = {Xtens: train_X, ytens: train_y})\n        temp_train_acc = sess.run(accuracy, feed_dict={Xtens: train_X, ytens: train_y})\n        temp_test_acc = sess.run(accuracy, feed_dict={Xtens: test_X, ytens: test_y})\n        \n        cost_trace.append(temp_cost)\n        train_acc.append(temp_train_acc)\n        test_acc.append(temp_test_acc)\n        \n        if (it + 1) % 100 == 0:\n            print('iteration: {:4d} loss: {:5f} train_acc: {:5f} test_acc: {:5f}'.format(it + 1, temp_cost, temp_train_acc, temp_test_acc))\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"141338ce-0538-470b-926f-55ed92d59551","_uuid":"3d2484305cf1b0ef2e40276e8eab4b216a5f0b05","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.plot(cost_trace)\nplt.title('Cross Entropy Loss')\nplt.xlabel('iteration of gradient descent')\nplt.ylabel('cost')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f6ac5919-bf72-4a86-b090-a3f78d7ee133","_uuid":"1b8b668d47cca0d172d2b203decc78463c72e595","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.plot(train_acc, 'b-', label='train accuracy')\nplt.plot(test_acc, 'k-', label='test accuracy')\nplt.xlabel('iteration of gradient descent')\nplt.ylabel('accuracy')\nplt.title('Train and Test Accuracy')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4c9902de-e3ca-4619-a36e-5d93fc99ee46","_kg_hide-input":false,"_uuid":"3810df25a3e960a1769948ce9b6c7dd934362993","trusted":false},"cell_type":"code","source":"learning_rate_nn = 5\nnum_epochs = 50000\nbatch_size = 30","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9bfdfab1-6d7f-4fc8-8d1d-310e835c6a3d","collapsed":true,"_uuid":"be0a508f1f96ea5f44c1d73e99d4a0b7febd1a63","trusted":false},"cell_type":"code","source":"Xtens = tf.placeholder(tf.float64, shape = (None, 32))\nytens = tf.placeholder(tf.float64, shape = (None, 1))\n\nW1 = tf.Variable(tf.random_normal([32, 16], dtype = tf.float64)*0.01)\nb1 = tf.zeros(shape = (1,16), dtype = tf.float64)\nW2 = tf.Variable(tf.random_normal([16, 16], dtype = tf.float64)*0.01)\nb2 = tf.zeros(shape = (1,16), dtype = tf.float64)\nW3 = tf.Variable(tf.random_normal([16, 1], dtype = tf.float64)*0.01)\nb3 = tf.zeros(shape = (1,1), dtype = tf.float64)\n\nZ1 = tf.matmul(Xtens, W1) + b1\nA1 = tf.nn.relu(Z1)\n\nZ2 = tf.matmul(A1, W2) + b2\nA2 = tf.nn.relu(Z2)\n\nlogits_nn = tf.matmul(A2, W3) + b3\n\ncost_nn = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = ytens, logits = logits_nn))\noptimizer_nn = tf.train.AdamOptimizer(learning_rate = learning_rate_nn).minimize(cost_nn)\n\nprediction_nn = tf.round(tf.sigmoid(logits_nn))\ncorrect_nn = tf.cast(tf.equal(prediction_nn, ytens), dtype=tf.float32)\naccuracy_nn = tf.reduce_mean(correct_nn)\n\ninit_nn = tf.global_variables_initializer() ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3fca7c52-3e6a-4ae4-8b19-6966c4f25cb5","_uuid":"848a7de6ba1ef21e1a18517d2e683a9c7683148c","trusted":false,"collapsed":true},"cell_type":"code","source":"cost_trace_nn = []\ntrain_acc_nn = []\ntest_acc_nn = []\nwith tf.Session() as sess2:\n    sess2.run(init_nn)\n    \n    for epoch in range(num_epochs):\n#         for minibatch in range(math.ceil((train_X.shape[1] + 1)/batch_size)):\n#             minibatch_X = train_X[:, minibatch*batch_size:min((minibatch + 1)*batch_size,train_X.shape[1])]\n#             minibatch_y = train_y[:, minibatch*batch_size:min((minibatch + 1)*batch_size,train_X.shape[1])]\n        _, temp_cost_nn = sess2.run([optimizer_nn, cost_nn], feed_dict = {Xtens: train_X, ytens: train_y})\n        temp_train_acc_nn = sess2.run(accuracy_nn, feed_dict={Xtens: train_X, ytens: train_y})\n        temp_test_acc_nn = sess2.run(accuracy_nn, feed_dict={Xtens: test_X, ytens: test_y})\n        \n        cost_trace_nn.append(temp_cost_nn)\n        train_acc_nn.append(temp_train_acc_nn)\n        test_acc_nn.append(temp_test_acc_nn)\n        \n        if (epoch + 1) % 100 == 0:\n            print('epoch: {:4d} loss: {:5f} train_acc: {:5f} test_acc: {:5f}'.format(epoch + 1, temp_cost_nn, temp_train_acc_nn, temp_test_acc_nn))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"38590540-7394-49e7-8e01-05180860b81c","_uuid":"d78d94602000dceeb68063af9fc91082b08dabac","trusted":false,"collapsed":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n# fix random seed for reproducibility\nnp.random.seed(7)\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(train_X, train_y, epochs=100, batch_size=10)\n\n# evaluate the model\nscores = model.evaluate(train_X, train_y)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n\ntestscores = model.evaluate(test_X, test_y)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], testscores[1]*100))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}