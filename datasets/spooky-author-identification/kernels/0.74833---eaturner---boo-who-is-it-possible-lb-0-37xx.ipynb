{"nbformat": 4, "cells": [{"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n"], "metadata": {"_cell_guid": "36612a2d-9ddd-4c0c-ab54-373a88022773", "_uuid": "969e01894d6bb8facc5db88fe5122978a2909df3", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "sub = pd.read_csv('../input/sample_submission.csv')"], "metadata": {"_cell_guid": "01a41492-dd35-4c4d-aa93-39a2f3f931b2", "_uuid": "66c9bb1f59bd22a0ce7305bb9e83e02b0f1cbab2", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["We are going to analyze all the grams and use those as features for our classifer, and we are going to assume a largely linear association between the indicators and the labels."], "metadata": {"_cell_guid": "e3db8cd4-e6b7-4609-83d0-54a7bedc4940", "_uuid": "160d4b196389a1a13d1603b0b926754430810067"}, "cell_type": "markdown"}, {"source": ["# Feature Extraction"], "metadata": {"_cell_guid": "95f31e10-65bd-492f-a89d-b4407c154567", "_uuid": "252f1a58ae05254c33f830c5451c05b4507e0f82"}, "cell_type": "markdown"}, {"source": ["Here, we are going to using the count vectorizer and only consider uni-grams, bi-grams, and tri-grams. We will also normalize the counts, and feed that into each classifer.."], "metadata": {"_cell_guid": "2ebe6d35-f863-4408-ab98-982ad7f2b0e1", "_uuid": "d9b095b1acfd8d809133e04255b05a1c4d146e89"}, "cell_type": "markdown"}, {"source": ["word = []\n", "\n", "for text in train['text']:\n", "    word.append( text )\n", "\n", "for text in test['text']:\n", "    word.append( text )"], "metadata": {"_cell_guid": "469358bb-6696-4b16-b241-b83923e3b6a7", "_uuid": "a4610ddd246b29695926ed9623b12ac618e0e3d0", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "\n", "count_vec = CountVectorizer( ngram_range = (1, 3) )\n", "tfid_ = TfidfTransformer( )\n", "\n", "print('Extracing Count Information')\n", "count_vec.fit(word)\n", "train_sparse = count_vec.transform( train['text'] )\n", "test_sparse = count_vec.transform( test['text'] )\n", "\n", "print('Normalizing Count Information')\n", "tfid_.fit( train_sparse )\n", "\n", "train_tfid = tfid_.transform( train_sparse )\n", "test_tfid = tfid_.transform( test_sparse )"], "metadata": {"_cell_guid": "000a4dfe-d1b2-4cf7-80dc-c338c9db80bc", "_uuid": "526872d409f011af56885b125d574cb914937be7", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["author_dict = { 'EAP' : 0, 'HPL' : 1, 'MWS' : 2 }\n", "\n", "author_labels = train['author'].apply(author_dict.get)\n", "train = train.drop('author', axis = 1)\n", "train.drop('id', axis = 1, inplace = True)"], "metadata": {"_cell_guid": "2e558024-c407-4159-8c36-b70b0c9044dd", "_uuid": "31bee561bf5986772a7c5df48900494da008db4a", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Models\n", "\n", "We will perform LogisticRegression, Stochastic Gradient Descent and Naive Bayes on the dataset."], "metadata": {"_cell_guid": "aecf434b-e49a-4ba5-ba4c-5852c02814e9", "_uuid": "2c09074f57bdee36717c6a34fdde9094d559a7ce"}, "cell_type": "markdown"}, {"source": ["test_preds = {}"], "metadata": {"_cell_guid": "82586e92-1c46-40cd-86ad-1c63c2e2b6ac", "_uuid": "95b27008e1567d9abe5bf22b85b5ae2702954278", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## SGD w/log loss"], "metadata": {"_cell_guid": "681044c6-c85b-4dd0-9c92-19ad9247799f", "_uuid": "6cc6ea5117bcc7e796b9f996f53b761013e79905"}, "cell_type": "markdown"}, {"source": ["from sklearn.linear_model import SGDClassifier\n", "\n", "sgd_clf = SGDClassifier(loss = 'log', max_iter = 2000, n_jobs = -1)\n", "\n", "sgd_clf.fit( train_tfid, author_labels )\n", "\n", "test_preds['sgd_clf'] = sgd_clf.predict_proba( test_tfid )"], "metadata": {"_cell_guid": "b6610741-baff-4dc3-807e-c79a1032d4a2", "_uuid": "8e4c31f5cfab46445d038e61e5a2ecd3843258dc", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Naive Bayes"], "metadata": {"_cell_guid": "6d10a059-272d-43cd-a28c-eda4feda9813", "_uuid": "39a927bd097bbdba8a5b82fc82045a6779931763"}, "cell_type": "markdown"}, {"source": ["from sklearn.naive_bayes import MultinomialNB\n", "\n", "nb = MultinomialNB( )\n", "\n", "nb.fit( train_tfid, author_labels )\n", "\n", "test_preds['nb_clf'] = nb.predict_proba( test_tfid )"], "metadata": {"_cell_guid": "7d30c4ef-1616-4dc9-860a-b23280555e46", "_uuid": "99e924935478c8a1ca69b9baa7c21bd5f5692f55", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Logistic Regression"], "metadata": {"_cell_guid": "3714316e-f544-4b7f-b260-441256373743", "_uuid": "96e708bf80b24e301f6b9c90a11419974a44576f"}, "cell_type": "markdown"}, {"source": ["from sklearn.linear_model import LogisticRegression\n", "\n", "log_clf = LogisticRegression( solver = 'saga', multi_class = 'multinomial', \n", "                             max_iter = 500, n_jobs = -1)\n", "\n", "log_clf.fit( train_tfid, author_labels )\n", "\n", "test_preds['log_clf'] = log_clf.predict_proba( test_tfid )"], "metadata": {"_cell_guid": "9e486d3a-d5b3-4a28-90f2-05ff186ed715", "_uuid": "75024296a0594f2513e3d0686a1646d390e32a58", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Stack\n", "\n", "For the stack, we will use the harmonic mean to average our values, and to squash any outliers in the predictions."], "metadata": {"_cell_guid": "cd761c15-c426-4454-94c8-45754b29cbe0", "_uuid": "a0cbf4e5877ca34e878145609b8941adf1fb045e"}, "cell_type": "markdown"}, {"source": ["cols = ['EAP', 'HPL', 'MWS']\n", "sub[cols] = 0.0\n", "\n", "n = len( test_preds.keys() )\n", "\n", "for key in test_preds.keys():\n", "    sub[cols] += (1.0/n)*( test_preds.get(key) ** -1.0)\n", "    \n", "sub[cols] = ( sub[cols].values ) ** -1.0"], "metadata": {"_cell_guid": "d47f82f6-10af-4b9a-9140-d45a7abda44d", "_uuid": "7e446cb68419c5e2815495c74c505d68613aabb8", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["Time to submit."], "metadata": {"_cell_guid": "b05bbc37-3921-4268-9bb3-ac32b455c76c", "_uuid": "84efdc57f972dcf5d0b99d7661e636216b2859f0"}, "cell_type": "markdown"}, {"source": ["sub.to_csv('sub.csv', index = False)"], "metadata": {"_cell_guid": "babd81e2-9e66-4f6a-892e-45432040b1f4", "_uuid": "29542db529896ff551e67e485812513c362f50fd", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Conclusion\n", "\n", "These parameters in each of the feature extraction models and the classifers are not optimized.  With some tinkering with the parameters, it's possible to get the LB score.  Hope this help! "], "metadata": {"_cell_guid": "cd5da61c-dbdc-49ab-badc-c69cf9da4c60", "_uuid": "1a40ceb7cc6aa09145aa62d772017b60a56435f0"}, "cell_type": "markdown"}], "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}