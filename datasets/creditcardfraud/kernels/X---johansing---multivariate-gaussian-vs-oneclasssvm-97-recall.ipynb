{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c8b495ce-32a0-2464-26d5-de7a61bc6232"
      },
      "source": [
        "My first public Kaggle notebook. Using Recall and Precision to judge the predictions. Trying out some ideas for novelty / outlier detection. Implemented my own Multivariate Gaussian outlier detection function and compare to scikit OneClassSVM. \n",
        "\n",
        "I reach 97% recall with 0.01 precision. This corresponds to catching 97% of all frauds, but giving a false alert 99% of the time. Any feedback on this result is much appreciated.\n",
        "\n",
        "Note: I don't think using accuracy as a measure of how well your prediction algorithm works is useful here. If we simply set all predicitons to \"No Fraud\", we obtain an accuracy of over 99%. For more information you can read this https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eab95640-a513-9eec-5779-a938f3b12a65"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f9a714e2-4af1-2138-f483-5e7a6d18aa52"
      },
      "outputs": [],
      "source": [
        "#load data\n",
        "data = pd.read_csv(\"../input/creditcard.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "feb3661f-ae44-e71f-1cbf-4fe6955e2af1"
      },
      "outputs": [],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "07423314-7a20-9d45-1f2d-2eaa0a9dc2a1"
      },
      "outputs": [],
      "source": [
        "data.groupby((\"Class\")).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8aca0d7e-cf41-7bad-cd24-4d5fa6138f7f"
      },
      "source": [
        "Seems like the values for V1, V2, etc are on average much farther from 0 for fraud.\n",
        "\n",
        "Let's check out some correlations matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "df1b39ab-68e5-c8ba-57dc-b2c7b0882fb2"
      },
      "outputs": [],
      "source": [
        "#correlation matrix\n",
        "f, ax = plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(data.drop(['Amount','Time'],1).corr(), vmax=.8, square=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "62e6b34d-d58b-4b45-accc-1a90b1b217a2"
      },
      "source": [
        "* Class correlates most with V1 - V18 and not (or barely) with V19 - V28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d5568c8-5284-2bf6-d0ff-39fc32033eb0"
      },
      "outputs": [],
      "source": [
        "#correlation matrix for only Fraud\n",
        "f, (ax1, ax2) = plt.subplots(1,2,figsize=(13, 5))\n",
        "sns.heatmap(data.query(\"Class==1\").drop(['Class','Time'],1).corr(), vmax=.8, square=True, ax=ax1)\n",
        "ax1.set_title('Fraud')\n",
        "sns.heatmap(data.query(\"Class==0\").drop(['Class','Time'],1).corr(), vmax=.8, square=True, ax=ax2);\n",
        "ax2.set_title('Legit')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "682c7a44-7aa0-933a-7906-ae86a5aed721"
      },
      "source": [
        "* Strong correlations between the different V for Fraud data\n",
        "* Much less correlation for Legit data\n",
        "* Correlation between the data seems to be an important key (This should be captured by Multivariate Gaussian)\n",
        "* Seems like Amount correlates as well. Thus, I should perhaps include it...\n",
        "\n",
        "\n",
        "#### Check out some distributions\n",
        "They should ideally be gaussian for non-fraud examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2393d5e1-b0cf-427b-0add-a4a512775477"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,3))\n",
        "data.query(\"Class==1\").hist(column='V6',bins=np.linspace(-10,10,20),ax=ax1,label='Fraud')\n",
        "ax1.legend()\n",
        "data.query(\"Class==0\").hist(column='V6',bins=np.linspace(-10,10,20),ax=ax2,label='Legit')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,3))\n",
        "data.query(\"Class==1\").hist(column='V2',bins=np.linspace(-10,10,20),ax=ax1,label='Fraud')\n",
        "ax1.legend()\n",
        "data.query(\"Class==0\").hist(column='V2',bins=np.linspace(-10,10,20),ax=ax2,label='Legit')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "164b813a-95de-027d-3b4d-95ca624fa058"
      },
      "source": [
        "(try it for different Vi)\n",
        "\n",
        "For Legit transactions, the Vi are centered around 0 and look kind of gaussian. For frauds, they are off-center"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ccc64a93-e9e1-00a8-d4dd-360b1807ce2f"
      },
      "outputs": [],
      "source": [
        "bins=np.linspace(-10,50,40)\n",
        "data.query(\"Class==1\").hist(column='Amount',bins=bins)\n",
        "data.query(\"Class==0\").hist(column=\"Amount\",bins=bins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7279669e-5dd1-6eff-3f1b-984de268b4d7"
      },
      "outputs": [],
      "source": [
        "data.query(\"Class==1\").hist(column=\"Time\")#,bins=np.linspace(-10,10,20))\n",
        "data.query(\"Class==0\").hist(column=\"Time\")#,bins=np.linspace(-10,10,20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "18666f7d-148d-6a62-6d69-4444cdd776a3"
      },
      "source": [
        "* **TIME** makes no difference apparently\n",
        "* **AMOUNT** hard to say... Does not really look like a gaussian. Frauds seem to have a tendency for low amounts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c60793b8-ac03-f64b-a582-41ab5776dae0"
      },
      "source": [
        "### For now drop \"Amount\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1ce1701f-a003-0a5b-d20c-5e92e6a9c44b"
      },
      "outputs": [],
      "source": [
        "X_Legit = data.query(\"Class==0\").drop([\"Amount\",\"Class\",\"Time\"],1)\n",
        "y_Legit = data.query(\"Class==0\")[\"Class\"]\n",
        "\n",
        "X_Fraud = data.query(\"Class==1\").drop([\"Amount\",\"Class\",\"Time\"],1)\n",
        "y_Fraud = data.query(\"Class==1\")[\"Class\"]\n",
        "\n",
        "#split data into training and cv set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_Legit, y_Legit, test_size=0.33, random_state=42)\n",
        "print(len(X_test))\n",
        "X_test = X_test.append(X_Fraud)\n",
        "print(len(X_Fraud),'   ', len(X_test))\n",
        "y_test = y_test.append(y_Fraud)\n",
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d05a738b-9b37-4857-73d5-203dabf53b9d"
      },
      "outputs": [],
      "source": [
        "data.plot.scatter(\"V21\",\"V22\",c=\"Class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9a41b04d-275d-19ff-a0a0-dc551c6fa74c"
      },
      "source": [
        "V22 and V21 are definitely anticorrelated. The distribution looks like a Gaussian.\n",
        "\n",
        "### Multivariate Gaussian\n",
        "OneClassSVM is further below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e46fa957-eb99-0890-6bed-eff387a6e63a"
      },
      "outputs": [],
      "source": [
        "# Write my own Multivariate Gaussian outlier detection\n",
        "X = X_train\n",
        "m = len(X)\n",
        "mu = 1./m * X.mean()\n",
        "Sigma=0\n",
        "for i in range(m):\n",
        "    Sigma += np.outer((X.iloc[i]-mu) , (X.iloc[i]-mu))\n",
        "Sigma*=1./m\n",
        "Sig_inv = np.linalg.inv(Sigma)\n",
        "Sig_det = np.linalg.det(Sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6fbe078-6e01-9420-2a11-3e2abd131f3c"
      },
      "outputs": [],
      "source": [
        "np.matrix(Sigma).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03774929-9ee7-44df-3d3b-1ec34f86e9ed"
      },
      "outputs": [],
      "source": [
        "# This function calculates the probability for a Gaussian distribution\n",
        "def prob(x_example):\n",
        "    n=len(Sigma)\n",
        "    xminusmu = x_example - mu\n",
        "    return 1./((2*np.pi)**(n/2.) * Sig_det**0.5) * np.exp(-0.5* xminusmu.dot(Sig_inv).dot(xminusmu))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f4cd5536-8fbd-9e0c-2180-365c426f1689"
      },
      "outputs": [],
      "source": [
        "Sigma.diagonal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69f34e5f-2034-5606-31b0-76d7882cac51"
      },
      "outputs": [],
      "source": [
        "# Check out some resulting probablilities for Fraud examples\n",
        "for i in range(10):\n",
        "    print(prob(X_Fraud.iloc[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "423de91e-cb40-1848-f7a4-5c8422edcc1a"
      },
      "outputs": [],
      "source": [
        "# Check out some resulting probablilities for NON-Fraud examples\n",
        "for i in range(10):\n",
        "    print(prob(X_train.iloc[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5abe2e6-0bce-856e-eb38-d0aaa62d1a74"
      },
      "outputs": [],
      "source": [
        "# Picking out 100 training examples to test how many are misclassified as false positive\n",
        "ptrain_result = np.apply_along_axis(prob, 1, X_train.head(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ffc35579-dccc-dcc4-f4d1-bd7de589e0ad"
      },
      "outputs": [],
      "source": [
        "sum(ptrain_result < 1e-13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "062b22b1-4986-d7ca-a5f0-a02b5da13171"
      },
      "source": [
        "With an epsilon of 1e-13, roughly 50% of the test samples are falsely classified as Fraud. Let's see how many are classified correctly using that epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8529f248-e16e-5a88-dff8-855f41ca9bbb"
      },
      "outputs": [],
      "source": [
        "# Copying this to a variable with a new name because i am using the same \n",
        "# variable below with 'Amount' included as feature\n",
        "pTest_result = np.apply_along_axis(prob, 1, X_test)\n",
        "pTest_result_prev = np.copy(pTest_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b51fa5da-750d-9f7e-e132-eb6967c5fcda"
      },
      "outputs": [],
      "source": [
        "epsilon = 1e-13\n",
        "yTest_result_prev = (pTest_result_prev < epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a06fd2ff-b018-abc1-ad20-d6dcf78a302e"
      },
      "outputs": [],
      "source": [
        "tp = sum(yTest_result_prev  & y_test)\n",
        "tn = sum((~ yTest_result_prev)  & (~ y_test))\n",
        "fp = sum((yTest_result_prev)  & (~ y_test))\n",
        "fn = sum((~ yTest_result_prev)  & ( y_test))\n",
        "\n",
        "print(\"true_pos \",tp)\n",
        "print(\"true_neg \",tn)\n",
        "print(\"false_pos \",fp)\n",
        "print(\"false_neg \",fn)\n",
        "\n",
        "recall = tp / (tp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "F1 = 2*recall*precision/(recall+precision)\n",
        "print(\"recall=\",recall,\"\\nprecision=\",precision)\n",
        "print(\"F1=\",F1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "30188d18-5562-9c3b-ddf8-59dbd0b45270"
      },
      "source": [
        "Thus, I obtain a recall of 97%, but a low precision of 0.01, which means only 1 out of 100 fraud 'detections' are actual frauds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c02bdcea-91b9-2a54-5628-18a434b83224"
      },
      "source": [
        "#### rescale \"Amount\" and use it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8bf9958e-e366-5c8a-45d2-b435c1a31ffa"
      },
      "outputs": [],
      "source": [
        "data[\"Amountresc\"] = (data[\"Amount\"])/data[\"Amount\"].var()\n",
        "\n",
        "X_Legit = data.query(\"Class==0\").drop([\"Amount\",\"Class\",\"Time\"],1)\n",
        "y_Legit = data.query(\"Class==0\")[\"Class\"]\n",
        "\n",
        "X_Fraud = data.query(\"Class==1\").drop([\"Amount\",\"Class\",\"Time\"],1)\n",
        "y_Fraud = data.query(\"Class==1\")[\"Class\"]\n",
        "\n",
        "#split data into training and cv set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_Legit, y_Legit, test_size=0.33, random_state=42)\n",
        "X_test = X_test.append(X_Fraud)\n",
        "y_test = y_test.append(y_Fraud)\n",
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "64ccd32a-14a9-ab8d-7668-c9bde18ac0ab"
      },
      "outputs": [],
      "source": [
        "# Use my outlier detection\n",
        "X = X_train\n",
        "m = len(X)\n",
        "mu = 1./m * X.mean()\n",
        "Sigma=0\n",
        "for i in range(m):\n",
        "    Sigma += np.outer((X.iloc[i]-mu) , (X.iloc[i]-mu))\n",
        "Sigma*=1./m\n",
        "Sig_inv = np.linalg.inv(Sigma)\n",
        "Sig_det = np.linalg.det(Sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d9aaa15a-07a4-d777-7f29-04b8c52d53af"
      },
      "outputs": [],
      "source": [
        "pTest_result = np.apply_along_axis(prob, 1, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6f95ab57-2d50-639b-3ea0-ab4bc4e84edd"
      },
      "outputs": [],
      "source": [
        "epsilon = 2e-11\n",
        "yTest_result = (pTest_result < epsilon)\n",
        "\n",
        "tp = sum(yTest_result  & y_test)\n",
        "tn = sum((~ yTest_result)  & (~ y_test))\n",
        "fp = sum((yTest_result)  & (~ y_test))\n",
        "fn = sum((~ yTest_result)  & ( y_test))\n",
        "\n",
        "print(\"true_pos \",tp)\n",
        "print(\"true_neg \",tn)\n",
        "print(\"false_pos \",fp)\n",
        "print(\"false_neg \",fn)\n",
        "\n",
        "recall = tp / (tp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "F1 = 2*recall*precision/(recall+precision)\n",
        "print(\"recall=\",recall,\"\\nprecision=\",precision)\n",
        "print(\"F1=\",F1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c1cfea26-2003-9b60-fc8b-1675c622dfea"
      },
      "source": [
        "### Conclusion\n",
        "No real improvement. Note, that I am using a larger epsilon\n",
        "\n",
        "\n",
        "\n",
        "### Next up\n",
        "I can try the Novelty Detection algorithm by scikit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f69accd8-9342-a2a8-c8ef-0ff216e26282"
      },
      "outputs": [],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bd077698-fc80-5a82-0d37-ec2956d04569"
      },
      "outputs": [],
      "source": [
        "## Use only part of training set, otherwise it takes very long\n",
        "Xsmall = X_train.head(20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "70f2fbd1-6992-065e-a97d-d5fb20b5662d"
      },
      "outputs": [],
      "source": [
        "# fit the model\n",
        "clf = svm.OneClassSVM(kernel=\"rbf\", nu=0.01, gamma=0.3)\n",
        "clf.fit(Xsmall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a5dab85-fbc7-f93a-7130-2529b7dc6b16"
      },
      "outputs": [],
      "source": [
        "y_pred_test = clf.predict(X_test)\n",
        "y_pred_test = np.array([y==-1 for y in y_pred_test])\n",
        "\n",
        "tp = sum(y_pred_test  & y_test)\n",
        "tn = sum((~ y_pred_test)  & (~ y_test))\n",
        "fp = sum((y_pred_test)  & (~ y_test))\n",
        "fn = sum((~ y_pred_test)  & ( y_test))\n",
        "\n",
        "\n",
        "print(\"true_pos \",tp)\n",
        "print(\"true_neg \",tn)\n",
        "print(\"false_pos \",fp)\n",
        "print(\"false_neg \",fn)\n",
        "\n",
        "recall = tp / (tp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "F1 = 2*recall*precision/(recall+precision)\n",
        "print(\"recall=\",recall,\"\\nprecision=\",precision)\n",
        "print(\"F1=\",F1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d5a1762-6085-3590-020a-41e0b3e283f3"
      },
      "outputs": [],
      "source": [
        "# ## Some results from different test runs\n",
        "# # nu=0.01, gamma=0.3\n",
        "# true_pos  478\n",
        "# true_neg  55794\n",
        "# false_pos  38030\n",
        "# false_neg  14\n",
        "# recall= 0.971544715447 \n",
        "# precision= 0.0124130050899\n",
        "# F1= 0.0245128205128\n",
        "\n",
        "# # nu=0.05, gamma=0.3\n",
        "# true_pos  478\n",
        "# true_neg  55774\n",
        "# false_pos  38050\n",
        "# false_neg  14\n",
        "# recall= 0.971544715447 \n",
        "# precision= 0.0124065614618\n",
        "# F1= 0.0245002562788\n",
        "\n",
        "# # nu=0.05, gamma=0.2\n",
        "# true_pos  463\n",
        "# true_neg  68954\n",
        "# false_pos  24870\n",
        "# false_neg  29\n",
        "# recall= 0.941056910569 \n",
        "# precision= 0.0182765562705\n",
        "# F1= 0.0358567279768\n",
        "\n",
        "# # nu=0.5, gamma=0.5\n",
        "# true_pos  487\n",
        "# true_neg  36001\n",
        "# false_pos  57823\n",
        "# false_neg  5\n",
        "# recall= 0.989837398374 \n",
        "# precision= 0.00835191219345\n",
        "# F1= 0.0165640624469\n",
        "\n",
        "# # nu=0.5, gamma=0.1\n",
        "# true_pos  478\n",
        "# true_neg  47316\n",
        "# false_pos  46508\n",
        "# false_neg  14\n",
        "# recall= 0.971544715447 \n",
        "# precision= 0.0101732430937\n",
        "# F1= 0.0201356417709"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fe0b80d9-dad4-3c66-ac85-cf81ef0a70cc"
      },
      "source": [
        "- With nu=0.5, gamma=0.1:  97% of frauds are detected, but only 1 in 100 detections is actual fraud (i.e. 99% false alert). Seems fine to me... But: It's not better than Multivariate Gaussian (but much slower).\n",
        "- with a smaller nu we get larger F1 and higher precision BUT smaller recall...\n",
        "- Same thing goes for gamma... Larger gamma means larger recall and smaller precision. \n",
        "- TODO: Plot precision, recall and F1 as a function of mu and gamma.\n",
        "- TODO (?): Add new features V1xV3, V1xV5, V1xV7 to make use of the strong correlation between the two for fraud detection and see if it improves results. For multivariate Gaussian, the correlations should already be included. I am not sure if this is also the case for OneClass SVM with a Gaussian Kernel...\n",
        "\n",
        "### Try another classification algorithm - IsolationForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d8253ff-268e-c2cd-1934-35da13d41d41"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "rng = np.random.RandomState(42)\n",
        "\n",
        "\n",
        "clf = IsolationForest(max_samples=10, random_state=rng)\n",
        "clf.fit(X_train.head(100000))\n",
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "#y_pred_outliers = clf.predict(X_outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "59126d86-b7b2-2517-ff90-89c87d6739cd"
      },
      "outputs": [],
      "source": [
        "y_pred_test = clf.predict(X_test)\n",
        "y_pred_test = np.array([y==-1 for y in y_pred_test])\n",
        "\n",
        "tp = sum(y_pred_test  & y_test)\n",
        "tn = sum((~ y_pred_test)  & (~ y_test))\n",
        "fp = sum((y_pred_test)  & (~ y_test))\n",
        "fn = sum((~ y_pred_test)  & ( y_test))\n",
        "\n",
        "\n",
        "print(\"true_pos \",tp)\n",
        "print(\"true_neg \",tn)\n",
        "print(\"false_pos \",fp)\n",
        "print(\"false_neg \",fn)\n",
        "\n",
        "recall = tp / (tp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "F1 = 2*recall*precision/(recall+precision)\n",
        "print(\"recall=\",recall,\"\\nprecision=\",precision)\n",
        "print(\"F1=\",F1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d01eab29-dfe6-5f45-ffa4-45c532586a03"
      },
      "source": [
        "This does not look very promising and I know too little about Random Forest... \n",
        "\n",
        "In fact, isolation forest seems to be for outlier detection, not for novely detection (http://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection). So maybe it's not that useful here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "30a85d0b-2586-7dcc-dcad-c367c02f642c"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}