{"cells":[{"metadata":{"_uuid":"4ff6a3cbd6b7b080ae59d80052aaa879b0ef6ee2"},"cell_type":"markdown","source":"# PetFinder.my Adoption Prediction\n**Author**: Enrique Herreros  \n**First version date**: 29/12/2018  \n**Description**: A Kernel with an Exploratory Data Analysis to inspect superficially all the data that have been provided to us to predict adoption speed \n\n**Notes** \nIn this competition you will predict the speed at which a pet is adopted, based on the pet’s listing information present on PetFinder.\nSometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted.\nThe data included open text, tabular, image data and results from running Google Vision API. See below for details.\nThis is a Kernels-only competition.\nAt the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data.\n\n**File descriptions** \ntrain.csv - Tabular/text data for the training set\ntest.csv - Tabular/text data for the test set\nsample_submission.csv - A sample submission file in the correct format\nbreed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\ncolor_labels.csv - Contains ColorName for each ColorID\nstate_labels.csv - Contains StateName for each StateID\n\n# Table of contents\n1. Libraries and Data loading\n* Sneak peek\n  * Tabular\n  * Images\n* Null count\n* Duplicates\n* Correlation matrix\n * Dogs\n * Cats\n* Distribution\n * Univariate\n * Bivariate with target\n * Bivariate between correlated variables\n* Base Model\n* Submission\n"},{"metadata":{"_uuid":"68e5ebd84d8190cce8f11fdd26f991c8affb8541"},"cell_type":"markdown","source":"# Libraries and Data loading"},{"metadata":{"_uuid":"82edc4b46bf5d3602dac6e00231ac78a24b9b9ab"},"cell_type":"markdown","source":"Libraries loading"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\n\n\nfrom IPython.display import display, HTML","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6048b0b8ae3339ff167e76e6a7ddb66844b63716"},"cell_type":"markdown","source":"Visualization configuration"},{"metadata":{"trusted":true,"_uuid":"2170ff80e72583911fe0c45eaf7cf72c20108aba"},"cell_type":"code","source":"# Table printing large\nplt.rcParams['figure.figsize'] = (15, 7)\npd.set_option(\"display.max_columns\", 400)\npd.options.display.max_colwidth = 250\npd.set_option(\"display.max_rows\", 100)\n# High defition plots\n%config InlineBackend.figure_format = 'retina'\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c85ef9303ee4a55864fc63680a988194e6e7a426"},"cell_type":"markdown","source":"Global variables"},{"metadata":{"trusted":true,"_uuid":"a8c60e159508ca5e7e83a3d4bfe214f276806597"},"cell_type":"code","source":"base_path_data = \"../input/\"\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f6f277d869c29038bc005ed21682d1e01a0e0ae"},"cell_type":"markdown","source":"Train and test data"},{"metadata":{"trusted":true,"_uuid":"4c9741e0f3c3b5cc8c4daa7899a98b0946b61267"},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train/train.csv\")\ndf_test = pd.read_csv(\"../input/test/test.csv\")\n\nprint(f\"train.csv shape is {df_train.shape}\")\nprint(f\"test.csv shape is {df_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7347faab773bfc47dc7ce86a001e25dc2f36ce6e"},"cell_type":"code","source":"print(\"Basic statistics of the train set\")\ndisplay(df_train.describe(include=\"all\").T)\n\nprint(\"Basic statistics of the test set\")\ndisplay(df_test.describe(include=\"all\").T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4014de9a21b1cf0578ea08c496d19cb8c2a5e556"},"cell_type":"markdown","source":"# Sneak peek"},{"metadata":{"trusted":true,"_uuid":"7537f08b34773f24ab95f98ed030040f6e165ee6"},"cell_type":"markdown","source":"## Tabular"},{"metadata":{"_uuid":"fe9f649bbdc06eb8cc4386e117a0c296c05b8820"},"cell_type":"markdown","source":"Main data set"},{"metadata":{"trusted":true,"_uuid":"f78acc043ffef70440743ce5e7a1aa35aaa9e5f7"},"cell_type":"code","source":"df_train.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54b3a0587878c68ea0e50abe53291fb811b02659"},"cell_type":"markdown","source":"Breed, colors and states mapping values"},{"metadata":{"trusted":true,"_uuid":"c1a93b2f51b22c4e68c121614b6f2d07d0ed5c18","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"df_breed_labels = pd.read_csv(os.path.join(base_path_data, \"breed_labels.csv\"))\ndf_color_labels = pd.read_csv(os.path.join(base_path_data, \"color_labels.csv\"))\ndf_state_labels = pd.read_csv(os.path.join(base_path_data, \"state_labels.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a775cc4b0790be7c8d89d8e8525ef53764ae9efa","_kg_hide-input":true},"cell_type":"code","source":"print(f\"breed_labels.csv shape is {df_breed_labels.shape}\")\ndf_breed_labels.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"806f34f158cc23c63dd3ba82db3e317a73536d6a","_kg_hide-input":true},"cell_type":"code","source":"print(f\"color_labels.csv shape is {df_color_labels.shape}\")\ndf_color_labels.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bbb7d101d5deb32439c8318a98f9b5f27d7701c","_kg_hide-input":true},"cell_type":"code","source":"print(f\"state_labels.csv shape is {df_state_labels.shape}\")\ndf_state_labels.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e94b9edb807da76637103188e5607ad9a5ea3bb1"},"cell_type":"markdown","source":"Example of output from Google Vision API"},{"metadata":{"trusted":true,"_uuid":"b169725afd1fe7a4ff8cd9ea18c63bca38ab5ff2"},"cell_type":"code","source":"# sentiment\nwith open(os.path.join(base_path_data, \"train_sentiment\", \"048cd8bc0.json\")) as f:\n    data = json.load(f)\n\npprint(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06d734f3b22f28b2bc38ef9f36d96ca2580bf4ce"},"cell_type":"code","source":"# metadata\nwith open(os.path.join(base_path_data, \"train_metadata\", \"000fb9572-6.json\")) as f:\n    data = json.load(f)\n\npprint(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"826e51df807dca225ad317c78c7ae2dd628a5284"},"cell_type":"markdown","source":"I'm not sure if the classes predicted from the Google API would be of interest in this problem as the whole data set is about dogs and cats. I think that it would be more interesting to have aesthetic quality, blurriness or stuff related to quality of the picture, that could attract users towards certain pet or the other. We like it or not, humans feel attracted mainly by what they see."},{"metadata":{"_uuid":"4d09b4619d43c32184e43ec315b6c9a369c7c01f"},"cell_type":"markdown","source":"Are there any shared values of RescuerID between train and test?"},{"metadata":{"trusted":true,"_uuid":"0f0267504b13935e428817e37e5c8d18fcaa5268"},"cell_type":"code","source":"set(df_train.RescuerID.unique()).intersection(set(df_test.RescuerID.unique()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30d221bd9d937691f55d5b600f0ed05f2becf099"},"cell_type":"markdown","source":"Nope. What about common names?"},{"metadata":{"trusted":true,"_uuid":"18b77ffdef7224184df96311eca979c79e9ccd91"},"cell_type":"code","source":"common_names = list(set(df_train.Name.unique()).intersection(set(df_test.Name.unique())))\nlen(common_names)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65b2f99afc7e5c4d76c2ec5d38394caee47557e7"},"cell_type":"markdown","source":"Wow, more than 800 names in common, let's see a few of them..."},{"metadata":{"trusted":true,"_uuid":"854391caf7b7657637829d87c35a558c6a0cfa6e"},"cell_type":"code","source":"common_names[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac1253338bcd1c09c6bcac94e282121f7064e275"},"cell_type":"markdown","source":"## Images\n"},{"metadata":{"_uuid":"9b760980cf5393946cb2f1fb1bd5f1956a231053"},"cell_type":"markdown","source":"Let's take a look at random images from the train set together with the pet's name and its adoption speed "},{"metadata":{"trusted":true,"_uuid":"c442320e17230c82b0bb02353d6c9bcd87cb75ea","_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(8,3, figsize=(15, 20))\nimages_train = os.listdir(\"../input/train_images/\")\nfig.suptitle(\"24 random pet images\")\nimages_train = np.random.choice(images_train, 24)\nfor i, img in enumerate(images_train):\n    image = Image.open(\"../input/train_images/\" + img)\n    pet_id = img.split(\"-\")[0]\n    axes[i//3, i%3].imshow(image)\n    axes[i//3, i%3].grid(False)\n    axes[i//3, i%3].set_axis_off()\n    axes[i//3, i%3].set_title(\"Name: {}\\nAdoptionSpeed: {}\".format(*list(map(str, df_train[df_train.PetID==pet_id][[\"Name\", \"AdoptionSpeed\"]].values.tolist()[0]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4a524584cc5bbdbac38196844c34941e03040d1"},"cell_type":"markdown","source":"> **ALL VERY CUTE!**"},{"metadata":{"_uuid":"fbbdd9996435cd20a82a6b2dbfa9a582dba1c196"},"cell_type":"markdown","source":"# Null count"},{"metadata":{"_uuid":"f688acd99543ee8353604bdbe14483f770d5a53b"},"cell_type":"markdown","source":"Percentage of null values in the train set per column"},{"metadata":{"trusted":true,"_uuid":"a5aeabfd9f0a876a8e6a007301ea0919db9a5735"},"cell_type":"code","source":"100 * df_train.isnull().sum() / len(df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca6cc3091911f9b59742a5a78c97cb76744a3014"},"cell_type":"markdown","source":"Percentage of null values in the test set per column"},{"metadata":{"trusted":true,"_uuid":"1490462b26a123b2acc168af3dd6114e545866f2","_kg_hide-input":false},"cell_type":"code","source":"100 * df_test.isnull().sum() / len(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ccf3af1b94a4103f4a9e5c0c224eae17a782dbd"},"cell_type":"markdown","source":"In general, there are not many null values. There seems to be only null values in the Name and Description columns. Either PetFinder considers those 2 values as mandatory ones to get filled during pet card completition or the null values are encoded with a value, we will check later."},{"metadata":{"_uuid":"66466239fd61f1dc46237abdf08d54767362b82f"},"cell_type":"markdown","source":"# Duplicates"},{"metadata":{"trusted":true,"_uuid":"45958c2276e4fff22f6cecc4b51284a4a08c8788"},"cell_type":"code","source":"df_train[df_train.duplicated(keep=False)].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7b9a49cd9cf22db7fb8a5a9c1f4e5c25109f3e3"},"cell_type":"markdown","source":"There are no full-duplicates in the data set. Let's see if there are duplicates once we remove name, Pet ID and description (we keep rescuer ID though)"},{"metadata":{"trusted":true,"_uuid":"e7b2df6b41047b74478fee9d626b82712df45d32"},"cell_type":"code","source":"cols = [col for col in df_train.columns.ravel() if col not in [\"PetID\", \"Name\", \"Description\", \"AdoptionSpeed\"]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfef59281eec4d958ea29097f134612dc0314612"},"cell_type":"markdown","source":"In the train set"},{"metadata":{"trusted":true,"_uuid":"35df3ad8ef2758f66abf04ec36fd3942c245c181"},"cell_type":"code","source":"dups = df_train[df_train[cols].duplicated(keep=False)].sort_values(by=cols)\nprint(f\"Shape of matrix with duplicated rows not considering petid, name nor description {dups.shape}\")\ndups.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fd8dd4b22b8c98da13527b1236dd006ba369e82"},"cell_type":"markdown","source":"In the test set"},{"metadata":{"trusted":true,"_uuid":"68a2c0979183d88413f6f207ae11673253e8469d"},"cell_type":"code","source":"dups = df_test[df_test[cols].duplicated(keep=False)].sort_values(by=cols)\nprint(f\"Shape of matrix with duplicated rows not considering petid, name nor description {dups.shape}\")\ndups.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cde25f6121fabb9c2d962b6f3d6bc4aa8985df43"},"cell_type":"markdown","source":"\n## Correlation matrix"},{"metadata":{"trusted":true,"_uuid":"bfde28f64172b4cfa3d231aaf2dec0a6bd04a286"},"cell_type":"code","source":"def plot_correlation_matrix(df):\n    corr = df.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(20, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40d25e010c00846175b8a9a05c0c52dab155d6e4"},"cell_type":"markdown","source":"## All animals"},{"metadata":{"_uuid":"f9bb8289f18bbe7b68e5f59dbcb3dd46714ee47b"},"cell_type":"markdown","source":"Correlation matrix of whole train set"},{"metadata":{"trusted":true,"_uuid":"a45777f924c45c7e892cbeef849946c2cf9c90cf"},"cell_type":"code","source":"plot_correlation_matrix(df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dea4208662a7e516c79cbfaf4ede674b1ff9ec6"},"cell_type":"markdown","source":"Correlation matrix of whole test set"},{"metadata":{"trusted":true,"_uuid":"add6a980e89c062435501b657c0e558612183270"},"cell_type":"code","source":"plot_correlation_matrix(df_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dbe0f33eef64113c291f2e0e17e64151e1dba14"},"cell_type":"markdown","source":"## Only dogs"},{"metadata":{"_uuid":"a45109facc5d8b40696cf566fb8ffad6c3d567dd"},"cell_type":"markdown","source":"Correlation matrix of dogs part of the train set"},{"metadata":{"trusted":true,"_uuid":"dcba76b93ed7e0ca6187e39733bece682b8cdee2"},"cell_type":"code","source":"plot_correlation_matrix(df_train[df_train.Type==1].drop(columns=\"Type\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c6d8b39d5c61b8548e23b39a3f2a9587c6af97b"},"cell_type":"markdown","source":"Correlation matrix of dogs part of the test set"},{"metadata":{"trusted":true,"_uuid":"1568651527f44868113010005a2ab75118aefcaf"},"cell_type":"code","source":"plot_correlation_matrix(df_test[df_test.Type==1].drop(columns=\"Type\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0cd72848ea498a817fb7370ebfd48bd9335d203"},"cell_type":"markdown","source":"## Only cats"},{"metadata":{"_uuid":"d30955943a7b855176553f3fe4af8d78cf1dc75c"},"cell_type":"markdown","source":"Correlation matrix of cats part of the train set"},{"metadata":{"trusted":true,"_uuid":"ec586ce9e331b6f15673c96553d68df263053cd3"},"cell_type":"code","source":"plot_correlation_matrix(df_test[df_test.Type==2].drop(columns=\"Type\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a7976b619c4596cef79db65189006e8022792d1"},"cell_type":"markdown","source":"Correlation matrix of cats part of the test set"},{"metadata":{"trusted":true,"_uuid":"6d66e651ca5217fcf614e8bafd18f84aaa4ec236"},"cell_type":"code","source":"plot_correlation_matrix(df_test[df_test.Type==2].drop(columns=\"Type\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edc452bdc6ec9179f5cd04b9d1b54e116bec9953"},"cell_type":"markdown","source":"\n# Distribution"},{"metadata":{"trusted":true,"_uuid":"5b3534966548949a8e58ce63d50cb29d3704ae99"},"cell_type":"code","source":"feats = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1',\n       'Color2', 'Color3', 'MaturitySize', 'FurLength', 'Vaccinated',\n       'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State',\n       'VideoAmt', 'PhotoAmt', 'AdoptionSpeed']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"218f451ccbee45cdf1067be43e94dff88fe5afe4"},"cell_type":"markdown","source":"## Univariate\n"},{"metadata":{"_uuid":"bce590e538882941e527e67bed7f8c44eda86401"},"cell_type":"markdown","source":"First of all, let's see the target distribution"},{"metadata":{"trusted":true,"_uuid":"cd1b70c7d72fce58340abd23393c5606586ef032","_kg_hide-input":true},"cell_type":"code","source":"def plot_distribution(df, feat):\n    fig, ax = plt.subplots(figsize=(17,5))\n    sns.countplot(df_train[feat])\n    ax.xaxis.set_label_text(feat,fontdict= {'size':14})\n    ax.yaxis.set_label_text(\"Count\",fontdict= {'size':14})\n    plt.show()\n    print(f\"Total number of unique values for feature {feat} is {df[feat].nunique()}\")\n    print(100 * df_train[feat].value_counts(normalize=True, dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dd0bade062530d17f141ea61f463dac845e60c3"},"cell_type":"code","source":"plot_distribution(df_train, \"AdoptionSpeed\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fff68a1e0c6b34f8a102763a1d2ea3c9b9eba052"},"cell_type":"markdown","source":"And now for the rest of the features"},{"metadata":{"trusted":true,"_uuid":"9bc38103e692c9dfce3fc2f406f9075af2b63ef0","scrolled":false},"cell_type":"code","source":"for feat in feats:\n    print(f\"Univariate distribution of feature {feat}\")\n    plot_distribution(df_train, feat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0939ee5bae4947aab0c0c3287e51439a3c2c4b9"},"cell_type":"markdown","source":"The previous plots yield multiple things:\n1. Most of the features have very skewed distributions\n2. We should group together misrepresented values before one hot encoding them\n3. Not many publishings contain videos\n4. Most of the publishings have less than 5 pictures\n5. The target class is quite balanced (classes 1 to 4 have around 23% presence), apart from class 0 (less than 3% of the cases)\n6. Null encoding is sometimes represented with the value 0 = Not Specified in MaturitySize, FurLength or Health or value 3 = Not Sure in Vaccinated, Dewormed or Sterilized (as specified in the description)"},{"metadata":{"_uuid":"f6f221d3420d12d7e3f2eee480d2d57b6ae2d6d6"},"cell_type":"markdown","source":" ## Bivariate with target"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"10933083031c746d12c408ee44129e3688ddfed3"},"cell_type":"code","source":"for cat_c in feats:\n    if cat_c == \"AdoptionSpeed\": continue\n    nunique = df_train[cat_c].nunique()\n    print(f'{cat_c}:')\n    print(f'{nunique} unique values')\n    if nunique < 50:\n        print(f'\\nValues:\\n{100 * df_train[cat_c].value_counts(normalize=True, dropna=False)}')\n      \n        # Countplot\n        fig, ax = plt.subplots(figsize=(12,4))\n        sns.countplot(x=cat_c, hue=\"AdoptionSpeed\", data=df_train, orient=\"h\")\n        #ax.text(5,5,\"Boxplot After removing outliers\", fontsize=18, color=\"r\", ha=\"center\", va=\"center\")\n        ax.xaxis.set_label_text(cat_c,fontdict= {'size':14})\n        ax.yaxis.set_label_text(\"Count\",fontdict= {'size':14})\n        plt.xticks(rotation=90)\n        plt.show()\n    else:\n        # Distplot to see the distribution after outliers have been removed\n        sns.set_style(\"whitegrid\")\n        fig, ax = plt.subplots(figsize=(12,4))\n        for aspeed in range(5):\n            sns.distplot(df_train[df_train.AdoptionSpeed == aspeed][cat_c].dropna(), hist=False, rug=False, label=\"AdoptionSpeed = {}\".format(aspeed))\n        ax.xaxis.set_label_text(cat_c,fontdict= {'size':14})\n        plt.xticks(rotation=90)\n        plt.legend()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c4c63c9dd9550bb48081513aa4adde67164f129"},"cell_type":"markdown","source":"TO-DO: same approach but normalizing by AdoptionSpeed frequency"},{"metadata":{"_uuid":"20e944897daf05844fba6d811210af4ad730d531"},"cell_type":"markdown","source":"## Bivariate between correlated variables"},{"metadata":{"trusted":true,"_uuid":"7afd2e82b2d4dce2e4f23c5bd65abf04f133bff3"},"cell_type":"code","source":"c = df_train.corr().abs()\ns = c.unstack()\nso = s.sort_values(kind=\"quicksort\", ascending=False)\nso = pd.DataFrame(so[20:]).reset_index()\nso.columns = [\"var1\", \"var2\", \"corr\"]\nso = so[so[\"corr\"] > 0.3]\nso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"7256e31a0a22d6bfee5e8e7fc2567acd6640f3eb"},"cell_type":"code","source":"\nfor k, v in so.iterrows():\n    print(f'{v[\"var1\"]} vs {v[\"var2\"]} ({v[\"corr\"]})')\n    if (df_train[v[\"var1\"]].nunique() < 50) and (df_train[v[\"var2\"]].nunique() < 50):\n        fig, ax = plt.subplots(figsize=(12,4))\n        sns.countplot(x=v[\"var1\"], hue=v[\"var2\"], data=df_train, orient=\"h\")\n        #ax.text(5,5,\"Boxplot After removing outliers\", fontsize=18, color=\"r\", ha=\"center\", va=\"center\")\n        ax.xaxis.set_label_text(v[\"var1\"], fontdict= {'size':14})\n        ax.yaxis.set_label_text(\"Count\",fontdict= {'size':14})\n        plt.xticks(rotation=90)\n        plt.show()\n    else:\n        # Distplot to see the distribution after outliers have been removed\n        sns.set_style(\"whitegrid\")\n        fig, ax = plt.subplots(figsize=(12,4))\n        sns.scatterplot(x=v[\"var1\"], y=v[\"var2\"], data=df_train, hue=\"AdoptionSpeed\")\n        ax.xaxis.set_label_text(v[\"var1\"], fontdict= {'size':14})\n        #plt.xticks(rotation=90)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74a131e2af873fd7ac8bc7fe3cc504e0d4970c22"},"cell_type":"markdown","source":"# Base model"},{"metadata":{"trusted":true,"_uuid":"a56f9d796570f8f048ff468021c74bded29118c8"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nimport scipy as sp\nfrom sklearn import linear_model\nfrom functools import partial\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom collections import Counter\nimport json\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ad486053cdf786755f972baea6ba834c3ef8026"},"cell_type":"code","source":"# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4a0953f5edec27b306794519a6d80f878e3cb54"},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ccecae693d766303674dfb85da14044f4738ee6"},"cell_type":"code","source":"def rmse(actual, predicted):\n    return mean_squared_error(actual, predicted)**0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac0cca75fa75f7870fb1f0db8feca775cb9a7c55"},"cell_type":"code","source":"train_desc = df_train.Description.fillna(\"none\").values\ntest_desc = df_test.Description.fillna(\"none\").values\n\ntfv = TfidfVectorizer(min_df=3,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n    \n# Fit TFIDF\ntfv.fit(list(train_desc) + list(test_desc))\nX =  tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\n\nsvd = TruncatedSVD(n_components=180)\nsvd.fit(X)\nX = svd.transform(X)\n\nX_test = svd.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca8d10e607c10a931a554d0e8a2f40fe99403de1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a6759d2da83199e3f4802ee74a60751a85b289e"},"cell_type":"code","source":"train_desc = df_train.Description.fillna(\"none\").values\ntest_desc = df_test.Description.fillna(\"none\").values\n\nsvd_n_components = 200\n\ntfv = TfidfVectorizer(min_df=2,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        )\n    \n# Fit TFIDF\ntfv.fit(list(train_desc))\nX =  tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\nsvd = TruncatedSVD(n_components=svd_n_components)\nsvd.fit(X)\nprint(svd.explained_variance_ratio_.sum())\nprint(svd.explained_variance_ratio_)\nX = svd.transform(X)\nX = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(svd_n_components)])\ndf_train = pd.concat((df_train, X), axis=1)\nX_test = svd.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(svd_n_components)])\ndf_test = pd.concat((df_test, X_test), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2fcf901ccbe32daf1e18eb86e24d51a05709f22"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a90a02680f9f33b54d98b42c2fe9ca4406284659"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d631fbb3c4955d066ceb797f4b9d270fa550b13"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"425c58f3b6fec6d064a7bcc12d306502d9aea721"},"cell_type":"code","source":"# Thanks to beloruk1\n\ndef readFile(fn):\n    file = '../input/train_sentiment/'+fn['PetID']+'.json'\n    if os.path.exists(file):\n        with open(file) as data_file:    \n            data = json.load(data_file)  \n\n        df = json_normalize(data)\n        mag = df['documentSentiment.magnitude'].values[0]\n        score = df['documentSentiment.score'].values[0]\n        return pd.Series([mag,score],index=['mag','score']) \n    else:\n        return pd.Series([0,0],index=['mag','score'])\n    \ndef readTestFile(fn):\n    file = '../input/test_sentiment/' + fn['PetID'] + '.json'\n    if os.path.exists(file):\n        with open(file) as data_file:    \n            data = json.load(data_file)  \n\n        df = json_normalize(data)\n        mag = df['documentSentiment.magnitude'].values[0]\n        score = df['documentSentiment.score'].values[0]\n        return pd.Series([mag,score],index=['mag','score']) \n    else:\n        print(f'{file} does not exist')\n        return pd.Series([0,0],index=['mag','score'])\n    \ndf_train[['SentMagnitude', 'SentScore']] = df_train[['PetID']].apply(lambda x: readFile(x), axis=1)\ndf_test[['SentMagnitude', 'SentScore']] = df_test[['PetID']].apply(lambda x: readTestFile(x), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfe647ab2c81db07f7c140eabf30f7bc0ad19462"},"cell_type":"code","source":"# Not needed, as there's no overlap between RescuerID in train set and test set\n#lbl_enc = LabelEncoder()\n#lbl_enc.fit(df_train.RescuerID.values.tolist() + df_test.RescuerID.values.tolist())\n#df_train.RescuerID = lbl_enc.transform(df_train.RescuerID.values)\n#df_test.RescuerID = lbl_enc.transform(df_test.RescuerID.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dd809f2708ec4ba2841347455a840869c16c74a"},"cell_type":"markdown","source":"Features"},{"metadata":{"_uuid":"ef9d1034c3b0982f61f03297e52a9f9421d7d1a2"},"cell_type":"markdown","source":"Columns"},{"metadata":{"trusted":true,"_uuid":"cca58b085236520b0d5fe98b4c0af4b504b4051e"},"cell_type":"code","source":"y = df_train.AdoptionSpeed\ntrain = np.hstack((df_train.drop(['Name', 'Description', 'PetID', 'AdoptionSpeed', 'RescuerID'], axis=1).values, X))\ntest = np.hstack((df_test.drop(['Name', 'Description', 'PetID', 'RescuerID'], axis=1).values, X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3e0e98ad004b112532fd1061f5e4182dc4f4dcb"},"cell_type":"code","source":"df_train.columns.ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a706df386f28c43ec9a8b9307088a0fe0f37c4c1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d699f0a4af34eb16b1dd272ff64e7f1bc52d84f4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35d7860a69d0c362c9aa0710bc2cf8e44246f308"},"cell_type":"code","source":"target = df_train['AdoptionSpeed']\ntrain_id = df_train['PetID']\ntest_id = df_test['PetID']\ndf_train.drop(['Name', 'Description', 'PetID', 'AdoptionSpeed', 'RescuerID'], axis=1, inplace=True, errors='ignore')\ndf_test.drop(['Name', 'Description', 'PetID', 'RescuerID'], axis=1, inplace=True, errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"8f2e4c9d6c355555b96d979cb71d9b9ea37521c1"},"cell_type":"code","source":"def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n    kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], 5))\n    all_coefficients = np.zeros((5, 4))\n    feature_importance_df = pd.DataFrame()\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '/5')\n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        all_coefficients[i-1, :] = coefficients\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            qwk_scores.append(qwk)\n            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] = train.columns.values\n        fold_importance_df['importance'] = importances\n        fold_importance_df['fold'] = i\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n        i += 1\n    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n    pred_full_test = pred_full_test / 5.0\n    results = {'label': label,\n               'train': pred_train, 'test': pred_full_test,\n                'cv': cv_scores, 'qwk': qwk_scores,\n               'importance': feature_importance_df,\n               'coefficients': all_coefficients}\n    return results\n\nparams = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 80,\n          'max_depth': 9,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.85,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.01,\n          'min_child_samples': 150,\n          'min_child_weight': 0.1,\n          'verbosity': -1,\n          'data_random_seed': 3,\n          'early_stop': 100,\n          'verbose_eval': 100,\n          'num_rounds': 10000}\n\ndef runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n    print('Prep LGB')\n    d_train = lgb.Dataset(train_X, label=train_y)\n    d_valid = lgb.Dataset(test_X, label=test_y)\n    watchlist = [d_train, d_valid]\n    print('Train LGB')\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    print('Predict 1/2')\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    optR = OptimizedRounder()\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    print(\"Valid Counts = \", Counter(test_y))\n    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2/2')\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk\n\nresults = run_cv_model(df_train, df_test, target, runLGB, params, rmse, 'lgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a716c906b85ee9a56a486dec0affc593950fa796"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc3824fce5b1f29f5ce5f7e19e84df8a57fb4ba9","scrolled":false},"cell_type":"code","source":"imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\nimports.sort_values('importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93e1d7f673c4232051c2e2a3b84027c06065f60b"},"cell_type":"markdown","source":"Surprisingly, model considers of least importance the fact that the pet has ben dewormed, spayed / neutered, vaccinated. Gender is also not an important factor in this model. On the other hand, breed, number of photos, location, age and amount of pets to be adopted at once are the most important factors.\n\nI still want to explore many things:\n1. Reduce the amount of components in the SVD model\n2. Play with images. I will try to share with you a kernel where I validate the following hypothesis: visually similar pets / images have similar adoption speed\n3. Compare more single models: CATboost (of course :P), XGBoost, RF, etc\n4. ..."},{"metadata":{"_uuid":"7beb68fbf85270231678d820498ef1e66534f173"},"cell_type":"markdown","source":"Now, let's adjust the coefficients to optimize the Quadratic Weighted Kappa"},{"metadata":{"trusted":true,"_uuid":"a7fc6319de3b35ba7386cb8d2ffd4414840805b8"},"cell_type":"code","source":"optR = OptimizedRounder()\ncoefficients_ = np.mean(results['coefficients'], axis=0)\nprint(coefficients_)\ntrain_predictions = [r[0] for r in results['train']]\ntrain_predictions = optR.predict(train_predictions, coefficients_).astype(int)\nCounter(train_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03f0b8e186227ac23f010efe83c6704a2b04423d"},"cell_type":"code","source":"optR = OptimizedRounder()\ntest_predictions = [r[0] for r in results['test']]\ntest_predictions = optR.predict(test_predictions, coefficients_).astype(int)\nCounter(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fb2104d1fa41ab72a20cd287f8100320419452e"},"cell_type":"code","source":"pd.DataFrame(sk_cmatrix(target, train_predictions), index=list(range(5)), columns=list(range(5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5316cbba76b3050238eead4617d80fe1c85d364"},"cell_type":"code","source":"quadratic_weighted_kappa(target, train_predictions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"397329a63a55215d15abb83c6bc4828a5200f2e1"},"cell_type":"code","source":"rmse(target, [r[0] for r in results['train']])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d91a7135d42d78a767052a34121acd5cf6d4c9c8"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"761e16133f975d4b1c15d0e32094b62f86c48d14"},"cell_type":"code","source":"submission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_predictions})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbe3abd94c3ea0e3d7dfaba8bcca5a574994e3c6"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de9a9da3c64f67ab5b38d94e3afa8baaf5d187b0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}