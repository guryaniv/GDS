{"nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "code", "source": ["%reload_ext autoreload\n", "%autoreload 2\n", "%matplotlib inline\n", "\n", "import os\n", "import re\n", "import string\n", "import pandas as pd\n", "import numpy as np\n", "import sklearn\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction import stop_words\n", "from sklearn.metrics import log_loss, make_scorer\n", "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n", "\n", "from sklearn.grid_search import GridSearchCV\n", "from sklearn.grid_search import RandomizedSearchCV\n"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["train_file = \"../input/train.csv\"\n", "test_file = \"../input/test.csv\""], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["test = pd.read_csv(test_file)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["train = pd.read_csv(train_file)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["train.head(5)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["train['is_duplicate'].value_counts()"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["train[train['is_duplicate']==1].head(5)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["train.dropna(inplace = True)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["train.shape"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["def tokenize(text):\n", "    \"\"\"\n", "    Given a string, return a list of words normalized as follows.\n", "    Split the string to make words first by using regex compile() function\n", "    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those\n", "    char with a space character.\n", "    Split on space to get word list.\n", "    Ignore words < 3 char long.\n", "    Lowercase all words\n", "    Remove English stop words\n", "    \"\"\"\n", "    re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\n", "    regex = re.compile(re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n", "    \n", "    \n", "    regex = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])'+re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n", "    words = regex.sub(r' \\1 ', text).split()\n", "    words = [w.lower() for w in words]\n", "    words = [w.strip() for w in words]\n", "    words = [w for w in words if len(w) > 2]  # ignore a, an, to, at, be, ...\n", "    words = [w for w in words if w not in stop_words.ENGLISH_STOP_WORDS]\n", "    return words\n"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["veczr = CountVectorizer(lowercase = True, analyzer= 'word', stop_words='english', min_df = 0.0001, max_features= 3000)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["```min_df  = 0.0001``` - if the word occurs less than in 0.01% of total documents, ignore it"], "metadata": {}}, {"cell_type": "code", "source": ["voc = veczr.fit(pd.concat([train['question1'],train['question2']])) # Learn a vocabulary dictionary of all tokens in both questions "], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["q1 = voc.transform(train['question1']) # Transform documents (question 1) to document-term matrix \n", "q2 = voc.transform(train['question2']) # Same for question 2"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["from scipy.sparse import hstack\n", "\n", "X = hstack((q1,q2)) # stacking together two matrices\n", "X.shape\n", "# Convert from coo to csr format\n", "X = X.toarray()"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["y = train['is_duplicate']\n", "y.shape"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["x_train,x_val,y_train, y_val= train_test_split(X,train['is_duplicate'], test_size = 0.2, random_state =42)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["print(x_train.shape)\n", "print(y_train.shape)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["Let's just check how it works.\n", "```.fit``` of CountVectorizer returned us a vocubulary of words which it \"learned\" from question1 and question2.\n", "\n", "```.transform``` transformed questions into doc-term matrix, using the vocabulary.\n", "\n", "Let's see some words contained in the vocabulary:"], "metadata": {}}, {"cell_type": "code", "source": ["vocab = veczr.get_feature_names(); vocab[800:900]"], "outputs": [], "execution_count": null, "metadata": {"scrolled": true}}, {"cell_type": "markdown", "source": ["Now, let's take a look at question 1, say, at a position 90 in the intitial data frame."], "metadata": {}}, {"cell_type": "code", "source": ["train.loc[390,'question1'].split()"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["1. Okay, let's find the position of a word \"wedding\" in our vocabulary..."], "metadata": {}}, {"cell_type": "code", "source": ["veczr.vocabulary_['wedding'] # find the position of the word 'wedding'"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["Finanlly, let's see how many times the word \"wedding\" occurs in question 390? (Should be twice)"], "metadata": {}}, {"cell_type": "code", "source": ["X[390,2931] # Yep!"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["Now, let's train the Logistic Regression"], "metadata": {}}, {"cell_type": "markdown", "source": ["The competition uses the following evaluation metric:\n", "log-loss = (-1/N)sum(yi log(pi) + (1-yi) log(1-pi))"], "metadata": {}}, {"cell_type": "code", "source": ["mlr = LogisticRegression(C = 0.1, dual = True, n_jobs = -1)\n", "mlr.fit(x_train,y_train)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["# make predictions\n", "pred_train = mlr.predict(x_train)\n", "pred_prob_train = mlr.predict_proba(x_train)\n", "pred_val = mlr.predict(x_val)\n", "pred_prob_val = mlr.predict_proba(x_val)\n", "print(\"Accuracy of training:\",(pred_train.T == y_train).mean())\n", "print(\"log-loss of training is:\", log_loss(y_train,pred_prob_train))\n", "print(\"Accuracy of validation:\",(pred_val.T == y_val).mean())\n", "print(\"log-loss of validation is:\", log_loss(y_val,pred_prob_val))"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["# use binarized version\n", "mlr = LogisticRegression(C = 0.1, dual = True, n_jobs = -1)\n", "mlr.fit(x_train.sign(),y_train)\n", "pred_train = mlr.predict(x_train.sign())\n", "pred_prob_train = mlr.predict_proba(x_train.sign())\n", "pred_val = mlr.predict(x_val.sign())\n", "pred_prob_val = mlr.predict_proba(x_val.sign())\n", "print(\"Accuracy of training:\",(pred_train.T == y_train).mean())\n", "print(\"log-loss of training is:\", log_loss(y_train,pred_prob_train))\n", "print(\"Accuracy of validation:\",(pred_val.T == y_val).mean())\n", "print(\"log-loss of validation is:\", log_loss(y_val,pred_prob_val))"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["Now, instead of single words, let's use bigrams and trigrams as tokens"], "metadata": {}}, {"cell_type": "code", "source": ["veczr1 = CountVectorizer(lowercase = True, ngram_range= (1,3), stop_words='english', min_df = 0.0001, max_features= 3000)\n", "voc1 = veczr1.fit(pd.concat([train['question1'],train['question2']])) # Learn a vocabulary dictionary of all tokens in both questions "], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["q1 = voc1.transform(train['question1']) # Transform documents (question 1) to document-term matrix \n", "q2 = voc1.transform(train['question2']) # Same for question 2"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["X1 = hstack([q1,q2]) # statcking together two matrices\n", "X1.shape"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["x1_train,x1_val,y1_train, y1_val= train_test_split(X1,train['is_duplicate'], test_size = 0.2, random_state =42)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["# use binarized version\n", "mlr1 = LogisticRegression(C = 0.1, dual = True, n_jobs = -1)\n", "mlr1.fit(x1_train.sign(),y1_train)\n", "pred_train = mlr.predict(x1_train.sign())\n", "pred_prob_train = mlr.predict_proba(x1_train.sign())\n", "pred_val = mlr.predict(x1_val.sign())\n", "pred_prob_val = mlr.predict_proba(x1_val.sign())\n", "print(\"Accuracy of training:\",(pred_train.T == y1_train).mean())\n", "print(\"log-loss of training is:\", log_loss(y1_train,pred_prob_train))\n", "print(\"Accuracy of validation:\",(pred_val.T == y1_val).mean())\n", "print(\"log-loss of validation is:\", log_loss(y1_val,pred_prob_val))"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["## Random Forest"], "metadata": {}}, {"cell_type": "code", "source": ["mrf = RandomForestClassifier(n_estimators = 50, min_samples_leaf = 3, max_features = 0.5, n_jobs = -1, max_depth = 20, random_state = 42, oob_score = True)\n", "mrf.fit(x_train,y_train)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["def print_score(m):\n", "    res = [log_loss(y_train,m.predict_proba(x_train)), log_loss(y_val,m.predict_proba(x_val)),\n", "                m.score(x_train, y_train), m.score(x_val, y_val)]\n", "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n", "    print(res)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["### RF parameters tuning"], "metadata": {}}, {"cell_type": "markdown", "source": ["There are the following hyperparameters we can tune:\n", "\n", "- **n_estimators** - the number of decision trees in a forest\n", "- **max_depth** - the depth of a tree\n", "- **min_samples_leaf** - the minimum sample in the leaf. The node with less than this limit is not allowed to split\n", "- **max_features** - a fraction (from 0 to 1) of features to be considered in a tree"], "metadata": {}}, {"cell_type": "code", "source": ["log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["param_grid = {#\"n_estimators\": np.arange(25, 100, 25,dtype=int)}\n", "              \"max_depth\": np.arange(45, 105, 10)} \n", "              #\"min_samples_split\": np.arange(1,150,1),\n", "              #\"min_samples_leaf\": [10,50,100]}\n", "              #\"max_leaf_nodes\": np.arange(2,60,6),\n", "              #\"min_weight_fraction_leaf\": np.arange(0.1,0.4, 0.1)}"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["m = RandomForestClassifier(random_state=42, n_estimators = 100, n_jobs = -1, oob_score=True)\n", "m.fit(x_train, y_train)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["random_cv = RandomizedSearchCV(m, param_distributions = param_grid, cv = 3, scoring=log_loss_scorer)\n", "random_cv.fit(x_train, y_train)\n", "\n", "print(random_cv.best_score_)\n", "print(random_cv.best_params_)\n", "print(random_cv.best_estimator_)    "], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": [], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["from sklearn.model_selection import GridSearchCV\n", "\n", "grid_search = GridSearchCV(m, param_grid=param_grid, scoring = 'accuracy')\n", "grid_search.fit(x_train, y_train)\n"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["print(grid_search.cv_results_)\n", "result = grid_search.cv_results_"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["grid_search.n_splits_"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["md_score = result['mean_test_score']\n", "md = range(1,50,5)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["import matplotlib.pyplot as plt\n", "plt.plot(md,md_score)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}}], "metadata": {"language_info": {"name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}}