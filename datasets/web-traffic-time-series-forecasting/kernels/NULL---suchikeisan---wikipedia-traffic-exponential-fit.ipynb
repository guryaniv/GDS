{"cells":[{"metadata":{"_cell_guid":"9cd77c2d-4814-4f62-b50c-e3af1df1b3c3","_execution_state":"idle","_uuid":"8cecc98229a15a09f8ceb92cf78fe2fb60a74396"},"cell_type":"markdown","source":"This notebook reads the file of wikipedia traffic, trains model with it, makes prediction, and write the prediction to a file."},{"metadata":{"trusted":true,"_uuid":"9f0c75c4ac6cccdbffe25bb47abc48d8f7d93435"},"cell_type":"code","source":"import sys\nimport time\nimport random\nfrom scipy import optimize\nimport statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"407d908f-67b4-4353-80c6-d58bd8705b08","_execution_state":"idle","_uuid":"49478a39eb4fccacd8d1b1c50176221e5fee9e11","trusted":true},"cell_type":"code","source":"import datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d003a83e2fee3967bf74d3259cab94d59dd99d7"},"cell_type":"code","source":"# %matplotlib notebook\n# %matplotlib inline\nis_debug = True\n\n#I will run this notebook tonight on condition that this is True.\nis_night_run = False\n\nimport scipy\nscipy.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f02961bc0a27017e5dd39f3c3248c2e8375524e9"},"cell_type":"markdown","source":"# Analysis tool"},{"metadata":{"trusted":true,"_uuid":"204dab63a5d274500bd668b103d3a53686d387bc"},"cell_type":"code","source":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 18, 6\n\nclass Plotter(object):\n    def __init__(self):\n        self.xs = [i for i in range(2000)]\n    \n    def plot(\n            self, array1, low_x_array1=0, array2=None, low_x_array2=0,\n            yscale='linear'):\n        plt.figure(figsize=(18, 6))\n        plt.yscale(yscale)\n        plt.plot(\n            self.xs[low_x_array1:low_x_array1 + len(array1)],\n            array1, label='array1')\n        if array2 is not None:\n            plt.plot(\n                self.xs[low_x_array2:low_x_array2 + len(array2)], array2,\n                label='array2')\n#         plt.title(train_row[0])\n        plt.xlabel('Days')\n        plt.ylabel('Views')\n        plt.legend()\n\nplotter = Plotter()\n\ndef calc_correlation(y1, y2, lag):\n    n_elements = min(len(y1), len(y2))\n    norm_diff1 = (y1 - y1.mean()) / y1.std()\n    norm_diff2 = (y2 - y2.mean()) / y2.std()\n    return (norm_diff1[lag:n_elements] * norm_diff2[:n_elements-lag]).mean()\n\ndef calc_autocorrelation(y, lag):\n    return calc_correlation(y, y, lag)\n\nx = np.arange(0,10000,0.1)\ny = np.sin(x)\nlag = 3\nprint(sm.tsa.acf(y,lag))\nprint(calc_autocorrelation(y, lag))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8186d6f-bb92-4934-b375-efb67a174e81","_uuid":"0c9ed3d0b9f90689d08afcee82d20fab85d14baf"},"cell_type":"markdown","source":"# Reading data files"},{"metadata":{"_cell_guid":"83d4d67a-0968-4eff-9f5c-193b8d6cb11e","_execution_state":"idle","_uuid":"5049b9e529203fe671946d205b0959a75768a0b4","trusted":true},"cell_type":"code","source":"data_dir = r'../input/'\ntrain = pd.read_csv(\n    data_dir + r'train_2.csv',\n    skiprows=(lambda x: x % 100 != 0) if False else None\n).fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"scrolled":true,"trusted":true,"_uuid":"52461696df89ec85f0d9be25e085eab69701e5cf"},"cell_type":"code","source":"page_id_etc = pd.read_csv(data_dir + r'key_2.csv').fillna(0)\nprint(page_id_etc.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39a676eb5576c4e2858b194999a1531e9b34f680"},"cell_type":"code","source":"for i in range(65):\n    print(page_id_etc.iloc[i])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e49e4cfa-71e8-4658-bda5-4d6709ebf5f7","_execution_state":"idle","_uuid":"cfd7c899bc38c4b866bae07f7e5da0d052032d3e","trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"037f140c0b40b34710448aa6f80a253fff97a0fa"},"cell_type":"markdown","source":"# Evaluation Value\nThe evaluation is determined by SMAPE. The function to calculate it is following. (cf. https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/37232)"},{"metadata":{"trusted":true,"_uuid":"c1f6a892f1e2e6e9b212025d3c6211b6bbeec4ad"},"cell_type":"code","source":"import math\n\ndef smape_fast(y_true, y_pred):\n    out = 0.0\n    for i in range(y_true.shape[0]):\n        a = y_true[i]\n        b = y_pred[i]\n        c = a+b\n        if -1 < c < 1:\n            continue\n        out += math.fabs(a - b) / c\n    out *= (200.0 / y_true.shape[0])\n    return out","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"893c3fd3204e03d04319a6f3e4ccafaf442da031"},"cell_type":"markdown","source":"# Preparing for training and prediction"},{"metadata":{"_execution_state":"idle","_uuid":"55f665c31f11dc8103bae16014b6ab9e8144a121","code_folding":[],"scrolled":true,"trusted":true},"cell_type":"code","source":"def calc_median_of_day_of_week(data_all_days):\n    \"\"\"\n    Following equations are assumed:\n        data_all_days = rolling_week * week_dependency + residue\n        week_dependency[i] = median_of_day_of_week[i%7]\n    This function returns median_of_day_of_week.\n    \"\"\"\n    pds_original = pd.Series(data_all_days)\n    rolling_week = pds_original.rolling(\n        7, min_periods=1, center=True).mean()\n#     if is_debug:\n#         plotter.plot(\n#             pds_original[3:-3], 1000, rolling_week[3:-3], 1000, yscale='log')\n#         plt.show()\n\n    # +0.1 is for zero-division\n    original_over_rolling_week = pds_original / (rolling_week + 0.1)\n    number_of_days_of_week = len(data_all_days) // 7 + 1\n    data_each_day = []  # np.zeros((7, number_of_days_of_week))\n    for i in range(7):\n        data_each_day.append(original_over_rolling_week.iloc[i::7])\n#     print(data_each_day)\n    median_of_day_of_week = np.zeros(7)\n    for i in range(7):\n        median_of_day_of_week[i] = data_each_day[i].median()\n    return median_of_day_of_week\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"55f665c31f11dc8103bae16014b6ab9e8144a121","code_folding":[],"scrolled":true,"trusted":true},"cell_type":"code","source":"import scipy\n\ndef gaussian_like(x_data, y_scale, mu, width, exponent_x, bias):\n    x_mod = (x_data - mu) / width\n    return bias + y_scale * np.exp(-np.abs(x_mod)**exponent_x)\n\ndef calc_residuals(params, x_data, y_data):\n    y_model = gaussian_like(x_data, *params)\n    return (y_model - y_data)**2\n\ndef reduce_week_dependency(actual_train_data, mean_days1=11, mean_days2=3):\n    \"\"\"\n    @param actual_train_data         np.array\n    @param mean_days1\n    @param mean_days2\n    @return without_week_dependency  pd.Series\n    @return median_of_day_of_week    np.array or None\n                                     None means low weekly dependence.\n    \"\"\"\n\n    median_of_day_of_week = calc_median_of_day_of_week(actual_train_data)\n    if is_debug:\n        print('median of weekday', median_of_day_of_week)\n        original_data = actual_train_data.copy()\n    without_week_dependency = actual_train_data\n    is_week_dependent = (\n        np.count_nonzero(median_of_day_of_week) == len(median_of_day_of_week))\n    if is_week_dependent:\n        for i in range(len(without_week_dependency)):\n            without_week_dependency[i] /= median_of_day_of_week[i % 7]\n    else:\n        median_of_day_of_week = None\n    \n#     print(without_week_dependency)\n    rolling_median = pd.Series(without_week_dependency).rolling(\n        3, min_periods=1, center=True).median()\n    \n    rolling_mean = rolling_median.mean()\n#     print('mean_days1', mean_days1, '   rolling_mean', rolling_mean)\n#     if rolling_mean > 1e4:#1e4:\n#         number_of_mean_days = 3\n#     elif rolling_mean > 2e3: #2000:\n#         number_of_mean_days = 7\n#     else:\n#         number_of_mean_days = 14\n    number_of_mean_days = int(\n       np.floor(1000 * mean_days1 / (rolling_mean + 0.1) + mean_days2))\n#     if not is_night_run:\n#         print('number_of_mean_days', number_of_mean_days)\n    \n    rolling_median = rolling_median.rolling(\n        number_of_mean_days, min_periods=1, center=True).mean()\n#     plotter.plot(\n#         without_week_dependency, 1000, rolling_median.values, 1000,\n#         yscale='log')\n#     plt.show()\n    without_week_dependency = rolling_median\n    \n    if is_debug:\n        print('actual_train_data, without_week_dependency')\n        plotter.plot(original_data, 1000, without_week_dependency, 1000,\n                     yscale='log')\n        for i in range(1000, 1000 + len(without_week_dependency), 7):\n            plt.axvline(x=i, color='red',alpha=0.3)\n        plt.show()\n    \n    return without_week_dependency, median_of_day_of_week\n\nclass IncreasingTrend(RuntimeError):\n    pass\n\ndef train_and_pred_gaussian_like(\n        without_week_dependency, median_of_day_of_week,\n        low_idx_pred, up_idx_pred, page, min_bias=0.):\n    \"\"\"\n    without_week_dependency[0] is data of 1st day,\n    without_week_dependency[1] is data of 2nd day, ....\n    This predict (low_idx_pred+1)th day, (low_idx_pred+2)th day,\n    ..., (up_idx_pred)th day.\n\n    @param without_week_dependency   pd.Series\n    @param median_of_day_of_week     np.array\n    @param low_idx_pred              int\n    @param up_idx_pred               int\n    @param page                      str\n    @param min_bias                  int or float\n    @return predicted value          np.array\n    @return low_idx_fit              int\n    \"\"\"\n\n    # +1 is for 0-division avoiding. e.g. norm = 1e+5 + 1\n    norm = without_week_dependency.max() + 1\n#     norm = np.median(without_week_dependency) + 1\n\n    pred_data = np.zeros(up_idx_pred - low_idx_pred)\n\n    normalized_train_data = without_week_dependency / norm\n    normalized_min_bias = min_bias / norm\n\n    # Training\n\n    min_fit_points = 6\n    low_idx_fit = 3 #len(actual_train_data) - 10\n    sigmas = np.sqrt(without_week_dependency)\n    diff_state = 0\n    mean_1day = without_week_dependency.iloc[-1]\n    for i in reversed(without_week_dependency.index[3:]):\n#         print('without_week_dependency', i, without_week_dependency[i])\n        if without_week_dependency[i - 1] \\\n                > mean_1day + sigmas[i]:\n#             if diff_state == -1:\n#                 low_idx_fit = min(i, len(actual_train_data) - min_fit_points)\n#                 break\n            diff_state = 1\n            mean_1day = without_week_dependency[i - 1]\n        elif without_week_dependency[i - 1] \\\n                < mean_1day - sigmas[i]:\n            if diff_state == 1:\n                low_idx_fit = min(i, len(actual_train_data) - min_fit_points)\n                break\n#             diff_state = -1\n            if is_debug:\n                print('diff < 0')\n            raise IncreasingTrend()\n    if is_debug:\n        print('norm', norm, '  lower index for fit:', low_idx_fit)\n    \n    fitting_data = normalized_train_data.iloc[low_idx_fit:]\n    init_mu = fitting_data.idxmax()\n    init_y_scale = fitting_data[init_mu]\n    init_param = (init_y_scale, init_mu, 1.0, 0.5, normalized_min_bias)\n    \n    x_data = fitting_data.index #np.arange(low_idx_fit, len(actual_train_data))\n    \n    # (min, max) of [y_scale, mu, width, exponent_x, bias]\n    bounds=([0., x_data[0] - 1000., 1e-10, 0., normalized_min_bias],   \n            [1e20, x_data[0] + 1000., 10000., 10.,1e20])\n#     if is_debug:\n#         print('x:', x_data, 'fitting_data:\\n', fitting_data,\n#               'min_bias', normalized_min_bias,\n#               'p0', init_param)\n#         plotter.plot(fitting_data, 1000+low_idx_fit)\n#     popt, pcov = scipy.optimize.curve_fit(\n#         gaussian_like, x_data, fitting_data,\n#         bounds=bounds,\n#         p0=init_param, method='trf')\n\n    optimize_result = optimize.least_squares(\n        calc_residuals, init_param, bounds=bounds, args=(x_data, fitting_data))\n    if is_debug:\n        print('opt param', optimize_result.x)\n    popt = optimize_result.x\n    normalized_pred = gaussian_like(x_data, *popt)\n    \n    if is_debug:\n        plotter.plot(\n            fitting_data, 1000+low_idx_fit, \n            normalized_pred, 1000+low_idx_fit)\n        plt.title('data and pred')\n        plt.show()\n\n#   Prediction\n    if median_of_day_of_week is not None:\n        for i in range(low_idx_pred, up_idx_pred):\n            pred_data[i - low_idx_pred] = norm * gaussian_like(\n                i, *popt) * median_of_day_of_week[i % 7]\n    else:\n        for i in range(low_idx_pred, up_idx_pred):\n            pred_data[i - low_idx_pred] = norm * gaussian_like(i, *popt)\n    \n#     plotter.plot(\n#         actual_train_data[low_idx_fit:], 1000+low_idx_fit, \n#         pred_data[low_idx_fit:len(actual_train_data)], 1000+low_idx_fit)\n    \n    return pred_data, low_idx_fit\n\n\n# test\nx_data = np.array([2, 3, 4, 5, 6])\ny_model = gaussian_like(x_data, 10, 3, 1, 0.5, 100)\n# print(y_model)\n\nactual_train_data = pd.Series(\n    [200-x**2 for x in range(-5, 10)],\n    index=pd.date_range('2011/01/01', periods=15, freq='D'))\nprint('Test train data\\n', actual_train_data)\nwithout_week_dependency, median_of_day_of_week = reduce_week_dependency(\n    actual_train_data.values)\nprint('without_week_dependency', without_week_dependency)\npred, idx = train_and_pred_gaussian_like(\n    without_week_dependency, median_of_day_of_week, 2, 17, 'Test', 0.1)\nplotter.plot(actual_train_data, 1000, pred, 1000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a5385d0a34ba81080486a601c57627de9b6497b"},"cell_type":"code","source":"date_char_counts = len(\"YYYY-MM-DD\")\npage_id_etc['Date'] = [\n    page[-date_char_counts:] for page in tqdm(page_id_etc.Page)]\npage_id_etc['Page'] = [\n    page[:-date_char_counts-1] for page in tqdm(page_id_etc.Page)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"421bb6bf6f35e5f1ce91476506f37ea30d7e623c"},"cell_type":"code","source":"submit_work = page_id_etc.copy()\nsubmit_work['Visits'] = np.NaN\nprint(submit_work.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d60ecd8d3ea8f5ace9afe5fb9e35ff4bbd60fd60"},"cell_type":"code","source":"submit_work = submit_work.pivot(\n    index='Page', columns='Date', values='Visits'\n).astype('float32').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d4d8fb002f95b296382f6477e823cfda588e4074"},"cell_type":"code","source":"print(submit_work.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c15d9f9324d107b95d6fefed6103f17e36334ab1"},"cell_type":"code","source":"def parse_iso_format(iso_format):\n    \"\"\"\n    eg. If iso_format=='2017-09-10' then this return (2017, 9, 10) \n    \"\"\"\n    return tuple(int(x) for x in iso_format.split('-'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7acc98698eaffeb32f558d997d1971156e388db4"},"cell_type":"code","source":"# date1 = datetime.date(*parse_iso_format('2017-09-10'))\n# date2 = datetime.date(*parse_iso_format('2017-09-13'))\n# (date2 - date1).days\n\ntrain_file_first_day = datetime.date(*parse_iso_format(train.columns[1]))\nsubmission_last_day = datetime.date(*parse_iso_format(submit_work.columns[-1]))\nup_idx_submission = (submission_last_day - train_file_first_day).days + 1\n\nprint(up_idx_submission)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"8b9df370863ba003d010a9cb4cfd2b7718169ecc"},"cell_type":"code","source":"submit_work = submit_work.iloc[:, 1:]\ncolumns_name = submit_work.columns.name\nsubmit_work = train[['Page']].join(submit_work)\nsubmit_work.columns.name = columns_name\nprint(submit_work.head(2))\nsubmit_work.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac55be2e83fa4b673e0dc3ee3bd15ec004ed2f8d"},"cell_type":"markdown","source":"# Training and prediction"},{"metadata":{"code_folding":[],"trusted":true,"_uuid":"591bcebee70ee2a7e7e583e924f69ca15de9dd2d"},"cell_type":"code","source":"if train.columns[-1] == 'lang':\n    date_end = len(train.columns) - 1\nelse:\n    date_end = len(train.columns)\n    \ncols = train.columns[1:date_end]\nlow_idx_submission = up_idx_submission - (submit_work.shape[1] - 1)\n\n# training days\nnumber_of_actual_train = 90\nsmape_days = 21 #up_idx_train - (low_idx_train + low_idx_fit)  #21\nif is_night_run:  \n#     low_idx_train = (date_end - 1) - number_of_actual_train\n    original_up_idx_train = date_end - 1\n#     low_idx_pred = original_up_idx_train - smape_days\n    up_idx_pred = up_idx_submission\nelse:\n#     low_idx_train = 740 - number_of_actual_train\n    original_up_idx_train = 740\n#     low_idx_pred = low_idx_train + 10\n    up_idx_pred = up_idx_submission\n\ntrain_row_count = train.shape[0]\n\ndef train_and_predict(\n        hyper_params, train_row_start, row_step, is_variable_up_idx):\n    smape_sum = 0.0\n    smape_count = 0\n    if is_variable_up_idx or (not is_night_run):\n        low_idx_train = 740 - number_of_actual_train\n        low_idx_pred = low_idx_train + 10\n    else:\n        low_idx_train = (date_end - 1) - number_of_actual_train\n        low_idx_pred = original_up_idx_train - smape_days\n#     print('hyper_params', hyper_params)\n    for row_num in tqdm(range(train_row_start, train_row_count, row_step)):\n#     for row_num in [7001, 70002, 70003, 100001]:\n        if is_variable_up_idx or (not is_night_run):\n            up_idx_train = 740 - 50 + ((row_num // 1000) % 80)\n    #         up_idx_train = date_end - 1\n#             print('up_idx_train', up_idx_train)\n        else:\n            up_idx_train = original_up_idx_train\n\n        train_row = train.iloc[row_num, :]\n    #     print(train_row[0], train_row[-3:])\n    #     data_row = np.array(train_row[1:date_end],'f')\n    #     actual_train_data = data_row[low_idx_train:up_idx_train]\n\n        data_row = train_row[1:date_end].astype('float64')\n        data_row.index = pd.to_datetime(data_row.index)\n        try:\n            first_nonzero = data_row.nonzero()[0][0]\n    #         idx_min_data = data_row[\n    #             max(first_nonzero, up_idx_train - 400):up_idx_train].values.argmin()\n            col_for_min_bias = data_row[\n                max(first_nonzero, up_idx_train - 400):up_idx_train].idxmin()\n            idx_min_data = data_row[:col_for_min_bias].shape[0] - 1\n            if is_debug:\n                print('idx_min_data', idx_min_data,\n    #                   '  data_row[col_for_min_bias]', data_row[col_for_min_bias],\n    #                   '  data_row[:col_for_min_bias].shape', data_row[:col_for_min_bias].shape,\n    #                   '  data_row[idx_min_data-2:idx_min_data+2]', data_row.iloc[idx_min_data-2:idx_min_data+2],\n    #                   '  col_for_min_bias', col_for_min_bias,\n                      '  min data', data_row.iloc[idx_min_data])\n        except:\n            if is_debug:\n                print('idx_min is not found.')\n            idx_min_data = None\n\n    #     print(\n    #         '1st nonzero index:', first_nonzero, '   min_data_row:', min_data_row)\n#         print('low_idx_train', low_idx_train)\n        actual_train_data = data_row.iloc[low_idx_train:up_idx_train]\n\n        train_last_part = actual_train_data.iloc[-30:]\n    #     print('train_last_part', train_last_part)\n        train_last_part_median = train_last_part.median()\n        if train_last_part_median < 20.0: #30.0: #10.0:\n            last_mean = train_last_part.mean()\n            if is_debug:\n                print('last_part_median:', train_last_part_median,\n                      '  last_part_mean:', last_mean)\n            visit = last_mean if (\n                last_mean < train_last_part_median + 1.0\n                ) else train_last_part_median\n            pred = np.full((up_idx_pred - low_idx_pred), visit)\n            pred_no_trend = pred_with_trend = pred\n        else:\n    #         if is_debug: continue\n            (without_week_dependency, median_of_day_of_week\n                ) = reduce_week_dependency(\n                actual_train_data.values, *hyper_params)\n            pred_no_trend = np.full(\n                (up_idx_pred - low_idx_pred),\n                without_week_dependency.iloc[-30:].median())\n            if median_of_day_of_week is not None:\n                for i in range(low_idx_pred - low_idx_train, up_idx_pred - low_idx_train):\n                    pred_no_trend[i - low_idx_pred + low_idx_train] \\\n                        *= median_of_day_of_week[i % 7]\n            if idx_min_data is not None:\n                min_data_row = data_row[idx_min_data]\n    #             if median_of_day_of_week is not None:\n    #                  min_data_row /= median_of_day_of_week[\n    #                     (idx_min_data - low_idx_train) % 7]\n            else:\n                min_data_row = 0.0\n\n            rolling_week = data_row[:up_idx_train].rolling(\n                7, min_periods=1, center=True).mean()\n            rolling_drop0229 = rolling_week.drop(labels=pd.to_datetime(['2016-02-29']))\n            ac = calc_autocorrelation(rolling_drop0229.values, 365)\n            if is_debug:\n                print('autocorr:', ac)\n#             print('rolling_drop0229', rolling_drop0229)\n                plotter.plot(rolling_drop0229)\n                plt.show()\n#             train_diff = rolling_drop0229.diff().dropna()\n#             plotter.plot(train_diff[300:])\n#             plt.show()\n            \n    \n            try:\n                if ac > 0.6:\n#                     diff_1year_ago = rolling_drop0229[low_idx_pred - 100:low_idx_pred - 1\n#                                     ].values - rolling_drop0229[low_idx_pred - 100 - 365:low_idx_pred - 1 - 365].values\n#                     diff_1year_ago = np.median(diff_1year_ago)\n#                     if is_debug: print(diff_1year_ago)\n                    pred_with_trend = rolling_drop0229[\n                        low_idx_pred-365:up_idx_pred-365].values\n\n#                     pred_with_trend[0] = rolling_week[low_idx_pred - 1] + rolling_drop0229[low_idx_pred - 365]\n#                     for i in range(1, up_idx_pred - low_idx_pred):\n#                         pred_with_trend[i] = pred_with_trend[i - 1] \\\n#                            + rolling_drop0229[i + low_idx_pred - 1 - 365]\n                    if median_of_day_of_week is not None:\n                        for i in range(low_idx_pred - low_idx_train, up_idx_pred - low_idx_train):\n                            pred_with_trend[i - low_idx_pred + low_idx_train] \\\n                                *= median_of_day_of_week[i % 7]\n                    pred = pred_with_trend\n                else:\n                    pred_with_trend, low_idx_fit = train_and_pred_gaussian_like(\n                        without_week_dependency,\n                        median_of_day_of_week,\n                        low_idx_pred - low_idx_train,\n                        up_idx_pred - low_idx_train, \n                        train_row[0], min_data_row)\n                \n            \n                    smape_no_trend = smape_fast(\n                        data_row[up_idx_train - smape_days:up_idx_train],\n                        pred_no_trend[\n                            up_idx_train - smape_days - low_idx_pred\n                            :up_idx_train - low_idx_pred])\n                    smape_with_trend = smape_fast(\n                        data_row[up_idx_train - smape_days:up_idx_train],\n                        pred_with_trend[\n                            up_idx_train - smape_days - low_idx_pred\n                            :up_idx_train - low_idx_pred])\n                    if is_debug:\n                        print(\n                            'SMAPE no_trend, with_trend:',\n                            smape_no_trend, smape_with_trend)\n                    pred = pred_no_trend if smape_no_trend < smape_with_trend \\\n                        else pred_with_trend\n            except:\n                if is_debug:\n                    print(\"Curve fit failed at row#\", row_num, \":\", sys.exc_info()[0])\n    #             low_idx_fit = len(actual_train_data) - 1\n    #             pred_with_trend = train_and_pred_nn2_week(\n    #                 actual_train_data,\n    #                 low_idx_pred - low_idx_train,\n    #                 up_idx_pred - low_idx_train, \n    #                 train_row[0])\n                pred = pred_with_trend = pred_no_trend        \n        \n#         print('pred', pred[low_idx_submission - low_idx_pred:])\n        submit_work.iloc[row_num, 1:] = pred[\n            low_idx_submission - low_idx_pred:]\n\n        up_idx_smape = min(len(data_row), up_idx_pred)\n        if up_idx_train < up_idx_smape:\n            smape_row = smape_fast(\n                data_row[up_idx_train:up_idx_smape],\n                pred[up_idx_train - low_idx_pred:up_idx_smape - low_idx_pred])\n            smape_sum += smape_row\n            smape_count += 1\n        else:\n            smape_row = None\n\n        if is_debug: #and (not is_night_run):\n    #         plotter.plot(\n    #             data_row[low_idx_train:], low_idx_train, pred, low_idx_pred,\n    #             'linear')\n    #         plt.axvline(x=up_idx_train, color='red',alpha=0.3)\n    #         plt.show()\n            low_idx_plot = 0\n            plotter.plot(\n                data_row[low_idx_plot:], low_idx_plot, \n                pred_no_trend[:data_row.shape[0] - low_idx_pred], low_idx_pred, 'log')\n            plt.axvline(x=up_idx_train, color='red',alpha=0.3)\n            plt.title('pred_no_trend')\n            plt.show()\n    #         plotter.plot(\n    #             data_row[low_idx_plot:], low_idx_plot, \n    #             pred[:data_row.shape[0] - low_idx_pred], low_idx_pred, 'log')\n            plotter.plot(\n                data_row[low_idx_plot:], low_idx_plot, \n                pred, low_idx_pred, 'log')\n            plt.axvline(x=up_idx_train, color='red',alpha=0.3)\n            plt.title('pred')\n            plt.show()\n\n            print(\"SMAPE:\", smape_row, \"Row:\", row_num, train_row[0])\n            print(\"\\n\\n\")\n\n    #     if is_debug: break\n    if smape_count > 0:\n        mean_smape = smape_sum/float(smape_count)\n        print(\"mean SMAPE:\", mean_smape)\n        return mean_smape\n    else:\n        return np.nan","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"trusted":true,"_uuid":"f5b4a108fff90b0856cbbcb4f3dd15d2743fa22f","scrolled":true},"cell_type":"code","source":"#Optimize hyper parameters\nt_start = time.time()\n\noptimized_params = {}\nrow_step_for_optimize = 10000\nmaxiter = 1 if is_debug else 100\nfor i in range(0, row_step_for_optimize, row_step_for_optimize // 5):\n    res = optimize.minimize(\n        train_and_predict, np.array([11, 3]), method='Nelder-Mead',\n        args=(i, row_step_for_optimize, True), options={'maxiter':maxiter, 'xatol':0.1})\n    print('Elapsed time(s):',time.time() - t_start,\n          '  Optimized params:', res.x)\n    optimized_params[i] = res.x\n    if is_debug: break\n\noptimized_params = pd.DataFrame(optimized_params)\nprint('Optimized params')\nprint(optimized_params)\n\nusing_params = optimized_params.T.mean().values\n\nprint('Mean of params:', using_params)","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"scrolled":false,"trusted":true,"_uuid":"f71a65202b89d1a4341c0725612654ab6e7aa283"},"cell_type":"code","source":"# %%capture output\n\nif is_debug: using_params = [11.17037109, 2.93941406]\nprint('Mean of params:', using_params)\n\n#Main part\n\ntrain_row_start = 0\nif is_debug: train_row_start = 1000\nrow_step = 1 if (is_night_run and (not is_debug)) else 10000\nprint('row step:', row_step)\n\nt_start = time.time()\ntrain_and_predict(using_params, train_row_start, row_step, False)  \nprint('Elapsed time(s):', time.time() - t_start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d250518800633218162a433ee8bbfcca42c5002"},"cell_type":"code","source":"# print(type(output))\n# print(submit_work.iloc[train_row_start, :])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96699c798e0ca05e4a5e3d55495ad321396aeb01"},"cell_type":"markdown","source":"# Writing submission file"},{"metadata":{"trusted":true,"_uuid":"d82c34d1e460aa1b594e1f0f0003bc707f2b3158"},"cell_type":"code","source":"submit_save = pd.melt(\n    submit_work, id_vars=['Page'], var_name='Date', value_name='Visits')\nsubmit_save = submit_save.merge(page_id_etc, on=['Page','Date'])\nprint(submit_save.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7c95dfcbb2220dfcd3da78451701a021815c9f7"},"cell_type":"code","source":"submit_visits = submit_save['Visits']\nlarge_visits = submit_save.loc[submit_visits > 10000, 'Visits']\n# print(large_visits.iloc[0:5])\nprint('Max visits:', large_visits.max())\nsubmit_save.loc[np.isinf(submit_visits), 'Visits'] = 1e9\nsubmit_save.loc[submit_visits < 0.0, 'Visits']  = 0.0\nsubmit_save.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95892efe095f47c2d54360950c706f3e02fbc283"},"cell_type":"code","source":"if not is_debug:\n    submit_save[['Id', 'Visits']].to_csv(\n        'submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f0788b6b7a75bc0450f0d6a0126410baf339b85"},"cell_type":"markdown","source":"# Submission data check"},{"metadata":{"trusted":true,"_uuid":"1ae4237e8f2f10d79a0e2f6affb7ff828385d358"},"cell_type":"code","source":"large_visits = submit_save.loc[submit_visits > 10000, 'Visits']\n# print(large_visits.iloc[0:5])\nprint('Max visits:', large_visits.max())\nprint('Number of Ids of large visits:', len(large_visits))\nprint('Numver of Ids of which value < 0:',\n      len(submit_save.loc[submit_visits < 0.0, 'Visits']))\nprint('Numver of Ids of which value is finite:',\n      len(submit_save.loc[np.isfinite(submit_visits), 'Visits']))\nprint('Numver of Ids of which value is NaN:',\n      len(submit_save.loc[np.isnan(submit_save['Visits']), 'Visits']))\n\n# print(submit_save.loc[np.isnan(submit_save['Visits'])])\n\n# inf_rows = submit_work.loc[submit_work[np.isinf(submit_work)]]\ninf_rows = submit_work.loc[np.isinf(submit_work['2017-11-13'])]\nif not inf_rows.empty:\n    print('Infinity rows:', inf_rows[0:5])\n    \nnan_rows = submit_work.loc[np.isnan(submit_work['2017-11-13'])]\nif not nan_rows.empty:\n    print('NaN rows:', nan_rows[0:5])\n\nlarge_rows = []\nfor date in submit_work.columns[1:]:\n    large_rows.append(submit_work.loc[submit_work[date] > 1e12])\n# if not large_rows:\nfor rows in large_rows:\n    if not rows.empty:\n        print('Large rows:', rows.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8452f8d807a2c72c95bd64da8558e4aee671daa"},"cell_type":"code","source":"# submit_save_mod = submit_save.copy()\n# submit_save_mod.loc[np.isnan(submit_visits), 'Visits']  = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"490b6b7e000f12c5670e8c76ba72fed762d738c3"},"cell_type":"code","source":"# if not is_debug:\n#     submit_save_mod[['Id', 'Visits']].to_csv(\n#         data_dir + 'submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be34ea9ab87b32046a2d97ccf609c492bb69c207"},"cell_type":"markdown","source":"# References"},{"metadata":{"_uuid":"c8b304daadb4f65c3d74c8023a314df3acf55995"},"cell_type":"markdown","source":"* https://www.kaggle.com/muonneutrino/wikipedia-traffic-data-exploration/notebook\n* https://github.com/jfpuget/Kaggle/blob/master/WebTrafficPrediction/keras-kf-12-stage2-sept-10.ipynb"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{"height":"197.003px","width":"263.999px"},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"position":{"height":"517.706px","left":"753.615px","right":"20px","top":"128.999px","width":"469.339px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}