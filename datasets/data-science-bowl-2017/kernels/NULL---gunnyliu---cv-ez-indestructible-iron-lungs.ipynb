{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "758145f0-9a92-a150-40bb-adf54096ee0c"
      },
      "source": [
        "Hello World, we love preprocessing and hate lung cancer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "224e45ae-7c02-1154-e67e-e47639e1d4d9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a07f5103-d036-968f-40d7-57649bbb4864"
      },
      "source": [
        "# Preprocessing Agenda:\n",
        "\n",
        "* Averaging every 10 pixels\n",
        "* Decimating\n",
        "* Chunking for the CNN?\n",
        "* Flattening\n",
        "\n",
        "## Original Work: \n",
        "\n",
        "* **Loading the DICOM files**, and adding missing metadata  \n",
        "* **Converting the pixel values to *Hounsfield Units (HU)***, and what tissue these unit values correspond to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1d12eab6-3340-fa57-84a1-91fe13886996"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import dicom\n",
        "import os\n",
        "import scipy.ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from skimage import measure, morphology\n",
        "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
        "\n",
        "# gun dev note: one patient per folder\n",
        "INPUT_FOLDER = '../input/sample_images/'\n",
        "patients = os.listdir(INPUT_FOLDER)\n",
        "patients.sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "56147dd5-3127-8844-644d-0b1565c9dac3"
      },
      "source": [
        "# I/O: Scans to PyDicom Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f50e5b1-c1e8-14a8-591b-eb466e5adc0d"
      },
      "outputs": [],
      "source": [
        "# Load the scans in given folder path\n",
        "def load_scan(path):\n",
        "    # gun dev note: every folder contains multiple slices, which is \"a single scan\"\n",
        "    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n",
        "        \n",
        "    slices.sort(key = lambda x: float(x.ImagePositionPatient[2]))\n",
        "    \n",
        "    # gdn: for capturing hidden slice thickness field; assuming that the slice thickness is uniform across all slices\n",
        "    try:\n",
        "        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n",
        "    except:\n",
        "        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n",
        "        \n",
        "    for s in slices:\n",
        "        s.SliceThickness = slice_thickness\n",
        "        \n",
        "    return slices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79a91d86-1009-ac6d-14d4-a1c117f3dd64"
      },
      "source": [
        "# Houndsfield Units\n",
        "* HU: std measure of radiodensity\n",
        "* From Wikipedia:\n",
        "\n",
        "![HU examples][1]\n",
        "\n",
        "# Convert to proper HU units: \n",
        "* the pixel_array of each slice is encoded in unscaled HU\n",
        "* pixels that fall outside of these bounds get the fixed value -2000\n",
        "* multiplying with the rescale slope \n",
        "* adding the intercept\n",
        "\n",
        "  [1]: http://i.imgur.com/4rlyReh.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e60f402c-0cd9-f0d2-0c3d-3e7af98f9845"
      },
      "outputs": [],
      "source": [
        "def get_pixels_hu(slices):\n",
        "    image = np.stack([s.pixel_array for s in slices])\n",
        "    # Convert to int16 (from sometimes int16), \n",
        "    # should be possible as values should always be low enough (<32k)\n",
        "    image = image.astype(np.int16)\n",
        "\n",
        "    # Set outside-of-scan pixels to 0\n",
        "    # The intercept is usually -1024, so air is approximately 0\n",
        "    image[image == -2000] = 0\n",
        "    \n",
        "    # Convert to Hounsfield units (HU)\n",
        "    for slice_number in range(len(slices)):\n",
        "        \n",
        "        # each slice in slices has lots of \"hidden\" attributes\n",
        "        intercept = slices[slice_number].RescaleIntercept\n",
        "        slope = slices[slice_number].RescaleSlope\n",
        "        \n",
        "        if slope != 1:\n",
        "            image[slice_number] = slope * image[slice_number].astype(np.float64)\n",
        "            image[slice_number] = image[slice_number].astype(np.int16)\n",
        "            \n",
        "        image[slice_number] += np.int16(intercept)\n",
        "    \n",
        "    return np.array(image, dtype=np.int16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9a64cb22-2b5d-425c-7ced-0e977ca04fa7"
      },
      "source": [
        "# Visualization of HU Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b0012bd3-50f6-278b-de58-a2a39361f1bb"
      },
      "outputs": [],
      "source": [
        "# gdn: viz on patient num 0\n",
        "first_patient = load_scan(INPUT_FOLDER + patients[0])\n",
        "\n",
        "# gdn: display entire Dataset. use dir(\"search_term\") function call to find fields that match \n",
        "    # a pydicom Dataset instance is essentially a dict mapping\n",
        "    # large set of hidden fields contained in each Dataset\n",
        "print(len(first_patient))\n",
        "# gdn: don't print the repr unless you wanna see it real time; too many fields\n",
        "print(first_patient)\n",
        "\n",
        "first_patient_pixels = get_pixels_hu(first_patient)\n",
        "# gdn: viz of HU distribution in 0th patient scan\n",
        "    # what if... we run algorithms on this data?\n",
        "plt.hist(first_patient_pixels.flatten(), bins=80, color='c')\n",
        "plt.xlabel(\"Hounsfield Units (HU)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2f199017-59b7-d9a0-bfdc-c2dc7d295c1e"
      },
      "source": [
        "# Visualization of Random Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e07a9313-6bc5-5233-228f-cf336e4ef91f"
      },
      "outputs": [],
      "source": [
        "# 134 slices, 512 x 512 pixels per slice\n",
        "print(first_patient_pixels.shape)\n",
        "num_slices, num_rows, num_cols = first_patient_pixels.shape\n",
        "\n",
        "# Higher HU = whiter; Lower HU = darker\n",
        "# gdn: quick try at taking HU mean across all 134 slices of image\n",
        "plt.imshow(first_patient_pixels.mean(axis=0), cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "# gdn: visualize a random slice \n",
        "idx = np.random.randint(0, num_slices)\n",
        "plt.imshow(first_patient_pixels[idx], cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "\n",
        "# gdn: how can we quantify the viz though? how dark/light versus what HU value??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "894982a5-844c-ebb2-a48c-62a057cf5ec9"
      },
      "source": [
        "# Averaging per 10 Slices\n",
        "* intuitively, averaging across all e.g. 134 slices is not effective\n",
        "* try averaging on 10 slices instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3eda0d6a-19e5-7004-da4c-37f533f1d6c7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "input: a scan processed by get_pixels_hu; ndarray representation\n",
        "output: same scan, same representation, averaged per 10-slices\n",
        "'''\n",
        "def averaging_10_slices(scan):\n",
        "    # gdn: split into chunks of 10 slices each\n",
        "        # outputs a list\n",
        "    num_slices, num_rows, num_cols = scan.shape\n",
        "    num_chunks = num_slices / 10 + 1\n",
        "    split_scan = np.array_split(scan, num_chunks, axis=0)\n",
        "    \n",
        "    # gdh: take mean across axis 0 on each batch\n",
        "    split_scan_out = []\n",
        "    for each in split_scan:\n",
        "        split_scan_out.append(each.mean(axis=0))\n",
        "    \n",
        "    return np.stack(split_scan_out, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0195ee8e-ac45-ba01-da26-c91ca3b5b33f"
      },
      "outputs": [],
      "source": [
        "first_patient_pixels_avg10 = averaging_10_slices(first_patient_pixels)\n",
        "num_slices, num_rows, num_cols = first_patient_pixels_avg10.shape\n",
        "\n",
        "# gdn: viz all post-averaged slice\n",
        "for _ in range(num_slices):\n",
        "    plt.imshow(first_patient_pixels_avg10[_], cmap=plt.cm.gray)\n",
        "    plt.show()\n",
        "    \n",
        "# gdn: compare this quantitatively with the raw stuff?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ae70ecba-82a6-099b-ac6a-c2f69c055929"
      },
      "source": [
        "# Resampling\n",
        "A scan may have a pixel spacing of `[2.5, 0.5, 0.5]`, which means that the distance between slices is `2.5` millimeters. For a different scan this may be `[1.5, 0.725, 0.725]`, this can be problematic for automatic analysis (e.g. using ConvNets)! \n",
        "\n",
        "A common method of dealing with this is resampling the full dataset to a certain isotropic resolution. If we choose to resample everything to 1mm*1mm*1mm pixels we can use 3D convnets without worrying about learning zoom/slice thickness invariance. \n",
        "\n",
        "Whilst this may seem like a very simple step, it has quite some edge cases due to rounding. Also, it takes quite a while.\n",
        "\n",
        "Below code worked well for us (and deals with the edge cases):"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}