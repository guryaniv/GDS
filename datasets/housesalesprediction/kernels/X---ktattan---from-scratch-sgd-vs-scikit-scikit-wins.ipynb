{"cells":[{"metadata":{"collapsed":true,"_uuid":"341004bf10379871d67504ca8e1dac56654580dc"},"cell_type":"markdown","source":"# House price prediction\n\nWe use the classic [house price dataset from kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction) to fit a simple linear regression model to predict house price.\n\nThis notebook has code for ridge regression, stochastic gradient descent and cross validation coded from scratch."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"be14c656843c4be9cb5643fd8aca87abadbfddb7"},"cell_type":"code","source":"# import dependancies\nimport numpy as np\nimport pandas as pd\nimport scipy\nfrom scipy.stats import pearsonr\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\nimport matplotlib.pylab as plt\nimport seaborn as sns;\nsns.set_context('poster')\nsns.set_style('darkgrid')","execution_count":1,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"24b24be768e6bce50f56f0bb262d263c171833b8","collapsed":true},"cell_type":"code","source":"warnings.simplefilter(\"ignore\")\n\n# Read in data and drop nans\ndf = pd.read_csv('../input/kc_house_data.csv')\ndf.dropna(axis=0, how='any', inplace=True)\n# Drop date and zipcode as we will not use them in the analysis\ndf.drop(labels=['date','zipcode'],axis=1, inplace=True)\n# Create development dataset \"df\" and test dataset \"df_test\" \ndf, df_test = train_test_split(df, test_size=0.1)\ndf.reset_index(drop=True,inplace=True)\ndf_test.reset_index(drop=True,inplace=True)\n\n# Create a matrix from the columns of the development dataset\n# and scale these columns \n# Apply the same scaling to the test set\ncols = [i for i in list(df.columns) if i not in ['id','price']]\nx_dev = df.as_matrix(columns=cols)\nx_test = df_test.as_matrix(columns=cols)\n# Fit scaler only on dev set only\nscaler = StandardScaler().fit(x_dev)\ndf.drop(labels=cols,axis=1,inplace=True)\ndf_test.drop(labels=cols,axis=1,inplace=True)\nfor col in cols:\n    df[col] = scaler.fit_transform(x_dev[:,cols.index(col)].reshape(-1,1))\n    df_test[col] = scaler.fit_transform(x_test[:,cols.index(col)].reshape(-1,1))\n    \n\ndf.head()","execution_count":2,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8a769cc731c9597f57e39e26e6c9ea8c28172d5c","collapsed":true},"cell_type":"code","source":"df.shape","execution_count":3,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"c7fbf46f7400bee61d0c40a241abdef939fe8168","collapsed":true},"cell_type":"code","source":"for col in cols:\n    fig, ax = plt.subplots(figsize=(12,8))\n    df.plot(kind='scatter', x=col, y='price', ax=ax, s=10, alpha=0.5)\n    print(\"corr between price and\", col,\":\",pearsonr(df[col].values, df.price.values)[0])\n    plt.show()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"2e73510a2edb60dbfb72693204a8eac5513f7ebd"},"cell_type":"markdown","source":"# Check for covariance between features\n\nWe don't want correlated features in our model, as this makes the features redundant and could reduce the performance of the model."},{"metadata":{"trusted":false,"_uuid":"3ebdac6d0c23e92e482f0ab0beb8d0a698fbbf4b","collapsed":true},"cell_type":"code","source":"x_check_cov = df.as_matrix(columns=cols)\nx_check_cov.shape","execution_count":5,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"02cb4bdc80b653585af77f9c6fd0d12197bec835","collapsed":true},"cell_type":"code","source":"Sigma = (1/x_check_cov.shape[0])*np.dot(x_check_cov.T,x_check_cov)\nSigma = abs(Sigma)\nnp.tri","execution_count":6,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4a75b0a330ce84ce50d9d7918c001624b16f71cc","collapsed":true},"cell_type":"code","source":"mask = np.zeros_like(Sigma)\nmask[np.triu_indices_from(Sigma)] = True\nwith sns.axes_style(\"white\"):\n    fig, ax = plt.subplots(figsize=(15,10))\n    sns.heatmap(Sigma, mask=mask, square=True, vmin=0.4, ax=ax, linewidths=1, xticklabels=cols, yticklabels=cols)\n    plt.show()","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"5a00f44c7900ebb2b521e7eb8441af6b9cf8a6a8"},"cell_type":"markdown","source":"### Some features are correlated\n\nWe see features such as sqft_above and sqft_lving are highly correlated (almost 0.8). We will get rid of redundant features."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"84b45d7639a93074f6dea161395e5d4ffbbadee5","collapsed":true},"cell_type":"code","source":"discard_cols = ['grade','bathrooms','sqft_above','sqft_lot','sqft_lot15']\ncols = [i for i in list(df.columns) if i not in discard_cols]\ndf.drop(labels=discard_cols,axis=1,inplace=True)\ndf_test.drop(labels=discard_cols,axis=1,inplace=True)\ndf.head()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"9a884623df9228453bef9819228d75dcbbdeafd8"},"cell_type":"markdown","source":"# Regularized Ridge Regression\n\n\nWe'll be implmenting L2 regularized regression form scratch, using mini-batch (stochastic) gradient descent. The cost function of the model is given by\n\n$$J(W) = \\frac{1}{2m}\\sum_{i=1}^m (XW - y)^2 + \\frac{\\lambda}{2m}\\sum_{i=1}^nW^2$$\n\nwhere $X$, $W$, $y$, $\\lambda$ and $m$ are the data, model weights, targets, regression penalty and number of data points respectively.\n\nTo tune the hyperparameters, we will implement cross validation on a validation set.\n\nFinally, at the end we will train a model on a simpler dataset, only including the sqft_area feature and comparing the SSE of the simpler model to the SSE of the more complex model with more features."},{"metadata":{"trusted":false,"_uuid":"9205653f3e85d6d97f47c9c2d8d7f1183bf7f0eb","collapsed":true},"cell_type":"code","source":"class RidgeGD:\n    \"\"\"\n    Performs ridge regression on a dataset with L2 regularization (from scratch implementation)\n    \"\"\"\n    \n    def __init__(self,lambda_=1,alpha=0.01, batchSize=32,n_iter=100,output_every=10):\n        self.lambda_        = lambda_ # the penalty / regularization size\n        self.alpha          = alpha # the learning rate\n        self.batchSize      = batchSize # the size mini batch for gradient descent\n        self.n_iter         = n_iter # the numner of iterations in mini batch gradient descent\n        self.output_every   = output_every # how often to print error\n        \n    def cost(self,x,y,w):\n        \"\"\"\n        Calculate the cost with current weights\n        INPUT: data x, targets y, weights w\n        OUTPUT: cost\n        \"\"\"\n        # X ~ mxn\n        # y ~ mx1\n        # W ~ nx1\n        m = x.shape[0]\n        h = np.dot(x,w) # mxn * nx1 = mx1\n        error = h - y\n        w[0] = 0 # dont regularize bias\n        J = (1/(2*m))*np.sum(error**2) + (self.lambda_/(2*m))*np.sum(w**2)\n        return J\n     \n    def grad(self,x,w,y):\n        \"\"\"\n        Calculate the gradient of the cost function\n        INPUT: data x, targets y, weights w\n        OUTPUT: gradient of cost function\n        \"\"\"\n        m = x.shape[0]\n        h = np.dot(x,w)\n        error = h - y # mx1\n        w[0] = 0 # dont regularize bias term\n        partial = (1/m)*np.dot(x.T,error) + (self.lambda_/m)*w # nx1\n        return partial\n    \n    def update_weights(self,x,w,y):\n        \"\"\"\n        Update the model weights\n        INPUT: data x, targets y, current weights w\n        OUTPUT: updated weights\n        \"\"\"\n        partial = self.grad(x,w,y)\n        w = w - self.alpha*partial\n        return w\n    \n    def get_mini_batch(self,x,y,i):\n        \"\"\"\n        Get a minibatch of the data\n        INPUT: data x, targets y, iteration i\n        OUTPUT: subset of the data X,y\n        \"\"\"\n        x_mini = x[i*self.batchSize:(i+1)*self.batchSize,:]\n        y_mini = y[i*self.batchSize:(i+1)*self.batchSize,:]\n        return x_mini,y_mini\n    \n    def add_bias(self,x):\n        \"\"\"\n        Add a column of 1's as the first column in the data x\n        INPUT: data x\n        OUTPUT: data x with a column of 1's as first column\n        \"\"\"\n        x_bias = np.ones((x.shape[0],1))\n        x = np.concatenate((x_bias,x),axis=1)\n        return x\n    \n    def init_weights(self,x):\n        \"\"\"\n        Initialize the model weights at random\n        INPUT: data x\n        OUTPUT: random weights\n        \"\"\"\n        return (np.random.random((x.shape[1],1))*2 - 1)*1e-2\n    \n    def fit(self,x,y):\n        \"\"\"\n        Fit a model to the data using mini batch gradient descent\n        INPUT: data x, targets y\n        OUTPUT: model weights w\n        \"\"\"\n        if np.all(x[:,0] == np.ones(x.shape[0])):\n            pass\n        else:\n            x = self.add_bias(x)\n            \n        w = self.init_weights(x)\n        n = np.arange(len(x))\n        \n        \"\"\"\n        Perform mini batch gradient descent\n        \"\"\"\n        J = []\n        for epoch in range(1, self.n_iter + 1):\n            for i in range(0,round(x.shape[0]/self.batchSize)):\n                X_mini,Y_mini = self.get_mini_batch(x=x,y=y,i=i)\n                J.append(self.cost(x=X_mini,y=Y_mini,w=w))\n                w = self.update_weights(x=x,w=w,y=y)\n\n        return w\n    \n    def predict(self,x,w):\n        \"\"\"\n        Predict the target of new input data\n        INPUT: data x, learned weights w\n        OUTPUT: predicted targets\n        \"\"\"\n        x_bias = np.ones((x.shape[0],1))\n        x = np.concatenate((x_bias,x),axis=1)\n        return np.dot(x,w)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"e279e3d17adc465cb35151664f810380d7c0dd75"},"cell_type":"markdown","source":"# SSE\n\nSum of squared error (SSE), is defined as follows\n\n$$\\text{SSE} = \\sum_{i=1}^{m}(y_i - \\hat{y_i})^2$$\n\nwhere $y_i$ is the true target value, and $\\hat{y_i}$ is the predicted target value.\n\nWe will use SSE as a metric for the error of a model, and compare SSE values to determine the bets model hyperparameters and best ultimate model."},{"metadata":{"trusted":false,"_uuid":"7d8413b57e2c0a0f1d27757a0d40be930ddb17bd","collapsed":true},"cell_type":"code","source":"# The features we will be using to predict price\n[col for col in list(df.columns) if col not in ['id','price']]","execution_count":10,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5d1751b184c703aed49770830ee0a2d60d1fd2db"},"cell_type":"code","source":"# Create matrices X and Y for training and testing\nX = df.as_matrix(columns=[col for col in list(df.columns) if col not in ['id','price']])\nY = df.as_matrix(columns=['price'])\nX_test = df_test.as_matrix(columns=[col for col in df_test if col not in ['id','price']])\nY_test = df_test.as_matrix(columns=['price'])","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"9460fb1bcd8d53583e83bf84c40d8ab0b22ff85e"},"cell_type":"markdown","source":"## Cross Validate\n\nWe use cross validation to find the best training rate $\\alpha$ and penalty $\\lambda$"},{"metadata":{"trusted":false,"_uuid":"f0908649f3b4ee775f5107dbf8b1d21e0507131c","collapsed":true},"cell_type":"code","source":"def CrossValidate(x,y,alpha,lambda_):\n    \"\"\"\n    Use cross validation on the development data set to obtain the optimal hyperparameters\n    INPUT: data x, targets y, alpha (start, end, num), lambda (start, end, num)\n    OUTPUT: errs_dict, a dictionary of (alpha,lambda) keys and SSE error values\n    \"\"\"\n    \n    alphas = np.linspace(alpha[0],alpha[1],alpha[2])\n    lambdas_ = np.logspace(lambda_[0],lambda_[1],lambda_[2]).astype(int)\n    \n    # Use k-fold (10-fold) cross val\n    k=10\n    cv = KFold(n_splits=k, shuffle=False)\n    train_indices = [i[0] for i in cv.split(x)]\n    val_indices = [i[1] for i in cv.split(x)]\n    \n    errs_dict = {(a,l):0 for a,l in zip(alphas,lambdas_)}\n\n    # Loop over alphas and lambdas hyperparams\n    for a in alphas:\n        print(\"alpha:\",a)\n        for lam in lambdas_:\n            cv_sse_err = []\n            for i in range(k):\n                \n                x_train = x[train_indices[i],:]\n                y_train = y[train_indices[i],:]\n                \n                x_val = x[val_indices[i],:]\n                y_val = y[val_indices[i],:]\n                \n                model = RidgeGD(lambda_=lam, alpha=a, batchSize=64, n_iter=50, output_every=50)\n                W = model.fit(x=x_train,y=y_train)\n                \n                sse = sum((model.predict(x_val,W) - y_val)**2)[0]\n                cv_sse_err.append(sse)\n                \n            errs_dict[(a,lam)] = (1/k)*sum(cv_sse_err)\n    \n    return errs_dict","execution_count":12,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5fa65c6db10c7a7c96300ab3f0eb5e90af62709c","collapsed":true},"cell_type":"code","source":"# This takes about 10 minutes to run\n# errs_dict = CrossValidate(X,Y,(0.5,0.69,5),(1,2,5))","execution_count":13,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6fed3ec028c900089d4d82545c18be9887a0a689","collapsed":true},"cell_type":"code","source":"# Use the minimum value to train the final model\n# min(errs_dict, key=errs_dict.get)","execution_count":14,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a194aa55fc61711f3b140c1d413f30a69be480a1","collapsed":true},"cell_type":"code","source":"# Train model with optimim hyperparams above\nmodel = RidgeGD(lambda_=32,alpha=0.64, batchSize=32, n_iter=200, output_every=50)\n# Model weights\nW = model.fit(x=X,y=Y)","execution_count":86,"outputs":[]},{"metadata":{"_uuid":"a4143f9b2182126573fcfb6001bbfa2687e134eb"},"cell_type":"markdown","source":"## R-squared & RSS\n\nThe R-squared value is a metric used to evaluate a linear regression model. Similarly for RSS (residual sum of squares)\n\n$$R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}$$\n\nwhere $SS_{\\text{res}} = \\sum_i ( y_i - f_i)^2$ and $SS_{\\text{tot}} = \\sum_i ( y_i - \\bar{y_i})^2$.\n\nHere, $f_i$ is the model's prediction of the $i$'th observation and $y_i$ is the true target value of the $i$'th observation.\n\nAn $R^2$ value of 1 indicates a perfect model. A value of $x<1$ indicates that $x\\%$ of the variance of the data is explained by the model.\n\n\n### The RSS is given by\n\n$$RSS = \\sum_i (y_i - f_i)^2$$\n\nagain $f_i$ is the model's prediction of the $i$'th observation and $y_i$ is the true target value of the $i$'th observation.\n\nWe can see that RSS and $SS_{\\text{res}}$ are equivalent."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"340f172c0076499aa69e8de1fac554ca65fe3689"},"cell_type":"code","source":"class Metrics:\n    \"\"\"\n    Metrics we will use in evaluating our models\n    \"\"\"\n    \n    def r_squared(self, model, x, y, w=None, scikit_flag=False):\n        \"\"\"\n        Calculate the R^2 value of an input model\n        INPUT: model, X (data), y (targets), W (weights, if applicable), scikit_flag (if using scikit)\n        \"\"\"\n        if not scikit_flag:\n            f = model.predict(x=x, w=w)\n            sstot = ((y - np.mean(y))**2).sum()\n            ssres = ((y - f)**2).sum()\n            return 1 - (ssres/sstot)\n        else:\n            return model.score(X=x, y=y)\n    \n    def rss(self, model, x, y, w=None, scikit_flag=False):\n        if not scikit_flag:\n            f = model.predict(w=w, x=x)\n            return ((y - f)**2).sum()\n        else:\n            f = model.predict(X=x)\n            return ((y - f)**2).sum()","execution_count":87,"outputs":[]},{"metadata":{"_uuid":"fcbea5ad47131135f10c067d5cb93626190f0a00"},"cell_type":"markdown","source":"### Comparison to package - Scikit\n\nWe compare our from scratch implemtnation to that of scikit's ridge regression model."},{"metadata":{"trusted":false,"_uuid":"4bb340627cacdbe6e92c3a023cf611925d32165f","collapsed":true},"cell_type":"code","source":"# train the scikit model\nclf = Ridge(alpha=32, solver='saga')\nmodel_sk = clf.fit(X=X, y=Y)","execution_count":88,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c996e9015fdd5bcb5b699a36b3cce781f79a4e73","collapsed":true},"cell_type":"code","source":"mets = Metrics()\ntrain_r2 = mets.r_squared(model, X, Y, W)\ntest_r2 = mets.r_squared(model, X_test, Y_test, W)\ntrain_r2_scikit = mets.r_squared(model_sk, X, Y, scikit_flag=True)\ntest_r2_scikit = mets.r_squared(model_sk, X_test, Y_test, scikit_flag=True)\n\ntrain_rss = mets.rss(model, X, Y, W)\ntest_rss = mets.rss(model, X_test, Y_test, W)\ntrain_rss_scikit = mets.rss(model_sk, X, Y, scikit_flag=True)\ntest_rss_scikit = mets.rss(model_sk, X_test, Y_test, scikit_flag=True)","execution_count":89,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6f27dc1c88c1b17bc75b06975907fc7822b24788","collapsed":true},"cell_type":"code","source":"print(\"Model\\t\\t|\\tR2 Train\\t|\\tR2 Test\\t\\t|\\tRSS Train\\t|\\tRSS Test\")\nprint(\"---------------------------------------------------------------------------------------------------------------\")\nprint(\"From Scratch SGD\\t\", train_r2, \"\\t\", test_r2, \"\\t\", train_rss, \"\\t\", test_rss)\nprint(\"Scikit\\t\\t\\t\", train_r2_scikit, \"\\t\", test_r2_scikit, \"\\t\", train_rss_scikit, \"\\t\", test_rss_scikit)","execution_count":90,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f5b922283fbb1342fab206b47661b90bf8175d26"},"cell_type":"markdown","source":"## Conclusions\n\nOur from scratch SGD ridge regression model performs poorly compared to a standard open source package such as scikit; our test $R^2$ is only 0.38 while the scikit model achieved a test score of 0.65. Similarly, the RSS of the scikit model is much lower (an order of magnitude on training data and half as low on test data) using the scikit implementation.\n\nThis would indicate that our model could not find the (global) minimum, and became stuck somewhere in the feature / cost space during gradient descent. Perhaps a more sophistaced algorithm may have perforrmed better, such as momentum or Adam.\n\nWe will train a model on a simpler dataset, using only the sqft_area feature of the original dataset and calculating the SSE of the model.\n\n# Simpler Model -  sqft_area feature only"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"bad0289daa447eab468c02312254f5746b27b369","collapsed":true},"cell_type":"code","source":"# Create simple dataset\n# First, cross validate the model to find optimum hyperparms\nX_simple = df.as_matrix(columns=['sqft_living'])\n# errs_dict = CrossValidate(X_simple,Y,(0.98,1.3,5),(1,2,5))","execution_count":19,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"60e40ef19d58b0a39de2322262f33ec7c510899a","collapsed":true},"cell_type":"code","source":"# Use these optimum hyperparams in simple model\n# min(errs_dict, key=errs_dict.get)","execution_count":20,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5aabbc2fcc351ba6ea107d279857311507ba450d","collapsed":true},"cell_type":"code","source":"Y = df.as_matrix(columns=['price'])\nmodel_simple = RidgeGD(lambda_=10, alpha=1, batchSize=32, n_iter=200, output_every=50)\nW_simple = model_simple.fit(x=X_simple,y=Y)","execution_count":91,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"258d5272956ddb81c998e7baf295aabf941696bc"},"cell_type":"code","source":"# Make a test data set using only sqft_living\nX_test_simple = df_test.as_matrix(columns=['sqft_living'])","execution_count":92,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0f330d3571e82e98d5aaf6e67ccc316328b40d09","collapsed":true},"cell_type":"code","source":"clf = Ridge(alpha=100, solver='saga')\nmodel_sk = clf.fit(X=X_simple, y=Y)","execution_count":93,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c58c542fdc2e21ea05dc0b23048afe15c201305e","collapsed":true},"cell_type":"code","source":"mets = Metrics()\ntrain_r2_simple = mets.r_squared(model, X_simple, Y, W_simple)\ntest_r2_simple = mets.r_squared(model, X_test_simple, Y_test, W_simple)\ntrain_r2_scikit_simple = mets.r_squared(model_sk, X_simple, Y, scikit_flag=True)\ntest_r2_scikit_simple = mets.r_squared(model_sk, X_test_simple, Y_test, scikit_flag=True)\n\ntrain_rss_simple = mets.rss(model, X_simple, Y, W_simple)\ntest_rss_simple = mets.rss(model, X_test_simple, Y_test, W_simple)\ntrain_rss_scikit_simple = mets.rss(model_sk, X_simple, Y, scikit_flag=True)\ntest_rss_scikit_simple = mets.rss(model_sk, X_test_simple, Y_test, scikit_flag=True)","execution_count":94,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3921dd2f545007122f81883c5074bc0556d80233","collapsed":true},"cell_type":"code","source":"print(\"Model\\t\\t|\\tR2 Train\\t|\\tR2 Test\\t\\t|\\tRSS Train\\t|\\tRSS Test\")\nprint(\"---------------------------------------------------------------------------------------------------------------\")\nprint(\"From Scratch SGD\\t\", train_r2_simple, \"\\t\", test_r2_simple, \"\\t\", train_rss_simple, \"\\t\", test_rss_simple)\nprint(\"Scikit\\t\\t\\t\", train_r2_scikit_simple, \"\\t\\t\", test_r2_scikit_simple, \"\\t\", train_rss_scikit_simple, \"\\t\", test_rss_scikit_simple)","execution_count":95,"outputs":[]},{"metadata":{"_uuid":"6c5ff62cd0c8f9601f6fc22806200cb601d06b89"},"cell_type":"markdown","source":"### Plot from scratch model\n\nSWe'll plot our from scratch model to see how well it fits the data and since we only use one feature (sqft_living), it's easier to plot on an x-y axis."},{"metadata":{"trusted":false,"_uuid":"95f785b525d7802747e65317bbcdbe7b7a9dec8d","collapsed":true},"cell_type":"code","source":"def line(m,b,x):\n    \"\"\"\n    Return the points on a line\n    INPUT: m (line slope), b (line intersect / bias), x (x values of the line)\n    OUTPUT: an array of y values for every corresponding x value\n    \"\"\"\n    return m*x+b\n\n# Plot appropriate weights and biases and data to see how well line fits through data\nl = line(m=W_simple[1],b=W_simple[0],x=np.linspace(-2,12,1000))\nfig,ax = plt.subplots(figsize=(12,8))\nax.scatter(X_simple,Y,s=10,alpha=0.5)\nax.plot(np.linspace(-2,12,1000),l,color='r')\nax.set_ylabel(\"Price\")\nax.set_xlabel(\"Scaled Sqft House Area\")\nplt.show()","execution_count":127,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"69bad05e5ed07992df3f2b26a206f7bb757d6e4d","collapsed":true},"cell_type":"code","source":"print(\"Model\\t\\t\\t|\\tR2 Train\\t|R2 Test\\t|RSS Train\\t|\\tRSS Test\")\nprint(\"---------------------------------------------------------------------------------------------------------------\")\nprint(\"From Scratch SGD (complex)\\t\", round(train_r2,2), \"\\t\\t\", round(test_r2,2), \"\\t\\t\", round(train_rss,2), \"\\t\", round(test_rss,2))\nprint(\"Scikit (complex)\\t\\t\", round(train_r2_scikit,2), \"\\t\\t\", round(test_r2_scikit,2), \"\\t\\t\", round(train_rss_scikit,2), \"\\t\", round(test_rss_scikit,2))\nprint(\"From Scratch SGD (simple)\\t\", round(train_r2_simple,2), \"\\t\\t\", round(test_r2_simple,2), \"\\t\\t\", round(train_rss_simple,2), \"\\t\", round(test_rss_simple,2))\nprint(\"Scikit (simple)\\t\\t\\t\", round(train_r2_scikit_simple,2), \"\\t\\t\", round(test_r2_scikit_simple,2), \"\\t\\t\", round(train_rss_scikit_simple,2), \"\\t\", round(test_rss_scikit_simple,2))","execution_count":125,"outputs":[]},{"metadata":{"_uuid":"b95810dc3d855a47027c4a0451095ca1e4128083"},"cell_type":"markdown","source":"# Conclusions\n\nWe can see from the table above, that using our from scratch implementation, the simpler model, using only sqft_area, performed better than the complex from scratch model using all the features. \n\nHowever, the scikit implementation performed worse in the simple model than the complex model with many features.\n\nWe can conclude 2 things.\n\n1. Using more features yields a better performing model, and therefore there is more information to be gained by using more features.\n\n2. Always use an optimized library (scikit, tensorflow) when possible :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}