{"nbformat_minor": 0, "cells": [{"cell_type": "code", "outputs": [], "execution_count": null, "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "from sklearn import linear_model\n", "from sklearn.model_selection import train_test_split\n", "\n", "import seaborn as sns"], "metadata": {"_uuid": "90382ab6eb50bd63a9335b9997cc8891ede1b9f5", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "markdown", "source": "Since we saw in the model by Clemens Mzr (https://www.kaggle.com/clemensmzr/simple-multivariate-gaussian-anomaly-detection) and Johansing (https://www.kaggle.com/johansing/multivariate-gaussian-vs-oneclasssvm-97-recall) based on multivariate Gaussian that the frauds are no outliers and therefore outlier detection from a Gaussian distribution is not good for fraud detection, I try to implement a simple linear logistic regression (further down also with polynomial features) to see how it performs -- despite the only few fraud cases that presumably complicate the learning of a decision boundary.\nOne reaches an F1 score of 0.81 on the test set.", "metadata": {"_uuid": "315fd01af7fb9b2a308c99a438bd874affc96c06"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": ["Reading in the data\n", "===="], "metadata": {"_uuid": "648bd7446530fc94cf29eb24336fe00f6da578be"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": "data = pd.read_csv(\"../input/creditcard.csv\")\ndata.head()", "metadata": {"_uuid": "fbfbdc650cf16e08f7bbc89423f8d0c7d1db3d28", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "markdown", "source": ["Simple logistic regression\n", "====="], "metadata": {"_uuid": "95792afb053eef6579d4dac37d65e20ae128fb6e"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["data.shape"], "metadata": {"_uuid": "b26a1a9594d14bb42a444563cdf9b90fd01240d3", "collapsed": false}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["X = data[data.columns[0:30]]\n", "y = data[\"Class\"]"], "metadata": {"_uuid": "36b897f0ed91239034e9679e33318521e57d684e", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["X.shape"], "metadata": {"_uuid": "ef03e2e661f8f25b96c15ffb0f90f6ffc54c7b8d", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "markdown", "source": ["A quick try without splitting into training and test set."], "metadata": {"_uuid": "0844c144d5d4d88d7d16b94c24753835238acb7c"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": "lr = linear_model.LogisticRegression()\nlr.fit(X, y)\nprint(lr.score(X, y))", "metadata": {"_uuid": "011489083698f5405272c16ba0c9a0d70c0c2da5", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "markdown", "source": ["Getting the different evaluation measures for skewed data. average_precision_score is area under the precision-recall curve.\n", "score is the mean accuracy."], "metadata": {"_uuid": "88077b2935c95fa73a8889fff8f9e4c6a29c2f0a"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["from sklearn.metrics import f1_score, average_precision_score, precision_score, recall_score"], "metadata": {"_uuid": "a6455d6d85d510890e3a95d3f452f2ac3c2e13ff", "_execution_state": "idle", "collapsed": true}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["y_pred=lr.predict(X)\n", "print(\"Accuracy:\", lr.score(X, y))\n", "print(\"Precision:\", precision_score(y, y_pred))\n", "print(\"Recall:\", recall_score(y, y_pred))\n", "print(\"F1:\", f1_score(y, y_pred))\n", "print(\"Area under precision Recall:\", average_precision_score(y, y_pred))\n"], "metadata": {"_uuid": "6bcb705455b17ecc75a2668b22d878eff0d53a8f", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "markdown", "source": ["Regularization optimization\n", "====="], "metadata": {"_uuid": "d8773500513347aef386ec89b84c6c8717a659ba"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": ["Optimize regularization for f1 score."], "metadata": {"_uuid": "70af28f91fc58fc69ae648fab28d314ecc6b837e", "collapsed": true}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["from sklearn.model_selection import cross_val_score, train_test_split"], "metadata": {"_uuid": "ed9444c2846e6ca86ce8a69bd5050db1d8b54bcf", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "markdown", "source": ["split into train and test set."], "metadata": {"_uuid": "cb9db89fba5fb0d6968a2007d2094933088400b2"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["X_Legit=data.query(\"Class==0\").drop([\"Amount\",\"Class\"],1)\n", "y_Legit=data.query(\"Class==0\")[\"Class\"]\n", "X_Fraud=data.query(\"Class==1\").drop([\"Amount\",\"Class\"],1)\n", "y_Fraud=data.query(\"Class==1\")[\"Class\"]\n", "#split data into training and cv set\n", "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(X_Legit, y_Legit, test_size=0.3)\n", "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_Fraud, y_Fraud, test_size=0.3)\n", "X_test = X_test_l.append(X_test_f)\n", "y_test = y_test_l.append(y_test_f)\n", "X_train = X_train_l.append(X_train_f)\n", "y_train = y_train_l.append(y_train_f)"], "metadata": {"_uuid": "5fcd7f2e203845041900c429ecda9b94b71a7ef5", "_execution_state": "idle", "collapsed": true}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": "def cv_run(X_train, X_test, y_train, y_test):\n    bestC = 1.\n    bestScore = 0.\n    for C in [0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]:\n        lr = linear_model.LogisticRegression(C=C)\n        lr.fit(X_train, y_train)\n        y_pred_train=lr.predict(X_train)\n        score = f1_score(y_train, y_pred_train)\n        if score > bestScore:\n            bestC = C\n            bestScore = score\n    print( \"Best C:\", bestC)\n\n    lr = linear_model.LogisticRegression(C=bestC)\n    lr.fit(X_train, y_train)\n    y_pred_test=lr.predict(X_test)\n    y_pred_train=lr.predict(X_train)\n    print(\"Train score:\", lr.score(X_train, y_train))\n    print(\"Test score:\", lr.score(X_test, y_test))\n    print(\"Train F1:\", f1_score(y_train, y_pred_train))\n    print(\"Test F1:\", f1_score(y_test, y_pred_test))", "metadata": {"_uuid": "bb53da94ad0932e3c6dbb415c57d2de82a16bd47", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["cv_run(X_train, X_test, y_train, y_test)"], "metadata": {"_uuid": "8eb3126ee64318039afcfe9dff8cbc5f75023ea4", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "markdown", "source": "before we add polynomial features, we normalize the features. Shouldnt change much since it's mostly PCA data, but let's check anyways.", "metadata": {"_uuid": "88f49f15beebaedb407405a7b7e5bfbe959d2d34"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": ["Feature scaling\n", "======"], "metadata": {"_uuid": "e348b2ba0b595b999c1e5ad03776cd7f013237f7"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["from sklearn import preprocessing"], "metadata": {"_uuid": "2f372a050c0f6ff5a6529c8244b16b961130f717", "_execution_state": "idle", "collapsed": true}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["X_test_scaled = preprocessing.scale(X_test)\n", "X_train_scaled = preprocessing.scale(X_train)"], "metadata": {"_uuid": "d1bed4a5816274ce7ff9f6051c36302a39128d3c", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["cv_run(X_train_scaled, X_test_scaled, y_train, y_test)"], "metadata": {"scrolled": true, "_execution_state": "idle", "collapsed": false, "_uuid": "3a546bae00e49d96d38f8b7d72434a3fa967269e"}}, {"cell_type": "markdown", "source": "Feature scaling improved the F1 score by 4 %. The only features that should be really affected are Amount and Time.", "metadata": {"_uuid": "224c68695c5f1836b70ac2b74dcfc000638c7b2f", "_execution_state": "idle", "collapsed": false}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": ["# Adding polynomial features"], "metadata": {"_uuid": "819eb628d273609eecd8b506ef4475124c65b311"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["from sklearn.preprocessing import PolynomialFeatures"], "metadata": {"_uuid": "2406cbab76a4709f301f145b5aafe732f3ba848a", "_execution_state": "idle", "collapsed": true}}, {"cell_type": "markdown", "source": ["Adding polynomial features and then running the regularization optimization is pretty costly for this data set. With more then 2nd order polynomials I run out of memory. With second order polynomials I run out of time. So we just do it for a fixed regularization parameter C=2 and see what happens."], "metadata": {"_uuid": "1b189d0959af0ba0878ecec235254fde56dd1a35"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["poly = PolynomialFeatures(2)\n", "X_train_poly = poly.fit_transform(X_train_scaled)\n", "X_test_poly = poly.fit_transform(X_test_scaled) "], "metadata": {"_uuid": "e6153e246cb575e92bd5c1024d16804b07ee0e6f", "_execution_state": "idle", "collapsed": true}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["def lr_run(X_train, X_test, y_train, y_test):\n", "    lr = linear_model.LogisticRegression()\n", "    lr.fit(X_train, y_train)\n", "    y_test_pred=lr.predict(X_test)\n", "    print(\"Accuracy on training set:\", lr.score(X_train, y_train))\n", "    print(\"Accuracy on test set:\", lr.score(X_test, y_test))\n", "    print(\"Precision on test set:\", precision_score(y_test, y_test_pred))\n", "    print(\"Recall on test set:\", recall_score(y_test, y_test_pred))\n", "    print(\"F1 on test set:\", f1_score(y_test, y_test_pred))\n", "    print(\"Area under precision Recall on test set:\", average_precision_score(y_test, y_test_pred))\n"], "metadata": {"_uuid": "c7d5b3193a862a055f6e5abac0746f407d07b8e4", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["lr_run(X_train_poly, X_test_poly, y_train, y_test)"], "metadata": {"_uuid": "2820640046503ee407992b636162451415004a3f", "_execution_state": "idle", "collapsed": false}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["#poly = PolynomialFeatures(2)\n", "#X_poly = poly.fit_transform(X_scaled) \n", "#cv_run(X_poly, y_scaled)"], "metadata": {"_uuid": "cb39f8d68642bb99a15b31fc7a1d6f052d079f8e", "collapsed": false}}, {"cell_type": "markdown", "source": ["Removing \"Time\" to compare performance"], "metadata": {"_uuid": "256fadf3b7be6323062070a0526b0ed536e22b4d"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["X_train_wo_t = np.delete(X_train_scaled, 0, 1)\n", "X_test_wo_t = np.delete(X_test_scaled, 0, 1)"], "metadata": {"_uuid": "fb86fef7d93af11168bbf8be21190323982ef5c6", "collapsed": false}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["poly = PolynomialFeatures(2)\n", "X_train_wo_t_poly = poly.fit_transform(X_train_wo_t)\n", "X_test_wo_t_poly = poly.fit_transform(X_test_wo_t) "], "metadata": {"_uuid": "1a38a8dd82b47fc8800db13e6c1cad8ff3b542be", "collapsed": true}}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": ["lr_run(X_train_wo_t_poly, X_test_wo_t_poly, y_train, y_test)"], "metadata": {"_uuid": "838f9376feaf40e322a72d6b193885ea74aabe86", "collapsed": false}}, {"cell_type": "markdown", "source": ["So including the time of transaction, the F1 score goes actually down."], "metadata": {"_uuid": "2cd826a92654910fe2b86794c494f928f8c11b85"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "outputs": [], "execution_count": null, "source": [], "metadata": {"_uuid": "f9517447ade0427f0d8e196dd311a52ed87b1f44", "collapsed": true}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python", "name": "python", "version": "3.6.0", "file_extension": ".py"}}, "nbformat": 4}