{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90b34a9bf5c1bd3402c250fda467e0c589632f06"},"cell_type":"markdown","source":"First we want to understand the problem itself, so this is the link to the kaggle competition itself https://www.kaggle.com/c/costa-rican-household-poverty-prediction\n\nSummary Objective: Identify which households have the highest need for social welfare assistance Secundary Objectives:\n\n    * Create a kernel to solve the problem using machine learning techniques\n    * Explain the whole process a logic inside the kernel\nMotive behind the kaggle: They believe that new methods beyond traditional econometrics, personally I choose this competition as the final project for a data analytics course and also because it's a problem that involves my country.\n\nNotes: This kernel is not focus on EDA."},{"metadata":{"_uuid":"ebfa1576e3d4cfdbc6afdfd8fd83b98b8cc8474a"},"cell_type":"markdown","source":"## Exploratory Analysis\n\nIt will be handy to have in this notebook the target columns definition.\n * Target - the target is an ordinal variable indicating groups of income levels. \n   * 1 = extreme poverty \n   * 2 = moderate poverty \n   * 3 = vulnerable households \n   * 4 = non vulnerable households\n\nThis data contains 142 total co****lumns."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(\"Train dimensions: \", train.shape)\nprint(\"Test dimensions: \", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3acaba1678fe1f882132fd4b82b90bf8aff17459"},"cell_type":"markdown","source":"Just by looking at this data we noticied that there is an issue, there are insufficient rows on the training side compare to the testing side.\n\nThis represent a problem because most of the machine learning algorithms will faced problems recognizing patterns inside our data. An there is a probability of obtaining not a great score because of the lack of training data. \n\nMoving on, since the test and train row count is againt us, I want to check right away if there is unbalance data in the target column"},{"metadata":{"trusted":true,"_uuid":"3fc1d3bc215d2d560e977980bc84eeb26cd00bad"},"cell_type":"code","source":"targetDistribution = pd.value_counts(train['Target'].values, sort=True)\ntargetDistribution.plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90e893e58f5b0722cf6c3f6e628306ae21b11d3b"},"cell_type":"markdown","source":"### Unbalance data decision\n\nAnd we do have the target column unbalance, this makes us faced an important decision.\n\nDo we want our model to be tend to categorize a household as non-vulnerable or is so important to risk accuracy but categorize well every household. \n\nIf there are given us this data that means that many of people who ask for assistance is not appropiate to received it, so I will keep like that, besides we have limited data for this kernel so there is always the possibility of add more data to the model on those cases where the household decides to ask to revisit their case and discover if it's truly a family who deserves assistance and if so, consider more research about his case for accuracy purposes. "},{"metadata":{"_uuid":"b506fcb03bee629739c2d0280c8ddf1d23d99fa3"},"cell_type":"markdown","source":"### Preprocessing process\n"},{"metadata":{"trusted":true,"_uuid":"bb97305bdd09fd546d596cb221c5032c6ff51e96"},"cell_type":"code","source":"complete_df = pd.concat([train, test], ignore_index=True)\ncomplete_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63458db6cfcecf84738f42ed1d630e537ab03f6d"},"cell_type":"markdown","source":"Just by looking tail I noticed some missing data. Let's check if we have data missing. "},{"metadata":{"trusted":true,"_uuid":"fad807800b03a4fe1291ca890cd638876eb5d54d"},"cell_type":"code","source":"print(complete_df.columns[complete_df.isnull().any()].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5978f7dd7ebb11496dbfaeed3b42f9f6fbdaf9dc"},"cell_type":"code","source":"missing_set = complete_df[['v2a1', 'v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']]\ncolumns = missing_set.columns\npercent_missing = missing_set.isnull().sum() * 100 / len(missing_set)\nmissing_value_df = pd.DataFrame({'column_name': columns,\n                                 'percent_missing': percent_missing})\nprint(missing_value_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"763ee53f7fd7dc040a307060cfe75ca8e476d9c6"},"cell_type":"markdown","source":"So, let's remove those columns where percent_missing is above 70% and fill the rest, using mean values"},{"metadata":{"trusted":true,"_uuid":"071297f75f0b1fd4bdc9a6d9bf5076f62fb61104"},"cell_type":"code","source":"columns_to_drop = ['v2a1', 'v18q1', 'rez_esc']\ntrain.drop(columns_to_drop, inplace=True, axis=1)\ntest.drop(columns_to_drop, inplace=True, axis=1)\ntrain[\"SQBmeaned\"].fillna(train[\"SQBmeaned\"].mean(), inplace=True)\ntrain[\"meaneduc\"].fillna(train[\"meaneduc\"].mean(), inplace=True)\ntest[\"SQBmeaned\"].fillna(test[\"SQBmeaned\"].mean(), inplace=True)\ntest[\"meaneduc\"].fillna(test[\"meaneduc\"].mean(), inplace=True)\nprint(train.isnull().values.any())\nprint(test.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fced91f052337186c3ace27fb5dd887cab19c3a"},"cell_type":"markdown","source":"#### All missing data is handled now\nChecking closely the data for more preprocessing."},{"metadata":{"trusted":true,"_uuid":"a7211d15cf37cbe3b16c9872be7a5e275bb4dc7a"},"cell_type":"code","source":"pd.set_option('display.max_columns', 500)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc8958399e57499c958788f1ac5d222549b05fb8"},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"444a8c34cd5a9b4df56787d2588c2345503ba841"},"cell_type":"markdown","source":"No needed: idhogar, Id. So I will remove those\n\nFields: dependency, edjefe, edjefa are alphanumeric, so we want them to be numeric for better preprocessing."},{"metadata":{"trusted":true,"_uuid":"a3d2793f1a90b54752f84131334776df07781c69"},"cell_type":"code","source":"columns_to_drop2 = ['idhogar', 'Id']\ntrain.drop(columns_to_drop2, inplace=True, axis=1)\n\nid_test = test['Id']\nidHogar_test =  test['idhogar']\ntest.drop(columns_to_drop2, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4313c0dec285431227412ebf9bb03484a90cab40"},"cell_type":"markdown","source":"Get all unique values for columns: dependency, edjefe, edjefa"},{"metadata":{"trusted":true,"_uuid":"04737844a66549566977780f3dcdc122acd945f3"},"cell_type":"code","source":"print(train.dependency.unique())\nprint(train.edjefe.unique())\nprint(train.edjefa.unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22d1c1ac439b6f99c1e07b5f6556420875d7ceb5"},"cell_type":"markdown","source":"Checking the kernel, the data provider we need to convert yes to 1 and no to 0 for the fields that we detect are alphanumeric"},{"metadata":{"trusted":true,"_uuid":"abc38885180be92d17078a79a2661197ff45a327"},"cell_type":"code","source":"warnings.filterwarnings(action='once')\n\ntrain.dependency[train.dependency == 'yes'] = 1 \ntrain.dependency[train.dependency == 'no']   = 0\n\ntrain.edjefe[train.edjefe == 'yes'] = 1 \ntrain.edjefe[train.edjefe == 'no']   = 0\n\ntrain.edjefa[train.edjefa == 'yes'] = 1 \ntrain.edjefa[train.edjefa == 'no']   = 0\n\ntest.dependency[test.dependency == 'yes'] = 1 \ntest.dependency[test.dependency == 'no']   = 0\n\ntest.edjefe[test.edjefe == 'yes'] = 1 \ntest.edjefe[test.edjefe == 'no']   = 0\n\ntest.edjefa[test.edjefa == 'yes'] = 1 \ntest.edjefa[test.edjefa == 'no']   = 0\n\nprint(\"Unique values for dependency: \")\nprint(train.dependency.unique())\nprint(\"Unique values for edjefe: \")\nprint(train.edjefe.unique())\nprint(\"Unique values for edjefa: \")\nprint(train.edjefa.unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48effabcacad876565a62b895a528d4e3acef1ca"},"cell_type":"markdown","source":"### All data at this point is numeric"},{"metadata":{"_uuid":"f1dda03f586c0675dcf3bcfbbf2b2b9ed1ffa05f"},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{"_uuid":"75c886fc4da3e060859df96c11ddd778733e8ed3"},"cell_type":"markdown","source":"For analysis purpose is good to see which are the columns that impact the most in some simple model. "},{"metadata":{"trusted":true,"_uuid":"bce192648abfbfa1c860968f0e1a1c8481e99031"},"cell_type":"code","source":"y = train['Target'].values\ntrain_Feature = train.copy()\ntrain_Feature.drop(['Target'],inplace=True, axis=1 )\nX = train_Feature.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6773c19c122ce8256f55b1d45c6c0114abbc2cf2"},"cell_type":"code","source":"# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0,\n                             n_jobs =-1)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, train.columns[indices[f]], importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94d09d495087b07e5037d874d0301734fa0931c2"},"cell_type":"markdown","source":"#### I will eliminate the column that didn't represent much importance feature, in this case is elimbasu5 That means how they dispose their garbage."},{"metadata":{"trusted":true,"_uuid":"9e5cb4da189ecb6d147f79726c8dded73085d589"},"cell_type":"code","source":"columns_to_drop3 = ['elimbasu5']\ntrain.drop(columns_to_drop3, inplace=True, axis=1)\ntest.drop(columns_to_drop3, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ca066c1f7948fc2ef23285ae0abaf51c778e12c"},"cell_type":"markdown","source":"Right now we now which columns has more importance in the modeling section: \n1. feature meaneduc (0.026576)\n2. feature SQBmeaned (0.022418)\n3. feature dependency (0.021983)\n4. feature SQBhogar_nin (0.021325)\n5. feature hogar_nin (0.020729)\n6. feature cielorazo (0.019575)\n7. feature r4t1 (0.019300)\n8. feature qmobilephone (0.019253)\n9. feature SQBovercrowding (0.019142)\n10. feature overcrowding (0.018431)"},{"metadata":{"_uuid":"7520a4b447ff0054299279a121033d4c61a5108f"},"cell_type":"markdown","source":"#### Analysis by correlation"},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true,"_uuid":"55664c214233681418d9e3752f49b45b2e4d3e83"},"cell_type":"code","source":"f, ax = plt.subplots(figsize = (138,138))\nsns.heatmap(train.corr(),annot= True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce526bd9b61e1b0f38acb034f8737f2c7b496566"},"cell_type":"markdown","source":"We are dealing with a lot of columns obviously visualize the correlation is difficult so we will use a function to drop the columns higly correlated"},{"metadata":{"trusted":true,"_uuid":"7f1dea9eccb71e79a700a36573eb134dfe8368e8"},"cell_type":"code","source":"def trimm_correlated(df_in, threshold):\n    df_corr = df_in.corr(method='pearson', min_periods=1)\n    df_not_correlated = ~(df_corr.mask(np.tril(np.ones([len(df_corr)]*2, dtype=bool))).abs() > threshold).any()\n    un_corr_idx = df_not_correlated.loc[df_not_correlated[df_not_correlated.index] == True].index\n    df_out = df_in[un_corr_idx]\n    return df_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43c51b6dd1fff5225d87b2a0f428c2d03c854302"},"cell_type":"code","source":"y = train['Target'].values\ntrain.drop(['Target'],inplace=True, axis=1 )\ntrain = trimm_correlated(train, 0.95)\ntest = test[train.columns]\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0b7a29799cb781c57986d1a97e19d6e1c00eead"},"cell_type":"markdown","source":"Data is now without highly correlated columns."},{"metadata":{"_uuid":"08f5ac3326dbe6815c7593537a6fec6d5a4cd412"},"cell_type":"markdown","source":"### Pre - modeling\n"},{"metadata":{"trusted":true,"_uuid":"5fa0ad548a01d097b437eb9299d9030fe4d98d71"},"cell_type":"code","source":"X = train.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28b8b9d89eea8e2f02e3aed776cf4b6c83a99b1b"},"cell_type":"markdown","source":"For some quick modeling. \nReference:\nhttps://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"},{"metadata":{"trusted":true,"_uuid":"905505a327c70a8bdae334f8efac69d8ce939dc7"},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04dea1cceb5e3f954a3fb1785ac26d1d016d2fd0"},"cell_type":"code","source":"def evaluate(predictions, test_features, test_labels):\n    accuracy = accuracy_score(test_labels, predictions)\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    # Compute confusion matrix\n    cnf_matrix = confusion_matrix(test_labels, predictions)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=['1','2','3','4'],\n                          title='Confusion matrix, without normalization')\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"089a38f9661228a39efdb22ad4bc475cee33d555"},"cell_type":"code","source":"def runTestingModel(model, X, y, n_folds ,param_grid):\n    # Instantiate the grid search model\n    grid_search = GridSearchCV(estimator = model, param_grid = param_grid, \n                              cv = n_folds, n_jobs = -1, verbose = 2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    # Fit the grid search to the data\n    grid_search.fit(X_train, y_train)\n    print(grid_search.best_params_)\n    best_grid = grid_search.best_estimator_\n    predictions = best_grid.predict(X_test)\n    grid_accuracy = evaluate(predictions, X_test, y_test)\n    return best_grid, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f47e364ed478b27e088eafcbd632f3a0f16188f"},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ff05d7045a3eaa8f00f554d35f5c8fb9c116329"},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5c39e89b0180b97eba43e9e8daca8feaa7b7485"},"cell_type":"code","source":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'n_estimators': [100, 200, 300, 1000]\n}\nrf = RandomForestClassifier()\nmodel, predictions  = runTestingModel(rf, X, y, 3, param_grid)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"386323faa1ba75be89c32aad2a0962995aa6c74d"},"cell_type":"code","source":"catColumns =[]\nquanColumns =[]\nfor col in train.columns:\n    if len(train[col].unique()) < 15:\n        catColumns.append(col)\n    else:\n        quanColumns.append(col)\nprint(quanColumns)            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bb8acdb39fd434fc71f726b852f7e34ac5a31b6"},"cell_type":"code","source":"catColumns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e108e5a12011900b05f7b93c7c2790c8c220cd2e"},"cell_type":"markdown","source":"Closer look at some columns"},{"metadata":{"trusted":true,"_uuid":"7625f897d77bab43fc5160a12d38d048b8372c36"},"cell_type":"code","source":"print(train['escolari'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f31ea19cb9c6d8a36728e635a361a8b990ba06db"},"cell_type":"code","source":"print(train['overcrowding'].unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28afffc0b9c59d8705bfac0c0dc06595dcafa242"},"cell_type":"markdown","source":"Overcrowding is fine to be quantitaty. "},{"metadata":{"trusted":true,"_uuid":"3215b9a556ea73f83cad47a1b2d84eb11d21f8dd"},"cell_type":"code","source":"print(train['SQBhogar_nin'].unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05b868ce2608ed71cfd1d5aad35823a33a4f38ed"},"cell_type":"markdown","source":"SQBhogar_nin is not a category"},{"metadata":{"trusted":true,"_uuid":"749069110da08e635f186aefe7df2affd874c972"},"cell_type":"code","source":"catColumns.remove('SQBhogar_nin')\nquanColumns.append('SQBhogar_nin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"331c836427e76cab1ff133cbc00c290d91bdc2bd"},"cell_type":"code","source":"def age_buckets(x): \n    if x < 15: return 1\n    elif x < 30: return 2\n    elif x < 40: return 3\n    elif x < 50: return 4\n    else : return 5\n\ntrain['age'] = train.age.apply(age_buckets)\ntest['age'] = test.age.apply(age_buckets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e22879e781f00e7e855b3285386656bd77fb592"},"cell_type":"code","source":"train['age'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f55ce02667384e8dd4417ff886d4ce89c0345eaf"},"cell_type":"code","source":"print(catColumns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cde673f47fa439ffd0fc2eef607699317c31407"},"cell_type":"markdown","source":"No let's do a One Hot label encoding. \n\nhttps://www.ritchieng.com/machinelearning-one-hot-encoding/\nI will need to combine to solve columns difference"},{"metadata":{"trusted":true,"_uuid":"d847879437dc6228f9b27b6f98496267fae63d0b"},"cell_type":"code","source":"complete_df = pd.concat([train, test], keys=[0,1])\nprint(test.shape)\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c628c50c7ac20bc484f1bf740050f891b11e5e58"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder()\nenc.fit(complete_df[catColumns])\n\nonehotlabels = enc.transform(complete_df[catColumns]).toarray()\n\nonehotlabels_Train = onehotlabels[:9557]\nonehotlabels_Test = onehotlabels[9557:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bd6182cc9aeb919b608260f68ed0adffa175337"},"cell_type":"code","source":"onehotlabels_Train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f7ccee8ed9296324f0e12c953344c8dfb20f577"},"cell_type":"markdown","source":"Rescale Data for rest of columns"},{"metadata":{"trusted":true,"_uuid":"ddeb6ccfea3189fd240febc2f789bc7e69aaf711"},"cell_type":"code","source":"listType = list(train[quanColumns].dtypes)\nlistType","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46f73ab657fad7532321bb28ea433b428a5e3d54"},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5328d69d4ac8ddf8a3843505b67ffc0c5d374a44"},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_Test = scaler.fit_transform(test[quanColumns])\nrescaledX_Train = scaler.fit_transform(train[quanColumns])\nprint(onehotlabels_Train.shape)\nprint(rescaledX_Train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24d800654e990eda7d88061f510251919f3aaa2f"},"cell_type":"code","source":"X = np.concatenate((onehotlabels_Train ,rescaledX_Train),axis=1)\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdcada1bb2404c6a0595ec3d7b452fd20d26a3da"},"cell_type":"code","source":"X_test = np.concatenate((onehotlabels_Test ,rescaledX_Test),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21773d8d69492c9b1dd369b4609500a0e933bc13"},"cell_type":"markdown","source":"Save results for future modeling or analysis: \n\nX_train, y, test, id_test"},{"metadata":{"trusted":true,"_uuid":"6ab07e460f124255f661ebcdcd22d4ceac820bb5"},"cell_type":"code","source":"import pickle\nf = open('Variables.pckl', 'wb')\npickle.dump([X, y, X_test,id_test], f)\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58189b3f6bf1129483851ece7488721bbe4a13dc"},"cell_type":"code","source":"import pickle\n\nf = open('Variables.pckl', 'rb')\nX, y, X_test,id_test = pickle.load(f)\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d3f483c1338db260ec0241e0cbd346e969992f8","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100,220,500],\n    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n}\nrf = RandomForestClassifier()\nmodel, predictions = runTestingModel(rf, X, y, 2, param_grid)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c31096a061caef7bf39f23d1174be6a7621ddf0"},"cell_type":"markdown","source":"KNN MODEL\n"},{"metadata":{"trusted":true,"_uuid":"14b389d1eb4a4f97eb8911307aad53bd3947dde1"},"cell_type":"code","source":"k_range = list(range(1, 31))\nparam_grid = dict(n_neighbors=k_range)\nknn = KNeighborsClassifier()\nmodel, predictions = runTestingModel(knn, X, y, 5, param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ac37896a5796bef65409b6398d84d9ee16e0931"},"cell_type":"code","source":"from sklearn import svm\n\nCs = [1, 10,100, 1000]\ngammas = [0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\nsv = svm.SVC(kernel='rbf')    \nmodel, predictions = runTestingModel(sv, X, y, 3, param_grid)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62f86938964c9b57ceb12c732e91b14d32e92db4"},"cell_type":"markdown","source":"As expected from the beginning the testing data of the model follows the trend.  "},{"metadata":{"_uuid":"3259d3571b023f0ecc5eced677213ce537899328"},"cell_type":"markdown","source":"### Submission based on selected target"},{"metadata":{"trusted":true,"_uuid":"8a5227225a8144371f5d60dc232f678c9f316668"},"cell_type":"code","source":"def submit(selected_model, X_train, y, test, id_test):\n    \"\"\"Train and test a model on the dataset\"\"\"\n    model.fit(X_train, y)\n    predictions = model.predict(test)    \n    file = pd.DataFrame()\n    file['Id'] = id_test\n    file['Target'] = predictions\n    file.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db8d4aa98766c27ddc1fc3da8480f8378a02c0a4"},"cell_type":"code","source":"submit(model, X, y, X_test,id_test )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}