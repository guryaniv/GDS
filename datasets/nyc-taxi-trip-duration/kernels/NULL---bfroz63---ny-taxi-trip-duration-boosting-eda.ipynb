{"nbformat_minor": 1, "cells": [{"source": ["import cython\n", "import folium\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import math\n", "import warnings\n", "from IPython.display import display , HTML\n", "import scipy.stats as stats\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.metrics import classification_report\n", "from sklearn.model_selection import cross_val_score\n", "from scipy.spatial.distance import pdist\n", "#from sklearn.metrics.pairwise import euclidean_distances\n", "warnings.filterwarnings('ignore')\n", "# Method to show dataframe without showing index column\n", "def show(df):\n", "    return display(HTML(df.to_html(index=False)))\n", "\n", "# Pearsons number of bins for histogram\n", "def numBins(n):\n", "    return round(1 + (3.322 * (math.log10(n)) ) )\n", "\n", "%matplotlib inline"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": ["## Let's import the data"], "cell_type": "markdown", "metadata": {}}, {"source": ["base = pd.read_csv('train.csv')\n", "base.info()\n", "print('\\n')\n", "print('Unique id size: %i' % base.id.drop_duplicates().size)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["As we can see above, the data has :\n", "<li>None null values\n", "<li>Date fields are stored as strings (pickup_datetime, dropoff_datetime)\n", "<li>Categorical data stored as Object (store_and_fwd_flag)\n", "<li>1458644 registers... 122 MB in memory\n", "<li>Id column does not repeat... can be used as reference to joining data frames"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Strategy for the E.D.A.\n", "\n", "For the exploratory analysis we have to concern about:\n", "<li>Quality of the data\n", "<li>Computing Performance\n", "<li>Digging deep - Info is always good, let the machine decide what to do with it later\n", "<li>Mining...\n", "<li>Coffee break\n", "<li>Keep mining..."], "cell_type": "markdown", "metadata": {}}, {"source": ["So... first of all, treat some data.\n", "\n", "\n", "Let's parse the **dates** and change **store_and_fwd_flag** to category."], "cell_type": "markdown", "metadata": {}}, {"source": ["%%time\n", "base['pickup_datetime'] = pd.to_datetime(base.pickup_datetime) \n", "base['dropoff_datetime'] = pd.to_datetime(base.dropoff_datetime) \n", "base['trip_duration'] = base.trip_duration.astype('int64') \n", "base['store_and_fwd_flag'] = base['store_and_fwd_flag'].astype('category')\n", "base.info()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": false}}, {"source": ["We now have 112 MB memory usage... an enhancement for the dataframe processing (~10MB lighter). \n", "Also, it is easier to work with dates.\n", "\n", "<br>Now we are going to extract features from the datetime columns."], "cell_type": "markdown", "metadata": {}}, {"source": ["%%time\n", "tmp = base[['id','pickup_datetime','dropoff_datetime']]\n", "tmp['week_day_pickup'] = tmp.pickup_datetime.dt.dayofweek \n", "tmp['hour_pickup'] = tmp.pickup_datetime.dt.hour \n", "tmp['day_pickup'] = tmp.pickup_datetime.dt.day \n", "tmp['week_pickup'] = tmp.pickup_datetime.dt.week \n", "tmp['month_pickup'] = tmp.pickup_datetime.dt.month \n", "tmp['week_day_dropoff'] = tmp.dropoff_datetime.dt.dayofweek\n", "tmp['hour_dropoff'] = tmp.dropoff_datetime.dt.hour\n", "tmp['day_dropoff'] = tmp.dropoff_datetime.dt.day\n", "tmp['week_dropoff'] = tmp.dropoff_datetime.dt.week\n", "tmp['month_dropoff'] = tmp.dropoff_datetime.dt.month\n", "tmp.to_csv('date_trips.csv' , sep=';')\n", "del tmp"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true}}, {"source": ["**You may notice that we are going to extract some features and store them in a separate csv file. Later we are going to join the dataframes.**"], "cell_type": "markdown", "metadata": {}}, {"source": ["Now we are going to calculate the geodesic distance between each trip made.<br>"], "cell_type": "markdown", "metadata": {}}, {"source": ["%%time\n", "from geopy.distance import vincenty\n", "def getDistance(df):\n", "    return vincenty(\n", "        ( df['pickup_latitude']  , df['pickup_longitude'] ) , \n", "        ( df['dropoff_latitude'] , df['dropoff_longitude'] ) ).meters\n", "distance_df = base[ ['id','pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']]\n", "distance_df['trip_distance'] = distance_df.apply(getDistance , axis=1)\n", "distance_df[['id','trip_distance']].to_csv('trip_distance.csv' , sep=';')\n", "del distance_df"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["pd.to_datetime(base.pickup_datetime).describe()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true}}, {"source": ["Range date goes from Jan 1 to Jun 30 from 2016. We have 6 months of data."], "cell_type": "markdown", "metadata": {}}, {"source": ["### What we have so far...\n", "<li>We extracted features from the pickups and dropoffs \n", "<li>We extracted the trips distances\n", "\n", "#### What we still need to check:\n", "<li>How many vendors this dataset have?\n", "<li>Are there outliers in coordinates, trip duration or number of passengers?\n", "<li>Which seasons do people use more taxi?\n", "<li>Can we well define zones in our map?\n", "<li>What is the average trip duration? Is there any correlation between trip duration and trip destiny?"], "cell_type": "markdown", "metadata": {}}, {"source": ["## VENDORS"], "cell_type": "markdown", "metadata": {}}, {"source": ["sns.countplot(x='vendor_id' , data=base)\n", "print('Vendors amount:')\n", "base.vendor_id.value_counts()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true}}, {"source": ["## OUTLIERS\n", "\n", "### COORDINATES\n", "Let's plot in the map our boundaries coordinates."], "cell_type": "markdown", "metadata": {}}, {"source": ["#Pickup\n", "pickup_max_long , pickup_min_long = base.pickup_longitude.max() , base.pickup_longitude.min()\n", "pickup_max_lat , pickup_min_lat = base.pickup_latitude.max() , base.pickup_latitude.min()\n", "pickup_coord = base[(base['pickup_longitude'] == pickup_max_long) | (base['pickup_longitude'] == pickup_min_long) | \n", "         (base['pickup_latitude'] == pickup_max_lat) | (base['pickup_latitude'] == pickup_min_lat)]\n", "#Dropoff\n", "dropoff_max_long , dropoff_min_long = base.dropoff_longitude.max() , base.dropoff_longitude.min()\n", "dropoff_max_lat , dropoff_min_lat = base.dropoff_latitude.max() , base.dropoff_latitude.min()\n", "dropoff_coord = base[(base['dropoff_longitude'] == dropoff_max_long) | (base['dropoff_longitude'] == dropoff_min_long) | \n", "         (base['dropoff_latitude'] == dropoff_max_lat) | (base['dropoff_latitude'] == dropoff_min_lat)]"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": ["map=folium.Map(location=[pickup_coord['pickup_latitude'].median(),pickup_coord['pickup_longitude'].mean()],zoom_start=4)\n", "for ix , item in pickup_coord.iterrows():\n", "    folium.Marker([item['pickup_latitude'] , item['pickup_longitude']]).add_to(map)\n", "map"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["We have a lot of <span style=\"color:red\">outliers</span> here... <b>let's clean it up!</b>"], "cell_type": "markdown", "metadata": {}}, {"source": ["#### NORMALIZING COORDINATES"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Rounding the coordinates\n", "%pylab inline\n", "pylab.rcParams['figure.figsize'] = (15 , 5)\n", "fig, ax = plt.subplots(1,4)\n", "sns.distplot(base.pickup_latitude.round(0).unique() , ax=ax[0] , axlabel='pickup_lat')\n", "sns.distplot(base.pickup_longitude.round(0).unique() , ax=ax[1] , axlabel='pickup_lon')\n", "sns.distplot(base.dropoff_latitude.round(0).unique() , ax=ax[2] , axlabel='dropoff_lat')\n", "sns.distplot(base.dropoff_longitude.round(0).unique() , ax=ax[3] , axlabel='dropoff_lon')"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["Let's use the normal distribution to find the dense area and then use 2 deviations to clean it up and see how it looks."], "cell_type": "markdown", "metadata": {}}, {"source": ["##### PICKUP"], "cell_type": "markdown", "metadata": {}}, {"source": ["%%time\n", "base_pickup = base[['id' , 'pickup_latitude' , 'pickup_longitude']].copy()\n", "#Pickup\n", "pickup_mean_lat = base_pickup.pickup_latitude.mean()\n", "pickup_stddev_lat = base_pickup.pickup_latitude.std()\n", "pickup_mean_lon = base_pickup.pickup_longitude.mean()\n", "pickup_stddev_lon = base_pickup.pickup_longitude.std()\n", "\n", "def p_lat_outliers_z_score(df):\n", "    threshold = 3\n", "    z_scores = (df['pickup_latitude'] - pickup_mean_lat) / pickup_stddev_lat\n", "    if np.abs(z_scores) > threshold:\n", "        return 0\n", "    else:\n", "        return 1\n", "\n", "def p_lon_outliers_z_score(df):\n", "    threshold = 3\n", "    z_scores = (df['pickup_longitude'] - pickup_mean_lon) / pickup_stddev_lon\n", "    if np.abs(z_scores) > threshold:\n", "        return 0\n", "    else:\n", "        return 1\n", "    \n", "base_pickup['PICKUP_LAT_SCORE'] = base_pickup.apply(p_lat_outliers_z_score , axis=1)\n", "base_pickup['PICKUP_LON_SCORE'] = base_pickup.apply(p_lon_outliers_z_score , axis=1)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["##### DROPOFF"], "cell_type": "markdown", "metadata": {}}, {"source": ["%%time\n", "base_dropoff = base[['id' , 'dropoff_latitude' , 'dropoff_longitude']].copy()\n", "#Pickup\n", "dropoff_mean_lat = base_dropoff.dropoff_latitude.mean()\n", "dropoff_stddev_lat = base_dropoff.dropoff_latitude.std()\n", "dropoff_mean_lon = base_dropoff.dropoff_longitude.mean()\n", "dropoff_stddev_lon = base_dropoff.dropoff_longitude.std()\n", "\n", "def d_lat_outliers_z_score(df):\n", "    threshold = 3\n", "    z_scores = (df['dropoff_latitude'] - dropoff_mean_lat) / dropoff_stddev_lat\n", "    if np.abs(z_scores) > threshold:\n", "        return 0\n", "    else:\n", "        return 1\n", "\n", "def d_lon_outliers_z_score(df):\n", "    threshold = 3\n", "    z_scores = (df['dropoff_longitude'] - dropoff_mean_lon) / dropoff_stddev_lon\n", "    if np.abs(z_scores) > threshold:\n", "        return 0\n", "    else:\n", "        return 1\n", "    \n", "base_dropoff['DROPOFF_LAT_SCORE'] = base_dropoff.apply(d_lat_outliers_z_score , axis=1)\n", "base_dropoff['DROPOFF_LON_SCORE'] = base_dropoff.apply(d_lon_outliers_z_score , axis=1)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["## Rounding the coordinates\n", "%pylab inline\n", "pickup_coord = base_pickup[(base_pickup['PICKUP_LAT_SCORE'] == 1) & (base_pickup['PICKUP_LON_SCORE'] == 1)]\n", "dropoff_coord = base_dropoff[(base_dropoff['DROPOFF_LAT_SCORE'] == 1) & (base_dropoff['DROPOFF_LON_SCORE'] == 1)]\n", "pylab.rcParams['figure.figsize'] = (15 , 5)\n", "fig, ax = plt.subplots(1,4)\n", "sns.distplot(pickup_coord.pickup_latitude.unique() , ax=ax[0] , axlabel='pickup_lat' )\n", "sns.distplot(pickup_coord.pickup_longitude.unique() , ax=ax[1] , axlabel='pickup_lon' )\n", "sns.distplot(dropoff_coord.dropoff_latitude.unique() , ax=ax[2] , axlabel='dropoff_lat' )\n", "sns.distplot(dropoff_coord.dropoff_longitude.unique() , ax=ax[3] , axlabel='dropoff_lon' )"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true}}, {"source": ["max_long , min_long = pickup_coord.pickup_longitude.max() , pickup_coord.pickup_longitude.min()\n", "max_lat , min_lat = pickup_coord.pickup_latitude.max() , pickup_coord.pickup_latitude.min()\n", "plots = pickup_coord[(pickup_coord['pickup_longitude'] == max_long) | (pickup_coord['pickup_longitude'] == min_long) | \n", "         (pickup_coord['pickup_latitude'] == max_lat) | (pickup_coord['pickup_latitude'] == min_lat)]\n", "map=folium.Map(location=[plots['pickup_latitude'].median(),plots['pickup_longitude'].mean()],zoom_start=12,position='relative')\n", "for ix , item in plots.iterrows():\n", "    folium.Marker([item['pickup_latitude'] , item['pickup_longitude']]).add_to(map)\n", "map"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": false}}, {"source": ["Now we are good to go! We just need to pay atention to one more thing:<br>**Some La Guardia/JFK airports pickups/dropoffs are out of the normalization analysis. We need to put them back. So, we are going to use the airport destination analysis.**"], "cell_type": "markdown", "metadata": {}}, {"source": ["#### Definig La Guardia/JFK airports perimeters"], "cell_type": "markdown", "metadata": {}}, {"source": ["laguardia_zone = [ [40.764219 , 40.783339] , [-73.927234 , -73.853434]] #min/max latitude | min/max longitude\n", "jfk_zone = [ [40.611428 , 40.660023] , [-73.831494 , -73.744098] ] "], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": ["%%time\n", "def isCominFromLaGuardia(df):\n", "    if df['pickup_latitude'] >= laguardia_zone[0][0] and df['pickup_latitude'] <= laguardia_zone[0][1]:\n", "        if df['pickup_longitude'] >= laguardia_zone[1][0] and df['pickup_longitude'] <= laguardia_zone[1][1]:\n", "            return 1\n", "        else:\n", "            return 0\n", "    else:\n", "        return 0\n", "\n", "def isCominFromJFK(df):\n", "    if df['pickup_latitude'] >= jfk_zone[0][0] and df['pickup_latitude'] <= jfk_zone[0][1]:\n", "        if df['pickup_longitude'] >= jfk_zone[1][0] and df['pickup_longitude'] <= jfk_zone[1][1]:\n", "            return 1\n", "        else:\n", "            return 0\n", "    else:\n", "        return 0\n", "    \n", "coming_airport = base[ ['id','pickup_latitude','pickup_longitude'] ]\n", "coming_airport['coming_from_laguardia'] = coming_airport.apply(isCominFromLaGuardia , axis=1)\n", "coming_airport['coming_from_jfk'] = coming_airport.apply(isCominFromJFK , axis=1)\n", "#coming_airport = coming_airport[(coming_airport['laguardia'] == 1) | (coming_airport['jfk'] == 1)][['id','laguardia','jfk']]\n", "coming_airport.to_csv('coming_from_airports.csv' , sep=';')\n", "#del airport_df"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["%%time\n", "def isGoingToLaGuardia(df):\n", "    if df['dropoff_latitude'] >= laguardia_zone[0][0] and df['dropoff_latitude'] <= laguardia_zone[0][1]:\n", "        if df['dropoff_longitude'] >= laguardia_zone[1][0] and df['dropoff_longitude'] <= laguardia_zone[1][1]:\n", "            return 1\n", "        else:\n", "            return 0\n", "    else:\n", "        return 0\n", "\n", "def isGoingToJFK(df):\n", "    if df['dropoff_latitude'] >= jfk_zone[0][0] and df['dropoff_latitude'] <= jfk_zone[0][1]:\n", "        if df['dropoff_longitude'] >= jfk_zone[1][0] and df['dropoff_longitude'] <= jfk_zone[1][1]:\n", "            return 1\n", "        else:\n", "            return 0\n", "    else:\n", "        return 0\n", "    \n", "going_airport = base[ ['id','dropoff_latitude','dropoff_longitude'] ]\n", "going_airport['going_to_laguardia'] = going_airport.apply(isGoingToLaGuardia , axis=1)\n", "going_airport['going_to_jfk'] = going_airport.apply(isGoingToJFK , axis=1)\n", "#going_airport = going_airport[(going_airport['laguardia'] == 1) | (going_airport['jfk'] == 1)][['id','laguardia','jfk']]\n", "going_airport.to_csv('going_to_airports.csv' , sep=';')\n", "#del going_airport"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true}}, {"source": ["Now that we have the going to/coming from airports, we use them as valid coordinates, change their SCORES to 1."], "cell_type": "markdown", "metadata": {}}, {"source": ["df_coordinates = pd.merge(base_pickup,base_dropoff,on='id', how='left')\n", "df_coordinates = pd.merge(df_coordinates,going_airport[['id', 'going_to_laguardia', 'going_to_jfk']],on='id', how='left')\n", "df_coordinates = pd.merge(df_coordinates,coming_airport[['id', 'coming_from_laguardia', 'coming_from_jfk']],on='id', how='left')\n", "del going_airport, coming_airport, base_pickup, base_dropoff"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}}, {"source": ["df_coordinates.loc[((df_coordinates['PICKUP_LAT_SCORE'] == 0) | (df_coordinates['PICKUP_LON_SCORE'] == 0)) & ((df_coordinates['coming_from_laguardia'] == 1) | (df_coordinates['coming_from_jfk'] == 1)), ['PICKUP_LAT_SCORE', 'PICKUP_LON_SCORE']] = 1\n", "df_coordinates.loc[((df_coordinates['DROPOFF_LAT_SCORE'] == 0) | (df_coordinates['DROPOFF_LON_SCORE'] == 0)) & ((df_coordinates['going_to_laguardia'] == 1) | (df_coordinates['going_to_jfk'] == 1)), ['DROPOFF_LAT_SCORE', 'DROPOFF_LON_SCORE']] = 1"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": false, "collapsed": true}}, {"source": ["With all SCORES inserted, we can create a new feature called 'VALID_COORDS', which \"merge\" the scores into one feature, hence avoiding multicollinearity."], "cell_type": "markdown", "metadata": {}}, {"source": ["def valid_coords(df):\n", "    if df['PICKUP_LAT_SCORE'] == 1 and df['PICKUP_LON_SCORE'] == 1 and df['DROPOFF_LAT_SCORE'] == 1 and df['DROPOFF_LON_SCORE'] == 1:\n", "        return 1\n", "    else:\n", "        return 0\n", "df_coordinates['VALID_COORDS'] = df_coordinates.apply(valid_coords,  axis=1)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}}, {"source": ["df_coordinates.drop(['PICKUP_LAT_SCORE', 'PICKUP_LON_SCORE', 'DROPOFF_LAT_SCORE', 'DROPOFF_LON_SCORE'], axis=1, inplace=True)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}}, {"source": ["#saving to a file\n", "df_coordinates.to_csv('final_coord.csv' , sep=';')"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": ["### PASSENGERS"], "cell_type": "markdown", "metadata": {}}, {"source": ["ax = sns.countplot( x='passenger_count' , hue='vendor_id', data=base )"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": false}}, {"source": ["#### NORMALIZING PASSENGER COUNT "], "cell_type": "markdown", "metadata": {}}, {"source": ["print('We have %i trips with no passenger. We will not use that!' % base[base['passenger_count'] == 0].id.size)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true}}, {"source": ["### SEASONS"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Distribution for Date (Visions)"], "cell_type": "markdown", "metadata": {}}, {"source": ["df_date_trips= pd.read_csv('date_trips.csv' , sep=';')\n", "df_date_trips['pickup_datetime'] = pd.to_datetime(df_date_trips.pickup_datetime)\n", "del df_date_trips['Unnamed: 0']"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true}}, {"source": ["We only have 6 months in this dataset... Winter and Spring\n", "\n", "## Violin chart"], "cell_type": "markdown", "metadata": {}}, {"source": ["df = df_date_trips[['week_day_pickup' ,'hour_pickup','month_pickup']]\n", "ax = sns.violinplot(x=\"week_day_pickup\", y=\"hour_pickup\", data=df)\n", "del df"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true}}, {"source": ["We can see clearly that pickups from **19h to 23h** are decreasing as the months pass by, while the pickups from **0h to 5h** are increasing as the months pass by."], "cell_type": "markdown", "metadata": {}}, {"source": ["%%time\n", "def mapHour(df):\n", "    if df['hour_pickup'] < 6:\n", "        return 0 # dawn\n", "    elif df['hour_pickup'] < 12:\n", "            return 1 # morning\n", "    elif df['hour_pickup'] < 18:\n", "            return 2 # evening\n", "    elif df['hour_pickup'] < 24:\n", "            return 3 # night\n", "std = pd.DataFrame(df_date_trips.hour_pickup.drop_duplicates())\n", "std['T_HOUR'] = std.apply(mapHour , axis=1)\n", "model = df_date_trips.set_index('hour_pickup').join(std.set_index('hour_pickup')).reset_index()\n", "\n", "\n"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["def mapSeason(df):\n", "    if df['month_pickup'] < 4:\n", "        return 0 # Winter\n", "    else:\n", "        return 1 # Spring\n", "std = pd.DataFrame(df_date_trips.month_pickup.drop_duplicates())\n", "std['T_SEASON'] = std.apply(mapSeason , axis=1)\n", "model = model.reset_index().set_index('month_pickup').join(std.set_index('month_pickup')).reset_index()  \n", "model.to_csv('date_trips.csv' , sep=';')\n", "del model"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": ["### ZONES"], "cell_type": "markdown", "metadata": {}}, {"source": ["We can define coordinates \"zones\" using K-Means algorithm, using the previous validated coordinates.\n", "\n", "As an arbitrary choice, we (visually) preferred to use 25 clusters. Also, since we just want to label zones to each observation with valid coordinates, we can choose to use the \"pickup\" or \"dropoff\" coordinates. For this study, we choose \"pickup\"."], "cell_type": "markdown", "metadata": {}}, {"source": ["from sklearn.cluster import KMeans\n", "df_cluster_pickup = df_coordinates[df_coordinates['VALID_COORDS'] == 1][['id','pickup_latitude','pickup_longitude']]\n", "kmeans = KMeans(n_clusters=25, random_state=2, n_init=10).fit(df_cluster_pickup[['pickup_latitude','pickup_longitude']])\n", "df_cluster_pickup['PICKUP_CLUSTER'] = kmeans.labels_\n", "df_cluster_dropoff = df_coordinates[df_coordinates['VALID_COORDS'] == 1][['id','dropoff_latitude','dropoff_longitude']]\n", "kmeans = KMeans(n_clusters=25, random_state=2, n_init=10).fit(df_cluster_dropoff[['dropoff_latitude','dropoff_longitude']])\n", "df_cluster_pickup['DROPOFF_CLUSTER'] = kmeans.labels_"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": ["df_cluster = pd.merge(df_cluster_pickup,df_cluster_dropoff,on='id', how='left')\n", "plots_p = df_cluster.sample(df_cluster.pickup_latitude.size)\n", "plt.figure(figsize = (12,10))\n", "for label in plots_p.PICKUP_CLUSTER.unique():\n", "    plt.plot(plots_p.pickup_longitude[plots_p.PICKUP_CLUSTER == label],plots_p.pickup_latitude[plots_p.PICKUP_CLUSTER == label],'.', alpha = 0.3, markersize = 0.3)\n", "#plots_p.to_csv('clusters.csv' , sep=';')\n", "plt.title('NY Clusters')\n", "plt.show()\n", "del plots_p, df_cluster_pickup, df_cluster_dropoff, kmeans, df_coordinates"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["df_cluster.drop(['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'], axis=1, inplace=True)\n", "df_cluster.to_csv('clusters.csv' , sep=';')\n", "del df_cluster"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"language_info": {"mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "version": "3.6.2"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}}