{"cells":[{"metadata":{"trusted":true,"_uuid":"6310a92bb8253b521e09b04b005e551c1c9f2de5","scrolled":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nprint(tf.reduce_mean([[1,1],[2,3]], axis=0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"'''Trains an LSTM model on the IMDB sentiment classification task.\nThe dataset is actually too small for LSTM to be of any advantage\ncompared to simpler, much faster methods such as TF-IDF + LogReg.\n# Notes\n- RNNs are tricky. Choice of batch size is important,\nchoice of loss and optimizer is critical, etc.\nSome configurations won't converge.\n- LSTM loss decrease patterns during training can be quite different\nfrom what you see with CNNs/MLPs/etc.\n'''\nimport pandas as pd\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import SpatialDropout1D, Dropout, Conv1D, MaxPooling1D\nfrom keras.layers import Bidirectional\nmax_features = 20000\nmaxlen = 80  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 1024\n\ndf = pd.read_csv(r\"../input/train.tsv\", sep='\\t')\nX = df['Phrase']\ny = df['Sentiment']\n\ntokenizer = Tokenizer(num_words=max_features)\ny = to_categorical(y)\nx_train, x_test, y_train, y_test = train_test_split(X, y)\ntokenizer.fit_on_texts(x_train)\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\ndef create_ngram_set(input_list, ngram_value=2):\n    \"\"\"\n    Extract a set of n-grams from a list of integers.\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n    \"\"\"\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n\ndef add_ngram(sequences, token_indice, ngram_range=2):\n    \"\"\"\n    Augment the input list of list (sequences) by appending n-grams values.\n    Example: adding bi-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n    Example: adding tri-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n    \"\"\"\n    new_sequences = []\n    for input_list in sequences:\n        new_list = input_list[:]\n        for ngram_value in range(2, ngram_range + 1):\n            for i in range(len(new_list) - ngram_value + 1):\n                ngram = tuple(new_list[i:i + ngram_value])\n                if ngram in token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    return new_sequences\n\nngram_range = 2\nif ngram_range > 1:\n    print('Adding {}-gram features'.format(ngram_range))\n    # Create set of unique n-gram from the training set.\n    ngram_set = set()\n    for input_list in x_train:\n        for i in range(2, ngram_range + 1):\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n            ngram_set.update(set_of_ngram)\n\n    # Dictionary mapping n-gram token to a unique integer.\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features.\n    start_index = max_features + 1\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n    indice_token = {token_indice[k]: k for k in token_indice}\n\n    # max_features is the highest integer that could be found in the dataset.\n    max_features = np.max(list(indice_token.keys())) + 1\n\n    # Augmenting x_train and x_test with n-grams features\n    x_train = add_ngram(x_train, token_indice, ngram_range)\n    x_test = add_ngram(x_test, token_indice, ngram_range)\n    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n\n\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\nmodel_1 = Sequential()\nmodel_1.add(Embedding(max_features, 123))\nmodel_1.add(SpatialDropout1D(0.2))\nmodel_1.add(Dropout(0.25))\nmodel_1.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.2)))\nmodel_1.add(Dropout(0.5))\nmodel_1.add(Dense(5, activation='softmax'))\n\nmodel_1.summary()\n# try using different optimizers and different optimizer configs\nmodel_1.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['categorical_accuracy'])\n\nprint('Train...')\nmodel_1.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=10,\n          validation_data=(x_test, y_test))\nscore, acc = model_1.evaluate(x_test, y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e840ef68dadad594d6ea0578f982f7f6f921b2f","collapsed":true},"cell_type":"code","source":"from keras.layers import GRU\nprint('Build model...')\nmodel_2 = Sequential()\nmodel_2.add(Embedding(max_features, 128))\nmodel_2.add(SpatialDropout1D(0.2))\nmodel_2.add(Dropout(0.25))\nmodel_2.add(Bidirectional(GRU(512, dropout=0.2, recurrent_dropout=0.2)))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(Dense(5, activation='softmax'))\n\n# try using different optimizers and different optimizer configs\nmodel_2.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['categorical_accuracy'])\n\nprint('Train...')\nmodel_2.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=1,\n          validation_data=(x_test, y_test))\nscore, acc = model_2.evaluate(x_test, y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cef0eca6b25968b3f868036925388017733dfc8c","collapsed":true},"cell_type":"code","source":"print('Build model...')\nfrom keras.layers import Average \nfrom keras.layers import TimeDistributed, Lambda, Input, merge\nfrom keras import Model\nfrom keras import backend as K\n\ninput_x = Input((maxlen,))\nx = Embedding(max_features, 128)(input_x)\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(x)\nx = Dropout(0.5)(x)\nx = TimeDistributed(Dense(5, activation='softmax'))(x)\nprint(type(x))\nout = Lambda(lambda x:K.mean(x, 1))(x)\nmodel_3 = Model(input_x, out)\n\n# try using different optimizers and different optimizer configs\n\nmodel_3.summary()\nmodel_3.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['categorical_accuracy'])\n\nprint('Train...')\nmodel_3.fit(x_train, y_train,batch_size=batch_size,epochs=1,validation_data=(x_test, y_test))\nscore, acc = model_3.evaluate(x_test, y_test,batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"608c7c59c0ea3ce8c11fb4fa350dc392fd8cb879","collapsed":true},"cell_type":"code","source":"print('Build model...')\nfrom keras.layers import Average \nfrom keras.layers import TimeDistributed, Lambda, Input, merge\nfrom keras import Model\nfrom keras import backend as K\n\ninput_x = Input((maxlen,))\nx = Embedding(max_features, 128)(input_x)\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(GRU(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(x)\nx = Dropout(0.5)(x)\nx = TimeDistributed(Dense(5, activation='softmax'))(x)\nprint(type(x))\nout = Lambda(lambda x:K.mean(x, 1))(x)\nmodel_4 = Model(input_x, out)\n\n# try using different optimizers and different optimizer configs\n\nmodel_4.summary()\nmodel_4.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['categorical_accuracy'])\n\nprint('Train...')\nmodel_4.fit(x_train, y_train,batch_size=batch_size,epochs=1,validation_data=(x_test, y_test))\nscore, acc = model_4.evaluate(x_test, y_test,batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e1af6f604ac9076f944081ce56ceb568670ac06"},"cell_type":"code","source":"\nfrom keras.layers import add\nfrom keras.layers import Merge\n\ninput_x = Input((maxlen,))\nx = Embedding(max_features, 128)(input_x)\nt = SpatialDropout1D(0.2)(x)\nx = Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(t)\nmodel_t = Model(input_x, x)\n\n# try using different optimizers and different optimizer configs\n\n\n\nt = Flatten()(t)\nin_y = Dense(input_dim=128, output_dim=maxlen)(t)\ny = Activation('softmax')(in_y) # Learn a probability distribution over each  step.\n#Reshape to match LSTM's output shape, so that we can do element-wise multiplication.\ny = RepeatVector(1024)(y)\ny = Permute((2, 1))(y)\nattmodel = Model(input_x, y)\n\nfinalmodel = Sequential()\nfinalmodel.add(Merge([model_t, attmodel], 'mul'))  # Multiply each element with corresponding weight a[i][j][k] * b[i][j]\nfinalmodel.add(Dropout(0.5))\nfinalmodel.add(TimeDistributed(Dense(5, activation='softmax')))\nfinalmodel.add(Lambda(lambda x:K.mean(x, 1)))\n\n\nfinalmodel.summary()\nfinalmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['categorical_accuracy'])\n\nfinalmodel.fit(x_train, y_train,batch_size=batch_size,epochs=3,validation_data=(x_test, y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2bb4e10a59beac43cab0e3b4d1f412e98937aca","collapsed":true},"cell_type":"code","source":"#ensemble\ns = model_1.predict(x_test, verbose=1,  batch_size=batch_size)\nfor model in [model_2, model_3, model_4]:\n    s += model.predict(x_test, verbose=1, batch_size=batch_size)\nx = np.argmax(s,1)\n\nnp.mean(np.equal(x,np.argmax(y_test, 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"461cb8821b78f68105311be325b7802b68e6b6af","collapsed":true},"cell_type":"code","source":"testdf = pd.read_csv('../input/test.tsv', sep='\\t')\ntest = testdf['Phrase']\ntest = tokenizer.texts_to_sequences(test)\ntest = sequence.pad_sequences(test, maxlen=maxlen)\n#ensemble\ns = model_1.predict(test, verbose=True,  batch_size=batch_size)\nfor model in [model_2, model_3, model_4]:\n    s += model.predict(test, verbose=True,  batch_size=batch_size)\nx = np.argmax(s,1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72791b33efe5b5557dd595b9f2d0c1d3dbc83e37","collapsed":true},"cell_type":"code","source":"\nprint(x)\n\nout = pd.DataFrame()\nout['PhraseId'] =testdf['PhraseId']\nout['Sentiment'] = x\nout.to_csv('rs.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0faf055101fc5bf2bfea06c0e603c89d356034b1","collapsed":true},"cell_type":"code","source":"!cat rs.csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}