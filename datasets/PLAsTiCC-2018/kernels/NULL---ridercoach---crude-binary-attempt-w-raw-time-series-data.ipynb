{"cells":[{"metadata":{"_uuid":"3591f2de1029ded6d38857da57fbaec4772d3bc6"},"cell_type":"markdown","source":"I am new to machine learning and this is my second Kaggle competition.  My purpose in this kernel was just to get a working model, even though it is very crude and simple.  I used a Keras LSTM model, and for this first pass, just did a binary classifier of the most prominent class (90) and everything else.  I used only the raw flux readings, and was ruthless in throwing away data in order to get a nice, tidy set.  I ran this first pass of the model with only 200 observations (each observation having readings from passbands 1 through 5), and got an accuracy of 73%.\n\nMy  next step will be to expand this binary approach to one that handles all the classes, so that I can generate my first submission (which again will be very simplistic, but it will be a start.)  After that I will work on feature engineering to try to improve the accuracy."},{"metadata":{"trusted":true,"_uuid":"7a9f13ec15371b5fbe1d46e9e0cb5ddde3e1d1ab"},"cell_type":"code","source":"# here are the available input files for this competition\n!ls -l ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5068bb1439d4eced0bb7a29d23880d667313c210"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aaa4b66f34182df155c108ff81b5641c0da14ca"},"cell_type":"markdown","source":"# Initial Training Data Tidying"},{"metadata":{"trusted":true,"_uuid":"2288901c83c8c94becbd8de0272914fd8dcb5dd5"},"cell_type":"code","source":"# read the raw training data\ndf = pd.read_csv('../input/training_set.csv')\n\n# I have found from other analysis that the measurement for passband 0 \n# is only non-null when ALL the other passband measurements in the \n# same ostensible \"observation\" ARE null; we might be able to fix this \n# by adjusting the date binning, but for now let's just drop PB 0\ndf = df[df['passband'] != 0]\n\n# digitize the dates so that the different passband measurements \n# that are close together in time can hang together as one observation\nbins = np.array(np.arange(59000.0, 61000.0, 1.0, dtype=np.float64))\ndf['date_bin'] = np.digitize(df['mjd'], bins)\n\n# take just the columns needed for a first very simple attempt\ndf = df[['object_id', 'date_bin', 'passband', 'flux']]\n\n# pivot the passband readings into columns; we are working toward \n# input for a Keras LSTM model; the date digitizing has not worked \n# perfectly, but there are less than 2000 duplicate indexes in almost \n# a million rows, so just delete them\ndf = df.set_index(['object_id', 'date_bin', 'passband'])\ndf = df[~df.index.duplicated(keep='first')]\ndf = df.unstack(level=-1)\n\n# flatten the structure of the table\ndf = df.reset_index()\ndf.columns = ['object_id', 'date_bin', 'pb1', 'pb2', 'pb3', 'pb4', 'pb5']\n\n# we can drop all rows with any nulls and still have 100K rows, \n# so for now let's do it; we may have to revisit this is we don't \n# have a proper mix of target values left\ndf.dropna(inplace=True)\n\n# pull in the target values\ndfm = pd.read_csv('../input/training_set_metadata.csv')\ndf_tgt = dfm[['object_id', 'target']]\ndf = df.merge(df_tgt, on='object_id', how='left')\n\n# for the 1st attempt I would like to just try binary classification \n# between the most numerous class (90) and all the others, so make new target col\ndf['tgt90'] = (df['target'] == 90).astype(int)\n\n# check what we have\nprint('{} rows, {} IDs, {} tgts'.format(len(df), df['object_id'].nunique(), df['target'].nunique()))\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a836301c6fea841ec0e155c7e302da95b5a63bd"},"cell_type":"markdown","source":"# Choosing Training and Test Sets"},{"metadata":{"trusted":true,"_uuid":"3adfee546a36663fb99bcf203dba0271c586a707"},"cell_type":"code","source":"# roll up to object ID level; some of the aggregate values are \n# just for sanity checking\naggs = {'target':['min', 'max', 'size'], 'date_bin':['min', 'max'], 'tgt90':['min', 'max']}\nnew_cols = ['tgt-min', 'tgt-max', 'tgt-size', 'dbin-min', 'dbin-max', 'tgt90-min', 'tgt90-max']\ndf_agg = df.groupby('object_id').agg(aggs)\ndf_agg.columns = new_cols\ndf_agg['tgt-diff'] = df_agg['tgt-max'] - df_agg['tgt-min']\ndf_agg['dbin-diff'] = df_agg['dbin-max'] - df_agg['dbin-min']\n\n# about 25% of the objects have less than 50 timesteps so \n# lets discard them\ndf_agg = df_agg[df_agg['tgt-size'] >= 50]\n\nprint(df_agg.shape)\nprint(df_agg['tgt90-min'].value_counts())\ndf_agg.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84368515c601944b3f9c02edbb966a8544d04f5d"},"cell_type":"code","source":"# make lists of object IDs to use for extracting sets of observations\n\ndf_train, df_test = train_test_split(df_agg.sample(200), test_size=0.3, random_state=42)\nprint('train: {}, test: {}'.format(len(df_train), len(df_test)))\nprint(df_train['tgt90-min'].value_counts())\nprint(df_test['tgt90-min'].value_counts())\nidlist_train = df_train.index.values\nidlist_test = df_test.index.values\nprint('{:5d} training IDs: {}...'.format(len(idlist_train), idlist_train[:5]))\nprint('{:5d} testing  IDs: {}...'.format(len(idlist_test), idlist_test[:5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c833ed2e517745154e43dd98af8f2a771a056e12"},"cell_type":"code","source":"# use ID lists to create numpy arrays for input to Keras model\n# uses DF created in tidying section above\n\ntimesteps = 50\nfeatures = 5\n\ndef make_input_arrays_from_list(l):\n    X = np.zeros((len(l), timesteps, features))\n    y = np.zeros((len(l)))\n    dfx = df[df['object_id'].isin(l)]\n    for i, (oid, dfg) in enumerate(dfx.groupby('object_id')):\n        X[i] = (dfg.iloc[:50]).values[:, 2:7]\n        y[i] = dfg.iloc[0,-1]\n        \n    print('\\nX,y shapes: {} {}'.format(X.shape, y.shape))\n    print('y: {}, sum={}'.format(y[:10], sum(y)))\n    print('X[0, :2]:')\n    print(X[0, :2])\n    print('X[0, -2:]:')\n    print(X[0, -2:])\n    print('X[-1, :2]:')\n    print(X[-1, :2])\n    print('X[-1, -2:]:')\n    print(X[-1, -2:])\n    return X, y\n\ntrain_X, train_Y = make_input_arrays_from_list(idlist_train)\ntest_X, test_Y = make_input_arrays_from_list(idlist_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc9571ead4edc2682f242cf800c1845122b96845"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true,"_uuid":"ad1c41cb511f5ee0aea286b85664ef8332c2ccb2"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom keras.preprocessing import sequence\n\nepochs = 100\nbatch_size = 10\ntime_steps = 50\nfeatures = 5\n\n# build LSTM layers\nmodel = Sequential()\nmodel.add(LSTM(100, dropout=0.2, input_shape=(time_steps, features)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(train_X, train_Y, validation_data=(test_X, test_Y), epochs=epochs, batch_size=batch_size)\n\n# score model and log accuracy and parameters\nscores = model.evaluate(test_X, test_Y, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"859e2cd6733a777d0cc399283054f2de72d9773b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}