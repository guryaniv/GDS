{"metadata": {"language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.1"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"collapsed": false, "_cell_guid": "74c441ed-ff03-4ecc-be2b-f8016eb95419", "_uuid": "4a8b2fefcb801828d3540ecec8aa9d09bdbd63db", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "##Data analysis, feature engineering, and data cleaning, with some bonus performance tips##", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "7bd8e5f5-0efb-4110-a081-80bf1c91d384", "_uuid": "5a6c5630a7117110c18179b811f0323d119f154b", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "We will load the data and do some analysis, feature engineering and data cleaning. \n\nThe features will include measures of trip speed, direction, and alignment with the New York city street grid, as well as breaking out the timestamp into more useful components. \n\nOn the way, we'll learn how to avoid a common pandas performance issue, and find some interesting groups of outliers.", "cell_type": "markdown"}, {"metadata": {"_cell_guid": "70b2b34a-b666-412b-a46e-e8921370f419", "_uuid": "5e3202d8b18d39b691cfedbcec7c7b06cf358707", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "053d5af6-d0e6-44a3-821d-c8d58d4dcefe", "_uuid": "81ff9644db5b68686cece8d8412fefa8f5f1598d", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "Import various utilities for later", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "9344bbc9-443b-4aa1-85b7-45f8c5c05d4c", "_uuid": "3d1a9b5ac0dd5741f0ed14825dcadd4729081bb8", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "# date/time maths\nimport datetime as datetime\n\n# plotting tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# general maths\nimport math as math", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "cd00b400-dd2d-451d-a98d-ca5ccf383573", "_uuid": "6ddbfb8fbe5179cab7cf0219dd766cb7ce61cfa3", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data = pd.read_csv(\"../input/train.csv\", parse_dates=['pickup_datetime','dropoff_datetime'])\ntrain_data.head()", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "ee2dce47-6057-4257-8690-dad94f363d74", "_uuid": "8cb06e0d86f0c2ccb7f9997cd0fb92aa5ba94156", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "Get a summary of the data. Using `include='all'` means the datetime columns are included; by default non-numerics are excluded.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "9a0611c2-30d6-4855-884f-8a8fc42891a2", "_uuid": "fdb33d1f3d8618441056f9a862ad2a1875f3bd03", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data.describe(include ='all')", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "d7686c50-2481-42f4-86a8-4f9138a2822f", "_uuid": "45a43cf84218e42fb44e7805c8274ac0cd876065", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "**Interesting points**\n\n - At least one trip lasted a second! We should investigate outliers at this lower end of trip_duration.\n - At least one trip lasted around 350,000 seconds, or nearly 100 hours! We should investigate outliers at this higher end of trip duration.\n - The data ranges for the first 6 months of 2016. Any supplementary datasets we gather need to cover this time range.\n - ids are unique. One suspects they won't be of great value.\n - Vendor ID is either 1 or 2, probably distinguishing between two operators.\n - It looks like there aren't missing values as the numbers of entries on the top line are consistently identical, but let's check that to be sure.\n - Passenger count goes up to 9, and is generally low (over 50% of rides are for zero or 1 passenger)\n - Passenger count can be zero. One suspects this is user error by the cabbie, and may line up with the very short trip durations. Let's investigate with those outliers.\n - We have max and min values for latitude and longitude; we should look at outliers there too, but we will need a way to convert those to something we have a better understanding of.\n", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "a9512fcb-64cc-436c-9eee-d2e4b1bb5211", "_uuid": "19c37e0af85f71a2f7229049633a6b7b0ee92b1c", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "# Check for null entries\ntrain_data.isnull().sum()", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "86383e27-8bb5-4d20-bc33-7b1df1de3715", "_uuid": "dc56a46c97010ae61c4d1a388140638f0ebd5efb", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "Happily there are no missing data points to impute.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "1bc749bf-9626-4a27-bfbd-39dfd101ecb1", "_uuid": "40e8f1106f5c7d6fb63cee43fc5b2ede315e0955", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "2e975c2a-58b8-41d8-98e1-eeb1fb2d0c2a", "_uuid": "c19f9c5a68b02360bae5d670b01434f818302718", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "## Feature engineering ##\nBefore we examine the outliers, and look to run algorithms over it, let's think about any features that may help, and that are available to extract from the data provided.\n\n### Time of day ###\nWe want to model rush hour, people coming from from clubs at weekends, people heading from work to the airport on Friday evening, and so on. We need time of day, as we expect some cyclical behaviour here.\n\n### Day of week ###\nTraffic patterns will change daily due to work habits, people leaving for the weekend, and so forth. \n\n### Distance travelled and approximate speed ###\nWe can convert the longitude and latitude to a measure of point to point distance, and similarly get a measure of speed \"as the crow flies\".\n\nThe speed can't be used in our model, as it relies on the duration which we are trying to predict. However we can use it as a sanity check to filter out undesirable outliers from our training data. Unrealistically high speeds likely mean bad readings; unrealistically low speeds over long time periods probably mean someone has hired the cab for a protracted period on retainer. These latter cases will, I anticipate, be unpredictable. Better to just accept we will have some test error from such cases and exclude then than have them pollute our model for the cases which we can reasonably predict.\n\n### Day of month/month/date ###\nWe clearly want to separate out the date in some way. How best to do this? We might decide that the day of the month might have cyclical behaviour as we expect the day of the week to. I can't see a good reason for this, but might try to explore it graphically to see if there is some legitimacy to it.\n\nAs we have 1.45M rows, spread over approx 180 days (half a year), we have around 8k data points/day. That's enough for a good ML algorithm to work with without breaking it down further. So, instead of using day and month separately, I prefer for now to have the day and month combined into a date.\n\nTo make this easy for regression algorithms to work with we will represent date as an offset in days from the 1st Jan 2016. \n\n### Direction of travel ###\nManhattan's streets are laid out on a grid.  Traffic going one way in the morning may well reverse flow in the evening\nTraffic aligned with the grid may well have a more direct route. So perhaps direction of travel and/or grid alignment may be useful features. \n\nWhat direction do the streets go in? This link has a good analysis: http://www.charlespetzold.com/etc/AvenuesOfManhattan/\n\nIt turns out the \"north/south\" streets actually run 29 degrees clockwise of north/south.\n\n### Grid alignment ###\nHow close are we to running a trip in direct alignment with the Manhattan grid? Is our journey aligned with the Manhattan avenues or streets, or is the trip more along the diagonal of the grid? This is something we can extract from the data.\n\n### Gridwise distance ###\nImagine the start and end points of a journey aligned with the grid, so the journey doesn't require any turns. The taxi will only need to cover the point to point distance. \nConversely, if the journey is along the grid diagonal, the journey would have to go along two sides of a triangle, so the required distance would be longer.\nIn general, if we imagine the journey is on a grid aligned with that of Manhattan, the gridwise distance is the distance covered along the two sides of a right angled triangle where the hypotenuse is between the start and end points of the journey.\nThis may be a useful feature as it will help any algorithm factor in alignment with the grid.\n\n## Other data sets ##\nOther things I would like to have information on include weather, unusual traffic issues, and major events such as sport home games. These can't be extracted from the data set we have, but perhaps we can find them elsewhere.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "6f763537-c213-4b6c-a238-19f8d66cd813", "_uuid": "8d615c384cbfe37b195afeda7dabfb0385894b40", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "## A \"gotcha\" on generating features in pandas ##\nOne natural way, coming into python, to generate these features, is to:\n- Write a function that operates on a row\n- Apply that function to all rows using dataframe.apply.\nThis is give in one example below, and the way I first tried (being quite new to Python,). What with various bits of shunting data around, inferring, and so forth, it can be very slow. It seems particularly bad when there are datatype conversions, as there are with the direction function below.\n\nA faster approach is to use a vectorised version, which takes the input columns of the dataframe (ie pandas series objects) and returns a pandas series object for the new column. \n\nI have coded both approaches below so you can try for yourself. The slow way using apply takes maybe 20 minutes on my system; the vectorised version a few seconds!\n\nSee https://tomaugspurger.github.io/modern-4-performance.html for more information.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "4f6259a1-05ef-4316-a9b6-7ad86bbb7266", "_uuid": "02b24829b1c3e6039f425fab3ffe0c9454a500a7", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "d2c061df-f9df-4a9d-be9b-6b6eb2a76100", "_uuid": "ff3ce233de468c08a8e5da4ee08ec908b2a57a7d", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "# Let's get some features!\n# Showing that apply works - although it's slow. \ntrain_data['day_of_week'] = train_data['pickup_datetime'].apply(lambda dt: dt.weekday())\ntrain_data['time_of_day'] = train_data['pickup_datetime'].apply(lambda dt: dt.time())\n\n", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "7c464400-d19a-4f93-a391-f20b7d26da72", "_uuid": "3235348e055de02a8a59cb8f18c1a5e52f5a0686", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "# WARNING - SLOW VERSION WHICH OPERATES ON DATAFRAME ROWS. INCLUDED ONLY SO YOU CAN TRY OUT \n# JUST HOW SLOW THIS IS BY COMPARISON \n\n# At 40\u00b0 north or south*, the distance between a degree of longitude is 53 miles (85 kilometers).\n# We are pretty much 40 degrees North, so 85 Km will do.\n# Each degree of latitude is approximately 69 miles (111 kilometers) apart.\n# ref: https://www.thoughtco.com/degree-of-latitude-and-longitude-distance-4070616     \n\nLAT_SCALE_METRES = 111000\nLONG_SCALE_METRES = 85000\n\ndef latToM(lat) :\n    return lat * LAT_SCALE_METRES\n\ndef longToM(lng) :\n    return lng * LONG_SCALE_METRES\n\ndef distanceInM( row ) :\n    longDiffm = longDiffM(row)\n    latDiffm = latDiffM(row)\n    return np.sqrt( longDiffm * longDiffm + latDiffm * latDiffm)\n\ndef calcSpeedKmh( metres, secs) :\n    # km/hour = (metres / 1000) / (secs/ (60*60) )\n    return (3600 * metres) / (secs * 1000)\n\ndef speedInKmh (row) :\n    return calcSpeedKmh(row['distance_in_metres'], row['trip_duration'])       \n\ndef longDiffM(row) :\n    return  longToM( abs(row['dropoff_longitude'] - row['pickup_longitude']) )\n\ndef latDiffM(row) :\n    return latToM( abs(row['dropoff_latitude'] - row['pickup_latitude']))\n\ndef daysFromNewYear2016(dt) :\n   nyDay = datetime.date(2016,1,1)\n   return (dt.date() - nyDay ).days\n\ndef direction( row ) :\n    # longitude positive is e->w (as we are west of London, further west = higher value for longitude)\n    # latitude is s->n so ie higher latitude is further south as we are north of eqator.\n    # tangent = opposite/adjacent, ie lat dist / long dist    \n    # This function returns, for example:\n    # Heading westwards = 0, southwards = 1, eastwards = 2, northwards = -1.\n    # Heading northwest = -0.5, northeast = -1.5, southeast = 1.5, southewst = 0.5\n    # The closer the absolute value is to 1, the closer we are to going n/s\n    return 2 * math.atan2( (row['dropoff_latitude'] - row['pickup_latitude']), (row['dropoff_longitude'] - row['pickup_longitude']))/math.pi\n\n# 0 for going directly e/w or n/s\n# 0.5 for a diagonal movement\n# other values in between as appropriate.\n# likely a more elegant way to do this\ndef diffFromGridDirection ( row ) :\n    nsDiff = abs(abs(row['direction']) - 1)\n    ewDiff = min(abs(row['direction']), abs(abs(row['direction']) - 2))\n    return min(ewDiff, nsDiff)\n                \n            ", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "529f407d-30e4-44a5-87de-d98037cb4e2b", "_uuid": "71ef5131276fec5b315d6212ecab9a07f900f8e4", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "# At 40\u00b0 north or south*, the distance between a degree of longitude is 53 miles (85 kilometers).\n# We are pretty much 40 degrees North, so 85 Km will do.\n# Each degree of latitude is approximately 69 miles (111 kilometers) apart.\n# ref: https://www.thoughtco.com/degree-of-latitude-and-longitude-distance-4070616     \n\n# Vectorised version.\n# See https://tomaugspurger.github.io/modern-4-performance.html for more info on why this is \n# a big speed improvements over apply\n\n\nLAT_SCALE_METRES = 111000\nLONG_SCALE_METRES = 85000\n\ndef latToMVec(lat) :\n    return lat * LAT_SCALE_METRES\n\ndef longToMVec(lng) :\n    return lng * LONG_SCALE_METRES\n\ndef distanceInMVec( long1, long2, lat1, lat2 ) :\n    longDiffm = longDiffMVec(long1, long2)\n    latDiffm = latDiffMVec(lat1, lat2)\n    return np.sqrt( longDiffm * longDiffm + latDiffm * latDiffm)\n\ndef calcSpeedKmhVec( metres, secs) :\n    # km/hour = (metres / 1000) / (secs/ (60*60) )\n    return (3600 * metres) / (secs * 1000)\n\ndef speedInKmhVec ( metres, duration) :\n    return calcSpeedKmh(metres, duration)       \n\ndef longDiffMVec(long1, long2) :\n    return  longToMVec( abs(long2-long1) )\n\ndef latDiffMVec(lat1, lat2) :\n    return latToMVec( abs(lat2-lat1))\n\ndef daysFromNewYear2016Vec(dt) :\n   nyDay = datetime.date(2016,1,1)\n   return [(d.date() - nyDay ).days for d in dt]\n\ndef directionVec( long1, long2, lat1, lat2 ) :\n    # longitude positive is e->w (as we are west of London, further west = higher value for longitude)\n    # latitude is s->n so ie higher latitude is further south as we are north of eqator.\n    # tangent = opposite/adjacent, ie lat dist / long dist    \n    # I've given results in degrees as it's easier to sanity check the numbers are right.\n    # This function returns, for example:\n    # Heading westwards = 0, southwards = 90, eastwards = 180, northwards = -90.\n    # Heading northwest = -45, northeast = -135, southeast = 135, southewst = 45\n    # The closer the absolute value is to 1, the closer we are to going n/s\n    lngDiff = long2 - long1\n    latDiff = lat2 - lat1\n    atansFunc = np.vectorize( lambda y,x : 180 * math.atan2(y,x)/math.pi )\n    atans = atansFunc(latDiff, lngDiff)\n    #atans = pd.Series([math.atan2(y,x) for y,x in zip(latDiff, lngDiff)])\n    #return 180 * atans/math.pi\n    return atans\n    \n    \ndef shiftAntiClockwise( direction ) :\n    newDirection = direction + 29\n    if newDirection > 180 :\n       newDirection = 180 - newDirection\n    return newDirection\n\n\ndef gridDiff( direction ) :\n    # Work out how many degrees a direction differs from n/s or e/w.\n    # First rotate negative directions about e/w to map onto a semicircle\n    diff = direction % 90\n    \n    if diff < 45 :\n       return diff\n    else :\n       return 90 - diff\n\n\n# 0 for going directly e/w or n/s\n# 45 for a perfect diagonal.\ndef diffFromGridDirectionVec ( direction ) :\n    # To make the maths easier, let's rotate our direction 29 degrees anticlockwise.\n    # Then we check how close the result is to north/south or east/west alignment.\n    \n    # We use vectorise for performance, though not strictly necessary here.See:\n    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html\n    # https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html    \n    vFuncShift = np.vectorize( shiftAntiClockwise )\n    vFuncDiff = np.vectorize( gridDiff )\n    return vFuncDiff(vFuncShift(direction ))\n            \n   \ndef gridDistance ( distance, diffFromGridDirection) :\n    radians = (diffFromGridDirection * math.pi)/180\n    return math.sin( radians ) * distance + math.cos( radians ) * distance\n\ndef gridDistanceVec( distance, diffFromGridDirection) :\n     vFuncDist = np.vectorize(gridDistance)\n     return vFuncDist(distance, diffFromGridDirection)\n        \n        \n    ", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "b3c73c0e-d94f-45f0-9ab0-862798c3d8e4", "_uuid": "dbf993bfb86ed4ec3581347b1231fab030fc8ed8", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "449a4387-d2bb-4eff-b0d6-d9ba8e4b4ab4", "_uuid": "e343a27ff122fb14cc10da6f0fc46b254e43371c", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "ce29d5a0-be20-48c7-b5cd-7468188bad92", "_uuid": "6646d41c1b268207606cd443964135c4f38292c1", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data['distance_in_metres'] = distanceInMVec(train_data['pickup_longitude'], train_data['dropoff_longitude'], train_data['pickup_latitude'], train_data['dropoff_latitude'])\ntrain_data['days_from_new_year'] = daysFromNewYear2016Vec(train_data['pickup_datetime'])\ntrain_data['direction'] = directionVec(train_data['pickup_longitude'], train_data['dropoff_longitude'], train_data['pickup_latitude'], train_data['dropoff_latitude'])    \n", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "2f827dfe-f47d-4179-bdb6-d49fd8175bc3", "_uuid": "c131f881abbe00babc12d07b7d8dd3362255dc50", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data['speed_in_kmh'] = speedInKmhVec(train_data['distance_in_metres'], train_data['trip_duration'])\ntrain_data['diffFromGridDirection'] = diffFromGridDirectionVec(train_data['direction'])\n", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "bb2d414a-63cc-4ca1-b323-8cf2524e2a31", "_uuid": "e324b524979c45a2652995af60d88a5b29818890", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data['grid_distance'] = gridDistanceVec( train_data['distance_in_metres'], train_data['diffFromGridDirection'] )", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "77788a83-af8f-4e58-82ef-b643e8ab30ba", "_uuid": "3efb98838a092798354d61b2073525423e92c24d", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "# Slow way, using apply\n#train_data['distance_in_metres'] = train_data.apply(lambda row: distanceInM(row),axis=1)\n#train_data['grid_distance_in_metres'] = train_data.apply(lambda row: gridDistanceInM(row),axis=1)\n#train_data['direction'] = train_data.apply(lambda row: direction(row),axis=1)\n#train_data['days_from_new_year'] = train_data['pickup_datetime'].apply(lambda dt: daysFromNewYear2016(dt))\n\n\n", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "47097181-f229-45ec-ac7f-25e85a13ac40", "_uuid": "41f1f7f9d79345354f01249abcb11f2057821dbd", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "# Slow way, using apply\n#train_data['speed_in_kmh'] = train_data.apply(lambda row: speedInKmh(row),axis=1)\n#train_data['grid_speed_in_kmh'] = train_data.apply(lambda row: gridSpeedInKmh(row),axis=1)\n#train_data['diffFromHorVert'] = train_data.apply(lambda row: diffFromHorVert(row),axis=1)", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "687d7362-7e06-4bdd-a3f9-7c3ddd5b6d99", "_uuid": "afa68febf1f48400f295688857d336dfab4dd45e", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data.describe()", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "d1b3de94-984f-4e10-bcc9-4107f110a612", "_uuid": "d49bb8513a31f3c2e2c6cd4fc5ed1b5fb46e85c1", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "A quick sanity check.\n\n - Average speed is 14 km/h. This seems slow, but remember it's speed as the crow flies; most routes will be far from straight lines. So this seems like it is in the right ballpark.\n - The value of 9000 kmh seems wrong, and looking at that value, the pickup and dropoff longitude are identical to 6 decimal places, which seems deeply suspicious. It looks like using abnormally high (and perhaps low) speeds will indeed help identify and filter bogus data.\n - Max days since new year looks right - 181.\n - At least one trip covers an abnormal distance.  We should drill into those.\n- Grid alignments range form 0 to 45. We should check some of the close to zero values to see if they align on a map.\n- A reminder we need to review those passenger zero/very low duration trips", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "6ced5559-4078-4ba4-8100-a469554008c2", "_uuid": "7d393106a6dc04da2be43a161e3adae861fb7104", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "## Cleaning the data##\nFirst, let's look at those abnormally high speeds. Do they correlate with other outlier values? Do they look like genuinely bogus data or a problem with our preprocessing? What's a sensible cutoff for filtering them?\n\n### Speed outliers ###\n\n\n\n", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "3be074ca-705c-4157-baaa-ee09c1f3309c", "_uuid": "3c2c412fae2645221a2882caa3981d93a601553e", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data[train_data['speed_in_kmh'] > 200]", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "ca264829-3051-4b64-9e3a-8794049ca279", "_uuid": "fc628511ec106f4e5cca28ed406ad005bd4c88f1", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "The good-ish news is that these are only 68 rows in 1.45M: approx 1 row in 25,000.\n\nMany of these have very low trip duration. We might hope that they are due to noise in the very low duration, but this is not realistic in all cases. Some rows have managed to travel a km in a few seconds.\n\nClearly those with very high speed are not legit. Such speeds are not realistic and must be due to faulty readings (or perhaps bad code on our side...with more time I would sanity check a few values on google maps and calculate \"by hand\" to ensure there's been no mistake).\n\nWe will need to strip some of these outliers from our training data, but we don't want to throw out the baby with the bathwater. First we will visualise the data, and get a feel for the kind of distribution we have for distance, duration and speed. This will help inform our cutoff process for outliers.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "0b9dff83-27a0-4c78-b46d-f25550b38f9c", "_uuid": "7feaa552f50b5f7a6d06bb9aa9ebcb1f34452fa0", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "plt.figure(figsize=(12,8))\nsns.distplot(train_data['speed_in_kmh'].values, bins=50, kde=True)\nplt.xlabel('speed', fontsize=12)\nplt.show()          ", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "441e0cf3-473a-48bd-8988-af26110b1127", "_uuid": "1ccc6287e685dc02c662d78b1640003a0c7faded", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "Wow! The really high outliers skew things hugely. Let's strip those out of the plot, there are only a few of them after all", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "01ac351b-e7f1-4c12-977e-8e08e40a1c06", "_uuid": "7afcadf249dcf4d75288f419ae2ad30e742ee76a", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "plt.figure(figsize=(12,8))\n#sns.distplot(train_data['speed_in_kmh'].values, bins=50, kde=True)\n\nsns.distplot(train_data[train_data['speed_in_kmh'] < 200]['speed_in_kmh'].values, bins = 100, kde=True)\nplt.xlabel('speed', fontsize=12)\nplt.show()", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "202fb7a6-bb3c-48f9-a2f6-f8208da2e7cd", "_uuid": "c5e77497e21bb7c6b4fbc833cf497f2a9f3d42ed", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "This looks more like what we should expect. Traffic crawling along. Happily, this looks not too far from a normal distribution. It looks like a cutoff for speed between 50 and 100 kmh should work to get rid of some of the bogus data without affecting our training set much. Let's check that.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "a3dd431b-51a1-4a5a-8c52-241b091399b4", "_uuid": "42b17d251b04375bb8dd7715afca27248a0b1c4a", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data['speed_in_kmh'].quantile(q= (0.5,0.75, 0.99, 0.995, 0.999, 0.9995, 0.9997, 0.9999, 0.99999))", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "6e51eeb1-d77c-4add-8ebd-05767aeb8a64", "_uuid": "d157f1c007e24e80785d45e7848df804d4bacd8a", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "At this point, I wondered whether 50-60 kmh had anything in common. It may be they are legitimately going down freeways at night, for example.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "69b54f0a-aa4b-4412-8450-5cc64b9f2349", "_uuid": "6c9e53e8aff8e79d36f3aaf7806457023004d4f6", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data[ (train_data['speed_in_kmh'] > 65) & (train_data['speed_in_kmh'] < 120) & (train_data['distance_in_metres'] > 1000) ]", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "6de1a9a4-847a-4465-81b5-b223d5b3e696", "_uuid": "3b5afe6b0ae736d4d256c24001ffeb7050db4b57", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "Spot checking a few of these just confirmed they were almost certainly bad data - google maps suggesting journeys that would take 35 minutes over 15 miles were taking 5 minutes instead. \n\nFor now I will cut off at .99, so 40.853 kmh. I'm unsure whether I'm excluding some legit data points, and indeed whether I'm still include some bad ones; it would be good to do further analysis later if there is time.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "31b433fc-2840-4ea1-9733-2916ff87cb7e", "_uuid": "1b0543e95c616e9135bff8969d8707abf69c3ded", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data_cleaned = train_data[train_data['speed_in_kmh'] <= 40.853]", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "249f422b-1253-48e4-8fc7-7fc969aa60c1", "_uuid": "3a1e762b64336e23c2638ee8d55857ec0f11f57b", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data[ (train_data['trip_duration'] > 20000) ]", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "eed7b29b-e044-4019-ae11-c18eb9b0bfde", "_uuid": "c9df6137bc16166b8841f5d842c720b0f6129ded", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "Now, this is interesting. These long journeys all seem to last a similar amount of time, around 86000 seconds, and it looks like there is similarity in the pickup and dropoff point. Also, they all belong to taxi firm 2.\n\n86000 seconds is close to 24 hours. It looks like there are cases where taxi firm 2 hires you a taxi for 24 hours.\n\nIt's hard to know how we can handle these cases, unless we can find some way to identify them from the rest of the data. They are about 0.1% of the dataset, so my instinct is to filter them out initially, and perhaps drill into them more later. \n\nLet's confirm this theory by drilling in with a plot\n", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "bb133214-b4df-42aa-b7bc-a4490d920dfa", "_uuid": "22ef4e6df7ccfb6adcab603f465e7b0fc262de82", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "plt.figure(figsize=(12,8))\n#sns.distplot(train_data['speed_in_kmh'].values, bins=50, kde=True)\n\nbig_trips = train_data[(train_data['trip_duration'] > 7200) ]['trip_duration']\nsns.distplot(big_trips.values, bins = 100, kde=True)\nplt.xlabel('trip duration when over 2 hours', fontsize=12)\nplt.show()", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "deb3b06c-eca5-4723-9064-47ac6e2a4d45", "_uuid": "0e0c479634785ab1a4940af3a4521598a4b234c0", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "We do indeed have this big spike. I leave it to the reader to confirm this is at the 24 hour mark, and proceed to remove these longer trips from the cleaned data set.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "53c8d95f-78de-4177-b8de-e96011d05a74", "_uuid": "7bcaf827d2925141ccefe006a2901c96018eeb39", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data_cleaned = train_data_cleaned[train_data_cleaned['trip_duration'] < 70000]\n", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "f32f2883-11ac-4ec2-871c-de696f4aa1d7", "_uuid": "67f67062bad92bf1f1ad8d6ada44322067f31275", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "Next, I want to review these trips with very low duration (less than a minute). Do they correlate, for example, with zero passengers and minimal distance?", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "9ec98fd0-a6f5-44da-966b-9dbe714016a7", "_uuid": "f007c2dd3c4b3e8d7e00d311e88bfcaa93edf13a", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "train_data_cleaned[train_data_cleaned['trip_duration'] < 60].describe()", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "75580456-637e-43b1-984e-9779f72b7069", "_uuid": "aeb9908ab0823877d0ebdf10fa8b3fdf74c519de", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "It looks like the zero passenger trips are a minority of these cases - we can check those separately.\n\nThe percentiles for the distance are reassuring; these trips are a few hundred metres, so the passenger probably just changed their mind. As they are still point to point trips, there's no reason our ML algorithms can't handle them", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "7bfb5a0f-ddb1-441a-be4e-ecc2b65db596", "_uuid": "b7e711bfb060a0591c719ac2fb0c1fd544e3c554", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "Mostly tiny trips, as expected, though one was a 20km journey. For now, I'll leave these in the data set, as the smaller trips should be easy enough to predict.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "ed46fec7-00ca-4a37-ba6d-29000cb1a350", "_uuid": "46502a336ba2af9c307ced564f572eedd6e194f3", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "Some outliers, and some interesting spikes. Let's again filter out any huge outliers to get a nicer view", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "91d4537c-9398-4666-9c81-2510bcfe6272", "_uuid": "2c67d2ed31b35d6fdf0f003040a7ae7fafc0c59e", "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "## Next Steps ##\nWe are done for the moment, as I need to break for a while.\n\nWe might want to apply some more outlier analysis to the trip start and end points so we focus on New York and clean the data of resulting worrying outliers. I leave this to the reader for now.\n\nI will add a feature representing \"grid distance\", that is the distance assuming the taxi can only travel along the 29 degrees from north/south axis or perpendicular to it. \n\nWe should also check for correlations between our engineered features to see if they are useful.\n\nI then plan to run this through XGBoost, and try a neural network based approach.", "cell_type": "markdown"}, {"metadata": {"collapsed": false, "_cell_guid": "5f4d5737-ebc6-4a66-aba6-e5bb9f40eedb", "_uuid": "d85a298859f5c587d2ac65a0c671082a774c3015", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}, {"metadata": {"collapsed": false, "_cell_guid": "4f82eda5-0fe0-4d27-98f0-0be850827032", "_uuid": "9c934cdbea051e59a0d2f26319d96b27e2050dec", "trusted": false, "_execution_state": "idle"}, "execution_count": null, "outputs": [], "source": "", "cell_type": "code"}]}