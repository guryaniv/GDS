{"cells":[{"metadata":{"_uuid":"e94bacf94b72dd3aa9f98a0739720aaeb0bb4592"},"cell_type":"markdown","source":"# How to Score (0.0255 - 0.0245)"},{"metadata":{"_uuid":"c6ce5c887793b1e1e3966d948b393f39a1ec15c1"},"cell_type":"markdown","source":"Before I start explaining the code, I have to say that the most of this code was taken from [[This kernel](https://www.kaggle.com/anycode/simple-nn-baseline-3)] by [anycode](https://www.kaggle.com/anycode) Kaggler"},{"metadata":{"_uuid":"29449ddea25f9e4a474022e0707c5233eb862faa"},"cell_type":"markdown","source":"**Fisrt :** Let's load the needed dependencies :"},{"metadata":{"id":"5BJracUTf2a_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"40f7e71f-7f8f-40cc-b7fc-f777b2bb803a","trusted":true,"_uuid":"053e8ab055a5459dc7275032566f7c2ac093bda8"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt  \n\nfrom timeit import default_timer as timer\nfrom sklearn import preprocessing\n\n!pip install ultimate\nfrom ultimate.mlp import MLP \n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\n\nimport gc, sys\ngc.enable()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"L_dvgO8MmGNt","colab_type":"code","colab":{},"trusted":true,"_uuid":"25340d25393956af64192adf337cdc70f4b5b9f5"},"cell_type":"code","source":"def state(message,start = True, time = 0):\n    if(start):\n        print(f'Working on {message} ... ')\n    else :\n        print(f'Working on {message} took ({round(time , 3)}) Sec \\n')","execution_count":null,"outputs":[]},{"metadata":{"id":"LIvATfaPgK9t","colab_type":"code","colab":{},"trusted":true,"_uuid":"7318a8b680619e0e8afbd9c37a42875f2a4908cc"},"cell_type":"code","source":"INPUT_DIR = \"../input/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80d4c440be019b60242aeb51c283b6f1a50b36cd"},"cell_type":"markdown","source":"** I explained the following code with comments, I hope you understand it well**"},{"metadata":{"id":"pb-1FR4SgNvh","colab_type":"code","colab":{},"trusted":true,"_uuid":"6635ac14017e4233225c622dc6d8c59f4e5676f5"},"cell_type":"code","source":"def feature_engineering(is_train=True):\n    # When this function is used for the training data, load train_V2.csv :\n    if is_train: \n        print(\"processing train_V2.csv\")\n        df = pd.read_csv(INPUT_DIR + 'train_V2.csv')\n        \n        # Only take the samples with matches that have more than 1 player \n        # there are matches with no players or just one player ( those samples could affect our model badly) \n        df = df[df['maxPlace'] > 1]\n    \n    # When this function is used for the test data, load test_V2.csv :\n    else:\n        print(\"processing test_V2.csv\")\n        df = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n        \n    # Make a new feature indecating the total distance a player cut :\n    state('totalDistance')\n    s = timer()\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n    e = timer()\n    state('totalDistance', False, e - s)\n          \n\n    state('rankPoints')\n    s = timer()\n    # Process the 'rankPoints' feature by replacing any value of (-1) to be (0) :\n    df['rankPoints'] = np.where(df['rankPoints'] <= 0 ,0 , df['rankPoints'])\n    e = timer()                                  \n    state('rankPoints', False, e-s)\n    \n\n    target = 'winPlacePerc'\n    # Get a list of the features to be used\n    features = list(df.columns)\n    \n    # Remove some features from the features list :\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    features.remove(\"matchDuration\")\n    features.remove(\"matchType\")\n    \n    y = None\n    \n    # If we are processing the training data, process the target\n    # (group the data by the match and the group then take the mean of the target) \n    if is_train: \n        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n        # Remove the target from the features list :\n        features.remove(target)\n    \n    # Make new features indicating the mean of the features ( grouped by match and group ) :\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    # Put the new features into a rank form ( max value will have the highest rank)\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    \n    # If we are processing the training data let df_out = the grouped  'matchId' and 'groupId'\n    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n    # If we are processing the test data let df_out = 'matchId' and 'groupId' without grouping \n    else: df_out = df[['matchId','groupId']]\n    \n    # Merge agg and agg_rank (that we got before) with df_out :\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the max value of the features for each group ( grouped by match )\n    print(\"get group max feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    # Put the new features into a rank form ( max value will have the highest rank)\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    # Merge the new (agg and agg_rank) with df_out :\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the minimum value of the features for each group ( grouped by match )\n    print(\"get group min feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    # Put the new features into a rank form ( max value will have the highest rank)\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    # Merge the new (agg and agg_rank) with df_out :\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the number of players in each group ( grouped by match )\n    print(\"get group size feature\")\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n     \n    # Merge the group_size feature with df_out :\n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    # Make new features indicating the mean value of each features for each match :\n    print(\"get match mean feature\")\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    \n    # Merge the new agg with df_out :\n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    # Make new features indicating the number of groups in each match :\n    print(\"get match size feature\")\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    \n    # Merge the match_size feature with df_out :\n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    # Drop matchId and groupId\n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n    \n    # X is the output dataset (without the target) and y is the target :\n    X = np.array(df_out, dtype=np.float64)\n    \n    \n    del df, df_out, agg, agg_rank\n    gc.collect()\n\n    return X, y\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c05975ca8d9f84d6a401b4e02c1aea88a068d99"},"cell_type":"markdown","source":"### Process and scale the training data  :"},{"metadata":{"id":"ZAFoSDp1gSLu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":608},"outputId":"bc27e575-4c67-4e2d-e861-ca9532d4c50b","trusted":true,"_uuid":"416aec24dd82c27e63c5af2d7d3b8039255c1006"},"cell_type":"code","source":"%%time\n# Process the training data :\nx_train, y = feature_engineering(True)\n# Scale the data to be in the range (-1 , 1)\nscaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"vfTGA9EzggdB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"6bd921c4-11f7-46d4-9b24-fea309dfe352","trusted":true,"_uuid":"4919d7141d4a3bc97fd38abe573547d867e85471"},"cell_type":"code","source":"print(\"x_train\", x_train.shape, x_train.max(), x_train.min())\nscaler.transform(x_train)\nprint(\"x_train\", x_train.shape, x_train.max(), x_train.min())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc6b1fd2c89520c6b666b908b7ece6dd32486dc0"},"cell_type":"markdown","source":"### Scale the target to be in the range (-1 , 1)"},{"metadata":{"id":"LTigJv6XggWY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b539069c-2d32-40ef-d16f-74095c7923f8","trusted":true,"_uuid":"98e247099cb92b3e0e3672f185e4a501c3003469"},"cell_type":"code","source":"y = y * 2 - 1\nprint(\"y\", y.shape, y.max(), y.min())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2cec7ad02d49ebfc950f5d92913f6d510a7eba1"},"cell_type":"markdown","source":"## Define an MLP model and train it :"},{"metadata":{"trusted":true,"_uuid":"68aa5465fcdaf005f293634fc1fb9282c1119fac"},"cell_type":"code","source":"%%time\n# create NN_model\nNN_model = Sequential()\nNN_model.add(Dense(x_train.shape[1],  input_dim = x_train.shape[1], activation='relu'))\nNN_model.add(Dense(136, activation='relu'))\nNN_model.add(Dense(136, activation='relu'))\nNN_model.add(Dense(136, activation='relu'))\nNN_model.add(Dense(136, activation='relu'))\n\n# output Layer\nNN_model.add(Dense(1, activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba1a9f68626711f48217fa7c183406301ed5fe8c"},"cell_type":"code","source":"checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3cf8babc38cb0691d05ea488a384a5c047fa4fb"},"cell_type":"code","source":"%%time\nNN_model.fit(x=x_train, y=y, batch_size=1000,\n             epochs=30, verbose=1, callbacks=callbacks_list,\n             validation_split=0.15, validation_data=None, shuffle=True,\n             class_weight=None, sample_weight=None, initial_epoch=0,\n             steps_per_epoch=None, validation_steps=None)\ndel x_train, y\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7367e594d36776cf939ed9a75f5b1101ee62a3a"},"cell_type":"markdown","source":"## Process the test data and scale it :"},{"metadata":{"id":"QxYbGfSrggJJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"2d0f8b2a-cfbf-4804-b3c1-3fbca06f7649","trusted":false,"_uuid":"0ea711cf7886daf6edff5386c78ede1ee3ebfbac"},"cell_type":"code","source":"x_test, _ = feature_engineering(False)\nscaler.transform(x_test)\nprint(\"x_test\", x_test.shape, x_test.max(), x_test.min())\nnp.clip(x_test, out=x_test, a_min=-1, a_max=1)\nprint(\"x_test\", x_test.shape, x_test.max(), x_test.min())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0d48339d573f4c0987ba19eacce79e7335c82a0"},"cell_type":"markdown","source":"### Predict the target using the test data : "},{"metadata":{"id":"WOYIroTRggA1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"outputId":"896b0504-beae-47de-8e02-b4448400fb5e","trusted":false,"_uuid":"25f10b259cdf4152eb2af45adf27536c637c9081"},"cell_type":"code","source":"%%time\npred = NN_model.predict(x_test)\ndel x_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"674af22f065081b2cfbaf7eb61fe752ed1a461c2"},"cell_type":"markdown","source":"### Reshape the predictions and put them in the right range(0 , 1 )"},{"metadata":{"id":"geRcvFyngc6L","colab_type":"code","colab":{},"trusted":false,"_uuid":"c46e855cf0b13b462fe600c8190f262a0bc08e1e"},"cell_type":"code","source":"pred = pred.reshape(-1)\npred = (pred + 1) / 2","execution_count":null,"outputs":[]},{"metadata":{"id":"FWdw5ggPgeXS","colab_type":"code","colab":{},"trusted":false,"_uuid":"ae2bf6549c7a64553d080912de6d18c5d34cf0fd"},"cell_type":"code","source":"df_test = pd.read_csv(INPUT_DIR + 'test_V2.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"LxLZ53chgedw","colab_type":"code","colab":{},"trusted":false,"_uuid":"2b847ce9c5d44d7ce176497d8efda8bf191655c1"},"cell_type":"code","source":"%%time\nprint(\"fix winPlacePerc\")\nfor i in range(len(df_test)):\n    winPlacePerc = pred[i]\n    maxPlace = int(df_test.iloc[i]['maxPlace'])\n    if maxPlace == 0:\n        winPlacePerc = 0.0\n    elif maxPlace == 1:\n        winPlacePerc = 1.0\n    else:\n        gap = 1.0 / (maxPlace - 1)\n        winPlacePerc = round(winPlacePerc / gap) * gap\n    \n    if winPlacePerc < 0: winPlacePerc = 0.0\n    if winPlacePerc > 1: winPlacePerc = 1.0    \n    pred[i] = winPlacePerc","execution_count":null,"outputs":[]},{"metadata":{"id":"rHWD8gmRgeke","colab_type":"code","colab":{},"trusted":false,"_uuid":"b0faac02c6cb92da18f259eb0cd1c7d6e8aa4b71"},"cell_type":"code","source":"df_test['winPlacePerc'] = pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4521bf9f21fdd4d39252b3819eecd68aee015b0f"},"cell_type":"markdown","source":"## Create the submission file : "},{"metadata":{"id":"pi1Mml_Rge8K","colab_type":"code","colab":{},"trusted":false,"_uuid":"19a0da0d4d51894dc0d02dbbcd0b24a8e715439a"},"cell_type":"code","source":"submission = df_test[['Id', 'winPlacePerc']]\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be5818c11657c84475a79b9cad1861aea1e4b575"},"cell_type":"markdown","source":" Finally, I hope you understand every line of this kernel, also if you have any note don't hesitate to put it in a comment ."}],"metadata":{"colab":{"name":"tryAKernal.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU","language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}