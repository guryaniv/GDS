{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ef7792c3-1947-5fb9-24a9-16306b98af27"
      },
      "source": [
        "The start in his most basic form...\n",
        "-----\n",
        "\n",
        "I share this, just for giving speed to some who might think of this solution.\n",
        "\n",
        "We read the data\n",
        "----\n",
        " - nltk tokenizer, since this separates the '?' from the words\n",
        " - its enough to open them with a utf8 option to get rid of all errors, why does nobody tell this on this board ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3f42344f-a822-e0ef-a2be-450d2a2ab265"
      },
      "source": [
        "if you want to read the test file, simply change '../input/train.csv' to '../input/test.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4bae4126-27f5-c1be-bb25-fa0ea0210b4e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "#NLTK functioncs\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "stop = set(stopwords.words('english'))   #_____________ import stop words\n",
        "\n",
        "# timing function\n",
        "import time   \n",
        "start = time.clock() #_________________ measure efficiency timing\n",
        "\n",
        "\n",
        "# read data\n",
        "train = pd.read_csv('../input/test.csv',encoding='utf8')[:10000]  #_______________________ open data files\n",
        "print(train.head(30))\n",
        "train.fillna(value='leeg',inplace=True)\n",
        "#train=train.dropna(axis=0, how='any')  #clean empty rows that give trouble\n",
        "end = time.clock()\n",
        "print('open:',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "83dcbf43-a122-d486-5b02-97a18c5d54cd"
      },
      "source": [
        "What we learned not to do, since it seems to be waste of time;-)\n",
        "----\n",
        " - we don't clean the data, although the quora questions are full of rubisch, it even doesn't matter, rubbish is clustered out of the cloud.\n",
        " - we do not stem, its waste of time, although stemming should improve the results imho, but its way to slow and makes errors. It could speed up the tfidf vectorizing since you end up with less words\n",
        " - we do not remove stop words, even we do not remove the punctualizations, it's waste of time. Since a stopword like 'one' 'not' can be significant, and the questions are way to short to remove the stopwords like in a big text.\n",
        " - we do not spell check, although the quora questions are full of semantic errors, full of spelling errors, since spelling gives new rubbish. If we spell we should take it together with the original words\n",
        " - synsetting is nothing more usefull then stemming..... althoug one would expect to have a better tf-idf matrix with a meaning in a context\n",
        " - We don't bigram yet, its also consuming more memory then i would like, and have at disposition this time being.\n",
        "\n",
        "What we do is \n",
        "\n",
        " - searching the 'common words' called equivalent\n",
        " - searching the words in Question1 that are not in Question2\n",
        " - searching the words in Question2 that are not in Question1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c929cf3f-3631-68b2-b755-e713a89d9a44"
      },
      "outputs": [],
      "source": [
        "def cleantxt(q1,q2):\n",
        "    #print(q1,q2)\n",
        "    q1words = nltk.word_tokenize(q1)\n",
        "    q2words = nltk.word_tokenize(q2)\n",
        "    equq1 = [w for w in q1words if w in q2words]\n",
        "    #difq1 = [stemmer.stem(w.decode(\"utf8\")) for w in q1words if w not in q2words]  # stemming sometimes simplifies things\n",
        "    #difq2 = [stemmer.stem(w.decode(\"utf8\")) for w in q2words if w not in q1words ]\n",
        "    difq1 = [w for w in q1words if w not in q2words] \n",
        "    difq2 = [w for w in q2words if w not in q1words ]\n",
        "    #print(difq1,difq2)\n",
        "    #wsq1=[wordnet.synsets(w.decode(\"utf8\")) for w in difq1]\n",
        "    #wsq2=[wordnet.synsets(w.decode(\"utf8\")) for w in difq2]    #synsetting seems to find only stemmable words...\n",
        "    netto=list(set(difq1+difq2))\n",
        "    return q1words,q2words,difq1,difq2,equq1\n",
        "    \n",
        "\n",
        "q1=[]\n",
        "q2=[]\n",
        "di1=[]\n",
        "di2=[]\n",
        "eq=[]\n",
        "for xi in range(len(train)):\n",
        "    q1words,q2words,difq1,difq2,equq1=cleantxt(train.iloc[xi].question1,train.iloc[xi].question2)\n",
        "    q1.append(q1words)\n",
        "    q2.append(q2words)\n",
        "    di1.append(difq1)\n",
        "    di2.append(difq2)\n",
        "    eq.append(equq1)\n",
        "\n",
        "train['q1']=q1\n",
        "train['q2']=q2\n",
        "train['di1']=di1\n",
        "train['di2']=di2\n",
        "train['eq']=eq\n",
        "\n",
        "print(train.head(15))\n",
        "    \n",
        "end = time.clock()\n",
        "print('cleaned:',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "598235d5-7781-2ed8-f681-88c5394002a8"
      },
      "source": [
        "TFIDF model\n",
        "----\n",
        "Now its simple\n",
        "we use the tokenized questions to find all words, therefore we use s1.append(s2) to create one big textfile to simply find all words\n",
        "\n",
        "with that vocabulary, we create 5 tfidf's, you need 10Gigabyte free memory and a 64bit windows and 64bit Python to work this out.\n",
        "\n",
        "\n",
        " - ('Questions1 x Words', (2345796, 91315)) \n",
        " - ('Questions2 x Words',(2345796, 91315)) \n",
        " - ('Differenc1 x Words', (2345796, 91315))\n",
        " - ('Differenc2 x Words', (2345796, 91315))\n",
        " - ('Equality x Words', (2345796, 91315))\n",
        "\n",
        "with those tfidfs we make a very simple similarity test\n",
        "----\n",
        "*tf1_idf_matrix[:30]*.**dot** *(tf2_idf_matrix[:30].T)*.**diagonal()**.round(1)\n",
        "\n",
        "**the similarity= multiplication and taking the diagonal**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "59c52309-8e2e-9c3f-4e8f-fa19e2d733d1"
      },
      "outputs": [],
      "source": [
        "# Lets redo it but splitted... and use the existing vocabulary\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s1=train['q1'].map(lambda x: ' '.join(x),na_action=None)\n",
        "s2=train['q2'].map(lambda x: ' '.join(x),na_action=None)\n",
        "sd3=train['di1'].map(lambda x: ' '.join(x),na_action=None)                  \n",
        "sd4=train['di2'].map(lambda x: ' '.join(x),na_action=None)  \n",
        "se5=train['eq'].map(lambda x: ' '.join(x),na_action=None)  \n",
        "\n",
        "count_vectorizer = CountVectorizer(min_df=2)\n",
        "count_vectorizer.fit(s1.append(s2))  #Learn vocabulary and idf, return term-document matrix.\n",
        "\n",
        "\n",
        "count1_vectorizer = CountVectorizer(vocabulary=count_vectorizer.vocabulary_,min_df=2)\n",
        "count1_vectorizer.fit_transform(s1)  #Learn vocabulary and idf, return term-document matrix.\n",
        "freq1_term_matrix = count_vectorizer.transform(s1)\n",
        "count2_vectorizer = CountVectorizer(vocabulary=count_vectorizer.vocabulary_,min_df=2)\n",
        "count2_vectorizer.fit_transform(s2)\n",
        "#s2>s1\n",
        "freq2_term_matrix = count_vectorizer.transform(s2) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n",
        "count3_vectorizer = CountVectorizer(vocabulary=count_vectorizer.vocabulary_)\n",
        "count3_vectorizer.fit_transform(sd3)\n",
        "freq3_term_matrix = count_vectorizer.transform(sd3) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n",
        "count4_vectorizer = CountVectorizer(vocabulary=count_vectorizer.vocabulary_)\n",
        "count4_vectorizer.fit_transform(sd4)\n",
        "freq4_term_matrix = count_vectorizer.transform(sd4) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n",
        "count5_vectorizer = CountVectorizer(vocabulary=count_vectorizer.vocabulary_)\n",
        "count5_vectorizer.fit_transform(se5)\n",
        "freq5_term_matrix = count_vectorizer.transform(se5) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n",
        "#se5>s1\n",
        "\n",
        "tfidf1 = TfidfTransformer(norm=\"l2\")\n",
        "tf1_idf_matrix = tfidf1.fit_transform(freq1_term_matrix)\n",
        "tfidf2 = TfidfTransformer(norm=\"l2\")\n",
        "tf2_idf_matrix = tfidf2.fit_transform(freq2_term_matrix)\n",
        "tfidf3 = TfidfTransformer(norm=\"l2\")\n",
        "tf3_idf_matrix = tfidf3.fit_transform(freq3_term_matrix)\n",
        "tfidf4 = TfidfTransformer(norm=\"l2\")\n",
        "tf4_idf_matrix = tfidf4.fit_transform(freq4_term_matrix)\n",
        "tfidf5 = TfidfTransformer(norm=\"l2\")\n",
        "tf5_idf_matrix = tfidf5.fit_transform(freq5_term_matrix)\n",
        "\n",
        "\n",
        "print('Questions1 x Words', tf1_idf_matrix.shape)\n",
        "print('Questions2 x Words', tf2_idf_matrix.shape)\n",
        "#print('Differenc1 x Words', tf3_idf_matrix.shape)\n",
        "print('Differenc2 x Words', tf4_idf_matrix.shape)\n",
        "print('Equality x Words', tf5_idf_matrix.shape)\n",
        "\n",
        "#als je similariteit wilt zien...\n",
        "#print('Q similarity',tf1_idf_matrix[:10].dot(tf2_idf_matrix[:10].T) )\n",
        "\n",
        "print('example first 10 questions similarity')\n",
        "corr1=tf1_idf_matrix[:30].dot(tf2_idf_matrix[:30].T).diagonal().round(1)\n",
        "print(corr1)\n",
        "print('example equality 1 - eq ')\n",
        "corr2=tf1_idf_matrix[:30].dot(tf5_idf_matrix[:30].T).diagonal().round(1)\n",
        "print(corr2)\n",
        "print('example equality 2 - eq')\n",
        "corr3=tf2_idf_matrix[:30].dot(tf5_idf_matrix[:30].T).diagonal().round(1)\n",
        "print(corr3)\n",
        "print('example difference 1 - dif1 ')\n",
        "corr4=tf1_idf_matrix[:30].dot(tf3_idf_matrix[:30].T).diagonal().round(1)\n",
        "print(corr4)\n",
        "print('example difference 2 - dif2')\n",
        "corr5=tf2_idf_matrix[:30].dot(tf4_idf_matrix[:30].T).diagonal().round(1)\n",
        "print(corr5)\n",
        "print('example difference 1 - dif2')\n",
        "corr6=tf1_idf_matrix[:30].dot(tf4_idf_matrix[:30].T).diagonal().round(1)\n",
        "print(corr6)\n",
        "print('example difference 2 - dif1')\n",
        "corr7=tf2_idf_matrix[:30].dot(tf3_idf_matrix[:30].T).diagonal().round(1)\n",
        "print(corr7)\n",
        "\n",
        "\n",
        "    \n",
        "print(train[:31].question1,train[:31].question2)\n",
        "end = time.clock()\n",
        "print('doc word tfidf matrix:',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9986f2cd-9e07-320b-5548-9938d6e06695"
      },
      "source": [
        "Estimating all similarities\n",
        "----\n",
        "in batches of 1000, since its pure TF_IDF it doesn't matter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f69b00ee-a822-cc0f-e541-362064743a95"
      },
      "outputs": [],
      "source": [
        "submit=[]\n",
        "co1=[]\n",
        "co2=[]\n",
        "co3=[]\n",
        "co4=[]\n",
        "co5=[]\n",
        "co6=[]\n",
        "co7=[]\n",
        "batch=1000\n",
        "for xi in range(0,len(train),batch):\n",
        "    if xi+batch>len(train):\n",
        "        batch=len(train)-xi\n",
        "    corr1=tf1_idf_matrix[xi:xi+batch].dot(tf2_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n",
        "    corr2=tf1_idf_matrix[xi:xi+batch].dot(tf5_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n",
        "    corr3=tf2_idf_matrix[xi:xi+batch].dot(tf5_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n",
        "    corr4=tf1_idf_matrix[xi:xi+batch].dot(tf3_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n",
        "    corr5=tf2_idf_matrix[xi:xi+batch].dot(tf4_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n",
        "    corr6=tf1_idf_matrix[xi:xi+batch].dot(tf4_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n",
        "    corr7=tf2_idf_matrix[xi:xi+batch].dot(tf3_idf_matrix[xi:xi+batch].T).diagonal().round(2)    \n",
        "    co1.extend(corr1)\n",
        "    co2.extend(corr2)    \n",
        "    co3.extend(corr3)\n",
        "    co4.extend(corr4)\n",
        "    co5.extend(corr5)    \n",
        "    co6.extend(corr6)\n",
        "    co7.extend(corr7)    \n",
        "    #submit.extend(corr6/corr3)\n",
        "    submit.extend(corr1+(corr6/corr3-0.3))\n",
        "train['len']= train['eq'].map(lambda x: len(x))\n",
        "train['dif']=train['di2'].map(lambda x:len(x))\n",
        "\n",
        "end = time.clock()\n",
        "print('estimate if similarity of q1-diff2 is bigger then equality of q2 with common stem:',end-start) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "63062423-4e9a-be6c-ee90-11edb3bf5bd1"
      },
      "source": [
        "with the similarity we submit\n",
        "-----\n",
        "\n",
        " 1. similXY['isDUP']=(np.asarray(co1)+np.asarray(co2)+np.asarray(co3))/3\n",
        "---\n",
        "the first is simply the average similarity of Q1-Q2 + Q1-EQ + Q2-EQ\n",
        "this gives on the LB a score of 0.823\n",
        "\n",
        " 2. similXY['isNOT']=(np.asarray(co4)+np.asarray(co5))/2\n",
        "---\n",
        "this estimates the similarity between a Q1 and all the non common words. If two questions are 100% different you get 1, if two questions are identical, you get a 0 or nan\n",
        "So if we want to use this similarity, we have to reverse the value thats ABS( isNOT - 1 )  what makes the identical question a 1, and the 100% different question a 0\n",
        "this gives on the LB a score of 0.55\n",
        "\n",
        "3.similXY['is_duplicate']=similXY['isDUP']>similXY['isNOT']\n",
        "---\n",
        "this estimate gave me the idea i would score very well... **how disappointing... If you answer binary... the LB is 11....**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a0e2ca1c-98eb-012d-f5a4-c0cdd3b26a07"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "similXY=pd.DataFrame([])\n",
        "similXY['q12']=co1\n",
        "similXY['q1e12']=co2\n",
        "similXY['q2e12']=co3\n",
        "similXY['q1d1']=co4\n",
        "similXY['q2d2']=co5\n",
        "similXY['q1d2']=co6\n",
        "similXY['q2d1']=co7\n",
        "similXY['isDUP']=(np.asarray(co1)+np.asarray(co2)+np.asarray(co3))/3\n",
        "similXY['isNOT']=(np.asarray(co4)+np.asarray(co5)-(np.asarray(co6)/2+np.asarray(co7)/2))/2\n",
        "similXY['is_duplicate']=similXY['isDUP']>similXY['isNOT']\n",
        "similXY['is_duplicate']=similXY['is_duplicate']*1\n",
        "\n",
        "similXY.fillna(value=0)\n",
        "print(similXY.head(30))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f2c3b370-3cf3-7de2-ca8c-37795ef37c50"
      },
      "source": [
        "Visualize\n",
        "----\n",
        "Making a visual shows a preview how good the model could score\n",
        "The better the separation the better the model should work\n",
        "\n",
        " - Beware: the **isduplicate is here my estimate not the one in the training dat**a !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea0cea74-c816-c97c-3914-735239b53987"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sns.set(style=\"white\", color_codes=True)\n",
        "similsample=similXY.sample(n=300)\n",
        "sns.pairplot(similsample, hue=\"is_duplicate\", size=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e8ab18b9-c445-debc-4a2c-81a453f00ecf"
      },
      "outputs": [],
      "source": [
        "submiss=pd.DataFrame(abs(similXY['isNOT']-1))\n",
        "submiss.fillna(value=0,inplace=True)\n",
        "submiss.to_csv('TfIdf_submission.csv')\n",
        "print(submiss.describe())\n",
        "\n",
        "# DUP/NOTDUP = 11\n",
        "# pure DUP   = 0.8233\n",
        "\n",
        "end = time.clock()\n",
        "print('cleaned:',end-start)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}