{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.tsv',sep='\\t')\ndf_test = pd.read_csv('../input/test.tsv',sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"616df727dd5376d2cb8c76d0e4702337caf56686"},"cell_type":"code","source":"#first describe the data\ndf_train.shape,df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cb10dd8bcac00d08f1945966bca4bba5562cb8c"},"cell_type":"code","source":"#top 5 rows\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"8c48379f3549199364a28c661d47dd0d54083423"},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44bb4bde6d76431c8112f5f3369192488e3fbf47"},"cell_type":"code","source":"len(list(df_test.Phrase.values)+list(df_train.Phrase.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4444b394bb99dd6efd6a40037987a43c67390667"},"cell_type":"code","source":"len(list(df_test.Phrase)+list(df_train.Phrase))\nfull_text = list(df_test.Phrase)+list(df_train.Phrase)\nlen(full_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"80b8fa65a4d1591be47aa600ec398cdcfa0b2ce0"},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk import FreqDist\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nstemmer=SnowballStemmer('english')\nlemma=WordNetLemmatizer()\nfrom string import punctuation\nimport re\ndef clean_review(review_col):\n    review_corpus=[]\n    for i in range(0,len(review_col)):\n        review=str(review_col[i])\n        review=re.sub('[^a-zA-Z]',' ',review)\n        #review=[stemmer.stem(w) for w in word_tokenize(str(review).lower())]\n        review=[lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]\n        review=' '.join(review)\n        review_corpus.append(review)\n    return review_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"37947aed9c30a458d37b3059f9a10383c2221ada"},"cell_type":"code","source":"df_train['clean_review'] = clean_review(df_train.Phrase)\ndf_test['clean_review'] = clean_review(df_test.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dfac674493abb76a533fcca775742091a686d7a"},"cell_type":"code","source":"import os\nimport gc\nfrom keras.preprocessing import sequence,text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7c74d58206bf827d0201bafacf7a1839b3a067f"},"cell_type":"code","source":"train_text = df_train.clean_review.values\ntest_text = df_test.clean_review.values\ny = to_categorical(df_train.Sentiment.values)\nprint(train_text.shape,test_text.shape,y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46c81c1260a38edc8926f75269f565c569e6f259"},"cell_type":"code","source":"X_train_text,X_val_text,y_train,y_val=train_test_split(train_text,y,test_size=0.2,stratify=y,random_state=123)\nprint(X_train_text.shape,y_train.shape)\nprint(X_val_text.shape,y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31e676a5100e453b1e692b2831dd6634191dbcfd"},"cell_type":"code","source":"all_words=' '.join(X_train_text)\nall_words=word_tokenize(all_words)\ndist=FreqDist(all_words)\nnum_unique_word=len(dist)\nnum_unique_word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f177007b0d5e42af1cfc416049c67d1ec7447d14","collapsed":true},"cell_type":"code","source":"len_x_train = []\nfor text in X_train_text:\n    words = word_tokenize(text)\n    len_x_train.append(len(words))\nMAX_REVIEW_LEN=np.max(len_x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc4c4aca57f90262135f5c42656a7943399f4e64"},"cell_type":"code","source":"#max features to be considered while tokening the training data\nmax_features = num_unique_word\n#max length of each review\nmax_words = MAX_REVIEW_LEN\n#as we can't pass entire dataset into training once we divide each epoches into no of iterations so the size of data in each iteratios is called batch size\nbatch_size = 128\n#word embedding vecotr lenght for each word in high dimensional space\nembedding_vecor_length = 250\n#number of time entire data passed forward and backward through entire network \nepochs = 10\n#no of classes in the output layer\nnum_classes=y.shape[1]\nnum_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ba32697d55cbe068c3262df8c12b235310910444"},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train_text))\n\nX_train = tokenizer.texts_to_sequences(X_train_text)\nX_val = tokenizer.texts_to_sequences(X_val_text)\nX_test = tokenizer.texts_to_sequences(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e4ac2b1e58aa2a78d4fb5dd5736088a730af747a"},"cell_type":"code","source":"#we will make each review to same length as max_review length so that every review as of same length, for less length padd with zeros\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_val = sequence.pad_sequences(X_val, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)\nprint(X_train.shape,X_val.shape,X_test.shape,y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"9fc8a8f36d9a85b33e53f77bde97e8c15863fe49"},"cell_type":"code","source":"#instantiate keras model with sequential constructor\nmodel=Sequential()\n#embedding layer is used to represent words in the meaningful vectors in a dimensional space, so it takes total number words,size of vector,\nmodel.add(Embedding(max_features,250,mask_zero=True))\n#LSTM layer with 128 neurons, dropout for configuring the input dropout and recurrent_dropout for configuring the recurrent dropout\nmodel.add(LSTM(128,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\n#return_sequences=True argument,\n#What this does is ensure that the LSTM cell returns all of the outputs from the unrolled LSTM cell through time. If this argument is left out, the LSTM cell will simply provide the output of the LSTM cell from the last time step\nmodel.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n#since it's classification problem we use dense layer with no of classes.\nmodel.add(Dense(num_classes,activation='softmax'))\n#finally compiling model, loss = 'categorical_crossentropy' because of many classes, adam optimizer because its effective “all round” with adaptive stepping, metrics is ‘categorical_accuracy’ --  which can let us see how the accuracy is improving during training\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n#summary of model\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ac895dcaa4226af46e3e1ecf35663cd5ac393c3","scrolled":true},"cell_type":"code","source":"%%time\n\nhistory=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=5, batch_size=batch_size, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c26b37c1222984c7e7e06d98b06bf732e62e0efe"},"cell_type":"code","source":"#lets predict on the test data\ny_pred=model.predict_classes(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f184429e13997c915f8b63001945f83f1ea95886"},"cell_type":"code","source":"sub=pd.read_csv('../input/sampleSubmission.csv')\nsub.head()\nsub.Sentiment=y_pred\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4361144bed0eee6a212729e9dea6677a709b3981"},"cell_type":"code","source":"sub.to_csv('sub1.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"615e1ab99f76648613330843d6ecc84afaba0477"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}