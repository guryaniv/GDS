{"cells":[{"metadata":{"trusted":true,"_uuid":"54c76de9fe3919b5149aa565da42a959aff66061","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom tensorflow.contrib.layers import fully_connected, dropout\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6e11701e2cf7bd60eacd6518b1966de512522fe9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"39e5b252aa8142e2b03f4565448c2ea515803029"},"cell_type":"code","source":"submission = pd.read_csv('../input/submission/submission.csv')\nsubmission.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08c6ff3dbf6d2f45308669a048dc76a44061cb47","collapsed":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep='\\t')\ntest_data = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82443babf1e254d117f3f243c1e980f6f394c862"},"cell_type":"markdown","source":"shape: (156060,4)  \ncolumns: ['PhraseId', 'SentenceId', 'Phrase', 'Sentiment']"},{"metadata":{"trusted":true,"_uuid":"2906d923b593de10fe1895fb6086ffdc7c732fb2","collapsed":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cb619048fe78d8f441f9a8eb32ec2731a8703db","collapsed":true},"cell_type":"code","source":"train_data[\"SentenceId\"].unique().size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3bb376250e76b3cf73630c42dd6c67b43a64139e"},"cell_type":"code","source":"corpus = train_data['Phrase'].append(test_data['Phrase']).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"23d3af7c8d2a1b2acac57fea961a1ed2f03addb6"},"cell_type":"code","source":"assert corpus.shape[0] == train_data['Phrase'].shape[0] + test_data['Phrase'].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"96fc5cbd43f0a2ff9ebe9fefe5e9e8fdb53fcebc"},"cell_type":"code","source":"def tokenize(x):\n    return [x] if len(x) == 1 else TweetTokenizer().tokenize(x)\n     \ntfVectorizer = TfidfVectorizer(tokenizer=tokenize)\ntf_idf_total = tfVectorizer.fit_transform(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2487b12f094eadee4f70af5a7265acbf50ce36d3","collapsed":true},"cell_type":"code","source":"tf_idf_total = tf_idf_total.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1f59e4910e169893d67ce5787d01a2cb5eb88011"},"cell_type":"code","source":"tf_idf_dense = tf_idf_total[:train_data['Phrase'].shape[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b51d386e345e53dc67021eb59c2e6ac471290f22"},"cell_type":"code","source":"tf_idf_dense.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"22c63ee7e4db6a173b15895f22c9c6a29a9a9f8f"},"cell_type":"code","source":"tfVectorizer.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a1a158fea0fdf463c7b2f623e2928777b70ce4b5"},"cell_type":"code","source":"train_vocab = tfVectorizer.vocabulary_\ntrain_vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a12545b8b0cbdf2a73850877796084d21b8a7081"},"cell_type":"code","source":"tfVectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4375640b8708e05c4ce5d4f31576a52b38f7ebdc"},"cell_type":"code","source":"def top_tfidf_words(row, features, top_n=20):\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ndef top_words_in_doc(Xtr, features, row_id, top_n=20):\n    row = np.squeeze(Xtr[row_id].toarray())\n    return top_tfidf_words(row, features, top_n)\n\ndef top_mean_words(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=10):\n    if grp_ids:\n        D = Xtr[grp_ids].toarray()\n    else:\n        D = Xtr.toarray()\n\n    D[D < min_tfidf] = 0\n    tfidf_means = np.mean(D, axis=0)\n    return top_tfidf_words(tfidf_means, features, top_n)\n\ndef top_words_by_class(Xtr, y, features, min_tfidf=0.1, top_n=20):\n    dfs = []\n    labels = np.unique(y)\n    for label in labels:\n        ids = np.where(y==label)\n        feats_df = top_mean_words(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n        feats_df.label = label\n        dfs.append(feats_df)\n    return dfs\n\ndef plot_tfidf_classWords_h(dfs, num_class=9):\n    fig = plt.figure(figsize=(12, 100), facecolor=\"w\")\n    x = np.arange(len(dfs[0]))\n    for i, df in enumerate(dfs):\n        #z = int(str(int(i/3)+1) + str((i%3)+1))\n        ax = fig.add_subplot(num_class, 1, i+1)\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        ax.set_frame_on(False)\n        ax.get_xaxis().tick_bottom()\n        ax.get_yaxis().tick_left()\n        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=16)\n        ax.set_ylabel(\"Word\", labelpad=16, fontsize=16)\n        ax.set_title(\"Class = \" + str(df.label), fontsize=25)\n        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n        ax.barh(x, df.tfidf, align='center')\n        ax.set_yticks(x)\n        ax.set_ylim([-1, x[-1]+1])\n        yticks = ax.set_yticklabels(df.feature)\n        \n        for tick in ax.yaxis.get_major_ticks():\n                tick.label.set_fontsize(20) \n        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"215671fa0ccbf035adac929836ca4dc2e0b8ebd1"},"cell_type":"code","source":"def get_batch(index, batch_size = 2000):\n    \n    index = shuffle(index)\n    \n    batch_index = []\n    for sample in index:\n        batch_index.append(sample)\n        \n        if len(batch_index) == batch_size:\n            yield batch_index\n            batch_index= []\n        \n    if len(batch_index) > 0:\n        yield batch_index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a07be615f20dc0fa607e00507c10d4dd1361a7b"},"cell_type":"markdown","source":"## test data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6bc5847c192715dce6271a9e09bf837870da708d"},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d65703521857f91729fdb06e66c83c125375fb4f"},"cell_type":"code","source":"tf_idf_dense2 = tf_idf_total[train_data['Phrase'].shape[0]:]\ntf_idf_dense2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1ed5609d139d7f505f15cbe7233c0d190228c0f1"},"cell_type":"code","source":"assert len(tf_idf_dense) + len(tf_idf_dense2) == len(tf_idf_total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"856ee1bcc27eaa296b14205d52136a535f9fb9b8"},"cell_type":"code","source":"tfVectorizer2.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a9f326e843798b8677f2608b1d88bc0dfa3fc8bb"},"cell_type":"code","source":"test_vocab = tfVectorizer2.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"76430d02ca90d005e66f076a094c844656fa317c"},"cell_type":"code","source":"tfVectorizer2.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8a5cc830c1a55c982851bc187d8a06c2e49957ee"},"cell_type":"code","source":"n_sentance, n_words = tf_idf_dense.shape\n\ninput_sentence = tf.placeholder(dtype=tf.float32, shape = [None, n_words])\nlabels = tf.placeholder(dtype=tf.int32, shape = [None])\nhidden_layer1 = fully_connected(input_sentence, num_outputs =  50)\nhidden_layer1 = dropout(hidden_layer1, keep_prob=0.7)\nhidden_layer2 = fully_connected(hidden_layer1, num_outputs =  30)\nhidden_layer2 = dropout(hidden_layer2, keep_prob=0.8)\noutput = fully_connected(hidden_layer2, activation_fn=None, num_outputs =  5)\n\nlabel_onehot = tf.one_hot(labels, depth = 5)\nloss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = label_onehot, logits = output)\nloss = tf.reduce_mean(loss)\ny_ = tf.nn.softmax(output)\n\noptimize = tf.train.AdamOptimizer().minimize(loss)\n\npredictions = tf.cast(tf.argmax(y_,1), tf.int32)\nacc, acc_op = tf.metrics.accuracy(labels=labels, predictions=predictions)\nbatch_acc = 1 - tf.reduce_mean(tf.cast(tf.cast(labels - predictions, tf.bool), tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50da6badba44508d8af3ee9cfa9a70dee0a060ae"},"cell_type":"markdown","source":"## train_test_split validation"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"08952c5745d967f7eb51debb65ebbf74e6b0ad1e"},"cell_type":"code","source":"x_train, x_vad, y_train, y_vad = train_test_split(tf_idf_dense, train_data['Sentiment'], test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"80d382aef4d1f13dc2173d83338644cad1593e94"},"cell_type":"code","source":"print(x_train.shape)\nprint(x_vad.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e7987d4b4bd5f6ee5aea3329496dc2fbad6ec0b4"},"cell_type":"code","source":"i, j, max_epoch, patience, loss_max, = 0, 0, 50, 5, np.inf \nn = 1\noptimal_i = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9370359626183aae1b6e52681e0a434d5f5732bf"},"cell_type":"code","source":"\noptimal_i = 38\ngpu_options = tf.GPUOptions(allow_growth=True, visible_device_list='0')\nwith tf.Session(config = tf.ConfigProto(gpu_options=gpu_options)) as sess:\n    # retrain model by using all data with optimal_i\n    sess.run(tf.global_variables_initializer())\n    index = np.arange(len(tf_idf_dense))\n    for i in range(optimal_i):\n        sess.run(tf.local_variables_initializer())\n        for batch_index in list(get_batch(index)):\n            batch_x, batch_y = tf_idf_dense[batch_index], train_data['Sentiment'].values[batch_index]\n            _, loss_value, accuracy = sess.run([optimize, loss, acc_op], feed_dict= {input_sentence: batch_x,  labels: batch_y})\n        print('accuracy for epoch {} is {}, loss is {}'.format(i, accuracy, loss_value)) \n        \n    # --test--\n    sess.run(tf.local_variables_initializer())\n    pred_value = sess.run(predictions, feed_dict = {input_sentence: tf_idf_dense2}) \n    print(pred_value.shape)\n    sentiment = test_data[['PhraseId']]\n    sentiment['Sentiment'] = pred_value\n    sentiment.to_csv('submission.csv', index = False)\n    \ntf.reset_default_graph()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}