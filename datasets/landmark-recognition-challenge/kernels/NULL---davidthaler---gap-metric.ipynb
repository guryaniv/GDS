{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"### GAP metric\nThe metric for this competition, [global average precision](https://www.kaggle.com/c/landmark-recognition-challenge#evaluation), is not one of the standard evaluation metrics in, for example, scikit-learn. Here is the code that I am using to compute it. Feel free to use it if you like. And if you see any errors, please comment, and we'll get them fixed up\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef GAP_vector(pred, conf, true, return_x=False):\n    '''\n    Compute Global Average Precision (aka micro AP), the metric for the\n    Google Landmark Recognition competition. \n    This function takes predictions, labels and confidence scores as vectors.\n    In both predictions and ground-truth, use None/np.nan for \"no label\".\n\n    Args:\n        pred: vector of integer-coded predictions\n        conf: vector of probability or confidence scores for pred\n        true: vector of integer-coded labels for ground truth\n        return_x: also return the data frame used in the calculation\n\n    Returns:\n        GAP score\n    '''\n    x = pd.DataFrame({'pred': pred, 'conf': conf, 'true': true})\n    x.sort_values('conf', ascending=False, inplace=True, na_position='last')\n    x['correct'] = (x.true == x.pred).astype(int)\n    x['prec_k'] = x.correct.cumsum() / (np.arange(len(x)) + 1)\n    x['term'] = x.prec_k * x.correct\n    gap = x.term.sum() / x.true.count()\n    if return_x:\n        return gap, x\n    else:\n        return gap\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4b2d2a5ca4c96b4151654ff7d56cf8684fdfe436"},"cell_type":"code","source":"# Generate some random predictions on 3 classes\nypred = np.random.choice([1,2,3], 10)\nytrue = np.random.choice([1,2,3], 10)\nconf = np.random.random(10)\ngap, x = GAP_vector(ypred, conf, ytrue, True)\ngap","execution_count":8,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d2a4d6201dd8e69d2cf0efaa6a6677f77688ef0d"},"cell_type":"code","source":"x","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"a2adeab6daeaf409dae2a3123988be0a2f65d394"},"cell_type":"markdown","source":"In the data frame, correct is relevance, prec_k is the precision at rank k and term is the entire term under the summation in the formula:\n$$ \\frac{1}{M}\\sum_{i=1}^{N}Pr(i) rel(i)$$"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b9a70dc2969772e55dd743f367f824dd51661d39"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}