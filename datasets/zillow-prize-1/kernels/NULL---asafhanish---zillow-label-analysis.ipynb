{"cells": [{"execution_count": null, "metadata": {"collapsed": false, "_uuid": "d0fcfacdc4c9398b066db964832363a1eaad7094", "_execution_state": "idle"}, "source": "# This notebook explore the Zillow logerror prediction data\n\n*  [Logerror labels](#labelexplore)\n    * [Distribution](#labeldistribution)\n    * [Secular trends](#labelsecular)", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "e67fb0d0f12a1ed93ab2a5bc07a34711e1111e1f", "_execution_state": "idle"}, "source": "**\n\nExplore the logerror label\n--------------------------\n\n**\n<a id='labelexplore'> </a>\n* Distribution\n* Secular trends", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "cell_type": "code", "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n", "outputs": [], "metadata": {"_uuid": "f54e0019da98bdbb8954713e66f092033b1d6ff0", "_execution_state": "idle"}}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "e6393647f79b35f9a3fe9ae6a9658248344c5104", "_execution_state": "idle"}, "source": "\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nfrom sklearn import linear_model", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "3ea85cd6cfbe7f9ebc3f0bd3cc7a1c4c5ad35859", "_execution_state": "idle"}, "source": "%matplotlib inline\nimport matplotlib.pyplot as plt", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "ab0a4ceb2296f649f576516e09dc512da09621a0", "_execution_state": "idle"}, "source": "train = pd.read_csv(\"../input/train_2016.csv\", parse_dates=[\"transactiondate\"])\ntrain.shape", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "ca794041d42e9f08bdc9528abd939d2f76d3129c", "_execution_state": "idle"}, "source": "<a id='labeldistribution'> </a>\n\n\n\n**Lets look at the distribution of logerror values**\n----------------------------------------------------\n\nLooks like there is tight clustering around zero, but with some outliers", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "dce5ff15b8d5a761a28ac768cda6a45ad550b1c5", "_execution_state": "idle"}, "source": "fig,ax = plt.subplots(ncols=2,nrows=1,figsize=(16,5))\nax[0].hist(train['logerror'],bins=100)\nax[0].set_title('Full range or logerror',fontsize=18,fontweight='bold')\nax[1].hist(train['logerror'],bins=100,range=(-.6,.6))\nax[1].set_title('Zoom in',fontsize=18,fontweight='bold')\nplt.tight_layout()", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "a3c68050bc6ae677bcf348da98b2989c1b2160a7", "_execution_state": "idle"}, "source": "print('There is a slight right shift in the data...')\nmedian = np.percentile(train['logerror'],50)\nprint('* The median value is: {:.3f}'.format(median))\nabove_zero = (train['logerror']>0).mean()\nprint('* Values above zero: {:.0%}'.format(above_zero))\nrange95 = np.percentile(train['logerror'],[2.5,97.5])\nprint('* 95% of values are between: {:.2f}'.format(range95[0]) + ' and {:.2f}'.format(range95[1]))", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "ed354e4d0c372be1b37653bfbeb5ba7284ff07d5", "_execution_state": "idle"}, "source": "**\n\nlog error is kind of opaque. What does it mean in terms of delta dollar error (actual zestimate - actual sales price)?\n------------------------------------------------------------------------\n\n**\nFor more context, lets convert various log error scenarios to dollar amounts", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "40ef6cdde71cd3a61f90f2f4ad733cf02d67fb65", "_execution_state": "idle"}, "source": "def get_zest_price(sales_price,log_error):\n    log_sales_price  = np.log10(sales_price)\n    log_zest_price = log_sales_price + log_error\n    return 10**log_zest_price\n    \ndef get_delta_error(sales_price, log_error):\n    zest_price = get_zest_price(sales_price, log_error)\n    return zest_price - sales_price\n\ndef get_percent_diff(sales_price, log_error):\n    delta = get_delta_error(sales_price, log_error)\n    return float(delta) / sales_price\n\ndef print_stories(logerror):\n    sales_points = [150000, 250000, 500000, 750000, 1000000]\n    print('an absolute log error of ' + str(logerror) +' is in the {:.0%}'.format((abs(train['logerror']) <=logerror).mean())+ ' percentile of error, which means...')\n    for sales_price in sales_points:\n        zest_price = get_zest_price(sales_price, logerror)\n        delta = zest_price - sales_price\n        print('a sales price of ${:,.0f}'.format(sales_price) +  ' means the zestimate was ${:,.0f}'.format(zest_price) + ', with the difference of ${:,.0f}'.format(delta))", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "df43f3ea2286691975a451064e5001952d6d41cd", "_execution_state": "idle"}, "source": "plt.figure(figsize=(16,7))\nvalues = np.linspace(-.2,.2,100)\nplt.plot(values, [get_percent_diff(100000, le) for le in values])\nyticks = np.linspace(-.7,.7,15)\nplt.yticks(yticks,['{:.0%}'.format(yt) for yt in yticks])\nplt.ylabel('% error (Zestimat - sales price)',fontsize=16)\nxticks = np.linspace(-.2,.2,17)\nplt.xticks(xticks)\nplt.xlabel('logerror',fontsize=16)\nplt.tight_layout()", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "ed5cce575d2cff83fc9f01ae755fde9354318bf5", "_execution_state": "idle"}, "source": "a log error of 0.1 means the zestimate was about 25% higher the the sales price.  #perspective", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "bd6b52dae5313dd8f95a2a9eaa608ddc434a4a64", "_execution_state": "idle"}, "source": "# low error of .005\nprint('Low error')\nprint('-----------')\nprint_stories(.005)\nprint('#####################')\nprint(' ')\n\n# median error of .0325\nprint('median error')\nprint('-----------')\nprint_stories(.0325)\nprint('#####################')\nprint(' ')\n\n\n# high error of .2\nprint('high error')\nprint('-----------')\nprint_stories(.2)\nprint('#####################')\nprint(' ')", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "057a5980f7006dacc2e6edf73d8116e4c07c4bcb", "_execution_state": "idle"}, "source": "I don't know about you guys, but I feel better after having done this translation.  An outlier logerror of 0.2 seems so small, but on a $500k house means the zestimate was 290k too high", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "2309efcffec6eb8176308522e1cccc41196f265e", "_execution_state": "idle"}, "source": "<a id='labelsecular'> </a>\n\n\n**\n\nSecular trends\n--------------\n\n**\nAny changes to the label over time?", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "e99e8123e851ee626ba3c14e3902de9ee232065e", "_execution_state": "idle"}, "source": "# I want to trend weekday sales because of low activity on weekends\ntrain['WeekDay'] = [d.weekday()<5 for d in train['transactiondate']]\n\npiv = pd.pivot_table(train[train['WeekDay']==True], index='transactiondate',values='logerror',aggfunc=[np.size,np.sum,np.median])", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "594a9b4e49332e303cf96e7f219291fee6a053bd", "_execution_state": "idle"}, "source": "plt.figure(figsize=(16,5))\nplt.title('Daily (weekday) homesales volume',fontsize=20,fontweight='bold' )\nplt.plot_date(piv.index,piv['size'],alpha=.6)\nplt.plot_date(piv.index,piv['size'].rolling(window=30).mean(),'r-',linewidth=5)\nplt.tight_layout()", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "2176fe149c0cf54f9bed45ac9c98bbdaed5c919a", "_execution_state": "idle"}, "source": "There is the data dropoff in October.  Also, sale volume seems stable through late sprint into early fall?", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "9113f4b11ec98ecc76990a6ee3b02676a9da420c", "_execution_state": "idle"}, "source": "**\n\nWhat about changes in the log error over time?\n----------------------------------------------\n\n**\nThe graph below is rolling 30 day average of log errors\n* rolling 30 days average is calculated as sum of log errors divided by count of all records.", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "7dfba2b603fa2d86ebd7c8f6eaaed4b7f4450aca", "_execution_state": "idle"}, "source": "piv_all = pd.pivot_table(train, index='transactiondate',values='logerror',aggfunc=[np.size,np.sum])\npiv_restricted = pd.pivot_table(train[abs(train['logerror'])<.2], index='transactiondate',values='logerror',aggfunc=[np.size,np.sum])", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "07924d2ca1893d3442c469f49c76a31c143368d2", "_execution_state": "idle"}, "source": "plt.figure(figsize=(16,5))\nrolling_avg_all = piv_all['sum'].rolling(window=30).sum() / piv_all['size'].rolling(window=30).sum()\nrolling_avg_rest = piv_restricted['sum'].rolling(window=30).sum() / piv_restricted['size'].rolling(window=30).sum()\nplt.plot_date(piv_all.index,rolling_avg_all,label = '30 Day rolling Avg: All values')\nplt.plot_date(piv_restricted.index,rolling_avg_rest,label = '30 Day rolling Avg: No Outliers')\nplt.ylim(0,0.03)\nplt.legend(loc=0,fontsize=16)\nplt.title('Logerror secular trends', fontsize=20,fontweight='bold')\nplt.tight_layout()", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "e7a433d6bbc5d3c74008e103597e24d53ac3a2a0", "_execution_state": "idle"}, "source": "Cool!  It looks like the logerror is the lowest (zestimate is most accurate) in the late spring.  I wonder what would cause these secular trends?", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "154778b33bc037274148be8156f64b28baa557ea", "_execution_state": "idle"}, "source": "#### Lets look into this trend a bit more\n* restrict this analysis to pre Oct data to work around the missing data", "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "2febe43a9ac48f27203732bb856935245133e369", "_execution_state": "idle"}, "source": "train_pre_oct_16 = train[(train['transactiondate']<datetime(2016,10,16))]\npiv_pre = pd.pivot_table(train_pre_oct_16, index='transactiondate',values='logerror',aggfunc=[np.size,np.sum])\nrolling_avg_all_pre = piv_pre['sum'].rolling(window=30).sum() / piv_pre['size'].rolling(window=30).sum()\nscatter_df = pd.DataFrame(list(zip(piv_pre['size'].rolling(window=30).mean(),rolling_avg_all_pre)),columns=['Rolling size','Rolling avg'])\nscatter_df = scatter_df[scatter_df['Rolling size'].notnull()]", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "c8a5044cd96c2da86fa4021c5819ab08eca406e8", "_execution_state": "idle"}, "source": "\nsns.jointplot(\"Rolling size\", \"Rolling avg\", data=scatter_df, kind='scatter',\n                  xlim=(0,600), ylim=(0,.03),color=\"r\", size=7)\n\nplt.ylabel('Logerror')\nplt.xlabel('Sales volume ')\nplt.tight_layout()", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "53fb92eed6245dc480d5e00ec098f4b0552c1efe", "_execution_state": "idle"}, "source": "X_input = np.array([[v] for v in scatter_df['Rolling size'].values])\ny_input = np.array(scatter_df['Rolling avg'].values)\nregr = linear_model.LinearRegression()\nregr.fit(X_input, y_input)", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "12e479c5d4f23d1a36e4084da332b85e73043821", "_execution_state": "idle"}, "source": "plt.figure(figsize=(8,8))\nplt.scatter(X_input,y_input)\nplt.plot(X_input, regr.predict(X_input), color='blue',\n         linewidth=3)\nplt.xlabel('Daily Sales volume')\nplt.ylabel('Log error')\nplt.title('Modeling log error by daily sales volume', fontsize=18, fontweight='bold')\nplt.text(225,.025,'change of {:,.3f}'.format(regr.coef_[0]* 100)+' for ever 100 additional daily sales')\nplt.tight_layout()", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": false, "_uuid": "874b6f65d5d4597af3f852700cdcfd709dea5d38", "_execution_state": "idle"}, "source": "There you have it.  Additional sales associated with lower logerror.  I wish there were multiple years of data to start whittling away at all the hidden confounders and biases.", "outputs": [], "cell_type": "markdown"}], "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.0"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 0}