{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport csv\nimport json\nimport string\nimport keras\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nfrom math import floor\nimport spacy\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nimport time\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nimport regex as re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d76d701e3b215c726aea4918de3dedbadb2d4f06"},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3417472c83aadc664ed8ff3bef16cb5d479ba6c"},"cell_type":"code","source":"def word_locate(sentence, location): \n    count_words = 0\n    count_chars = 2 #2 is to count for the two spaces in the beginning\n    for word in sentence.split():\n        count_words += 1\n        if location == count_chars:\n            return word, count_words\n        count_chars += len(word)\n        count_chars += 1 #for space","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e85dc3d2a9bb72f560778e8d6c3e883a8339e30"},"cell_type":"code","source":"def split_sent_by_comma(sentence):\n    #splitting sentence if structure is (subj+obj+verb, subj+obj+verb), that is subj immediately following comma\n    doc = nlp(sentence)\n    prev_token_dep = \"\"\n    prev_token_text = \"\"\n    list_of_sub_sent = []\n    curr_sent = \"\"\n    for token in doc:\n        if prev_token_dep == \"punct\" and token.dep_ == \"nsubj\":\n            list_of_sub_sent.append(curr_sent)\n            curr_sent = \"\"\n            prev_token_text = \"\"\n        curr_sent += prev_token_text\n        if token.dep_ != \"punct\":\n            curr_sent += \" \" #there is space between words, but not before comma\n        prev_token_dep = token.dep_\n        prev_token_text = token.text\n    list_of_sub_sent.append(curr_sent)\n    return list_of_sub_sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea54acd0b3e224c0d64566b9416ab6c6641b0863"},"cell_type":"code","source":"def find_name_between_paran(sentence):\n    capture = \"\"\n    trigger_on = 0\n    for char in sentence:\n        if char == \")\":\n            trigger_on = 0\n        if trigger_on == 1:\n            capture += char\n        if char == \"(\":\n            trigger_on = 1\n    return capture","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc9368118fd33957843a66b792f612942b431e01"},"cell_type":"code","source":"def find_name_between_commas(sentence):\n    list_of_names = []\n    for sent in sentence.split(\",\"):\n        if len(sent.split()) == 1:\n            if sent.split()[0][0] in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]:\n                list_of_names.append(sent.split()[0])\n    if len(list_of_names) != 1:\n        list_of_names.append(\"none\")\n    return list_of_names[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7171326011f486e4cced5e660070a80a587555a"},"cell_type":"code","source":"analyze = \"Despite this, it failed to chart in any other country. Bonnie Tyler (then known as Gaynor Hopkins) spent seven years performing in local pubs and clubs around South Wales between 1969 and 1976, first as part of Bobbie Wayne & the Dixies, and then with her own band Imagination.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4fb8b232ec08b0e2b73605e18c171aa47eb0d139"},"cell_type":"code","source":"print(find_name_between_paran(analyze))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"598becad1d149c87b4ecbc97f7c45f5a950fe8c8"},"cell_type":"code","source":"def which_name_first(sentence, name1, name2):\n    name1_check = 0\n    for word_punct in sentence.split():\n        for word_comma in word_punct.split(\";\"):\n            for word in word_comma.split(\",\"):\n                if word == name2 and name1_check == 0:\n                    return name2\n                if word == name1:\n                    name1_check = 1\n    return name1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea3dcccd221eff627f211a448421094551ff4add"},"cell_type":"code","source":"def curr_prev_sentence(sentence, loc):\n    current_sentence = \"\"\n    prev_sentence = \"\"\n    trunc_curr_sentence = \"\"\n    remainder_curr = \"\"\n    detect = 0\n    count = 0\n    for char in sentence:\n        count += 1\n        current_sentence += char\n        remainder_curr += char\n        if ((char == \".\" or char == \";\") and detect == 0 and sentence[count] != \",\"): #the last arguement to prevent ., as in sent #4\n            prev_sentence = current_sentence \n            current_sentence = \"\"\n        if char == \".\" and detect == 1:\n            return current_sentence, prev_sentence, trunc_curr_sentence, remainder_curr\n        if count == loc:\n            detect = 1\n            trunc_curr_sentence = current_sentence\n            remainder_curr = \"\"\n    return current_sentence, prev_sentence, trunc_curr_sentence, remainder_curr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"094fa6a158ad47b4098a76c5196ea6d9245bf868"},"cell_type":"code","source":"def remove_last_word(sentence):\n    new_sent = sentence.split()\n    new_sent = new_sent[:-1]\n    return \" \".join(new_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7a6b210f60ed54287fefc9e0b6bfc43c61da95c"},"cell_type":"code","source":"def check_if_name(word):\n    if word[0] in [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]:\n        return True\n    else:\n        return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"831c866d851a6078dc970758f7aa87f97affaecf"},"cell_type":"code","source":"def remove_first_word(sentence):\n    new_sent = sentence.split()\n    new_sent = new_sent[1:]\n    return \" \".join(new_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"254700c9511b9cb96cf077d8d39495257442b039"},"cell_type":"code","source":"def find_name_words(sentence):\n    name = \"none\"\n    for word in sentence.split():\n        if check_if_name(word):\n            return word\n    return name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa82f0610c0300b18ada1dfc2ab67a48a276380e"},"cell_type":"code","source":"def find_subject(sentence): #finds last subject\n    doc = nlp(sentence)\n    subject = \"none\"\n    prev_tok_dep = \"\"\n    for token in doc:\n        if (token.dep_ == \"nsubj\" or token.dep_ == \"nsubjpass\") and prev_tok_dep != \"xcomp\" \\\n        and token.text != \"she\" and token.text != \"he\" and token.text != \"She\" and token.text != \"He\" and token.text != \"who\":\n            subject = token.text\n        prev_tok_dep = token.dep_\n    return subject","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a0cc3dbe12c271e02e5b25d91d8a7bbd2a90920"},"cell_type":"code","source":"def find_first_subject(sentence): #finds first subject\n    doc = nlp(sentence)\n    prev_tok_dep = \"\"\n    for token in doc:\n        if (token.dep_ == \"nsubj\" or token.dep_ == \"nsubjpass\") and prev_tok_dep != \"xcomp\" \\\n            and token.text != \"she\" and token.text != \"he\" and token.text != \"She\" and token.text != \"He\" and token.text != \"who\":\n            return token.text\n        prev_tok_dep = token.dep_\n    return \"none\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4ee213692594eda7ad83015b33c3e0a11f02ba0"},"cell_type":"code","source":"def find_second_subject(sentence): #finds second subject\n    doc = nlp(sentence)\n    subject = \"none\"\n    prev_tok_dep = \"\"\n    count = 0\n    for token in doc:\n        if (token.dep_ == \"nsubj\" or token.dep_ == \"nsubjpass\") and prev_tok_dep != \"xcomp\" \\\n            and token.text != \"she\" and token.text != \"he\" and token.text != \"She\" and token.text != \"He\" and token.text != \"who\":\n            count += 1\n            if count == 2:\n                subject = token.text\n        prev_tok_dep = token.dep_\n    return subject","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1540abb078f657ddafac1a28b5e9003f06d0b56"},"cell_type":"code","source":"def find_dobj(sentence): #find first dobj\n    doc = nlp(sentence)\n    for token in doc:\n        if token.dep_ == \"dobj\" and (token.text != \"him\" and token.text != \"her\"):\n            return token.text\n    return \"none\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d2bb21b5fbb13fc3b16dae528f104125c8061cd"},"cell_type":"code","source":"def find_last_dobj(sentence): #finds last dobj\n    doc = nlp(sentence)\n    dobj = \"none\"\n    for token in doc:\n        if token.dep_ == \"dobj\" and (token.text != \"him\" and token.text != \"her\"):\n            dobj = token.text\n    return dobj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d7b02e43830308899d3ca1748015de0c3b73dad"},"cell_type":"code","source":"def find_last_possessing_noun(sentence): #find last poss\n    doc = nlp(sentence)\n    poss = \"none\"\n    for token in doc:\n        if token.dep_ == \"poss\" and (token.head.pos_ == \"PROPN\" or token.head.pos_ == \"NOUN\"):\n            poss = token.text\n    return poss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d695a7c93b5e9bf20a3f2aba854464f4fefc8516"},"cell_type":"code","source":"def find_possessing_noun(sentence): #find first poss\n    doc = nlp(sentence)\n    for token in doc:\n        if token.dep_ == \"poss\" and (token.head.pos_ == \"PROPN\" or token.head.pos_ == \"NOUN\"):\n            return token.text\n    return \"none\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aeb84ea1bb630b3259ca0db140399e7b95e79175"},"cell_type":"code","source":"def find_appos(sentence): #returns first appos ; sometimes Spacy mislabels nsubj as appos\n    doc = nlp(sentence)\n    for token in doc:\n        if token.dep_ == \"appos\" and token.head.pos_ == \"PROPN\":\n            return token.text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff8a4dbf6fc9787713486508d5e9b135ebcb67cc"},"cell_type":"code","source":"with open('../input/gap-training-data/gap-development.tsv') as tsvfile:\n    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n    test_ids= []\n    text_list = []\n    pronoun_list = []\n    pronoun_offset_list = []\n    correct_name_list = []\n    previous_sents_list = []\n    current_sents_list = []\n    sent_num = 0\n    word_btwn_comma_list = []\n    word_btwn_paran_list = []\n    trun_dobj_list = []\n    trun_subj_list = []\n    prev_dobj_list = []\n    prev_subj_list = []\n    prev_last_subj_list = []\n    prev_first_subj_list = []\n    prev_second_subj_list = []\n    prev_last_dobj_list = []\n    curr1st_subj_list = []\n    curr2nd_subj_list = []\n    curr1st_dobj_list = []\n    curr2nd_dobj_list = []\n    curr1st_appos_list = []\n    \n    for row in reader:\n        test_ids.append(row['ID'])\n        text = row['Text']\n        proffset = int(row['Pronoun-offset']) \n        sent_num += 1\n        current, prev, trunc_curr, remainder = curr_prev_sentence(text, proffset)\n        previous_sents_list.append(prev)\n        current_sents_list.append(current)\n        trunc_curr = remove_last_word(trunc_curr)\n        prev_subj = find_subject(prev)\n        prev_first_subj = find_first_subject(prev)\n        prev_first_subj_list.append(prev_first_subj)\n        prev_second_subj = find_second_subject(prev)\n        prev_second_subj_list.append(prev_second_subj)\n        trunc_curr_subj = find_subject(trunc_curr)\n        trunc_curr_dobj = find_dobj(trunc_curr)\n        prev_dobj = find_dobj(prev)\n        prev_name_btn_comma = find_name_words(find_name_between_commas(prev))\n        curr_name_btn_paran = find_name_words(find_name_between_paran(current))\n        curr_1st_subj = find_first_subject(current)\n        curr_2nd_subj = find_second_subject(current)\n        curr_1st_dobj = find_dobj(current)\n        if not check_if_name(curr_1st_dobj):\n            curr_1st_dobj = find_possessing_noun(current)\n        curr_2nd_dobj = find_last_dobj(current)\n        if not check_if_name(curr_2nd_dobj):\n            curr_2nd_dobj = find_last_possessing_noun(current)\n        curr_1st_appos = find_appos(current)\n            \n        if prev_name_btn_comma != \"none\" and prev_name_btn_comma != which_name_first(prev,prev_name_btn_comma,prev_subj) :\n            word_btwn_comma_list.append(prev_name_btn_comma)\n        else:\n            word_btwn_comma_list.append(\"none\")\n        \n        trun_dobj_list.append(trunc_curr_dobj)\n        trun_subj_list.append(trunc_curr_subj)\n        prev_dobj_list.append(prev_dobj)\n        prev_subj_list.append(prev_subj)\n        \n        \n        if row['A-coref'] == 'TRUE':\n            correct_name_list.append(row['A'])\n        elif row['B-coref'] == 'TRUE':\n            correct_name_list.append(row['B'])\n        else:\n            correct_name_list.append('Neither')\n\n        prev_last = split_sent_by_comma(row['Text'])[-1]\n        \n        prev_last_subj = find_subject(prev_last)\n        prev_last_dobj = find_dobj(prev_last)\n        \n        prev_last_subj_list.append(prev_last_subj)\n        prev_last_dobj_list.append(prev_last_dobj)\n        curr1st_subj_list.append(curr_1st_subj)\n        curr2nd_subj_list.append(curr_2nd_subj)\n        curr1st_dobj_list.append(curr_1st_dobj)\n        curr2nd_dobj_list.append(curr_2nd_dobj)\n        curr1st_appos_list.append(curr_1st_appos)\n        word_btwn_paran_list.append(curr_name_btn_paran)\n        \n        text_list.append(\" %%%%%%%%%%%%%%%%%%%%%% \")\n        text_list.append(sent_num)\n        text_list.append(\" ********************** \")\n        text_list.append(text)\n        pronoun_list.append(row['Pronoun'])\n        pronoun_offset_list.append(proffset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0eeba4a621b255a10ec1d7e9298ce89304cb62e6"},"cell_type":"code","source":"results_df = pd.DataFrame({\"correct\":correct_name_list})\nresults_df['prv_obj'] = prev_dobj_list\nresults_df['pr_ls_sbj'] = prev_subj_list\nresults_df['pr_1_sbj'] = prev_first_subj_list\nresults_df['pr_2_sbj'] = prev_second_subj_list\nresults_df['c1st_sj'] = curr1st_subj_list\nresults_df['c2nd_sj'] = curr2nd_subj_list\nresults_df['c1st_ob'] = curr1st_dobj_list\nresults_df['c2nd_ob'] = curr2nd_dobj_list\nresults_df['c1st_ap'] = curr1st_appos_list\nresults_df['w_bt_pa'] = word_btwn_paran_list\nresults_df['pronoun'] = pronoun_list\nresults_df['offset'] = pronoun_offset_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3683819f4b09b7d90c877248954f6ca50a744750"},"cell_type":"code","source":"results_df.head(250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b83560ee40992a692589f0dff0d73306df96556e"},"cell_type":"code","source":"print(text_list[229])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44b87798f04c7ac40b20c8a74c81cc06c5c8d595"},"cell_type":"code","source":"analyze = \"Despite this, it failed to chart in any other country. Bonnie Tyler (then known as Gaynor Hopkins) spent seven years performing in local pubs and clubs around South Wales between 1969 and 1976, first as part of Bobbie Wayne & the Dixies, and then with her own band Imagination.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4917855249f6fcbe29526f7d5260921fa1715c0d"},"cell_type":"code","source":"current, prev, trunc_curr, remainder = curr_prev_sentence(analyze, 246)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf4b3afaa2fe7cd60999b59bfc0810816e8a1040"},"cell_type":"code","source":"print(find_first_subject(current))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20e6b2dc5d6b91abc9c8b117c75406f81d469f71"},"cell_type":"code","source":"print(find_last_dobj(analyze))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2ae4022649f9bdf8e108c50dfceab48e0a06934"},"cell_type":"code","source":"doc = nlp(analyze)\nfor token in doc:\n    print(token.text, token.dep_, token.head.pos_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9bcb28a485044af0916b3661eabc10ffa6597d7"},"cell_type":"code","source":"print(text_list[900:950])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380968288210f4362fb4a27912bcd5bb708e7681"},"cell_type":"code","source":"out_df = pd.DataFrame({\"ID\":test_ids})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c158045b84bf51f3507b4449a691fd3a83ba5cdd"},"cell_type":"code","source":"out_df['A'] = results_A\nout_df['B'] = results_B\nout_df['NEITHER'] = results_N","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8db0e3031a72172c8443efcdcb040bcda04917c1"},"cell_type":"code","source":"out_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}