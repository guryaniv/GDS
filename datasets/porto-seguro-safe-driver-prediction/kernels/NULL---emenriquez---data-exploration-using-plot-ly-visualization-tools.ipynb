{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "2268cad667e9e62ffd6efa08274624c02a9433df", "_cell_guid": "5820341b-3e20-4cc5-add1-7131867e157b"}, "cell_type": "markdown", "source": ["# Porto Seguro's Safe Driver Prediction"]}, {"metadata": {"_uuid": "85f161435c8d4567eb0c1927ff83bcba0f4e973b", "_cell_guid": "57644803-1ec4-4b5e-9be4-891c9eeff547"}, "cell_type": "markdown", "source": ["****This notebook was written using guidance from a really great tutorial by Kaggle user Anisotropic:\n", "### [Interactive Porto Insights - A Plot.ly Tutorial](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial)\n", "\n", "\n", "\n", "My attempt with this notebook is to explore the interactive data visualization tools offered by plot.ly, and perform some optimization using any insights I can gain from the data.\n", "\n", "In this project, Porto Seguro has provided driver characteristic data related to car insurance claims. The objective is as follows:\n", "\n", "## Problem Description\n", "Predict the probability that a driver will file a claim in the next year."]}, {"metadata": {"_uuid": "7c50ad1395f1d402991c8096ca71b0c6199a4bb6", "_cell_guid": "daf40163-a4d4-4f35-9fb2-ba9c71897516"}, "cell_type": "markdown", "source": ["### Data Exploration"]}, {"metadata": {"_uuid": "5360754adc992ce0c090c73205d430285c1a21da", "_cell_guid": "ff87bd7f-58b1-445b-9b02-daa4b6fdc48d"}, "cell_type": "markdown", "source": ["We first begin by loading in the necessary packages for the data analysis"]}, {"metadata": {"_uuid": "ad7f6535ef586d7cfcb76d8f86490ad033d5271e", "_cell_guid": "64a47cbd-1e3c-4774-8b64-1154335856f0", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# for general handling and manipulation of table data\n", "import pandas as pd\n", "import numpy as np\n", "\n", "# for visualization of missing data entries\n", "import missingno as msno\n", "\n", "# for generation of interactive data visualization\n", "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n", "import plotly.graph_objs as go\n", "init_notebook_mode(connected=True)\n", "\n", "# for random forest model predictions and result analysis\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import confusion_matrix\n", "\n", "# start a random seed for reproducibility of results\n", "np.random.seed(1)"]}, {"metadata": {"_uuid": "2d33f81e303b9da1f58f511a6776fa08f76c6c5c", "_cell_guid": "28388714-a61d-48c6-bdb5-1a60dca79d7f"}, "cell_type": "markdown", "source": ["Next we will load the data and get a sense of the general structure of information available"]}, {"metadata": {"_uuid": "18982dd09776f99c168bb42e9baf1de4b4a4bd8c", "_cell_guid": "637c2a39-25ae-4f8c-8d8b-69b87ea80bf9", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["train_data = pd.read_csv('../input/train.csv')"]}, {"metadata": {"_uuid": "e4f46149fa6ba0d7ec19e1b20d6f0504698bd574", "_cell_guid": "9b0ea149-f381-4ce6-80a7-603a08448519", "collapsed": true, "scrolled": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print('train_data({0[0]},{0[1]})'.format(train_data.shape))\n", "\n", "print('Number of training examples: {0}'.format(train_data.shape[0]))\n", "print('Number of features for each example: {0}'.format(train_data.shape[1]))"]}, {"metadata": {"_uuid": "c90a9fd09831f448764c20068584d377c6ba8171", "_cell_guid": "46672b2c-9790-4de5-8e0a-8421feed3b34"}, "cell_type": "markdown", "source": ["Below is a list of the labels for each of the 59 data features"]}, {"metadata": {"_uuid": "c25ff01308730095e256321ce082ce71ac7bcc92", "_cell_guid": "400c4422-3090-4862-b6e4-e8706a909ee1", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["pd.DataFrame(data = {'Feature Label': train_data.columns})"]}, {"metadata": {"_uuid": "676332745beb441f25cf98ed2daa3eca26c7567b", "_cell_guid": "71ef50c4-1a16-44b2-bfad-de512457a477"}, "cell_type": "markdown", "source": ["The data has the following feature structure within its 59 features:\n", "\n", "ID - identifier of the driver \n", "\n", "target - indicates whether or not a claim was filed for that driver\n", "\n", "For the rest of the feature labels:\n", "1. The \"ps\" prefix occurs before all features (presumably for \"Porto Seguro\")\n", "\n", "2. Features are then grouped into the following categories\n", "\n", "    a. 'ind' - contains 18 features (these features are relating to **ind**ividual)\n", "\n", "    b. 'reg' - contains 3 features (these features are relating to **reg**ion)\n", "\n", "    c. 'car' - contains 15 features (these features are relating to  **car**)\n", "\n", "    d. 'calc' - contains 20 features (these are **calc**ulated features)\n", "\n", "3. In addition, some features have post have the following suffixes:\n", "    a. 'cat' - used to denote that the feature is categorical\n", "    b. 'bin' - used to denote that the feature is binary\n", "    \n", "In order to keep things a little neater, we will drop the \"ps\" prefix from the feature labels."]}, {"metadata": {"_uuid": "bdda156093df329fc21b86e98c14ee1e8983108c", "_cell_guid": "5d967845-0c54-4023-a989-11fcd1eb5d5d", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["no_ps = [train_data.columns[x][3:] for x in range(2, len(train_data.columns))]\n", "train_data.columns = train_data.columns[:2].tolist() + no_ps"]}, {"metadata": {"_uuid": "47a96e87cf4e557f3f0fe587c72098391d043c37", "_cell_guid": "28d26461-c2e0-4f90-a181-247a20c8d670"}, "cell_type": "markdown", "source": ["We are also given the information from Porto Seguro that missing data in this dataset is marked by a '-1' value. Any columns with a '-1' value present are listed below along with the number of missing entries for each of these columns."]}, {"metadata": {"_uuid": "1140316dd820845518ede1a04842ec8a4a6be73e", "_cell_guid": "794e88d8-11fe-45dd-825b-3ac6af5beb42", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["NA_columns = train_data.columns[train_data.isin(['-1']).any()]\n", "NA_data_counts = train_data.isin(['-1']).sum()[train_data.isin(['-1']).any()]\n", "pd.DataFrame(data = NA_data_counts, columns = ['# of missing entries'])"]}, {"metadata": {"_uuid": "1d0c2e11bdcab4d564e9443a0f75890f53b420f9", "_cell_guid": "6f655d8e-a308-49fb-8a60-faccdc56b17b"}, "cell_type": "markdown", "source": ["We can see that a few columns such as 'reg_03', 'car_03_cat' and 'car_05_cat', among others, have a significant amount of missing data entries.  This is something that we will explore later on, but for now we will ignore the missing values and work with the original training data set provided.\n", "\n", "We can also visualize the missing data in these columns by using the missingno package."]}, {"metadata": {"_uuid": "19276a961370dddc957c55675cb624a5606d1bf6", "_cell_guid": "9bf7866d-46e8-403c-a974-be259c09283a", "collapsed": true, "scrolled": false}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["NA_data = train_data[NA_columns].replace(-1, np.NaN)\n", "msno.matrix(df=NA_data, color = (0, 0.3, 0.3))"]}, {"metadata": {"_uuid": "8288a12ae0184103654c7e7a0f017af03404441b", "_cell_guid": "0528911f-bf42-4ebf-a7e7-6846a4e42c26"}, "cell_type": "markdown", "source": ["### Visualization\n", "\n", "Let's now begin to visualize different aspects of the data to see what we can learn about the information contained in the features we've been given.\n", "\n", "Starting off we will look at the target variable, which is the main feature of interest we are asked to predict. The pie graph below shows the relative distribution of the target variable."]}, {"metadata": {"_uuid": "9ced7a29cb5306c6e6180af246e2ef483e021ec5", "_cell_guid": "03e28aa6-b5aa-4978-8423-1675d2ed7e94", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["labels = [\"Target = 0\", \"Target = 1\"]\n", "values = train_data[\"target\"].value_counts().values\n", "\n", "trace = go.Pie(labels = labels, values = values)\n", "layout = go.Layout(title = 'Distribution of Target Variable')\n", "\n", "fig = go.Figure(data = [trace], layout = layout)\n", "iplot(fig)"]}, {"metadata": {"_uuid": "9f321a993c0d263fe68dcdef9ed37f1e9f45ac5c", "_cell_guid": "a611f6ca-59d1-4d93-89cf-4ff90ca9968b"}, "cell_type": "markdown", "source": ["As we can see, the target variable is highly skewed, with only 3.64% of drivers in the training data filing a claim. This is something that we will have to keep in mind when evaluating the results of our predictions later on."]}, {"metadata": {"_uuid": "9040006c6df54d1a49fd399cfe0a86adc1915518", "_cell_guid": "2d11cbc8-36a1-4703-b9d3-8b6bef7e88ff"}, "cell_type": "markdown", "source": ["Next we will take a look at the binary variables within the dataset"]}, {"metadata": {"_uuid": "acb1df250ae5808a252814652fb9d5f3af5e5b6b", "_cell_guid": "59b16dff-afcb-47e6-86f1-6a0e7cc331d4", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["bin_columns = train_data.columns[train_data.columns.str.contains('_bin')]\n", "\n", "print(\"# of binary features: {0}\".format(len(bin_columns)))"]}, {"metadata": {"_uuid": "1fded4bcf0af5925c7929b480e950447b3209562", "_cell_guid": "6729f7d6-02b2-4f54-b98a-a1121fbdc1a6"}, "cell_type": "markdown", "source": ["We can use a stacked bar graph to effectively get a sense of the distribution of values within the binary features, shown below:"]}, {"metadata": {"_uuid": "7c669ec9357e090dabe261efc154ad3d1eddfe68", "_cell_guid": "09ea2d53-8196-480f-bbbd-e328b60eda1f", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["bin_counts = train_data[bin_columns].apply(pd.value_counts)\n", "\n", "trace = []\n", "for i in range(bin_counts.shape[0]):\n", "    trace_temp = go.Bar(\n", "        x= np.asarray(bin_columns),\n", "        y= bin_counts.values[i],\n", "        name = bin_counts.index[i]\n", "    )\n", "    trace.append(trace_temp)\n", "\n", "layout = go.Layout(\n", "    barmode = 'stack',\n", "    title = 'Distribution of Binary Features'\n", ")\n", "\n", "fig = go.Figure(data = trace, layout = layout)\n", "iplot(fig)"]}, {"metadata": {"_uuid": "dfc30377f98c828adbd8cca7a920ec4c66787c3d", "_cell_guid": "f2e18b01-153b-4391-8fe4-5c7711bc8460"}, "cell_type": "markdown", "source": ["We should also get some information on the categorical variables in this dataset. We can do this by subsetting the categorical feature columns."]}, {"metadata": {"_uuid": "147db2a684765b5bc9c3cc00338b6a2065de9800", "_cell_guid": "6f679310-c353-4b41-90f3-d19dd584a88f", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["cat_columns = train_data.columns[train_data.columns.str.contains('_cat')]\n", "cat_data = pd.DataFrame(data = {'# of levels': train_data[cat_columns].max()})\n", "\n", "cat_data"]}, {"metadata": {"_uuid": "7ebe4edc2a5c792f54ac5584e4c29547ba952a04", "_cell_guid": "a89a4474-e32e-4568-bcc3-5eb667df340c"}, "cell_type": "markdown", "source": ["Similar to our binary distribution, below is a visualization of the distribution within each categorical feature. The legend on the right shows the labels provided each category. Keep in mind that '-1' denotes missing data."]}, {"metadata": {"_uuid": "1056c27a631744dc8bdfbdb4badba40b5536cc81", "_cell_guid": "1e6a6959-72cc-45e3-b121-37c517ecb28e", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["cat_counts = train_data[cat_columns].apply(pd.value_counts)\n", "\n", "trace = []\n", "for i in range(cat_counts.shape[0]):\n", "    trace_temp = go.Bar(\n", "        x= np.asarray(cat_columns),\n", "        y= cat_counts.values[i],\n", "        name = cat_counts.index[i]\n", "    )\n", "    trace.append(trace_temp)\n", "\n", "layout = go.Layout(\n", "    barmode = 'stack',\n", "    title = 'Distribution of Categorical Features'\n", ")\n", "\n", "fig = go.Figure(data = trace, layout = layout)\n", "iplot(fig)"]}, {"metadata": {"_uuid": "150e7f9cb31302add2fe855298eeacac6a5ef046", "_cell_guid": "3efa8ebc-f6f7-4b43-a2f8-bfa8b6d6f00e"}, "cell_type": "markdown", "source": ["Finally, we will take a look at the remaining features"]}, {"metadata": {"_uuid": "3b6abc3cb8e32d85328fe6e310d9946c5cec6671", "_cell_guid": "310b2132-2293-401b-b375-432453559236", "collapsed": true, "scrolled": false}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#Isolate the columns that are not binary or categorical\n", "misc_columns = train_data.columns.drop(cat_columns).drop(bin_columns).drop([\"id\", \"target\"])\n", "\n", "#Split these columns by group\n", "ind_columns = misc_columns[misc_columns.str.contains('ind')]\n", "reg_columns = misc_columns[misc_columns.str.contains('reg')]\n", "car_columns = misc_columns[misc_columns.str.contains('car')]\n", "calc_columns = misc_columns[misc_columns.str.contains('calc')]\n", "\n", "#create boxplots for 'ind' columns\n", "trace1 = []\n", "for i in range(len(ind_columns)):\n", "    trace_temp = go.Box(\n", "        y= np.random.choice(train_data[ind_columns[i]], 2000, replace=False),\n", "        name = ind_columns[i]\n", "    )\n", "\n", "    trace1.append(trace_temp)\n", "\n", "layout1 = go.Layout(\n", "    title = 'Distribution of \"ind\" Features'\n", ")\n", "\n", "# create boxplots for 'reg' columns\n", "trace2 = []\n", "for i in range(len(reg_columns)):\n", "    trace_temp = go.Box(\n", "        y= np.random.choice(train_data[reg_columns[i]], 2000, replace=False),\n", "        name = reg_columns[i],\n", "        boxpoints = 'suspectedoutliers'\n", "    )\n", "\n", "    trace2.append(trace_temp)\n", "\n", "layout2 = go.Layout(\n", "    title = 'Distribution of \"reg\" Features'\n", ")\n", "\n", "# create boxplots for 'car' columns\n", "trace3 = []\n", "for i in range(len(car_columns)):\n", "    trace_temp = go.Box(\n", "        y= np.random.choice(train_data[car_columns[i]], 2000, replace=False),\n", "        name = car_columns[i],\n", "        boxpoints = 'suspectedoutliers',\n", "    )\n", "\n", "    trace3.append(trace_temp)\n", "\n", "layout3 = go.Layout(\n", "    title = 'Distribution of \"car\" Features'\n", ")\n", "\n", "# create boxplots for 'calc' columns\n", "trace4 = []\n", "for i in range(len(calc_columns)):\n", "    trace_temp = go.Box(\n", "        y= np.random.choice(train_data[calc_columns[i]], 2000, replace=False),\n", "        name = calc_columns[i],\n", "        boxpoints = 'suspectectedoutliers'\n", "    )\n", "\n", "    trace4.append(trace_temp)\n", "\n", "layout4 = go.Layout(\n", "    title = 'Distribution of \"calc\" Features'\n", ")\n", "\n", "fig1 = go.Figure(data = trace1, layout = layout1)\n", "fig2 = go.Figure(data = trace2, layout = layout2)\n", "fig3 = go.Figure(data = trace3, layout = layout3)\n", "fig4 = go.Figure(data = trace4, layout = layout4)\n", "\n", "\n", "iplot(fig1)\n", "iplot(fig2)\n", "iplot(fig3)\n", "iplot(fig4)"]}, {"metadata": {"_uuid": "391fbd559cee94afa06dd92a731aa698d7dedc29", "_cell_guid": "b866c7b1-0550-41dd-bc1e-1e7ae2614898", "collapsed": true}, "cell_type": "markdown", "source": ["We can see from the distributions above that while some features have relatively small ranges (e.g. between 0 and 1) while others have relatively larger ranges, such as ps_calc_10 and ps_ind_15, it may be useful to consider normalization in order to improve the performance of our chosen model when it comes time to optimize our predictions.\n", "\n", "Note that a random sample of 2000 drivers was selected for the box plots, since the load is pretty heavy on my machine, but it can be adapted for a larger subset.\n", "\n", "For now, we will work with the data as-is and begin fitting the data to a Random Forest (RF) model in order to get a baseline performance that we will later attempt to optimize."]}, {"metadata": {"_uuid": "9ee5d2793f0b86e4d88adc6c48430e53d85dc8f3", "_cell_guid": "46d98118-b210-48e9-8575-a012ee266e51", "collapsed": true, "scrolled": false}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Separate rows where target is 0 or 1\n", "target_data = [train_data[\"target\"] == 0, train_data[\"target\"] == 1]\n", "\n", "\n", "trace = []\n", "trace_temp = go.Box(\n", "    y= np.random.choice(train_data['car_13'][target_data[0]],\n", "                        2000,\n", "                        replace=False\n", "                       ),\n", "    name = 'Target = 0',\n", "    boxpoints = 'all',\n", "    boxmean = 'sd'\n", ")\n", "\n", "trace.append(trace_temp)\n", "\n", "trace_temp = go.Box(\n", "    y= np.random.choice(train_data['car_13'][target_data[1]],\n", "                        2000,\n", "                        replace=False\n", "                       ),\n", "    name = 'Target = 1',\n", "    boxpoints = 'all',\n", "    boxmean = 'sd'\n", ")\n", "\n", "trace.append(trace_temp)\n", "\n", "layout = go.Layout(\n", "    title = 'car_13 Feature Distribution',\n", "    width = 900,\n", "    height = 1000,\n", ")\n", "\n", "fig = go.Figure(data = trace, layout = layout)\n", "iplot(fig)"]}, {"metadata": {"_uuid": "955a61b107d50bfe010f8197dd04e2fd89e6af70", "_cell_guid": "b4de6723-4c8d-478f-beae-cc943c87af69"}, "cell_type": "markdown", "source": ["## Data Pre-processing"]}, {"metadata": {"_uuid": "e0d6b8b3badabc39aa5d794eca3e2fdd0b1b83a7", "_cell_guid": "7ef539b6-ce69-4001-a514-678c05223fcb", "collapsed": true}, "cell_type": "markdown", "source": ["Let's begin preparing our data to build our predictive models. To start, we will split the training data into training (\"train\") and cross-validation (\"CV\") sets. The data will be split with the following distribution:\n", "\n", "    90% train\n", "    10% CV"]}, {"metadata": {"_uuid": "e0e1165a3e7e2cfbd4b8668ac2265c1d84fa943c", "_cell_guid": "20ba2161-2afa-4688-bfef-b190ccbf71ad", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Randomly select 10% of data for CV set\n", "CV_index = np.random.choice(train_data.shape[0], int(train_data.shape[0]*.1), replace = False)\n", "CV = train_data.iloc[CV_index, :]\n", "\n", "# Use remaining 90% for model training data\n", "train = train_data.drop(CV_index, axis = 0)\n", "\n", "assert(train_data.shape[0] == train.shape[0]+CV.shape[0])\n", "\n", "print('Number of training examples: {0}'.format(train.shape[0]))\n", "print('Number of cross-validation examples: {0}'.format(CV.shape[0]))"]}, {"metadata": {"_uuid": "6dde05841ec16e5d1873ba9e4b0abd4a5f926b53", "_cell_guid": "66d0f1e3-9e5f-4319-a944-44a758de03a3"}, "cell_type": "markdown", "source": ["Before we get started, we want to make sure that both our training and cross-validation sets contain a distribution of 0's and 1's for the target data"]}, {"metadata": {"_uuid": "805ec2320b6081fd83da47420920136b096b94e3", "_cell_guid": "f4c83ac6-21bc-4a60-9026-1112b19b5428", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["labels = [\"Target = 0\", \"Target = 1\"]\n", "train_values = train[\"target\"].value_counts().values\n", "CV_values = CV[\"target\"].value_counts().values\n", "\n", "#Distribution of training data set Target variable\n", "trace1 = go.Pie(labels =  labels,\n", "                values = train_values,\n", "                domain= {\"x\": [0, 0.45]},\n", "                hole = 0.3\n", "               )\n", "\n", "#Distribution of CV set Target variable\n", "trace2 = go.Pie(labels = labels,\n", "                values = CV_values,\n", "                domain= {\"x\": [0.55, 1]},\n", "                hole = 0.3\n", "               )\n", "\n", "layout = go.Layout(title = 'Distribution of Target Variable',\n", "                    annotations = [{\"text\": \"Train\",\n", "                                    \"font\": {\"size\": 20},\n", "                                    \"showarrow\": False,\n", "                                    \"x\": 0.19,\n", "                                    \"y\": 0.5\n", "                                   },\n", "                                   {\"text\": \"CV\",\n", "                                    \"font\": {\"size\": 20},\n", "                                    \"showarrow\": False,\n", "                                    \"x\": 0.8,\n", "                                    \"y\": 0.5\n", "                                   },\n", "                                  ]\n", "                   )\n", "\n", "fig = go.Figure(data = [trace1, trace2], layout = layout)\n", "\n", "iplot(fig)"]}, {"metadata": {"_uuid": "9a59d82801f0420f1200b132c8294d5eaa18dd61", "_cell_guid": "50d959f9-6e6b-4ddd-9d52-e35e9ac084c9"}, "cell_type": "markdown", "source": ["Everything looks ok, so it's time to setup our model and make some predictions!"]}, {"metadata": {"_uuid": "059360a8b9e02ed7b1e588d422a69b4578799677", "_cell_guid": "581fb818-1971-4d6c-bcd1-70628e1fe259", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Set Random Forest Model parameters\n", "rf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\n", "\n", "#Fit the model to our training data\n", "rf.fit(train.drop([\"id\", \"target\"], axis=1), train.target)\n", "print('------training done-------')"]}, {"metadata": {"_uuid": "942c1e87c816b97023b785914269ecac6e4489cd", "_cell_guid": "6a4414e3-2a64-4640-b505-0762d26c032c"}, "cell_type": "markdown", "source": ["Now that we have trained our random forest model we should take a look at the feature importances that the model decided accounts for the variance in the Target variable."]}, {"metadata": {"_uuid": "a664d37ea08ffe6c47bf8084961a50bc7e63d8fa", "_cell_guid": "32bcf982-f26c-4ba5-807c-9351b35b1fad", "collapsed": true, "scrolled": false}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# make a list of the data features\n", "features = train.drop([\"id\", \"target\"],axis=1).columns.values\n", "\n", "# Extract feature importances\n", "importance_df = pd.DataFrame(\n", "    data = {'features': features,\n", "            'Importance': rf.feature_importances_\n", "           }\n", ").sort_values(by='Importance',\n", "              ascending=True)\n", "\n", "# Feature importance barplot\n", "trace = go.Bar(\n", "    x=importance_df.iloc[:, 0],\n", "    y=importance_df.iloc[:, 1],\n", "    marker=dict(color = importance_df.iloc[:, 0],\n", "                colorscale = 'Viridis',\n", "                reversescale = True\n", "               ),\n", "    name = 'Random Forest Feature Importance',\n", "    orientation = 'h'\n", ")\n", "\n", "layout = go.Layout(title='Barplot of Feature Importances',\n", "                   width = 900,\n", "                   height = 2000,\n", "                  )\n", "fig = go.Figure(data=[trace], layout = layout)\n", "iplot(fig)"]}, {"metadata": {"_uuid": "776b2bf4092112f54d1f4727864be0730b5d85e1", "_cell_guid": "0476ba8b-1feb-44ca-a60f-63641331dc2a", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#Predict probability of filing a claim (Target = 0 or 1)\n", "rf_proba = rf.predict_proba(CV.drop([\"id\", \"target\"], axis=1))\n", "\n", "#Isolate column of predicted probabilities that Target = 1\n", "rf_proba_0 = rf_proba[:, 1].reshape(rf_proba.shape[0],1)"]}, {"metadata": {"_uuid": "5ff76f5430b940370defecb8fd3a7b9a5d78d35d", "_cell_guid": "b99b4023-e782-4c26-bf8b-6176ea2f6ebc"}, "cell_type": "markdown", "source": ["For this project the metric we will use to evaluate the performance of our model is based on the gini coefficient. This coefficient will rank the drivers in order from most to least likely to file a claim, and will compare the ranking between our predictions and the actual sorting according to our CV data.\n", "\n", "Calculation of gini coefficient. Adapted from code submitted on \n", "Allstate Claim Prediction Challenge discussion boards\n", "[here](https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703)"]}, {"metadata": {"_uuid": "cbb76257f631aa81ec8154be4816329a3b0cb22b", "_cell_guid": "27ab374c-224a-4334-8b75-914211611e86", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["def gini(a, p):\n", "    data = np.asarray(np.c_[a, p, np.arange(len(a))],\n", "                      dtype=np.float\n", "                     )\n", "    data = data[\n", "        np.lexsort((data[:,2], -1*data[:,1]))\n", "    ]\n", "    totalLosses = data[:,0].sum()\n", "    giniSum = data[:,0].cumsum().sum() / totalLosses\n", "\n", "    giniSum -= (len(a) + 1) / 2.\n", "    return giniSum / len(a)\n", "\n", "def gini_norm(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "\n", "gini_coef = gini_norm(CV['target'], rf_proba_0)\n", "\n", "print('Normalized Gini Coefficient: {0:0.3f}'.format(gini_coef))"]}, {"metadata": {"_uuid": "491195b84a276592478f5cf2ff5ac8f63b69d2ce", "_cell_guid": "5c031c04-13f8-4cc6-a725-6a9a6c09ee73"}, "cell_type": "markdown", "source": ["With this, we are able to get a gini coefficient score of 0.256. It's better than random guessing, which should be around 0, but there is still a long way to go here.\n", "\n", "Next we will perform some error analysis to see where our model is lacking..."]}, {"metadata": {"_uuid": "6766a17be30ecfe41c09fc89a75e59641512c3f8", "_cell_guid": "ca1eb0e2-e61d-40e5-8de0-6e29556ecdc3"}, "cell_type": "markdown", "source": ["### In Progress..."]}]}