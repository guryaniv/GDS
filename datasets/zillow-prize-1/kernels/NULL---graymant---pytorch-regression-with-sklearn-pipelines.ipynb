{"cells": [{"metadata": {"_cell_guid": "a5a7cd57-2d5f-48f0-b1ef-00969ff886b2", "_uuid": "4d90a5748f6c6b944c3aebaa90a7294677d1d20c"}, "cell_type": "markdown", "source": ["## Introduction\n", "\n", "The objective of this notebook is to show how to use pytorch as a scikit learn regressor to make predictions. The advantage of using it in this way, is that you can tune the hyperparameters for the model using sklearn grid or random parameter search.\n", "\n", "The cleaning and formatting of the data is done using sklearn pipelines. The idea behind this is that it makes it easy to construct different features for different models and the various forms of feature engineering can be included in the gridsearch. In this notebook however, we only use one pipeline so hopefully I'll have time to demonstrate this at a later date.\n", "\n", "Ok, lets get started by importing what we need."]}, {"metadata": {"_cell_guid": "19a54d50-0f9a-4f86-b245-96f3ecc74fe6", "_uuid": "faadfb4caeb3c4e20ee030111661b4afdbc674d4", "collapsed": true}, "execution_count": null, "source": ["import torch\n", "from torch.autograd import Variable\n", "import torch.utils.data as data_utils\n", "import torch.nn.init as init\n", "\n", "import numpy as np\n", "import pandas as pd\n", "\n", "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n", "from sklearn.metrics import mean_absolute_error\n", "from sklearn.externals.joblib import Memory\n", "from sklearn.preprocessing import Imputer\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.pipeline import make_pipeline\n", "\n", "from tempfile import mkdtemp\n", "import datetime\n", "from dateutil.parser import parse\n", "import inspect\n", "from numbers import Number\n", "import math\n", "\n", "import zlib\n", "import zipfile"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "2402d2f5-b52b-471d-806f-97e0388f4719", "_uuid": "a88a4a4400690ea2ae347b30a3e4b7c61b4a6112"}, "cell_type": "markdown", "source": ["## Pytorch regressor\n", "\n", "So first up, let's construct scikit learn regressor containing a pytorch model. In case you want to try this outside of kaggle kernels, I've included the code to run the model on the GPU.\n", "\n", "This regressor is currently missing the ability to pass in a test set and I haven't included dropout in the model. You can add this yourself, or I'll include the code if anyone mentions it would be useful in the comments."]}, {"metadata": {"_cell_guid": "edd7f0fd-1f63-4c16-921b-c77a59da87a0", "_uuid": "a6e97f2114448d1cdbe3db2940f314a1b63ae737", "collapsed": true}, "execution_count": null, "source": ["class PytorchRegressor(BaseEstimator, RegressorMixin):\n", "    \"\"\"A pytorch regressor\"\"\"\n", "\n", "    def __init__(self, output_dim=1, input_dim=100, hidden_layer_dims=[100, 100],\n", "                 num_epochs=1, learning_rate=0.01, batch_size=128, shuffle=False,\n", "                 callbacks=[], use_gpu=True, verbose=1):\n", "        \"\"\"\n", "        Called when initializing the regressor\n", "        \"\"\"\n", "        self._history = None\n", "        self._model = None\n", "        self._gpu = use_gpu and torch.cuda.is_available()\n", "\n", "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n", "        values.pop(\"self\")\n", "\n", "        for arg, val in values.items():\n", "            setattr(self, arg, val)\n", "\n", "    def _build_model(self):\n", "        self._layer_dims = [self.input_dim] + \\\n", "            self.hidden_layer_dims + [self.output_dim]\n", "\n", "        self._model = torch.nn.Sequential()\n", "\n", "        # Loop through the layer dimensions and create an input layer, then\n", "        # create each hidden layer with relu activation.\n", "        for idx, dim in enumerate(self._layer_dims):\n", "            if (idx < len(self._layer_dims) - 1):\n", "                module = torch.nn.Linear(dim, self._layer_dims[idx + 1])\n", "                init.xavier_uniform(module.weight)\n", "                self._model.add_module(\"linear\" + str(idx), module)\n", "\n", "            if (idx < len(self._layer_dims) - 2):\n", "                self._model.add_module(\"relu\" + str(idx), torch.nn.ReLU())\n", "\n", "        if self._gpu:\n", "            self._model = self._model.cuda()\n", "\n", "    def _train_model(self, X, y):\n", "        torch_x = torch.from_numpy(X).float()\n", "        torch_y = torch.from_numpy(y).float()\n", "        if self._gpu:\n", "            torch_x = torch_x.cuda()\n", "            torch_y = torch_y.cuda()\n", "\n", "        train = data_utils.TensorDataset(torch_x, torch_y)\n", "        train_loader = data_utils.DataLoader(train, batch_size=self.batch_size,\n", "                                             shuffle=self.shuffle)\n", "\n", "        loss_fn = torch.nn.MSELoss(size_average=False)\n", "\n", "        optimizer = torch.optim.Adam(\n", "            self._model.parameters(), lr=self.learning_rate)\n", "\n", "        self._history = {\"loss\": [], \"val_loss\": [], \"mse_loss\": []}\n", "\n", "        finish = False\n", "        for epoch in range(self.num_epochs):\n", "            if finish:\n", "                break\n", "\n", "            loss = None\n", "            idx = 0\n", "            for idx, (minibatch, target) in enumerate(train_loader):\n", "                y_pred = self._model(Variable(minibatch))\n", "\n", "                loss = loss_fn(y_pred, Variable(\n", "                    target.cuda().float() if self._gpu else target.float()))\n", "\n", "                optimizer.zero_grad()\n", "                loss.backward()\n", "                optimizer.step()\n", "\n", "            y_labels = target.cpu().numpy() if self._gpu else target.numpy()\n", "            y_pred_results = y_pred.cpu().data.numpy() if self._gpu else y_pred.data.numpy()\n", "\n", "            error = mean_absolute_error(y_labels, y_pred_results)\n", "\n", "            self._history[\"mse_loss\"].append(loss.data[0])\n", "            self._history[\"loss\"].append(error)\n", "\n", "            if self.verbose > 0:\n", "                print(\"Results for epoch {}, loss {}, mse_loss {}\".format(epoch + 1,\n", "                                                                          error, loss.data[0]))\n", "            for callback in self.callbacks:\n", "                callback.call(self._model, self._history)\n", "                if callback.finish:\n", "                    finish = True\n", "                    break\n", "\n", "    def fit(self, X, y):\n", "        \"\"\"\n", "        Trains the pytorch regressor.\n", "        \"\"\"\n", "\n", "        assert (type(self.input_dim) ==\n", "                int), \"input_dim parameter must be defined\"\n", "        assert (type(self.output_dim) == int), \"output_dim must be defined\"\n", "\n", "        self._build_model()\n", "        self._train_model(X, y)\n", "\n", "        return self\n", "\n", "    def predict(self, X, y=None):\n", "        \"\"\"\n", "        Makes a prediction using the trained pytorch model\n", "        \"\"\"\n", "        if self._history == None:\n", "            raise RuntimeError(\"Regressor has not been fit\")\n", "\n", "        results = []\n", "        split_size = math.ceil(len(X) / self.batch_size)\n", "\n", "        # In case the requested size of prediction is too large for memory (especially gpu)\n", "        # split into batchs, roughly similar to the original training batch size. Not\n", "        # particularly scientific but should always be small enough.\n", "        for batch in np.array_split(X, split_size):\n", "            x_pred = Variable(torch.from_numpy(batch).float())\n", "            y_pred = self._model(x_pred.cuda() if self._gpu else x_pred)\n", "            y_pred_formatted = y_pred.cpu().data.numpy() if self._gpu else y_pred.data.numpy()\n", "            results = np.append(results, y_pred_formatted)\n", "\n", "        return results\n", "\n", "    def score(self, X, y, sample_weight=None):\n", "        \"\"\"\n", "        Scores the data using the trained pytorch model. Under current implementation\n", "        returns negative mae.\n", "        \"\"\"\n", "        y_pred = self.predict(X, y)\n", "        return mean_absolute_error(y, y_pred) * -1"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "6b028791-44ae-4441-93ee-8e725b18701a", "_uuid": "8d2a31b68d8c8120a4519d06a492b7cf295f4fe4"}, "cell_type": "markdown", "source": ["## Data pipeline\n", "Now lets build the various components that will later make up our data pipeline. This will do things such as removing outliers, filling missing fields and label encoding. These can be combined with the build in scikit learn transformers (PCA etc) to create the final input data."]}, {"metadata": {"_cell_guid": "3c505e7f-d61b-4026-aa15-f632ebed2f90", "_uuid": "017b4b9ecdf1a8ea81e49c986d6e85cec2fbdc65", "collapsed": true}, "execution_count": null, "source": ["class OutlierRemover(BaseEstimator, TransformerMixin):\n", "    def __init__(self, field=\"logerror\", min_val=-0.4, max_val=0.4):\n", "        self.min_val = min_val\n", "        self.max_val = max_val\n", "        self.field = field\n", "\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        return data.query(\n", "            \"{field} > {min_val} and {field} < {max_val}\".format(\n", "                field=self.field,\n", "                min_val=self.min_val,\n", "                max_val=self.max_val))\n", "\n", "class DateEncoder(BaseEstimator, TransformerMixin):\n", "    def __init__(self):\n", "        self.base_date = datetime.datetime(1600, 1, 1, 0, 0)\n", "\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        data[\"transactiondate\"] = data[\"transactiondate\"].apply(\n", "            lambda date: date if isinstance(date, Number) else (\n", "                parse(str(date)) - self.base_date).days)\n", "        return data\n", "\n", "\n", "class LabelEncodeObjects(BaseEstimator, TransformerMixin):\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        for c in data.columns:\n", "            if data[c].dtype == 'object':\n", "                lbl = LabelEncoder()\n", "                lbl.fit(list(data[c].values))\n", "                data[c] = lbl.transform(list(data[c].values))\n", "        return data\n", "\n", "\n", "class LabelEncodeCols(BaseEstimator, TransformerMixin):\n", "    def __init__(self, cols=[]):\n", "        self.cols = cols\n", "\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        for col in self.cols:\n", "            lbl = LabelEncoder()\n", "            lbl.fit(list(data[col].values))\n", "            data[col] = lbl.transform(list(data[col].values))\n", "        return data\n", "\n", "\n", "class NaFiller(BaseEstimator, TransformerMixin):\n", "    def __init__(self, fill_val=-1):\n", "        self.fill_val = fill_val\n", "\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        return data.fillna(self.fill_val)\n", "\n", "\n", "class NaColFiller(BaseEstimator, TransformerMixin):\n", "    def __init(self, fill_val=-1, cols=[]):\n", "        self.fill_val = fill_val\n", "        self.cols = cols\n", "\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        for col in cols:\n", "            data[col] = data[col].fillna(self.fill_val)\n", "        return data\n", "\n", "\n", "class NaColMeanFiller(BaseEstimator, TransformerMixin):\n", "    def __init(self, cols=[]):\n", "        self.cols = cols\n", "\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        for col in cols:\n", "            data[col] = data[col].fillna(data[col].mean)\n", "        return data\n", "\n", "\n", "class ColumnDropper(BaseEstimator, TransformerMixin):\n", "    def __init__(self, cols=[]):\n", "        self.cols = cols\n", "\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        return data.drop(self.cols, axis=1)\n", "\n", "\n", "class Cloner(BaseEstimator, TransformerMixin):\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        return data.copy()\n", "\n", "\n", "class ColumnOrderer(BaseEstimator, TransformerMixin):\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data):\n", "        return data.sort_index(axis=1)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "25c3d9ba-4de0-45bc-a82d-e15ea151b4aa", "_uuid": "f7fea484455ec872a6e4653630a9f53ac337f4cc"}, "cell_type": "markdown", "source": ["Now it's time to read in the data "]}, {"metadata": {"_cell_guid": "205f71e2-45c0-4ca6-8797-cdc571cc94fe", "_uuid": "02aab99a74ced014e17263c46ba963c897fe3d8e"}, "execution_count": null, "source": ["prop = pd.read_csv('../input/properties_2016.csv')\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n", "\n", "df_train = train.merge(prop, how='left', on='parcelid')"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "4348a097-ea69-454b-bfa6-28a7377c2876", "_uuid": "83358cc9d3ccf996bc34db867e9ecf0348da7232"}, "cell_type": "markdown", "source": ["Lets drop the outliers and split into x and y"]}, {"metadata": {"_cell_guid": "eddac068-9aa1-4ee5-ae30-6892d69646f4", "_uuid": "30c97a726e62a9ec772a37ae235e8105c4433c1d", "collapsed": true}, "execution_count": null, "source": ["def make_train_set():\n", "    reduced = OutlierRemover().transform(df_train)\n", "    x_train = reduced.drop([\"logerror\"], axis = 1)\n", "    y_train = reduced[\"logerror\"]\n", "    return (x_train, y_train)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "962bc4f7-bdbe-4d38-a612-3449b285f119", "_uuid": "6a9fefb2f17ea08d7242c3f6e8021ac002f9325e"}, "cell_type": "markdown", "source": ["Just a technical point, add caching for the data pipeline"]}, {"metadata": {"_cell_guid": "20280d3f-bcf2-462a-968b-a4f87a36247e", "_uuid": "2b5e206e20d71830a871b2ad84a1f6589c565367", "collapsed": true}, "execution_count": null, "source": ["cachedir = mkdtemp()\n", "memory = Memory(cachedir=cachedir, verbose=0)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "26788ca5-a60b-48d4-97a6-e84990dff18e", "_uuid": "4ca80ecdd991ab92a08bcffd5c7eeb2bd2db08d4"}, "cell_type": "markdown", "source": ["Add a list of columns to be dropped and id columns to be used by our data processing pipeline."]}, {"metadata": {"_cell_guid": "9a0d5060-6e6d-40df-8555-23acac95874f", "_uuid": "bba4194379e602e44d0ff4c942899bad80ac6cf9", "collapsed": true}, "execution_count": null, "source": ["drop_cols = ['finishedsquarefeet6', 'finishedsquarefeet12', 'finishedsquarefeet13',\n", "             'finishedsquarefeet15', 'parcelid']\n", "id_cols = ['heatingorsystemtypeid', 'propertylandusetypeid', 'storytypeid', \n", "           'airconditioningtypeid','architecturalstyletypeid', 'buildingclasstypeid', \n", "           'buildingqualitytypeid', 'typeconstructiontypeid']"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "f71de58e-9ca9-4cb0-a59b-ff4cebadf450", "_uuid": "b20974e07b4743dbce641b22f0554bdeeb95a3f2"}, "cell_type": "markdown", "source": ["Now we can construct the pipeline and transform the training data ready to train the model."]}, {"metadata": {"_cell_guid": "b40fbf94-b247-4ca4-b859-af064cf2ffb1", "_uuid": "726024fa26d31920c69b89a560438bddfc15c84e", "collapsed": true}, "execution_count": null, "source": ["dp = make_pipeline(Cloner(), DateEncoder(), ColumnDropper(cols=drop_cols),\n", "                   NaFiller(), LabelEncodeObjects(), LabelEncodeCols(cols=id_cols), ColumnOrderer(),\n", "                   StandardScaler())\n", "\n", "x_train_df, y_train = make_train_set()\n", "x_train = dp.fit_transform(x_train_df)"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "a7ba85aa-3cb4-48c3-b513-ae322fe84b1e", "_uuid": "11c95c9e476fbb0c0b89dd02e6c576baef8606b4"}, "cell_type": "markdown", "source": ["Now we are ready to build and train out pytorch models. As an example of what can be done, I have selected and arbitrary set of different hidden layers and made 4 different models. \n", "\n", "As mentioned in the introduction, to optimise this, you could use a grid or random search to find the optimal network structure and hyperparameters. But for now, this should show what can be done."]}, {"metadata": {"_cell_guid": "ee885fab-422c-48bd-9756-30a3805c4d3f", "_uuid": "c34797d3a262e6b54ca5c3d0ab04f25e70ab210d", "collapsed": true}, "execution_count": null, "source": ["num_features = 54\n", "batch_size = 4096\n", "learning_rate = 0.0007\n", "\n", "clf1 = PytorchRegressor(input_dim=num_features, hidden_layer_dims=[1200, 500, 100, 10],\n", "                        learning_rate=0.0005, batch_size=batch_size, num_epochs=10)\n", "clf2 = PytorchRegressor(input_dim=num_features, hidden_layer_dims=[500, 500],\n", "                        learning_rate=learning_rate, batch_size=batch_size, num_epochs=10)\n", "clf3 = PytorchRegressor(input_dim=num_features, hidden_layer_dims=[1500, 500, 10],\n", "                        learning_rate=learning_rate, batch_size=batch_size, num_epochs=10)\n", "clf4 = PytorchRegressor(input_dim=num_features, hidden_layer_dims=[1000, 800, 500, 200, 100, 10],\n", "                        learning_rate=learning_rate, batch_size=batch_size, num_epochs=10)\n", "\n", "estimators = [clf1, clf2, clf3, clf4]"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "4d1418f3-d8db-4fc5-91f0-80ffc9ea31b0", "_uuid": "1646546306e4926a4629d5247ad8b523bc5471c6"}, "cell_type": "markdown", "source": ["We are now ready to train the various classifiers. I am going to use the same set of training data for each, but for diversity, you could create different pipelines that are optimal for different models."]}, {"metadata": {"_cell_guid": "d56e91da-d68b-487f-a211-1465afe86a57", "_uuid": "5fcf0a0f32da9e0b024968ac914163eaa813704c"}, "execution_count": null, "source": ["for idx, estimator in enumerate(estimators):\n", "    print(\"Fitting\", idx + 1)\n", "    estimator.fit(x_train, y_train.as_matrix())"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "a03a7c6c-f7a3-4110-9aba-82d09cf0ec0a", "_uuid": "f192ca9ca54fad5731501613403cae17177c30f6"}, "cell_type": "markdown", "source": ["Just to sanity check, let's have a look at the predictions on the training data to check they look ok and are approximately on the right scale."]}, {"metadata": {"_cell_guid": "b9655f51-7bb8-4d3d-8b71-c1dacb55b63b", "_uuid": "50215bf3361d7ced745e9fff16cc8fd23f6dab3a"}, "execution_count": null, "source": ["print(\"Classifier 1\")\n", "print(clf1.predict(x_train))\n", "print(\"Classifier 2\")\n", "print(clf2.predict(x_train))\n", "print(\"Classifier 3\")\n", "print(clf3.predict(x_train))\n", "print(\"Classifier 4\")\n", "print(clf4.predict(x_train))"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "ff72810d-1bcd-4465-9cfd-9364c6e65189", "_uuid": "69d8806d856715aadd929714bf0d482c5b553232"}, "cell_type": "markdown", "source": ["Yep, looks ok. So lets move on to the final predictions. Lets run the full set of properties through the pipeline and make predictions on it.\n", "\n", "I'll encode the date up front, but this is really just an optimisation to make the pipeline quicker so this code can be safely ignored.\n", "\n", "After calculating the predictions, we'll add them all to the submission file in the correct format."]}, {"metadata": {"_cell_guid": "d0fad2c5-d28d-48e4-b652-ac19029b92a7", "_uuid": "054aa47fe32c4606dbb68d8f43001355ec03ff97", "collapsed": true}, "execution_count": null, "source": ["def encode_date(date):\n", "    base_date = datetime.datetime(1600, 1, 1, 0, 0)\n", "    return (parse(str(date)) - base_date).days\n", "\n", "full_predict = prop\n", "full_predict[\"transactiondate\"] = encode_date(\"2016-10-1\")"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "1e4963b4-2856-4512-9e1b-3bdd390c3f39", "_uuid": "2a350b4b90b67eb3dc383349af5cd6dc7c62a7fb"}, "execution_count": null, "source": ["predict_dates= [\"2016-10-1\", \"2016-11-1\", \"2016-12-1\", \"2017-10-1\", \"2017-11-1\", \"2017-12-1\"]\n", "full_results = { \"ParcelId\" : full_predict[\"parcelid\"]}\n", "\n", "for idx, date in enumerate(predict_dates):\n", "    print(\"Predicting\", date)\n", "    full_predict[\"transactiondate\"] = encode_date(date)\n", "    x_pred = dp.fit_transform(full_predict)\n", "    \n", "    pred1 = clf1.predict(x_pred)\n", "    pred2 = clf2.predict(x_pred)\n", "    pred3 = clf3.predict(x_pred)\n", "    pred4 = clf4.predict(x_pred)\n", "    pred_all = 0.25 * pred1 + 0.25 * pred2 + 0.25 * pred3 + 0.25 * pred4\n", "    \n", "    date_header = date.replace(\"-\", \"\")[:-1]\n", "    full_results[date_header] = pred_all\n", "\n", "## Reorder dataframe to ensure ParcelId is at the start\n", "submission = pd.DataFrame(full_results)\n", "pid = submission['ParcelId']\n", "submission.drop(labels=['ParcelId'], axis=1,inplace = True)\n", "submission.insert(0, 'ParcelId', pid)\n", "\n", "## Show the head of the submission to see that all is in order\n", "submission"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "ef09db37-cf23-45fa-b518-da7bd061a5cc", "_uuid": "4c738bf8d4bb812772a0946468e8875fdfcf77f1"}, "cell_type": "markdown", "source": ["Finally, we can save the predictions to disk and zip them ready for submission. Or the unzipped version can be combined with other models."]}, {"metadata": {"_cell_guid": "5d7efe34-a227-4289-914f-aa21dbdc5c59", "_uuid": "892cb12ad193a84463727f8af8dfb5983362fc34", "collapsed": true}, "execution_count": null, "source": ["sub_file_name = 'pytorch_predictions.csv'\n", "submission.to_csv(sub_file_name, index=False, float_format='%.4f')\n", "\n", "zf = zipfile.ZipFile(sub_file_name + '.zip', mode='w')\n", "\n", "print('Creating zip')\n", "zf.write(sub_file_name, compress_type=zipfile.ZIP_DEFLATED)\n", "zf.close()"], "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "4edd76b8-1f85-4602-a5e6-5b15382f9a77", "_uuid": "c690335fc64285f21c96428571e23c862d6aa841", "collapsed": true}, "execution_count": null, "source": [], "cell_type": "code", "outputs": []}], "nbformat": 4, "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1}