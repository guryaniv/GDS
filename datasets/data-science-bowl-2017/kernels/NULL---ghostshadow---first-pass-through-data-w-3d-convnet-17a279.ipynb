{"nbformat_minor": 0, "cells": [{"outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "1aad7a07-c9aa-fe7b-6727-d1e570905b5f", "_uuid": "b406b2ed3c475dfeb95e860548d434fa8b871011"}, "execution_count": null, "source": "import os\nimport pandas as pd\nimport dicom\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nIMG_PX_SIZE = 64\nFC_SIZE = 16 * 16 * 16 * 8\n\nN_CLASSES = 2\nBATCH_SIZE = 10\nHM_EPOCHS = 10\n\ndata_dir = '../input/sample_images/'\npatients = os.listdir(data_dir)\nlabels_df = pd.read_csv('../input/stage1_labels.csv', index_col=0)", "cell_type": "code"}, {"execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "994495ad2da13879e382bfbb344a70d429fe9107", "collapsed": false}, "source": "def process_data(patient,labels_df):    \n    label = labels_df.get_value(patient, 'cancer')\n    path = data_dir + patient\n    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n\n    # Resize the x-y axis\n    slices = [cv2.resize(np.array(each_slice.pixel_array),(IMG_PX_SIZE,IMG_PX_SIZE)) for each_slice in slices]\n    slices = np.array(slices)\n\n    # Rotate the cuboid \n    slices = np.transpose(slices,[2,1,0])\n\n    # Resize the new x-y axis\n    new_slices = []\n    for i in range(len(slices)):\n        new_slices.append(cv2.resize(slices[i],(IMG_PX_SIZE,IMG_PX_SIZE)))\n    new_slices = np.array(new_slices)    \n\n    # Rotate back (Optional: Conv3D doesn't care)\n    new_slices = np.transpose(new_slices,[2,1,0])\n\n    #print(slices.shape,new_slices.shape)\n\n    # Normalize\n    new_slices = np.array(new_slices,dtype='float32')\n    new_slices -= new_slices.min()\n    new_slices /= new_slices.max()\n    #print(new_slices.mean())\n    \n    return new_slices,label", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "c39978a9853b4690f613cd7cedfa82d9ee3dbf38", "collapsed": false}, "source": "much_data = []\nprint('Total Patients: ',len(patients))\n\nfile_name = 'muchdata-{}-{}-{}.npy'.format(IMG_PX_SIZE,IMG_PX_SIZE,IMG_PX_SIZE)\n\nif os.path.isfile(file_name):\n    print('File found, loaded')\n    much_data = np.load(file_name)\nelse:\n    print('File not found, creating')\n    for num,patient in enumerate(patients):\n        if num % 100 == 0:\n            print(num)\n        try:\n            img_data,label = process_data(patient,labels_df)\n            much_data.append([img_data,label])\n        except KeyError as e:\n            print('This is unlabeled data!')\n    np.save(file_name, much_data)\n    print('Saved')", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "0cb7a8dbc684ce4e92745e7a8329ba2bbbdbaa08", "collapsed": false}, "source": "# Added shapes\nx = tf.placeholder('float',[None,IMG_PX_SIZE,IMG_PX_SIZE,IMG_PX_SIZE])\ny = tf.placeholder('float',[None,N_CLASSES])\n\nkeep_rate = 0.8\n\ndef conv3d(x, W):\n    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n\ndef maxpool3d(x):\n    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "8e829c4873e7c888b230191708247e8b403041fa", "collapsed": false}, "source": "def convolutional_neural_network(x):\n    # Changed the variables\n    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,4])),\n               'W_conv2':tf.Variable(tf.random_normal([3,3,3,4,8])),\n               'W_fc':tf.Variable(tf.random_normal([FC_SIZE,1024])),\n               'out':tf.Variable(tf.random_normal([1024, N_CLASSES]))}\n\n    biases = {'b_conv1':tf.Variable(tf.random_normal([4])),\n               'b_conv2':tf.Variable(tf.random_normal([8])),\n               'b_fc':tf.Variable(tf.random_normal([1024])),\n               'out':tf.Variable(tf.random_normal([N_CLASSES]))}\n\n    #                            image X      image Y        image Z\n    x = tf.reshape(x, shape=[-1, IMG_PX_SIZE, IMG_PX_SIZE, IMG_PX_SIZE, 1])\n\n    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n    conv1 = maxpool3d(conv1)\n\n    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n    conv2 = maxpool3d(conv2)\n    \n    print(conv2.get_shape())\n\n    fc = tf.reshape(conv2,[-1, FC_SIZE])\n    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n    fc = tf.nn.dropout(fc, keep_rate)\n\n    output = (tf.matmul(fc, weights['out'])+biases['out'])\n\n    return output", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "6bac7192b4f3db8d8680797f2037db291eeceb6f", "collapsed": false}, "source": "validation_size = int(.2 * len(much_data))\n\ntrainX = []\ntrainY = []\nvalidateX = []\nvalidateY = []\n\nfor data in much_data[:-validation_size]:\n    trainX.append(data[0])\n    trainY.append(np.eye(N_CLASSES)[data[1]])\n\ntrainX = np.array(trainX)\ntrainY = np.array(trainY)\n\nfor data in much_data[-validation_size:]:\n    validateX.append(data[0])\n    validateY.append(np.eye(N_CLASSES)[data[1]])\n\nvalidateX = np.array(validateX)\nvalidateY = np.array(validateY)", "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "95361490a746976804e6d1a39f821d8e86082d5e", "collapsed": false}, "source": "def train_neural_network(x):    \n    prediction = convolutional_neural_network(x)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)    \n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        \n        successful_runs = 0\n        total_runs = 0\n        \n        for epoch in range(HM_EPOCHS):\n            epoch_loss = 0\n            try:\n                indices = [np.random.randint(len(trainX)) for _ in range(BATCH_SIZE)]\n                indices = list(set(indices))\n                \n                batchX = np.take(trainX,indices,0)\n                batchY = np.take(trainY,indices,0)\n                _, c = sess.run([optimizer, cost], feed_dict={x: batchX, y: batchY})\n                epoch_loss += c\n            except Exception as e:                    \n                print(str(e))\n            \n            print('Epoch', epoch+1, 'completed out of',HM_EPOCHS,\n                  'Batch size: ',len(indices),'loss:',epoch_loss)\n\n            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n\n            print('Accuracy:',accuracy.eval({x:validateX, y:validateY}))\n            \n        print('Done. Finishing accuracy:')\n        print('Accuracy:',accuracy.eval({x:validateX, y:validateY}))\n\ntrain_neural_network(x)", "outputs": [], "cell_type": "code"}], "metadata": {"language_info": {"name": "python", "version": "3.6.1", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "_is_fork": false, "_change_revision": 0}, "nbformat": 4}