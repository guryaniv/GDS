{"cells":[{"metadata":{"_uuid":"c3108ed389c051980f763a7b75970c95f4cd4d97"},"cell_type":"markdown","source":"# Introduction\n**\"[Distilling the Knowledge in a Neural Network](http://arxiv.org/abs/1503.02531)\" was introduced by Geoffrey Hinton, Oriol Vinyals, Jeff Dean in Mar 2015. In this kernel, I would like to share some experiments to distill the knowledge from a [LGBM teacher](https://www.kaggle.com/tanreinama/lightgbm-minimize-leaves-with-gaussiannb) (LB:0.899) to a neural network. The student network has not surpassed the teacher model yet (LB:0.894). But, I hope I can make it happen before this competition ends.**\n\n"},{"metadata":{"_uuid":"33da2d6967f8899638219ae1698411ada9d02b7e"},"cell_type":"markdown","source":"# Please upvote if you find this kernel interesting ^_^"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\n# np.random.seed(8)\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, scale\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport keras\nfrom keras.models import Model\nfrom keras import regularizers\nimport tensorflow as tf\nfrom keras.losses import binary_crossentropy\nimport gc\nimport scipy.special\nfrom tqdm import *\nfrom scipy.stats import norm, rankdata\n\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n\n\nBATCH_SIZE = 1024\nNUM_FEATURES = 1200","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b6e65eca995c134ad532f7a8acb7b106575dde9"},"cell_type":"markdown","source":"## **Load the dataset, and the prediction of 5-fold LGBM**\n"},{"metadata":{"trusted":true,"_uuid":"90cce1d9df7f0a4a838bdbe1ae3e190a9ecd7147"},"cell_type":"code","source":"train = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\ntest = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\ntrain_knowledge = pd.read_csv('../input/santander-2019-distillation/lgbm_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c34513f569d5fa7fa79c39f995f09b555c2e2bf"},"cell_type":"code","source":"y = train['target']\ny_knowledge = train_knowledge['target']\nid_code_train = train['ID_code']\nid_code_test = test['ID_code']\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7f26e493b56fe58fdf82fe2936bff8b5c61a846"},"cell_type":"markdown","source":"## Adding some features, the credit belong to these kernels: https://www.kaggle.com/karangautam/keras-nn, https://www.kaggle.com/ymatioun/santander-linear-model-with-additional-features\n"},{"metadata":{"trusted":true,"_uuid":"ac0a1aa3268892879ccb3ebc9e36e39a4612c0dd"},"cell_type":"code","source":"for feature in features:\n    # train['mean_'+feature] = (train[feature].mean()-train[feature])\n    # train['z_'+feature] = (train[feature] - train[feature].mean())/train[feature].std(ddof=0)\n    train['sq_'+feature] = (train[feature])**2\n    # train['sqrt_'+feature] = np.abs(train[feature])**(1/2)\n    train['c_'+feature] = (train[feature])**3\n    # train['p4_'+feature] = (train[feature])**4\n    # train['r1_'+feature] = np.round(train[feature], 1)\n    train['r2_'+feature] = np.round(train[feature], 2)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7624675031843a1c065c982ca501e95a45e4ef90"},"cell_type":"code","source":"for feature in features:\n    # test['mean_'+feature] = (train[feature].mean()-test[feature])\n    # test['z_'+feature] = (test[feature] - train[feature].mean())/train[feature].std(ddof=0)\n    test['sq_'+feature] = (test[feature])**2\n    # test['sqrt_'+feature] = np.abs(test[feature])**(1/2)\n    test['c_'+feature] = (test[feature])**3\n    # test['p4_'+feature] = (test[feature])**4\n    # test['r1_'+feature] = np.round(test[feature], 1)\n    test['r2_'+feature] = np.round(test[feature], 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a98a587502fef9575b74f94dd7e4f41188ab4b37"},"cell_type":"markdown","source":"## Normalize and split data\n"},{"metadata":{"trusted":true,"_uuid":"b631bca7d0fcacc45ca7f35e14717619843e46c5"},"cell_type":"code","source":"class GaussRankScaler():\n\n    def __init__( self ):\n        self.epsilon = 1e-9\n        self.lower = -1 + self.epsilon\n        self.upper =  1 - self.epsilon\n        self.range = self.upper - self.lower\n\n    def fit_transform( self, X ):\n\n        i = np.argsort( X, axis = 0 )\n        j = np.argsort( i, axis = 0 )\n\n        assert ( j.min() == 0 ).all()\n        assert ( j.max() == len( j ) - 1 ).all()\n\n        j_range = len( j ) - 1\n        self.divider = j_range / self.range\n\n        transformed = j / self.divider\n        transformed = transformed - self.upper\n        transformed = scipy.special.erfinv( transformed )\n        ############\n        # transformed = transformed - np.mean(transformed)\n\n        return transformed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"108750758b3cd9eeec6fd646585b73af93b162ec"},"cell_type":"code","source":"SPLIT = len(train)\ntrain = train.append(test)\ndel test; gc.collect()\n# print(train.shape)\nscaler = GaussRankScaler()\nsc = StandardScaler()\nfor feat in tqdm(features):\n    # train[feat] = scaler.fit_transform(train[feat])\n    train[feat] = sc.fit_transform(train[feat].values.reshape(-1, 1))\n    train[feat+'_r'] = rankdata(train[feat]).astype('float32')\n    train[feat+'_n'] = norm.cdf(train[feat]).astype('float32')\n\nfeats = [c for c in train.columns if c not in (['ID_code', 'target'] + features)]\nfor feat in tqdm(feats):\n    train[feat] = sc.fit_transform(train[feat].values.reshape(-1, 1))\n\ntrain = train.drop(['target', 'ID_code'], axis=1)\ntest = train[SPLIT:].values\ntrain = train[:SPLIT].values\n# test = test.drop(['ID_code'], axis=1)\nprint('Done!!')\nprint(train.shape)\n# train.head()\n# train[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f3998f86595789305d3fec999bbc76adc7971a1"},"cell_type":"code","source":"train = np.reshape(train, (-1, NUM_FEATURES, 1))\ntest = np.reshape(test, (-1, NUM_FEATURES, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f3220d0506c8e89ce8a99d1cc8fe77e83275a17"},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid  = train_test_split(train, y, y_knowledge, stratify=y, test_size=0.2, random_state=8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51a4ef6c1d48eb4d83f18248256bf2884f02504c"},"cell_type":"markdown","source":"## Define our student network\n"},{"metadata":{"trusted":true,"_uuid":"09e980734152207ef1a7cec18d717733e40edfba"},"cell_type":"code","source":"function = 'relu'\n# function = keras.layers.advanced_activations.LeakyReLU(alpha=.001)\n\ndef create_model(input_shape, n_out):\n    input_tensor = Input(shape=input_shape)\n    x = Dense(16, activation=function)(input_tensor)\n    x = Flatten()(x)\n    out_put = Dense(n_out, activation='sigmoid')(x)\n    model = Model(input_tensor, out_put)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4b5cc0565c0c1ab30cf2958e98468e4493f0189"},"cell_type":"markdown","source":"## Some necessary functions\n"},{"metadata":{"trusted":true,"_uuid":"5cd85490ab33a4e8583427955fc29a1de218132b"},"cell_type":"code","source":"def auc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8ad256648019aa6908bdda89245d24b2865564a"},"cell_type":"code","source":"gamma = 2.0\nalpha=.25\nepsilon = K.epsilon()\n\ndef focal_loss(y_true, y_pred):\n    pt_1 = y_pred * y_true\n    pt_1 = K.clip(pt_1, epsilon, 1-epsilon)\n    CE_1 = -K.log(pt_1)\n    FL_1 = alpha* K.pow(1-pt_1, gamma) * CE_1\n    \n    pt_0 = (1-y_pred) * (1-y_true)\n    pt_0 = K.clip(pt_0, epsilon, 1-epsilon)\n    CE_0 = -K.log(pt_0)\n    FL_0 = (1-alpha)* K.pow(1-pt_0, gamma) * CE_0\n    \n    loss = K.sum(FL_1, axis=1) + K.sum(FL_0, axis=1)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51906f2d2a1ce3d937df6d401c831cd58a7f9b4e"},"cell_type":"code","source":"def mixup_data(x, y, alpha=1.0):\n    # y = np.array(y)\n    # print(y)\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    sample_size = x.shape[0]\n    index_array = np.arange(sample_size)\n    np.random.shuffle(index_array)\n    \n    mixed_x = lam * x + (1 - lam) * x[index_array]\n    mixed_y = (lam * y) + ((1 - lam) * y[index_array])\n    # print((1 - lam) * y[index_array])\n    # print((lam * y).shape,((1 - lam) * y[index_array]).shape)\n    return mixed_x, mixed_y\n\ndef make_batches(size, batch_size):\n    nb_batch = int(np.ceil(size/float(batch_size)))\n    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n\n\ndef batch_generator(X,y,batch_size=128,shuffle=True,mixup=False):\n    y = np.array(y)\n    # print(X.shape[0], y.shape[0])\n    sample_size = X.shape[0]\n    index_array = np.arange(sample_size)\n    \n    while True:\n        if shuffle:\n            np.random.shuffle(index_array)\n        batches = make_batches(sample_size, batch_size)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = index_array[batch_start:batch_end]\n            X_batch = X[batch_ids]\n            y_batch = y[batch_ids]\n            \n            if mixup:\n                # print('before', X_batch.shape, y_batch.shape)\n                X_batch,y_batch = mixup_data(X_batch,y_batch,alpha=1.0)\n            # print('*****************')    \n            yield X_batch,y_batch","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"703fe0fa610133323f862f412c8e22c3beaf0ad2"},"cell_type":"markdown","source":"## Experiment 1\nFirstly, we check the performance of simple feed forward neural network."},{"metadata":{"trusted":true,"_uuid":"3c657dc6f052fb182cc46027283ae2929dd7a014"},"cell_type":"code","source":"# model = create_model((train.shape[1],), 1)\nmodel = create_model((NUM_FEATURES,1), 1)\n# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\nmodel.summary()\n\ncheckpoint = ModelCheckpoint('feed_forward_model.h5', monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n                                   verbose=1, mode='min', epsilon=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=9)\ncallbacks_list = [checkpoint, reduceLROnPlat, early]\ntr_gen = batch_generator(x_train,y_train,batch_size=BATCH_SIZE,shuffle=True,mixup=True)\n\nhistory = model.fit_generator(# x_train,y_train,\n                                tr_gen,\n                                steps_per_epoch=np.ceil(float(len(x_train)) / float(BATCH_SIZE)),\n                                epochs=10,\n                                verbose=1,\n                                callbacks=callbacks_list,\n                                validation_data=(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d024753128c64a5d31a6d861bcb1d7f38c65366"},"cell_type":"code","source":"model.load_weights('feed_forward_model.h5')\nprediction = model.predict(x_valid, batch_size=512, verbose=1)\nroc_auc_score(y_valid, prediction)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc3a1cf3d0df24df14f636f863f1bc67aac7659f"},"cell_type":"markdown","source":"## Knowledge distillation\nThe basic idea is that you feed both groundtruth and the prediction from the teacher model to the student network.\nSoft targets (the prediction of the teacher model) contains more information than the hard labels (groundtruth) due to the fact that they encode similarity measures between the classes."},{"metadata":{"trusted":true,"_uuid":"4bc6654afc5e5a834efc5ca09a44605eb62b6bee"},"cell_type":"code","source":"y_train = np.vstack((y_train, y_knowledge_train)).T\ny_valid = np.vstack((y_valid, y_knowledge_valid)).T\n\nprint(y_train.shape)\ny_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a70bf72aca056687f154690347a66bba3f8026d2"},"cell_type":"code","source":"def knowledge_distillation_loss_withBE(y_true, y_pred, beta=0.1):\n\n    # Extract the groundtruth from dataset and the prediction from teacher model\n    y_true, y_pred_teacher = y_true[: , :1], y_true[: , 1:]\n    \n    # Extract the prediction from student model\n    y_pred, y_pred_stu = y_pred[: , :1], y_pred[: , 1:]\n\n    loss = beta*binary_crossentropy(y_true,y_pred) + (1-beta)*binary_crossentropy(y_pred_teacher, y_pred_stu)\n\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6452176b0ffc214ea09c24c8b5a83a2ffd102c51"},"cell_type":"code","source":"def auc_2(y_true, y_pred):\n    y_true = y_true[:, :1]\n    y_pred = y_pred[:, :1]\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\ndef auc_3(y_true, y_pred):\n    y_true = y_true[:, :1]\n    y_pred = y_pred[:, 1:]\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be939049ab91a59440245cc4243561fd60e4bc4c"},"cell_type":"markdown","source":"## Experment 2\nWe set the ratio between teacher's prediction and groundtruth is 1:9, and use the basic binary crossentropy loss."},{"metadata":{"trusted":true,"_uuid":"c3b91e105d49537cae2d1e43294ffa4483e4298f"},"cell_type":"code","source":"# model = create_model((train.shape[1],), 2)\nmodel = create_model((NUM_FEATURES,1), 2)\nmodel.compile(loss=knowledge_distillation_loss_withBE, optimizer='adam', metrics=[auc_2])\n\ncheckpoint = ModelCheckpoint('student_model_BE.h5', monitor='val_auc_2', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n                                   verbose=1, mode='max', epsilon=0.0001)\nearly = EarlyStopping(monitor=\"val_auc_2\", \n                      mode=\"max\", \n                      patience=9)\ncallbacks_list = [checkpoint, reduceLROnPlat, early]\n\nhistory = model.fit(x_train,y_train,\n                    epochs=10,\n                    batch_size = BATCH_SIZE,\n                    validation_data=(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efa0411e6495f5ff309650a6d77ddeca65948278"},"cell_type":"markdown","source":"## Experment 3\nWe set the ratio between teacher's prediction and groundtruth is 1:9, and use the focal loss."},{"metadata":{"trusted":true,"_uuid":"442f656984e998fc9d425b19736936c6db73bd43"},"cell_type":"code","source":"def knowledge_distillation_loss_withFL(y_true, y_pred, beta=0.1):\n\n    # Extract the groundtruth from dataset and the prediction from teacher model\n    y_true, y_pred_teacher = y_true[: , :1], y_true[: , 1:]\n    \n    # Extract the prediction from student model\n    y_pred, y_pred_stu = y_pred[: , :1], y_pred[: , 1:]\n\n    loss = beta*focal_loss(y_true,y_pred) + (1-beta)*binary_crossentropy(y_pred_teacher, y_pred_stu)\n\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6641a36d26185a8b98fc4b7001a18814951e4101"},"cell_type":"code","source":"# model = create_model((train.shape[1],), 2)\nmodel = create_model((NUM_FEATURES,1), 2)\nmodel.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2, auc_3])\n\ncheckpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n                                   verbose=1, mode='max', epsilon=0.0001)\nearly = EarlyStopping(monitor=\"val_auc_2\", \n                      mode=\"max\", \n                      patience=9)\ncallbacks_list = [checkpoint, reduceLROnPlat, early]\n\nhistory = model.fit(x_train,y_train,\n                    epochs=10,\n                    batch_size = BATCH_SIZE,\n                    callbacks=callbacks_list,\n                    validation_data=(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec61ffa8dc03af2c7a8c88e59bee2edb8b34d740"},"cell_type":"markdown","source":"## Experment 4\nTuning hyper parameter \"Temperature\"."},{"metadata":{"trusted":true,"_uuid":"3283fdd87f84de160fbcfd8002aed16a7cd866ef"},"cell_type":"code","source":"from scipy.special import logit\n\ndef sigmoid(x, derivative=False):\n  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n\nTEMPERATURE = 2\n\ny_knowledge_logit = logit(y_knowledge)\ny_temperature = sigmoid(y_knowledge_logit/TEMPERATURE)\n\n# del x_train, x_valid; gc.collect()\n\nx_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid  = train_test_split(train, y, y_temperature,\n                                                                                             stratify=y, test_size=0.2, random_state=8)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac8842dc0d6dacb8c062cf7cfa3edcd4799e4a34"},"cell_type":"code","source":"y_train = np.vstack((y_train, y_knowledge_train)).T\ny_valid = np.vstack((y_valid, y_knowledge_valid)).T\n\nprint(y_train.shape)\ny_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ecf279fbb4693b6e6e2d55d1d048e520c858f22"},"cell_type":"code","source":"# model = create_model((train.shape[1],), 2)\nmodel = create_model((NUM_FEATURES,1), 2)\nmodel.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2,auc_3])\n\ncheckpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n                                   verbose=1, mode='max', epsilon=0.0001)\nearly = EarlyStopping(monitor=\"val_auc_2\", \n                      mode=\"max\", \n                      patience=9)\ncallbacks_list = [checkpoint, reduceLROnPlat, early]\n\nhistory = model.fit(x_train,y_train,\n                    epochs=10,\n                    batch_size = 1024,\n                    callbacks=callbacks_list,\n                    validation_data=(x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ad7d4d62ed9f722334fbff7ccb6a513d15b98d1"},"cell_type":"code","source":"# run k-fold\nnum_fold = 5\nfolds = list(StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=7).split(train, y))\n# del x_train, x_valid; gc.collect()\n\ny_test_pred_log = np.zeros(len(train))\ny_train_pred_log = np.zeros(len(train))\nprint(y_test_pred_log.shape)\nprint(y_train_pred_log.shape)\nscore = []\n\nfor j, (train_idx, valid_idx) in enumerate(folds):\n    print('\\n===================FOLD=',j)\n    x_train, x_valid = train[train_idx], train[valid_idx]\n    y_train, y_valid = y[train_idx], y[valid_idx]\n    y_knowledge_train, y_knowledge_valid = y_temperature[train_idx], y_temperature[valid_idx]\n    \n    y_train = np.vstack((y_train, y_knowledge_train)).T\n    y_valid = np.vstack((y_valid, y_knowledge_valid)).T\n    \n    # model = create_model((train.shape[1],), 2)\n    model = create_model((NUM_FEATURES,1), 2)\n    model.compile(loss=knowledge_distillation_loss_withFL, optimizer='adam', metrics=[auc_2, auc_3])\n\n    checkpoint = ModelCheckpoint('student_model_FL.h5', monitor='val_auc_2', verbose=1, \n                                 save_best_only=True, mode='max', save_weights_only = True)\n    reduceLROnPlat = ReduceLROnPlateau(monitor='val_auc_2', factor=0.5, patience=4, \n                                       verbose=1, mode='max', epsilon=0.0001)\n    early = EarlyStopping(monitor=\"val_auc_2\", \n                          mode=\"max\", \n                          patience=9)\n    callbacks_list = [checkpoint, reduceLROnPlat, early]\n    history = model.fit(x_train,y_train,\n                        epochs=100,\n                        batch_size = BATCH_SIZE,\n                        callbacks=callbacks_list,\n                        validation_data=(x_valid, y_valid))\n    \n    model.load_weights('student_model_FL.h5')\n    prediction = model.predict(x_valid,\n                               batch_size=512,\n                               verbose=1)\n    # print(prediction.shape)\n    # prediction = np.sum(prediction, axis=1)/2\n    score.append(roc_auc_score(y_valid[:,0], prediction[:,1]))\n    # score.append(roc_auc_score(y_valid[:,0], prediction))\n    prediction = model.predict(test,\n                               batch_size=512,\n                               verbose=1)\n    # y_test_pred_log += np.sum(prediction, axis=1)/2\n    y_test_pred_log += np.squeeze(prediction[:, 1])\n    \n    prediction = model.predict(x_valid,\n                               batch_size=512,\n                               verbose=1)\n    # y_train_pred_log += np.sum(prediction, axis=1)/2\n    y_train_pred_log[valid_idx] += np.squeeze(prediction[:, 1])\n    \n    del x_train, x_valid, y_train, y_valid, y_knowledge_train, y_knowledge_valid\n    gc.collect()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5751d2bbb3a9b71a71a7c21f69f7d2173568cbba"},"cell_type":"code","source":"print(\"OOF score: \", roc_auc_score(y, y_train_pred_log/num_fold))\nprint(\"average {} folds score: \".format(num_fold), np.sum(score)/num_fold)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2739c50d9ec082329206a058ab45c5fc9d9beca"},"cell_type":"code","source":"# make submission\nsubmit = pd.read_csv('../input/santander-customer-transaction-prediction/sample_submission.csv')\nsubmit['ID_code'] = id_code_test\nsubmit['target'] = y_test_pred_log/num_fold\nsubmit.to_csv('submission.csv', index=False)\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c877067e96b93fe594097e3b6851888fc6826d00"},"cell_type":"markdown","source":"# Please upvote if you find this kernel interesting ^_^"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}