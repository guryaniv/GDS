{"nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "version": "3.6.3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3"}}, "cells": [{"outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import matplotlib.gridspec as gridspec\n", "import seaborn as sns\n", "import os\n", "import h2o\n", "import numpy as np\n", "%matplotlib inline  \n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "6f99ab25-51c5-4827-86c1-0686dbc3e8b1", "_uuid": "e63fa41087539c71a785810b00640913594b1c62"}}, {"outputs": [], "source": ["data = reviews = pd.read_csv('../input/creditcard.csv')\n", "data['Class'] = data['Class'].astype('category')\n", "cols = list(data.columns)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "source": ["data.groupby('Class')['Class'].count()"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": ["h2o.init()"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": ["creditcard = h2o.upload_file(path ='../input/creditcard.csv')"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": ["creditcard['Class'] = creditcard['Class'].asfactor()"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "source": ["df = creditcard.as_data_frame()"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "source": ["cols = list(df.columns)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}}, {"metadata": {}, "cell_type": "markdown", "source": ["# Exploratory Data Analysis\n", "\n", "## Check for Null values\n", "\n", "If we found a null value we need to figure out how to deal with the missing data.\n", "\n", "Usualy we can assing a unique value to identify the missing value (for example if the variable is a positive integer we assing -1 value at the nulls), then we create a new feature (in form of a boolean flag) indicating the missing value in that row. Example, the feature age_of_client is null, we assing -1 on that row, and true on the new feature called age_is_missing."]}, {"outputs": [], "source": ["df.isnull().sum()"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"metadata": {}, "cell_type": "markdown", "source": ["As transactions with Amount == 0 are impossible, let's check if there any in the dataset."]}, {"outputs": [], "source": ["df.query('Amount == 0').groupby('Class').count()"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"metadata": {}, "cell_type": "markdown", "source": ["Oops, theses transactions with zero Amount are strange... Usualy we will double check the ELT and integrations, probably this zeros came from an error from the pre-processing phase.\n", "But, as we are using a dataset from a external provider (Kaggle), the most secure choice here is ignore this rows."]}, {"outputs": [], "source": ["df.query('Amount != 0').to_csv('creditcard_amount_positive.csv')\n", "creditcard = h2o.upload_file(path ='creditcard_amount_positive.csv')\n", "creditcard['Class'] = creditcard['Class'].asfactor()\n", "df = creditcard.as_data_frame()"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": ["num_cols = len(cols[:-1])\n", "plt.figure(figsize=(12,num_cols*4))\n", "gs = gridspec.GridSpec(num_cols, 1)\n", "for i, cn in enumerate(df[cols[:-1]]):\n", "    ax = plt.subplot(gs[i])\n", "    sns.distplot(df[cn][df.Class == 1], bins=50)\n", "    sns.distplot(df[cn][df.Class == 0], bins=50)\n", "    ax.set_xlabel('')\n", "    ax.set_title('histogram of feature: ' + str(cn))\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"metadata": {}, "cell_type": "markdown", "source": ["Looking the charts we can guess that variables V13, V15, V20, V22, V23, V24, V25, V26 have similar curves between fraud and no fraud, maybe we can remove then because is dificult to find a treshold to separate fraud and no fraud.\n", "\n", "Other ways to verifiy this:\n", "- Analyze the covariance between the distribuitions of fraud and not fraud in each variable\n", "- Analyze the Near Zero Variance\n", "- Run a significance test (chisquare test or Student t-test) to verify if the distribuitions of fraud and no fraud in each variable is diferent\n", "\n", "But for now let's use all variables and let the algorithm decide which variable is important."]}, {"metadata": {}, "cell_type": "markdown", "source": ["## Run significance test\n", "\n", "Only for demonstration purposes."]}, {"outputs": [], "source": ["from scipy import stats\n", "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n", "variables = cols[:-1]\n", "keep = []\n", "p_value_alpha = 0.05 #defult p-value for statistical significance\n", "\n", "for variable in variables:\n", "    fraud_v = df[variable][df.Class == 1]\n", "    not_fraud_v = df[variable][df.Class == 0].sample(len(fraud_v))\n", "    p_value = stats.ttest_ind(not_fraud_v, fraud_v).pvalue\n", "    if p_value >= p_value_alpha:\n", "        print(\"Distributions are equal. Discard {} variable\".format(variable))\n", "    else:\n", "        print(\"Distributions are diferent. Keep {} variable\".format(variable))\n", "        keep.append(variable)"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"metadata": {}, "cell_type": "markdown", "source": ["Now we have a list of possible variables to exclude. The best way to verify our choice is build the model with and without the variables and assess the specificity, sensitivity and financial analysis.\n", "\n", "# Modeling"]}, {"outputs": [], "source": ["from h2o.estimators.random_forest import H2ORandomForestEstimator\n", "train, valid = creditcard.split_frame(ratios=[0.7])\n", "response_var = 'Class'\n", "features = [col for col in cols if col != response_var]\n", "naive_rf_model = H2ORandomForestEstimator()\n", "naive_rf_model.train(x=features, y=response_var, training_frame=train, validation_frame=valid)\n", "performance_train = naive_rf_model.model_performance(train=True)"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": ["# for metrics\n", "import itertools\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import roc_curve, auc\n", "\n", "def plot_confusion_matrix(cm, classes,\n", "                          normalize=False,\n", "                          title='Confusion matrix',\n", "                          cmap=plt.cm.Blues):\n", "    \"\"\"\n", "    This function prints and plots the confusion matrix.\n", "    Normalization can be applied by setting `normalize=True`.\n", "    \"\"\"\n", "    if normalize:\n", "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "        print(\"Normalized confusion matrix\")\n", "    else:\n", "        print('Confusion matrix, without normalization')\n", "\n", "    print(cm)\n", "\n", "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    plt.title(title)\n", "    plt.colorbar()\n", "    tick_marks = np.arange(len(classes))\n", "    plt.xticks(tick_marks, classes, rotation=45)\n", "    plt.yticks(tick_marks, classes)\n", "\n", "    fmt = '.2f' if normalize else 'd'\n", "    thresh = cm.max() / 2.\n", "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "        plt.text(j, i, format(cm[i, j], fmt),\n", "                 horizontalalignment=\"center\",\n", "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n", "\n", "    plt.tight_layout()\n", "    plt.ylabel('True label')\n", "    plt.xlabel('Predicted label')"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}}, {"metadata": {}, "cell_type": "markdown", "source": ["## Metrics\n", "### Tain dataset\n", "#### Confusion Matrix"]}, {"outputs": [], "source": ["preds = naive_rf_model.predict(train)\n", "cm = confusion_matrix(train.as_data_frame()['Class'], preds.as_data_frame()['predict'])\n", "plot_confusion_matrix(cm, ['N\u00e3o-Fraude', 'Fraude'], False)"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": ["fpr, tpr, threshold = roc_curve(train.as_data_frame()['Class'], preds.as_data_frame()['predict'])\n", "plt.plot(fpr, tpr, color='darkorange',\n", "         lw=2, label='ROC curve')\n", "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n", "plt.xlim([-0.04, 1.0])\n", "plt.ylim([-0.04, 1.05])\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "plt.title('Curva ROC')\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"metadata": {}, "cell_type": "markdown", "source": ["### Validation dataset\n", "#### Confusion Matrix"]}, {"outputs": [], "source": ["predictions = naive_rf_model.predict(valid)\n", "cm = confusion_matrix(valid.as_data_frame()['Class'], predictions.as_data_frame()['predict'])\n", "plot_confusion_matrix(cm, ['N\u00e3o-Fraude', 'Fraude'], False)"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"metadata": {}, "cell_type": "markdown", "source": ["#### Confusion Matrix"]}, {"outputs": [], "source": ["fpr, tpr, threshold = roc_curve(valid.as_data_frame()['Class'], predictions.as_data_frame()['predict'])\n", "plt.plot(fpr, tpr, color='darkorange',\n", "         lw=2, label='ROC curve')\n", "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n", "plt.xlim([-0.04, 1.0])\n", "plt.ylim([-0.04, 1.05])\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "plt.title('Curva ROC')\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"metadata": {}, "cell_type": "markdown", "source": ["#### Finantial analysys\n", "\n", "Assuming a take-rate of 10% and a chargeback loss of 100%\n"]}, {"outputs": [], "source": ["valid_class = valid['Class'].as_data_frame()\n", "valid_amount = valid['Amount'].as_data_frame()\n", "take_rate = 0.1"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "source": ["valid_data = pd.concat([predictions.as_data_frame(), valid_amount, valid_class], axis=1)"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": ["total = valid_data.groupby('Class')['Amount'].sum()\n", "print(\"Fraud: {:06.2f} Gross profit: {:06.2f} Net: {:06.2f}\".format(total[1], total[0] * take_rate, (total[0] * take_rate) - total[1]))"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": ["def correct_predict(row):\n", "    if row['Class'] == row['predict'] and row['predict'] == 0:\n", "        return row['Amount'] * take_rate\n", "    elif row['Class'] == row['predict'] and row['predict'] == 1:\n", "        return -row['Amount']\n", "    return 0\n", "\n", "def missed_profit(row):\n", "    if row['Class'] != row['predict'] and row['predict'] == 0:\n", "        return -row['Amount'] * take_rate\n", "    else:\n", "        return 0\n", "    \n", "def missed_loss(row):\n", "    if row['Class'] != row['predict'] and row['predict'] == 1:\n", "        return -row['Amount']\n", "    return 0\n", "\n", "valid_data['correct_predict'] = valid_data.apply(lambda row: correct_predict(row), axis=1)\n", "valid_data['missed_profit'] = valid_data.apply(lambda row: missed_profit(row), axis=1)\n", "valid_data['missed_loss'] = valid_data.apply(lambda row: missed_loss(row), axis=1)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "source": ["avoided_loss = valid_data.query('correct_predict < 0')['correct_predict'].sum()\n", "corrected_no_fraud = valid_data.query('correct_predict > 0')['correct_predict'].sum()\n", "missed_profit = valid_data.query('missed_profit < 0')['missed_profit'].sum()\n", "missed_loss = valid_data.query('missed_loss < 0')['missed_loss'].sum()"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "source": ["pd.DataFrame([[-avoided_loss, -missed_profit, -missed_loss, corrected_no_fraud]], \n", "             columns=['avoided loss', 'missed profit', 'missed loss', 'net'])"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": ["print(\"An increase of ${:06.2f} in the net profit\".format(754372.79 - 737697.15))"], "execution_count": null, "cell_type": "code", "metadata": {}}, {"outputs": [], "source": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}}]}