{"metadata": {"language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "version": "3.6.3", "name": "python", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["**Ge men er:**\n", "\n", "I was trying to open these file by excel, that's silly and never work out LOL\n", "\n", "All right, here is the problem. We need to predict** if a driver will file an insurance claim next year**. The train data file has 59 columns, 595212 rows; the test data file has 58 columns, and 892816 rows; and the sample_submission file has 2 columns and 892816 rows.\n", "\n", "The only different column between train and test is 'target' column. The 2 columns in submission are id and target. These id in train and test has no overlap. \n", " \n", " Ge Men er, **please** continue reading and feel free to **add comments**."]}, {"cell_type": "code", "metadata": {"_cell_guid": "515c7d9e-8bdf-48aa-b14e-27b4bfd4f31a", "_uuid": "4b6293fdc6cac2070cc27f482865fd317abb9695"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt # for drawing plot graph\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["train_df = pd.read_csv(\"../input/train.csv\")\n", "train_df.shape"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["train_df.head(100)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["plt.figure(figsize=(8,6))\n", "plt.scatter(range(train_df.shape[0]), np.sort(train_df.target.values))\n", "plt.xlabel('index', fontsize=12)\n", "plt.ylabel('target', fontsize=12)\n", "plt.show()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**Q1: What's the meaning of target?**\n", "\n", "\n", "**MF Answer: ** I guess they want us to predict the probability of whether or not a claim was filed for the driver, so it should be in the range of [0,1]. \n", "\n", "Because I'm not sure if the target value could be 0.3, 0.8 or even 0.5. I made the plot gragh. The above graph shows the target value in train data file are either 1 or 0. Uh-Oh:(\n", "\n", "But, in the sample_submission file, they have all the target value as 0.0364. Do you think it's a hint for us, the value could be any REAL value between [0,1]? \n", "Or we just rougly tend to think every driver's skill is as bad as YANG LAO SHI's. Thus, when target value less or equal than 0.5, we would give a piecewise to make it equals to 0 and elsewhere equals to 1.\n", "\n", "I also find a smiliar discussion on https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40222\n", ", please search rkoptelov or Daniel Moller.\n", "\n", ">>**HY Answer:** You can write anything here, such as agree:) Seriously, we can see the precentage of 0 and 1 are very different, probably 9:1. Do you think 0 means having a claim or not? Or it doesn't matter??? But how could that work?!\n", "\n", "**Q2: What's the meaning of the other columns?**\n", "\n", "We observed the column name is a combination of three or four parts, the fourth part(if has) could be cat or bin.\n", "\n", "I find this answer from the** data description**. \n", "\n", "2.1  In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). \n", "\n", "2.2 In addition, feature names include the postfix** bin to indicate binary features and cat to indicate categorical features**.\n", "\n", "2.3  **Features without these designations are either continuous or ordinal**. Values of **-1** indicate that the feature was **missing** from the observation. \n", "\n", ">>Do you think we can do more in this part to understand features better, such as same first two parts, compute coefficient or we will compute each two anyhow. "]}, {"cell_type": "code", "metadata": {}, "source": ["test_df = pd.read_csv(\"../input/test.csv\")\n", "test_df.shape"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["test_df.head(100)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["sample_df = pd.read_csv(\"../input/sample_submission.csv\")\n", "sample_df.shape"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["sample_df.head()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": [">>Let's check the missing data of this case to see how bad it is."]}, {"cell_type": "code", "metadata": {}, "source": ["missing_df = train_df.apply(pd.value_counts);\n", "missing_df.head(1)"], "execution_count": null, "outputs": []}], "nbformat": 4}