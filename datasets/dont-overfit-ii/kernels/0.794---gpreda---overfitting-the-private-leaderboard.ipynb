{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Overfitting the private leaderboard</font></center></h1>\n\n\n<br>\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Prepare the data analysis</a>  \n- <a href='#3'>Data exploration</a>   \n - <a href='#31'>Check the data</a>   \n - <a href='#32'>Density plots of features</a>   \n - <a href='#33'>Distribution of mean and std</a>   \n  - <a href='#34'>Distribution of min and max</a>   \n - <a href='#35'>Distribution of skew and kurtosis</a>     \n - <a href='#36'>Features correlations</a>   \n- <a href='#4'>Feature engineering</a>  \n - <a href='#41'>Add aggregation functions</a>  \n - <a href='#42'>Add noise</a>  \n- <a href='#5'>Model</a>\n- <a href='#6'>Submission</a>  \n- <a href='#7'>References</a>"},{"metadata":{"_uuid":"fa44d3d09a93efc6e27432ce40d412b781153a2f"},"cell_type":"markdown","source":"# <a id='1'>Introduction</a>  \n\nIn this challenge, Kagglers are invited to not overfit. \n\nTrain data has 250 rows, test data has 20,000 rows.\n\nThere are 300 features.\n\nThis Kernel will start by exploring the data and check what engineered features will improve the model (well, reduce the overfitting).\n\n"},{"metadata":{"_uuid":"f4c5d0b42a79c143d4b1a0423b0c51265fd718fd"},"cell_type":"markdown","source":"# <a id='2'>Prepare for data analysis</a>  \n\n\n## Load packages\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"2041989e97107b61bb6659706ead46cc448c8e9e","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13cdc14c0a53b11fd7438a87bff04335666a5482"},"cell_type":"markdown","source":"## Load data   \n\nLet's check what data files are available."},{"metadata":{"_kg_hide-input":true,"_uuid":"80167a92eedaecb6878667e35c55e4dbf8d3dc20","trusted":true},"cell_type":"code","source":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"../input/dont-overfit-2/\"\nelse:\n    PATH=\"../input/\"\nos.listdir(PATH)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a37cbcca50a9993268b90dbb9ab6ddb72be0632c"},"cell_type":"markdown","source":"Let's load the train and test data files."},{"metadata":{"_kg_hide-input":true,"_uuid":"cdd6627b37c9b162dd2d7f677576c94e35571fd8","trusted":true},"cell_type":"code","source":"%%time\ntrain_df = pd.read_csv(PATH+\"train.csv\")\ntest_df = pd.read_csv(PATH+\"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c026a2812283de0951bd65dab107cdac65de718"},"cell_type":"markdown","source":"# <a id='3'>Data exploration</a>  \n\n## <a id='31'>Check the data</a>  \n\nLet's check the train and test set."},{"metadata":{"_kg_hide-input":true,"_uuid":"b3034013c64910fb504a5e06f125e6a8907b4822","trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a78f8652d9b1746d7b806520bc5c95cbfd4e98eb"},"cell_type":"markdown","source":"Train has only 250 rows and has also 302 columns, test has 19,750 rows and 301 columns.  \n\nLet's glimpse train and test dataset."},{"metadata":{"_kg_hide-input":true,"_uuid":"578e7bbfc29f66e5462ce3e76212cd88daf62b07","trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1377d3312bd3e3a1f53f7c4f984cc719050c9e9f","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2765909925fb79f28a290878e60985d03586537f"},"cell_type":"markdown","source":"Train contains:  \n\n* **id** (string);  \n* **target**;  \n* **300** numerical variables, named from **0** to *299**;\n\nTest contains:  \n\n* **id* (string);  \n* **300** numerical variables, named from **0** to **299**;\n\n\nLet's check if there are any missing data. We will also chech the type of data.\n\nWe check first train."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"d6fe6feb53c3ed8099e88a09c6fe8abcae1148a3","scrolled":false,"trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"69364c674952470cc7c05523d473ec73c154cffb","trusted":true},"cell_type":"code","source":"%%time\nmissing_data(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"ff021a0b3643e7035f9c44d6aebfc1c569db2e42"},"cell_type":"markdown","source":"Only **id** is an integer, **target** and the 300 features are float64. There are no missing data.\n\nHere we check test dataset."},{"metadata":{"_kg_hide-input":true,"_uuid":"5f5307322ba84efa68b0e11955d8c1f0ca8a3862","trusted":true},"cell_type":"code","source":"%%time\nmissing_data(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"276f66f7dac9ed4efd263db844f5de9630172c9a"},"cell_type":"markdown","source":"There are no missing data either in test data. The data types are similar with the ones in train.   \n\nLet's see the distribution of train and test numerical data, using `describe`."},{"metadata":{"_kg_hide-input":true,"_uuid":"6531e959a0fdb5baa10543561c7c4e4f52055de2","scrolled":false,"trusted":true},"cell_type":"code","source":"%%time\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"dda3b5d935290c97230bfc210c6c2c797c5ddd30","trusted":true},"cell_type":"code","source":"%time\ntest_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89e692dd2139fe6f45ad095fd0b4a76ca261d1fe"},"cell_type":"markdown","source":"We can make few observations here:   \n\n* standard deviation is very close to 1 for all features, in both train and test set;  \n* all features are approximately centered to 0, with mean values close to 0;  \n* min and max absolute values for features in train data looks to be smaller than the ones for the test data;  \n* mean value for target variable is 0.64 which will imply that 64% of target values are 1.\n\n"},{"metadata":{"_uuid":"7a885a301b603eba2daf2fa77b597c46c17fc83d"},"cell_type":"markdown","source":"Let's check the distribution of **target** value in train dataset."},{"metadata":{"_uuid":"ddfd7aabff0f726f5c705a269f8b2b19de1f16eb","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.countplot(train_df['target'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"33fc0a0e09ab216508840990538df42f74ec4ed9","trusted":true},"cell_type":"code","source":"print(\"There are {}% target values with 1\".format(100 * train_df[\"target\"].value_counts()[1]/train_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91ccf5e568d09fdcf528d65f6e3b5f1b956c8002"},"cell_type":"markdown","source":"Let's plot now the train data (all the data) using a heatmap, separatelly for target values 0 and 1.  \n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7d0e78947dbba55bfa2fe9796a5177aee997be80"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0, train_df.columns.values[2:302]]\nt1 = train_df.loc[train_df['target'] == 1, train_df.columns.values[2:302]]\nplt.subplots(1,1,figsize=(20, 7.2))\nplt.title(\"Values in training set for target = 0\")\nsns.heatmap(t0, cmap=\"Spectral\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc2769b2ff2bb1250228c1891d3911f06c7d3a5e","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.subplots(1,1,figsize=(20, 12.8))\nplt.title(\"Values in training set for target = 1\")\nsns.heatmap(t1, cmap='Spectral')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49c811772a752068ed4db9d7aa9fe367f25f36dc"},"cell_type":"markdown","source":"\n## <a id='32'>Density plots of features</a>  \n\nLet's show now the density plot of variables in train dataset. \n\nWe represent with different colors the distribution for values with **target** value **0** and **1**."},{"metadata":{"_kg_hide-input":true,"_uuid":"81cb8ce7d50d8d28442e58cc466df15b2a9a540e","trusted":true},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c4bf888bdc41d9aa2376ee40d6c8e9b65021895"},"cell_type":"markdown","source":"The first 100 values are displayed in the following cell. Press <font color='red'>**Output**</font> to display the plots."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"fb2181272cac8b497815f4c24e6885ab7894d0ca","trusted":true},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0, t1, '0', '1', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f331f575dccb9ca1b772944bf76bd7d9975be68"},"cell_type":"markdown","source":"The next 100 values are displayed in the following cell. Press <font color='red'>**Output**</font> to display the plots."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"762e0e2dba1f2752075a38088f23f67b73199ca9","trusted":true},"cell_type":"code","source":"features = train_df.columns.values[102:202]\nplot_feature_distribution(t0, t1, '0', '1', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d35e00030ce15a9586a86f8e91e71fdc7fefea84"},"cell_type":"markdown","source":"The next 100 values are displayed in the following cell. Press <font color='red'>**Output**</font> to display the plots."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"c5fad8866a209307e10d9f87f21aac8057980b6f"},"cell_type":"code","source":"features = train_df.columns.values[202:302]\nplot_feature_distribution(t0, t1, '0', '1', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e642c042790cad0b7dc475b4551cacb1e6a5085"},"cell_type":"markdown","source":"We can observe that most of features present significant different distribution for the two target values.  \n\nAlso some features, like **77**, **95**, **147** shows a distribution that resambles to a bivariate distribution.\n\n\nLe't s now look to the distribution of the same features in parallel in train and test datasets. \n\nThe first 100 values are displayed in the following cell. Press <font color='red'>**Output**</font> to display the plots."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"d272dfd96354a78a9c5a8341451319c6bec99f66","trusted":true},"cell_type":"code","source":"features = train_df.columns.values[2:102]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8cb4d1a65251adaff21ddb2c141d9340b8d9f3a"},"cell_type":"markdown","source":"The next 100 values are displayed in the following cell. Press <font color='red'>**Output**</font> to display the plots."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"fc0117d9ca8b8101199bde5cb14df4d7d5b55aff","trusted":true},"cell_type":"code","source":"features = train_df.columns.values[102:202]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f76e99451f216ad229070e151e9e60933d9f138"},"cell_type":"markdown","source":"The next 100 values are displayed in the following cell. Press <font color='red'>**Output**</font> to display the plots."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"c75684ec486c0044c69353fc2989d76dfdd03232"},"cell_type":"code","source":"features = train_df.columns.values[202:302]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e672244ee1e3db4cc58fad13cba6ec8502cd19f3"},"cell_type":"markdown","source":"The train and test seems to be well ballanced with respect of  distribution of the numeric variables for most of the features.   \n\nThere are few features that shows some differences in distribution between train and test, for example: **2**, **8**, **12**, **16**, **37**, **72**, **84**, **100**, **103**, **104**, **123**, **144**, **155**, **181**, **202**, **203**, **204**, **229**, **241**, **264**, **288**.\n\n\n## <a id='33'>Distribution of mean and std</a>  \n\nLet's check the distribution of the mean values per row in the train and test set."},{"metadata":{"_kg_hide-input":true,"_uuid":"c7638aaac9fa67c4fda63b8432314c4e0463e83a","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train_df[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e642bb0bfdd3ada9bfa6d4e6bfd3086a34a5d5f"},"cell_type":"markdown","source":"Mean values per row for test data are close to a normal distribution while the mean values per row for train data shows multiple peak values. Most of the values are  between +/- 0.1.\n\nLet's check the distribution of the mean values per columns in the train and test set."},{"metadata":{"_kg_hide-input":true,"_uuid":"c44a185c11f42b6839cc67f91cafb688d24c71a3","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train_df[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a018358241a1c22e88ba1a8bcd9e34e14af35f9"},"cell_type":"markdown","source":"These are the values that we already observed earlier that are mostly centered around 0. We can see that train is actually showing a larger spread of these values, while test values have a smaller deviation and a distribution closer to a normal one.\n\nLet's show the distribution of standard deviation of values per row for train and test datasets."},{"metadata":{"_kg_hide-input":true,"_uuid":"0513f92fd496b2f3f2bff1b96764e8fbb1b156fe","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train_df[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca2364c3f20f2cc86408117bf3d7a6b05fc76199"},"cell_type":"markdown","source":"The average standard deviation per rows is 1 and most of values are between 1 +/- 0.1.\n\nLet's check the distribution of the standard deviation of values per columns in the train and test datasets."},{"metadata":{"_kg_hide-input":true,"_uuid":"f0388d0ac4eae973c419ab252faa6f263d1d6aaf","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train_df[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1d309f7f41dedbf0a85b27a9054651fca0d680b"},"cell_type":"markdown","source":"Standard deviation values per columns in train dataset are between 0.9 and 1.1 while in test dataset are much smaller, confined between 0.99 and 1.01.\n\nLet's check now the distribution of the mean value per row in the train dataset, grouped by value of target."},{"metadata":{"_kg_hide-input":true,"_uuid":"2765e1da68de729690210efded3d65fcbc2d9dfb","trusted":true},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea2a01a3d0e8751fd05419e48eb0b9c9b423764c"},"cell_type":"markdown","source":"Let's check now the distribution of the mean value per column in the train dataset, grouped by value of target."},{"metadata":{"_kg_hide-input":true,"_uuid":"c019e19f8539f298de3e3177e5acae5b4e4f2771","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0734bd00863f50d4bd82c9a9929778a008d041a"},"cell_type":"markdown","source":"## <a id='34'>Distribution of min and max</a>  \n\nLet's check the distribution of min per row in the train and test set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"93dadcb01b743071433652a040964b960ae4cfae"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of min values per row in the train and test set\")\nsns.distplot(train_df[features].min(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eae90157fa3986f52169ed095262c2ad94299d6"},"cell_type":"markdown","source":"\nA long queue to the lower values for both, extended as long as to -5.5 for test set, is observed.\n\nLet's now show the distribution of min per column in the train and test set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c9db92f94fde2a4e541b354b535178f85ce18d8f"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of min values per column in the train and test set\")\nsns.distplot(train_df[features].min(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].min(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56bd2dff5a1e3d4ed143a58d4c6289e368e6b82a"},"cell_type":"markdown","source":"The distribution of min values per columns (i.e. per variables) is quite notably different for train and test set.\n\n\nLet's check now the distribution of max values per rows for train and test set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"51cebc02531aa513ae11979e093c47a3925dcffa"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of max values per row in the train and test set\")\nsns.distplot(train_df[features].max(axis=1),color=\"brown\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].max(axis=1),color=\"lightblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7df74507105a2958e798d432b0e2b659b486383"},"cell_type":"markdown","source":"Both distribution shows a long queue toward larger values, with test extended more, up to 5.5., while train has values as large as 4.5.   \n\nLet's show now the max distribution on columns for train and test set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1ce426f8406e60bfea047a2c2f0bbe6ebf047f0d"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of max values per column in the train and test set\")\nsns.distplot(train_df[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].max(axis=0),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"360b74bbbc9ac8b38ed2abb9a4724483e72ba79f"},"cell_type":"markdown","source":"The two distributions are neatly separated.  \n\n\nLet's  show now the distributions of min values per row in train set, separated on the values of target (0 and 1)."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"aafc3066a73cb17e6f1c6831a6eaae1f96251326"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per row in the train set\")\nsns.distplot(t0[features].min(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f7f418fb8f74b2329ff20157fd31f47defe8196"},"cell_type":"markdown","source":"We show here the distribution of min values per columns in train set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7bdc1977045e369cf8b5c347daa160e724b47dc9"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per column in the train set\")\nsns.distplot(t0[features].min(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"786c269356367597c1e0154f3a48df997c9adb45"},"cell_type":"markdown","source":"We can observe a relative good separation between the two distributions, with the values for **target = 0** with lower peaks and with a longer queue toward larger values (up to close to -1), while the mins for **target = 1** are extended only until -1.5.\n\nLet's show now the distribution of max values per rown in the train set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d365dd1336da66b528836cd11d55ef8e34bd4128"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per row in the train set\")\nsns.distplot(t0[features].max(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8371718d87ae9092225f91dc21b6b26285f84b9"},"cell_type":"markdown","source":"Let's show also the distribution of max values per columns in the train set."},{"metadata":{"_uuid":"e35d822c4de25f3f9a6b85c06d8c74f5c9a77681","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per column in the train set\")\nsns.distplot(t0[features].max(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb67f160f30e5307f75a006422166ca12371956d"},"cell_type":"markdown","source":"We can observe a relative good separation between the two distributions.\n"},{"metadata":{"_uuid":"bd96d8a3af3be4a17d129ba6c816829efcd3bb5b"},"cell_type":"markdown","source":"## <a id='35'>Distribution of skew and kurtosis</a>  \n\nLet's see now what is the distribution of skew values per rows and columns.\n\nLet's see first the distribution of skewness calculated per rows in train and test sets.\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9d4622bc88b0595a14668fe9379736e01674b437"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of skew per row in the train and test set\")\nsns.distplot(train_df[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].skew(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80278591aa7f79fff0e1da7945920675abcab2d7"},"cell_type":"markdown","source":"Let's see first the distribution of skewness calculated per columns in train and test set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2a47da5cabf227b2261ccf7534e53426beb2b478"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of skew per column in the train and test set\")\nsns.distplot(train_df[features].skew(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].skew(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1715ac0d3d335a7e15ba22891a1151d1c9030490"},"cell_type":"markdown","source":"Let's see now what is the distribution of kurtosis values per rows and columns.\n\nLet's see first the distribution of kurtosis calculated per rows in train and test sets."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c5c4b652b4748f011f3228686a99aeb512c9dd30"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of kurtosis per row in the train and test set\")\nsns.distplot(train_df[features].kurtosis(axis=1),color=\"darkblue\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].kurtosis(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"633836f58c6a1427fd23d87d3af0fd950e95851e"},"cell_type":"markdown","source":"\nLet's see first the distribution of kurtosis calculated per columns in train and test sets."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d0a829d00a82ed5d6c1254b2ffe39ffe19b75fa9"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:302]\nplt.title(\"Distribution of kurtosis per column in the train and test set\")\nsns.distplot(train_df[features].kurtosis(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].kurtosis(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4b2ba9cb4587a48ce5bef324648ec50991b08c7"},"cell_type":"markdown","source":"Let's see now the distribution of skewness on rows in train separated for values of target 0 and 1."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"46e14df1d9a64714dc2a6e28e766a613287e2996"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per row in the train set\")\nsns.distplot(t0[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7de8279f48610b8bfec11ce5b4f68cfd575c607"},"cell_type":"markdown","source":"Let's see now the distribution of skewness on columns in train separated for values of target 0 and 1."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"839b7df2efb1975b29374ac687a0a98482396721"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per column in the train set\")\nsns.distplot(t0[features].skew(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afc9116813ac27fcb65fdffe656874e2b45ead49"},"cell_type":"markdown","source":"Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"44a98034143d6ac6219d744a7380713695d88604"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per row in the train set\")\nsns.distplot(t0[features].kurtosis(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afcad24c6bdbb23196405218f23b50be6c110b98"},"cell_type":"markdown","source":"Let's see now the distribution of kurtosis on columns in train separated for values of target 0 and 1."},{"metadata":{"trusted":true,"_uuid":"f040b79abe13b9e51ce3eac525ec35be86119a7c"},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per column in the train set\")\nsns.distplot(t0[features].kurtosis(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba1a78179550950bb949a6a74e6a9b3174d79847"},"cell_type":"markdown","source":"## <a id='36'>Features correlation</a>  \n\nWe calculate now the correlations between the features in train set.  \nThe following table shows the first 10 the least correlated features."},{"metadata":{"_kg_hide-input":true,"_uuid":"072a3440f41f3dfa7867123b89822ac4ce6711b3","trusted":true},"cell_type":"code","source":"%%time\ncorrelations = train_df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4ba85e77762531185e4613d9182aff287046681"},"cell_type":"markdown","source":"Let's look to the top most correlated features, besides the same feature pairs."},{"metadata":{"_kg_hide-input":true,"_uuid":"581fb3b0b326ecaa3899eb2139c50d76496b21ae","trusted":true},"cell_type":"code","source":"correlations.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee25869baaea4754506c3c3a41e066c58ebeccaa"},"cell_type":"markdown","source":"Let's see also the least correlated features."},{"metadata":{"_kg_hide-input":true,"_uuid":"dc8d633100856f81b13596c37a7eeea1afe7844b","trusted":true},"cell_type":"code","source":"correlations.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81222dd5a76f10a1a4b1058c138b2e4bed704fbc"},"cell_type":"markdown","source":"The correlation between the features is  small (in the range of `no correlation` for the best correlated features. \n\n"},{"metadata":{"_uuid":"1e6985cb71c1e9933bf1e0f855c181e2c06e9398"},"cell_type":"markdown","source":"# <a id='4'>Feature engineering</a>  \n\n\nLet's calculate for starting few aggregated values for the existing features."},{"metadata":{"_uuid":"41110c042ead6abca5f3a85ff3b341392a044949"},"cell_type":"markdown","source":"## <a id='41'>Add aggregation functions</a>  "},{"metadata":{"_kg_hide-input":true,"_uuid":"eb5d483c51ae4ed87ef4601ec9bcc51db03aafcf","trusted":true},"cell_type":"code","source":"%%time\ni = 1\nfor df in [test_df, train_df]:\n    idx = df.columns.values[i:i+300]\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e128ec458d73f1a53ccef210bcbc31c73193e2ba"},"cell_type":"markdown","source":"Let's check the new created features."},{"metadata":{"_kg_hide-input":true,"_uuid":"779591ca9e40907f9d91c77dc2eaab5586c73814","trusted":true},"cell_type":"code","source":"train_df[train_df.columns[302:]].head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"8f956116511f02a14be9a9d6d592766e9716d493","trusted":true},"cell_type":"code","source":"test_df[test_df.columns[301:]].head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"39fd7391bce60935a69b8f71695079afb4229ce7","trusted":true},"cell_type":"code","source":"def plot_new_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,4,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45f5255308a52ed464b069ab2e51df26e01312f4"},"cell_type":"markdown","source":"Let's check the distribution of these new, engineered features.  \n\nWe plot first the distribution of new features, grouped by value of corresponding `target` values."},{"metadata":{"_kg_hide-input":true,"_uuid":"fe19f0a041c81d15a66b52fb6a8ed8c854b52ad8","trusted":true},"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[302:]\nplot_new_feature_distribution(t0, t1, 'target: 0', 'target: 1', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78a92363ea2600631499bf7408ecf35a5eaa514f"},"cell_type":"markdown","source":"Let's show the distribution of new features values for train and test."},{"metadata":{"_kg_hide-input":true,"_uuid":"16ef1f4eecaebd64561fb6cea501c1d0cd4fc65c","trusted":true},"cell_type":"code","source":"features = train_df.columns.values[302:]\nplot_new_feature_distribution(train_df, test_df, 'train', 'test', features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"514c57aec33654bfabe59204b1367eb19ac1cca0"},"cell_type":"markdown","source":"Let's check how many features we have now."},{"metadata":{"_uuid":"783ac0da84eae3b48c27f317a957ebc2b48091e1","trusted":true},"cell_type":"code","source":"print('Train and test columns: {} {}'.format(len(train_df.columns), len(test_df.columns)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c2f011806ac74d979d985bbc14e18b4dcfcc23b"},"cell_type":"markdown","source":"## <a id='42'>Add noise</a>  \n\nWe will add more data in the training set by injecting noise in the existing training data. This will account for a data multiplication technique and as well as a regularization technique, aiming to reduce overfitting on the existing training data."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4637faffc8469ea8f18121e1f90a4bad9b5a1bb0"},"cell_type":"code","source":"def apply_noise(data, noise_level):\n    idxt = data[['id', 'target']]\n    features = data.columns.values[2:]\n    appended_data = []\n    for feature in features:\n        signal = data[feature]\n        noise_factor = (np.abs(signal)).mean() * noise_level\n        noise =  np.random.normal(0, noise_level, signal.shape)\n        jittered = signal + noise\n        appended_data.append(pd.DataFrame(jittered))\n    appended_data = pd.concat(appended_data, axis=1)\n    data_jittered = pd.concat([idxt, pd.DataFrame(appended_data)], axis=1)\n    return data_jittered","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a834b5f6f66133b8c190acb09f2db1eebe2d608b"},"cell_type":"code","source":"noise_train_df = []\nfor i in tqdm_notebook(range(0,2)):\n    t = apply_noise(train_df, noise_level = i * 0.05)\n    noise_train_df.append(t)\nnoise_train_df = pd.concat(noise_train_df, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3baa8029673797c108f2b10330735d17d4a6baa0"},"cell_type":"code","source":"print(\"Shape train with noise added:\",noise_train_df.shape)\ntrain_df = noise_train_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac818e478d560bee541240eac9190ee5042eab48"},"cell_type":"markdown","source":"# <a id='5'>Model</a>  \n\nFrom the train columns list, we drop the ID and target to form the features list."},{"metadata":{"_kg_hide-input":true,"_uuid":"567d06495fb26ca26881c0fb9a8ef80b388ab901","trusted":true},"cell_type":"code","source":"features = [c for c in train_df.columns if c not in ['id', 'target']]\n#using https://www.kaggle.com/alexandregeorges/glmnet-train-rfe-boruta-cv ?\n#features = ['33', '65', '217', '91', '199', '69', '82', '117', '73', '295',\n#           '130', '108', '258', '18', '189', '194', '43', '145', '80','24', \n#           '56', '214', '268', 'max', 'min', 'std', 'mean', 'skew','kurt' ]\ntarget = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93ad2705121f7744e933824a8723538518d5d3ae"},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e48d029f6bf9421e401c0b2e3bfe269dfbad4e8"},"cell_type":"markdown","source":"We define the hyperparameters for the model."},{"metadata":{"_kg_hide-input":true,"_uuid":"8a57bca14c4c82fb80bafd0c27c42bf1a405b531","trusted":true},"cell_type":"code","source":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.83,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.81,\n    'learning_rate': 0.005,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 90,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 11,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e93df12ae1b6e514da38772132fb35ef048d8e73"},"cell_type":"markdown","source":"We run the model."},{"metadata":{"_kg_hide-input":true,"_uuid":"595ba3944cb9365e292819afcd8406de8166bd0d","trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=9, shuffle=True, random_state=4422)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 100000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], \n                    verbose_eval=200, early_stopping_rounds = 200)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70299b2aa81a678c0bee8ca183f064c82b673ba4"},"cell_type":"markdown","source":"Let's check the feature importance."},{"metadata":{"_kg_hide-input":true,"_uuid":"9f989c0d69a89f3964265615b6c64ad6010cec37","trusted":true},"cell_type":"code","source":"def plot_feature_importance():\n    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:50].index)\n    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n    plt.figure(figsize=(12,10))\n    sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n    plt.title('Features importance (averaged/folds)')\n    plt.tight_layout()\n    plt.savefig('FI.png')\nplot_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94f2cac9789db26b27c88db136d1366c70b5cc33"},"cell_type":"markdown","source":"# <a id='6'>Submission</a>  \n\nWe submit the solution."},{"metadata":{"_kg_hide-input":true,"_uuid":"4c1965242e30db114a3a28d850fea148d34c06c9","trusted":true},"cell_type":"code","source":"sub_df = pd.DataFrame({\"id\":test_df[\"id\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cac53504a582a6480206aa925928f4be435d0ac0"},"cell_type":"markdown","source":"# <a id='7'>References</a>    \n\n[1] https://www.kaggle.com/gpreda/elo-world-high-score-without-blending  \n[2] https://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-897   \n[3] https://www.kaggle.com/gpreda/santander-eda-and-prediction   \n\n"},{"metadata":{"_uuid":"b82fbe1cac93b01278ebef47951df65033cf2cc5"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"c916d778cf28bf35d16d0c71dcbadc83084cbeb4"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}