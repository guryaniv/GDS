{"cells":[{"metadata":{"_uuid":"f179e68e2be0fa1293b71634c60114c19e4cf8be"},"cell_type":"markdown","source":"\n## TL; DR\n\nThis is a fork of  **Wiston Van**'s basic LSTM model [https://www.kaggle.com/winstonvan/the-van-plan-for-kaggle-swaggle](http://)\n\nWe slapped an Attention layer on top of the LSTM results, also we leave the prediction probabilites alone instead of rounding it.\n\n**Anyway, Happy Thanksgiving!**\n"},{"metadata":{"_uuid":"59c9b08d046c688df9148867991fd83d18af407a"},"cell_type":"markdown","source":"**Update**\n\nAdded 10-fold CV instead of the original train/test split, accuracy went up a bit"},{"metadata":{"_uuid":"c3e505e5752b59bcfbc0c0bb517457c400e12084"},"cell_type":"markdown","source":"**Update 2**\n\nJust realized that by replacing LSTM with GRU, the score went up a bit more."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a4ddd7f3767d700e20ff6a8ff5379e2d7eac8fd"},"cell_type":"code","source":"from keras import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import *\nfrom keras.models import Sequential,Model\nfrom keras.layers import *\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97349574898cd7c633cc39cf71f141a8c14e555c"},"cell_type":"code","source":"# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a98cf37a9d166af2346a8e8f883132afef5a55fb"},"cell_type":"code","source":"def get_model():\n    model = Sequential()\n    model.add(BatchNormalization(input_shape=(10, 128)))\n#     model.add(Bidirectional(RNN(64, activation='relu', return_sequences=True)))\n    model.add(Bidirectional(GRU(128, dropout=0.4, recurrent_dropout=0.4, activation='relu', return_sequences=True)))\n    model.add(Attention(10))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"751eae6693b29964ad798069ac70f2192f2b73da"},"cell_type":"code","source":"def get_model_v2():\n    inp = Input(shape=(10,128))\n    x = Bidirectional(CuDNNLSTM(40, return_sequences=True))(inp)\n    y = Bidirectional(CuDNNGRU(40, return_sequences=True))(x)\n    \n    atten_1 = Attention(10)(x) # skip connect\n    atten_2 = Attention(10)(y)\n    avg_pool = GlobalAveragePooling1D()(y)\n    max_pool = GlobalMaxPooling1D()(y)\n    \n    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n    outp = Dense(1, activation=\"sigmoid\")(conc)    \n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3da90dfb7b9f74e4ea74319a3148a9e353e596ef"},"cell_type":"code","source":"train = pd.read_json('../input/train.json')\ntest = pd.read_json('../input/test.json')\nsample_submission = pd.read_csv('../input/sample_submission.csv')\nxtrain = [k for k in train['audio_embedding']]\nfor idx,x in enumerate(xtrain):\n    if len(x) > 10:\n        print(idx)\ntest_data = test['audio_embedding'].tolist()\nytrain = train['is_turkey'].values\n# Pad the audio features so that all are \"10 seconds\" long\nx_train = pad_sequences(xtrain,maxlen=10)\n# for x in xtrain:\n#     x = np.array(x)\n#     if x.shape[0] < 10:\n#         x_ = np.zeros((10,128))\n#         x_[:x.shape[0],:] = x\n#         n_pads = math.ceil((10 - x.shape[0])/x.shape[0])\n#         len_to_pad = min(x.shape[0],10-x.shape[0])\n#         for i in range(n_pads):\n#             x_[x.shape[0]*(i+1):x.shape[0]*(i+2),:] = x[:len_to_pad if i < n_pads - 1 \\\n#                                                         else 10 - x.shape[0] - i*len_to_pad,:]\n#         x = x_\n#     x_train.append(x)\nx_train = np.array(x_train)\ny_train = np.asarray(ytrain)\n\nx_test = pad_sequences(test_data,maxlen=10)\n# for x in test_data:\n#     x = np.array(x)\n#     if x.shape[0] < 10:\n#         x_ = np.zeros((10,128))\n#         x_[:x.shape[0],:] = x\n#         n_pads = math.ceil((10 - x.shape[0])/x.shape[0])\n#         len_to_pad = min(x.shape[0],10-x.shape[0])\n#         for i in range(n_pads):\n#             x_[x.shape[0]*(i+1):x.shape[0]*(i+2),:] = x[:len_to_pad if i < n_pads - 1 \\\n#                                                         else 10 - x.shape[0] - i*len_to_pad,:]\n#         x = x_\n#     x_test.append(x)\nx_test = np.array(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e40248c4e9ad296b3318f452e96015a1fefc57d"},"cell_type":"code","source":"kf = KFold(n_splits=10, shuffle=True, random_state=42069)\npreds = []\nfold = 0\naucs = 0\nfor train_idx, val_idx in kf.split(x_train):\n    x_train_f = x_train[train_idx]\n    y_train_f = y_train[train_idx]\n    x_val_f = x_train[val_idx]\n    y_val_f = y_train[val_idx]\n    model = get_model()\n    model.fit(x_train_f, y_train_f,\n              batch_size=256,\n              epochs=16,\n              verbose = 0,\n              validation_data=(x_val_f, y_val_f))\n    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n    preds_val = model.predict([x_val_f], batch_size=512)\n    preds.append(model.predict(x_test))\n    fold+=1\n    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n    aucs += auc(fpr,tpr)\n    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\nprint(\"Cross Validation AUC = {}\".format(aucs/10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a275580f02056a483b6ab038a1502aed650e8915"},"cell_type":"code","source":"preds = np.asarray(preds)[...,0]\npreds = np.mean(preds, axis=0)\nsub_df = pd.DataFrame({'vid_id':test['vid_id'].values,'is_turkey':preds})\n# sub_df.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43353c9d33ef856597c33e331898eb4a6ce4337b"},"cell_type":"markdown","source":"## Pseudo-labeling\n\nSince the model seems to be doing quite well, we tried to experiment with pseudo-labeling (using test data with high confidences as training data), however there's is currently no improvements."},{"metadata":{"trusted":true,"_uuid":"314f0c75058337e3646fd5f1585635baa94b0c65"},"cell_type":"code","source":"probs = sub_df.is_turkey.values\nn,bins,_ = plt.hist(probs,bins=100)\nprint(n, bins)\npos_threshold = 0.99\nneg_threshold = 0.01\npseudo_index = np.argwhere(np.logical_or(probs > pos_threshold, probs < neg_threshold ))[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"806bfcecc37ce3756d93a2210f9bbbc09126d5f0"},"cell_type":"code","source":"pseudo_x_train = x_test[pseudo_index]\npseudo_y_train = probs[pseudo_index]\npseudo_y_train[pseudo_y_train > 0.5] = 1\npseudo_y_train[pseudo_y_train <= 0.5] = 0\n# x_train = np.concatenate([x_train, pseudo_x_train],axis=0)\n# y_train = np.concatenate([y_train,pseudo_y_train])\n# print(x_train.shape, y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5897ccc9557214f0626f84b4fd8424322fd99af"},"cell_type":"code","source":"kf = KFold(n_splits=10, shuffle=True, random_state=42069)\npreds = []\nfold = 0\naucs = 0\nfor train_idx, val_idx in kf.split(x_train):\n    x_train_f = x_train[train_idx]\n    y_train_f = y_train[train_idx]\n    x_val_f = x_train[val_idx]\n    y_val_f = y_train[val_idx]\n    model = get_model()\n    model.fit(pseudo_x_train, pseudo_y_train,\n              batch_size=256,\n              epochs=3,\n              verbose = 0,\n              validation_data=(x_val_f, y_val_f))\n    model.fit(x_train_f, y_train_f,\n              batch_size=256,\n              epochs=16,\n              verbose = 0,\n              validation_data=(x_val_f, y_val_f))\n    preds_val = model.predict([x_val_f], batch_size=512)\n    preds.append(model.predict(x_test))\n    fold+=1\n    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n    aucs += auc(fpr,tpr)\n    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\nprint(\"Cross Validation AUC = {}\".format(aucs/10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c3d4eb4e76f36ef24690c3e2c180329a43579a"},"cell_type":"code","source":"preds = np.asarray(preds)[...,0]\npreds = np.mean(preds, axis=0)\nsub_df = pd.DataFrame({'vid_id':test['vid_id'].values,'is_turkey':preds})\nsub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}