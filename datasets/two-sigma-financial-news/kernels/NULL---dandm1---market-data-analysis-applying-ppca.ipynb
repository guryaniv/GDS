{"cells":[{"metadata":{"_uuid":"4829e7070b6c7f68215a55fa21025c59f6487fda"},"cell_type":"markdown","source":"# Market Data Analysis applying PPCA\nI thought I would try applying a Principal Component Analysis to the market data - to see if this allowed the prediction problem to be reduced any, and the market data to be shared across assets. \n\nThis starts the same way as every Kernel - load libraries, load the data feathers and look at a few rows of the data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')\nmarket_train_df = env.get_training_data()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8300f5793d4bc64f0be0f39855c504bdd31b923","scrolled":true},"cell_type":"code","source":"market_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0564f07dcc5f1d7ca586d77b98b35d61f4b1330b"},"cell_type":"markdown","source":"## Probabalistic Principle Component Analysis\n\nNext we need to define the PCA class. Because we are working with data containing (a whole lot) of missing data, the standard PCA approach won't work and we need to look at a probabilistic PCA. This works iteratively - estimating values for the missing data, using those to perform a PCA and repeating until convergence.\n\nThe implementation I have used is a small adaptation of the implementation by Allen Tran at:\n[https://github.com/allentran/pca-magic/](https://github.com/allentran/pca-magic/)\n\nThanks to Allen for making this available on a permissive license."},{"metadata":{"trusted":true,"_uuid":"8a108c5851d76ac0c1a2892eff67fe57a6b02b36"},"cell_type":"code","source":"import os\n\nimport numpy as np\nfrom scipy.linalg import orth\n\n\nclass PPCA():\n\n    def __init__(self):\n\n        self.raw = None\n        self.data = None\n        self.C = None\n        self.means = None\n        self.stds = None\n        self.eig_vals = None\n\n    def _standardize(self, X):\n\n        if self.means is None or self.stds is None:\n            raise RuntimeError(\"Fit model first\")\n\n        return (X - self.means) / self.stds\n\n    def fit(self, data, d=None, tol=1e-4, min_obs=10, verbose=False):\n\n        self.raw = data\n        self.raw[np.isinf(self.raw)] = np.max(self.raw[np.isfinite(self.raw)])\n\n        valid_series = np.sum(~np.isnan(self.raw), axis=0) >= min_obs\n\n        data = self.raw[:, valid_series].copy()\n        N = data.shape[0]\n        D = data.shape[1]\n\n        self.means = np.nanmean(data, axis=0)\n        self.stds = np.nanstd(data, axis=0)\n\n        data = self._standardize(data)\n        observed = ~np.isnan(data)\n        missing = np.sum(~observed)\n        data[~observed] = 0\n\n        # initial\n\n        if d is None:\n            d = data.shape[1]\n        \n        if self.C is None:\n            C = np.random.randn(D, d)\n        else:\n            C = self.C\n        CC = np.dot(C.T, C)\n        X = np.dot(np.dot(data, C), np.linalg.inv(CC))\n        recon = np.dot(X, C.T)\n        recon[~observed] = 0\n        ss = np.sum((recon - data)**2)/(N*D - missing)\n\n        v0 = np.inf\n        counter = 0\n\n        while True:\n\n            Sx = np.linalg.inv(np.eye(d) + CC/ss)\n\n            # e-step\n            ss0 = ss\n            if missing > 0:\n                proj = np.dot(X, C.T)\n                data[~observed] = proj[~observed]\n            X = np.dot(np.dot(data, C), Sx) / ss\n\n            # m-step\n            XX = np.dot(X.T, X)\n            C = np.dot(np.dot(data.T, X), np.linalg.pinv(XX + N*Sx))\n            CC = np.dot(C.T, C)\n            recon = np.dot(X, C.T)\n            recon[~observed] = 0\n            ss = (np.sum((recon-data)**2) + N*np.sum(CC*Sx) + missing*ss0)/(N*D)\n\n            # calc diff for convergence\n            det = np.log(np.linalg.det(Sx))\n            if np.isinf(det):\n                det = abs(np.linalg.slogdet(Sx)[1])\n            v1 = N*(D*np.log(ss) + np.trace(Sx) - det) \\\n                + np.trace(XX) - missing*np.log(ss0)\n            diff = abs(v1/v0 - 1)\n            if verbose:\n                print('\\rAt iteration {} the diff is {:8.6f} (target {})'.format(counter,diff,tol),end='')\n            if (diff < tol) and (counter > 5):\n                break\n\n            counter += 1\n            v0 = v1\n\n\n        C = orth(C)\n        vals, vecs = np.linalg.eig(np.cov(np.dot(data, C).T))\n        order = np.flipud(np.argsort(vals))\n        vecs = vecs[:, order]\n        vals = vals[order]\n\n        C = np.dot(C, vecs)\n\n        # attach objects to class\n        self.C = C\n        self.data = data\n        self.eig_vals = vals\n        self._calc_var()\n\n    def transform(self, data=None):\n\n        if self.C is None:\n            raise RuntimeError('Fit the data model first.')\n        if data is None:\n            return np.dot(self.data, self.C)\n        missing = np.isnan(data)\n        #Obtain mean of columns as you need, nanmean is just convenient.\n        it = 0\n        if np.sum(missing) > 0:\n            col_mean = np.nanmean(data, axis=0)\n            data[missing] = np.take(col_mean, np.where(missing)[0])\n            change = 1\n            while(change>1e-3):\n                CC = np.dot(self.C.T, self.C)\n                X = np.dot(np.dot(data, self.C), np.linalg.inv(CC))\n                proj = np.dot(X, self.C.T)\n                change = np.max(np.abs(data[missing]-proj[missing]))\n                print('\\rIteration {}. Change is {:6.4f}'.format(it,change),end='')\n                data[missing] = proj[missing]\n                it += 1\n        return np.dot(data, self.C)\n\n    def _calc_var(self):\n\n        if self.data is None:\n            raise RuntimeError('Fit the data model first.')\n\n        data = self.data.T\n\n        # variance calc\n        var = np.nanvar(data, axis=1)\n        total_var = var.sum()\n        self.var_exp = self.eig_vals.cumsum() / total_var\n\n    def save(self, fpath):\n\n        np.save(fpath, self.C)\n        \n    def load(self, fpath):\n\n        assert os.path.isfile(fpath)\n\n        self.C = np.load(fpath)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb308fb4ef3ccf6db010b4a5cc797f93094b5d72"},"cell_type":"markdown","source":"Before we do any training, lets clean up the data some - removing outliers and and changing the date-time values to simple dates. We already have gaps in our data - so making some more won't hurt much. We drop anything which shows prices on the day changing by more than a factor of 5, 1 days absolute returns greater than 0.5 and 10 day absolute returns greater than 1.5 (approx .5 * sqrt(10))."},{"metadata":{"trusted":true,"_uuid":"4c460c8c0d5a123e2e999405c9766d930b0ffdd2"},"cell_type":"code","source":"market_train_df['assetCode'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1678b968c370c7d978153086451139e97a1c2be9"},"cell_type":"code","source":"market_train_df.drop(market_train_df[(market_train_df['open']>market_train_df['close']*5) | (market_train_df['close']>market_train_df['open']*5)].index,inplace=True)\nmarket_train_df.drop(market_train_df[(np.abs(market_train_df['returnsOpenPrevRaw1'])>0.5) | (np.abs(market_train_df['returnsClosePrevRaw1'])>0.5)].index,inplace=True)\nmarket_train_df.drop(market_train_df[(np.abs(market_train_df['returnsOpenPrevRaw10'])>1.5) | (np.abs(market_train_df['returnsClosePrevRaw10'])>1.5)].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53d99f1f6b8ae88f3c774cf04290917398c8f264"},"cell_type":"code","source":"market_train_df['time']=market_train_df['time'].dt.floor('D')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c637fc82d0c7c6855e2ec62e0cc4ea0668ed81ad"},"cell_type":"code","source":"market_train_df['assetCode'].count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1042c60c6d5034949bbbe591648cb1f2b4d0bb2"},"cell_type":"markdown","source":"And now we need to convert the relevant dataframe to a data matrix (including NaNs and all) for use in our PPCA. In theory this should save the results - but I can't make that work.\n\nI create 128 components from the data - seems like 100 doesn't give enough accuracy, but 200 may be beyond the point of diminishing returns.  Something of a question of taste however."},{"metadata":{"trusted":true,"_uuid":"10ec00e2fddfd31222c91535ced6c5c02e792ab6"},"cell_type":"code","source":"def DfToData(df,valueCol):\n    data = df.pivot(index='time',columns='assetCode',values=valueCol).values\n    asset_means = np.nanmean(data,axis=0)\n    data -= asset_means\n    return data\n\ndataList = []\ndataList.append(DfToData(market_train_df,'returnsClosePrevRaw10'))\ndataList.append(DfToData(market_train_df,'returnsClosePrevMktres10'))\ndata = np.concatenate(dataList,axis=0)\n\ndata.shape\nppca = PPCA()\n\nppca.fit(data, d=128, verbose=True,min_obs=1)\nppca.save('ppca.np')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8afe854dac50912dc58892d46dac19a47d7a9f51"},"cell_type":"markdown","source":"Now one of the things that would be really useful here would be if the PPCA told us something fundamental about the relationship between assets - not just the relationship between 10 day returns. To test if this is the case, we will need to apply the PPCA transform to Series from the DataFrame that are not the one used for training. "},{"metadata":{"trusted":true,"_uuid":"eb3e4ca38a9245a4dba9b03a1a188b4c86cc7933"},"cell_type":"code","source":"# A function to transform a column from the  using a PPCA which was trained on a different column\ndef PpcaTransform(df,column,ppca):\n    return ppca.transform(DfToData(df,column))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d85c28cef9cd225b30cdc78bca97ece595cc6230","scrolled":true},"cell_type":"code","source":"print('Residual 1 day opening price returns')\nOpen1d = PpcaTransform(market_train_df,'returnsOpenPrevRaw1',ppca)\nprint('\\n\\nResidual 1 day closing price returns')\nClose1d = PpcaTransform(market_train_df,'returnsClosePrevRaw1',ppca)\nprint('\\n\\nResidual 10 day opening price returns')\nOpen10d = PpcaTransform(market_train_df,'returnsOpenPrevRaw10',ppca)\nprint('\\n\\nResidual 10 day closing price returns')\nClose10d = PpcaTransform(market_train_df,'returnsClosePrevRaw10',ppca)\nprint('\\n\\nDaily volumes')\nVolume = PpcaTransform(market_train_df,'volume',ppca)\nprint('\\n\\nThe target forward-looking returns')\nTargets = PpcaTransform(market_train_df,'returnsOpenNextMktres10',ppca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3bc568e61c39e402b667fcd2f1e10a332365246"},"cell_type":"code","source":"Market_Features = np.concatenate([Open1d,Close1d,Open10d,Close10d,Volume],axis=1)\nnp.save('Features.np',Market_Features)\nnp.save('Targets.np',Targets)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71416cb9acffd59b36e34f55129092d7678bfca1"},"cell_type":"markdown","source":"## Visualisation\n\nOK - all very textual so far, let's look at some images.\n\nFirst things first - can we see any significant structure in the principle components returned?"},{"metadata":{"trusted":true,"_uuid":"7211927b8cba76904fde2f5e0544b905d9b4825f"},"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(30,10))\nim=ax.imshow(ppca.C,aspect='auto',cmap='RdBu')\n\nplt.colorbar(im)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14c8cd710fba50dc7abf4f77fbee0680a9b4a1ea"},"cell_type":"markdown","source":"So, clearly a consistent bias on the first component (to the negative - as it happens) otherwise hard to discern much. In a first pass there were some assets that seem to be particularly strong on quite a few components - but those have disappeared with better datafiltering.\n\nNext, what do the PPCA timeseries look like for the returns we used for training?"},{"metadata":{"trusted":true,"_uuid":"a6156be8a6e559f0a499fe1b2b12f6ef0d31f5e2"},"cell_type":"code","source":"plt.figure(figsize=(30, 15))\nlabels = range(ppca.C.shape[1])\nfor values,label in zip(Close10d.T,labels):\n    plt.plot(values,label=label)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e4475a2135e6746c7a0b00dc59188f2f4c429ff"},"cell_type":"markdown","source":"For the most part that is so much noise. A bit of structure - some points clearly much more volatile than others. The first component (the blue line) showing the largest moves, as would be normal for a PCA.\n\nHow about the volume transformed with the same analysis?"},{"metadata":{"trusted":true,"_uuid":"2b10f571d844d0af8f20fd63693de2e30d6a33b6"},"cell_type":"code","source":"plt.figure(figsize=(30, 15))\nplt.plot(Volume)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"043116c0c6c9a0ec3183a201f682ac922dd9552f"},"cell_type":"markdown","source":"This would seem to have less obvious output structure. Possibly still some dominance from the first component - but much harder to tell. But then - what would really be expected from a PCA of daily volumes?\n\nWe'd better check that the transformation is actually working to fit the market. Let's try that for an arbitrary asset - showing how much of the assets movement can be explained by the PCA and how much is residual noise."},{"metadata":{"trusted":true,"_uuid":"9ca937b15f925145c72924f0159c0e5d8d25f513","scrolled":true},"cell_type":"code","source":"asset = 2060\n\ndata = DfToData(market_train_df,'returnsClosePrevRaw10')\npca_history = Close10d\n\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10),sharey=True)\nreconstructed = np.sum(ppca.C[asset]*ppca.stds[asset]*pca_history,axis=1)\nax1.plot(reconstructed)\nax1.plot(data[:,asset])\nax2.plot(data[:,asset]-reconstructed)\nprint('Data Std Dev       : {:6.4f}'.format(np.nanstd(data[:,asset])))\nprint('Reconstructed StDev: {:6.4f}'.format(np.nanstd(reconstructed)))\nprint('Noise StDev        : {:6.4f}'.format(np.nanstd(data[:,asset]-reconstructed)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c847bbe464a8c449b9bc481b7953a69432d61df"},"cell_type":"markdown","source":"So yes, a strong reduction in noise if we look at the returns we trained with.\n\nHow about if we use a different set of returns?"},{"metadata":{"trusted":true,"_uuid":"34f9a6cb1baae40e343d7c7f3bab18ccddd1782e","scrolled":true},"cell_type":"code","source":"data2 = DfToData(market_train_df,'returnsOpenPrevRaw1')\npca_history = Open1d\n\nppca.eig_vals\nasset = 2060\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10),sharey=True)\nreconstructed2 = np.sum(ppca.C[asset]*ppca.stds[asset]*pca_history,axis=1)\nax1.plot(reconstructed)\nax1.plot(data2[:,asset])\nax2.plot(data2[:,asset]-reconstructed2)\nprint('Data Std Dev       : {:6.4f}'.format(np.nanstd(data2[:,asset])))\nprint('Reconstructed StDev: {:6.4f}'.format(np.nanstd(reconstructed2)))\nprint('Noise StDev        : {:6.4f}'.format(np.nanstd(data2[:,asset]-reconstructed2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f62363d53bf503637e1a93f5ab87509d9c2576a"},"cell_type":"markdown","source":"So not so much on the noise reduction there. Seems like the PCA might just be telling us things about 10 day closing returns.\n\nIf we look at the returns on the remaining noise on the training data we see:"},{"metadata":{"trusted":true,"_uuid":"1c9563ad99629cad9742446d1459ab51eb0a73f4"},"cell_type":"code","source":"noise = data[:,asset]-reconstructed\nplt.hist(noise[~np.isnan(noise)],bins=25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"665063148542a0c675636b41546b34421d6e8292"},"cell_type":"markdown","source":"Still pretty heavy tailed - not that Normal - but still shouldn't be hard to model with a residue term.\n\nHowever, for this competition getting the size of the noise exactly right isn't really that important. All we really care about is predicting the correct direction. Let's have a look at a scatter plot to see how we've done for that on the training data (remembering this is still just one arbitrary asset):"},{"metadata":{"trusted":true,"_uuid":"dddd9ac7dba539cac08db7e5ec3c655e92479c1e"},"cell_type":"code","source":"plt.scatter(data[:,asset],reconstructed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6db337198802218a7928ae1a5cbf555682bc288"},"cell_type":"markdown","source":"Looks promising\n\nAnd for the one days returns?"},{"metadata":{"trusted":true,"_uuid":"3079f779f51c9239aa1a3f0dfc15337632636ffc"},"cell_type":"code","source":"plt.scatter(data2[:,asset],reconstructed2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1551dbe566d1e0e49a8269a337858a6b525aaac0"},"cell_type":"markdown","source":"Still a clear structure - but not as good as for the 10 day returns.\n\nBut this is just the time series for one asset. To confirm the point, lets look at all of them. Firstly for the Series that we used in training."},{"metadata":{"trusted":true,"_uuid":"0dab20cc01f5d037dfd6fcb9002ef12728daec4f"},"cell_type":"code","source":"def DfWithErrorStats(data,pca_history):\n    data_std = []\n    recon_std = []\n    noise_std = []\n    count_nan = []\n    correct_count = []\n    wrong_count = []\n    net_return = []\n    for asset in range(data.shape[1]):\n        reconstructed = np.sum(ppca.C[asset]*ppca.stds[asset]*pca_history,axis=1)\n        data_std.append(np.nanstd(data[:,asset]))\n        recon_std.append(np.nanstd(reconstructed))\n        noise_std.append(np.nanstd(data[:,asset]-reconstructed))\n        count_nan.append((~np.isnan(data[:,asset])).sum())\n        correct = 0\n        wrong = 0\n        net = 0\n        for orig,recon in zip(data[:,asset],reconstructed):\n            if not np.isnan(orig):\n                if ((orig > 0) and (recon > 0)) or ((orig < 0) and (recon < 0)):\n                    correct += 1\n                else:\n                    wrong += 1\n                net += np.sign(recon)*orig\n        correct_count.append(correct)\n        wrong_count.append(wrong)\n        net_return.append(net)\n\n    std_df = pd.DataFrame({'Count':count_nan,'DataStd':data_std,'ReconStd':recon_std,'NoiseStd':noise_std, 'Correct':correct_count, 'Wrong':wrong_count, 'Net':net_return})\n    std_df['Ratio']=std_df['NoiseStd']/std_df['DataStd']\n    std_df['Accuracy']=std_df['Correct']/(std_df['Correct']+std_df['Wrong'])\n    del data_std\n    del recon_std\n    del noise_std\n    return std_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db42445b51c491eba6acc4a100f85d0645abfa6e"},"cell_type":"code","source":"data = market_train_df.pivot(index='time',columns='assetCode',values='returnsClosePrevRaw10').values\nasset_means = np.nanmean(data,axis=0)\ndata -= asset_means\n\npca_history = Close10d\n\nstats_train_df = DfWithErrorStats(data,pca_history)\nstats_train_df.sort_values(['Count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"095f5f607c4232eb11d7b793a6e57a89cb9e60ee"},"cell_type":"code","source":"stats_train_df['Accuracy'].hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6007b53f1099fd346a2de17f10c10f5e4c63b452"},"cell_type":"code","source":"stats_train_df['Correct'].sum()/(stats_train_df['Correct'].sum()+stats_train_df['Wrong'].sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae8e0a43bde63709db88cca86ef7936aa55e20bd"},"cell_type":"markdown","source":"So what we are seeing is basically random accuracy for the assets with just a few observations - no big surprises there - but typically 80%+ accuracy for series that have reasonable sample sizes. Most assets correctly classify if the stock will go up or down in 80% plus of cases. Overall accuracy classified in 84% of cases. \n\nI did try this for a range of different numbers of components (before applying filtering to the data) - and got the accuracies listed below.  218 seemed to be the maximum components I could fit:\n* 218 PC - 88.19%\n* 200 PC - 87.9%\n* 128 PC - 84.74%\n* 100 PC - 82.81%\n* 50 PC - 78.78%\n\nNext we look at one of the other input series:"},{"metadata":{"trusted":true,"_uuid":"a84aa5974368fb3b7cdb870e78d24e37ec78ba94"},"cell_type":"code","source":"data = market_train_df.pivot(index='time',columns='assetCode',values='returnsOpenPrevRaw1').values\nasset_means = np.nanmean(data,axis=0)\ndata -= asset_means\n\npca_history = Open1d\n\nstats_1day_df = DfWithErrorStats(data,pca_history)\nstats_1day_df.sort_values(['Count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c48b6166b14a06d4aed18ef625149e4c7652350"},"cell_type":"code","source":"stats_1day_df['Accuracy'].hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca25b8b82a21700a5512506eab12af5ebed4428b"},"cell_type":"code","source":"stats_1day_df['Correct'].sum()/(stats_train_df['Correct'].sum()+stats_train_df['Wrong'].sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ff838b2bd164644a9f7348e8a5d811f17b0ea7b"},"cell_type":"markdown","source":"So not as good here. Only 75% accuracy overall and most series classified accurately 70% to 80% of the time. Remains to be seen if that will be good enough to use PCAed data for training.\n\nNext lets analyse the volume"},{"metadata":{"_uuid":"73bc2986584e373e14e941f4ff057fa1181bdab3","trusted":true},"cell_type":"code","source":"data = market_train_df.pivot(index='time',columns='assetCode',values='volume').values\nasset_means = np.nanmean(data,axis=0)\ndata -= asset_means\n\npca_history = Volume\n\nstats_vol_df = DfWithErrorStats(data,pca_history)\nstats_vol_df.sort_values(['Count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c89afe7641b9ab6949d3ef91e9fc4e1686bafce"},"cell_type":"code","source":"stats_vol_df['Accuracy'].hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be7abf7b5f9427bbfdbaa2eaafd11e9a0fdbbb53"},"cell_type":"code","source":"stats_vol_df['Correct'].sum()/(stats_train_df['Correct'].sum()+stats_train_df['Wrong'].sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e17e61f9297c01d9d3bc3a78388da5ca891d94f"},"cell_type":"markdown","source":"OK - so that's not so good. The relationship between PCA volume and actual volume (talking about deviations from the mean volume for each asset here) is only slightly better than random - and a sizeable chunk of the assets are worse than random. So PCA volume is out as a driver.\n\nFinally the important one - does this PCA also work on the (forward looking) target values as well as the backwards looking drivers?"},{"metadata":{"trusted":true,"_uuid":"ad3f5e158af3d86f92e26d52f9c93e39193edadb"},"cell_type":"code","source":"data = market_train_df.pivot(index='time',columns='assetCode',values='returnsOpenNextMktres10').values\nasset_means = np.nanmean(data,axis=0)\ndata -= asset_means\n\npca_history = Targets\n\nstats_target_df = DfWithErrorStats(data,pca_history)\nstats_target_df.sort_values(['Count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2eea1a05c61f3ff6ad0f4345f8ef35188b4e59cf"},"cell_type":"code","source":"stats_target_df['Accuracy'].hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"791e4ceedb697afd0f702c5ced7861578d5c05a6"},"cell_type":"code","source":"stats_target_df['Correct'].sum()/(stats_train_df['Correct'].sum()+stats_train_df['Wrong'].sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c53fbc020f6878bf10c11d1945c96d9867443f1b"},"cell_type":"markdown","source":"That's not terrible. Just passes the 2/3 test required for being right as a trader to make money. It is disappointing though - would have expected better when building the principle components off historical 10 day returns."},{"metadata":{"trusted":true,"_uuid":"54b49b9a67c3a604794d1fdb3749439027a08a75"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}