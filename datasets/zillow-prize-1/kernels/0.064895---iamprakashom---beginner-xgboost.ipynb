{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_uuid": "e2eff04a6c7ec87a73e88784dbc958f88525e002", "_cell_guid": "f1cbd81b-b8c8-43ce-b415-310f7e9440b1"}, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import xgboost as xgb\n", "import random\n", "import datetime as dt\n", "import gc\n", "\n", "import seaborn as sns #python visualization library \n", "color = sns.color_palette()\n", "\n", "#%matplotlib inline\n", "np.random.seed(1)\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "execution_count": 2}, {"metadata": {}, "cell_type": "code", "source": ["# We need to load the datasets that will be needed to train our machine learning algorithms, handle our data and make predictions. \n", "train = pd.read_csv('../input/train_2016_v2.csv' , parse_dates=[\"transactiondate\"]) \n", "properties = pd.read_csv('../input/properties_2016.csv')   \n", "test = pd.read_csv('../input/sample_submission.csv') \n", "test= test.rename(columns={'ParcelId': 'parcelid'}) #To make it easier for merging datasets on same column_id later"], "outputs": [], "execution_count": 3}, {"metadata": {}, "cell_type": "code", "source": ["### Analyse the Dimensions of our Datasets.\n", "print(\"Training Size:\" + str(train.shape))\n", "print(\"Property Size:\" + str(properties.shape))\n", "print(\"Sample Size:\" + str(test.shape))"], "outputs": [], "execution_count": 4}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": ["### Type Converting the DataSet ###\n", "# The processing of some of the algorithms can be made quick if data representation is made in int/float32 instead of int/float64. \n", "# Therefore, in order to make sure that all of our columns types are in float32, we are implementing the following lines of code #\n", "\n", "for c, dtype in zip(properties.columns, properties.dtypes):\n", "    if dtype == np.float64:        \n", "        properties[c] = properties[c].astype(np.float32)\n", "    if dtype == np.int64:\n", "        properties[c] = properties[c].astype(np.int32)\n", "\n", "for column in test.columns:\n", "    if test[column].dtype == int:\n", "        test[column] = test[column].astype(np.int32)\n", "    if test[column].dtype == float:\n", "        test[column] = test[column].astype(np.float32)"], "outputs": [], "execution_count": 7}, {"metadata": {}, "cell_type": "code", "source": ["### Let's do some feature engineering\n", "\n", "#living area proportions \n", "properties['living_area_prop'] = properties['calculatedfinishedsquarefeet'] / properties['lotsizesquarefeet']\n", "#tax value ratio\n", "properties['value_ratio'] = properties['taxvaluedollarcnt'] / properties['taxamount']\n", "#tax value proportions\n", "properties['value_prop'] = properties['structuretaxvaluedollarcnt'] / properties['landtaxvaluedollarcnt']\n", "\n", "###Merging the Datasets ###\n", "# We are merging the properties dataset with training and testing dataset for model building and testing prediction #\n", "\n", "df_train = train.merge(properties, how='left', on='parcelid') \n", "df_test = test.merge(properties, how='left', on='parcelid')\n", "\n", "\n", "### Remove previos variables to keep some memory\n", "del properties, train\n", "gc.collect();\n", "\n", "\n", "print('Memory usage reduction...')\n", "df_train[['latitude', 'longitude']] /= 1e6\n", "df_test[['latitude', 'longitude']] /= 1e6\n", "\n", "df_train['censustractandblock'] /= 1e12\n", "df_test['censustractandblock'] /= 1e12\n", "\n", "\n", "### Let's do some pre-exploratory analysis to identify how much missing values do we have in our datasets. \n", "### Thanks to Nikunj-Carefully dealing with missing values. Ref. https://www.kaggle.com/nikunjm88/carefully-dealing-with-missing-values \n", "\n", "# Let's do some engineering with fireplaceflag variable.\n", "\n", "print(df_train.fireplaceflag.isnull().sum())\n", "print(df_train.fireplacecnt.isnull().sum())\n", "# By using fireplacecnt variable we can recover some fields of fireplaceflag\n", "\n", "df_train['fireplaceflag']= \"No\"\n", "df_train.loc[df_train['fireplacecnt']>0,'fireplaceflag']= \"Yes\"\n", "\n", "# Remaining Missing fireplacecnt will be replaced with 0.\n", "index = df_train.fireplacecnt.isnull()\n", "df_train.loc[index,'fireplacecnt'] = 0\n", "\n", "#Tax deliquency flag - assume if it is null then doesn't exist\n", "index = df_train.taxdelinquencyflag.isnull()\n", "df_train.loc[index,'taxdelinquencyflag'] = \"None\"\n", "\n", "\n", "# Similar step performed for Pool/Spa/hot tub\n", "print(df_train.hashottuborspa.value_counts())\n", "print(df_train.pooltypeid10.value_counts())\n", "\n", "#lets remove 'pooltypeid10' as it has more missing values\n", "print(df_train.hashottuborspa.value_counts())\n", "print(df_train.pooltypeid10.value_counts())\n", "\n", "#Assume if the pooltype id is null then pool/hottub doesnt exist \n", "index = df_train.pooltypeid2.isnull()\n", "df_train.loc[index,'pooltypeid2'] = 0\n", "\n", "index = df_train.pooltypeid7.isnull()\n", "df_train.loc[index,'pooltypeid7'] = 0\n", "\n", "index = df_train.poolcnt.isnull()\n", "df_train.loc[index,'poolcnt'] = 0"], "outputs": [], "execution_count": 8}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": ["### Label Encoding For Machine Learning & Filling Missing Values ###\n", "# We are now label encoding our datasets. LabelEncoding ensures that all of our categorical variables are in numerical representation. \n", "# Also note that we are filling the missing values in our dataset with a zero before label encoding them. \n", "# This is to ensure that label encoder function does not experience any problems while carrying out its operation #\n", "\n", "from sklearn.preprocessing import LabelEncoder  \n", "\n", "lbl = LabelEncoder()\n", "for c in df_train.columns:\n", "    df_train[c]=df_train[c].fillna(0)\n", "    if df_train[c].dtype == 'object':\n", "        lbl.fit(list(df_train[c].values))\n", "        df_train[c] = lbl.transform(list(df_train[c].values))\n", "\n", "for c in df_test.columns:\n", "    df_test[c]=df_test[c].fillna(0)\n", "    if df_test[c].dtype == 'object':\n", "        lbl.fit(list(df_test[c].values))\n", "        df_test[c] = lbl.transform(list(df_test[c].values))     "], "outputs": [], "execution_count": 9}, {"metadata": {}, "cell_type": "code", "source": ["### Removing the Outliers\n", "log_errors = df_train['logerror']\n", "df_train = df_train[df_train.logerror < np.percentile(log_errors, 99.5)]\n", "df_train = df_train[df_train.logerror > np.percentile(log_errors, 0.5)]\n", "\n", "### Rearranging the DataSets ###\n", "\n", "# We will now drop the features that serve no useful purpose. We will also split our data \n", "# and divide it into the representation to make it clear which features are to be treated as \n", "# determinants in predicting the outcome for our target feature. Make sure to include the same \n", "# features in the test set as were included in the training set #\n", "\n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n", "                         'propertycountylandusecode' ], axis=1)\n", "\n", "x_test = df_test.drop(['parcelid', 'propertyzoningdesc',\n", "                       'propertycountylandusecode', '201610', '201611', \n", "                       '201612', '201710', '201711', '201712'], axis = 1) \n", "\n", "x_train = x_train.values\n", "y_train = df_train['logerror'].values\n", "\n", "### Cross Validation ###\n", "\n", "# We are dividing our datasets into the training and validation sets so that we could monitor and the test \n", "# the progress of our machine learning algorithm. This would let us know when our model might be over \n", "# or under fitting on the dataset that we have employed. #\n", "\n", "from sklearn.model_selection import train_test_split\n", "\n", "X = x_train\n", "y = y_train \n", "\n", "Xtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=42)\n", "\n", "###Implement the Xgboost### \n", "\n", "# We can now select the parameters for Xgboost and monitor the progress of results on our validation set. \n", "# The explanation of the xgboost parameters and what they do can be found on the following link \n", "# http://xgboost.readthedocs.io/en/latest/parameter.html #\n", "\n", "dtrain = xgb.DMatrix(Xtrain, label=ytrain)\n", "dvalid = xgb.DMatrix(Xvalid, label=yvalid)\n", "dtest = xgb.DMatrix(x_test.values)\n", "\n", "# Try different parameters! \n", "xgb_params = {'min_child_weight': 5, 'eta': 0.035, 'colsample_bytree': 0.5, 'max_depth': 4,\n", "            'subsample': 0.85, 'lambda': 0.8, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n", "            'eval_metric': 'mae', 'objective': 'reg:linear' }           \n", "\n", "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n", "\n", "model_xgb = xgb.train(xgb_params, dtrain, 1000, watchlist, early_stopping_rounds=100,\n", "                  maximize=False, verbose_eval=10)\n", "\n", "###Predicting the results###\n", "\n", "# Let us now predict the target variable for our test dataset. All we have to do now is just fit \n", "# the already trained model on the test set that we had made merging the sample file with properties dataset #\n", "\n", "Predicted_test_xgb = model_xgb.predict(dtest)"], "outputs": [], "execution_count": 10}, {"metadata": {}, "cell_type": "code", "source": ["### Submitting the Results ###\n", "# Once again load the file and start submitting the results in each column #\n", "sample_file = pd.read_csv('../input/sample_submission.csv') \n", "for c in sample_file.columns[sample_file.columns != 'ParcelId']:\n", "    sample_file[c] = Predicted_test_xgb\n", "\n", "print('Preparing the csv file ...')\n", "sample_file.to_csv('xgb_predicted_results.csv', index=False, float_format='%.4f')\n", "print(\"Finished writing the file\")"], "outputs": [], "execution_count": 11}]}