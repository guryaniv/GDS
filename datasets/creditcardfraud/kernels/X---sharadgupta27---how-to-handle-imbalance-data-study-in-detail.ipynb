{"cells":[{"metadata":{"_cell_guid":"b132247d-6f04-5a90-1eec-7f0872a70a98","_uuid":"4bdfb77abf5228a0c8d72b75f00e9676149ee009"},"cell_type":"markdown","source":"##Hi all as we know credit card fraud detection will have a imbalanced data i.e having more number of normal class than the number of fraud class\n\n###In this I will use Basic method of handling imbalance data which are\n ** This all I have done by using Analytics Vidya's blog please find the link [Analytics Vidya](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/)  **\n\nUndersampling:- it means taking the less number of majority class (In our case taking less number of Normal transactions so that our new data will be balanced\n\nOversampling: it means using replicating the data of minority class (fraud class) so that we can have a balanced data\n\nSMOTE: it is also a type of oversampling but in this we will make the synthetic example of Minority data and will give as a balanced data\n\nFirst I will start with the Undersampling and will try to classify using these Models\n1. Decision Tree Classifier/ Random Forest Classifier\n\n2. Logistic regression\n\n3. SVM\n\n4. XGboost"},{"metadata":{"_cell_guid":"73c46801-7f23-67a2-47e8-03256085f659","_uuid":"0905cceb065fba0c94498292ac5cdabb1115fe23","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"228eec75-5856-5411-7e59-a4d419c2e520","_uuid":"bdbbfe427b1a7f07689a20a5fe42393867ae292e"},"cell_type":"markdown","source":"###Lets start with Importing Libraries and data"},{"metadata":{"_cell_guid":"7acc51d2-e9db-908f-9b91-386e23629483","_uuid":"43109d6e73a4525d773f352b851c657e8b9bfe78","trusted":false},"cell_type":"code","source":"import pandas as pd # to import csv and for data manipulation\nimport matplotlib.pyplot as plt # to plot graph\nimport seaborn as sns # for intractve graphs\nimport numpy as np # for linear algebra\nimport datetime # to dela with date and time\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler # for preprocessing the data\nfrom sklearn.ensemble import RandomForestClassifier # Random forest classifier\nfrom sklearn.tree import DecisionTreeClassifier # for Decision Tree classifier\nfrom sklearn.svm import SVC # for SVM classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split # to split the data\nfrom sklearn.cross_validation import KFold # For cross vbalidation\nfrom sklearn.model_selection import GridSearchCV # for tunnig hyper parameter it will use all combination of given parameters\nfrom sklearn.model_selection import RandomizedSearchCV # same for tunning hyper parameter but will use random combinations of parameters\nfrom sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83f83459-d9f6-e2fd-9d11-77ee19a48ff0","_uuid":"3cca698acdb380702e0216c9dfe9f22b6f18ecde","trusted":false},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcard.csv\",header = 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9022d43c-a547-8ab0-bbfc-dab6fb4b7c11","_uuid":"27c18f615c52914891ffc038d8cb2256cbdf6934"},"cell_type":"markdown","source":"####Now explore the data to get insight in it"},{"metadata":{"_cell_guid":"be938861-a429-3bd1-eb90-45b7616770ee","_uuid":"61c3a21f6efc9f7f8a9fd9df1cfc0633e7c57bb1","trusted":false},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6a3e8d76-0840-cf04-d4f1-e68ffddd442f","_uuid":"0b97d347919338c29d4822689c5b3207138d51b5"},"cell_type":"markdown","source":"1. Hence we can see there are 284,807 rows and 31 columns which is a huge data\n2.  Time is also in float here mean it can be only seconds starting from a particular time"},{"metadata":{"_cell_guid":"03b4ce97-1f9f-ab4a-bcda-18afa1e775e6","_uuid":"cb382dc331d1a0685bc1859f988e6e15e086f954","trusted":false},"cell_type":"code","source":"# Now lets check the class distributions\nsns.countplot(\"Class\",data=data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b07af191-1a1b-27fb-9b3c-462f2df5cf20","_uuid":"3df7e313c8c137a9a868e9db81d6ca682e8ec7bd"},"cell_type":"markdown","source":"1. As we know data is imbalanced and this graph also confirmed it "},{"metadata":{"_cell_guid":"8c79df42-9432-e39e-cb27-6c53245f9ac5","_uuid":"ed4893c7d503007d738f675a784e5cbef3b150f2","trusted":false},"cell_type":"code","source":"# now let us check in the number of Percentage\nCount_Normal_transacation = len(data[data[\"Class\"]==0]) # normal transaction are repersented by 0\nCount_Fraud_transacation = len(data[data[\"Class\"]==1]) # fraud by 1\nPercentage_of_Normal_transacation = Count_Normal_transacation/(Count_Normal_transacation+Count_Fraud_transacation)\nprint(\"percentage of normal transacation is\",Percentage_of_Normal_transacation*100)\nPercentage_of_Fraud_transacation= Count_Fraud_transacation/(Count_Normal_transacation+Count_Fraud_transacation)\nprint(\"percentage of fraud transacation\",Percentage_of_Fraud_transacation*100)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6bdeb418-1ad4-4101-a97f-218cad9d60b7","_uuid":"06f992cf3a06d2f023fb6a223f6bdd344ef3235b"},"cell_type":"markdown","source":"1. Hence in data there is only 0.17 % are the fraud transcation while 99.83 are valid transcation\n2. So now we have to do resampling of this data\n3. before doing resampling lets have look at the amount related to valid transcation and fraud transcation"},{"metadata":{"_cell_guid":"29942ac2-ea51-aa4f-ae2f-4d245908fb45","_uuid":"3675479cfde79a928306d5fa2aa9dbcd6a6eab54","trusted":false},"cell_type":"code","source":"Fraud_transacation = data[data[\"Class\"]==1]\nNormal_transacation= data[data[\"Class\"]==0]\nplt.figure(figsize=(10,6))\nplt.subplot(121)\nFraud_transacation.Amount.plot.hist(title=\"Fraud Transacation\")\nplt.subplot(122)\nNormal_transacation.Amount.plot.hist(title=\"Normal Transaction\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c54fd02-d25e-0763-5c15-b35e3b0df4d2","_uuid":"74947063c20e43ff7c8093b6897c81817d8b5fb1","trusted":false},"cell_type":"code","source":"# the distribution for Normal transction is not clear and it seams that all transaction are less than 2.5 K\n# So plot graph for same \nFraud_transacation = data[data[\"Class\"]==1]\nNormal_transacation= data[data[\"Class\"]==0]\nplt.figure(figsize=(10,6))\nplt.subplot(121)\nFraud_transacation[Fraud_transacation[\"Amount\"]<= 2500].Amount.plot.hist(title=\"Fraud Tranascation\")\nplt.subplot(122)\nNormal_transacation[Normal_transacation[\"Amount\"]<=2500].Amount.plot.hist(title=\"Normal Transaction\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bf6856cc-3e0a-5579-b901-83bd51f3ffc4","_uuid":"9577a270604ec7a051884e2b6969996d9c6b740e"},"cell_type":"markdown","source":"1. Here now after exploring data we can say there is no pattern in data\n2. Now lets start with resmapling of data"},{"metadata":{"_cell_guid":"e7c06498-d474-1e15-ef69-7ead60df63c6","_uuid":"211ce94bc78ecf1529ac97d00c99cbacb29b7f89"},"cell_type":"markdown","source":"###ReSampling - Under Sampling"},{"metadata":{"_cell_guid":"08fb5763-fadd-e215-0ad4-33c9ee97ec7b","_uuid":"f236720f60d249a41ce5c423e1cf1b197f63d8ae"},"cell_type":"markdown","source":"Before re sampling lets have look at the different accuracy matrices\n\nAccuracy = TP+TN/Total\n\nPrecison = TP/(TP+FP)\n\nRecall = TP/(TP+FN)\n\nTP = True possitive means no of possitve cases which are predicted possitive\n\nTN = True negative means no of negative cases which are predicted negative\n\nFP = False possitve means no of negative cases which are predicted possitive\n\nFN= False Negative means no of possitive cases which are predicted negative\n\nNow for our case recall will be a better option because in these case no of normal transacations will be very high than the no of fraud cases and sometime a fraud case will be predicted as normal. So, recall will give us a sense of only fraud cases\n\nResampling\n\nin this we will resample our data with different size\n\nthen we will try to use this resampled data to train our model\n\nthen we will use this model to predict for our original data"},{"metadata":{"_cell_guid":"df60d504-c140-7de8-9343-bb7a4c4eb7e7","_uuid":"646cc6f1b91e6d463899788a679997e248075f51","trusted":false},"cell_type":"code","source":"# for undersampling we need a portion of majority class and will take whole data of minority class\n# count fraud transaction is the total number of fraud transaction\n# now lets us see the index of fraud cases\nfraud_indices= np.array(data[data.Class==1].index)\nnormal_indices = np.array(data[data.Class==0].index)\n#now let us a define a function for make undersample data with different proportion\n#different proportion means with different proportion of normal classes of data\ndef undersample(normal_indices,fraud_indices,times):#times denote the normal data = times*fraud data\n    Normal_indices_undersample = np.array(np.random.choice(normal_indices,(times*Count_Fraud_transacation),replace=False))\n    undersample_data= np.concatenate([fraud_indices,Normal_indices_undersample])\n    undersample_data = data.iloc[undersample_data,:]\n    \n    print(\"the normal transacation proportion is :\",len(undersample_data[undersample_data.Class==0])/len(undersample_data[undersample_data.Class]))\n    print(\"the fraud transacation proportion is :\",len(undersample_data[undersample_data.Class==1])/len(undersample_data[undersample_data.Class]))\n    print(\"total number of record in resampled data is:\",len(undersample_data[undersample_data.Class]))\n    return(undersample_data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12f3b2ed-4231-cddc-9959-d13491af2497","_uuid":"81cf4a1d2a23d1f62d10d7488f37dafad559d26f","trusted":false},"cell_type":"code","source":"## first make a model function for modeling with confusion matrix\ndef model(model,features_train,features_test,labels_train,labels_test):\n    clf= model\n    clf.fit(features_train,labels_train.values.ravel())\n    pred=clf.predict(features_test)\n    cnf_matrix=confusion_matrix(labels_test,pred)\n    print(\"the recall for this model is :\",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n    fig= plt.figure(figsize=(6,3))# to plot the graph\n    print(\"TP\",cnf_matrix[1,1,]) # no of fraud transaction which are predicted fraud\n    print(\"TN\",cnf_matrix[0,0]) # no. of normal transaction which are predited normal\n    print(\"FP\",cnf_matrix[0,1]) # no of normal transaction which are predicted fraud\n    print(\"FN\",cnf_matrix[1,0]) # no of fraud Transaction which are predicted normal\n    sns.heatmap(cnf_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\n    plt.title(\"Confusion_matrix\")\n    plt.xlabel(\"Predicted_class\")\n    plt.ylabel(\"Real class\")\n    plt.show()\n    print(\"\\n----------Classification Report------------------------------------\")\n    print(classification_report(labels_test,pred))\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4990e8ea-0e93-342c-4c1a-39117c1814d3","_uuid":"7ea6c8027cb49a8f2277831cb4e1377ffc49f91c","trusted":false},"cell_type":"code","source":"def data_prepration(x): # preparing data for training and testing as we are going to use different data \n    #again and again so make a function\n    x_features= x.ix[:,x.columns != \"Class\"]\n    x_labels=x.ix[:,x.columns==\"Class\"]\n    x_features_train,x_features_test,x_labels_train,x_labels_test = train_test_split(x_features,x_labels,test_size=0.3)\n    print(\"length of training data\")\n    print(len(x_features_train))\n    print(\"length of test data\")\n    print(len(x_features_test))\n    return(x_features_train,x_features_test,x_labels_train,x_labels_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c5529ce-71ca-b24b-b544-e3d1443115f9","_uuid":"ab455e595f514d63911b8c7f7fd321ab4d60a3c8","trusted":false},"cell_type":"code","source":"# before starting we should standridze our ampount column\ndata[\"Normalized Amount\"] = StandardScaler().fit_transform(data['Amount'].reshape(-1, 1))\ndata.drop([\"Time\",\"Amount\"],axis=1,inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"95e5165c-4ad8-1a17-021c-9024c26bd80a","_uuid":"537f89b0c33d5d778e27686c5e2fab1add001760"},"cell_type":"markdown","source":"### Logistic Regression with Undersample Data"},{"metadata":{"_cell_guid":"cebaabc9-548c-81b1-c0bf-e0ef638cab2a","_uuid":"8ebaef4e319a5842f51499417a7ad9a9834d35e8","trusted":false},"cell_type":"code","source":"# Now make undersample data with differnt portion\n# here i will take normal trasaction in  0..5 %, 0.66% and 0.75 % proportion of total data now do this for \nfor i in range(1,4):\n    print(\"the undersample data for {} proportion\".format(i))\n    print()\n    Undersample_data = undersample(normal_indices,fraud_indices,i)\n    print(\"------------------------------------------------------------\")\n    print()\n    print(\"the model classification for {} proportion\".format(i))\n    print()\n    undersample_features_train,undersample_features_test,undersample_labels_train,undersample_labels_test=data_prepration(Undersample_data)\n    print()\n    clf=LogisticRegression()\n    model(clf,undersample_features_train,undersample_features_test,undersample_labels_train,undersample_labels_test)\n    print(\"________________________________________________________________________________________________________\")\n    \n# here 1st proportion conatain 50% normal transaction\n#Proportion 2nd contains 66% noraml transaction\n#proportion 3rd contains 75 % normal transaction","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6dc3fc4b-a979-60ce-5c86-e2d912b65e57","_uuid":"83e0e6d343fd84ecd501381431c2e8d699a7b58b"},"cell_type":"markdown","source":" 1. As the number of normal transaction is increasing the recall for fraud transcation is decreasing\n 2. TP = no of fraud transaction which are predicted fraud\n 3.  TN = no. of normal transaction which are predicted normal\n 4.  FP =  no of normal transaction which are predicted fraud\n 5.  FN =no of fraud Transaction which are predicted normal"},{"metadata":{"_cell_guid":"8386da63-0882-bc9c-e84b-c679d307c341","_uuid":"03067b56ebbd7c863fa081321f57b2eb2a2e881f","trusted":false},"cell_type":"code","source":"#let us train this model using undersample data and test for the whole data test set \nfor i in range(1,4):\n    print(\"the undersample data for {} proportion\".format(i))\n    print()\n    Undersample_data = undersample(normal_indices,fraud_indices,i)\n    print(\"------------------------------------------------------------\")\n    print()\n    print(\"the model classification for {} proportion\".format(i))\n    print()\n    undersample_features_train,undersample_features_test,undersample_labels_train,undersample_labels_test=data_prepration(Undersample_data)\n    data_features_train,data_features_test,data_labels_train,data_labels_test=data_prepration(data) \n    #the partion for whole data\n    print()\n    clf=LogisticRegression()\n    model(clf,undersample_features_train,data_features_test,undersample_labels_train,data_labels_test)\n    # here training for the undersample data but tatsing for whole data\n    print(\"_________________________________________________________________________________________\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f7d70d65-2785-06aa-781a-334052095a1d","_uuid":"5bf4312a946362839bd731d240e705e22fc87286"},"cell_type":"markdown","source":"1. Here we can see it is following same recall pattern as it  was for under sample data that's sounds good but if we have look at the precision is very less\n\n2. So we should built a model which is correct overall\n\n3. Precision is less means we are predicting other class wrong like as for our third part  there were 953 transaction are predicted fraud it means we and recall is good then it means we are catching fraud transaction very well but we are catching innocent transaction also i.e which are not fraud.\n\n4. So with recall our precision should be better\n\n5. if we go by this model then we are going to put 953 innocents in jail with the all criminal who have actually done this\n6. Hence we are mainly lacking in the precision how can we increase our precision\n7. Don't get confuse with above output showing that the two training data and two test data first one is for undersample data  while another one is for our whole data"},{"metadata":{"_cell_guid":"f375e55d-d9c6-6e87-6d60-b18afc2e65b2","_uuid":"118cd84d00b267ed8fd45572f8dec5fca33f77f6"},"cell_type":"markdown","source":"1.**Try with SVM and then Random Forest in same Manner**\n\n2. from Random forest we can get which features are more important"},{"metadata":{"_cell_guid":"45540e79-c2e0-4b52-6b55-5e3c8bb2f397","_uuid":"1a42987610d8c15e9dcc8cf6cc048a122413bbe6"},"cell_type":"markdown","source":"*SVM with Undersample data*"},{"metadata":{"_cell_guid":"ff78a5e3-dd54-5626-b505-9022a21690c5","_uuid":"bac2472f425f47d862adefad9c79df6cedc4334d","trusted":false},"cell_type":"code","source":"for i in range(1,4):\n    print(\"the undersample data for {} proportion\".format(i))\n    print()\n    Undersample_data = undersample(normal_indices,fraud_indices,i)\n    print(\"------------------------------------------------------------\")\n    print()\n    print(\"the model classification for {} proportion\".format(i))\n    print()\n    undersample_features_train,undersample_features_test,undersample_labels_train,undersample_labels_test=data_prepration(Undersample_data)\n    print()\n    clf= SVC()# here we are just changing classifier\n    model(clf,undersample_features_train,undersample_features_test,undersample_labels_train,undersample_labels_test)\n    print(\"________________________________________________________________________________________________________\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f479f7b-48de-778a-bae3-85fafa34f5a1","_uuid":"2cbaeb113d513ca394d055edc6d93842b86026f1"},"cell_type":"markdown","source":"1. Here recall and precision are approximately equal to Logistic Regression \n\n2. Lets try for whole data"},{"metadata":{"_cell_guid":"421e574f-9fc1-af06-b54d-ce2dd2d6b51d","_uuid":"d0ee8f4bce91dd013313477503a2aee1c4d199ec","trusted":false},"cell_type":"code","source":"#let us train this model using undersample data and test for the whole data test set \nfor i in range(1,4):\n    print(\"the undersample data for {} proportion\".format(i))\n    print()\n    Undersample_data = undersample(normal_indices,fraud_indices,i)\n    print(\"------------------------------------------------------------\")\n    print()\n    print(\"the model classification for {} proportion\".format(i))\n    print()\n    undersample_features_train,undersample_features_test,undersample_labels_train,undersample_labels_test=data_prepration(Undersample_data)\n    data_features_train,data_features_test,data_labels_train,data_labels_test=data_prepration(data) \n    #the partion for whole data\n    print()\n    clf=SVC()\n    model(clf,undersample_features_train,data_features_test,undersample_labels_train,data_labels_test)\n    # here training for the undersample data but tatsing for whole data\n    print(\"_________________________________________________________________________________________\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"35809c9d-f0ed-bffb-5a5d-4276b8baf8e8","_uuid":"4458f986ff96e320f83268d9e2c9b61be2b008ad"},"cell_type":"markdown","source":"1. A better recall but precision is not improving much \n\n2 .so to improve precision we must have to tune the  hyper parameter of these models\n\n3 That I will do in next version \n\n4 For now lets try with my favorite Random Forest classifier "},{"metadata":{"_cell_guid":"3a5628e6-25ce-f859-129e-08c2af9f1c91","_uuid":"462e60e7fc83d145ff97188b66b5e99a9abcf1f6","trusted":false},"cell_type":"code","source":"# Random Forest Classifier with undersample data only\nfor i in range(1,4):\n    print(\"the undersample data for {} proportion\".format(i))\n    print()\n    Undersample_data = undersample(normal_indices,fraud_indices,i)\n    print(\"------------------------------------------------------------\")\n    print()\n    print(\"the model classification for {} proportion\".format(i))\n    print()\n    undersample_features_train,undersample_features_test,undersample_labels_train,undersample_labels_test=data_prepration(Undersample_data)\n    print()\n    clf= RandomForestClassifier(n_estimators=100)# here we are just changing classifier\n    model(clf,undersample_features_train,undersample_features_test,undersample_labels_train,undersample_labels_test)\n    print(\"________________________________________________________________________________________________________\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15036191-41bd-e56f-027b-de4f22814380","_uuid":"b3ae6a544a68c9f8bea85b717eb80b4474820c70","trusted":false},"cell_type":"code","source":"#let us train this model using undersample data and test for the whole data test set \nfor i in range(1,4):\n    print(\"the undersample data for {} proportion\".format(i))\n    print()\n    Undersample_data = undersample(normal_indices,fraud_indices,i)\n    print(\"------------------------------------------------------------\")\n    print()\n    print(\"the model classification for {} proportion\".format(i))\n    print()\n    undersample_features_train,undersample_features_test,undersample_labels_train,undersample_labels_test=data_prepration(Undersample_data)\n    data_features_train,data_features_test,data_labels_train,data_labels_test=data_prepration(data) \n    #the partion for whole data\n    print()\n    clf=RandomForestClassifier(n_estimators=100)\n    model(clf,undersample_features_train,data_features_test,undersample_labels_train,data_labels_test)\n    # here training for the undersample data but tatsing for whole data\n    print(\"_________________________________________________________________________________________\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"281c3b33-2a4f-fa5b-b683-bcc9b8620a4b","_uuid":"3d69778644607039c14dc53ebd1d6b1628d06c2a"},"cell_type":"markdown","source":"1. for the third proportion the precision is 0.33 which is better than others\n\n2. Lets try to get only import features using  Random Forest Classifier \n\n3. After it i will do analysis only for one portion that is 0.5 %"},{"metadata":{"_cell_guid":"4d65aa61-7029-01cc-f1ff-90b919e86375","_uuid":"2c4e966257e89b3aa92618b0eace3d044c5154eb","trusted":false},"cell_type":"code","source":"featimp = pd.Series(clf.feature_importances_,index=data_features_train.columns).sort_values(ascending=False)\nprint(featimp) # this is the property of Random Forest classifier that it provide us the importance \n# of the features use","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2bd6d67-b2d6-c115-0fa4-dd79467b3274","_uuid":"0b5a108be9d96451816adee2c1fda552e20b69c1"},"cell_type":"markdown","source":"1.  we can see this is showing the importance of feature for the making decision \n\n2. V14 is having a very good importance compare to other features\n\n3. Lets use only top 5 (V14,V10,V12,V17,V4) feature to predict using Random forest classifier only for 0.5 % "},{"metadata":{"_cell_guid":"609033b6-4bb4-e70c-98cc-308e66146c9b","_uuid":"b3b156f9363500009c10d16caae21c944483d073","trusted":false},"cell_type":"code","source":"# make a new data with only class and V14\ndata1=data[[\"V14\",\"V10\",\"V12\",\"V17\",\"V4\",\"Class\"]]\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73f9712b-81d4-beed-611e-519d885e6519","_uuid":"17a32c641355d6158f637b1bbd941e42952c8c0b","trusted":false},"cell_type":"code","source":"Undersample_data1 = undersample(normal_indices,fraud_indices,1)\n#only for 50 % proportion it means normal transaction and fraud transaction are equal so passing \nUndersample_data1_features_train,Undersample_data1_features_test,Undersample_data1_labels_train,Undersample_data1_labels_test = data_prepration(Undersample_data1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c4be1eb4-38d4-4aad-7ed6-ecc0a6965c8a","_uuid":"6a661879c5ae948844f2bbca7adf5364b4e03c43","trusted":false},"cell_type":"code","source":"clf= RandomForestClassifier(n_estimators=100)\nmodel(clf,Undersample_data1_features_train,Undersample_data1_features_test,Undersample_data1_labels_train,Undersample_data1_labels_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"651a09d1-bc75-e3bf-7c7f-743b832bbd05","_uuid":"c7ca1538d24e1246c7713468f6e00c4effd06cc7"},"cell_type":"markdown","source":"###Over Sampling"},{"metadata":{"_cell_guid":"d81c2e60-996a-cfe1-29eb-99f389184875","_uuid":"0b43088fda6f857859fcc45a98fda8bfedabd5f0"},"cell_type":"markdown","source":" 1. In my previous version I got the 100 recall and 98 % precision by using Random forest with the over sampled data but in real it was due to over fitting because i was taking whole fraud data and was training for that and I was doing the testing on the same data.\n\n 2. Please find link of previous version for more understanding [Link](https://www.kaggle.com/gargmanish/d/dalpozz/creditcardfraud/fraud-detection-100-recall-98-precision/run/1033018)\n\n3. Thanks to Mr. Dominik Stuerzer for help "},{"metadata":{"_cell_guid":"790cdeb8-dcb6-42f2-7d1c-88745940a019","_uuid":"5fe5617149e43383fec55b92d3ebe9238cf0b51f","trusted":false},"cell_type":"code","source":"# now we will divied our data sets into two part and we will train and test and will oversample the train data and predict for test data\n# lets import data again\ndata = pd.read_csv(\"../input/creditcard.csv\",header = 0)\nprint(\"length of training data\",len(data))\nprint(\"length of normal data\",len(data[data[\"Class\"]==0]))\nprint(\"length of fraud  data\",len(data[data[\"Class\"]==1]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"18eb81f6-3851-3e3f-bb1b-b50b0d3b34f3","_uuid":"306697242e129dd1bbcb42a7dd692419e37c4f3f","trusted":false},"cell_type":"code","source":"data_train_X,data_test_X,data_train_y,data_test_y=data_prepration(data)\ndata_train_X.columns\ndata_train_y.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1507f0e1-ef07-8565-da7a-65e22b5b87ee","_uuid":"16987b01db61deb2b9923c65469e6bbe1d29c80c","trusted":false},"cell_type":"code","source":"# ok Now we have a traing data\ndata_train_X[\"Class\"]= data_train_y[\"Class\"] # combining class with original data\ndata_train = data_train_X.copy() # for naming conevntion\nprint(\"length of training data\",len(data_train))\n# Now make data set of normal transction from train data\nnormal_data = data_train[data_train[\"Class\"]==0]\nprint(\"length of normal data\",len(normal_data))\nfraud_data = data_train[data_train[\"Class\"]==1]\nprint(\"length of fraud data\",len(fraud_data))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29c2774c-9633-c9a1-1d71-c86eb92506f2","_uuid":"36e4f5f2d26acb11427217644d65adcd8871e11e","trusted":false},"cell_type":"code","source":"# Now start oversamoling of training data \n# means we will duplicate many times the value of fraud data\nfor i in range (365): # the number is choosen by myself on basis of nnumber of fraud transaction\n    normal_data= normal_data.append(fraud_data)\nos_data = normal_data.copy() \nprint(\"length of oversampled data is \",len(os_data))\nprint(\"Number of normal transcation in oversampled data\",len(os_data[os_data[\"Class\"]==0]))\nprint(\"No.of fraud transcation\",len(os_data[os_data[\"Class\"]==1]))\nprint(\"Proportion of Normal data in oversampled data is \",len(os_data[os_data[\"Class\"]==0])/len(os_data))\nprint(\"Proportion of fraud data in oversampled data is \",len(os_data[os_data[\"Class\"]==1])/len(os_data))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7db27f2c-d6fb-d903-932f-2db782c8e43f","_uuid":"8875644374d5c5974f26d765bd143f019b9d3192"},"cell_type":"markdown","source":" 1. The proportion now becomes the 60 % and 40 % that is good now"},{"metadata":{"_cell_guid":"2d1ab58d-f74f-93da-a441-11f9e130b600","_uuid":"1a604232f026408574e179c07c34c21c40c95643","trusted":false},"cell_type":"code","source":"# before applying any model standerdize our data amount \nos_data[\"Normalized Amount\"] = StandardScaler().fit_transform(os_data['Amount'].reshape(-1, 1))\nos_data.drop([\"Time\",\"Amount\"],axis=1,inplace=True)\nos_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ba7423e-7d1f-b1cc-81f9-a5c18e35fcfe","_uuid":"cf944e0e47abdaf871d713512770dc21c066f6bd","trusted":false},"cell_type":"code","source":"# Now use this oversampled data for trainig the model and predict value for the test data that we created before\n# now let us try within the the oversampled data itself\n# for that we need to split our oversampled data into train and test\n# so call our function data Prepration with oversampled data\nos_train_X,os_test_X,os_train_y,os_test_y=data_prepration(os_data)\nclf= RandomForestClassifier(n_estimators=100)\nmodel(clf,os_train_X,os_test_X,os_train_y,os_test_y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5d82cb81-3be2-0c73-74e6-0ee28311398f","_uuid":"dc6273535dddb5b508333d5636acb48e3ecced6b"},"cell_type":"markdown","source":"**Observations**\n\n 1. As it have too many sample of  same fraud data so may be the all which are present in train data are present in test data also so we can say it is over fitting \n 2. So lets try with test data that one which we created in starting of oversampling segment no fraud transaction from that data have been repeated here\n 3. Lets try\n\n "},{"metadata":{"_cell_guid":"afbe419c-4309-51d9-e860-6730afbdae95","_uuid":"42f01931a1727a38124ec3f6b5078309b53ffcc3","trusted":false},"cell_type":"code","source":"# now take all over sampled data as trainging and test it for test data\nos_data_X = os_data.ix[:,os_data.columns != \"Class\"]\nos_data_y = os_data.ix[:,os_data.columns == \"Class\"]\n#for that we have to standrdize the normal amount and drop the time from it\ndata_test_X[\"Normalized Amount\"] = StandardScaler().fit_transform(data_test_X['Amount'].reshape(-1, 1))\ndata_test_X.drop([\"Time\",\"Amount\"],axis=1,inplace=True)\ndata_test_X.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"caea4e43-aa7d-c1f6-04a0-be33c13d503e","_uuid":"57abccd8cf5ea4f2f3dcc182fa45edc1c03361bc","trusted":false},"cell_type":"code","source":"# now use it for modeling\nclf= RandomForestClassifier(n_estimators=100)\nmodel(clf,os_data_X,data_test_X,os_data_y,data_test_y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f625858-5e45-49c3-bb44-8e6c66daffd7","_uuid":"76f72956e379fc0da2abaef586ece89822dcda04"},"cell_type":"markdown","source":"**Observations**\n\n 1. Now here we can see recall decrease to only 83 % which is not bad  but not good also\n 2. The precision is 0.93 which is good \n 3. from these observation we can say that the oversampling is better than the Under sampling because on Under sampling we were loosing a large amount of data or we can say a good amount of information so why the there precision was very low "},{"metadata":{"_cell_guid":"5c41995b-c8fa-554c-92a3-b2076522917c","_uuid":"93c3a0fabf9b2bb53241ad2b9faa5b3a3704dc4c"},"cell_type":"markdown","source":"###SMOTE"},{"metadata":{"_cell_guid":"54995fac-4954-1b8c-948d-466017159b0d","_uuid":"155575a4a590186d4d38eaa192754f8c063691a8","trusted":false},"cell_type":"code","source":"# Lets Use SMOTE for Sampling\n# As I mentioned it is also a type of oversampling but in this the data is not replicated but they are created \n#lets start with importing libraries\nfrom imblearn.over_sampling import SMOTE\ndata = pd.read_csv('../input/creditcard.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9cc6383f-5f47-511a-ef25-8da061390d34","_uuid":"418d2b4129ecced4fc50451fbf8a2281a0eaf5d2","trusted":false},"cell_type":"code","source":"os = SMOTE(random_state=0) #   We are using SMOTE as the function for oversampling\n# now we can devided our data into training and test data\n# Call our method data prepration on our dataset\ndata_train_X,data_test_X,data_train_y,data_test_y=data_prepration(data)\ncolumns = data_train_X.columns\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"54312e1d-58d8-184f-8596-48a8084a0ee7","_uuid":"1fb67be1f6fa75ef9524c1ccc040071f745111e6","trusted":false},"cell_type":"code","source":"# now use SMOTE to oversample our train data which have features data_train_X and labels in data_train_y\nos_data_X,os_data_y=os.fit_sample(data_train_X,data_train_y)\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=[\"Class\"])\n# we can Check the numbers of our data\nprint(\"length of oversampled data is \",len(os_data_X))\nprint(\"Number of normal transcation in oversampled data\",len(os_data_y[os_data_y[\"Class\"]==0]))\nprint(\"No.of fraud transcation\",len(os_data_y[os_data_y[\"Class\"]==1]))\nprint(\"Proportion of Normal data in oversampled data is \",len(os_data_y[os_data_y[\"Class\"]==0])/len(os_data_X))\nprint(\"Proportion of fraud data in oversampled data is \",len(os_data_y[os_data_y[\"Class\"]==1])/len(os_data_X))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59f55f2a-cb6a-47d9-7e8a-67a7af8e89a5","_uuid":"a51cf3c1358febfdea48af33abd530cb9d6e1781"},"cell_type":"markdown","source":" 1. By using Smote we are getting a 50 - 50 each\n\n 2. No need of checking here in over sampled data itself from previous we know it will be overfitting\n\n 3. let us check with the test data direct\n"},{"metadata":{"_cell_guid":"4f9b50e0-433d-607b-e082-84d88467b705","_uuid":"9db242e9334932430d07ed493e066c590dbe6c1d","trusted":false},"cell_type":"code","source":"# Let us first do our amount normalised and other that we are doing above\nos_data_X[\"Normalized Amount\"] = StandardScaler().fit_transform(os_data_X['Amount'].reshape(-1, 1))\nos_data_X.drop([\"Time\",\"Amount\"],axis=1,inplace=True)\ndata_test_X[\"Normalized Amount\"] = StandardScaler().fit_transform(data_test_X['Amount'].reshape(-1, 1))\ndata_test_X.drop([\"Time\",\"Amount\"],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d15d05b6-18d4-85cd-2488-527a8333e203","_uuid":"3f9e3bce91c1e41f2863f1f1bd50874e8ebccb53","trusted":false},"cell_type":"code","source":"# Now start modeling\nclf= RandomForestClassifier(n_estimators=100)\n# train data using oversampled data and predict for the test data\nmodel(clf,os_data_X,data_test_X,os_data_y,data_test_y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"953729c6-5ef8-4a49-18ca-7444a5df6ba0","_uuid":"fdd12a00d15dfb4708e4d81a671e8f019c25ff11"},"cell_type":"markdown","source":"**observation **\n\n 1. The recall is nearby the previous one done by over sampling \n 2. The precision decrease in this case"},{"metadata":{"_cell_guid":"af98be8c-a128-5300-1317-22069454181e","_uuid":"518a22e92e347d846a960a08f5d1e26da16ccae8","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}