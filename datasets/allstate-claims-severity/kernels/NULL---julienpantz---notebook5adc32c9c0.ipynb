{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "13b51871-7958-bf62-6af8-1d71dee8207a"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "75f4b07e-231d-7fe4-c51a-7c53004f92e5"
      },
      "source": [
        "#Loss prediction\n",
        "\n",
        "The problem is to regress categorical and continuous variables into loss predictions. The performance measure is the mean absolute error:\n",
        "$ MAE = \\sum | y^{true}_i - y^{predict}_i |$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cfa9c0f2-8d02-8143-cd36-8f3282b60444"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_validation import KFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import fbeta_score, make_scorer\n",
        "from sklearn import tree\n",
        "from sklearn.cross_validation import cross_val_score\n",
        "import time\n",
        "from numpy.random import RandomState\n",
        "prng = RandomState(1234567890)\n",
        "import sklearn ; print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "13efa161-a37c-9760-7d4b-fa4c7082315c"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5cba53f1-d50f-ab87-9879-d5da1414ff3d"
      },
      "source": [
        "#Preprocessing\n",
        "The data is clean.\n",
        "We need to encode the categorical labels (preprocess fct). We choose to ignore the categorical values only present in the training or testing set. \n",
        "We need to transform the data such that mean and median are close such that we do not have to use MAE in the algos. Solving MAE is too slow because the median needs sorting which is $O(NlnN)$ while the mean is a sum so done in $O(N)$.\n",
        "\n",
        "Follow https://www.kaggle.com/iglovikov/allstate-claims-severity/xgb-1114/code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "90c72a0f-c943-5c37-4de0-9a6281cef34c"
      },
      "outputs": [],
      "source": [
        "test['loss'] = np.nan #Add a Loss column with NaN to test DF\n",
        "joined = pd.concat([train, test])\n",
        "\n",
        "for column in list(train.select_dtypes(include=['object']).columns):\n",
        "    # \n",
        "    if train[column].nunique() != test[column].nunique(): # could fail in theory\n",
        "        set_train = set(train[column].unique()) # Do we need unique since we have set\n",
        "        set_test = set(test[column].unique())\n",
        "        remove_train = set_train - set_test\n",
        "        remove_test = set_test - set_train\n",
        "        remove = remove_train.union(remove_test)\n",
        "        def filter_cat(x):\n",
        "            if x in remove:\n",
        "                return np.nan\n",
        "            return x\n",
        "\n",
        "        joined[column] = joined[column].apply(lambda x: filter_cat(x), 1)\n",
        "        \n",
        "    #pd.factorize encode the (sorted) values\n",
        "    joined[column] = pd.factorize(joined[column].values, sort=True)[0]\n",
        "    \n",
        "#HACK We set test['loss'] = NaN to recognize it \n",
        "train = joined[joined['loss'].notnull()]\n",
        "test = joined[joined['loss'].isnull()]\n",
        "y = train['loss']\n",
        "train = train.drop(['id','loss'],1)\n",
        "test = test.drop(['id','loss'],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf999213-bc1b-1bbf-6f68-a6b3372704f9"
      },
      "outputs": [],
      "source": [
        "def transform(y,is_inverse=None):\n",
        "    shift = 200\n",
        "    if is_inverse:\n",
        "        return np.exp(y)-shift\n",
        "    return np.log(y+shift)\n",
        "\n",
        "def scorer(x,y):\n",
        "    return mean_absolute_error(transform(x,True),transform(y,True))\n",
        "\n",
        "custom_scorer = make_scorer(scorer,greater_is_better=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bdd01747-d787-6e08-10b3-f858cffd4c73"
      },
      "outputs": [],
      "source": [
        "print('mean',np.mean(y),'median',np.median(y))\n",
        "#print(train.head(5))\n",
        "\n",
        "X = train.values\n",
        "X_test = test.values\n",
        "fy = transform(y)\n",
        "print('mean',transform(np.mean(fy),True),'median',transform(np.median(fy),True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6cefd875-3c82-b5b2-7db0-cd80015ec2bb"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b72121b3-7601-e4cf-15b6-746032aed9a0"
      },
      "source": [
        "##Regression Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3acfc190-4fa2-a2b6-7f4c-51445fd3ab97"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    t0 = time.time()\n",
        "    reg = tree.DecisionTreeRegressor(max_depth=9,min_samples_split=2)\n",
        "    scores = cross_val_score(reg, X,fy, cv=3,scoring=custom_scorer)\n",
        "    d = time.time()-t0\n",
        "    print(scores.mean(),d)\n",
        "\n",
        "    #reg = tree.DecisionTreeRegressor()    \n",
        "    #reg = reg.fit(x,fy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d90f23c3-637d-8006-e149-5109d14d736a"
      },
      "source": [
        "##Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1dee251a-0ba5-81b1-f2ad-4b43637de830"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    params = [5,10,25,50,100]\n",
        "    params = [5]\n",
        "    for param in params:\n",
        "        t0 = time.time()\n",
        "        forest = RandomForestRegressor(n_estimators = param,criterion='mse',n_jobs=-1,random_state=prng)\n",
        "        scores = cross_val_score(forest, X,fy, cv=3,scoring=custom_scorer)\n",
        "        d = time.time()-t0\n",
        "        print(param,scores.mean(),d)\n",
        "\n",
        "    #5 -1319.6123701 37.71500015258789\n",
        "    #10 -1266.15301356 102.43400001525879\n",
        "    #25 -1231.19439691 213.24000000953674\n",
        "    #50 -1219.07043097 292.1300001144409\n",
        "    #100 -1212.35944528 633.6279997825623\n",
        "\n",
        "if False:\n",
        "    forest = RandomForestRegressor(n_estimators = 100,criterion='mse',n_jobs=-1,random_state=prng)\n",
        "    forest = forest.fit(X,fy)\n",
        "    feature_impportances =  sorted(zip(forest.feature_importances_,train.columns.values),reverse=True)\n",
        "    #Print the top features\n",
        "    print(feature_impportances)\n",
        "    for a,b in feature_impportances:\n",
        "        print(a,b)\n",
        "features = ['cat80','cont14','cat101','cont7','cont2','cat79','cat103','cat100','cat12','cat111',\n",
        "            'cat112','cont8','cont5','cont3','cat81','cont4','cont6','cat53','cont1','cat110',\n",
        "            'cont13','cont12','cont10','cont11','cat57','cat1','cont9','cat114','cat113']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8da39694-4467-a779-5a2e-6df479a78805"
      },
      "outputs": [],
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6466a48-46b2-41ed-860a-40296f9b9e6a"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "def evalerror(preds, dtrain):\n",
        "    labels = dtrain.get_label()\n",
        "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n",
        "\n",
        "RANDOM_STATE = 1234\n",
        "\n",
        "params = {\n",
        "        'min_child_weight': 1,\n",
        "        'eta': 0.01,\n",
        "        'colsample_bytree': 0.5,\n",
        "        'max_depth': 12,\n",
        "        'subsample': 0.8,\n",
        "        'alpha': 1,\n",
        "        'gamma': 1,\n",
        "        'silent': 1,\n",
        "        'verbose_eval': True,\n",
        "        'seed': RANDOM_STATE\n",
        "    }\n",
        "\n",
        "xgtrain = xgb.DMatrix(X, label=y)\n",
        "xgtest = xgb.DMatrix(X_test)\n",
        "\n",
        "run_cv=False\n",
        "run_model=False\n",
        "\n",
        "if run_cv:\n",
        "    cv = xgb.cv(params, xgtrain, num_boost_round=10, nfold=5, stratified=False,\n",
        "         early_stopping_rounds=50, verbose_eval=1, show_stdv=True, feval=evalerror, maximize=False)\n",
        "\n",
        "if run_model:\n",
        "    model = xgb.train(params, xgtrain, int(2012 / 0.9), feval=evalerror)\n",
        "    prediction = np.exp(model.predict(xgtest)) - shift"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}