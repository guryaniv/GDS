{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0a7e9a83-c0f4-7cf1-82c1-eb46822278b4"
      },
      "source": [
        "**Credit card fraud detection using isolation forest algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "df0b2092-9358-4366-5b27-5735894e6490"
      },
      "source": [
        "The data set provided here is related to credit card transactions. The objective of the challenge is to maximinise the anamolous transaction detection and at the same time reduce the false negatives (Falsely detecting a normal observation as anamolous).  There are three fundamental approaches to detect anomalies; they are isolation, density and distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12502234-81ca-88c6-234a-97503034737e"
      },
      "source": [
        "\u2022\tMost existing anomaly detection approaches, including classification-based methods [Abe et al. 2006; Shi and Horvath 2006], Replicator Neural Network (RNN) [Williams et al. 2002], one-class SVM [Tax and Duin 2004] and clustering-based methods [He et al. 2003],\n",
        "\n",
        "\u2022\tTheir anomaly detection abilities are usually a \u2018side-effect\u2019 or by-product of an algorithm originally designed for a purpose other than anomaly detection (such as classification or clustering). This leads to two major drawbacks: (i) these approaches are not optimized to detect anomalies\u2014as a consequence, these approaches often under-perform resulting in too many false alarms (having normal instances identified as anomalies) or too few anomalies being detected; (ii) many existing methods are constrained to low dimensional data and small data size because of the legacy of their original algorithms.\n",
        "\n",
        "\u2022\tAnomalies are \u2018few and different\u2019, which make them more susceptible to a mechanism we called Isolation. Anomalies are more likely to be isolated closer to the root of an iTree; whereas normal points are more likely to be isolated at the deeper end of an iTree.\n",
        "\n",
        "\u2022\tIn isoforest algorithm, there are two training parameters and one evaluation parameter in this method: the training parameters are the number of trees to build and subsampling size; the evaluation parameter is the tree height limit during evaluation.\n",
        "\n",
        "\u2022\tAdvantages: Exploit subsampling, do not use distance or density function, therefore reduces computational cost, linear time complexity, feasible with large data and many features. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "99c2a8bf-210b-4b0d-4ecc-38e2481709f2"
      },
      "source": [
        "**I will be using isolation forest algorithm to score the observation and to detect anamoly**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2349d407-61e9-5d67-bb8d-f0eb3cb9a068"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "cc =  pd.read_csv(\"../input/creditcard.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "00f7b5d9-ba74-5127-8bc9-eac6f0848424"
      },
      "source": [
        "Check the column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aff366ef-684e-db65-ae39-627cd027eeab"
      },
      "outputs": [],
      "source": [
        "cc.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3f59b129-42f9-1df9-70bc-90e1aae90142"
      },
      "source": [
        "Since isoforest is an unsupervised learning algorithm, it will only use feature data. 'cc_train' dataframe is created with only feature data. The label 'class' is dropped from the cc_train dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "00dd7ff1-2a50-8d9c-758b-b0b7084ee0c4"
      },
      "outputs": [],
      "source": [
        "cc_train= cc.drop('Class', 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "68f8839f-d75b-e09a-1b63-c5883a4cd27c"
      },
      "source": [
        "There are two training parameters in this method: the training parameters are the number of trees to build and subsampling size **(Liu and Ting, 2008)**. The are two hyperparameters which can be tunned to ajudt the algorithm. It must be noted that number of tree converges after a certain point. The default vale is 256 for sub sample size and 100 trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "480ad8c3-7425-a5cf-2b05-8a9d8912e308"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "clf = IsolationForest(n_estimators=100, max_samples=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d1bf7108-abfc-1cc4-0d7f-5e6203dbff77"
      },
      "outputs": [],
      "source": [
        "#Train the model with the data.\n",
        "clf.fit(cc_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "384a6d73-5f03-c025-bb48-02cda73d20b8"
      },
      "source": [
        "Random and recursive partition of data is carried out, which is represented as a tree (random forest). The end of the tree is reached once the recursive partition of data is finished. It is expected that the distance taken to reach the outlier is far less than that for the normal data. The distance of the path is averaged and normalised to calculate the anomaly score. Anomaly score of 1 is considered as an outlier, values close to 0 is considered normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c04f136-7464-022c-6cc3-6241b6a44237"
      },
      "outputs": [],
      "source": [
        "# The Anomaly scores are calclated for each observation and stored in 'scores_pred'\n",
        "scores_pred = clf.decision_function(cc_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "caa63c2d-22f2-156c-7d29-eff2bd2ccd53"
      },
      "outputs": [],
      "source": [
        "#verify the length of scores and number of obersvations.\n",
        "print(len(scores_pred))\n",
        "print(len(cc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fec9b85c-80d2-027b-25af-d38fcbfc7afb"
      },
      "outputs": [],
      "source": [
        "# scores_pred is added to the cc dataframe \n",
        "cc['scores']= scores_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9eafdc99-7752-ad2f-de3d-1ca02b433b3d"
      },
      "outputs": [],
      "source": [
        "#I oberved an conflict with the name 'class'. Therefore, I have changed the name from class to category\n",
        "cc= cc.rename(columns={'Class': 'Category'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e7352a23-7248-30b8-3935-2d30e50b4e97"
      },
      "outputs": [],
      "source": [
        "# Based on (Liu and Ting, 2008), anomalous observation is scored close to 1 \n",
        "# and non anamolous observations are scored close to zero. \n",
        "# I have written a simple loop that will count the number of observation that has score more than 0.5 and is actually anomalous.\n",
        "counter =0\n",
        "for n in range(len(cc)):\n",
        "    if (cc['Category'][n]== 1 and cc['scores'][n] >=0.5):\n",
        "        counter= counter+1\n",
        "print (counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "85f1cc23-82e9-73d9-50dd-4ae7d17e863e"
      },
      "source": [
        "Clearly, there is something worg with the compatiblity of the algorithm. Lest investigate how the algorithm has scored each observation. Since we have the actual label, it is easy to compare the isolation score to the label. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "747f7949-f4a8-3232-b494-7cda00967fe5"
      },
      "outputs": [],
      "source": [
        "# For convinience, divide the dataframe cc based on two labels. \n",
        "avg_count_0 = cc.loc[cc.Category==0]    #Data frame with normal observation\n",
        "avg_count_1 = cc.loc[cc.Category==1]    #Data frame with anomalous observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "75601af8-70e0-a655-974f-17b79ccb50f6"
      },
      "outputs": [],
      "source": [
        "#Plot the combined distribution of the scores \n",
        "%matplotlib inline\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab.inline\n",
        "normal = plt.hist(avg_count_0.scores, 50,)\n",
        "plt.xlabel('Score distribution')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(\"Distribution of isoforest score for normal observation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e7264d87-6bfc-d0cc-6a2e-2bf500ed4dc0"
      },
      "outputs": [],
      "source": [
        "#Plot the combined distribution of the scores \n",
        "%matplotlib inline\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab.inline\n",
        "normal = plt.hist(avg_count_1.scores, 50,)\n",
        "plt.xlabel('Score distribution')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(\"Distribution of isoforest score for anomalous observation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "98bc3779-1ca2-b212-fc5b-db636faab8a0"
      },
      "source": [
        "The above graph shows that the distribustion of the score assigned to anomalous and nonanomlous observations are not even. Most of the normal observations are scored more than +0.05 with a mean centered around 0.1. Whereas, distribution of the anomalous observations are spread from -0.1 to +0.5. Inorder to visualise it properly, oth the histogram are drawn separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dc526b76-cfbb-2b0d-d360-a0ecb5e838a0"
      },
      "source": [
        "Liu and Ting (2008) suggests that for the normal observations, algorithm has a score of close to zero and anomalous observation has a score between 0.5 and 1. In this case, it is observed that the pattern of the scores are not consistent with the literature. The algorithm is calculating lower scores for anomalous observations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "608f618a-90bc-3587-7608-745a54e34251"
      },
      "source": [
        "Summary:\n",
        "\n",
        " 1. It was observed that isoforest can be used as an unsupervised learning for anomaly detection.\n",
        " 2. In this particular case, isoforest is not suitable because it tends to score less than 0.05 for anomalous observation. This is against what is proposed by (Liu and Ting, 2008).\n",
        " 3. This score can be used to carry out smote analysis. We can eliminate observations with score greater than 0.05 and increase the proposition of the minority class, significantly.\n",
        " 4. I will be soon submitting another kernel to test once class SVM for anomaly detection."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}