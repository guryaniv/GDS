{"cells":[{"metadata":{"_uuid":"9d9679936b612872e0ffaa3a03a279b102672f6b"},"cell_type":"markdown","source":"# Pretrained CNNs on pet images\n\nIn this notebook we will try to make use of Keras pretrained CNNs for pet AdoptionSpeed classification.\nWhile score of the network itself probably won't be very good, it can be used as input for LGBM model or for additional feature extraction when calibrated on this dataset.\n\nWhat is more, notebook demonstrates usage of keras.utils.Sequence-based data generator, which can be easily modified to account for your needs. For example, here we add data augmentations based on `albumentations` package."},{"metadata":{"trusted":true,"_uuid":"38010cd12248a221a77564d1cc83744f38b0f7dd"},"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pprint\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom PIL import Image\n\n%matplotlib inline\n\npd.options.display.max_rows = 128\npd.options.display.max_columns = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01ab91c32a24cd2f020d5033ab018dc7c8f7ab7f"},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12, 9)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d680f5bc1379d25f95e8957786c2362f300d404a"},"cell_type":"markdown","source":"### load core DFs (train and test):"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"fb46765c63f644fecba7322a8375f3a4ee2943de"},"cell_type":"code","source":"train = pd.read_csv('../input/train/train.csv')\ntest = pd.read_csv('../input/test/test.csv')\nsample_submission = pd.read_csv('../input/test/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97727b3bed2f4c2602a0fe60674fa4a5b2806c60"},"cell_type":"markdown","source":"### load mapping dictionaries:"},{"metadata":{"trusted":true,"_uuid":"c6c3911532278a2d75b96808558b890c475f1a22"},"cell_type":"code","source":"labels_breed = pd.read_csv('../input/breed_labels.csv')\nlabels_state = pd.read_csv('../input/color_labels.csv')\nlabels_color = pd.read_csv('../input/state_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"267475330ad7f389bfb368e830e61b2aa8626ba6"},"cell_type":"code","source":"# Train files:\ntrain_image_files = sorted(glob.glob('../input/train_images/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('../input/train_metadata/*.json'))\ntrain_sentiment_files = sorted(glob.glob('../input/train_sentiment/*.json'))\n\nprint('num of train images files: {}'.format(len(train_image_files)))\nprint('num of train metadata files: {}'.format(len(train_metadata_files)))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n\n# Test files:\ntest_image_files = sorted(glob.glob('../input/test_images/*.jpg'))\ntest_metadata_files = sorted(glob.glob('../input/test_metadata/*.json'))\ntest_sentiment_files = sorted(glob.glob('../input/test_sentiment/*.json'))\n\nprint('num of test images files: {}'.format(len(test_image_files)))\nprint('num of test metadata files: {}'.format(len(test_metadata_files)))\nprint('num of test sentiment files: {}'.format(len(test_sentiment_files)))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"7a322b35e645cd0013190fb04e442939dfeb6ff2"},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\n\nprint('train:')\n# Images:\ntrain_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\nprint(len(train_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images / train_df_ids.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"579f85de2933475c3476c9f0a6f938729dbf18c9"},"cell_type":"code","source":"print('test:')\n# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\nprint(len(test_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images / test_df_ids.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64cbd1f67510132f48813d86bfcc453e7c6cc261"},"cell_type":"code","source":"import albumentations as A\nimport numpy as np\nimport keras\n\nfrom keras import optimizers\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.applications import mobilenet\nfrom keras.applications import mobilenet_v2\n\nfrom keras.callbacks import *\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import *\nfrom keras.models import Model, load_model, save_model\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Keras Data Generator for loading data from disk\n# Based on: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\nclass PetfinderDataGenerator(keras.utils.Sequence):\n\n    def __init__(self, \n                 img_list, \n                 labels_list,\n                 parser,\n                 batch_size=16,\n                 image_size=(224, 224),\n                 n_channels=3,\n                 n_classes=5,\n                 shuffle=True):\n\n        # List of images used for data loading\n        self.img_list = img_list\n        # List of corresponding labels\n        self.labels_list = labels_list\n        # Parser to use for image loading\n        self.parser = parser\n        # Batch size\n        self.batch_size = batch_size\n        # Size of image to be fed to the model\n        self.image_size = image_size\n        # Image channels\n        self.n_channels = n_channels\n        # Target number of classes\n        self.n_classes = n_classes\n        # Whether to shuffle image_list\n        self.shuffle = shuffle\n\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.img_list) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[\n            index * self.batch_size:(index + 1) * self.batch_size]\n\n        # Find list of IDs\n        img_list_temp = [self.img_list[k] for k in indexes]\n        labels_list_temp = [self.labels_list[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(img_list_temp, labels_list_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.img_list))\n        if self.shuffle is True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, img_list_temp, label_list_temp):\n        'Generates data containing batch_size samples'\n        \n        # Initialization\n        X = np.empty((self.batch_size, *self.image_size, self.n_channels), dtype=np.float32)\n        y = np.empty((self.batch_size, self.n_classes), dtype=np.float32)\n\n        # Generate data\n        for i in range(len(img_list_temp)):\n\n            img_temp = self.parser.load_image(img_list_temp[i])\n            label_temp = label_list_temp[i]\n\n            X[i], y[i] = img_temp, label_temp\n\n        return X, y\n    \n\n# Helper class for image loading and augmentation\nclass PetfinderImageParser(object):\n    \n    def __init__(self,\n                 preproc_func,\n                 image_size=(224, 224),\n                 transform=False,\n                 resize=False, \n                 debug=False):\n        \n        self.image_size = image_size\n        self.preproc_func = preproc_func\n        self.transform = transform\n        self.resize = resize\n        self.debug = debug\n    \n        \n    def load_image(self, img_filename, preprocess=True):\n        \n        image = cv2.imread(img_filename)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            image = self.transform(image=image)['image']\n        \n        if image.shape[:2] != self.image_size and self.resize:\n            image = cv2.resize(image, self.image_size)\n            \n        if preprocess:\n            image = self.preproc_func(image)\n        \n        return image\n    \n\ndef PetfinderResNet(input_size, num_classes=5):\n    \n    base_model = ResNet50(\n        input_shape=input_size, \n        include_top=False,\n        weights='imagenet')\n    \n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(256)(x)\n    x = PReLU()(x)\n    x = Dense(256)(x)\n    x = PReLU()(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs=base_model.input, outputs=x)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n\ndef PetfinderMobileNet(input_size, num_classes=5):\n    \n    base_model = mobilenet.MobileNet(\n        input_shape=input_size, \n        include_top=False,\n        weights='imagenet')\n    \n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(256)(x)\n    x = PReLU()(x)\n    x = Dense(256)(x)\n    x = PReLU()(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs=base_model.input, outputs=x)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n\ndef PetfinderMobileNetV2(input_size, num_classes=5):\n    \n    base_model = mobilenet_v2MobileNetV2(\n        input_shape=input_size, \n        include_top=False,\n        weights='imagenet')\n    \n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(256)(x)\n    x = PReLU()(x)\n    x = Dense(256)(x)\n    x = PReLU()(x)\n    x = Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs=base_model.input, outputs=x)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f09830ea50e06653f7e3ab512bbab83e5552c6c"},"cell_type":"code","source":"# Debug parameter for training only on subset of data \n# for quick experiments\nDEBUG = True\n\n\n# Unique IDs from train and test:\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif DEBUG:\n    train_pet_ids = train_pet_ids[:32]\n    test_pet_ids = test_pet_ids[:16]\n\n    \nprint(len(train_pet_ids), len(test_pet_ids))\n\n# Merge AdoptionSpeed (target columns) onto images DF:\ntrain_df_imgs = train_df_imgs.merge(train[['PetID', 'AdoptionSpeed']], how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"babba4e3576f71ac6f017b5f56542b8d53163a1b"},"cell_type":"markdown","source":"### train/valid split:"},{"metadata":{"trusted":true,"_uuid":"66f77d318fbac99d037abbb2a615de12e2f4687b"},"cell_type":"code","source":"# Train/valid split based on PetID\ntr_ids, valid_ids = train_test_split(train_pet_ids, test_size=0.2, random_state=1337)\n\ntr_df = train_df_imgs.loc[train_df_imgs.PetID.isin(tr_ids)].reset_index(drop=True)\nvalid_df = train_df_imgs.loc[train_df_imgs.PetID.isin(valid_ids)].reset_index(drop=True)\n\nprint(tr_df.shape, valid_df.shape)\n\nassert len(set(tr_df.PetID.unique()).intersection(valid_df.PetID.unique())) == 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60ca59081eeda008d0701896a367d17040fcf75c"},"cell_type":"markdown","source":"### training setup:"},{"metadata":{"trusted":true,"_uuid":"eabb5100d4d634c9ee690da9d423dee0bfaab4fc"},"cell_type":"code","source":"# Cropping borders:\nimage_crop_pad = 64\n# Batch size for model training:\nbatch_size = 64\n\n# Image size for training:\nimage_size = (128, 128)\ninput_size = image_size + (3,)\nprint(image_size, input_size)\n\n# Choose which model to use\nmodel_touse = 'MobileNet'\nassert model_touse in ['MobileNet', 'MobileNetV2', 'ResNet50']\n\n# Choose proper preprocessing function for model\nif model_touse == 'MobileNet':\n    preproc_func = mobilenet.preprocess_input\nelif model_touse == 'MobileNetV2':\n    preproc_func = mobilenet_v2.preprocess_input\nelif model_touse == 'ResNet50':\n    preproc_func = preprocess_input\n    \n\ntrain_steps_per_epoch = tr_df.shape[0] // batch_size\n\n\n# Define augmentations for training & validation:\ntrain_aug = A.Compose([\n    A.HorizontalFlip(),\n    A.RandomRotate90(),\n    # First resize to retain some variability in random crops\n    # but avoid situations where pet is completely removed from the crop\n    A.Resize(image_size[0] + image_crop_pad,\n             image_size[1] + image_crop_pad),\n    A.RandomScale(0.25),\n    A.RandomCrop(image_size[0], image_size[1])\n])\n\nvalid_aug = A.Compose([\n    # Do the same for valid\n    A.Resize(image_size[0] + image_crop_pad,\n             image_size[1] + image_crop_pad),\n    A.CenterCrop(image_size[0], image_size[1])\n])\n\n\n# Initialize training data generator:\nfilenames_tr = tr_df.image_filename.values\ny_tr = tr_df.AdoptionSpeed.values\ny_tr = to_categorical(y_tr)\nprint('train set shapes:')\nprint(filenames_tr.shape, y_tr.shape)\n\ntr_parser = PetfinderImageParser(preproc_func, image_size, train_aug)\ntr_datagen = PetfinderDataGenerator(\n    filenames_tr, y_tr, tr_parser, \n    image_size=image_size,\n    batch_size=batch_size)\n\n\n# Initialize validation data generator:\nfilenames_valid = valid_df.image_filename.values\ny_valid = valid_df.AdoptionSpeed.values\ny_valid = to_categorical(y_valid)\nprint('valid set shapes:')\nprint(filenames_valid.shape, y_valid.shape)\n\nvalid_parser = PetfinderImageParser(preproc_func, image_size, valid_aug)\nvalid_datagen = PetfinderDataGenerator(\n    filenames_valid, y_valid, valid_parser, \n    image_size=image_size, \n    batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e960ff11d589ee0b28dfa81b0419275a92d7061a"},"cell_type":"markdown","source":"### inspect training samples:"},{"metadata":{"trusted":true,"_uuid":"8ce1de779357922f87f86d91b6b931b831d526ed"},"cell_type":"code","source":"plt.style.use('default')\nplt.rcParams['figure.figsize'] = (16, 12)\n\n\nN_COLS = 3\nN_ROWS = 3\n\nfig, ax = plt.subplots(N_COLS, N_ROWS)\n\nfor c in range(N_COLS):\n    for r in range(N_ROWS):\n        ridx = np.random.randint(0, len(tr_df))\n        img_row = tr_df.iloc[ridx, :]\n        img_filename = img_row['image_filename']\n        pet_id = img_row['PetID']\n        pet_label = img_row['AdoptionSpeed']\n        \n        # Explicitly set preprocess to False for image inspection\n        # Should be set to True (it is by default) for model training\n        image = tr_parser.load_image(img_filename, preprocess=False)\n        ax[c, r].imshow(image)\n        ax[c, r].set_title('ID: {}, label: {}'.format(pet_id, pet_label), size=12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed92320d1f5719faf58339a36ec3fcb41f56152d"},"cell_type":"markdown","source":"### inspect validation samples:"},{"metadata":{"trusted":true,"_uuid":"0622cf98f6a2f4ae7934127e627830017922e272"},"cell_type":"code","source":"plt.style.use('default')\nplt.rcParams['figure.figsize'] = (16, 12)\n\n\nN_COLS = 3\nN_ROWS = 3\n\nfig, ax = plt.subplots(N_COLS, N_ROWS)\n\nfor c in range(N_COLS):\n    for r in range(N_ROWS):\n        ridx = np.random.randint(0, len(valid_df))\n        img_row = valid_df.iloc[ridx, :]\n        img_filename = img_row['image_filename']\n        pet_id = img_row['PetID']\n        pet_label = img_row['AdoptionSpeed']\n        \n        image = valid_parser.load_image(img_filename, preprocess=False)\n        ax[c, r].imshow(image)\n        ax[c, r].set_title('ID: {}, label: {}'.format(pet_id, pet_label), size=12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64c7936aca6fc0144da000cee059496b2853236a"},"cell_type":"markdown","source":"### inspect datagen output:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"79b7ec098f95e4c2d432ffb0a69c59c0e64744e2"},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10, 10)\n\n# train datagen:\nX_tr_temp, y_tr_temp = tr_datagen.__getitem__(0)\n\n# valid datagen:\nX_valid_temp, y_valid_temp = valid_datagen.__getitem__(0)\n\n\nfig, ax = plt.subplots(1, 2)\n\nax[0].imshow(X_tr_temp[0])\nax[0].set_title('train:')\n\nax[1].imshow(X_valid_temp[0])\nax[1].set_title('valid:')\n\n# Here we can see that when loaded for training by default images are preprocessed with ResNet function.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01e7a6af4853021da7ce1e45f99177283defd593"},"cell_type":"markdown","source":"### initialize model:"},{"metadata":{"trusted":true,"_uuid":"33e81912d49bddc1dd00df20377dea3e17b3b863"},"cell_type":"code","source":"if model_touse == 'MobileNet':\n    model = PetfinderMobileNet(input_size)\nelif model_touse == 'MobileNetV2':\n    model = PetfinderMobileNetV2(input_size)\nelif model_touse == 'ResNet50':\n    model = PetfinderResNet(input_size)\n    \nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e3ce21137773d3934c4c336d5f0899a7fdbdffd"},"cell_type":"markdown","source":"### train model:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f13b55341a5d12236f5b201f5d64ec3df3f444d6"},"cell_type":"code","source":"model_checkpoint = ModelCheckpoint(\n    'petfinder_cnn.h5' ,monitor='val_loss', mode='min',\n    save_best_only=True, save_weights_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    mode='min',\n    factor=0.5, \n    patience=5, \n    min_lr=0.0001, \n    verbose=1)\n\n\nmodel.fit_generator(\n    generator=tr_datagen,\n    steps_per_epoch=train_steps_per_epoch,\n    validation_data=valid_datagen,\n    epochs=1,\n    use_multiprocessing=True,\n    workers=2,\n    callbacks=[model_checkpoint, reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a68c951b1df5b1d8ae31f777086e283147dd9af"},"cell_type":"markdown","source":"## What now?\n\n- To train on full dataset set `DEBUG = False`, for quicker experiments set it to `True`\n- Try training a model longer to see if it will be able to achieve good classification performance\n- Train model using KFold and extract OOF predictions, then use it as LGBM feature\n- Try a NN which combines DF features with training on images\n\n### EDIT:\n\nDue to kernels problems, DEBUG setting has been set to just a few batches, because the training is very slow.\nI don't know if it's issue with loading from disk in the kernels of improper GPU utilization."},{"metadata":{"trusted":false,"_uuid":"4fff26fdb0f0ffdd0905f1d05e812f772f429e3c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}