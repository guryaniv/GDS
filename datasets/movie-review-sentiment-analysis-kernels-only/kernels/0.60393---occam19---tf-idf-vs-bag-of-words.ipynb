{"cells":[{"metadata":{"_uuid":"060f20b63f42a76892a1bd359b399ad9b30f2b47"},"cell_type":"markdown","source":"# Rotton Tomatoes Movie Review Sentiment Analysis\n### How useful is TF-IDF weighing in sentiment analysis?\n\nThis kernel works with data from the Movie Review Sentiment Analysis Competition on Kaggle.com. More info can be found at https://nlp.stanford.edu/sentiment/.\n\nIn the report created by the Stanford research group, each sentence is seperated into it's component pieces and given a positive/negative scoring. This is summed up as a word tree to give each sentence an overall sentiment.\n<img src=\"Capture.png\">\n*Image taken from nlp.stanford.edu*\n\nThe objective of this kernel is to explore using tf-idf weighting factors to assist traditional classifier models. Then we will compare these models to simple 'bag of words' text analysis options. Is TF-IDF underutilized in sentiment analysis?"},{"metadata":{"_uuid":"43f4b1e08e52031ee627381cee0b0bccc45ca7ce"},"cell_type":"markdown","source":"## Step 1: Reading SA data"},{"metadata":{"trusted":true,"_uuid":"6b4791827f3b0aacbc468c95d0223a4762b8e775"},"cell_type":"code","source":"import csv\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split   \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5d9eeeab57236e9a823d4fb9b2faa9bef2dc92c"},"cell_type":"code","source":"train = pd.read_csv('../input/train.tsv', sep='\\t')\ntest = pd.read_csv('../input/test.tsv',  sep='\\t')\nsampleSub = pd.read_csv('../input/sampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fa6c6931960a727318cfd33f4b874d5496ce66d"},"cell_type":"markdown","source":"## Step 2: Analysing data\n\n### Dataframe analysis"},{"metadata":{"trusted":true,"_uuid":"f4176cfc28bf4f9838390c162d1aa96027f8c17d"},"cell_type":"code","source":"print(train.shape, \"\\n\", \n      test.shape\n     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90ca609b5ca5159a45507e1dd1c7eab827dcc04b"},"cell_type":"code","source":"print (train.isnull().values.any(), \"\\n\",\n      test.isnull().values.any()\n      )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebdcee42ef769119835a50519cc6ffdae2958b77"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56fb6b863f0a59578c2b544994b0e1817a3b0818"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"481b6021c154003aae52320843615509602c0f4c"},"cell_type":"markdown","source":"Number of unique sentences in the training / testing dataset"},{"metadata":{"trusted":true,"_uuid":"bbb979806b5069ae4ef871735473aac079c3c902"},"cell_type":"code","source":"len(train.groupby('SentenceId').nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7b60cb862564cc4e6baeaa6a49ce58015ae3406"},"cell_type":"code","source":"len(test.groupby('SentenceId').nunique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43989d858c37027a4771e42b18ed6f4f99c5e111"},"cell_type":"markdown","source":"Let's create a dataset with only full sentences. Exploring data this way will gives us cleaner graphs that aren't biased toward longer sentences. We can also add a label for the sentiment value to increased readability."},{"metadata":{"trusted":true,"_uuid":"7a0b350fe65f994366a125f21a6d4fd9ad4614c7"},"cell_type":"code","source":"#Create df of full sentences\nfullSent = train.loc[train.groupby('SentenceId')['PhraseId'].idxmin()]\n\n#Change sentiment to increase readability\nfullSent['sentiment_label'] = ''\nSentiment_Label = ['Negative', 'Somewhat Negative', \n                  'Neutral', 'Somewhat Positive', 'Positive']\nfor sent, label in enumerate(Sentiment_Label):\n    fullSent.loc[train.Sentiment == sent, 'sentiment_label'] = label\n    \nfullSent.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dc32c94be48ab2dc2d752857790476e78ff104a"},"cell_type":"markdown","source":"## NLP Analysis\n\nIn natural language processing, the gold standard of text analysis is TF-IDF analysis. TF-IDF is short for \"term frequency–inverse document frequency\", and it focus on finding the 'valuable' classification words for each document. Term frequency of a word in a document increases it's score, and document frequency (the number of documents where the word shows up) decreases the score.\n\nScikit-learn provides a TfidfVectorizer class, which has the ability to remove common stop words (like a, the, and, or). I also created a CountVectorizer class object for comparison in the evaluation section below. Below we'll try to find the most 'valuable words' for each sentiment group. "},{"metadata":{"trusted":true,"_uuid":"951b6b65447dc70483a32479c651e654f43a165d"},"cell_type":"code","source":"#Add non-helpful stopwords to stopword list\nStopwords = list(ENGLISH_STOP_WORDS)\nStopwords.extend(['movie','movies','film','nt','rrb','lrb',\n                      'make','work','like','story','time','little'])\n\n#Create tfidf vectorizer object & fit to full sentence training data\ntfidf_vectorizor = TfidfVectorizer(min_df=5, \n                             max_df=0.5,\n                             analyzer='word',\n                             strip_accents='unicode',\n                             ngram_range=(1, 3),\n                             sublinear_tf=True, \n                             smooth_idf=True,\n                             use_idf=True,\n                             stop_words=Stopwords)\n\ntfidf_vectorizor.fit(list(fullSent['Phrase']))\n\n\n#Create bag of word vectorizer for comparison in evaluation section\nBoW_vectorizer = CountVectorizer(strip_accents='unicode',\n                                 stop_words=Stopwords,\n                                 ngram_range=(1,3),\n                                 analyzer='word',\n                                 min_df=5,\n                                 max_df=0.5)\n\nBoW_vectorizer.fit(list(fullSent['Phrase']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"716d7cab3e03fc3e41b3b653c75d64dbe3bc255c"},"cell_type":"code","source":"#functions to create graphics below from tf-idf matrices\n#adapted from : https://buhrmann.github.io/tfidf-analysis.html\ndef top_tfidf_feats(row, features, top_n=20):\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ndef top_feats_in_doc(Xtr, features, row_id, top_n=20):\n    row = np.squeeze(Xtr[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)\n\ndef top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=10):\n    if grp_ids:\n        D = Xtr[grp_ids].toarray()\n    else:\n        D = Xtr.toarray()\n\n    D[D < min_tfidf] = 0\n    tfidf_means = np.mean(D, axis=0)\n    return top_tfidf_feats(tfidf_means, features, top_n)\n\ndef top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=16):\n    dfs = []\n    labels = np.unique(y)\n    for label in labels:\n        ids = np.where(y==label)\n        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n        feats_df.label = label\n        dfs.append(feats_df)\n    return dfs\n\ndef plot_tfidf_classfeats_h(dfs, num_class=9):\n    fig = plt.figure(figsize=(12, 100), facecolor=\"w\")\n    x = np.arange(len(dfs[0]))\n    for i, df in enumerate(dfs):\n        ax = fig.add_subplot(num_class, 1, i+1)\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        ax.set_frame_on(False)\n        ax.get_xaxis().tick_bottom()\n        ax.get_yaxis().tick_left()\n        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=16)\n        ax.set_ylabel(\"Word\", labelpad=16, fontsize=16)\n        ax.set_title(str(df.label) + ' Sentiment Class', fontsize=25)\n        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n        ax.barh(x, df.tfidf, align='center')\n        ax.set_yticks(x)\n        ax.set_ylim([-1, x[-1]+1])\n        ax.invert_yaxis()\n        yticks = ax.set_yticklabels(df.feature)\n        \n        for tick in ax.yaxis.get_major_ticks():\n                tick.label.set_fontsize(20) \n        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99cb46200fb351d911a86426e2098a919b04cbc7"},"cell_type":"markdown","source":"The graphs below shows the words with the highest tf-idf score in each class. The matplotlib code is adapted from https://buhrmann.github.io/tfidf-analysis.html, a great analysis of what tf-idf scores can show you about a set of documents in a corpus. Note we use the full sentence dataset created above."},{"metadata":{"trusted":true,"_uuid":"bdb5bfa2c34156ccc28de5bed4f75afb7772251d","scrolled":false},"cell_type":"code","source":"class_Xtr = tfidf_vectorizor.transform(fullSent['Phrase'])\nclass_y = fullSent['sentiment_label']\nclass_features = tfidf_vectorizor.get_feature_names()\nclass_top_dfs = top_feats_by_class(class_Xtr, class_y, class_features)\nplot_tfidf_classfeats_h(class_top_dfs, 7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef5ab01991c5393a37d84dcf0512169074fe0c49"},"cell_type":"markdown","source":"## Part 3: Sentiment Analysis using Machine Learning Models"},{"metadata":{"_uuid":"106436f6eadf09c47fb4c7eb20fbaf3ab3ed3ae0"},"cell_type":"markdown","source":"### Train/Test Splitting"},{"metadata":{"trusted":true,"_uuid":"2184af165a4ec971c5c23d37eea98e23e24ed267"},"cell_type":"code","source":"phrase = np.array(train['Phrase'])\nsentiment = np.array(train['Sentiment'])\n# build train and test datasets\nphrase_train, phrase_test, sentiment_train, sentiment_test = train_test_split(phrase, \n                                                                              sentiment, \n                                                                              test_size=0.2, \n                                                                              random_state=4)\n\n#TF-IDF\ntrain_tfidfmatrix = tfidf_vectorizor.fit_transform(phrase_train)\ntest_tfidfmatrix = tfidf_vectorizor.transform(phrase_test)\n\n#Vectorizer (Bag of Words Model)\ntrain_simplevector = BoW_vectorizer.transform(phrase_train)\ntest_simplevector = BoW_vectorizer.transform(phrase_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae8fd12aae0568dfe641485d7fd7b0c8aa366aeb"},"cell_type":"code","source":"def train_model_predict (classifier, train_features, train_labels,\n                      test_features):\n    classifier.fit(train_features, train_labels)\n    predictions = classifier.predict(test_features)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b23ecc60c93a01b865434141f8389fed91410a1"},"cell_type":"markdown","source":"### Naive Bayes Model"},{"metadata":{"_uuid":"d4dbe872fc69cbe7f886df55f6e20398d9fa1987"},"cell_type":"markdown","source":"#### TF-IDF vectorization"},{"metadata":{"trusted":true,"_uuid":"0a7e661552f920d746c0f9f76616978d0c3924e9"},"cell_type":"code","source":"model1 = MultinomialNB() \nNBPredictions = train_model_predict(model1, train_tfidfmatrix, sentiment_train,\n                             test_tfidfmatrix)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1a49f85922150a46f58780eeaf077996cdec75f"},"cell_type":"markdown","source":"#### 'Bag of Words' vectorization"},{"metadata":{"trusted":true,"_uuid":"1f6cfc0de53462023964ae702d2e0b7ebebf178d"},"cell_type":"code","source":"NBPredictions2 = train_model_predict(model1, train_simplevector, sentiment_train,\n                             test_simplevector)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23cd6fca45200cb0d37d1d56e58fd56a56df8f5e"},"cell_type":"markdown","source":"### Logistic Regression Model"},{"metadata":{"_uuid":"f97f210e2765dd4f6fb6b87b09d4524cbc591bcc"},"cell_type":"markdown","source":"#### TF-IDF vectorization"},{"metadata":{"trusted":true,"_uuid":"beea923dd42f1f26bb31d580206e914a6d62fdf6"},"cell_type":"code","source":"model2 = LogisticRegression(solver = 'liblinear', multi_class = 'ovr')\nLogisticRegressionPredictions = train_model_predict(model2, train_tfidfmatrix, sentiment_train,\n                             test_tfidfmatrix)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f807b7c61ec2c1149c86f3798b2362f3136b988"},"cell_type":"markdown","source":"#### 'Bag of Words' vectorization"},{"metadata":{"trusted":true,"_uuid":"7404434dc081077e2be966d0d5be0b30fb10641d"},"cell_type":"code","source":"LogisticRegressionPredictions2 = train_model_predict(model2, train_simplevector, sentiment_train,\n                             test_simplevector)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eb67d639091a18d6ced1ff695aee8540a0705a2"},"cell_type":"markdown","source":"## Part 4 : Evaluation"},{"metadata":{"_uuid":"7d4ef24daac23d3c4f5e32ed34739108e76f457f"},"cell_type":"markdown","source":"Since we don't care about certain types of scoring over others (ie weighing mislabeling more than correctly labeling), we'll use F1 scores as shorthand for how good the model is."},{"metadata":{"trusted":true,"_uuid":"57950e72c8fe3287c68d745e2f3a272d47450954"},"cell_type":"code","source":"def get_metrics(true_labels, predicted_labels, feature):  \n    print(feature)\n    print('Accuracy:', np.round(metrics.accuracy_score(true_labels, \n                                               predicted_labels), 4))\n    print('Precision:', np.round(metrics.precision_score(true_labels, \n                                               predicted_labels,\n                                               average='weighted'), 4))\n    print('Recall:', np.round(metrics.recall_score(true_labels, \n                                               predicted_labels,\n                                               average='weighted'), 4))\n    print('F1 Score:', np.round(metrics.f1_score(true_labels, \n                                               predicted_labels,\n                                               average='weighted'), 4))\n    print('\\n')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bc991f2dbd650d39757cf78861e7e52546052b3","scrolled":true},"cell_type":"code","source":"get_metrics(NBPredictions, sentiment_test, 'Naive Bayes & TF-IDF Scores: ')\nget_metrics(NBPredictions2, sentiment_test, 'Naive Bayes & Bag of Words Scores: ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1d7456df4d976277ba1244510569df574aa731b"},"cell_type":"code","source":"get_metrics(LogisticRegressionPredictions, sentiment_test, 'Logistic Regression & TF-IDF Scores: ')\nget_metrics(LogisticRegressionPredictions2, sentiment_test, 'Logistic Regression & Bag of Words Scores: ')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23f02f086d4508c0c4fa02f1845225bff5decb03"},"cell_type":"markdown","source":"### Table of F1 scores of different modelling methods\n\n|            |NaiveBayes|LogReg|\n|------------|----------|------|\n|   TF-IDF   | 0.6577   |0.6615|\n|Bag of Words| 0.6386   |0.6485|"},{"metadata":{"_uuid":"c58ad9757d8b5ea91b679b0c40cfa84d3ac58cd5"},"cell_type":"markdown","source":"## Part 5: Conclusion"},{"metadata":{"_uuid":"c14fe53be31fb8424b6eeea02d7af6aad1ea3507"},"cell_type":"markdown","source":"TF-IDF weighing does have a positive effect on the correctness of the model. The difference between a bag-of-words weighing schema and TF-IDF schema is slight but consistent between both classifiers.\n\nWe could also run a GridSearch to find the most correct parameters for each model, though we run the risk of overfitting for the Kaggle test data. Future testing can be done on different models and more or less optimizeed ones - including nerual networks.\n\nReferences and sources used:\n\nhttps://buhrmann.github.io/tfidf-analysis.html\nhttp://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\nhttps://www.dataquest.io/blog/naive-bayes-tutorial/"},{"metadata":{"_uuid":"ddc5e8f1751cca4cdbf273bacc4d8899957c2417"},"cell_type":"markdown","source":"#### Application to competition test set"},{"metadata":{"trusted":true,"_uuid":"182c307f6a0917dd74fbd8de2be994cdff210b93","_kg_hide-output":true},"cell_type":"code","source":"train_tfidf = tfidf_vectorizor.fit_transform(train['Phrase'])\nmodel2.fit(train_tfidf, train['Sentiment'])\ntest_tfidf = tfidf_vectorizor.transform(test['Phrase'])\npredictions = model2.predict(test_tfidf)\n\ntest['Sentiment'] = predictions\nsubmission = test[['PhraseId','Sentiment']]\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}